{"id": "35553327", "url": "https://en.wikipedia.org/wiki?curid=35553327", "title": "Aaeon", "text": "Aaeon\n\nAAEON Technology Inc. was first founded in 1992 in Taiwan. and has expanded globally since, establishing offices in United States, China, Singapore, Germany and the Netherlands. AAEON manufactures and markets a wide range of OEM/ODM industrial PCs worldwide. Its product lines include Embedded Boards & Computer On Module, Applied Computing & Network Appliances, All-in-One HMI Systems & Displays, Digital Signage & Self-Service Kiosk as well as Cloud Computing. In collaboration with manufacturers and vendors such as Seagate, Blue Chip Technology and Sequans, Many of its products are applied in industries such as Machine and Factory Automation, Chemical, Medical, Finance, Education, Transportation. In addition, Digital Signage and Kiosk applications are more prevalent in recent years.\n\nHeadquartered in Taipei, Taiwan, AAEON has obtained many certifications throughout the years. It received the ISO-9001 certification and the ISO-14001 certification in 1994 and 1997, respectively. AAEON was awarded the TL9000 certification in 2002 and the ISO 13485 Medical certification in 2007. In addition to numerous awards from Intel and Siemens, they have been the recipient of the Taiwan Symbol of Excellence Award for the past 6 years. In 2000, AAEON acquired Astech Technology, Inc as part of its Panel PC Division. AAEON became a public limited company on the Taiwan Stock Exchange in 2001 under TAIEX: 2463. AAEON was removed from the TSE after joining AsusTek Computer Inc. (ASUS) in 2011.\n"}
{"id": "25584223", "url": "https://en.wikipedia.org/wiki?curid=25584223", "title": "Adolphe Clément-Bayard", "text": "Adolphe Clément-Bayard\n\nGustave Adolphe Clément from 1909 Clément-Bayard (22 September 1855 – 10 March 1928) was a French entrepreneur. An orphan who became a blacksmith and a \"Compagnon du Tour de France\", he went on to race and manufacture bicycles, pneumatic tyres, motorcycles, automobiles, aeroplanes and airships.\n\nIn 1894 he was a passenger in the winning vehicle in the world's first competitive motor event. Albert Lemaître's Peugeot was judged to be the winner of the Paris–Rouen \"Competition for Horeseless Carriages (Concours des Voitures sans Chevaux)\".\n\nAs a result of selling the manufacturing rights to his \"Clément\" car he added Bayard to the name of his new business. The company name honoured the Chevalier Pierre Terrail, seigneur de Bayard who saved the company's town of Mézières from an Imperial army during the Siege of Mézières in 1521.\n\nIn 1909, five years after the successful launch of the \"Clément-Bayard\" automobile brand, he applied for and obtained the consent of the Conseil d'Etat to change his name and those of his descendants to Clément-Bayard. Clément-Bayard was appointed a Commander of the Légion d'honneur in 1912.\n\nMost of his manufacturing empire was destroyed by World War 1, by German ransacking, by conversion to war production for France, and by the subsequent weak economic market. In 1922 the Clément-Bayard company was sold to André Citroën and the factory at Levallois-Perret was the centre of 2CV manufacturing for the next 40 years.\n\nAdolphe Clément, the son of a grocer, was born at rue du Bourg, Pierrefonds, Oise. He was the second of five children of Leopold Adolphus Clément and Julie Alexandrine Rousselle. His mother died when he was seven years old and although his father remarried he also died 2 years later when Adolphe was nine years old. For the next seven years he was raised by his stepmother who had remarried a school teacher. Adolphe studied at the primary school in Pierrefonds and then at the College of Villers-Cotterêts. He worked in the family business by delivering groceries, and at 13 chose to be apprenticed to a farrier/blacksmith.\n\nDuring the winter of 1871–1872, the 16-year-old Adolphe left Pierrefonds to travel around France as a \"Compagnon du Tour de France\", an organization of craftsmen and artisans dating from the Middle Ages. He had saved 30 francs (circa 100 Euros in 2006) by doing multiple jobs for three years. He subsisted in each city by working in forges owned by the Compagnons du Tour de France, shoeing horses, repairing metal and doing any kind of work. He reached Paris in 1872 followed by Orléans and Tours where he encountered 'Truffault cycles'. This led him to acquire 2 wooden cart wheels and build an iron bicycle frame.\n\nCycle racing had begun in 1869 (Paris–Rouen), so in 1873 Truffaut lent the 18-year-old Clément an iron bicycle with solid rubber tires to race in Angers. He finished 6th and was exhilarated to read his name in the newspapers.\n\nAdolphe Clément married Céleste Angèle Roguet and they had four children, Albert, Angèle, Jeanne and Maurice. Albert died while racing at the 1907 French Grand Prix. Angèle (1880–1972) was widowed from Albert Dumont, an engineer and director at the Levallois factory. Angele then remarried Numa Joseph Edouard \"Petit\" Sasias (1882–1927), a 'Fonctionnaire aux Affaires Etrangères, ex-Secrétaire à la Présidence du Conseil, with whom she had one son. Jeanne became divorced from Fernand Charron, racing driver and manager of the plant at Levallois-Perret, subsequently living alone. Maurice married Renée Hammond and had three children Andrée, Jacqueline and Albert (nicknamed \"Billy\" to avoid confusion and memories of his uncle Albert).\n\nThe \"Domaine Bois D'Aucourt\" in Pierrefonds was originally a 17th-century hunting lodge of the \"Sun King\" Louis XIV which had been upgraded circa 1822. Located west of both the Château de Pierrefonds and his own birthplace on the rue du Bourg, Adolphe Clément bought the property around 1904 and employed architect Edward Redont to renovate and remodel it.\n\nLatterly the mansion 'Domaine du Bois d'Aucourt' at Pierrefonds was used by his son Maurice, while Adolphe continued living at 35 Avenue du Bois de Boulogne, Neuilly-sur-Seine.\n\nBy 1893 Clément owned the \"Vélodrome de la Seine\" near the site of the factory at Levallois-Perret. \"La plus belle et la plus vite piste du monde\"\". It was managed by Tristan Bernard who also managed the Vélodrome Buffalo, and its events were an integral part of Parisian life, being regularly attended by personalities such as Toulouse-Lautrec. Clément reportedly sold or converted this around 1900.\n\nOn achieving business success he used the Latinate format of his name, \"Gustavus Adolphus\", and later (circa 1909) received permission from the Conseil d'État to change his surname to \"Clément-Bayard\".\n\nThe death of his son Albert while racing at the 1907 French Grand Prix had a lasting effect on him. In 1913 he was elected as mayor of Pierrefonds and, on taking office he ceded control in 1914 of Clément-Bayard to his son Maurice who was passionate about aviation.\n\nIn 1928 he died of a heart attack while driving along the rue Laffitte to a meeting of a 'Board of Directors' in Paris.\n\nIn 1876, after 2 years of cycle racing, working and saving, Adolphe had enough money to start in business, so he opened a bicycle repair works in Bordeaux, aged 21. The next stage of his business plan was to move to Marseille where he learned how to manufacture steel tubes for bicycles. The following year he moved to Lyon and began manufacturing whole bicycles under the name 'Clément SA cycles'.\n\nThe following year, circa 1878, he moved to Paris and opened a cycle business, \"A. Clément & Cie\", at 20 Rue Brunel near the Place de l'Etoile. Here he also ran a cycling school and was competing in cycle races. In Paris his business backers were monsieur de Graffenried and monsieur de Montgeron.\n\nAt the end of 1878 Adolphe partnered the cycling champion Charles Terront at the 'Six-Days' cycling event at the Agricultural Hall in London. He also opened a sales showroom at 31 rue 'du 4-September', in Paris and started a poster advertising campaign, a new concept.\n\nIn September 1879, Clément built an iron smelter in Tulle, in the Limousin where there was a good supply of water power, but he did not have sufficient finance to make it viable and Tulle was too remote from Paris, so he had to sell the plant.\n\nBy 1880 the \"Clément\" cycle manufacturing business at Rue Brunel, had circa 150 employees building bicycles.(\"Image and description of 1880 Clément cycle\") The machines were regarded as high quality and by 1890 \"Clément\" was the leading cycle brand in France.\n\nThe Gladiator Cycle Company, a French bicycle manufacturer, was founded by Alexandre Darracq and Paul Aucoq in 1891 at Le Pré-Saint-Gervais in northeast Paris. Adolphe Clément was a major investor in this venture.\n\nIn 1895 Gladiator introduced its first internal combustion, a naphtha powered tricycle.\n\nIn 1896 Adolphe Clément who held the extremely profitable manufacturing rights for Dunlop tyres in France joined with a syndicate led by Dunlop's founder Harvey Du Cros to buy out the Gladiator Cycle Company and they merged it into a major bicycle manufacturing conglomerate of Clément, Gladiator & Humber & Co Limited valued at 22 million francs (circa €60–80 million Euro in 2006).. The range of cycles was expanded with tricycles, quadricycles, and in 1902 a motorised bicycle, then cars and motorcycles.\n\nShortly after the purchase of Gladiator cycles in 1896, Adolphe Clément began to build the new factory at Levallois-Perret in northwest Paris, which also produced various cars from 1898 (\"see below\") and went on to build the Citroën 2CV for nearly forty years.\n\nFrom 1895 Clément cycles started to focus on motorized vehicles. In 1895 it introduced its first internal combustion vehicle, a naphtha powered tricycle. In 1902 they offered a motorized bicycle with a 142 cc engine bolted to the frame, using overhead valves and a detachable cylinder head; the inlet valve 'automatic' (controlled by engine suction), the exhaust valve mechanically operated. A coil-and-battery ignition was used, and a two-barrel carburetor controlled by small levers attached to the frame top tube. An external flywheel kept the crankcase very small, and a long belt from the engine pulley to a 'dummy' rim on the rear wheel was tensioned by a small 'jockey' pulley on the seat tube. The front brake pressed direct on the front tire, the rear was a 'coaster' brake activated by back-pedaling. This 'motorisation adaptation' was sold on both Clément and Gladiator cycles.\n\nIn Britain these popular motorised cycles were known as Clément-Garrards. James Lansdowne Norton built Clément bicycle frames under license, and used the Clément clip-on engine for his first Norton motorcycles.\n\nIn 1889 Clément saw a Dunlop pneumatic tyre in London and acquired the French manufacturing rights for 50,000 francs. This success led to his millionaire status. The company he formed with a capital of 700,000 francs paid 100 per cent dividend in its first year of operation. Dunlop France, and occasionally Clément-Dunlop, was a regular entrant of motor racing and cycle racing teams.\n\nClément is reported to have begun manufacture of Clément Tyres in 1878 to fit to the early cycles, but the French identity was lost with the overwhelming success of his Dunlop pneumatics. After World War I, Clément Pneumatics was established in Italy and was a leading supplier of \"Clément Pneumatici\" bicycle tyres throughout much of the 20th century. A leading international manufacturer during the 1950s,1960s and 1970s, it was associated with racing cyclists such as Eddy Merckx, Jacques Anquetil, Felice Gimondi, and Ole Ritter. It was purchased by Pirelli in the 1980s and manufacturing was moved to Thailand until 1995 when Pirelli vacated the bicycle tyre market. Various licensing arrangements were of little consequence until, in 2010 the name was licensed to Donnelly Sports and the American, Don Kellogg, who recommenced manufacture in Thailand.\n\nBy 1898 the new Clément-Gladiator company was building cars and marketing them as both Cléments and Gladiators. Gladiators were imported into England by the Motor Power Company, which was co-owned by S. F. Edge and Harvey du Cros founder of Dunlop. Financed by Harvey du Cros Herbert Austin built Clement-Gladiator cars at Longbridge from early 1906, selling them as Austin cars.\n\nFrom 1901 \"Clément-Gladiator\" cars were built at the Levallois-Perret factory and by 1902 production was over 1,000 cars per annum, 800+ of which were sold in England.\n\nAfter 1903 the Clément-Gladiator name continued to be used on the shaft-drive cars made at the Pre-Saint-Gervais factory, whilst chain-driven vehicles were marketed as Gladiators. The Clément name was dropped in 1907 and in 1909 another French manufacturer, Vinot et Deguingand, took over Gladiator and transferred production to Puteaux. At this time the Pre-Saint-Gervais factory reverted to making bicycles.\n\nIn 1897 Clément invested one million francs (the equivalent of about three 3 million Euros at 2006 valuation) in Panhard & Levassor, part of their five million francs capitalisation. This established the main business and eventually led to the creation of Clément-Panhard marque.\n\nClément-Gladiator was divided in 1903, Charles Chetwynd-Talbot founding the English arm \"Clément-Talbot Ltd\", while Adolphe Clément formed Clément-Bayard on a former military site at Mézières (now Charleville-Mézières). He chose the name Bayard in commemoration of the Chevalier Pierre Terrail, seigneur de Bayard who saved the town of Mézières in 1521. A statue of the Chevalier stood in front of the Mézières factory, and the image was incorporated into the company logo. After the split both marques built very similar cars, but the specifications gradually diverged.\n\nIn 1922 the Clément-Bayard company was sold to André Citroën, in whom Adolphe also invested financially, and the factory at Levallois-Perret was the centre of 2CV manufacturing for the next 40 years.\n\nClément was a director of Panhard-Levassor, and when the factory could not meet the production requirements for circa 500 units of the 1898 'voiture légère' ('dog cart') model, he undertook manufacture under licence at his factory in Levallois-Perret. It was designed by airship pioneer Commandant Arthur Krebs, of Panhard, and used a tubular chassis, centre-pivot steering, near-horizontal rear-mounted engine with automatic inlet valve and hot-tube ignition, driving through a constant-mesh gear-train, and final drive by side chains; early models had no reverse gear.\n\nAround 1902 a series of Clément-Rothschild bodied automobiles, based on the Panhard-Levassor chassis, were produced by Carrosserie Clément-Rothschild at 33 Quai Michelet, Levallois-Perret, either adjacent to or in Adolphe Clement's Levallois-Perret factory. There may have been two Rothschild coach-building enterprises active in Paris at that time, because J. Rothschild & Fils traded from 131 Avenue Malakoff but had been founded by Austrian-born Josef Rothschild in 1838 in Levallois-Perret, and was building automobile coachwork by 1894. By 1896 the business had been purchased by Edmond Rheims and Leon Auscher and pioneered aluminium composite coachwork.\n\nSome Clément-Panhards were exported to Great Britain where they were variously sold as Clément-Stirling and Stirling-Panhard, by the Scottish coachbuilder Stirling.\nAdolphe Clément was a major shareholder in the company, along with Chetwynd-Talbot who was chairman, A. Lucas, and E. Lamberjack both of France. Both marques ( Clément-Bayard and Clément-Talbot) built very similar cars, but by 1907 the specifications diverged.\n\nOn 11 October 1902 Clément-Talbot was formally incorporated, and subsequently of land was purchased for a new factory in Ladbroke Grove, North Kensington in west London, between the Great Western Railway line and the 'Edinburgh road' before it was renamed 'Barlby road'. The factory was a high status operation whose brick workshops used the latest saw-tooth roof line, glazed to maximise natural light. It was equipped with the most modern machine tools and the reception area was laid out like a miniature palace, marble Ionic columns, gilded frescoes and stained glass windows etched with the Shrewsbury coat of arms. The building is now known as Ladbroke Hall.\n\nThe company traded as Clément-Talbot and the factory was titled Clément-Talbot, but after the first year of trading the cars were always known as Talbots.\n\nIn 1905 Adolphe Clément-Bayard created the Diatto-Clément Societa Anonima in partnership with Diatto, who had been coachbuilders in Turin since 1835. The cars, known as \"Torino\"s were built in Turin under licence from Clément. The first cars were the 20-25HP which used 3,770cc four-cylinder engines. These were followed by 10-12HP (1,884cc two-cylinder) and 14-18HP (2,724cc four-cylinder) models. This series was a success and was followed by a six-cylinder model. In 1909 Clément-Bayard left the business and the company was renamed Societa Fonderie Officine Frejus.\n\nIn 1906 Adolphe Clément-Bayard set up the 'Clément Motor Company'in Coventry to build Gladiators under licence. It used the motto \"Simply Clément, nothing else\" to avoid confusion with Clément-Talbots which by then were known only as Talbot. Various sources record that motorcars were manufactured and sold under the \"Clément\" brand between 1907 (1908) and 1914. The company is recorded as Clément Motor company Ltd., Coventry, Warwickshire.\n\nAdolphe Clément-Bayard had no direct involvement in the nascent motor industry until around 1897, but he was a passenger in Albert Lemaître's (Peugeot) that was judged to be the official winner of what is considered to be the world's first motor race on 22 July 1894, from Paris to Rouen.\n\nThe event was a publicity exercise organised by Pierre Giffard of \"Le Petit Journal\" newspaper and consisted of 69 cars starting a selection event before 25 were allowed into the main event, the race from Paris (Porte Maillot) to Rouen. Albert Lemaître completed the course in 9 hours 18 minutes at an average speed of , followed by Auguste Doriot (Peugeot), René Panhard (Panhard) and Émile Levassor (Panhard). Count Jules-Albert de Dion reached Rouen 3’30” ahead of Albert Lemaître but as cars were judged on speed, handling and safety characteristics the official winners were Peugeot and Panhard. De Dion's steam car needed a stoker which was forbidden.\n\nClement was classified 20th in the 1901 Paris–Berlin Trail. Driving Panhard number 18 he completed the event in 21 hours 53 minutes and 38 seconds.\n\nClément-Bayard started building automobiles in 1903 and then started building racing cars in 1904. The racing team included Albert Clément, Jacques Guders, Rene Hanriot, Marc-Philippe Villemain, 'Carlès', \"De la Touloubre\" and A. Villemain, and Pierre Garcets.\n\nAlbert Clément finished 10th at \"L' Eliminatoires Françaises de la Coupe Internationale\", held at the Forest of Argonne on 20 May 1904. This was an eliminating contest for the French entry into the \"Coupe Internationale\" (\"Gordon Bennett Race\") where only three cars were allowed per country. Clement finished in 7 hours 10 minutes 52.8 seconds.\n\nAlbert Clément won the \"II Circuit des Ardennes des Voiturettes\" on 24 July 1904 at Bastogne in 4h 26m 52.6seconds at an average speed of 53.91 km/h.\n\nClément drove his Clément-Bayard into third place at the \"III Circuit des Ardennes\" race at Bastogne, on 25 July 1904.\n\nClément finished second at the 1904 \"I.W.K. Vanderbilt Cup Race\" on Long Island on 8 October 1904.\n\nRene Hanriot finished tenth in 8 hours 23 minutes 39.6s at the \"II Eliminatoires Françaises de la Coupe Internationale\" at the Auvergne on 16 June. This was a qualifier for the \"Coupe Internationale\" (\"Gordon Bennett Race\").\n\nAt the 1905 Vanderbilt cup on Long Island Clément drove an 80-hp Clément-Bayard (France #12) but suffered reliability problems.\n\nClément retired his Clément-Bayard after the first 166 km lap of the \"II Coppa Florio\" at Brescia Italy on 4 September 1905. His team-mate 'Carlès' retired after 2 laps.\n\nClément-Bayard entered 3 cars for the inaugural 1906 French Grand Prix at Le Mans where Albert Clément finished third in his 100Hp machine. He completed the 1,238 km event in 12 hours 49 minutes 46.2seconds. Clément lead the race at the end of laps 2 and 5 on the second day. Punctures were common and Michelin introduced a detachable rim, saving 11 minutes over manually replacing the tyre. This enabled Felice Nazzaro (FIAT) to take second place from Clément.\n\nAlbert Clément finished 6th in the \"V Circuit des Ardennes\" on 13 August 1906 at Bastogne. He completed the 7 lap 961 km race in 6 hours 2 minutes 55.2 seconds in a 100Hp Clément-Bayard. His team-mates A. Villemain and Pierre Garcet finished 11th and 12th.\n\nAt the 1906 Vanderbilt cup Clément finished 4th driving a Clément-Bayard (France #15) and completing the ten laps averaging .\n\nAlbert Clément died while practising for the 1907 French Grand Prix on 17 May. Of the three other Clément-Bayard entries, Pierre Garcet and Elliott Shepard, finished seventh and eighth respectively. Clément's car was entered by 'Alezy' who retired after four laps.\n\nThe company entered 3 cars for the 1908 French Grand Prix, using a 12,963 cc six-cylinder overhead camshaft engine. Victor Rigal finished 4th.\nIn 1905 Clément-Bayard won the \"Coupe de Calais\" and 'finished well' at the \"Course de Dourdan\". In both 1907 and 1908 Clément-Bayard won the \"Coupe de l’Automobile-Club de Cannes', and in 1908 it also won the \"Tour de France Automobile\".\n\nClément-Bayard was an early French manufacturer of aircraft engines and lighter-than-air vehicles, with the earliest flights occurring in 1908. Clément-Bayard created the world's first series production aircraft.\n\nThe company worked with Louis Capazza to produce the 'planeur (glider) Bayard-Clément' that was unveiled in \"L'Aérophile\" on 15 May 1908.\n\nThe company also started working with Alberto Santos-Dumont in 1908 to build his \"Demoiselle No 19\" monoplane that he had designed to compete for the \"Coupe d'Aviation Ernest Archdeacon\" prize from the Aéro-Club de France. The plane was small and stable, but they planned a production run of 100 units, built 50 and sold only 15 for 7,500 francs for each airframe. It was the world's first series production aircraft. By 1909 it was offered with a choice of 3 engines, Clément-Bayard 20 hp; Wright 4-cyl 30 hp (Clément-Bayard had the license to manufacture Wright engines); and Clément-Bayard 40 hp designed by Pierre Clerget. It achieved 120 km/h.\n\nPierre Clerget designed a range of Clément-Bayard aircraft engines including a 7-cylinder supercharged radial, the 4-cyl 40 hp used on the \"Demoiselle\", a 4-cyl 100 hp used on 'Hanriot Etrich' monoplanes, and a V8 200 hp airship engine.\n\nIn 1910 the Clément-Bayard Monoplane No. 1 was introduced at the Paris show.\n\nBy 1912 Clément-Bayard built a biplane plus three different models of horizontally opposed aircraft engines.\n\nIn November 1912 The Clément-Bayard Monoplane No. 5 was introduced. It was powered by a Gnome rotary engine which had 7 cylinders and produced . The pilot sat in an aluminium and leather tub.\n\nIn 1913 a three-seater biplane was introduced as part of the military project, the Clément-Bayard No. 6. It was configured for two observers in front of the pilot, and was powered by either a 4-cyl Clément-Bayard or 4-cylinder Gnome engine.\n\nIn 1914 Clément-Bayard produced a steel scouting monoplane powered by either a motor or a Gnome et Rhône engine. The nickel steel armour was designed for protection against rifle fire.\nIn 1908 'Astra Clément-Bayard' began manufacturing airships at a new factory in La Motte-Breuil in response to a French Army decision to commence airship operations.\n\nThe Clément-Bayard No.1 airship was offered to the French government but was too expensive so it was bought by Tsar Nicholas II for the Russian army.\n\nIn 1910 the \"Clément-Bayard No.2\", piloted by Maurice Clément-Bayard, was the first airship to cross the Channel, travelling over 380 km in 6 hours. The army ordered 3 copies.\n\nThe airship hangar in La Motte-Breuil is still maintained by Clément-Talbot Ltd.\n\nSeven Clément-Bayard airships were completed.\n\n\n\n\n\nBy 1910 Clément-Bayard vociferously warned of Germany's warlike ambitions, and in 1912 he was assaulted by a hostile German mob. Thus,when Germany invaded France he was a marked man. In September 1914 the Germans reached the outskirts of Pierrefonds and shelled the \"Domaine du Bois d'Aucourt\", although by then it was being looked after by Carlo Bugatti, the Art Nouveau furniture and jewellery designer and father of Ettore Bugatti who also lived in the town. Adolphe remained in Paris with his family.\n\nAdolphe ceded control of Clément-Bayard to his son Maurice in 1914 before the start of the war, but the consequences for the company were disastrous. The \"La Macérienne\" factory at Mézières was lost to the Germans in the opening weeks, as were his home, mayoral town and factories at Pierrefonds. The industrial machinery was shipped back to Germany, and the forges, foundries and smelter were destroyed. \"La Macérienne\" was gutted and used as an indoor riding school for German officers.\n\nAutomobile production at Levallois-Perret in Paris was suspended in August 1914 and the factory was turned over to war production, military equipment and military vehicles, aero engines, airships and planes.\n\nIn 1922, Clément-Bayard was appointed director and vice-president of the new Bank of Ardennes, which was established in Charleville on 12 April 1922.\n\nThe Dreyfus affair split France at the end of the 19th century over the guilt or innocence of a soldier, Alfred Dreyfus, who had been convicted of selling secrets to the Germans. In 1900 Clément-Bayard was one of the leading \"anti-Dreyfusard\" industrialists, along with comté Jules-Albert de Dion, who cancelled all advertising in the \"Drefusard\" newspaper \"Le Vélo\" and started a rival daily sports paper, \"L'Auto-Velo\". The roots of both the Tour de France cycle race and L'Équipe newspaper, result from Clément's hostile \"anti-Dreyfusard\" stance. The Dreyfus affair was eventually concluded with the official exoneration of Dreyfus (as an innocent person who had been framed). With the end of official inquiries it may be said that Clément-Bayard and de Dion had been wrong for a decade.\n\nIn 1912 Clément-Bayard was appointed a Commander of the Légion d'honneur.\n\nIn 1928 he died of a heart attack while driving to a meeting of a 'Board of Directors' in Paris.\n\nHis tomb is located at the 'Domaine du Bois d'Aucourt d'Adolphe Clément-Bayard' at Pierrefonds which has been a protected Historic Monument since 2004.\n\nThe rue Clément-Bayard runs through the centre of Pierrefonds, Oise.\n\nIn 2005 a 50 CHF gold coin was minted to commemorate the centenary of the Geneva Motor Show, with the theme \"Clément 1905\"\n\n\na. By 1896 the title of Humber cycles had been acquired by entrepreneur and fraudster Harry Lawson. The cycle factory of Thomas Humber at Beeston, Nottinghamshire started adding the soubriquet 'Genuine Humber' to its logo.\n\n\n"}
{"id": "4857850", "url": "https://en.wikipedia.org/wiki?curid=4857850", "title": "Affective design", "text": "Affective design\n\nThe notion of affective design emerged from the field of human–computer interaction (HCI) and more specifically from the developing area of affective computing. Affective design involves designing interfaces to enable human-computer interactions where emotional information is communicated by the user in a\nnatural and comfortable way - the computer processes the emotional information and may adapt or respond to try to improve the interaction in some way.\n\nAffective computing aims to deliver affective interfaces capable of eliciting certain emotional experiences from users. Similarly, affective design attempts to define the subjective emotional relationships between consumers and products and to explore the affective properties that products intend to communicate through their physical attributes. It aims to deliver artefacts capable of eliciting maximum physio-psychological pleasure consumers may obtain through all of their senses.\n"}
{"id": "42982075", "url": "https://en.wikipedia.org/wiki?curid=42982075", "title": "Agricultural Market Information System", "text": "Agricultural Market Information System\n\nThe Agricultural Market Information System (AMIS) is an inter-agency platform to enhance food market transparency and encourage international policy coordination in times of crisis. It was established at the request of the Group of Twenty (G20) in 2011. Countries participating in AMIS encompass the main producing and consuming countries of major food crops covered by the initiative: wheat, maize, rice and soybeans. AMIS is hosted by the Food and Agriculture Organization of the United Nations (FAO) in Rome/Italy and supported by a joint Secretariat, which currently (September 2016) consists of eleven international organizations and entities. Apart from FAO, these are the Group on Earth Observations Global Agricultural Monitoring (GEOGLAM) initiative, the International Fund for Agricultural Development (IFAD), the International Food Policy Research Institute (IFPRI), the International Grains Council (IGC), the Organisation for Economic Co-operation and Development (OECD), the World Food Program (WFP), the World Trade Organization (WTO), the United Nations Conference on Trade and Development (UNCTAD), the United Nations High-Level Task Force on the Global Food Security Crisis (UN-HLTF), and the World Bank.\n\nAMIS was created as a tool to address excessive food price volatility and to strengthen global food security in a period of heightened insecurity in international food markets. Its creation is thus intrinsically linked to the two consecutive price hikes that occurred in 2007/08 and 2010.\n\nAfter the 2007-08 world food price crisis led to social unrest in a number of countries and drastically worsened the food security situation, the world experienced another food price shock in the summer of 2010 when the Russian Federation announced an export ban on wheat in response to a severe drought and wildfires that threatened much of the country’s crop.\n\nUnder the auspices of its Intergovernmental Groups on Grains and Rice, FAO invited all its members to Rome for an extraordinary meeting in September 2010 to discuss the troubled market conditions and to stimulate a coordinated response. While the event failed to yield any immediate results, it can be credited for triggering constructive discussions that eventually led to the creation of AMIS. The meeting acknowledged that unexpected price hikes and volatility were “amongst major threats to food security and that their root causes need to be addressed.” In particular it recognized “the lack of reliable and up-to-date information on crop supply and demand and export availability” as well as “insufficient market transparency at all levels including in relation to futures markets” among the main drivers of the most recent disturbances in world food markets. It further emphasized the need “to enhance market information and transparency”, calling for improved “monitoring of planting intentions, crop development and domestic market information.”\n\nThese ideas were taken up during the G20 Summit in Seoul in November 2010, which asked a number of international institutions to identify the best ways to manage and mitigate risks of food price volatility without distorting markets. The ensuing report was presented to the French Presidency of the G20 in June 2011, concluding with a list of ten recommendations, among which to establish AMIS. In the final declaration of the G20 Summit in Cannes, heads of state and government of the G20 countries stressed the importance of improving \"market information and transparency in order to make international markets for agricultural commodities more effective.\" In order to address these challenges, they decided to launch AMIS that was officially inaugurated in September 2011.\n\nParticipants in AMIS include G20 countries plus Spain and seven additional major exporting and importing countries of the AMIS crops. These are: Egypt, Kazakhstan, Nigeria, the Philippines, Thailand, Ukraine, and Vietnam. G20 members are Argentina, Australia, Brazil, Canada, China, European Union, France, Germany, India, Indonesia, Italy, Korea, Japan, Mexico, Russian Federation, Saudi Arabia, South Africa, Turkey, United Kingdom, and the United States.\n\nAccording to the Terms of Reference that established AMIS, the following objectives are central:\n\nAMIS consists of three main bodies:\n"}
{"id": "1183272", "url": "https://en.wikipedia.org/wiki?curid=1183272", "title": "Alan J. Heeger", "text": "Alan J. Heeger\n\nAlan Jay Heeger (born January 22, 1936) is an American physicist, academic and Nobel Prize laureate in chemistry.\n\nHeeger was born in Sioux City, Iowa, to a Jewish family. He grew up in Akron, Iowa, where his father owned a general store. At age nine, following his father's death, the family moved to Sioux City.\n\nHeeger earned a B.S. in physics and mathematics from the University of Nebraska-Lincoln in 1957, and a Ph.D in physics from the University of California, Berkeley in 1961. From 1962 to 1982 he was on the faculty of the University of Pennsylvania. In 1982 he commenced his present appointment as a professor in the Physics Department and the Materials Department at the University of California, Santa Barbara. His research has led to the formation of numerous start-up companies including Uniax, Konarka, and Sirigen, founded in 2003 by Guillermo C. Bazan, Patrick J. Dietzen, Brent S. Gaylord. Alan Heeger was a founder of Uniax, which was acquired by DuPont.\n\nHe won the Nobel Prize for Chemistry in 2000 along with Alan G. MacDiarmid and Hideki Shirakawa \"for their discovery and development of conductive polymers;\" They published their results on polyacetylene a conductive polymer in 1977\n\nHe had won the Oliver E. Buckley Prize of the American Physical Society in 1983 and, in 1995, the Balzan Prize for Science of Non-Biological Materials.\n\nHis sons are the neuroscientist David Heeger and the immunologist Peter Heeger.\n\nIn October 2010, Heeger participated in the USA Science and Engineering Festival's Lunch with a Laureate program where middle and high school students engage in an informal conversation with a Nobel Prize-winning scientist over a brown-bag lunch. Heeger is also a member of the USA Science and Engineering Festival's Advisory Board. Heeger has been a judge of the \"STAGE\" International Script Competition three times (2006, 2007, 2010).\n\n\"Perhaps the greatest pleasure of being a scientist is to have an abstract idea, then to do an experiment (more often a series of experiments is required) that demonstrates the idea was correct; that is, Nature actually behaves as conceived in the mind of the scientist. This process is the essence of creativity in science. I have been fortunate to have experienced this intense pleasure many times in my life.\" \nAlan J Heeger, Never Lose Your Nerve! \n\nJournal Articles:\nTechnical Reports:\n, World Scientific Publishing, \n\n\n"}
{"id": "40386444", "url": "https://en.wikipedia.org/wiki?curid=40386444", "title": "Analog front-end", "text": "Analog front-end\n\nAn analog front-end (AFE or analog front-end controller AFEC) is a set of analog signal conditioning circuitry that uses sensitive analog amplifiers, often operational amplifiers, filters, and sometimes application-specific integrated circuits for sensors, radio receivers, and other circuits to provide a configurable and flexible electronics functional block, needed to interface a variety of sensors to an, antenna, analog to digital converter or in some cases to a microcontroller.\n\nAFE hardware modules are used as interface sensors for many kinds of analog and digital systems, providing hardware modularity.\nFor example, Texas Instruments markets health monitoring AFEs as the ADS1298, AFE4400 and AFE4490.\nA radio frequency AFE is used in radio receivers, known as an RF front end.\nAtmel markets analog front-ends for smart meters.\nAnalog Devices markets a CN0209 product for test and measurement applications.\n"}
{"id": "13643223", "url": "https://en.wikipedia.org/wiki?curid=13643223", "title": "Barton evaporation engine", "text": "Barton evaporation engine\n\nThe Barton Evaporation Engine (BEE) is a heat engine invented in 2004 by Dr Noel Barton of Sunoba Pty Ltd. The concept is patented in Australia (Australian patent 2007240126).\n\nThe evaporation engine works by evaporative cooling of dry air at reduced pressure. Key steps are: (1) adiabatic expansion of unsaturated air; (2) evaporative cooling at reduced pressure; and (3) re-compression back to atmospheric pressure with further evaporation. Net work is available in the cycle, so the engine produces power and cooled moist air from water and hot dry air:\n\nThe remarkable property of the evaporation engine is that the temperature of an air stream is reduced at the same time that power is produced. This occurs without violation of the 2nd Law of Thermodynamics because the entropy increase as water is evaporated outweighs the entropy decrease as the air cools.\n\nWith a modest amount of passive solar pre-heating, the engine theoretically is able to produce power in hot arid climates. As well as being a heat engine, the evaporation engine can also be used as an evaporative cooler.\n\nThe evaporation engine has broadly comparable theoretical efficiency to simple Rankine steam turbines, without need for high-pressure boiler or condenser. The evaporation engine can function well on industrial waste heat, particularly the exhaust gas of open-cycle gas turbines.\n\nThe thermodynamic cycle can be achieved by at least three separate mechanisms. The most straightforward mechanism is a piston-cylinder device, for which a full thermodynamic analysis was published in 2008. Barton also built an experimental piston-cylinder engine. that provided confirmation of the theory.\n\nAs a second option, the evaporation engine can also be configured in continuous-flow form, for which a full analysis was published in 2012.\n\nThere is a third possible manifestation based on the Bernoulli effect for compressible gases. As a compressible gas flows through a narrow orifice, the pressure and temperature decrease, thereby allowing the possibility of evaporative cooling at reduced pressure in the high-speed section. On recovery to slow speeds, there will be surplus pressure that can drive a turbine. Barton has also analyzed this mechanism. The analysis has not been published but is available on request to Sunoba Pty Ltd. The Bernoulli turbine would face extreme (perhaps insurmountable) difficulties in construction, much more so than with the other two versions.\n\nIn general, the efficiency of the evaporation engine increases with the inlet temperature and the expansion ratio. As an example of the output from a piston-cylinder engine, air at 30°C and 47% relative humidity pre-heated to 85°C can theoretically deliver 4.9 kJ work output per kg of dry air by evaporation of 19 ml of water per kg of air at an expansion ratio of 1.64. If the cycle time is 1 second, the theoretical power output would be 4.9 kW/kg of air.\n\nBarton (at www.sunoba.com.au/previous) gives an example of the evaporation engine as an evaporative cooler, that is operating on ambient air without heating prior to the inlet. The inlet conditions were: temperature 45°C, partial pressures 99.3 kPa (air) and 2 kPa (vapour). The volume expansion ratio was 1.2 and the outlet conditions were: temperature 25.5°C, partial pressures 98.1 kPa (air) and 3.2 kPa (vapour). Under these conditions, the net work available in the cycle is 788 J/kg dry air.\n\nIf the inlet air is sourced from an open-cycle gas turbine exhaust at around 500°C, Barton has shown that the evaporation engine can provide about a 20% boost to the power output of the gas turbine. It should be noted, however, that the power boost depends sensitively on the adiabatic efficiency of expansion and compression.\n\nA key issue with this engine is the water consumption, which can be prohibitive for low expansion ratios and low inlet temperatures. The engine works best in hot dry climates, but those are typically the locations where water is most scarce.\n\nOther studies by Barton involving the evaporation engine include:\n\nAbstracts and comments on all cited articles are available at www.sunoba.com.au/references.\n\n\n"}
{"id": "3188435", "url": "https://en.wikipedia.org/wiki?curid=3188435", "title": "Boudouard reaction", "text": "Boudouard reaction\n\nThe Boudouard reaction, named after Octave Leopold Boudouard, is the redox reaction of a chemical equilibrium mixture of carbon monoxide and carbon dioxide at a given temperature. It is the disproportionation of carbon monoxide into carbon dioxide and graphite or its reverse:\n\nThe Boudouard Reaction to form carbon dioxide and carbon is exothermic at all temperatures. However, the standard enthalpy of the Boudouard reaction becomes less negative with increasing temperature, as shown to the side.\n\nWhile the formation enthalpy of is higher than that of , the formation entropy is much lower. Consequently, the standard free energy of formation of from its component elements is almost constant and independent of the temperature, while the free energy of formation of decreases with temperature. At high temperatures, the forward reaction becomes endergonic, favoring the (exergonic) reverse reaction toward CO, even though the forward reaction is still exothermic.\n\nThe effect of temperature on the extent of the Boudouard reaction is indicated better by the value of the equilibrium constant than by the standard free energy of reaction. The value of log(K) for the reaction as a function of temperature in Kelvin (valid between 500–) is approximately:\n\nThe implication of the change in K with temperature is that a gas containing may form elemental carbon if the mixture cools below a certain temperature. The thermodynamic activity of carbon may be calculated for a / mixture by knowing the partial pressure of each species and the value of K. For instance, in a high temperature reducing environment, such as that created for the reduction of iron oxide in a blast furnace or the preparation of carburizing atmospheres, carbon monoxide is the stable oxide of carbon. When a gas rich in is cooled to the point where the activity of carbon exceeds one, the Boudouard Reaction can take place. Carbon monoxide then tends to disproportionate into carbon dioxide and graphite, which forms soot.\n\nIn industrial catalysis, this is not just an eyesore; sooting (also called coking) can cause serious and even irreversible damage to catalysts and catalyst beds. This is a problem in the catalytic reforming of petroleum and the steam reforming of natural gas.\n\nThe reaction is named after the French chemist, Octave Leopold Boudouard (1872–1923), who investigated this equilibrium in 1905.\n\nAlthough the damaging effect of carbon monoxide on catalysts is undesirable, this reaction has been used in producing graphite flakes, filamentous graphite and lamellar graphite crystallites, as well as producing carbon nanotubes. In graphite production, catalysts used are molybdenum, magnesium, nickel, iron and cobalt, while in carbon nanotube production, molybdenum, nickel, cobalt, iron and Ni-MgO catalysts are used.\n\nThe Boudouard reaction is an important process inside a blast furnace. The reduction of iron oxides is not achieved by carbon directly, as reactions between solids are typically very slow, but by carbon monoxide. The resulting carbon dioxide undergoes a (reverse) Boudouard reaction upon contact with coke carbon.\n"}
{"id": "104020", "url": "https://en.wikipedia.org/wiki?curid=104020", "title": "C-4 (explosive)", "text": "C-4 (explosive)\n\nC-4 or Composition C-4 is a common variety of the plastic explosive family known as Composition C. A similar British plastic explosive, based on RDX but with different plasticizer than Composition C-4, is known as PE-4 (Plastic Explosive No. 4). C-4 is composed of explosives, plastic binder, plasticizer to make it malleable, and usually a marker or odorizing taggant chemical.\n\nC-4 has a texture similar to modelling clay and can be molded into any desired shape. C-4 is metastable and can be exploded only by the shock wave from a detonator or blasting cap.\n\nThe Composition C-4 used by the United States Armed Forces contains 91% RDX (\"Research Department Explosive\", an explosive nitroamine), 5.3% dioctyl sebacate (DOS) or dioctyl adipate (DOA) as the plasticizer (to increase the plasticity of the explosive), 2.1% polyisobutylene (PIB, a synthetic rubber) as the binder, and 1.6% of a mineral oil often called \"process oil.\" Instead of \"process oil,\" low-viscosity motor oil is used in the manufacture of C-4 for civilian use.\n\nThe British PE4 consists of 88.0% cyclonite, 1.0% pentaerythrite dioleate and 11.0% DG-29 lithium grease (corresp. to 2.2% lithium stearate and 8.8% paraffin oil BP); a taggant (2,3-dinitro-2,3-dimethylbutane, DMNB) is added at a minium of 0.10% weight of the plastic explosive, typically at 1.0% mass. The newer PE7 consists of 88.0% cyclonite, 1.0% DMNB taggant and 11.0% of a plasticizer composed of low molecular mass hydroxyl-terminated polybutadiene, along with an antioxidant and an agent preventing hardening of the binder upon prolonged storage. The PE8 consists of 86.5% cyclonite, 1.0% DMNB taggant and 12.5% of a binder composed of di(2-ethylhexyl) sebacate thickened with high molecular mass polyisobutylene.\nTechnical data according to the Department of the Army for the Composition C-4 follows.\nC-4 is manufactured by combining the above ingredients with binder dissolved in a solvent. Once the ingredients have been mixed, the solvent is extracted through drying and filtering. The final material is a solid with a dirty white to light brown color, a putty-like texture similar to modeling clay, and a distinct smell of motor oil.\n\nDepending on its intended usage and on the manufacturer, there are differences in the composition of C-4. For example, a 1990 U.S. Army technical manual stipulated that Class IV composition C-4 consists of 89.9±1% RDX, 10±1% polyisobutylene, and 0.2±0.02% dye that is itself made up of 90% lead chromate and 10% lamp black. RDX classes A, B, E, and H are all suitable for use in C-4. Classes are measured by granulation.\n\nThe manufacturing process for Composition C-4 specifies that wet RDX and plastic binder are added in a stainless steel mixing kettle. This is called the aqueous slurry-coating process. The kettle is tumbled to obtain a homogeneous mixture. This mixture is wet and must be dried after transfer to drying trays. Drying with forced air for 16 hours at 50 °C to 60 °C is recommended to eliminate excess moisture.\n\nC-4 is very stable and insensitive to most physical shocks. C-4 cannot be detonated by a gunshot or by dropping it onto a hard surface. It does not explode when set on fire or exposed to microwave radiation. Detonation can only be initiated by a shockwave, such as when a detonator inserted into it is fired.\nWhen detonated, C-4 rapidly decomposes to release nitrogen and carbon oxides as well as other gases. The detonation proceeds at an explosive velocity of .\n\nA major advantage of C-4 is that it can easily be molded into any desired shape to change the direction of the resulting explosion.\n\nMilitary grade C-4 is commonly packaged as the M112 demolition block. The demolition charge M112 is a rectangular block of Composition C-4 approximately 2 inches by 1.5 inches and 11 inches long, weighing . The M112 is wrapped in a sometimes olive color Mylar-film container with a pressure-sensitive adhesive tape on one surface.\n\nThe M112 demolition blocks of C-4 are commonly manufactured into the M183 \"demolition charge assembly\", which consists of 16 M112 block demolition charges and four priming assemblies packaged inside military Carrying Case M85. The M183 is used to breach obstacles or demolish large structures where larger satchel charges are required. Each priming assembly includes a five- or twenty-foot length of detonating cord assembled with detonating cord clips and capped at each end with a booster. When the charge is detonated, the explosive is converted into compressed gas. The gas exerts pressure in the form of a shock wave, which demolishes the target by cutting, breaching, or cratering.\n\nOther forms include the mine-clearing line charge (MICLIC) and M18A1 Claymore Mine.\n\nComposition C-4 exists in the US Army Hazardous Components Safety Data Sheet on sheet number 00077.\n\nImpact tests done by the US military indicate composition C-4 is less sensitive than composition C-3 and is fairly insensitive. The insensitivity is attributed to using a large amount of binder in its composition. A series of shots were fired at vials containing C-4 in a test referred to as \"the rifle bullet test\". Only 20 percent of the vials burned, and none exploded. While C-4 passed the Army's bullet impact and fragment impact tests at ambient temperature, it did in fact fail the shock stimulus, sympathetic detonation and shaped charge jet tests.\n\nAdditional tests were done including the \"pendulum friction test\", which measured a five-second explosion temperature of 263 °C to 290 °C. The minimum initiating charge required is 0.2 grams of lead azide or 0.1 grams of tetryl.\n\nThe results of 100 °C heat test are: 0.13 percent loss in the first 48 hours, no loss in the second 48 hours, and no explosions in 100 hours. The vacuum stability test at 100 °C yields 0.2 cubic centimeters of gas in 40 hours. Composition C-4 is essentially nonhygroscopic.\n\nThe shock sensitivity of C-4 is related to the size of the nitramine particles. The finer they are the better they help to absorb and suppress shock. Using 3-nitrotriazol-5-one (NTO), or 1,3,5-triamino-2,4,6-trinitrobenzene (TATB) (available in two particle sizes (5 µm, 40 µm)), as a substitute for RDX, is also able to improve stability to thermal, shock, and impact/friction stimulus; however, TATB is not cost-effective, and NTO is more difficult to use in the manufacturing process.\n\nSensitivity test values reported by the US Army follow.\n\nC-4 produced for use by the U.S. military, commercial C-4 (also produced in the United States), and C-4 (otherwise known as PE-4) from England each have their own unique properties and are not identical. The analytical techniques of time-of-flight secondary ion mass spectrometry and X-ray photoelectron spectroscopy have been demonstrated to discriminate finite differences in different C-4 sources. Chemical, morphological structural differences, and variation in atomic concentrations are detectable and definable.\n\nC-4 has toxic effects on humans when ingested. Within a few hours multiple generalized seizures, vomiting, and changes in mental activity occur. A strong link to central nervous dysfunction is observed. If ingested, patients may be administered a dose of active charcoal to adsorb some of the toxins, and haloperidol intramuscularly and diazepam intravenously to help the patient control seizures until it has passed. However, ingesting small amounts of C-4 is not known to cause any long-term impediment.\n\nIf C-4 is marked with a taggant, such as DMNB, it can be detected with an explosive vapor detector before it has been detonated.\n\nA variety of methods for explosive residue analysis may be used to identify C-4. These include optical microscope examination and scanning electron microscopy for unreacted explosive, chemical spot tests, thin-layer chromatography (TLC), X-ray crystallography, and infrared spectroscopy for products of the explosive chemical reaction. Small particles of C-4 may be easily identified by mixing with thymol crystals and a few drops of sulfuric acid. The mixture will become rose colored upon addition of a small quantity of ethyl alcohol.\n\nRDX has a high birefringence, and the other components commonly found in C-4 are generally isotropic; this makes it possible for forensic science teams to detect trace residue on fingertips of individuals who may have recently been in contact with the compound. However, positive results are highly variable and the mass of RDX can range between 1.7 and 130 ng, each analysis must be individually handled using magnifying equipment. The cross polarized light images obtained from microscopic analysis of the fingerprint are analyzed with gray-scale thresholding to improve contrast for the particles. The contrast is then inverted in order to show dark RDX particles against a light background. Relative numbers and positions of RDX particles have been measured from a series of 50 fingerprints left after a single contact impression.\n\nMilitary and commercial C-4 are blended with different oils. It is possible to distinguish these sources by analyzing this oil by high-temperature gas chromatography–mass spectrometry. The oil and plasticizer must be separated from the C-4 sample, typically by using a non-polar organic solvent such as pentane followed by solid phase extraction of the plasticizer on silica. This method of analysis is limited by manufacturing variation and methods of distribution.\n\nC-4 is a member of the Composition C family of chemical explosives. Variants have different proportions and plasticisers and include composition C-2, composition C-3, and composition C-4. The original RDX based material was developed by the British during World War II, and redeveloped as Composition C when introduced to US military service. It was replaced by Composition C-2 around 1943, and later redeveloped around 1944 as Composition C-3. The toxicity of C-3 was reduced, the concentration of RDX was increased, it improved safety of usage and storage. Research on a replacement for C-3 was begun prior to 1950, but the new material, C-4, did not begin pilot production until 1956. C-4, was submitted for patent as \"Solid Propellant and a Process for its Preparation\" March 31, 1958 by the Phillips Petroleum Company.\n\nU.S. soldiers during the Vietnam War era would sometimes use small amounts of C-4 as a fuel for heating rations as it will burn unless detonated with a primary explosive. However, burning C-4 produces poisonous fumes, and soldiers are warned of the dangers of personal injury when using the plastic explosive.\n\nAmongst field troops in Vietnam it became common knowledge that ingestion of a small amount of C-4 would produce a \"high\" similar to that of ethanol. Others would ingest C-4, commonly obtained from a Claymore mine, to induce temporary illness in the hope of being sent on sick leave.\n\nTerrorist groups have used C-4 worldwide in acts of terrorism and insurgency, as well as domestic terrorism and state terrorism.\n\nComposition C-4 is recommended in al-Qaeda’s traditional curriculum of explosives training. In October 2000, the group used C-4 to attack the USS \"Cole\", killing 17 sailors. In 1996, Saudi Hezbollah terrorists used C-4 to blow up the Khobar Towers, a U.S. military housing complex in Saudi Arabia. Composition C-4 has also been used in improvised explosive devices by Iraqi insurgents.\n\nHomemade C-4 is a popular subject amongst domestic anarchist terrorists in the United States and is the subject of a chapter in the original \"The Anarchist Cookbook\" which details how to separate RDX from Composition-4. \n\nIn 1987, North Korean agents used C-4 as part of their operation to bomb Korean Air Flight 858.\n\n"}
{"id": "12593908", "url": "https://en.wikipedia.org/wiki?curid=12593908", "title": "Chemical Computing Group", "text": "Chemical Computing Group\n\nChemical Computing Group is a software company specializing in research software for computational chemistry, bioinformatics, cheminformatics, docking, pharmacophore searching and molecular simulation. The company's main customer base consists of pharmaceutical and biotechnology companies, as well as academic research groups. It is a private company that was founded in 1994; it is based in Montreal, Quebec, Canada. Its main product, Molecular Operating Environment (MOE), is written in a self-contained programming system, the Scientific Vector Language (SVL).\n\n\nMOE is a leading drug discovery software platform that integrates visualization, modeling and simulations, as well as methodology development, in one package. MOE scientific applications are used by biologists, medicinal chemists and computational chemists in pharmaceutical, biotechnology and academic research. MOE runs on Windows, Linux, Unix, and MAC OS X.\n\nMain application areas: Structure-Based Design, Fragment-Based Design, Pharmacophore Discovery, Medicinal Chemistry Applications, Biologics Applications, Protein and Antibody Modeling, Molecular Modeling and Simulations, Cheminformatics & QSAR\n\n\nPSILO is a protein structure database system that provides an easily accessible, consolidated repository for macromolecular and protein-ligand structural information. It allows research organizations to systematically track, register and search both experimental and computational macromolecular structural data. A web-browser interface facilitates the searching and accessing of public and private structural data.\n\nOther institutions developing software for computational chemistry:\n\n"}
{"id": "3882401", "url": "https://en.wikipedia.org/wiki?curid=3882401", "title": "Distribution uniformity", "text": "Distribution uniformity\n\nDistribution Uniformity or \"DU\" in irrigation is a measure of how uniformly water is applied to the area being watered, expressed as a ratio to avoid confusing it with efficiency. The distribution uniformity is often calculated when performing an irrigation audit. The DU should not be confused with the coefficient of uniformity (CU) which is often preferred for describing the performance of overhead pressurized systems.\n\nThe most common measure of DU is the Low Quarter DU expressed as DUlq, which is a measure of the average of the lowest quarter of samples, divided by the average of all samples. The higher the DUlq, the better the coverage of the area measured. If all samples are equal, the DUlq is 1.0. There is no universal value of DUlq for satisfactory system performance. A value of >.80 is considered above average. \n\nDistribution uniformity may be helpful as a starting point for irrigation scheduling. For example, an irrigator might want to apply not less than one inch of water to the area being watered. If the DU were 0.75, then the total amount to be applied would be the desired amount of water, divided by the DU. In this case, the required irrigation would be 1.33 inches of water, so that only a very small area received less than one inch. The lower the DU, the less uniform the distribution at the plane of data collection and the more water that may be needed to meet the minimum requirement. \n\nCatchments are commonly used to determine sprinkler DU and one must be reminded that data collection occurs above grade and more importantly above the root zone where plant uptake normally occurs. Many factors may affect water distribution or redistribution between catchment plane and root zone; slope, plant canopy, thatch, mulch, infiltration rate, etc.. Soil type and root horizon may nullify the need for high DUlq sprinklers. \n\nLow sprinkler DUlq does not guarantee inefficiency, nor does high DUlq guarantee efficiency. \n\nAn alternative is Christiansen's Uniformity Coefficient (CU), defined as the average depth of irrigation water applied minus the average absolute deviation from this depth, all divided by the average depth applied. (ASAE, 1998) \n"}
{"id": "2606066", "url": "https://en.wikipedia.org/wiki?curid=2606066", "title": "Electron capture detector", "text": "Electron capture detector\n\nAn electron capture detector (ECD) is a device for detecting atoms and molecules in a gas through the attachment of electrons via electron capture ionization. The device was invented in 1957 by James Lovelock and is used in gas chromatography to detect trace amounts of chemical compounds in a sample.\n\nThe electron capture detector is used for detecting electron-absorbing components (high electronegativity) such as halogenated compounds in the output stream of a gas chromatograph. The ECD uses a radioactive beta particle (electron) emitter in conjunction with a so-called makeup gas flowing through the detector chamber. The electron emitter typically consists of a metal foil holding 10 millicuries (370 MBq) of the radionuclide . Usually, nitrogen is used as makeup gas, because it exhibits a low excitation energy, so it is easy to remove an electron from a nitrogen molecule. The electrons emitted from the electron emitter collide with the molecules of the makeup gas, resulting in many more free electrons. The electrons are accelerated towards a positively charged anode, generating a current. There is therefore always a background signal present in the chromatogram. As the sample is carried into the detector by the carrier gas, electron-absorbing analyte molecules capture electrons and thereby reduce the current between the collector anode and a cathode. The analyte concentration is thus proportional to the degree of electron capture. ECD detectors are particularly sensitive to halogens, organometallic compounds, nitriles, or nitro compounds.\n\nIt is not immediately obvious why the capture of electrons by electronegative analytes reduces the current that flows between the anode and cathode: the molecular negative ions of the analyte carry the same charge as the electrons that were captured. The key to understanding why the current decreases is to ask where charged entities can go \"besides\" being collected at the anode and cathode. The answer is recombination of negative ions or electrons with the positive ions of the makeup gas before these charged entities can be collected at anode and cathode respectively. Negative and positive ions recombine much more rapidly than electrons and positive ions; it is this more rapid neutralization that is the origin of the observed decrease in current. Examination of the rate balance equation with all charge production and loss mechanisms considered reveals that the current collected when the electron capture detector is saturated with analyte is not zero: it is half the current collected when no analyte is present. To laboratory chromatographers his theoretical result is a well known experimental observation.\n\nDepending on the analyte, an ECD can be 10-1000 times more sensitive than a flame ionization detector (FID), and one million times more sensitive than a thermal conductivity detector (TCD). An ECD has a limited dynamic range and finds its greatest application in analysis of halogenated compounds. The detection limit for electron capture detectors is 5 femtograms per second (fg/s), and the detector commonly exhibits a 10,000-fold linear range. This made it possible to detect halogenated compounds such as pesticides and CFCs, even at levels of only one part per trillion (ppt), thus revolutionizing our understanding of the atmosphere and pollutants.\n"}
{"id": "3547771", "url": "https://en.wikipedia.org/wiki?curid=3547771", "title": "Entrenching tool", "text": "Entrenching tool\n\nAn entrenching tool, E-tool, or trenching tool is a collapsible spade used by military forces for a variety of military purposes. Survivalists, campers, hikers and other outdoors groups have found it to be indispensable in field use. Modern entrenching tools are usually collapsible and made using steel, aluminum, or other light metals.\n\nEntrenching tools go back at least to the times of the Roman Legion. Julius Caesar, as well as other ancient writers, documented the use of spades and other digging implements as important tools of war. The Roman Legion when on the march would dig a ditch and rampart around their camps every night where established camps were not available.\n\nSiege tactics throughout history required the digging of fortifications and often mining of walls was attempted, where saps were dug to a wall’s foundation, and collapsing the wall was attempted.\n\nIn more modern times the siege tactics of the Napoleonic Wars used spades and pickaxes as entrenching tools to dig trenches towards the walls of the fortifications being besieged, to allow men and munitions to get close enough to fire cannons at the walls to open a breach. Being too long and heavy to be transported by individual soldiers, entrenching shovels and spades were normally carried in the supply carts (logistics train) of a military column; only pioneer or engineer troops typically carried spades or shovels as part of their individual equipment. This frequently led to situations in which the infantry did not have access to entrenching equipment when it was needed. As one U.S. army infantry officer noted, \"the intrenching [sic] tools of an army rarely get up to the front until the exigency for their use has passed.\"\n\nIn 1870, the U.S. Army introduced the trowel bayonet, intended for individual soldiers as both a weapon and an entrenching tool. This was followed by the development of separate trowel and spade tools, small one-hand implements that could be carried as part of a soldier's individual equipment. While the entrenching trowel or spade gradually gave way in the U.S. and other modern armies to larger, heavier, and more effective entrenching tools, the concept of supplying each infantry soldier with a means of digging his own entrenchments or breastworks continued as a tactical doctrine.\n\nThe first truly modern entrenching tool was invented in 1869 by the Danish officer Mads Johan Buch Linnemann. In 1870, it was patented and supplied to the Danish Army. The next year it was adopted by the much bigger Austrian Army, and Linnemann founded a factory in Vienna to produce his spade. It was later introduced to Germany, France, Romania and Russia, though only Russia recognized Linnemann's patent rights, and paid him 30,000 rubles and ordered 60,000 spades. The Russians called it the MPL-50 (small infantry spade-50 cm (20 in) length) and still use it to this day. This little spade can also be used as an axe, a hammer, an oar, or as a frying pan for cooking food.\n\nDuring World War I entrenching tools became extremely important with the introduction of trench warfare. Entrenching tools designed for the individual infantryman soon appeared with short straight handles, T-handles, D-handles, and other designs. \n\nThe British entrenching tool of this period was a two part design, with a metal head and a wooden handle, the metal head consisted of an adze/spade blade and a pick spike, used alone the head could be used as a spade with the pick spike serving as a handle. Between the blade and the spike was a ring into which the handle could be inserted at right angles to the head, with the handle inserted the tool could be used as a pick mattock. Besides being used for digging defensive fighting positions, entrenching tools were used for digging latrines and graves.\n\nDuring World War I, the entrenching spade was also pressed into service as a melee weapon. In the close confines of a trench, rifles and fixed bayonets were often too long for effective use, and entrenching tools were often used as auxiliary arms for close-quarter fighting. From 1915, soldiers on both sides routinely sharpened the edges of entrenching shovels for use as weapons.\n\n\"In 1938, the (German) foldable spade, appeared, being the precursor of all modern spades of this kind, including the 1943 American copy.\" Folding designs became increasingly popular, usually consisting of a fixed handle with a folding shovel head, and sometimes incorporating a pick into the design. Like all individual entrenching tools, they were designed to be easily carried as part of an infantry soldier's standard pack equipment.\n\nThe British 1937 Pattern entrenching tool added a bayonet lug to the tool's handle, allowing the Lee-Enfield spike bayonet to be mounted on the end and converting the tool into a mine prodder.\n\nEntrenching tools, if strongly built, can also be used as ancillary weapons. Some entrenching tools can be even sharpened on one or both sides of the blade to be used as cutting tools or weapons; in fact, when used as such, the tool's sharp, thick edges are strong enough to cut through flesh and bone. During the Second World War, entrenchment tools were used in close quarters combat between German and Soviet forces, notably in the brutal hand-to-hand fighting during the Battle of Stalingrad.\n\nThe United States Army folding spade, or entrenching tool, has evolved from a single fold spade with a straight handle, to a tri-fold design with a modified “D” handle design with all steel construction, to a similar light weight plastic and steel tri-fold design adopted by NATO as the standard issue entrenching tool. Other folding variants have also been issued. The latest light weight plastic tri-fold design is thirty percent lighter than the all-steel tri-fold was: instead of .\nThe Glock \"Feldspaten\" (field spade) features a hardened metal spade blade that can be locked in 3 positions for digging, shoveling, and chopping, and a telescopic handle made out of fiberglass-reinforced nylon containing a long hardened metal sawblade. The entrenching tool weighs and fully extended is long. The spade and handle can be collapsed and shortened for easy transport and storage into a 260 mm × 150 mm × 60 mm (10 in × 6 in ×  in) package.\n\nSoviet Spetsnaz units were well trained in the use of the standard short-handled Russian entrenching shovel (\"saperka\") as a weapon; by the nature of their missions, such tools were only rarely used for digging or entrenching positions. Modern commando forces, too, are trained to fight with entrenchment tools.\n\nMany millions of surplus entrenching tools have made their way into the hands of civilians. They are commonly used for camping, gardening and by war re-enactment groups. Some people collect the older issue entrenching tools as military memorabilia.\n\n"}
{"id": "8033728", "url": "https://en.wikipedia.org/wiki?curid=8033728", "title": "FM transmitter (personal device)", "text": "FM transmitter (personal device)\n\nA personal FM transmitter is a low-power FM radio transmitter that broadcasts a signal from a portable audio device (such as an MP3 player) to a standard FM radio. Most of these transmitters plug into the device's headphone jack and then broadcast the signal over an FM broadcast band frequency, so that it can be picked up by any nearby radio. This allows portable audio devices to make use of the louder or better sound quality of a home audio system or car stereo without requiring a wired connection. They are often used in cars but may also be in fixed locations such as broadcasting from a computer sound card throughout a building.\n\nBeing low-powered, most transmitters typically have a short range of 100–300 feet (30–100 metres), depending on the quality of the receiver, obstructions and elevation. Typically they broadcast on any FM frequency from 87.5 to 108.0 MHz in most of the world, 76.0 - 95.0 MHz for Japan, 65.0 - 74.2 MHz for Russia (or 88.1 to 107.9 MHz in the US and Canada).\n\nPersonal FM transmitters are commonly used as a workaround for playing portable audio devices on car radios that don't have an Auxiliary \"AUX\" input jack or Bluetooth audio connectivity. They are also used to broadcast a stationary audio source, like a computer or a television, around a home. They can also be used for low-power broadcasting and pirate radio but only to a very limited audience in near proximity. They can also be used as a \"talking sign\" in real estate sales or similar.\n\nThe legality and maximum permitted power levels or field strengths of these devices varies by country. In 2006 these devices became legal in most countries in the European Union. \n\nIn the UK Stautory Instrument IR2030/26/2 2011/0401/UK (from December 2011) permits unlicenced use of devices that can be shown to radiate less than 50 nanowatts (-43dBm), on a 0.2MHz raster in the range 87.5–108MHz.\n\nIndustry Canada permits transmitters that have an output lower than 100 µV/m at 30 meters (approximately 1 microwatt output). \n\nIn the United States, Part 15 of the U.S. Federal Communications Commission rules specifies that no license is needed if FM transmitters have a Maximum Effective Radiated Power (ERP) of 0.01 microwatts or 250 µV/m measured at 3 meters. \n\nIn Japan, no license is needed for devices with a signal strength of less than 500 µV/m at 3 meters.\n\n\n"}
{"id": "1416309", "url": "https://en.wikipedia.org/wiki?curid=1416309", "title": "Falcon Northwest", "text": "Falcon Northwest\n\nFalcon Northwest is a personal computer manufacturing company located in Medford, Oregon, United States which was founded in 1992 by its current president, Kelt Reeves. The company began its existence focusing on high-end systems for Flight Simulation. In 1993 Falcon Northwest bought out Bay Engineering, who served CAD application users, and continued to serve those customers for several years.\n\nFalcon Northwest maintains one facility located in Medford, Oregon, and it chooses not to outsource any of its operations. Falcon Northwest's operations are mostly restricted to online and telephone orders, as the company has not signed any distribution deals with any major retailers.\n\nFalcon Northwest’s system lineup began with the Mach V series and slowly expanded over time. The Mach V models were built with components that Falcon Northwest believed to offer the highest performance in PC games. In 2000, the company added the Talon line at a lower price than the Mach V line. Later, the company brought out the FragBook line of laptops. Falcon Northwest also offers a small form factor (SFF) PC called the FragBox, a miniature system intended for LAN parties. For customers looking for something a little more personal, Falcon Northwest provides custom-painted cases.\n\nFalcon Northwest's magazine advertisements over the years have usually consisted of game art and its characters surrounding one of their systems. Their ads have included games such as \"Half-Life\", \"Unreal\", \"Alien vs. Predator\", and \"MechWarrior\", among many others. Their earliest advertisements played off the company's Falcon name and related it with the F-16 Fighting Falcon fighter plane, perhaps relating to the Falcon series of simulations. Many of the company's ads are available for viewing on their web site.\n\nThe company has occasionally had customized hardware made specifically for its machines. One example was the \"Falcon Northwest Special Edition Maxi Gamer Xentor 32\", an upgraded Nvidia RIVA TNT2 Ultra 3D accelerator card. The card used special low-latency RAM and hand-picked accelerator chips.\n\nIn 2005, Falcon Northwest's President, Kelt Reeves, had a heated exchange with HardOCP, an online magazine geared towards the PC hardware enthusiast and PC gaming communities. HardOCP received a system from Falcon that performed less than optimally, with noted instability, and reported these findings to their reader base.\n\nIn the exchange that followed, Reeves disputed HardOCP's findings. He cited that HardOCP had come forward and publicized incorrect claims with questionable objectivity with regard to which piece of hardware was the true cause of the instability. Reeves stated that both he and HardOCP had initially assumed that the problem lay with the motherboard but upon further analysis discovered that the video card was at fault. Falcon Northwest's quality assurance testing included 14 hours of looping the 3D graphics benchmark 3DMark 2005 in an 85 °F (29 °C) room. Upon receipt of the returned system, it could no longer complete that test without crashing. Falcon Northwest offered to repair the system and send it back to HardOCP for testing, but the magazine refused this solution, stating that it could interfere with the anonymous testing process.\n\n\n"}
{"id": "24797106", "url": "https://en.wikipedia.org/wiki?curid=24797106", "title": "Friends of Dard Hunter", "text": "Friends of Dard Hunter\n\nThe Friends of Dard Hunter (FDH) was established in 1981 to preserve and promote hand papermaking and book arts in the spirit and tradition of Dard Hunter. With over 345 members, the group produces a quarterly newsletter, The Bull & Branch, and the annual Membership Directory.\n\nThe Friends of Dard Hunter is an organization founded to assure the preservation of the Dard Hunter Paper Museum’s collection, now housed at the Robert C. Williams Paper Museum at Georgia Tech in Atlanta, Georgia, and it is an organization dedicated to promoting and facilitating the study, use, creation, and appreciation of handmade paper\n\nFDH hosts an annual conference:\n\n\n\n"}
{"id": "25654551", "url": "https://en.wikipedia.org/wiki?curid=25654551", "title": "H&amp;R Block Tax Software", "text": "H&amp;R Block Tax Software\n\nH&R Block at Home was a tax preparation program offered by H&R Block. As of 2014, both online and software versions of the product go by the flagship name, H&R Block. It was previously called \"TaxCut\" and from 2008-2013 named \"H&R Block at Home\"\n\nH&R Block is a tax preparation company, headquartered in Kansas City. H&R Block offers in-person tax filing and consumer tax software for online tax preparation and electronic filing (e-file) from their website.\n\nThere are a variety of software and online products including, H&R Block Online Free, H&R Block Online Deluxe, H&R Block Online Premium, H&R Block Basic Tax Software, H&R Block Deluxe Tax Software, H&R Block Premium Tax Software and H&R Block Premium & Business Tax Software. Either the online or software versions will prepare and file one's federal and state income tax returns with the IRS with the option of electronically filing (e-filing) and direct depositing an applicable tax refund into a specified bank account.\n\nCompetitors for H&R Block software include: TurboTax and TaxAct\n\n"}
{"id": "21293270", "url": "https://en.wikipedia.org/wiki?curid=21293270", "title": "History of graphic design", "text": "History of graphic design\n\nGraphics (from Greek , \"graphikos\") are visual statements committed to a surface, such as a wall, a canvas, pottery, a computer screen, paper, stone, even to the landscape itself. The term \"graphics\" relates to the creation of signs, charts, logos, graphs, drawings, line art, symbols, geometric designs and so on. Graphic design is the art or profession of combining text, pictures, and ideas in advertisements, publications, or websites. In its broadest definition, therefore, it refers to the whole history of art, although painting and other aspects of the subject are more usually treated as art history.\n\nHundreds of graphic designs of animals by prehistoric social groups in the Chauvet Cave, located in the south of France, which were created earlier than 30,000 BCE; similar designs in the Lascaux cave of France completed earlier than 14,000 BCE; designs left by hunters in the Bhimbetka rock shelters in India before 7,000 BCE; Aboriginal rock art in the Kakadu National Park of Australia; and many other rock or cave paintings in other parts of the world show that graphic art emerged very early in the development of prehistoric human cultures worldwide. This history – along with that of writing, which had begun at least by the late 4th millennium BCE – together constitute the foundation of graphic design.\n\nMany books in the classical world were illustrated, although only a handful of original examples survive. Medieval religious illuminated manuscripts extensively feature graphics. Among these books are the Gospel books of Insular art, created in the monasteries of the British Isles. The graphics in these books reflect the influence of the Animal style associated with the \"barbarian\" peoples of Northern Europe, with much use of interlace and geometric decoration.\n\nIn Islamic countries graphic designs were created to decorate the text of the holy book of Islam, the Qur'an. Muslim scribes used black ink and golden paper to write and draw, using an angled alphabet called Kuffi, or Kufi. Such writings appeared in the 8th century and reached their apex in the 10th century. Later, decorations of the margins of pages, displaying a variety of graphic techniques, were added in order to beautify the book. In the 12th century, the Naskh alphabet was invented; it featured curves instead of the angled lines of Kufi script. Other styles, such as Mohaghegh, Reyhan, Sols, Reghaa, and Toghii, appeared later on.\nIt is believed that playing cards were invented in China. Chinese playing cards, as we understand the term today, date from at least 1294, when Yen Sengzhu and Zheng Pig-Dog were apparently caught gambling in Enzhou (in modern Shandong Province). Cards entered Europe from the Islamic empire. The earliest authentic references to playing cards in Europe date from 1377. Europe changed the Islamic symbols such as scimitars and cups into graphical representations of kings, queens, knights and jesters. Different European countries adopted different suit systems. For instance, some Italian, Spanish and German decks of cards even today do not have queens. During the 15th century, German printers introduced a woodblock printing technique to produce playing cards. Lower production costs enabled the printed playing cards' quick exportation throughout Europe. The substitution of wood-block printing and hand coloring with copper-plate engraving during the 16th century was the next significant innovation in the manufacture of playing cards. The mass printing of playing cards was revolutionized by the introduction of color lithography in the early 19th century.\n\nThe Byzantine Empire began when the Emperor Constantine moved the headquarters of the Roman Empire from Rome to Byzantium (present day Istanbul) which he renamed Constantinople. The Byzantine empire, although marked by periodic revivals of a classical aesthetic of the art of the Roman empire and ancient Greek, was above all marked by the development of a new aesthetic which Josef Strzygowski viewed it as a product of \"oriental\" influences. The subject matter of Byzantine art was primarily religious and imperial. Byzantine art is more spiritual in content (figures presented as representations of the soul rather than the body) and yet more \"worldly\" in form with a show of gold, silver, precious and semi-precious stones.\nFrom ancient times graphic design has been used for decoration of pottery and ceramics\n\nLess is more. That is the basic premise of a minimalist color poster design. The Dutch painter Piet Mondrian in the years 1920-21 courageously introduced the style of minimalism in painting. His simple geometric compositions, together with the use of only three basic colors, blue, yellow, and red, in combination with black and white created\na new venue for the graphic designers. He demonstrated that with simple relocation of these colors, and experiment with the proportionality of various square surfaces one can achieve extremely different ambiances and various feelings. For the graphic designers who intend to convey a message with a minimum interference from the extraneous elements his experiment in minimalism was a valuable gift.\n\nA rebus (Latin: \"by things\") is a kind of word puzzle which uses pictures to represent words or parts of words, such as \"T,4,2\" instead of \"tea for two\". In 1977, the New York State Department of Commerce recruited Milton Glaser, a productive graphic designer to work on a marketing campaign for New York State. Glaser created this rebus-style icon which became a major success and has continued to be sold for years. Rebus has played an important role in creation of alphabets.\n\nHeraldry is the practice of designing and displaying coat of arms and heraldic badge and is rather common among all nations. For example, Romans used eagle as their coat of arms, French used fleur de lis, and Persians used the sign of their god, Ahura Mazda. Historically, it has been variously described as \"the shorthand of history\" and \"the floral border in the garden of history.\". It comes from the Germanic compound \"*harja-waldaz\", \"army commander\". The origins of heraldry lie in the need to distinguish participants in combat when their faces were hidden by iron and steel helmets. Eventually a formal system of rules developed into ever more complex forms of heraldry.\n\nA trademark is a distinctive sign or indicator used by an individual, company or other entity to identify its products or services and to distinguish them from those of other producers. A trademark is a type of intellectual property, and typically a name, word, phrase, logo, symbol, design, image, or a combination of these elements.\n\nRebranding means staying relevant as competition heats up and sales start to stagnate. In such circumstances companies often seek to breathe new life into the brand through rebranding. The idea behind it is that the assumptions made when the brand was established may no longer hold true.\n\nThe logo of the Socialist Party (France). The rose symbol represents; community (the flower's petals), socialism (its red color), taking care of those who are less able to compete (the fragility), the struggle (the thorns), cultural life (beauty). Historically, the red rose became the party's emblem during the nineteen-seventies. The fist symbol was a sign of resistance. Although the Mitterrand Socialists turned the fist into a graphic holding a rose.\n\nKnown worldwide by its panda logo, the Switzerland-based World Wildlife Fund (WWF) participates in international efforts to protect endangered species and their habitats.\n\nMédecins Sans Frontières, or Doctors Without Borders, is best known for its humanitarian projects in war-torn regions and developing countries facing endemic disease. Their logo using a minimalist approach creates its visual impact.\n\nIn 1921, Otto Neurath, an Austrian social scientist, introduced graphic design in order to facilitate the understanding of various social and economical trends through the creative use of statistical charts. In 1924, Neurath advocated the establishment of the Museum of Economy and Society, an institution for public education and social information. In May 1925, the Museum's first graphical displays was opened to the public. The exhibition showed various complicated social and economical trends. By using charts which were to be intuitive and interesting the attempt was to make those concepts easy to grasp. This style of presentation at the time was called the Viennese method, but now it is known as ISOTYPE charts (International System of Typographic Picture Education).\n\nOtto Neurath (1882–1945) was an enthusiast of sociology. After obtaining his PhD he worked on planning the war economy of the Austro-Hungarian empire. However, by 1919 he was engaged in the planning for a wholly new economic system of the chaotic and short-lived Bavarian Soviet Republic. He proposed for the abolition of money, but before this could be implemented, the republic was bloodily overthrown by Weimar's Social Democrats. Neurath escaped to Vienna, where he became an activist for the self-help squatters' movement. In the 1920s he joined the Vienna Circle of Logical Positivists, who attempted to establish a scientific foundation for philosophy; and at the same time he pioneered the graphic methods that became Isotype and were shown in the \"Museum of Society and Economy\". He fled Vienna after the collapse of its social democratic city government in 1934. Neurath's final years were spent in Britain, as postwar planner for the Midlands town of Bilston.\n\nAs Lupton argues: Neurath suggested \"two central rules for generating the vocabulary of international pictures: reduction, for determining the style of individual signs; and consistency, for giving a group of signs the appearance of a coherent system\". Reduction means finding the simplest expression of an object. For instance, silouette is a basic technique for reduction. It emulates the shadow of the image without any human intervention. Thus, it is a natural cast rather than a cultural interpretation. The sign as geometric representation of reality is both a rhetorical connotation and a practical technique for many symbol designers. Martin Krampen suggested \"simplified realism;\" he urged designers to \"start from silhouette photographs of objects...and then by subtraction...obtain silouette pictographs.\"\n\nGerd Arntz (1900–1988) was born in a German family of traders and manufacturers. He was a socio-political activist in Düsseldorf, where he joined a movement that aimed to turn Germany into a radical-socialist state form. As a revolutionary artist, Arntz was connected to the Cologne-based ‘progressive artists group’ (Gruppe progressiver Künstler Köln) and depicted the life of workers and the class struggle in abstracted figures on woodcuts. Published in leftist magazines, his work was noticed by Otto Neurath who for his ‘Vienna method of visual statistics’ needed a designer of pictograms that could summarize a subject at a glance. Neurath invited the young artists to come to Vienna in 1928, and work on further developing his ISOTYPE. Arntz designed around 4000 different pictograms and abstracted illustrations for this system.\n\nNeurath's motto was ‘words divide, images unite’. Many of his designs together with those of his protégé Gerd Arntz were the forebears of pictograms we now encounter everywhere, such as the man and woman on toilet doors. As Marina Vishmidt suggests: \"Neurath's pictograms owe much to the Modernist belief that reality may be modified by being codified – standardised, easy-to-grasp templates as a revolution in human affairs.\n\nThe logos and pictograms for Olympic Games change every four years and the sponsoring city develops its own logos. Pictograms first appeared at the Olympics in London in 1948. They came into wide use, since they simultaneously communicate a message to a large number of people who speak different languages. In the absence of such signs in venues such as Olympic village there would be a need for many written signs in different languages, for example for rowing such as; Roning، Κωπηλασία، Aviron, قایق رانی، and ボート競技 which not only would be costly but also may confuse the viewers. Symbols for individual sports developed by Masasa Katzoumie and Yoshiro Yamashita in Tokyo Olympics in 1964.\n\nPictograms in Mexico Olympic Games, 1968. A group of Olympic identity program designers collaborated on the creation of these symbols, which were employed to designate the events and installations for both the sports program and the Cultural Olympiad.\n\nInspired by the pictograms of Gerd Arntz, Otl Aicher, design director for the Munich 1972 games, in the words of Michael Bierut \"developed a set of pictograms of such breathtaking elegance and clarity that they would never be topped. Aicher (1922-1991), founder of the Ulm design school and consultant to Braun and Lufthansa, was the quintessential German designer: precise, cool and logical\".\n\nOlympic Games pictograms of Barcelona in 1992 were influenced by Aicher's work. However, the geometric shapes were abandoned in favour of the characteristic line of the emblem created by Josep. M. Trias and its stylized simplification of the human body in three parts.\n\nTwenty-four sport pictograms and a series of sport illustrations for the 2010 Winter Games are created by Dutch illustrator Irene Jacobs of I’m JAC Design.\n\nStatistics is becoming increasingly more important in modern society. Various computer software can easily transform a large set of data into charts, graphs, and statistics of various types in an attempt to provide us with succinct information to make decisions.\n\nRaymond Loewy was one of the best known industrial designers of the 20th century. Born in France, he spent most of his professional career in the United States. Among his many contributions were the Shell logo, the Greyhound bus, the S-1 locomotive, the Lucky Strike package, Coldspot refrigerators and the Studebaker Avanti. Loewy was first approached by the greyhound corporation to redesign its logo. The company's logo looked like a 'fat mongrel' he said. So, he created a slimmed-down version that is still used today.\n\nWilliam Golden is one of the pioneers of American graphic design. He was born in lower Manhattan, the youngest of twelve children. His only formal schooling was at the Vocational School for Boys, where he learned photoengraving and the basics of commercial design. In conjunction with the Didot typeface, Golden developed the famous CBS Eye logo. It has been suggested that the eye was inspired by an article in Alexey Brodovitch's \"Portfolio\" about the subject of Shaker design.\n\nPlacard and posters existed from the ancient times. The Persian reliefs that depicted the important historical events; and the Greek \"axons\" and the Roman \"Albums\", with their decorative designs and announcements, were quite similar to today's posters. In ancient Greece the name of athletes, and games schedules were written on columns that were slowly turning on an axis. Romans used whitewashed walls in their markets in which sellers, money lenders, and slave traders wrote their announcements and advertised for their products, and to attract the attention of customers they added attractive designs to their announcements.\n\nAround 1450, Johann Gutenberg's printing press made books widely available in Europe. The book design of Aldus Manutius developed the book structure which would become the foundation of western publication design. With the development of the lithographic process, invented by a Czech named Alois Senefelder in 1798 in Austria, the creation of posters become feasible. Although handmade posters existed before, they were mainly used for government announcements. William Caxton, who in 1477 started a printing company in England, produced the first printed poster.\nIn 1870, the advertising poster emerged.\nEngraving is the practice of incising a design onto a hard, usually flat surface, by cutting grooves into it. The process was developed in Germany in the 1430s from the engraving used by goldsmiths to decorate metalwork. Engravers use a hardened steel tool called a burin to cut the design into the surface, most traditionally a copper plate. Gravers come in a variety of shapes and sizes that yield different line types.\n\nEtching is the process of using strong acid or mordant to cut into the unprotected parts of a metal surface to create a design in intaglio in the metal. This technique is believed to have been invented by Daniel Hopfer (c. 1470-1536) of Augsburg, Germany, who decorated armour in this way, and applied the method to printmaking. Etching soon came to challenge engraving as the most popular printmaking medium. Its great advantage was that, unlike engraving which requires special skill in metalworking, etching is relatively easy to learn for an artist trained in drawing.\n\nPerhaps it would be possible to consider William Morris the father of modern graphics. In the second half of the 19th century his Kelmscott Press produced many graphic designs, and created a collectors market for this kind of art. In Oxford he associated with artists like Burne-Jones, and Dante Gabriel Rossetti. Together they formed the Pre-Raphaelites group, and their ideas influenced the modern graphic design considerably.\n\nAfter the Second World War, with the emergence new color printing technology and particularly appearance of computers the art of posters underwent a new revolutionary phase. People can create color poster on their laptop computers and create color prints at a very low cost. Unfortunately, the high cost sophisticated printing processes can only be afforded mostly by the government entities and large corporations. With the emergence of internet the role of posters in conveying information has greatly diminished. However, some artist still use the chromolithography in order to create works of arts in the form of print. In this regard the difference between painting and print has been narrowed considerably.\n\nThe word \"psychedelic\" means \"mind manifesting\". Psychedelic art is art inspired by the psychedelic experience induced by drugs, and refers above all to the art movement of the 1960s counterculture. Psychedelic visual arts were a counterpart to psychedelic rock music. Concert posters, album covers, lightshows, murals, comic books, underground newspapers and more reflected revolutionary political, social and spiritual sentiments inspired by psychedelic states of consciousness.\n\nAlthough San Francisco remained the hub of psychedelic art into the early 1970s, the style also developed internationally. Pink Floyd worked extensively with London-based designers, Hipgnosis to create graphics to support the concepts in their albums like this cover of \"Soundtrack from the Film 'More\"'. \"Life\" magazine's cover and lead article for the September 1, 1967 issue at the height of the Summer of Love focused on the explosion of psychedelic art on posters and the artists as leaders in the hippie counterculture community.\n\nYellow Submarine was a milestone in graphic design, inspired by the new trends in art, it sits alongside the dazzling Pop Art styles of Andy Warhol, Martin Sharp, Alan Aldridge and Peter Blake. Heinz Edelman was hired by TVC as the art director for this film. Before making Yellow Submarine, TVC had produced The Beatles, a 39 episode TV series \"produced\" by Al Brodax and King Features. Despite the critical acclaim of his design work for the film, Edelman never worked on another animated feature.\n\nPeter Max's art work was a part of the psychedelic movement in graphic design. His work was much imitated in commercial illustration in the late 1960s and early 1970s. In 1970, many of Max's products and posters were featured in the exhibition \"The World of Peter Max\" which opened at the M.H. de Young Memorial Museum in San Francisco. He appeared on the cover of \"Life\" magazine with an eight-page feature article as well as \"The Tonight Show Starring Johnny Carson\" and \"The Ed Sullivan Show\".\n\nThe distinctive aesthetics of Japanese graphic design have been admired over many decades, winning awards at prestigious international venues.\n\nThe works of Japanese graphic designers are noted for their resourcefulness, powerful visual expression and extraordinary technical quality of print.\n\nThe distinctive artistic language and typographic sophistication show particularly in Japanese poster-design. The Japanese poster is a compelling pictorial medium and an original work of art, reflecting in full the designer's creative talent.\n\nThe poster \"Revolution promotes production\", created by He Shuxui, celebrates traditional ceramic painting techniques. A plaque in the background commemorates a group of ceramic workers as an outstanding productive unit, 1974.\n\nA worker named Wang Qing Cang created the poster \"The three countries of Indo Zhina (Lao, Cambodia, Vietnam) will win!\". On the upper left side, it says \"Enemies are getting sicker and sicker every day, and we are getting better and better every day.\" (The U.S. supported Indo Zhina (Indochina) governments while China supported their communist guerilla forces.) October 1964.\n\nThe poster \"Mao Ze Dong at Jing Gang Mountain\" depicts a young Mao Ze Dong sitting against a background of Mount Jing Gang. Jing Gang Shan (Jing Gang Mountain) symbolizes the Mao Ze Dong leadership and his vision to unite the oppressed masses to fight against and fight against the ruling class. Created by Liu Chun Hua and Wang Hui, October 1969.\n\nThe poster, \"Time is Money\", features the famous Canadian doctor Norman Bethune (Dr. Bai Qiuen in Chinese), racing to rescue another patient. Bethune became an early proponent of universal health care, the success of which he observed during a visit to the Soviet Union. As a doctor in Montreal, Bethune frequently sought out the poor and gave them free medical care. As a thoracic surgeon, he traveled to Spain (1936–1937) and to China (1938–1939) to perform battlefield surgical operations on war casualties. Created by Zhang Xin Gua. Hebei People's Publishing House.\n\nRichard Avedon was an American photographer. Avedon capitalized on his early success in fashion photography and expanded into the realm of fine art. This is a solarised poster portraits of the Beatles, originally produced for 9 January 1967 edition of the American magazine \"Look\".\n\nThe Barack Obama \"hope\" poster is an iconic image of Barack Obama designed by artist Shepard Fairey. The image became one of the most widely recognized symbols of Obama's campaign message, spawning many variations and imitations, including some commissioned by the Obama campaign. In January 2009, after Obama had won the election, Fairey's mixed-media stenciled portrait version of the image was acquired by the Smithsonian Institution for its National Portrait Gallery.\n\ndo\n\nWith the arrival of computer aided graphic design an assortment of novel effects, digital techniques, and innovative styles have been emerged in poster designs. With software such as Adobe Photoshop, Corel and Windows' Paint program, image editing has become very cheap, and artists can experiment easily with a variety of color schemes, filters and special effects. For instance, utilizing various filters of Photoshop, many artists have created \"vectored\" designs in posters where a photographic image is solarized, sharpened, rendered into watercolor or stained glass effects or converted into bare lines with block colors. Other designs created soft or blurry styles, ripple or cascade effects and other special filters.\n\nGraphic design is used in advertising to announce a persuasive message by an identified sponsor; or a promotion by a firm of its products to its existing and potential customers. Egyptians used papyrus to make sales messages and wall posters. Commercial messages and political campaign displays have been found in the ruins of Pompeii and ancient Arabia. Lost and found advertising on papyrus was common in Ancient Greece and Ancient Rome. Wall or rock painting for commercial advertising is another manifestation of an ancient advertising form, which is present to this day in many parts of Asia, Africa, and South America.\n\nIn the early 20th century, Germany became the cradle of many of the avant-garde art movements particularly for posters. This created the \"Plakatstil\" or \"Poster style\" movement. This movement became very influential and had a considerable impact on the graphic design for posters. Posters in this style would feature few but strong colours, a sharp, non-cluttered, minimal composition and bold, clear types.\n\nLudwig Hohlwein was born in Germany in 1874. He was trained and practiced as an architect until 1906 when he switched to poster design. Hohlwein's adaptations of photographic images was based on a deep and intuitive understanding of graphical principles. His creative use of color and architectural compositions dispels any suggestion that he uses photos as a substitute for creative design.\n\nfor Riquet Pralinen Tea c. 1920-1926. Hohlwein was born in the Rhine-Main region of Germany, though he and his work are associated with Munich and Bavaria in southern Germany. There were two schools of Gebrauchsgrafik in Germany at the time, North and South. Hohlwein's high tonal contrasts and a network of interlocking shapes made his work instantly recognizable.\n\nPoster historian Alain Weill comments that \"Hohlwein was the most prolific and brilliant German posterist of the 20th century... Beginning with his first efforts, Hohlwein found his style with disconcerting facility. It would vary little for the next forty years. The drawing was perfect from the start, nothing seemed alien to him, and in any case, nothing posed a problem for him. His figures are full of touches of color and a play of light and shade that brings them out of their background and gives them substance\"\n\nOver the course of his career, which lasted well into the 1950s, Lucian Bernhard became a prolific designer not only of innovative posters but of trademarks, packaging, type, textiles, furniture, and interior design.\n\nThe internationally recognized artist Otl Aicher was a graphic designer, urban planner, photographer, and the mastermind behind the imagery for the 1972 Munich Olympics and the Rotis typeface. Growing as a child in Nazi Germany, Aicher, along with his friends Hans and Sophie Scholl, organized the anti-Nazi political organization Die Weisse Rose (the White Rose). In 1943, the Scholls and Aicher were arrested by the Nazi party. While Aicher was released, the Scholls went to trial where they were found guilty of treason and executed. After the war Aicher went on to help rebuild his ravaged city of Ulm and to found the influential international school of design, Hochschule für Gestaltung (HfG).\n\nIn Munich's original bid for 1972 Olympic one of the main promises was to create a synthesis between sport and art. Otl Aicher was appointed as the head of the committee's visual design group, and his mandate was to deploy art in a relatively new role of promoting this global public event. From the start, posters were high on the agenda of the organizing committee, and ideas were discussed as early as September 1967 to publish a series of art posters that would ‘relate artistic activity to the Olympic Games and engage the best artists to collaborate’, and also to commission an internationally known artist for the official poster.\n\nOtl Aicher created the official posters for the 1972 Olympic Games in Munich. As he pointed out in his essay \"Entwurf der Moderne\" (Designing the Modern Era), the German word entwerfen, meaning \"to draft \"or \"design\", also contains the verb werfen, meaning \"to throw\". But where? To whom? What? And with what intention? As Benjamin Secher writes: \"He devised an invigorating, almost Day-glo palette for the Olympics that was utterly free of red and black - banned for their association with the German flag. Athletes depicted in the official posters for each sport had their uniforms stripped of any national identifier, leaving the emphasis firmly on individual effort. Even the logo for the Games, a graphic of a radiant sun, hammered home the message of universality and, above all, optimism.\"\n\nAicher developed a comprehensive system to articulate the games' character across a wide range of materials, from signage to printed pieces and even staff uniforms. As the introduction to his exhibition at San Francisco Museum of Modern Art states: \"His works including official posters and sporting event tickets, demonstrate the design tools Aicher used to join individual elements to the collective: structural grids, a bold and animating color palette, and ingenious pictograms. Aicher's orderly and pleasant design nimbly carried the weight of modern German history as it repositioned the nation's hospitality on the world stage\".\n\nThis is a poster of 1972 Olympics Yachting in Germany designed by Otl Aicher. Using a bright color scheme, borrowed from 60s pop art and psychedelic art, and combining it with German modernism Aicher creates this visual graphic program.\n\nNike's \"My Butt is Big poster\" appears to convey a bold and honest statement. The only part of a body in the picture is a butt. The text of a poem on the right repeats the curved form of the woman's bottom which is repeated again with some vividly colored splosh of red and purple dots in the background. The background is white, which contrasts with the darker skin of the model. The statement, \"My butt is big\" is red and larger than the rest of the poem.\nImage:Courvoisier Cognac.jpg | This is a modern advertisement poster for Courvoisier Cognac. A balanced composition of the hands, feet, and face of the figure on a black background appear to convey the message of this poster.\n\nThis is a look alike poster advertisement for Wendy's \"where's the Beef?\" campaign. In the TV version of this ad, Clara Peller, a gray-haired actress, stared at an unimpressive looking hamburger and asked, \"Where's the beef?\" This simple message was so sharp that by asking the same question about his rival's program Vice President Walter Mondale effectively neutralized Colorado Senator Gary Hart's momentum in the 1984 presidential campaign.\n\nThis is a perfume advertisement for Chanel No 5. The combination of the female figure with the number 5, together with the striking color of dress have resulted in creation of its visual graphic impact.\n\nA comic refers to a magazine or book of narrative artwork and, virtually always, dialog and descriptive prose. Despite the term, the subject matter in comic is not necessarily humorous; in fact, it is often serious and action-oriented. Due to the fact that graphic design constitutes the main foundation of comics it plays a crucial role in conveying various narratives through its compositional devices, line drawings and colouring scheme.\n\nSuperman, from the cover art of Superman, issue 204 (April 2004). Art by Jim Lee and Scott Williams. Superman is widely considered to be an American cultural icon. Created by American writer Jerry Siegel and Canadian-born artist Joe Shuster in 1932. The character first appeared in \"Action Comics\" in 1938. The character's appearance is distinctive and iconic: a red, blue and yellow costume, complete with cape and with a stylized \"S\" shield on his chest.\n\nShang-Chi was created by writer Steve Englehart and artist Jim Starlin. He has no special superpowers, but he exhibits extraordinary skills in the martial arts. 1972\n\nThis is Steranko's Contessa Valentina Allegra di Fontaine, from \"Strange Tales\", (Volume 168, May 1968). Lichtenstein's Drowning Girl, and its word balloon appears to have been inspired by a comic similar to this work.\n\nSelecting the old-fashioned comic strip as subject matter, Roy Lichtenstein used the splash page of a romance story lettered by Ira Schnapp in Secret Hearts, (volume 83, November 1962), and slightly reworked the art and dialogue by re-lettering Schnapp's original word balloon. This precise composition, titled Drowning Girl (1963) is now part of the permanent collection of the Museum of Modern Art, New York.\n\nCover of \"Wanted\" a graphic novel by Mark Millar, J. G. Jones, Paul Mounts.\n\nThe cover of \"Too Cool to be Forgotten\", a comics novel by Alex Robinson. Robinson's draftsmanship balances graphic panels with realism.\n\nPoster for \"Persepolis\" (2000), L'Association French edition by Marjane Satrapi an Iranian graphic novelist. Persepolis was adapted into an animated film of the same name which debuted at the Cannes Film Festival in May 2007 and shared a Special Jury Prize.\n\nCover of \"\" (1988). Art by Brian Bolland.\n\nThe aim of graphic design is to make a web site understandable, memorable and attractive to the end user as well to present its content in a user friendly fashion\n. The web dates back to the early 1980s at CERN, a European high energy physics research facility. Tim Berners-Lee who did the initial development stage was interested in the ability to link academic papers electronically and to utilize the internet in order to correspond with people in other laboratories around the world. He is credited with the construction of the first website in August 1991.\n\nToday graphic design has penetrated into all aspects of modern life. In particular modern architecture has been influenced by graphics.\nGraphic design is derived from the demands of a culture and economy, thus creating inevitable biases. This bias is limited to the art form of graphic design because other forms of art are often derive from an internal state. Work of graphic designers is rooted in controversy as the US aims to evolve toward a society of equality. Specifically graphic design has a direct impact on the views and attitude of women in culture. Designs are limited to interpretation making the biases indirect and misinterpreted. While beliefs and attitudes change in the US, graphic design will inevitably change as well. \n"}
{"id": "28245717", "url": "https://en.wikipedia.org/wiki?curid=28245717", "title": "IMX-101", "text": "IMX-101\n\nIMX-101 is a high-performance insensitive high explosive composite mixture developed by BAE Systems and the United States Army to replace TNT in artillery shells, starting as soon as 2011. IMX stands for \"Insensitive Munitions eXplosives\", which refers to the purpose of IMX-101: to provide explosive force equivalent to TNT without its sensitivity to shocks such as gunfire, explosions from improvised explosive devices, fire, and shrapnel. For example, it is believed that a training incident in Nevada which killed seven Marines would not have occurred with the new explosive. On March 23, 2013, the United States Army ordered $780 million worth of the explosive, with a production of millions of pounds annually, to be produced by BAE at Holston Army Ammunition Plant in Tennessee. The new explosive will cost $8 per pound, compared to $6 per pound for TNT.\n\nTime Magazine called IMX-101 one of the \"50 best inventions of 2010\".\n\nIt is composed of ingredients including 2,4-dinitroanisole (DNAN) and nitrotriazolone (NTO). The nominal composition is DNAN, NQ (nitroguanidine), NTO. Trace amounts of MNA (N-methyl-p-nitroaniline) are included in some formulations to aid in processing.\n\nThe performance of PAX-28, a thermobaric, containing a mixture of RDX, DNAN, Al, AP and MNA was found to have an indoor explosive equivalency factor of 1.62 when compared to Composition B. OSX-12 is being studied as a replacement to PAX-28.\n\nLike Composition B, IMX formulations are melt-castable without thermal degradation, and are thus processed into munitions by a melt pour process starting with a batch melt kettle heated by a steam heat exchanger.\n\n\n"}
{"id": "290330", "url": "https://en.wikipedia.org/wiki?curid=290330", "title": "Incendiary device", "text": "Incendiary device\n\nIncendiary weapons, incendiary devices or incendiary bombs are weapons designed to start fires or destroy sensitive equipment using fire (and sometimes used as anti-personnel weaponry), that use materials such as napalm, thermite, magnesium powder, chlorine trifluoride, or white phosphorus. Though colloquially often known as bombs, they are not explosives but in fact are designed to slow the process of chemical reactions and use ignition rather than detonation to start and or maintain the reaction. Napalm for example, is petroleum especially thickened with certain chemicals into a 'gel' to slow, but not stop, combustion, releasing energy over a longer time than an explosive device. In the case of napalm, the gel adheres to surfaces and resists suppression.\n\nA range of early thermal weapons were in use ancient and early armies using hot pitch, oil, resin, animal fat and other similar compounds. Substances such as quicklime and sulfur could be toxic and blinding. Incendiary mixtures, such as the petroleum-based Greek fire, were launched by throwing machines or administered through a siphon. Sulfur- and oil-soaked materials were sometimes ignited and thrown at the enemy, or attached to spears, arrow and bolts and fired by hand or machine. Some siege techniques—such as mining and boring—relied on combustibles and fire to complete the collapse of walls and structures.\n\nTowards the latter part of the period, gunpowder was invented, which increased the sophistication of the weapons, starting with fire lances.\n\nThe first incendiary devices to be dropped during World War I fell on coastal towns in the south west of England on the night of 18–19 January 1915. The small number of German bombs, also known as firebombs, were finned containers filled with kerosene and oil and wrapped with tar-covered rope. They were dropped from Zeppelin airships. On 8 September 1915, Zeppelin L-13 dropped a large number of firebombs, but even then the results were poor and they were generally ineffective in terms of the damage inflicted. They did have a considerable effect on the morale of the civilian population of the United Kingdom.\n\nAfter further experiments with 5-litre barrels of benzol, in 1918, the B-1E Elektron fire bomb (German: \"Elektronbrandbombe\") was developed by scientists and engineers at the Griesheim-Elektron chemical works. The bomb was ignited by a thermite charge, but the main incendiary effect was from the magnesium and aluminium alloy casing, which ignited at 650° Celsius, burned at 1,100 °C and emitted vapour that burned at 1,800 °C. A further advantage of the alloy casing was its lightness, being a quarter of the density of steel, which meant that each bomber could carry a considerable number. The German High Command devised an operation called \"The Fire Plan\" (German: \"Der Feuerplan\"), which involved the use of the whole German heavy bomber fleet, flying in waves over London and Paris and dropping all the incendiary bombs that they could carry, until they were either all shot down or the crews were too exhausted to fly. The hope was that the two capitals would be engulfed in an inextinguishable blaze, causing the Allies to sue for peace. Thousands of Elektron bombs were stockpiled at forward bomber bases and the operation was scheduled for August and again in early September 1918, but on both occasions, the order to take off was countermanded at the last moment, perhaps because of the fear of Allied reprisals against German cities. The Royal Air Force had already used their own \"Baby\" Incendiary Bomb (BIB) which also contained a thermite charge. A plan to fire bomb New York with new long range Zeppelins of the L70 class was proposed by the naval airship fleet commander Peter Strasser in July 1918, but it was vetoed by Admiral Reinhard Scheer.\n\nIncendiary bombs were used extensively in World War II as an effective bombing weapon, often in a conjunction with high-explosive bombs. Probably the most famous incendiary attacks are the bombing of Dresden and the bombing of Tokyo on 10 March 1945. Many different configurations of incendiary bombs and a wide range of filling materials such as isobutyl methacrylate (IM) polymer, napalm, and similar jellied-petroleum formulas were used, many of them developed by the US Chemical Warfare Service. Different methods of delivery, e.g. small bombs, bomblet clusters and large bombs, were tested and implemented. For example, a large bomb casing was filled with small sticks of incendiary (bomblets); the casing was designed to open at altitude, scattering the bomblets in order to cover a wide area. An explosive charge would then ignite the incendiary material, often starting a raging fire. The fire would burn at extreme temperatures that could destroy most buildings made of wood or other combustible materials (buildings constructed of stone tend to resist incendiary destruction unless they are first blown open by high explosives).\nThe German \"Luftwaffe\" started the war using the 1918-designed one-kilogram magnesium alloy B-1E \"Elektronbrandbombe\"; later modifications included the addition of a small explosive charge intended to penetrate the roof of any building which it landed on. Racks holding 36 of these bombs were developed, four of which could, in turn, be fitted to an electrically triggered dispenser so that a single He 111 bomber could carry 1,152 incendiary bombs, or more usually a mixed load. Less successful was the \"Flammenbombe\", a 250 kg or 500 kg high explosive bomb case filled with an inflammable oil mixture, which often failed to detonate and was withdrawn in January 1941.\n\nIn World War II, incendiaries were principally developed in order to destroy the many small, decentralised war industries located (often intentionally) throughout vast tracts of city land in an effort to escape destruction by conventionally aimed high-explosive bombs. Nevertheless, the civilian destruction caused by such weapons quickly earned them a reputation as terror weapons with the targeted populations. The Nazi regime began the campaign of incendiary bombings at the start of World War II with the bombing of Warsaw, and continued with the London Blitz and the bombing of Moscow, among other cities. Later, an extensive reprisal was enacted by the Allies in the strategic bombing campaign that led to the near-annihilation of many German cities. In the Pacific War, during the last seven months of strategic bombing by B-29 Superfortresses in the air war against Japan, a change to firebombing tactics resulted in the death of 500,000 Japanese and the homelessness of 5 million more. Sixty-seven Japanese cities lost significant areas to incendiary attacks. The most deadly single bombing raid in history was Operation Meetinghouse, an incendiary attack that killed some 100,000 Tokyo residents in one night.\n\nThe incendiary bomb, developed by ICI, was the standard light incendiary bomb used by RAF Bomber Command in very large numbers, declining slightly in 1944 to 35.8 million bombs produced (the decline being due to more bombs arriving from the United States). It was the weapon of choice for the British \"dehousing\" plan. The bomb consisted of a hollow body made from aluminium-magnesium alloy with a cast iron/steel nose, and filled with thermite incendiary pellets. It was capable of burning for up to ten minutes. There was also a high explosive version and delayed high explosive versions (2–4 minutes) which were designed to kill rescuers and firefighters. It was normal for a proportion of high explosive bombs to be dropped during incendiary attacks in order to expose combustible material and to fill the streets with craters and rubble, hindering rescue services.\n\nTowards the end of World War Two, the British introduced a much improved incendiary bomb, whose fall was retarded by a small parachute and on impact sent out an extremely hot flame for ; This, the \"Incendiary Bomb, 30-lb., Type J, Mk I\", burned for approximately two minutes. Articles in late 1944 claimed that the flame was so hot it could crumble a brick wall. And for propaganda purposes the RAF dubbed the new incendiary bomb the Superflamer.\nAround fifty-five million incendiary bombs were dropped on Germany by Avro Lancasters alone.\n\nMany incendiary weapons developed and deployed during World War II were in the form of bombs and shells whose main incendiary component is white phosphorus (WP), and can be used in an offensive anti-personnel role against enemy troop concentrations, but WP is also used for signalling, smoke screens, and target-marking purposes. The U.S. Army and Marines used WP extensively in World War II and Korea for all three purposes, frequently using WP shells in large 4.2-inch chemical mortars. WP was widely credited by many Allied soldiers for breaking up numerous German infantry attacks and creating havoc among enemy troop concentrations during the latter part of World War II. In both World War II and Korea, WP was found particularly useful in overcoming enemy human wave attacks.\n\nModern incendiary bombs usually contain thermite, made from aluminium and ferric oxide. It takes very high temperatures to ignite, but when alight, it can burn through solid steel. In World War II, such devices were employed in incendiary grenades to burn through heavy armour plate, or as a quick welding mechanism to destroy artillery and other complex machined weapons.\n\nA variety of pyrophoric materials can also be used: selected organometallic compounds, most often triethylaluminium, trimethylaluminium, and some other alkyl and aryl derivatives of aluminium, magnesium, boron, zinc, sodium, and lithium, can be used. Thickened triethylaluminium, a napalm-like substance that ignites in contact with air, is known as thickened pyrophoric agent, or TPA.\n\nNapalm was widely used by the United States during the Korean War, most notably during the battle \"Outpost Harry\" in South Korea during the night of June 10–11, 1953. Eighth Army chemical officer Donald Bode reported that on an \"average good day\" UN pilots used 70,000 gallons of napalm, with approximately 60,000 gallons of this thrown by US forces. Winston Churchill, among others, criticized American use of napalm in Korea, calling it \"very cruel\", as the US/UN forces, he said, were \"splashing it all over the civilian population\", \"tortur[ing] great masses of people\". The American official who took this statement declined to publicize it.\n\nDuring the Vietnam War, the U.S. Air Force developed the CBU-55, a cluster bomb incendiary fuelled by propane, a weapon that was used only once in warfare. Napalm however, became an intrinsic element of U.S. military action during the Vietnam War as forces made increasing use of it for its tactical and psychological effects. Reportedly about 388,000 tons of U.S. napalm bombs were dropped in the region between 1963 and 1973, compared to 32,357 tons used over three years in the Korean War, and 16,500 tons dropped on Japan in 1945.\n\nNapalm proper is no longer used by the United States, although the kerosene-fuelled Mark 77 MOD 5 Firebomb is currently in use. The United States has confirmed the use of Mark 77s in Operation Iraqi Freedom in 2003.\n\nSignatory states are bound by Protocol III of the UN Convention on Conventional Weapons which governs the use of incendiary weapons:\n\n\nProtocol III states though that incendiary weapons do not include:\n\n"}
{"id": "29171859", "url": "https://en.wikipedia.org/wiki?curid=29171859", "title": "Interior protection", "text": "Interior protection\n\nInterior protection is the general term for the installation of temporary dust and debris containment systems during re-roofing, remodeling, or other construction related projects. A temporary interior protection system can be installed in almost any environment, from manufacturing plants, production facilities, class A office space, hospitals, schools or warehouses. Interior protection services include suspended ceilings, construction wall barriers, high structure cleaning and other types of custom applications. Using reinforced engineered poly film and proven installation methods, temporary interior protection systems prevent re-roofing and remodeling dust and debris from cross contaminating clean room type environments within commercial facilities, retail stores, class A office space, and just about any type of environment you want kept clean.\n\nThe foundation of any interior protection project is a high-quality suspended ceiling. Using reinforced engineered poly film and innovative installation methods to prevent contamination due to falling dust and debris during re-roofing and remodeling projects. Suspended ceilings are installed before the roof replacement or repair begins, remain in place throughout the project, and are removed upon completion. Therefore, providing valuable uninterrupted protection and peace of mind. Suspended ceilings offer security against falling dust and debris and other types of contamination.\n\nAnother application of interior protection is construction wall barriers. They are used to separate a construction project that produces dust, debris, and/or odor from the rest of business operations. This includes scarifying and re-flooring project, concrete cutting, remodeling, installation of new equipment, tenant improvements, restoration, and reparation. Temporary Construction wall barriers involve installing the poly engineered film floor to ceiling creating a frameless wall – a protective envelope separating the workplace from the work zone. Construction wall barriers can also be specifically customized to meet your project requirements, including doorways, tunnels, person-doors, and air filters. While not only preventing dust and debris from spreading, a vertical wall or temporary construction partition also confines contractors to their area, keeps employees safe, and prevents spread of unwanted odors.\n\nEvery re-roofing or remodeling project creates dust and debris. Although the Suspended Ceiling catches the contamination, another aspect of interior protection is High Structure Cleaning. High Structure Cleaning is performed during take down of the suspended cover by using hand brooms, brushes, HEPA vacuums and other tools to clean and remove non-adhered dust and debris captured on pipes, ductwork, metal beams, trusses, and other horizontal surfaces. High Structure Cleaning service provides additional assurances against site contamination.\n\nEvery re-roofing or remodeling project has distinct challenges. Equally, no two temporary interior protection installs are alike. In the course of interior protection numerous situations may arise which require custom applications. Using proven methods and installation techniques. A temporary interior protection system can be adapted to unique situations and circumstances, such as zipper doors, curtain walls, and suspended netting.\n\nSpecial attention needs to be given to safety as related to fire sprinklers when dust protection is installed as a ceiling. A sprinkler impairment plan or solution needs to be discussed with the safety officer, the building insurance company and the local Fire Marshall prior to any interior protection system installation. Installing the interior protection above the sprinklers is not a solution as a fire could cause a localized hole directly above the fire and this hole would act as a vent trapping the heat above the fire sprinklers or the heat could cause the protection to droop onto sprinkler head thus covering the spray head reducing the effectiveness of the water spray.\n"}
{"id": "2363918", "url": "https://en.wikipedia.org/wiki?curid=2363918", "title": "Lift chair", "text": "Lift chair\n\nLift chairs are chairs that feature a powered lifting mechanism that pushes the entire chair up from its base and so assists the user to a standing position.\n\nIn the United States, lift chairs qualify as Durable Medical Equipment under .\n\nIn a February 1989 report released by the Inspector General of the US Department of Health and Human Services, it was found that: lift chairs might not possibly meet Medicare's requirements for Durable Medical Equipment and lift chair claims need to be re-regulated. The report was stimulated by an increase in lift chair claims between 1984 and 1985 from 200,000 to 700,000. A New York Times article stated that aggressive TV ads were pushing consumers to inquire about lift chairs and, once consumers called in, a form was sent to them for their physicians to sign. Some companies would ship lift chairs before receiving a physician's signature; therefore, forcing the physicians to sign or else their patient will be forced to pay for the chair.\n\n"}
{"id": "13706125", "url": "https://en.wikipedia.org/wiki?curid=13706125", "title": "List of emerging technologies", "text": "List of emerging technologies\n\nEmerging technologies are those technical innovations which represent progressive developments within a field for competitive advantage.\n\n\n"}
{"id": "35880640", "url": "https://en.wikipedia.org/wiki?curid=35880640", "title": "Luminous at Darling Quarter", "text": "Luminous at Darling Quarter\n\nLuminous Interactive digital art platform was the brainchild of Lend Lease Group in collaboration with Ramus Illumination, and is billed, according to the Wall Street Journal, as 'the world's biggest interactive permanent light display'. It was officially opened in the Darling Quarter Precinct in Sydney Central Business District (CBD) on 18 May 2012. \n\nLuminous at Darling Quarter is a permanent platform solely for illuminated digital 'art' – both animated and static. The 'canvas' extends over four levels of two campus-style buildings, covering 557 windows in total, and presents a digital façade spanning a distance of 150 metres. The artistic and design direction was set by international light artist Bruce Ramus, with Ramus studio responsible for design management and production of the work. Fixtures were manufactured by Australian architectural lighting specialists Klik Systems using advanced LED systems.\n\nInteractive content is available from 6pm, Friday through Sunday, with touch screen kiosks in Tumbalong Park allowing the general public to paint their own digital designs and play over-scaled arcade games on the buildings in front of them. Web surfers can also play using their smart phone or contribute on their computer through a dedicated website. The online component of Luminous has similarities to Project Blinkenlights, a Berlin digital light installation, but expands the concept for the general public, allowing user-friendly interaction in an unprecedented way.\n\nThe Commonwealth Bank of Australia occupying the buildings is joint partner in the project along with the Sydney Harbour Foreshore Authority and Lend Lease. The consortium selected Bruce Ramus as the first artist, Artistic Director and Lighting Designer for Luminous. Canadian-born Ramus made his name lighting stage shows for U2 and David Bowie, and is design mentor for Sydney Opera House.\n\nAt Darling Quarter, each window forms a 'pixel' in the canvas, lit with Klik Systems's energy efficient LEDs, and seen from a distance can form a coherent animated picture. A sophisticated colour palette is available thanks to white LEDs added to the standard RGB (red, green, blue) colours. The system is also capable of integrating music: graphic synchronization allows for sound-based designs to be visualized on the canvas.\n\nContrary to many light installations, direct light is minimized to viewers, instead using the automated timber louvers to reflect light from Klik System's linear LEDs (running along the windowsills, angled upwards with a 10-degree spreader lens). The entire project offsets almost all its energy use, thanks to photo-voltaic panels on the roof of both Darling Quarter buildings. The project allegedly uses the same power as 5 household vacuum cleaners.\n\nLend Lease designed and constructed Darling Quarter for owner APPF Commercial and first proposed the idea of a permanent space for illuminated art. Darling Quarter precinct includes a community green, children’s playground, and a large number of world-food restaurants, cafes and bars, and reflects Lend Lease’s enthusiasm for iconic new spaces for future generations.\n\nThe digital façade is in action six evenings a week. On Tuesdays, Wednesdays and Thursdays, displaying ‘soft’ images (such as clouds or waves). On Friday, Saturday and Sunday 'Luminous' displays interactive artworks that aim to invite debate and make art critics – and artists - of every citizen and visitor. \n\nSydney has a growing reputation for light shows, particularly due to the annual VIVID Sydney festival and the SPARC DESIGN lighting conferences at the Museum of Modern Art, which are regularly sponsored by Klik Systems and other long term supporters in the Australian lighting industry. Vivid Sydney was inaugurated in 2008, curated in 2010 by Laurie Anderson and Lou Reed and is billed as an international carbon-neutral festival of Light, Music and Ideas. The 2012 musical line-up includes Sufjan Stevens and Florence & The Machine.\n\n"}
{"id": "784621", "url": "https://en.wikipedia.org/wiki?curid=784621", "title": "Many-body theory", "text": "Many-body theory\n\nMany-body theory (or many-body physics) is an area of physics which provides the framework for understanding the collective behavior of large numbers of interacting particles, often on the order of Avogadro's number. In general terms, many-body theory deals with effects that manifest themselves only in systems containing large numbers of constituents. While the underlying physical laws that govern the motion of each individual particle may (or may not) be simple, the study of the collection of particles can be extremely complex. In some cases emergent phenomena may arise which bear little resemblance to the underlying elementary laws.\n\n"}
{"id": "810160", "url": "https://en.wikipedia.org/wiki?curid=810160", "title": "Mercedes-Euklid", "text": "Mercedes-Euklid\n\nThe Mercedes-Euklid is a German-invented calculator from the early twentieth century. It was built by \"Mercedes Büro-Maschinen Werke AG\" in Thuringia, Germany in 1905. The first manual mechanical models utilized a proportional-lever design invented by Christel Hamann in 1903.\n\nAn electric model, the Mercedes Euklid 30, was released circa 1945, though it may not have been the company's first electric calculator. In the 1960s, another electric model was released under the name \"Cellatron.\"\n"}
{"id": "1268216", "url": "https://en.wikipedia.org/wiki?curid=1268216", "title": "Mercury(I) chloride", "text": "Mercury(I) chloride\n\nMercury(I) chloride is the chemical compound with the formula HgCl. Also known as the mineral calomel (a rare mineral) or mercurous chloride, this dense white or yellowish-white, odorless solid is the principal example of a mercury(I) compound. It is a component of reference electrodes in electrochemistry.\n\nThe name calomel is thought to come from the Greek καλός \"beautiful\", and μέλας \"black\"; or καλός and μέλι \"honey\" from its sweet taste. The \"black\" name (somewhat surprising for a white compound) is probably due to its characteristic disproportionation reaction with ammonia, which gives a “spectacular” black coloration due to the finely dispersed metallic mercury formed. It is also referred to as the mineral \"horn quicksilver\" or \"horn mercury\".\n\nCalomel was taken internally and used as a laxative, for example to treat George III in 1801, and disinfectant, as well as in the treatment of syphilis, until the early 20th century. Until fairly recently, it was also used as a horticultural fungicide, most notably as a root dip to help prevent the occurrence of clubroot amongst crops of the Brassicaceae family.\n\nMercury became a popular remedy for a variety of physical and mental ailments during the age of \"heroic medicine\". It was used by doctors in America throughout the 18th century, and during the revolution, to make patients regurgitate and release their body from \"impurities\". Benjamin Rush was one particular well-known advocate of mercury in medicine and used calomel to treat sufferers of yellow fever during its outbreak in Philadelphia in 1793. Calomel was given to patients as a purgative or cathartic until they began to salivate and was often administered to patients in such great quantities that their hair and teeth fell out.\n\nShortly after yellow fever struck Philadelphia, the disease broke out in Jamaica. A war of words erupted in the press concerning the best treatment for yellow fever: bleeding; or calomel. Anecdotal evidence indicates calomel was more effective than bleeding.\n\nMormon prophet Joseph Smith's eldest brother Alvin Smith died in 1823 from mercury poisoning from calomel.\n\nLewis and Clark brought along the wonder drug of the day, mercury chloride (otherwise known as calomel), as a pill, a tincture, \"and\" an ointment. Modern researchers used that same mercury, found deep in latrine pits, to retrace the locations of their respective locations and campsites. \n\nMercury is unique among the group 12 metals for its ability to form the M–M bond so readily. HgCl is a linear molecule. The mineral calomel crystallizes in the tetragonal system, with space group I4/m 2/m 2/m. The unit cell of the crystal structure is shown below:\n\nThe Hg–Hg bond length of 253 pm (Hg–Hg in the metal is 300 pm) and the Hg–Cl bond length in the linear HgCl unit is 243 pm. The overall coordination of each Hg atom is octahedral as, in addition to the two nearest neighbours, there are four other Cl atoms at 321 pm. Longer mercury polycations exist.\n\nMercurous chloride forms by the reaction of elemental mercury and mercuric chloride:\nIt can be prepared via metathesis reaction involving aqueous mercury(I) nitrate using various chloride sources including NaCl or HCl.\nAmmonia causes HgCl to disproportionate:\n\nMercurous chloride is employed extensively in electrochemistry, taking advantage of the ease of its oxidation and reduction reactions. The calomel electrode is a reference electrode, especially in older publications. Over the past 50 years, it has been superseded by the silver/silver chloride (Ag/AgCl) electrode. Although the mercury electrodes have been widely abandoned due to the dangerous nature of mercury, many chemists believe they are still more accurate and are not dangerous as long as they are handled properly. The differences in experimental potentials vary little from literature values. Other electrodes can vary by 70 to 100 millivolts.\n\nMercurous chloride decomposes into mercury(II) chloride and elemental mercury upon exposure to UV light.\nThe formation of Hg can be used to calculate the number of photons in the light beam, by the technique of actinometry. \n\nBy utilizing a light reaction in the presence of mercury(II) chloride and ammonium oxalate, mercury(I) chloride, ammonium chloride and carbon dioxide are produced.\n\nThis particular reaction was discovered by J.M. Eder (hence the name Eder reaction) in 1880 and reinvestigated by W. E. Rosevaere in 1929.\n\nMercury(I) bromide, HgBr, is a light yellow, whereas mercury(I) iodide, HgI, is greenish in colour. Both are poorly soluble. Mercury(I) fluoride is unstable in the absence of a strong acid.\n\nMercurous chloride is toxic, although due to its low solubility in water it is generally less dangerous than its mercuric chloride counterpart. It was used in medicine as a diuretic and purgative (laxative) in the United States from the late 1700s through the 1860s. Calomel was also a common ingredient in teething powders in Britain up until 1954, causing widespread mercury poisoning in the form of pink disease, which at the time had a mortality rate of 1 in 10. These medicinal uses were later discontinued when the compound's toxicity was discovered.\n\nIt has also found uses in cosmetics as soaps and skin lightening creams, but these preparations are now illegal to manufacture or import in many countries including the US, Canada, Japan and the European Union. A study of workers involved in the production of these preparations showed that the sodium salt of 2,3-dimercapto-1-propanesulfonic acid (DMPS) was effective in lowering the body burden of mercury and in decreasing the urinary mercury concentration to normal levels.\n\n"}
{"id": "33818014", "url": "https://en.wikipedia.org/wiki?curid=33818014", "title": "Nervous system network models", "text": "Nervous system network models\n\nNetwork of human nervous system comprises nodes (for example, neurons) that are connected by links (for example, synapses). The connectivity may be viewed anatomically, functionally, or electrophysiologically. These are presented in several Wikipedia articles that include Connectionism (a.k.a. Parallel Distributed Processing (PDP)), Biological neural network, Artificial neural network (a.k.a. Neural network), Computational neuroscience, as well as in several books by Ascoli, G. A. (2002), Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Gerstner, W., & Kistler, W. (2002), and Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986) among others. The focus of this article is a comprehensive view of modeling a neural network (technically neuronal network based on neuron model). Once an approach based on the perspective and connectivity is chosen, the models are developed at microscopic (ion and neuron), mesoscopic (functional or population), or macroscopic (system) levels. Computational modeling refers to models that are developed using computing tools.\n\nThe nervous system consists networks made up of neurons and synapses connected to and controlling tissues as well as impacting human thoughts and behavior. In modeling neural networks of the nervous system one has to consider many factors. The brain and the neural network should be considered as an integrated and self-contained firmware system that includes hardware (organs), software (programs), memory (short term and long term), database (centralized and distributed), and a complex network of active elements (such as neurons, synapses, and tissues) and passive elements (such as parts of visual and auditory system) that carry information within and in-and-out of the body.\n\nWhy does one want to model the brain and neural network? Although highly sophisticated computer systems have been developed and used in all walks of life, they are nowhere close to the human system in hardware and software capabilities. So, scientists have been at work to understand the human operation system and try to simulate its functionalities. In order to accomplish this, one needs to model its components and functions and validate its performance with real life. Computational models of a well simulated nervous system enable learning the nervous system and apply it to real life problem solutions.\n\nWhat is brain and what is neural network? Section 2.1 addresses the former question from an evolutionary perspective. The answer to the second question is based on the neural doctrine proposed by Ramon y Cajal (1894). He hypothesized that the elementary biological unit is an active cell, called neuron, and the human machine is run by a vast network that connects these neurons, called neural (or neuronal) network. The neural network is integrated with the human organs to form the human machine comprising the nervous system.\n\nInnumerable number of models of various aspects of the nervous system has been developed and there are several Wikipedia articles identified above that have been generated on the subject. The purpose of this article is to present a comprehensive view of all the models and provide the reader, especially a novice, to the neuroscience, with reference to the various sources.\n\nThe basic structural unit of the neural network is connectivity of one neuron to another via an active junction, called synapse. Neurons of widely divergent characteristics are connected to each other via synapses, whose characteristics are also of diverse chemical and electrical properties. In presenting a comprehensive view of all possible modeling of the brain and neural network, an approach is to organize the material based on the characteristics of the networks and the goals that need to be accomplished. The latter could be either for understanding the brain and the nervous system better or to apply the knowledge gained from the total or partial nervous system to real world applications such as artificial intelligence, Neuroethics or improvements in medical science for society.\n\nOn a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).\n\nSterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) classify the biological model of neuroscience into nine levels from ion channels to nervous system level based on size and function. Table 1 is based on this for neuronal networks.\n\nSporns, O. (2007) presents in his article on brain connectivity, modeling based on structural and functional types. A network that connects at neuron and synaptic level falls into the microscale. If the neurons are grouped into population of columns and minicolumns, the level is defined as mesoscale. The macroscale representation considers the network as regions of the brain connected by inter-regional pathways.\n\nArbib, M. A. (2007) considers in the modular model, a hierarchical formulation of the system into modules and submodules.\n\nThe neuronal signal comprises a stream of short electrical pulses of about 100 millivolt amplitude and about 1 to 2 millisecond duration (Gerstner, W., & Kistler, W. (2002) Chapter 1). The individual pulses are action potentials or spikes and the chain of pulses is called spike train. The action potential does not contain any information. A combination of the timing of the start of the spike train, the rate or frequency of the spikes, and the number and pattern of spikes in the spike train determine the coding of the information content or the signal message.\n\nThe neuron cell has three components – dendrites, soma, and axon as shown in Figure 1. Dendrites, which have the shape of a tree with branches, called arbor, receive the message from other neurons with which the neuron is connected via synapses. The action potential received by each dendrite from the synapse is called the postsynaptic potential. The cumulative sum of the postsynaptic potentials is fed to the soma. The ionic components of the fluid inside and outside maintain the cell membrane at a resting potential of about 65 millivolts. When the cumulative postsynaptic potential exceeds the resting potential, an action potential is generated by the cell body or soma and propagated along the axon. The axon may have one or more terminals and these terminals transmit neurotransmitters to the synapses with which the neuron is connected. Depending on the stimulus received by the dendrites, soma may generate one or more well-separated action potentials or spike train. If the stimulus drives the membrane to a positive potential, it is an excitatory neuron; and if it drives the resting potential further in the negative direction, it is an inhibitory neuron.\nThe generation of the action potential is called the “firing.” The firing neuron described above is called a spiking neuron. We will model the electrical circuit of the neuron in Section 3.6. There are two types of spiking neurons. If the stimulus remains above the threshold level and the output is a spike train, it is called the Integrate-and-Fire (IF) neuron model. If output is modeled as dependent on the impulse response of the circuit, then it is called the Spike Response Model (SRM) (Gestner, W. (1995)).\n\nThe spiking neuron model assumes that frequency (inverse of the rate at which spikes are generated) of spiking train starts at 0 and increases with the stimulus current. There is another hypothetical model that formulates the firing to happen at the threshold, but there is a quantum jump in frequency in contrast to smooth rise in frequency as in the spiking neuron model. This model is called the rate model. Gerstner, W., & Kistler, W. (2002), and Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) are good sources for a detailed treatment of spiking neuron models and rate neuron models.\n\nThe concept of artificial neural network (ANN) was introduced by McColloch, W. S. & Pitts, W. (1943) for models based on behavior of biological neurons. Norbert Wiener (1961) gave this new field the popular name of cybernetics, whose principle is the interdisciplinary relationship among engineering, biology, control systems, brain functions, and computer science. With the computer science field advancing, the von Neumann-type computer was introduced early in the neuroscience study. But it was not suitable for symbolic processing, nondeterministic computations, dynamic executions, parallel distributed processing, and management of extensive knowledge bases, which are needed for biological neural network applications; and the direction of mind-like machine development changed to a learning machine. Computing technology has since advanced extensively and computational neuroscience is now able to handle mathematical models developed for biological neural network. Research and development are progressing in both artificial and biological neural networks including efforts to merge the two.\n\nThe “triune theory of the brain” McLean, P. (2003) is one of several models used to theorize the organizational structure of the brain. The most ancient neural structure of the brain is the brain stem or “lizard brain.” The second phase is limbic or paleo-mammalian brain and performs the four functions needed for animal survival – fighting, feeding, fleeing, and fucking. The third phase is the neocortex or the neo-mammalian brain. The higher cognitive functions which distinguish humans from other animals are primarily in the cortex. The reptilian brain controls muscles, balance, and autonomic functions, such as breathing and heartbeat. This part of the brain is active, even in deep sleep. The limbic system includes the hypothalamus, hippocampus, and amygdala. The neocortex includes the cortex and the cerebrum. It corresponds to the brain of primates and, specifically, the human species. Each of the three brains is connected by nerves to the other two, but each seems to operate as its own brain system with distinct capacities. (See illustration in Triune brain.)\n\nThe connectionist model evolved out of Parallel Distributed Processing framework that formulates a metatheory from which specific models can be generated for specific applications. PDP approach (Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986)) is a distributed parallel processing of many inter-related operations, somewhat similar to what’s happening in the human nervous system. The individual entities are defined as units and the units are connected to form a network. Thus, in the application to nervous system, one representation could be such that the units are the neurons and the links are the synapses.\n\nThere are three types of brain connectivity models of a network (Sporns, O. (2007)). “Anatomical (or structural) connectivity” describes a network with anatomical links having specified relationship between connected “units.” If the dependent properties are stochastic, it is defined as “functional connectivity.” “Effective connectivity” has causal interactions between distinct units in the system. As stated earlier, brain connectivity can be described at three levels. At microlevel, it connects neurons through electrical or chemical synapses. A column of neurons can be considered as a unit in the mesolevel and regions of the brain comprising a large number of neurons and neuron populations as units in the macrolevel. The links in the latter case are the inter-regional pathways, forming large-scale connectivity.\nFigure 2 shows the three types of connectivity. The analysis is done using the directed graphs (see Sporns, O. (2007) and Hilgetag, C. C. (2002)). In the structural brain connectivity type, the connectivity is a sparse and directed graph. The functional brain connectivity has bidirectional graphs. The effective brain connectivity is bidirectional with interactive cause and effect relationships. Another representation of the connectivity is by matrix representation (See Sporns, O. (2007)). Hilgetag, C. C. (2002) describes the computational analysis of brain connectivity.\n\nArbib, M. A. (2007) describes the modular models as follows. “Modular models of the brain aid the understanding of a complex system by decomposing it into structural modules (e.g., brain regions, layers, columns) or functional modules (schemas) and exploring the patterns of competition and cooperation that yield the overall function.” This definition is not the same as that defined in functional connectivity. The modular approach is intended to build cognitive models and is, in complexity, between the anatomically defined brain regions (defined as macrolevel in brain connectivity) and the computational model at the neuron level.\n\nThere are three views of modules for modeling. They are (1) modules for brain structures, (2) modules as schemas, and (3) modules as interfaces. Figure 3 presents the modular design of a model for reflex control of saccades (Arbib, M. A. (2007)). It involves two main modules, one for superior colliculus (SC), and one for brainstem. Each of these is decomposed into submodules, with each submodule defining an array of physiologically defined neurons. In Figure 3(b) the model of Figure 3(a) is embedded into a far larger model which embraces various regions of cerebral cortex (represented by the modules Pre-LIP Vis, Ctx., LIP, PFC, and FEF), thalamus, and basal ganglia. While the model may indeed be analyzed at this top level of modular decomposition, we need to further decompose basal ganglia, BG, as shown in Figure 3(c) if we are to tease apart the role of dopamine in differentially modulating (the 2 arrows shown arising from SNc) the direct and indirect pathways within the basal ganglia (Crowley, M. (1997)).\nNeural Simulation Language (NSL) has been developed to provide a simulation system for large-scale general neural networks. It provides an environment to develop an object-oriented approach to brain modeling. NSL supports neural models having as basic data structure neural layers with similar properties and similar connection patterns. Models developed using NSL are documented in Brain Operation Database (BODB) as hierarchically organized modules that can be decomposed into lower levels.\n\nAs mentioned in Section 2.4, development of artificial neural network (ANN), or neural network as it is now called, started as simulation of biological neuron network and ended up using artificial neurons. Major development work has gone into industrial applications with learning process. Complex problems were addressed by simplifying the assumptions. Algorithms were developed to achieve a neurological related performance, such as learning from experience. Since the background and overview have been covered in the other internal references, the discussion here is limited to the types of models. The models are at the system or network level.\n\nThe four main features of an ANN are topology, data flow, types of input values, and forms of activation (Meireles, M. R. G. (2003), Munakata, T. (1998)). Topology can be multilayered, single-layered, or recurrent. Data flow can be recurrent with feedback or non-recurrent with feedforward model. The inputs are binary, bipolar, or continuous. The activation is linear, step, or sigmoid. Multilayer Perceptron (MLP) is the most popular of all the types, which is generally trained with back-propagation of error algorithm. Each neuron output is connected to every neuron in subsequent layers connected in cascade and with no connections between neurons in the same layer. Figure 4 shows a basic MLP topology (Meireles, M. R. G. (2003)), and a basic telecommunication network (Subramanian, M. (2010)) that most are familiar with. We can equate the routers at the nodes in telecommunication network to neurons in MLP technology and the links to synapses.\n\nComputational science is an interdisciplinary field that combines engineering, biology, control systems, brain functions, physical sciences, and computer science. It has fundamental development models done at the lower levels of ions, neurons, and synapses, as well as information propagation between neurons. These models have established the enabling technology for higher-level models to be developed. They are based on chemical and electrical activities in the neurons for which electrical equivalent circuits are generated. A simple model for the neuron with predominantly potassium ions inside the cell and sodium ions outside establishes an electric potential on the membrane under equilibrium, i.e., no external activity, condition. This is called the resting membrane potential, which can be determined by Nernst Equation (Nernst, W. (1888)). An equivalent electrical circuit for a patch of membrane, for example an axon or dendrite, is shown in Figure 5. E and E are the potentials associated with the potassium and sodium channels respectively and R and R are the resistances associated with them. C is the capacitance of the membrane and I is the source current, which could be the test source or the signal source (action potential). The resting potential for potassium-sodium channels in a neuron is about -65 millivolts.\nThe membrane model is for a small section of the cell membrane; for larger sections it can be extended by adding similar sections, called compartments, with the parameter values being the same or different. The compartments are cascaded by a resistance, called axial resistance. Figure 6 shows a compartmental model of a neuron that is developed over the membrane model. Dendrites are the postsynaptic receptors receiving inputs from other neurons; and the axon with one or more axon terminals transmits neurotransmitters to other neurons.\nThe second building block is the Hodgkin-Huxley (HH) model of the action potential. When the membrane potential from the dendrites exceeds the resting membrane potential, a pulse is generated by the neuron cell and propagated along the axon. This pulse is called the action potential and HH model is a set of equations that is made to fit the experimental data by the design of the model and the choice of the parameter values.\n\nModels for more complex neurons containing other types of ions can be derived by adding to the equivalent circuit additional battery and resistance pairs for each ionic channel. The ionic channel could be passive or active as they could be gated by voltage or be ligands. The extended HH model has been developed to handle the active channel situation.\n\nAlthough there are neurons that are physiologically connected to each other, information is transmitted at most of the synapses by chemical process across a cleft. Synapses are also computationally modeled. The next level of complexity is that of stream of action potentials, which are generated, whose pattern contains the coding information of the signal being transmitted. There are basically two types of action potentials, or spikes as they are called, that are generated. One is “integrate-and-fire” (the one we have so far addressed) and the other which is rate based. The latter is a stream whose rate varies. The signal going across the synapses could be modeled either as a deterministic or a stochastic process based on the application (See Section 3.7). Another anatomical complication is when a population of neurons, such as a column of neurons in visionary system, needs to be handled. This is done by considering the collective behavior of the group (Kotter, R., Nielson, P., Dyhrfjeld-Johnson, J., Sommer, F. T., & Northoff, G. (2002)).\n\nThe action potential or the spike does not itself carry any information. It is the stream of spikes, called spike train, that carry the information in its number and pattern of spikes and timing of spikes. The postsynaptic potential can be either positive, the excitatory synapse or negative, inhibitory synapse. In modeling, the postsynaptic potentials received by the dendrites in the postsynaptic neuron are integrated and when the integrated potential exceeds the resting potential, the neuron fires an action potential along its axon. This model is the Integrate-and-Fire (IF) model that was mentioned in Section 2.3. Closely related to IF model is a model called Spike Response Model (SRM) (Gerstner, W. (1995) Pages 738-758) that is dependent on impulse function response convoluted with the input stimulus signal. This forms a base for a large number of models developed for spiking neural networks.\n\nThe IF and SR model of spike train occurs in Type I neurons, in which the spike rate or spike frequency of the occurrence increases smoothly with the increase in stimulus current starting from zero. Another phenomenon of spike train generation happens in Type II neurons, where firing occurs at the resting potential threshold, but with a quantum jump to a non-zero frequency. Models have been developed using the rate (frequency) of the spike train and are called rate-based models.\n\nWhat is important for understanding the functions of the nervous system is how the message is coded and transported by the action potential in the neuron. There are two theories on how the signal that is being propagated is coded in the spikes as to whether it is pulse code or rate code. In the former, it is the time delay of the first spike from the time of stimulus as seen by the postsynaptic receiver that determines the coding. In the rate code, it is average rate of the spike that influences the coding. It is not certain as to which is really the actual physiological phenomenon in each case. However, both cases can be modeled computationally and the parameters varied to match the experimental result. The pulse mode is more complex to model and numerous detailed neuron models and population models are described by Gerstner and Kistler in Parts I and II of Gerstner, W., & Kistler, W. (2002) and Chapter 8 of Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011). Another important characteristic associated with SR model is the spike-time-dependent-plasticity. It is based on Hebb’s postulate on plasticity of synapse, which states that “the neurons that fire together wire together.” This causes the synapse to be a long-term potentiation (LTP) or long-term depression (LTD). The former is the strengthening of the synapse between two neurons if the postsynaptic spike temporally follows immediately after the presynaptic spike. Latter is the case if it is reverse, i.e., the presynaptic spike occurs after the postsynaptic spike. Gerstner, W. & Kistler, W. (2002) in Chapter 10 and Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) in Chapter 7 discuss the various models related to Hebbian models on plasticity and coding.\n\nThe challenge involved in developing models for small, medium, and large networks is one of reducing the complexity by making valid simplifying assumptions in and extending the Hodgkin-Huxley neuronal model appropriately to design those models ( see Chapter 9 of Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Kotter, R., Nielson, P., Dyhrfjeld-Johnson, J., Sommer, F. T., & Northoff, G. (2002), and Chapter 9 of Gerstner, W., & Kistler, W. (2002)). Network models can be classified as either network of neurons propagating through different levels of cortex or neuron populations interconnected as multilevel neurons. The spatial positioning of neuron could be 1-, 2- or 3-dimensional; the latter ones are called small-world networks as they are related to local region. The neuron could be either excitatory or inhibitory, but not both. Modeling design depends on whether it is artificial neuron or biological neuron of neuronal model. Type I or Type II choice needs to be made for the firing mode. Signaling in neurons could be rate-based neurons, spiking response neurons, or deep-brain stimulated. The network can be designed as feedforward or recurrent type. The network needs to be scaled for the computational resource capabilities. Large-scale thalamocortical systems are handled in the manner of the Blue Brain project (Markam, H. (2006)).\n\nNo generalized modeling concepts exist for modeling the development of anatomical physiology and morphology similar to the one of behavior of neuronal network, which is based on HH model. Shankle, W. R., Hara, J., Fallon, J. H., and Landing, B. H. (2002) describe the application of neuroanatomical data of the developing human cerebral cortex to computational models. Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) discuss aspects of the nervous system of computational modeling in the development of nerve cell morphology, cell physiology, cell patterning, patterns of ocular dominance, and connection between nerve cell and muscle, and retinotopic maps. Carreira-Perpinan, M. A. & Goodhill, G. J. (2002) deal with the optimization of the computerized models of the visual cortex.\n\nWith the enormous number of models that have been created, tools have been developed for dissemination of the information, as well as platforms to develop models. Several generalized tools, such as GENESIS, NEURON, XPP, and NEOSIM are available and are discussed by Hucka, M. (2002).\n\n"}
{"id": "41620941", "url": "https://en.wikipedia.org/wiki?curid=41620941", "title": "Norwegian Telecom Museum", "text": "Norwegian Telecom Museum\n\nThe Norwegian Telecom Museum (\"Telemuseet\", formerly \"Norsk Telemuseum\") is a Norwegian museum with a collection and several exhibits, including a permanent exhibit at the Norwegian Museum of Science and Technology. The administration of the Norwegian Telecom Museum is maintained in the Kjelsås neighbourhoods of Oslo. The museum is mainly funded by the Telenor Group.\n\nThe museum operates nationwide, with exhibitions and collections in several places. The main exhibition of the museum is located in the Norwegian Museum of Science and Technology at Kjelsås in Oslo. Other location sites include Stavanger, Jeløy, Kristiansand. Lier, Lærdal, Lødingen, Sørvågen, Tromsø and Trondheim.\n\n"}
{"id": "19260962", "url": "https://en.wikipedia.org/wiki?curid=19260962", "title": "Offshore concrete structure", "text": "Offshore concrete structure\n\nOffshore concrete structures have been in use successfully for about 30 years. They serve the same purpose as their steel counterparts in the oil and gas production and storage. The first concrete oil platform has been installed in the North Sea in the Ekofisk field in 1973 by Phillips Petroleum. Since then 47 major concrete offshore structures have been built, whereby 21 of the 47 concrete substructures have been designed (concept and detail designs ) by Dr. techn. Olav Olsen.\n\nConcrete offshore structures are mostly used in the petroleum industry as drilling, extraction or storage units for crude oil or natural gas. Those large structures house machinery and equipment needed to drill and/or extract oil and gas. But concrete structures are not only limited to applications within the oil and gas industry. Several conceptual studies have shown recently, that concrete support structures for offshore wind turbines are very competitive compared to common steel structures, especially for larger water depths.\n\nDepending on the circumstances, platforms may be attached to the ocean floor, consist of an artificial island, or be floating. Generally, offshore concrete structures are classified into fixed and floating structures. Fixed structures are mostly built as concrete gravity based structures (CGS, also termed as caisson type), where the loads bear down directly on the uppermost layers as soil pressure. The caisson provides buoyancy during construction and towing and acts also as a foundation structure in the operation phase. Furthermore, the caisson could be used as storage volume for oil or other liquids.\n\nFloating units may be held in position by anchored wires or chains in a spread mooring pattern. Because of the low stiffness in those systems, the natural frequency is low and the structure can move in all six degrees of freedom. Floating units serve as productions units, storage and offloading units (FSO) or for crude oil or as terminals for liquefied natural gas (LNG). A more recent development is concrete sub-sea structures.\n\nConcrete offshore structures show an excellent performance. They are highly durable, constructed of almost maintenance-free material, suitable for harsh and/or arctic environment (like ice and seismic regions), can carry heavy topsides, often offer storage capacities, are suitable for soft grounds and are very economical for water depths larger than 150m. Most gravity-type platforms need no additional fixing because of their large foundation dimensions and extremely high weight.\n\nSince the 1970s, several fixed concrete platform designs have been developed. Most of the designs have in common a base caisson (normally for storage of oil) and shafts penetrating the water surface to carry the topside. In the shafts normally utility systems for offloading, drilling, draw down and ballast are put up.\n\nConcrete offshore platforms of the gravity-base type are almost always constructed in their vertical attitude. This allows the inshore installation of deck girders and equipment and the later transport of the whole structure to the installation site.\n\nThe most common concrete designs are:\n\nCondeep refers to a make of gravity base structure for oil platforms developed and fabricated by Norwegian Contractors in Norway. Condeep usually consists of a base of concrete oil storage tanks from which one, three or four concrete shafts rise. The original Condeep always rests on the sea floor, and the shafts rise to about 30m above the sea level. The platform deck itself is not a part of the construction.\nThe Condeep Platforms Brent B (1975) and Brent D (1976) were designed for a water depth of 142m in the Brent oilfield operated by Shell. Their main mass is represented by the storage tank (ca. 100m diameter and 56m high, consisting of 19 cylindrical compartments with 20m diameter). Three of the cells are extended into shafts tapering off at the surface and carrying a steel deck. The tanks serve as storage of crude oil in the operation phase. During the installation these tanks have been used as ballast compartment.\nAmong the largest Condeep type platform are the Troll A platform and the Gullfaks C. Troll A was built within four years and deployed in 1995 to produce gas from the Troll oil field which was developed by Norske Shell, since 1996 operated by Statoil.\nA detailed overview about Condeep platforms is given in a separate article.\n\nConcrete Gravity Base Structures (CGBS) is a further development of the first-generation Condeep drilling/production platforms installed in the North Sea between the late 1970s and mid '90s. The CGBS have no oil storage facilities and the topside installations will be carried out in the field by a float-over mating method. Current or most recent projects are:\n\nThe first concrete gravity platform in the North Sea was a C G Doris platform, the Ekofisk Tank, in Norwegian waters.\nThe structure has a shape not unlike a marine sea island and is surrounded by a perforated breakwater wall (Jarlan patent).\nThe original proposal of the French group C G DORIS (Compagnie General pour les Developments Operationelles des Richesses Sous-Marines) for a prestressed post-tensioned concrete \"island\" structure was adopted on cost and operational grounds. DORIS was general contractor responsible for the structural design: the concrete design was prepared and supervised on behalf of DORIS by Europe-Etudes. Further example for the C G DORIS designs are the Frigg platforms, the Ninian Central Platform and the Schwedeneck platforms.\nThe design typically consists of a large volume caisson based on the sea floor merging into a monolithic structure, which is offering the base for the deck. The single main leg is surrounded by an outer breaker wall perforated with so called Jarlan holes. This wall is intended to break up waves, thus reducing their forces.\n\nThis design is quite similar to the Condeep type.\n\nTo achieve its goal and extract oil within five years after discovering the Brent reservoir Shell\ndivided up the construction of four offshore platforms. Redpath Dorman Long at Methil in Fife, Scotland getting Brent A, the two concrete Condeeps B and D were to be built in Norway by Norwegian Contractors (NC) of Stavanger, and C (also concrete) was to be built by McAlpine at Ardyne Point on the Clyde (which is known as the ANDOC (Anglo Dutch Offshore Concrete) design). The ANDOC design can be considered as the British construction industry's attempt to compete with Norway in this sector. McAlpine constructed three concrete platforms for the North Sea oil industry at Ardyne Point. The ANDOC type is very similar to the Sea Tank design, but the four concrete legs terminate and steel legs take over to support the deck.\n\nThe Arup dry-build Concrete Gravity Substructure (CGS) concept was originally developed by Arup in 1989 for Hamilton Brothers' Ravenspurn North. The Arup CGS are designed to be simple to install, and are fully removable. Simplicity and repetition of concrete structural elements, low reinforcement and pre-stress densities as well as the use of normal density concrete lead to economical construction costs. Typical for the Arup CGS is the inclined installation technique. This technique helps to maximise economy and provide a robust offshore emplacement methodology. Further projects have been the Malampaya project in the Philippines and the Wandoo Full Field Development on the North West Shelf of Western Australia.\n\nSince concrete is quite resistant to corrosion from salt water and keeps maintenance costs low, floating concrete structures have become increasingly attractive to the oil and gas industry in the last two decades. Temporary floating structures such as the Condeep platforms float during construction but are towed out and finally ballasted until they sit on the sea floor. Permanent floating concrete structures have various uses including the discovery of oil and gas deposits, in oil and gas production, as storage and offloading units and in heavy lifting systems.\n\nCommon designs for floating concrete structures are the barge or ship design, the platform design (semi-submersible, TLP) as well as the floating terminals e.g. for LNG.\n\nFloating production, storage, and offloading systems (FPSOS) receive crude oil from deep-water wells and store it in their hull tanks until the crude is transferred into tank ships or transport barges. In addition to FPSO’s, there have been a number of ship-shaped Floating Storage and Offloading (FSO) systems (vessels with no production processing equipment) used in these same areas to support oil and gas developments. An FSO is typically used as a storage unit in remote locations far from pipelines or other infrastructures.\n\nSemi-submersible marine structures are typically only movable by towing. Semi-submersible platforms have the principal characteristic of remaining in a substantially stable position, presenting small movements when they experience environmental forces such as the wind, waves and currents. Semi-Submersible platforms have pontoons and columns, typically two parallel spaced apart pontoons with buoyant columns upstanding from those pontoons to support a deck. Some of the semi-submersible vessels only have a single caisson, or column, usually denoted as a buoy while others utilize three or more columns extended upwardly from buoyant pontoons. For activities which require a stable offshore platform, the vessel is then ballasted down so that the pontoons are submerged, and only the buoyant columns pierce the water surface - thus giving the vessel a substantial buoyancy with a small water-plane area. The only concrete semi-submersible in existence is Troll B.\n\nA Tension Leg Platform is a buoyant platform, which is held in place by a mooring system. TLP mooring is different to conventional chained or wire mooring systems. The platform is held in place with large steel tendons fastened to the sea floor. Those tendons are held in tension by the buoyancy of the hull. Statoil's Heidrun TLP is the only one with a concrete hull, all other TLPs have steel hulls.\n\nFPSO or FSO systems are typically barge/ship-shaped and store crude oil in tanks located in the hull of the vessel. Their turret structures are designed to anchor the vessel, allow “weather vaning” of the units to accommodate environmental conditions, permit the constant flow of oil and production fluids from vessel to undersea field, all while being a structure capable of quick disconnect in the event of emergency.\n\nThe first barge of prestressed concrete has been designed in the early 1970s as an LPG (liquefied petroleum gas) storage barge in the Ardjuna Field (Indonesia). This barge is built of reinforced and prestressed concrete containing cylindrical tanks each having a cross-section perpendicular to its longitudinal axes that comprises a preferably circular curved portion corresponding to the bottom.\n\nFollowing table summarizes the major eziating offshore concrete structures.\n\n\n"}
{"id": "8968544", "url": "https://en.wikipedia.org/wiki?curid=8968544", "title": "Optical line termination", "text": "Optical line termination\n\nAn optical line termination (OLT), also called an optical line terminal, is a device which serves as the service provider endpoint of a passive optical network. It provides two main functions:\n\nThe diagram below depicts an OLT within a fiber-optic network.\nOLTs include the following features:\n"}
{"id": "39060006", "url": "https://en.wikipedia.org/wiki?curid=39060006", "title": "Payroll automation", "text": "Payroll automation\n\nPayroll automation refers to the use of computers to produce paychecks and manage benefit payments for a company or community. Often, payroll automation is integrated into the company’s enterprise resource planning system that provides an overall view of the company’s or community’s finances; in addition to payroll, it can manage customer relationships, production, personnel resources, invoicing and accounting.\n\nPayroll management consists of several stages and procedures that require expertise in financial administration, such as employment contract management. \nPayroll management performs the following tasks:\n\nThe travel costs and travel invoices from the employees are usually processed together with payroll.\n\nPayroll functions can be automated using software to facilitate the collection, organization and storage of all information required for payroll calculations and regulatory agency reportage requirements. If the payroll software is not purchased as part of a comprehensive business management system, it can usually be combined with the company’s existing solutions for accounting, sales ledger, working hour management and recruiting. Information that has been captured in one part of the system can be used by other modules. Hours registered in the work management system, for example, are automatically transferred to the wage calculation system.\n\nEffective payroll automation collects all relevant information in one place in electronic format, reducing mistakes by eliminating the need to synchronize and manage otherwise duplicate data sets.\n\nThe American Payroll Association (APA) estimates that automation reduces costs related to payroll management by up to 80%, which is partially explained by the reduced mistakes in wage payments and invoicing.\n\nWell planned, modern payroll software provides the following benefits:\n"}
{"id": "10525403", "url": "https://en.wikipedia.org/wiki?curid=10525403", "title": "Penile prosthesis", "text": "Penile prosthesis\n\nA penile prosthesis, or penile implant, is a medical device which is surgically implanted within the corpora cavernosa of the penis during a surgical procedure. The device is indicated for use in men with organic or treatment-resistant impotence or erectile dysfunction that is the result of various physical conditions such as cardiovascular disease, diabetes, pelvic trauma, Peyronie's disease, or as the result of prostate cancer treatments. Less commonly, a penile prosthesis may also be used in the final stage of plastic surgery phalloplasty to complete female to male gender reassignment surgery as well as during total phalloplasty for adult and child patients that need male genital modification.\n\nA penile implant is one treatment option available to individuals who are unable to achieve or maintain an erection adequate for successful sexual intercourse or penetration. Its primary use is for men with erectile dysfunction from vascular conditions (cardiovascular disease, high blood pressure, diabetes), congenital anomalies, iatrogenic, accidental penile or pelvic trauma, Peyronie's disease, or as a result of prostate cancer treatments. This implant is normally considered when less invasive medical treatments such as oral medications (PDE5 inhibitors: Viagra, Levitra, Cialis), penile injections, or vacuum erection devices are unsuccessful, provide an unsatisfactory result, or are contraindicated. For example, many drugs used to treat erectile dysfunction are unsuitable for patients with heart problems and may interfere with other medications.\n\nSometimes a penile prosthesis is implanted during surgery to alter, construct or reconstruct the penis in phalloplasty. The British Journal of Urology International reports that unlike metoidioplasty for female to male sexual reassignment patients, which may result in a penis that is long but narrow, current total phalloplasty neophallus creation using a musculocutaneous latissimus dorsi flap could result in a long, large volume penis which enables safe insertion of any type of penile prosthesis.\n\nThis same technique enables male victims of minor to serious iatrogenic, accidental or intentional penile trauma injuries (or even total emasculation) caused by accidents, child abuse or self-mutilation to have penises suitable for penile prosthesis implantation enabling successful sexual intercourse.\n\nIn some cases of genital reconstructive surgery, implantation of a semirigid prosthesis is recommended for three months after total phalloplasty to prevent phallic retraction. It can be replaced later with an inflatable one.\n\nThere are two primary types of penile prosthesis: noninflatable semirigid devices, and inflatable devices. Noninflatable, semirigid devices consist of rods implanted into the erection chambers of the penis and can be bent into position as needed for sexual penetration. With this type of implant the penis is always semi-rigid and therefore may be difficult to conceal.\n\nHydraulic, inflatable prosthesis also exist and were first described in 1973 by Brantley Scott et al. These saline-filled devices consist of inflatable cylinders placed in the erection chambers of the penis, a pump placed in the scrotum for patient-activated inflation/deflation, and a reservoir placed in the abdomen which stores the fluid. The device is inflated by squeezing the pump several times to transfer fluid from the reservoir to the chambers in the penis. After intercourse, a valve next to the pump is manually operated, allowing fluid to be released from the penis (not instantaneously; squeezing the penis may be necessary), causing the penis to return to a flaccid or semi-flaccid condition. Almost all implanted penile prosthesis devices perform satisfactorily for a decade or more before needing replacement. Some surgeons recommend these due to the opinion that they are more easily concealed and provide the highest levels of patient/partner satisfaction.\n\n\n\n"}
{"id": "204272", "url": "https://en.wikipedia.org/wiki?curid=204272", "title": "Pipe tool", "text": "Pipe tool\n\nA pipe tool is any of a variety of small gadgets designed to aid in packing, smoking, and emptying tobacco pipes.\n\nA \"Czech tool\" or \"three-in-one\" pipe tool consists of a pick, a reamer, and a tamper:\n\n\nA \"pipe nail\" is a nail-shaped tool with a tamper at one end and a reamer at the other. Tampers and reamers may also be made as separate tools.\n"}
{"id": "7301525", "url": "https://en.wikipedia.org/wiki?curid=7301525", "title": "Pugmill", "text": "Pugmill\n\nA pugmill or pug mill is a machine in which clay or other materials are mixed into a plastic state or a similar machine for the trituration of ore. Industrial applications are found in pottery, bricks, cement and some parts of the concrete and asphalt mixing processes. A pugmill may be a fast continuous mixer. A continuous pugmill can achieve a thoroughly mixed, homogeneous mixture in a few seconds, and the right machines can be matched to the right application by taking into account the factors of agitation, drive assembly, inlet, discharge, cost and maintenance. Mixing materials at optimum moisture content requires the forced mixing action of the pugmill paddles, while soupy materials might be mixed in a drum mixer. A typical pugmill consists of a horizontal boxlike chamber with a top inlet and a bottom discharge at the other end, 2 shafts with opposing paddles, and a drive assembly. Some of the factors affecting mixing and residence time are the number and the size of the paddles, paddle swing arc, overlap of left and right swing arc, size of mixing chamber, length of pugmill floor, and material being mixed.\n\nRoad Base - Dense well-graded aggregate, uniformly mixed, wetted, and densely compacted for building the foundation under a pavement.\n\nLime Addition to asphalt – Lime may be added to the cold feed of an asphalt plant to strengthen the binding properties of the asphalt.\n\nFlyash Conditioning – Wetting fly ash in a pugmill to stabilize the ash so that it won’t create dust. Some flyashes have cementitious properties when wetted and can be used to stabilize other materials.\n\nWaste stabilization – various waste streams are remediated with pugmills forcing the mixing of the wastes with remediation agents.\n\nRoller-compacted concrete – (RCC) or rolled concrete is a special blend of concrete that has the same ingredients as conventional concrete but in different ratios. It has cement, water, and aggregates, but RCC is much drier and essentially has no slump. RCC is placed in a manner similar to paving, often by dump trucks or conveyors, spread by bulldozers or special modified asphalt pavers. After placement it is compacted by vibratory rollers. \nThe “stiff” nature of RCC may require a paddle type pugmill to force the materials to mix completely and discharge easily.\n\nCeramics pug mills, or commonly just \"pugs\", are not used to grind or mix, rather they extrude clay bodies prior to shaping processes. Some can be fitted with a vacuum system that ensures the extruded clay bodies have no entrapped air. According to the 1913 edition of Webster's Dictionary, a clay pug mill typically consists of an upright shaft armed with projecting knives, which is caused to revolve in a hollow cylinder, tub, or vat, in which the clay body is placed.\n\nPugmills that run intermittently are used in the kaolin mining industry to mix certain grades of kaolin clay with water.\n\n\n"}
{"id": "19363997", "url": "https://en.wikipedia.org/wiki?curid=19363997", "title": "RY (test signal)", "text": "RY (test signal)\n\nRYRYRYRY... is a character string that was widely used to test a five-level teleprinter or RTTY channel. The characters \"R\" and \"Y\" are \"01010\" and \"10101\" in 5-bit ITA2 code, also known as Baudot. Thus they are Boolean complements of each other.\n\nSwitching between the two characters is a stressful test for electromechanical teleprinters.\nRepeated over and over, \"RYRYRYRY...\" outputs a carrier wave that regularly and rapidly shifts back and forth in frequency.\n\nThis signal pattern also provided a test for signal polarity; if polarity was reversed, the test signal would print as \"SG\".\n\nThe corresponding string of complementary characters in 7-bit ASCII is \"U*U*U*U*...\" \n\nThe RYRYRY sequence usage can be seen on the following DDH47 weather station broadcast excerpt:\n"}
{"id": "4498244", "url": "https://en.wikipedia.org/wiki?curid=4498244", "title": "Salsola soda", "text": "Salsola soda\n\nSalsola soda, more commonly known in English as opposite-leaved saltwort, oppositeleaf Russian thistle, or barilla plant, is a small (to 0.7 m tall), annual, succulent shrub that is native to the Mediterranean Basin. It is a halophyte (a salt-tolerant plant) that typically grows in coastal regions and can be irrigated with salt water.\n\nThe plant has great historical importance as a source of soda ash, which was extracted from the ashes of \"Salsola soda\" and other saltwort plants. Soda ash is one of the alkali substances that are crucial in glassmaking and soapmaking. The famed clarity of 16th century \"cristallo\" glass from Murano and Venice depended upon the purity of \"Levantine soda ash,\" and the nature of this ingredient was kept secret. Spain had an enormous 18th century industry that produced soda ash from the saltworts ( in Spanish). Soda ash is now known to be predominantly sodium carbonate. In 1807, Sir Humphry Davy isolated a metallic element from caustic soda; he named the new element \"sodium\" to indicate its relationship to \"soda.\" Before soda was somewhat synonymous (in U.S. English) with soft drinks, the word referred to \"Salsola soda\" and other saltwort plants, and to \"sodas\" derived from soda ash.\n\nWhile the era of farming for soda ash is long past, \"S. soda\" is still cultivated as a vegetable that enjoys considerable popularity in Italy and with gourmets around the world. Its common names in Italian include \"barba di frate\", \"agretti\", and \"liscari sativa\" (short: lischi or lischeri). Of its culinary value, Frances Mayes has written that \"Spinach is the closest taste, but while \"agretti\" has the mineral sharpness of spinach, it tastes livelier, full of the energy of spring.\"\n\nThis annual, succulent plant can grow into small shrubs up to 0.7 m tall (sometimes called subshrubs). It has fleshy green leaves with either green or red stems. The tiny flowers develop from inflorescences that grow out of the base of the leaves near the stem.\n\n\"S. soda\" is native in Eurasia and North Africa. Historically, it was well known in Italy, Sicily, and Spain. In modern Europe, it is also found on the Atlantic coasts of France and Portugal and on the Black Sea coast.\nIt has become naturalized along the Pacific coast of North America,\nand there is concern about its invasiveness in California's salt marshes. It is also reported to be naturalized in South America.\n\nThe ashes obtained by the burning of \"S. soda\" can be refined to make a product called soda ash, which is one of the alkali materials essential to making soda-lime glass, soap, and many other products. The principal active ingredient is sodium carbonate, with which the term \"soda ash\" is now nearly synonymous. The processed ashes of \"S. soda\" contain as much as 30% sodium carbonate.\n\nA high concentration of sodium carbonate in the ashes of \"S. soda\" occurs if the plant is grown in highly saline soils (i.e. in soils with a high concentration of sodium chloride), so that the plant's tissues contain a fairly high concentration of sodium ions. \"S. soda\" can be irrigated with sea water, which contains about 40 g/l of dissolved sodium chloride and other salts. When these sodium-rich plants are burned, the carbon dioxide that is produced presumably reacts with this sodium to form sodium carbonate.\nIt is surprising to find a higher concentration of sodium than of potassium in plant tissues; the former element is usually toxic, and the latter element is essential, to the metabolic processes of plants. Thus, most plants, and especially most crop plants, are \"glycophytes\", and suffer damage when planted in saline soils. \"S. soda\", and the other plants that were cultivated for soda ash, are \"halophytes\" that tolerate much more saline soils than do glycophytes, and that can thrive with much larger densities of sodium in their tissues than can glycophytes.\n\nThe biochemical processes within the cells of halophytes are typically as sensitive to sodium as are the processes in glycophytes. Sodium ions from a plant's soil or irrigation water are toxic primarily because they interfere with biochemical processes within a plant's cells that require potassium, which is a chemically similar alkali metal element. The cell of a halophyte such as \"S. soda\" has a molecular transport mechanism that sequesters sodium ions into a compartment within the plant cell called a \"vacuole.\" The vacuole of a plant cell can occupy 80% of the cell's volume; most of a halophyte plant cell's sodium can be sequestered in the vacuole, leaving the rest of the cell with a tolerable ratio of sodium to potassium ions.\n\nIn addition to \"S. soda\", soda ash has also been produced from the ashes of \"Salsola kali\" (another saltwort plant), of glasswort plants, and of kelp, a type of seaweed. The sodium carbonate, which is water-soluble, is \"lixiviated\" from the ashes (extracted with water), and the resulting solution is boiled dry to obtain the finished soda ash product. A very similar process is used to obtain potash (mainly potassium carbonate) from the ashes of hardwood trees. Because halophytes must also have potassium ions in their tissues, even the best soda ash derived from them also contains some potash (potassium carbonate), as was known by the 19th century.\n\nPlants were a very important source of soda ash until the early 19th century. In the 18th century, Spain had an enormous industry producing \"barilla\" (one type of plant-derived soda ash) from saltwort plants. Similarly, Scotland had a large 18th-century industry producing soda ash from kelp; this industry was so lucrative that it led to overpopulation in the Western Isles of Scotland, and one estimate is that 100,000 people were occupied with \"kelping\" during the summer months. The commercialization of the Leblanc process for synthesizing sodium carbonate (from salt, limestone, and sulfuric acid) brought an end to the era of farming for soda ash in the first half of the 19th century.\n\nThe Italian name \"agretti\" is commonly used in English to refer to the edible leaves of \"S. soda\"; \"barba di frate\" (or friar's beard) is the most common of the Italian names. This plant is not a summer green and should be started early indoors or in autumn. The seed is notorious for poor germination at about 30 to 40% standard, much like rosemary. Though the plant is often grown in saltwater-irrigated land in the Mediterranean Basin, it will grow without salt water. \"S. soda\" is harvested in bunches when small, or cropped regularly to encourage new growth when mature. It is most commonly boiled and eaten as a leafy vegetable; the recommendation is to cook it in boiling water until the leaves soften, and to serve while some bite (crunch) remains (much like samphire). It can also be eaten raw; it is said to \"taste grassy and slightly salty with a pleasant, crunchy texture.\" \n\n\"S. soda\" is sometimes confused with a plant known in Japan as \"okahijiki\" (land seaweed), which is actually the species \"S. komarovi\". The harvested leaves of the two species have a similar appearance.\n\n\"S. soda\" has also been studied as a bioremediation \"biodesalinating companion plant\" for crops such as tomatoes and peppers when they are grown in saline soils. The \"Salsola soda\" extracts enough sodium from the soil to improve the growth of the crop plant, and better crop yields result despite the competition of the two plants for the remaining minerals from the soil.\n\n\n"}
{"id": "38920422", "url": "https://en.wikipedia.org/wiki?curid=38920422", "title": "Single-core", "text": "Single-core\n\nA single-core processor is a microprocessor with a single core on a chip, running a single thread at any one time. The term became common after the emergence of multi-core processors (which have several independent processors on a single chip) to distinguish non-multi-core designs. For example, Intel released a Core 2 Solo and Core 2 Duo, and one would refer to the former as the 'single-core' variant. Most microprocessors prior to the multi-core era are single-core. The class of many-core processors follows on from multi-core, in a progression showing increasing parallelism over time.\n\nProcessors remained single-core until it was impossible to achieve performance gains from the increased clock speed and transistor count allowed by Moore's law (there were diminishing returns to increasing the depth of a pipeline, increasing CPU cache sizes, or adding execution units).\n\n"}
{"id": "5496415", "url": "https://en.wikipedia.org/wiki?curid=5496415", "title": "Spin Hall effect", "text": "Spin Hall effect\n\nThe spin Hall effect (SHE) is a transport phenomenon predicted by Russian physicists Mikhail I. Dyakonov and Vladimir I. Perel in 1971. It consists of the appearance of spin accumulation on the lateral surfaces of an electric current-carrying sample, the signs of the spin directions being opposite on the opposing boundaries. In a cylindrical wire, the current-induced surface spins will wind around the wire. When the current direction is reversed, the directions of spin orientation is also reversed.\n\nThe spin Hall effect is a transport phenomenon consisting of the appearance of spin accumulation on the lateral surfaces of a sample carrying electric current. The opposing surface boundaries will have spins of opposite sign. It is analogous to the classical Hall effect, where \"charges\" of opposite sign appear on the opposing lateral surfaces in an electric-current carrying sample in a magnetic field. In the case of the classical Hall effect the charge build up at the boundaries is in compensation for the Lorentz force acting on the charge carriers in the sample due to the magnetic field. No magnetic field is needed for the SHE which is a purely spin-based phenomenon. The SHE belongs to the same family as the anomalous Hall effect, known for a long time in ferromagnets, which also originates from spin-orbit interaction.\n\nThe spin Hall effect (direct and inverse) was predicted by Russian physicists Mikhail I. Dyakonov and Vladimir I. Perel in 1971. They have also introduced for the first time the notion of \"spin current\".\n\nIn 1983 Averkiev and Dyakonov have proposed a way to measure the inverse SHE under optical spin orientation in semiconductors. The first experimental demonstration of the inverse SHE, based on this idea, has been done by Bakun et al. in 1984\n\nThe term \"spin Hall effect\" was introduced by Hirsch who re-predicted this effect in 1999.\n\nExperimentally, the (direct) spin Hall effect was observed in semiconductors more than 30 years after the original prediction.\n\nTwo possible mechanisms give origin to the spin Hall effect, in which an electric current (composed of moving charges) transforms into a spin current (a current of moving spins without charge flow). The original (extrinsic) mechanism devised by Dyakonov and Perel consisted of spin-dependent Mott scattering, where carriers with opposite spin diffuse in opposite directions when colliding with impurities in the material. The second mechanism is due to intrinsic properties of the material, where the carrier's trajectories are distorted due to spin–orbit interaction as a consequence of the asymmetries in the material.\n\nOne can intuitively picture the intrinsic effect by using the classical analogy between an electron and a spinning tennis ball. The tennis ball deviates from its straight path in air in a direction depending on the sense of rotation, also known as the Magnus effect. In a solid, the air is replaced by an effective electric field due to asymmetries in the material, the relative motion between the magnetic moment (associated to the spin) and the electric field creates a coupling that distort the motion of the electrons.\n\nSimilar to the standard Hall effect, both the extrinsic and the intrinsic mechanisms lead to an accumulation of spins of opposite signs on opposing lateral boundaries.\n\nThe spin current is described by a second-rank tensor \"q\", where the first index refers to the direction of flow, and the second one – to the spin component that is flowing. Thus \"q\" denotes the flow density of the \"y\"-component of spin in the \"x\"-direction. Introduce also the vector \"q\" of charge flow density (which is related to the normal current density j=\"e\"q), where \"e\" is the elementary charge. The coupling between spin and charge currents is due to spin-orbit interaction. It may be described in a very simple way by introducing a single dimensionless coupling parameter \"ʏ\".\n\nNo magnetic field is needed for SHE. However, if a strong enough magnetic field is applied in the direction perpendicular to the orientation of the spins at the surfaces, spins will precess around the direction of the magnetic field and the SHE will disappear. Thus in the presence of magnetic field, the combined action of the direct and inverse SHE leads to a change of the sample resistance, an effect that is of second order in spin-orbit interaction. This was noted by Dyakonov and Perel already in 1971 and later elaborated in more detail by Dyakonov. In recent years, the spin Hall magnetoresistance was extensively studied experimentally both in magnetic and non-magnetic materials (heavy metals, such as Pt, Ta, Pd, where the spin-orbit interaction is strong).\n\nA transformation of spin currents consisting in interchanging (\"swapping\") of the spin and flow directions (\"q\" → \"q\") was predicted by Lifshits and Dyakonov. Thus a flow in the \"x\"-direction of spins polarized along \"y\" is transformed to a flow in the \"y\"-direction of spins polarized along \"x\". This prediction has yet not been confirmed experimentally.\n\nThe direct and inverse SHE can be monitored by optical means. The spin accumulation induces circular polarization of the emitted light, as well as the Faraday (or Kerr) polarization rotation of the transmitted (or reflected) light. Observing the polarization of emitted light allows the SHE to be observed.\n\nMore recently, the existence of both direct and inverse effects was demonstrated not only in semiconductors, but also in metals.\n\nThe SHE can be used to manipulate electron spins electrically. For example, in combination with the electric stirring effect, the SHE leads to spin polarization in a localized conducting region.\n\nFor a review of spin Hall effect, see for example, .\n"}
{"id": "8460275", "url": "https://en.wikipedia.org/wiki?curid=8460275", "title": "Stibadium", "text": "Stibadium\n\nThe \" stibadium \" (plural: \"stibadia\") is a later form of the Roman \"lectus triclinaris\", the reclining seat used by diners in the triclinium. Originally, the \"lecti\" were arranged in a group of three in a semi-circle. The \"stibadium\" was a single semi-circular couch, fitting up to a dozen people, which replaced the triple group of \"lecti\" in the dining-room, frequently in alcoves around the centre of the room. \n\nThe \"stibadium\" was originally an outdoor seat but was introduced indoors in the 2nd-3rd centuries B.C. because the shape was more convenient for entertaining and as \"triclinia\" became larger and more elaborate.\n\nFilms about ancient Roman \"convivia\" often feature a \"stibadium\" rather than a \"lectus\". \n\n\n"}
{"id": "17894750", "url": "https://en.wikipedia.org/wiki?curid=17894750", "title": "Storagewall", "text": "Storagewall\n\nA storage wall, in building trade jargon, is a system of floor to ceiling cupboards, storage elements, and doorways that can be pre-assembled or assembled on site when building an office. Storage walls are either built against pre existing walls, or can be used as a room divider / acoustic wall themselves.\n\nStoragewall is usually manufactured in MFC, and can be custom made to specific measurements to fit in with office requirements. Many manufacturers offer standard width size from 400mm to 1200mm. There are often different depths such as 400mm, 500mm or 600mm. Ceiling height is usually defined and cupboards built to match that height.\n\nThe efficiency of wall storage enables maximum floor to ceiling storage with minimal footprint by \"going vertical\" allowing them to store many more lever arch files than standard storage units.\n\nStorage wall systems are created typically using pegboard type dowel systems, however some do an aluminium based system to provide structural strength.\n\nThe internal options of these systems are quite extensive - usually having shelves, drawers, roll out filing frames, pigeon holes, coat hangers and many more convenient storage methods. With the office place changing to become more open and interactive Storagewall is starting to become more popular, keeping storage to the perimeter of the office freeing up central working areas.\n\nA collection of Storage Wall Images is here: and here [2]\n\n"}
{"id": "484241", "url": "https://en.wikipedia.org/wiki?curid=484241", "title": "Strategic information system", "text": "Strategic information system\n\nStrategic information systems (SIS) are information systems that are developed in response to corporate business initiative. They are intended to give competitive advantage to the organization. They may deliver a product or service that is at a lower cost, that is differentiated, that focuses on a particular market segment, or is innovative.\n\nStrategic information management (SIM) is a salient feature in the world of information technology (IT). In a nutshell, SIM helps businesses and organizations categorize, store, process and transfer the information they create and receive. It also offers tools for helping companies apply metrics and analytical tools to their information repositories, allowing them to recognize opportunities for growth and pinpoint ways to improve operational efficiency.\n\nSome of the key ideas of storefront writers are summarized. These include Michael E. Porter's Competitive Advantage and the Value Chain, Charles Wiseman's Strategic Perspective View and the Strategic Planning Process, F. Warren McFarlan's Competitive Strategy with examples of Information Service's Roles, and Gregory Parson's Information Technology Management at the industry-, firm-, and at the strategy level.\n\nThe concept of SIS was first introduced into the field of information systems in 1982-83 by Dr. Charles Wiseman, President of a newly formed consultancy called \"Competitive Applications,\" (cf. NY State records for consultancies formed in 1982) who gave a series of public lectures on SIS in NYC sponsored by the Datamation Institute, a subsidiary of Datamation Magazine.\n\nThe following quotations from the preface of the first book establishes the basic idea behind the notion of SIS:\n\nA SIS is a computer system that implements business strategies; They are those systems where information services resources are applied to strategic business opportunities in such a way that the computer systems affect the organization's products and business operations. Strategic information systems are always systems that are developed in response to corporate business initiative. The ideas in several well-known cases came from information Services people, but they were directed at specific corporate business thrusts. In other cases, the ideas came from business operational people, and Information Services supplied the technological capabilities to realize profitable results.\n\nMost information systems are looked on as support activities to the business. They mechanize operations for better efficiency, control, and effectiveness, but they do not, in themselves, increase corporate profitability. They are simply used to provide management with sufficient dependable information to keep the business running smoothly, and they are used for analysis to plan new directions. Strategic information systems, on the other hand, become an integral and necessary part of the business, and they affect the profitability and growth of a company. They open up new markets and new businesses. They directly affect the competitive stance of the organization, giving it an advantage against the competitors.\n\nMost literature on strategic information systems emphasizes the dramatic breakthroughs in computer systems, such as American Airlines' Sabre System and American Hospital Supply's terminals in customer offices. These, and many other highly successful approaches are most attractive to think about, and it is always possible that an equivalent success may be attained in your organization. There are many possibilities for strategic information systems, however, which may not be dramatic breakthroughs, but which will certainly become a part of corporate decision making and will, increase corporate profitability. The development of any strategic information systems always enhances the image of information Services in the organization, and leads to information management having a more participatory role in the operation of the organization.\n\nThe three general types of information systems that are developed and in general use are financial systems, operational systems, and strategic systems. These categories are not mutually exclusive and, in fact, they always overlap to some. Well-directed financial systems and operational systems may well become the strategic systems for a particular organization.\n\n\nAll businesses should have both long-range and short-range planning of operational systems to ensure that the possibilities of computer usefulness will be seized in a reasonable time. Such planning will project analysis and costing, system development life cycle considerations, and specific technology planning, such as for computers, databases, and communications. There must be computer capacity planning, technology forecasting, and personnel performance planning.\n\nOperational systems, then, are those that keep the organization operating under control and most cost effectively. Any of them may be changed to strategic systems if they are viewed with strategic vision.\n\n\nThere is general agreement that strategic systems are those information systems that may be used gaining competitive advantage. How is competitive advantage gained?. At this point, different writers list different possibilities, but none of them claim that there may not be other openings to move through.\n\nSome of the more common ways of thinking about gaining competitive advantage are:\n\nAlmost any data processing system may be called \"strategic\" if it aligns the computer strategies with the business strategies of the organization, and there is close cooperation in its development between the information Services people and operational business managers. There should be an explicit connection between the organization's business plan and its systems plan to provide better support of the organization's goals and objectives, and closer management control of the critical information systems.\n\nMany organizations that have done substantial work with computers since the 1950s have long used the term \"strategic planning\" for any computer developments that are going to directly affect the conduct of their business. Not included are budget, or annual planning and the planning of developing Information Services facilities and the many \"housekeeping\" tasks that are required in any corporation. Definitely included in strategic planning are any information systems that will be used by operational management to conduct the business more profitably. A simple test would be to ask whether the president of the corporation, or some senior vice presidents, would be interested in the immediate outcome of the systems development because they felt it would affect their profitability. If the answer is affirmative, then the system is strategic.\n\nStrategic system, thus, attempt to match Information Services resources to strategic business opportunities where the computer systems will affect the products and the business operations. Planning for strategic systems is not defined by calendar cycles or routine reporting. It is defined by the effort required to affect the competitive environment and the strategy of a firm at the point in time that management wants to move on the idea.\n\nEffective strategic systems can only be accomplished, of course, if the capabilities are in place for the routine basic work of gathering data, evaluating possible equipment and software, and managing the routine reporting of project status. The calendarized planning and operational work is absolutely necessary as a base from which a strategic system can be planned and developed when a priority situation arises. When a new strategic need becomes apparent, Information Services should have laid the groundwork to be able to accept the task of meeting that need.\n\nStrategic systems that are dramatic innovations will always be the ones that are written about in the literature. Consultants in strategic systems must have clearly innovative and successful examples to attract the attention of senior management. It should be clear, however, that most Information Services personnel will have to leverage the advertised successes to again funding for their own systems. These systems may not have an Olympic effect on an organization, but they will have a good chance of being clearly profitable. That will be sufficient for most operational management, and will draw out the necessary funding and support. It helps to talk about the possibilities of great breakthroughs, if it is always kept in mind that there are many strategic systems developed and installed that are successful enough to be highly praised within the organization and offer a competitive advantage, but will not be written up in the Harvard Business Review.\n\nAnother way of characterizing strategic information systems is to point out some of the key ideas of the foremost apostles of such systems.\n\nMichael E. Porter, Professor of Business Administration, Harvard Business School, has addressed his ideas in two keystone books. Competitive Strategy: Techniques for Analyzing Industries and Competitors, and his newer book, Competitive Advantage, present a framework for helping firms actually create and sustain a competitive advantage in their industry in either cost or differentiation. Dr. Porter's theories on competitive advantage are not tied to information systems, but are used by others to involve information services technologies. In his book, Dr. Porter says that there are two central questions in competitive strategy:\n\nNeither of these question is sufficient alone to guide strategic choices. Both can be influenced by competitor behavior, and both can be shaped by a firm's actions. It is imperative that these questions be answered by analysis, which will be the starting point for good strategic thinking, and will open up possibilities for the role of information systems.Industry profitability is a function of five basic competitive forces:\n\n\nPorter's books give techniques for getting a handle on the possible average profitability of an industry over time. The analysis of these forces is the base for estimating a firm's relative position and competitive advantage. In any industry, the sustained average profitability of competitors varies widely. The problem is to determine how a business can outperform the industry average and attain a sustainable competitive advantage. It is possible that the answer lies in information technology together with good management. Porter claims that the principal types of competitive advantage are low cost producer, differentiation, and focus. A firm has a competitive advantage if it is able to deliver its product or service at a lower cost than its competitors. If the quality of its product is satisfactory, this will translate into higher margins and higher returns. Another advantage is gained if the firm is able to differentiate itself in some way. Differentiation leads to offering something that is both unique and is desired, and translates into a premium price. Again, this will lead to higher margins and superior performance. It seems that two types of competitive advantage, lower cost and differentiation, are mutually exclusive. To get lower cost, you sacrifice uniqueness. To get a premium price, there must be extra cost involved in the process. To be a superior performer, however, you must go for competitive advantage in either cost or differentiation. Another point of Porter's is that competitive advantage is gained through a strategy based on scope. It is necessary to look at the breadth of a firm's activities, and narrow the competitive scope to gain focus in either an industry segment, a geographic area, a customer type, and so on. Competitive advantage is most readily gained by defining the competitive scope in which the firm is operating, and concentrating on it. Based on these ideas of type and scope, Porter gives a useful tool for analysis which he calls the value chain. This value chain gives a framework on which a useful analysis can be hung. The basic notion is that to understand competitive advantage in any firm, one cannot look at the firm as a whole. It is necessary to identify the specific activities which the firm performs to do business. Each firm is a collection of the things that it does that all add up to the product being delivered to the customer. These activities are numerous and are unique to every industry, but it is only in these activities where cost advantage or differentiation can be gained. The basic lis that the firm's activities can be divided into nine generic types. Five are the primary activities, which are the activities that create the product, market it and deliver it; four are the support activities that cross between the primary activities.\n\nThe primary activities are:\n\nThe support activities are not directed to the customer, but they allow the firm to perform its primary activities. The four generic types of support activities are:\n\nThe basic idea is that competitive advantage grows out of the firm's ability to perform these activities either less expensively than its competitors, or in a unique way. Competitive advantage should be linked precisely to these specific activities, and not thought of broadly at a firm-wide level. This is an attractive way of thinking for most information Services people, as it is, fundamentally, the systems analysis approach. Computer people are trained to reduce systems to their components, look for the best application for each component, then put together an interrelated system. Information technology is also pervasive throughout all parts of the value chain. Every activity that the firm performs has the potential to embed information technology because it involves information processing. As information technology moves away from repetitive transaction processing and permeates all activities in the value chain, it will be in a better position to be useful in gaining competitive advantage. Porter emphasizes what he call the linkages between the activities that the firm performs. No activities in a firm are independent, yet each department is managed separately. It is most important to understand the cost linkages that are involved so that the firm may get an overall optimization of the production rather than departmental optimizations. A typical linkage might be that if more is spent in procurement, less is spent in operations. If more testing is done in operations, after-sales service costs will be lower. Multifunctional coordination is crucial to competitive advantage, but it is often difficult to see. Insights into linkages give the ability to have overall optimization. Any strategic information system must be analyzed across all departments in the organization. Cost and Competitive Advantage. Cost leadership is one of Porter's two types of competitive advantage. The cost leader delivers a product of acceptable quality at the lowest possible cost. It attempts to open up a significant and sustainable cost gap over all other competitors. The cost advantage is achieved through superior position in relation to the key cost drivers. Achieving cost leadership usually requires trade-offs with differentiation. The two are usually incompatible. A firm's relative cost position cannot be understood by viewing the firm as a whole. Overall cost grows out of the cost performing discrete activities. Cost position is determined by the cumulative cost of performing all value activities. To sustain cost advantage, Porter gives a number of cost drivers which must be understood in detail because the sustainability of cost advantage in an activity depends on the cost drivers of that activity. Again, this type of detail is best obtained by classical systems analysis methods.\n\nSome of the cost drivers which must be analyzed, understood, and controlled are:\n\nCare must be taken in the evaluation and perception of cost drivers because there are pitfalls if the thinking is incremental and indirect activities are ignored. Even though the manufacturing activities, for example, are obvious candidates for analyses, they should not have exclusive focus. Linkages must be exploited and cross-subsidies avoided.\n\nPorter gives five steps to achieving cost leadership:\n\nDifferentiation is the second of Porter's two types of competitive advantage. In the differentiation strategy, one or more characteristics that are widely value by buyers are selected. The purpose is to achieve and sustain performance that is superior to any competitor in satisfying those buyer needs. A differentiator selectively adds costs in areas that are important to the buyer. Thus, successful differentiation leads to premium prices, and these lead to above-average profitably if there is approximate cost parity. To achieve this, efficient forms of differentiation must be picked, and costs must be reduced in areas that are irrelevant to the buyer needs. Buyers are like sellers in that they have their own value chains. The product being sold will represent one purchased input, but the seller may affect the buyer's activities in other ways. Differentiation can lower the buyer's cost and improve the buyer's performance, and thus create value, or competitive advantage, for the buyer. The buyer may not be able to assess all the value that a firm provides, but it looks for signals of value, or perceived value.\n\nA few typical factors which may lower the buyer's costs are:\n\nPorter points out that differentiation is usually costly, depending on the cost drivers of the activities involved. A firm must find forms of differentiation where it has a cost advantage in differentiating. Differentiation is achieved by enhancing the sources of uniqueness. These may be found throughout the value chain, and should be signaled to the buyer. The cost of differentiation can be turned to advantage if the less costly sources are exploited and the cost drivers are controlled. The emphasis must be on getting a sustainable cost advantage in differentiating. Efforts must be made to change the buyer's criteria by reconfiguring the value chain to be unique in new ways, and by preemptively responding to changing buyer or channel circumstances. Differentiation will nor work if there is too much uniqueness, or uniqueness that the buyers do not value. The buyer's ability to pay a premium price, the signaling criteria, and the segments important to the buyer must all be understood. Also, there cannot be over reliance on sources of differentiation that competitors can emulate cheaply or quickly.\n\nPorter lists seven steps to achieving differentiation:\n\nFocus Strategies for Advantage. Porter's writings also discuss focus strategies. He emphasizes that a company that attempts to completely satisfy every buyer does not have a strategy. Focusing means selecting targets and optimizing the strategies for them. Focus strategies further segment the industry. They may be imitated, but can provide strategic openings. Clearly, multiple generic strategies may be implemented, but internal inconsistencies can then arise, and the distinctions between the focused entities may become blurred. Porter's work is directed towards competitive advantage in general, and is not specific to strategic information systems. It has been reviewed here at some length, however, because his concepts are frequently referred to in the writings of others who are concerned with strategic information systems. The value chain concept has been widely adopted, and the ideas of low cost and differentiation are accepted. This section, therefore, is an introduction into a further discussion of strategic information systems. The implementation of such systems tends to be an implementation of the factors elucidated by Porter.\n\nWiseman applied the concepts of SIS in work at GTE (Implementors of SIS for competitive advantage) and other companies, and in his consulting work. His book extends Porter's thinking in many practical ways in the Information Systems area, and discusses many examples of strategic systems.\n\nWiseman emphasizes that companies have begun to use information systems strategically to reap significant competitive advantage. He feels that the significance of these computer-based products and services does not lie in their technological sophistication or in the format of the reports they produce; rather, it is found in the role played by these information systems in the firm's planning and implementation in gaining and maintaining competitive advantage.\n\nWiseman points out that although the use of information systems may not always lead to competitive advantage, it can serve as an important tool in the firm's strategic plan. Strategic systems must not be discovered haphazardly. Those who would be competitive leaders must develop a systematic approach for identifying strategic information systems (SIS) opportunities. Both business management and information management must be involved.\n\nA framework must be developed for identifying SIS opportunities. There will certainly be competitive response, so one should proceed with strategic thrusts based on information technology. These moves are just as important as other strategic thrusts, such as acquisition, geographical expansion, and so on. It is necessary to plan rationally about acquisition, major alliances with other firms, and other strategic thrusts.\n\nIMB’S Business Systems Planning (BSP) and MIT's Critical Success Factor (CSF) methodologies are ways to develop information architectures and to identify conventional information systems, which are primarily used for planning and control purposes. To identify SIS, a new model or framework is needed. The conventional approach works within the perceived structures of the organization. An effective SIS approach arises from the forging of new alliances that expand the horizon of expectation. Such an approach is most difficult to attain, and can only work with top management support. Innovations, however, frequently, come from simply a new look at existing circumstances, from a new viewpoint. Information Services people must start to look systematically at application opportunities related to managers.\n\nWiseman believes that the range of opportunities is limited by the framework adopted. He contrasts the framework for Conventional IS Opportunities with the framework for Strategic IS Opportunities.\n\nIn the conventional view, there are two information system thrusts: to automate the basic processes of the firm, or to satisfy the information needs of managers, professionals, or others. There are three generic targets: strategic planning, management control, and operational control. In this perspective, there are, thus, six generic opportunity areas.\n\nIn the strategic view of IS opportunities, there are five strategic information thrusts and three strategic targets. This gives fifteen generic opportunity areas. This opens up the range and perspective of management vision.\n\nSustainable competitive advantage can mean many things to different firms. Competitive advantage may be with respect to a supplier, a customer, or a rival. It may exist because of a lower price, because of desirable features, or because of the various resources that a firm possesses. Sustainability is also highly relative, depending upon the business. In established businesses, it may refer to years, and the experience that the firm develops may be quite difficult to emulate. In other industries, a lead of a few weeks or months may be all that is necessary.\n\nWiseman uses the term strategic thrusts for the moves that companies make to gain or maintain some kind of competitive edge, or to reduce the competitive edge of one of the strategic targets. Information technology can be used to support or to shape one or more of these thrusts. Examining the possibilities of these thrusts takes imagination, and it is helped by understanding what other firms have done in similar situations. This is why so many examples are presented in the literature. Analogy is important.\n\nThere is no question that there is considerable overlap between conventional information systems and strategic information systems. Systems are complex and a great deal of data is involved. The idea is to look at this complexity in a new light, and see where competitive advantage might possibly be gained. Note that Wiseman takes Porter's three generic categories: low cost producer, differentiation, and focus, and extends them to five categories: differentiation, cost, innovations, growth, and alliance.\n\nCost may be move that not only reduces the costs, but also reduces the costs of selected strategic targets so that you will benefit from preferential treatment. A strategic cost thrust may also aim at achieving economies of scale. The examples always seem obvious when they are described, but the opportunities can usually only be uncovered by considerable search.\n\nInnovation is another strategic thrust that can be supported or shaped by information technology in either product or process. In many financial firms, the innovative product is really an information system. Innovation requires rapid response to opportunities to be successful, but this carries with it the question of considerable risk. There can be no innovation without risk, whether information systems are included or not. Innovation, however, can achieve advantage in product or process that results in a fundamental transformation in the way that type of business is conducted.\n\nGrown achieves an advantage by expansion in volume or geographical distribution. It may also come from product-time diversification. Information systems can be of considerable help in the management of rapid growth.\n\nAlliance gains competitive advantage by gaining growth, differentiation, or cost advantages through marketing agreements, forming joint ventures, or making appropriate acquisitions.\n\nThe Strategic Planning Process. Wiseman advocates brainstorming and the systematic search for SIS opportunities. He describes his SIS Planning Process in five phases:\n\n\nWiseman points out that the whole idea is designed to introduce the strategic perspective on information systems, stimulate the systematic search for SIS opportunities, and evaluate and select a set of projects that are expected to secure the greatest competitive advantage for the firm. In the idea-generation meetings of Phases 2, 3, and 5 of the process, there are always seven explicit steps:\n\n\nWiseman says that typical SIS idea-generation meetings will last for days. Each step takes about two hours, at least. The process generates many good SIS ideas, and a few will always be considered worth implementation. Top management begins to focus their attention on SIS opportunities. The ideas that are generated can produce significant competitive advantage.\n\n"}
{"id": "21694203", "url": "https://en.wikipedia.org/wiki?curid=21694203", "title": "Stratospheric Aerosol and Gas Experiment", "text": "Stratospheric Aerosol and Gas Experiment\n\nThe Stratospheric Aerosol and Gas Experiment (SAGE) is a series of remote sensing satellite instruments used to study the chemical composition of earth's atmosphere. Specifically, SAGE has been used to study the Earth's ozone layer and aerosols at the troposphere through the stratosphere. The SAGE instruments use solar occultation measurement technique to determine chemical concentrations in the atmosphere. Solar occultation measurement technique measures sunlight through the atmosphere and ratios that measurement with a sunlight measurement without atmospheric attenuation. This is achieved by observing sunrises and sunsets during a satellite orbit. Physically, the SAGE instruments measure ultraviolet/visible energy and this is converted via algorithms to determine chemical concentrations. SAGE data has been used to study the atmospheres aerosols, ozone, water vapor, and other trace gases.\n\nThere has been 3 series of SAGE instruments.\n\n"}
{"id": "16808149", "url": "https://en.wikipedia.org/wiki?curid=16808149", "title": "Taipei Public Library Beitou Branch", "text": "Taipei Public Library Beitou Branch\n\nTaipei Public Library Beitou Branch () is a public library in Beitou Park, located within Beitou District, Taipei, Taiwan. It is Taiwan's first green library.\n\nThe library was opened in November 2006.\n\nIt is a two-storey building and it is notable as being constructed to be an eco-friendly green building, in which the building was designed to curb water and electricity consumption. It was designed by Bio-Architecture Formosana. The building uses large windows to reduce the consumption of lighting electricity. The roof was designed to be partially covered with photovoltaic cells to generate electricity and also designed to capture rain water to be stored and used to flush toilets.\n\nThe library is accessible within walking distance East from Xinbeitou Station of the Taipei Metro.\n\n"}
{"id": "29991137", "url": "https://en.wikipedia.org/wiki?curid=29991137", "title": "Technological and industrial history of 21st-century Canada", "text": "Technological and industrial history of 21st-century Canada\n\nThe technological and industrial history of Canada encompasses the country's development in the areas of transportation, communication, energy, materials, public works, public services (health care), domestic/consumer and defense technologies. The 21st century has become the Internet Age is way both literal and metaphorical. The technology that dominates this period of time is wireless technology, cloud computing, HD/3D TV, mega oil, \"greentech\" and nanotechnology. Most technologies diffused in Canada came from other places; only a small number actually originated in Canada. For more about those with a Canadian origin, see Invention in Canada.\n\nTechnology is a major cultural determinant, no less important in shaping human lives than philosophy, religion, social organization, or political systems. In the broadest sense, these forces are also aspects of technology. The French sociologist Jacques Ellul defined \"la technique\" as the totality of all rational methods in every field of human activity so that, for example, education, law, sports, propaganda, and the social sciences are all technologies in that sense. At the other end of the scale, common parlance limits the term's meaning to specific industrial arts.\n\nThe Internet has become an essential part of daily life and is found in most Canadian homes, businesses and government offices. In December 2006, there were 22,000,000 Internet users representing 65.9% of the population and 7,675,533 Internet broadband connections. In 1988, the first .ca Canadian web address, upei.ca, was assigned by John Demco of the University of British Columbia (UBC) to the University of Prince Edward Island. The one millionth .ca address, krauslaw.ca was assigned in 2008 by the Canadian Internet Registration Authority, formed in 1998, to Brent Kraus of Calgary for the promotion of his law firm. As of the end of 2010, Canadians, on a per-capita basis, were the most intensive users of Internet in the world. \nDuring this period, the web search engine became an integral part of use of the Internet. The first such programme, the \"Archie search engine\", was developed by McGill University student Alan Emtage in 1990. Since then, search engines, which have been mostly developed in the US, have evolved and become more versatile and powerful. Notable engines include Lycos (1994), Alta Vista (1995), Magellan (1995), Google (1998), Yahoo! Search (2004), MSN Search (2005), and Bing (2009). These Internet tools are available to web users from countries around the world, including Canada.\n\nE-mail, a very popular feature of the Internet, predated that technology by decades. E-mail type functionality was a feature of a computer sharing technology developed at MIT in the US in 1961. It was also part of the US-developed Semi Automatic Ground Environment (SAGE) component of the North American air defense system created in the fifties and sixties and which included a facility at RCAF North Bay, Ontario. However, it only became a publicly used service with the development of the Internet. A number of US providers now offer this worldwide service to Canadian users, including MSN Hotmail in 1996, Yahoo! Mail in 1997, AOL Mail in 2004 and Gmail in 2004.\n\nThe development of special software allowed the Internet to be used to make computer to computer phone calls. In August 2003, a service known as Skype became available to Internet users around the world including Canada. It has since become extremely popular.\n\nOther web sites including those for social networking such as Facebook (2004), which as of 2008 has 17 million Canadian profiles, MySpace (2003), with 4.5 million Canadian profiles as of 2008, and Twitter (2006). Video and photo sharing sites such as YouTube (2005), with 14.5 million Canadian visits per month, and Vancouver-developed Flickr (2004), have become extremely popular in Canada. The popular Canadian-developed on-line dating service Lavalife went on line for the first time in 1997. In 2010, more than 2 million Canadians were members of LinkedIn, a social networking website, developed in the US in 2003 to enable workers to network for professional and career reasons. In recognition of the importance of the Canadian market, the company opened a Canadian office in 2010.\n\nThe big Canadian banks, including the Royal Bank of Canada, the Toronto Dominion Bank, the Canadian Imperial Bank of Commerce and the Bank of Nova Scotia made their customer accounts available on line as the web gained prominence. On-line investing has grown in popularity in the new century a number of Canadian firms offer sites for this service including, QTrade Investor, BMO InvestorLine, E*Trade Canada (now Scotia iTrade), TD Waterhouse, Credential Direct, RBC Direct Investing, CIBC Investor’s Edge, Disnat, ScotiaMcLeod, National Bank Direct Brokerage, and Virtual Brokers.\n\nOther businesses established a retail presence, notably Amazon.com in 1995, which became popular enough in Canada to merit a separate Canadian site, Amazon.ca, beginning in 2002. The Internet auction site eBay, launched in 1995, gave rise to a Canadian spin-off eBay.ca in 2000. PayPal has been operated by eBay since 2002, and is widely used by Canadians to cover the financial aspect of eBay transactions.\n\nMost large Canadian corporations, including telephone and utility companies, now provide on-line customer billing.\n\nThe Government of Canada has been especially notable in establishing a very diverse and friendly on-line presence for the public. Initially, the basis for this service was a suite of technologies referred to as the Government Enterprise Network (GENet). In the fall of 2003, the government began to replace these with improved technologies known collectively as the Secure Channel Network (SCNet), which make available a wide range of services. For example, in recent years it has become possible for Canadians to file their yearly income tax returns using an Internet service provided by Revenue Canada known as NETFILE.\n\nThe Internet has also become an important source of information, marked by the popularity of such sites as Wikipedia and Google Earth. Wikipedia, an on-line encyclopedia, was established in the US in 2001 by Jimmy Wales and Larry Sanger and presently has over 5,000,000 articles in English and a large number in other languages. Many articles have been contributed to both the English and French language versions of Wikipedia by Canadians, and many of these relate to important aspects of Canadian life. There are thousands of Canadians who use the service every day in both English and French. Google Earth, a virtual globe, is an on-line feature offered by Google since 2005. It provides aerial views of the Earth and is viewed by thousands of web users, many of which are Canadian, every day. Another Internet information service, the telephone directory Canada 411.ca, has become very popular since introduced in recent years. This has led to the delivery of the residential paper telephone book, introduced to Canada in 1878, being cancelled in major cities in 2010.\n\nIn 2010, the Government of Ontario announced its intention of beginning to offer on-line Internet gambling to the residents of Ontario in 2012. It will join the PlayNow.com Internet gambling site established by the government of British Columbia in 2010.\n\nThe Internet has been the target of cyberattacks over the years. One of the most notable attacks was made by 15-year-old hacker Michel Calce, alias \"Mafiaboy\", from the home of his parents in Montreal. Using a technique known as \"distributed denial-of-service\" (DDoS), he paralyzed the websites of Yahoo, CNN, E*Trade, Dell, eBay and Amazon in February 2000.\n\nStar Choice (Shaw Direct) of Calgary, Alberta, and Expressvu Bell TV of Montreal began offering Canada-wide direct-to-home digital satellite television service in 1997. As of 2008, they had 900,000 and 1.8 million subscribers respectively. Star Choice broadcast the first high-definition television programme in Canada in 2000 and began broadcasting HD full-time in 2004. HD channels have been continuously added since that date. In April 2009, Star Choice changed its name to Shaw Direct. By 2009 satellite delivered Bell TV was delivering 45 HD channels.\n\nRogers Cable, Canada's largest cable company, began to offer its Digital Television service in 2001. Video on demand (VOD), a technology that allows digital cable subscribers to order and watch movies at a time of their choice, has been available to Canadians since 2002, the year that Rogers Communication Inc., began to offer its Rogers on Demand service. By 2009 the service was available to 3.5 million homes. Shaw Communications Inc., Canada's second largest cable company offers a similar service. Rogers introduced personal video recorders (PVRs) to customers in 2003.\n\nThe CBC began broadcasting digital over-the-air HDTV in 2005. A national government regulatory body, the Canadian Radio, Television and Telecommunications Commission, has stated that all over-the-air TV broadcasting will be digital by August 2011.\n\nDuring this period, efforts to convert over-the-air AM and FM radio to digital technology failed. A technique known as Digital Audio Broadcasting (DAB) (see Countries using DAB/DMB), was introduced to Canada in November 1999. However the technology never caught on, partly because of the chicken and egg phenomenon. When the technology was introduced, there were few listeners equipped with DAB receivers, and this in turn provided little incentive for broadcasters to convert their very successful AM and FM operations to DAB. DAB came to an end in Canada in 2010. However digital satellite radio has been successful. Two Toronto-based companies, Sirius Canada and XM Canada introduced direct-to-home/car, digital satellite radio service in December 2005 and by 2008 had 750,000 and 400,000 subscribers respectively. In 1999, Telesat launched the first of four Nimiq direct broadcast satellites which provide the space-based satellite transmitters for these services.\n\nIn 2003, Bell Canada introduced an improved speech recognition system for its 310-2355 customer routing service in Ontario. Bell Canada users speak with the programme through \"Emily\", a young female-sounding artificial voice. In 2005, Skype, a voice-and-video-over-Internet technology became available to users around the world, including Canadians. The technique, which bypasses the traditional telephone network, allows people to use the Internet as a type of telephone and to both talk to and see each other during calls. It is used mainly for long-distance communication.\n\nThe proliferation of multiple communications technologies has itself created the need to combine them effectively, resulting in a new technology, unified communications. This technique blends instant messaging, e-mail, voice mail, short message service, web-conferencing, fax, audio, video, cellphone, VIOP and other telecommunications services into a single system. Cooke Aquaculture Inc. of Blacks Harbour New Brunswick uses just such a system, developed by Cisco Systems Canada Co. to manage its fish farm operations.\n\nDigital media were first introduced to Canada in the 1980s, when the CD and DVD became popular with consumers.\n\nThe traditional media began to develop an on-line presence in the new century. Newspapers including Canada's two English-language \"national\", dailies, \"The Globe and Mail\" and the National Post went on line as did the weekly Maclean's news magazine. The French-language press did the same including the daily \"La Presse\" and the bi-weekly \"L'actualité\" newsmagazine.\n\nTelevision broadcasters got into the game, including the English-language national networks, the Canadian Broadcasting Corporation (CBC), CTV Television Network (CTV) and Canwest/Global and the French-language networks, Radio Canada, TVA (TV network) and TQS (now V). In 2009, a number of news services, including Thomson Reuters and Canadian Press, began to offer wireless Internet news services formatted for access by hand-held 3G devices such as the Blackberry.\n\nIn 2009, Indigo Books and Music began to offer a digital book service known as Shortcovers. In late 2009, that electronic service was expanded and renamed Kobo (Kobo Inc.). It offers customers about 2,000,000 book titles in electronic form that can be viewed on an electronic reader. A variety of electronic books or readers have gained a place in Canada beginning with the introduction of the Sony Librie reader in 2004 and the Kindle in 2009. Kobo intends to introduce its own reader in 2010 with code based on an open source concept. In 2010 a number of Canadian libraries, including the Ottawa Public Library began offering books on loan via a downloadable Kindle format.\n\nWith the release of the iPad in Canada in 2010, digital media providers have begun to format their digital media offerings to make them compatible for iPad viewing.\n\nThe downloading of music from the Internet to computers and other storage devices including the iPod, has become very popular in recent years. Music can be downloaded peer-to-peer or from about 500 on-line sites in 40 countries. In Canada one site of note, Puretracks, has been offering a library of about 1.3 million popular songs in Windows Media Audio and MP3 format for download since 2003.\n\nThe film industry has also moved to adopt digital cinema technology. The technology of cinematic special effects has become a notable feature of the film production, with over 2300 Canadian companies, including, Side Effects Software, Toon Boom Animation, Image Engine, (Vancouver), Intelligent Creatures, (Toronto), Intrigue FX and Rainmaker Digital Effects (CIS) in Vancouver, being involved in the field. The National Film Board of Canada began to digitize its extensive archives and later in 2008 will announce the availability of its films on-line.\n\nIn Toronto, Cineplex Entertainment, through Technicolor Digital Cinema has installed the Canadian made Christie CP2000 DLP Cinema projector in the Scotiabank Theatre in Toronto, making it the first Canadian cinema operating this new technology, which provides sharp images and uncompressed digital sound. It can also project 3-D features with Real D Cinema. Cineplex plans to have 25 cinemas across Canada equipped with this new technology in the near future. A Montreal company, D-Box began to offer motion seats for cinemas in 2008. These seats physically move in a way designed to enhance the movie-going experience. The movement is induced by a digital signal specially embedded in the film which activates pistons in the seat that produce the physical movement. Canadian cinemas equipped with the devices in 2010 include, the Cinéma Beloeil, in Beloeil, Quebec and the Cineplex Odeon Queensway Theatre in Etobicoke, Ontario\n\nCompanies such as Electronic Arts, Ubisoft Montreal, BioWare and Next Level Games are active in the technologies related to the development and manufacture of video games. As of 2010 video game publishers and developers in Canada were found in major cities across the country including: Vancouver 47, Toronto 33, Montreal 22, Ottawa 13.\n\nThe use of mobile devices for accessing the Internet through a wireless local-area-network, wireless LAN in Canada has increased dramatically in recent years.\n\nGeographic areas having access to a wireless local-area-network are often referred to as having Wi-Fi service.\n\nBy 2006 Internet providers began making \"mobile\" Internet connection available to their customers with companies such as Bell Canada offering their \"unplugged\" service. This type of service uses the laptop computer and plug-in modem to allow mobile Internet connection in many places across Canada. \"Wireless\" Internet communications have also been facilitated through the introduction of the widely popular Research In Motion, BlackBerry handheld email and telephone machine and the introduction in 2008, by Rogers, of the \"Rocket\" wireless Internet stick for laptops.\n\nIn 2007 Canadian wireless carriers began to convert their DAVE! systems from the CDMA standard which restricted the user to service within North America to the GSM standard used by most carriers around the world. Videotron Telecom Ltee., one of the winners of the Canadian government wireless spectrum auction of 2008, announced that it would invest C$255 million to build a wireless network in Quebec, using the High Speed Packet Access, (HSPA) technical standard.\n\nIn 2009, 3G wireless Internet technology became widely available to Canadians through national networks operated by Bell Mobility, Rogers, and Telus. The use of the Netbook a small portable computer that takes advantage of 3G technology to provide access the Internet became popular in Canada beginning in 2009.\n\nAs of 2009, the downloading of applications and data (music, videos, etc.) via smartphone is becoming increasingly popular in Canada. The bandwidth represented by this use represents up to 40 time the bandwidth used by cellphones for voice calls, putting a tremendous load on existing cellphone networks and driving Rogers Communications Inc., Bell Canada and Telus to invest heavily in expanding the capacity of their networks.\n\nIn 2008 the government of Canada, as part of an effort to increase competition in the mobile communications industry, gave a number of new companies including, Public Mobile Holdings Inc., Globalive Communications Inc. and DAVE Wireless Inc. approval to establish new wireless operations in Canada to compete with the three incumbents. Bell Mobility introduced a smartphone with the Google developed Android operating system in 2009. In 2010 Google made its Nexus One available to Canadian consumers who can obtain these devices from Rogers Wireless, Telus, Bell and Wind Mobile.\n\nAs the result of a CRTC decision, cellphone providers in Canada, as of 2010, were able to locate within a radius of 300 metres, the geographic position of a handset used for making a 911 call.\n\nIn 2009, Canada's three cell phone companies, Rogers Communications Inc., Bell Canada and Telus created a jointly owned company, Enstream LP, which offers a cash transfer service via cellphone. To use the service the subscriber first downloads special software called Zoompass, from Enstream to his or her phone. With this software, a Bell, Fido, PC Mobile, Rogers, Solo Mobile, or Telus subscriber can then withdraw up to $1000 daily from his bank account, or credit card account and transfer the amount to another subscriber who uses the same Zoompass software. Enstream plans to make the service increasingly flexible with the end goal of converting the cellphone into an electronic wallet or purse. In August 2010, Telus began offering the FaceTime service for its iPhone 4 customers.\n\nFoursquare, a cellphone-based mobile social networking service, was introduced to Canada in 2010. The service allows cellphone users to download the Foursquare software to their mobile phones and use it to stay connected with friends and colleagues using the same software and to obtain information on their physical location.\n\nTelus Canada began offering a telephone service called Tigits across Canada in early 2011. Tigits provides a temporary anonymous telephone number for those who subscribe to the service. Developed by Toronto businessman, Sean Miller, Tigits allows the subscriber to protect his/her real number by giving the temporary Tigits number to others. When a Tigits subscriber calls the other person, the person called sees only the Tigits number on his digital display and not the real number of the person calling. When the other person calls the Tigits number of the Tigits subscriber the call is forwarded to the real number of the Tigits subscriber, thus protecting his/her anonymity.\n\nThe University of Montreal has recently experimented with ways to improve the administration of justice by creating a digital court room in which mock trials are held using modern technology to speed the proceedings. The \"courtroom\" has facilities for filing documents electronically. Witnesses can testify by video link or holographically from a remote location. Documents can be served on parties through social media. According to those involved with the project, barriers to its application in real courtroom settings are not technological but rather emotional, with judges and lawyers being resistant to change.\n\nEvidence that cloud computing had begun to take hold in Canada by 2009 is reflected in the organization of the first Canadian Cloud Computing Conference held on 9 February of that year, in Toronto. Cloud computing involves the use of information processing and data storage on computers that are located away from the site of the user and owner of the data. The user, a corporation or individual, communicates with the remote computer through the Internet. The growing popularity of the small netbook computer is in part due to the fact that it is ideally suited to take advantage of cloud computing. The technique allows the user to focus more on processing and storage than on equipment and software acquisition and maintenance. However, it also raises questions relating to privacy and security, in that confidential data may be transmitted, processed and stored on facilities beyond the geographic reach of the owner of the data. Cloud computing providers offer three types of service, platform-as-a-service (PaaS), software-as-a-service (SaaS) and infrastructure-as-a-service (IaaS). As of 2009, Toronto area companies appeared to be the leaders of cloud computing in Canada. An initiative based in Kitchener/Waterloo has brought together a group of computer service providers to create Canadian Cloud Computing, which has developed the Trusted Canadian Cloud. This cloud computing service, which uses facilities based exclusively in Canada, was first demonstrated at Canada 3.0 Digital Media Conference held in Stratford, Ontario in 2010.\n\nQuantum computing in Canada is also gaining a foothold as evidenced by D-Wave Systems, a Burnaby-based quantum computing company founded in 1999 where in May 2013 it was announced that a collaboration between NASA, Google and the Universities Space Research Association (USRA) launched a Quantum Artificial Intelligence Lab using a 512 qubit D-Wave Two that would be used for research into machine learning, among other fields of study. Though the field is still in its infancy, experiments have been carried out in which quantum computational operations were executed on a very small number of qubits. Both practical and theoretical research continues, and many national governments and military funding agencies support quantum computing research to develop quantum computers for both civilian and national security purposes, such as cryptanalysis.\n\nThe use of facial recognition technology (FRT) has grown in Canada in recent years. Nineteen of 27 Ontario Lottery and Gaming Corporation casinos, which receive 40,000,000 visitors a year, are using FRT to automatically identify 15,000 problem gamblers who have voluntarily placed themselves on a self-exclusion list. The Canadian Bankers Association has been using FRT since 2008 to investigate debit card fraud. The Insurance Corporation of British Columbia has been using FRT since 2008 to counter fraudulent attempts by individuals to obtain a driver's license. In the summer of 2010, the technology was used by the Toronto Police to identify suspects wanted for vandalism or violent acts committed during the G-20 Summit there. The Department of Foreign Affairs has begun to issue e-passports with a chip that will enable the use of facial recognition technology beginning in 2012. Social media organizations such a Facebook, with millions of Canadian users have also adopted the use of FRT in their operations. All these applications and others raise privacy concerns.\n\nInternet television began to make inroads in Canada in 2009 with communications providers including, Bell TV, Telus, Rogers Communications and Quebecor (Videotron) investing in the Internet bandwidth necessary to provide their subscribers with TV programmes and movies. Bell TV and Rogers Communications Inc. introduced Internet TV to their subscribers in the fall of 2009. Telus began to offer its rebranded Optik, IPTV service in June 2010. That same month, Quebecor began to offer the Illico Web service with 32 channels (24 in French) to its subscribers through its Videotron subsidiary. The company has stated that the new service will eventually become a \"mirror\" of its TV offerings. Bell began providing a rebranded IPTV service called Fibe, in September 2010. In 2010, Shaw Communications announced its intention to begin providing its customers with Internet TV. Halifax based EastLink is also investing in IPTV.\n\nThe delivery of movies and television programmes through the Internet in Canada was also given a boost with the introduction a streaming video Internet service by the US based Netflix, via its Canadian server at Netflix.ca in 2010. Canadian-owned movie delivery services were also introduced. Cineplex began to offer a movie download service and a streaming on-line Internet video service was introduced at Zip.ca.\n\nThe first efforts towards providing Canadian viewers with 3D TV were made in 2010. Early in the year two international consumer product manufacturers, Samsung and Sony began to market flat screen, digital, high definition, 3D television sets in Canada. The 3D effect is only available with the use of a pair of special glasses worn by the viewer. Each company has its own proprietary viewing standard, so that the glasses of one company cannot be used to view the 3D television of another. DVD movies recorded in 3D provide what is, as yet, a limited source of programming.\n\nBroadcasters also began to take steps to provide 3D programming to viewers equipped with these new 3D sets. The World Cup soccer championship played in South Africa was broadcast in 3D by the host broadcaster and the signal was offered in Canada by a number of TV providers. On 27 July 2010, the satellite delivered Bell TV began to offer its subscribers a full-time 3D Oasis pay TV channel. The CBC also announced in August 2010 that it would broadcast the first Canadian produced 3D programme on 20 September 2010. It would be available to all viewers in Canada with an HD television set but the 3D effect would only be available when the programme was viewed with special glasses which will be distributed free at Canada Post Offices across Canada before the programme.\n\nOn 2 August 2010, a Toronto company, InteraXon, announced that it had developed technology to control machines by human thought. The technique involves the use of a head set to detect \"alpha\" and \"beta\" brain waves. The head set in turn produces a \"control signal\" that can be used to program electrical appliances ranging from lights to home appliances to computers. The technology was demonstrated at the 2010 Winter Olympic Games in Vancouver, where visitors used their brain waves to control the lighting on three landmarks, the CN tower in Toronto, the Parliament Buildings in Ottawa and Niagara Falls. The technology is based on the research of Dr. Steve Mann, a University of Toronto professor who initially developed thought-controlled computing technology. The company foresees that the headset will evolve into a small wireless Bluetooth device that will be available in consumer electronics retail stores within two years.\n\nAnalog technology has dominated the history of the communications system in Canada for almost 160 years. It formed the basis for the telegraph, beginning in the 1850s, the telephone in the 1880s, recorded sound, the 20th century, radio, the 1920s, computers and television, the 1950s and cable TV in the 1960s.\n\nHowever, digital technology has slowly replaced analog technology in all these domains in the past 40 years. The transformation began with the telephone system, in the 1970s and microchips and microcomputers in the early 1980s. Indeed, it was the combination of the telephone system and computers through a common digital link that permitted the latter machines to communicate with each other at distance. Further digital advances lead to the digital camera, CD, DVD and mobile communications, later in that decade, the Internet in the 1990s as well as land based and satellite TV and radio, wireless communications etc., in the first decade of the new century.\n\nWith the CRTC mandated end to analog television broadcasting in Canada in August 2011, the analog age in Canada will for all practical purposes come to an end.\n\nEnergy concerns have had a large impact on automobile manufacturers. Fuel efficient hybrid vehicles such as the Chevrolet Tahoe, Saturn Vue, Toyota Prius, Toyota Camry Hybrid, Toyota Highlander Hybrid, Ford Escape Hybrid, Honda Insight and Honda Civic Hybrid have become available to Canadian consumers since the start of the 21st century and the rising cost of gasoline is making them increasingly attractive in spite of their generally higher cost. As of 2009, the Ford Fusion Hybrid was the most fuel efficient mid-sized car available in Canada. In 2008 Ford Canada began the operation of the Flex assembly line, using the Flex technique at its plant in Oakville, Ontario. This technology allows the production of three different automobile types, in this case, the Ford Edge, the Ford Flex and the Ford Lincoln MKX, on the same assembly line. In 2004 Mercedes-Benz introduced the diminutive and fuel efficient Smart Fortwo automobile to the Canadian market. Multinational car manufacturers have also announced their intentions to introduce the all-electric car to world markets including Canada. General Motors has announced the availability of its Chevrolet Volt in Canada in 2011 as has Mitsubishi for its MiEV, while Nissan has announced the Canadian introduction of the Nissan Leaf in 2012.\n\nThe management of automobile traffic in large urban areas through the use of \"smart\" electronic traffic management systems has become popular in recent years. Such systems are now in place in Toronto (1993), Ottawa, Calgary and Halifax. The city of Montreal will take the first steps for the installation of such a system in 2011. 500 video cameras and other street and highway mounted sensors will provide information for a central computer which will be used to control traffic lights to improve traffic flow and reduce accidents. Another traffic management tool involves the use of satellite tracking. Promoted by a Toronto company, Skymeter Corp, but not yet deployed in Canada, the system, a type of toll road in the sky, is designed to reduce traffic jams through the use of automobile based transponder/GPS systems and satellites. The satellite tracks the route of a particular vehicle at a particular time and then charges the user based on a systems of \"tolls\" based on the roads taken and the time of day. The tolls are publicized for users and designed to encourage road use in a way that minimizes traffic jams. The computerized billing system keeps an account of the charges and bills the customer incurs on a regular basis.\n\nIn 2010, the City of Montreal began to deploy 800 wireless, networked, solar-powered, Linux based, electronic parking payment stations to replace up to 10,000 existing mechanical parking meters. Each parking space has a code and the motorist can pay the required parking rate, with cash or credit card, from any station in the city (he/she must of course remember his code). The system, developed by 8D Technologies of Quebec also allows metre maids to check for parking violations by wirelessly interrogating a parking station with a hand-held device from his/her vehicle. The devise provides a digital map of all parking spaces near the station and marks those spaces with vehicles in violation with a red symbol. Other cities across Canada are installing similar machines.\n\nA light-rail urban passenger train known as the O-Train, began operation in Ottawa in 2001 providing limited service in a north-south corridor, today's Trillium Line. There are plans to expand the system to serve the downtown core as well as the western and eastern suburbs of the city by 2016.\n\nGlobal positioning technology has become an important feature of business and consumer life. After 23 years of military development, the U.S. military global positioning system became operational in 1995. Originally designed for the precise targeting of weapons and other military purposes, the U.S. government made the system available to civilians in 1996. Industrial users such as transportation companies and resource companies began to make use of the technology for the tracking of vehicles and the location of field operations. Receivers for the consumer market, were also produced and made available in Canada and became popular with outdoorsmen and women. In 2004 a GPS feature became available on some mobile phones and stand alone units for car navigation were available to Canadians by 2008.\n\nThe 11 September 2001 terrorist attack on the U.S. has resulted in increased security along the Canada-U.S. border. In 2004, Canada and the U.S. signed the Canada-U.S. Agreement on Science and Technology Cooperation for Critical Infrastructure Protection and Border Security designed to speed the introduction of a number of electronic, wireless, computer and detection technologies to scrutinize cross-border traffic while at the same time limiting the disruption to the flow of people and goods. The use of these technologies is particularly important at the Windsor Detroit border crossing which is the busiest in the world.\n\nIn 2008, the Government of Canada announced the initiation of two important transportation projects. In the first instance the government stated that it will acquire, for the Canadian Coast Guard, a new $700 million, CCG Polar Class icebreaker for patrolling the Northwest Passage. The ship will enter service in 2017. The government also announced the construction of a second international bridge between Windsor, Ontario and Detroit, Michigan, to help relieve the pressure on the heavily overloaded, 80-year-old Ambassador Bridge. The $5 billion project will include connections from the Canadian ends of both bridges to the nearby Highway 401 (Ontario). As of December 2010 construction had yet to start.\n\nThe field of transportation also saw the Premiers of Ontario and Quebec in 2007 talking of yet another study of a high speed train in the Quebec City – Windsor Corridor.\n\nBetween 2006 and 2009, Air Canada \"made over\" the cabins of all its aircraft providing each passenger seat with a number of new technologies including, a Personal AVOD (with a 230 mm touch-screen LCD) offering 200 hours of video and audio entertainment, interactive games, \na three-prong 120 V AC plug for laptops, a USB port and XM Radio Canada. The largest airplane in the world the Airbus A380, in this case operated by Emirates Airline, began regular service between Toronto's Lester B. Pearson Airport and Dubai in 2009. By 2009, most major airports in Canada were equipped with stand alone self-service customer check-in kiosks, which provided the passenger with a boarding pass for his/her flight. This represented the further extension of the technique known as the e-ticket which became the standard for purchasing an aircraft ticket several years earlier. In January 2010, the Government of Canada announced the use of full body scanner for the security checking of passengers boarding planes in Canada bound for the US. The scanners will be installed at the airports in Montreal, Toronto, Vancouver, Calgary, Edmonton and Halifax. The use of biometrics will become an important technique in the screening of those wishing to enter Canada. It is planned that between 2011 and 2013 the Department of Immigration and Citizenship will begin to deploy digital face and fingerprint scanning systems at overseas Canadian Visa offices for the issuance of visas to those who intending to visit Canada.\n\nAir navigation coverage has recently been improved through the deployment of Automated Dependent Surveillance-Broadcast (ADS-B) technology in parts of northern Canada. First introduced in the Hudson Bay area in January 2009, the service will eventually be expanded to cover all of northern Canada. The technique involves the use of ground-based transmitter/receivers and special electronic equipment aboard aircraft flying through northern airspace. This special equipment automatically transmits information relating to the aircraft position (determined by a GPS on the airplane) every second, to the ground-based receiving station, a number of which are located in the north. The station then transmits this information to an area control centre, operated by Nav Canada, Canada's national air navigation system operator, where the it is displayed on \"radar\" screens which are used by air traffic controllers to monitor Canadian airspace.\n\nThe importance of the shipping container has been emphasized by recent developments in Winnipeg. \"CentrePort Canada, an 8,000-hectare inland port being developed on the city’s edge is a one-stop shop for air, truck and rail shipments and is designed to reroute North American trade through the middle of the country...CentrePort (recently) announced an agreement with two Chinese partners, including the country’s largest private shipping company, Minsheng International Freight Co...(that)...will create a new container-based rail system that will quickly move crops from the Canadian prairies into the Chinese market.\" Rail services will be provided by Canadian Pacific Railway and Canadian National Railway. A new highway, the CentrePort Canada Way, is under construction to divert the heavy truck traffic associated with the new facility away from urban roads.\n\nIn this century, the largest engineering undertaking by far is the tar sands project in northern Alberta. This has seen the investment of up to $60 billion to develop and build gigantic tar sand mining, transportation, separation and refining facilities to produce oil from the gritty bitumen tar. The project is highly controversial for a number of reasons not the least of which is environmental. As of 2005, operations included the Suncor Mine, Syncrude Mine, Shell Canada Mine and others producing 760,000 barrels of oil a day. A large number of corporations from a number of countries plan to invest in the tar sands, including Suncor Energy, Syncrude, Shell/Chevron/Marathon, and Petro-Canad. Recovery techniques include steam-assisted gravity drainage (SAGD) and cyclic steam stimulation (CSS). More recently Cenovus Energy of Calgary has developed the \"Solvent Aided Process\" SAP for heavy oil recovery. This involved injecting butane or other organic solvent, along with steam into a horizontal chamber dug in the oil sands. The solvent and steam allow the oil to flow into another chamber below the first. It is then pumped to the surface from this chamber.\n\nOil sands recovery techniques create huge amounts of contaminated waste water, which is stored in \"tailing ponds\". In 2010, there were about 170 square kilometres of these ponds in the oil sands region of Alberta. Left to a normal process of degradation it would take decades for this waste to become environmentally safe. On 27 August 2010, Shell Canada announced the opening of a commercial plant designed to speed the cleaning of the waste in these ponds, at its oil sands production facility in Alberta. The plant uses a technique developed by Shell Canada at a cost of C$30 million. Known as \"atmospheric fines drying\" or AFD, it takes the thick liquid output of the oil production process and over a period of several weeks with the use of a special flocculant and drying techniques, reduces it to a safe dirt-like compound. Shell Canada is making this technology available free of charge to other oil sands production companies.\n\nGeological formations of shale gas are being explored as a new source of energy. A technique known as horizontal drilling is used to create a horizontal bore hole, through a formation. Water under high pressure is then pumped into the bore hole where it fractures the shale and allows the gas to escape the rock and seep up the bore hole. There are a number of shale gas fields in Canada including the Shallow Colorado basin in Alberta, Saskatchewan and Manitoba, the Bakken, in Saskatchewan and Manitoba, the Antrim in Southern Ontario and the Utica in south east Quebec. The technique is not without problems for the fracturing can affect aquifers causing contamination and deviation. To date efforts at production in Canada have been limited to exploratory wells.\n\nCanaport, the first liquified natural gas (LNG) port terminal facility of its kind in Canada, began operation in Saint John, New Brunswick in 2009. LNG is seen as a substitute for conventional gas.\n\nIn 2008, the Government of Ontario announced plans for the construction of two new reactors at the existing Darlington nuclear power facility, but suspended the project in 2009. Competing designs included the ACR-1000 by Atomic Energy of Canada Ltd., the EPR by the French company Areva Group and the AP1000 by the US based Westinghouse Electric Co. Llc.. The government of Saskatchewan is considering the construction of two nuclear reactors in Lloydminster and the government of New Brunswick is proposing the addition of another reactor at its Point Lepreau nuclear power facility.\n\nIn 2006, the Government of Ontario instructed the provincial hydro utility to provide all of its customers with digital smart hydro meters by 2010 as a first step towards the creation of a Smart Grid, which would conserve electricity. The project had been largely completed as of that date. In BC, BC Hydro announced in 2010 the replacement of existing hydro metres with digital smart meters for its 1.8 million residential and commercial customers, by 2012.\n\nIn Vancouver, the Vancouver Fuel Cell Vehicle Program, a pilot project, was introduced in 2005 to study the use of hydrogen as a power source for cars. The three-year undertaking, a first in Canada for fuel cell powered automobiles, studies the operation of a fleet of five Ford Focus FCV’s (fuel cell vehicles), in \"real world\" conditions, in Vancouver and Victoria. The project is the initiative of a consortium made up of the Governments of Canada and British Columbia, Fuel Cells Canada, and Ford Motor Company.\n\nConcerns with energy efficiency have also led to the introduction of the compact fluorescent lamp for domestic, commercial and industrial use and the federal government stated in 2007 that the sale of incandescent light bulbs would be phased out by 2012. The technology of the LED lamp has been known for a century. In recent years, it has become a popular replacement for incandescent bulbs because of its low power consumption. RenewABILITY Energy of Waterloo has developed a technique for recovering heat from domestic waste water. Known as the Power-Pipe, it channels hot waste water through cold water waiting to be used and heats it.\n\nThe use of clean-burning biofuels such as ethanol has become significant in recent years. At the present time, Canada's largest manufacturers of ethanol include GreenField Ethanol and Husky Energy, which produce 500 million litres and 260 million litres of ethanol a year respectively from corn and wheat. Other companies are also at work in the field, including Enerkem of Montreal, which makes ethanol from old telephone poles at a facility in Westbury, Quebec and Iogen of Ottawa, which makes cellulosic ethanol from wheat straw. Since 2007, the Government of Ontario has required that all gasoline sold in the province contains at least 5% ethanol.\nA federal regulatory change in 2009 will require all oil refiners in Canada to provide an ethanol content of at least 5% in their gasoline by September 2010. The Fischer-Tropsch process is the basis for a proposal by AP Fuels of Montreal to establish five biorefineries in Canada. The plan calls for the use of this technique to transform certain types of trees, notably popular and birch, into gas and then to liquid-biodiesel, which burns with reduced CO output.\n\nThe technology of \"clean\" coal has also become important. Western Canada has abundant coal supplies but the use of coal in recent years has been criticized for environmental reasons. To counter this criticism, coal and coal-fired electricity producers have formed the Canadian Clean Power Coalition. This organization promotes a number of projects which use a variety of \"clean\" coal technologies. These include the EPCOR Integrated Gasification Combined Cycle (IGCC) plant for the Genesse Power Station in Alberta. The IGCC plant gasifies coal and uses the clean gas to drive a gas turbine. The process also produces steam, which is used to turn a steam turbine. Both turbines are used to produce electricity. The process also captures CO from the gas combustion, which is in turn used for enhanced oil recovery or is sequestered underground.\n\nEnergy concerns have inspired the development of wind farms that use modern windmills to generate electricity from this renewable resource. One of the first modern windmills was built at Cap Chat in Quebec in the eighties, but most wind farms have been built since 2000. As of 2008, 10 megawatt wind farms in Canada were distributed as follows: Alberta 10, Quebec 5, Ontario 5, PEI 4, Saskatchewan 3, Manitoba 2 and Nova Scotia 2. In 2008 Hydro-Québec announced the construction of 1000 windmills at 15 new sites located mostly in the St. Lawrence River Valley. By 2015, that utility expects that 10% of the province's electricity will be provided by wind power. In 2008, in British Columbia, BC Hydro has issued a Clean Power Call for proposals for environmentally friendly energy production and one company, Naikun Wind Energy, has responded with Canada’s first plan to develop off-shore wind power by installing windmills at sea in the Hecate Strait off the north coast of B.C.\n\nIn 2010, the Government of Ontario signed an agreement with Samsung and the Korea Electric Power Corporation to build and operate wind and solar electrical generating farms across southern Ontario. The C$5 to C$7 billion project is described as the largest of its type in the world and will begin with installations in Chatham-Kent and Essex-Haldimand counties in southwestern Ontario. It is foreseen that the wind turbines will generate up to 2,000 MW and the solar power facilities up to 500 MW. This will permit the closure of all of the coal-fired electric generating plants in Ontario by 2014. \nA private company, OptiSolar Farms Canada Inc., is using silicon solar panels to develop what will become the largest solar power farm in North America. The facility, under construction in a field near Sarnia, will begin to produce 60 megawatts of electricity for Ontario consumers by the end of 2008.\n\nThe use of geothermal energy has grown in Canada in recent years although its overall importance as an energy source is still very small. The use of geothermal energy in Canada falls into two broad categories: commercial use to produce electricity and consumer use for home heating. In Canada, the former is limited to a facility in Meager Mountain British Columbia, a site with a potential for 100–250 MW, which has recently (2010) begun to produce for the BC Hydro grid. In the case of consumer use, a hole similar to that used for a domestic water well is drilled in the ground near the residence in question. Water is pumped to the surface and passed through a heat exchanger where some of its heat is removed and transferred to a closed loop water system in the house. The cooler water is then returned to the ground. The water in the closed loop is circulated throughout the structure where it passes through radiators and heats the house.\n\nThe undesirable environmental effects of industrial processes and atmospheric pollution in particular, have become a topic of increasing public concern in the new millennium. Among the most notable polluters in Canada in 2006 were electric power generators: ATCO, Emera (Nova Scotia Power), Ontario Power Generation, SaskPower and TransAlta, mining companies: HudBay Minerals, Teck Cominco, Vale Inco and Xstrata, oil and gas companies: Imperial Oil, Shell Canada, and Trans Canada, oil sands companies: Syncrude and Suncor and the manufacturing enterprise, SMC Canada.\n\nEfforts to reduce the release of CO gas into the atmosphere lead to the initiation of the Weyburn-Midale CO Project in Saskatchewan in 2000. Presently the world’s largest CO sequestration effort, this $80 million undertaking involves the injection of waste CO gas from industrial processes into the ground for storage instead its release into the atmosphere. There are presently two underground sequestration facilities, one at Weyburn operated by Encana and the other at Midale operated by Apache Canada.\n\nIn recent years bio-waste has been used for the production of heat and electricity. Sanitary landfill sites are notable in this regard. Often, systems for the collection of methane gas are progressively installed as the sites are filled. This gas is then used at on site cogeneration facilities for the production of heat and electricity. A number of landfill sites including those in Kanata, Petrolia, Watford and Napanee, Ontario and Sainte-Sophie, Drummondville and Magog in Quebec have been selected for the location of cogeneration facilities.\nIn Ottawa the cogeneration facility at the Pickard (Sewage Treatment) Centre which has been in operation since 1998, provides all the heat and electrical energy needed to operate the centre.\n\nOstara Nutrient Recovery Technologies Inc. of Vancouver has developed techniques to recover phosphorus and other nutrients from waste water. Since 2007, these have been put to use at the Gold Bar Treatment Plant in Edmonton, the world's first industrial scale waste-water nutrient treatment facility. The recovered products are recycled and sold as environmentally safe commercial fertilizer. Other Ostara nutrient recovery projects are underway at Lulu Island (Vancouver), Penticton, B.C. and in the US.\n\nEfforts to save fuel have also led to efforts to reduce the weight of vehicles through the increased use of composite material. Aircraft manufacturers have been especially notable in this regard and produced new large but relatively light aircraft such as the Boeing B-787 Dreamliner with this new material. Orders for this new machine have been made by a number of major world airlines, including Air Canada. In 2008, Bombardier of Montreal announced the production of the new C Series of 100- to 130-seat passenger jets which will also make extensive use of composites. They will also be used extensively in the 7000 and 8000 series of long range business jets announced by that company in 2010.\n\n3D printing has become an important industrial process in Canada. The technique uses a computer to drive the 3D printing device. This machine builds 3D shaped objects through successive passes of a \"printing head\" which lays down layers of plastic or other material to progressively build a 3D physical object. As of 2010, about 100, 3D printers were in use with manufacturing enterprises in Canada.\n\nThe techniques of diamond mining have been introduced to Canada in recent years. Over 600 kimberlite formations have been found throughout Canada. Open pit mining techniques have been used to produce diamonds from two of these, Ekati, beginning in 1998 and Diavik in 2003.\n\nNanotechnology involves the manipulation of atoms and molecules to produce processes and products for human use. At present, the field is the subject of much research, but the use of these processes and products in Canada is not yet widespread. However the technology remains important because of its potential for great future influence. Some nano-products have made their way to the market in items such as cosmetics, and certain industrial products available in Canada.\n\nMost of the activity in Canada is found in research. In 2001, the Canadian government established the National Institute for Nanotechnology in Edmonton. The Institute conducts nano-research in a number of fields including the life sciences, supramolecular assembly, molecular scale devices and nano-sensors. As of 2010, a number of Canadian universities offer engineering degrees in nanotechnology. Of particular note is the Waterloo Institute for Nanotechnology which will be in operation in 2011 and will conduct research related to nano-engineered materials, nano-electronics design and fabrication, nano-instrumentation and nano-biosystems.\n\nThe use of nanomaterials is not without controversy. As of February 2009, the Government of Canada requires all industries to report the use of nanomaterials in their products. In 2010 the government banned the use of manufactured nano-materials and nanotechnology in organic food production.\n\nThe construction of skyscrapers has continued apace in recent years with Toronto and Calgary accounting for most of the new structures. These include: Bankers Hall West, Calgary, 2000, the TransCanada Tower, Calgary, 2001, One Wall Centre, Vancouver, 2001, One King Street West, Toronto, 2005, West 1, Toronto, 2005, Harbourview Estates 2, Toronto, 2005, Residences of College Park 1, Toronto, 2006, Living Shangri-La, Vancouver, 2008, the Hilton Fallsview Hotel Tower, Niagara Falls, 2008, Quantum 2 (Minto Midtown), Toronto, 2008, the Bay Adelaide Centre West, Toronto, 2009, the RBC Centre, Toronto, 2009, Success, Toronto, 2009, Montage, Toronto, 2009, the Ritz-Carlton, Toronto, 2010, Centennial Place, Calgary, 2010, Maple Leaf Square North and South, Toronto, 2010, Jamieson Place, Calgary, 2010, Festival Tower, Toronto, 2010, The Bow (skyscraper), Calgary, 2011, Trump International Hotel and Tower, Toronto, 2011, The Uptown Residences, Toronto, 2011, Eighth Avenue Place (Calgary), 2011, the Four Seasons, Toronto, 2011, The Private Residences, Vancouver, 2011, the Burano, Toronto, 2011, Absolute World North and South, Mississauga, 2011, the Marriott Courtyard Hotel, Montreal, 2012, the Shangri-La Toronto, 2012, and the L Tower, Toronto, 2012.\n\nNew hydro-electric projects have been completed as well including the 230-MW Rocher-de-Grand-Mère station, on Quebec's Saint-Maurice River (2004).\n\nNew bridges and roads of note include the Golden Ears Bridge, Vancouver, 2009, the Middle Arm Bridge, Vancouver, 2009 the North Arm Bridge, Vancouver, 2009 and the Sea to Sky Highway, Vancouver/Whistler, 2009. In 2009, in northern Quebec, Hydro-Québec initiated construction of the C$6 billion Romaine River Complex, a series of four rock filled hydro generating dams that will be completed between 2014 and 2020.\n\nA different type of public facility was introduced to the citizens of Toronto in 2010, when the city approved a contract for the installation of 20 self-cleaning public toilets. The first of their kind in Canada, each of the devices, which are placed throughout the city, resembles a bus shelter. The user pays 25 cents for twenty minutes of occupancy. The facility cleans itself automatically after each use.\n\nIn 2001, the Federal government created Canada Health Infoway, in independent, not-for-profit, federally funded organization composed of the 14 Canadian federal, provincial and territorial Deputy Ministers of health. Infoway has a mandate to accelerate the Canada-wide use of electronic health records and electronic health information systems. As of 2008, more than $1.3 billion has been invested in the system. By 2010, Infoway plans to have electronic health records for 50% of the population available to authorized health professionals, and expects to have electronic health records for all Canadians by 2016. The project involves undertakings in a number of fields, including diagnostic imaging systems, drug information systems, telehealth, laboratory information systems and public health surveillance.\n\nTelus, one of Canada's largest telephone companies, announced an agreement with Microsoft of Canada for the use of the latters' HealthVault (2007) consumer health records software in 2009. Telus intends to use the software to allow its 11 million Canadian subscribers to access information relating to their health care.\n\nMedical technology in Ontario was improved in 2009 with the implementation of the government operated ePrescribing system a service that allows doctors to send prescriptions for patient pharmaceuticals directly to the pharmacist through a private computer network. This technique eliminates the problem with illegible handwriting, thus improving patient safety. The system has been initially introduced in Sault Ste. Marie and Collingwood with plans for making it available province-wide by 2012. eHealth Ontario, announced in 2010, the signing of a C$46 million contract for the establishment of a diabetes registry, for the management of patients with this disease. The registry will eventually be expanded for the management of patients with other chronic diseases.\n\nSince 2008, Real Time Radiology (based in Mississauga, Ontario) has provided interpretation of medical images to remote sites on a Canada-wide basis. Through use of the Internet and a highly automated computer process, a team of 50 radiologists working for the company across Canada interprets medical images sent from distant locations where the services of a radiologist are not available. The results are returned electronically to the remote locations and form the basis for patient treatment there. The Gattuso Rapid Diagnostic Centre at the Princess Margaret Hospital in Toronto, through the acquisition of new diagnostic equipment that can prepare tissue samples for pathological analysis within hours, began offering same day breast cancer diagnosis for patients in 2009. \n\nTechniques for the mass production of drugs were improved in the early part of the new century. In Ste. Foy, Quebec, the international drug maker GlaxoSmithKline established a manufacturing complex for the mass production of vaccines. As of 2009, the facility is capable of producing 14,000,000 doses per month. The facility may be used for the production of a vaccine for the H1N1 flu virus for the entire population of Canada (around 35,000,000 people as of 2014), should that become necessary. Also in 2009, public preparations for a possible pandemic included the placement of containers of liquid hand sanitizer for use in public places.\n\nThe PharmaTrust prescription medication dispending machine was introduced to the Canadian public in 2008. The apparatus, which physically resembles and functions like an ATM or soft drink dispenser, allows a user to purchase and receive medically approved prescription drugs, without visiting a pharmacy. Developed by PCA Services Inc. of Oakville, Ontario, one of the first has been installed in the Sunnybrook Health Sciences Centre, in Toronto.\n\nLasers made their way into routine dentistry by the middle of the first decade, offering faster treatments, less pain and more precise results. They are used to remove tartar, treat soft tissues such as gums and to prepare cavities for filling. Of particular interest in the latter instance is the fact that this treatment is so painless that the use of a needle to inject a local anesthetic is usually unnecessary. Laser treatment results in little bleeding, a lower risk of infection and a quicker healing. Another innovation was the use of computer milled ceramic implants for repairing cavities. The use of a non-toxic chemical such as hydrogen peroxide or carbamide peroxide for tooth bleaching has become popular in the new century.\n\nIn 2002, two Vancouver doctors, dermatologist Alastair Carruthers and ophthalmologist Jean Carruthers, pioneered the cosmetic use of the well known botulinum toxin. The pair noticed that subcutaneous injections of small amounts of the toxin had the effect of removing age wrinkles from the skin. The Botox procedure, as it became known, quickly gained popularity around the world.\n\nCommercial DNA profiling has become available in Canada in recent years. For a fee, it is possible to order a number of specific tests including those for paternity, maternity, siblingship and ancestry. Companies offering this service include Genetrack Biolabs established in Vancouver, B.C. in 2003 and DNA Canada of Kingston, Ontario, established in 2005.\n\nEstablished in 2002 in Burnaby, British Columbia, Lifebank Cryogenics Corporation provides, on a commercial basis, a client-based service for the processing and cryogenic storage of stem cells from the umbilical cords of new-born babies. The cells may be of help in the treatment of disease that might affect the donor.\n\nDomestic construction has witnessed the introduction of improved building techniques and the smart home (home automation). Both the hydraulic lift and the concrete pump/crane, are now commonly used for home construction. Furthermore, homes are built with the electronics necessary for Internet connection throughout the premises. Household systems, such as heating and cooling, lighting, communications, entertainment and even food storage and cooking are now all linked to each other through the web. In the kitchen the glass-topped stove has become popular. The living room has seen the introduction of the very large flat screen, digital plasma TV, LCD TV and LED TV technologies, which have undergone dramatic price reduction in the last few years and have replaced the cathode-ray TV in consumer appliance/electronic stores. Also popular with consumers is the iPod portable music player introduced to Canadians in 2001 and the iPhone which was made available to Canadians by Rogers Wireless in 2008. The digital camera which was introduced to Canadians in the eighties has for the most part replaced the film camera in recent years. The electronic book or E-book has gained a place in Canada beginning with the introduction of the Sony Librie reader in 2004 and the Kindle in 2009. In 2010 the iPad wireless web surfing device became available to Canadians. Other such devices have been introduced in Canada including the BlackBerry PlayBook (available in 2011). The Blu-ray Disc and associated player have been marketed in Canada since 2009. The \"Guitar Hero\" music video game released in 2007 has enjoyed great success in Canada as has the Wii video game released that same year.\n\nAlthough 3D video games based on anaglyph image technology have been available in Canada since their introduction to the market in 1987, their popularity increased in 2009 partly as a result of marketing efforts by the maker of the 3D film \"Avatar\". Popular formats include Windows (3D), PS3 (3D), PSP, Wii, Xbox 360 (3D), DS and iPhone. Users must wear special glasses with a different coloured lens over each eye in order to experience the 3D effect.\n\nIn 2008, the large Canadian banks, including the Bank of Nova Scotia, the Royal Bank of Canada, the Toronto-Dominion Bank and the Canadian Imperial Bank of Commerce, began issuing Visa credit cards with an embedded microchip for enhanced security. Also in 2008, MasterCard Canada introduced the PayPass electronic payment system to Canada. The system uses a card/tag/phone equipped with an embedded computer chip and radio frequency antennae which is tapped on a PayPass reader at participating grocery stores, convenience stores, fast food restaurants or gas stations. The card/tag/phone, wirelessly transmits information about the customer to the reader which in turn electronically charges the appropriate sum to the customer's account. A similar concept using cell phones equipped with Near Field Communications (NFC) was introduced in 2009. Known as payWave, the technique is the result of cooperation between Visa, the Royal Bank of Canada and Rogers Communications. It is intended for fast, mobile, low-cost \"micro-payment\" transactions of items such as fast food, coffee, and subway tokens.\n\nBeginning in 2006, omega-3 oil became an additive in a number of foods sold in Canada.\n\nThe personal blood level alcohol tester or breathalyser was introduced to Canadians in 2010. The device, known as the BAQ Tracker, works the same way as those used by police. The user blows into a tube on the small portable hand-held machine and a digital readout of his or her blood alcohol level instantly appears on a display. Developed by Ladybug Technologies of Cambridge, Ontario, it sells for about $300.\n\nIn the 21st century, Canada's government has shown renewed interest in the acquisition of military technology, especially with its commitment to the war in Afghanistan. Equipment has been improved, including the CF-18 fighter with addition of laser-guided bombs and there are plans to update the Aurora patrol aircraft. The air force has also taken possession of the gigantic new C-17 Globemaster III long-range transport aircraft and has begun to renew the fleet of Hercules transport aircraft. The army has acquired the new Leopard 2 tank and C-777 long-range gun, and in 2009 announced the acquisition of the Close Combat Vehicle. In 2003, the Forces took possession of their first tactical unmanned aerial vehicle (TUAV), the French-designed CU-161 Sperwer, and the Heron UAV in February 2009. Used for the war in Afghanistan, these machines provide an intelligence, surveillance and reconnaissance (ISR) capability for the Forces. In 2008, the Air Force announced that it would acquire its first attack helicopters (Griffons equipped with light machine guns) for service there as well. In 2006, the Navy undertook the Halifax Class Modernization/Frigate Equipment Life Extension Project (HCM/FELIX) to modernize its 12 Halifax Class Frigates. New equipment will include improved computer fire control systems, sensors and the decoy-based Rheinmettal Multi Ammunition Softkill System, a passive missile defence system. Acquisitions pending include the CH-148 Cyclone ASW helicopter, the Chinook helicopter, new Arctic patrol vessels for the navy and a new ice breaker for the Canadian Coast Guard. In July 2010, the Government of Canada announced the C$9 billion purchase of 65 F-35A fighters for delivery beginning in 2016.\n\nIn 2009, the Canadian government announced a C$880 million upgrade, including new facilities, of the signal intelligence capability of the Communications Security Establishment Canada in Ottawa, to be completed by 2015. The importance of electronic warfare on the battlefield, as demonstrated in the War in Afghanistan, was highlighted in April 2010 by the formation of 21 Electronic Warfare Regiment at CFB Kingston. The unit, the first new regiment to be formed in the Canadian Army since WWII, is being equipped with the most modern electronic warfare technology and will practice both defensive and offensive electronic warfare.\n\nThe Polar Epsilon project, approved in 2005 and slated to be fully operational by 2011, uses Radarsat 2 to provide military commanders with imagery of Canada's Arctic. Another surveillance project, Polar Breeze (until recently classified secret), will use shore-based sea surface search radar, satellite-based (Radarsat 2) imagery and underwater listening devices to monitor sea surface and underwater traffic in the choke points of the Northwest Passage. The Canadian Forces have also acquired updated electronic equipment to conduct more advanced electronic warfare to face the new cybernetic threat and conduct cybernetic warfare (cyber-warfare).\n\nThe Taser (also known as the conducted energy weapon) has been adopted for use by Canadian police forces, including the RCMP, in recent years. The technology presently deployed was developed in the US in 1999. Intended for use as a \"non-lethal\" weapon, the Taser fires darts trailing wires connected to a battery in the hand-held pistol. The darts strike and lodge themselves in the suspect. The battery delivers, through the wires, a jolt of electricity that incapacitates the suspect. Its use in Canada has led to considerable controversy following the deaths of four individuals who were tasered by police in separate incidents in 2007.\n\nIn the earlier parts of Canada's history, the state often played a crucial role in the diffusion of these technologies, in some cases through a monopoly enterprise, in others with a private \"partner\". In more recent times, the need for the role of the state has diminished in the presence of a larger private sector.\n\nIn the latter part of the 20th century, there is evidence that Canadian values prefer public expenditures on social programmes at the expense of public spending on the maintenance and expansion of public technical infrastructure. This can be seen in the fact that in 2008 the Federation of Canadian Municipalities estimated that it would take $123 billion to restore and repair aging urban infrastructure across Canada.\n\n"}
{"id": "1086397", "url": "https://en.wikipedia.org/wiki?curid=1086397", "title": "Texas Department of Public Safety", "text": "Texas Department of Public Safety\n\nThe Texas Department of Public Safety (DPS) is a department of the government of the state of Texas. DPS is responsible for statewide law enforcement and vehicle regulation. The Public Safety Commission oversees DPS. However, under state law, the Governor of Texas may assume personal command of the department during a public disaster, riot, insurrection, or formation of a dangerous resistance to enforcement of law, or to perform his constitutional duty to enforce law. The commission's five members are appointed by the governor and confirmed by the Texas Senate, to serve without pay for staggered, six-year terms. The commission formulates plans and policies for enforcing criminal, traffic and safety laws, for preventing and detecting crime, for apprehending law violators and for educating citizens about laws and public safety. The DPS director and assistant director report to the commission. The director's staff includes the Director, Steven McCraw, who holds the rank of colonel, and Deputy Director David Baker, who holds the rank of lieutenant colonel.\n\nThe agency is headquartered at 5805 North Lamar Boulevard in Austin.\n\nOn August 10, 1935 the formation of the Department of Public Safety along with 103 other bills were created by the Texas Legislature. The newly formed department was the new home for the Texas Rangers, The Highway Patrol, and crime laboratory. \n\nDPS is divided into thirteen divisions:\n\n\nThe Administrative Services Division serves as the indirect staff to the director and provides information technology, law enforcement support, finance, administration, and regulatory licensing for the entire department.\n\nThe Administration Section maintains DPS property, provides training to other divisions, and operates the Crime Records Service. The Crime Records Service maintains criminal justice information and issues concealed handgun licenses.\n\nIn 2009, the Department of Public Safety created the Criminal Investigations Division (CID) as part of a major restructuring of the department. The CID consists of 700 members, including 573 commissioned officers and 129 civilian support personnel. The CID Assistant Director's Office consists of the assistant director, deputy assistant director, an administrative major, and four civilian support personnel.\n\nThe CID is divided into four different sections, which are specialized by function:\n\n\nThe CID sections work together to prevent, suppress, and solve crime in cooperation with city, county, state, and federal law enforcement agencies. Multi-jurisdictional violations typically investigated by CID include terrorism, gang-related organized crime, illegal drug trafficking, motor vehicle theft, gambling, public corruption, fraud, theft, and counterfeit documents.\n\nThe Driver License Division is responsible for the issuing and revocation of Texas driver licenses and identification cards.\n\nThe Emergency Management Division is responsible for coordinating statewide emergency planning and response. Typical emergencies are weather-related (hurricanes, floods, tornadoes). The DEM is also responsible for administering Texas' AMBER Alert network.\n\nThe Texas Highway Patrol Division is the unit of the department most frequently seen by citizens. Uniformed troopers of the highway patrol are responsible for enforcing traffic and criminal law, usually in unincorporated areas, and serve as the uniformed Texas state police.\n\nTroopers in the Highway Patrol Division also serve a capitol security role, as well as operating the DPS Bike Patrol, Motor Patrol, and Mounted Horse Patrol, all of which serve the Texas Capitol Complex in Austin.\n\nThe Intelligence and Counterterrorism Division (ICT) plays a leading role in the department's goal of combating terrorism and organized crime.\n\nICT manages and operates the Texas Joint Crime Information Center (TXJCIC), formerly called the Texas Fusion Center, which serves as the centerpiece in establishing and maintaining a statewide information sharing network. Through the development, acquisition, analysis and dissemination of criminal intelligence information, the Texas Joint Crime Information Center supports criminal investigations across the state on a 24/7 basis. Texas Joint Crime Information Center personnel include non-commissioned analytical experts and a small number of commissioned officers. Also participating in the Texas Joint Crime Information Center are personnel from various other law enforcement and public safety agencies, such as Texas Department of Criminal Justice, Texas Parks & Wildlife Department, Department of Homeland Security, Department of the Treasury, Federal Bureau of Investigation, Drug Enforcement Administration, Immigration and Customs Enforcement, and Air and Army National Guard. ICT analysts also work at other regional fusion and intelligence centers located throughout Texas.\n\nICT also oversees security at DPS headquarters and the Texas Capitol Complex, a 46 square block area in downtown Austin. The Capitol Complex includes the State Capitol, state office buildings, parking lots and garages, and private office buildings. Security at the Capitol Complex is the responsibility of ICT's Capitol District, which is charged with protecting state property and buildings, and providing a safe environment for state officials, employees, and the general public. The Capitol District provides total police service within the Capitol Complex, including traffic enforcement, parking enforcement, and criminal investigations.\n\nArguably the most well-known division of the DPS is the Texas Rangers. Rangers are responsible for state-level criminal investigation, among other duties. Texas Rangers consists of over 140 rangers.\nA horse back patrol mainly in texas capital grounds.\n\n\nThe governing body of the Department of Public Safety is a five-member Public Safety Commission, with all members being appointed by the Governor of Texas. The Commission is responsible for appointing the director of the department. The director is assisted in managing the Department by two deputy directors and several division directors. Most divisions report to the director through one of the two deputy directors, however, the Texas Rangers Division, the Emergency Management Division and the Legal Affairs Division all report directly to the director.\n\nThe commission also appoints an inspector general to act as an inspector for the department, and a chief audit executive as part of the internal audit department known as the Chief Auditor's Office, who are both independent of the director.\n\n\n\n"}
{"id": "856797", "url": "https://en.wikipedia.org/wiki?curid=856797", "title": "Trunk (luggage)", "text": "Trunk (luggage)\n\nA trunk, also known as a travel trunk, is a large cuboid container designed to hold clothes and other personal belongings. They are most commonly used for extended periods away from home, such as for boarding school, or long trips abroad. Trunks are differentiated from chests by their more rugged construction due to their intended use as luggage, instead of the latter's pure storage.\n\nAmong the many styles of trunks there are Jenny Lind, Saratoga, monitor, steamer or Cabin, barrel-staves, octagon or bevel-top, wardrobe, dome-top, barrel-top, wall trunks, and even full dresser trunks. These differing styles often only lasted for a decade or two as well, and—along with the hardware—can be extremely helpful in dating an unmarked trunk.\n\nAlthough trunks have been around for thousands of years in China and elsewhere, the most common styles seen and referred to today date from the late 18th century to the early 20th century, when they were supplanted in the market by the cheaper and lighter suitcase.\n\nTrunks were generally constructed with a base trunk box made of pine which was then covered with protective and decorative materials. Some of the earliest trunks are covered with studded hide or leather and look much like the furniture of the same period (which makes sense as trunk manufacturing was sometimes an offshoot of a furniture business.) Later coverings include paper, canvas, plain or embossed tin, with an uncounted assortment of hardware and hardwood slats to keep it all down.\n\nThere were hundreds of trunk manufacturers in the United States and a few of the larger and well known companies were Rhino Trunk & Case, C.A. Taylor, Haskell Brothers, Martin Maier, Romadka Bros., Goldsmith & Son, Crouch & Fitzgerald, M. M. Secor, Winship, Hartmann, Belber, Oshkosh, Seward, and Leatheroid. One of the largest American manufacturers of trunks at one point — Seward Trunk Co. of Petersburg, Virginia — still makes them for school and camp, and another company — Shwayder Trunk Company of Denver, Colorado — would eventually become Samsonite. Another is the English luxury goods manufacturer H.J. Cave trading since 1839. Their Osilite trunk was used by such famous customers as T.E. Lawrence and Ruth Vincent Some of the better known French trunk makers were Louis Vuitton, Goyard, Moynat, and Au Départ. Other malletiers still in existence include Noble and Graff and La Malle Bernard.\n\nThe easiest way for the casual observer to date any trunk is still by examining its style, so a short description of each aforementioned major variety follows.\n\nJenny Lind trunks have a distinctive hour glass or keyhole shape when viewed from the side. They were named after the Swedish singer of the same name who toured America in 1850 - 1852 along with PT Barnum. \n\nSaratoga trunks were the premium trunks of many makers (or the exclusive design of many premium trunk makers) and actually can encompass nearly every other style of trunk manufactured if loosely defined, although generally they are limited to before the 1880s. The most readily recognizable feature of Saratogas are their myriad (and generally very complex) compartments, trays, and heavy duty hardware.\n\nMonitor-tops (incorrectly known as water-fall trunks from the furniture) date from the late 1870s to the late 1910s, and are characterized by their rounded front and rear corners to form a lying-down \"D\" when viewed from the side. Earlier examples usually included labor-intensive hardwood slats that were curved with the top, while there was a revival much later with rarer, all-metal ones being constructed.\n\nSteamer trunks (named after their location of storage in the cabin of a steam ship, or \"steamer\") which are sometimes referred to as flat-tops, first appeared in the late 1870s, although the greater bulk of them date from the 1880–1920 period. They are distinguished by either their flat or slightly curved tops and were usually covered in canvas, leather or patterned paper and about tall to accommodate steamship luggage regulations. There has been much debate and discourse on what these types of trunks are actually called. In some old catalogs, these trunks were called \"packers\", and the \"steamer\" trunk actually referred to a trunk that is often called a cabin trunk. An orthodox name for this type of trunk would be a \"packer\" trunk, but since it has been widely called a steamer for so long, it is now a hallmark of this style.\n\nCabin trunks, which are sometimes called \"true\" steamer trunks, were the equivalent of today's carry-on luggage. They were low-profiled and small enough to fit under the berths of trains or in the cabin of a steamer, hence their name. Most were built with flat tops and had inner tray compartments to store the owner's valuables deemed too precious to keep stowed away in the baggage (luggage) car or ship's hold.\n\nHat trunks were square shaped trunks that were popular in the 1860s to the 1890s. Today, they are mostly called \"half-trunks\". They were smaller and easier to carry, and could hold up to six hats or bonnets. Most were flat tops, but some had domed lids (which were very elegant). This trunk style was popular with Victorian women, hence antique trunk labels often calling this type a \"ladies' trunk\". Hat trunks generally sell for more than any other average trunk style because they are smaller and are rather rare to find.\n\nBarrel-staves are sometimes referred to as a form of dome-top trunk, but generally date from a decade or more earlier and are notable for having horizontal slats instead of vertical, giving it a distinctive look and construction. These were generally made from the late 1870s to the mid-1880s.\n\nBevel-tops are separated into an early and a late (or revival) period, the former generally dating from the 1870–1880 period, and the latter from 1890 to 1900. They are characterized by a distinct trapezoidal shape when viewed from the side, although the earlier period tended to have a much shorter flattened top section than the later did. These tend to be extremely rare, although are not as popular or sought-after as many of the other varieties.\n\nWardrobe trunks generally must be stood on end to be opened and have drawers on one side and hangers for clothes on the other. Many of the better wardrobe lines also included buckles/tie-downs for shoes, removable suitcases/briefcases, privacy curtains, mirrors, make-up boxes, and just about anything else imaginable. These are normally very large and heavy as they were used for extended travel by ship or train.\n\nA dome-top trunk has a high, curved top that can rise up to heights of . A variety of construction methods—including curfing, molded ply, barrel construction, and so forth—were used to form the inner boxes. Included in this classification are camel-backs, which are distinguished by having a central, vertically running top slat that is higher than its fellows, hunch-backs or hump-backs which is the same but has no slat in the center of the top, and barrel-tops (not to be confused with barrel staves), which have high arching slats that are all the same height, a distinction that can be discerned by laying a ruler flat across the tops of the slats. These trunks date from 1870s-1900s, although there are a few shops still manufacturing them today. They are not only the most common trunks referred to as antique, but also are among the most popular.\n\nWall trunks are made with a special hinges so that when opened the trunk could still be put flat up against a wall. The two main manufacturers include Clinton and Miller, which can be easily noted by the name on the hinges. In good condition these are comparatively sought-after trunks for a specialty type, although are in the middling range when it comes to price.\n\nDresser trunks also known as pyramidal trunks, due to their shape, are a unique form of wall-trunk that generally date from the 1900–1910 era. They are characterized by a lid that opens up nearly the entire front half of the trunk, allowing it to rest on the bottom. Two prominent manufacturers of this trunk style were F. A. Stallman and Homer Young & Co.\n\nOak-slat trunks incorporating many construction-styles (e.g. dome-top, flat-top, beveled-top) were built on a wooden frame, where the malletier would fit thin oak slats vertically side-by-side until the entire trunk was covered. To a Victorian, this would show the complexity of the trunk and astuteness of the malletier, and was an indication of wealth to any purchaser. Oak-slat trunks were built by several companies, including the Excelsior Company, MM (Martin Maier) Company, Clinton Wall Trunk Manufactory, and El Paso Slat Trunk Company. Some oak-slat trunks were made with alternating colors on the vertical slats.\n\nFootlockers are trunk-like pieces of luggage used in military contexts. Generally these are designed for economy, ruggedness, and ease of transport rather than aesthetic qualities.\n\nDuring the steamer trunk restoration process when the inside paper covering is removed, dated notes in lead pencil made by the original craftsman may be found, as well as the circular saw blade impressions made on the rough-cut wood at the saw mill, both of which give added character and value to the restored trunk.\n\nThere were numerous tray and lid compartments in Victorian trunks, ranging from basic to complex. A basic tray system comprised a hat box, a shirt compartment, a coin box, and a document box. A complex tray system, however, could consist two hat boxes, several other shirt compartments, a coin box, several document boxes and even secret compartments strategically placed so that people of unwanted access would pass up if not wary. Beautiful lithographs would be placed over the lids or dome of the trunk and truly capture the Victorian aesthetic of that day. There were numerous chromolithographs that a trunk maker could use, and they could be indicative of who the trunk was intended for, such as ladies or men. A bride's chest usually had a lot of floral pictures or lithographs of other ladies, while men's had pictures of \"village\" or country scenes. It was up to the malletier what to put on the lids and trays.\n\n"}
{"id": "37638616", "url": "https://en.wikipedia.org/wiki?curid=37638616", "title": "Tujeon", "text": "Tujeon\n\nTujeon are the traditional playing cards of Korea.. Korean writing of Tujeon is , with the literal meaning of \"fighting tablets\". A deck typically contains forty, sixty or eighty cards: nine numeral cards, and one General (\"jang\"), to each suit. In a full eight-suited deck, the suits and their generals are as follows:\nThe physical cards are very long and narrow, typically measuring about tall and across. They are made of oiled paper, leather or silk. The backs are usually decorated with a stylized feather design.\n\nIn his 1895 book \"Korean Games, with notes on the corresponding games of China and Japan\", ethnographer Stewart Culin suggested that tujeon originated from the similarly-shaped symbolic bamboo \"arrows\" used for divination in sixth-century Korea. This hypothesis, however, is supported mainly by visual similarity, and remains unsubstantiated.\n\nWriting from the early 19th century, (1788-1856) claimed that (b. 1613) brought the Chinese card game of Madiao back to Korea. Yi also claimed Jang simplified the cards to create tujeon while in prison and taught the game to prisoners and guards. Jang himself is believed to have died in prison. King Jeongjo (r. 1776-1800) issued several ineffective bans against tujeon after gambling was causing serious social problems.\n\nBy the early 19th century, tujeon evolved somewhat from its original form: decks were typically only forty to sixty cards in size, using four or six of the eight suits; and the numeral cards were no longer marked to distinguish their suit, being used interchangeably. Only the generals kept their suits. The cards were replaced by hanafuda during the Japanese occupation but some tujeon rules were transferred over to the Japanese cards.\n\nBy far the most popular game was \"gabo japgi\", so much so that the name was used interchangeably with tujeon. Also known as \"yeot bang mangyi\" (엿방망이, \"sweetmeat pestle\"), it is a baccarat-like game related to the Chinese domino game \"kol-ye-si\" (골여시). It is played with the 60 card deck and the object is to reach \"gabo\" or \"kapo\" which is gambling slang for 9. The game seems to be derived from Kabufuda games where the goal is to reach \"kabu\" or \"kaho\" which is also slang for 9. Both \"kabu\" and \"kapo\" are possibly descended from the Portuguese \"cavo\" which was slang for a stake or wager. Another similar game is Komi, played with Ganjifa cards, from Odisha, India along Portugal's old trade routes. Baccarat did not appear in Europe until mid-19th century France and was preceded by a simpler game called Macao, further hinting at a Portuguese connection. Another possible point of origin is China, where the games of Pai Gow (dominoes) and \"San zhang\" (cards) also have the target of reaching 9 but they are also more complicated and award bonuses for certain combinations.\n\nAnother popular game was \"dong dang\" (동당), an early rummy game similar to Khanhoo.\n\nPlaying the Tujeon cards is a theme used in several period drama series. Among them:\n(to be completed)\n"}
{"id": "30380229", "url": "https://en.wikipedia.org/wiki?curid=30380229", "title": "Wet nanotechnology", "text": "Wet nanotechnology\n\nWet nanotechnology (also known as wet nanotech) involves working up to large masses from small ones.\n\nWet nanotechnology requires water in which the process occurs. The process also involves chemists and biologists trying to reach larger scales by putting together individual molecules. While Eric Drexler put forth the idea of nano-assemblers working dry, wet nanotech appears to be the likely first area in which something like a nano-assembler may achieve economic results. Pharmaceuticals and bioscience are central features of most nanotech start-ups. Richard A.L. Jones calls nanotechnology that steals bits of natural nanotechnology and puts them in a synthetic structure \"biokleptic nanotechnology\". He calls building with synthetic materials according to nature's design principles \"biomimetic nanotechnology\".\n\nUsing these guiding principles could lead to trillions of nanotech robots, that resemble bacteria in structural properties, entering a person's blood stream to do medical treatments.\n\nWet nanotechnology is an anticipated new sub-discipline of nanotech that is going to mostly be dominated by the different forms of wet engineering. The processes that will be used are going to take place in aqueous solutions and are very close to that of biotechnology manufacturing / bio-molecular manufacturing which is largely concerned with the production of biomolecules like proteins and DNA/RNA. There is some overlap of Biotechnology and Wet nanotechnology because living things are inherently bottom-up engineered and any exploitation of this by biotechnologists means they dabble in bottom-up engineering (though mostly at the level of producing macromolecules like proteins and nucleic acids from there monomer units. Wet nanotech, however, seeks to analyse living things and their components as engineering systems and aims to understand them completely to have complete control of the behavior of the system and to derive principles and methods that can be applied more broadly to bottom up manufacturing, to manipulate matter on the atomic and molecular scales and to creating machines or devices at the nanometer and microscopic scales. Biotech is mostly about exploiting living systems in anyway possible. Molecular Biology and related disciplines compare the mechanism of function of proteins in particular - and nucleic acids to a lesser extent - as like \"molecular machines\". In order for engineers to mimic these nanoscale machines in a way that they could be produced with some efficiency, they must look into bottom-up manufacturing. Bottom-up manufacturing deals with manipulating individual atoms during the manufacturing process, so that there is absolute control of their placement and interactions.\n\nThen from the atomic scale, nanomachines could be made and even be designed to self-replicate themselves as long as they are designed in an environment with copious amount of the needed materials. Because individual atoms are being manipulated in the process, bottom-up manufacturing is often referred to as “atom by atom” manufacturing. If the manufacturing of nanomachines can be made more readily available through improved techniques, there could be a large economic and social impact. This would start with improvements in making microelectromechanical systems and then would allow for the creation of nanoscale biological sensors along with things that have not been thought of yet. This is because “wet” nanotech is only in the beginning of its life. Scientists and engineers alike feel that biomimetics is a great way to start looking at creating nanoscale machines. Humans have only had a few thousand years to try to learn about the mechanics of things at really small scales. However, nature has been working on perfecting the design and functionality of nanomachines for millions of years. This is why there are already nanomachines, such as ATP synthase, working in our bodies that have an unheard of 95% efficiency.\n\nWet nanotechnology is a form of wet engineering as opposed to dry engineering. There are different fields that deal with those two types of engineering. Biologists, from the point of view of nanotechnology, deal with wet engineering. They study processes that happen in life, and for the most part those processes take place in aqueous environments. Our bodies are made up mostly of water.\n\nElectrical and mechanical engineers are on the other side of the line in dry engineering. They are involved with processes and manufacturing that does not occur in aqueous environments.\n\nFor the most part, wet engineering deals with “soft” materials that allow for flexibility which is vital at the nanoscale in biological manufacturing. Dry engineers mostly handle things with rigid structures and parts. These differences stem for the fact that the forces that the two types of engineers must deal with are very different. At a larger scale, most things are dominated by Newtonian physics. However, when one looks at the nanoscale, especially in biological matters, the dominating force is Brownian motion.\n\nBecause nanotechnology in the new age is going to most likely deal with both dry and wet in conjunction with each other, there is going to have to be a change in the way society looks at engineering and manufacturing. People will have to be not only well educated in engineering but also in biology because the integration of the two is how there will be the largest improvements in nanotechnology.\n\nWith the existence of natural nanomachines, “a complex precision microscopic-sized machine that fits the standard definition of a machine”, such as ATP synthase and T4 bacteriophage, scientists and biologists know that they are capable of making similar types of machines at the same scale. However, nature has had a long time to perfect the building and creation of these nanomachines and humankind has only just begun to look into them with greater interest.\n\nThis interest may have been sparked because of the existence of nanomachines such as ATP synthase (adenosine triphosphate), which is the “second in importance only to DNA”. ATP is the main energy converter that our bodies contain and without it, life as we know it would not be able to flourish or even survive.\n\nBrownian motion is a random, constantly fluctuating force that acts on a body in environments that are at a microscale. This force is one that mechanical engineers and physicists are not used to dealing with because, at the larger scale that humankind tends to think of things, this force is not one that needs to be taken into account. People think of gravity, inertia, and other physics based forces that act on us all the time, however at the nanoscale those forces are mostly “negligible”.\n\nIn order for nanomachines to be recreated by man, either there will need to be discoveries that allow us to understand how to “exploit” Brownian motion as nature does or find a way to work around it by using materials that are rigid enough to stand up to these forces. The way that nature has been able to exploit Brownian motion is through self-assembly. This force pushes and pulls all of the proteins and amino acids around in our bodies and sticks them together in all sorts of combinations. The combinations that do not work separate and continue with their random attachment however, the combinations that do work produce things like ATP synthase. Through this process nature has been able to make a nanomachine that is 95% efficient, which is a feat that man has not been able to accomplish yet. This is all because nature does not try to work around the forces; it uses them at its advantage.\n\nGrowing cells in culture to take advantage of their internal chemical synthesis machinery can be considered a form of nanotechnology but this machinery has also been manipulated outside of living cells.\n"}
