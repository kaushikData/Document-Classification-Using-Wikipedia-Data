{"id": "2519156", "url": "https://en.wikipedia.org/wiki?curid=2519156", "title": "AS 1100", "text": "AS 1100\n\nAS 1100 is an Australian Standard for technical drawing including both mechanical and architectural designs. AS 1100 standard drawings contain attributes that are universal around Australia. The standard is published by Standards Australia.\n\nThe standard consists of six parts,\n\nYou cannot view these without purchasing a licence first\n"}
{"id": "35611164", "url": "https://en.wikipedia.org/wiki?curid=35611164", "title": "Advanced Concept Technology Demonstration", "text": "Advanced Concept Technology Demonstration\n\nAn Advanced Concept Technology Demonstration (ACTD) enables the evaluation of mature advanced technology for usage by the United States Military. ACTDs allow technology evaluation earlier and cheaper than would be possible through the formal acquisition of new production capabilities. ACTDs must be sponsored by an operational user with approval and oversight from the Deputy Under Secretary of Defense for Advanced Systems and Concepts (DUSD(AS&C)). No ACTDs were initiated after 2006, when the DUSD(AS&C) initiated the follow-on Joint Concept Technology Demonstration (JCTD) program to emphasize multiservice technology development and improved planning for transition to operations.\n\nThe following programs were completed under the ACTD framework\n\n"}
{"id": "28787917", "url": "https://en.wikipedia.org/wiki?curid=28787917", "title": "Anatoliy O. Morozov", "text": "Anatoliy O. Morozov\n\nAnatoliy O. Morozov (born May 9, 1939) is a Ukrainian scientist in the field of cybernetics. He is a Corresponding Member of the National Academy of Sciences of Ukraine, a Full member (Academician) of the International Academy of Information Science and the Academy of Technological Sciences of Russia and the President of the Academy of Technological Sciences of Ukraine.\n\nMorozov was born in Kiev, in the Ukrainian SSR of the Soviet Union, in present-day Ukraine. He was educated at the Kyiv Polytechnic Institute and was a student and follower of Victor Glushkov. From 1961 to 1992 he worked at the Institute of Cybernetics, Kiev. Since then he is with the Institute of Mathematical Machines and Systems Problems.\n\nMorozov was the chief designer of the automated control system of the Lviv manufacturing project, supervised by Glushkov. In 1970 Glushkov and the developers of Lviv were awarded the State Award of Ukrainian Socialist Republic.\n\nSince May 1, 1986, Morozov has been involved in the liquidation of consequences resulting from the Chernobyl disaster. He worked directly in a failure-zone for the greater part of a year. He headed the Special Design Bureau of Mathematical Machines and Systems of the Institute Cybernetics Academy of Sciences, Ukrainian Soviet Socialist Republic and organized the Special Design Office of Experts of the Academy of Sciences of the Ukrainian Soviet Socialist Republic. These bodies organized the ministries and departments of the Ukrainian Soviet Socialist Republic concerning the situational forecasting of radiation pollution of the Dnipro River and the territory of Ukraine. For this work he was awarded the Order of the Red Banner of Labour.\n\nMorozov is the Chief Designer of the RADA system, a hardware/software complex for the support of decisions by public authorities at various levels used by the Verkhovna Rada (Parliament of Ukraine). It is also used by various councils at different state levels in Ukraine and in several other countries. In 1998 he and the group of developers were awarded the State Award of Ukraine for this project.\n\nHe has many state awards and honours, including the following:\n\n"}
{"id": "12654117", "url": "https://en.wikipedia.org/wiki?curid=12654117", "title": "Battleshort", "text": "Battleshort\n\nBattleshort (sometimes \"battle short\") is a condition in which some military equipment can be placed so it does not shut down when circumstances would be damaging to the equipment or personnel. The origin of the term is to bridge or \"short\" the fuses of an electrical apparatus before entering combat, so that the fuse blowing will not stop the equipment from operating.\n\nAccording to Allied Ordnance Publication AOP-38-3, a NATO publication, a battleshort is \"The capability to bypass certain safety features in a system to ensure completion of the mission without interruption due to the safety feature.\" It also says, \"Examples of bypassed safety features are circuit overload protection, and protection against overheating\".\n\nIn peaceful situations one would want equipment to shut down so it is not damaged. In a battle or emergency, where the survival of the vessel (or other protected asset) is dependent upon the continued operation of the equipment, it is sometimes wiser to risk equipment damage than have the equipment shut down when it is needed. For example, the electrical drives to elevate and traverse the guns of a combat warship may have \"battleshort\" fuses, which are simply copper bars of the correct size to fit the fuse holders, as failure to return fire in a combat situation may cause loss of the ship and crew and damaging or overheating of the electrical motors is a preferable alternative.\n\nBattleshorts have been used in some non-combat situations as well, including the Firing Room/Mission Control spaces at NASA during the manned Apollo missions — specifically the Moon landings.\n"}
{"id": "2636486", "url": "https://en.wikipedia.org/wiki?curid=2636486", "title": "Billhook", "text": "Billhook\n\nThe billhook is a traditional cutting tool used widely in agriculture and forestry for cutting smaller woody material such as shrubs and branches and is distinct from the sickle. It is very common in the wine-growing countries of Europe. Elsewhere, it either developed locally such as in China, India and Japan, or was introduced by European settlers, such as in North and South America, South Africa and Australia.\n\nThe blade is usually made from a medium-carbon steel in varying weights and lengths, but typically long. Blades are straight near the handle but have an increasingly strong curve towards the end. The blade is generally sharpened only on the inside of the curve, but double-edged billhooks, or \"broom hooks\", also have a straight secondary edge on the back.\n\nThe blade is fixed to a wooden handle, in Europe usually made from ash due to its strength and ability to deal with repeated impact. Handles are mostly long and may be caulked or round. Longer handles are sometimes be used for heavier patterns, making the tool double-handed. The blade and handle are usually linked by a tang passing through the handle, but sometimes a socket that encloses the blade. Some styles of billhook have scales of hardwood or horn fitted to the handle.\n\nSome billhooks (for example the Kent pattern) have a single-bevelled blade, available in both right- and left-handed versions, others (such as the Machynlleth pattern) have dished blades (concave one side and convex the other), or a pronounced thickened nose (such as the Monmouth pattern). The reasons for many of these variations are now lost.\n\nThe use of a billhook is between that of a knife and an axe. It is often used for cutting woody plants such as saplings and small branches, for hedging and for snedding (stripping the side shoots from a branch). In France and Italy it is widely used for pruning grape vines. The billhook is the European equivalent of tools such as machetes, parangs, and kukris.\n\nThe billhook's use as a cutting tool goes back to the Bronze Age, and a few examples survive from this period—for example found in the sea around Greece. Iron examples from the later Iron Age have been found in pre-Roman settlements in several English counties as well as in France, Germany where it is called \"Hippe\" or \"Sechsle\" and Switzerland where it is called \"Gertel\".\n\nThe tool has developed a large variety of names in different parts of Britain, including bill, hedging bill, hand bill, hook bill, billhook, billook, brishing hook and broom hook. In American English a billhook may sometimes be referred to as a \"fascine knife\".\n\nMade on a small scale in village smithies and in larger industrial sites (e.g. Old Iron Works, Mells) the billhook is still relatively common throughout most of western Europe. During the 19th and early 20th centuries the larger manufacturers offered up to 200 or so different regional styles and shapes of blade, sometimes in a range of different sizes from long in steps. The French firm of Talabot boasted in their 1930 catalogue that they held over 3000 different patterns in their archives.\n\nBillhooks would have once been made by the local smith to the user's specifications but now sizes and shapes are largely standardised. The handles are mostly rat-tail tang, except the Yorkshire having such a long handle that a tang is just not practical—they have a socket instead. The smaller hooks have variations in the shape of the handle: round, oval and pistol-grip.\n\nBillhooks are almost universally made from ordinary steel of a moderate carbon content. High-carbon steel is not often used since an extremely sharp and hard edge is not necessary, and a slightly lower carbon content makes the hook easier to sharpen in the field. Hygiene and cosmetic appearance are unimportant so more expensive stainless steel is not used.\n\nBillhooks have a relatively thick blade since they are typically used for cutting thick and woody vegetation. The nose is sometimes also thickened to bring the sweet spot further forward and to optimise the chopping action. The edge of a billhook is not bevelled to a very narrow angle to avoid binding in green wood.\n\nThe hooked front of the blade is designed to prevent the edge from hitting the ground, which would quickly damage or blunt it. Billhooks were the tool of choice for clearing areas of brush and shrubs, since this activity requires chopping close to the ground. In German speaking countries, the billhook is known as a \"Rodeaxt\", which translates to \"clearing axe\".\n\nA billhook may vary in shape depending from which part of the UK it originates; there are eleven main types.\n\n\nThe southern group of hedgers use hooks often designed for other woodland work beside hedging They are all single-edged, and vary from moderately heavy to very light.\n\n\n\nA variety of other hooks were also made by most edge-tool makers (including pea and bean hooks, gorse or furze hooks, trimming hooks, staff hooks, slashers, pruning hooks) that are closely related to the billhook, although they may differ in shape, width or thickness of blade, length of handle etc. Another very close relation is the meat cleaver—sizes and handle-fixing of these are often very similar to billhooks. In some other European countries the same name is used for both tools, and it can be difficult to identify if the tool is intended for cutting wood or animal bones.\n\nUsage of billhooks also varies from country to country—in Sweden they were often used for cutting fodder for livestock (in the UK a gorse or furze hook would have been used); in France and Italy they were widely used for pruning vines (only recently has wine making come back to the UK), and miniature billhooks were used for harvesting grapes during the 'vendange' in France; in the Netherlands they were often used in a carpenter's workshop (in the UK use of a small hand axe was more common), and they were also found in the coopers' workshops in France (known as a cochoir, and used in the making of wooden barrel hoops). In the Balkans they were used for harvesting maize. In Finland they are used to cut branches from trees and cutting down small trees (known as vesuri). Images of billhooks often appear on coats of arms of towns and villages (particularly in winemaking areas of Alsace, the Black Forest, Hungary and Switzerland) and have been found carved into boundary stones in parts of Germany and onto rock faces in Italy.\n\nBillhooks are currently in common use by thatchers, coppicers, hurdle makers, charcoal burners and often by other traditional craftsmen, farmers and woodsmen. They are also the primary tool for hedgelayers.\n\nIn the medieval period a weapon similar to the halberd was called a bill or billhook. It consisted of a pole with a bill-like blade mounted below a spearhead, with spikes added to the back of the blade to increase the versatility of the weapon against cavalry and armour. The English in particular were known for using massed billmen rather than pikes or halberds in the Renaissance period, notably at the Battle of Flodden in 1513, when the Scottish king James IV was felled by an arrow and bill.\n\nThe billhook is an issued tool in some armed forces (see fascine knife). It is used for cutting brushwood for making fascines (brushwood bundles) and gabions—originally for the construction of cannon emplacements, and later for machine gun emplacements. It is also issued to the pioneer corps of most regiments. In the Indian Army, it is given the name 'knife gabion'.\n\nA non-military use as a weapon was a \"pruning bill\", described as the weapon used in the Pierre Rivière parricide case of 1835.\n\nThe Finnish military engineer NCOs have a billhook as the part of their personal gear instead of field shovel. It also doubles as a sidearm. Officers have a field axe.\n\n\n"}
{"id": "40249151", "url": "https://en.wikipedia.org/wiki?curid=40249151", "title": "Bowling Iron Works", "text": "Bowling Iron Works\n\nThe Bowling Iron Works was an iron working complex established around 1780 in the district of East Bowling part of the township and manor of Bowling, now in the southeast of Bradford in Yorkshire, England. The operation included mining coal and iron ore, smelting, refining, casting and forging to create finished products.\n\nIron is said to have been worked in the vicinity of Bradford in Roman times. \nThe monks of Rievaulx Abbey to the east were working iron on land owned by their monastery in 1150, and forgemen are mentioned in 1358.\nSurface coal was being extracted from outcrops and shallow pits by 1360, and coal mines were worked by 1502.\nThe Bowling Ironworks were established in the 1780s to smelt and forge iron from the Black Bed ironstone deposits using coal from the Better Bed seam, both of which lay under the site.\nThe ironstone yields about 32% iron.\nThe Better Bed coal is free of sulfur, making it ideal for furnaces used in smelting, puddling and forging.\nThe Black Bed coal, nearer to the surface, could be sold or used for firing boilers and other purposes.\n\nMining began in Jeremiah Rawson's estate, then extended into nearby estates as the deposits became exhausted, always mining the same beds of minerals.\nIn 1794 the company purchased from Francis Lindley Wood (owner of Bolling Hall and Lord of the Manor) the rights to 90 acres of coal and iron stone in Hall Lane, Broomfields. In 1806 the company purchased additional mineral rights to parts of Sir Francis Lindley Wood's Bowling Hall Estate .\nThe rights to other land was purchased in later years, often after extensive negotiation.\nIn 1816 the company purchased from Sir Francis all his remaining lands and mineral rights in Bowling and in 1821 purchased from him the Lordship of the Manor.\n\nThe same seams of ironstone and coal were exploited by the Low Moor Iron Company, founded in 1788, and then by the Bierley Iron Company from around 1810.\n\nThe first foundry was established at Bowling around 1784 by a group of businessmen including John Sturges, an ironmaster with works at Wakefield, and Richard Paley, an iron merchant of Leeds.\nThe other partners were John Sturges junior, William Sturges and John Ewell. Ewell left the partnership in 1792.\nThe company took the name of John Sturges & Company.\n\nIron was brought to the foundry from Wakefield in the early years.\nThe first products were items for domestic use such as laundry irons, ovens, boilers, window sash weights and clock weights.\nThe smelting plant was established in 1788, producing pig iron that was used in the boiler plates for the first steam engine at the Low Moor works. \nAs late as 1792 pig iron was delivered to Wakefield and wrought iron received in exchange.\nIn 1804 the partnership was extended, bringing in John Green Paley, George Paley, Thomas Mason and the Reverend John Simons.\n\nThe Bowling works were selling large quantities of guns, shot and shells to the British government before 1790. Cast iron was used for guns before the invention of wrought iron, and the cast iron guns were subject to rigorous production controls and quality tests.\nOn 27 July 1796 the partners signed an agreement with Matthew Boulton and James Watt to pay royalties on the two steam engines in use at the works, and to be allowed to make additional steam engines at the works, paying royalties.\nWhen Sir William Armstrong invented wrought-iron guns, some of the first coils he used were Bowling iron. \nBowling iron, or Best Yorkshire, became well known around the world.\n\nSteam hammers were installed at the Bowling Ironworks where the steam cylinder was bolted to the back of the hammer, \nthus reducing the height of the machine.\nThese were designed by John Charles Pearce, who took out a patent for his steam hammer design several years before James Nasmyth's patent expired.\nOn 14 November 1848 a new partnership was created with the name of Bowling Iron Co., confirmed by act of parliament in August 1849. \nThe company was incorporated and registered in 1870.\nAn 1891 description said the ironworks lay in,\nThe plant at that time included blast furnaces and refineries used in the first stages of iron manufacture, puddling and ball furnaces with high brick or iron chimneys, a shed housing the steam hammers, steelworks, a large machine shop, boiler works, a large foundry and other workshops and buildings. A narrow gauge railway was used to move material within the works, and a line to the Great Northern Railway was used to ship the products.\nA network of tramways brought minerals from the pits to the works, with wagons pulled by wire ropes powered by stationary engines.\nFour large pumps were being used to keep the mines dry, with some of the water used in the ironworks.\n\nThe Bowling Iron Company went into liquidation in 1898.\nIn 1903 the company was reorganized as The Bowling Iron Company. It was liquidated in 1921.\n\nBy 1840 Bradford was known for having some of the most smoke-filled air in Britain.\nAs early as 1803 an act had stated that, \"Engine chimneys are to be erected of sufficient height as not to create a nuisance by the emission of smoke. All owners of engines etc. are to construct fireplaces thereof in such a manner as most effectually to destroy and consume the smoke arising therefrom.\" However, little was done to enforce the laws. \nThere was a general feeling that the factories provided work, so should not be pushed too hard to reduce pollution.\nAn 1841 account said, \"The condition of Bradford is dreadful. Lowmoor iron-forges most extensively spread their suffocating exhalations on the one side ... On the other side, Bowling Iron \"Hell\" (for it is one truly) casts a still denser atmosphere and sulphurous stench...\"\nThe Bowling Iron Company was fined on 12 December 1874, but only for £5 with £9. 10s. costs for ten offences.\nThe population suffered high levels of respiratory diseases, peaking in 1890 during an influenza epidemic.\n\nThe method of paying the men led them into temptation of intemperance and extravagance:\nAccording to H. Hartopp, manager of the Bowling Works,\n\nThe company built St. John's Church, Bowling, at its own expense of ₤5,000. The church was consecrated on 8 February 1842.\nIt is built in the Lancet-Gothic style and is in size, with a spire.\nThe church was the first to be built in England of iron and stone, with only the rafters made of wood.\nIn April 1847 St. John's school in Bowling was visited. The Bowling Iron company had erected the building, but did not contribute much to its expenses. There were on average 150 children at the school, of whom 12 were factory workers.\nOnly seven children had reached aged 13. The schoolroom had poor lighting and ventilation, and there was no mistress to instruct the older children.\n\nIn the mines, hurrier-boys would drag carriages along tracks to the miners,\nthen drag the filled carriages back to the pit shaft and hook them to a chain so they could be pulled up.\nAn 1843 inquiry into child labor said of Jabez Scott, aged fifteen, a worker at the Bowling Iron Works: \"Work is very hard; sleeps well sometimes, sometimes is very ill tired and cannot sleep so weel [sic].\"\nAn 1847 report noted that there was considerable temptation to employ boys in the mines when they were below the legal age, then ten years of age.\n\"Managers, &c., of mines, complain that the work required of boys in seams of coal not more than 18 inches to two feet thick, is done at a disadvantage, unless they are brought to it from their earliest years.\"\n\nIn 1875 there was an explosion in the Bowling Iron Company's Crosses Pit mine, where 40 men and boys were working. Four ironstone getters and hurriers were killed, and others injured. The cause seems to have been accidental ignition of gunpowder being used to break up large pieces of scale that contained the ironstone.\nA select committee of the House of Commons in 1877 heard that in West Yorkshire, including the Bowling Iron Works, the colliers employed boys as assistants, rather the boys being employed by the company, although the boys had their names entered and were subject to the colliery rules. At that time Bowling employed about 2,000 workmen in the collieries, and it was estimated that about two men died each year. Accident rates were higher in the iron works, despite employment being lower.\nBoys were employed in various capacities in the ironworks, such as wheeling hot lumps of puddled iron to the hammers.\n\nNotes\n\nCitations\n\nSources\n"}
{"id": "7147897", "url": "https://en.wikipedia.org/wiki?curid=7147897", "title": "CALS (DOD)", "text": "CALS (DOD)\n\nCALS (Continuous Acquisition and Life-cycle Support) is a United States Department of Defense initiative for electronically capturing military documentation and linking related information. \n\nThe initiative has developed a number of standard specifications\n(protocols) for the exchange of electronic data with commercial suppliers. These standards are often referred to as simply \"CALS\". CALS standards have been adopted by several other allied nations. \n\nThe CALS initiative has endorsed IGES and STEP as formats for digital data. \n\nCALS includes standards for electronic data interchange, electronic technical documentation, and guidelines for process improvement. \n\n\nCALS was known formerly as \"Computer-aided Acquisition and Logistic Support\".\n\nSources and references that need formatted properly:\nOffice of the Assistant Director for Telecommunications\nand Information Systems\nHeadquarters Defense Logistics Agency\nCameron Station\nAlexandria, VA 22314 \n\n\nOverview articles\n\n"}
{"id": "945343", "url": "https://en.wikipedia.org/wiki?curid=945343", "title": "Charles Hampden-Turner", "text": "Charles Hampden-Turner\n\nCharles Hampden-Turner (29 September 1934 in London, England) is a British management philosopher, and Senior Research Associate at the Judge Business School at the University of Cambridge since 1990. He is the creator of \"Dilemma Theory\" and co-founder and Director of Research and Development at the Trompenaars-Hampden-Turner Group, in Amsterdam.\n\nHampden-Turner was born in London in 1934 and grew up in Cambridge, in a house on the site where Robinson College now stands. He was educated at Wellington College, a military public school attended by his father, and did his national service with the same regiment his father had served in, the Suffolk Regiment. \n\nOn finishing military service, Hampden-Turner attended Trinity College, Cambridge. He spoke often in the Cambridge Union Society and was elected Chairman of the Cambridge University Conservative Association.\n\n\n\n"}
{"id": "23989039", "url": "https://en.wikipedia.org/wiki?curid=23989039", "title": "Choke manifold", "text": "Choke manifold\n\nIn oil and gas production a choke manifold is used to lower the pressure from the well head. It consist of a set of high pressure valves and at least two chokes. These chokes can be fixed or adjustable or a mix of both. The redundancy is needed so that if one choke has to be taken out of service, the flow can be directed through another one. By lowering pressure the retrieved gases can be flared off on site.\n\n"}
{"id": "25127952", "url": "https://en.wikipedia.org/wiki?curid=25127952", "title": "Cider mill", "text": "Cider mill\n\nA cider mill is the location and equipment used to crush apples into apple juice for use in making apple cider, hard cider, applejack, apple wine, pectin and other products derived from apples. More specifically, it refers to a device used to crush or grind apples as part of the overall juice production.\n\nThe mills used to manufacture, ferment, store, and ship juice products are usually located near apple orchards. Historically, the types of structure and machinery have varied greatly — including horse powered, water driven, and machine operated mills. The presses can be fixed or portable.\n\nCider mills were subject to legal proceedings in New York state in the 1800s over whether they were \"fixed to freeholds\" and other cases addressing legal designation as to what kind of property a cider mill is.\n\nCider-making takes place in numerous countries and regions. As with the cider itself, the various techniques used in milling and pressing the apples vary with each cider-making tradition. In most traditions, cider milling traditionally takes place in two stages: first, milling or grinding the apples into a pulpy mass called pulp, and a second stage, pressing the pulp to release the juice or \"must\". The remaining solids after juice extraction is \"pomace\" or \"pommage\".\n\nSome mills provide custom pressing of a farmer's apples. In this way, apple varieties can be blended to make a cider of mixed juice types, for instance, a combination of sweet and aromatic juices. Various types of apple are recommended for cidering. Alcoholic cider can also be produced and is known as hard cider or applejack. Cider is stored and fermented in wooden barrels, carboys, stainless tanks, or glass jugs.\n\nIn 19th Century New England, apple farmers paid a mill owner a fee to crush apples into juice. A typical cider mill would look like many other small barns and sheds, with a set of large doors in the center of the longer side. Most cider mills were 20-30' long by 20-25' in width. At Old Sturbridge Village in Massachusetts, 19th Century cider mill equipment is still used to make cider. In \"The Marble Faun\", author Nathaniel Hawthorne contrasted the wine-making in Italy with the cider-making process of \"New England vintages, where the big piles of golden and rosy apples lie under the orchard trees, in the mild, autumnal sunshine; and the creaking cider-mill, set in motion by a circumgyratory horse, is all a-gush with the luscious juice.\"\n\nMilling, grinding, or crushing can take various forms, depending on the quantity of apples to be crushed and the mode of power available.\n\nThe earliest and most basic form of cider mill consists of little more than an enclosed area where apples are pounded by large wooden pestles.\n\nIn England, Jersey, and northern France, the traditional form was a \"horse-mill\" or \"stone mill\". A horse-mill consists of a circular trough made of stone, in which is set either one or two large stone wheels called \"runners\". At the center is a pivot point or \"nut\". A horse is harnessed to the outside of the wheel, and driven in a circle, slowly grinding the apples to a pulpy mass called pommage. Through the early 19th century, this was the dominant form in England. By the early 20th century in Britain, however, the stone mills had largely fallen out of fashion, increasingly replaced by the roller mills, though they continued to be the primary form in France. Though the stone mill had been introduced to and used by the American Colonists, its usage was not well recorded, and by the end of the 19th century it was essentially unknown in the United States.\n\nIn Germany, apples were traditionally grated by hand rather than crushed.\n\nA later innovation was the toothed roller-mill. These mills use toothed cylinders made of stone or metal to grind the apples into pomace. Such mills are portable, and produce a pomace that is finer than that of the large horse-mills. It was first introduced to England in 1689 by agriculturalist John Worlidge, who adapted it from the sugar-cane crushers used in the West Indies. Yet as of the beginning of the 19th-century, such mills could not handle the same quantity in bulk as the horse-driven mills. \n\nAfter the apples have been ground into pomace, the pomace must be squeezed to extract the juice. This is done in a device called a cider press, which like the cider mill, takes various forms.\n\nOne form is a large horse-operated lever press or screw press. This method was common in Britain, Jersey, and northern France, as well as the United States. This form involves either of two methods to hold the loose pomace in place as it is pressed. The first is to use alternating layers of straw and pomace, creating a mixture known as \"cheese\". The other is to wrap the pomace in cloth. The German tradition used smaller, hand-operated lever presses in the same manner.\n\nAn alternate form is the \"hand press\" (sometimes called a \"Continental Press\" in England), a small screw-press operated by hand. These presses dispense with the various methods of covering the pomace, and instead use a container made of wooden staves.\n\nBy the turn of the 20th century, hydraulic presses had begun to be introduced.\n\nAfter the juices had been extracted, the leftover pressings are variously known as \"math\", \"cake\", \"powz\", \"mure\" or simply \"pommage\". It might either be watered and pressed again to produce a weak cider known as ciderkin, or when still fresh, used as animal feed.\n\n\n"}
{"id": "1010476", "url": "https://en.wikipedia.org/wiki?curid=1010476", "title": "Computer Professionals' Union", "text": "Computer Professionals' Union\n\nComputer Professionals' Union (CPU or CP-Union) is a mass organization of information and communications technology (ICT) professionals, practitioners, and workers in the Philippines. It is registered in the Philippines as a non-stock, non-profit, non-government organization that promotes activist ICT principles and organize ICT professionals to provide ICT services to Filipino people. Their office is located at Quezon City and their current National Coordinator is Rick Bahague.\n\nSome of CPU initiatives include Software Freedom Day celebration in the Manila, promotion of Free and Open Source Software including Drupal, and collaboration with Wikimedia Philippines.\n\nThe organization was started in 2001 by a group of information communications technology practitioners. They officially registered under the Philippine Securities and Exchange Commission in 2008 as a non-profit and non-stock corporation.\n\nCPU is notable for its belief that the ICT sector in the Philippines is controlled and dominated by foreign monopoly capitalists, which stunts the growth and development of Filipino technology and economy. CPU supports a truly nationalist and democratic government that will advance and promote a people's ICT. CPU believes that, like farmers, workers, and other sectors in the country, ICT workers also need to organize in order to advance their specific needs.\n\nA year before the 2010 Philippine general elections, CPU warns sophisticated cheating with the Philippines' first automated polls. They had hosted a national conference in University of the Philippines in Diliman, Quezon City to discuss the automated election system (AES). Rick Bahague, CPU National Coordinator said that the goal of the conference was to gather experiences and best practices in technologies relevant to AES. He further said that software bugs in the AES system can affect machines to be used in the elections and the automated election system is vulnerable to manipulation from inside or outside attacks.\n\nDuring the height of the protests against Stop Online Piracy Act (SOPA) and PROTECT IP Act (PIPA) in January 2012, CPU expressed strong opposition to it. They said that SOPA and PIPA that were being pushed in the United States Congress attack free speech and expression and would have impacts to human rights groups, bloggers, advocacy groups and all content creators in the web. They believed that any website can be closed without due process.\n\nIn September 2012, Philippine President Aquino signed the Cybercrime Prevention Act of 2012 enacted by 15th Congress of the Philippines. The law had led numerous sectors including Computer Professionals' Union to protest it. According to CPU, sections of the law may have various interpretations that may lead to intentional or non-intentional misinterpretations by State authorities wherein computer users can be punished without due process. They further said that Section 19 of the law has become far worse than SOPA and PIPA.\n\nIn advancing CPU's advocacy, these are their principles:\n\n"}
{"id": "46342258", "url": "https://en.wikipedia.org/wiki?curid=46342258", "title": "Darrieus–Landau instability", "text": "Darrieus–Landau instability\n\nThe Darrieus–Landau instability is an intrinsic flame instability that occurs in premixed flames due to the thermal expansion of the gas produced by the combustion process. It was predicted independently by Georges Jean Marie Darrieus and Lev Landau.\n\nThe instability analysis behind the Darrieus–Landau instability considers a planar, premixed flame front subjected to very small perturbations. It is useful to think of this arrangement as one in which the unperturbed flame is stationary, with the reactants (fuel and oxidizer) directed towards the flame and perpendicular to it with a velocity u1, and the burnt gases leaving the flame also in a perpendicular way but with velocity u2. The analysis assumes that the flow is an incompressible flow, and that the perturbations are governed by the linearized Euler equations and, thus, are inviscid. With these considerations, the main result of this analysis is that, if the density of the burnt gases is less than that of the reactants, which is the case in practice due to the thermal expansion of the gas produced by the combustion process, the flame front is unstable to perturbations of any wavelength. Another result is that the rate of growth of the perturbations is inversely proportional to their wavelength; thus small flame wrinkles (but larger than the characteristic flame thickness) grow faster than larger ones. In practice, however, diffusive and buoyancy effects that are not taken into account by the analysis of Darrieus and Landau may have a stabilizing effect.\n"}
{"id": "7407877", "url": "https://en.wikipedia.org/wiki?curid=7407877", "title": "Dense Inert Metal Explosive", "text": "Dense Inert Metal Explosive\n\nDense Inert Metal Explosive (DIME) is an experimental type of explosive that has a relatively small but effective blast radius. It is manufactured by producing a homogeneous mixture of an explosive material (such as phlegmatized HMX or RDX) and small particles of a chemically inert material such as tungsten. It is intended to limit the distance at which the explosion causes damage, to avoid collateral damage in warfare.\n\nThe phrase \"inert metal\" refers to a metal that is not chemically active and therefore not part of the chemical reaction that causes the explosion, as opposed to some metals, such as aluminium, that do form part of the chemical reaction—e.g. in Tritonal.\n\nAn emerging criticism of DIME weapons is that they might turn out to have strong biological effects in those who are hit by the micro-shrapnel from these types of explosives.\n\nDIME mixtures have been studied for some time, but apparently only began to be adopted for weapons after the year 2000.\n\nDIME weapons consist of a carbon fiber casing filled with a mixture of explosive and very dense microshrapnel, consisting of very small particles (1–2 mm) or powder of a heavy metal. To date, tungsten alloy (heavy metal tungsten alloy, or HMTA) composed of tungsten and other metals such as cobalt and nickel or iron has been the preferred material for the dense microshrapnel or powder.\n\nTwo common HMTA alloys are:\n\n\nUpon detonation of the explosive, the casing disintegrates into extremely small particles, as opposed to larger pieces of shrapnel which results from the fragmentation of a metal shell casing. The HMTA powder acts like micro-shrapnel which is very lethal at close range (about ), but loses momentum very quickly due to air resistance, coming to a halt within approximately 40 times the diameter of the charge. This increases the probability of killing people within a few meters of the explosion while reducing the probability of causing death and injuries or damage farther away. Survivors close to the lethal zone may still have their limbs amputated by the HMTA microshrapnel, which can slice through soft tissue and bone.\n\nThe carcinogenic effects of heavy metal tungsten alloys (HMTA) (along with depleted uranium [DU]) have been studied by the U.S. Armed Forces since at least the year 2000. These alloys were found to cause neoplastic transformations of human osteoblast cells.\n\nA more recent U.S. Department of Health and Human Services study, from 2005, has found that HMTA shrapnel rapidly induces rhabdomyosarcoma in laboratory rats.\n\nTungsten alloy carcinogenicity may be most closely related to the nickel content of the alloys used in weapons to date; however, pure tungsten and tungsten trioxide are also suspected of carcinogenic and other toxic properties, and have been shown to have such effects in animal studies.\n\nIn 2009, a group of Italian scientists affiliated with the New Weapons Research Committee (NWRC) watchdog group pronounced DIME wounds \"untreatable\" because the powdered tungsten they dispense cannot be removed surgically.\n\nIn July/August 2006, doctors in the Gaza Strip reported unusual wounds caused by Israel Defense Forces attacks against Palestinians, claiming that they were from previously unknown weapons. A lab analysis of the metals found in the victims' bodies was reportedly \"compatible with the hypothesis\" that DIME weapons were involved. Israel denied possessing or using such weapons, and an Israeli military expert said that the wounds were consistent with ordinary explosives.\n\nDr. Mads Gilbert and Dr. Erik Fosse, working on wounded from the 2008–2009 Israel–Gaza conflict, reported injuries that they believed were caused by some new type of weapon used by Israel, which they speculated were DIME bombs. Gilbert and Fosse made the same accusations during the ongoing 2014 Gaza conflict.\n\n\n"}
{"id": "29150815", "url": "https://en.wikipedia.org/wiki?curid=29150815", "title": "Draw sheet", "text": "Draw sheet\n\nA draw sheet is a small bed sheet placed crosswise over the middle of the bottom sheet of a mattress to cover the area between the person's upper back and thighs, often used by medical professionals to move patients. It can be made of plastic, rubber, or cotton, and is about half the size of a regular sheet. It can be used in place of a mattress pad if a rubber mattress is used. The draw sheet may or may not be tucked into the sides of the bed. When a draw sheet is used to move patients, it is sometimes known as a lift sheet. Nursing manuals recommend that, when a plastic or rubber draw sheet is used, a cotton drawsheet is placed over it. If a folded sheet is used as a draw sheet, the folded edge of the sheet is positioned at the person's upper body. Draw sheets used as lift sheets are generally not tucked in, though sometimes after the move, they are.\n\n"}
{"id": "1406812", "url": "https://en.wikipedia.org/wiki?curid=1406812", "title": "Energy harvesting", "text": "Energy harvesting\n\nEnergy harvesting (also known as power harvesting or energy scavenging or ambient power) is the process by which energy is derived from external sources (e.g., solar power, thermal energy, wind energy, salinity gradients, and kinetic energy, also known as ambient energy), captured, and stored for small, wireless autonomous devices, like those used in wearable electronics and wireless sensor networks.\n\nEnergy harvesters provide a very small amount of power for low-energy electronics. While the input fuel to some large-scale generation costs resources (oil, coal, etc.), the energy source for energy harvesters is present as ambient background. For example, temperature gradients exist from the operation of a combustion engine and in urban areas, there is a large amount of electromagnetic energy in the environment because of radio and television broadcasting.\n\nOne of the earliest applications of ambient power collected from ambient electromagnetic radiation (EMR) is the crystal radio.\n\nThe principles of energy harvesting from ambient EMR can be demonstrated with basic components.\n\nEnergy harvesting devices converting ambient energy into electrical energy have attracted much interest in both the military and commercial sectors. Some systems convert motion, such as that of ocean waves, into electricity to be used by oceanographic monitoring sensors for autonomous operation. Future applications may include high power output devices (or arrays of such devices) deployed at remote locations to serve as reliable power stations for large systems. Another application is in wearable electronics, where energy harvesting devices can power or recharge cellphones, mobile computers, radio communication equipment, etc. All of these devices must be sufficiently robust to endure long-term exposure to hostile environments and have a broad range of dynamic sensitivity to exploit the entire spectrum of wave motions.\n\nEnergy can also be harvested to power small autonomous sensors such as those developed using MEMS technology. These systems are often very small and require little power, but their applications are limited by the reliance on battery power. Scavenging energy from ambient vibrations, wind, heat or light could enable smart sensors to be functional indefinitely. Several academic and commercial groups have been involved in the analysis and development of vibration-powered energy harvesting technology, including the Control and Power Group and Optical and Semiconductor Devices Group at Imperial College London, IMEC and the partnering Holst Centr, AdaptivEnergy, LLC, ARVENI, MIT Boston, Victoria University of Wellington, Georgia Tech, UC Berkeley, Southampton University, University of Bristol, Micro Energy System Lab at The University of Tokyo, Nanyang Technological University, PMG Perpetuum, ReVibe Energy, Vestfold University College, National University of Singapore, NiPS Laboratory at the University of Perugia, Columbia University, Universidad Autónoma de Barcelona and USN & Renewable Energy Lab at the University of Ulsan (Ulsan, South Korea). The National Science Foundation also supports an Industry/University Cooperative Research Center led by Virginia Tech and The University of Texas at Dallas called the Center for Energy Harvesting Materials and Systems.\n\nTypical power densities available from energy harvesting devices are highly dependent upon the specific application (affecting the generator's size) and the design itself of the harvesting generator. In general, for motion powered devices, typical values are a few µW/cm³ for human body powered applications and hundreds of µW/cm³ for generators powered from machinery. Most energy scavenging devices for wearable electronics generate very little power.\n\nIn general, energy can be stored in a capacitor, super capacitor, or battery. Capacitors are used when the application needs to provide huge energy spikes. Batteries leak less energy and are therefore used when the device needs to provide a steady flow of energy. Compared to batteries, super capacitors have virtually unlimited charge-discharge cycles and can therefore operate forever enabling a maintenance-free operation in IoT and wireless sensor devices \n\nCurrent interest in low power energy harvesting is for independent sensor networks. In these applications an energy harvesting scheme puts power stored into a capacitor then boosted/regulated to a second storage capacitor or battery for the use in the microprocessor or in the data transmission. The power is usually used in a sensor application and the data stored or is transmitted possibly through a wireless method.\n\nThe history of energy harvesting dates back to the windmill and the waterwheel. People have searched for ways to store the energy from heat and vibrations for many decades. One driving force behind the search for new energy harvesting devices is the desire to power sensor networks and mobile devices without batteries. Energy harvesting is also motivated by a desire to address the issue of climate change and global warming.\n\nThere are many small-scale energy sources that generally cannot be scaled up to industrial size:\n\nA possible source of energy comes from ubiquitous radio transmitters. Historically, either a large collection area or close proximity to the radiating wireless energy source is needed to get useful power levels from this source. The nantenna is one proposed development which would overcome this limitation by making use of the abundant natural radiation (such as solar radiation).\n\nOne idea is to deliberately broadcast RF energy to power and collect information from remote devices: This is now commonplace in passive radio-frequency identification (RFID) systems, but the Safety and US Federal Communications Commission (and equivalent bodies worldwide) limit the maximum power that can be transmitted this way to civilian use. This method has been used to power individual nodes in a wireless sensor network\n\nAirflow can be harvested by various turbine and non-turbine generator technologies. For example, Zephyr Energy Corporation’s patented Windbeam micro generator captures energy from airflow to recharge batteries and power electronic devices. The Windbeam’s novel design allows it to operate silently in wind speeds as low as 2 mph. The generator consists of a lightweight beam suspended by durable long-lasting springs within an outer frame. The beam oscillates rapidly when exposed to airflow due to the effects of multiple fluid flow phenomena. A linear alternator assembly converts the oscillating beam motion into usable electrical energy. A lack of bearings and gears eliminates frictional inefficiencies and noise. The generator can operate in low-light environments unsuitable for solar panels (e.g. HVAC ducts) and is inexpensive due to low cost components and simple construction. The scalable technology can be optimized to satisfy the energy requirements and design constraints of a given application.\n\nThe flow of blood can also be used to power devices. For instance, the pacemaker developed at the University of Bern, uses blood flow to wind up a spring which in turn drives an electrical micro-generator.\n\nPhotovoltaic (PV) energy harvesting wireless technology offers significant advantages over wired or solely battery-powered sensor solutions: virtually inexhaustible sources of power with little or no adverse environmental effects. Indoor PV harvesting solutions have to date been powered by specially tuned amorphous silicon (aSi)a technology most used in Solar Calculators. In recent years new PV technologies have come to the forefront in Energy Harvesting such as Dye Sensitized Solar Cells (DSSC). The dyes absorbs light much like chlorophyll does in plants. Electrons released on impact escape to the layer of TiO and from there diffuse, through the electrolyte, as the dye can be tuned to the visible spectrum much higher power can be produced. At a DSSC can provide over per cm².\n\nThe piezoelectric effect converts mechanical strain into electric current or voltage. This strain can come from many different sources. Human motion, low-frequency seismic vibrations, and acoustic noise are everyday examples. Except in rare instances the piezoelectric effect operates in AC requiring time-varying inputs at mechanical resonance to be efficient.\n\nMost piezoelectric electricity sources produce power on the order of milliwatts, too small for system application, but enough for hand-held devices such as some commercially available self-winding wristwatches. One proposal is that they are used for micro-scale devices, such as in a device harvesting micro-hydraulic energy. In this device, the flow of pressurized hydraulic fluid drives a reciprocating piston supported by three piezoelectric elements which convert the pressure fluctuations into an alternating current.\n\nAs piezo energy harvesting has been investigated only since the late 1990s, it remains an emerging technology. Nevertheless, some interesting improvements were made with the self-powered electronic switch at INSA school of engineering, implemented by the spin-off Arveni. In 2006, the proof of concept of a battery-less wireless doorbell push button was created, and recently, a product showed that classical wireless wallswitch can be powered by a piezo harvester. Other industrial applications appeared between 2000 and 2005, to harvest energy from vibration and supply sensors for example, or to harvest energy from shock.\n\nPiezoelectric systems can convert motion from the human body into electrical power. DARPA has funded efforts to harness energy from leg and arm motion, shoe impacts, and blood pressure for low level power to implantable or wearable sensors. The nanobrushes are another example of a piezoelectric energy harvester. They can be integrated into clothing. Multiple other nanostructures have been exploited to build an energy-harvesting device, for example, a single crystal PMN-PT nanobelt was fabricated and assembled into a piezoelectric energy harvester in 2016. Careful design is needed to minimise user discomfort. These energy harvesting sources by association affect the body. The Vibration Energy Scavenging Project is another project that is set up to try to scavenge electrical energy from environmental vibrations and movements. Microbelt can be used to gather electricity from respiration. Besides, as the vibration of motion from human comes in three directions, a single piezoelectric cantilever based omni-directional energy harvester is created by using 1:2 internal resonance. Finally, a millimeter-scale piezoelectric energy harvester has also already been created.\n\nThe use of piezoelectric materials to harvest power has already become popular. Piezoelectric materials have the ability to transform mechanical strain energy into electrical charge. Piezo elements are being embedded in walkways to recover the \"people energy\" of footsteps. They can also be embedded in shoes to recover \"walking energy\". Researchers at MIT developed the first micro-scale piezoelectric energy harvester using thin film PZT in 2005. Arman Hajati and Sang-Gook Kim invented the Ultra Wide-Bandwidth micro-scale piezoelectric energy harvesting device by exploiting the nonlinear stiffness of a doubly clamped microelectromechanical systems (MEMSs) resonator. The stretching strain in a doubly clamped beam shows a nonlinear stiffness, which provides a passive feedback and results in amplitude-stiffened Duffing mode resonance. Typically, piezoelectric cantilevers are adopted for the above-mentioned energy harvesting system. One drawback is that the piezoelectric cantilever has gradient strain distribution, i.e., the piezoelectric transducer is not fully utilized. To address this issue, triangle shaped and L-shaped cantilever are proposed for uniform strain distribution.\n\nIn 2018, Soochow University researchers reported hybridizing a triboelectric nanogenerator and a silicon solar cell by sharing a mutual electrode. This device can collect solar energy \"or\" convert the mechanical energy of falling raindrops into electricity.\n\nBrothers Pierre Curie and Jacques Curie gave the concept of piezoelectric effect in 1880. Piezoelectric effect converts mechanical strain into voltage or electric current and generates electric energy from motion, weight, vibration and temperature changes as shown in the figure.\n\nConsidering piezoelectric effect in thin film lead zirconate titanate formula_1 PZT, microelectromechanical systems (MEMS) power generating device has been developed. During recent improvement in piezoelectric technology, Aqsa Abbasi \"(also known as Aqsa Aitbar, General secretory at IMS, IEEE MUET Chapter and Director Media at HYD MUN \") diffentiated two modes called formula_2 and formula_3 in vibration converters and re-designed to resonate at specific frequencies from an external vibration energy source, thereby creating electrical energy via the piezoelectric effect using electromechanical damped mass.\nHowever, Aqsa further developed beam-structured electrostatic devices that are more difficult to fabricate than PZT MEMS devices versus a similar because general silicon processing involves many more mask steps that do not require PZT film. Piezoelectric formula_2 type sensors and actuators have a cantilever beam structure that consists of a membrane bottom electrode, film, piezoelectric film, and top electrode. More than mask steps are required for patterning of each layer while have very low induced voltage. Pyroelectric crystals that have a unique polar axis and have spontaneous polarization, along which the spontaneous polarization exists. These are the crystals of classes , , , , , , , . The special polar axis—crystallophysical axis — coincides with the axes , , and of the crystals or lies in the unique straight plane . Consequently, the electric centers of positive and negative charges are displaced of an elementary cell from equilibrium positions, i.e., the spontaneous polarization of the crystal changes. Therefore, all considered crystals have spontaneous polarization formula_5. Since \npiezoelectric effect in pyroelectric crystals arises as a result of changes in their spontaneous polarization under external effects (electric fields, mechanical stresses). As a result of displacement, Aqsa Abbasi introduced change in the components formula_6 along all three axes formula_7. Suppose that formula_7 is proportional to the mechanical stresses causing in a first approximation, which results formula_9 where represents the mechanical stress and represents the piezoelectric modules.\n\nPZT thin films have attracted attention for applications such as force sensors, accelerometers, gyroscopes actuators, tunable optics, micro pumps, ferroelectric RAM, display systems and smart roads, when energy sources are limited, energy harvesting plays an important role in the environment. Smart roads have the potential to play an important role in power generation. Embedding piezoelectric material in the road can convert pressure exerted by moving vehicles into voltage and current.\n\nPiezoelectric sensors are most useful in Smart-road technologies that can be used to create systems that are intelligent and improve productivity in the long run. Imagine highways that alert motorists of a traffic jam before it forms. Or bridges that report when they are at risk of collapse, or an electric grid that fixes itself when blackouts hit. For many decades, scientists and experts have argued that the best way to fight congestion is intelligent transportation systems, such as roadside sensors to measure traffic and synchronized traffic lights to control the flow of vehicles. But the spread of these technologies has been limited by cost. There are also some other smart-technology shovel ready projects which could be deployed fairly quickly, but most of the technologies are still at the development stage and might not be practically available for five years or more.\n\nThe pyroelectric effect converts a temperature change into electric current or voltage. It is analogous to the piezoelectric effect, which is another type of ferroelectric behavior. Pyroelectricity requires time-varying inputs and suffers from small power outputs in energy harvesting applications due to its low operating frequencies. However, one key advantage of pyroelectrics over thermoelectrics is that many pyroelectric materials are stable up to 1200 ⁰C or higher, enabling energy harvesting from high temperature sources and thus increasing thermodynamic efficiency.\n\nOne way to directly convert waste heat into electricity is by executing the Olsen cycle on pyroelectric materials. The Olsen cycle consists of two isothermal and two isoelectric field processes in the electric displacement-electric field (D-E) diagram. The principle of the Olsen cycle is to charge a capacitor via cooling under low electric field and to discharge it under heating at higher electric field. Several pyroelectric converters have been developed to implement the Olsen cycle using conduction, convection, or radiation. It has also been established theoretically that pyroelectric conversion based on heat regeneration using an oscillating working fluid and the Olsen cycle can reach Carnot efficiency between a hot and a cold thermal reservoir. Moreover, recent studies have established polyvinylidene fluoride trifluoroethylene [P(VDF-TrFE)] polymers and lead lanthanum zirconate titanate (PLZT) ceramics as promising pyroelectric materials to use in energy converters due to their large energy densities generated at low temperatures. Additionally, a pyroelectric scavenging device that does not require time-varying inputs was recently introduced. The energy-harvesting device uses the edge-depolarizing electric field of a heated pyroelectric to convert heat energy into mechanical energy instead of drawing electric current off two plates attached to the crystal-faces.\n\nIn 1821, Thomas Johann Seebeck discovered that a thermal gradient formed between two dissimilar conductors produces a voltage. At the heart of the thermoelectric effect is the fact that a temperature gradient in a conducting material results in heat flow; this results in the diffusion of charge carriers. The flow of charge carriers between the hot and cold regions in turn creates a voltage difference. In 1834, Jean Charles Athanase Peltier discovered that running an electric current through the junction of two dissimilar conductors could, depending on the direction of the current, cause it to act as a heater or cooler. The heat absorbed or produced is proportional to the current, and the proportionality constant is known as the Peltier coefficient. Today, due to knowledge of the Seebeck and Peltier effects, thermoelectric materials can be used as heaters, coolers and generators (TEGs).\n\nIdeal thermoelectric materials have a high Seebeck coefficient, high electrical conductivity, and low thermal conductivity. Low thermal conductivity is necessary to maintain a high thermal gradient at the junction. Standard thermoelectric modules manufactured today consist of P- and N-doped bismuth-telluride semiconductors sandwiched between two metallized ceramic plates. The ceramic plates add rigidity and electrical insulation to the system. The semiconductors are connected electrically in series and thermally in parallel.\n\nMiniature thermocouples have been developed that convert body heat into electricity and generate 40μW at 3V with a 5 degree temperature gradient, while on the other end of the scale, large thermocouples are used in nuclear RTG batteries.\n\nPractical examples are the finger-heartratemeter by the Holst Centre and the thermogenerators by the Fraunhofer Gesellschaft.\n\nAdvantages to thermoelectrics:\n\nOne downside to thermoelectric energy conversion is low efficiency (currently less than 10%). The development of materials that are able to operate in higher temperature gradients, and that can conduct electricity well without also conducting heat (something that was until recently thought impossible ), will result in increased efficiency.\n\nFuture work in thermoelectrics could be to convert wasted heat, such as in automobile engine combustion, into electricity.\n\nThis type of harvesting is based on the changing capacitance of vibration-dependent capacitors. Vibrations separate the plates of a charged variable capacitor, and mechanical energy is converted into electrical energy.\nElectrostatic energy harvesters need a polarization source to work and to convert mechanical energy from vibrations into electricity. The polarization source should be in the order of some hundreds of volts; this greatly complicates the power management circuit. Another solution consists in using electrets, that are electrically charged dielectrics able to keep the polarization on the capacitor for years.\nIt's possible to adapt structures from classical electrostatic induction generators, which also extract energy from variable capacitances, for this purpose. The resulting devices are self-biasing, and can directly charge batteries, or can produce exponentially growing voltages on storage capacitors, from which energy can be periodically extracted by DC/DC converters.\n\nMagnets wobbling on a cantilever are sensitive to even small vibrations and generate microcurrents by moving relative to conductors due to Faraday's law of induction. By developing a miniature device of this kind in 2007, a team from the University of Southampton made possible the planting of such a device in environments that preclude having any electrical connection to the outside world. Sensors in inaccessible places can now generate their own power and transmit data to outside receivers.\n\nOne of the major limitations of the magnetic vibration energy harvester developed at University of Southampton is the size of the generator, in this case approximately one cubic centimeter, which is much too large to integrate into today's mobile technologies. The complete generator including circuitry is a massive 4 cm by 4 cm by 1 cm nearly the same size as some mobile devices such as the iPod nano. Further reductions in the dimensions are possible through the integration of new and more flexible materials as the cantilever beam component. In 2012, a group at Northwestern University developed a vibration-powered generator out of polymer in the form of a spring. This device was able to target the same frequencies as the University of Southampton groups silicon based device but with one third the size of the beam component.\n\nA new approach to magnetic induction based energy harvesting has also been proposed by using ferrofluids. The journal article, \"Electromagnetic ferrofluid-based energy harvester\", discusses the use of ferrofluids to harvest low frequency vibrational energy at 2.2 Hz with a power output of ~80 mW per g.\n\nCommercially successful vibration energy harvesters based on magnetic induction are still relatively few in number. Examples include products developed by Swedish company ReVibe Energy, a technology spin-out from Saab Group. Another example is the products developed from the early University of Southampton prototypes by Perpetuum. These have to be sufficiently large to generate the power required by wireless sensor nodes (wsn)but in M2M applications this is not normally an issue. These harvesters are now being supplied in large volumes to power wsn's made by companies such as GE and Emerson and also for train bearing monitoring systems made by Perpetuum.\nOverhead powerline sensors can use magnetic induction to harvest energy directly from the conductor they are monitoring.\n\nAnother way of energy harvesting is through the oxidation of blood sugars. These energy harvesters are called biobatteries. They could be used to power implanted electronic devices (e.g., pacemakers, implanted biosensors for diabetics, implanted active RFID devices, etc.). At present, the Minteer Group of Saint Louis University has created enzymes that could be used to generate power from blood sugars. However, the enzymes would still need to be replaced after a few years. In 2012, a pacemaker was powered by implantable biofuel cells at Clarkson University under the leadership of Dr. Evgeny Katz.\n\nTree metabolic energy harvesting is a type of bio-energy harvesting. Voltree has developed a method for harvesting energy from trees. These energy harvesters are being used to power remote sensors and mesh networks as the basis for a long term deployment system to monitor forest fires and weather in the forest. According to Voltree's website, the useful life of such a device should be limited only by the lifetime of the tree to which it is attached. A small test network was recently deployed in a US National Park forest.\n\nOther sources of energy from trees include capturing the physical movement of the tree in a generator. Theoretical analysis of this source of energy shows some promise in powering small electronic devices. A practical device based on this theory has been built and successfully powered a sensor node for a year.\n\nA metamaterial-based device wirelessly converts a 900 MHz microwave signal to 7.3 volts of direct current (greater than that of a USB device). The device can be tuned to harvest other signals including Wi-Fi signals, satellite signals, or even sound signals. The experimental device used a series of five fiberglass and copper conductors. Conversion efficiency reached 37 percent. When traditional antennas are close to each other in space they interfere with each other. But since RF power goes down by the cube of the distance, the amount of power is very very small. While the claim of 7.3 volts is grand, the measurement is for an open circuit. Since the power is so low, there can be almost no current when any load is attached.\n\nThe change in air pressure due to temperature changes or weather patterns vs. a sealed chamber has been used to provide power for mechanical clocks such as the Atmos clock.\n\nWhat is an Atmospheric pressure change?\n\nAn Atmospheric change is a force exerted on a surface by air above as gravity pulls it to Earth. This is often measured by the barometer, a slight mercury in the glass tube rises and drops depending on changes in altitude and pressure. Usually, atmosphere pressure drops as altitude increases, also vice versa. Atmospheric pressure consists of molecules in constant motion, colliding with each other about 10 collisions per molecule per second at 10 °C (50 °F), this is part of the kinetic energy of the atmosphere. With given devices/technology (Wind Turbine) in the contemporary era, industries/individuals can harvest or convert atmospheric energy into Electrical energy with low-cost efficient or even free! Overall an Atmospheric pressure is a great utility that can be devised into our economy for top-tier energy harvest and future solution for many other issues.\n\nA relatively new concept of generating energy is to generate energy from oceans. Large masses of waters are present on the planet which carry with them great amounts of energy. The energy in this case can be generated by tidal streams, ocean waves, difference in salinity and also difference in temperature. Efforts are underway to harvest energy this way as it holds a great potential and would be a renewable form of energy. United States Navy recently was able to generate electricity using difference in temperatures present in the ocean. The project didn't yield that much energy but sure there is a lot of potential.\n\nAnother idea is to generate an artificial flow in ocean to generate energy. If a flow of considerable magnitude could be generated it would produce large amounts of energy.\n\nElectroactive polymers (EAPs) have been proposed for harvesting energy. These polymers have a large strain, elastic energy density, and high energy conversion efficiency. The total weight of systems based on EAPs(electroactive polymers) is proposed to be significantly lower than those based on piezoelectric materials.\n\nNanogenerators, such as the one made by Georgia Tech, could provide a new way for powering devices without batteries. As of 2008, it only generates some dozen nanowatts, which is too low for any practical application.\n\nNoise has been the subject of a proposal by NiPS Laboratory in Italy to harvest wide spectrum low scale vibrations via a nonlinear dynamical mechanism that can improve harvester efficiency up to a factor 4 compared to traditional linear harvesters.\n\nCombinations of different types \n"}
{"id": "56619590", "url": "https://en.wikipedia.org/wiki?curid=56619590", "title": "Fatty acid photodecarboxylase", "text": "Fatty acid photodecarboxylase\n\nFatty acid photodecarboxylase is an enzyme that converts fatty acids to alkanes or alkenes, which can then, in principle, be used as biofuel. The enzyme has been found in \"Chlorella variabilis\".\n"}
{"id": "21064516", "url": "https://en.wikipedia.org/wiki?curid=21064516", "title": "Fiberglass sheet laminating", "text": "Fiberglass sheet laminating\n\nFiberglass sheet laminating is the process of taking a thin fiberglass sheet and laminating it to another material in order to provide strength and support to that material.\n\nFiberglass is composed of very fine strands of glass. It has many different purposes, one of which is used for strength. The strength of fiberglass depends on the size of the glass strands, the temperature, and the humidity.\n\nFiberglass sheet, resin, wood or metal roller, brush or other tool to spread epoxy, material to be strengthened\n\nStart by applying the epoxy to the fiberglass sheet. Continue carefully but quickly until all areas are sufficiently covered by the epoxy. Next, start at one end of the material to be strengthened and stick the epoxy covered fiberglass to the material, being sure to smooth out any bubbles that may form between the material and fiberglass. If the epoxy hardens before you are able to stick the fiberglass to the material, recoat and apply again. After the fiberglass sheet has been applied, use a roller to press the fiberglass firmly to the other sheet to ensure complete bonding has occurred.\n\nCertain laminating techniques use two steps of applying the epoxy to form resin impregnated fiber glass sheets. In the first step there is a resin solvent mixture which is partially cured so it will not redissolve in a second coating of the same mixture. The same resin mixture is subsequently given to the covered fiberglass with moderately cured resin in the second step. This second glaze which covers the first fills in the empty spaces between the fibers. The second coating is also only partially cured. This partial curing of the second layer furthers the curing of the first epoxy layer. This process also produces a thin sticky layer. The first coating acts like a sealed insulating sheet, preventing glass fiber contact with conductive planes. The second coating fills the planes and can form adhesive bonds to cores and conductive layers.\n\n\n"}
{"id": "37821607", "url": "https://en.wikipedia.org/wiki?curid=37821607", "title": "Field-induced polymer electroluminescent technology", "text": "Field-induced polymer electroluminescent technology\n\nField-induced polymer electroluminescent (FIPEL) technology is a low power electroluminescent light source. Three layers of moldable light-emitting polymer blended with a small amount of carbon nanotubes glow when an alternating current is passed through them. The technology can produce white light similar to that of the Sun, or other tints if desired. It is also more efficient than compact fluorescent lamps in terms of the energy required to produce light. As cited from the Carroll Research Group at Wake Forest University, \"To date our brightest device - without output couplers - exceeds 18,000 cd/m2.\" This confirms that FIPEL technology is a viable solution for area lighting.\n\nFIPEL lights are different from LED lighting, in that there is no junction. Instead, the light emitting component is a layer of polymer containing an iridium compound which is doped with multi-wall carbon nanotubes. This planar light emitting structure is energized by an AC field from insulated electrodes. The lights can be shaped into many different forms, from mimicking conventional light bulbs to unusual forms such as 2-foot-by-4-foot flat sheets and straight or bent tubes. The technology was developed by a team headed by Dr. David Carroll of Wake Forest University in Winston-Salem, North Carolina.\n\n"}
{"id": "39148606", "url": "https://en.wikipedia.org/wiki?curid=39148606", "title": "Foreign exchange aggregator", "text": "Foreign exchange aggregator\n\nA foreign exchange aggregator or FX Aggregator is a class of systems used in Forex trading to aggregate the liquidity from several liquidity providers.\n\nAggregators usually provide two main functions; they allow FX traders to compare price from different liquidity venues such as banks-global market makers or ECNs like Currenex, FXall or Hotspot FX and to have a consolidated view of the market. They allow traders to trade with many participants using a single API or a single trading terminal. Some of the systems support order sweeping (an order is split into the chunks which are sent to the respective counterparties based on the price, time and other attributes of the quotes from these counterparties), other systems route the whole order to a single liquidity provider who is chosen by an order routing algorithm embedded into an aggregator.\n\nFX Aggregator implementation is complex as the technology needs to be fast (Latencies in microseconds) and flexible.\nSome banks developed their own FX Aggregators and others bought existing products from technology vendors.\n\nThere are many aggregators offered in the market: smartTrade LiquidityFX, Thomson Reuters Dealing Aggregator, Liquid-X, Liquidity Pool, FlexTrade, BidFX, Quotix, Integral, Currenex, LMAX Exchange, MarketFactory, EBS Direct, DealHub, Seamless FX, Gold-i Matrix and others.\n"}
{"id": "27019598", "url": "https://en.wikipedia.org/wiki?curid=27019598", "title": "Gangway connection", "text": "Gangway connection\n\nA gangway connection (or, more loosely, a corridor connection) is a flexible connector fitted to the end of a railway coach, enabling passengers to move from one coach to another without danger of falling from the train.\n\nThe London and North Western Railway (LNWR) was the first British railway to provide passengers with the means to move from one coach to another whilst the train was in motion. In 1869 the LNWR built a pair of saloons for the use of Queen Victoria; these had six-wheel underframes (the bogie coach did not appear in Britain until 1874), and the gangway was fitted to only one end of each coach. The Queen preferred to wait until the train had stopped before using the gangway.\nIn 1887, George M. Pullman introduced his patented vestibule cars. Older railroad cars had open platforms at their ends, which were used both for joining and leaving the train, but could also be used to step from one car to the next. This practice was dangerous, and so Pullman decided to enclose the platform to produce the vestibule. For passing between cars, there was a passageway in the form of a steel-framed rectangular diaphragm mounted on a buffing plate above the centre coupler. The vestibule prevented passengers from falling out, and protected passengers from the weather when passing between cars. In the event of an accident, the design also helped prevent cars from overriding each other, reducing the risk of telescoping. Pullman's vestibule cars were first used in 1887. Among the first to use them was the Pennsylvania Railroad on the \"Pennsylvania Limited\" service to Chicago.\n\nThe Great Northern Railway introduced the Gould-design gangway connection to Great Britain in 1889, when E.F. Howlden was Carriage and Wagon Superintendent.\n\nOn 7 March 1892, the Great Western Railway (GWR) introduced a set of gangwayed coaches on their to Birkenhead service. Built to the design of William Dean, it was the first British side-corridor train to have gangway connections between all the coaches, although they were provided not to enable passengers to move around the train, but rather to allow the guard to reach any compartment quickly. Electric bells were provided so that he could be summoned. When the guard was not so required, he kept the communicating doors locked. Passengers could still use the side-corridor within the coach to reach the toilet. The gangway connections of the early GWR corridor coaches were offset to one side. Some coaches intended for use at the ends of trains had the gangway connection fitted at one end only. The GWR introduced restaurant cars in 1896; gangway connections were fitted, but passengers wishing to use the restaurant car were expected to board it at the start of their journey, and remain there: the connections were still not for public use.\nOn 17 May 1923, the GWR introduced some new coaches on their South Wales services; some of these coaches had British Standard gangway connections and screw couplers as used on many other GWR coaches; some had Pullman-type gangway connections and Laycock \"buckeye\" couplers; and there were some with one type at one end, and the other end having the other type. In 1925 the GWR started to use the \"suspended\" form of gangway connection instead of the \"scissors\" pattern. From 1938, GWR coaches which were expected to need coupling to LNER or SR coaches were fitted with gangway adaptors, to allow the dissimilar types to be connected.\n\nFrom the beginning, the London, Midland and Scottish Railway used the British Standard type of gangway connector, with its \"scissors\" pattern as used by the GWR. Some coaches needed for LNER or SR lines were given gangway adaptors, so that they could safely couple to coaches fitted with the Pullman-type gangway.\n\nOn the formation of British Railways on 1 January 1948, operators decided to produce a new range of standard coaches, instead of perpetuating existing designs—but the new types had to be compatible with the old. Two of the pre-BR companies (the GWR and the London, Midland and Scottish Railway) favoured the British Standard gangway, whereas the other two (the London and North Eastern Railway and the Southern Railway) used the Pullman type. In the design of their new Mark 1 coaches, British Railways decided to standardise on the Pullman type in view of its resistance to telescoping. These gangways consisted of a flat steel plate, having a large aperture for the passageway. At the bottom it was riveted to the buffing plate, whilst the top was supported on the coach end by two telescopic spring units. On the coach end was a wooden doorframe; this was connected to the faceplate by a flexible diaphragm made from plasticised asbestos. When two coaches were coupled, a curtain was used to cover the inside surfaces of the diaphragms and faceplates. The doorframe was fitted with a lockable door, of either sliding or hinged type, depending on the interior layout of that end of the coach.\n\nCoaches built for the Travelling Post Office (TPO) services normally had their gangway connections offset to one side. There were two main reasons: there was a perceived security risk should these coaches be coupled to ordinary passenger-carrying coaches, the differing gangway positions minimising the risk of intrusion; and more space was available for sorting tables, the postal workers being able to walk in a straight line between vans without disturbing the sorters. A disadvantage was that when a van was added to a TPO train, it might need to be turned around before it could be used. After the formation of British Railways, most new Mark 1 TPO vans were provided with centre gangways, though a batch intended to work with older vans were given offset gangways. These were altered to the standard arrangement in 1973. Until then, they had been the only BR Mark 1 gangwayed coaches not to have the Pullman gangway.\n\nThe London and North Eastern Railway (LNER) decided that from the start of their summer timetable on 1 May 1928, the \"Flying Scotsman\" service would run non-stop over the between and . The locomotives to be used were of that railway's class A1, and the schedule was for the journey to be completed in hours. This was too long to allow a single crew to handle the train without a rest; means were therefore sought by which the crew could be changed at approximately the half-way point.\nThe LNER's locomotive design team, headed by Nigel Gresley, produced a new design of tender that was slightly longer than the old, but built as high and wide as possible without compromising the loading gauge. A passageway was incorporated along the right-hand side, and at the rear end a Pullman type gangway connection was fitted, together with a buckeye coupler, both of which were compatible with LNER coaches; the gangway was of concertina pattern, and was pressed against the corresponding gangway on the leading coach by means of sprung pistons. Although a normal gangway connection was used, the passageway through the tender was only high and wide, and the floor of the passage was above the bottom of the water tank, giving a high step at both ends. The passageway was illuminated by a single circular window in the tender rear panel, placed high up and to the right of the corridor connection. Ten of these tenders were placed in service between April and September 1928, of which three were attached to new locomotives of Class A3; two were attached to existing Class A3 locomotives, and five attached to Class A1 locomotives. The design was patented by Gresley in August 1928.\n\nIn service, the relief crew travelled in the front coach of the train, and as the train approached the half-way point, they left their seats and made their way forward through the corridor tender to the locomotive cab. On their arrival, the previous crew then handed over the controls and went back to the seats in the train vacated by the relief crew.\n\nAnother corridor tender was built in 1929 for use with the new Class W1 4-6-4 no. 10000; four more were built in 1935 with the first four locomotives of the new Class A4, and a final seven were built with the 1937 batch of Class A4 locomotives, making a total of 22. The original ten were reconditioned in 1936–1937 and attached to other Class A4 locomotives. In May 1948, the 1929-built corridor tender was transferred to a locomotive of Class A4, after which all 22 remained with this class until withdrawal.\n\nIn the USA, the Milwaukee Road class A 4-4-2s of 1935 built for the hour \"Hiawatha\" express also used a corridor tender.\n\nIn urban transit, open gangways are most commonly used in light rail and streetcars, where the railcars are divided into two or more sections linked by gangways. Articulated buses similarly have extensions connected with a gangway. Open gangways have also become increasingly used for heavy rail rapid transit rolling stock. Bombardier's Movia design is an example of an open gangway heavy rail car, which operates on a variety of subway systems around the world. It provides a way to seamlessly move between cars at any time, without passing through doors and a dangerous open area that is often against the rules. It also raises the capacity of metro cars by about 10%, a significant improvement for systems such as the New York City Subway where infrastructure and timetables are near or at capacity.\n\nThe Toronto Transit Commission (TTC) was the first North American transit authority to use open gangway rolling stock, with its Toronto Rocket railcars delivered to the Toronto subway system from 2010. The Société de Transport de Montréal (STM) upgraded its fleet of subway cars with Azur (MPM-10) trains that included open gangways. The Metropolitan Transportation Authority (MTA) is considering the design for the New York City Subway's planned R211 order. Initially the order will consist of 545 cars, of which 20 will be in experimental open gangway formations. The order includes an option to purchase up to 640 additional open gangway cars, the decision of which will be based on the success of the prototype set.\n\nA Walk-through head is a type of gangway connection that is installed in a train set that is intended to enable the passage from one train to the next, when they are interconnected.\n\nWith most trains, it is possible, as similar to a locomotive towed train, to walk from one carriage to another, but a passage between the trains is less common.\n\n\n"}
{"id": "637228", "url": "https://en.wikipedia.org/wiki?curid=637228", "title": "Hulett", "text": "Hulett\n\nThe Hulett was an ore unloader that was widely used on the Great Lakes of North America. It was unsuited to tidewater ports because it could not adjust for rising and falling tides, although one was used in New York City.\n\nThe Hulett was invented by George Hulett of Ohio in the late 19th century; he received a patent for his invention in 1898. The first working machine was built the following year at Conneaut Harbor in Conneaut, Ohio. It was successful, and many more were built along the Great Lakes, especially the southern shore of Lake Erie to unload boats full of taconite from the iron mines near Lake Superior. Substantial improvements were later made on the design by Samuel T. Wellman.\n\nThe Hulett machine revolutionised iron ore shipment on the Great Lakes. Previous methods of unloading lake freighters, involving hoists and buckets and much hand labor, cost approximately 18¢/ton. Unloading with Huletts cost only 5¢/ton. Unloading only took 5–10 hours, as opposed to days for previous methods. Lake boats changed to accommodate the Hulett unloader, and became much larger, doubling in length and quadrupling in capacity.\n\nBy 1913, 54 Hulett machines were in service. Two were built at Lake Superior (unloading coal) and five at Gary, Indiana, but the vast majority were along the shores of Lake Erie. The additional unloading capacity they brought helped permit a greater than doubling of the ore traffic in the 1900–1912 period. A total of approximately 75 Huletts were built. One was installed in New York City to unload garbage. \n\nThe lake's Huletts were used until about 1992, when self-unloading boats were standard on the American side of the lake. All have since been scrapped. In 1999, only six remained, the group of four at Whiskey Island in Cleveland, Ohio the oldest. Another set was used unloading barges of coal in South Chicago until 2002 and were demolished in the Spring of 2010. \n\nIn spite of the Cleveland machines being on the National Register of Historic Places and designated as a Historic Mechanical Engineering Landmark, they were demolished in 2000 by the Cleveland Port Authority to enable development of the underlying land. The Port Authority disassembled and retained two Huletts, to enable their reconstruction at another site, but the reconstruction has not yet happened.\n\nThe electrically powered Hulett unloader rode on two parallel tracks along the docks, one near the edge and one further back, ordinarily with four railroad tracks in between. Steel towers, riding on wheeled trucks, supported girders that spanned the railroad tracks. \n\nAlong these girders ran a carriage which could move toward or away from the dock face. This in turn carried a large walking beam which could be raised or lowered; at the dock end of this was a vertical column with a large scoop bucket on the end. A parallel beam was mounted halfway down this column to keep the column vertical as it was raised or lowered. The machine's operator, stationed in the vertical beam above the bucket for maximum cargo visibility, could spin the beam at any angle. The scoop bucket was lowered into the ship's hold, closed to capture a quantity (10 tons approx.) of ore, raised, and moved back toward the dock. The workmen who operated the Hulett uploaders were known as Ore Hogs. \n\nTo reduce the required motion of the carriage, a moving receiving hopper ran between the main girders. It was moved to the front for the main bucket to discharge its load, and then moved back to dump it into a waiting railroad car, or out onto a cantilever frame at the back to dump the load onto a stockpile.\n\nThe Hulett could move along the dock to align with the holds on an ore boat. When the hold was almost empty, the Hulett could not finish the job itself. Workmen entered the hold and shoveled the remaining ore into the Hulett's bucket. In a later development, a wheeled excavator was chained to the Hulett's bucket and lowered into the hold to fill the Hulett.\n\n"}
{"id": "2846234", "url": "https://en.wikipedia.org/wiki?curid=2846234", "title": "Jeff Prosise", "text": "Jeff Prosise\n\nJeff Prosise is a technical author on Microsoft Windows applications. He is very experienced in Microsoft Windows technologies like MFC, .NET framework, C# and others.\n\nJeff Prosise is the author of the book \"Programming Windows with MFC\", published by Microsoft Press. His book is about the MFC, a C++ based proprietary programming library for Microsoft Windows. This book, along with Charles Petzold's popular \"Programming Windows\" book are considered to be 'bibles' in Windows programming. While Petzold's book is known as the 'Windows API bible', Prosise's book is often referred to as the 'MFC bible'.\n\nSome of his other publications are \"Programming Microsoft .NET\", \"Programming Windows with C#\" and \"How Computer Graphics Work\" (co-author Gary Suen). His previous books, published back in the 1980s and 1990s, included \"PC Magazine DOS 6 Techniques And Utilities\", \"PC Magazine DOS 6 Memory Management With Utilities\", and \"Windows Desktop Utilities\".\n\n"}
{"id": "13452838", "url": "https://en.wikipedia.org/wiki?curid=13452838", "title": "Joseph Gayetty", "text": "Joseph Gayetty\n\nJoseph C. Gayetty (b. 1827? Massachusetts - d.__ ) was an American inventor credited with the invention of commercial toilet paper. It was the first and remained only one of the few commercial toilet papers from 1857 to 1890 remaining in common use until the invention of splinter-free toilet paper in 1935 by the Northern Tissue Company.\n\nUnited States Census records from 1860 show Gayetty lived in New York City with his family; his birthplace in the census records is listed as Pennsylvania.\n\nHe first marketed toilet paper on December 8, 1857. Each sheet of pure Manila hemp paper was watermarked \"J C Gayetty N Y\". The original product contained aloe as a lubricant and was marketed as an anti-hemorrhoid medical product.\n\nGayetty was attacked as a quack by at least one medical society. Yet his advertisement of the same year called his product \"The Greatest Necessity of the Age\" and warned against the perils of using toxic inked papers on sensitive body parts. A different advertisement, also printed in 1859, says his business was located at 41 Ann Street, and he was selling 1,000 sheets for one dollar.\n\nThe Gayetty name and product were involved in a lawsuit that was filed in 1891, when B.T. Hoogland's Sons, toilet paper dealers, filed suit against the Gayetty Paper Company, specifically Harry K. Gayetty, for trademark infringement. B.T. Hoogland and Son's claim was that they were entitled to the use of the Gayetty name due to an unpaid debt. A paper dated December 5, 1866, was allegedly given to a creditor in lieu of $25 debt and subsequently sold to B.T. Hoogland (senior) for one dollar. However, on January 1, 1866, J.C. Gayetty had entered a ten-year contract for the exclusive right to sell and vend in his name with Demas Barnes and Company, which had taken out a copyright on the product on October 27, 1891. The suit was dismissed in 1894, but another suit was brought. B.T. Hoogland's Sons next sued to stop Harry K. Gayetty and the Diamond Mills Paper Company from using the Gayetty name, and in this case they were successful. Harry Gayetty appealed, but lost at the Appellate Court. Finally in July 1900, the New York Supreme Court permanently enjoined the Diamond Mills Paper Company and Harry K. Gayetty from using the name on any similar paper product labels.\n\nIn 1900, an advertisement shows that B.T. Hoogland's Sons of New York were distributing the watermarked \"Papel Medicado De Gayetty\" and giving credit to the invention of the paper in 1857 by Joseph C. Gayetty, Inventor. Nearly the same advertisement was run in English in 1907. The product continued to be marketed until the 1920s.\n"}
{"id": "7582832", "url": "https://en.wikipedia.org/wiki?curid=7582832", "title": "Karl Steinbuch", "text": "Karl Steinbuch\n\nKarl W. Steinbuch (June 15, 1917 in Stuttgart-Bad Cannstatt – June 4, 2005 in Ettlingen) was a German computer scientist, cyberneticist, and electrical engineer. He was an early and influential researcher of German computer science, and was the developer of the Lernmatrix, an early implementation of artificial neural networks. Steinbuch also wrote about the societal implications of modern media.\n\nSteinbuch studied at the University of Stuttgart and in 1944 he received his PhD in physics. In 1948 he joined (SEL, part of the ITT group) in Stuttgart, as a computer design engineer and later as a director of research and development, where he filed more than 70 patents. There Steinbuch completed the first European fully transistorized computer, the ER 56 marketed by SEL. In 1958 he became professor and director of the institute of technology for information processing (ITIV) of the University of Karlsruhe, where he retired in 1980.\n\nIn 1967 he began publishing books, in which he tried to influence German education policy. Together with books from colleagues like Jean Ziegler from Switzerland, Eric J. Hobsbawm from UK, and John Naisbitt his books predicted what he regarded as the coming education disaster of the emerging civic lobby society.\n\nSteinbuch coined in 1957 the term \"Informatik\", the German word for computer science, which gave \"informatics\", and the term ().\n\n\nSteinbuch wrote several books and articles, including:\n\n"}
{"id": "38462667", "url": "https://en.wikipedia.org/wiki?curid=38462667", "title": "Lead market", "text": "Lead market\n\nLead Market is a term used in innovation theory and denotes a country or region, which pioneers the successful adoption of an innovative design. It sends signalling effects to other \"lag\" markets, which in turn helps in triggering a process of global diffusion. Marian Beise, one of the foremost propounders of this theory as it has been understood so far, states: \"Innovations that have been successful with local users in lead markets have a higher potential of becoming adopted world-wide than any other design preferred in other countries\". Christoph Bartlett and Sumantra Ghoshal have described lead markets as \"[…] markets that provide the stimuli for most global products and processes of a multinational company. Local innovations in such markets become useful elsewhere as the environmental characteristics that stimulated such innovations diffuse to other locations\". To illustrate a lead market with some examples, Germany can be seen as a lead market for renewable energies and (premium) automobiles, while the United States would suit as a lead market for information technologies including e-commerce.\n\nLead markets are thought to help reduce market and technology uncertainty in the process of new product development; and can be seen as an important driver for internationalization of research and development (R&D).\n\nLead markets have been supposed to be characterized by factors such as high per capita income, high level of customer sophisitication, and a competitive \"eco-system\". Such factors are subsumed into 5 \"groups of advantages\", namely demand advantage, market structure advantage, cost advantage, export advantage, and transfer advantage. Some scholars have also spoken of a regulation advantage, which emanates from favourable government policies (cf. Rennings and Smidt, 2010). Nevertheless, some other scholars have pointed towards regulation's impact on all other factors. For example, Tiwari and Herstatt (2012: 100) state: \"[...] we consider it appropriate, not to treat regulation as a separate group since policy factors influence all other groups of advantages and are implicitly covered by them.\"\n\nThe theory of lead markets has been criticized for not being \"consistent and stringent\". While conventional wisdom, due to its emphasis on customer affluence and sophistication as inducers of innovation, has tended to see lead markets exist in economically highly developed countries; recent research, notably by Rajnish Tiwari and Cornelius Herstatt of Hamburg University of Technology (e.g. Tiwari and Herstatt, 2011a, Tiwari and Herstatt, 2011b), has emphasized the need to update/extend the lead market model to adjust it to the changing ground realities in the face of globalization and sustained economic growth in developing nations. Using examples of several frugal innovations from India, and more specifically the case of small car industry in India's automotive sector, Rajnish Tiwari in his dissertation at Hamburg University of Technology (TUHH) has proposed that lead markets can emerge in developing nations as well provided the market size enables significant economies of scale and the country's innovation system is endowed with the necessary technological capabilities. These two conditions, seen in conjunction with several other factors such as embeddedness in the international trade and access to open global innovation networks (OGINs), can in many instances compensate other typical drawbacks of developing countries, such as low purchasing power. As a consequence, Tiwari's dissertation has proposed to update the model to include technology advantage in the model, while combining the export and transfer advantages as one single group. Factors within the individual groups of advantages have been also update/modified. The evolution of India's small car sector has also provided some valuable insights into possible emergence of a lead market in a country. This model, if validated by further research, would help enable an ex ante analysis of lead markets, whereas so far mostly ex post, macroeconomic analyses have been the norm.\n\n"}
{"id": "45353040", "url": "https://en.wikipedia.org/wiki?curid=45353040", "title": "List of Brazilian inventions and discoveries", "text": "List of Brazilian inventions and discoveries\n\nBrazilian inventions and discoveries are items, processes, techniques or discoveries which owe their existence either partially or entirely to a person born in Brazil or to a citizen of Brazil.\n\n"}
{"id": "5814331", "url": "https://en.wikipedia.org/wiki?curid=5814331", "title": "Martin Tajmar", "text": "Martin Tajmar\n\nTajmar completed his PhD in numerical plasmaphysics at the Vienna University of Technology, Austria, in 1999, and is now an external lecturer for the university. He also published the textbook \"Advanced Space Propulsion Systems\" in 2003.\n\nIn a 2003 paper, Tajmar proposed that a gravitational effect may explain the long-standing discrepancy between the mass of Cooper pairs first measured in superconductors by Janet Tate \"et al.\" and the theoretically-expected value.\n\nIn 2006 Tajmar and several coworkers announced their claim to have measured a gravitomagnetic version of the frame-dragging effect caused by a superconductor with an accelerating or decelerating spin. As of April 2008, the effect has not yet been observed independently.\n\nIn February 2008 Tajmar filed an international patent application for a \"Process for the generation of a gravitational field and a gravitational field generator.\"\n\nIn June 2008, Tajmar reported a new phenomenon suggesting that signals could be induced in a gyroscope resulting from a new property of rotating low-temperature helium. He also reported that because the rings in the experiment were accelerated pneumatically, and not with high acceleration, the earlier reported results could not be discounted. His further research suggests the anomaly may indeed be coming from liquid helium in the setup.\n\n\n"}
{"id": "38495380", "url": "https://en.wikipedia.org/wiki?curid=38495380", "title": "Mechanical, electrical, and plumbing", "text": "Mechanical, electrical, and plumbing\n\nMechanical, electrical and plumbing (MEP) refers to these aspects of building design and construction. In commercial buildings these aspects are often designed by an engineering firm specializing in MEP. MEP design is important for design decision-making, accurate documentation, performance and cost-estimation, construction planning, managing and operating the resulting facility. \n\nAs with other aspect of buildings, MEP drafting, design and documentation were traditionally done manually. Computer-aided design had some advantages over this, in some instances including 3D modeling. Building information modeling provides holistic design and parametric change management of the MEP design.\n\n"}
{"id": "49905143", "url": "https://en.wikipedia.org/wiki?curid=49905143", "title": "Medical Unit, Self-contained, Transportable", "text": "Medical Unit, Self-contained, Transportable\n\nMedical Unit, Self-contained, Transportable (MUST) was a type of medical equipment system developed for field hospitals in the United States Army in the late 1950s and early 1960s. The system used inflatable shelters for ward and patient care space, and expandable shelters for operating rooms and other sections. They were powered by auxiliary power units which used JP-4 as fuel, producing power and air conditioning for the hospital in addition to air to keep the shelters inflated. A 60 bed surgical hospital in Vietnam could use up to 3,000 gallons of JP-4 per day to keep the hospital inflated and operational.\n\nThe units were manufactured by Missouri Research Manufacturing Company, Inflated Products Company, Firestone Tire and Rubber Company, Brunswick Corporation, with spare parts supplied by Coats & Clark Company and Scoville Manufacturing Company (zippers,) Beckett Lace and Velcro (fasteners.)\n\nAccording to Major General Spurgeon Neel, a commander of the 44th Medical Brigade in South Vietnam:\n\nMUST-equipped surgical hospitals were operated for several years in Vietnam with mixed success. These units consisted of three basic elements, each of which could be airlifted and dispatched by truck or helicopter. The expandable surgical element was a self-contained, rigid-panel shelter with accordion sides. The air-inflatable ward element was a double-walled fabric shelter providing a free-space area for ward facilities. The utility element or power package contained a multifuel gas turbine engine which supplied electric power for air-conditioning, refrigeration, air heating and circulation, water heating and pumping, air pressure for the inflatable elements, and compressed air or suction. In addition, other expandables were used for central materiel supply, laboratory, X-ray, pharmacy, dental, and kitchen facilities.\n"}
{"id": "1336667", "url": "https://en.wikipedia.org/wiki?curid=1336667", "title": "Meteor (rocket)", "text": "Meteor (rocket)\n\nMeteor is a designation of a series of Polish sounding rockets. The Meteor rockets were built between 1963 and 1974.\n\nMeteor was the one and two stages meteorological rockets, using the solid fuel, constructed for the research of the top layers of terrestrial atmosphere, also directions and forces of winds from 18 to more than 50 km above the Earth surface. These rockets were designed by Polish engineers of Warsaw Aviation Institute (among them was Professor Jacek Walczewski and engineer Adam Obidziński) and had been produced by WZK-Mielec factory.\n\nThe first launching site of the sounding rockets in Poland was Błędowska Desert, where since 1958 to 1963, the rockets of different types had been launched; among others RD and Rasko. During a flight, the biological experiment with earlier trained two white mouses was conducted (the RM-2D rocket achieved the altitude of 1580 meters).\nSince 1965 to April 1970, the Meteor-1 rockets had been launched from \"spaceport\" located 5 km from Ustka town. This programme had been continued to 1974, when rockets were bearing out from the area of experimental center founded there during the years of the Second World War, located on west side of Łeba town. Currently, it is in the museum (the starting place with ramp and the radar bases).\n\nThere was 224 flights of \"Meteor-1\" rocket series (including prototypes). The valuable data both meteorological and connected with rocket techniques were found as the result of these researches.\n\nThe \"Meteor-1\", \"Meteor-2H\" and \"Meteor 2K\" (the largest civilian rocket developed in Poland) were single-stage rockets. The \"Meteor 3\" was a two-stage rocket, developed from \"Meteor 1\".\n\nMeteor rockets had been launched from Łeba and Ustka. Five Meteor rockets missions were conducted around 1970 from Zingst, in the former Eastern Germany. The programme of flights of Meteor-2 was finished during the same year, when Poland started to participate in Interkosmos research, using the Vertical rockets.\n\nOne stage, but two units rocket called \"Meteor-1\" had the length of 2470 mm and the initial mass of 32.5 kg. The flight lasted for 80 second and reaching at the peak altitude of 36.5 km. The motor ignite for 2–3 seconds and reached the maximum velocity of 1100 meters per second. In 1965, 6 rockets of \"Meteor-1\" type was launched and after this time: 12 in 1966, 40 in 1967, 45 in 1968, 36 in 1969, 34 in 1970 and 4 in 1971.\n\nThe charge of metal dipoles was released by rockets and this material had later been observed on radar screens. It was the base of derivation of winds strength in the stratosphere and winds directions in the same atmospheric layer. After experiments that had been conducted in the years 1965-1966, during \"The Year of the Quiet Sun\", a cyclic pattern of variation in case of directions of these atmospheric flows was concluded.\n\nThe one stage \"Meteor-2K\" was the most advanced version of the Meteor rocket. On 7 October 1970, the flight took place and reached the altitude of 90 kilometers. This rocket had been used as sounding of the ionosphere, reaching the level of boundary between D and E layers. 10 flights of this version were realized, when the measurements of temperature were made. The length of rocket frame is 4.5 meters and some are more longer than an English rocket, Petrel (in service since 1968). The \"Meteor-2\" had not been produced in serial way. The cost of this rocket prototype was eight times higher than for the copy of \"Meteor-1\". The weight of useful charge, in form of \"RAMZES\" recovery probe, is 10 kg.\n\nA two-stage rocket called \"Meteor-3\" was a developed version of \"Meteor-1\". The range of flight was increased and the rocket gained possibility of launching of few charges of dipoles. It can reach at ceiling altitudes between 67 and 74 kilometers. An idea of version \"S\" project had been considered. This model could be launch from airplane frame, about 5 kilometers above the surface.\n\n\"Meteor-4\" rocket has ten times more thrust than \"Meteor-2\". This version could reach above 100 km. According to design, this rocket is longer than 5 meters and had the initial total mass of 407 kg with useful weight of 10 kg. 175 seconds into the flight, it would reach at the altitude of 120 kilometers.\n\nAn incomplete list of Meteor launches\n\n"}
{"id": "41739172", "url": "https://en.wikipedia.org/wiki?curid=41739172", "title": "Ministry of Culture and Information (Saudi Arabia)", "text": "Ministry of Culture and Information (Saudi Arabia)\n\nThe Ministry of Culture and Information () is one of the governmental bodies of Saudi Arabia and part of the cabinet. The main function of the ministry is to regulate the media of Saudi Arabia and the communications between Saudi Arabia and other countries. It is headquartered in Riyadh.\n\nOn April 22, 2017, Dr. Awwad Alawwad was appointed to lead the Ministry. Dr Alawwad’s primary mandate is to revitalize the culture and media industries at home, support government communications abroad and strengthen Saudi Arabia’s cultural relations around the world.\n\nThe ministry was founded in 1962 as the ministry of information. In 2003, its portfolio was expanded to include cultural affairs and was renamed as the ministry of culture and information. \n\nIyad bin Amin Madani served in the post between February 2005 and 14 February 2009. His successor was Abdulaziz Khoja. Khoja's appointment was regarded as part of the then King Abdullah's reform initiatives. His tenure as the minister of culture and information ended on 29 January 2015 when Adel Al Toraifi was appointed to the post. \n\nThe ministry has \"responsibility for all the Saudi media and other channels of information\". The ministry has been called the \"main agent of censorship\" in the kingdom. It is charged with the purification of culture prior to it being permitted circulation to the public. A special unit, the management of publications department, \"analyzes all publications and issues directives to newspapers and magazines\" stating that way in which a given topic must be treated. \n\nCensorship is strict enough for works of the minister of culture and information himself: the former minister Abdulaziz Khojah's own works of poetry were banned in Saudi Arabia.\n\nThe ministry also oversees the activities of the following bodies: King Fahd Cultural Centre, Administration of Folklore, Saudi Society for Culture and Arts, General Administration of Cultural Activities and Literary Clubs, and General Administration for Literary Clubs. It is also responsible for the activities of the General Administration for Public Libraries and the General Administration for Cultural Relations. The Saudi Press Agency is also part of the ministry.\n\nIn London and Tunis, the ministry has information offices.\n\n"}
{"id": "44578211", "url": "https://en.wikipedia.org/wiki?curid=44578211", "title": "Ministry of Information Policy (Ukraine)", "text": "Ministry of Information Policy (Ukraine)\n\nThe Ministry of Information Policy () or MIP is a government ministry in Ukraine established on 2 December 2014. It was created concurrently with the formation of the Second Yatsenyuk Government, after the October 2014 Ukrainian parliamentary election. The ministry oversees information policy in Ukraine. According to the first Minister of Information, Yuriy Stets, one of the goals of its formation was to counteract \"Russian information aggression\" amidst pro-Russian unrest across Ukraine, and the ongoing Russian military intervention of 2014. Ukrainian president Petro Poroshenko mentioned that the main function of the ministry is to stop \"the spreading of biased information about Ukraine\".\n\nA proposal to establish an information ministry for Ukraine was first put forth on 30 November 2014 by Internal Affairs Ministry advisor Anton Herashchenko. He said that ministry could protect \"Ukraine's information space from Russian propaganda and counter propaganda in Russia, in the temporarily occupied territories of Crimea and eastern Ukraine\". The proposal was made amidst ongoing efforts to form a government, following the October 2014 Ukrainian parliamentary election. Ukrainian president Petro Poroshenko advocated for the establishment of such a ministry through the night on 1–2 December. It was quickly pushed through parliament with little fanfare. The formation of the Second Yatsenyuk Government was announced on 2 December, with Poroshenko ally Yuriy Stets confirmed as the first Minister for Information Policy. One day after his appointment, Stets published the ministry's regulations, which were based on a draft he wrote in 2007–09. According to these regulations, the ministry is meant to \"develop and implement professional standards in the media sphere\", \"ensure freedom of speech\", and prevent the spread of \"incomplete, outdated, or unreal information.\n\nPrior to its establishment, many Ukrainian journalists protested the creation of the ministry. They cited concerns that the ministry would \"open the way to grave excesses\" in restricting free speech, and that the ministry would inhibit journalists' work. Journalists demonstrating outside the parliament building said that the creation of the ministry was equivalent to \"a step back to the USSR\". The ministry was given the satirical appellation \"Ministry of Truth\" (), a reference to George Orwell's dystopian novel \"Nineteen Eighty-Four\". Reporters without Borders strongly opposed the creation of the ministry, and said that it was a \"retrograde step\". Petro Poroshenko Bloc politician Serhiy Leshchenko called for the ministry's immediate dissolution, whilst Poroshenko Bloc politician Svitlana Zalishchuk said that ministry's implementation should be put on hold, and that its regulations should be redrafted.\n\nNewly appointed Minister for Information Policy Yuriy Stets said that one of the primary goals of the ministry was to counteract \"Russian information aggression\" amidst the prolonged 2013–2014 crisis in Ukraine, and the ongoing war in the Donbass region. According to Stets, no other Ukrainian government institution was capable of handling this task. He stated that \"different states with different historical and cultural experiences in times of crisis came to need to create a body of executive power that would control and manage the information security of the country\". Stets also said that the ministry \"will in no way try to impose censorship or restrict freedom of speech\". President Poroshenko told journalists on 7 December 2014 that the main purpose of the ministry is to stop external \"information attacks on Ukraine\" by promoting the \"truth about Ukraine\" across the world. Poroshenko added that it was \"foolish\" to think that the ministry would become an organ of censorship.\n\nThe ministry was officially established by a resolution of the Ukrainian government on 14 January 2015. The resolution contained the duties and regulations of the ministry. According to the resolution, the primary objectives of the MIP are to \"protect the information sovereignty of Ukraine\", and to \"implement media reforms to spread socially important information\".\n\nA statement released by the ministry on 19 February 2015 announced the creation of an \"information force\" to counter misinformation on social media and across the internet. The force is targeted at Russia, which has been said to employ an \"army of trolls\" to spread false information and propaganda during the Ukrainian crisis.\n\nYuriy Stets resigned from his post of Minister of Information Policy on 8 December 2015. He withdrew his letter of resignation on 4 February 2016, and continued in the post.\n\nUnited States Ambassador to Ukraine Geoffrey Pyatt stated late January 2016 \"It's a huge mistake to create a 'Ministry of Truth' that tries to generate alternative stories. That is not the way to defeat this information warfare\".\n\nThe approved staff of the Ministry of Information Policy of Ukraine for 2015 includes 29 employees. Given the Ukrainian practice, the figure seems to be very small.However, the experience of many countries and contemporary understanding of management prove that these are the public institutions with a small staff working based on delegation principles, which are much more capable to implement global state tasks.\n\nDespite strict requirements of Ukrainian legislation, the Ministry of Information Policy is able to successfully perform all assigned tasks with minimum resources.\" to \"The approved staff of the Ministry of Information Policy of Ukraine for 2015 includes 29 employees, so we are a \"lean and mean\" department. Contemporary management strategies suggest that public institutions with a small staff working on delegated principles, can be highly effective to performing government tasks.\n\nThe Ministry of Information Policy aim is to successfully perform all assigned tasks with minimum resources, and within the strict requirements of Ukrainian legislation.\n\n\nAccording to Ukrainian legislation, social advertising - information of any kind, common in any form, whoch aims to achieve social goals, promotion of human values and the distribution of which is not intended to make a profit.\n\nMIP launched such social campaigns:\n\nThe Ministry has video and photo materials that it claims prove Russian military presence on the territory of Ukrainian Donbass.\n\nOn February 23, 2014 MIP created the Internet platform \"Information armies Ukraine\". Today the site pages and relevant social networks are used by more than 10,000 users. Monthly dozens of anti-Ukrainian users are blocked. Information attack of FSB security services is blocked, monthly almost 80,000 users get information on the fake of Russian propaganda. On each post there are dozens of false answers, which provided facts and arguments, so that the effectiveness of anti-Ukrainian propaganda in social networks plummeted.\n\nThe Ministry of Defense of Ukraine jointly with the Ministry of Information Policy of Ukraine continue to take the application process for project «Embedded journalists» - attaching media to military units in the ATO area and journalists are invited to participate. The journalist is not involved in the fighting, but he is subject to the relevant officer and lives in the same conditions as the rest of the soldiers. Today, Ukrainian and foreign journalists are working successfully with the military in the area ATO within the project Embedded journalism. Journalists from BBC, CNN, Washington Post, London Evening Standard, The Independent, Newsweek, France Press, Polsat, Daily Signal, Hanslucas, Tsenzor.NET, \"Radio Liberty\", Inter, Business Ukraine, New time have already participated. Results for the three months of the work: 18 video, 15 articles and three films in the foreign media.\n\n"}
{"id": "16531764", "url": "https://en.wikipedia.org/wiki?curid=16531764", "title": "Multi-junction solar cell", "text": "Multi-junction solar cell\n\nMulti-junction (MJ) solar cells are solar cells with multiple p–n junctions made of different semiconductor materials. Each material's p-n junction will produce electric current in response to different wavelengths of light. The use of multiple semiconducting materials allows the absorbance of a broader range of wavelengths, improving the cell's sunlight to electrical energy conversion efficiency.\n\nTraditional single-junction cells have a maximum theoretical efficiency of 33.16%. Theoretically, an infinite number of junctions would have a limiting efficiency of 86.8% under highly concentrated sunlight.\n\nCurrently, the best lab examples of traditional crystalline silicon (c-Si) solar cells have efficiencies between 20% and 25%, while lab examples of multi-junction cells have demonstrated performance over 46% under concentrated sunlight. Commercial examples of tandem cells are widely available at 30% under one-sun illumination, and improve to around 40% under concentrated sunlight. However, this efficiency is gained at the cost of increased complexity and manufacturing price. To date, their higher price and higher price-to-performance ratio have limited their use to special roles, notably in aerospace where their high power-to-weight ratio is desirable. In terrestrial applications, these solar cells are emerging in concentrator photovoltaics (CPV), with a growing number of installations around the world.\n\nTandem fabrication techniques have been used to improve the performance of existing designs. In particular, the technique can be applied to lower cost thin-film solar cells using amorphous silicon, as opposed to conventional crystalline silicon, to produce a cell with about 10% efficiency that is lightweight and flexible. This approach has been used by several commercial vendors, but these products are currently limited to certain niche roles, like roofing materials.\n\nTraditional photovoltaic cells are commonly composed of doped silicon with metallic contacts deposited on the top and bottom. The doping is normally applied to a thin layer on the top of the cell, producing a pn-junction with a particular bandgap energy, E.\n\nPhotons that hit the top of the solar cell are either reflected or transmitted into the cell. Transmitted photons have the potential to give their energy, \"hν\", to an electron if \"hν\" ≥ E, generating an electron-hole pair. In the depletion region, the drift electric field \"E\" accelerates both electrons and holes towards their respective n-doped and p-doped regions (up and down, respectively). The resulting current \"I\" is called the generated photocurrent. In the quasi-neutral region, the scattering electric field E accelerates holes (electrons) towards the p-doped (n-doped) region, which gives a scattering photocurrent \"I\" (\"I\"). Consequently, due to the accumulation of charges, a potential \"V\" and a photocurrent \"I\" appear. The expression for this photocurrent is obtained by adding generation and scattering photocurrents: \"I = I + I + I\".\n\nThe \"J-V\" characteristics (J is current density, i.e. current per unit area) of a solar cell under illumination are obtained by shifting the \"J-V\" characteristics of a diode in the dark downward by \"I\". Since solar cells are designed to supply power and not absorb it, the power \"P = V·I\" must be negative. Hence, the operating point \"(V, J)\" is located in the region where \"V\">0 and \"I\"<0, and chosen to maximize the absolute value of the power |\"P\"|.\n\nThe theoretical performance of a solar cell was first studied in depth in the 1960s, and is today known as the Shockley–Queisser limit. The limit describes several loss mechanisms that are inherent to any solar cell design.\n\nThe first are the losses due to blackbody radiation, a loss mechanism that affects any material object above absolute zero. In the case of solar cells at standard temperature and pressure, this loss accounts for about 7% of the power. The second is an effect known as \"recombination\", where the electrons created by the photoelectric effect meet the electron holes left behind by previous excitations. In silicon, this accounts for another 10% of the power.\n\nHowever, the dominant loss mechanism is the inability of a solar cell to extract all of the power in the light, and the associated problem that it cannot extract any power at all from certain photons. This is due to the fact that the photons must have enough energy to overcome the bandgap of the material.\n\nIf the photon has less energy than the bandgap, it is not collected at all. This is a major consideration for conventional solar cells, which are not sensitive to most of the infrared spectrum, although that represents almost half of the power coming from the sun. Conversely, photons with more energy than the bandgap, say blue light, initially eject an electron to a state high above the bandgap, but this extra energy is lost through collisions in a process known as \"relaxation\". This lost energy turns into heat in the cell, which has the side-effect of further increasing blackbody losses.\n\nCombining all of these factors, the maximum efficiency for a single-bandgap material, like conventional silicon cells, is about 34%. That is, 66% of the energy in the sunlight hitting the cell will be lost. Practical concerns further reduce this, notably reflection off the front surface or the metal terminals, with modern high-quality cells at about 22%.\n\nLower, also called narrower, bandgap materials will convert longer wavelength, lower energy photons. Higher, or wider bandgap materials will convert shorter wavelength, higher energy light. An analysis of the AM1.5 spectrum, shows the best balance is reached at about 1.1 eV (about 1100 nm, in the near infrared), which happens to be very close to the natural bandgap in silicon and a number of other useful semiconductors.\n\nCells made from multiple materials layers can have multiple bandgaps and will therefore respond to multiple light wavelengths, capturing and converting some of the energy that would otherwise be lost to relaxation as described above.\n\nFor instance, if one had a cell with two bandgaps in it, one tuned to red light and the other to green, then the extra energy in green, cyan and blue light would be lost only to the bandgap of the green-sensitive material, while the energy of the red, yellow and orange would be lost only to the bandgap of the red-sensitive material. Following analysis similar to those performed for single-bandgap devices, it can be demonstrated that the perfect bandgaps for a two-gap device are at 1.1 eV and 1.8 eV.\n\nConveniently, light of a particular wavelength does not interact strongly with materials that are of bigger bandgap. This means that you can make a multi-junction cell by layering the different materials on top of each other, shortest wavelengths (biggest bandgap) on the \"top\" and increasing through the body of the cell. As the photons have to pass through the cell to reach the proper layer to be absorbed, transparent conductors need to be used to collect the electrons being generated at each layer.\n\nProducing a tandem cell is not an easy task, largely due to the thinness of the materials and the difficulties extracting the current between the layers. The easy solution is to use two mechanically separate thin film solar cells and then wire them together separately outside the cell. This technique is widely used by amorphous silicon solar cells, Uni-Solar's products use three such layers to reach efficiencies around 9%. Lab examples using more exotic thin-film materials have demonstrated efficiencies over 30%.\n\nThe more difficult solution is the \"monolithically integrated\" cell, where the cell consists of a number of layers that are mechanically and electrically connected. These cells are much more difficult to produce because the electrical characteristics of each layer have to be carefully matched. In particular, the photocurrent generated in each layer needs to be matched, otherwise electrons will be absorbed between layers. This limits their construction to certain materials, best met by the III-V semiconductors.\n\nThe choice of materials for each sub-cell is determined by the requirements for lattice-matching, current-matching, and high performance opto-electronic properties.\n\nFor optimal growth and resulting crystal quality, the crystal lattice constant \"a\" of each material must be closely matched, resulting in lattice-matched devices. This constraint has been relaxed somewhat in recently developed metamorphic solar cells which contain a small degree of lattice mismatch. However, a greater degree of mismatch or other growth imperfections can lead to crystal defects causing a degradation in electronic properties.\n\nSince each sub-cell is connected electrically in series, the same current flows through each junction. The materials are ordered with decreasing bandgaps, E, allowing sub-bandgap light (\"hc/λ < e·E\") to transmit to the lower sub-cells. Therefore, suitable bandgaps must be chosen such that the design spectrum will balance the current generation in each of the sub-cells, achieving current matching. Figure C(b) plots spectral irradiance \"E\"(λ), which is the source power density at a given wavelength λ. It is plotted together with the maximum conversion efficiency for every junction as a function of the wavelength, which is directly related to the number of photons available for conversion into photocurrent.\n\nFinally, the layers must be electrically optimal for high performance. This necessitates usage of materials with strong absorption coefficients α(λ), high minority carrier lifetimes τ, and high mobilities µ.\n\nThe favorable values in the table below justify the choice of materials typically used for multi-junction solar cells: InGaP for the top sub-cell (E = 1.8 − 1.9 eV), InGaAs for the middle sub-cell (E = 1.4 eV), and Germanium for the bottom sub-cell (E = 0.67 eV). The use of Ge is mainly due to its lattice constant, robustness, low cost, abundance, and ease of production.\n\nBecause the different layers are closely lattice-matched, the fabrication of the device typically employs metal-organic chemical vapor deposition (MOCVD). This technique is preferable to the molecular beam epitaxy (MBE) because it ensures high crystal quality and large scale production.\n\nThe metallic contacts are low-resistivity electrodes that make contact with the semiconductor layers. They are often aluminum. This provides an electrical connection to a load or other parts of a solar cell array. They are usually on two sides of the cell. And are important to be on the back face so that shadowing on the lighting surface is reduced.\n\nAnti-reflective (AR) coating is generally composed of several layers in the case of MJ solar cells. The top AR layer has usually a NaOH surface texturation with several pyramids in order to increase the transmission coefficient \"T\", the trapping of the light in the material (because photons cannot easily get out the MJ structure due to pyramids) and therefore, the path length of photons in the material. On the one hand, the thickness of each AR layer is chosen to get destructive interferences. Therefore, the reflection coefficient \"R\" decreases to 1%. In the case of two AR layers \"L\" (the top layer, usually ) and \"L\" (usually ), there must be formula_1 to have the same amplitudes for reflected fields and \"nd\" = 4λ,\"nd\" = λ/4 to have opposite phase for reflected fields. On the other hand, the thickness of each AR layer is also chosen to minimize the reflectance at wavelengths for which the photocurrent is the lowest. Consequently, this maximizes \"J\" by matching currents of the three subcells. As example, because the current generated by the bottom cell is greater than the currents generated by the other cells, the thickness of AR layers is adjusted so that the infrared (IR) transmission (which corresponds to the bottom cell) is degraded while the ultraviolet transmission (which corresponds to the top cell) is upgraded. Particularly, an AR coating is very important at low wavelengths because, without it, \"T\" would be strongly reduced to 70%.\n\nThe main goal of tunnel junctions is to provide a low electrical resistance and optically low-loss connection between two subcells. Without it, the p-doped region of the top cell would be directly connected with the n-doped region of the middle cell. Hence, a pn junction with opposite direction to the others would appear between the top cell and the middle cell. Consequently, the photovoltage would be lower than if there would be no parasitic diode. In order to decrease this effect, a tunnel junction is used. It is simply a wide band gap, highly doped diode. The high doping reduces the length of the depletion region because\n\nHence, electrons can easily tunnel through the depletion region. The J-V characteristic of the tunnel junction is very important because it explains why tunnel junctions can be used to have a low electrical resistance connection between two pn junctions. Figure D shows three different regions: the tunneling region, the negative differential resistance region and the thermal diffusion region. The region where electrons can tunnel through the barrier is called the tunneling region. There, the voltage must be low enough so that energy of some electrons who are tunneling is equal to energy states available on the other side of the barrier. Consequently, current density through the tunnel junction is high (with maximum value of formula_3, the peak current density) and the slope near the origin is therefore steep. Then, the resistance is extremely low and consequently, the voltage too. This is why tunnel junctions are ideal for connecting two pn junctions without having a voltage drop. When voltage is higher, electrons cannot cross the barrier because energy states are no longer available for electrons. Therefore, the current density decreases and the differential resistance is negative. The last region, called thermal diffusion region, corresponds to the J-V characteristic of the usual diode:\n\nIn order to avoid the reduction of the MJ solar cell performances, tunnel junctions must be transparent to wavelengths absorbed by the next photovoltaic cell, the middle cell, i.e. E > E.\n\nA window layer is used in order to reduce the surface recombination velocity \"S\". Similarly, a back-surface field (BSF) layer reduces the scattering of carriers towards the tunnel junction. The structure of these two layers is the same: it is a heterojunction which catches electrons (holes). Indeed, despite the electric field \"E\", these cannot jump above the barrier formed by the heterojunction because they don't have enough energy, as illustrated in figure E. Hence, electrons (holes) cannot recombine with holes (electrons) and cannot diffuse through the barrier. By the way, window and BSF layers must be transparent to wavelengths absorbed by the next pn junction i.e. E > E and E > E. Furthermore, the lattice constant must be close to the one of InGaP and the layer must be highly doped (\"n\" ≥ 10 cm).\n\nFor maximum efficiency, each subcell should be operated at its optimal J-V parameters, which are not necessarily equal for each subcell. If they are different, the total current through the solar cell is the lowest of the three. By approximation, it results in the same relationship for the short-circuit current of the MJ solar cell: \"J = min (J, J, J)\" where \"J\"(λ) is the short-circuit current density at a given wavelength λ for the subcell \"i\".\n\nBecause of the impossibility to obtain \"J, J, J\" directly from the total J-V characteristic, the quantum efficiency \"QE\"(λ) is utilized. It measures the ratio between the amount of electron-hole pairs created and the incident photons at a given wavelength λ. Let φ(λ) be the photon flux of corresponding incident light in subcell \"i\"and\"QE\"(λ) be the quantum efficiency of the subcell \"i\". By definition, this equates to:\n\nThe value of formula_6 is obtained by linking it with the absorption coefficient formula_7, i.e. the number of photons absorbed per unit of length by a material. If it is assumed that each photon absorbed by a subcell creates an electron/hole pair (which is a good approximation), this leads to:\n\nSimilarly, because\n\nThe values of formula_12 are then given by the J-V diode equation:\n\nWe can estimate the limiting efficiency of ideal infinite multi-junction solar cells using the graphical quantum-efficiency (QE) analysis invented by C. H. Henry.\nTo fully take advantage of Henry's method, the unit of the AM1.5 spectral irradiance should be converted to that of photon flux (i.e., number of photons/m/s). To do that, it is necessary to carry out an intermediate unit conversion from the power of electromagnetic radiation incident per unit area per photon energy to the photon flux per photon energy (i.e., from [W/m/eV] to [number of photons/m2/s/eV]). For this intermediate unit conversion, the following points have to be considered: A photon has a distinct energy which is defined as follows.\n\nwhere E is photon energy, h is Planck's constant (h = 6.626*10 [J∙s]), c is speed of light (c = 2.998*10 [m/s]), f is frequency [1/s], and λ is wavelength [nm].\n\nThen the photon flux per photon energy, dn/dhν, with respect to certain irradiance E [W/m/eV] can be calculated as follows.\n\n(2): formula_14 = E/{h∙(c/λ)} = E[W/(m∙eV)]∙λ∙(10 [m])/(1.998∙10 [J∙s∙m/s]) = E∙λ∙5.03∙10 [(# of photons)/(m∙s∙eV)] \n\nAs a result of this intermediate unit conversion, the AM1.5 spectral irradiance is given in unit of the photon flux per photon energy, [number of photons/m/s/eV], as shown in Figure 1.\n\nBased on the above result from the intermediate unit conversion, we can derive the photon flux by numerically integrating the photon flux per photon energy with respect to photon energy. The numerically integrated photon flux is calculated using the Trapezoidal rule, as follows.\n\n(3): formula_15\n\nAs a result of this numerical integration, the AM1.5 spectral irradiance is given in unit of the photon flux, [number of photons/m2/s], as shown in Figure 2.\nIt is should be noted that there are no photon flux data in the small photon energy range from 0 eV to 0.3096 eV because the standard (AM1.5) solar energy spectrum for hν < 0.31 eV are not available. Regardless of this data unavailability, however, the graphical QE analysis can be done using the only available data with a reasonable assumption that semiconductors are opaque for photon energies greater than their bandgap energy, but transparent for photon energies less than their bandgap energy. This assumption accounts for the first intrinsic loss in the efficiency of solar cells, which is caused by the inability of single-junction solar cells to properly match the broad solar energy spectrum.\nHowever, the current graphical QE analysis still cannot reflect the second intrinsic loss in the efficiency of solar cells, radiative recombination. To take the radiative recombination into account, we need to evaluate the radiative current density, J, first. According to Shockley and Queisser method,\nJ can be approximated as follows.\n\n(4): formula_16\n\n(5): formula_17\n\nwhere E is in electron volts and n is evaluated to be 3.6, the value for GaAs. The incident absorbed thermal radiation J is given by J with V = 0.\n\n(6): formula_18\n\nThe current density delivered to the load is the difference of the current densities due to absorbed solar and thermal radiation and the current density of radiation emitted from the top surface or absorbed in the substrate. Defining J = en, we have\n\n(7): J = J + J − J\n\nThe second term, J, is negligible compared to J for all semiconductors with E. ≥ 0.3 eV, as can be shown by evaluation of the above J equation. Thus, we will neglect this term to simplify the following discussion. Then we can express J as follows.\n\n(8): formula_19\n\nThe open-circuit voltage is found by setting J = 0.\n\n(9): formula_20\n\nThe maximum power point (J, V) is found by stetting the derivative formula_21. The familiar result of this calculation is\n\n(10): formula_22\n\n(11): formula_23\n\nFinally, the maximum work (W) done per absorbed photon, Wm is given by\n\n(12): formula_24\n\nCombining the last three equations, we have\n\n(13): formula_25\n\nUsing the above equation, W (red line) is plotted in Figure 3 for different values of E (or n).\n\nNow, we can fully use Henry's graphical QE analysis, taking into account the two major intrinsic losses in the efficiency of solar cells. The two main intrinsic losses are radiative recombination, and the inability of single junction solar cells to properly match the broad solar energy spectrum. The shaded area under the red line represents the maximum work done by ideal infinite multi-junction solar cells. Hence, the limiting efficiency of ideal infinite multi-junction solar cells is evaluated to be 68.8% by comparing the shaded area defined by the red line with the total photon-flux area determined by the black line. (This is why this method is called \"graphical\" QE analysis.) Although this limiting efficiency value is consistent with the values published by Parrott and Vos in 1979: 64% and 68.2% respectively,\nthere is a small gap between the estimated value in this report and literature values. This minor difference is most likely due to the different ways how to approximate the photon flux from 0 eV to 0.3096 eV. Here, we approximated the photon flux from 0 eV to 0.3096 eV as the same as the photon flux at 0.31 eV.\n\nThe majority of multi-junction cells that have been produced to date use three layers (although many tandem a-Si:H/mc-Si modules have been produced and are widely available). However, the triple junction cells require the use of semiconductors that can be tuned to specific frequencies, which has led to most of them being made of gallium arsenide (GaAs) compounds, often germanium for the bottom-, GaAs for the middle-, and GaInP for the top-cell.\n\nDual junction cells can be made on Gallium arsenide wafers. Alloys of Indium gallium phosphide in the range InGaP through InGaP serve as the high band gap alloy. This alloy range provides for the ability to have band gaps in the range of 1.92eV to 1.87eV. The lower GaAs junction has a band gap of 1.42eV.\n\nTriple junction cells consisting of indium gallium phosphide (InGaP), gallium arsenide (GaAs) or indium gallium arsenide (InGaAs) and germanium (Ge) can be fabricated on germanium wafers. Early cells used straight gallium arsenide in the middle junction. Later cells have utilized InGaAs, due to the better lattice match to Ge, resulting in a lower defect density.\n\nDue to the huge band gap difference between GaAs (1.42eV), and Ge (0.66eV), the current match is very poor, with the Ge junction operated significantly current limited.\n\nCurrent efficiencies for commercial InGaP/GaAs/Ge cells approach 40% under concentrated sunlight. Lab cells (partly using additional junctions between the GaAs and Ge junction) have demonstrated efficiencies above 40%.\n\nIndium phosphide may be used as a substrate to fabricate cells with band gaps between 1.35eV and 0.74eV. Indium Phosphide has a band gap of 1.35eV. Indium gallium arsenide (InGaAs) is lattice matched to Indium Phosphide with a band gap of 0.74eV. A quaternary alloy of Indium gallium arsenide phosphide can be lattice matched for any band gap in between the two.\n\nIndium phosphide-based cells have the potential to work in tandem with gallium arsenide cells. The two cells can be optically connected in series (with the InP cell below the GaAs cell), or in parallel through the use of spectra splitting using a Dichroic filter.\n\nIndium gallium nitride (InGaN) is a semiconductor material made of a mix of gallium nitride (GaN) and indium nitride (InN). It is a ternary group III/V direct bandgap semiconductor. Its bandgap can be tuned by varying the amount of indium in the alloy from 0.7 eV to 3.4 eV, thus making it an ideal material for solar cells. However, its conversion efficiencies because of technological factors unrelated to bandgap are still not high enough to be competitive in the market.\n\nMany MJ photovoltaic cells use III-V semiconductor materials. GaAsSb-based heterojunction tunnel diodes, instead of conventional InGaP highly doped tunnel diodes described above, have a lower tunneling distance. Indeed, in the heterostructure formed by GaAsSb and InGaAs, the valence band of GaAsSb is higher than the valence band of the adjoining p-doped layer. Consequently, the tunneling distance \"d\" is reduced and so the tunneling current, which exponentially depends of \"d\", is increased. Hence, the voltage is lower than that of the InGaP tunnel junction.\nGaAsSb heterojunction tunnel diodes offer other advantages. The same current can be achieved by using a lower doping. Secondly, because the lattice constant is larger for GaAsSb than Ge, one can use a wider range of materials for the bottom cell because more materials are lattice-matched to GaAsSb than to Ge.\n\nChemical components can be added to some layers. Adding about one percent of Indium in each layer better matches lattice constants of the different layers. Without it, there is about 0.08 percent of mismatching between layers, which inhibits performance. Adding aluminium to the top cell increases its band gap to 1.96 eV, covering a larger part of the solar spectrum and obtain a higher open-circuit voltage \"V\".\n\nThe theoretical efficiency of MJ solar cells is 86.8% for an infinite number of pn junctions, implying that more junctions increase efficiency. The maximum theoretical efficiency is 37, 50, 56, 72% for 1, 2, 3, 36 pn junctions, respectively, with the number of junctions increasing exponentially to achieve equal efficiency increments. The exponential relationship implies that as the cell approaches the limit of efficiency, the increase cost and complexity grow rapidly. Decreasing the thickness of the top cell increases the transmission coefficient \"T\".\n\nFinally, an InGaP hetero-layer between the p-Ge layer and the InGaAs layer can be added in order to create automatically the n-Ge layer by scattering during MOCVD growth and increase significantly the quantum efficiency \"QE\"(λ) of the bottom cell. InGaP is advantageous because of its high scattering coefficient and low solubility in Ge.\n\nSolar spectrum at the Earth surface changes constantly depending on the weather and sun position. This results in the variation of φ(λ), \"QE\"(λ), α(λ) and thus the short-circuit currents \"J\". As a result, the current densities \"J\" are not necessarily matched and the total current becomes lower. These variations can be quantified using the average photon energy (APE) which is the ratio between the spectral irradiance G(λ) (the power density of the light source in a specific wavelength λ) and the total photon flux density. It can be shown that a high (low) value for APE means low (high) wavelengths spectral conditions and higher (lower) efficiencies. Thus APE is a good indicator for quantifying the effects of the solar spectrum variations on performances and has the added advantage of being independent of the device structure and the absorption profile of the device.\n\nLight concentrators increase efficiencies and reduce the cost/efficiency ratio. The three types of light concentrators in use are refractive lenses like Fresnel lenses, reflective dishes (parabolic or cassegraine), and light guide optics. Thanks to these devices, light arriving on a large surface can be concentrated on a smaller cell. The intensity concentration ratio (or “suns”) is the average intensity of the focused light divided by 1 kW/m (reasonable value related to solar constant). If its value is \"X\" then the MJ current becomes \"X\" higher under concentrated illumination.\n\nUsing concentrations on the order of 500 to 1000, meaning that a 1 cm cell can use the light collected from 0.1 m (as 1 m equal 10000 cm), produces the highest efficiencies seen to date. Three-layer cells are fundamentally limited to 63%, but existing commercial prototypes have already demonstrated over 40%. These cells capture about 2/3 of their theoretical maximum performance, so assuming the same is true for a non-concentrated version of the same design, one might expect a three-layer cell of 30% efficiency. This is not enough of an advantage over traditional silicon designs to make up for their extra production costs. For this reason, almost all multi-junction cell research for terrestrial use is dedicated to concentrator systems, normally using mirrors or fresnel lenses.\n\nUsing a concentrator also has the added benefit that the number of cells needed to cover a given amount of ground area is greatly reduced. A conventional system covering 1 m would require 625 16 cm cells, but for a concentrator system only a single cell is needed, along with a concentrator. The argument for concentrated Multi-junction cells has been that the high cost of the cells themselves would be more than offset by the reduction in total number of cells. However, the downside of the concentrator approach is that efficiency drops off very quickly under lower lighting conditions. In order to maximize its advantage over traditional cells and thus be cost competitive, the concentrator system has to track the sun as it moves to keep the light focused on the cell and maintain maximum efficiency as long as possible. This requires a solar tracker system, which increases yield, but also cost.\n\nAs of 2014 multi-junction cells were expensive to produce, using techniques similar to semiconductor device fabrication, usually metalorganic vapour phase epitaxy but on \"chip\" sizes on the order of centimeters.\n\nA new technique was announced that year that allowed such cells to use a substrate of glass or steel, lower-cost vapors in reduced quantities that was claimed to offer costs competitive with conventional silicon cells.\n\nThere are four main categories of photovoltaic cells: conventional mono and multi crystalline silicon (c-Si) cells, thin film solar cells (a-Si, CIGS and CdTe), and multi-junction (MJ) solar cells. The fourth category, emerging photovoltaics, contains technologies that are still in the research or development phase and are not listed in the table below.\n\nMJ solar cells and other photovoltaic devices have significant differences \"(see the table above)\". Physically, the main property of a MJ solar cell is having more than one pn junction in order to catch a larger photon energy spectrum while the main property of the thin film solar cell is to use thin films instead of thick layers in order to decrease the cost efficiency ratio. , MJ solar panels are more expensive than others. These differences imply different applications: MJ solar cells are preferred in space and c-Si solar cells for terrestrial applications.\n\nThe efficiencies of solar cells and Si solar technology are relatively stable, while the efficiency of solar modules and multi-junction technology are progressing.\n\nMeasurements on MJ solar cells are usually made in laboratory, using light concentrators (this is often not the case for the other cells) and under standard test conditions (STCs). STCs prescribe, for terrestrial applications, the AM1.5 spectrum as the reference. This air mass (AM) corresponds to a fixed position of the sun in the sky of 48° and a fixed power of 833 W/m. Therefore, spectral variations of incident light and environmental parameters are not taken into account under STC.\n\nConsequently, performance of MJ solar cells in terrestrial environment is inferior to that achieved in laboratory. Moreover, MJ solar cells are designed such that currents are matched under STC, but not necessarily under field conditions. One can use \"QE\"(λ) to compare performances of different technologies, but \"QE\"(λ) contains no information on the matching of currents of subcells. An important comparison point is rather the output power per unit area generated with the same incident light.\n\nAs of 2010, the cost of MJ solar cells was too high to allow use outside of specialized applications. The high cost is mainly due to the complex structure and the high price of materials. Nevertheless, with light concentrators under illumination of at least 400 suns, MJ solar panels become practical.\n\nAs less expensive multi-junction materials become available other applications involve bandgap engineering for microclimates with varied atmospheric conditions.\n\nMJ cells are currently being utilized in the Mars rover missions. \n\nThe environment in space is quite different. Because there is no atmosphere, the solar spectrum is different (AM0). The cells have a poor current match due to a greater photon flux of photons above 1.87eV vs. those between 1.87eV and 1.42eV. This results in too little current in the GaAs junction, and hampers the overall efficiency since the InGaP junction operates below MPP current and the GaAs junction operates above MPP current. To improve current match, the InGaP layer is intentionally thinned to allow additional photons to penetrate to the lower GaAs layer.\n\nIn terrestrial concentrating applications, the scatter of blue light by the atmosphere reduces the photon flux above 1.87eV, better balancing the junction currents. Radiation particles that are no longer filtered can damage the cell. There are two kinds of damage: ionisation and atomic displacement. Still, MJ cells offer higher radiation resistance, higher efficiency and a lower temperature coefficient.\n\n\n"}
{"id": "14069922", "url": "https://en.wikipedia.org/wiki?curid=14069922", "title": "Nanoionic device", "text": "Nanoionic device\n\nNanoionic hard drives use nanoionic technology allowing for smaller devices while doing away with moving parts and the mechanical failures which are associated with previous HDD drives. Nanoionic hard drives are currently the most state of the art drives on the market and nanoionics was not utilized in hard drives until February 2014. Nanoionic devices were first proposed in 1992: \"The results obtained show that it is possible to form arrays of electrochemical devices with single elements ~10 nm in size in the films\". The basis of design of nanoionic devices is the creation of nanostructures with nanoionic parameter \"λ\" / \"L\" ~ 1, where \"L\" is the size of device structure, and \"λ\" is the characteristic size of specific region where the property of fast ionic transport is realized. “Possibilities to influence on these specific regions < λ > in a controllable manner may appear in short sized devices”. Ion - electronic hybrid devices should be considered as a step on a way to the future nanoelectronics-nanoionics (nanoelionics) that was first proposed in 1996.\n\nThe First HDD Hard Drive was made in 1956 by IBM. It weighed nearly 2000 pounds and only could hold 5 MB. The first HDD Drive was 60\" long, 68\" tall, and 29\" wide. It was used in IBM's RAMAC 305,the first computer to use HDD drives.\n\nSince 1956 the principle of storing information via magnetic domains has not changed much. As time progressed HDD drives were able to store more information than the original and the parts became much smaller, but the moving parts that are used in HDD drives were here to stay until nanoionic technologies began to be utilized in hard drives.\n\nIn 1976 the first Solid State Device or SSD was made by Dataram and could store up to 2 MB. The SSD did not become popular until 2001 when SSD the SSD industry its revenues reached $25 million a year and was listed on INC 500 fastest growing private companies. The reason for this slow growth was that SSD were expensive. In 1978 1 GB would have cost $1 million. Even in 2001, Adtron’s S35PC 3.5” SSD drive which had 14 GB storage cost $42,000.\n\nThe idea to utilized this technology began at Arizona State University in 1992 and in 1996 the nanoionic supercapacitor was the first device to use nanoionics. The first drive to have this technology was invented by Dr. Michael Kozicki at Arizona State University 1996.This technology was then used in multiple universities around the globe, but did npt become available to the public until February 2014.\n\nTraditional hard drives run on mechanical parts, which uses the permanent disk to store your personal data. The hard disk drives usually have a 10 centimeter diameter and are a centimeter thick. These drives use a magnetic recording technique. We, as users, can easily manipulate our data on this medium i.e. add, copy, cut or paste data on it though this technique. The principle of magnetic flux is used for such processes. Basically the hard drive remembers which data is where and how to manipulate the data according to the user’s preference by remembering the flux patterns.\nThe average hard drives contain the specific parts:\n\nSSD, regarded as the future of data storage, is a technology recently available to the consumers. However expensive, these state of the art drives are considerably smaller. Data on such drives is statically twice as safer i.e. such drives crash less.\nFirst of all, these drives do not involve magnetic principles. These drives use semiconductors to store data unlike the disk platter used in the Hard Disk Drives. SSDs use the principle of flash drive. It consists of no actual mechanical parts i.e. no movements of parts is involved. Usually the HDD data is referred to as volatile memory, this really means that when the computer loses power, all the memory is lost. However, in the case of Solid State Drives, there are chips which deliver non-volatile memory meaning the data is stored even when there is a loss of power with the machine.\n\nNanoionics devices rely on the fundamentals of electrochemistry. Currently the hard drives are made up of solid materials which weigh less and generate more power; solids have one polarity for ions that are moving. The ions are what the electrodes are made of. Electrodes are key components in nanoionic devices. Those electrodes can be made out of ZrO, a metal which is coated in LaNiO/LaCuO, or BiV(metal)O. Where “metal” is any metal found in the transition metal group like copper. These nanoionic devices are made up of smaller devices that are that are spaced less than a tenth nanometer away. Due to the small spaced distance between the materials in the nanoionic device smaller ions are necessary for the pathway.Metals in the first group of the periodic table are small, but they are too reactive. So there needs to be a compromise between the reactivity of chemicals and size. That is why materials like Cu or Ag fulfill the needs of the nanoionic drives.\n\nIn the nanoionic device there would be chalcogenide glass with has the metal like gold and an element for group six infused in the glass. This glass is the electrode for the nanoionic. In the oxidation reaction the Ag then loses the electron and turns into Ag. This reaction only happens when a switch confirms the reaction to happen and these switches are used for the binary information storage. That binary information storage is where all the data is saved on the hard drive. All this is dependent on the small current of ions, hence the name nanoionic, to allow the reaction to happen. All the components of the switch, right metals, and solid have to come together to make the nanoionic happen correctly.\n\nDue to nanoionic devices being new technology, they lack memory capacity. Smaller drives take up less charge to change the electrolytes in their nonvolatile state. The smaller size is the only way the nanoionic devices are feasible with today’s technology because it needs the reduction in resistance. These smaller sizes can hold a lot of memory and can still work in a computer.\n\n"}
{"id": "15240606", "url": "https://en.wikipedia.org/wiki?curid=15240606", "title": "Nose prosthesis", "text": "Nose prosthesis\n\nA nose (nasal) prosthesis is a type of craniofacial prosthesis used to replace an absent nose. Modern prosthetic noses are typically made of polydimethylsiloxane, a flexible material made to move with the skin. It is retained to the patient daily by using an adhesive or osseointegrated implants.\n"}
{"id": "32080920", "url": "https://en.wikipedia.org/wiki?curid=32080920", "title": "Old Bess (beam engine)", "text": "Old Bess (beam engine)\n\nOld Bess is an early beam engine built by the partnership of Boulton and Watt. The engine was constructed in 1777 and worked until 1848.\n\nThe engine is most obviously known simply for being an early example of an engine built by Boulton and Watt. However it also played a far more important role in the development of steam engines for being the first engine designed to work with an early cutoff, and so to use the expansion of the steam for greater efficiency.\n\nIt is now preserved in the Power Gallery of the Science Museum, London. It is the oldest surviving Watt engine, and the third-oldest surviving beam engine.\n\nWatt's first engine at Kinneil had been unsuccessful and the parts were taken down and re-used at Boulton's Soho Manufactory in Birmingham. The reworked engine was more successful there, and encouraged Boulton to invest further in this developing steam technology and in Watt's inventions.\n\nThe Manufactory had been built to use a water wheel to drive its machinery, and the site had been chosen on that basis, but there were concerns over seasonal lack of water to power the wheel. Similar problems in the iron industry had inspired the development of the water-returning engine: a steam pump that could raise water to drive the wheel, in times of low water on the river. The Kinneil Engine had been built as a pump, for use in a coal mine, and so was suitable for this new task. Watt's rotative beam engine had not yet been considered and so the only way to produce rotary work to drive machinery in the Manufactory was by water power.\n\nIn 1777 Boulton and Watt decided to build a second engine for use at Soho, either to supplement the Kinneil Engine or primarily to experiment with Watt's new idea of expansive working of the steam. The new engine was also to be a water-returning engine Like the earlier Newcomen engines, it was only capable of pumping water rather than driving machinery directly.\n\nAs early as 1769, Watt was considering the possibility of working steam expansively, as recorded in a letter of 28 May to Dr. Small. Early engines were incapable of this, as they used a single valve for both inlet and exhaust. As Watt had already begun to use separate valves for each function, it would now be possible to control their timing independently, i.e. to apply lead to the timing of the inlet valve. Watt decided to construct a new engine to demonstrate this principle and was confident of the substantial savings in coal consumption to be offered.\n\nConstruction began in 1777 with the ordering of a 33-inch cylinder (84cm). The engine was erected and working at Soho by August, although still incomplete.\n\nThe engine always worked as a water pump and was equipped with two cast iron cylinders at opposite ends of the beam, one for the working cylinder and one for the pump. The pump cylinder was taller and thinner, of diameter and tall, designed for a working stroke within this, although only was used in practice. The pumped water was delivered at a head of . The engine was later described as being of 30 hp in power.\n\nThe beam was typical for early single-acting beam engines, pulling through wrought iron chains running over a curved arch-head at each end of the beam. At some later point, possibly when reconstructed after the fire, this beam was strengthened by being strutted and bridled with the additional timber and iron triangular trusses that are seen above the beam today.\n\nInitial operation of the engine was unsatisfactory. Watt was away in Cornwall and Boulton wrote to him, describing the engine's actions as \"very fierce\". His opinion was that the engine's cylinder was too large for the work expected of it. This led to it being worked at a pressure of around 5.7 psi, whereas if it were operating at 8 psi, it would be less jerky and violent. This action was so remarkable as to give the engine its initial nickname of \"Beelzebub\".\n\nNeither Watt nor Boulton had a solution to the engine's behaviour. Watt's experience with the Chacewater engine, a rebuilt Smeaton engine, at Wheal Busy, suggested that cutoff led to a violent action. In September he recommended throttling the steam supply to the engine. Boulton favoured further experimentation with cutoff (i.e. valve timing) and in 1779 suggested that a series of more scientific measurements be tried. With hindsight, Boulton's approach was the more thermodynamically efficient, although this lesson was not fully appreciated by locomotive drivers right to the end of steam power.\n\nOne morning in July 1778 the engine house was discovered to be on fire. The fire spread rapidly and within half an hour the roof was burned to the ground. The timberwork and soldered copper piping of the engine were also destroyed or damaged. Even the beam itself, a substantial timber construction, was \"rendered unfit\" by the fire. Fortunately the most important part of the engine, its cylinder, survived the fire relatively intact. Within three weeks the engine had been reconstructed and was expected to be working again shortly.\n\nWhile the engine had been damaged, Watt, who was once again away in Cornwall, had advised that \"no repair farther that the roof ought to be gone about at present\", with his intention being to reconstruct the engine in some improved manner. The nature of these reconstructions, and their effectiveness, remains unclear.\n\nBy 1781, the engine had been working in its reconstructed form for some years. Whatever the improvements had been, they left Watt and Boulton still agreeing that this was, \"one of the worst engines they had\".\n\nThe engine operated at the Soho Mint, until the mint's closure in 1848. It was sold for £48, then re-sold for £58 and placed on display on an island in Derrington Pool, outside the metal-rolling works of its new owner, a Mr. Walker. Although much of the Mint's coining machinery had been re-sold for further use at the new Birmingham Mint, this aged and thoroughly obsolete engine appears to have been one of the first artefacts of industrial archaeology to be deliberately preserved.\n\nThe engine was later re-sold and then presented to the Commissioners of Patents for their Patent Office Museum, which would in turn become the Science Museum. The engine is thus not only one of the Museum's oldest exhibits, but also one of the first to enter its collection. When first displayed, the engine was erected in an open-fronted representation of a brick-built engine house. It is now displayed on free-standing timberwork, allowing a closer inspection of the cylinders.\n\nFor an engine that had been described as \"one of the worst engines they had\" when almost new, it had a relatively long working life of over 70 years. Its name had also shifted from the violent \"Beelzebub\" to the rather more friendly \"Old Bess\", indicating a more satisfactory performance. Reports by both Joseph Harrison, Artificer of the Soho Mint and William Buckle support this.\n\nIn 2009 it was awarded an Engineering Heritage Award by the IMechE.\n\nSeveral confusions exist about this engine, and have been widely repeated.\n\nThe first of these confusions is that this was Watt's \"second\" engine (after the Kinneil engine) and the first by the Boulton and Watt partnership. In fact several engines were built between 1773 (the return of the Kinneil engine to Birmingham) and the construction of \"Beelzebub\" in 1777. These include such well-known engines as the 1775 38-inch blowing engine for John Wilkinson's blast furnaces at New Willey, near Broseley, and the 50-inch pumping engine for Bloomfield Colliery near Tipton.\n\nThe identity of \"Beelzebub\" is also confused. Some sources describe this as referring to the rebuilt Kinneil engine at Birmingham. There are two pieces of evidence to support the view that \"Old Bess\" and \"Beelzebub\" are the same engine. Firstly the name \"Beelzebub\" derives from its violent action when used experimentally for expansive working, an experiment applied to the engine built new in 1777. Secondly \"Beelzebub\" is described as a 33 inch engine, as is \"Old Bess\" on display today. There is no record of Boulton & Watt ever building another engine of this dimension.\n\n"}
{"id": "21482500", "url": "https://en.wikipedia.org/wiki?curid=21482500", "title": "PCMOS", "text": "PCMOS\n\nProbabilistic complementary metal-oxide semiconductor (PCMOS) is a semiconductor manufacturing technology invented by Pr. Krishna Palem of Rice University and Director of NTU's Institute for Sustainable Nanoelectronics (ISNE). The technology hopes to compete against current CMOS technology. Proponents claim it uses 30 times less electricity while running seven times faster than the current fastest technology.\n\nPCMOS-based system on a chip architectures were shown to be gains that are as high as a substantial multiplicative factor of 560 when compared to a competing energy-efficient CMOS based realization on applications based on probabilistic algorithms such as hyper-encryption, bayesian networks, random neural networks and probabilistic cellular automata.\n"}
{"id": "5736157", "url": "https://en.wikipedia.org/wiki?curid=5736157", "title": "Panclastite", "text": "Panclastite\n\nPanclastites are a class of Sprengel explosives similar to oxyliquits. They were first suggested in 1881 by Eugène Turpin, a French chemist. They are a mixture of liquid dinitrogen tetroxide serving as oxidizer with a suitable fuel, e.g. carbon disulfide, in the 3:2 volume ratio. Other fuel being used is nitrobenzene. Possible alternative fuels are e.g. nitrotoluene, gasoline, nitromethane, or halocarbons.\n\nPanclastites are shock-sensitive and difficult to handle, requiring their mixing immediately before use; also the dinitrogen tetroxide is highly corrosive and explodes in contact with some chemicals. Despite their brisance and detonation velocity being comparable with TNT, panclastites have virtually no use today.\n\nDuring World War I, due to shortages of other explosives, French used some panclastite-class mixtures, which they called anilites, in small aircraft bombs. The mixing of the chemicals was triggered by airflow spinning a propeller on the nose of the bomb after it was dropped, mixing the previously separated chemicals inside. The resulting mixture was so sensitive the bombs did not need a fuze to explode on impact.\n\nIn the 1880s, Germans were testing torpedoes with panclastite warhead. Carbon disulfide and nitrogen tetroxide were stored in separate glass compartments, which were broken when the torpedo was launched and the chemicals mixed, and later were detonated by a contact fuse.\n\n"}
{"id": "3381059", "url": "https://en.wikipedia.org/wiki?curid=3381059", "title": "Parom", "text": "Parom\n\nThe Parom (\"ferry\" in Russian) is a space tug that has been proposed by RKK Energia. The purpose of this vehicle is to replace most of the Progress' active components. Progress spacecraft have flown re-supply missions since 1978. Nikolai Bryukhanov, RKK Energia's deputy general designer, said in May 2005 that the Federal Space Agency had received a design for a new space system. According to him, the system's operation principle is completely different from that used by Progress. A launch vehicle first places a Parom reusable inter-orbit \"tug\" into a 200 km orbit. As this spacecraft will not carry any consignments, other rockets will orbit payload containers that will be docked by Parom. The tug will then deliver them to the ISS or another orbiter.\n\n\"Any Russian or foreign launch vehicle can orbit such containers,\" Bryukhanov said. The size of the container and its shape depend on payload characteristics. \"This can be an airtight instrument module or a fuel tanker,\" the deputy general designer continued. \"Moreover, depressurized platform featuring large scientific equipment and auxiliary systems, i.e., solar batteries that cannot be stored inside the airtight module\".\n\nIn layout, the Parom will be built around a pressurized transfer passage with docking ports at each end: each of these two docking ports can be used to dock with the cargo container, the Kliper, the space station or any other spacecraft. It will have its own engines, along with propellant transfer lines to feed fuel from the cargo container into its own tanks or into the space station’s or another spacecraft's tank. It will also have engines scaled to handle cargo modules weighing up to 30 tonnes (around 60,000 pounds), twice the mass of the largest station sections carried into orbit aboard space shuttles and Proton rockets.\n\n\n"}
{"id": "37887483", "url": "https://en.wikipedia.org/wiki?curid=37887483", "title": "Phoslock", "text": "Phoslock\n\nPhoslock is the commercial name for a bentonite clay in which the sodium and/or calcium ions are exchanged for lanthanum. The addition of this element allows it to bind with phosphates to form rhabdophane (<chem>LaPO4.\\mathit{n}H2O</chem>) and thereby remove them from the water column. It is used in lake restoration projects as a tool to manage eutrophication and manage algal blooms (specifically cyanobacteria or blue green algae) by reducing phosphorus, one of the major contributing factors to algal growth.\n\nIt was developed in Australia by the CSIRO in the late 1990s by Dr Grant Douglas (US Patent 6350383) as a way of utilising the ability of lanthanum to bind phosphate in freshwater natural aquatic systems. The first large-scale trial took place in January 2000 in the Canning River, Western Australia. More recent CSIRO research has involved the development of a nanoclay hybrid with a demonstrated ability to remove dissolved phosphorus from both natural and wastewaters. \nDuring its development, patenting and commercialisation by CSIRO and subsequent commercial production, Phoslock has been a subject in academic research (,) and has been used globally in lake restoration projects. The largest number of whole lake applications and the most comprehensive pre and post-application monitoring has taken place in Europe, primarily Germany (where it is sold under the tradename Bentophos), the Netherlands and the UK.)\n"}
{"id": "23007105", "url": "https://en.wikipedia.org/wiki?curid=23007105", "title": "RTFB", "text": "RTFB\n\nThe most usual meaning is in reference to instruction manuals, and means \"Read the F'ing Book\" more politely rendered as \"Read the Fine Book.\"\n\n\"RTFB\" is a parodical extension to the Internet slang term \"RTFM\" and its extension \"RTFS\".\n\n\"RTFB\" is short for \"read the fucking binary\", in a similar way as \"RTFM\" means \"read the fucking manual\" and \"RTFS\" means \"read the fucking source\". While both \"RTFM\" and \"RTFS\" have legitimate uses, \"RTFB\" is usually intended only as a parody of them. While any user, no matter how tech-savvy, can be expected to read the manual, and experienced computer programmers can deduce the working logic of a computer program directly from reading its human-readable source code, \"RTFB\" expects users to directly read the machine language code that the microprocessor executes natively. Today's microprocessors are usually so complex that, except for trivial textbook programs, this is a difficult task even for experienced programmers, and because of anti-reverse-engineering laws, may even be illegal.\n\nRTFB has also been used meaning \"Read the fucking Bible\" in relation to WWJD (What Would Jesus Do) jokes.\n\nA more recent use, applied to members of the legislative branch, means \"read the fucking bill\". It is a reaction to the practice of passing massive legislation where the final bill is available for only a few hours before it is voted upon.\n\n"}
{"id": "750151", "url": "https://en.wikipedia.org/wiki?curid=750151", "title": "Robert Coldwell Wood", "text": "Robert Coldwell Wood\n\nRobert Coldwell Wood (September 16, 1923 – April 1, 2005) was an American political scientist, academic and government administrator, and professor of political science at MIT. From 1965 to 1969, Wood served as the Under Secretary of the Department of Housing and Urban Development under President Lyndon B. Johnson, and for two weeks as acting secretary of the department.\n\nAfter his return to MIT, he directed the Joint Center for Urban Studies at MIT and Harvard. He also had a joint appointment as chairman of the Massachusetts Bay Transportation Authority. He served as president of the University of Massachusetts (1970-1977), overseeing expansion of programs, including construction of a campus in south Boston.\n\nWood was born on September 16, 1923 in St Louis, Missouri, the son of Mary (née Bradshaw) Wood and Thomas Frank Wood. He won a scholarship to Princeton University, interrupting his studies during World War II to serve in the U.S. Army. Wood saw action during Battle of the Bulge, won a Bronze Star, and rose to the rank of sergeant.\n\nAfter graduating from Princeton University, Wood earned three degrees from Harvard University: a master's in public administration, and a master's and a doctorate in government.\n\nWood taught political science at Massachusetts Institute of Technology from 1959 to 1965. From 1965 to 1969, Wood served as the Under Secretary of the newly created Department of Housing and Urban Development under President Lyndon B. Johnson.\n\nIn 1968, Wood was awarded the Wiener Medal for Cybernetics from the American Society for Cybernetics. Following the resignation of Robert C. Weaver as Secretary of HUD, Wood served briefly in the position for two weeks before George Romney took office.\n\nWood returned to MIT, where he directed the Joint Center for Urban Studies at MIT and Harvard. At the same time, he was appointed as head of the Massachusetts Bay Transportation Authority (MBTA).\n\nFrom 1970 to 1977 he served as president of the University of Massachusetts. During these years, Wood led the expansion of the university, to include UMass Medical Center in Worcester and its Boston campus. He also played a key role in bringing the John F. Kennedy Library and Museum to its site at Columbia Point, next to UMass-Boston. He also taught at Wesleyan University.\n\nWood married the former Margaret Byers, on March 22, 1952. They had three children, including the actor Frank Wood and the Governor of New Hampshire and U.S. Senator Maggie Hassan. Wood died from stomach cancer at his home in Boston, Massachusetts, on April 1, 2005.\n\nWood's best-known books are: \n\n"}
{"id": "41680", "url": "https://en.wikipedia.org/wiki?curid=41680", "title": "Scrambler", "text": "Scrambler\n\nIn telecommunications, a scrambler is a device that transposes or inverts signals or otherwise encodes a message at the sender's side to make the message unintelligible at a receiver not equipped with an appropriately set descrambling device. Whereas encryption usually refers to operations carried out in the digital domain, scrambling usually refers to operations carried out in the analog domain. Scrambling is accomplished by the addition of components to the original signal or the changing of some important component of the original signal in order to make extraction of the original signal difficult. Examples of the latter might include removing or changing vertical or horizontal sync pulses in television signals; televisions will not be able to display a picture from such a signal. Some modern scramblers are actually encryption devices, the name remaining due to the similarities in use, as opposed to internal operation.\n\nIn telecommunications and recording, a \"scrambler\" (also referred to as a \"randomizer\") is a device that manipulates a data stream before transmitting. The manipulations are reversed by a \"descrambler\" at the receiving side. Scrambling is widely used in satellite, radio relay communications and PSTN modems. A scrambler can be placed just before a FEC coder, or it can be placed after the FEC, just before the modulation or line code. A scrambler in this context has nothing to do with encrypting, as the intent is not to render the message unintelligible, but to give the transmitted data useful engineering properties.\n\nA scrambler replaces sequences (referred to as \"whitening sequences\") into other sequences without removing undesirable sequences, and as a result it changes the probability of occurrence of vexatious sequences. Clearly it is not foolproof as there are input sequences that yield all-zeros, all-ones, or other undesirable periodic output sequences. A scrambler is therefore not a good substitute for a line code, which, through a coding step, removes unwanted sequences.\n\nA scrambler (or randomizer) can be either:\n\nThere are two main reasons why scrambling is used:\n\n\nScramblers are essential components of physical layer system standards besides interleaved coding and modulation. They are usually defined based on linear feedback shift registers (LFSRs) due to their good statistical properties and ease of implementation in hardware.\n\nIt is common for physical layer standards bodies to refer to lower-layer (physical layer and link layer) encryption as scrambling as well. This may well be because (traditional) mechanisms employed are based on feedback shift registers as well.\nSome standards for digital television, such as DVB-CA and MPE, refer to encryption at the link layer as scrambling.\n\n\n\"Additive scramblers\" (they are also referred to as \"synchronous\") transform the input data stream by applying a pseudo-random binary sequence (PRBS) (by modulo-two addition). Sometimes a pre-calculated PRBS stored in the read-only memory is used, but more often it is generated by a linear-feedback shift register (LFSR).\n\nIn order to assure a synchronous operation of the transmitting and receiving LFSR (that is, \"scrambler\" and \"descrambler\"), a \"sync-word\" must be used.\n\nA sync-word is a pattern that is placed in the data stream through equal intervals (that is, in each frame). A receiver searches for a few sync-words in adjacent frames and hence determines the place when its LFSR must be reloaded with a pre-defined \"initial state\".\n\nThe \"additive descrambler\" is just the same device as the additive scrambler.\n\nAdditive scrambler/descrambler is defined by the polynomial of its LFSR (for the scrambler on the picture above, it is formula_1) and its \"initial state\".\n\n\"Multiplicative scramblers\" (also known as \"feed-through\") are called so because they perform a \"multiplication\" of the input signal by the scrambler's transfer function in Z-space. They are discrete linear time-invariant systems.\nA multiplicative scrambler is recursive, and a multiplicative descrambler is non-recursive. Unlike additive scramblers, multiplicative scramblers do not need the frame synchronization, that is why they are also called \"self-synchronizing\". Multiplicative scrambler/descrambler is defined similarly by a polynomial (for the scrambler on the picture it is formula_2), which is also a \"transfer function\" of the descrambler.\n\nScramblers have certain drawbacks: \n\nThe first voice scramblers were invented at Bell Labs in the period just before World War II. These sets consisted of electronics that could mix two signals or alternatively \"subtract\" one signal back out again. The two signals were provided by a telephone and a record player. A matching pair of records was produced, each containing the same recording of noise. The recording was played into the telephone, and the mixed signal was sent over the wire. The noise was then subtracted out at the far end using the matching record, leaving the original voice signal intact. Eavesdroppers would hear only the noisy signal, unable to understand the voice.\n\nOne of those, used (among other duties) for telephone conversations between Winston Churchill and Franklin D. Roosevelt was intercepted and unscrambled by the Germans. At least one German engineer had worked at Bell Labs before the war and came up with a way to break them. Later versions were sufficiently different that the German team was unable to unscramble them. Early versions were known as \"A-3\" (from AT&T Corporation). An unrelated device called SIGSALY was used for higher-level voice communications.\n\nThe noise was provided on large shellac phonograph records made in pairs, shipped as needed, and destroyed after use. This worked, but was enormously awkward. Just achieving synchronization of the two records proved difficult. Post-war electronics made such systems much easier to work with by creating pseudo-random noise based on a short input tone. In use, the caller would play a tone into the phone, and both scrambler units would then listen to the signal and synchronize to it. This provided limited security, however, as any listener with a basic knowledge of the electronic circuitry could often produce a machine of similar-enough settings to break into the communications.\n\nIt was the need to synchronize the scramblers that suggested to James H. Ellis the idea for non-secret encryption, which ultimately led to the invention of both the RSA encryption algorithm and Diffie–Hellman key exchange well before either was reinvented publicly by Rivest, Shamir, and Adleman, or by Diffie and Hellman.\n\nThe latest scramblers are not scramblers in the truest sense of the word, but rather digitizers combined with encryption machines. In these systems the original signal is first converted into digital form, and then the digital data is encrypted and sent. Using modern public-key systems, these \"scramblers\" are much more secure than their earlier analog counterparts. Only these types of systems are considered secure enough for sensitive data.\n\nVoice inversion scrambling can be as simple as inverting the frequency bands around a static point to various complex methods of changing the inversion point randomly and in real time and using multiple bands.\n\nThe \"scramblers\" used in cable television are designed to prevent casual signal theft, not to provide any real security. Early versions of these devices simply \"inverted\" one important component of the TV signal, re-inverting it at the client end for display. Later devices were only slightly more complex, filtering out that component entirely and then adding it by examining other portions of the signal. In both cases the circuitry could be easily built by any reasonably knowledgeable hobbyist. (See Television encryption.)\n\nElectronic kits for scrambling and descrambling are available from hobbyist suppliers. Scanner enthusiasts often use them to listen in to scrambled communications at car races and some public-service transmissions. It is also common in FRS radios. This is an easy way to learn about scrambling.\n\nThe term \"scrambling\" is sometimes incorrectly used when jamming is meant.\n\nDescramble in cable television context is the act of taking a scrambled or encrypted video signal that has been provided by a cable television company for premium television services, processed by a scrambler and then supplied over a coaxial cable and delivered to the household where a set-top box reprocesses the signal, thus descrambling it and making it available for viewing on the television set. A descrambler is a device that restores the picture and sound of a scrambled channel. A descrambler must be used with a cable converter box to be able to unencrypt all of the premium & pay-per-view channels of a Cable Television System.\n\n"}
{"id": "2887549", "url": "https://en.wikipedia.org/wiki?curid=2887549", "title": "Sigma-Aldrich", "text": "Sigma-Aldrich\n\nSigma-Aldrich Corporation is an American chemical, life science and biotechnology company owned by Merck KGaA.\n\nCreated by the 1975 merger of Sigma Chemical Company and Aldrich Chemical Company, Sigma-Aldrich since grew through various acquisitions until it had over 9,600 employees and was listed on the Fortune 1000 at the time of its acquisition by Merck. The company is headquartered in St. Louis and has operations in approximately 40 countries.\n\nIn September 2014, the German company Merck KGaA announced that it would be acquiring Sigma-Aldrich for $17 billion USD. The acquisition was completed in November 2015 and Sigma-Aldrich became a subsidiary of Merck KGaA. The company is currently a part of Merck's life science business and in combination with Merck's earlier acquired Millipore, operates as MilliporeSigma.\n\nSigma Chemical Company of St. Louis and Aldrich Chemical Company of Milwaukee were both American specialty chemical companies when they merged in August 1975. The company grew throughout the 1980s and 1990s, with significant expansion in facilities, acquisitions and diversification into new market sectors.\n\n\n\n\n\n\nKey numbers for Sigma-Aldrich.\n\nRevenues:\n\nProducts:\n\nCustomers:\n\nGeographies (% of 2008 sales):\n\nAldrich is a supplier in the research and fine chemicals market. Aldrich provides organic and inorganic chemicals, building blocks, reagents, advanced materials and stable isotopes for chemical synthesis, medicinal chemistry and materials science. Aldrich's chemicals catalog, the \"Aldrich Catalog and Handbook\" is often used as a handbook due to the inclusion of structures, physical data, and literature references.\nSigma is the Sigma-Aldrich's main biochemical supplier, with offerings including antibiotics, buffers, carbohydrates, enzymes, forensic tools, hematology and histology, nucleotides, proteins, peptides, amino acids and their derivatives.\nSigma RBI produces specialized products for use in the field of cell signaling and neuroscience. Their offerings range from standard biochemical reagents to specialized research tools, including ligands for receptors and ion channels, enzyme inhibitors, phosphospecific antibodies, key signal transduction enzymes, and assay kits for cell signaling.\nISOTEC provides isotopically labeled products for protein structure determination, peptide synthesis, proteomics, metabolic research, magnetic resonance imaging, nuclear magnetic resonance, breath test substrates, agriculture, as well as gas and gas mixes.\nRiedel-de Haën was incorporated with Sigma-Aldrich in 1999 and manufactures reagents and standards.\nSupelco is the chromatography products branch of Sigma-Aldrich. It provides chromatography columns and related tools for environmental, government, food and beverage, pharmaceutical, biotechnology, medical and chemical laboratories; sample preparation products and chemical reference standards.\nSigma-Aldrich Fine Chemicals (SAFC) is the fine chemical supply branch of Sigma-Aldrich specializing in raw materials for cell culture products; customized services for raw materials, manufacturing of active pharmaceutical ingredients.\n\nSigma Life Science provides products such as custom DNA/RNA oligos; custom DNA and LNA probes; siRNA; isotopically-labelled peptides and peptide libraries.\nSigma Advanced Genetic Engineering (SAGE) Labs is a division within Sigma-Aldrich that specializes in genetic manipulation of in vivo systems for special research and development applications. It was formed in 2008 to investigate zinc finger nuclease technology and its application for disease research models. Located in St. Louis, Missouri, SAGE Labs have developed knockout rats for the study of human diseases and disorders (such as autism), which are sold for up to US$95,000. SAGE also announced its first successful effort in creating a \"knockout rabbit\". Its facilities include a specific pathogen free, biosecure vivarium as well as research and development labs.\n\nCarbolabs produces research quantities of chemicals produced by phosgenation reactions. The company was acquired in 1998.\n\nBioReliance provides testing and manufacturing services to pharmaceutical and biopharmaceutical companies that span the product cycle from early pre-clinical development to licensed production.\nThe company was acquired by Sigma Aldrich in January 2012.\n\nSigma-Aldrich is located in 40 countries world-wide. In territories where they are not present, Sigma-Aldrich operates through a network of authorized distributors. A list of them can be found below:\n\nThe company also operates an eCommerce platform \"which offers 24-hour delivery in major markets.\"\n\nSigma-Aldrich has been recognized for its commitment to the community and environment numerous times:\n\nUdit Batra became the CEO of MilliporeSigma in 2014. Batra led the merger of SigmaAldrich and Merck Millipore, closing in November 2015.\n\n"}
{"id": "42443867", "url": "https://en.wikipedia.org/wiki?curid=42443867", "title": "Silicon Slopes", "text": "Silicon Slopes\n\nSilicon Slopes refers to the metropolitan area that primarily originates in the Salt Lake City, Utah metropolitan area but also includes Provo, Utah and Park City, Utah and surrounding areas. Served by the Salt Lake City International Airport and less than a two-hour flight from Silicon Valley, CA, Silicon Slopes has been mentioned in news media, including NPR coverage about the NSA Utah Data Center in the region.\n\nIn reference to California's Silicon Valley, Utah's \"Silicon Slopes\" encompasses a cluster of information technology, software development, and hardware manufacturing and research firms along the Wasatch Front. In particular, this grouping includes memory process technology companies SanDisk and the Intel/Micron joint venture IM Flash Technologies, video game software development companies EA Sports, and data analysis software including Adobe Systems.\n\nIt is one of a growing number of technology communities (List of places with \"Silicon\" names) and technology centers (List of technology centers) gaining status from their relationship to or similarity with Silicon Valley, California.\n\nThe term \"Silicon Slopes\" is the brainchild of Josh James (founder and CEO of Domo) for the purpose of creating a branding campaign to promote Utah's growing technology community. The nickname is derived from \"Silicon Valley,\" substituting Utah's mountains (slopes) for Northern California's Santa Clara valley.\n\nUtah has a decades-long history of government contract work and innovative Utah businesses' early involvement in the tech industry, including the creation of the Internet as the fourth node of ARPANET. These foundations were built over years, and established Utah's ability to become a leading technology center. Utah also has a strong Internet backbone; it was a Google Fiber early expansion city with service now available in Provo and Salt Lake City \n\nUtah has a long history of partnerships with the U.S. Department of Defense that have contributed to laying the groundwork for the state's high-tech business environment and infrastructure. David C. Evans, a native of Salt Lake City, was one of the original pioneers of computer science in Utah and its groundbreaking work with the DoD. During the early 1960s, Evans worked as the head of the computer science department at the University of California at Berkeley, where he was also in charge of the university's work for the Pentagon's Advanced Research Project Agency (ARPA). In 1965, Evans was recruited back to Salt Lake City to create a computer science department at the University of Utah, and he brought his DoD contacts with him, including Ivan Sutherland. Evans and Sutherland continued their work on ARPA for the DoD with their colleagues in California and helped to establish ARPANET, an \"early packet switching network and the first network to implement the protocol suite TCP/IP.\" Both of these technologies form the technical foundation of the internet. In 1969, the University of Utah was one of the original four nodes of ARPANET, cementing its place in military and technological history.\n\nDue to this early partnership with the DoD, Utah was able to encourage more joint ventures with the military. Not only was Utah capable of developing high-tech infrastructure, many geographic and natural characteristics were appealing to the DoD. Utah is geographically isolated from both the east and west coasts, providing higher security and less vulnerability to attacks. Utah also has a low incidence of large-scale natural disasters and wide open spaces that provide enough room for chemical weapons testing and drone pilot training.\n\nSome notable Utah partnerships with the U.S. military include Hill Air Force Base, Utah Test and Training Range, Dugway Proving Ground, and the Tooele Chemical Agent Disposal Facility.\n\nIn recent years, Utah has also become home to a growing number of big data processing centers. Some of these are government partnerships, such as the largest NSA data storage facility in the United States, located in Bluffdale, Utah. In order to meet the demand and facilitate more partnerships, the University of Utah recently added a new Big Data certificate program within its School of Computing. The program began in the Fall of 2014. Utah is already ahead of the curve in the national trend on big data, and training students how to understand the technicality of big data analysis will continue to attract business and military operations to Utah.\n\nExamples of early local tech businesses and founders that helped attract more start-ups to the area include: Evans & Sutherland, founded in Salt Lake City by David Evans and Ivan Sutherland in 1968, as the world's first computer graphics company (in operation for over four decades supplying advanced computer graphics technologies to the market); David C. Evans, founder and first chairman of the University of Utah School of Computing from 1965-1973; James H. Clark, founder of Silicon Graphics, Inc; John Warnock, a co-founder of Adobe Systems; Alan Ashton, co-founder of Wordperfect; Edwin Catmull, co-founder of Pixar.\n\nNovell, Inc., a Software development company founded in 1979 by Ray Noorda, produced software to connect desktop computers so they could share peripheral devices, like a Printer (computing) and Hard disks. As the price of desktop computers began to fall, Novell captured a large segment of the market with its Netware program. At their height in the early 1990s, Novell controlled 65% of the market for network operating systems in the high-tech industry.\n\nGovernor Michael Leavitt (1993-2003) was instrumental in luring many tech companies to Utah. During his time in office, Leavitt made monthly trips to Silicon Valley and used his slogan, \"We have workers, we have space, we have proximity,\" to increase his influence there. He specifically highlighted the challenges facing the Silicon Valley region: natural geographic boundaries and traffic congestion. Utah, he maintained, was the place to grow with ease. Leavitt was a key factor in enticing eBay to locate their main customer service center in Utah and in bringing in new research operations for Intel. Governor Leavitt laid the groundwork for his successors to build on his achievements and continue to make Utah a business-friendly state, particularly for high-tech companies.\n\nUtah's economic stability is also reflected in its history as a center for industrial banks and as a secondary financial hub for investment banks like Goldman Sachs that encourage venture capitalism in the state. Industrial banks, also known as Industrial Loan Corporations (ILCs), are a niche form of banking that is nearly exclusively located in Utah. Examples of Utah ILCs include BMW Bank, Pitney Bowes Bank, and Target Bank. Due to this distinction, Utah is the fourth-largest center for state-chartered banking in the nation, with nearly $280 billion in assets within its borders.\n\nUtah's industrial banks began in the early 1900s before Congress passed the Bank Holding Company Act of 1956, which severed the ties between banking and commercial corporations. Utah's ILCs were grandfathered in, so Utah's industrial-bank charter is not subject to the Bank Holding Company Act. This means that the ILCs are not regulated by the Federal Reserve; they are overseen by the FDIC and Utah's Department of Financial Services. This allows the banks to be free from more onerous federal regulation and also allows them to be more responsive to changes in the economy. By providing sound oversight in this area of banking, Utah has built credibility in the marketplace and earned a distinguished reputation in banking. This has helped to create the pro-business climate that is attractive to venture capitalists and start-up technology companies.\n\nEvidence of the prominence of the Silicon Slopes tech industry is the growing amount of venture capital coming into the area. In the first nine months of 2014, the dollar-per-deal average in the Silicon Slopes was the highest in the country, at an average of $51.3 million per deal. This high average can be attributed to big deals with key players such as Qualtrics and Domo. Qualtrics, a customer analytics software firm, brought in $150 million in venture capital funding in September 2014.\n\nUtah has continually ranked number one for best state for businesses. There is a continuous focus to create partnerships between businesses, government, education, and communities. The Governor's Office of Economic Development is based on Governor Gary Herbert's commitment to economic development statewide. In the most recent State New Economy Index, performed by the Kauffman Foundation in 2010, Utah was ranked first in the nation for Economic Dynamism and inventor patents, while placing third in fastest growing firms.\n\n\n\nUtah's economic stability has grown to meet the standards of the tech giants that occupy the Silicon Slopes area made possible in 2009 by Governor Gary Herbert who focused on four cornerstones to strengthen the economy of Utah: jobs, energy, education and self-determination. Governor Herbert credits the economic momentum in Utah to collaboration between corporate and government partnerships. The Governors Office of Economic Development, led by Spencer P. Eccles, coordinated with Governor Herbert to build on the economic development cornerstones, calling it economic development 2.0. The updated objectives to help sustain the economic growth in Utah, allowing the Silicon Slope regions to expand, strengthen, and grow existing Utah businesses, increase innovation, entrepreneurship and investment, national and international business, and prioritize education to develop the workforce of the future.\n\nUtah's location is enticing to both companies and employees. With close proximity to the Rocky Mountains, many activities are available year-round. Four distinct seasons offer such year-round activities as skiing, hiking, and rock climbing. Utah also has many national parks, including Zion National Park, Bryce Canyon National Park, and Canyonlands National Park. Salt Lake City is a Delta Airlines hub and is perfectly situated to allow for business day trips from Silicon Valley due to direct, 90-minute flights that cut travel expenses and limit travel time.\n\nUtah offers many incentives that are more attractive than other cities in the Western region: the cost of living is lower than in Seattle or Portland; the climate is not as hot as Phoenix or Albuquerque; geographic proximity is closer than Austin, allowing for less travel time and cost; and the ski slopes can be reached in less than an hour, unlike Denver.\n\nWith Salt Lake City as the headquarters for The Church of Jesus Christ of Latter-day Saints, the \"Mormon culture\" is predominant within Utah. This has both positives and negatives for attracting technology companies to the state. Utah workers have a reputation for being hard-working, dedicated, and loyal. This \"Mormon work ethic\" is attractive to tech executives because the workforce in Utah is known for its reliability and stability. Silicon Valley workers are notorious for short job tenure and frequently bouncing between multiple tech ventures. However, workers in Silicon Valley are also willing to stretch themselves to the limits - working extreme hours and participating in marathon sessions over weekends and holidays when deadlines are looming. They spend a lot of time after work together as well, making connections and forming networks. \"Mormon culture\" values family over all else, and many Silicon Slopes workers are unwilling to put in the same amount of hours as their counterparts in Silicon Valley, and they are much less likely to spend extracurricular time with their coworkers.\n\n\"Mormon culture\" can also explain the diverse and multi-linguistic workforce. Thousands of young Utahns serve missions for their church around the world and return to Utah with language skills they are able to apply in their education and future workforce participation. Mormons also tend to marry early and are more likely to have larger-than-average families. They value education and encourage their children to go to college and earn degrees.\n\nUtah boasts a highly educated, technologically proficient, multilingual workforce, speaking over 130 languages. Many residents of Utah lived and worked abroad which contributes to the flexibility and capabilities of immediately contributing to companies. Utah also maintains the youngest population in the United States due to its higher-than-average birth rate. The median age in Utah is 29.6 years, compared to the nationwide median of 37.3 years. This young labor force is very attractive to employers as baby boomers throughout the nation retire and many states find it difficult to replenish their workforce due to falling birth rates.\n\nProminent schools that contribute to the Utah workforce include three major universities,Brigham Young University, University of Utah and Utah State University having a primary emphasis on entrepreneurship, innovation, and research and development, followed by Utah Valley University, Weber State University, Salt Lake Community College, Dixie State University and Westminster College. In the early 2000s, the Utah Legislature allocated around $100 million to the state's universities in order to bolster their computer science programs and significantly increase the number of graduates in the field. The universities also provided matching funding and created many new courses and areas of technological study. Computer science graduates continue to rise in Utah, with Northern Utah universities boasting 663 graduate students in 2013, up 47 percent from 2009. Of the six schools listed above, students graduating in Computer Science with a bachelor's degree have risen 44 percent since 2009. Students graduating with a master's degree rose 46 percent and those getting doctorates increased by 61 percent. These computer science programs are garnering interest due to demand from the companies found in the Silicon Slopes. As of August 2015, the Utah Technology Council achieved \"mainstreaming Computer Science\" in high schools by allowing students to take credited rigorous Computer Science courses as a Science credit option.\n\nThe University of Utah's Entertainment Arts & Engineering undergraduate program is No. 1 for game design in the U.S., and its graduate program consistently ranks in the top three according to The Princeton Review. The EAE program began in 2007 and is an interdisciplinary program between engineering, computer science, and fine arts. It has professional facilities and a motion capture system that allows students to have real-world experience, making them very attractive to game developers and tech companies upon graduation. Beyond video games, students also produce games and apps that are used in a wide variety of industries, especially in the medical field. The EAE program collaborates closely with the Center for Medical Innovation at the University of Utah.\n\nBYU also has a strong computer science program, and its Center for Animation is regularly ranked among the top five schools for animation in the U.S. The program began in 2010 after a wealthy homebuilder, Ira A. Fulton, purchased a supercomputer for the school, allowing them to produce top quality animation. Many of the faculty members came from companies such as Disney, Pixar, DreamWorks, and Warner Brothers. Student-produced films win regular awards and have been screened at many film festivals, including Sundance and Cannes.\n\nThe University of Utah and BYU are also both ranked within the top 25 schools for entrepreneurship according to The Princeton Review. The Rollins Center for Entrepreneurship and Technology at BYU is ranked No. 2 in the U.S. It first began in 1971 and now focuses most specifically on tech start-ups. Over the past ten years, students of the program have started 196 companies and have attracted more than $772 million in funding. The entrepreneurship program at the University of Utah began in 2004, and it is currently ranked at No. 24. Over the past ten years, its students have started 167 companies and raised nearly $750,000. The program continues to be strengthened, and at the start of the fall semester in August 2016, Lassonde Studios, an institute that combines 400 residences with a 20,000-square-foot garage (used for building prototypes and holding events and conferences) will be opened to create a hub for innovators, makers, and entrepreneurs of all disciplines to connect.\n\nA key contributor to Silicon Slopes' successful tech industry is the well-planned Internet infrastructure. Salt Lake City is in a fortunate geographic location because it sits along the major east-west Internet corridor where ten major service providers interconnect with each other and deliver high-speed services to the area. This \"internet backbone\" is a critical aspect of Utah's high-tech industry success.\n\nIn Provo, the city government started planning a fiber-optic Internet infrastructure in the late 1990s. This high-speed Internet system was funded as a private-public partnership called iProvo and construction was completed in 2006. Google Fiber later acquired iProvo in 2013 and now offers Internet speeds up to 1,000 Mbit/s for businesses and households.\n\nIn response to the high-tech industry located in Salt Lake City and its relationship with Provo, Google officially announced in March 2015 that Salt Lake City would be one of the next places to receive Google Fiber. Construction is currently ongoing, but significant progress has been made, and residents and businesses will soon be able to sign-up for lightning-fast internet service. Requests for business permits in the area have drastically increased in response to the soon-to-be-available service.\n\nThe State of Utah has offered a variety of incentives to encourage large tech companies to call Utah home, and the Silicon Slopes uprising would suggest that these perks have been motivating. The Utah Governor's Office of Economic Development allows a variety of grants and tax incentives to companies willing to either relocate or expand their enterprise, depending on the stability of the company and the types of jobs that are being brought to the state. From the state's perspective, they are trying to attract companies will positions that require a high level of education or skill to motivate Utah graduates to remain here in the state and help drive economic growth.\n\nNumerous publications and studies rank Utah as one of the top \"best states for business\" and most \"fiscally fit\" In the U.S. Chamber of Commerce's \"Enterprising States\" report, Utah ranks in the top five in every export category and was the only state to finish in the top 10 for all six metrics. Utah tops Governing Magazine's list of the best managed states.\n\nTo qualify for the tax benefits or grant programs associated with the Silicon Slopes region, there are specific guidelines in place to determine if the state will offer incentives to encourage relocation. These incentives are evaluated on a case by case basis by the Governor's Office and the Executive Director. Some of these standards include the industry the company is in, historical successes of the company, revenues that are raised and the types of jobs that were created. The jobs that are being brought in need to require specific qualifications and maintain certain salaries to maintain a well-educated, professional workforce within the state. To monitor these qualifications, grants and tax credits are only awarded after each corporation has proven its ability to provide the jobs and revenue required.\n\nA motivational factor for the State of Utah to provide incentives to corporations for expansion or relocation is the development of rural areas or deteriorating regions. The Silicon Slopes region is located between two of Utah's largest metropolises, Salt Lake City and Provo. Historically, this area has been largely underdeveloped, with the exception of the much smaller cities of Alpine, Highland, American Fork, Lehi, Pleasant Grove, Lindon, and Orem City. Orem is the largest of the, with a population of just under 100,000. To accelerate the development of this area and surrounding areas, the Governor's Office of Economic Development (GOED) established the Economic Development Tax Increment Financing (EDTID) tax credit.\n\nAdditionally,the Utah Office of Rural Development created the Utah Enterprise Zone Tax Credit to motivate business to choose locations that would benefit these rural areas. These credits can compound depending on employee wages compared to surrounding businesses, and are structured to encourage strong economic growth and professional retention to the area.\n\nThe United States Department of Agriculture (USDA) has also partnered with the State of Utah to provide other motivating factors to prospective employees of Silicon Slope companies. Homes can be purchased with specifically structured mortgages that require little to no down payment, and can include extra financing to update or refurnish older homes.\n"}
{"id": "22121847", "url": "https://en.wikipedia.org/wiki?curid=22121847", "title": "Smartrac", "text": "Smartrac\n\nSmartrac N.V. is a Dutch manufacturer of high security RFID inlays. It is the world's largest supplier of inlays for ePassports. Since 2006 its shares are listed on the Frankfurt Stock Exchange.\n\nThe JPMorgan-owned private equity firm One Equity Partners agreed to the leveraged buyout of Smartrac on 30 August 2010, resulting in the removal of the company from the TecDAX index on 19 November 2010.\n"}
{"id": "1249227", "url": "https://en.wikipedia.org/wiki?curid=1249227", "title": "Surface micromachining", "text": "Surface micromachining\n\nSurface micromachining builds microstructures by deposition and etching structural layers over a substrate. This is different from Bulk micromachining, in which a silicon substrate wafer is selectively etched to produce structures. \n\nGenerally, polysilicon is used as one of the substrate layers while silicon dioxide is used as a \"sacrificial layer.\" The sacrificial layer is removed or etched out to create any necessary void in the thickness direction. Added layers tend to be vary in size from 2-5 micrometres. The main advantage of this machining process is the ability to build electronic and mechanical components (functions) on the same substrate. Surface micro-machined components are smaller compared to their bulk micro-machined counterparts.\n\nAs the structures are built on top of the substrate and not inside it, the substrate's properties are not as important as in bulk micro-machining. Expensive silicon wafers can be replaced by cheaper substrates, such as glass or plastic. The size of the substrates may be larger than a silicon wafer, and surface micro-machining is used to produce thin-film transistors on large area glass substrates for flat panel displays. This technology can also be used for the manufacture of thin film solar cells, which can be deposited on glass, polyethylene terepthalate substrates or other non-rigid materials.\n\nMicro-machining starts with a silicon wafer or other substrate upon which new layers are grown. These layers are selectively etched by photo-lithography; either a wet etch involving an acid, or a dry etch involving an ionized gas (or plasma). Dry etching can combine chemical etching with physical etching or ion bombardment. Surface micro-machining involves as many layers as are needed with a different mask (producing a different pattern) on each layer. Modern integrated circuit fabrication uses this technique and can use as many as 100 layers. Micro-machining is a younger technology and usually uses no more than 5 or 6 layers. Surface micro-machining uses developed technology (although sometimes not enough for demanding applications) which is easily repeatable for volume production.\n\nA sacrificial layer is used to build complicated components, such as movable parts. For example, a suspended cantilever can be built by depositing and structuring a sacrificial layer, which is then selectively removed at the locations where the future beams must be attached to the substrate (i.e. the anchor points). A structural layer is then deposited on top of the polymer and structured to define the beams. Finally, the sacrificial layer is removed to release the beams, using a selective etch process that does not not damage the structural layer.\n\nMany combinations of structural and sacrificial layers are possible. The combination chosen depends on the process. For example, it is important for the structural layer not to be damaged by the process used to remove the sacrificial layer.\n\nSurface Micro-machining can be seen in action in the following MEMS (Microelectromechanical) products:\n\n\n"}
{"id": "29284770", "url": "https://en.wikipedia.org/wiki?curid=29284770", "title": "Thermal scanning probe lithography", "text": "Thermal scanning probe lithography\n\nThermal scanning probe lithography (t-SPL) is a form of scanning probe lithography (SPL) whereby material is structured on the nanoscale using scanning probes, primarily through the application of thermal energy.\n\nRelated fields are \"thermo-mechanical\" \"SPL\" (see also Millipede memory), \"thermochemical\" \"SPL\" (or thermochemical nanolithography) where the goal is to influence the local chemistry, and \"thermal\" Dip Pen Lithography as an additive technique.\n\nScientists around Daniel Rugar and John Mamin at the IBM research laboratories in Almaden have been the pioneers in using heated AFM (atomic force microscope) probes for the modification of surfaces. In 1992, they used microsecond laser pulses to heat AFM tips to write indents as small as 150 nm into the polymer PMMA at rates of 100 kHz. In the following years, they developed cantilevers with resonance frequencies above 4 MHz and integrated resistive heaters and piezoresistive sensors for writing and reading of data. This thermo-mechanical data storage concept formed the basis of the Millipede project which was initialized by Peter Vettiger and Gerd Binnig at the IBM Research laboratories Zurich in 1995. It was an example of a memory storage device with a large array of parallel probes, which was however never commercialized due to growing competition from non-volatile memory such as flash memory. The storage medium of the Millipede memory consisted of polymers with shape memory functionality, like e.g. cross-linked polystyrene, in order to allow to write data indents by plastic deformation and erasing of the data again by heating. However, evaporation instead of plastic deformation was necessary for nanolithography applications to be able to create any pattern in the resist. Such local evaporation of resist induced by a heated tip could achieved for several materials like Pentaerythritol tetranitrate, cross-linked polycarbonates, and Diels-Alder polymers. Significant progress in the choice of resist material was made in 2010 at IBM Research in Zurich, leading to high resolution and precise 3D-relief patterning with the use of the self-amplified depolymerization polymer polyphthalaldehyde (PPA) and molecular glasses as resist, where the polymer decomposes into volatile monomers upon heating with the tip without the application of mechanical force and without pile-up or residues of the resist.\n\nThe thermal cantilevers are fabricated from silicon wafers using bulk- and surface micro-machining processes. Probes have a radius of curvatures below 5 nm, enabling sub-10 nm resolution in the resist. The resistive heating is carried out by integrated micro-heaters in the cantilever legs which are created by different levels of doping. The time constant of the heaters lies between 5 μs to 100 μs. Electromigration limits the longterm sustainable heater temperature to 700–800 °C. The integrated heaters enable in-situ metrology of the written patterns, allowing feedback control, field stitching without the use of alignment markers and using pre-patterned structures as reference for sub-5 nm overlay. Pattern transfer for semiconductor device fabrication including reactive ion etching and metal lift-off has been demonstrated with sub-20 nm resolution.\n\nDue to the ablative nature of the patterning process, no development step (as in: selective removal of either the exposed or non-exposed regions of the resist as for e-beam and optical lithography) is needed, neither are optical proximity corrections. Maximum linear writing speeds of up to 20 mm/s have been shown with throughputs in the 10 - 10 μm h range which is comparable to single-column, Gaussian-shaped e-beam using HSQ as resist. The resolution of t-SPL is determined by the probe tip shape and not limited by the diffraction limit or by the focal spot size of beam approaches, however, tip-sample interactions during the in-situ metrology process create tip wear, limiting the lifetime of the probes. In order to extend the lifetime of the probe tips, Ultrananocrystalline diamond (UNCD) and Silicon-Carbide (SiC)-coated tips or wear-less floating contact imaging methods have been demonstrated. No electron damage or charging is caused to the patterned surfaces due to the absence of electron or ion beams.\n\n"}
{"id": "39244081", "url": "https://en.wikipedia.org/wiki?curid=39244081", "title": "Treehouse (company)", "text": "Treehouse (company)\n\nTreehouse or (Teamtreehouse) is an online technology school that offers beginner to advanced courses in web design, web development, mobile development and game development taught by a team of expert teachers. Its courses are aimed at beginners looking to learn coding skills for a career in the tech industry.\n\nThe Treehouse learning program includes videos combined with interactive quizzes and code challenges. Treehouse Tracks are guided curricula, composed of courses that train students in large topic areas. Treehouse Organizations is designed to assist businesses, organizations, schools and community programs in technology training. Companies including Simple and LivingSocial currently use Treehouse to recruit new employees based on their progress and achievements on Treehouse.\n\nIn April 2012, Treehouse raised $4.75M in funding. In April 2013, Treehouse closed a US$7 million Series-B fundraising round led by Social+Capital and Kaplan bringing its total raised capital to $12.6 million. In May 2016, Treehouse launched the Techdegree Program, a learning program focused on helping people prepare for a tech career.\n\nTreehouse was founded by Ryan Carson, a computer science graduate from Colorado State University. After graduating, Carson worked for Fingal, a design agency in London before moving to Bath, United Kingdom. In 2004, Ryan Carson and his wife Gillian Carson founded Carsonified, a company built towards training web designers and developers based in Bath, United Kingdom. In addition to training designers and developers, Carsonified also created web applications and hosted web industry events, conferences and workshops. In 2008 Carsonified sold its file-sharing application DropSend to the Florida-based company Webminds. In 2011, Ryan Carson sold Carsonified which is now named Future Insights and started working on his new project, Treehouse.\n\nRyan Carson founded Treehouse in 2011, a project that emerged from Carson's previous company, Carsonified, and its video-tutorial service Think Vitamin Membership. Carson decided to redesign and rebrand the service as Treehouse because the name \"reflects the wonder of learning as a child.\" Treehouse's mission is to offer affordable technology education to users globally. In 2011, Treehouse opened their first office in Orlando, Florida, In 2012 they opened their second office and moved Treehouse HQ to Portland, Oregon. In July 2013, Treehouse released its first iPad app for accessing Treehouse's content. Treehouse also released an Android application in 2014 and added a course for Apple's Swift programming language. Carson is currently the active CEO of Treehouse.\n\nIn May 2016, Treehouse announced the launch of the Techdegree Program. The Techdegree program is a guided learning program that is designed to help students prepare for entry-level development jobs. The Techdegree program is available for six competencies: Android, front-end web development, Full Stack Javascript, iOS, Java, and Python.\n\nProgram length varies by Techdegree. Below is a breakout of how much time it takes to complete a program, including courses and projects.\n\nUp until 2015, Treehouse had a four-day work week and no managers, and practiced open allocation. In August 2015 the company introduced middle management and temporarily stopped practicing the four-day work weeks to help its growing size (with about 100 employees). In early 2016, they resumed the four-day work week.\n\nOriginally, co-founder Ryan Carson established the short work week in an effort to prevent burnout and reward employees by limiting the number of hours they worked.\n\n\n"}
