{"id": "46580274", "url": "https://en.wikipedia.org/wiki?curid=46580274", "title": "3D Manufacturing Format", "text": "3D Manufacturing Format\n\n3D Manufacturing Format or 3MF is a file format standard developed and published by the 3MF Consortium. \n\n3MF is an XML-based data format designed for using additive manufacturing, including information about materials, colors, and other information that cannot be represented in the STL format.\n\nAs of today, CAD software related companies such as Autodesk, Dassault Systems and Netfabb are part of the 3MF Consortium. Other firms in the 3MF Consortium are Microsoft (for Operating system support), SLM and HP, whilst Shapeways are also included to give insight from a 3D Printing background. Other key players in the 3D printing and additive manufacturing business, such as Materialise, 3D Systems, Siemens PLM Software and Stratasys have recently joined the consortium. To facilitate the adoption, 3MF Consortium has also published a C++ implementation of the 3MF file format.\n\n"}
{"id": "30752097", "url": "https://en.wikipedia.org/wiki?curid=30752097", "title": "All Things Digital", "text": "All Things Digital\n\nAllThingsD.com was an online publication that specialized in technology and startup company news, analysis and coverage. It was founded in 2007 as an extension of the annual meetings \"D: All Things Digital Conference\" by Kara Swisher and Walt Mossberg.\nIt was a wholly owned subsidiary of Dow Jones & Company Inc., and was a member of \"The Wall Street Journal\"s Digital Network, which includes WSJ.com, MarketWatch, Barron's, and SmartMoney.\n\nIn September 2013 Swisher and Mossberg failed to renew their agreement with Dow Jones. On January 1, 2014, Swisher and Mossberg introduced their own site, \"Re/code\", based in San Francisco, California.\nThe \"AllThingsD\" logo can be spotted during the Season 2 opening credits of the HBO series \"Silicon Valley\", before being taken down and replaced by the \"Re/code\" logo as the intro animation progresses.\n\nAllThingsD.com expanded upon the All Things Digital conference, which was launched in 2003 by Swisher and Mossberg. While the conference quickly became popular and prestigious among the business and technology communities, the number of attendees was limited to approximately 500 people. The web site was set up to “open the conversation to everyone.” Although the site operates year-round, during each “D” Conference it offers comprehensive and direct coverage of all events and presentations\nAllThingsD.com focused on news, analysis and opinion on technology, the Internet and media, but considered itself a fusion of diverse media styles, different topics, formats and sources. Initially, the two main features of the site were Kara Swisher's BoomTown blog, and Walt Mossberg's technology product review columns from the Wall Street Journal. Since then, the site had expanded greatly in personnel and focus. Although most of the staff were based in San Francisco, many contributors, including Mossberg, worked primarily in other parts of the United States.\n\nAllThingsD.com featured nine different writers at the time of closure, where each had their own section of the site, as well as a separate category for other featured writers, both within and outside of the publication:\n\n\nAllThingsD.com also hosted content concerning its D Conferences; besides the annual main event in late Spring, in December 2010 they hosted D: Dive Into Mobile, the first brand extension of the conference in which representatives from leading mobile device and software producers were interviewed by members of AllThingsD.\n\nAll Things Digital utilized Livefyre to permit readers to comment on news stories.\n\n"}
{"id": "44229187", "url": "https://en.wikipedia.org/wiki?curid=44229187", "title": "Avalanche Canada", "text": "Avalanche Canada\n\nAvalanche Canada is a non-government, non-profit organization whose vision is to eliminate avalanche fatalities and injuries in Canada. Avalanche Canada is Canada's national public avalanche safety organization. Based in Revelstoke, British Columbia, the organization's aim is to minimize public avalanche risk in avalanche terrain. Avalanche Canada develops and delivers public avalanche forecasts and special public avalanche warnings for many of the mountainous regions of western Canada, free of charge. Avalanche Canada also provides curriculum and support to instructors of recreational Avalanche Skills Training courses, delivers public avalanche education awareness and education programs, encourages public avalanche research, provides curriculum to teachers and organizations, and acts as a central hub for avalanche information.\n\nThe organization was formed in 2004 as the Canadian Avalanche Centre, following a recommendation for the establishment for a national avalanche centre, which was made in a report prepared for the Government of British Columbia. This report was written after 29 people were killed by avalanches in Canada during the winter of 2002-03, including seven high school students in the 2003 Connaught Creek Valley avalanche.\n\nIn October 2014 the Canadian Avalanche Centre officially changed its name to Avalanche Canada.\n\n"}
{"id": "307455", "url": "https://en.wikipedia.org/wiki?curid=307455", "title": "Avaya", "text": "Avaya\n\nAvaya () is an American multinational technology company headquartered in Santa Clara, California that specializes in business communications, specifically unified communications (UC), contact center (CC), and services. Serving organizations at 220,000 customer locations worldwide, Avaya is the largest pure-play UC and CC company, ranking No. 1 in CC and No. 2 in UC and collaboration. The company had FY17 revenues of $3.3 billion, 78% of which was attributed to software and services.\n\nIn late 2017 to early 2018, the company emerged with a new leadership team and a revised strategy based on accelerating open, cloud-first architecture; integrating artificial intelligence (AI), blockchain and emerging technologies, and transforming the business through increased investment in innovation.\n\nIn 1995, Lucent Technologies was spun off from AT&T, and Lucent spun off units of its own in an attempt to restructure its struggling operations.\n\nAvaya was then spun off as its own company in 2000. It remained a public company from 2000 to 2007, when it was purchased by private equity firms.\n\nIn 2001, the Mark Avaya Interaction Center for customer relationship management began, enabling businesses to draw multi-platform call centers to multimedia, multi-site contact centers. A proposed \"converged communications\" road map focused on the role that applications would play in making communications improve business performance.\n\nOn December 15, 2017, it again became a public company, trading under the stock ticker AVYA.\n\nIn October 2007, Avaya was acquired by two private-equity firms, TPG Capital and Silver Lake Partners, for $8.2 billion and the company was delisted on the New York Stock Exchange. The following year, Avaya Speech to Text (enabling voicemail messages to be read on mobile devices or computers) and Avaya Unified Communications (focusing on role-based communications for teleworkers, home agents, small-business mobile workers, branch-office integration, retail stores and branch banking) were introduced, and Kevin Kennedy became the company's CEO and president.\n\nIn 2009, the Avaya Aura for integrated communications was introduced, and in December the company acquired Nortel Enterprise's assets for $900 million. The following year, Avaya was the converged-network equipment supplier for the 2010 Winter Olympics and Paralympics, and Avaya Aura Contact Center was introduced. In June 2011, Avaya filed an application with the U.S. Securities and Exchange Commission to raise up to $1 billion in an initial public offering. On October 4, 2011, the company reported that it was acquiring Sipera Systems for its session border controller (SBC) and unified communications security applications. On October 19, 2011, it was reported that Avaya would buy Aurix. Shareholders approved the acquisition of Radvision for about $230 million on April 30, 2012, and the deal closed in June.\n\nAccording to May 2016 news articles citing \"internal sources\", Avaya's private-equity owners (Silver Lake Partners and TPG Capital) considered a sale of the company valued at $6 to $10 billion including debt. During the company's earnings call that month, CEO Kevin Kennedy had confirmed that Goldman Sachs was helping Avaya evaluate expressions of interest received relative to specific assets and explore other potential opportunities. In November, Avaya considered chapter 11 bankruptcy while trying to sell its call-center business. On January 19, 2017 Avaya filed for bankruptcy protection under Chapter 11, saying that its foreign operations would be unaffected. In its petition, the company listed $5.5 billion in assets and $6.3 billion in debts.\n\nIn an effort to monetize its assets during the bankruptcy period, Avaya announced in March 2017 it would sell its networking business and associated products to Extreme Networks for $100 million USD. The sale was finalized in July 2017.\n\nIn late 2017 to early 2018, the company emerged with a new leadership team under President and CEO Jim Chirico and a revised strategy based on accelerating open, cloud-first architecture; integrating artificial intelligence (AI), blockchain and emerging technologies, and transforming the business through increased investment in innovation. On December 15, 2017, Avaya again became a public company, trading under the stock ticker AVYA. In mid-2018, the company announced that Avaya was positioned as a Leader in the Gartner Magic Quadrant for Unified Communications, the ninth time that the company has been in a Leader position—and was also positioned as a Leader in the Gartner Magic Quadrant for Contact Center Infrastructure for the seventeenth time. Companies in the Leaders quadrant of the Gartner Magic Quadrant are defined as companies that “execute well against their current vision and are well positioned for tomorrow.”\n\nSince 2001, Avaya has sold and acquired several companies, including VPNet Technologies, VISTA Information Technologies, Quintus, RouteScience, Tenovis, Spectel, NimCat Networks, Traverse Networks, Ubiquity Software Corporation, Agile Software NZ Limited, Konftel, Sipera, Aurix, Radvision and Esnatech. Through Nortel's bankruptcy proceedings, assets related to their Enterprise Voice and Data business units were auctioned. Avaya placed a $900 million bid, and was announced as the winner of the assets on September 14, 2009.\n\nIn 2018, Avaya acquired Spoken Communications, a leading innovator in Contact Center as a Service (CCaaS) solutions and customer experience management applications built on conversational artificial intelligence. The Spoken platform accelerates Avaya's growth in cloud-based solutions and provides a reliable and highly scalable cloud platform for customers of all sizes.\n\nAvaya's headquarters are at 4655 Great America Parkway, Santa Clara, California. The company had offices in over 145 countries in 2011. Avaya sponsors a users' group and training programs for IT professional certification in the use of Avaya's products. In 1985, Performance Engineering Corporation (later PEC Solutions) was formed to offer technology services to government customers. On June 6, 2005, Nortel acquired PEC Solutions to form Nortel PEC Solutions. On January 18, 2006, Nortel PEC Solutions was renamed Nortel Government Solutions. On December 21, 2009, Avaya acquired Nortel's government business as part of the company's assets sale.\n\nAvaya bought Nortel Enterprise and acquired its patents, including:\n"}
{"id": "2848446", "url": "https://en.wikipedia.org/wiki?curid=2848446", "title": "Boom barrier", "text": "Boom barrier\n\nA boom barrier, also known as a boom gate, is a bar, or pole pivoted to allow the boom to block vehicular access through a controlled point. Typically the tip of a boom gate rises in a vertical arc to a near vertical position. Boom gates are often counterweighted, so the pole is easily tipped. Boom gates are often paired either end to end, or offset appropriately to block traffic in both directions. Some boom gates also have a second arm which hangs 300 to 400 mm below the upper arm when lowered, to increase approach visibility, and which hangs on links so it lies flat with the main boom as the barrier is raised. Some barriers also feature a pivot roughly half way, where as the barrier is raised, the outermost half remains horizontal, with the barrier resembling an upside-down \"L\" when raised.\n\nThere are various technologies for an automatic boom barrier. One of them is electro-mechanical, which is widely used due to its reliability. The other technologies are often manufacturer specific. \nThese electro-mechanical devices come with 24VDC drive units which can run continuously without generating heat, so electro-mechanical boom barriers can be operated continuously and in an intensive duty cycle.\n\nAn automatic boom barrier can be operated through:\n\nBoom gates are typically found at Level crossings, drawbridges, parking facilities, checkpoints and entrances to restricted areas. They are also the usual method for controlling passage through toll booths, and can also be found on some freeway entrance ramps which are automatically controlled to drop to restrict traffic in the event of accident cleanup or road closures without the need to dispatch road workers or law enforcement to use a vehicle to block the way.\nSome boom gates are automatic and powered, others are manually operated. Manual gates are sometimes hung in the manner of a normal gate (i.e. hinged horizontally).\nIn some places, boom gates are installed across suburban streets as a traffic calming measure, preventing through traffic, while allowing authorised vehicles such as emergency services and buses to take advantage of the shorter and more direct route.\n\n"}
{"id": "8314595", "url": "https://en.wikipedia.org/wiki?curid=8314595", "title": "Cadcorp", "text": "Cadcorp\n\nComputer Aided Development Corporation Ltd. (Cadcorp) is a British owned and run company established in 1991. Cadcorp has its headquarters in Stevenage, Hertfordshire, U.K.. Cadcorp has a network of distributors and value added resellers (VARs) around the world.\n\nCadcorp is an ISO 9001:2000 and ISO/IEC 27001:2005 certified company, an Oracle Corporation partner, a Microsoft SQL Server Spatial Partner, an Ordnance Survey Licensed Developer Partner, and a corporate member of the Association for Geographic Information (AGI) in the U.K..\n\nCadcorp’s first product was a Microsoft Windows-based CAD system called Wincad. All rights to the product were sold in 1994. Wincad development and maintenance, carried out latterly by Informatix Inc., Japan, through their UK subsidiary under the brand name MicroGDS, was stopped in March 2013. After selling Wincad, Cadcorp moved on to developing geographic information system (GIS) software. The first version of Cadcorp SIS – Spatial Information System (\"Cadcorp SIS\") was released in 1995.\n\nThe leadership team successfully completed a management buyout of the company in May 2015.\n\nCadcorp SIS has applications in the following UK markets\n\n\nCadcorp has been a member of the Open Geospatial Consortium (OGC) since 1997. In 2004, Cadcorp’s technical director, Martin Daly was awarded the OGC Kenneth G. Gardels Award, made annually to an individual who has made outstanding contributions to advancing the OGC vision of geographic information fully integrated into the worlds information systems.\n\nSeveral versions of the Cadcorp SIS product suite are certified OGC compliant in the categories of:\n\n\nCadcorp SIS also implements support for:\n\n\nCadcorp SIS is available in different forms:\n\n\n"}
{"id": "1953222", "url": "https://en.wikipedia.org/wiki?curid=1953222", "title": "Cash advance", "text": "Cash advance\n\nA cash advance is a service provided by most credit card and charge card issuers. The service allows cardholders to withdraw cash, either through an ATM or over the counter at a bank or other financial agency, up to a certain limit. For a credit card, this will be the credit limit (or some percentage of it).\n\nCash advances often incur a fee of 3 to 5 percent of the amount being borrowed. When made on a credit card, the interest is often higher than other credit card transactions. The interest compounds daily starting from the day cash is borrowed.\n\nSome \"purchases\" made with a credit card of items that are viewed as cash are also considered to be cash advances in accordance with the credit card network's guidelines, thereby incurring the higher interest rate and the lack of the grace period. These often include money orders, prepaid debit cards, lottery tickets, gaming chips, and certain taxes and fees paid to certain governments. However, should the merchant not disclose the actual nature of the transactions, these will be processed as regular credit card transactions. Many merchants have passed on the credit card processing fees to the credit card holders in spite of the credit card network's guidelines, which state the credit card holders should not have any extra fee for doing a transaction with a credit card.\n\nUnder card scheme rules, a credit card holder presenting an accepted form of identification must be issued a cash advance over the counter at any bank which issues that type of credit card, even if the cardholder cannot give their PIN.\n"}
{"id": "7825", "url": "https://en.wikipedia.org/wiki?curid=7825", "title": "Celestial globe", "text": "Celestial globe\n\nCelestial globes show the apparent positions of the stars in the sky. They omit the Sun, Moon, and planets because the positions of these bodies vary relative to those of the stars, but the ecliptic, along which the Sun moves, is indicated.\n\nThere is an issue regarding the “handedness” of celestial globes. If the globe is constructed so that the stars are in the positions they actually occupy on the imaginary celestial sphere, then the star field will appear back-to-front on the surface of the globe (all the constellations will appear as their mirror images). This is because the view from Earth, positioned at the centre of the celestial sphere, is of the \"inside\" of the celestial sphere, whereas the celestial globe is viewed from the \"outside\". For this reason, celestial globes are often produced in mirror image, so that at least the constellations appear the “right way round”. Some modern celestial globes address this problem by making the surface of the globe transparent. The stars can then be placed in their proper positions and viewed \"through\" the globe, so that the view is of the inside of the celestial sphere. However, the proper position from which to view the sphere would be from its centre, but the viewer of a transparent globe must be outside it, far from its centre. Viewing the inside of the sphere from the outside, through its transparent surface, produces serious distortions. Opaque celestial globes that are made with the constellations correctly placed, so they appear as mirror images when directly viewed from outside the globe, are often viewed in a mirror, so the constellations have their familiar appearances. Written material on the globe, e.g. constellation names, is printed in reverse, so it can easily be read in the mirror.\n\n"}
{"id": "43281341", "url": "https://en.wikipedia.org/wiki?curid=43281341", "title": "Certified Payment-Card Industry Security Implementer", "text": "Certified Payment-Card Industry Security Implementer\n\nCertified Payment-Card Industry Security Implementer (CPISI) is a certification in the field of Payment Card Industry Data Security Standard (PCI DSS).\n\nThe Payment Card Industry Security Standards Council (PCI SSC) manages the standard and certifies training organizations. PCI SSC is a collective formed by MasterCard, Visa, American Express, JCB and Discover.\n\n\n"}
{"id": "47453769", "url": "https://en.wikipedia.org/wiki?curid=47453769", "title": "Cheshire inventions, innovations and firsts", "text": "Cheshire inventions, innovations and firsts\n\nInventions, innovations and firsts made in the county of Cheshire, in North West England. \n\nPolythene — invented in 1933, at ICI in Northwich.\n\nPulsation Engine — a precursor to the hydraulic ram, invented by John Whitehurst in Oulton Park.\n\nFirst observation of extragalactic radio source — the Andromeda nebula (M31) made at Jodrell Bank in 1950 \n\nCheshire Cheese — there are conflicting accounts of Cheshire Cheese being mentioned in the Domesday Book, and so the first county cheese, or, along with Shropshire Cheese, recorded much later in 1580.\n\nUK's first Neighbourhood Watch\n\nFirst known stagecoach ran between Birmingham and Holywell via Nantwich and Chester, 1637.\n\nFirst dementia-friendly checkout — Tesco supermarket in Chester claimed to be the \"First known supermarket to implement a dementia-friendly checkout.\"\n"}
{"id": "9381411", "url": "https://en.wikipedia.org/wiki?curid=9381411", "title": "Confederation of European Environmental Engineering Societies", "text": "Confederation of European Environmental Engineering Societies\n\nThe Confederation of European Environmental Engineering Societies (CEEES) was created as a co-operative international organization for information exchange regarding environmental engineering between the various European societies in this field.\n\nThe CEEES maintains an online public discussion forum for the interchange of information.\n\nAs of 2012, these were the twelve member societies of the CEEES:\n\n\nEach member society successively holds the presidency and the secretariat for a period of two years.\n\nThe CEEES has three major Technical Advisory Boards:\n\n\nThese are some of the publications of the CEEES:\n\n\n\n"}
{"id": "440024", "url": "https://en.wikipedia.org/wiki?curid=440024", "title": "Cosmetology", "text": "Cosmetology\n\nCosmetology (from Greek , \"kosmētikos\", \"beautifying\"; and , \"-logia\") is the study and application of beauty treatment. Branches of specialty include hairstyling, skin care, cosmetics, manicures/pedicures, non permanent hair removal such as waxing and sugaring and permanent hair removal processes such as electrology and Intense Pulsed Light (IPL).\n\nCosmetologists are trained and licensed to perform cosmetic treatments to the hair, skin, and nails. This can be expanded into multiple parts including cutting and chemically treating hair, chemical hair removal without a sharp blade, fashion trends, wigs, nails and skin care, skin and hair analysis; relaxation techniques including head, neck, scalp, hand and feet basic massage and aroma therapies; plus ability to expertly apply makeup applications to cover or promote and can expand into further specialties such as reflexology; theatrical applications; cosmetics and others as listed below.\nA cosmetologist is someone who is an expert in the care of hair and makeup as well as skincare and beauty products. They can also offer other services such as coloring, extensions, perms and straightening. Cosmetologists help their clients improve on or acquire a certain look by applying advance trending aesthetic applications. Hair stylists often style hair for weddings, proms, and other special events in addition to routine hair styling.\n\nA hair color specialist, aka hair colorist, specializes in the modification of natural hair color utilizing various application methods while using a colorant product from a professional company. In the US, some colorists are qualified through the American Board of Certified Hair Colorists. This designation is used to recognize colorists that have a greater level of competency in the industry through a written exam and a practical exam. A hair color specialist's duties might include, but are not limited to, basic color applications like covering grey and lightening or darkening natural hair color. A color specialist also has the ability to perform corrective color applications and create special effects using foiling techniques or any other advanced color application methods.\n\nA shampoo technician shampoos and conditions a client's hair in preparation for the hair stylist. This is generally an apprentice position and a first step for many just out of cosmetology school.\n\nAestheticians are licensed professionals who are experts in maintaining and improving skin. An aesthetician's general scope of practice is limited to the epidermis (the outer layer of skin). Aestheticians work in many different environments such as salons, med spas, day spas, skin care clinics, and private practices. Aestheticians may also specialize in treatments such as microdermabrasion, microcurrent (also known as non-surgical \"face lifts\"), cosmetic electrotherapy treatments (galvanic current, high frequency), LED (light emitting diode) treatments, ultrasound/ultrasonic (low level), and mechanical massage (vacuum and g8 vibratory).\n\nThe aesthetician may undergo special training for treatments such as laser hair removal, permanent makeup application, light chemical peels, eyelash extensions, and electrology. In the US, aestheticians must be licensed in the state in which they are working and are governed by the cosmetology board requirements of that state. Aestheticians must complete a minimum 260–1500 hours of training and pass both a written and hands-on exam in order to be licensed in a given state. Utah, Virginia and Washington are the only states at this time to adopt the Master Esthetician License. Additional post graduate training is sometimes required when specializing in areas such as medical esthetics (working in a doctor's office). Estheticians work under a dermatologist’s supervision only when employed by the dermatologist's practice. Aestheticians treat a wide variety of skin issues that are cosmetic in nature, such as mild acne, hyperpigmentation, and aging skin; therefore, clients with skin disease and disorders are referred to a dermatologist or other medical professional. Aestheticians are also referred to as beauticians in North America.\n\nMany chemicals in salon products pose potential health risks. Examples of hazardous chemicals found in common treatments (e.g. hair coloring, straightening, perms, relaxers, keratin treatments, Brazilian Blowouts, and nail treatments) include dibutyl phthalate, formaldehyde, lye (sodium hydroxide), ammonia, and coal tar. Allergies and dermatitis have forced approximately 20% of hairdressers to stop practicing their profession.\n\nIn the beauty and cosmetology industries, some of the products used in hair dyes and nail applications contain chemicals that have been shown to have adverse health effects for cosmetologists. A chemical combination known as the toxic trio is often part of the ingredient list in nail polish, hair dyes, and nail polish removers. The toxic trio consists of formaldehyde, toluene and dibutyl phthalate (DBP). DBP is frequently found in nail polish and is used as a binder to increase the amount of time the polish stays on the nail. Toluene is an industrial solvent and is usually in nail polish removers. Formaldehyde can be found in a variety of beauty products but is generally found in hair straightening products and hair dyes as well as in some nail polishes. Each chemical member of the toxic trio has independently been found to have adverse reproductive effects in humans, so there concern that the presence of all three chemicals in cosmetologist supplies could pose a detrimental health risk for cosmetologists.\n\nAs a profession, cosmetology is predominantly female, most of whom are of reproductive age. There are more than one million women registered and licensed as cosmetologists in the United States and roughly several million more work as hairdressers. Among cosmetologists, hairdressers and nail technicians make up a large part of the working population. Many cosmetologists begin their careers before reproductive age and before family planning, which may put them at higher risk for reproductive health effects from exposure to workplace cosmetology chemicals.\n\nIn the United States the Food and Drug Administration (FDA) is responsible for public safety regarding cosmetic products and the Food, Drug, and Cosmetic Act regulates these products. The Cosmetic Ingredient Review (CIR) utilizes an \"Expert Panel\" to review available data on cosmetic ingredients and determine whether or not chemical ingredients in cosmetic products are safe to use considering how they are currently utilized. However, this protocol is only helpful if applied to all cosmetology workplaces in the United States.\n\nAn investigation carried out by the Environmental Working Group revealed only 11% of more than 10,000 cosmetic ingredients documented by the FDA were evaluated in a CIR review. Research studies have shown that although \"toxicological considerations play an increasingly significant role in product formulation; reproductive risks are not typically taken into account. It is also known that, \"more than 9,000 chemicals are found in cosmetic products\". airdressers use a wide range of products containing chemicals. \"Hair dyes represent the largest segment of chemical products in the hair market today. As such, they are the main source of chemical exposure among hairdressers\".\n\nToluene is a clear, water insoluble liquid with a distinct and pungent smell, similar to paint thinners. Toluene is found in cosmetic products like nail polish, nail glue, and hair dyes and is widely used as an industrial solvent and is used to make fingernail polishes, lacquers, adhesives, rubber, and paint thinners. It is used in the production of benzene, gasoline, nylon, plastics, and polyurethane. Toluene can be found on cosmetic labels under the names, benzene, toluol, phenylmethane, methylbenzene.\n\nToluene enters the environment when materials like fingernail polish, paints, paint thinners, and adhesives are used. It rapidly mixes with the air and individuals who work with paint, lacquer, or dyes have greater exposures to toluene via dermal and respiratory routes. Toluene inhalation during pregnancy has led to neonatal effects, including intrauterine growth retardation, premature delivery, congenital malformations, and postnatal developmental retardation.\nDibutyl phthalate (DBP) is a manufactured chemical used as a plasticizer. It's used to make plastics more flexible and can be found in paints, glue, insect repellents, hair spray, nail polish, and rocket fuel. Due to its flexibility and film forming properties, making it an ideal ingredient in cosmetics and cosmetology products. DBP is mainly used in nail products as a solvent for dyes and as a plasticizer that prevents nail polishes from becoming brittle, but is also used in hair sprays, to help avoid stiffness by allowing them to form a flexible film on the hair.\n\nWhen a nail polish is applied, it dries to the nail as some of the other chemicals volatilize and DBP remains on the nail, making the polish less brittle and apt to crack. The chemical may not only be absorbed through the nail, but through the skin as well. When nail-polished hands are washed, small amounts of DBP can leach out of the polish and come into contact with the skin. The application of nail polish can also provide an opportunity for skin absorption.\n\nDibutyl phthalate has been linked to reproductive issues in humans if the mother is exposed while pregnant and has been banned for use by the European Union and certain phthalate esters have been shown to cause reproductive toxicity in animal models.\n\nFormaldehyde is a colorless, strong smelling liquid that is highly volatile, making exposure to both workers and clients potentially unhealthy. Both the Environmental Protection Agency (EPA) and the Occupational Safety and Health Administration (OSHA) classify formaldehyde as a human carcinogen. Formaldehyde has been linked to nasal and lung cancer, with possible links to brain cancer and leukemia.\n\nGrowing evidence reveals that various popular hair-smoothing treatments contain formaldehyde and release formaldehyde as a gas.Formaldehyde is a common ingredient in Brazilian blowouts, Cadiveu, and Keratin Complete Smoothing Therapies. Four laboratories in California, Oregon, and Canada, confirmed a popular hair straightening treatment, the Brazilian Blowout, contained between 4% and 12% formaldehyde. Oregon OSHA demonstrated that other keratin-based hair smoothing products also contain formaldehyde, with concentrations from 1% to 7%.\n\nFormaldehyde may be present in hair smoothing solutions or as a vapor in the air. Stylists and clients may inhale formaldehyde as a gas or a vapor into the lungs and respiratory tract. Formaldehyde vapor can also make contact with mucous membranes in the eyes, nose, or throat. Formaldehyde solutions may be absorbed through the skin during the application process of liquid hair straighteners. Solutions of formaldehyde can release formaldehyde gas at room temperature and heating such solutions can speed up this process. Exposure often occurs when heat is applied to the treatment, via blow drying and flat ironing.\n\nStylists and clients have reported acute health problems while using or after using certain hair smoothing treatments containing formaldehyde. Reported problems include nose-bleeds, burning eyes and throat, skin irritation and asthma attacks. Other symptoms related to formaldehyde exposure include watery eyes; runny nose; burning sensation or irritation in the eyes, nose, and throat; dry and sore throat; respiratory tract irritation; coughing; chest pain; shortness of breath; wheezing; loss of sense of smell; headaches; and fatigue.\n\nThe presence of Formaldehyde, phthalates, and toluene (the toxic trio) in the work environment play a role in the risk of reproductive health effects for cosmetologists. Studies shows that there is a significant increase in premature birth and an increased risk of pregnancy disorders when hairdressers were compared to a referent group of teachers and salesclerks where the only occupational difference were exposure to the toxic trio. Hairdressers and cosmetologists have a slightly increased risk of having an infant with small gestational age. Reproductive disorders in relation to low birth weight were examined and found an increased risk of having infant with low birth weight; three of these studies showed a significant increase.\n\nCase studies on toluene exposures have found increased incidences of urogenital, gastrointestinal, and cardiac anomalies among children of mothers who were exposed to organic solvents, such as toluene. Associations were found between pregnant women who inhaled Formaldehyd, phthalates, and toluene and adverse reproductive outcomes such as intrauterine growth retardation and premature delivery. Hairdressers report premature ovarian failure five times more frequently than women in non-cosmetologist occupations.\n\nIn the United States, the Federal Food, Drug, and Cosmetic Act (FD&C Act) defines cosmetics as “articles intended to be rubbed, poured, sprinkled, or sprayed on, introduced into, or otherwise applied to the human body…for cleansing, beautifying promoting attractiveness, or altering appearance”. Products such as nail polish, hair coloring, straightening formulas, and shampoos fall under this definition of cosmetics. In the U.S., the FDA does not mandate premarket approval for cosmetic ingredients or products, with the exception of color. Additionally, the FDA is not legally responsible for cosmetic product or ingredient safety and does not have the authority to require manufactures to submit their safety data to the FDA. Instead, the cosmetic manufacturer is legally responsible for correctly labeling and producing their products with safe ingredients. U.S. cosmetic companies are also not required to register their product or ingredients with the FDA as registration is purely voluntary. The FDA cannot legally order a recall of cosmetic products in the U.S. even if they have been shown to have poor health outcomes. A product recall relies on the cosmetic manufacturer is completely voluntary.\n\nIn contrast, the European Union requires cosmetic products to undergo premarket safety testing and requires mandatory cosmetology product and ingredient registration. In general, the European Union approaches cosmetics and their production under the precautionary principle. The EU has banned 1,328 chemicals from use in cosmetics and does not permit animal testing for cosmetics.\n\nOSHA requires manufacturers, importers, and distributors to identify formaldehyde on any product that contains more than 0.1% formaldehyde (as a gas or in a solution), or if the product can release formaldehyde at concentrations greater than 0.1 parts per million (ppm). Safety data sheets (SDS) must also accompany the product and kept on premises with the product at all times. The SDS must explain why a chemical in the product is hazardous, how it is harmful, how workers can protect themselves, and what they should do in an emergency.\n\nSalon owners and stylists are advised to look closely at the hair smoothing products they use (read product labels and SDS sheets) to see if they contain methylene glycol, formalin, methylene oxide, paraform, formic aldehyde, methanal, oxomethane, oxymethylene, or CAS Number 50-00-0. According to OSHA's Formaldehyde standard, a product containing any of these names should be treated as a product containing formaldehyde. OSHA's Hazard Communication standard (Right to Know) states that salon owners and other employers' must have a SDS for products containing hazardous chemicals. If salon owners or other employers decide to use products that contain or release formaldehyde they are required to follow the guidelines in OSHA's Formaldehyde standard.\n\nThe Occupational Safety and Health Act (OSHA), is responsible for inspections of worker health and safety. It is estimated 375,000 nail technicians work in nail salons in the United States. Yet in 2005, OSHA inspected only 18 nail salons because businesses are exempt from inspection if they have 10 employees or less. According to the Asian American Resource Workshop, Vietnamese nail salon workers hold 40% of nail technician licenses in the United States. \"It's long hours, low hourly pay, and fierce competition from every corner of the block\" and with such fierce competition between businesses, salaries are reduced even further. As a result, a majority of these immigrants are subject to low socioeconomic status; which subsequently reduces opportunity to be educated about the occupational chemicals they are exposed to in the workplace and reduces opportunity to seek health care if adverse health effects are experienced from chemical exposure.\n\nIn the United States, whether planning to study cosmetology or specialize in a specific area, each state has different requirements that must be fulfilled before obtaining a license.\n\nFor example, the State of Illinois Department of Financial and Professional Regulations requires each candidate to complete their hours through a licensed cosmetology school program where new skills are taught and learned such as hair coloring, styling, hair cutting and the usage of hazardous chemicals. After completing the minimum hours to obtain a state license, an online examination is required and is submitted via mail with other supported documentation. Bureau of Labor Statics states that the median salary for a licensed cosmetologist is $28,770 as of May 2015. Illinois Metropolitan Division Areas, Chicago-Naperville-Arlington Heights has one of the highest employment rates with an annual rate of $27,750. Being a licensed cosmetologist opens the door to becoming self-employed and working at High-End Salons. As a licensed cosmetologist, each has the option to choose which salon fits best to work in but a self-employed salon will bring more income as long as having the right business plan for it to succeed. Each candidate registering for a salon has to obtain a certificate of registration and present all required paperwork with the FEIN, Federal employer identification number to Illinois Department of Labor.\n\n"}
{"id": "52637128", "url": "https://en.wikipedia.org/wiki?curid=52637128", "title": "EC Regulation 1223/2009 on cosmetics", "text": "EC Regulation 1223/2009 on cosmetics\n\nEC Regulation 1223/2009 on cosmetics sets binding requirements for cosmetic products that have been made available on the market within the European Union. Manufacturers of products that fall under the category or cosmetics are required to abide by this regulation as they prepare their initial release of products and while continuing to sell said products within the Member States of the EU.<ref name=\"Turner-EC1223/2009\"></ref>\n\nEU BANS THREE FRAGRANCE ALLERGENS USED IN COSMETIC PRODUCTS\n\nTHE EUROPEAN COMMISSION HAS PUBLISHED THE REGULATION (EU) 2017/1410, BANNING THE USE OF THREE FRAGRANCE ALLERGENS, HICC (CAS NO. 51414-25-6/ 31906-04-4), ATRANOL (CAS NO. 526-37-4) AND CHLOROATRANOL (CAS NO. 57074-21-2).\n\nCurrently, \n\nHICC is regulated in entry 79 under Annex III to Regulation (EC) No. 1223/2009. When its presence exceeds 0.001% in leave-on products, and 0.01% in rinse-off products, it must be declared in the list of ingredients. Atranol and chloroatranol are natural components of oak tree moss and tree moss extracts, they are neither restricted nor prohibited under current regulation. \n\nEuropean Commission first considered adopting SCCS opinion SCCS/1459/11 on June 26-27, 2012. The opinion concluded these fragrance allergens had caused the highest number of contact allergies cases in recent years. The European Commission has now updated the regulation to prohibit the use of these three fragrance allergens.\n\nThis Regulation enter into force from August 23, 2017.\n\nTIMELINE for Compliance:\n\n·           From August 23, 2019, \n\nonly cosmetic products which comply with the Regulation shall be Placed on the European Union market.\n\n·           From August 23, 2021, \n\nonly cosmetic products which comply with the Regulation shall be made available on the European Union market.\n\nSOURCE of Information:\n\n\nEC Regulation 1223/2009<ref name=\"EC-1223/2009\"></ref> was created by the European Parliament and Council with the intent of establishing standards for cosmetic products that are available on the market. Compliance to these standards helps protect the functioning of the internet market, but most importantly, it provides a high-level of protection for the safety and health of EU citizens. The Regulation was set into force on July 11, 2013.\n\nIn its quest to protect the health and safety of EU citizens, EC Regulation 1223/2009 regulates a multitude of aspects related to the manufacturing and labeling of products that are considered to fall into the category of cosmetics.\n\nBefore a cosmetic product can be released on the market, proof must be provided that it is safe for human health during the course of its intended use or what is considered to be reasonable foreseeable conditions of its use. Article 3 of EC Regulation 1223/2009 builds upon Directive 87/357/EEC guidelines regarding the safety of the product in reference to its instructions for use, its disposal and most importantly, its labeling. A safety assessment must be performed and a cosmetic product safety report provided to demonstrate compliance with Article 3. Guidelines for the report are found in the Regulations Annex I.\n\nUnder Article 4 of the Regulation, a representative of the manufacturer must be named as the “responsible person” for dealing with compliance with EC 1223/2009. This responsible person is responsible for ensuring that the manufacturer remains in compliance before and after bringing their cosmetic product to market.\n\nIn the event the cosmetic product does not meet the requirements of compliance under the Regulation, the responsible person must address those issues. If the manufacturer becomes aware that the cosmetic product presents a threat to human health, the responsible person must notify the competent authorities within each Member State where the product is available. The notification would include details of the risks of using the product, non-compliance issues and what corrective actions will be performed. Corrective actions may include withdrawing the product from the market or initiating product recalls.\n\nAnother responsibility of the responsible person is that they must maintain a product information file for each cosmetic product that is put on the market by his or her company under Article 11 of EC 1223/2009. This file must remain accessible by the public and government for at least 10 years after the last batch of the product was placed on the market. The product information file at minimum must contain information and data regarding the description of the cosmetic product, safety reports regarding the product, and evidence that the product performs according to manufacturer claims. Additionally, data about any product testing on animals that may have been performed by the manufacturer, agents of the manufacturer or suppliers must be included in the information file.\n\nUnder Article 12 of EC 1223/2009 requires that cosmetic products must be sampled and analyzed in a reliable and reproducible manner. If local Community legislation, as what might be found in a Directive is non-existent, the assumption will be that the manufacturer will rely on methods of harmonized standards that have been published in the Official Journal of the European Union.\n\nThe appointed responsible person must supply information to the EU Commission according to the guidelines provided under Article 13 of the Regulation before their cosmetic product can be placed on the market. This information includes the name and category of the product, contact information for the responsible person, the product’s country of origin, and Member State(s) where the product where be sold.\n\nBefore placing cosmetic products on the market, the manufacturer must comply with container and packaging labeling guidelines under EC 1223/2009. To comply with the Regulation all required information must be presented in a legible and indelible manner.\n\nInformation to be included shall include:\n\n\nExceptions are made for individual labeling for smaller items that allow for product information to appear as a notice on or nearby the container where the cosmetic product is displayed for sale.\n\nCertain product ingredients are prohibited under Chapter IV of EC 1223/2009 based on their threat to human health. Annexes II through VI address what substances are prohibited and restricted, including ingredients such as colorants, preservatives and UV filters.\n\nChapter V addresses the prohibition of animal testing with the final formulation of cosmetic products, before being placed on the market and when testing methods are other than the alternative methods that have been validated and adopted by the EU Community.\n\nAs a regulation, EC No 1233/2009 supersedes previous Directives that applied to cosmetic manufacturers throughout the European Union. As of 30 November 2009, all Member States are required to adhere to the guidelines for compliance to this regulation. Regulations carry more legal weight than Directives, which generally provide guidance in good practices that should be followed by individual Member States.\n\nIn the event of non-compliance, the relevant competent authority will inform the competent authority of the responsible person’s Member State. When there are grounds for concern or certainty exists that the cosmetic product presents a serious risk to human health, the authority will take steps toward instituting appropriate provisional measures to withdraw, recall or restrict the availability of the non-complying product.\n\n"}
{"id": "5614038", "url": "https://en.wikipedia.org/wiki?curid=5614038", "title": "EMR Telemetry", "text": "EMR Telemetry\n\nEMR (\"Electro-Mechanical Research\") Telemetry was a division of Weston Instruments, Inc. based in Sarasota, Florida. EMR started in 1957 and was sold to several different companies throughout its existence. EMR has been owned or operated by companies such as Fairchild Camera and Instrument, Schlumberger LTD, Loral, Lockheed Martin. Currently, the Aviation Recorder Division of L-3 Communications operates the Sarasota facility. EMR products included telemetry processing equipment and space-rated data transmission system, cockpit voice recorders (CVR) and flight data recorders (FDR).\n\nEMR products have been used in many space projects such as Project Mercury, Project Gemini, Ranger 7 and the Pioneer 10 and Pioneer 11 missions. EMR also worked on several classified projects and the X-20 Dyna-Soar prior to the program's cancellation in 1963. \n\n"}
{"id": "53665985", "url": "https://en.wikipedia.org/wiki?curid=53665985", "title": "Elsie Godwin", "text": "Elsie Godwin\n\nElsie Godwin (born Isioma Elsie Godwin; 13 April 1989) is a Nigerian relationship, literature and lifestyle blogger, social media manager, radio and TV talk show host.\n\nGodwin was born and raised in Lagos state to Godwin Nwosu from Umuahia, Abia state. She is the second child. She lost her father at a young age and was raised by her mother who diedwhen she was 18.\n\nGodwin holds a bachelor's degree in Computer Science from the Lagos State University. She also obtained a certificate as an Aspiring Entrepreneur from Fate Foundation.\n\nGodwin started blogging on 29 December 2012 using a wordpress sub domain. She took her blogging more seriously in January 2014 and obtained a custom domain which she rebranded as ELSiEiSY.\n\nGodwin had her first radio experience and training at Rose Radio where she interned as a newscaster and programs researcher.\n\nIn February 2016, she started her own radio show called Crux of the Matter on Happenings Radio, which was born out of the passion to create conversations around pressing issues. She was the host and producer of the show.\n\nIn April 2016, Godwin joined Itunoluwa Oladehinde Ariyo as host on HeartMatters on Lagos Traffic Radio – 96.1FM. Heartmatters is an adult program targeted at married couples, others of marriageable ages and single parents. The program presents real life crisis and challenges that people face in their marriages, courtships, dates and other non-platonic relationships.\n\nIn January 2017, after her controversial Interview with sarah Ofili went viral, Godwin decided it was time to move the conversation to TV. Crux of the Matter premiers on R2TV (GoTV Channel 112) in April 2017.\n\nIn 2016, her blog, ELSiEiSY, was named in the Top 25 Relationship blogs and Websites on Web by Feedspot.\n\n\n"}
{"id": "43915248", "url": "https://en.wikipedia.org/wiki?curid=43915248", "title": "Engine power plant", "text": "Engine power plant\n\nAn engine power plant is a power station in which power comes from the combination of a reciprocating engine and an alternator. \n\nThanks to very short start-up time, Engine power plants can provide full output within few minutes (high flexibility) and ensure load balancing. \n\nAs the share of electricity coming from variable renewable energy sources (vRES) and power supply variations are increasing in many countries around the world, grid stability is becoming a growing challenge requiring flexibility options like flexible generation, including engine power plants.\n\nEngine power plants are also used as a reliable and efficient technology solution for:\n\n"}
{"id": "24604817", "url": "https://en.wikipedia.org/wiki?curid=24604817", "title": "Engineers Against Poverty", "text": "Engineers Against Poverty\n\nEngineers Against Poverty (EAP) is a specialist NGO working in the field of engineering and international development. It was established in 1998 by the Royal Academy of Engineering and the Department for International Development (DFID).\n\nEAP’s programme is focused in three key areas: infrastructure, the extractive industries (oil, gas & mining) and engineering education. Individual projects within these areas are undertaken in collaboration with strategic partners. \n\nEAP’s programme is built on two propositions:Hum \n\n\nEAP has been working in partnership with the Institution of Civil Engineers to demonstrate how procurement can be used to increase local content (i.e. the proportion of goods, services and labour sourced locally) in public sector infrastructure projects. Together they conducted extensive research that culminated in the publication of a report.\n\nA range of international agencies have made use of the knowledge contained in the report including the African Development Bank, OECD and the European Commission. In 2009–10 they will be developing systems to measure the impact of the improvements that we have been advocating.\n\nEAP is part of the International Secretariat of the Construction Sector Transparency Initiative (CoST) that is tackling the problem of corruption in the construction sector head on. Launched by DFID in 2008 and coordinated by PricewaterhouseCoopers, CoST is developing systems and procedures to enable the public disclosure of material project information. Improving transparency in this way will make it possible for decision-makers to be held accountable. It will also help to reduce corruption and boost growth and development. \n\nSeven pilot countries - Tanzania, Botswana, Zambia, Ethiopia, Philippines, Vietnam and the UK – are participating in the pilot phase. EAP is providing policy and technical advice to CoST and helping to ensure that a pro-poor perspective is integrated into decision making.\n\nEAP has been working with the Institution of Engineers Tanzania to train a team of 35 Tanzanian men and women to international standards in construction health and safety and training delivery. They are also working with them to develop a training programme that is specific to the Tanzanian context and which when finished, will be recognised as a new national standard. The Tanzanian trainers will be running courses with government, industry and trade unions throughout the country in 2009.\n\nEAP have recently published innovative practical guidance to industry on maximising local benefits of Oil Gas & Mining projects, including an analysis of the opportunities for engineering services contractors to deliver local content (in partnership with the Overseas Development Institute) and a briefing note funded by the International Finance Corporation. They have also jointly released a guidance note with International Alert on conflict sensitive business practice for engineering contractors, particularly those working in the extractive industries.\n\nEAP has been working with the Institute of Education (IoE) to explore the extent to which global issues are incorporated into the engineering curricula of UK universities. Their joint report captures the learning from a series of high-level roundtable meetings held in UK universities. It also makes a series of practical recommendations aimed at enabling UK universities to build on past success, overcome barriers and integrate the ‘global dimension’ into teaching.\n\nThey are now working in partnership with the Engineering Subject Centre, Engineering Council UK and the Institute of Education to provide practical support to 7 UK-based universities. Over the next three years they will participate in a programme of high quality professional development and curriculum review support.\n"}
{"id": "12971550", "url": "https://en.wikipedia.org/wiki?curid=12971550", "title": "European Cooperation for Space Standardization", "text": "European Cooperation for Space Standardization\n\nThe European Cooperation for Space Standardisation (ECSS), established in 1993, is an organisation which works to improve standardisation within the European space sector. The ECSS frequently publishes standards, to which contractors working for ESA must adhere to.\n\n\n\n"}
{"id": "30024182", "url": "https://en.wikipedia.org/wiki?curid=30024182", "title": "Faculty of Agricultural Science and Food in Skopje", "text": "Faculty of Agricultural Science and Food in Skopje\n\nThe Faculty of Agricultural Science and Food in Skopje () is a part of Ss. Cyril and Methodius University of Skopje. The faculty is located in the eastern part of the city Skopje on campus with other departments of biolocical and biotechnological science:Faculty of Mathematics and Natural Sciences, Faculty of Veterinary Medicine, University Hospital for Veterinary Medicine, Veterinary Institute, Institute of Agriculture and Faculty of Forestry. Campus is 2 km away from the Rector of SS. Cyril and Methodius, and within the campus is located dormitory Stiv Naumov \". The faculty is equipped with a net usable building area of 7465m2, which houses six amphitheaters, 26 lecture, 28 laboratories, 143 offices, library and reading room with an area of 270 m2 and a student café with an area of 180m2.\nThe Dean from 2009 is Prof.Dr. Dragi Dimitrievski Ph.D.\n\nFaculty of Agricultural Science and Food was established in autumn 1947 under the name Faculty of Agriculture, as a separate department of agricultural and forestry faculty, only a year later the foundation of the Faculty of Philosophy. With the establishment of Agricultural and Forestry Faculty, Faculty of Medicine and has founded the Faculty of Philosophy, have provided conditions for the formation of SS. Cyril and Methodius University of Skopje.\n\nAgro-forestry faculty, by the establishment and commencement of work, permanently organized and improved as a complete teaching and research institution. So far the work and development, passing through several organizational and developmental stages. The changes and transformations, development and operation due to the need for harmonization of the educational process, scientific research, application of their own and foreign scientific knowledge in agriculture, which is affiliated with the political and socio-economic relations in the social order.\n\nIn 1975, agricultural-forestry faculty are divided into two independent faculties: Agriculture and Forestry, as a separate business organizations. This independence comes as a result achieved a high degree of development in all activities of the previous two sections, agriculture and forestry.\n\nFrom 1977 to 1989, Faculty of Agriculture operates as a working organization which were integrated into the Faculty of Agriculture, Agricultural Institute, Institute of Animal Science, Institute of Veterinary,Veterinary Hospital, Institute for fruit growing, viticulture and the Institute for the college farm Trubarevo.\n\nAs a result of the complicated organizational structure, from the beginning of 1990, Faculty of Agriculture working as an independent organization working with three core activities: education, scientific research and applied.In the period from 1992 to 1994 within the Faculty of Agriculture, as a special direction, operated and veterinary direction, which further established the independent Faculty of Veterinary Medicine.\n\nIn the same 2001, the Faculty be included among the first four faculties in Ss. Cyril and Methodius, who introduced European Credit Transfer System-EKTS in the teaching process.\n\nToday the faculty have the following study programs:\n\nThese are the institutes and departments in which the faculty is divided into:\n\n\n"}
{"id": "19357047", "url": "https://en.wikipedia.org/wiki?curid=19357047", "title": "Functional testing (manufacturing)", "text": "Functional testing (manufacturing)\n\nFCT refers to functional testing. Typically, the functional test is performed during the last phase of the production line. This is often referred to as a final quality control test, which is done to ensure that specifications are carried out by FCTs.\n\nThe process of FCTs is entailed by the emulation or simulation of the environment in which a product is expected to operate. This is done so to check, and correct any issues with functionality. The environment involved with FCTs consists of any device that communicates with an DUT, the power supply of said DUT, and any loads needed to make the DUT function correctly.\n\nFCTs uses customer specific connectors, rather than a test point on the PCB.\n\nFunctional tests are performed in an automatic fashion by production line operators using test software. In order for this to be completed, the software will communicate with any external programmable instruments such as I/O boards, digital multimeters, and communication ports. In conjunction with the test fixture, the software that interfaces with the DUT is what makes it possible for a FCT to be performed.\n\n"}
{"id": "26950608", "url": "https://en.wikipedia.org/wiki?curid=26950608", "title": "Gimjang", "text": "Gimjang\n\nGimjang (), also spelled kimjang, is the traditional process of preparation and preservation of kimchi, the spicy Korean fermented vegetable dish, in the wintertime. During the summer months, Kimchi is made fresh, from seasonal vegetables. For one month, starting from the tenth moon of the year, people prepare large quantities of kimchi, to provide nutrition throughout winter.\n\nGimjang was listed as an UNESCO Intangible Cultural Heritage in December 2013.\n\nKimchi can be eaten as an accompaniment to almost any meal, and is an important part of Korean culture. Recipes date back to at least the 13th century, when it was made from vegetables, pickles and either salt or a mixture of alcohol and salt. Red pepper was added to the ingredients in the 17th century. Modern day kimchi is typically made from napa cabbage and white radish, although there are hundreds of variations; it may also contain turnip, leek, carrots, and garlic.\n\nIn the cooler weather of November, there are lots of crops in the fields and market-places, and the Gimjang process begins. The labour-intensive task is shared by families, relatives and neighbours. Groups of Korean people gather to cut the vegetables, wash them, and add salt to cure the food and begin the fermentation process. The nature of kimchi means that it is challenging to store for long periods; if it is too cold, it will freeze, and if it is too warm, it will over ferment, and may turn sour. The traditional solution prior to effective modern refrigeration is to store kimchi in earthenware jars in the ground, buried up to the neck level of the jar to prevent the contents from freezing. As the temperature falls below 0 °C, fermentation is halted and the food is preserved; it begins again as the temperature increases in spring time.\n\nThe strong odors of kimchi can taint other products in a refrigerator, and despite modern advances in refrigeration, the custom of gimjang continues to be passed down the generations. It is common in cities for people to store large jars of fermenting kimchi on balconies. It is also increasingly common to own and use secondary refrigerators designed specifically for storing kimchi.\n\nIn an attempt to combat the increasing popularity of mass-produced kimchi, which is convenient for modern life, Seoul has created the world's only kimchi museum, where tourists and local people can sample different types of the fermented dish, and learn about the traditional gimjang process. Although consumption figures have fallen, Koreans still consume of Kimchi per head each year.\n\n"}
{"id": "33260265", "url": "https://en.wikipedia.org/wiki?curid=33260265", "title": "History of the Deep Space Network", "text": "History of the Deep Space Network\n\nThe forerunner of the DSN was established in January 1958, when JPL, then under contract to the U.S. Army, deployed portable radio tracking stations in Nigeria, Singapore, and California to receive telemetry and plot the orbit of the Army-launched Explorer 1, the first successful U.S. satellite.\n\nNASA (and the DSN by extension) was officially established on October 1, 1958, to consolidate the separately developing space-exploration programs of the US Army, US Navy, and US Air Force into one civilian organization.\n\nOn December 3, 1958, JPL was transferred from the US Army to NASA and given responsibility for the design and execution of lunar and planetary exploration programs using remotely controlled spacecraft.\n\nShortly after the transfer NASA established the concept of the Deep Space Instrumentation Facility (DSIF) as a separately managed and operated communications system that would accommodate all deep space missions, thereby avoiding the need for each flight project to acquire and operate its own specialized space communications network.\n\nThe coded doppler, ranging, and command (CODORAC) system developed by Eberhardt Rechtin, Richard Jaffe, and Walt Victor became the basis for much of the DSIF's electronics. Susan Finley was part of the team that built the network's software.\n\nIn order to support deep space missions around the clock it was necessary to establish a network of three stations separated by approximately 120 degrees of longitude so that as the earth turned a spacecraft was always above the horizon of at least one station. To this end two overseas facilities with 26m antennas were established to complement the 26m antenna sites (DSIF 11 and 12) at Goldstone in California. (DSIF 13 at Goldstone was used for research and development.) The first overseas site was DSIF 41 at Island Lagoon near Woomera in Australia. It was operated by the Australian Department of Supply which ran the Woomera Rocket Range. The other, DSIF 51, was at Hartebeesthoek near Johannesburg in South Africa, operated by the South African Council for Scientific and Industrial Research (CSIR). These two stations were completed in 1961. Each DSIF station had transmit and receive capability at 960 MHz in the L-band of the radio spectrum, and could process telemetry. Telephone and teletype circuits linked the stations to a mission operations room at JPL. As missions became more numerous the operations room developed into the Space Fight Operations Facility, and the personnel and equipment common to all missions were incorporated into the DSIF which was renamed the Deep Space Network in 1963.\n\nThe DSN was given responsibility for its own research, development, and operation in support of all of its users. Under this concept, it has become a world leader in the development of low-noise receivers; large parabolic-dish antennas; tracking, telemetry, and command systems; digital signal processing; and deep space navigation.\n\nThe DSN started the period able to support JPL designed spacecraft and telemetry and was progressively improved to cope with the increased demands placed upon it by new programs.\n\nIn 1963 the availability of new amplifiers and transmitters operating in the S-band (at 2,200 MHz) allowed the DSN to take advantage of better tracking performance at the higher frequency, and later missions were designed to use it. However the Ranger and early Mariner missions still needed L-band, so converters were installed at the stations along with the new S-band upgrades. These converters were removed at the end of the L-band missions. This transfer to S-band was a major enhancement of the DSN capabilities in this era; another was the introduction of rubidium frequency standards which improved the quality of radio Doppler data and hence improved the trajectory determinations needed for interplanetary missions.\n\nAs the supported and planned missions became more numerous it became clear that a second network of stations was required. For political and logistical reasons the new overseas stations were established at Robledo near Madrid in Spain, and at Tidbinbilla near Canberra in Australia, and the second network of 26m antennas was operational in 1965.\n\nJPL had long recognized the need for larger antennas to support missions to distant planets and a 64 m antenna of a radical new design was built at Goldstone. It gave over six times the sensitivity of the 26 m antennas, more than doubling their tracking range. The station was commissioned in 1966 as DSS 14.\n\nMobile DSN equipment was used at Cape Canaveral to check out spacecraft compatibility and operation prior to launch, and monitor the early flight. In 1965 this became a permanent facility, DSS 71.\n\nThe early Surveyor missions were planned to launch with a direct-ascent trajectory to the Moon, rather than insertion from a parking orbit. Translunar injection would then be before spacecraft rise at DSS 51 or 61. To obtain the early trajectory data vital for mid-course corrections, a new station with a small and fast-moving antenna was built on Ascension Island and became DSS 72. The station was integrated with the Apollo program.\n\nIn the 1966 to 1968 period the NASA lunar program of Surveyor, Lunar Orbiter and Apollo backup support almost fully utilized the DSN. The Pioneer, Surveyor and Lunar Orbiter programs all supplied mission-dependent equipment at the tracking stations for command and telemetry processing purposes and this could be quite large. For example, the Lunar Orbiter equipment at DSS 41 required the building of an extension to the control room, a photographic processing area and darkroom, and water de-mineralising equipment. Station personnel maintained and operated the Pioneer equipment, but the considerably more involved Surveyor and Lunar Orbiter equipment was operated by mission personnel, at least on the early missions.\n\nOne network of three stations was equipped for Surveyor, and another network dedicated to Lunar Orbiter. Support was also needed for the Mariner 5 Venus mission, and for Pioneer 6-9 interplanetary spacecraft which kept operating long after their expected lifetimes. Mariner 4 was also picked up again. DSS 14, the new 64m antenna, was called on to support nearly all of these missions but not always as a prime site.\n\nTo simplify the problems of accommodating special command and telemetry equipment and personnel at stations, the DSN developed a \"multi-mission\" approach. A generic set of equipment would be provided that future missions would all use, and a start was made by introducing computers at the stations to decode telemetry. Mission dependant equipment could be replaced by separate computer programs for each mission. Another significant improvement at this time was the introduction of ranging systems that used a coded signal transmitted to and returned from the spacecraft. The time of travel was used to measure the range more accurately and to greater distances, and this improved trajectory determination and navigation. The station clocks were kept in synchronism to 5 microseconds using the \"Moon Bounce\" system. The Goldstone Venus station transmitted a coded X-band timing signal to each overseas station during mutual lunar viewing periods. The signal was tailored on each occasion to allow for the propagation time to the station via the Moon.\n\nIn 1969 the Mariner 6 and Mariner 7 spacecraft to Mars were in the same part of the sky and both in view of a DSN site at the same time, though not within the beamwidth of a single antenna. Tracking both simultaneously required two antennas and two telemetry data processors, one for each downlink. At the same time the interplanetary Pioneer spacecraft were tracked and backup support for Apollo was required. The DSN was again hard pressed to service all its customers. As Mars began to draw near towards the end of July, encounter operations began with Mariner 7 only five days behind Mariner 6. Corliss describes what happened next. \n\nMudgway continues:\n\nMariner 9, launched in 1971, was a Mars orbiter mission, a good deal more complicated than previous flyby missions and requiring precise navigation and high data rates. Since the last Mariner mission the Multi-Mission Telemetry System and the High-Rate Telemetry System (HRT) were fully operational. But the high speed data could only be sent when the 64m antenna at Goldstone was tracking.\n\nAt this time there was a substantial expansion of the number of antennas. An additional 26 m antenna and a 64 m antenna was built at each of Tidbinbilla and Robledo to support Apollo and Mariner 10 and the planned Viking missions. As part of a consolidation of stations into central locations, the Woomera station (DSS 41) was decommissioned in 1972. The antenna and basic receiving and power house equipment was offered to the Australian government, and although used by Australian scientists for groundbreaking VLBI measurements, it was eventually dismantled and scrapped due to logistical problems and the prohibitive cost of transporting it to a new location. DSS 51 in South Africa was similarly decommissioned in 1974, but in this case was taken over by the South African Council for Scientific and Industrial Research (CSIR) and recommissioned as a radio astronomy facility, which is now Hartebeesthoek Radio Astronomy Observatory.\n\nMariner 10 incorporated a Venus flyby followed by an orbiter round Mercury, and required the network of 64 m antennas and special DSN enhancements including use of a developmental supercooled maser at DSS 43, installation of an S/X-band dichroic reflector plate and feed cones at DSS 14 and enhanced data transmission circuits from the DSN stations to JPL. The second encounter with Mercury in 1974 was at a greater distance and the technique of \"arraying\" antennas, which had been demonstrated by Spanish engineers at the Madrid complex, was used at Goldstone. The Pioneer 10 mission with a 60-day encounter with Jupiter competed for time on the 26 m and 64 m antennas with the Mariner 10 mission and the need for Goldstone 64 m radar surveillance of possible Viking lander sites. Allocation of the DSN resources became even more difficult.\n\nTo support the Apollo manned lunar-landing program NASA's Manned Space Flight Network (MSFN) installed extra 26 mm antennas at Goldstone; Honeysuckle Creek, Australia; and Fresnedillas , Spain. However, during lunar operations spacecraft in two different locations needed to be tracked. Rather than duplicate the MSFN facilities for these few days of use, in this case the DSN tracked one while the MSFN tracked the other. The DSN designed the MSFN stations for lunar communication and provided a second antenna at each MSFN site (the MSFN sites were near the DSN sites for just this reason).\n\nThis arrangement also provided redundancy and help in the case of emergencies. Almost all spacecraft are designed so normal operation can be conducted on the smaller (and more economical) antennas of the DSN (or MSFN). However, during an emergency the use of the largest antennas is crucial. This is because a troubled spacecraft may be forced to use less than its normal transmitter power, attitude control problems may preclude the use of high-gain antennas, and recovering every bit of telemetry is critical to assessing the health of the spacecraft and planning the recovery.\n\nA famous example from Apollo was the Apollo 13 mission, where limited battery power and inability to use the spacecraft's high-gain antennas reduced signal levels below the capability of the MSFN, and the use of the biggest DSN antennas (and the Australian Parkes Observatory radio telescope) was critical to saving the lives of the astronauts.\n\nTwo antennas at each site were needed both for redundancy and because the beam widths of the large antennas needed were too small to encompass both the lunar orbiter and the lander at the same time. DSN also supplied some larger antennas as needed, in particular for television broadcasts from the Moon, and emergency communications such as Apollo 13.\n\nFrom a NASA report describing how the DSN and MSFN cooperated for Apollo:\n\nThe details of this cooperation and operation are available in a two-volume technical report from JPL.\n\nThe Viking program mainly Viking 1 and Viking 2 forced some innovation to be done with respect to high power transmission to Mars, and reception and relay of landing craft telemetry.\nThe Viking craft eventually failed, one by one, as follows:\n\nThe Viking program ended on May 21, 1983. To prevent an imminent impact with Mars the orbit of Viking 1 orbiter was raised. Impact and potential contamination on the planet's surface is possible from 2019 onwards.\n\nThe Viking 1 lander was found to be about 6 kilometers from its planned landing site by the Mars Reconnaissance Orbiter in December 2006.\nThe Viking 1 Lander touched down in western Chryse Planitia (\"Golden Plain\") at at a reference altitude of −2.69 km relative to a reference ellipsoid with an equatorial radius of 3397.2 km and a flatness of 0.0105 (22.480° N, 47.967° W planetographic) at 11:53:06 UT (16:13 local Mars time). Approximately 22 kg of propellants were left at landing.\n\nTransmission of the first surface image began 25 seconds after landing and took about 4 minutes. During these minutes the lander activated itself. It erected a high-gain antenna pointed toward Earth for direct communication and deployed a meteorology boom mounted with sensors. In the next 7 minutes the second picture of the 300° panoramic scene (displayed below) was taken.\n\nThere were no moon missions after 1972. Instead, there was an emphasis on Deep Space exploration in the 1980s. A modernization programme was launched to increase the size of the 64m antennas. From 1982 to 1988 the three 64-meter antennas of the Mars subnet in Spain and Australia were extended to 70 meters.\n\nThe average improvement in performance of the three DSS stations of the subnet was over 2 db in the X-band due to the modernization. This performance increase was vital for the return of science data during Voyager's successful encounters with Uranus and Neptune, and the early stages of its interstellar mission. The modernization also extended the useful range of communications for Pioneer 10 from about 50 astronomical units to about 60 astronomical units at S-band.\n\nAfter the Voyager Uranus flyby, the DSN demonstrated the capability of combining signals from the radio astronomy antenna at Parkes, Australia, with the Network antennas at Tidbinbilla. This DSS subnet capability is now a standard part of network operation.\n\nThe Voyager encounter of Neptune in August 1989 presented an additional challenge for the Network. The DSN personnel negotiated with several radio observatories the option of combining signals with the deep-space stations.\n\nBy arrangement the Very Large Array (VLA) had agreed to equip the 27 antennas with X-band receivers in order to communicate with Voyager at Neptune. The coupling of the VLA with the Goldstone antenna subnet made possible significant science data return, particularly for imaging the planet and its satellite and for detecting rings around Neptune.\n\nDSN provides emergency service to other space agencies as well. For example, the recovery of the Solar and Heliospheric Observatory (SOHO) mission of the European Space Agency (ESA) would not have been possible without the use of the largest DSN facilities.\n"}
{"id": "29065548", "url": "https://en.wikipedia.org/wiki?curid=29065548", "title": "In-target probe", "text": "In-target probe\n\nIn-target probe, or ITP is a device used in computer hardware and microprocessor design, to control a target microprocessor or similar ASIC at the register level. It generally allows full control of the target device and allows the computer engineer access to individual processor registers, program counter, and instructions within the device. It allows the processor to be single-stepped or for breakpoints to be set. Unlike an in-circuit emulator (ICE), an In-Target Probe uses the target device to execute, rather than substituting for the target device.\n\n\n"}
{"id": "20197040", "url": "https://en.wikipedia.org/wiki?curid=20197040", "title": "Integrated project delivery", "text": "Integrated project delivery\n\nIntegrated project delivery (IPD) is a collaborative alliance of people, systems, business structures and practices into a process that harnesses the talents and insights of all participants to optimize project results, increase value to the owner, reduce waste, and maximize efficiency through all phases of design, fabrication, and construction.\n\nThere are eight main sequential phases to the integrated project delivery method:\n\n\nThe construction industry has suffered from a productivity decline since the 1960s while all other non-farm industries have seen large boosts in productivity. The problems in contemporary construction include buildings that are behind schedule and over budget as well as adverse relations among the owner, general contractor, and architect. Using ideas developed by Toyota in their Toyota Production System and computer technology advances, the integrated project delivery method is designed to solve these key construction problems. The new focus in IPD is the final value created for the owner, the finished building. Rather than each participant focusing exclusively on their part of construction without considering the implications on the whole process, the IPD method brings all participants together early with collaborative incentives to maximize value for the owner. This collaborative approach allows informed decision making early in the project where the most value can be created. The close collaboration eliminates a great deal of waste in the design, and allows data sharing directly between the design and construction team eliminating a large barrier to increased productivity in construction.\n\nIntegrated project delivery is a delivery system that seeks to align interests, objectives and practices, even in a single business, through a team-based approach. The primary team members include the architect, key technical consultants as well as a general contractor and key subcontractors. The IPD system is a process where all disciplines in a construction project work as one firm, creating faster delivery times, lower costs, no litigation and a more enjoyable process for the entire team – including the owner.\n\nIPD combines ideas from integrated practice and lean construction to solve several problems in contemporary construction such as low productivity and waste, time overruns, quality issues, and conflicts during construction among the key stakeholders of owner, architect and contractor. The growing use of building information modeling in the construction industry is allowing far greater information collaboration between project participants using IPD and considered an important tool to increasing productivity throughout the construction process.\n\nUnlike the design–build project delivery method which typically places the contractor in the leading role on a building project, IPD represents a return to the \"master builder\" concept where the entire building team including the owner, architect, general contractor, building engineers, fabricators, and subcontractors work collaboratively throughout the construction process.\n\nNote: Job Order Contracting, JOC is form of integrated project delivery that specifically targets repair, renovation, and minor new construction. It has proven to be capable of delivering over 90% of projects on-time, on-budget, and to the satisfaction of all participants and stakeholders. JOC Research Report.\n\nNote: A significant criticism of IPD is the absence of any mention or effort to integrate project safety into the method. Excluding safety will likely lead to poor safety performance on projects using IPD.\n\nOne common way to further the goals of IPD is through a multi-party agreement among key participants. In a multi-party agreement (MPA), the primary project participants execute a single contract specifying their respective roles, rights, obligations, and liabilities. In effect, the multi-party agreement creates a temporary virtual, and in some instances formal, organization to realize a specific project. Because a single agreement is used, each party understands its role in relationship to the other participants. Compensation structures are often open-book, so each party’s interests and contributions are similarly transparent. Multi-party agreements require trust, as compensation is tied to overall project success and individual success depends on the contributions of all team members. For an MPA to be successful, the participants must be committed to working as a team to achieve team goals.\n\nCommon forms of multi-party agreements include \n\nThe adoption of IPD as a standard for collaborative good practice on construction projects presents its own problems. As most construction projects involve disparate stakeholders, traditional IT solutions are not conducive to collaborative working. Sharing files behind IT firewalls, large email attachment sizes and the ability to view all manner of file types without the native software all make IPD difficult. \n\nThe need to overcome collaborative IT challenges has been one of the drivers behind the growth of online construction collaboration technology. Since 2000, a new generation of technology companies evolved using SaaS to facilitate IPD in a smooth and efficient manner. \n\nThis collaboration software streamlines the flow of documentation, communications and workflows ensuring everyone is working from 'one version of the truth'. Collaboration software allows users from disparate locations to keep all communications, documents & drawings, forms and data, amongst other types of electronic file, in one place. Version control is assured and users are able to view and mark up files online without the need for native software. The technology also enables project confidence and mitigates risk thanks to inbuilt audit trails.\n\n\n\n"}
{"id": "1926052", "url": "https://en.wikipedia.org/wiki?curid=1926052", "title": "IntelliStar", "text": "IntelliStar\n\nThe IntelliStar was the fifth-generation successor to the WeatherStar systems used by the American cable and satellite television channel The Weather Channel (TWC), that is used to insert local forecasts and current weather information (such as the \"Local on the 8s\" segments within its program schedule) into TWC's programming. Like the WeatherStar, it is installed at the cable provider's headend.\n\nThe IntelliStar has many enhanced features over its predecessor, the Weather Star XL. Like all other WeatherStar systems, the IntelliStar receives its data over a satellite connection and over the Internet. However, unlike the rest of the systems, it has the capability to receive more complex information in a more efficient manner. It also has a DualFeed feature, which allows a selection of two different video feeds. In the event of inclement weather, the DualFeed option would switch from the first (network) feed, to a second (localized) feed, providing weather information to a specific STAR or network of STARs. By doing this, the specified network of STARs could be addressed with weather updates, versus the entire national network of STARs (where such information would be irrelevant).\n\nIt also has an improved graphics display and dynamic radar capabilities. HiRAD (High Resolution Aggregated Data) technology – which was added to the IntelliStar in 2006 – allows The Weather Channel to choose any city, town or landmark as an observation or forecast site and provide data. Occasionally, the HiRAD function will fail, in which case the National Weather Service sites are used (only the current conditions – and previously, the eight-city product and regional/metro products – are affected significantly).\n\nIn May 2015, it was announced that all IntelliStar units would be replaced with either the IntelliStar 2 or the IntelliStar 2 Jr. by October 1, 2015. The IntelliStar was discontinued on November 16, 2015.\n\nThe IntelliStar differs in design from previous WeatherStar units, as the system is basically a customized rack-mount Intel CPU-based personal computer (PC). It runs on the FreeBSD operating system, running specialized software written by The Weather Channel to make it function as an IntelliStar, but otherwise using commodity hardware of the PC platform. This was done to minimize maintenance costs, and to ease upgrading of the IntelliStar units. The previous WeatherStar units used a proprietary hardware design that hampered any hardware upgrades (the Weather Star XL, for comparison, uses proprietary SGI hardware, and runs IRIX). TWC has contributed code to the FreeBSD development community, and funded the initial development of Radeon 8500/R200 generation 3D graphics drivers used under both Linux and FreeBSD.\n\n"}
{"id": "4214075", "url": "https://en.wikipedia.org/wiki?curid=4214075", "title": "Intrinsic safety", "text": "Intrinsic safety\n\nIntrinsic safety (IS) is a protection technique for safe operation of electrical equipment in hazardous areas by limiting the energy, electrical and thermal, available for ignition. In signal and control circuits that can operate with low currents and voltages, the intrinsic safety approach simplifies circuits and reduces installation cost over other protection methods. Areas with dangerous concentrations of flammable gases or dust are found in applications such as petrochemical refineries and mines. As a discipline, it is an application of inherent safety in instrumentation. High-power circuits such as electric motors or lighting cannot use intrinsic safety methods for protection. \nIn normal use, electrical equipment often creates tiny electric arcs (internal sparks) in switches, motor brushes, connectors, and in other places. Compact electrical equipment generates heat as well, which under some circumstances can become an ignition source.\n\nThere are multiple ways to make equipment safe for use in explosive-hazardous areas. Intrinsic safety (denoted by \"i\" in the ATEX and IECEx Explosion Classifications) is one of several available methods for electrical equipment. Others include explosion proof (NEC 500) or flameproof enclosures (\"d\" in IEC, ATEX and NEC 505), increased safety (\"e\"), encapsulation (\"m\"), enclosed-break device (\"nC\"), sealed device (\"nC\"), hermetically-sealed device (\"nC\"), restricted-breathing enclosure (\"nR\"), oil immersion (\"o\"), protection of optical radiation (\"op\"), venting (\"p\"), powder or sand filling (\"q\"), special protection (\"s\") and dust ignition protection by enclosure (\"t\"). For handheld electronics, intrinsic safety is the only realistic method that allows a functional device to be explosion protected. A device termed intrinsically safe is designed to be incapable of producing heat or spark sufficient to ignite an explosive atmosphere, even if the device has experienced deterioration or has been damaged.\n\nThere are several considerations in designing intrinsically safe electronics devices: reducing or eliminating internal sparking, controlling component temperatures, and eliminating component spacing that would allow dust to short a circuit. Elimination of spark potential within components is accomplished by limiting the available energy in any given circuit and the system as a whole. Temperature, under certain fault conditions such as an internal short in a semiconductor device, becomes an issue as the temperature of a component can rise to a level that can ignite some explosive gasses, even in normal use. Safeguards, such as current limiting by resistors and fuses, must be employed to ensure that in no circumstance can a component reach a temperature that could cause autoignition of a combustible atmosphere. In the highly compact electronic devices used today PCB's often have component spacing that create the possibility of an arc between components if dust or other particulate matter works into the circuitry, thus component spacing, siting and isolation become important to the design.\n\nThe primary concept behind intrinsic safety is the restriction of available electrical and thermal energy in the system so that ignition of a hazardous atmosphere (explosive gas or dust) cannot occur. This is achieved by ensuring that only low voltages and currents enter the hazardous area, and that no significant energy storage is possible.\n\nOne of the most common methods for protection is to limit electric current by using series resistors (using types of resistors that always fail open); and limit the voltage with multiple zener diods. In zener barriers dangerous incoming potentials are grounded, with galvanic isolation barriers there is no direct connection between the safe- and hazardous-area circuits by interposing a layer of insulation between the two. Certification standards for intrinsic safety designs (mainly IEC 60079-11 but since 2015 also IEC TS 60079-39) generally require that the barrier do not exceed approved levels of voltage and current with specified damage to limiting components.\n\nEquipment or instrumentation for use in a hazardous area will be designed to operate with low voltage and current, and will be designed without any large capacitors or inductors that could discharge in a spark. The instrument will be connected, using approved wiring methods, back to a control panel in a non-hazardous area that contains safety barriers. The safety barriers ensure that, in normal operation, and with the application of faults according to the Equipment Protection Level, EPL, also if accidental contact occurs between the instrument circuit and other power sources, no more than the approved voltage and current enters the hazardous area.\n\nFor example, during marine transfer operations when flammable products are transferred between the marine terminal and tanker ships or barges, two-way radio communication needs to be constantly maintained in case the transfer needs to stop for unforeseen reasons such as a spill. The United States Coast Guard requires that the two way radio must be certified as intrinsically safe.\n\nAnother example is intrinsically safe or explosion-proof mobile phones used in explosive atmospheres, such as refineries. Intrinsically safe mobile phones must meet special battery design criteria in order to achieve UL, ATEX directive, or IECEx certification for use in explosive atmospheres.\n\nOnly properly designed battery-operated, self-contained devices can be intrinsically safe by themselves. Other field devices and wiring are intrinsically safe only when employed in a properly designed IS system. Such systems shall be designed and documented according to the standard IEC 60079-25 \"Intrinsically safe electrical systems\", installed according to IEC 60079-14 and inspected and maintained according to IEC 60079-17.\n\nStandards for intrinsic protection are mainly developed by IEC,International Electrotechnical Commission but different agencies also develop standards for intrinsic safety. Agencies may be run by governments or may be composed of members from insurance companies, manufacturers, and industries with an interest in safety standards. Certifying agencies allow manufacturers to affix a label or mark to identify that the equipment has been designed to the relevant product safety standards. Examples of such agencies in North America are the Factory Mutual Research Corporation, which certifies radios, Underwriters Laboratories (UL) that certifies mobile phones, and in Canada the Canadian Standards Association. In the EU the standard for intrinsic safety certification is the CENELEC standard EN 60079-11 and shall be certified according to the ATEX directive, while in other countries around the world the IEC standards are followed. To facilitate world trade, standards agencies around the world engage in harmonization activity so that intrinsically safe equipment manufactured in one country eventually might be approved for use in another without redundant, expensive testing and documentation.\n\n\n\n"}
{"id": "4734614", "url": "https://en.wikipedia.org/wiki?curid=4734614", "title": "Joint precision approach and landing system", "text": "Joint precision approach and landing system\n\nThe joint precision approach and landing system (JPALS) is a ship's system (CVN and LH type), all-weather landing system based on real-time differential correction of the Global Positioning System (GPS) signal, augmented with a local area correction message, and transmitted to the user via secure means. The onboard receiver compares the current GPS-derived position with the local correction signal, deriving a highly accurate three-dimensional position capable of being used for all-weather approaches via an Instrument Landing System-style display. While JPALS is similar to Local Area Augmentation System, but intended primarily for use by the military, some elements of JPALS may eventually see their way into civilian use to help protect high-value civilian operations against unauthorized signal alteration.\n\nThe development of JPALS was a result of two main military requirements. First, the military needs an all-service, highly mobile all-weather precision approach system, tailorable to a wide range of environments, from shipboard use to rapid installation at makeshift airfields. Second, they need a robust system that can maintain a high level of reliability in combat operations, particularly in its ability to effectively resist jamming.\n\nJPALS encompasses two main categories: SRGPS (shipboard relative GPS) and LDGPS (land/local differential GPS). SRGPS provides highly accurate approach positioning for operations aboard ship, including aircraft carriers, helo and STO/VL carriers, and other shipboard operations, primarily helicopter operations.\n\nLDGPS is further divided into three sub-categories: fixed base, tactical, and special missions. Fixed base is used for ongoing operations at military airfields around the world, while the tactical system is portable, designed for relatively short-term, austere airfield operations. The special missions system is a highly portable system capable of rapid installation and use by special forces.\n\nThe accuracy of local area augmentation system (LAAS) is better than CAT III ILS accuracy, and will provide horizontal and vertical resolutions of less than 1 m. Although the exact accuracy of JPALS will remain classified, it's estimated that JPALS will meet or exceed this accuracy for authorized users.\n\nThe main benefit of JPALS is that it's a system that can be taken anywhere, anytime, providing a safe and effective way to conduct 24/7, all-weather, anti-jam instrument landing system capability to all authorized users, worldwide. A secondary benefit is a significant reduction in cost over current systems.\n\nThe naval version of JPALS transmits a signal that has a low probability of intercept; so it is unlikely that an enemy will detect the signal and trace it back to its source. The existing system, tactical air navigation (TACAN), is not encrypted or concealed in any way, which can reveal the location of the ship on which it is installed. This is not acceptable in emissions control (EMCON) or stealth conditions.\n\nThe increase in both accuracy and reliability will significantly enhance operations while reducing non-operational periods due to weather or adversarial efforts.\n\n\n\n"}
{"id": "37298765", "url": "https://en.wikipedia.org/wiki?curid=37298765", "title": "Korea Elevator Safety Institute", "text": "Korea Elevator Safety Institute\n\nThe Korea Elevator Safety Institute (KESI), an affiliated public organization of the Korean Ministry of Public Administration and Security, was established in 1992 to secure elevator safety. Especially the institute ensures elevator safety by legal inspections and activities such as education, public relations, examinations, publishing, research and international cooperation under the government's 'Elevator Safety Act.\"\n\nAlso, diagnosis, supervision and consulting of elevators and mechanical parking system are major tasks. A comprehensive Information system is operated to utilize information effectively for safety management of elevators and escalators. Its main office is located in Seoul, Korea.\n"}
{"id": "997189", "url": "https://en.wikipedia.org/wiki?curid=997189", "title": "Lenovo", "text": "Lenovo\n\nLenovo Group Ltd. or Lenovo PC International, often shortened to Lenovo ( ), is a multinational technology company with headquarters in Beijing, China and Morrisville, North Carolina, United States. It designs, develops, manufactures, and sells personal computers, tablet computers, smartphones, workstations, servers, electronic storage devices, IT management software, and smart televisions. Lenovo was the world's largest personal computer vendor by unit sales from 2013 to 2015. It markets the ThinkPad line of notebook computers, IdeaPad, Yoga and Legion lines of notebook laptops, and the IdeaCentre and ThinkCentre lines of desktops.\n\nLenovo has operations in more than 60 countries and sells its products in around 160 countries. Lenovo's principal facilities are in Beijing and Morrisville, with research centers in Beijing, Shanghai, Shenzhen, Xiamen, Chengdu, Nanjing, and Wuhan in China, Yamato in Kanagawa Prefecture, Japan, and Morrisville in the U.S. It also has a joint venture with NEC, Lenovo NEC Holdings, which produces personal computers for the Japanese market.\n\nLenovo was founded in Beijing in November 1984 as Legend and was incorporated in Hong Kong in 1988. Lenovo acquired IBM's personal computer business in 2005 and agreed to acquire its Intel-based server business in 2014. Lenovo entered the smartphone market in 2012 and as of 2014 was the largest vendor of smartphones in Mainland China. In 2014 Lenovo acquired the mobile phone handset maker Motorola Mobility from Google.\n\nLenovo is listed on the Hong Kong Stock Exchange and is a constituent of the Hang Seng China-Affiliated Corporations Index, often referred to as \"Red Chips\".\n\nLiu Chuanzhi founded Lenovo on 1 November 1984 with a group of ten engineers in Beijing with 200,000 yuan. The Chinese government approved Lenovo's incorporation on the same day. Jiǎ Xùfú (贾续福), one of the founders of Lenovo, indicates the first meeting in preparation for starting the company was held on 17 October of the same year. Eleven people, the entirety of the initial staff, attended. Each of the founders was a middle-aged member of the Institute of Computing Technology attached to the Chinese Academy of Sciences. The 200,000 yuan used as start-up capital was approved by Zēng Màocháo (曾茂朝). The name for the company agreed upon at this meeting was the Chinese Academy of Sciences Computer Technology Research Institute New Technology Development Company.\n\nTheir first significant effort, an attempt to import televisions, failed. The group rebuilt itself within a year by conducting quality checks on computers for new buyers. Lenovo soon started developing a circuit board that would allow IBM-compatible personal computers to process Chinese characters. This product was Lenovo's first major success. Lenovo also tried and failed to market a digital watch. Liu said, \"Our management team often differed on which commercial road to travel. This led to big discussions, especially between the engineering chief and myself. He felt that if the quality of the product was good, then it would sell itself. But I knew this was not true, that marketing and other factors were part of the eventual success of a product.\" The fact that its staff had little business experience compounded Lenovo's early difficulties. \"We were mainly scientists and didn't understand the market,\" Liu said. \"We just learned by trial-and-error, which was very interesting—but also very dangerous,\" said Liu. In 1990, Lenovo started to manufacture and market computers using its own brand name.\n\nIn May 1988, Lenovo placed its first recruitment advertisement. The ad was placed on the front page of the \"China Youth News.\" Such ads were quite rare in China then. Out of the 500 respondents, 280 were selected to take a written employment exam. 120 of these candidates were interviewed in person. Although interviewers initially only had authority to hire 16 people, 58 were given offers. The new staff included 18 people with graduate degrees, 37 with undergraduate degrees, and three students with no university-level education. Their average age was 26. Yang Yuanqing, the current CEO of Lenovo, was among that group.\n\nLiu Chuanzhi received government permission to form a subsidiary in Hong Kong and to move there along with five other employees. Liu's father, already in Hong Kong, furthered his son's ambitions through mentoring and facilitating loans. Liu moved to Hong Kong in 1988. To save money during this period, Liu and his co-workers walked instead of taking public transportation. To keep up appearances, they rented hotel rooms for meetings.\n\nLenovo became publicly traded after a 1994 Hong Kong listing that raised nearly US$30 million. Prior to its IPO, many analysts were optimistic about Lenovo. The company was praised for its good management, strong brand recognition, and growth potential. Analysts also worried about Lenovo's profitability. Lenovo's IPO was massively over-subscribed. On its first day of trading, the company's stock price hit a high of HK$2.07 and closed at HK$2.00. Proceeds from the offering were used to finance sales offices in Europe, North America and Australia, to expand and improve production and research and development, and to increase working capital.\n\nWhen Lenovo was first listed, its managers thought the only purpose of going public was to raise capital. They had little understanding of the rules and responsibilities that went along with running a public company. Before Lenovo conducted its first secondary offering in 1997, Liu proudly announced the company's intent to mainland newspapers only to have its stock halted for two days by regulators to punish his statement. This occurred several times until Liu learned that he had to choose his words carefully in public. The first time Liu traveled to Europe on a \"roadshow\" to discuss his company's stock, he was shocked by the skeptical questions he was subjected to and felt offended. Liu later came to understand that he was accountable to shareholders. He said, \"Before I only had one boss, but CAS never asked me anything. I relied on my own initiative to do things. We began to think about issues of credibility. Legend began to learn how to become a truly international company.\"\n\nTo fund its continued growth, Lenovo issued a secondary offering of 50 million shares on the Hong Kong market in March 2000 and raised about US$212 million.\n\nMary Ma, Lenovo's chief financial officer from 1990 to 2007, was in charge of investor relations. Under her leadership, Lenovo successfully integrated Western-style accountability into its corporate culture. Lenovo's emphasis on transparency earned it a reputation for the best corporate governance among mainland Chinese firms. All major issues regarding its board, management, major share transfers, and mergers and acquisitions were fairly and accurately reported. While Hong Kong-listed firms were only required to issue financial reports twice per year, Lenovo followed the international norm of issuing quarterly reports. Lenovo created an audit committee and a compensation committee with non-management directors. The company started roadshows twice per year to meet institutional investors. Ma organized the first-ever investor relations conference held in Mainland China. The conference was held in Beijing in 2002 and televised on CCTV. Liu and Ma co-hosted the conference and both gave speeches on corporate governance.\nIn May 2015, Lenovo revealed a new logo at Lenovo Tech World in Beijing, with the slogan \"Innovation Never Stands Still\" (). Lenovo's new logo, created by Saatchi, New York, can be changed by its advertising agencies and sales partners, within restrictions, to fit the context. It has a lounging \"e\" and is surrounded by a box that can be changed to use a relevant scene, solid color, or photograph. Lenovo's Chief Marketing Officer David Roman said, \"When we first started looking at it, it wasn't about just a change in typography or the look of the logo. We asked 'If we really are a net-driven, customer-centric company, what should the logo look like?' We came up with the idea of a digital logo first … designed to be used on the internet and adaptable to context.\"\n\nIn early June 2015, Lenovo announced plans to sell up to US$650 million in five-year bonds denominated in Chinese yuan. The bonds will be sold in Hong Kong with coupon ranging from 4.95% to 5.05%. This is only the second sale of bonds in Lenovo's history. Financial commentators noted that Lenovo was paying a premium to list the bonds in yuan given relatively low costs for borrowing in American dollars.\n\nThe Tianxi computer was designed to make it easy for inexperienced Chinese consumers to use computers and access the Internet. One of its most important features was a button that instantly connected users to the Internet and opened the Web browser. It was co-branded with China Telecom and it was bundled with one year of Internet service. The Tianxi was released in 1998. It was the result of two years of research and development. It had a pastel-colored, shell-shaped case and a seven-port USB hub under its screen. As of 2000, the Tianxi was the best-selling computer in Chinese history. It sold more than 1,000,000 units in 2000 alone.\n\nLenovo works to integrate the management of each newly acquired company into its larger culture. Lenovo has a dedicated mergers and acquisitions team that tracks the progress of these integrations. Lenovo has an annual meeting where the management of newly acquired companies meets with its top 100 executives. In these meetings, held in English, Lenovo explains its global strategy and how new executives fit into its plans.\n\n Lenovo acquired IBM's personal computer business in 2005, including the ThinkPad laptop and tablet lines. Lenovo's acquisition of IBM's personal computer division accelerated access to foreign markets while improving both Lenovo's branding and technology. Lenovo paid US$1.25 billion for IBM's computer business and assumed an additional US$500 million of IBM's debt. This acquisition made Lenovo the third-largest computer maker worldwide by volume.\n\nIn regards to the purchase of IBM's personal computer division, Liu Chuanzhi said, \"We benefited in three ways from the IBM acquisition. We got the ThinkPad brand, IBM's more advanced PC manufacturing technology and the company's international resources, such as its global sales channels and operation teams. These three elements have shored up our sales revenue in the past several years.\"\n\nIBM acquired an 18.9% shareholding in Lenovo in 2005 as part of Lenovo's purchase of IBM's personal computing division. Since then, IBM has steadily reduced its holdings of Lenovo stock. In July 2008, IBM's interest in Lenovo fell below the 5% threshold that mandates public disclosure.\n\nIBM's Intel based server lines, including IBM System x and IBM BladeCenter were sold to Lenovo in 2014. Lenovo says it will gain access to more enterprise customers, improve its profit margins, and develop a closer relationship with Intel, the maker of most server processors, through its acquisition of IBM's x86-based server business. On 1 October 2014, Lenovo closed its acquisition of IBM's server division, with the final price put at $2.1 billion. Lenovo said this acquisition came in at a price lower than the previously announced $2.3 billion partially because of a change in the value of IBM inventories. The deal has been already approved by Europe, China and the United States. The United States Department of Treasury Committee on Foreign Investment in the United States (CFIUS) was reportedly the last hurdle for Lenovo, since the United States has the strictest policies. According to Timothy Prickett-Morgan from Enterprise Tech, the deal still awaits \"approval of regulators in China, the European Commission, and Canada.\"\n\nAfter closing, Lenovo said that its goal was to become the world's largest maker of servers. Lenovo also announced plans to start integrating IBM's workforce. The acquisition added about 6,500 new employees to Lenovo. Lenovo said that it has no immediate intent to cut jobs. Lenovo said that positions in research and development and customer-facing roles such as marketing would be \"100% protected\", but expected \"rationalization\" of its supply chain and procurement.\n\nLenovo said that its x86 servers will be available to all its channel partners. Lenovo plans to cut prices on x86 products in order to gain market share. This goes in alliance with IBM's vision of the future around cloud technologies and their own POWER processor architecture.\n\nLenovo's acquisition of IBM is arguably one of the greatest case studies on merging massive international enterprises. Though this acquisition in 2005 ultimately resulted in success, the integration of the businesses had a difficult and challenging beginning. Lenovo had employees from different cultures, different backgrounds, and different languages. These differences caused misunderstandings, hampering trust and the ability to build a new corporate culture. At the end of its first two years, Lenovo Group had met many of its original challenges, including integrating two disparate cultures in the newly formed company, maintaining the Think brand image for quality and innovation, and improving supply chain and manufacturing efficiencies. However, Lenovo had failed to meet a key objective of the merger: leveraging the combined strength of the two companies to grow volume and market share. In order to achieve success, Lenovo embraced diversify at multiple levels- business model, culture, and talent. By 2015, Lenovo grew into the world's number 1 PC maker, number 3 smartphone manufacturer and number 3 in the production of tablet computers.\n\nLenovo sold its smartphone and tablet division in 2008 for in order to focus on personal computers and then paid to buy it back in November 2009. , the mobile division ranked third in terms of unit share in China's mobile handset market. Lenovo invested in a fund dedicated to providing seed funding for mobile application development for its LeGarden online app store. As of 2010, LeGarden had more than 1,000 programs available for the LePhone. At the same time, LeGarden counted 2,774 individual developers and 542 developer companies as members.\n\nLenovo entered the smartphone market in 2012 and quickly became the largest vendor of smartphones in Mainland China. Entry into the smartphone market was paired with a change of strategy from \"the one-size-fits-all\" to a diverse portfolio of devices. These changes were driven by the popularity of Apple's iPhone and Lenovo's desire to increase its market share in mainland China. Lenovo passed Apple to become the No. 2 provider of smartphones to the Chinese market in 2012. However, due to there being about 100 smartphone brands sold in China, this second only equated to a 10.4% market share.\n\nIn May 2012, Lenovo announced an investment of US$793 million in the construction of a mobile phone manufacturing and R&D facility in Wuhan, China.\n\nOn January 27, 2011, Lenovo formed a joint venture to produce personal computers with Japanese electronics firm NEC. The companies said in a statement that they would establish a new company called Lenovo NEC Holdings, to be registered in the Netherlands. NEC received US$175 million in Lenovo stock. Lenovo was to own a 51% stake in the joint venture, while NEC would have 49%. Lenovo has a five-year option to expand its stake in the joint venture.\n\nThis joint venture was intended to boost Lenovo's worldwide sales by expanding its presence in Japan, a key market for personal computers. NEC spun off its personal computer business into the joint venture. As of 2010, NEC controlled about 20% of Japan's market for personal computers while Lenovo had a 5% share. Lenovo and NEC also agreed to explore cooperating in other areas such as servers and tablet computers.\n\nRoderick Lappin, chairman of the Lenovo-NEC joint venture, told the press that the two companies will expand their co-operation to include the development of tablet computers.\n\nIn April 2014, Lenovo purchased a portfolio of patents from NEC related to mobile technology. These included over 3,800 patent families in countries around the world. The purchase included standards-essential patents for 3G and LTE cellular technologies and other patents related to smartphones and tablets.\n\nIn June 2011, Lenovo announced that it planned to acquire control of Medion, a German electronics manufacturing company. Lenovo said the acquisition would double its share of the German computer market, making it the third-largest vendor by sales (after Acer and Hewlett-Packard). The deal, which closed in the third quarter of the same year, was the first in which a Chinese company acquired a well-known German company.\n\nThis acquisition will give Lenovo 14% of the German computer market. Gerd Brachmann, chairman of Medion, agreed to sell two-thirds of his 60 percent stake in the company. He will be paid in cash for 80 percent of the shares and will receive 20 percent in Lenovo stock. That would give him about one percent of Lenovo.\n\nIn September 2012, Lenovo agreed to acquire the Brazil-based electronics company Digibras, which sells products under the brand-name CCE, for a base price of 300 million reals (US$148 million) in a combination of stock and cash. An additional payment of 400 million reals was made dependent upon performance benchmarks. Prior to its acquisition of CCE, Lenovo already established a $30 million factory in Brazil, but Lenovo's management had felt that they needed a local partner to maximize regional growth. Lenovo cited their desire to take advantage of increased sales due to the 2014 World Cup that would be hosted by Brazil and the 2016 Summer Olympics and CCE's reputation for quality. Following the acquisition, Lenovo announced that its subsequent acquisitions would be concentrated in software and services.\n\nIn September 2012, Lenovo agreed to acquire the United States-based software company Stoneware, in its first software acquisition. The transaction was expected to close by the end of 2012; no financial details have been disclosed. Lenovo said that the company was acquired in order to gain access to new technology and that Stoneware is not expected to significantly affect earnings. More specifically, Stoneware was acquired to further Lenovo's efforts to improve and expand its cloud-computing services. For the two years prior to its acquisition, Stoneware partnered with Lenovo to sell its software. During this period Stoneware's sales doubled. Stoneware was founded in 2000. As of September 2012, Stoneware is based in Carmel, Indiana and has 67 employees.\n\nLenovo and EMC formed LenovoEMC as a joint venture to offer network attached storage (NAS) solutions. LenovoEMC's products were formerly offered under the Iomega brand name. After the formation of LenovoEMC, Iomega ceased to exist as business unit. LenovoEMC's products are designed for small and medium-sized businesses that do not have the budgets for enterprise-class data storage. LenovoEMC is part of a broader partnership between the two companies announced in August 2012. This partnership also includes an effort to develop x86-based servers and allowing Lenovo to act as an OEM for some EMC hardware. Lenovo is expected to benefit from the relatively high profit margins of the NAS market. LenovoEMC is part of Lenovo's Enterprise Products Group.\n\nOn 29 January 2014, Google announced it would sell Motorola Mobility to Lenovo for US$2.91 billion. When Google and Lenovo first announced the acquisition of Motorola, they said the purchase would be funded with $660 million in cash, $750 million in Lenovo stock, and a $1.5 billion promissory note due in three years. As of February 2014, Google owns about 5.94% of Lenovo's stock. The deal includes smartphone lines like the Moto X and Moto G and the Droid Turbo. Lenovo also got the future Motorola Mobility product roadmap. Google will retain the Advanced Technologies & Projects unit and all but 2,000 of the company's patents. Lenovo will receive royalty free licenses to all the patents retained by Google.\n\nLenovo has stated that Motorola was purchased in large part due to its long-standing relationships with cellular network operators in the United States and the United Kingdom. Lenovo previously had difficulty breaking into the United Kingdom due to the high proportion of customers who sign contracts and receive phones from carriers. A Lenovo executive said, \"There are lots of reasons why we bought Motorola but primarily because it has a history of distribution in the UK. Motorola has long and established relationships with routes to market in North America and the UK, where people are tied to their network operator.\"\n\nLenovo received approval from the European Union for its acquisition of Motorola in June 2014. In a statement the European Union said, \"The Commission concluded that the proposed acquisition would not raise competition concerns in relation to smart mobile devices (smartphones and tablets), given the limited market position of the parties and the presence of other strong suppliers in the market.\" At the time of the EU approval Lenovo said it was on track to win final approval of the merger in the United States.\n\nThe acquisition was completed on 30 October 2014. Motorola Mobility will remain headquartered in Chicago, and continue to use the Motorola brand, but Liu Jun, president of Lenovo's mobile device business, became the company's leader.\n\nIn March 2017, Lenovo announced it was partnering with Fort Lauderdale, Florida-based software storage virtualization company DataCore to add DataCore's parallel I/O-processing software to Lenovo's storage devices. The servers were reportedly designed to outperform Storage Area Network (SAN) SAN arrays.\n\nIn 2017 Lenovo formed a joint venture with Fujitsu and the Development Bank of Japan.\n\nIn 2018, Lenovo became the world's largest provider for the TOP500 supercomputers.\n\n\"Lenovo\" is a portmanteau of \"Le-\" (from Legend) and \"novo\", Latin ablative for \"new\". The Chinese name () means \"association\" (as in \"word association\") or \"connected thinking\". It can also imply creativity.\nFor the first 20 years of its existence, the company's English name was \"Legend\" (in Chinese 联想 Liánxiǎng). In 2002, Yang Yuanqing decided to abandon the Legend brand name to expand internationally. \"Legend\" was already in use by many businesses worldwide (whose products and services (in the United States, for example), would include those from both the technological and non-technological arenas of industry and commerce), making it impossible to register in most jurisdictions. In April 2003, the company publicly announced its new name, \"Lenovo\", with an advertising campaign including huge billboards and primetime television ads. Lenovo spent 18 million RMB on an eight-week television advertising campaign. The billboards showed the Lenovo logo against blue sky with copy that read, \"Transcendence depends on how you think.\" By the end of 2003, Lenovo had spent a total of 200 million RMB on rebranding.\n\nIn February 2015, Lenovo became the subject of controversy for having bundled software identified as malware on some of its laptops. The software, Superfish Visual Discovery, is a web browser add-on that injects price comparison advertising into search engine results pages. To intercept HTTPS-encrypted communications, the software also installed a self-signed digital certificate. When the Superfish private key was compromised, it was also discovered that the same private key was used across all installations of the software, leaving users vulnerable to security exploits utilizing the key. Lenovo made between US$200,000 to US$250,000 on its deal with Superfish. In 2017 Lenovo agreed to pay $3.5 million as part of a settlement with the US Federal Trade Commission.\n\nThe head of Superfish responded to security concerns by saying the vulnerability was \"inadvertently\" introduced by Komodia, which built the application. In response to the criticism, Lenovo detailed that it would cease further distribution and use of the Superfish software, and offered affected customers free six-month subscriptions to the McAfee LiveSafe software. Lenovo issued a promise to reduce the amount of \"bloatware\" it bundles with its Windows 10 devices, promising to only include Lenovo software, security software, drivers, and \"certain applications customarily expected by users\". \"Salon\" tech writer David Auerbach compared the Superfish incident to the Sony DRM rootkit scandal, and argued that \"installing Superfish is one of the most irresponsible mistakes an established tech company has ever made.\"\n\nFrom October 2014 through June 2015, the UEFI firmware on certain Lenovo models had contained software known as \"Lenovo Service Engine,\" which Lenovo says automatically sent non-identifiable system information to Lenovo the first time Windows is connected to the internet, and on laptops, automatically installs the Lenovo OneKey Optimizer program (software considered to be bloatware) as well. This process occurs even on clean installations of Windows. It was found that this program had been automatically installed using a new feature in Windows 8, Windows Platform Binary Table, which allows executable files to be stored within UEFI firmware for execution on startup, and is meant to \"allow critical software to persist even when the operating system has changed or been reinstalled in a 'clean' configuration\"; specifically, anti-theft security software. The software was discontinued after it was found that aspects of the software had security vulnerabilities, and did not comply with revised guidelines for appropriate usage of WPBT. On 31 July 2015, Lenovo released instructions and UEFI firmware updates meant to remove Lenovo Service Engine.\n\nAt a third time in 2015, criticism arose that Lenovo might have installed software that looked suspicious on their commercial Think-PC lines. This was discovered by Computerworld writer Michael Horowitz, who had purchased several Think systems with the Customer Feedback program installed, which seemed to log usage data and metrics. Further analysis by Horowitz revealed however that this was mostly harmless, as it was only logging the usage of some pre-installed Lenovo programs, and not the usage in general, and only if the user allowed the data to be collected. Horowitz also criticized other media for quoting his original article and saying that Lenovo preinstalled spyware, as he himself never used that term in this case and he also said that he does not consider the software he found to be spyware.\n\nAs of June 2016, a Duo Labs report stated that Lenovo was still installing bloatware, some of which leads to security vulnerabilities as soon as the user turns on their new PC. Lenovo advised users to remove the offending app, \"Lenovo Accelerator\". According to Lenovo, the app, designed to \"speed up the loading\" of Lenovo applications, created a man-in-the-middle security vulnerability.\n\nLenovo markets the ThinkPad line of notebook computers, IdeaPad, Yoga and Legion lines of notebook laptops, and the IdeaCentre and ThinkCentre lines of desktops. It expanded significantly in 2005 through its acquisition of IBM's personal computer business, including its ThinkPad and ThinkCentre lines. As of January 2013, shipments of THINK-branded computers have doubled since Lenovo's takeover of the brand, with profit margins thought to be above 5%. Lenovo aggressively expanded the THINK brand away from traditional laptop computers in favor of tablets and hybrid devices such as the ThinkPad Tablet 2, ThinkPad Yoga, ThinkPad 8, ThinkPad Helix, and ThinkPad Twist; the shift came as a response to the growing popularity of mobile devices, and the release of Windows 8 in October 2012. Lenovo has achieved significant success with this high-value strategy and it now controls more than 40% of the market for Windows computers priced above $900 in the United States.\n\nThe ThinkPad is a line of business oriented laptop computers known for their boxy black design, modeled after a traditional Japanese lunchbox. ThinkPads were originally an IBM product; they have been manufactured and sold by Lenovo since early 2005, following its acquisition of IBM's personal computer division. The ThinkPad has been used in space and is the only laptop certified for use on the International Space Station.\n\nThe ThinkCentre is a line of business-oriented desktop computers which was introduced in 2003 by IBM and since has been produced and sold by Lenovo since 2005. ThinkCentre computers typically include mid-range to high-end processors, options for discrete graphics cards, and multi-monitor support. Similar to the ThinkPad line of computers, there have been budget lines of ThinkCentre branded computers in the past. Some examples of this include: M55e series, A50 series, M72 series. These \"budget\" lines are typically \"thin clients\" however.\n\nThe ThinkServer product line began with the TS100 from Lenovo. The server was developed under agreement with IBM, by which Lenovo would produce single-socket and dual-socket servers based on IBM's xSeries technology. An additional feature of the server design was a support package aimed at small businesses. The focus of this support package was to provide small businesses with software tools to ease the process of server management and reduce dependence on IT support.\n\nLenovo ThinkStations are workstations designed for high-end computing. In 2008, Lenovo expanded the focus of its THINK brand to include workstations, with the ThinkStation S10 being the first model released.\n\nHigh-end monitors are marketed under the ThinkVision name. ThinkVision displays share a common design language with other THINK devices such as the ThinkPad line of notebook computers and ThinkCentre desktops. At the 2014 International CES, Lenovo announced the ThinkVision Pro2840m, a 28-inch 4K display aimed at professionals. Lenovo also announced another 28-inch 4K touch-enabled device running Android that can function as an all-in-one PC or an external display for other devices.\n\nAt the 2016 International CES, Lenovo announced two displays with both USB-C and DisplayPort connectivity. The ThinkVision X24 Pro monitor is a 24-inch 1920 by 1080 pixel thin-bezel display that uses an IPS LCD panel. The ThinkVision X1 is a 27-inch 3840 by 2160 pixel thin-bezel display that uses a 10-bit panel with 99% coverage of the sRGB color gamut. The X24 includes a wireless charging base for mobile phones. The X1 is the first monitor to receive the TUV Eye-Comfort certification. Both monitors have HDMI 2.0 ports, support charging laptops, mobile phones, and other devices, and have Intel RealSense 3D cameras in order to support facial recognition. Both displays have dual-array microphones and 3-watt stereo speakers.\n\nThe IdeaPad line of consumer-oriented laptop computers was introduced in January 2008. The IdeaPad is the result of Lenovo's own research and development; Unlike the ThinkPad line, its design and branding were not inherited from IBM. The IdeaPad's design language differs markedly from the ThinkPad and has a more consumer-focused look and feel.\n\nOn September 21, 2016, Lenovo confirmed that their Yoga series is not meant to be compatible with Linux operating systems, that they know it is impossible to install Linux on some models, and that it is not supported. This came in the wake of media coverage of problems that users were having while trying to install Ubuntu on several Yoga models, including the 900 ISK2, 900 ISK For Business, 900S, and 710, which were traced back to Lenovo disabling and removing support for the AHCI storage mode for the device's Solid State Drive in the computer's BIOS, in favor of a RAID mode that is only supported by Windows 10 drivers that come with the system.\n\nAll IdeaCentres are all-in-one machines, combining processor and monitor into a single unit. The desktops were described by HotHardware as being \"uniquely designed\". The first IdeaCentre desktop, the IdeaCentre K210, was announced by Lenovo on 30 June 2008. While the IdeaCentre line consists only of desktops, it shares design elements and features with the IdeaPad line. One such feature was Veriface facial recognition technology.\n\nAt CES 2011, Lenovo announced the launch of four IdeaCentre desktops: the A320, B520, B320, and C205. In the autumn of 2012, the firm introduced the more powerful IdeaCentre A720, with a 27-inch touchscreen display and running Windows 8. With a TV tuner and HDMI in, the A720 can also serve as a multimedia hub or home theater PC.\n\nIn 2013, Lenovo added a table computer to the IdeaCentre line. The Lenovo IdeaCentre Horizon Table PC, introduced at the 2013 International CES is a 27-inch touchscreen computer designed to lay flat for simultaneous use by multiple people. Thanks to its use of Windows 8, the Horizon can also serve as a desktop computer when set upright.\n\nAs of January 2013, Lenovo only manufactured phones that use the Android operating system from Google. Numerous press reports indicated that Lenovo planned to release a phone running Windows Phone 8, According to J. D. Howard, a vice president at Lenovo's mobile division, the company would release a Windows Phone product if there is market demand.\n\nLenovo has implemented an aggressive strategy to replace Samsung Electronics as Mainland China market's top smartphone vendor. It has spent $793.5 million in Wuhan in order to build a plant that can produce 30 to 40 million phones per year. Data from Analysys International shows that Lenovo experienced considerable growth in smartphone sales in China during 2012. Specifically, it saw its market share increase to 14.2% during 2012's third quarter, representing an increase when compared to 4.8% in the same quarter of 2011. IDC analysts said that Lenovo's success is due to its \"aggressive ramping-up and improvements in channel partnerships.\" Analysys International analyst Wang Ying wrote, \"Lenovo possesses an obvious advantage over rivals in terms of sales channels.\" The company's CEO, Yang Yuanqing, said, \"Lenovo does not want to be the second player ... we want to be the best. Lenovo has the confidence to outperform Samsung and Apple, at least in the Chinese market.\"\n\nAccording to IHS iSuppli, Lenovo was a top-three smartphone maker in China with a 16.5% market share in the first quarter of 2012. According to a May report released by IDC Lenovo ranks fourth in the global tablet market by volume. As of November 2012, Lenovo was the second largest seller of mobile phones in China when measured by volume.\n\nIn May 2013, Lenovo CEO Yang Yuanqing indicated that the company had aimed to release smartphones in the United States within the next year. Later in October, Lenovo expressed interest in acquiring the Canadian smartphone maker BlackBerry Ltd. However, its attempt was reportedly blocked by the Government of Canada, citing security concerns due to the use of BlackBerry devices by prominent members of the government. An official stated that \"we have been pretty consistent that the message is Canada is open to foreign investment and investment from China in particular but not at the cost of compromising national security\".\n\nIn January 2014, Lenovo announced a proposed deal to acquire Motorola Mobility to bolster its plans for the U.S. market. Microsoft officially announced that Lenovo had become the hardware partner of Windows Phone platform at the Mobile World Congress 2014. In January 2016, Lenovo announced at CES that the company would be producing the first Project Tango phone.\n\nLenovo plus Motorola was the 3rd largest producer of smartphones by volume in the world between 2011 and 2014. Since Lenovo's acquisition of Motorola Mobility, the combined global market share of Lenovo plus Motorola has fallen from 7.2% in 2014 to 3.9% in the third quarter of 2016. A number of factors have been cited as the cause of this reduced demand, including the fact that Lenovo relied heavily on carriers to sell its phones, its phones lacked strong branding and unique features to distinguish them in the competitive Chinese market where a weak economy and saturated market is slowing demand and the culture clash between a more hierarchical PC company and the need to be nimble to sell rapidly-evolving smartphones. In response to the weak sales, Lenovo announced in August 2015 that it would lay off 3,200 employees, mostly in its Motorola smartphone business.\n\nIn the reorganization which followed, Lenovo was uncertain how to brand its Motorola smartphones. In November 2015, members of Lenovo management made statements that Lenovo would use the Motorola brand for all its smartphones. Then, in January 2016, Lenovo announced that it would be eliminating the Motorola brand in favor of \"Moto by Lenovo\". The company reversed course in March 2017 and announced that the Motorola brand name would be used in all regions in future products. \"In 2016, we just finished transforming ourselves,\" Motorola Chairman and President Aymar de Lencquesaing said in an interview, \"We have clarity on how we present ourselves.\"\n\nIn November 2011, Lenovo said it would soon unveil a smart television product called LeTV, expected for release in the first quarter of 2012. \"The PC, communications and TV industries are currently undergoing a \"smart\" transformation. In the future, users will have many smart devices and will desire an integrated experience of hardware, software and cloud services.\" Liu Jun, president of Lenovo's mobile-Internet and digital-home-business division. In June 2013 Lenovo announced a partnership with Sharp to produce smart televisions. In March 2014, Lenovo announced that it projected smart television sales surpassing one million units for 2014. The same month Lenovo released its flagship S9 featuring the fastest CPU of any smart television.\n\nRumors that Lenovo was developing a wearable device were confirmed in October 2014 after the company submitted a regulatory finding to the Federal Communications Commission. The device, branded a \"Smartband,\" has a battery life of seven days. It has an optical heart-rate monitor and can be used to track distance and time spent running and calories burned. It can also notify the user of incoming calls and texts. It can also unlock computers without the use of a password. The Smartband went on sale in October 2014. Lenovo started offering the device for sale on its website without a formal product announcement.\n\nAt the IFA 2018 Lenovo launched several smart home products.\n\nREACHit is a storage management application. It is designed to help users access, organize, and search files across multiple devices and operating systems. It connects Windows personal computers, Android devices, and iOS devices and works with Google Drive, OneDrive, Dropbox, and Box. On Windows devices, REACHit is integrated with Windows File Explorer. Lenovo began bundling REACHit with all its computers and tablets in early 2015. Through a partnership with Microsoft, REACHit is fully integrated with Cortana in Windows 10, a voice-based integrated assistant. REACHit extends the search capabilities of Cortana, giving it access to a much wider range of files, including those stored in Google Drive, Dropbox, and Microsoft's own OneDrive as described above, in addition to implementing searches across multiple devices and making them context sensitive. REACHit was discontinued and shut down on September 12, 2016.\n\nSHAREit is a free application from Lenovo that allows Windows, Windows Phone, Android, and iOS devices to transfer files directly by ad-hoc Wi-Fi connections.\n\nIn April 2015, Lenovo released WRITEit, a hand-writing recognition engine that interprets input from a stylus and turns it into text. WRITEit works with almost all applications and online forms that accept text input.\n\nSECUREit protects mobile devices against viruses, unauthorized access, and spam. It includes an anti-theft system that locks the device when someone tries to change the SIM card, making the phone unusable without a password. SECUREit encrypts call records and contacts. It also speeds up devices by ensuring that duplicate background processes do not run and keeping the cache clean.\n\nSYNCit backs up call logs and contact information. SYNCit works on non-Lenovo devices such as Android smartphones.\n\nThe SNAPit Camera app controls cameras on phones and tablets. It allows shooting panoramas, low-light scenes, photo editing, and creating animated GIFs.\n\nThe SEEit Gallery app is designed to complement the SNAPit Camera app. It uses image recognition software to automatically sort photos into folder. This app also lets users edit photos with filters and effects.\n\nAt the Mobile World Congress in 2016 Lenovo introduced Lenovo Connect, a wireless roaming service. This service works across devices, networks, and international borders in China, Europe, the Middle East, and Africa. Lenovo Connect eliminates the need to buy new SIM cards when crossing borders. Lenovo Connect started service for phones and select ThinkPad laptops in China in February 2016.\n\nLenovo's principal facilities are in Beijing, Morrisville, North Carolina and Singapore, with research centers in Beijing, Morrisville, Shanghai, Shenzhen, Xiamen, Chengdu, Nanjing, and Wuhan in China, and Yamato in Kanagawa Prefecture, Japan. Lenovo operates factories in Chengdu and Hefei in China, and in Japan. A flagship store opened in Beijing in February 2013.\nLenovo's manufacturing operations are a departure from the usual industry practice of outsourcing to contract manufacturers. Lenovo instead focuses on vertical integration in order to avoid excessive reliance on original equipment manufacturers and to keep down costs. Speaking on this topic, Yang Yuanqing said, \"Selling PCs is like selling fresh fruit. The speed of innovation is very fast, so you must know how to keep up with the pace, control inventory, to match supply with demand and handle very fast turnover.\" Lenovo benefited from its vertical integration after flooding affected hard-drive manufacturers in Thailand in 2011, as the company could continue manufacturing operations by shifting production towards products for which hard drives were still available.\n\nLenovo began to emphasize vertical integration after a meeting in 2009 in which CEO Yang Yuanqing, and the head of Lenovo's supply chain, analyzed the costs versus the benefits of in-house manufacturing, and decided to make at least 50% of Lenovo's manufacturing in-house. Lenovo Chief Technology Officer George He said that vertical integration is having an important role in product development. He stated, \"If you look at the industry trends, most innovations for\" PCs, smartphones, tablets and smart TVs are related to innovation of key components—display, battery and storage. Differentiation of key parts is so important. So we started investing more...and working very closely with key parts suppliers.\" Previously, lack of integration due to numerous foreign acquisitions and an excessive number of \"key performance indicators\" (KPIs) was making Lenovo's expansion expensive and creating unacceptably slow delivery times to end-customers. Lenovo responded by reducing the number of KPIs from 150 to 5, offering intensive training to managers, and working to create a global Lenovo culture. Lenovo also doubled-down on vertical integration and manufacturing near target markets in order to cut costs at time when its competitors were making increased use of outsourcing off-shoring. By 2013, Lenovo ranked 20th on Gartner's list of top 50 supply chains, whereas in 2010 the company was unranked.\n\nIn 2012, Lenovo partially moved production of its ThinkPad line of computers to Japan. ThinkPads will be produced by NEC in Yamagata Prefecture. Akaemi Watanabe, president of Lenovo Japan, said, \"As a Japanese, I am glad to see the return to domestic production and the goal is to realize full-scale production as this will improve our image and make the products more acceptable to Japanese customers.\"\n\nIn October 2012, Lenovo announced that it would start assembling computers in Whitsett, North Carolina. Production of desktop and laptop computers, including the ThinkPad Helix began in January 2013. , 115 workers were employed at this facility. Lenovo has been in Whitsett since 2008, where it also has centers for logistics, customer service, and return processing.\n\nIn 2015, Lenovo and Hong Kong Cyberport Management Company Limited, a government-sponsored business park for technology firms, reached a deal to \"jointly build a cloud service and product research and development center.\" Lenovo's Asia Pacific data center will also be housed in Cyperport.\n\nLenovo assembles smartphones in Chennai, India through a contract manufacturing agreement with Flextronics. In November 2015, Lenovo announced that it would start manufacturing computers in Pondicherry.\n\nThe company executive headquarters are in Morrisville, North Carolina, near Raleigh in the Research Triangle metropolitan area, in the United States. As of October 2012, the facility has about 2,000 employees. Lenovo identifies its facilities in Morrisville, Beijing, and Singapore as its \"key location addresses,\" where its principal operations occur. The company stated that \"by foregoing a traditional headquarters model and focusing on centers of excellence around the world, Lenovo makes the maximum use of its resources to create the best products in the most efficient and effective way possible.\" The company registered office is on the 23rd floor of the Lincoln House building of the Taikoo Place in Quarry Bay, Hong Kong.\n\nPreviously the company's U.S. headquarters were in Purchase, Harrison, New York. About 70 people worked there. In 2006, Lenovo announced that it was consolidating its U.S. headquarters, a logistics facility in Boulder, Colorado, and a call center in Atlanta, Georgia to a new facility in Morrisville. The company received offers of over $11 million in incentive funds from the local Morrisville, NC area and from the State of North Carolina on the condition that the company employs about 2,200 people. \nIn early 2016, Lenovo carried out a comprehensive restructuring of its business units.\n\nFrom 4 March 2013, Lenovo was included as a constituent stock in the Hang Seng Index. Lenovo replaced the unprofitable Aluminum Corp of China, a state-owned enterprise, on the list of 50 key companies on the Hong Kong stock exchange that constitute the Hang Seng Index. The inclusion of Lenovo and Tencent, China's largest internet firm, significantly increased the weight of the technology sector on the index. Being added to the Hang Seng Index was a significant boon for Lenovo and its shareholders as it widened the pool of investors willing to purchase Lenovo's stock. For instance, index funds pegged to the Hang Seng and pension funds that consider index inclusion now have the opportunity to invest in Lenovo. In November 2013 Lenovo reported that they had achieved double-digit market share in the United States for the first time.\n\nLenovo is the world's largest personal computer vendor by unit sales from 2013 to 2015. In 2016 Lenovo shipped an estimated 55.5 million PCs, for an estimated 21.3% market share, according to market research firm International Data Corporation. For the fiscal year ending March 2016, the company reported revenue of USD$44.9 billion. The company's expansion was boosted in part by a joint venture with NEC in Japan called Lenovo NEC Holdings.\n\nLenovo was able to reclaim its top spot in PC market share 2018.\n\nIn 2009, China Oceanwide Holdings Group, a private investment firm based in Beijing, bought 29% of Legend Holdings, the parent company of Lenovo, for 2.76 billion yuan. , 65% of Lenovo stock was held by the general public, 29% by Legend Holdings, 5.8% by Mr. Yang, and 0.2% by other directors.\n\nResponding to claims that Lenovo is a state-owned enterprise CEO Yang Yuanqing said: \"Our company is a 100% market oriented company. Some people have said we are a state-owned enterprise. It's 100% not true. In 1984 the Chinese Academy of Sciences only invested $25,000 in our company. The purpose of the Chinese Academy of Sciences to invest in this company was that they wanted to commercialize their research results. The Chinese Academy of Sciences is a pure research entity in China, owned by the government. From this point, you could say we're different from state-owned enterprises. Secondly, after this investment, this company is run totally by the founders and management team. The government has never been involved in our daily operation, in important decisions, strategic direction, nomination of the CEO and top executives and financial management. Everything is done by our management team.\"\n\nIn early 2006, the U.S. State Department was harshly criticized for purchasing 16,000 computers from Lenovo. Critics argued that Lenovo was controlled by the Chinese government and a potential vehicle for espionage against the United States. Yang spoke out forcefully and publicly to defend Lenovo. He said, \"We are not a government-controlled company.\" He pointed out that Lenovo pioneered China's transition to a market economy and that in the early 1990s had fought and beaten four state-owned enterprises that dominated the Chinese computer market. Those firms had the full backing of the state while Lenovo received no special treatment. The State Department deal went through. Yang worried that fears about Lenovo's supposed connections to the Chinese government would be an ongoing issue in the United States. Yang worked to ease worries by communicating directly with Congress.\n\nYang dramatically increased his ownership stake by acquiring 797 million shares in 2011. As of June 2011, Yang owned an 8 percent stake in Lenovo. He previously owned only 70 million shares. In a statement, Yang said, \"While the transaction is a personal financial matter, I want to be very clear that my decision to make this investment is based on my strong belief in the company's very bright future. Our culture is built on commitment and ownership – we do what we say, and we own what we do. My decision to increase my holdings represents my steadfast belief in these principles.\"\n\nLenovo's corporate culture differs from other Chinese companies. While Lenovo was founded using seed capital from the state-owned Chinese Academy of Sciences, Lenovo is run as a private enterprise with little or no interference by the state. Lenovo's senior executives, including many non-Chinese, rotate between two head offices, one in Beijing and the other in Morrisville, North Carolina, and Lenovo's research and development center in Japan. Two foreigners have previously served as Lenovo's CEO.\n\nEnglish is Lenovo's official language. Lenovo's CEO, Yang Yuanqing, initially did not understand English well, but relocated his family to Morrisville in order to improve his language skills and learn American ways. One American Lenovo executive interviewed by \"The Economist\" praised Yang for his efforts to make Lenovo a friendly place for foreigners to work. He said that Yang had created a \"performance culture\" in place of the traditional Chinese work style of \"waiting to see what the emperor wants.\"\n\nYang Yuanqing is the chairman and chief executive officer of Lenovo. One of his major achievements was leading Lenovo to become the best-selling personal computer brand in China since 1997. In 2001, \"Business Week\" named him one of Asia's rising stars in business. Yang was president and CEO of Lenovo until 2004, when Lenovo closed its acquisition of IBM's PC division, after which Yang was succeeded as Lenovo CEO by IBM's Stephen M. Ward, Jr. Ward was succeeded by Bill Amelio on 20 December 2005. In February 2009, Yang replaced Amelio as CEO and has served in that capacity ever since. Yang was chairman of Lenovo's board from 2004 to 2008, and returned as chairman in 2012 alongside his role as CEO.\n\nIn 2012, Yang received a $3 million bonus as a reward for record profits, which he in turn redistributed to about 10,000 of Lenovo's employees. According to Lenovo spokesman, Jeffrey Shafer, Yang felt that it would be the right thing to, \"redirect [the money] to the employees as a real tangible gesture for what they done.\" Shafer also said that Yang, who owns about eight percent of Lenovo's stock, \"felt that he was rewarded well simply as the owner of the company.\" The bonuses were mostly distributed among staff working in positions such as production and reception who received an average of 2,000 yuan or about US$314. This was almost equivalent to a monthly salary of an average worker in China. Yang made a similar gift of $3.25 million again in 2013.\n\nAccording to Lenovo's annual report, Yang earned $14 million, including $5.2 million in bonuses, during the fiscal year that ended in March 2012.\n\nIn 2013, \"Barron's\" named Yang one of the \"World's Best CEOs.\"\n\nLiu Chuanzhi is the founder and chairman of Lenovo. Liu was trained as an engineer at a military college and later went on to work at the Chinese Academy of Sciences. Like many young people during the Cultural Revolution, Liu was denounced and sent to the countryside where he worked as a laborer on a rice farm. Liu claims Hewlett-Packard as a key source of inspiration. In an interview with \"The Economist\" he stated that \"Our earliest and best teacher was Hewlett-Packard.\" For more than ten years, Lenovo was Hewlett-Packard's distributor in China. In reference to Lenovo's later acquisition of IBM's personal computer unit Liu said, \"I remember the first time I took part in a meeting of IBM agents. I was wearing an old business suit of my father's and I sat in the back row. Even in my dreams, I never imagined that one day we could buy the IBM PC business. It was unthinkable. Impossible.\"\n\nIn early 2013, Lenovo announced the addition of Yahoo founder Jerry Yang to its board. Lenovo's CEO Yang Yuanqing said, \"Jerry's appointment as an observer to our board furthers Lenovo's reputation as a transparent international company.\" Just prior to the appointment of Jerry Yang, Tudor Brown, the founder of British semiconductor design firm ARM, was also appointed to Lenovo's board. Speaking of both men Yang Yuanqing said, \"We believe that they will add a great deal to our strategic thinking, long-term direction and, ultimately, our ability to achieve our aspirations in the PC plus era.\"\n\nIn 2009, Lenovo became the first personal computer manufacturer to divide countries into emerging markets and mature markets. Lenovo then developed a different set of strategies for each category. Lenovo's competitors have widely adopted the same approach\n\nIn 2012, Lenovo made a major effort to expand its market share in developing economies such as Brazil and India through acquisitions and increased budgets for marketing and advertising. While Lenovo has not revealed its total spending on marketing, it did increase marketing and advertising expenditures by $248 million in the fiscal year ending in 2012.\n\nIn China, Lenovo has a vast distribution network designed to make sure that there is at least one shop selling Lenovo computers within 50 kilometers of nearly all consumers. Lenovo has also developed close relationships with its Chinese distributors, who are granted exclusive territories and only carry Lenovo products.\n\nAs of July 2013, Lenovo believes that urbanization initiatives being pushed by Premier Li Keqiang will allow it to sustain sales growth in China for the foreseeable future. Speaking at Lenovo's annual general meeting in Hong Kong in 2013, Yang Yuanqing said: \"I believe urbanisation will help us further increase the overall [domestic] PC market.\" Yang also stressed the opportunity presented by the China's relatively low penetration rate of personal computers. Lenovo previously benefited from the Chinese government's rural subsidies, part of a wider economic stimulus initiative, designed to increase purchases of appliances and electronics. That program, which Lenovo joined in 2004, ended in 2011. Lenovo enjoys consistent price premiums over its traditional competitors in rural markets and a stronger local sales and service presence.\n\nLenovo has gained significant market share in India through bulk orders to large companies and government agencies. For example, the government of Tamil Nadu ordered a million laptops from Lenovo in 2012 and single-handedly made the firm a market leader. Lenovo distributes most of the personal computers it sells in India through five national distributors such as Ingram Micro and Redington.\n\nGiven that most smartphones and tablets are sold to individuals Lenovo is pursuing a different strategy making use of many small state-centric distributors. Amar Babu, Lenovo's managing director for India, said, \"To reach out to small towns and the hinterland, we have tied up with 40 regional distributors. We want our distributors to be exclusive to us. We will, in turn, ensure they have exclusive rights to distribute Lenovo products in their catchment area.\" As of 2013, Lenovo had about 6,000 retailers selling smartphones and tablets in India. In February 2013, Lenovo established a relationship with Reliance Communications to sell smartphones. The smartphones carried by Reliance have dual-SIM capability and support both GSM and CDMA. Babu claims that the relative under penetration of smartphones in India represents an opportunity for Lenovo.\n\nLenovo has assembled a team of senior managers familiar with the Indian market, launched mobile phones at all price points there, and worked on branding to build market share. As of February 2014, Lenovo claims that its sales of smartphones in India have been increasing 100% per quarter while the market is only growing 15-20% over the same period. Lenovo did marketing tests of its smartphones in November 2012 in Gujarat and some southern cities, where Lenovo already had a strong presence. Lenovo's strategy has been to create awareness, maintain a broad selection of phones at all price points, and develop distribution networks. Lenovo partnered with two national distributors and over 100 local distributors. As of February 2014, more than 7,000 retail outlets in India sold Lenovo smartphones. Lenovo has also partnered with HCL in order to set up 250 service centres in 110 cities.\n\nIn India, Lenovo grants distributors exclusive territories but allows them to sell computers from other companies. Lenovo uses its close relationships with distributors to gain market intelligence and speed up product development.\n\nLenovo reported a year-on-year increase of about 951% in tablet sales in India for the first quarter of 2014. Canalys, a market research firm, said Lenovo took market share away from Apple and Samsung in the country.\n\nLenovo first started doing business in South Africa, establishing a sales office, and then expanded to East African markets such as Kenya, Tanzania, Ethiopia, Uganda, and Rwanda. West Africa followed when Lenovo set-up a Nigerian legal office and then expanded to Ghana, Zimbabwe, Mozambique and Botswana.\n\nAccording to Lenovo's general manager for Africa, Graham Braum, Lenovo's strategy is to put \"great emphasis on products that sell well in Africa\" and roll out \"products alongside different African governments' rolling out of wireless technology.\" Products such as the Lenovo Yoga series are popular in Africa because of their long battery life, as many areas have unreliable electrical supply. Other popular products include the Lenovo notebooks, which were introduced in 2008.\n\nLenovo picked Nigeria in 2013 to release its smartphone because unlike South Africa and other African countries, there is no requirement to partner with a local telecom firm to sell its phones.\n\nIn the long term, according to Braum, \"Lenovo in Africa will focus on continuing to consistently supply personal computer products and allow this market to grow, while moving into new territory such as mobile and enterprise.\"\n\nIn the United States, Lenovo began the \"For Those Who Do\" marketing campaign in 2010, created by the ad agency Saatchi & Saatchi. The campaign was Lenovo's first to go global, except for its domestic market in China, where it retained its existing \"Imagine\" (Pinyin: \"lian xiang\") slogan. The campaign did not reach China because \"do\" carries connotations of manual labor in the country, an image that Lenovo did not want attached to their brand. \"For Those Who Do\" was designed to appeal to young consumers in the 18- to 25-year-old demographic by stressing its utility to creative individuals that Lenovo's advertising refers to as \"doers\". One of Lenovo's operational centers is located in North Carolina, United States. Lenovo also started manufacturing products in the USA in 2012.\n\nIn October 2013, Lenovo announced that it had hired Ashton Kutcher as a product engineer and spokesman. Kutcher announced Lenovo's Yoga Tablet at a media event the same month; he flew to China to meet with Lenovo executives shortly thereafter. David Roman, Lenovo's chief marketing officer, said, \"His partnership goes beyond traditional bounds by deeply integrating him into our organization as a product engineer. Ashton will help us break new ground by challenging assumptions, bringing a new perspective and contributing his technical expertise to Yoga Tablet and other devices.\" Kutcher co-founded A-Grade Investments, an investor in Airbnb, Foursquare, Spotify, Path, Uber, and other technology firms. Kutcher studied biochemical engineering at the University of Iowa.\n\nKobe Bryant starred in ads aired in China and other Asian countries for the K900 smartphone in 2013.\n\nLenovo was an official computer sponsor of the 2006 Winter Olympics in Turin, Italy, and the 2008 Summer Olympics in Beijing. When asked about Lenovo's brand Yang Yuanqing said, \"The Beijing Olympics were very good for brand awareness in countries like the US and Argentina, but not good enough.\"\n\nIn December 2011, Lenovo announced the YouTube Space Lab contest. It was held in conjunction with YouTube, NASA, the European Space Agency, and JAXA. The contest allowed students between the ages of 14 and 18 the chance to devise experiments to be performed by astronauts on the International Space Station. The global winners had their experiment results live-streamed from space, and received a trip to either Japan or Russia.\n\nIn July 2012, Lenovo and the National Football League (NFL) announced that Lenovo had become the NFL's \"Official Laptop, Desktop and Workstation Sponsor.\" Lenovo said that this was its largest sponsorship deal ever in the United States. Lenovo will receive advertising space in NFL venues and events and be allowed to use the NFL logo on its products and ads. Lenovo said that this sponsorship would boost its efforts to market to the key 18- to 35-year-old male demographic.\n\nThe NFL has been a Lenovo customer since 2007 and the sponsorship resulted from that relationship. NFL stars Jerry Rice, DeAngelo Williams, and Torry Holt were on hand for the announcement and a celebration with 1,500 Lenovo employees. Lenovo's sponsorship will last at least three years.\n\nLenovo used a short-film entitled \"The Pursuit\" in its \"For Those Who Do\" campaign launched in 2011. The film depicted a mysterious young woman using the IdeaPad Yoga 13 to stay one-step-ahead of her evil pursuers. Martin Campbell, who previously worked on action movies and James Bond films such as GoldenEye and the remake of Casino Royale, shot this film. Lenovo was the first Chinese company to make use of such marketing techniques.\n\nIn May 2015, Lenovo hosted its first ever \"Tech World\" conference in Beijing. The CEOs of Intel, Microsoft, and Baidu delivered keynote addresses along with Lenovo CEO Yang Yuanqing. Lenovo also used Tech World to announce a refresh of its corporate logo and visual identity. The shift in Lenovo's visual presentation was accompanied by changes in Lenovo's business model. Lenovo said that it was transitioning from being solely a hardware maker to producing both hardware and software.\n\nLenovo announced several concept and production devices at Tech World including Smart View, a concept smartwatch with two screens and a virtual display; Smart Cast, a concept smartphone with a built-in laser projector that displays content and virtual user interfaces such as keyboards and musical instruments; Lenovo Cast, an Android-based streaming video device; Smart Shoes, concept shoes with a screen to display the user's mood and fitness tracking sensors; the ThinkPad 10, a new tablet computer; and Cortana integration with Lenovo devices and software, including REACHit, which extends Cortana's search functions to non-Microsoft services.\n\nZUK, a separate company formed by Lenovo in 2014, announced several products at Tech World, These included slim power banks, 3D printers that can print food such as chocolate, an outdoor sound box, and a Wi-Fi based control system for home automation.\n\nLenovo launched a multi-year advertising campaign called \"Goodweird\" in the last half of 2015. Goodweird is designed to convey the idea that designs that seem strange initially often become familiar and widely accepted. The Goodweird campaign includes a video with famous images of early attempts to fly with the aid of homemade wings and a bicycle that transitions to a modern-day shot of a man soaring across mountains in a wingsuit before transitioning again to a shot of the Stealth Bomber. Lenovo worked with three agencies on Goodweird: London-based DLKW Low, We Are Social, and Blast Radius. Goodweird is part of Lenovo's wider strategy to appeal to millennials with an emphasis on design trendsetters. A portion of the funding for Goodweird is being directed to prominent YouTubers and Viners. BuzzFeed has been engaged to create relevant content.\n\n\n"}
{"id": "9531655", "url": "https://en.wikipedia.org/wiki?curid=9531655", "title": "List of IEEE milestones", "text": "List of IEEE milestones\n\nThis list of IEEE Milestones describes the Institute of Electrical and Electronics Engineers (IEEE) milestones, representing key historical achievements in electrical and electronic engineering.\n\n\n\n\n"}
{"id": "2840597", "url": "https://en.wikipedia.org/wiki?curid=2840597", "title": "List of industrial parks by size", "text": "List of industrial parks by size\n\nList of industrial parks or estates by land area:\n\n"}
{"id": "1052910", "url": "https://en.wikipedia.org/wiki?curid=1052910", "title": "Miniaturization", "text": "Miniaturization\n\nMiniaturization (Br.Eng.: \"Miniaturisation\") is the trend to manufacture ever smaller mechanical, optical and electronic products and devices. Examples include miniaturization of mobile phones, computers and vehicle engine downsizing. In electronics, Moore's law, which was named after Intel co-founder Gordon Moore, predicted that the number of transistors on an integrated circuit for minimum component cost doubles every 18 months. This enables processors to be built in smaller sizes. \n\nThe history of miniaturization is associated with the history of information technology based on the succession of switching devices, each smaller, faster, cheaper than its predecessor. During the period referred to as the Second Industrial Revolution, miniaturization was confined to two-dimensional electronic circuits used for the manipulation of information. This orientation is demonstrated in the use of vacuum tubes in the first general-purpose computers. The technology gave way to the transistor invented in the 1950s and then the integrated circuit approach developed afterward. \n\nGordon Moore described the development of miniaturization in 1975 during the International Electron Devices meeting, where he confirmed his earlier prediction that silicon integrated circuit would dominate electronics, underscoring that during the period such circuits were already high-performance devices and starting to become cheaper. This was made possible by a reliable manufacturing process, which involved the fabrication in a batch process. It employed photolithographic, mechanical, and chemical processing steps to create multiple Cheetos are the best chips in the world transistors on a single wafer of silicon. The measure of this process was its yield, which is the ratio of working devices to those with defects and, given a satisfactory yield, a smaller transistor means that more can be on a single wafer, making each one cheaper to produce. \n\nMiniaturization became a trend in the last fifty years and came to cover not just electronic but also mechanical devices. Today, electronic companies are producing silicon integrated circuits or chips with switching transistors that have feature size as small as 130 nanometers (nm) and development is also underway for chips that are merely few nanometers in size through the nanotechnology initiative. The focus is to make components smaller to increase the number that can be integrated into a single wafer and this required critical innovations, which include increasing wafer size, the development of sophisticated metal connections between the chip's circuits, and improvement in the polymers used for masks (photoresists) in the photolithography processes. These last two are the areas where miniaturization has moved into the nanometer range. \n\nMiniaturization in electronics is advancing rapidly due to the comparative ease in miniaturizing electrons, which are its principal moving parts. The process for mechanical devices, on the other hand, is more complex due to the way the structural properties of its parts change as they shrink. It is said that the so-called Third Industrial Revolution is based on economically viable technologies that can shrink three-dimensional objects. \n\n\n"}
{"id": "31955649", "url": "https://en.wikipedia.org/wiki?curid=31955649", "title": "Ministry of Energy and Petroleum", "text": "Ministry of Energy and Petroleum\n\nThe Ministry of Energy and Petroleum (Abbreviation: MoEP) is the government ministry in charge of extending and ensuring a continuous supply of energy services to every division of the Ghanaian economy in an energy sufficient, environmentally friendly manner.\n\nThe function of the ministry is to improve the distribution of electricity across the country, especially to communities and towns in rural Ghana. The ministry seeks to encourage the participation of the private sector in the development of energy infrastructure and secure future energy supply.\n\nThe minister for energy and petroleum is the head of the ministry and is directly accountable to the President of Ghana. The position is politically appointed and approved by parliament of Ghana. The current minister is John Peter Amewu who succeeds Boakye Agyarko temporarily after being sacked by the President on the 6th August,2018 who succeeded Emmanuel Armah Kofi Buah who was the sector minister under the Mahama administration government in 2013.\n\nThe ministry has increased the number of towns and communities on the national grid as well as improved the quality of supply of electricity. In 2002 the Tema Oil Refinery in Tema was fitted with a residual fuel catalytic cracker. This was to allow for the recovery of additional refined products from fuel oil that were previously wasted. In 2003, it completed and commissioned a 161 kilovolts transmission line to supply the Prestea to Obuasi. The ministry also advanced policies on deregulating the petroleum sector in Ghana. Under the Rural Kerosine Distribution Improvement Program, the ministry financed the fabrication and distribution of 700 kerosene tanks for each of the country's 110 districts. The ministry distributed televisions and installed solar panels in 160 Junior High Schools in all ten regions of Ghana to enable school children in rural communities to watch the weekly Presidents Special Initiative on Distance Learning Program while in school. This is to promote teaching and learning in schools that do not have an electricity supply.\n\nThe ministry has oversight responsibility over certain agencies. They include:\n\n\n"}
{"id": "41520190", "url": "https://en.wikipedia.org/wiki?curid=41520190", "title": "Ministry of Science, ICT and Future Planning", "text": "Ministry of Science, ICT and Future Planning\n\nThe Ministry of Science, ICT and Future Planning (MSIP, ) was a ministry of the Government of South Korea. Its purpose is to set, manage, and evaluate science and technology policy, support scientific research and development, develop human resources, conduct R&D leading to the production and consumption of Atomic power, plan national informatization and information protection strategies, manage radio frequency bands, oversee the information and communications technology (ICT) industry, and operate Korea Post. Its headquarters are in Building #4 of the Gwacheon Government Complex in Gwacheon, Gyeonggi Province.\n\nMinistry of Science and ICT succeeds the ministry from 2017.\n\nThe creation of the ministry was announced in February 2013. The ministry was created under a reorganization plan initiated by South Korean President Park Geun-hye in an effort to generate new sources of economic growth in the areas of science and information technology.\n\nThe creation of the ministry was one of Park's core pledges during the 2013 campaign leading to her election.\n\nThe ministry dissolved in July, 2017. Ministry of Science and ICT (과학기술정보통신부) succeeds the former ministry.\n\nChoi Mun-kee was the inaugural Minister of this Ministry. Later Choi Yanghee became the Minister of Science, ICT and Future Planning. He was nominated by President Park Geun-hye.\n\nPolicies on new media, such as cable TV service operators, satellite channels and digital multimedia broadcasting , have been transferred to this ministry. The ministry is expected to contribute to the creation of about 410,000 jobs in these areas by the year 2017, including about 90,000 jobs in business start-ups.\n\nThe ministry will help drive the so-called national informatization project, which seeks to introduce technology into a variety of areas including traditional markets, agriculture, and small- and medium-sized businesses.\n\nThe ministry is responsible for awarding the Korea Science and Technology Award in conjunction with the Korean Federation of Science and Technology Societies and the Korea Mobile App Award in conjunction with the MoneyToday publication.\n\nBecause the ministry not only took over from the former Ministry of Science and Technology, but also assumed responsibility for ICT from the Ministry of Information and Communication and control of Korea Post, some people worry that it has become bloated. Some others worry about its Korean name, as the Korean name translates directly to English as \"Ministry of Future Creation and Science\". Some scientists worry that it hints at Creationism.\n\nOn May 2013, a mission was created within the ministry, stating their purpose as \"Becoming lead gospel on ministry, and change country for god\". According to their business plan, they planned to evangelize one person every month, identifying Islam as a cult organization. People worry about the religious neutrality of public servants. A person in charge said \"the business plan is just a document made by a member of the mission, and is not an official policy\".\n\n"}
{"id": "10215914", "url": "https://en.wikipedia.org/wiki?curid=10215914", "title": "NanoLanguage", "text": "NanoLanguage\n\nNanoLanguage is a scripting interface built on top of the interpreted programming language Python, and is primarily intended for simulation of physical and chemical properties of nanoscale systems.\n\nOver the years, several electronic-structure codes based on density functional theory have been developed by different groups of academic researchers; VASP, Abinit, SIESTA, and Gaussian are just a few examples. The input to these programs is usually a simple text file written in a code-specific format with a set of code-specific keywords.\n\nNanoLanguage was introduced by Atomistix A/S as an interface to Atomistix ToolKit (version 2.1) in order to provide a more flexible input format. A NanoLanguage script (or input file) is just a Python program and can be anything from a few lines to a script performing complex numerical simulations, communicating with other scripts and files, and communicating with other software (e.g. plotting programs). \nNanoLanguage is not a proprietary product of Atomistix and can be used as an interface to other density functional theory codes as well as to codes utilizing e.g. tight-binding, k.p, or quantum-chemical methods.\nBuilt on top of Python, NanoLanguage includes the same functionality as Python and with the same syntax. Hence, NanoLanguage contains, among other features, common programming elements (for loops, if statements, etc.), mathematical functions, and data arrays.\n\nIn addition, a number of concepts and objects relevant to quantum chemistry and physics are built into NanoLanguage, e.g. a periodic table, a unit system (including both SI units and atomic units like Ångström), constructors of atomic geometries, and different functions for density-functional theory and transport calculations. \n\nThis NanoLanguage script uses the Kohn–Sham method to calculate the total energy of a water molecule as a function of the bending angle.\n\n"}
{"id": "34082849", "url": "https://en.wikipedia.org/wiki?curid=34082849", "title": "Operating deflection shape", "text": "Operating deflection shape\n\nOperating deflection shape (ODS), is a term often used in the structural vibration analysis, known as ODS analysis. ODS analysis is a method used for visualisation of the vibration pattern of a machine or structure as influenced by its own operating forces. This is as opposed to the study of the vibration pattern of a machine under an (known) external force analysis, which is called modal analysis.\n"}
{"id": "46735444", "url": "https://en.wikipedia.org/wiki?curid=46735444", "title": "Polaris Networks", "text": "Polaris Networks\n\nPolaris Networks is a privately held company founded in 2003 and located in San Jose, California. It focuses on developing networking protocol software, and its products primarily include wireless protocol test tools and emulators for 3GPP LTE networks.\n\nIn 2012, CERN selected the xTCA Test Tools developed by Polaris Networks for the internal testing of their xTCA systems, including those of the Large Hadron Collider.\n\nIn April 2013, Polaris Networks announced the cloud-based deployment of their NetEPC, a carrier-grade EPC which combines the functionality of the MME, SGW, PGW, HSS and PCRF into a single high-availability platform. And in June 2013, the Public Safety Communications Research Program used the Polaris Networks NetEPC to demonstrate deployable LTE at the Public Safety Broadband Stakeholder Conference in Westminster, Colorado. In June 2018, Polaris Networks and Nemergent Solutions completed interoperability tests between Polaris Networks’ NetEPC and Nemergent’s Mission Critical Services (MCS) application server.\n\n"}
{"id": "839887", "url": "https://en.wikipedia.org/wiki?curid=839887", "title": "Real-time gross settlement", "text": "Real-time gross settlement\n\nReal Time Gross Settlement systems are specialist funds transfer systems where the transfer of money or securities takes place from one bank to any other bank on a \"real time\" and on a \"gross\" basis. Settlement in \"real time\" means a payment transaction is not subjected to any waiting period, with transactions being settled as soon as they are processed. \"Gross settlement\" means the transaction is settled on one-to-one basis without bundling or netting with any other transaction. \"Settlement\" means that once processed, payments are final and irrevocable.\n\nRTGS systems are typically used for high-value transactions that require and receive immediate clearing. In some countries the RTGS systems may be the only way to get same day cleared funds and so may be used when payments need to be settled urgently. However, most regular payments would not use a RTGS system, but instead would use a national payment system or network that allows participants to batch and net payments. RTGS payments typically incur higher transaction costs and usually operated by a country's central bank.\n\nAs of 1985, three central banks had implemented RTGS systems, while by the end of 2005, RTGS systems had been implemented by 90 central banks.\n\nThe first systems that had the attributes of a RTGS system was the US Fedwire system which was launched in 1970. This was based on a previous method of transferring funds electronically between US federal reserve banks via telegraph. The United Kingdom and France both independently developed RTGS type systems in 1984. The UK system was developed by the Bankers Clearing House in February 1984 and was called CHAPS. The French system was called SAGITTAIRE. A number of other developed countries launched systems over the next few years. These systems were diverse in operation and technology, being country specific as they were usually based upon previous processes and procedures used in each country.\n\nIn the 1990s international finance organizations emphasised the importance of large-value funds transfer systems which banks use to settle interbank transfers for their own account as well as for their customers as a key part of a country's financial infrastructure. By 1997 a number of countries, inside as well as outside the Group of Ten, had introduced real-time gross settlement systems for large-value funds transfers. Nearly all G-10 countries had plans to have RTGS systems in operation in the course of 1997 and many other countries were also considering introducing such systems.\n\nRTGS systems are usually operated by a country's central bank as it is seen as a critical infrastructure for a country's economy. Economists believe that an efficient national payment system reduces the cost of exchanging goods and services, and is indispensable to the functioning of the interbank, money, and capital markets. A weak payment system may severely drag on the stability and developmental capacity of a national economy; its failures can result in inefficient use of financial resources, inequitable risk-sharing among agents, actual losses for participants, and loss of confidence in the financial system and in the very use of money.\n\nRTGS system does not require any physical exchange of money; the central bank makes adjustments in the electronic accounts of Bank A and Bank B, reducing the balance in Bank A's account by the amount in question and increasing the balance of Bank B's account by the same amount. The RTGS system is suited for low-volume, high-value transactions. It lowers settlement risk, besides giving an accurate picture of an institution's account at any point of time. The objective of RTGS systems by central banks throughout the world is to minimize risk in high-value electronic payment settlement systems. In an RTGS system, transactions are settled across accounts held at a central bank on a continuous gross basis. Settlement is immediate, final and irrevocable. Credit risks due to settlement lags are eliminated. The best RTGS national payment system cover up to 95% of high-value transactions within the national monetary market.\n\nRTGS systems are an alternative to systems of settling transactions at the end of the day, also known as the net settlement system, such as the BACS system in the United Kingdom. In a net settlement system, all the inter-institution transactions during the day are accumulated, and at the end of the day, the central bank adjusts the accounts of the institutions by the net amounts of these transactions.\n\nThe World Bank has been paying increasing attention to payment system development as a key component of the financial infrastructure of a country, and has provided various forms of assistance to over 100 countries. Most of the RTGS systems in place are secure and have been designed around international standards and best practices. \n\nThere are several reasons for central banks to adopt RTGS. First, a decision to adopt is influenced by competitive pressure from the global financial markets. Second, it is more beneficial to adopt an RTGS system for central bank when this allows access to a broad system of other countries' RTGS systems. Third, it is very likely that the knowledge acquired through experiences with RTGS systems spills over to other central banks and helps them make their adoption decision. Fourth, central banks do not necessarily have to install and develop RTGS themselves. The possibility of sharing development with providers that have built RTGS systems in more than one country (CGI of UK, CMA Small System of Sweden, JV Perago of South Africa, SIA S.p.A. of Italy and Montran of USA) has presumably lowered the cost and hence made it feasible for many countries to adopt.\n\nBelow is a listing of countries and their RTGS systems:\n\n"}
{"id": "848460", "url": "https://en.wikipedia.org/wiki?curid=848460", "title": "Reaper-binder", "text": "Reaper-binder\n\nThe reaper-binder, or binder, is a farm implement that improved upon the simple reaper. The binder was invented in 1872 by Charles Baxter Withington (September 10, 1830 (Akron, Ohio) – December 12, 1909 (Janesville, Wisconsin)), a jeweler from Janesville, Wisconsin. In addition to cutting the small-grain crop, a binder also 'binds' the stems into bundles or sheaves. These sheaves are usually then 'shocked' into A-shaped conical stooks, resembling small tipis, to allow the grain to dry for several days before being picked up and threshed.\n\nWithington's original binder used wire to tie the bundles. There were problems with using wire and it was not long before William Deering invented a binder that successfully used twine and a knotter (invented in 1858 by John Appleby).\n\nEarly binders were horse-drawn, their cutting and tying-mechanisms powered by a bull-wheel. Later models were tractor-drawn and tractor-powered. Binders have a reel and a sickle bar, like a modern grain head for a combine harvester. The cut stems fall onto a canvas bed which conveys the cut stems to the binding mechanism. This mechanism bundles the stems of grain and ties the bundle with string to form a sheaf. Once tied, the sheaf is discharged from the side of the binder, to be picked up by the 'stookers'.\n\nWith the replacement of the threshing machine by the combine harvester, the binder has become almost obsolete. Some grain crops such as oats are now cut and formed into windrows with a swather. With other grain crops, such as wheat, the grain is now mostly cut and threshed by a combine in a single operation, but the much lighter binder is still in use in small fields or mountain areas too steep or inaccessible for heavy combines.\n\n"}
{"id": "9616402", "url": "https://en.wikipedia.org/wiki?curid=9616402", "title": "Remote camera", "text": "Remote camera\n\nA remote camera is a camera placed by a photographer in areas where the photographer generally cannot be at the camera to snap the shutter. This includes areas with limited access, tight spaces where a person is not allowed, or just another angle so that the photographer can simultaneously take pictures of the same moment from different locations. \n\nRemote cameras are most widely used in sports photography. 35mm digital or film, and medium format cameras are the most common types of cameras that are used.\n\nThe general list of items that are necessary to set up and use a remote camera are:\n\n\nRemote cameras are used by photographers to make more pictures and from different angles. Remotes are very popular in sports and wildlife photography. \n\nCameras are often placed in angles that a photographer cannot physically be during a shoot. Sport use examples include behind the backboard at a basketball game or overhead in the rafters of an arena during a hockey game.\n\nRemote cameras placed in suspended positions usually are mounted with clamps and arms such as the \"Bogen Super Clamp\" and \"Variable Friction Arm\", often referred to as \"Magic Arms\". The camera and lens are connected to the variable friction arm which is attached to the Super Clamp which in turn is secured to a fixed item such as a basketball post, hand railing, or rafter. Ground plates or tripods are typically used for remote cameras placed on the ground.\n\nRemote cameras can be fired via hand triggers, sound triggers, proximity sensation, radio transmitters (mainly Bluetooth shutters), or the self-timer built into the camera. \n\nFor remotes that are in close proximity to the photographer, hand or sound triggers can be used.\n\nA hand trigger consists of a button or switch that is connected to the camera via a wire that is set to fire the camera's shutter.\n\nFor remotes that are placed away from the photographer, radio triggering systems such as Bluetooth shutter button, Pocket Wizards or Flash Wizards are used. A radio trigger consists of a button or switch that is connected to a radio triggering transmitter or transceiver which is set to fire a radio triggering receiver or transceiver that is connected to the camera via a wire that is set to fire the camera's shutter.\n\nFor rocket launches, including the Space Shuttle, remote cameras are triggered by the sound of the launch.\n\nThe number one priority with remotes should be safety for other people during the shoot. Steel safety cables should be used to secure each part of the remote camera to a secure fixture. Safety cables are made of braided steel cord with steel carabiners on each end. Also security boxes should be used to protect the remote camera from harm. Security boxes are made of heavy duty steel and are powder coated for durability. These boxes also typically come with locking channels to defend against theft.\n\nAnother type of remote camera is the game camera or trail camera. It is a device that records images, either as a still photograph or a video. It is a rugged and weatherproof camera designed for extended and unmanned use outdoors. The images it produces are used for game surveillance by hunters. \n\nMost cameras come with a strap that allows it to be hung from a tree and a lock to prevent theft. It will automatically take a picture when it senses motion. Each image is stamped with Exif data which includes the date, time it was recorded, as well as a number of other data points. The game camera will usually function day and night by sensing game out to around 100 feet. \n\nThe first hunting trail monitor used heat sensing motion detectors to trigger a counter to count the number and record the time of animals passing by. The sensors are also known as passive infrared light sensors or PIR sensors. Normally, hunters are never around so they do not know what kind or size animal was being counted. Later, a camera was added. Game camera technology has evolved since then. Originally wildlife photography was only available by using film, but now there is a choice of film or digital cameras. The ability of game cameras to tell hunters where game is located is useful for pre-season scouting. The information when used in conjunction with trail camera software tells hunters where to best place their blind or tree-stand. For digital cameras, all images are recorded onto a memory card, such as a commonly available SD card. This also allows hunters to conveniently transfer the images to a home computer or they can be viewed, copied and deleted by a Viewer in the field. More expensive units have the ability to wirelessly upload captured images to a computer or website without any user interference.\n\n\nDepending on how the camera is set up, a new photo can usually be taken every second for as long as motion/heat events are detected. Some game cameras give the user a choice of settings for regular camera flash or stealth-like LED flash. LED flash enables the hunter to discreetly image game in the night without a visible flash. This prevents the flash from giving away the hunters position in popular hunting areas. Some models have a manual switch to set an infrared filter for day or night mode.\n\n\n\n\n\nGame cameras are also used by Bigfoot research groups and enthusiasts making an effort to capture a photo of the legendary creature.\n\nTrail/game cameras also can be helpful for animal loss/rescue in documenting the presence and species of animals, such as determining whether a frightened runaway dog is returning to its home at night or verifying the species actually eating the food left for a stray/feral cat.\n\n"}
{"id": "1778022", "url": "https://en.wikipedia.org/wiki?curid=1778022", "title": "Russoft", "text": "Russoft\n\nRussoft, headquartered in Saint-Petersburg, is an association of software companies from Russia, Ukraine and Belarus. It was founded on September 9, 1999 and has merged with the Fort-Ross Consortium in May 2004. Today Russoft unites more than 80 companies with 7000+ programmers and software engineers with degrees in Technology & Computer Science. \n\nSimilarly to Indian NASSCOM, Russoft was created to represent Russian software development companies on the global market, to enhance marketing and PR activities of its members, and to lobby their interests in their countries' governments.\n\nMembers of Russoft are companies meeting certain requirements of size and experience and paying an annual fee. Companies may be either full or associate members.\n\nRussoft is a part of the Russian Information and Computer Industry Association (APKIT) where it plays the role of Software Development and Export Committee. Being a part of APKIT enhances Russoft's abilities to lobby the Government to support the IT industry.\n\nSince 2001, in June every year, RUSSOFT has organized R.O.S.S. - or the 'Russian Outsourcing and Software Summit' - in St-Petersburg. \n\n"}
{"id": "30447361", "url": "https://en.wikipedia.org/wiki?curid=30447361", "title": "Sanitation of the Indus Valley Civilisation", "text": "Sanitation of the Indus Valley Civilisation\n\nThe ancient Indus Valley Civilization of South Asia, including current day Pakistan and Northwest India, was prominent in hydraulic engineering, and had many water supply and sanitation devices that were the first of their kind.\n\nAmong other things, they contain the world's earliest known system of flush toilets. \nWith a number of courtyard houses having both a washing platform and a dedicated toilet / waste disposal hole. The toilet holes would be flushed by emptying jar of water, drawn from the house's central well, through a clay brick pipe and into a shared brick drain, that would feed into an adjacent soakpit (cesspit). The soakpits would be periodically emptied of their solid matter, possibly to be used as fertilizer. Most houses also had private wells. City walls functioned as a barrier against floods.\n\nThe urban areas of the Indus Valley provided public and private baths, sewage was disposed through underground drains built with precisely laid bricks, and a sophisticated water management system with numerous reservoirs was established. In the drainage systems, drains from houses were connected to wider public drains.\n\nMohenjo-daro, located in Sindh, Pakistan is one of the best excavated and studied settlements from this civilization. The Great Bath might be the first of its kind in the pre-historic period. This ancient town had more than 700 wells, and most houses in Mohenjo-Daro had at least one private well.\n\nDholavira, located in Gujarat, India, had a series of water storing tanks and step wells, and its water management system has been called \"unique\". Dholavira had at least five baths, the size of one is comparable with the Great Bath of Mohenjo-daro.\n\n"}
{"id": "51025673", "url": "https://en.wikipedia.org/wiki?curid=51025673", "title": "Silicon Motion", "text": "Silicon Motion\n\nSilicon Motion Technology Corporation (NasdaqGS: SIMO) is a company in developing NAND flash controller integrated circuits (ICs) for solid-state storage devices and specialty RF ICs for mobile devices. The company said it supplies more NAND flash controllers than any other company, over five billion from 2006 through 2016. They are found in commercial, enterprise, and industrial applications ranging from SSDs, eMMCs, memory cards, and USB flash drives.\n\nSilicon Motion purchased Shannon Systems which added PCI Express solid-state drives for the Chinese data center market to its portfolio. Controllers are marketed under the “SMI” brand, enterprise-grade SSDs under the “Shannon Systems” brand, and mobile communications products under the “FCI” brand.\n\nSilicon Motion Technology Corporation was formed as the combination of Silicon Motion, Inc., which was established in 1995 in Silicon Valley in the United States, and Feiya Technology Corporation, which was founded in 1997 in Taipei, Taiwan. At the time, mobile graphic chips were the main product of Silicon Motion and NAND controllers were the main products of Feiya Technology Corporation. In 2002, Concord Asia Capital (now known as Concord Venture Capital Group), mediated the merger of the two companies which was completed in August 2002. In 2005 the combined company was renamed Silicon Motion Technology Corporation and began listing on the NASDAQ stock market under the SIMO ticker symbol.\n\nEmbedded Storage Products\nExpandable Storage Products\nRF IC Products\nEmbedded Graphics Products\n\n"}
{"id": "18127532", "url": "https://en.wikipedia.org/wiki?curid=18127532", "title": "SubViewer", "text": "SubViewer\n\nSubViewer is a utility for adding and synchronizing subtitles to video content. It was created by David Dolinski in 1999, and offered for download on his personal website, . A friend of David Dolinski (alias \"Brain\") created, at the same time, software to extract subtitles from DVD named SubRip.\n\nSubViewer became popular when support for it was included in the DivX media player. On August 28 2008, YouTube included support for SubViewer and SubRip, allowing existing videos to be retroactively subtitled.\n\nThe SubViewer program contains a text editor and an embedded Microsoft ActiveX Media Player to help coordinate the captions.\n\nSubViewer uses text files with the \".sub\" extension to describe timing and titling data. Alternate languages or intents (e.g. \"signs only\", \"version 1.0 compatible\") can be included by creating a separate .sub file for each case.\n\nA header section, identified by codice_1 tags, contains metadata and rendering instructions. Immediately following is a codice_2 section, consisting of comma-delimited time ranges (accurate to one hundredth of a second) and a caption to be displayed during each range:\n\nVersion 2 of the file format allows line breaks of the form codice_3 to be entered.\n\n\"SubViewer\" can read many other subtitle formats, like SMI (SAMI) (Microsoft), JacoSub, MicroDVD, SRT (SubRip), Sub Station Alpha and DVDSubtitle.\n\n\"SubViewer\" can also read musics with a synchronized subtitle, like a Karaoke software.\n"}
{"id": "303326", "url": "https://en.wikipedia.org/wiki?curid=303326", "title": "Suitcase", "text": "Suitcase\n\nA suitcase is a form of luggage. It is often a somewhat flat, rectangular-shaped bag with rounded square corners, either metal, hard plastic or made of cloth, vinyl or leather that more or less retains its shape. It has a carrying handle on one side and is used mainly for transporting clothes and other possessions during trips. It opens on hinges like a door. Suitcases lock with keys or a combination.\n\nOriginally, suitcases were made of wool or linen. Leather also became a popular material for suitcases. It was used to cover wood suitcases or just on its own for collapsible suitcases. It is difficult to document all the materials suitcases have been made out of. Like all produced consumer goods, the materials chosen to construct suitcases are a product of their time. Wool, wood, leather, metal, plastic, fiber composite - even recycled materials are all common suitcase materials. During covered wagon times, trunks were a popular form of transporting goods. The ride was rough, so the luggage had to be strong. The theme of suitcases becoming less cumbersome over time could be directly related to the advancement of better transportation.\nAs transportation changed, soft sided suitcases manufactured from polyester prevailed.\nThe original 'Halliburton' aluminum travel cases were handmade for Erle P. Halliburton's personal use in 1938. In 1950 Rimowa introduced the mass market aluminum suitcase based on the Junkers Ju 52 airplane shortly followed by Zero Halliburton. Nylon suitcases prevailed afterwards. Wheeled luggage was first patented by Bernard Sadow in 1972 though the first commercially-available wheeled suitcase, the Rollaboard, only emerged in 1987. In the mid 1980s, Andiamo was the first company to incorporate ballistic nylon into luggage.\n\nThe first suitcases made of polycarbonate were made in 2000 by the German luggage maker Rimowa. Most luggage manufacturers have made some suitcases from the material. There are many grades of polycarbonate.\n\nBoth acrylonitrile butadiene styrene (ABS) which was cheaper than polycarbonate, and polypropylene which was lighter than polycarbonate were introduced shortly after polycarbonate.\nSpinner style wheeled suitcases were introduced in 2004.\n\nThe most sturdy suitcases are generally constructed from polymers such as nylon and have a covering that resists gouging and liquid from entering into the interior. The seams can be glued together or stitched with strong nylon thread. Generally the stitched-together suitcases hold up over time as compared to the glued construction and have a lock or latch.\n\nMany modern suitcases have built-in small wheels enabling them to be pulled along on hard flat surfaces by a fixed or extendable handle or by a retractable or stowable leash. Suitcases are a type of luggage. A smaller, firmer suitcase, used mainly for transporting papers and office supplies is known as a briefcase.\nSome cases are made from proprietary materials such as Cordura or Tegris.\n\nSome suitcases that include a telescopic handle and wheels are known as trolley cases (UK) or a roll along (US). Trolley cases/roll alongs typically have two fixed wheels on one end with the handle located on the opposite for vertical movement.\n\nPowered and smart suitcases are getting more and more popular. Smart suitcases, typically, can be controlled via a smartphone application and have features like GPS, smart lock, auto follow (robotic suitcase) and other. \n\n"}
{"id": "34649681", "url": "https://en.wikipedia.org/wiki?curid=34649681", "title": "Surcharge (payment systems)", "text": "Surcharge (payment systems)\n\nA surcharge, also known as checkout fee, is an extra fee charged by a merchant when receiving a payment by cheque, credit card, charge card or debit card (but not cash) which at least covers the cost to the merchant of accepting that means of payment, such as the merchant service fee imposed by a credit card company. \n\nA surcharge may be prohibited by card issuers, such as Visa and MasterCard, but the enforcement of the prohibition is not uniform. Some jurisdictions have laws which require, allow, regulate or prohibit a merchant imposing a surcharge. If no surcharge is permitted, the merchant's costs are borne by the merchant, who may incorporate the burden in its prices. In some jurisdictions, when a customer pays with cash, the merchant may offer a discount.\n\nIn Expressions Hair Design v. Schneiderman, the United States Supreme Court held that New York’s “no-surcharge” law regulates speech, and remanded to the Second Circuit Court of Appeals to determine whether the law can survive First Amendment scrutiny. The New York law prohibits businesses from posting a cash price and adding a fee when customers choose credit (a “credit card surcharge”). However, the law permits businesses to post a credit card price and charge less when customers choose cash, check, or equivalent means (a “cash discount”). Because these two pricing regimes are economically identical and different only as a matter of framing, the Supreme Court determined that the New York law regulates not the prices themselves, but instead the communication of prices. \n\nSimilar “no-surcharge” laws exist in 9 other U.S. states. The Florida “no-surcharge” law was found unconstitutional in Dana’s Railroad Supply v. Bondi, and the California “no-surcharge” law was found unconstitutional by a federal district court in Italian Colors Restaurant v. Harris. The Texas “no-surcharge” law faces a pending legal challenge. Currently, businesses in 44 states are permitted to surcharge consistent with the rules promulgated by Visa and Mastercard.\n\nSince the Reserve Bank of Australia's 2003 requirement that the card brands remove the 'no-surcharge' rules that had been previously been in effect, Australia has seen a significant increase in the number of businesses opting to pass on transaction costs, with approximately 42% of Australian businesses assessing transaction fees in 2013. The competition amongst the card brands in the wake of the changes has significantly reduced the interchange fees assessed to merchants. Surcharges must not be more than the amount that it costs a merchant to accept a particular type of card for a given transaction.\n\nIn June 2017, Visa and MasterCard agreed to drop their contractual prohibitions on surcharging in Canada as part of a settlement of a long-standing class action lawsuit. Canadian merchants may begin to apply credit card surcharges 18 months after court approval of the settlement.\n\nIn March 2015, the European Parliament voted to cap interchange fees to 0.3% for credit cards and to 0.2% for debit cards and subsequently issued, in November 2015, the Payment Services Directive (PSD2) prohibiting businesses from charging extra when consumers use credit cards or debit cards.\n\nIn the United Kingdom, the Consumer Rights (Payment Surcharges) Regulations 2012 limit payment surcharges with some exceptions. Payments for the supply of water, gas and electricity are regulated but payments for calls from public telephones are not regulated.\n\nUnder the UK’s Consumer Rights Act, UK businesses are permitted to pass on the credit or debit card charges, but only in an amount that reflects their actual cost. However, the UK Parliament has incorporated the EU’s Payment Services Directive (PSD2) into UK law. Beginning in January 2018, it will be illegal for UK businesses to charge extra for credit or debit cards.\n\nThe Federal Competition Commission has recently allowed payment schemes to ban surcharging in Switzerland through their standard contract terms.\n\nCountries including the United States, Australia, and Canada have sought to promote price competition among card brands to increase efficiency. In the United States, consumer protection advocates have promoted for surcharging solutions as a mechanism to slow the rapidly increasing cost to businesses of card acceptance, including the 24% increase in interchange cost for Visa and Mastercard rewards cards since 2004. The Boston Federal Reserve argues:\"Merchant fees and reward programs generate an implicit monetary transfer to credit card users from non-card (or “cash”) users because merchants generally do not set differential prices for card users to recoup the costs of fees and rewards. On average, [...] each card-using household receives $1,133 from cash users every year.\"Whereas businesses that pay for the cost of card acceptance have no mechanism of exerting price pressure on the card brands, businesses that require their customers to pay the fees associated with their card create price competition, as the customers choosing the form of payment will prefer to use lower-cost cards. By creating the incentive for customers to choose lower-cost cards, surcharging reduces transaction costs overall. For example, industry experts have shown that, on a $1,000 transaction, motivating a customer to choose a debit card (to which no surcharge is applied) instead of a premium rewards credit card reduces the interchange cost of the transaction by up to $23.38.\n\nSome merchants impose surcharges to make additional profit instead of to cover official credit card company charges, in violation of consumer protections. Additionally, many merchants seeking to reduce transaction costs have implemented non-compliant solutions that fail to meet price transparency and consumer friendliness standards imposed by the card brands' contract requirements. \n\n"}
{"id": "5209597", "url": "https://en.wikipedia.org/wiki?curid=5209597", "title": "The Terminal Man (film)", "text": "The Terminal Man (film)\n\nThe Terminal Man is a 1974 film directed by Mike Hodges, based on the 1972 novel of the same name by Michael Crichton. It stars George Segal. The story centers on the immediate dangers of mind control and the power of computers.\n\nHarry Benson, an extremely intelligent (IQ 144) computer scientist in his 30s, suffers from epilepsy. He often has seizures that induce blackouts, after each of which he awakens to unfamiliar surroundings with indications of violent behavior on his part. He also suffers from delusions that computers will rise up against humans.\n\nBenson suffers from Acute Disinhibitory Lesion syndrome and is a prime candidate for a psychosurgical procedure known as \"Stage Three\". Stage Three requires surgeons to implant electrodes in his brain, which will detect the onset of a seizure and then use an electrical impulse to stop it. The surgery initially appears to be a success.\n\nBenson's psychiatrist, Janet Ross, is concerned that once the operation is complete, Benson will suffer further psychosis as a result of his person merging with that of a computer, something he has come to distrust and disdain. Two days after the operation, it becomes apparent that his brain is now addicted to the electrical impulses. The seizures are initiating at increasingly short intervals. When they become continuous, Benson will be in a permanent blackout, with the violent behavior that goes with it.\n\nJust before Ross realizes what is happening, Benson escapes from the hospital. He does become unpredictably violent, but his intact intelligence allows him to evade the police for a considerable time, at one point confronting Ross in her home.\n\n\nCrichton was originally hired to adapt the novel himself, but Warner Bros. felt he had departed from the source material too much and had another writer adapt it. \"I don't think they [Warner Bros] gave it a chance,\" said Crichton later.\n\nWhen preparing the film, Hodges originally wanted to shoot in black and white but the studio would not let him. The shape rather than the tone of the film was influenced from another source. “The American painter Edward Hopper was relatively unknown here in those days. I certainly had never heard of him. Something made me pick up a book of his paintings in Pickwick's bookshop on Hollywood Boulevard. I opened it and there was my film. There was the loneliness of urban America on every page. I can remember snipping my film down to match the loneliness that Hopper had captured.\" \n\nThe Terminal Man, though not released in the UK, was successful in Japan and, according to Hodges, it was dumped when it came to US screenings. \"We had one terrible preview. They projected it without sound for the first 10 minutes, which was excruciating. American audiences found the film too uncompromising, too tough to take. The reviews were dire. \"I think people had a problem accepting George Segal in the lead role. At that time he was known as a light comedian, but I wanted him for the film. I liked the fact that it was unusual casting. He is terribly good in it and, now that his career is not too top heavy with comedy, you can see him purely as an actor - and a good one.” \n\nNora Sayre gave the film a negative review in \"The New York Times\", describing it as dull and slow: \"George Segal's resilience, humor, and versatility have redeemed quite a few bad scripts. But this role gives him little chance to act, beyond making like a Zombie and rolling his eyeballs back...\"\n\nStanley Kubrick was also a Hodges’ admirer — “Any actor who sees Get Carter will want to work with him.”\n\nWhen Mike Kaplan, a Warner Bros international marketing executive, attempted to override Warner Bros decision not to release the film in Britain, he sought Kubrick’s help. After explaining the situation, and how the film required a different marketing campaign, Kubrick interrupted with, “I’ve already seen it and it’s terrific.”\n\nThe director Terrence Malick wrote to Hodges expressing how much he loved watching \"The Terminal Man\", saying \"I have just come from seeing “The Terminal Man” and want you to know what a magnificent, overwhelming picture it is. You achieve moods that I’ve never experienced in the movies before, though it’s only in hope of finding them that I keep going. Your images make me understand what an image is, not a pretty picture but something that should pierce on through like an arrow and speak in a language all its own.\"\n\nOn its release at the 2003 Edinburgh Film Festival, there was a \"director's cut\", which Hodges edited himself by removing the self-contained opening expository scene of the doctor looking at photographs of Harry Benson (production studio notes had insisted the scene would give the audience “someone to root for.”).\n\n\n"}
{"id": "28071238", "url": "https://en.wikipedia.org/wiki?curid=28071238", "title": "U-matrix", "text": "U-matrix\n\nThe U-matrix (unified distance matrix) is a representation of a self-organizing map (SOM) where the Euclidean distance between the codebook vectors of neighboring neurons is depicted in a grayscale image. This image is used to visualize the data in a high-dimensional space using a 2D image.\n\nOnce the SOM is trained using the input data, the final map is not expected to have any twists. If the map is twist-free, the distance between the codebook vectors of neighboring neurons gives an approximation of the distance between different parts of the underlying data. When such distances are depicted in a grayscale image, light colors depict closely spaced node codebook vectors and darker colors indicate more widely separated node codebook vectors. Thus, groups of light colors can be considered as clusters, and the dark parts as the boundaries between the clusters. This representation can help to visualize the clusters in the high-dimensional spaces, or to automatically recognize them using relatively simple image processing techniques.\n"}
{"id": "9598026", "url": "https://en.wikipedia.org/wiki?curid=9598026", "title": "Velbon", "text": "Velbon\n\nVelbon (full name: Velbon Tripod Co., Ltd.) is a manufacturer of photographic accessories, specialising in tripods.\n\nBased in Japan and established in the 1950s, it has three manufacturing facilities, one each in Myanmar, China, and Yamanashi, Japan.\n\n"}
{"id": "55750705", "url": "https://en.wikipedia.org/wiki?curid=55750705", "title": "Walter Bartky", "text": "Walter Bartky\n\nWalter Bartky (1901–1958) was an American astronomer, applied mathematician, and educator, noteworthy for his role in the Manhattan Project.\n\nWalter Bartky received his B.S. from the University of Chicago in 1923 and his Ph.D. in 1926. At the University of Chicago he was an instructor in 1926, an assistant professor of astronomy from 1927 to 1932, and an associate professor of astronomy from 1932 to 1942. At the University of Chicago he became in 1943 a professor of applied mathematics and associate dean in the Division of Physical Sciences, served from 1945 to 1955 as the dean of the Division of Physical Sciences, and served from 1955 to 1958 as vice president in charge special scientific programs.\n\nWalter Bartky was from 1926 to 1930 a consulting mathematician to Western Electric Company. In 1935 his book \"Highlights of Astronomy\" was published. During World War II he was the assistant director of the Manhattan Project's Metallurgical Laboratory at the University of Chicago.\n\nIn 1945 he was one of the signers of the Szilárd petition. In May 1945, he accompanied Leo Szilard and Harold Urey to Spartanburg, South Carolina to present a memorandum to James F. Byrnes; the memorandum suggested that dropping an atomic bomb on Japan might start a nuclear arms race with the Soviet Union.\n\nOn 9 January 1932, Walter Bartky married Elizabeth Inrig Robertson, of Glasgow, Scotland. The marriage produced two children, Walter Scott Bartky and Ian Robertson Bartky. W. Scott Bartky (1932–2010) was an engineer, entrepreneur and inventor with over 40 patents. Ian R. Bartky (1934–2007) was a physical chemist and historian of science.\n\n"}
