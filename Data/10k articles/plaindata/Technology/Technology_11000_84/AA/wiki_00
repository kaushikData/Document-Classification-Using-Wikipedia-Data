{"id": "1575583", "url": "https://en.wikipedia.org/wiki?curid=1575583", "title": "Acid gas", "text": "Acid gas\n\nAcid gas is a particular typology of natural gas or any other gas mixture containing significant quantities of hydrogen sulfide (HS), carbon dioxide (CO), or similar acidic gases.\n\nThe terms \"acid gas\" and \"sour gas\" are often incorrectly treated as synonyms. Strictly speaking, a sour gas is any gas that specifically contains hydrogen sulfide in significant amounts; an acid gas is any gas that contains significant amounts of acidic gases such as carbon dioxide (CO) or hydrogen sulfide. Thus, carbon dioxide by itself is an acid gas but not a sour gas.\n\nBefore a raw natural gas containing hydrogen sulfide and/or carbon dioxide can be used, the raw gas must be treated to reduce impurities to acceptable levels and this is commonly done with an amine gas treating process. The removed HS is most often subsequently converted to by-product elemental sulfur in a Claus process or alternatively converted to valuable sulfuric acid in a WSA Process unit.\n\nProcesses within oil refineries or natural-gas processing plants that remove mercaptans and/or hydrogen sulfide are commonly referred to as 'sweetening' processes because they result in products which no longer have the sour, foul odors of mercaptans and hydrogen sulfide.\n\nHydrogen sulfide is a toxic gas. It also restricts the materials that can be used for piping and other equipment for handling sour gas, as many metals are sensitive to sulfide stress cracking.\n\nCarbon dioxide at concentrations of 7% to 10.1% cause dizziness, headache, visual and hearing dysfunction, and unconsciousness within a few minutes to an hour. Concentrations above 17% are lethal when exposed for more than one minute.\n\n\n<br>\n"}
{"id": "14952464", "url": "https://en.wikipedia.org/wiki?curid=14952464", "title": "Alcohol thermometer", "text": "Alcohol thermometer\n\nThe alcohol thermometer or spirit thermometer is an alternative to the mercury-in-glass thermometer and has similar functions. Unlike the mercury-in-glass thermometer, the contents of an alcohol thermometer are less toxic and will evaporate quickly. The ethanol version is the most widely used due to the low cost and relatively low hazard posed by the liquid in case of breakage.\n\nAn organic liquid is contained in a glass bulb which is connected to a capillary of the same glass and the end is sealed with an expansion bulb. The space above the liquid is a mixture of nitrogen and the vapor of the liquid. For the working temperature range, the meniscus or interface between the liquid is within the capillary. With increasing temperature, the volume of liquid expands and the meniscus moves up the capillary. The position of the meniscus shows the temperature against an inscribed scale.\n\nThe liquid used can be pure ethanol, toluene, kerosene or isoamyl acetate, depending on manufacturer and working temperature range. Since these are transparent, the liquid is made more visible by the addition of a red or blue dye. One half of the glass containing the capillary is usually enamelled white or yellow to give a background for reading the scale.\n\nThe range of usefulness of the thermometer is set by the boiling point of the liquid used. In the case of the ethanol-filled thermometer, the upper limit for measurement is 78 °C (172.4 °F), which makes it useful for measuring daytime, nighttime and body temperatures, although not for anything much hotter than these. \n\nEthanol-filled thermometers are used in preference to mercury for meteorological measurements of minimum temperatures and can be used down to −70 °C (-94 °F). The physical limitation of the ability of a thermometer to measure low temperature is the freezing point of the liquid used. Ethanol freezes at −114.9 °C (−174.82 °F). If an alcohol thermometer utilizes a combination of ethyl alcohol, toluene, and pentane, its lower temperature range may be extended to measure temperatures down to as low as −200 °C (-328 °F). However, the measurement temperature range c. −200 °C to 78 °C, is highly dependent upon the type of alcohol used.\n\nThe alcohol thermometer was the earliest efficient, modern-style instrument of temperature measurement. As is the case with many early, important inventions, several people are credited with the invention. These include Ferdinando II de' Medici, Grand Duke of Tuscany, who in 1654 made sealed tubes partly filled with alcohol or urine, with a bulb and stem, depending on the expansion of a liquid, and independent of air pressure. Other sources, including the Encyclopædia Britannica, credit German scientist Daniel Gabriel Fahrenheit with inventing the alcohol thermometer in 1709. Fahrenheit was a skilled glassblower and his alcohol thermometer was the world's first reliable thermometer.\n"}
{"id": "1032936", "url": "https://en.wikipedia.org/wiki?curid=1032936", "title": "Anti-radiation missile", "text": "Anti-radiation missile\n\nAn anti-radiation missile (ARM) is a missile designed to detect and home in on an enemy radio emission source. Typically, these are designed for use against an enemy radar, although jammers and even radios used for communications can also be targeted in this manner.\n\nMost ARM designs to date have been intended for use against ground-based radars. Commonly carried by specialist aircraft in the Suppression of Enemy Air Defenses (SEAD) role (known to the United States Air Force as \"Wild Weasels\"), the primary purpose of this type of missile is to degrade enemy air defenses in the first period of a conflict in order to increase the chances of survival for the following waves of strike aircraft. They can also be used to quickly shut down unexpected surface-to-air missile (SAM) sites during an air raid. Often, SEAD escort aircraft also carry cluster bombs, which can be used to ensure that, after the ARM disables the SAM system's radar, the command post, missile launchers, and other components or equipment are also destroyed to guarantee that the SAM site stays down.\n\nEarly ARMs, such as the AGM-45 Shrike, were not particularly intelligent; they would simply home in on the source of radiation and explode when they got near it. SAM operators learned to turn their radar off when an ARM was fired at them, then turn it back on later, greatly reducing the missile's effectiveness. This led to the development of more advanced ARMs such as the AGM-78 Standard ARM and AGM-88 HARM missiles, which have inertial guidance systems (INS) built-in. This allows them to remember the radar's direction if it is turned off and continue to fly towards it. ARMs are less likely to hit the radar if the radar is turned off shortly after the missile is launched, as the longer the radar is off (and assuming it never turns back on), the more error is introduced into the missile's course. The ALARM even has an added loiter mode, with a built in parachute, enabling it to descend slowly until the radar lights up, when the rocket motor will re-ignite. Even a temporary shut down of the enemy's missile guidance radar can be of a great advantage to friendly aircraft during battle.\n\nThe DRDO Anti-Radiation Missile is under development by India.\n\nThe MAR-1 is under development by Brazil.\n\nSeveral surface-to-surface missiles, like the P-700 Granit, P-500 Bazalt, MM40 Exocet, B611MR, and Otomat, include a home-on-jam capability wherein the receiver component of their active radar homing is used to home in on enemy radar, ECM or communications. This makes these missiles significantly harder to defeat with ECM and distraction countermeasures, and makes the use of semi-active missiles against them dangerous.\n\nDue to experiences with jamming by US-built aircraft in Vietnam and during Middle Eastern wars in the late 1960s, the Soviets designed an alternative tracking mode for their S-75 (SA-2) missiles, which allowed them to track a jamming target without needing to actively send out any radar signals. This was achieved by the SAM site's radar receiver locking on to radio noise emissions generated by an aircraft's jamming pod. In cases of heavy jamming, missiles were often launched exclusively in this mode; this passive tracking meant that SAM sites could track targets without needing to emit any radar signals, and so American anti-radiation missiles could not be fired back in retaliation. Recently, the People's Republic of China developed the FT-2000 system to counter AEW and AWACS targets. This system is based on the HQ-9, which is in turn based on the S-300PMU. These anti-radiation missile systems have been marketed to Pakistan and various other countries.\n\nMore recently, air-to-air ARM designs have begun to appear, notably the Russian Vympel R-27EP. Such missiles have several advantages over other missile guidance techniques; they do not trigger radar warning receivers (conferring a measure of surprise), and they can have a longer range (since the battery life of the seeker head is the limiting factor to the range of most active radar homing systems).\n\nIn the 1970s, Hughes Aerospace had a project called BRAZO (Spanish for ARM). Based on a Raytheon AIM-7 Sparrow, it was meant to offer an air-to-air capability against proposed Soviet AWACS types and also some other types with extremely powerful radar sets, such as the MiG-25. The project did not proceed.\n\n\n"}
{"id": "3093639", "url": "https://en.wikipedia.org/wiki?curid=3093639", "title": "Arriflex D-20", "text": "Arriflex D-20\n\nThe Arriflex D-20 is a film-style digital motion picture camera made by Arri first introduced in November 2005. The camera's attributes are its optical viewfinder, modularity, and 35mm-width CMOS sensor. The camera was discontinued in 2008 and the Arriflex D-21 was introduced.\n\nThe D-20 uses a single CMOS sensor the width of a Super 35 film gate aperture. Effectively the D-20, when used with current 35 mm PL mount motion picture lenses, yields the same field of view and depth of field as Super 35 mm film motion picture cameras.\n\nThe D-20 captures images in two main modes:\n\nFollowing recording formats are possible:\n\nData Mode:\n\nHD Mode – HD-SDI (SMPTE 292M):\n\nHD Mode – dual link HD-SDI (SMPTE 372M):\n\nThe D-20 has a mechanical shutter, variable from 11.2° to 180° or an electronic shutter that simulates a 270° mechanical shutter at 24frame/s. The camera is capable of running at speeds from 1 to 60frame/s. Numerous components of the camera were borrowed from Arri film camera models (most notably the 435ES), assuring compatibility with Arri film camera accessories and support equipment.\n\nThough the D-20 system is capable of variable speeds from 1 to 60 frames per second, in RAW Data mode these are currently limited to 23.976 frame/s, 24 frame/s, 25 frame/s, 29.97 frame/s & 30 frame/s per second.\n\nThe sensitivity of the D-20 in video mode is regulated by the application of LUTs (Look Up Tables) prior to output. With sensitivity settings ranging from ISO 100 to ISO 800 with linear responses, the D-20 also offers log curve options designed to mimic the response of film negative. Unlike some other digital cameras, the D-20 does not offer gain boost, instead relying on the advantages of adding gain in the post production process. The sensitivity of the camera in Data mode is regulated by applying LUTs in the outboard processing of the image.\n\nOf the currently available high resolution digital motion picture cameras, the Dalsa Origin, the SI 2K and the D-20 feature detachable optical viewfinders. \n\nSome advantages of an optical over purely electronic viewing systems include:\nSome disadvantages an optical viewing introduces to a camera system:\n\nElectronic viewing options can be added to the camera. Typically, an additional monitor is used to both view and evaluate images for the camera.\n\nThe D-20 also accepts a traditional film style video assist system, sharing the image available through the optical viewfinder. This can be useful in Steadicam and remote crane applications where visibility outside the sensor capture area is preferable.\n\nLike Arri mechanical cameras, the D-20 is modularly constructed so that both the mechanical and electronic components are upgradable over time, the camera however was discontinued in 2008 and the D-21 was introduced.\n\n\nOther single-chip Super 35 mm film-sized digital motion picture cameras directly in competition with the D-20 include the Panavision Genesis camera, the Sony F35 camera and the Red One camera.\n\n"}
{"id": "568967", "url": "https://en.wikipedia.org/wiki?curid=568967", "title": "Backward chaining", "text": "Backward chaining\n\nBackward chaining (or backward reasoning) is an inference method described colloquially as working backward from the goal. It is used in automated theorem provers, inference engines, proof assistants, and other artificial intelligence applications.\n\nIn game theory, researchers apply it to (simpler) subgames to find a solution to the game, in a process called \"backward induction\". In chess, it is called retrograde analysis, and it is used to generate table bases for chess endgames for computer chess.\n\nBackward chaining is implemented in logic programming by SLD resolution. Both rules are based on the modus ponens inference rule. It is one of the two most commonly used methods of reasoning with inference rules and logical implications – the other is forward chaining. Backward chaining systems usually employ a depth-first search strategy, e.g. Prolog.\n\nBackward chaining starts with a list of goals (or a hypothesis) and works backwards from the consequent to the antecedent to see if any data supports any of these consequents. An inference engine using backward chaining would search the inference rules until it finds one with a consequent (Then clause) that matches a desired goal. If the antecedent (If clause) of that rule is known to be true, then it is added to the list of goals (for one's goal to be confirmed one must also provide data that confirms this new rule).\n\nFor example, suppose a new pet, Fritz, is delivered in an opaque box along with two facts about Fritz:\nThe goal is to decide whether Fritz is green, based on a rule base containing the following four rules:\n\n\nWith backward reasoning, an inference engine can determine whether Fritz is green in four steps. To start, the query is phrased as a goal assertion that is to be proved: \"Fritz is green\".\n\n1. Fritz is substituted for X in rule #3 to see if its consequent matches the goal, so rule #3 becomes:\nSince the consequent matches the goal (\"Fritz is green\"), the rules engine now needs to see if the antecedent (\"If Fritz is a frog\") can be proved. The antecedent therefore becomes the new goal:\n\n2. Again substituting Fritz for X, rule #1 becomes:\nSince the consequent matches the current goal (\"Fritz is a frog\"), the inference engine now needs to see if the antecedent (\"If Fritz croaks and eats flies\") can be proved. The antecedent therefore becomes the new goal:\n\n3. Since this goal is a conjunction of two statements, the inference engine breaks it into two sub-goals, both of which must be proved:\n\n4. To prove both of these sub-goals, the inference engine sees that both of these sub-goals were given as initial facts. Therefore, the conjunction is true:\ntherefore the antecedent of rule #1 is true and the consequent must be true:\ntherefore the antecedent of rule #3 is true and the consequent must be true:\n\nThis derivation therefore allows the inference engine to prove that Fritz is green. Rules #2 and #4 were not used.\n\nNote that the goals always match the affirmed versions of the consequents of implications (and not the negated versions as in modus tollens) and even then, their antecedents are then considered as the new goals (and not the conclusions as in affirming the consequent), which ultimately must match known facts (usually defined as consequents whose antecedents are always true); thus, the inference rule used is modus ponens.\n\nBecause the list of goals determines which rules are selected and used, this method is called goal-driven, in contrast to data-driven forward-chaining inference. The backward chaining approach is often employed by expert systems.\n\nProgramming languages such as Prolog, Knowledge Machine and ECLiPSe support backward chaining within their inference engines.\n\n\n"}
{"id": "43303745", "url": "https://en.wikipedia.org/wiki?curid=43303745", "title": "Banker's lamp", "text": "Banker's lamp\n\nThe banker's lamp (or Emeralite) is a style of electric table lamp characterised by a brass stand, green glass lamp shade and pull-chain switch (though modern versions may make use of alternate switch types). Other examples can feature different colors of glass.\n\nThe first patent for a banker's lamp was filed on 11 May 1909 by Harrison D. McFaddin and were produced and sold under the brand name Emeralite (\"emerald\" and \"light\"). Emeralite lamp shades were produced by the J. Schreiber & Neffen factory located in the city of Rapotín, Moravia. Later competitors were sold as \"Greenalite\", \"Verdelite\" and \"Amronlite\".\n\n"}
{"id": "50171542", "url": "https://en.wikipedia.org/wiki?curid=50171542", "title": "Boda Skins", "text": "Boda Skins\n\nNathan Alexander was born in Manchester. Nathan Alexander is the founder and CEO of Boda Skins. Boda Skins was founded in 2012 as an eBay store and has continued to grow into an online brand.\n\nIn 2012 Nathan started a brand named Boda Skins after spending some time in a small town in Turkey sourcing napa leather and designing leather jackets. These were then sold on an eBay store. Due to high demand, the business then expanded and opened up the first \"House of Boda\" based in Manchester, UK. In 2015 Boda Skins was branching further afield and opened its second House of Boda in Colorado USA and currently, looking at the Asian market to open up the third.\n\nBoda Skins have equipped celebrities including Olivia Palermo, Khloe Kardashian, Wiz Khalifa, and Ruby Rose to name a few.\n"}
{"id": "42256877", "url": "https://en.wikipedia.org/wiki?curid=42256877", "title": "Cellular Dynamics International", "text": "Cellular Dynamics International\n\nFujifilm Cellular Dynamics, Inc. (FCDI) is a large scale manufacturer of human cells, created from induced pluripotent stem cells, for use in basic research, drug discovery and regenerative medicine applications.\n\nFujifilm Cellular Dynamics, Inc (FCDI) was founded in 2004 by pluripotent stem cell biologist, James Thomson. He recognized that the way pluripotent stem cell technology could impact human health in the near future was to industrialize the process of manufacturing human cells in large quantities with high quality and consistency.\n\nPrior to acquisition, the company then named Cellular Dynamics International, Inc. tendered an initial public offering in July 2013 under the ticker symbol ICEL. On March 30, 2015, Fujifilm Holdings Corporation announced an agreement to acquire Cellular Dynamics International for $307 million or $16.50/share on a fully diluted basis. The company was subsequently renamed Fujifilm Cellular Dynamics, Inc. \n\nHuman cells are considered to be a more predictive model and a replacement for other cellular models that serve as proxies, including animal cells, immortalized cell lines, and cadaveric cells.\n\nFCDI offers several terminally differentiated cell types as catalog products: iCell® Cardiomyocytes, iCell Neurons, iCell Endothelial Cells, and iCell Hepatocytes. CDI’s MyCell® Products portfolio enables customer-provided donor samples to be reprogrammed, gene edited, and differentiated. The company also has several prototype products derived from pluripotent stem cells, including iCell Cardiac Progenitors, iCell Hematopoietic Progenitors, iCell Astrocytes, and iCell Skeletal Myoblasts.\n\nFCDI has two locations. Their corporate headquarters are located in Madison, Wisconsin and a second facility resides at the Buck Institute for Aging in Novato, California. In 2013, FCDI was awarded a contract from the California Institute for Regenerative Medicine to reprogram and bank iPS cells from 3,000 individuals.\n\n, FCDI has more than 800 patents and licensed technologies as part of their technology portfolio.\n\nFCDI appeared on \"MIT Technology Review\"'s \"Disruptive Companies\" list in both 2011 and 2012.\n\nFCDI was recognized by \"The Scientist\" for having among the year's \"Top 10 Innovations\" in 2010 for the iCell Cardiomyocyte product, and in 2012 for the MyCell Product. The iCell Cardiomyocyte product was also recognized in 2010 by \"MIT Technology Review\" as being among the year's \"Top 10 Emerging Technologies\". In 2011, the same product received Gold Winner status in \"The Wall Street Journal\"'s \"Technology Innovation Award\".\n\nFCDI began to develop drug compound safety applications early in the company’s product development cycle. Several published studies have used FCDI products to investigate mechanisms of toxicity and have leveraged cellular models to identify toxicity earlier in the drug development process. Unexpected toxicity is one of the leading reasons that drugs are pulled from the market or from late stage clinical trials and toxicity issues greatly increase the cost of drug development.\n\nFCDI cells enable new strategies for disease modeling and drug discovery work. Because FCDI’s MyCell Products are created using custom iPS cell reprogramming and differentiation processes, they provide biologically relevant human cells from donors with specific disease-associated genotypes and phenotypes. The company’s iCell and MyCell cells are readily adapted to screening platforms and demonstrate functionality using widely accepted readout technologies. FCDI’s products are used in high throughput screens, and have been used as supporting data in Investigational New Drug (IND) submissions to the FDA.\n\nReprogramming technology enables new ways to study disease mechanisms and modeling. Researchers can reprogram diseased cells of interest to study how a particular disease affects those cell types and to discover methods of repairing the cells. FCDI’s iCell and MyCell products are being used to research the development of regenerative medicine approaches, including: regenerative medicine compound screening, allogeneic and autologous cell therapy, tissue engineering, and transplantation. Specifically in the area of tissue engineering, FCDI’s iCell and MyCell products are being employed across a variety of technologies, such as implantable devices, de-cellularized organ reconstitution and 3D bioprinting.\n\nFCDI is actively engaged in a number of large-scale iPS cell reprogramming and banking projects, with the goal of creating broadly available resources of iPS cells that represent normal human diversity, disease states and adverse drug reactions. In 2013, CIRM awarded FCDI a grant to derive 3 iPS cell lines from each of 3,000 donors that represent a multitude of disease states. This project follows a $6.3 million grant awarded by the National Heart, Lung, and Blood Institute to FCDI and the Medical College of Wisconsin to investigate the mechanisms underlying left ventricular hypertrophy. FCDI’s role in this project is to generate iPS cell lines and cardiomyocytes from 250 donors selected from the Hypertension Genetic Epidemiology Network (HyperGEN) GWAS study.\n\nThe company’s first product, iCell Cardiomyocytes, have been used extensively in pharmaceutical research and drug development \napplications. Other stem cell derived Cardiomyocytes are available commercially from GE Healthcare, Cellectis, Reprocell and others. iCell Cardiomyocytes have been found to display electrical properties similar to those of human cardiomyocytes. iCell Cardiomyocytes have also been used for drug safety testing, toxicology testing, drug screening and Investigational New Drug (IND) filings. iCell Cardiac Progenitor Cells (CPCs) are part of the FCDI Cardiac portfolio; these cells launched in 2014 for use in heart failure research.\n\niCell Neurons have been used across several different research areas, including Parkinson’s disease, toxicity, autism, Alzheimer’s disease, and virology FCDI expanded their neural product portfolio by launching iCell Astrocytes and iCell Dopaminergic Neurons. Other manufacturers of stem cell derived neurons include ArunA Biomedical and ReproCell.\n\niCell Hepatocytes are used in a variety of ways, including prediction of hepatotoxicity in drug development applications. Adverse and unexpected hepatic side effects are one of the most common reasons for drugs to be removed from the market after launch or in late phase clinical trialsiCell Hepatocytes are used as a tool for better predicting hepatic toxicity earlier in the drug discovery process. Other companies that provide hepatocytes include Cellectis and Life Technologies.\n\nAt least 5 of the top 10 Leading Causes of Death in the United States have a vascular component related to their disease. Endothelial cells (cells that line the interior of blood vessels and allow nutrients to pass back and forth to the body’s organs and tissues) play an important role in the study of vascular contributions of many leading causes of death. iCell Endothelial cells were launched as a tool to model vascular biology. Other types of endothelial cells are available from ATCC and Life Technologies. iCell Hematopoietic Progenitor Cells are multipotent progenitor blood cells that can give rise to many different types of blood cells that can then be used for a variety of research purposes, including cell therapy, autoimmune disease, and cancer research.\n\nFCDI offers a family of products called MyCell Products that involve custom reprogramming, genetic engineering and differentiation of a customer’s own cell samples. The technology driving MyCell Products enables customers to study their disease of interest or correct a genetic disease phenotype using genome editing technology. Studies have shown that disease cell reprogramming can result in cells that display the particular disease morphology, providing an opportunity to study the disease as never before.\n"}
{"id": "13523148", "url": "https://en.wikipedia.org/wiki?curid=13523148", "title": "Central Utah Project Completion Act", "text": "Central Utah Project Completion Act\n\nThe Central Utah Project Completion Act (CUPCA) enacted on October 30, 1992, removed responsibility for completing the Central Utah Project, a federal water project, from the United States Bureau of Reclamation. The Central Utah Project Completion Act then distributed responsibility for the project:\n\n\nThe Central Utah Project was authorized under the Colorado River Storage Project Act (CRSPA) (Public Law 84-485) on April 11, 1956, as a participating project of the Colorado River Storage Project to help meet Utah’s long-term water needs. Construction progress on the Central Utah Project proceeded slowly because of: the complexity of the project; complex environmental analyses; and inadequate and sporadic Federal funding. The slow progress prompted state and local officials to ask Congress to empower the Central Utah Water Conservancy District to complete the planning and construction of the remaining portion of the CUP, specifically the Bonneville Unit.\n\nCongress responded to local concerns by enacting the Central Utah Project Completion Act in 1992. For the first time in history, Congress designated a local entity (the Central Utah Water Conservancy District) as the planning and construction entity FOR a major Federal water project.\n\n• Water for the Future – The Central Utah Project Completion Act authorizes sufficient Federal funds to complete the Central Utah Project. The construction of CUP facilities provides water for Utah’s future—including the future of the Uinta Basin. The Uinta Basin Replacement Project is a key element of the construction authorized under Central Utah Project Completion Act.\n\n• Recreation Opportunities – Central Utah Project facilities provide a range of recreational opportunities.\n\n• Fiscal Responsibility – The Central Utah Project Completion Act created a cost-sharing environment under which local funds augment federal funding in the planning and construction of Central Utah Project features—which engenders additional fiscal responsibility.\n\n• Environmental Commitments – The Central Utah Project Completion Act created the Utah Reclamation Mitigation and Conservation Commission to coordinate and plan mitigation measures to meet environmental mitigation and conservation measures (including environmental commitments that preceded the Central Utah Project Completion Act). The Central Utah Project Completion Act also includes provisions for maintaining steam flows at prescribed minimum rates for the benefit of aquatic and riparian habitat.\n\n• Water Conservation – The Central Utah Project Completion Act authorizes substantial funding for the planning and implementation of water conservation measures and projects.\n\n• Local Development – The Central Utah Project Completion Act provides funding for local water development projects in areas that do not benefit directly from the Central Utah Project.\n\n• Ute Indian Rights Settlement – CUPCA encourages the Northern Ute Tribe to quantify by compact its federal reserved water rights. It also settle long-outstanding claims against the United States arising out of the construction of the CUP by authorizing substantial funds to compensate the Ute Tribe for construction projects not undertaken by the United States. These funds provide for agricultural development, fish and wildlife enhancement, recreation improvement, and economic development.\n"}
{"id": "331448", "url": "https://en.wikipedia.org/wiki?curid=331448", "title": "Color depth", "text": "Color depth\n\nColor depth or colour depth (see spelling differences), also known as bit depth, is either the number of bits used to indicate the color of a single pixel, in a bitmapped image or video framebuffer, or the number of bits used for each color component of a single pixel. For consumer video standards, such as High Efficiency Video Coding (H.265), the bit depth specifies the number of bits used for each color component. When referring to a pixel, the concept can be defined as bits per pixel (bpp), which specifies the number of bits used. When referring to a color component, the concept can be defined as bits per component, bits per channel, bits per color (all three abbreviated bpc), and also bits per pixel component, bits per color channel or bits per sample (bps). Color depth is only one aspect of color representation, expressing the precision with which colors can be expressed; the other aspect is how broad a range of colors can be expressed (the gamut). The definition of both color precision and gamut is accomplished with a color encoding specification which assigns a digital code value to a location in a color space.\n\n<gallery class=\"center\" showfilename=\"yes\" widths=\"300px\" heights=\"225px\" caption=\"Comparison: same image on five different color depths (bits). Different looks (color/greyscale/black-and-white ... dithering), but also different file sizes.\">\n32 bit.png|4,294,967,296 colors<br>98 KB\n</gallery>\n\nWith the relatively low color depth, the stored value is typically a number representing the index into a color map or palette (a form of vector quantization). The colors available in the palette itself may be fixed by the hardware or modifiable by software. Modifiable palettes are sometimes referred to as pseudocolor palettes.\n\n\nOld graphics chips, particularly those used in home computers and video game consoles, often have the ability to use a different palette per sprites and tiles in order to increase the maximum number of simultaneously displayed colors, while minimizing use of then-expensive memory (& bandwidth). For example, in the ZX Spectrum, the picture is stored in a two-color format, but these two colors can be separately defined for each rectangular block of 8x8 pixels.\n\nThe palette itself has a color depth (number of bits per entry). While the best VGA systems only offered an 18-bit (262,144 color) palette from which colors could be chosen, all color Macintosh video hardware offered a 24-bit (16 million color) palette. 24-bit palettes are pretty much universal on any recent hardware or file format using them.\n\nIf pixels contain more than 12 bits, an indexed palette takes more memory than the pixels (for typical screen sizes and palette depths), so such systems tend to directly specify the color directly in the pixel.\n\nA very limited but true direct color system, there are 3 bits (8 possible levels) for each of the R and G components, and the two remaining bits in the byte pixel to the B component (four levels), enabling 256 (8 × 8 × 4) different colors. The normal human eye is less sensitive to the blue component than to the red or green (two thirds of the eye's receptors process the longer wavelengths), so it is assigned one bit less than the others. Used, amongst others, in the MSX2 system series of computers in the early to mid 1990s.\n\nDo not confuse with an indexed color depth of 8bpp (although it can be simulated in such systems by selecting the adequate table).\n\n\"High color\" supports 15/16-bit for three RGB colors. In 16-bit direct color, there can be 4 bits (16 possible levels) for each of the R, G, and B components, plus optionally 4 bits for alpha (transparency), enabling 4,096 (16 × 16 × 16) different colors with 16 levels of transparency. Or in some systems there can be 5 bits per color component and 1 bit of alpha (32,768 colors, just fully transparent or not); or there can be 5 bits for red, 6 bits for green, and 5 bits for blue, for 65,536 colors with no transparency. These color depths are sometimes used in small devices with a color display, such as mobile telephones, and is sometimes considered sufficient to display photographic images.\n\nThe term \"high color\" has recently been used to mean color depths greater than 24 bits.\n\nAlmost all of the least expensive LCDs (such as typical twisted nematic types) provide 18-bit color (64 × 64 × 64 = 262,144 combinations) to achieve faster color transition times, and use either dithering or frame rate control to approximate 24-bit-per-pixel true color, or throw away 6 bits of color information entirely. More expensive LCDs (typically IPS) can display 24-bit or greater color depth.\n\n24 bits almost always uses 8 bits of each of R, G, B. As of 2018, 24-bit color depth is used by virtually every computer and phone display and the vast majority of image storage formats. Almost all cases where there are 32 bits per pixel mean that 24 are used for the color, and the remaining 8 are the alpha channel or unused.\n\n2 gives 16,777,216 color variations. The human eye can discriminate up to ten million colors and since the gamut of a display is smaller than the range of human vision, this means this should cover that range with more detail than can be perceived. However displays do not evenly distribute the colors in human perception space so humans can see the changes between some adjacent colors as color banding. Monochromatic images set all three channels to the same value, resulting in only 256 different colors and thus more visible banding. Some software attempts to dither the gray level into the color channels to increase this, although in modern software this is much more used for subpixel rendering to increase the space resolution on LCD screens where the colors have slightly different positions.\n\nThe DVD-Video and Blu-ray Disc standards support a bit depth of 8-bits per color in YCbCr with 4:2:0 chroma subsampling. YCbCr can be losslessly converted to RGB.\n\nMacintosh systems refer to 24-bit color as \"millions of colors\". The term \"True color\" is sometime used to mean what this article is calling \"Direct color\". It is also often used to refer to all color depths greater or equal to 24.\n\n\"Deep color\" consists of a billion or more colors, 2 is 1.073 billion. Color depths of 30, 36, and 48 bits per pixel are in use, also referred to as 10, 12, or 16 bits per RGB channel/sample/component. Often an alpha channel of the same size is added, resulting in 40, 48, or 64 bits used for each pixel.\n\nSome earlier systems placed three 10-bit channels in a 32-bit word, with 2 bits unused (or used as a 4-level alpha channel). The Cineon file format that was popular for motion pictures used this. Some SGI systems had 10 (or more) bit D/A converters for the video signal and could be set up to interpret data stored this way for display. BMP files define this as one of its formats, and it is called \"HiColor\" by Microsoft.\n\nImage editing software such as Photoshop started using 16 bits per channel fairly early. The primary reason this was done was to reduce the quantization on intermediate results (if an operation divided by 4 and then multiplied by 4, it would lose the bottom 2 bits of 8-bit data, but if 16 bits were used it would lose none of the 8-bit data). Digital cameras were able to produce 10 or 12 bits per channel in their raw data, and 16 bits is the smallest addressable unit that was larger than this and would allow raw data to be worked with. Unfortunately these systems did not take advantage of 16 bits for high dynamic range, and some assign almost mystical capabilities to 16 bits that are not actually true.\n\nVideo cards with 10 bits per component started coming to market in the late 1990s. An early example was the Radius ThunderPower card for the Macintosh, which included extensions for QuickDraw and Adobe Photoshop plugins to support editing 30-bit images.\n\nThe HDMI 1.3 specification defines bit depths of 30 bits (1.073 billion colors), 36 bits (68.71 billion colors), and 48 bits (281.5 trillion colors).\nIn that regard, the Nvidia Quadro graphics cards manufactured after 2006 support 30-bit deep color as do some models of the Radeon HD 5900 series such as the HD 5970. The ATI FireGL V7350 graphics card supports 40-bit and 48-bit color.\n\nThe DisplayPort specification also supports color depths greater than 24 bpp in version 1.3 through \"VESA Display Stream Compression, which uses a visually lossless low-latency algorithm based on predictive DPCM and YCoCg-R color space and allows increased resolutions and color depths and reduced power consumption.\"\n\nAt WinHEC 2008, Microsoft announced that color depths of 30 bits and 48 bits would be supported in Windows 7, along with the wide color gamut scRGB.\n\nHigh Efficiency Video Coding (HEVC or H.265) defines the Main 10 profile, which allows for 8- or 10-bits per sample with 4:2:0 chroma subsampling. The Main 10 profile was added at the October 2012 HEVC meeting based on proposal JCTVC-K0109 which proposed that a 10-bit profile be added to HEVC for consumer applications. The proposal stated that this was to allow for improved video quality and to support the Rec. 2020 color space that will be used by UHDTV. The second version of HEVC has five profiles that allow for a bit depth of 8-bits to 16-bits per sample.\n\nSome systems started using those bits for numbers outside the 0-1 range rather than for increasing the resolution. Numbers greater than 1 were for colors brighter than the display could show, as in high-dynamic-range imaging (HDRI). Negative numbers can increase the gamut to cover all possible colors, and for storing the results of filtering operations with negative filter coefficients. The Pixar Image Computer used 12 bits to store numbers in the range [-1.5,2.5), with 2 bits for the integer portion and 10 for the fraction. The Cineon imaging system used 10-bit professional video displays with the video hardware adjusted so that a value of 95 was black and 685 was white, the amplified signal tended to reduce the lifetime of the CRT.\n\nMore bits also encouraged the storage of light as linear values, where the number directly corresponds to the amount of light emitted. Linear levels makes calculation of light (in the context of computer graphics) much easier. However, linear color results in disproportionately more samples near white and fewer near black, so the quality of 16-bit linear is about equal to 12-bit sRGB.\n\nFloating point numbers can represent linear light levels spacing the samples semi-logarithmically. Floating point representations also allow for drastically larger dynamic ranges as well as negative values. Around 1993, computer hardware had progressed to the point where the speed of floating point math began exceeding integer math in most practical situations. Most systems first supported 32-bit per channel single-precision, which far exceeded the accuracy required for most applications. In 1999, Industrial Light & Magic released the open standard OpenEXR image file format which supports 16-bit-per-channel half-precision floating-point numbers. At values near 1.0, half precision floating point values have only the precision of an 11-bit integer value, leading some graphics professionals to reject half-precision in situations where the extended dynamic range is not needed.\n\nVirtually all television displays and computer displays form images by varying the strength of just three primary colors: red, green, and blue. For example, bright yellow is formed by roughly equal red and green contributions, with no blue contribution.\n\nAdditional color primaries can widen the color gamut of a display, as you are not limited to the shape of a triangle in the CIE 1931 color space. Recent technologies such as Texas Instruments's \"BrilliantColor\" augment the typical red, green, and blue channels with up to three other primaries: cyan, magenta and yellow. Mitsubishi and Samsung, among others, use this technology in some TV sets to extend the range of displayable colors. The Sharp Aquos line of televisions has introduced Quattron technology, which augments the usual RGB pixel components with a yellow subpixel. However, formats and media supporting these extended color primaries are extremely uncommon.\n\nFor storing and working on images, it is possible to use \"imaginary\" primary colors that are not physically possible so that the triangle does enclose a much larger gamut, so whether more than three primaries results in a difference to the human eye is not yet proven, since humans are primarily trichromats, though tetrachromats exist.\n\n"}
{"id": "20034299", "url": "https://en.wikipedia.org/wiki?curid=20034299", "title": "Conar Instruments", "text": "Conar Instruments\n\nConar Instruments was a company located in Washington, D.C.. It was an expansion of the National Radio Institute's (NRI) student supply division. The purpose of this division was primarily to supply test equipment to NRI students and graduates. \n\nThe name 'Conar' was derived from the first letters of 'COmpany, NAtional Radio.' \n\nConar was able to enter the market with a complete series of radio-TV test-equipment kits of proven design developed by NRI's technical staff.\n\nArticle on the RadioMuseum Website\n"}
{"id": "27882571", "url": "https://en.wikipedia.org/wiki?curid=27882571", "title": "Deep energy retrofit", "text": "Deep energy retrofit\n\nA deep energy retrofit is a whole-building analysis and construction process that uses \"integrative design\" to achieve much larger energy savings than conventional energy retrofits. Deep energy retrofits can be applied to both residential and non-residential (“commercial”) buildings. A deep energy retrofit typically results in savings of 30 percent or more, perhaps spread over several years, and may significantly improve the building value.\n\nThe term \"deep energy retrofit\" is often used interchangeably with \"deep green retrofit\" and \"deep retrofit\". A deep green retrofit may have less focus on energy efficiency and may emphasize obtaining certification from a green building rating system, such as LEED. The definition of the term continues to be refined and debated.\n\nConventional energy retrofits focus on isolated system upgrades (i.e. lighting and HVAC equipment). These retrofits are generally simple and fast, but they often miss opportunity for saving more energy cost-effectively.\n\nDeep energy retrofits achieve much greater energy efficiency by taking a whole-building approach, addressing many systems at once. It is most economical and convenient to take this approach on buildings with overall poor efficiency performance, with multiple systems nearing the end of useful life, and perhaps other reasons.\n\nA deep energy retrofit combines energy efficiency measures such as energy efficient equipment, air sealing, moisture management, controlled ventilation, insulation, and solar control so that dramatic energy savings are achieved alongside optimal building performance.\n\nDurability, good interior air quality and energy efficiency are attained by sound building science practices. In a deep energy retrofit, filling a wall cavity with effective insulation also requires careful consideration of how that wall will dry if moisture does happen to get past its skin. Using very high R-value insulation systems on the exterior of the building enclosure is often one of the hallmarks of a deep energy retrofit. Where exactly the dewpoint will fall in (or out) of those thickened walls—and in what climate zone—becomes crucial. Careful detailing, flashing and air sealing of windows and other building penetrations is also key to a successful deep energy retrofit.\n\nSystems thinking is required for these kinds of retrofits, where highly efficient windows are \"tuned\" to their orientation, and mechanical systems and heat recovery ventilation units are sized and integrated with how the walls, roof and basement are being air sealed, moisture-managed and insulated.\n\nA Level III energy audit, as defined by ASHRAE, is required in order to complete a commercial building deep energy retrofit. Also known as an investment grade audit, this type of energy audit features analysis of the interactions between efficiency strategies and their life cycle cost. Upon selection and implementation of measures, the energy savings are verified using the International Performance Measurement and Verification Protocol.\n\nDeep energy retrofits make use of energy modeling tools that integrate with an organization's pro forma or other financial decision making mechanisms. Smartphone technologies have simplified the retrofit process as a number of audit and retrofit tools have appeared over the last 5 years to speed up retrofits and maximize efficiency in the field.\n\nA building that has undergone a deep energy retrofit is well positioned for a green building rating such as LEED.\n\nThere have been a number of studies done to determine and quantify the benefits afforded to owners, tenants, and various other stakeholders from the successful completion of deep energy retrofits.\n\nCommon owner related benefits include: \n\n\nCommon benefits to tenants include: \n\n\nThe Empire State Building is undergoing a deep energy retrofit process that is projected to be completed in 2013. Upon completion, the project team, consisting of representatives from Johnson Controls, Rocky Mountain Institute, Clinton Climate Initiative, Jones Lang LaSalle, and NYSERDA will have achieved an annual energy use reduction of 38% and $4.4 million.\n\nA notable achievement of the project is that instead of replacing the chillers as originally planned, the design team were able to first reduce the building’s required cooling capacity by 1600 tons, allowing for a chiller retrofit instead of replacement which would have been $17.3 million more in capital costs.\n\nThe City-County Building recently underwent a deep energy retrofit process that is projected to be completed in September 2011. Upon completion, the project team, consisting of representatives from the Indianapolis Marion County Building Authority, Indianapolis Office of Sustainability, Rocky Mountain Institute, and Performance Services will have achieved an annual energy reduction of 46% and $750,000 annual energy savings.\n\n"}
{"id": "28961152", "url": "https://en.wikipedia.org/wiki?curid=28961152", "title": "EN 10025", "text": "EN 10025\n\nEN 10025 - Hot rolled products of structural steels refers to a set of European standards which specify the technical delivery conditions for hot rolled products of structural steels. The standards consist of the following parts:\n\n\n\n\n"}
{"id": "34044702", "url": "https://en.wikipedia.org/wiki?curid=34044702", "title": "EUCARIS", "text": "EUCARIS\n\nEUCARIS (short for European Car and Driving License Information System) is an information exchange system that provides an infrastructure and software to countries to share, among others, their car- and driving licence-registration information, helping fight car theft and registration-fraud. EUCARIS is developed by and for governmental authorities and is able to support all kinds of transport related information exchange based on treaties, directives, bi- and multilateral agreements.\n\nIn the early nineties five European vehicle and driving licence registration authorities (BE, DE, UK, LU and NL) took the initiative to set up a network for data communication and to give European countries the opportunity to share vehicle and driver registration information.\n\nThe system is operational since 1994 and used Tuxedo as main technology for the exchange. In 2007 this system was replaced by the second generation of EUCARIS (a.k.a. EUCARIS II) which uses open standards (XML/Webservice/SOAP). The current EUCARIS software is built using the Microsoft .NET platform.\n\nEUCARIS currently supports the following treaties, council decisions, and bi- or multilateral agreements:\n\nEUCARIS is a multilateral treaty that provides opportunities to countries to share their car and driving licence registration information, helping to fight car theft and registration fraud.\n\nThe Prüm treaty, also partly adapted by Council Decision 2008/615/JHA and 2008/616/JHA. The treaty and Council Decisions supports cross-border cooperation, particularly in combating terrorism and cross-border crime.\n\nExchange of vehicle and its owner/holder date for the enforcement of traffic violations, toll collection, etc.\n\neCall is a project intended to bring rapid assistance to motorists involved in a collision anywhere in the European Union. Member states participating in eCall may use the EUCARIS platform for exchanging messages. EUCARIS will support eCall from 2012.\n\nRESPER (Réseau permis de conduire / Drivers License Network) is a network to be established across the European Union which should be fully operational by 19/1/2013. EUCARIS will be able to connect to the RESPER network giving the Member States a choice whether they want to use EUCARIS to exchange driving licence information via RESPER. EUCARIS will support RESPER from 2012.\n\nERRU (European Register of Road Transport Undertakings) allows competent authorities to better monitor the compliance of road transport undertakings across the European Union which should be fully operational by 1/1/2013. EUCARIS will be able to connect to the ERRU network giving the Member States a choice whether they want to use EUCARIS to exchange information via ERRU. EUCARIS will support ERRU from 2012.\n\nThe table below shows the currently participating member states who are actively exchanging data:\n\n"}
{"id": "515580", "url": "https://en.wikipedia.org/wiki?curid=515580", "title": "Ethnomethodology", "text": "Ethnomethodology\n\nEthnomethodology is the study of methods people use for understanding and producing the social order in which they live. It generally seeks to provide an alternative to mainstream sociological approaches. In its most radical form, it poses a challenge to the social sciences as a whole. On the other hand, its early investigations led to the founding of conversation analysis, which has found its own place as an accepted discipline within the academy. According to Psathas, it is possible to distinguish five major approaches within the ethnomethodological family of disciplines (see ).\n\nEthnomethodology provides methods which have been used in ethnographic studies to produce accounts of people's methods for negotiating everyday situations. It is a fundamentally descriptive discipline which does not engage in the explanation or evaluation of the particular social order undertaken as a topic of study. However, applications have been found within many applied disciplines, such as software design and management studies.\n\nThe term's meaning can be broken down into its three constituent parts: \"ethno\" – \"method\" – \"ology\", for the purpose of explanation. Using an appropriate Southern California example: \"ethno\" refers to a particular socio-cultural group (for example, a particular, local community of surfers); \"method\" refers to the methods and practices this particular group employs in its everyday activities (for example, related to surfing); and \"ology\" refers to the systematic description of these methods and practices. The focus of the investigation used in our example is the social order of surfing, the ethnomethodological interest is in the \"how\" (the methods and practices) of the production and maintenance of this social order. In essence ethnomethodology attempts to create classifications of the social actions of individuals within groups through drawing on the experience of the groups directly, without imposing on the setting the opinions of the researcher with regards to social order, as is the case with sociological studies.\n\nThe approach was originally developed by Harold Garfinkel, who attributed its origin to his work investigating the conduct of jury members in 1954. His interest was in describing the common sense methods through which members of a jury produce themselves in a jury room \"as a jury\". Thus, their methods for: establishing matters of fact; developing evidence chains; determining the reliability of witness testimony; establishing the organization of speakers in the jury room itself; and determining the guilt or innocence of defendants, etc. are all topics of interest. Such methods serve to constitute the social order of being a juror for the members of the jury, as well as for researchers and other interested parties, in that specific social setting.\n\nThis interest developed out of Garfinkel's critique of Talcott Parsons' attempt to derive a general theory of society. This critique originated in his reading of Alfred Schutz, though Garfinkel ultimately revised many of Schutz's ideas. Garfinkel also drew on his study of the principles and practices of financial accounting; the classic sociological theory and methods of Durkheim and Weber; and the traditional sociological concern with the Hobbesian \"problem of order\".\n\nFor the ethnomethodologist, participants produce the order of social settings through their shared sense making practices. Thus, there is an essential natural reflexity between the activity of making sense of a social setting and the ongoing production of that setting; the two are in effect identical. Furthermore, these practices (or methods) are witnessably enacted, making them available for study. This opens up a broad and multi-faceted area of inquiry. John Heritage writes, \"In its open-ended reference to [the study of] any kind of sense-making procedure, the term represents a signpost to a domain of uncharted dimensions rather than a staking out of a clearly delineated territory.\"\n\nEthnomethodology has perplexed commentators, due to its radical approach to questions of theory and method.\n\nWith regard to theory, Garfinkel has consistently advocated an attitude of ethnomethodological indifference, a principled agnosticism with regard to social theory which insists that the shared understandings of members of a social setting under study take precedence over any concepts which a social theorist might bring to the analysis from outside that setting. This can be perplexing to traditional social scientists, trained in the need for social theory and a multiplicity of theoretical references by Anne Rawls, in her introduction to \"Ethnomethodology's Program\" might be interpreted to suggest a softening of this position towards the end of Garfinkel's life. However, the position is consistent with ethnomethodology's understanding of the significance of \"member's methods\", and with certain lines of philosophical thought regarding the philosophy of science (Polanyi 1958; Kuhn 1970; Feyerabend 1975), and the study of the actual practices of scientific procedure. It also has a strong correspondence with the later philosophy of Ludwig Wittgenstein, especially as applied to social studies by Peter Winch. References are also made in Garfinkel's work to Husserl (Transcendental Phenomenology), Gurwitsch (Gestalt Theory), and, most frequently, of course, to the works of the social phenomenologist Alfred Schutz (Phenomenology of the Natural Attitude), among others. On the other hand, the authors and theoretical references cited by Garfinkel do not constitute a rigorous theoretical basis for ethnomethodology. Ethnomethodology is not Durkheimian, although it shares some of the interests of Durkheim; it is not phenomenology, although it borrows from Husserl and Schutz's studies of the lifeworld (\"Lebenswelt\"); it is not a form of Gestalt theory, although it describes social orders as having Gestalt-like properties; and, it is not Wittgensteinian, although it makes use of Wittgenstein's understanding of rule-use, etc. Instead, these borrowings are only fragmentary references to theoretical works from which ethnomethodology has appropriated theoretical ideas for the expressed purposes of \"doing\" ethnomethodological investigations.\n\nSimilarly, ethnomethodology advocates no formal methods of enquiry, insisting that the research method be dictated by the nature of the phenomenon that is being studied. Ethnomethodologists have conducted their studies in a variety of ways, and the point of these investigations is \"to discover the things that persons in particular situations \"do\", the methods \"they\" use, to create the patterned orderliness of social life.\" Michael Lynch has noted that: \"Leading figures in the field have repeatedly emphasised that there is no obligatory set of methods [employed by ethnomethodologists], and no prohibition against using any research procedure whatsoever, if it is \"adequate\" to the particular phenomena under study\".\n\n\nSince ethnomethodology has become anathema to certain sociologists, and since those practicing it like to perceive their own efforts as constituting a radical break from prior sociologies, there has been little attempt to link ethnomethodology to these prior sociologies. However, whilst ethnomethodology is distinct from sociological methods, it does not seek to compete with it, or provide remedies for any of its practices. The Ethnomethodological approach differs as much from the sociological approach as sociology does from psychology even though both speak of social action. This does not mean that ethnomethodology does not use traditional sociological forms as a sounding board for its own programmatic development, or to establish benchmarks for the differences between traditional sociological forms of study and ethnomethodology as it only means that ethnomethodology was not established in order to: repair, criticize, undermine, or 'poke fun' at traditional sociological forms . In essence the distinctive difference between sociological approaches and ethnomethodology is that the latter adopts a commonsense attitude towards knowledge.\n\nIn contrast to traditional sociological forms of inquiry, it is a hallmark of the ethnomethodological perspective that it does not make theoretical or methodological appeals to: outside assumptions regarding the structure of an actor or actors' characterisation of social reality; refer to the subjective states of an individual or groups of individuals; attribute conceptual projections such as, \"value states\", \"sentiments\", \"goal orientations\", \"mini-max economic theories of behavior\", etc., to any actor or group of actors; or posit a specific \"normative order\" as a transcendental feature of social scenes, etc.\n\nFor the ethnomethodologist, the methodic realisation of social scenes takes place within the actual setting under scrutiny, and is structured by the participants in that setting through the reflexive accounting of that setting's features. The job of the Ethnomethodologist is to describe the methodic character of these activities, not account for them in a way that transcends that which is made available in and through the actual accounting practices of the individual's party to those settings.\n\nThe differences can therefore be summed up as follows:\n\n\nEven though ethnomethodology has been characterised as having a \"phenomenological sensibility\", and reliable commentators have acknowledged that \"there is a strong influence of phenomenology on ethnomethodology...\" (Maynard and Kardash 2007:1484), orthodox adherents to the discipline—those who follow the teachings of Garfinkel—do not represent it as a branch, or form, of phenomenology, or phenomenological sociology.\n\nThe confusion between the two disciplines stems, in part, from the practices of some ethnomethodologists (including Garfinkel), who sift through phenomenological texts, recovering phenomenological concepts and findings relevant to their interests, and then transpose these concepts and findings to topics in the study of social order. Such interpretive transpositions do not make the ethnomethodologist a phenomenologist, or ethnomethodology a form of phenomenology.\n\nTo further muddy the waters, some phenomenological sociologists seize upon ethnomethodological findings as examples of applied phenomenology; this even when the results of these ethnomethodological investigations clearly do not make use of phenomenological methods, or formulate their findings in the language of phenomenology. So called phenomenological analyses of social structures that do not have prima facie reference to any of the structures of intentional consciousness should raise questions as to the phenomenological status of such analyses.\n\nGarfinkel speaks of phenomenological texts and findings as being \"appropriated\" and intentionally \"misread\" for the purposes of exploring topics in the study of social order. These appropriations and methodical \"misread[ings]\" of phenomenological texts and findings are clearly made for the purposes of furthering ethnomethodological analyses, and should not be mistaken for logical extensions of these phenomenological texts and findings.\n\nLastly, there is no claim in any of Garfinkel's work that ethnomethodology is a form of phenomenology, or phenomenological sociology. To state that ethnomethodology has a \"phenomenological sensibility\" or that \"there is a strong influence of phenomenology on ethnomethodology\" is not the equivalent of describing ethnomethodology as a form of phenomenology (see Garfinkel/Liberman 2007:3–7).\n\nEven though ethnomethodology is not a form of phenomenology, the reading and understanding of phenomenological texts, and developing the capability of seeing phenomenologically is essential to the actual doing of ethnomethodological studies. As Garfinkel states in regard to the work of the phenomenologist Aron Gurwitsch, especially his \"Field of Consciousness\" (1964: ethnomethodology's phenomenological \"urtext\"): \"you can't do anything unless you do read his texts.\"\n\nAccording to George Psathas, five types of ethnomethodological study can be identified (Psathas 1995:139–155). These may be characterised as:\n\n\nFurther discussion of the varieties and diversity of ethnomethodological investigations can be found in Maynard & Clayman's work.\n\nThe relationship between ethnomethodology and conversation analysis has been contentious at times, given their overlapping interests, the close collaboration between their founders and the subsequent divergence of interest among many practitioners. In as much as the study of social orders is \"inexorably intertwined\" with the constitutive features of talk about those social orders, ethnomethodology is committed to an interest in both conversational talk, and the role this talk plays in the constitution of that order. Talk is seen as indexical and embedded in a specific social order. It is also naturally reflexive to and constitutive of that order. Anne Rawls pointed out: \"Many, in fact most, of those who have developed a serious interest in ethnomethodology have also used conversational analysis, developed by Sacks, Schegloff, and Jefferson, as one of their research tools.\"\n\nOn the other hand, where the study of conversational talk is divorced from its situated context—that is, when it takes on the character of a purely technical method and \"formal analytic\" enterprise in its own right—it is not a form of ethnomethodology. The \"danger\" of misunderstanding here, as Rawls notes, is that conversational analysis can become just another formal analytic enterprise, like any other formal method which brings an analytical toolbox of preconceptions, formal definitions, and operational procedures to the situation/setting under study. When such analytical concepts are generated from within one setting and conceptually applied (generalised) to another, the (re)application represents a violation of the strong form of the \"unique adequacy requirement\" of methods.\n\n\n"}
{"id": "36940120", "url": "https://en.wikipedia.org/wiki?curid=36940120", "title": "Flex-Foot Cheetah", "text": "Flex-Foot Cheetah\n\nThe Flex-Foot Cheetah is a prosthetic human foot replacement developed by biomedical engineer Van Phillips, who had lost a leg below the knee at age 21; the deficiencies of existing prostheses led him to invent this new prosthesis.\n\nThe Flex-Foot Cheetah and similar models are worn by Oscar Pistorius and other amputee athletes in the Paralympics and elsewhere. It is made from carbon fibre, and unlike all previous foot prostheses, it stores kinetic energy from the wearer's steps as potential energy, like a spring, allowing the wearer to run and jump. It is now (as of September 2012) made by Össur.\n\nCarbon fiber is actually a carbon-fiber-reinforced polymer, and is a strong, light-weight material used in a number of applications, including sporting goods like baseball bats, car parts, helmets, sailboats, bicycles and other equipment where rigidity and high strength-to-weight ratio is important. The polymer used for this equipment is normally epoxy, but other polymers are also used, depending on the application, and other reinforcing fibers may also be included. In the blade manufacturing process, sheets of impregnated material are cut into square sheets and pressed onto a form to produce the final shape. From 30 to 90 sheets may be layered, depending on the expected weight of the athlete, and the mold is then autoclaved to fuse the sheets into a solid plate. This method reduces air bubbles that can cause breaks. Once the result is cooled, it is cut into the shape of the blades, each of which costs between $15,000 and $18,000.\n\nAbout 90 percent of amputee Paralympics runners use a variation of the original Flex-Foot design, as well as thousands of athletes around the world. \"Bladerunners\" seen at the Paralympics who have lost both feet run in the T43 class, but runners with one blade and a natural foot run in the T44 class.\n\n\n\n"}
{"id": "25660882", "url": "https://en.wikipedia.org/wiki?curid=25660882", "title": "Footprint (electronics)", "text": "Footprint (electronics)\n\nA footprint or land pattern is the arrangement of pads (in surface-mount technology) or through-holes (in through-hole technology) used to physically attach and electrically connect a component to a printed circuit board. The land pattern on a circuit board matches the arrangement of leads on a component.\n\nComponent manufacturers often produce multiple pin-compatible product variants to allow systems integrators to change the exact component in use without changing the footprint on the circuit board. This can provide large cost savings for integrators, especially with dense BGA components where the footprint pads may be connected to multiple layers of the circuit board.\n\n"}
{"id": "19964125", "url": "https://en.wikipedia.org/wiki?curid=19964125", "title": "GIS and aquatic science", "text": "GIS and aquatic science\n\nGeographic Information Systems (GIS) has become an integral part of aquatic science and limnology. Water by its very nature is dynamic. Features associated with water are thus ever-changing. To be able to keep up with these changes, technological advancements have given scientists methods to enhance all aspects of scientific investigation, from satellite tracking of wildlife to computer mapping of habitats. Agencies like the US Geological Survey, US Fish and Wildlife Service as well as other federal and state agencies are utilizing GIS to aid in their conservation efforts.\n\nGIS is being used in multiple fields of aquatic science from limnology, hydrology, aquatic botany, stream ecology, oceanography and marine biology. Applications include using satellite imagery to identify, monitor and mitigate habitat loss. Imagery can also show the condition of inaccessible areas. Scientists can track movements and develop a strategy to locate locations of concern. GIS can be used to track invasive species, endangered species, and population changes.\n\nOne of the advantages of the system is the availability for the information to be shared and updated at any time through the use of web-based data collection.\n\nIn the past, GIS was not a practical source of analysis due to the difficulty in obtaining spatial data on habitats or organisms in underwater environments. With the advancement of radio telemetry, hydroacoustic telemetry and side-scan sonar biologists have been able to track fish species and create databases that can be incorporated into a GIS program to create a geographical representation. Using radio and hydroacoustic telemetry, biologists are able to locate fish and acquire relatable data for those sites, this data may include substrate samples, temperature, and conductivity. Side-scan sonar allows biologists to map out a river bottom to gain a representation of possible habitats that are used. These two sets of data can be overlaid to delineate the distribution of fish and their habitats for fish. This method has been used in the study of the pallid sturgeon.\n\nOver a period of time large amounts of data are collected and can be used to track patterns of migration, spawning locations and preferred habitat. Before, this data would be mapped and overlaid manually. Now this data can be entered into a GIS program and be layered, organized and analyzed in a way that was not possible to do in the past. Layering within a GIS program allows for the scientist to look at multiple species at once to find possible watersheds that are shared by these species, or to specifically choose one species for further examination.\nThe US Geological Survey (USGS) in, cooperation with other agencies, were able to use GIS in helping map out habitat areas and movement patterns of pallid sturgeon. At the Columbia Environmental Research Center their effort relies on a customized ArcPad and ArcGIS, both ESRI (Environmental Systems Research Institute) applications, to record sturgeon movements to streamline data collection. A relational database was developed to manage tabular data for each individual sturgeon, including initial capture and reproductive physiology. Movement maps can be created for individual sturgeon. These maps help track the movements of each sturgeon through space and time. This allowed these researchers to prioritize and schedule field personnel efforts to track, map, and recapture sturgeon.\nMacrophytes are an important part of healthy ecosystems. They provide habitat, refuge, and food for fish, wildlife, and other organisms. Though natural occurring species are of great interest so are the invasive species that occur alongside these in our environment. GIS is being used by agencies and their respective resource managers as a tool to model these important macrophyte species. Through the use of GIS resource managers can assess the distributions of this important aspect of aquatic environments through a spatial and temporal scale. The ability to track vegetation change through time and space to make predictions about vegetation change are some of the many possibilities of GIS. Accurate maps of the aquatic plant distribution within an aquatic ecosystem are an essential part resource management. \n\nIt is possible to predict the possible occurrences of aquatic vegetation. For example, the USGS has created a model for the American wild celery (Vallisneria americana) by developing a statistical model that calculates the probability of submersed aquatic vegetation. They established a web link to an Environmental Systems Research Institute (ESRI) ArcGIS Server website *Submersed Aquatic Vegetation Model to make their model predictions available online. These predictions for distribution of submerged aquatic vegetation can potentially have an effect on foraging birds by creating avoidance zones by humans. If it is known where these areas are, birds can be left alone to feed undisturbed. When there are years where the aquatic vegetation is predicted to be limited in these important wildlife habitats, managers can be alerted.\n\nInvasive species have become a great conservation concern for resource managers. GIS allows managers to map out plant locations and abundances. These maps can then be used to determine the threat of these invasive plants and help the managers decide on management strategies. Surveys of these species can be conducted and then downloaded into a GIS system. Coupled with this, native species can be included to determine how these communities respond with each other. By using known data of preexisting invasive species GIS models could predict future outbreaks by comparing biological factors. The Connecticut Agricultural Experiment Station Invasive Aquatic Species Program (CAES IAPP) is using GIS to evaluate risk factors. GIS allows managers to georeference plant locations and abundance. This allows for managers to display invasive communities alongside native species for study and management.\n\n\n"}
{"id": "42246223", "url": "https://en.wikipedia.org/wiki?curid=42246223", "title": "HRG gyrocompass", "text": "HRG gyrocompass\n\nA HRG gyrocompass is a compass and instrument of navigation. It is the latest generation of maintenance free instrument.\n\nThe HRG gyrocompass is a complete unit, which unlike a conventional compass, has no rotating or other moving parts. It uses Hemispherical Resonant gyroscope, accelerometers and computers to compute True north. It has an outstanding reliability. Its operational Mean Time Between Failure (MTBF) values are improved over a Fiber Optic Gyrocompass and also conventional mechanical gyrocompass. \nThe HRG gyrocompass is also immune to several environmental conditions.\n\n\n"}
{"id": "185853", "url": "https://en.wikipedia.org/wiki?curid=185853", "title": "Hans Bethe", "text": "Hans Bethe\n\nHans Albrecht Bethe (; July 2, 1906 – March 6, 2005) was a German-American nuclear physicist who made important contributions to astrophysics, quantum electrodynamics and solid-state physics, and won the 1967 Nobel Prize in Physics for his work on the theory of stellar nucleosynthesis.\n\nFor most of his career, Bethe was a professor at Cornell University. During World War II, he was head of the Theoretical Division at the secret Los Alamos laboratory which developed the first atomic bombs. There he played a key role in calculating the critical mass of the weapons and developing the theory behind the implosion method used in both the Trinity test and the \"Fat Man\" weapon dropped on Nagasaki in August 1945.\n\nAfter the war, Bethe also played an important role in the development of the hydrogen bomb, though he had originally joined the project with the hope of proving it could not be made. Bethe later campaigned with Albert Einstein and the Emergency Committee of Atomic Scientists against nuclear testing and the nuclear arms race. He helped persuade the Kennedy and Nixon administrations to sign, respectively, the 1963 Partial Nuclear Test Ban Treaty and 1972 Anti-Ballistic Missile Treaty (SALT I).\n\nHis scientific research never ceased and he was publishing papers well into his nineties, making him one of the few scientists to have published at least one major paper in his field during every decade of his careerwhich, in Bethe's case, spanned nearly seventy years. Freeman Dyson, once one of his students, called him the \"supreme problem-solver of the 20th century\".\n\nBethe was born in Strasbourg, which was then part of Germany, on July 2, 1906, the only child of Anna (née Kuhn) and Albrecht Bethe, a \"privatdozent\" of physiology at the University of Strasbourg. Although his mother, the daughter of a professor at the University of Strasbourg, was Jewish, he was raised a Protestant like his father. Despite having a religious background, he was not religious in later life, and described himself as an atheist.\nHis father accepted a position as professor and director of the Institute of Physiology at the University of Kiel in 1912, and the family moved into the director's apartment at the Institute. He was initially schooled privately by a professional teacher as part of a group of eight girls and boys. The family moved again in 1915 when his father became the head of the new Institute of Physiology at the University of Frankfurt am Main.\n\nBethe attended the Goethe-Gymnasium in Frankfurt, Germany. His education was interrupted in 1916, when he contracted tuberculosis, and he was sent to Bad Kreuznach to recuperate. By 1917, he had recovered sufficiently to attend the local \"realschule\", and the following year he was sent to the \"Odenwaldschule\", a private, coeducational boarding school. He attended the \"Goethe-Gymnasium\" again for his final three years of secondary schooling, from 1922 to 1924.\n\nHaving passed his \"abitur\", Bethe entered the University of Frankfurt in 1924. He decided to major in chemistry. The instruction in physics was poor, and while there were distinguished mathematicians in Frankfurt like Carl Ludwig Siegel and Otto Szász, Bethe disliked their approaches, which presented mathematics without reference to the other sciences. Bethe found that he was a poor experimentalist who destroyed his lab coat by spilling sulfuric acid on it, but he found the advanced physics taught by the associate professor, Walter Gerlach, more interesting. Gerlach left in 1925, and was replaced by Karl Meissner, who advised Bethe that he should go to a university with a better school of theoretical physics, specifically the University of Munich, where he could study under Arnold Sommerfeld.\n\nBethe entered the University of Munich in April 1926, where Sommerfeld took him on as a student on Meissner's recommendation. Sommerfeld taught an advanced course on differential equations in physics, which Bethe enjoyed. Because he was such a renowned scholar, Sommerfeld frequently received advance copies of scientific papers, which he put up for discussion at weekly evening seminars. When Bethe arrived, Sommerfeld had just received Erwin Schrödinger's papers on wave mechanics.\n\nFor his PhD thesis, Sommerfeld suggested that Bethe examine electron diffraction in crystals. As a starting point, Sommerfeld suggested Paul Ewald's 1914 paper on X-ray diffraction in crystals. Bethe later recalled that he became too ambitious, and, in pursuit of greater accuracy, his calculations became unnecessarily complicated. When he met Wolfgang Pauli for the first time, Pauli told him: \"After Sommerfeld's tales about you, I had expected much better from you than your thesis.\" \"I guess from Pauli,\" Bethe later recalled, \"that was a compliment.\"\n\nAfter Bethe received his doctorate, Erwin Madelung offered him an assistantship in Frankfurt, and in September 1928 Bethe moved in with his father, who had recently divorced his mother. His father met Vera Congehl earlier that year, and married her in 1929. They had two children, Doris, born in 1933, and Klaus, born in 1934. Bethe did not find the work in Frankfurt very stimulating, and in 1929 he accepted an offer from Ewald at the \"Technische Hochschule\" in Stuttgart. While there, he wrote what he considered to be his greatest paper, \"Zur Theorie des Durchgangs schneller Korpuskularstrahlen durch Materie\" (\"The Theory of the Passage of Fast Corpuscular Rays Through Matter\"). Starting from Max Born's interpretation of the Schrödinger equation, Bethe produced a simplified formula for collision problems using a Fourier transform, which is known today as the Bethe formula. He submitted this paper for his \"habilitation\" in 1930.\n\nSommerfeld recommended Bethe for a Rockefeller Foundation Travelling Scholarship in 1929. This provided $150 a month (about $,000 in 2017 dollars) to study abroad. In 1930, Bethe chose to do postdoctoral work at the Cavendish Laboratory at the University of Cambridge in England, where he worked under the supervision of Ralph Fowler. At the request of Patrick Blackett, who was working with cloud chambers, Bethe created a relativistic version of the Bethe formula. Bethe was also known for his sense of humor, and with Guido Beck and Wolfgang Riezler, two other postdoctoral research fellows, created a hoax paper \"On the Quantum Theory of the Temperature of Absolute Zero\" where he calculated the fine structure constant from the absolute zero temperature in Celsius units. The paper poked fun at a certain class of papers in theoretical physics of the day, which were purely speculative and based on spurious numerical arguments such as Arthur Eddington's attempts to explain the value of the fine structure constant from fundamental quantities in an earlier paper. They were forced to issue an apology.\n\nFor the second half of his scholarship, Bethe chose to go to Enrico Fermi's laboratory in Rome in February 1931. He was greatly impressed by Fermi and regretted that he had not gone to Rome first. Bethe developed the Bethe ansatz, a method for finding the exact solutions for the eigenvalues and eigenvectors of certain one-dimensional quantum many-body models. He was influenced by Fermi's simplicity and Sommerfeld's rigor in approaching problems, and these qualities influenced his own later research.\n\nThe Rockefeller Foundation offered an extension of Bethe's fellowship, allowing him to return to Italy in 1932. In the meantime, Bethe worked for Sommerfeld in Munich as a \"privatdozent\". Since Bethe was fluent in English, Sommerfeld had Bethe supervise all his English-speaking postdoctoral fellows, including Lloyd P. Smith from Cornell University. Bethe accepted a request from Karl Scheel to write an article for the \"Handbuch der Physik\" on the quantum mechanics of hydrogen and helium. Reviewing the article decades later, Robert Bacher and Victor Weisskopf noted that it was unusual in the depth and breadth of its treatment of the subject, yet required very little updating for the 1959 edition. Bethe was then asked by Sommerfeld to help him with the \"handbuch\" article on electrons in metals. The article covered the basis of what is now called solid state physics. Bethe took a very new field and provided a clear, coherent and complete coverage of it. His work on the \"handbuch\" articles occupied most of his time in Rome, but he also co-wrote a paper with Fermi on another new field, quantum electrodynamics, describing the relativistic interactions of charged particles.\n\nIn 1932, Bethe accepted an appointment as an assistant professor at the University of Tübingen, where Hans Geiger was the professor of experimental physics. One of the first laws passed by the new National Socialist government was the Law for the Restoration of the Professional Civil Service. Due to his Jewish background, Bethe was dismissed from his job at the University, which was a government post. Geiger refused to help, but Sommerfeld immediately gave Bethe back his fellowship at Munich. Sommerfeld spent much of the summer term of 1933 finding places for Jewish students and colleagues.\n\nBethe left Germany in 1933, moving to England after receiving an offer for a position as lecturer at the University of Manchester for a year through Sommerfeld's connection to William Lawrence Bragg. He moved in with his friend Rudolf Peierls and Peierls' wife Genia. Peierls was a fellow German physicist who had also been barred from academic positions in Germany because his parents were Jewish. This meant that Bethe had someone to speak to in German, and did not have to eat English food. Their relationship was professional as well as personal. Peierls aroused Bethe's interest in nuclear physics. After James Chadwick and Maurice Goldhaber discovered the photodisintegration of deuterium, Chadwick challenged Bethe and Peierls to come up with a theoretical explanation of this phenomenon. This they did on the four-hour train ride from Cambridge back to Manchester. Bethe would investigate further in the years ahead.\n\nIn 1933, the physics department at Cornell was looking for a new theoretical physicist, and Lloyd Smith strongly recommended Bethe. This was supported by Bragg, who was visiting Cornell at the time. In August 1934, Cornell offered Bethe a position as an acting assistant professor. Bethe had already accepted a fellowship for a year to work with Nevill Mott at the University of Bristol for a semester, but Cornell agreed to let him start in the spring of 1935. Before leaving for the United States, he visited the Niels Bohr Institute in Copenhagen in September 1934, where he proposed to Hilde Levi, who accepted. However, the match was opposed by Bethe's mother, who did not want him marrying a Jewish girl, despite being Jewish herself, and Bethe broke off their engagement a few days before their wedding date in December. Niels Bohr and James Franck were so shocked by Bethe's behavior that he was not invited to the Institute again until after World War II.\n\nBethe arrived in the United States in February 1935, and joined the faculty at Cornell University on a salary of $3,000. Bethe's appointment was part of a deliberate effort on the part of the new head of its physics department, Roswell Clifton Gibbs, to move into nuclear physics. Gibbs had hired Stanley Livingston, who had worked with Ernest Lawrence, to build a cyclotron at Cornell. To complete the team, Cornell needed an experimentalist, and, on the advice of Bethe and Livingston, recruited Robert Bacher. Bethe received requests to visit Columbia University from Isidor Isaac Rabi, Princeton University from Edward Condon, University of Rochester from Lee DuBridge, Purdue University from Karl Lark-Horovitz, the University of Illinois at Urbana–Champaign from Francis Wheeler Loomis, and Harvard University from John Hasbrouck Van Vleck. Gibbs moved to prevent Bethe from being poached by having him appointed as a regular assistant professor in 1936, with an assurance that promotion to professor would soon follow.\n\nTogether with Bacher and Livingston, Bethe published a series of three articles, which summarized most of what was known on the subject of nuclear physics until that time, an account that became informally known as \"Bethe's Bible\", and remained the standard work on the subject for many years. In this account, he also continued where others left off, filling in gaps in the older literature. Loomis offered Bethe a full professorship at the University of Illinois at Urbana–Champaign, but Cornell matched the offer, and the salary of $6,000. He wrote to his mother:\n\nOn March 17, 1938, Bethe attended the Carnegie Institute and George Washington University's fourth annual Washington Conference of Theoretical Physics. There were only 34 invited attendees, but they included Gregory Breit, Subrahmanyan Chandrasekhar, George Gamow, Donald Menzel, John von Neumann, Bengt Strömgren, Edward Teller and Merle Tuve. Bethe initially declined the invitation to attend, because the conference's topic, stellar energy generation, did not interest him, but Teller persuaded him to come. At the conference, Strömgren detailed what was known about the temperature, density and chemical composition of the Sun, and challenged the physicists to come up with an explanation. Gamow and Carl Friedrich von Weizsäcker had proposed in a 1937 paper that the Sun's energy was the result of a proton–proton chain reaction:\n\nBut this did not account for the observation of elements heavier than helium. By the end of the conference, Bethe, working in collaboration with Charles Critchfield, had come up with a series of subsequent nuclear reactions that explained how the Sun shines:\nThat this did not explain the processes in heavier stars was not overlooked. At the time there were doubts about whether the proton–proton cycle described the processes in the Sun, but more recent measurements of the Sun's core temperature and luminosity show that it does. When he returned to Cornell, Bethe studied the relevant nuclear reactions and reaction cross sections, leading to his discovery of the carbon-nitrogen-oxygen cycle (CNO cycle):\n\nThe two papers, one on the proton–proton cycle, co-authored with Critchfield, and the other on the carbon-oxygen-nitrogen (CNO) cycle, were sent to the \"Physical Review\" for publication. After \"Kristallnacht\", Bethe's mother had become afraid to remain in Germany. Taking advantage of her Strasbourg origin, she was able to emigrate to the United States in June 1939 on the French quota, rather than the German one, which was full. Bethe's graduate student Robert Marshak noted that the New York Academy of Sciences was offering a $500 prize for the best unpublished paper on the topic of solar and stellar energy. So Bethe, in need of $250 to release his mother's furniture, withdrew the CNO cycle paper and sent it in to the New York Academy of Sciences. It won the prize, and Bethe gave Marshak $50 finder's fee and used $250 to release his mother's furniture. The paper was subsequently published in the \"Physical Review\" in March. It was a breakthrough in the understanding of the stars, and would win Bethe the Nobel Prize in Physics in 1967. In 2002, at age 96, Bethe sent a handwritten note to John N. Bahcall congratulating him on the use of solar neutrino observations to show that the CNO cycle accounts for about 7% of the Sun's energy; the neutrino observations had started with Raymond Davis Jr., whose experiment was based on Bahcall's calculations and encouragement, and led to Davis's receiving a share of the 2002 Nobel Prize.\n\nBethe married Rose Ewald, the daughter of Paul Ewald, on September 13, 1939, in a simple civil ceremony. They had two children, Henry and Monica. (Henry was a contract bridge expert and former husband of Kitty Munson Cooper.) He became a naturalized citizen of the United States in March 1941. Writing to Sommerfeld in 1947, Bethe confided that \"I am much more at home in America than I ever was in Germany. As if I was born in Germany only by mistake, and only came to my true homeland at 28.\"\n\nWhen the Second World War began, Bethe wanted to contribute to the war effort, but was unable to work on classified projects until he became a citizen. Following the advice of the Caltech aerodynamicist Theodore von Kármán, Bethe collaborated with his friend Teller on a theory of shock waves which are generated by the passage of a projectile through a gas. Bethe considered it one of their most influential papers. He also worked on a theory of armor penetration, which was immediately classified by the Army, making it inaccessible to Bethe, who was not an American citizen at the time.\n\nAfter receiving security clearance in December 1941, Bethe joined the MIT Radiation Laboratory, where he invented the Bethe-hole directional coupler, which is used in microwave waveguides such as those used in radar sets. In Chicago in June 1942, and then at the University of California, Berkeley, in July, he participated in a series of meetings at the invitation of Robert Oppenheimer, which discussed the first designs for the atomic bomb. They went over the preliminary calculations by Robert Serber, Stan Frankel, and others, and discussed the possibilities of using uranium-235 and plutonium. Teller then raised the prospect of a thermonuclear device, Teller's \"Super\" bomb. At one point Teller asked if the nitrogen in the atmosphere could be set alight. It fell to Bethe and Emil Konopinski to perform the calculations to prove that this could not occur. \"The fission bomb had to be done,\" he later recalled, \"because the Germans were presumably doing it.\"\n\nWhen Oppenheimer was put in charge of forming a secret weapons design laboratory, Los Alamos, he appointed Bethe director of the T (Theoretical) Division, the laboratory's smallest but most prestigious division. This move irked the equally qualified but more difficult to manage Teller and Felix Bloch, who had coveted the job. A series of disagreements between Bethe and Teller between February and June 1944 over the relative priority of Super research led to Teller's group being removed from T Division and placed directly under Oppenheimer. In September it became part of Fermi's new F Division.\n\nBethe's work at Los Alamos included calculating the critical mass and efficiency of uranium-235 and the multiplication of nuclear fission in an exploding atomic bomb. Along with Richard Feynman, he developed a formula for calculating the bomb's explosive yield. After August 1944, when the laboratory was reorganized and reoriented to solve the problem of the implosion of the plutonium bomb, Bethe spent much of his time studying the hydrodynamic aspects of implosion, a job which he continued into 1944. In 1945, he worked on the neutron initiator, and later on radiation propagation from an exploding atomic bomb. The Trinity nuclear test validated the accuracy of T Division's results. When it was detonated in the New Mexico desert on July 16, 1945, Bethe's immediate concern was for its efficient operation, and not its moral implications. He is reported to have commented: \"I am not a philosopher.\"\n\nAfter the war, Bethe argued that a crash project for the hydrogen bomb should not be attempted, though after President Harry Truman announced the beginning of such a project, and the outbreak of the Korean War, Bethe signed up and played a key role in the weapon's development. Though he would see the project through to its end, Bethe hoped that it would be impossible to create the hydrogen bomb. He would later remark in 1968 on the apparent contradiction in his stance, having first opposed the development of the weapon and later helping to create it:\n\nAs for his own role in the project, and its relation to the dispute over who was responsible for the design, Bethe later said that:\n\nIn 1954, Bethe testified on behalf of J. Robert Oppenheimer during the Oppenheimer security hearing. Specifically, Bethe argued that Oppenheimer's stances against developing the hydrogen bomb in the late 1940s had not hindered its actual development, a topic which was seen as a key motivating factor behind the hearing. Bethe contended that the developments which led to the successful Teller–Ulam design were a matter of serendipity and not a question of manpower or logical development of previously existing ideas. During the hearing, Bethe and his wife also tried hard to convince Edward Teller against testifying. However, Teller did not agree, and his testimony played a major role in the revocation of Oppenheimer's security clearance. While Bethe and Teller had been on very good terms during the prewar years, the conflict between them during the Manhattan Project, and especially during the Oppenheimer episode, permanently marred their relationship.\n\nAfter the war ended, Bethe returned to Cornell. In June 1947, he participated in the Shelter Island Conference. Sponsored by the National Academy of Sciences and held at the Ram's Head Inn on Shelter Island, New York, the conference on the \"Foundations of Quantum Mechanics\" was the first major physics conference held since the war. It was a chance for American physicists to come together, pick up where they had left off before the war, and establish the direction of post-war research.\n\nA major talking point at the conference was the discovery by Willis Lamb and his graduate student Robert Retherford shortly before the conference began that one of the two possible quantum states of hydrogen atoms had slightly more energy than predicted by the Paul Dirac's theory; this became known as the Lamb shift. Oppenheimer and Weisskopf suggested that this was a result of quantum fluctuations of the electromagnetic field. Pre-war quantum electrodynamics (QED) gave absurd, infinite values for this; but the Lamb shift showed that it was both real and finite. Hans Kramers proposed renormalization as a solution, but no one knew how to do the calculation.\n\nBethe managed to work it out on the train from New York to Schenectady. He arrived at a value of 1040 MHz, extremely close to that obtained experimentally by Lamb and Retherford. He did so by realising that it was a non-relativistic process, which greatly simplified the calculations. His paper, published in the \"Physical Review\" in August 1947 was only two pages long and contained just 12 mathematical equations, but was enormously influential. Hitherto, it had been assumed that the infinities meant that QED was fundamentally flawed, and that a new, radical theory was required. Bethe demonstrated that this was not necessary.\nOne of Bethe's most famous papers is one he never wrote: the 1948 Alpher–Bethe–Gamow paper. George Gamow added Bethe's name (in absentia) without consulting him, knowing that Bethe would not mind, and against Ralph Alpher's wishes. This was apparently a reflection of Gamow's sense of humor, wanting to have a paper title that would sound like the first three letters of the Greek alphabet. As one of the \"Physical Review\"s reviewers, Bethe saw the manuscript and struck out the words \"in absentia\".\nBethe believed that the atomic nucleus was like a quantum liquid drop. He investigated the nuclear matter problem by considering the work done by Keith Brueckner on perturbation theory. Working with Jeffrey Goldstone, he produced a solution for the case where there was an infinite hard-core potential. Then, working with Baird Brandow and Albert Petschek, he came up with an approximation that converted the scattering equation into an easily solved differential equation. This then led him to the Bethe-Faddeev equation, a generalisation of Ludvig Faddeev's approach to three-body scattering. He then used these techniques to examine the neutron stars, which have densities similar to those of nuclei.\n\nBethe continued to do research on supernovae, neutron stars, black holes, and other problems in theoretical astrophysics into his late nineties. In doing this, he collaborated with Gerald E. Brown of Stony Brook University. In 1978, Brown proposed that they collaborate on supernovae. These were reasonably well understood by this time, but the calculations were still a problem. Using techniques honed from decades of working with nuclear physics, and some experience with calculations involving nuclear explosions, Bethe tackled the problems involved in stellar gravitational collapse, and the way in which various factors affected a supernova explosion. Once again, he was able to reduce the problem to a set of differential equations, and solve them.\n\nAt age 85, Bethe wrote an important article about the solar neutrino problem, in which he helped establish the conversion mechanism for electron neutrinos into muon neutrinos proposed by Stanislav Mikheyev, Alexei Smirnov and Lincoln Wolfenstein to explain a vexing discrepancy between theory and experiment. Bethe argued that physics beyond the Standard Model was required to understand the solar neutrino problem, because it assumed that neutrinos have no mass, and therefore cannot metamorphosize into each other; whereas the MSW effect required this to occur. Bethe hoped that corroborating evidence would be found by the Sudbury Neutrino Observatory (SNO) in Ontario, Canada, by his 90th birthday, but he did not get the call from SNO until June 2001, when he was nearly 95.\n\nIn 1996, Kip Thorne approached Bethe and Brown about LIGO, the Laser Interferometer Gravitational-Wave Observatory, designed to detect the gravitational waves from merging neutron stars and black holes. Since Bethe and Brown were good at calculating things that could not be seen, could they look at the mergers? The 90-year-old Bethe quickly became enthused, and soon began the required calculations. The result was a 1998 paper on the \"Evolution of Binary Compact Objects Which Merge\", which Brown regarded as the best that the two produced together.\n\nIn 1968, Bethe, along with IBM physicist Richard Garwin, published an article criticising in detail the anti-ICBM defense system proposed by the Department of Defense. The two physicists described in the article that nearly any measure taken by the US would be easily thwarted with the deployment of relatively simple decoys. Bethe was one of the primary voices in the scientific community behind the signing of the 1963 Partial Test Ban Treaty prohibiting further atmospheric testing of nuclear weapons.\n\nDuring the 1980s and 1990s, Bethe campaigned for the peaceful use of nuclear energy. After the Chernobyl disaster, Bethe was part of a committee of experts that analysed the incident. They concluded that the reactor suffered from a fundamentally faulty design and human error also had significantly contributed to the accident. \"My colleagues and I established,\" he explained \"that the Chernobyl disaster tells us about the deficiencies of the Soviet political and administrative system rather than about problems with nuclear power.\" Throughout his life Bethe remained a strong advocate for electricity from nuclear energy, which he described in 1977 as \"a necessity, not merely an option.\"\n\nIn the 1980s he and other physicists opposed the Strategic Defense Initiative missile system conceived by the Ronald Reagan administration. In 1995, at the age of 88, Bethe wrote an open letter calling on all scientists to \"cease and desist\" from working on any aspect of nuclear weapons development and manufacture. In 2004, he joined 47 other Nobel laureates in signing a letter endorsing John Kerry for President of the United States as someone who would \"restore science to its appropriate place in government\".\n\nHistorian Gregg Herken wrote:\n\nBethe's hobbies included a passion for stamp-collecting. He loved the outdoors, and was an enthusiastic hiker all his life, exploring the Alps and the Rockies. He died in his home in Ithaca, New York on March 6, 2005 of congestive heart failure. He was survived by his wife Rose and two children. At the time of his death, he was the John Wendell Anderson Professor of Physics, Emeritus at Cornell University.\n\nBethe received numerous honors and awards in his lifetime and afterwards. He became a Fellow of the American Academy of Arts and Sciences in 1947, and that year was received the National Academy of Sciences's Henry Draper Medal. He was awarded the Max Planck Medal in 1955, the Franklin Medal in 1959, the Royal Astronomical Society's Eddington Medal and the United States Atomic Energy Commission's Enrico Fermi Award in 1961, the Rumford Prize in 1963, the Nobel Prize in Physics in 1967, the National Medal of Science in 1975, Oersted Medal in 1993, the Bruce Medal in 2001, and the Benjamin Franklin Medal for Distinguished Achievement in the Sciences by the American Philosophical Society posthumously in 2005.\n\nBethe was elected Foreign Member of the Royal Society (ForMemRS) in 1957, and he gave the 1993 Bakerian Lecture at the Royal Society on the Mechanism of Supernovae.\nIn 1978 he was elected a Member of the German Academy of Sciences Leopoldina.\n\nCornell named the third of five new residential colleges, each of which is named after a distinguished former member of the Cornell faculty, Hans Bethe House after him, as was the Hans Bethe Center, 322 4th St. NE, Washington, DC, home to the Council for a Livable World, where Bethe was a longtime board member, and the Bethe Center for Theoretical Physics at University of Bonn in Germany. He also has an asteroid, 30828 Bethe, that was discovered in 1990 named after him, as was the American Physical Society's Hans Bethe Prize.\n\n\n\n"}
{"id": "54569058", "url": "https://en.wikipedia.org/wiki?curid=54569058", "title": "Helium Systems", "text": "Helium Systems\n\nHelium, Inc (or simply Helium) is an Internet-of-Things developer platform founded in 2013 by Amir Haleem, Sean Carey and Shawn Fanning. Helium has received a total of $38 million in seed, series A, and series B funding. Prominent investors include: Mark Benioff, SV Angel, FirstMark, Khosla Ventures, and GV (formerly Google Ventures).\n\nHelium's 'Atom' Radio module, which forms the core hardware component of its networking capability, contains a dual-band, 802.15.4 modem, an Atmel ATECC508A security key component. \n\nThe company previously developed a product that would monitor refrigeration systems and report out-of-range temperatures wirelessly. However, they pivoted and in July 2017 announced a new offering to provide a hardware, and software platform for developers to build IoT applications. The new offering was positioned as a way for developers to streamline the ability to prototype, deploy and scale a long-range wireless network that connects thousands of end devices, and gives companies a simple way to deliver data from device to the cloud and application layer.\n\nSeed round: $2,800,000, Nov 1, 2013\n\nSeries A: $16,000,000, Dec 9, 2014\n\nSeries B: $20,000,000 Apr 25, 2016\n\nHelium.com was founded in 2006 as a Yahoo! Answers-style website created from user generated content paid on a revenue sharing model. Over time the site evolved from short answers to stand-alone articles, and began to offer writers the opportunity to earn from selling articles as well as passive income. The sale of articles to businesses and publishers is now its primary business, and the revenue share arm has closed to new members, effective May 21, 2014. The sites which display revenue share articles will be closed down on December 15, 2014.\n\nThe site was acquired by Helium Systems and is defunct now.\n\nBeginning life as HeliumKnowledge.com, Helium.com was launched in October 2006. Its original format was similar to Yahoo! Answers, with contributors answering questions with short, informative answers. Over time, the length of articles increased and they became stand-alone articles instead of answers to questions.\n\nHelium originally pitched itself as an alternative to Wikipedia. For writers, Helium's marketing focused on the opportunity to earn passive income in perpetuity. Then Helium began to enter partnerships with publishers, enabling writers to compete to sell articles to those publishers.\n\nR.R. Donnelley purchased Helium.com in 2011 to expand its online presence.\n"}
{"id": "4207168", "url": "https://en.wikipedia.org/wiki?curid=4207168", "title": "ICON Computing", "text": "ICON Computing\n\nICON Computing was a 1990s software service company specializing in object technology, founded by Desmond D'Souza, and was part of the UML Partners consortium. They were acquired by Platinum Technology in 1998. \n\n"}
{"id": "42812494", "url": "https://en.wikipedia.org/wiki?curid=42812494", "title": "Information Systems Associates FZE", "text": "Information Systems Associates FZE\n\nInformation Systems Associates FZE (commonly known as ISA) is an aviation software house serving airlines, airports and travel agents. ISA is a privately owned company formed in 2005. The company produces a computer reservation system 'under the name \"aeroMART SELL\", formerly AccelAero. The company is headquartered in Sharjah, UAE and the development center is based in Colombo, Sri Lanka. Software produced includes airline, airport, e-business suites, revenue management, crew & ground operations, and holidays etc.\n\nRecently World Travel Award™ has announced ISA as the “World's Leading PSS Provider 2015” at the red-carpet Gala Ceremony held at Mazagan Beach & Golf Resort - Morocco.\n\nISA software is Java based and totally web-based solutions.\n\nUnder ACCELaero brand there are three main product suites, including:\n\naeroMART\n\naeroPORT\n\naeroLINE\n\nCustomers include\n\n"}
{"id": "18306709", "url": "https://en.wikipedia.org/wiki?curid=18306709", "title": "Instruments used in dermatology", "text": "Instruments used in dermatology\n\nInstruments used specially in dermatology are as follows:\n"}
{"id": "11213016", "url": "https://en.wikipedia.org/wiki?curid=11213016", "title": "Juris Zarins", "text": "Juris Zarins\n\nJuris Zarins (Zariņš) (born 1945, in Germany) is an American-Latvian archaeologist and professor at Missouri State University, who specializes in the Middle East.\n\nZarins is ethnically Latvian, but was born in Germany at the end of the Second World War. His parents emigrated to the United States soon after he was born. He graduated from high school in Lincoln, Nebraska in 1963 and earned a B.A. in anthropology from the University of Nebraska in 1967. He served in the U.S. Army in Vietnam before completing his Ph.D. in Ancient Near Eastern Languages and Archaeology at the University of Chicago in 1974. He then served as archaeological adviser to the Department of Antiquities of the Kingdom of Saudi Arabia before coming to Missouri State in 1978.\n\nHe discussed Ubar in a 1996 NOVA interview saying \"There's a lot of confusion about that word. If you look at the classical texts and the Arab historical sources, Ubar refers to a region and a group of people, not to a specific town. People always overlook that. It's very clear on Ptolemy's second century map of the area. It says in big letters \"Iobaritae\" And in his text that accompanied the maps, he's very clear about that. It was only the late medieval version of \"One Thousand and One Nights\", in the fourteenth or fifteenth century, that romanticised Ubar and turned it into a city, rather than a region or a people.\"\n\nIn 2007, following further research and excavation, a paper partly authored by him offered a different view of the meaning of the name \"Ubar\":\n\nZarins has published many articles on a number of topics concerning the archaeology of the Near East, which include the domestication of the horse, early pastoral nomadism, and the obsidian, indigo, and frankincense trades. He received an Excellence in Research Award from Missouri State in 1988. He has proposed that the Semitic languages arose as a result of a circum Arabian nomadic pastoral complex, which developed in the period of the desiccation of climates at the end of the pre-pottery phase in the Ancient Near East.\n\nZarins argued that the Garden of Eden was situated at the head of the Persian Gulf, where the Tigris and Euphrates Rivers run into the sea, from his research on this area using information from many different sources, including LANDSAT images from space. In this theory, the Bible's Gihon River would correspond with the Karun River in Iran, and the Pishon River would correspond to the Wadi Batin river system that once drained the now dry, but once quite fertile central part of the Arabian Peninsula. His suggestion about the Pishon River is supported by James A. Sauer (1945–1999) formerly of the American Center of Oriental Research although strongly criticized by the archaeological community.\n\n"}
{"id": "40959323", "url": "https://en.wikipedia.org/wiki?curid=40959323", "title": "Leersia hexandra", "text": "Leersia hexandra\n\nLeersia hexandra is a species of grass known by the common names southern cutgrass, clubhead cutgrass, and swamp rice grass. It has a pantropical distribution. It is also an introduced species in many regions, sometimes becoming invasive, and it is an agricultural weed of various crops, especially rice. It is also cultivated as a forage for livestock.\n\nThis species is a perennial grass growing from rhizomes and stolons. The hollow stems are decumbent and creeping and root easily where their nodes contact the substrate. They produce erect shoots that can exceed one meter tall. It is an aquatic or semi-aquatic grass, and the erect stem parts may float in water. These stems can grow densely in aquatic habitat and become matted, forming what are often referred to as \"carpets\".\n\nThe leaf sheath has a fleshy base covered in white hairs and the ligule can be stiff and dry, becoming \"papery\". The leaves have sharp-pointed blades up to 30 centimeters long which are flat or rolled, the edges sometimes rolling at night or when the blade dries. The blades are sometimes hairless, but are usually coated in very rough hairs, making them so rough to the touch that they are \"unpleasant to handle\". They also have very sharp edges, and the midrib has backward-facing, spiny hairs that give it a cutting edge. The \"retrorsely spinulose midrib of the leaf can inflict most painful lacerations\".\n\nThe panicle is narrow or spreading and erect or nodding, and up to about 12 centimeters long. The branches are almost fully lined with overlapping spikelets each up to half a centimeter long. The spikelets may be greenish or purplish in color, or sometimes tinged with orange or brick red. They are surrounded by white or purplish bracts that have characteristic comb-like hairs along their greenish nerves. The flower has six stamens. After the spikelets fall, the panicle branches have a zig-zag shape. Fertile seed is rarely produced and the grass commonly reproduces vegetatively by sprouting from the rhizome or the nodes on the stem. Large stands of the grass are often clones.\n\nThis grass looks very similar to rice and other species of the genus \"Oryza\". It is a member of the rice tribe Oryzeae and sometimes grows in rice paddies.\n\nThis plant grows in shallow freshwater habitat and on wet and moist land. It can be found in marshes, swamps, ponds, irrigation ditches, flooded rice fields, and on other moist agricultural land and floodplains. It is mostly tropical, but it can grow in some temperate climates. It can persist for a time in drier conditions during drought.\n\nThe grass provides food and shelter for animals. Many water birds feed on it. In Tanzania it is a dominant plant in the swamps where the shoebill (\"Balaeniceps rex\") and wattled crane (\"Bugeranus carunculatus\") build their nests. On the Llanos of Colombia and Venezuela it is the second most important food of the resident herds of capybara (\"Hydrochoerus hydrochaeris\"), composing up to 29% of their diet.\n\nIt is one of the two host plants of the brown planthopper (\"Nilaparvata lugens\"), the other being cultivated rice. While it has been observed on many other plant species, it can only complete its life cycle on cutgrass or rice. There are two strains of the planthopper, one that only lays eggs on rice and one that favors cutgrass; the rice strain does not effectively reproduce on cutgrass and vice versa, even when sympatric. While they can be crossed in laboratory tests, the two strains do not interbreed in the wild.\n\nThe grass is a weed of several crops, including tea, rubber, maize, and sugarcane, but especially rice. It is a relative of the rice plant and it thrives in paddy fields. Its vegetation \"carpets\" clog irrigation waterways, causing flooding and erosion. It hosts many rice pests, including the brown planthopper, the green planthopper (\"Nilaparvata bakeri\"), the green rice leafhopper (\"Nephotettix malayanus\"), the rice gall midge (\"Orseolia oryzae\"), and the moth \"Helcystogramma arotraeum\". It hosts the rice stem nematode, which causes ufra disease of rice. It is susceptibe to many plant viruses that infect rice plants, such as rice grassy stunt virus, rice transitory yellowing virus, and rice tungro virus. It is susceptible to fungi such as pathogenic \"Xanthomonas oryzae\", which causes leaf blight of rice, and \"Cochliobolus miyabeanus\", which causes brown spot.\n\nDespite its sharp leaf edges, the grass is palatable to cattle and it is maintained as a pasture grass on swampy land and cut for hay.\n\nThis species is a hyperaccumulator of heavy metals, with the ability to take up large amounts of chromium, copper, and nickel from water and soil. Its ability to absorb chromium in particular has been described as \"extraordinary\". It is considered to be a potential agent of phytoremediation in efforts to clean up metal-contaminated soils and water. Targets could include industrial wastewater, such as that discharged from electroplating factories, and the contaminated soils around such facilities.\n"}
{"id": "25564921", "url": "https://en.wikipedia.org/wiki?curid=25564921", "title": "Link (Indonesia)", "text": "Link (Indonesia)\n\nLink is an interbank network in Indonesia. It connects four state owned banks. The banks are Bank Mandiri, Bank Tabungan Negara, BNI 46, and Bank Rakyat Indonesia. This network is owned by State-owned Banks Association (HIMBARA). This network provides cash withdrawal and inquiry services in their network.\n"}
{"id": "47047888", "url": "https://en.wikipedia.org/wiki?curid=47047888", "title": "Low-level design", "text": "Low-level design\n\nLow-level design (LLD) is a component-level design process that follows a step-by-step refinement process. This process can be used for designing data structures, required software architecture, source code and ultimately, performance algorithms. Overall, the data organization may be defined during requirement analysis and then refined during data design work. Post-build, each component is specified in detail.\n\nThe LLD phase is the stage where the actual software components are designed.\n\nDuring the detailed phase the logical and functional design is done and the design of application structure is developed during the high-level design phase.\n\nA design is the order of a system that connects individual components. Often, it can interact with other systems. Design is important to achieve high reliability, low cost, and good maintain-ability.\nWe can distinguish two types of program design phases:\n\n\nStructured flow charts and HIPO diagrams typify the class of software design tools and these provide a high-level overview of a program. The advantages of such a design tool is that it yields a design specification that is understandable to nonprogrammers and it provides a good pictorial display of the module dependencies.\n\nA disadvantage is that it may be difficult for software developers to go from graphic-oriented representation of software design to implementation. Therefore, it is necessary to provide little insight into the algorithmic structure describing procedural steps to facilitate the early stages of software development (generally using PDLs).\n\nThe goal of LLD or a low-level design document (LLDD) is to give the internal logical design of the actual program code. Low-level design is created based on the high-level design. LLD describes the class diagrams with the methods and relations between classes and program specs. It describes the modules so that the programmer can directly code the program from the document.\n\nA good low-level design document makes the program easy to develop when proper analysis is utilized to create a low-level design document. The code can then be developed directly from the low-level design document with minimal debugging and testing.\nOther advantages include lower cost and easier maintenance.\n\nYou can find an example of HLD here: Sample HLD, after iterations that was initially developed as LLD:Sample LLD.\n"}
{"id": "49474389", "url": "https://en.wikipedia.org/wiki?curid=49474389", "title": "MOOP (electrical safety)", "text": "MOOP (electrical safety)\n\nMeans Of Operator Protection, or MOOP, is a concept introduced in the standard for medical electrical equipment IEC 60601-1.\n"}
{"id": "34327456", "url": "https://en.wikipedia.org/wiki?curid=34327456", "title": "McCurdy’s Armor", "text": "McCurdy’s Armor\n\nMcCurdy’s Armor is a patented portable armored wall system that protects against small arms, mortars, rockets, artillery, armor-piercing rounds and suicide bombs. Constructed of aluminum frames that unfold and connect to one another using steel connector pins, McCurdy’s Armor can be deployed into a full guard post by a three-person team in less than ten minutes, using no hand tools or heavy equipment. The system can be dismantled and redeployed in a similar amount of time. McCurdy’s Armor includes bullet proof glass windows that provide full situational awareness and, when necessary, can be opened and used as gun ports.\n\nMcCurdy’s Armor is made by Dynamic Defense Materials, a defense manufacturing company that was founded in 2004 by Robert Lipinski. Dynamic Defense Materials is headquartered in Marlton, New Jersey. The company uses manufacturers in Pennsylvania, New York, Ohio and Texas to produce McCurdy’s Armor.\n\n\nOriginally called Evaloch, McCurdy’s Armor was officially renamed in 2009 to honor LCpl. Ryan S. McCurdy, who was mortally wounded from a sniper attack in Fallujah, Iraq on January 5, 2006. At the time of the attack, LCpl. McCurdy was on duty with Cpl. Clifton Trotter, who was shot in the neck and severely injured. LCpl. McCurdy was killed by sniper fire when he left his position in an attempt to pull Cpl. Trotter to safety. Cpl. Trotter survived the attack.\n\nJoe Dimond, General Manager of Dynamic Defense Materials and a ten-year Marine veteran, served with LCpl. McCurdy in Fallujah, Iraq. In honor of LCpl. McCurdy, Dynamic Defense Materials donates a percentage of all sales to the Ryan McCurdy Fund, a charity that has been set up in LCpl. McCurdy’s name to provide a safe place for children to learn and grow in Baton Rouge, LA.\n\n"}
{"id": "12907435", "url": "https://en.wikipedia.org/wiki?curid=12907435", "title": "Modified Harvard architecture", "text": "Modified Harvard architecture\n\nThe modified Harvard architecture is a variation of the Harvard computer architecture that allows the contents of the instruction memory to be accessed as if it were data. Most modern computers that are documented as Harvard architecture are, in fact, modified Harvard architecture.\n\nThe original Harvard architecture computer, the Harvard Mark I, employed entirely separate memory systems to store instructions and data. The CPU fetched the next instruction and loaded or stored data simultaneously and independently. This is in contrast to a von Neumann architecture computer, in which both instructions and data are stored in the same memory system and (without the complexity of a CPU cache) must be accessed in turn. The physical separation of instruction and data memory is sometimes held to be the distinguishing feature of modern Harvard architecture computers. With microcontrollers (entire computer systems integrated onto single chips), the use of different memory technologies for instructions (e.g. flash memory) and data (typically read/write memory) in von Neumann machines is becoming popular. The true distinction of a Harvard machine is that instruction and data memory occupy different address spaces. In other words, a memory address does not uniquely identify a storage location (as it does in a von Neumann machine); it is also necessary to know the memory space (instruction or data) to which the address belongs.\n\nA computer with a von Neumann architecture has the advantage over \"pure\" Harvard machines in that code can also be accessed and treated the same as data, and vice versa. This allows, for example, data to be read from disk storage into memory and then executed as code, or self-optimizing software systems using technologies such as just-in-time compilation to write machine code into their own memory and then later execute it. Another example is self-modifying code, which allows a program to modify itself. A disadvantage of these methods are issues with executable space protection, which increase the risks from malware and software defects. In addition, in these systems it is notoriously difficult to document code flow, and also can make debugging much more difficult.\n\nAccordingly, some pure Harvard machines are specialty products. Most modern computers instead implement a \"modified\" Harvard architecture. Those modifications are various ways to loosen the strict separation between code and data, while still supporting the higher performance concurrent data and instruction access of the Harvard architecture.\n\nThe most common modification builds a memory hierarchy with a CPU cache separating instructions and data. This unifies all except small portions of the data and instruction address spaces, providing the von Neumann model. Most programmers never need to be aware of the fact that the processor core implements a (modified) Harvard architecture, although they benefit from its speed advantages. Only programmers who write instructions into data memory need to be aware of issues such as cache coherency.\n\nAnother change preserves the \"separate address space\" nature of a Harvard machine, but provides special machine operations to access the contents of the instruction memory as data. Because data is not directly executable as instructions, such machines are not always viewed as \"modified\" Harvard architecture:\n\nA few Harvard architecture processors, such as the MAXQ, can execute instructions fetched from any memory segment – unlike the original Harvard processor, which can only execute instructions fetched from the program memory segment.\nSuch processors, like other Harvard architecture processors – and unlike pure von Neumann architecture – can read an instruction and read a data value simultaneously, if they're in separate memory segments, since the processor has (at least) two separate memory segments with independent data buses.\nThe most obvious programmer-visible difference between this kind of modified Harvard architecture and a pure von Neumann architecture is that – when executing an instruction from one memory segment – the same memory segment cannot be simultaneously accessed as data.\n\nThree characteristics may be used to distinguish modified Harvard machines from pure Harvard and von Neumann machines:\nFor pure Harvard machines, there is an address \"zero\" in instruction space that refers to an instruction storage location and a separate address \"zero\" in data space that refers to a distinct data storage location. By contrast, von Neumann and split-cache modified Harvard machines store both instructions and data in a single address space, so address \"zero\" refers to only one location and whether the binary pattern in that location is interpreted as an instruction or data is defined by how the program is written. However, just like pure Harvard machines, instruction-memory-as-data modified Harvard machines have separate address spaces, so have separate addresses \"zero\" for instruction and data space, therefore this does not distinguish this type of modified Harvard machines from pure Harvard machines.\nThis is the point of pure or modified Harvard machines, and why they co-exist with the more flexible and general von Neumann architecture: separate memory pathways to the CPU allow instructions to be fetched and data to be accessed at the same time, improving throughput. The pure Harvard machines have separate pathways with separate address spaces. Split-cache modified Harvard machines have such separate access paths for CPU caches or other tightly coupled memories, but a unified address space covers the rest of the memory hierarchy. A von Neumann processor has only that unified address space. From a programmer's point of view, a modified Harvard processor in which instruction and data memories share an address space is usually treated as a von Neumann machine until cache coherency becomes an issue, as with self-modifying code and program loading. This can be confusing, but such issues are usually visible only to systems programmers and integrators. Other modified Harvard machines are like pure Harvard machines in this regard.\nThe original Harvard machine, the Mark I, stored instructions on a punched paper tape and data in electro-mechanical counters. This, however, was entirely due to the limitations of technology available at the time. Today a Harvard machine such as the PIC microcontroller might use 12-bit wide flash memory for instructions, and 8-bit wide SRAM for data. In contrast, a von Neumann microcontroller such as an ARM7TDMI, or a modified Harvard ARM9 core, necessarily provides uniform access to flash memory and SRAM (as 8 bit bytes, in those cases).\n\nOutside of applications where a cacheless DSP or microcontroller is required, most modern processors have a CPU cache which partitions instruction and data.\n\nThere are also processors which are Harvard machines by the most rigorous definition (that program and data memory occupy different address spaces), and are only \"modified\" in the weak sense that there are operations to read and/or write program memory as data. For example, LPM (Load Program Memory) and SPM (Store Program Memory) instructions in the Atmel AVR implement such a modification. Similar solutions are found in other microcontrollers such as the PIC and Z8Encore!, many families of digital signal processors such as the TI C55x cores, and more. Because instruction execution is still restricted to the program address space, these processors are very unlike von Neumann machines.\n\nHaving separate address spaces creates certain difficulties in programming with high-level languages that do not directly support the notion that tables of read-only data might be in a different address space from normal writable data (and thus need to be read using different instructions). The C programming language can support multiple address spaces either through non-standard extensions or through the now standardized extensions to support embedded processors.\n\n"}
{"id": "44524545", "url": "https://en.wikipedia.org/wiki?curid=44524545", "title": "Optoelectrowetting", "text": "Optoelectrowetting\n\nOptoelectrowetting (OEW) is a method of liquid droplet manipulation used in microfluidics applications. This technique builds on the principle of electrowetting, which has proven useful in liquid actuation due to fast switching response times and low power consumption. Where traditional electrowetting runs into challenges, however, such as in the simultaneous manipulation of multiple droplets, OEW presents a lucrative alternative that is both simpler and cheaper to produce. OEW surfaces are easy to fabricate, since they require no lithography, and have real-time, reconfigurable, large-scale manipulation control, due to its reaction to light intensity.\n\nThe traditional electrowetting mechanism has been receiving increasing interest due to its ability to control tension forces on a liquid droplet. As surface tension acts as the dominant liquid actuation force in nano-scale applications, electrowetting has been used to modify this tension at the solid-liquid interface through the application of an external voltage. The applied electric field causes a change in the contact angle of the liquid droplet, and in turn changes the surface tensions across the droplet. Precise manipulation of the electric field allows control of the droplets. The droplet is placed on an insulating substrate located in between an electrode.\n\nThe optoelectrowetting mechanism adds a photoconductor underneath the conventional electrowetting circuit, with an AC power source attached. Under normal (dark) conditions, the majority of the system's impedance lies in the photoconducting region, and therefore the majority of the voltage drop occurs here. However, when light is shined on the system, carrier generation and recombination causes the conductivity of the photoconductor spikes and results in a voltage drop across the insulating layer, changing the contact angle as a function of the voltage. The contact angle between a liquid and electrode can be described as:\n\nwhere V, \"d\", ε, and γ are applied voltage, thickness of the insulation layer, dielectric constant of the insulation layer, and the interfacial tension constant between liquid and gas. In AC situations, such as OEW, V is replaced with the \"RMS\" voltage. It should also be noted that the frequency of the AC power source is adjusted so that the impedance of the photoconductor dominates in the dark state. The shift in the voltage drop across the insulating layer therefore reduces the contact angle of the droplet as a function of the light intensity. By shining an optical beam on one edge of a liquid droplet, the reduced contact angle creates a pressure difference throughout the droplet, and pushes the droplet's center of mass towards the illuminated side. Control of the optical beam results in control of the droplet's movement.\n\nUsing 4 mW laser beams, OEW has proven to move droplets of deionized water at speeds of 7mm/s.\n\nTraditional electrowetting runs into problems because it requires a two-dimensional array of electrodes for droplet actuation. The large number of electrodes leads to complexity for both control and packaging of these chips, especially for droplet sizes of smaller scales. While this problem can be solved through integration of electronic decoders, the cost of the chip would significantly increase.\n\nDroplet manipulation in electrowetting-based devices are usually accomplished using two parallel plates which sandwiches the droplet and is actuated by digital electrodes. The minimum droplet size that can be manipulated is determined by the size of pixilated electrodes. This mechanism provides a solution to the size limitation of physical pixilated electrodes by utilizing dynamic and reconfigurable optical patterns and enables operations such as continuous transport, splitting, merging, and mixing of droplets. SCOEW is conducted on open, featureless, and photoconductive surfaces. This configuration creates a flexible interface that allows simple integration with other microfluidic components, such as sample reservoirs through simple tubing.\n\nOptoelectrowetting can also be achieved using the photocapacitance in a liquid-insulator-semiconductor junction. The photo-sensitive electrowetting is achieved via optical modulation of carriers in the space charge region at the insulator-semiconductor junction which acts as a photodiode – similar to a charge-coupled device based on a metal-oxide-semiconductor.\n\nElectrowetting presents a solution to one of the most challenging tasks in lab-on-a-chip systems in its ability to handle and manipulate complete physiological compounds. Conventional microfluidic systems aren't easily adaptable to handle different compounds, requiring reconfiguration that often results in the device being impractical as a whole. Through OEW, a chip with one power source can be readily used with a variety of substances, with potential for multiplexed detection.\n\nPhoto-actuation in microelectromechanical systems (MEMS) has been demonstrated in proof-of-concept experiments., Instead of a typical substrate, a specialized cantilever is placed on top of the liquid-insulator-photoconductor stack. As light is shined on the photoconductor, the capillary force from the drop on the cantilever changes with the contact angle, and deflects the beam. This wireless actuation can be used as a substitute for complex circuit-based systems currently used for optical addressing and control of autonomous wireless sensors\n\n\n"}
{"id": "14979976", "url": "https://en.wikipedia.org/wiki?curid=14979976", "title": "Pile cap", "text": "Pile cap\n\nA pile cap is a thick concrete mat that rests on concrete or timber piles that have been driven into soft or unstable ground to provide a suitable stable foundation. It usually forms part of the foundation of a building, typically a multi-story building, structure or support base for heavy equipment. The cast concrete pile cap distributes the load of the building into the piles. A similar structure to a pile cap is a \"raft\", which is a concrete foundation floor resting directly onto soft soil which may be liable to subsidence.\n\nThe mat is made of concrete which is an aggregate of small rocks and cement. This mixture has to be supported by a framework to avoid sagging and fracture while setting. This process is known as shuttering and reinforcing. The materials used are long twisted steel bars between the piles held in shape by thinner tie wires. Once this steel mat is laid, timber is attached around the perimeter to contain the wet concrete mixture. Once poured, (usually as a series of small loads), the concrete is stirred to remove any air pockets that might weaken the structure when set. The concrete undergoes a chemical change as it hardens and this produces a lot of heat. Sometimes, if the mass of concrete is very large, pipes carrying refrigerant coolant are used in the mass to assist the setting process to prevent the concrete from cracking.\n"}
{"id": "4533478", "url": "https://en.wikipedia.org/wiki?curid=4533478", "title": "Plants for a Future", "text": "Plants for a Future\n\nPlants For A Future (PFAF) is an online not for profit resource for those interested in edible and useful plants, with a focus on temperate regions. The project currently has a site in the South West of England where many of the plants are being grown on a trial basis, and maintains a small mail order catalogue. The organization's emphasis is on perennial plants.\n\nPFAF is a registered educational charity with strong ethical principals and the following objectives:\n\nThe website contains an online database of over 7000 plants that can be grown in the UK; the data is created/collated by Ken Fern, and can be either used online free of charge, or downloaded for a small sum.\n\n\n\n"}
{"id": "957085", "url": "https://en.wikipedia.org/wiki?curid=957085", "title": "Popek and Goldberg virtualization requirements", "text": "Popek and Goldberg virtualization requirements\n\nThe Popek and Goldberg virtualization requirements are a set of conditions sufficient for a computer architecture to support system virtualization efficiently. They were introduced by Gerald J. Popek and Robert P. Goldberg in their 1974 article \"Formal Requirements for Virtualizable Third Generation Architectures\". Even though the requirements are derived under simplifying assumptions, they still represent a convenient way of determining whether a computer architecture supports efficient virtualization and provide guidelines for the design of virtualized computer architectures.\n\nSystem virtual machines are capable of virtualizing a full set of hardware resources, including a processor (or processors), memory and storage resources and peripheral devices. \nA virtual machine monitor (VMM, also called hypervisor) is the piece of software that provides the abstraction of a virtual machine. There are three properties of interest when analyzing the environment created by a VMM:\n\n\nIn the terminology of Popek and Goldberg, a VMM must present all three properties. In the terminology used in the reference book of Smith and Nair (2005), VMMs are typically assumed to satisfy the equivalence and resource control properties, and those additionally meeting the performance property are called \"efficient VMMs\".\n\nPopek and Goldberg describe the characteristics that the instruction set architecture (ISA) of the physical machine must possess in order to run VMMs which possess the above properties.\nTheir analysis derives such characteristics using a model of \"third generation architectures\" (e.g., IBM 360, Honeywell 6000, DEC PDP-10) that is nevertheless general enough to be extended to modern machines. This model includes a processor that operates in either system or user mode, and has access to linear, uniformly addressable memory. It is assumed that a subset of the instruction set is available only when in system mode and that memory is addressed relative to a relocation register. I/O and interrupts are not modelled.\n\nTo derive their virtualization theorems, which give sufficient (but not necessary) conditions for virtualization, Popek and Goldberg introduce a classification of instructions of an ISA into 3 different groups:\n\n\nThe main result of Popek and Goldberg's analysis can then be expressed as follows.\n\nTheorem 1. For any conventional third-generation computer, an \"effective\" VMM may be constructed if the set of sensitive instructions for that computer is a subset of the set of privileged instructions.\n\nIntuitively, the theorem states that to build a VMM it is sufficient that all instructions that could affect the correct functioning of the VMM (sensitive instructions) always trap and pass control to the VMM. This guarantees the resource control property. Non-privileged instructions must instead be executed natively (i.e., efficiently). The holding of the equivalence property also follows.\n\nThis theorem also provides a simple technique for implementing a VMM, called \"trap-and-emulate virtualization\", more recently called \"classic virtualization\": because all sensitive instructions behave nicely, all the VMM has to do is trap and emulate every one of them.\n\nA related problem is that of deriving sufficient conditions for recursive virtualization, that is, the conditions under which a VMM that can run on a copy of itself can be built. Popek and Goldberg present the following (sufficient) conditions.\n\nTheorem 2. A conventional third-generation computer is recursively virtualizable if:\n\n\nSome architectures, like the non-hardware-assisted x86, do not meet these conditions, so they cannot be virtualized in the classic way. But architectures can still be fully virtualized (in the x86 case meaning at the CPU and MMU level) by using different techniques like binary translation, which replaces the sensitive instructions that do not generate traps, which are sometimes called critical instructions. This additional processing however makes the VMM less efficient in theory, but hardware traps have non-negligible performance cost as well. A well-tuned caching binary translation system may achieve comparable performance, and it does in the case of x86 binary translation relative to first generation x86 hardware assist, which merely made sensitive instructions trappable. Effectively this gives a theorem with different sufficiency conditions.\n\nTheorem 3. A \"hybrid\" VMM may be constructed for any third generation machine in which the set of user sensitive instructions are a subset of the set of privileged instructions:\n\nThe conditions for ISA virtualization expressed in Theorem 1 may be relaxed at the expense of the efficiency property. VMMs for non-virtualizable ISAs (in the Popek and Goldberg's sense) have routinely been built.\n\nThe virtualization of such architectures requires correct handling of \"critical instructions\", i.e., sensitive but unprivileged instructions. One approach, known as \"patching\", adopts techniques commonly used in dynamic recompilation: critical instructions are discovered at run-time and replaced with a trap into the VMM. Various mechanisms, such as the caching of emulation code or hardware assists, have been proposed to make the patching process more efficient. A different approach is that of paravirtualization, which requires guest operating systems to be modified (\"ported\") before running in the virtual environment.\n\nThis section presents some relevant architectures and how they relate to the virtualization requirements.\n\nThe PDP-10 architecture has a few instructions which are sensitive (alter or query the processor's mode) but not privileged. These instructions save or restore the condition codes containing USER or IOT bits:\n\n\nAll sensitive instructions in the System/370 are privileged: it satisfies the virtualization requirements.\n\nThe Motorola MC68000 has a single unprivileged sensitive instruction:\n\n\nThis instruction is sensitive because it allows access to the entire status register, which includes not only the condition codes but also the user/supervisor bit, interrupt level, and trace control. In most later family members, starting with the MC68010, the MOVE from SR instruction was made privileged, and a new MOVE from CCR instruction was provided to allow access to the condition code register only.\nThe IA-32 instruction set of the Pentium processor contains 18 sensitive, unprivileged instructions. They can be categorized in two groups:\n\n\nThe introduction of the AMD-V and Intel VT-x instruction sets in 2005 allows x86 processors to meet the Popek and Goldberg virtualization requirements.\n\nThe effort needed to support virtualization on the IA-64 architecture is described in a 2000 article by Magenheimer and Christian.\n\nA \"hyperprivileged\" mode for the UltraSPARC architecture was specified in \"UltraSPARC Architecture 2005\".' It defines a \"sun4v\" platform which is a super-set of the \"sun4u\" platform, but is still compliant to the SPARC v9 Level-1 specification.\n\nAll sensitive instructions in the PowerPC instruction set are privileged.\n\nThe efficiency requirement in Popek and Goldberg's definition of a VMM concerns only the execution of non-privileged instructions, which must execute natively. This is what distinguishes a VMM from the more general class of hardware emulation software. Unfortunately, even on an architecture that meets Popek and Goldberg's requirements, the performance of a virtual machine can differ significantly from the actual hardware. Early experiments performed on the System/370 (which meets the formal requirements of Theorem 1) showed that performance of a virtual machine could be as low as 21% of the native machine in some benchmarks. The cost of trapping and emulating privileged instructions in the VMM can be significant. This led the IBM engineers to introduce a number of hardware assists, which roughly doubled the performance of the System/370 virtual machines. Assists were added in several stages. In the end, there were over 100 assists on the late models System/370.\n\nOne of the main driving factors for the development of hardware assists for the System/370 was virtual memory itself. When the guest was an operating system that itself implemented virtual memory, even non-privileged instructions could experience longer execution times - a penalty imposed by the requirement to access translation tables not used in native execution (see shadow page tables).\n\n"}
{"id": "22898151", "url": "https://en.wikipedia.org/wiki?curid=22898151", "title": "Puretic power block", "text": "Puretic power block\n\nThe Puretic power block is a special kind of mechanised winch used to haul nets on fishing vessels. The power block is a large powered aluminium pulley with a hard rubber-coated sheave. While many men were needed for the back-breaking work of hauling a purse seine manually, the same work could be done by fewer men with a power block.\n\nThe Puretic power block revolutionized the technology of hauling fishing nets, particularly purse seine nets. According to the Food and Agriculture Organization of the United Nations (FAO), \"no single invention has contributed more to the success of purse seine net hauling\" than the power block, which was \"the linch-pin in the mechanization of purse seining\".\n\nThe power block was invented by a Croatian fisherman, Mario Puratić, and patented in 1953. In English, Puratić is usually spelt Puretic, sometimes Puretich.\n\nWhile he was working as a tuna and sardine fisherman from San Pedro, California, Puratić started thinking about the difficulty of hauling seine nets. The original power block he designed was essentially a simple winch which used a V-shaped roller coated with hard rubber. It was suspended from a davit, and powered from the warp end of the winch by a looping rope. In 1954, the power block was trialled by American purse seiners in the Pacific. They have evolved since then, and nowadays power blocks are powered by hydraulic pumps. Their speed, torque and direction can be remotely controlled from the bridge during operations. As a result, retrieving the net is safer and requires less manual work.\n\nOther important innovations of the same period were the development of synthetic fishing nets and sonar detection devices. The combination of the automated power block hauling synthetic nets with sonar sensing revolutionised the industry.\n\nThe use of power blocks was found to have further advantages. It is possible to fish in rougher weather with the power block, because the steady force exerted by the net stabilizes the vessel. When deployments are made which miss the schooling fish, the net can be rapidly retrieved and redeployed on the same school. The power block also makes it easy to get nets aboard in emergencies, such as a sudden shift in tides or winds, or shark attacks on the catch and net.\n\nOn the other hand, the increase in the effectiveness of purse seine fishing led to herring schools choosing to school deeper in the water. Nets that could operate in deeper water needed to be bigger and heavier. That in turn meant bigger vessels, which pushed the smaller herring vessels out of business.\n\nSeine fishing industries rapidly recognized the value of the power block, and by 1960, most northern seine vessels were using the power block. Nowadays power blocks come with dozens of configurations and sizes. They are installed on over twenty thousand fishing vessels across the major purse seining fisheries around the world. These fisheries haul huge schools of tuna, salmon, herring, sardines, anchovies and menhaden from the sea, accounting for a large part the world total fish catch.\n\nBetween 1969 and 1979, the Puretic power block was pictured on the reverse side of Canadian five dollar banknote issues. In 1975 Mario Puratić was given the National Inventor of the Year Award by the Intellectual Property Owners Education Foundation for his invention of the power block.\n"}
{"id": "2044496", "url": "https://en.wikipedia.org/wiki?curid=2044496", "title": "Radio masts and towers", "text": "Radio masts and towers\n\nRadio masts and towers are, typically, tall structures designed to support antennas (also known as aerials) for telecommunications and broadcasting, including television. There are two main types: guyed and self-supporting structures. They are among the tallest man-made structures.\n\nMasts are often named after the broadcasting organizations that originally built them or currently use them. \n\nIn the case of a mast radiator or radiating tower, the whole mast or tower is itself the transmitting antenna.\n\nThe terms \"mast\" and \"tower\" are often used interchangeably. However, in structural engineering terms, a tower is a self-supporting or cantilevered structure, while a mast is held up by stays or guys. Broadcast engineers in the UK use the same terminology. A mast is a ground-based or rooftop structure that supports antennas at a height where they can satisfactorily send or receive radio waves. Typical masts are of steel lattice or tubular steel construction. Masts themselves play no part in the transmission of mobile telecommunications.\nMasts (to use the civil engineering terminology) tend to be cheaper to build but require an extended area surrounding them to accommodate the guy wires. Towers are more commonly used in cities where land is in short supply.\n\nThere are a few borderline designs that are partly free-standing and partly guyed, called additionally guyed towers. For example:\n\nExperimental radio broadcasting began in 1905, and commercial radio broke through in the 1920s.\n\nUntil August 8, 1991, the Warsaw radio mast was the world's tallest supported structure on land; its collapse left the KVLY/KTHI-TV mast as the tallest. There are over 50 radio structures in the United States that are 600 m (1968.5 ft) or taller.\n\nThe steel lattice is the most widespread form of construction. It provides great strength, low weight and wind resistance, and economy in the use of materials. Lattices of triangular cross-section are most common, and square lattices are also widely used. Guyed masts are often used; the supporting guy lines carry lateral forces such as wind loads, allowing the mast to be very narrow and simply constructed. \n\nWhen built as a tower, the structure may be parallel-sided or taper over part or all of its height. When constructed of several sections which taper exponentially with height, in the manner of the Eiffel Tower, the tower is said to be an Eiffelized one. The Crystal Palace tower in London is an example.\nGuyed masts are sometimes also constructed out of steel tubes. This construction type has the advantage that cables and other components can be protected from weather inside the tube and consequently the structure may look cleaner.\nThese masts are mainly used for FM-/TV-broadcasting, but sometimes also as mast radiator. The big mast of Mühlacker transmitting station is a good example of this. \nA disadvantage of this mast type is that it is much more affected by winds than masts with open bodies. Several tubular guyed masts have collapsed. In the UK, the Emley Moor and Waltham TV stations masts collapsed in the 1960s. In Germany the Bielstein transmitter collapsed in 1985.\nTubular masts were not built in all countries. In Germany, France, UK, Czech, Slovakia, Japan and the former Soviet Union, many tubular guyed masts were built, while there are nearly none in Poland or North America.\n\nIn several cities in Russia and Ukraine several tubular guyed masts with crossbars running from the mast structure to the guys were built in the 1960s. All these masts, which are designed as 30107 KM, are exclusively used for FM and TV transmission and, except for the mast in Vinnytsia, are between 150 and 200 metres tall.\nThe crossbars of these masts are equipped with a gangway that holds smaller antennas, though their main purpose is oscillation damping.\n\nReinforced concrete towers are relatively expensive to build but provide a high degree of mechanical rigidity in strong winds. This can be important when antennas with narrow beamwidths are used, such as those used for microwave point-to-point links, and when the structure is to be occupied by people.\n\nIn the 1950s, AT&T built numerous concrete towers, more resembling silos than towers, for its first transcontinental microwave route. Many are still in use today.\n\nIn Germany and the Netherlands most towers constructed for point-to-point microwave links are built of reinforced concrete, while in the UK most are lattice towers.\n\nConcrete towers can form prestigious landmarks, such as the CN Tower in Toronto. In addition to accommodating technical staff, these buildings may have public areas such as observation decks or restaurants.\n\nThe Stuttgart TV tower was the first tower in the world to be built in reinforced concrete. It was designed in 1956 by the local civil engineer Fritz Leonhardt.\n\nFiberglass poles are occasionally used for low-power non-directional beacons or medium-wave broadcast transmitters. Carbon fibre monopoles and towers have traditionally been too expensive but recent developments in the way the carbon fibre tow is spun have resulted in solutions that offer strengths similar or exceeding steel for a fraction of the weight - now allowing monopole and towers to be built in locations that were too expensive or difficult to access with the heavy lifting equipment that is needed for steel structure.\n\nThere are fewer wooden towers now than in the past. Many were built in the UK during World War II because of a shortage of steel. In Germany before World War II wooden towers were used at nearly all medium-wave transmission sites, but all of these towers have since been demolished, except for the Gliwice Radio Tower.\n\nFerryside television relay station is an example of a TV relay transmitter using a wooden pole.\n\nShorter masts may consist of a self-supporting or guyed wooden pole, similar to a telegraph pole. Sometimes self-supporting tubular galvanized steel poles are used: these may be termed monopoles.\n\nIn some cases, it is possible to install transmitting antennas on the roofs of tall buildings. In North America, for instance, there are transmitting antennas on the Empire State Building, the Willis Tower, 4 Times Square, and One World Trade Center. The North Tower (1WTC) of the original World Trade Center also had a 360-foot (110m) telecommunications antenna atop its roof, constructed in 1978-1979, and began transmission in 1980. When the buildings collapsed, several local TV and radio stations were knocked off the air until backup transmitters could be put into service. Such facilities also exist in Europe, particularly for portable radio services and low-power FM radio stations. In London, the BBC erected in 1936 a mast for broadcasting early television on one of the towers of a Victorian building, the Alexandra Palace. It is still in use.\n\nMany people view bare cellphone towers as ugly and an intrusion into their neighbourhoods. Even though people increasingly depend upon cellular communications, they are opposed to the bare towers spoiling otherwise scenic views. Many companies offer to 'hide' cellphone towers in, or as, trees, church towers, flag poles, water tanks and other features. There are many providers that offer these services as part of the normal tower installation and maintenance service. These are generally called \"stealth towers\" or \"stealth installations\", or simply concealed cell sites.\n\nThe level of detail and realism achieved by disguised cellphone towers is remarkably high; for example, such towers disguised as trees are nearly indistinguishable from the real thing, even for local wildlife (who additionally benefit from the artificial flora). Such towers can be placed unobtrusively in national parks and other such protected places, such as towers disguised as cacti in Coronado National Forest.\n\nEven when disguised, however, such towers can create controversy; a tower doubling as a flagpole attracted controversy in 2004 in relation to the U.S. Presidential campaign of that year, and highlighted the sentiment that such disguises serve more to allow the installation of such towers in subterfuge away from public scrutiny rather than to serve towards the beautification of the landscape.\n\nDisguised cell sites sometimes can be introduced into environments that require a low-impact visual outcome, by being made to look like trees, chimneys or other common structures.\n\nA mast radiator is a radio tower or mast in which the whole structure works as an antenna. It is used frequently as a transmitting antenna for long or medium wave broadcasting.\n\nStructurally, the only difference is that a mast radiator may be supported on an insulator at its base. In the case of a tower, there will be one insulator supporting each leg.\n\nA special form of the radio tower is the \"telescopic mast\". These can be erected very quickly. Telescopic masts are used predominantly in setting up temporary radio links for reporting on major news events, and for temporary communications in emergencies. They are also used in tactical military networks. They can save money by needing to withstand high winds only when raised, and as such are widely used in amateur radio.\n\nTelescopic masts consist of two or more concentric sections and come in two principal types:\n\nA tethered balloon or a kite can serve as a temporary support. It can carry an antenna or a wire (for VLF, LW or MW) up to an appropriate height. Such an arrangement is used occasionally by military agencies or radio amateurs. The American broadcasters TV Martí broadcast a television program to Cuba by means of such a balloon.\n\nThere has recently (2013) been interest in using unmanned aerial vehicles (drones) for telecom purposes. It is not clear what advantages a drone would have over a balloon, however one major disadvantage is their limited duration when compared to a balloon.\n\nFor two VLF transmitters wire antennas spun across deep valleys are used. The wires are supported by small masts or towers or rock anchors. See List of spans: Antenna spans across valleys. The same technique was also used at Criggion radio station.\n\nFor ELF transmitters ground dipole antennas are used. Such structures require no tall masts. They consist of two electrodes buried deep in the ground at least a few dozen kilometres apart. From the transmitter building to the electrodes, overhead feeder lines run. These lines look like power lines of the 10 kV level, and are installed on similar pylons.\n\n\nFor transmissions in the shortwave range, there is little to be gained by raising the antenna more than a few wavelengths above ground level. Shortwave transmitters rarely use masts taller than about 100 metres.\n\nBecause masts, towers and the antennas mounted on them require maintenance, access to the whole of the structure is necessary. Small structures are typically accessed with a ladder. Larger structures, which tend to require more frequent maintenance, may have stairs and sometimes a lift, also called a service elevator.\n\nTall structures in excess of certain legislated heights are often equipped with aircraft warning lamps, usually red, to warn pilots of the structure's existence. In the past, ruggedized and under-run filament lamps were used to maximize the bulb life. Alternatively, neon lamps were used. Nowadays such lamps tend to use LED arrays.\n\nHeight requirements vary across states and countries, and may include additional rules such as requiring a white flashing strobe in the daytime and pulsating red fixtures at night. Structures over a certain height may also be required to be painted with contrasting color schemes such as white and orange or white and red to make them more visible against the sky.\n\nIn some countries where light pollution is a concern, tower heights may be restricted so as to reduce or eliminate the need for aircraft warning lights. For example, in the United States the 1996 Telecommunications Act allows local jurisdictions to set maximum heights for towers, such as limiting tower height to below 200 feet and therefore not requiring aircraft illumination under U.S. Federal Communications Commission (FCC) rules. The limit is more commonly set to 190 or 180 feet to allow for masts extending above the tower.\n\nOne problem with radio masts is the danger of wind-induced oscillations. This is particularly a concern with steel tube construction. One can reduce this by building cylindrical shock-mounts into the construction. One finds such shock-mounts, which look like cylinders thicker than the mast, for example, at the radio masts of DHO38 in Saterland. There are also constructions, which consist of a free-standing tower (usually from reinforced concrete), onto which a guyed radio mast is installed. The best known such construction is the Gerbrandy Tower in Lopik (the Netherlands). Further towers of this building method can be found near Smilde (the Netherlands) and Fernsehturm, Waldenburg, Baden-Württemberg, Germany).\n\nRadio, television and cell towers have been documented to pose a hazard to birds. Reports have been issued documenting known bird fatalities and calling for research to find ways to minimize the hazard that communications towers can pose to birds. There have also been instances of rare birds nesting in cell towers and thereby preventing repair work due to legislation intended to protect them.\n\nSince June 2010, Telecom operators in the USA can erect new telecom masts or towers as the government has lifted the moratorium, which was earlier placed on the issuance of permits for the construction of telecommunication towers.\n\n\n"}
{"id": "22836400", "url": "https://en.wikipedia.org/wiki?curid=22836400", "title": "Radiogram (device)", "text": "Radiogram (device)\n\nIn British English, a radiogram is a piece of furniture that combined a radio and record player. The word \"radiogram\" is a portmanteau of \"radio\" and \"gramophone\". The corresponding term in American English is console.\n\nRadiograms reached their peak of popularity in the post-war era, supported by a rapidly growing interest in records. Originally they were made of polished wood to blend with the furniture of the 1930s, with many styled by the leading designers of the day. An expensive instrument of entertainment for the house, fitted with a larger loudspeaker than the domestic radio, the radiogram soon began to develop features such as the record autochanger, which would accept six or seven records and play them one after another. Certain recordings could be ordered as a box set which would combine the recorded piece in order, to suit an autochanger set-up. In the 1940s and 1950s, sales of the radiogram, coupled with the then-new F.M. waveband, and the advent of the 45 rpm single and the LP record, meant that many manufacturers considered the radiogram to be more important than the fledgling television set sales. Later models took on the modern lines, piano gloss finish and plastic and gilt trim of the 1960s. Stereogram versions became available to take advantage of stereo records.\n\nAs valve radio development ended in the late 1960s and transistors began to take over, radiograms started to become obsolete. By the late 1970s, they had been replaced by more compact equipment, such as the hi-fi and the music centre.\n\nSince radiograms were manufactured in such huge numbers they are not as rare or valuable as TV sets or table radios from the same period. An exception to this are models from certain manufacturers which have become collectable such as Hacker Radio Ltd., Dynatron, Blaupunkt, Braun, and SABA.\n\n"}
{"id": "46340395", "url": "https://en.wikipedia.org/wiki?curid=46340395", "title": "Restrictions on geographic data in China", "text": "Restrictions on geographic data in China\n\nDue to national security concerns, the use of geographic information in China is restricted to entities that obtain a special authorization from the administrative department for surveying and mapping under the State Council. Consequences of the restriction include fines for unauthorized surveys, lack of geotagging information on many cameras when the GPS chip detects a location within China, incorrect alignment of street maps with satellite maps in various applications, and seeming unlawfulness of crowdsourced mapping efforts such as OpenStreetMap.\n\nAccording to articles 7, 26, 40 and 42 of the \"Surveying and Mapping Law of the People's Republic of China\", private surveying and mapping activities have been illegal in mainland China since 2002. The law prohibits\n\nFines range from 10,000 to 500,000 CNY ( – USD). Foreign individuals or organizations that wish to conduct surveying must form a Chinese-foreign joint venture.\n\nBetween 2006 and 2011, the authorities pursued nearly 40 illegal cases of mapping and surveying. The media has reported on other cases of unlawful surveys:\n\nAs a consequence, major digital camera manufacturers including Panasonic, Leica, FujiFilm, Nikon and Samsung restrict location information within China.\n\nOpenStreetMap, the crowdsourced project to assemble a map of the world, advises that \"private surveying and mapping activities are illegal in China\".\n\nChinese regulations mandate that approved map service providers in China use a specific coordinate system, called GCJ-02. Baidu Maps uses yet another coordinate system - BD-09, which seems to be based on GCJ-02.\n\nGCJ-02 (colloquially Mars Coordinates) is a geodetic datum formulated by the Chinese State Bureau of Surveying and Mapping (), and based on WGS-84. It uses an obfuscation algorithm which adds apparently random offsets to both the latitude and longitude, with the alleged goal of improving national security.\n\nA marker with GCJ-02 coordinates will be displayed at the correct location on a GCJ-02 map. However, the offsets can result in a 100 - 700 meter error from the actual location if a WGS-84 marker (such as a GPS location) is placed on a GCJ-02 map, or vice versa. The Google.com street map is offset by 50–500 meters from its satellite imagery, while the Google.cn map is not. Yahoo! Maps also displays the street map without major errors when compared to the satellite imagery. MapQuest overlays OpenStreetMap data perfectly as well.\n\nDespite the secrecy surrounding the GCJ-02 obfuscation, several open-source projects exist that provide conversions between GCJ-02 and WGS-84, for languages including C#, C, Go, Java, JavaScript, PHP, Python, R, and Ruby. They appear to be based on leaked code for the WGS to GCJ part. Other solutions to the conversion involve interpolating coordinates based on regression from a data set of Google China and satellite imagery coordinates. An attempt by Wu Yongzheng using FFT analysis gave a result much like the leaked code.\n\nFrom the leaked code, GCJ-02 uses parameters from the SK-42 reference system. The parameters were used to calculate lengths of one degree of latitude and longitude, so that offsets in meters previously calculated can be converted to degrees for the WGS-84 input coordinates.\n\nBD-09 is a geographic coordinate system used by Baidu Maps, adding further obfuscation to the already cryptic GCJ-02 \"to better protect users' privacy\". Baidu provides an API call to convert from Google or GPS (WGS-84) coordinates into Baidu coordinates. Similar to GCJ-02, there are no APIs to convert in the other direction, but open source implementations in R and various other languages exist.\n\nGCJ-02 appears to use multiple high-frequency noises of the form formula_1, effectively generating a transcendental equation and thus eliminating analytical solutions. However, the open-source \"reverse\" transformations make use of the properties of GCJ-02 that the transformed coordinates are not too far from WGS-84 and are mostly monotonic related to corresponding WGS-84 coordinates:\nfrom typing import Callable\ncoords = complex\nC2C = Callable[[coords], coords]\n\ndef rev_transform_rough(bad: coords, worsen: C2C) -> coords:\n\ndef rev_transform(bad: coords, worsen: C2C) -> coords:\n\nThe rough method is reported to give some 1~2 meter accuracy for wgs2gcj, while the exact ([[fixed point iteration]]) method is able to get \"centimeter accuracy\" in two calls to the forward function. Since the two properties ensure some basic functionality of the coordinate system, it's unlikely that the methods will change with new coordinate systems. The [[#BD-09|BD]]-to-GCJ code works in a manner much like the rough method, except that it removes the explicitly-applied constant shift of ~20 seconds of arc on both coordinates first and works in polar coordinates like the forward function does. \n\nThe establishment of working conversion methods both ways largely obsoletes datasets for deviations mentioned below.\n\n[[Image:Google.com Maps in China coordinate system misalignment.png|thumb|right|Google Maps displays satellite imagery using the WGS-84 coordinate system, and street maps using the GCJ-02 datum]]\nThe China GPS shift (or offset) problem is a class of issues stemming from the difference between the [[#GCJ-02|GCJ-02]] and [[WGS-84]] [[geodetic datum|datum]]s. [[Global Positioning System]] coordinates are expressed using the WGS-84 standard and when plotted on street maps of China that follow the GCJ-02 coordinates, they appear off by a large (often over 500 meters) and variable amount. Authorized providers of location-based services and digital maps (such as [[AutoNavi]] or [[NavInfo]]) must purchase a \"shift correction\" algorithm that enables plotting GPS locations correctly on the map. Satellite imagery and user-contributed street map data sets, such as those from [[OpenStreetMap]] also display correctly because they have been collected using GPS devices (albeit technically illegally - see [[#Legislation|Legislation]]).\n\nSome map providers, such as [[Here (company)|Here]], choose to also offset their satellite imagery layer to match the GCJ-02 street map.\n\nGoogle has worked with Chinese [[location-based service|LBS]] provider [[AutoNavi]] since 2006 to source its maps in China.\ngoogle.\"cn\"/maps (formerly Google Ditu) uses the GCJ-02 system for both its street maps and satellite imagery. However, the WGS-84 positions reported by a browser are depicted at the wrong positions. On the contrary, google.\"com\"/maps also uses GCJ-02 data for the street map, but doesn't shift the satellite imagery layer, which continues to use WGS-84 coordinates, with the benefit that WGS-84 positions can still be overlaid correctly on the satellite image (but not the street map). Google Earth also uses WGS-84 to display the satellite imagery.\n\nOverlaying GPS tracks on [[Google Maps|Google.com Maps]] and any street maps sourced from Google.com via its API, will lead to a similar display offset problem, because GPS tracks use WGS-84, and Google.com maps use GCJ-02. The issue has been reported numerous times on the Google Product Forums since 2009, with 3rd party applications emerging to fix it. Data sets with offsets for large lists of Chinese cities existed for sale. The problem was observed as early as 2008, and the causes were unclear, with (misguided) speculation that imported GPS chips were tampered with code that caused incorrect reporting of coordinates.\n\nUnder [[One Country Two Systems]], legislation in mainland China does not apply in [[Hong Kong]] and [[Macau]] [[Special administrative regions of China|SARs]] and there are no similar restrictions in the SARs. Therefore, the GPS shift problem does not apply. However, at the border between the SARs and mainland China, the data shown by online maps are broken where the shifted data and correct data overlap. This poses problems to users travelling across the border, especially visitors not aware of the issue.\n\n[[Category:Censorship in China]]\n[[Category:Internet censorship in China]]\n[[Category:Web mapping]]\n[[Category:Geographic coordinate systems]]\n[[Category:Geodesy]]"}
{"id": "29769799", "url": "https://en.wikipedia.org/wiki?curid=29769799", "title": "Reverse architecture", "text": "Reverse architecture\n\nReverse architecture is a process of deducing the underlying architecture and design of a system by observing its behaviour. It has its roots in the field of reverse engineering.\n\nPracticing reverse architecture is used to decipher the logistics of building. There are a variety of techniques available, the most notable being architecture driven modelling.\n\n\n"}
{"id": "3460869", "url": "https://en.wikipedia.org/wiki?curid=3460869", "title": "Shave brush", "text": "Shave brush\n\nA shave brush or shaving brush is a small brush with a handle parallel to the bristles used to apply shaving soap or shaving cream to the face when shaving. Shave brushes are often decorative; antique handles are often made from materials such as ivory or even gold, though the bristle load may be composed of any number of natural or synthetic materials. The shave brush is used most often today by \"wet shavers\" in tandem with a double-edged safety razor or a straight razor. However, this is not always the case, as shavers of all varieties may employ the tool.\n\nThe modern shaving brush may be traced to France during the 1750s. The French call a shaving brush \"blaireau\" or \"badger.\" Quality of these brushes differed greatly, as materials used to fashion the handles varied from the common to the exotic. It was not uncommon for handles to be made of ivory, gold, silver, tortoiseshell, crystal, or porcelain. The more expensive brushes used badger hair, with cheaper ones using boar's or horse's hair. In the 1800s when the folding-handle straight razor design made it practical for men to shave themselves rather than visit a barber, a shave brush became a status symbol, and an expensive or eccentric brush was a way of asserting one's personality or even affluence. The recent rapid rise in the popularity of \"wet shaving\" has raised demand for high quality and custom shaving brushes. \n\nModern shave brushes are similar in appearance, composition and function to their centuries-old predecessors. Although a variety of different materials are still used to fashion shave brush handles, synthetic handles of nylon, urethane or plastic are the most common even by the most expensive shave brush manufacturers. Benefits of synthetic handles include a lesser chance of breakage and resistance to damage by moisture. A limited number of consumers prefer natural materials such as wood or exotic materials such as tortoiseshell. A shave brush's handle, regardless of its material composition, rarely affects the overall performance of the brush.\n\nA shave brush's price is usually determined more by its bristle load than any other factor except for brushes with very exotic handles. The most expensive brushes often use exotic materials in the handle. The bristles are fixed together into a knot that is installed into the handle. The best quality brushes are hand knotted. Badger and boar brushes are the most commonly found animals used for shaving brush fibers. Badger species include the Eurasian badger and the hog badger. Badger brushes are often referred to as two band, or three band. Perhaps all badger hair fibers have three bands, but those used in the brushes conceal the lighter bottom band. Nonetheless, both types of bristle make desirable shaving brushes. Lower-quality brushes are often machine made and the bristles may be trimmed, resulting in sharp, prickly tips.\n\nSynthetic shave brushes, most often made using nylon bristles, are available in a range of prices and gradations in quality. Comparable to traditional shaving brushes, synthetic fiber brushes can quickly create a rich lather using relatively little shaving soap or cream. The synthetic fibers dry faster than natural hair and are less sensitive to everyday use.\n\nBoar's hair brushes are relatively inexpensive, but can be of very high quality. A well-made boar brush will break in with use; the bristles begin to split at their tips, resulting in a brush that is very soft but has considerable backbone. Unlike badger hair and synthetic fibers, boar bristles absorb water, so the brush should be soaked before use.\n\nBadger hair brushes come in a variety of grades, but there is not an industry standard that specifically defines grades of badger hair. Generally speaking, though, there are basic classifications that many manufacturers use to describe the quality of hair used in their brushes. The most common gradations of badger hair are \"pure\" badger, \"best\" badger, and \"super\" or \"silvertip\" badger. While some companies insist on using other gradations (for example, Vulfix's high-end brushes distinguish between \"super\" and \"silvertip\"), these three are commonly accepted among wet shavers and are most often used to describe the quality of a shave brush.\n\nPure badger are badger hair brushes that use the most common hair from the underbelly of a badger, the hair which covers around 60% of a badger's body. This hair varies greatly in softness, pliability and color. Pure badger hair is usually dark in color, but fluctuates from a light tan to a near-black or silvery sheen. The hair is coarser than 'best' or 'silvertip' hair due to its larger shaft. Brushes made exclusively with pure badger hair cost significantly less than finer badger hair. Most often, pure badger brush hairs are trimmed to shape, resulting in somewhat stiff, rough ends.\n\nBest badger are brushes made with the finer and more pliable hairs from 20 - 25% of the badger's body. It is longer in length and lighter in color than 'pure' badger hair. A 'best' badger brush is more densely filled with hair than the 'pure' badger brush and will produce a correspondingly greater lather. However, some wet shavers argue that the variance between the quality of a 'pure' and a 'best' badger brush is negligible. Best badger and better quality brush hairs are often fit so that the ends do not have to be cut to shape.\n\nA super badger brush is more expensive than either 'best' or 'pure'. While some call this hair 'silvertip', it is often highly graded 'pure' hair bleached on the ends to resemble silvertip.\n\nThough it is composed of 'pure' badger hairs, 'super' is graded and sorted to such a degree that its performance is superior to that of 'best'. The brush is not prickly.\n\nOne way to determine if a brush bears a 'super' or 'silvertip' badger hair load is to look at the color of the bristle tips. A true 'silvertip' brush has tips that are an off-white. A 'super' brush on the other hand has bristle tips that are a more sterile, slightly greyed white; moreover, the light color of the tips does not extend as far down the shaft of the hair.\n\nSilvertip badger is the most expensive and rare type of badger hair. The tips on this hair appear white naturally, without bleaching. A \"flared\" bristle load results in the 'silvertip' brush's fluffy appearance and lends the brush its ability to hold a large amount of water. Due to its water retention capacity, a 'silvertip' brush can create well-formed shaving lather quickly and easily.\n\nSome manufacturers such as Plisson, Simpsons and Rooney sell shaving brushes in a grade beyond silvertip. While the names these companies give this 'extra silvertip' vary, the properties remain fairly consistent between manufacturers as compared to the 'ordinary silvertip' brush. These brushes differ in appearance (the tip is whiter and extends further down the shaft; additionally, the hair under the tip is pure black as opposed to dark grey in color) and feel (the extra silvertip feels slightly firmer and less \"prickly\" on the face when lathering).\n\nBecause badgers are a protected species in North America and most of Europe, virtually all commercial badger hair comes from mainland China, which supplies knots of hair in various grades to brush makers in both China and Europe. In rural Northern China, badgers multiply to the point of becoming a crop nuisance, and village cooperatives are licensed by the national government to hunt badgers and sell the hair to processors. Procter & Gamble stopped using badger hair in its Art of Shaving products following a PETA investigation of several badger-hair farms and brush-making factories in Shijiazhuang, China, and video that showed a worker beating a badger with a chair leg before slaughtering it for its fur. \n\nBoar bristles are available cheaply from many sources. Brushes made in China or India with boar bristle are supplied wholesale, while even the cheapest wholesale Badger brush costs at least $10; even the cost difference between badger brushes with resin handles vs. expensive horn handles shows that, except with exotic materials such as sterling silver, special woods, ivory, bone or custom materials, badger hair is the costliest element of a brush. It is common for boar-hair brushes to have part of the bristles dyed to resemble badger hair.\n\nBrushes with nylon-only bristles are available.\n\nHorse hair brushes are coming back, after a hiatus of nearly 100 years following an anthrax scare around World War I. Material for horse hair shaving brushes is cut from the horse's mane or tail, and the animal is not harmed.\n\nA fibrous bristle load holds significant amounts of water which mix with the soap lifted from a shaving mug or scuttle. The more water a brush holds, the moister and richer a lather will be. Thicker and more emollient lather translates to less razor skipping and dragging.\n\nBringing a shave brush across one's skin produces a mild exfoliation. Because a shave brush is most often used with a shave soap, this effect often replaces the pre-shave routine of washing and applying lotion to the face.\n\nA shave brush also lifts facial hair before a shave, requiring less pressure from the razor.\n\n"}
{"id": "28904", "url": "https://en.wikipedia.org/wiki?curid=28904", "title": "Short Message Peer-to-Peer", "text": "Short Message Peer-to-Peer\n\nShort Message Peer-to-Peer (SMPP) in the telecommunications industry is an open, industry standard protocol designed to provide a flexible data communication interface for the transfer of short message data between External Short Messaging Entities (ESMEs), Routing Entities (REs) and Message Centres.\n\nSMPP is often used to allow third parties (e.g. value-added service providers like news organizations) to submit messages, often in bulk, but it may be used for SMS peering as well. SMPP is able to carry short messages including EMS, voicemail notifications, Cell Broadcasts, WAP messages including WAP Push messages (used to deliver MMS notifications), USSD messages and others. Because of its versatility and support for non-GSM SMS protocols, like UMTS, IS-95 (CDMA), CDMA2000, ANSI-136 (TDMA) and iDEN, SMPP is the most commonly used protocol for short message exchange outside SS7 networks.\n\nSMPP (Short Message Peer-to-Peer) was originally designed by Aldiscon, a small Irish company that was later acquired by Logica (since 2016, after a number of changes Mavenir). The protocol was originally created by a developer, Ian J Chambers, to test functionality of the SMSC without using SS7 test equipment to submit messages. In 1999, Logica formally handed over SMPP to the SMPP Developers Forum, later renamed as The SMS Forum and now disbanded. The SMPP protocol specifications are still available through the website which also carries a notice stating that it will be taken down at the end of 2007. As part of the original handover terms, SMPP ownership has now returned to Mavenir due to the disbanding of the SMS forum.\n\nTo date SMPP development is suspended and SMS forum is disbanded. From SMS forum website:\n\nJuly 31, 2007 - The SMS Forum, a non-profit organization with a mission to develop, foster and promote SMS (short message service) to the benefit of the global wireless industry will disband by July 27, 2007\n\nA press release, attached to the news, also warns that site will be suspended soon. In spite of this the site is still mostly functioning and specifications can still be downloaded (as of 31 January 2012).\n\nThe site has ceased operation according to Cormac Long, former technical moderator and webmaster for the SMS Forum. Please contact Mavenir for the SMPP specification. The files may also be available from other websites, including SMSCarrier - Documentation for SMPP APIs.\n\nContrary to its name, the SMPP uses the client-server model of operation. The Short Message Service Center (SMSC) usually acts as a server, awaiting connections from ESMEs. When SMPP is used for SMS peering, the sending MC usually acts as a client.\n\nThe protocol is based on pairs of request/response PDUs (protocol data units, or packets) exchanged over OSI layer 4 (TCP session or X.25 SVC3) connections. The well-known port assigned by the IANA for SMPP when operating over TCP is 2775, but multiple arbitrary port numbers are often used in messaging environments.\n\nBefore exchanging any messages, a bind command must be sent and acknowledged. The bind command determines in which direction will be possible to send messages; bind_transmitter only allows client to submit messages to the server, bind_receiver means that the client will only receive the messages, and bind_transceiver (introduced in SMPP 3.4) allows message transfer in both directions. In the bind command the ESME identifies itself using system_id, system_type and password; the address_range field designed to contain ESME address is usually left empty. The bind command contains interface_version parameter to specify which version of SMPP protocol will be used.\n\nMessage exchange may be synchronous, where each peer waits for a response for each PDU being sent, or asynchronous, where multiple requests can be issued without waiting and acknowledged in a skew order by the other peer; the number of unacknowledged requests is called a \"window\"; for the best performance both communicating sides must be configured with the same window size.\n\nThe SMPP standard has evolved during the time. The most commonly used versions of SMPP are:\n\n\nThe applicable version is passed in the interface_version parameter of a bind command.\n\nThe SMPP PDUs are binary encoded for efficiency. They start with a header which may be followed by a body:\n\nEach PDU starts with a header. The header consists of 4 fields, each of length of 4 octets:\n\n\nAll numeric fields in SMPP use the big endian order, which means that the first octet is the Most Significant Byte (MSB).\n\nThis is an example of the binary encoding of a 60-octet \"submit_sm\" PDU. The data is shown in Hex octet values as a single dump and followed by a header and body break-down of that PDU.\n\nThis is best compared with the definition of the submit_sm PDU from the SMPP specification in order to understand how the encoding matches the field by field definition.\n\nThe value break-downs are shown with decimal in parentheses and Hex values after that. Where you see one or several hex octets appended, this is because the given field size uses 1 or more octets encoding.\n\nAgain, reading the definition of the submit_sm PDU from the spec will make all this clearer.\n\n 'command_length', (60) ... 00 00 00 3C\n\n 'service_type', () ... 00\n\nNote that the text in the short_message field must match the data_coding. When the data_coding is 8 (UCS2), the text must be in UCS-2BE (or its extension, UTF-16BE). When the data_coding indicates a 7-bit encoding, each septet is stored in a separate octet in the short_message field (with the most significant bit set to 0). SMPP 3.3 data_coding exactly copied TP-DCS values of GSM 03.38, which make it suitable only for GSM 7-bit default alphabet, UCS2 or binary messages; SMPP 3.4 introduced a new list of data_coding values:\n\nThe meaning of the data_coding=4 or 8 is the same as in SMPP 3.3. Other values in the range 1-15 are reserved in SMPP 3.3. Unfortunately, unlike SMPP 3.3, where data_coding=0 was unambiguously GSM 7-bit default alphabet, for SMPP 3.4 and higher the GSM 7-bit default alphabet is missing in this list, and data_coding=0 may differ for various Short message service centers—it may be ISO-8859-1, ASCII, GSM 7-bit default alphabet, UTF-8 or even configurable per ESME. When using data_coding=0, both sides (ESME and SMSC) must be sure they consider it the same encoding. Otherwise it is better not to use data_coding=0. It may be tricky to use GSM 7-bit default alphabet, some Short message service centers requires data_coding=0, others e.g. data_coding=241.\n\nDespite its wide acceptance, the SMPP has a number of problematic features:\n\n\nAlthough data_coding values in SMPP 3.3 are based on the GSM 03.38, since SMPP 3.4 there is no data_coding value for GSM 7-bit alphabet (GSM 03.38). However, it is common for DCS=0 to indicate the GSM 7-bit alphabet, particularly for SMPP connections to SMSCs on GSM mobile networks.\n\nAccording to SMPP 3.4 and 5.0 the data_coding=0 means ″SMSC Default Alphabet″. Which encoding it really is, depends on the type of the SMSC and its configuration.\n\nOne of the encodings in CDMA standard C.R1001 is Shift-JIS used for Japanese. SMPP 3.4 and 5.0 specifies three encodings for Japanese (JIS, ISO-2022-JP and Extended Kanji JIS), but none of them is identical with CDMA MSG_ENCODING 00101. It seems that the Pictogram encoding (data_coding=9) is used to carry the messages in Shift-JIS in SMPP.\n\nWhen a submit_sm fails, the SMSC returns a submit_sm_resp with non-zero value of command_status and ″empty″ message_id.\n\n\nFor the best compatibility, any SMPP implementation should accept both variants of negative submit_sm_resp regardless of the version of SMPP standard used for the communication.\n\nComment from Cormac Long \nThe original intention of error scenarios was that no body would be returned in the PDU response. This was the standard behavior exhibited on all Aldiscon/Logica SMSC and also in most of the other vendors. When SMPP 3.4 was being taken on by the WAP forum, several clarifications were requested on whether a body should be included with NACKed response and measures were taken to clarify this in several places in the specification including the submit_sm section and also in the bind_transceiver section. What should have been done was to add the clarification that we eventually added in V5.0.. that bodies are not supposed to be included in error responses. Some vendors have been very silly in their implementations including bodies on rejected bind_transmitter responses but not on bind_transceiver responses etc. The recommendation I would make to vendors.. as suggested above.. accept both variants. But its also wise to allow yourself issue NACKed submit_sm_resp and deliver_sm_resp PDUs with and without an empty body. In the case of these two PDUs, that empty body will look like a single NULL octet at the end of the stream. The reason you may need this ability to include what I call dummy bodies with NACKed requests is that the other side of the equation may be unable or unwilling to change their implementation to tolerate the missing body.\n\nThe only way how to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose. While SMPP 3.3 states that Message ID is a C-Octet String (Hex) of up to 8 characters (plus terminating '\\0'), the SMPP 3.4 states that the id field in the Delivery Receipt Format is a C-Octet String (Decimal) of up to 10 characters. This splits SMPP implementations to 2 groups:\n\n\nSince introduction of Tag-Length-Value (TLV) parameters in version 3.4, the SMPP may be regarded an extensible protocol. In order to achieve the highest possible degree of compatibility and interoperability any implementation should apply the Internet robustness principle: ″Be conservative in what you send, be liberal in what you accept″. It should use a minimal set of features which are necessary to accomplish a task. And if the goal is communication and not quibbling, each implementation should overcome minor nonconformities with standard:\n\n\nInformation applicable to one version of SMPP can often be found in another version of SMPP; e.g. the only way how to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose.\n\nThe SMPP protocol is designed on a clear-text binary protocol which needs to be considered if using for potentially sensitive information such as one-time passwords via SMS. There are, however, implementations of SMPP over secure SSL/TLS if required.\n\n\n"}
{"id": "1941400", "url": "https://en.wikipedia.org/wiki?curid=1941400", "title": "Survival kit", "text": "Survival kit\n\nA survival kit is a package of basic tools and supplies prepared in advance as an aid to survival in an emergency. Civil and military aircraft, lifeboats, and spacecraft are equipped with survival kits.\n\nSurvival kits, in a variety of sizes, contain supplies and tools to provide a survivor with basic shelter against the elements, help him or her to keep warm, meet basic health and first aid needs, provide food and water, signal to rescuers, and assist in finding the way back to help. Supplies in a survival kit normally contain a knife (often a Swiss army knife or a multi-tool), matches, tinder, first aid kit, bandana, fish hooks, sewing kit, and a flashlight.\n\nCivilians such as forestry workers, surveyors, or bush pilots, who work in remote locations or in regions with extreme climate conditions may also be equipped with survival kits. Disaster supplies are also kept on hand by those who live in areas prone to earthquakes or other natural disasters. For the average citizen to practice disaster preparedness, some towns will have survival stores to keep survival supplies in stock.\n\nThe American Red Cross recommends an emergency preparedness kit that is easy to carry and use in the event of an emergency or disaster.\n\nThe general contents of an emergency survival kit depend on the location. Basic components in a survival kit address the needs of first aid, food, water, shelter, navigation, and signalling.\n\nA variety of materials are recommended for emergency shelters, and vary between geographic regions. Options often included in survival kits may consist of:\n\n\nFirst aid kits will often include a combination of the following:\n\n\nMost survival kits involve sustenance for short periods of time, to be used and replenished before contents spoil.\n\n\nSince the primary goal of a survival kit for lost or injured persons is rescue, this part of the kit may be considered the most essential. Key elements for rescue include:\n\n\nSurvival kit tools emphasize portability and versatility. Tools recommended for many types of survival kit include:\n\nLifeboat survival kits are stowed in inflatable or rigid lifeboats or life rafts; the contents of these kits are mandated by coast guard or maritime regulations. These kits provide basic survival tools and supplies to enable passengers to survive until they are rescued. In addition to relying on lifeboat survival kits, many mariners will assemble a \"ditch bag\" or \"abandon ship bag\" containing additional survival supplies. Lifeboat survival kit items typically include:\n\n\n\n\n\nSurvival kits for military aviators are often modified according to the environment of operations:\n\n\nThe US Army uses several basic survival kits, mainly for aviators, some of which are stored in carrying bags. Aviators in planes with ejection seats have survival kits in a vest and the seat pan, the survival vest worn by US helicopter crews also contains some basic survival items.\n\nAstronauts are provided with survival kits due to the difficulty of predicting where a spacecraft will land on its return to earth, especially in the case of an equipment failure. In early US space flights, the kit was optimised for survival at sea; the one provided for John Glenn on the first American space flight in Friendship 7 contained \"a life raft, pocket knife, signaling mirror, shark repellent, seawater desalting tablets, sunscreen, soap, first aid kit, and other items\". A survival kit was provided for the Apollo program which was \"...designed to provide a 48-hour postlanding (water or land) survival capability for three crewmen between 40 degrees North and South latitudes\". It contained \"a survival radio, a survival light assembly, desalter kits, a machete, sunglasses, water cans, sun lotion, a blanket, a pocket knife, netting and foam pads\".\n\nThe kits provided for Soviet and Russian Cosmonauts are optimised for survival in the temperate and sub-arctic mountains, forests and grasslands in the east of the country. Soyuz spacecraft kits include \"food rations, water bottles, warm clothing, rope for making a shelter using the capsule’s parachute, fish hooks and miscellaneous other survival gear\". The TP-82 Cosmonaut survival pistol, was provided to defend against predators such as wolves or bears. It was able to fire conventional bullets, shotgun cartridges and flares; the folding stock could be used as a shovel and it also had a fold-out machete.\n\n\"Mini survival kits\" or \"Altoids tin\" survival kits are small kits that contain a few basic survival tools. These kits often include a small compass, waterproof matches, minimum fishing tackle, large plastic bag, small candle, jigsaw blade, craft knife or scalpel blade, and/or a safety pin/s. Pre-packaged survival kits may also include instructions in survival techniques, including fire-starting or first aid methods. In addition, parachute cord can be wrapped around the tin. The parachute cord can be used for setting up an emergency shelter or snaring small animals. They are designed to fit within a container roughly the size of a mint tin.\n\nOther small kits are wearable and built into everyday carry survival bracelets or belts. Most often these are paracord bracelets with tools woven inside. Several tools such as firestarter, buckles, whistles and compass are on the exterior of the gear and smaller tools are woven inside the jewelry or belt and only accessible by taking the bracelet apart.\n\nLightweight survival kits are generally seen as a backup means of survival; however, these can kits can be extensive, and have come to include tools that are generally found in larger kits as survival technology advances. Some examples of these tools are high power flashlights, rapid use saws, signal devices such as mini signal mirrors, and water purification methods.\n\nAnother level in some preparedness plans are Vehicle Kits. In some cases, supplies and equipment may be loaded into vehicle such as a van or truck with bicycle racks and an extra “reserve” gas tank. Some survivalists also carry a small (e.g., 250 cc) off-road-capable motorcycle in the van or truck.\n\nFood supplies in a bug-out vehicle include hundreds of pounds of wheat, rice, and beans, and enough honey, powdered milk, canned goods, bottled fruit, vitamins, dehydrated fruits and vegetables, salt, pepper, spices, and oil for several months. In addition, the kits often contain high-calorie energy bars, a cooking kit, utensils, liquid soap, and towels. The water supplies may include bottled water, filtering kit, bottles, collapsible water containers, and chlorine bleach for water purification. Food preparation and washing equipment may include items such as a grain grinder, a bread mixer, a strainer, a manual can opener, a steam canner with canning jars and O-rings, cutlery, knives, an electric 12-volt cooler icebox, kerosene lamps and heaters, kerosene or propane stoves, extra fuel, a clothes wringer, a foot-operated treadle sewing machine, and an electric hot plate (which would require an inverter to operate off a car battery).\n\nThe medical supplies may include a blood pressure gauge, stethoscope, scissors, tweezers, forceps, disposable scalpels, two thermometers (oral and rectal), inflatable splints, bandages, sutures, adhesive tape, gauze, burn ointment, antibiotic ointment, aspirin, rubbing alcohol, ipecac syrup, sterile water, cotton rags, soap, and cotton swabs.\n\nThe transportation items may include bicycles with off-road tires and suspension, emergency tools and spare auto parts (e.g., fuses, fan belts, light bulbs, head light, tire pump, etc.), and an inflatable raft with paddles.\n\nIn addition, the kits may contain typical individual \"survival kit\" items, such as nylon tarps, extra clothes and coats, blankets, sleeping bags, matches or other fire starting equipment, a compass and maps, flashlights, toilet paper, soap, a pocket knife and bowie knife, a fishing kit, a portable camping stove, a power inverter, backpack, paper and pencil, a signaling mirror, whistle, cable saw, bleach, insect repellent, magnifying glass, rope and nylon cord, pulleys, and a pistol and ammunition.\n\nThe communications equipment may include a multi-band receiver/scanner, a citizens band (CB) radio, portable \"walkie-talkies\" with rechargeable batteries, and a portable battery-powered television. The power supplies may include a diesel or gasoline generator with a one-month fuel supply, an auto battery and charger, extension cord, flashlights, rechargeable batteries (with recharger), an electric multi meter, and a test light. Defense items include a revolver, semi-automatic pistol, rifle, shotgun, ammunition, mace or pepper spray, and a large knife such as a KA-BAR or a bowie knife.\n\nTools may include cutting tools such as saws, axes and hatchets; mechanical advantage aids such as a pry bar or wrecking bar, ropes, pulleys, or a 'come-a-long\" hand-operated winch; construction tools such as pliers, chisels, a hammer, screwdrivers, a hand-operated twist drill, vise grip pliers, glue, nails, nuts, bolts, and screws; mechanical repair tools such as an arc welder, an oxy-acetylene torch, a propane torch with a spark lighter, a solder iron and flux, wrench set, a nut driver, a tap and die set, a socket set, and a fire extinguisher. As well, some survivalists bring barterable items such as fishing line, liquid soap, insect repellent, light bulbs, can openers, extra fuels, motor oil, and ammunition.\n\nThe US government's Homeland Security website provides a list of in-home emergency kit items. The list focuses on the basics of survival: fresh water, food, clean air and materials to maintain body warmth. These basic survival items comprised into a kit are known as a Bug-out bag. The recommended basic emergency kit items include:\n\n\nBelow is list of commonly recommended items for an emergency earthquake kit:\n\n\nFor hurricanes, National Oceanic and Atmospheric Administration (NOAA) recommends that the 'disaster bag' include:\n\n\nThe term \"survival kit\" may also refer to the larger, portable survival kits prepared by survivalists, called \"bug-out bags\" (BOBs), \"Personal Emergency Relocation Kits\" (PERKs) or \"get out of Dodge\" (GOOD) kits, which are packed into backpacks, or even duffel bags. These kits are designed specifically to be more easily carried by the individual in case alternate forms of transportation are unavailable or impossible to use.\n\nThese bags contain supplies such as food, water purification equipment, clothing, medical equipment, communications gear, and tools.\n\n\n\n"}
{"id": "24812935", "url": "https://en.wikipedia.org/wiki?curid=24812935", "title": "Technomimetics", "text": "Technomimetics\n\nTechnomimetics are molecular systems that can mimic man-made devices. The term was first introduced in 1997. The current set of technomimetic molecules includes motors, rotors, gears, gyroscopes, tweezers, and other molecular devices. Technomimetics can be considered as the essential components of molecular machines and have the primary use in molecular nanotechnology.\n"}
{"id": "53838656", "url": "https://en.wikipedia.org/wiki?curid=53838656", "title": "The X-Files (season 11)", "text": "The X-Files (season 11)\n\nThe eleventh season of the American science fiction television series \"The X-Files\" premiered on January 3, 2018, on Fox. The season consists of ten episodes and concluded on March 21, 2018. It follows newly re-instated Federal Bureau of Investigation (FBI) agents Fox Mulder (David Duchovny) and Dana Scully (Gillian Anderson). The season's storyline picks up directly after last season's finale and the search for Mulder and Scully's son William is the main story arc of the season.\n\nDana Scully (Gillian Anderson) wakes up in the hospital after having a seizure, now realizing the events of \"My Struggle II\" were a vision and haven't actually happened yet. Mulder initially presumes Scully’s ramblings are a product of her illness but leaves the hospital to investigate. Agent Jeffrey Spender later appears at Scully’s bedside, revealing that someone is looking for William. He first balks at telling her the location of her son, revealing only the name of the family that adopted him - Van De Kamp. Mulder tails a henchman who he believes will take him to The Smoking Man, but he arrives somewhere else with mysterious conspirators; Mr. Y (A.C. Peterson) and Erika Price (Barbara Hershey). They try to negotiate with Mulder into turning over his son but Mulder refuses. Walter Skinner (Mitch Pileggi) tries to meet with Scully, but can't find her. As he goes to his car, he is met inside by The Smoking Man and Reyes. Meanwhile, Scully tries to leave the hospital but her seizures return causing her to crash her car. She is rescued by Agent Einstein and Agent Miller, and is readmitted to the hospital. Both agents leave the room. An assassin sent by Mr. Y and Erika enters and tries to suffocate Scully, but Mulder steps in and saves her by slicing the assassin's neck. As Skinner comes in, Mulder confronts him since he smells like smoke. In a flashback to when Skinner was in the car with the Smoking Man, the latter reveals (in a further flashback to 17 years, \"En Ami\") that \"he\", not Mulder, artificially impregnated Scully.\n\nScully opens her eyes in the dark, laying on her side on a bed in a mysterious house. She explores the mysterious house and ends up in an endless pattern of the same room. Mulder theorizes that the experience is a type of sleep paralysis. She goes back to her memories of her sleep paralysis, where she grabs a snow globe with a model of the boat in it. Mulder and Scully get to the dock, meeting with Detective Costa (Louis Ferreira). Later, Scully’s attention is drawn to a man that observes them intently from one of the portable walkways. Costa informs them that the victims asked if they found Ghouli. When Scully turns to check if the mystery guy is there, he’s gone. Both agents investigate the two teenagers about the slashing. Both of them go on about the experience and their boyfriend, Jackson Van De Kamp. The agents arrive at the Van De Kamp household, the same house Scully saw in her dream. Two gunshots are heard with a third coming from upstairs. Scully goes up and finds the bleeding corpse of Jackson Van De Kamp. As they leave the room and the door clicks closed, the zipper opens slowly. Jackson sits up and escapes the room. Mulder meets with Skinner at the Chimera, telling him to drop the investigation. Mulder confirms that Jackson's DNA matches Scully's. At the hospital, Jackson explains to Brianna that he’s been hiding because there are people after him. He reveals that Ghouli doesn't exist and it's just something he \"made up\". Jackson escapes the hospital after the death of the DOD members. The next day, Scully spots a rural gas station that has a windmill just outside. Scully meets with Peter Wong with Wong mentioning that he wishes he could know her better. The agents ask to see the recording tapes. In the recording, Scully is having a conversation with William. She smiles as Mulder holds her while they see their son, alive. Over the course of the season, they search for William. In the third episode of the \"Plus One\", Scully and Mulder are intimate again. In the season 11 finale, \"My Struggle IV\", she reveals to Mulder that she is pregnant with his child. Also in the finale, the Smoking Man shoots William, who has made himself look like Mulder.\n\n\n\n\nEpisodes marked with a double dagger () are episodes in the series' alien mythology arc.\n<onlyinclude></onlyinclude>\n\nIn May 2015, in an interview with \"Entertainment Weekly\", co-star David Duchovny stated that if the 2016 revival were successful, he would be interested in another renewal, stating, \"I would be open to doing another cycle. I don't know that I could do a 20-episode version of this show at this point in my life, and I don't know that Gillian could. But I think everybody is open ended on what happens after this. Certainly, we didn't bring it back with the idea of ending it.\" At the 2016 Television Critics Association panel for the series, creator Chris Carter stated, \"If we do well in the ratings, I can't imagine we wouldn't be asked to do more.\" Fox Television Group chairman and CEO Gary Newman stated that he and fellow executive Dana Walden would be open to another renewal, stating, \"We would love to do this again, so we'd be on board if schedules can be worked out.\" In an interview with \"Variety\" prior to the premiere, co-star Gillian Anderson, Duchovny, and Walden stated their willingness for another renewal. In an interview with \"TV Guide\", Carter reaffirmed plans for more episodes, but stressed that everyone involved is waiting on the success of the season. In May 2016, Walden confirmed that there had been conversations about an eleventh season possibly for the 2017–18 television season.\n\nOn April 20, 2017, Fox officially announced that \"The X-Files\" would be returning for an eleventh season of ten episodes, which would air in the 2017–18 television season after filming in mid-2017.\n\nThe writing staff for the eleventh season consists of Chris Carter, Glen Morgan, Darin Morgan, James Wong, and newcomers who were all previously part of \"The X-Files\" production staff, Gabe Rotter (writers assistant), Benjamin Van Allen (writers assistant) and Brad Follmer (Carter's personal assistant). In August 2017, it was announced that three female writers were added to the staff; Karen Nielsen (who worked as a script coordinator on season 10) wrote one episode, and Kristen Cloke and Shannon Hamblin wrote another episode based on a story by Glen Morgan. The season, consisting of 10 episodes, has two mythology episodes (the premiere and finale) and eight standalone episodes; however, several critics have considered episode five, \"Ghouli\", to also be a mythology episode.\n\nIn addition to Duchovny and Anderson, Mitch Pileggi was confirmed to reprise his role, as Walter Skinner. Also returning are William B. Davis as Cigarette Smoking Man and Annabeth Gish as Monica Reyes. In August 2017, it was confirmed that Robbie Amell and Lauren Ambrose would reprise their roles Agents Miller and Einstein from season ten. In September 2017, it was announced that Karin Konoval–who played Mrs. Peacock in the fourth-season episode \"Home\"–would play new characters in the eleventh season, in what Carter described as a \"tour-de-force performance\". Anderson commented that this is her final season of \"The X-Files\", saying, \"I've said from the beginning this is it for me.\" However, Chris Carter affirmed he would not do \"The X-Files\" without her character, saying, \"For me, \"The X-Files\" is Mulder and Scully. I think if it were without Scully, I wouldn't do it. That's not my \"X-Files\".\" The writers intended to bring back Robert Patrick as his character John Doggett, but he was unavailable due to scheduling issues as he was filming for his TV series \"Scorpion\". In February 2018, Carter stated in an interview that he could see the show continuing without Anderson.\n\nFilming began in August 2017 in Vancouver, British Columbia, where the previous season was filmed, along with the show's original five seasons. After controversy sparked regarding the lack of female writers or directors on the series, it was announced in August 2017 that Carol Banker, a script supervisor on the original series who also directed an episode of \"The Lone Gunmen\" and Holly Dale were added as directors.\n\nThe eleventh season has received generally positive reviews from critics and currently has a Metacritic score of 67 out of 100 based on 18 reviews. On Rotten Tomatoes, the season has an approval rating of 78% with an average rating of 6.99 out of 10 based on 30 reviews. The site's consensus reads, \"Though it may not make many new believers, \"The X-Files\" return to business as usual is a refreshing upgrade from the show's underwhelming previous outing.\"\n\nBased on the first five episodes available for review, Liz Shannon Miller of IndieWire gave the season a positive review with a \"B+\" grade. While Miller wrote that the season premiere \"is not good\", she was much more positive of the following four standalone episodes, writing that \"there are some intriguing mysteries, some eerie moments, solid action\" and praised Darin Morgan's comedy installment, calling it \"fantastic\". Overall, she concluded, \"Season 11 so far isn't flawless, but it's a lively, character-focused affair that feels far more unified than we'd ever anticipated, a massive improvement over Season 10 that gives us genuine hope for the second half. For the first time in a while, we're truly excited to see more.\" Matt Zoller Seitz of \"Vulture\" stated that compared to the previous season, \"This new batch of episodes is considerably stronger. Even the ones that don't really do much but spin their wheels do so with feeling, and when the show is great — as it is, yet again, in Darin Morgan's episode — it's downright sublime.\"\n\n"}
{"id": "2321349", "url": "https://en.wikipedia.org/wiki?curid=2321349", "title": "Topping out", "text": "Topping out\n\nIn building construction, topping out (sometimes referred to as topping off) is a builders' rite traditionally held when the last beam (or its equivalent) is placed atop a structure during its construction. Nowadays, the ceremony is often parlayed into a media event for public relations purposes. It has since come to mean more generally finishing the structure of the building, whether there is a ceremony or not.\n\nThe practice of \"topping out\" a new building can be traced to the ancient Scandinavian religious rite of placing a tree atop a new building to appease the tree-dwelling spirits displaced in its construction. Long an important component of timber frame building, it migrated initially to England and Northern Europe, thence to the Americas.\n\nA tree or leafy branch is placed on the topmost wood or iron beam, often with flags and streamers tied to it. A toast is usually drunk and sometimes workers are treated to a meal. In masonry construction the rite celebrates the bedding of the last block or brick.\n\nIn some cases a topping out event is held at an intermediate point, such as when the roof is dried-in, which means the roof can provide at least semi-permanent protection from the elements.\n\nThe practice remains common in the United Kingdom and assorted Commonwealth countries such as Australia, and Canada as well as Germany, Austria, Iceland, Chile, Czech Republic, Slovakia, Poland, Hungary, the Baltic States, and the United States, where the last beam of a skyscraper is painted white and signed by all the workers involved. In New Zealand, completion of the roof to a water-proof state is celebrated through a \"roof shout\", where workers are treated to cake and beer.\n\nThe tradition of \"pannenbier\" (literally \"(roof) tile beer\" in Dutch) is popular in the Netherlands and Flanders, where a national, regional or city flag is hung once the highest point of a building is reached. It stays in place until the building's owner provides free beer to the workers, after which it is lowered. It is considered greedy if it remains flown for more than a few days.\n\n\n"}
{"id": "14466070", "url": "https://en.wikipedia.org/wiki?curid=14466070", "title": "TransXChange", "text": "TransXChange\n\nTransXChange is a UK national XML based data standard for the interchange of bus route and timetable information between bus operators, the Vehicle and Operator Services Agency, local authorities and passenger transport executives, and others involved in the provision of passenger information.\n\nThe format is a UK national de facto standard sponsored by the UK Department of Transport. The standard is part of a family of coherent transport related XML standards that follow UK GovTalk guidelines and are based on the CEN Transmodel conceptual model.\n\nAlthough TransXChange is currently used mainly to exchange bus timetables, it may also be used for schedules for rail and other modes.\n\nTransXChange is intended as a successor to the widely used ATCO-CIF format for bus timetables and was developed as a modernised representation of ATCO-CIF content, using an XML representation based on the Transmodel Reference model for Public Transport.\n\n\nTransXChange is supported by all main UK suppliers of bus timetable systems and has also been used to exchange data for metro and other modes.\n\nTransXChange provides a rich model based representation of a bus timetable that can be used for a wide variety of purposes. TransXChange documents can be used to exchange the following information:\n\n\nTransXChange comprises:\n\nEach version of TransXChange is versioned in line with UK GovTalk guidelines.\n\nTransXChange is accompanied by a free tool,the TransXChange Publisher, which renders a TransXChange document into a human readable format, matrix and route map. PDF and html formats are supported.\n\nAddition modules are envisaged to cover ticketing and fares through the proposed FareXChange standard. TransXChange can also be used for other modes of transport - it is already used for metro and tram systems, route and timetable data.\n\n\n\n"}
{"id": "51473163", "url": "https://en.wikipedia.org/wiki?curid=51473163", "title": "Vacuum blasting", "text": "Vacuum blasting\n\nVacuum blasting is an abrasive blasting method, also referred to as dustless blasting or closed loop abrasive blasting. The method is characterized by a blast tool that does abrasive blasting and collects both used blast media, and loosened particles from the surface to be treated, simultaneously. The blast tool is equipped with a blast hose and a suction hose, that both run from the blast tool to a control unit. The control unit supplies the blast tool with pressurized air mixed with blast media, and sucks back dust, loosened particles and used blast media. The control unit continuously separates dust and loosened particles from the used blast media, and sends used blast media back into the pressurized air flow. The dust and loosened particles are collected in a waste bin.\n\nThe method is typically used in areas where dust and spill from regular abrasive blasting is not wanted, for example due to HSE-considerations. The blast process itself is slower than regular abrasive blasting, but requires less sheeting and scaffolding \n"}
{"id": "37184065", "url": "https://en.wikipedia.org/wiki?curid=37184065", "title": "Wirsol", "text": "Wirsol\n\nWirsol (own spelling \"WIRSOL\") is a brand of project developer and energy service company Wircon GmbH. Wircon GmbH was founded in December 2013. Its key activities are the provision of sustainable energy projects, primarily on the German market. The focus of the Wircon Group lies in the fields of large wind and photovoltaic projects; small roof-top photovoltaic systems for private customers; large-scale roof-top photovoltaic systems primarily for commercial customers and sustainable operation of renewable energy plants using these technologies. The company states that it has some 11 000 customers (as of February 2017).\n\nThe former Wirsol group was an international project corporation for the planning, construction and financing of renewable energy power plants, in particular solar power plants. The company’s headquarters were located in Germany. Further company offices were located in Spain, Italy, Belgium, Great Britain, Switzerland, Canada, the United States, Brazil, Japan, China, Malaysia and Maldives. On a global level, Wirsol was responsible for the installation of over 7 000 solar systems with a total output of 450 MWp. In autumn 2013 insolvency proceedings were initiated for the Wirsol corporate group, with Wircon GmbH taking over some divisions in March 2014.\n\n\nThe company was founded in Waghäusel (District of Karlsruhe in Germany), where its headquarters were also located, on 1 February 2003 by Markus Wirth, Hans Wirth and Stefan Riel as the “Die Hausrenovierer GmbH”. Having increasingly specialized in photovoltaics, in January 2004 the company was renamed “Wirth Solar GmbH”. Due to an imminent legal dispute with Würth Solar GmbH in 2007 the owners decided to change the name again, to “Wirsol Deutschland GmbH”, which became a subsidiary holding of the newly founded “Wirsol Solar AG” holding. In 2010 the Board of Directors was expanded by the founders, with Bernd Kästner becoming the chief financial officer. Nikolaus Krane, previously a member of the Conergy board, assumed responsibility for international finance products, large-scale projects and communications.\n\nTo promote business relations in Asia Wirsol set up Wirsol Solar Technology Beijing Ltd., a company site in China and, in June 2012, a site for Southeast Asian activities in Kuala Lumpur, Malaysia.\n\nIn October 2013 the Wirsol group initiated preliminary insolvency proceedings. The application was approved by Karlsruhe District Court on 24 October 2013 and a preliminary insolvency administrator was appointed, while the company’s management continued to focus on restructuring the business within the scope of insolvency plan proceedings. Wirsol Solar AG, Wirsol Deutschland GmbH and Wirsol Solar System GmbH fell under this planned insolvency.\n\nDue to its financial difficulties the Wirsol group planned to wind up its “Wind power” division, founded in November 2012, and eight of the company’s employees were given paid leave as of 4 November 2013.\n\nIn December 2013 Wirsol E-Mobility GmbH filed for insolvency on the grounds of impending insolvency. This application was approved by Karlsruhe District Court and a preliminary insolvency administrator was appointed. The measures affected three employees.\n\nIn March 2014 some parts of Wirsol GmbH’s German operations were taken over by Wircon GmbH. The new owner continued the Wirsol brand and offered employment to 80 of Wirsol’s 300 employees. WIRCON is owned by Dietmar Hopp.\n\nIn 2007 the company opened the Bruhrain solar test park in close proximity to its headquarters. At that time, a total surface area of 12 hectares made it the largest solar park in the state of Baden-Württemberg. With almost 31 000 modules installed, the plant generates an output up to 2.258 MWp.\n\nIn April 2010 the company installed a photovoltaic system at the Hockenheimring German Formula One race track with a rated output of 848.88 kWp. A total of 4 716 solar modules were installed along 405 metres of the track.\n\nOne of the largest projects finalized by Wirsol in the financial year 2011 was located in Mixdorf in Brandenburg, where the company built a system with a peak performance of 24.1 MWp on the 81-hectare site of a former Russian fuel depot.\n\nLikewise, in 2012 Wirsol’s high-output Luckau solar park, constructed on a conversion site in Brandenburg and with a performance of almost 21 MWp, was connected to the grid. The solar power plant is located at the former military airport in Alteno.\n\nAt the end of July 2013, the company completed construction of what was, at that time, Europe’s largest crystalline roof-top solar system. The photovoltaic system, located at freight forwarding company Pfenning Logistics’ 11-hectare distribution centre in Heddesheim, northern Baden, has a peak output of 8.1 MWp.\n\nThe total output of Wirsol’s projects in 2015 was in excess of 150 megawatts.\n\nIn 2016 Wirsol commenced its first tenants’ electricity projects in Baden-Württemberg and Bavaria, working in partnership with municipal housing associations and utility companies.\n\nWirsol has been participating in work on the intelligent grid in Baden-Württemberg since February 2017. The company is involved in the C/sells platform, the “showcase for intelligent energy – digital agenda for the energy revolution” (SINTEG), a funding programme initiated by the German Federal Ministry of Economics.\n\nIn August 2017 Wirsol realized two new large roof-top systems for long-established world market leader SEW -EURODRIVE. A total of four plants at two locations now generate total output in excess of two megawatts.\n\nWirsol’s Straubenhardt wind park, comprising 11 wind turbines generating a total output of approx. 33 megawatts, was connected to the grid in March 2018. The park is capable of supplying some 22 000 households with green electricity.\n\nIn 2012 Wirsol founded the “Wirsol RE Maldives” joint venture with Renewable Energy Maldives (REM) to install solar power stations on the Maldives. Further international projects are located in Spain (Bovera, Barcelona, Cáceres), England (Cornwall), Italy (Mola di Bari), Belgium (Wilrijk) and the US state of Colorado (Fort Collins).\n\nIn 2013 the company announced that work to build a 22-MWp solar park on the Japanese island of Honshu had begun and gave the starting signal for the Monte Plata solar park in the Dominican Republic. The latter park has, in the meantime, been sold to Soventix. Construction of the park in Japan was scheduled to begin in 2014.\n\nWith the Solarpark Lerchenborg (61 MWp), built in 2015, the company built what was, at the time, Scandinavia’s largest solar park. \nJanuary 2017 saw the announcement of the start-up of the largest solar park in the Netherlands, considered to be a flagship project and with overall output of 30 MWp.\n\nAlso in 2015, Wirsol installed Switzerland’s sixth largest supermarket roof-top system. The system’s total output is approx. 1.94 MWp.\n\nIn February 2017 Wirsol Energy Ltd refinanced 10 solar parks with overall output of 61 MWp and financing of 46 million pounds.\n\nWirsol entered the Australian market in March 2017, investing a total of 380 million Australian dollars and founding a subsidiary headquartered in Sydney. The company’s first projects were three solar parks – “Whitsunday” and “Hamilton” in Queensland and “Gannawarra” in Victoria – generating a total of 198 MWp.\n\nApril 2017 saw the company announce the connection of 19 solar parks to the grid. These sites, all located in England and Northern Ireland, generate outputs ranging from 20.5 MWp to 2.5 MWp, thus supplying a total of 105.5 MWp electricity. They were financed with loans amounting to approx. GBP 88 million.\n\nWirsol’s office in Sydney, Australia was inaugurated in May 2017.\n\nIn May 2017 the company also sold part of its solar park portfolio in the United Kingdom to investor Rockfire Capital. The portfolio comprises 19 solar parks, of which two are located in Northern Ireland. The 19 solar energy sites, each with outputs ranging from 2.5 MWp to 20.5 MWp, generate a total output of some 105 MWp. Taken in total, the portfolio produces enough energy to generate almost 100 gigawatt hours of electricity each year.\n\nWirsol became Australia’s largest solar park project developer in November 2017, achieving a market share of just under 12 percent. In addition to this, the company accounted for 19.5 percent of all projects being realized throughout Australia at that time.\n\nDecember 2017 saw Wirsol Energy Ltd acquire two non-government subsidized solar power plants in the United Kingdom – Outwood Solar Park, Essex with 7 MWp output and Trowse-Newton Solar Park, Norfolk with output of 9 MWp.\n\nFollowing a bidding procedure for a government tender in Australia, in March 2018 Wirsol Energy und Edify Energy were awarded the contract for a large-scale storage facility. A 25MW / 50MWh Tesla lithium-ion battery storage facility - the Gannawarra Energy Storage System – will be integrated into the 50MW Gannawarra Solar Farm.\n\nWirsol Solar AG became an official sponsor of soccer club TSG 1899 Hoffenheim in the 2011/2012 Bundesliga season, giving its name to the club’s stadium, the Wirsol Rhein-Neckar-Arena in Sinsheim. This sponsorship of the arena has been continued by Wircon GmbH with its Wirsol brand since the 2014/2015 season.\n"}
{"id": "794328", "url": "https://en.wikipedia.org/wiki?curid=794328", "title": "Yoke", "text": "Yoke\n\nA yoke is a wooden beam normally used between a pair of oxen or other animals to enable them to pull together on a load when working in pairs, as oxen usually do; some yokes are fitted to individual animals. There are several types of yoke, used in different cultures, and for different types of oxen. A pair of oxen may be called a \"yoke of oxen\", and yoke is also a verb, as in \"to \"yoke\" a pair of oxen\". Other animals that may be yoked include horses, mules, donkeys, and water buffalo.\n\nThe word \"yoke\" is believed to derive from Proto-Indo-European *yugóm (yoke), from verb *yeug- (join, unite). This root has descendants in almost all known Indo-European languages including German \"Joch\", Latin \"iugum\", Ancient Greek ζυγόν (zygon), Persian یوغ (yuğ), Sanskrit युग (yugá), Hittite 𒄿𒌑𒃷 (iúkan), Old Church Slavonic иго (igo), Lithuanian \"jungas\", Old Irish \"cuing\", Armenian լուծ (luç), etc. (all meaning \"yoke\"). The verb to \"subjugate\" derives from the Latin form.\n\nA \"bow yoke\" is a shaped wooden crosspiece bound to the necks of a pair of oxen (or occasionally to horses). It is held on the animals' necks by an oxbow, from which it gets its name. The oxbow is usually U-shaped and also transmits force from the animals' shoulders. A swivel between the animals, beneath the centre of the yoke, attaches to the pole of a vehicle or to chains (traces) used to drag a load.\n\nBow yokes are traditional in Europe, and in the United States, Australia and Africa.\n\nA \"head yoke\" fits onto the head of the oxen. It usually fits behind the horns, and has carved-out sections into which the horns fit; it may be a single beam attached to both oxen, or each ox may have a separate short beam (see picture). The yoke is then strapped to the horns of the oxen with yoke straps. Some types fit instead onto the front of the head, again strapped to the horns, and ox pads are then used for cushioning the forehead of the ox. A tug pole is held to the bottom of the yoke using yoke irons and chains. The tug pole can either be a short pole with a chain attached for hauling, or a long pole with a hook on the end that has no chain at all. Sometimes the pole is attached to a wagon and the oxen are simply backed over this pole, the pole is then raised between them and a backing bolt is dropped into the chains on the yoke irons in order to haul the wagon.\n\nHead yokes are widely used in southern Europe, much of South America and in Canada.\n\nA \"withers yoke\" is a yoke that fits just in front of the withers, or the shoulder blades, of the oxen. The yoke is held in position by straps, either alone or with a pair of wooden staves on either side of the ox's withers; the pull is however from the yoke itself, not from the staves. Withers yokes particularly suit zebu cattle, which have high humps on their withers.\n\nWithers yokes are widely used in Africa and India, where zebu cattle are common.\nAlthough all three yoke types are effective, each has its advantages and disadvantages. As mentioned above, withers yokes suit zebu cattle, and head yokes can of course only be used for animals with suitable horns. Head yokes need to be re-shaped frequently to fit the animals' horns as they grow; unlike other types, a single-beam head yoke fixes the heads of the oxen apart, helping them to stand quietly without fighting. A single-beam head yoke may offer better braking ability on downhill grades and appears to be preferred in rugged mountainous areas such as Switzerland, Spain and parts of Italy. Bow yokes need to be the correct size for the animal, and new ones are often made as the animal grows, but they need no adjustment in use. Whichever type is used, various lengths of yoke may be required for different agricultural implements or to adjust to different crop-row spacings.\n\nA yoke may be used with a single animal. Oxen are normally worked in pairs, but water buffalo in Asian countries are commonly used singly, with the aid of a bow-shaped withers yoke. Use of single bow or withers yokes on oxen is documented from North America, China, Zimbabwe, Tanzania and Switzerland, and several designs of single head or forehead yoke are recorded in Germany.\n\nThe yoke has connotations of subservience and toiling; in some ancient cultures it was traditional to force a vanquished enemy to pass beneath a symbolic yoke of spears or swords. The yoke may be a metaphor for something oppressive or burdensome, such as feudalism, imperialism, corvée, tribute, or conscription, as in the expressions the \"Norman Yoke\" (in England), the \"Tatar Yoke\" (in Russia), or the \"Turkish Yoke\" (in the Balkans).\n\nThe metaphor can also refer to the state of being linked or chained together by contract or marriage, similar to a pair of oxen. This sense is also the source of the word yoga, as linking with the divine.\n\nThe yoke is frequently used metaphorically in the Bible, first in Genesis regarding Esau. \n\nIn the 20th century, the yoke and arrows became a political symbol of the Falange political movement in Spain.\n\n\n"}
