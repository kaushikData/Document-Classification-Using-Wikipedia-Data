{"id": "7529400", "url": "https://en.wikipedia.org/wiki?curid=7529400", "title": "ArabDev", "text": "ArabDev\n\nArabDev is a Giza (Egypt)-based non-profit organization that uses Information and Communication Technology (ICT) to \"promote existing development projects and for innovative developmental initiatives\". ArabDev provides women and youth with an educational venue for livelihood opportunities, through improved access to information and its use in skill development as well as small and micro enterprises. \n\nIt was formed by its non-governmental organizations (NGOs) both in Egypt and in the wider Arab region. ArabDev seeks to support ICT capacity-building among disadvantaged communities and grassroots organizations to get access to better work and life opportunities.\n\nIts focus includes, using ICTs for:\n\n\nArabDev, which is a member of the Association for Progressive Communications, has reported recent initiatives to spread free software among youth in the disadvantaged south of Egypt. The group reported working with two middle schools in the Al Menia governorate, and also with the Faculty of Computer Engineering of the Menia University.\n\nFor this project, the goal is to establish a \"core group\" of FLOSS users in the Minya Governorate, which could act as a south Egyptian \"diffusion point\".\n\nSo far, it has created computer labs or telecentres in some schools, which also offer their services to the community. In the Association for Progressive Communications report for 2005-06, it was stated that ArabDev reached an agreement with the Egyptian education department to collect fees, so as to make this venture sustainable.\n\n"}
{"id": "1430450", "url": "https://en.wikipedia.org/wiki?curid=1430450", "title": "Automatic test equipment", "text": "Automatic test equipment\n\nAutomatic test equipment or automated test equipment (ATE) is any apparatus that performs tests on a device, known as the device under test (DUT), equipment under test (EUT) or unit under test (UUT), using automation to quickly perform measurements and evaluate the test results. An ATE can be a simple computer-controlled digital multimeter, or a complicated system containing dozens of complex test instruments (real or simulated electronic test equipment) capable of automatically testing and diagnosing faults in sophisticated electronic packaged parts or on wafer testing, including system on chips and integrated circuits. \nATE is widely used in the electronic manufacturing industry to test electronic components and systems after being fabricated. ATE is also used to test avionics and the electronic modules in automobiles. It is used in military applications like radar and wireless communication.\n\nSemiconductor ATE, named for testing semiconductor devices, can test a wide range of electronic devices and systems, from simple components (resistors, capacitors, and inductors) to integrated circuits (ICs), printed circuit boards (PCBs), and complex, completely assembled electronic systems. ATE systems are designed to reduce the amount of test time needed to verify that a particular device works or to quickly find its faults before the part has a chance to be used in a final consumer product. To reduce manufacturing costs and improve yield, semiconductor devices should be tested after being fabricated to prevent defective devices ending up with the consumer.\n\nThe semiconductor ATE architecture consists of master controller (usually a computer) that synchronizes one or more source and capture instruments (listed below). Historically, custom-designed controllers or relays were used by ATE systems. The Device Under Test (DUT) is physically connected to the ATE by another robotic machine called a handler or prober and through a customized Interface Test Adapter (ITA) or \"fixture\" that adapts the ATE's resources to the DUT.\n\nThe industrial PC is nothing but a normal desktop computer packaged in 19-inch rack standards with sufficient PCI / PCIe slots for accommodating the Signal stimulator/sensing cards. This takes up the role of a controller in the ATE. Development of test applications and result storage is managed in this PC. Most modern semiconductor ATEs include multiple computer-controlled instruments to source or measure a wide range of parameters. The instruments may include device power supplies (DPS), parametric measurement units (PMU), arbitrary waveform generators (AWG), digitizers, digital IOs, and utility supplies. The instruments perform different measurements on the DUT, and the instruments are synchronized so that they source and measure waveforms at the proper times. Based on the requirement of response-time, real-time systems are also considered for stimulation and signal capturing.\n\nThe mass interconnect is a connector interface between test instruments (PXI, VXI, LXI, GPIB, SCXI, & PCI) and devices/units under test (D/UUT). This section acts as a nodal point for signals going in/out between ATE and D/UUT.\n\nFor example, to measure a voltage of a particular semiconductor device, the Digital Signal Processing (DSP) instruments in the ATE measure the voltage directly and send the results to a computer for signal processing, where the desired value is computed. This example shows that conventional instruments, like an Ammeter, may not be used in many ATEs due to the limited number of measurements the instrument could make, and the time it would take to use the instruments to make the measurement. One key advantage to using DSP to measure the parameters is time. If we have to calculate the peak voltage of an electrical signal and other parameters of the signal, then we have to employ a peak detector instrument as well as other instruments to test the other parameters. If DSP-based instruments are used, however, then a sample of the signal is made and the other parameters can be computed from the single measurement.\n\nNot all devices are tested equally. Testing adds costs, so low-cost components are rarely tested completely, whereas medical or high costs components (where reliability is important) are frequently tested.\n\nBut testing the device for all parameters may or may not be required depending on the device functionality and end user. For example, if the device finds application in medical or life-saving products then many of its parameters must be tested, and some of the parameters must be guaranteed. But deciding on the parameters to be tested is a complex decision based on cost vs yield. If the device is a complex digital device, with thousands of gates, then test fault coverage has to be calculated. Here again, the decision is complex based on test economics, based on frequency, number and type of I/Os in the device and the end-use application...\n\nATE can be used on packaged parts (typical IC 'chip') or directly on the Silicon Wafer. Packaged parts use a handler to place the device on a customized interface board, whereas silicon wafers are tested directly with high precision probes. The ATE systems interact with the handler or prober to test the DUT.\n\nATE systems typically interface with an automated placement tool, called a \"handler\", that physically places the Device Under Test (DUT) on an Interface Test Adapter (ITA) so that it can be measured by the equipment. There may also be an Interface Test Adapter (ITA), a device just making electronic connections between the ATE and the Device Under Test (also called Unit Under Test or UUT), but also it might contain an additional circuitry to adapt signals between the ATE and the DUT and has physical facilities to mount the DUT. Finally, a socket is used to bridge the connection between the ITA and the DUT. A socket must survive the rigorous demands of a production floor, so they are usually replaced frequently.\n\nSimple electrical interface diagram: ATE → ITA → DUT (package) ← Handler\n\nWafer-based ATEs typically use a device called a Prober that moves across a silicon wafer to test the device.\n\nSimple electrical interface diagram: ATE → Prober → Wafer (DUT)\n\nOne way to improve test time is to test multiple devices at once. ATE systems can now support having multiple \"sites\" where the ATE resources are shared by each site. Some resources can be used in parallel, others must be serialized to each DUT.\n\nThe ATE computer uses modern computer languages (like C, C++, Java, Python, LabVIEW or Smalltalk) with additional statements to control the ATE equipment through standard and proprietary application programming interfaces (API). Also some dedicated computer languages exists, like Abbreviated Test Language for All Systems (ATLAS). Automatic test equipment can also be automated using a test execution engine such as National Instruments' TestStand.\n\nSometimes automatic test pattern generation is used to help design the series of tests.\n\nMany ATE platforms used in the semiconductor industry output data using Standard Test Data Format (STDF)\n\nAutomatic test equipment diagnostics is the part of an ATE test that determines the faulty components. ATE tests perform two basic functions. The first is to test whether or not the Device Under Test is working correctly. The second is when the DUT is not working correctly, to diagnose the reason. The diagnostic portion can be the most difficult and costly portion of the test. It is typical for ATE to reduce a failure to a cluster or ambiguity group of components. One method to help reduce these ambiguity groups is the addition of analog signature analysis testing to the ATE system. Diagnostics are often aided by the use of flying probe testing.\n\nThe addition of a high-speed switching system to a test system's configuration allows for faster, more cost-effective testing of multiple devices, and is designed to reduce both test errors and costs. Designing a test system's switching configuration requires an understanding of the signals to be switched and the tests to be performed, as well as the switching hardware form factors available.\n\nSeveral modular electronic instrumentation platforms are currently in common use for configuring automated electronic test and measurement systems. These systems are widely employed for incoming inspection, quality assurance, and production testing of electronic devices and subassemblies. Industry-standard communication interfaces link signal sources with measurement instruments in \"rack-and-stack\" or chassis-/mainframe-based systems, often under the control of a custom software application running on an external PC.\n\nThe General Purpose Interface Bus (GPIB) is an IEEE-488 (a standard created by the Institute of Electrical and Electronics Engineers) standard parallel interface used for attaching sensors and programmable instruments to a computer. GPIB is a digital 8-bit parallel communications interface capable of achieving data transfers of more than 8 Mbytes/s. It allows daisy-chaining up to 14 instruments to a system controller using a 24-pin connector. It is one of the most common I/O interfaces present in instruments and is designed specifically for instrument control applications. The IEEE-488 specifications standardized this bus and defined its electrical, mechanical, and functional specifications, while also defining its basic software communication rules. GPIB works best for applications in industrial settings that require a rugged connection for instrument control.\n\nThe original GPIB standard was developed in the late 1960s by Hewlett-Packard to connect and control the programmable instruments the company manufactured. The introduction of digital controllers and programmable test equipment created a need for a standard, high-speed interface for communication between instruments and controllers from various vendors. In 1975, the IEEE published ANSI/IEEE Standard 488-1975, IEEE Standard Digital Interface for Programmable Instrumentation, which contained the electrical, mechanical, and functional specifications of an interfacing system. This standard was subsequently revised in 1978 (IEEE-488.1) and 1990 (IEEE-488.2). The IEEE 488.2 specification includes the Standard Commands for Programmable Instrumentation (SCPI), which define specific commands that each instrument class must obey. SCPI ensures compatibility and configurability among these instruments.\n\nThe IEEE-488 bus has long been popular because it is simple to use and takes advantage of a large selection of programmable instruments and stimuli. Large systems, however, have the following limitations:\n\n\nThe LXI Standard defines the communication protocols for instrumentation and data acquisition systems using Ethernet. These systems are based on small, modular instruments, using low-cost, open-standard LAN (Ethernet). LXI-compliant instruments offer the size and integration advantages of modular instruments without the cost and form factor constraints of card-cage architectures. Through the use of Ethernet communications, the LXI Standard allows for flexible packaging, high-speed I/O, and standardized use of LAN connectivity in a broad range of commercial, industrial, aerospace, and military applications. Every LXI-compliant instrument includes an Interchangeable Virtual Instrument (IVI) driver to simplify communication with non-LXI instruments, so LXI-compliant devices can communicate with devices that are not themselves LXI compliant (i.e., instruments that employ GPIB, VXI, PXI, etc.). This simplifies building and operating hybrid configurations of instruments.\n\nLXI instruments sometimes employ scripting using embedded test script processors for configuring test and measurement applications. Script-based instruments provide architectural flexibility, improved performance, and lower cost for many applications. Scripting enhances the benefits of LXI instruments, and LXI offers features that both enable and enhance scripting. Although the current LXI standards for instrumentation do not require that instruments be programmable or implement scripting, several features in the LXI specification anticipate programmable instruments and provide useful functionality that enhances scripting's capabilities on LXI-compliant instruments.\n\nThe VXI bus architecture is an open standard platform for automated test based on the VMEbus. Introduced in 1987, VXI uses all Eurocard form factors and adds trigger lines, a local bus, and other functions suited for measurement applications. VXI systems are based on a mainframe or chassis with up to 13 slots into which various VXI instrument modules can be installed. The chassis also provides all the power supply and cooling requirements for the chassis and the instruments it contains. VXI bus modules are typically 6U in height.\n\nPXI is a peripheral bus specialized for data acquisition and real-time control systems. Introduced in 1997, PXI uses the CompactPCI 3U and 6U form factors and adds trigger lines, a local bus, and other functions suited for measurement applications. PXI hardware and software specifications are developed and maintained by the PXI Systems Alliance. More than 50 manufacturers around the world produce PXI hardware.\n\nThe USB connects peripheral devices, such as keyboards and mice, to PCs. The USB is a Plug and Play bus that can handle up to 127 devices on one port, and has a theoretical maximum throughput of 480 Mbit/s (high-speed USB defined by the USB 2.0 specification). Because USB ports are standard features of PCs, they are a natural evolution of conventional serial port technology. However, it is not widely used in building industrial test and measurement systems for a number of reasons; for example, USB cables are not industrial grade, are noise sensitive, can accidentally become detached, and the maximum distance between the controller and the device is 30 m. Like RS-232, USB is useful for applications in a laboratory setting that do not require a rugged bus connection.\n\nRS-232 is a specification for serial communication that is popular in analytical and scientific instruments, as well for controlling peripherals such as printers. Unlike GPIB, with the RS-232 interface, it is possible to connect and control only one device at a time. RS-232 is also a relatively slow interface with typical data rates of less than 20 kbytes/s. RS-232 is best suited for laboratory applications compatible with a slower, less rugged connection.\n\nJTAG/Boundary-scan can be implemented as a PCB-level or system-level interface bus for the purpose of controlling the pins of an IC and facilitating continuity (interconnection) tests on a test target (UUT) and also functional cluster tests on logic devices or groups of devices. It can also be used as a controlling interface for other instrumentation that can be embedded into the ICs themselves (see IEEE 1687) or instruments that are part of an external controllable test system.\n\nOne of the most recently developed test system platforms employs instrumentation equipped with onboard test script processors combined with a high-speed bus. In this approach, one \"master\" instrument runs a test script (a small program) that controls the operation of the various \"slave\" instruments in the test system, to which it is linked via a high-speed LAN-based trigger synchronization and inter-unit communication bus. Scripting is writing programs in a scripting language to coordinate a sequence of actions.\n\nThis approach is optimized for small message transfers that are characteristic of test and measurement applications. With very little network overhead and a 100Mbit/sec data rate, it is significantly faster than GPIB and 100BaseT Ethernet in real applications.\n\nThe advantage of this platform is that all connected instruments behave as one tightly integrated multi-channel system, so users can scale their test system to fit their required channel counts cost-effectively. A system configured on this type of platform can stand alone as a complete measurement and automation solution, with the master unit controlling sourcing, measuring, pass/fail decisions, test sequence flow control, binning, and the component handler or prober. Support for dedicated trigger lines means that synchronous operations between multiple instruments equipped with onboard Test Script Processors that are linked by this high speed bus can be achieved without the need for additional trigger connections.\n\n\n"}
{"id": "40084324", "url": "https://en.wikipedia.org/wiki?curid=40084324", "title": "Best Available Retrofit Technology", "text": "Best Available Retrofit Technology\n\nA Best Available Retrofit Technology or BART review or rule is required under the Regional Haze Rule by sections 169A and 169B of the Clean Air Act with respect to a source such as the Navajo Generating Station which might cause haze in a Federal Class 1 area such as Grand Canyon National Park.\n\nThe official name for BART is the Clean Air Visibility Rule (CAVR).\n\nBest available retrofit technology is defined by USC § 7491:in determining best available retrofit technology the State (or the Administrator in determining emission limitations which reflect such technology) shall take into consideration the costs of compliance, the energy and nonair quality environmental impacts of compliance, any existing pollution control technology in use at the source, the remaining useful life of the source, and the degree of improvement in visibility which may reasonably be anticipated to result from the use of such technology;\n\nA BART rule may take into account the interests of stakeholders associated with the source. In the case of the BART review of the Navajo Generating Station in Page, Arizona near the Grand Canyon after the EPA issued a proposed BART rule it solicited input from stakeholders: the Department of the Interior, the Central Arizona Project, the Navajo Nation, the Gila River Indian Community, the Salt River Project, the Environmental Defense Fund, and Western Resources Advocates who as a technical working group negotiated a \"Reasonable Progress Alternative to BART\" which was submitted by the Department of the Interior to the EPA on July 26, 2013 for consideration in development of a final rule.\n\n"}
{"id": "47856361", "url": "https://en.wikipedia.org/wiki?curid=47856361", "title": "Bitsbox", "text": "Bitsbox\n\nBitsbox is a startup that teaches children how to code apps. The company sells boxes with suggested apps for users to code on a virtual tablet.\n\nScott Lininger and Aidan Chopra co-founded Bitsbox, which in 2014 participated in the Boomtown seed accelerator program. The accelerator acquired 6% equity in Bitsbox in exchange for $20,000 in seed funding.\n\nIn January 2015 Bitsbox raised $253,696 from 2,943 people in a Kickstarter campaign, and in April 2015 the company began sending out monthly subscription boxes with apps to code. In August 2015 Bitsbox closed a $500,000 funding round.\n\nIn June 2017, it was reported that Bitxbox was working with Colorado schools, and conducted its first pilot program with Saint Vrain Valley School District in Longmont, Colo., to study how the coding could be used in the classrooms.\n\n\"Forbes\" contributor Michael Lindenmayer wrote, \"Bitsbox sets kids up for success\" because, \"children need to know how to code.\" Tamara Chuang of \"The Denver Post\" commented that Bitsbox, \"[Is] helping to buck gender stereotypes for kids\" since around half of its users are girls. CBS news anchor Kristine Johnson called Bitsbox, \"a brilliant idea\" and noted that the visuals are exciting for children.\n\n"}
{"id": "54589726", "url": "https://en.wikipedia.org/wiki?curid=54589726", "title": "Bluetooth mesh networking", "text": "Bluetooth mesh networking\n\nBluetooth mesh networking, conceived in 2015, adopted on is a protocol based upon Bluetooth Low Energy that allows for many-to-many communication over Bluetooth radio.\nIt has been defined in Mesh Profile Specification and Mesh Model Specification.\n\nCommunication is carried in the messages that may be up to 384 bytes long, when using Segmentation and Reassembly (SAR) mechanism, but most of the messages fit in one segment, that is 11 bytes. Each message starts with an opcode, which may be a single byte (for special messages), 2 bytes (for standard messages), or 3 bytes (for vendor-specific messages).\n\nEvery message has a source and a destination address, determining which devices process messages. Devices publish messages to destinations which can be single things / groups of things / everything.\n\nEach message has a sequence number that protects the network against replay attacks. \n\nEach message is encrypted and authenticated. Two keys are used to secure messages: (1) network keys – allocated to a single mesh network, (2) application keys – specific for a given application functionality, e.g. turning the light on vs reconfiguring the light.\n\nMessages have a time to live (TTL). Each time message is received and retransmitted, TTL is decremented which limits the number of \"hops\", eliminating endless loops.\n\nBluetooth Mesh is a flood network. It's based on the nodes relaying the messages: every relay node that receives a network packet that authenticates against a known network key that is not in message cache, that has a TTL ≥ 2 can be retransmitted with TTL = TTL - 1.\nMessage cache used to prevent relaying messages recently seen.\n\nBluetooth Mesh has a layered architecture, with multiple layers as below.\nIt's yet to be determined what are the practical limits of Bluetooth Mesh technology. There are some limits that are built into the specification, though:\nAs of version 1.0 of Bluetooth Mesh specification, the following standard models and model groups have been defined:\n\nFoundation models have been defined in the core specification. Two of them are mandatory for all mesh nodes.\n\n\n\n\nProvisioning is a process of installing the device into a network. It is a mandatory step to build a Bluetooth Mesh network.\n\nIn the provisioning process, a provisioner securely distributes a network key and a unique address space for a device. Provisioning protocol uses P256 Elliptic Curve Diffie-Hellman Key Exchange to create a temporary key to encrypt network key and other information. This provides security from a passive eavesdropper. \nIt also provides various authentication mechanisms to protect network information, from an active eavesdropper who uses Man-In-The-Middle attack, during provisioning process.\n\nA key unique to a device known as \"Device Key\" is derived from elliptic curve shared secret on provisioner and device during the provisioning process. This device key is used by the provisioner to encrypt messages for that specific device.\n\nThe provisioning can be performed using a Bluetooth GATT connection or advertising using the specific bearer.\n\n\n"}
{"id": "3016334", "url": "https://en.wikipedia.org/wiki?curid=3016334", "title": "Bone-anchored hearing aid", "text": "Bone-anchored hearing aid\n\nA bone-anchored hearing aid (BAHA) or bone-anchored hearing device, is a type of hearing aid based on bone conduction. It is primarily suited for people who have conductive hearing losses, unilateral hearing loss, single-sided deafness and people with mixed hearing losses who cannot otherwise wear 'in the ear' or 'behind the ear' hearing aids. They are more expensive than conventional hearing aids, and their placement involves invasive surgery which carries a risk of complications, although when complications do occur, they are usually minor.\n\nTwo of the causes of hearing loss are lack of function in the inner ear (cochlea) and when the sound has problems in reaching the nerve cells of the inner ear. Example of the first include age-related hearing loss and hearing loss due to noise exposure. A patient born without external ear canals is an example of the latter for which a conventional hearing aid with a mould in the ear canal opening would not be effective. Some with this condition have normal inner ear function, as the external ear canal and the inner ear are developed at different stages during pregnancy. With normal inner anatomy, sound conducted by the skull bone improves hearing.\n\nA vibrator with a steel spring over the head or in heavy frames of eyeglasses pressed towards the bone behind the ear has been used to bring sound to the inner ear. This has, however, several disadvantages, such as discomfort and pain due to the pressure needed. The sound quality is also impaired as much of the sound energy is lost in the soft tissue over the skull bone, particularly for the higher sound frequencies important for speech understanding in noise.\n\nBone-anchored hearing aids use a surgically implanted abutment to transmit sound by direct conduction through bone to the inner ear, bypassing the external auditory canal and middle ear. A titanium prosthesis is surgically embedded into the skull with a small abutment exposed outside the skin. A sound processor sits on this abutment and transmits sound vibrations to the titanium implant. The implant vibrates the skull and inner ear, which stimulate the nerve fibers of the inner ear, allowing hearing.\n\nThe surgery is often performed under local anaesthesia and as an outpatient procedure. An important piece of information for patients is that if they for whatever reason are not satisfied with the BAHA solution, removing the implant is easy. No other ear surgical procedure is reversible like this.\n\nBy bypassing the outer or middle ear, BAHA can increase hearing in noisy situations and help localise sounds. In addition to improved speech understanding, it results in a natural sound with less distortion and feedback compared with conventional hearing aids. The ear canal is left open for comfort, and helps to reduce any problems caused by chronic ear infections or allergies. In patients with single-sided sensorineural deafness, BAHA sends the sound by the skull bone from the deaf side to the inner ear of the hearing side. This transfer of sound gives a 360° sound awareness.\n\nBAHAs may facilitate normal language development.\n\nThis fairly common condition is often associated with continuous or intermittent drainage from the ear canal. These patients may also have a hearing loss and need amplification. A conventional air conduction aid with a mold placed in the ear canal opening may not be appropriate due to the drainage, and may even provoke drainage. If the hearing loss is significant an air conduction aid may have difficulty overcoming the dysfunction of the eardrum and middle ear bones. Bone conduction hearing device bypassing the middle ear may be a more appropriate treatment for these patients. Good transmission of sound in the bone, with reduced attenuation and distortion may be possible.\n\nA person with unilateral hearing loss may have functional difficulty hearing even when the other ear is normal, particularly in demanding situations such as noisy environments and when several people are speaking the same time. A complication in single-sided deafness is hearing impairment in the hearing ear. Conventional ear surgery involves a risk of hearing loss due to the surgical procedure. Most ear surgeons are thus reluctant to perform surgery on an only hearing ear. The BAHA surgery avoids this risk and may be an appropriate treatment. An extended trial of a BAHA system with a headband prior to surgery led to more realistic expectations. In the trial, 50% of the candidates wished to proceed to surgery.\n\nIrritation in the external ear canal due to inflammation or eczema may be a condition for which a conventional air conduction aid is not an appropriate treatment. Direct bone conduction may be an option.\n\nPatients with malformations are not always suitable for reconstructive surgery. Treacher Collins syndrome patients may have significant malformations with ossicular defects and an abnormal route of the facial nerve. These structures, as well as the inner ear, could be in danger at surgery.\n\nPatients with Down syndrome may have a narrow ear canal and middle ear malformation leading to impaired hearing. Some part of the cognitive delay seen in these children may be partly due to their poor hearing.\n\nThe surgery can only take place once the skull is at least 2.5 mm thick. Children who suffer certain syndromes may have a slighter build, thinner bone, or unusual anatomy. Other children may have a thicker skull at a younger age, so it is difficult to give a specific age for surgery.\n\nIn the U.S., the Food and Drug Administration only approves BAHA implantation of children aged five years or older.\n\nFor infants and young children prior to surgery, the sound processor can be worn on a head band or soft band which the infant wears to hold it against the skull.\n\nComplications of BAHA systems can be considered as either related to the bone (hard tissue) or the soft tissue.\n\n\nSoft-tissue complications are much more common, and most are managed with topical treatments. Children are more likely to suffer both kinds of complications than adults. Sometimes, a second surgical procedure is required. Complications are less likely with good wound hygiene. Other drawbacks of BAHA include accidental or spontaneous loss of the bone implant, and patient refusal for treatment due to stigma.\n\nThe bone behind the ear is exposed through a U-shaped or straight incision or with the help of a specially designed BAHA dermatome. A hole, 3 or 4 mm deep depending on the thickness of the bone, is drilled. The hole is widened and the implant with the mounted coupling is inserted under generous cooling to minimize surgical trauma to the bone.\n\nSome surgeons perform a reduction of the subcutaneous soft tissue. The rationale for this is to reduce the mobility between implant and skin to avoid inflammation at the penetration site. This reduction of the soft tissue has been questioned and some surgeons do not perform any or a minimum of it. The rationale for this is that any surgery will result in some scar tissue that could be the focus of infection. The infections seen early during the development of the surgical procedure could perhaps be explained by the lack of seal between implant and abutment allowing bacteria to enter the space. A new helium tight seal may be advantageous and prevent biofilm formation. This will also allow the surgeon to use longer abutments should a need exist. Three to six weeks later or even earlier, the audiologist will fit and adjust the hearing processor according to the patient's hearing level. The fitting will be made using a special program in a computer.\n\nThe original surgical procedure has been described in detail by Tjellström et al. 2001.\n\nAn area where skin is penetrated requires care and cleaning because of the risk of inflammation around the abutment. Daily cleaning is required.\n\nHearing is of importance for a normal speech development. The skull bone in children is often very thin and softer than in the adult. Surgery is thus often delayed until the age of four to five years. In the meantime, the child with bilateral atresia can be fitted with a band around the head with a coupling for a BAHA. This may be done at the age of one month. Infants at this age may tolerate this well.\n\nPatients with chronic ear infection where the drum and/or the small bones in the middle ear are damaged often have hearing loss, but difficulties in using a hearing aid fitted in the ear canal. Direct bone conduction through a vibrator attached to a skin-penetrating implant addresses these disadvantages.\n\nIn 1977, the first three patients were implanted with a bone-conduction hearing solution by Anders Tjellström at the Ear, Nose, and Throat Department at Sahlgrenska University Hospital in Gothenburg, Sweden. A 4-mm-long titanium screw with a diameter of 3.75 mm was inserted in the bone behind the ear, and a bone conduction hearing aid was attached.\n\nThe term osseointegration was coined by Professor Brånemark. During animal studies, he found the bone tissue attached to the titanium implant without any soft tissue in between. He also showed an such an implant could take a heavy load. His definition of osseointegration was \"direct contact between living bone and an implant that can take a load\".\n\nThe first clinical application of titanium was in oral surgery, where implants were used for retention of dentures. Brånemark sought an acoustic way to evaluate osseointegration. A patient with implants in the jaws was fitted with a bone vibrator on one of his implants. When tested, the patient experienced very loud sound even at low stimulation levels, indicating sound could propagate very well in the bone. It has later been shown by Håkansson that the sound transmission in bone is linear, indicating low distortion of the sound.\n\nThe implant in the bone is made of titanium and will osseointegrate. The hearing instrument is impedance-matched. Osseointegration has been defined as the direct contact between living bone and an implant that can take a load, with no soft tissue at the interface.\n\nIn the US, the cost of the Baha device is about $4,000. In the Netherlands, the cost of the device is around €3000 (in 2008). The cost of the titanium implant, surgery, and aftercare from surgeon and audiologist must also be considered.\n"}
{"id": "1232903", "url": "https://en.wikipedia.org/wiki?curid=1232903", "title": "Combustor", "text": "Combustor\n\nA combustor is a component or area of a gas turbine, ramjet, or scramjet engine where combustion takes place. It is also known as a burner, combustion chamber or flame holder. In a gas turbine engine, the \"combustor\" or combustion chamber is fed high pressure air by the compression system. The combustor then heats this air at constant pressure. After heating, air passes from the combustor through the nozzle guide vanes to the turbine. In the case of a ramjet or scramjet engines, the air is directly fed to the nozzle.\n\nA combustor must contain and maintain stable combustion despite very high air flow rates. To do so combustors are carefully designed to first mix and ignite the air and fuel, and then mix in more air to complete the combustion process. Early gas turbine engines used a single chamber known as a can type combustor. Today three main configurations exist: can, annular and cannular (also referred to as can-annular tubo-annular). Afterburners are often considered another type of combustor.\n\nCombustors play a crucial role in determining many of an engine's operating characteristics, such as fuel efficiency, levels of emissions and transient response (the response to changing conditions such as fuel flow and air speed).\n\nThe objective of the combustor in a gas turbine is to add energy to the system to power the turbines, and produce a high velocity gas to exhaust through the nozzle in aircraft applications. As with any engineering challenge, accomplishing this requires balancing many design considerations, such as the following: \n\nSources:\n\nAdvancements in combustor technology focused on several distinct areas; emissions, operating range, and durability. Early jet engines produced large amounts of smoke, so early combustor advances, in the 1950s, were aimed at reducing the smoke produced by the engine. Once smoke was essentially eliminated, efforts turned in the 1970s to reducing other emissions, like unburned hydrocarbons and carbon monoxide (for more details, see the \"Emissions\" section below). The 1970s also saw improvement in combustor durability, as new manufacturing methods improved liner (see \"Components\" below) lifetime by nearly 100 times that of early liners. In the 1980s combustors began to improve their efficiency across the whole operating range; combustors tended to be highly efficient (99%+) at full power, but that efficiency dropped off at lower settings. Development over that decade improved efficiencies at lower levels. The 1990s and 2000s saw a renewed focus on reducing emissions, particularly nitrogen oxides. Combustor technology is still being actively researched and advanced, and much modern research focuses on improving the same aspects.\n\nThe case is the outer shell of the combustor, and is a fairly simple structure. The casing generally requires little maintenance. The case is protected from thermal loads by the air flowing in it, so thermal performance is of limited concern. However, the casing serves as a pressure vessel that must withstand the difference between the high pressures inside the combustor and the lower pressure outside. That mechanical (rather than thermal) load is a driving design factor in the case.\nThe purpose of the diffuser is to slow the high speed, highly compressed, air from the compressor to a velocity optimal for the combustor. Reducing the velocity results in an unavoidable loss in total pressure, so one of the design challenges is to limit the loss of pressure as much as possible. Furthermore, the diffuser must be designed to limit the flow distortion as much as possible by avoiding flow effects like boundary layer separation. Like most other gas turbine engine components, the diffuser is designed to be as short and light as possible. \nThe liner contains the combustion process and introduces the various airflows (intermediate, dilution, and cooling, see \"Air flow paths\" below) into the combustion zone. The liner must be designed and built to withstand extended high temperature cycles. For that reason liners tend to be made from superalloys like Hastelloy X. Furthermore, even though high performance alloys are used, the liners must be cooled with air flow. Some combustors also make use of thermal barrier coatings. However, air cooling is still required. In general, there are two main types of liner cooling; film cooling and transpiration cooling. Film cooling works by injecting (by one of several methods) cool air from outside of the liner to just inside of the liner. This creates a thin film of cool air that protects the liner, reducing the temperature at the liner from around 1800 kelvins (K) to around 830 K, for example. The other type of liner cooling, transpiration cooling, is a more modern approach that uses a porous material for the liner. The porous liner allows a small amount of cooling air to pass through it, providing cooling benefits similar to film cooling. The two primary differences are in the resulting temperature profile of the liner and the amount of cooling air required. Transpiration cooling results in a much more even temperature profile, as the cooling air is uniformly introduced through pores. Film cooling air is generally introduced through slats or louvers, resulting in an uneven profile where it is cooler at the slat and warmer between the slats. More importantly, transpiration cooling uses much less cooling air (on the order of 10% of total airflow, rather than 20-50% for film cooling). Using less air for cooling allows more to be used for combustion, which is more and more important for high performance, high thrust engines.\nThe snout is an extension of the dome (see below) that acts as an air splitter, separating the primary air from the secondary air flows (intermediate, dilution, and cooling air; see \"Air flow paths\" section below).\nThe dome and swirler are the part of the combustor that the primary air (see \"Air flow paths\" below) flows through as it enters the combustion zone. Their role is to generate turbulence in the flow to rapidly mix the air with fuel. Early combustors tended to use \"bluff body domes\" (rather than swirlers), which used a simple plate to create wake turbulence to mix the fuel and air. Most modern designs, however, are \"swirl stabilized\" (use swirlers). The swirler establishes a local low pressure zone that forces some of the combustion products to recirculate, creating the high turbulence. However, the higher the turbulence, the higher the pressure loss will be for the combustor, so the dome and swirler must be carefully designed so as not to generate more turbulence than is needed to sufficiently mix the fuel and air. \n\nThe fuel injector is responsible for introducing fuel to the combustion zone and, along with the swirler (above), is responsible for mixing the fuel and air. There are four primary types of fuel injectors; pressure-atomizing, air blast, vaporizing, and premix/prevaporizing injectors. Pressure atomizing fuel injectors rely on high fuel pressures (as much as ) to atomize the fuel. This type of fuel injector has the advantage of being very simple, but it has several disadvantages. The fuel system must be robust enough to withstand such high pressures, and the fuel tends to be heterogeneously atomized, resulting in incomplete or uneven combustion which has more pollutants and smoke.\n\nThe second type of fuel injector is the air blast injector. This injector \"blasts\" a sheet of fuel with a stream of air, atomizing the fuel into homogeneous droplets. This type of fuel injector led to the first smokeless combustors. The air used is just same amount of the primary air (see \"Air flow paths\" below) that is diverted through the injector, rather than the swirler. This type of injector also requires lower fuel pressures than the pressure atomizing type.\n\nThe vaporizing fuel injector, the third type, is similar to the air blast injector in that primary air is mixed with the fuel as it is injected into the combustion zone. However, the fuel-air mixture travels through a tube within the combustion zone. Heat from the combustion zone is transferred to the fuel-air mixture, vaporizing some of the fuel (mixing it better) before it is combusted. This method allows the fuel to be combusted with less thermal radiation, which helps protect the liner. However, the vaporizer tube may have serious durability problems with low fuel flow within it (the fuel inside of the tube protects the tube from the combustion heat).\n\nThe premixing/prevaporizing injectors work by mixing or vaporizing the fuel before it reaches the combustion zone. This method allows the fuel to be very uniformly mixed with the air, reducing emissions from the engine. One disadvantage of this method is that fuel may auto-ignite or otherwise combust before the fuel-air mixture reaches the combustion zone. If this happens the combustor can be seriously damaged.\nMost igniters in gas turbine applications are electrical spark igniters, similar to automotive spark plugs. The igniter needs to be in the combustion zone where the fuel and air are already mixed, but it needs to be far enough upstream so that it is not damaged by the combustion itself. Once the combustion is initially started by the igniter, it is self-sustaining and the igniter is no longer used. In can-annular and annular combustors (see \"Types of combustors\" below), the flame can propagate from one combustion zone to another, so igniters are not needed at each one. In some systems ignition-assist techniques are used. One such method is oxygen injection, where oxygen is fed to the ignition area, helping the fuel easily combust. This is particularly useful in some aircraft applications where the engine may have to restart at high altitude.\n\nThis is the main combustion air. It is highly compressed air from the high-pressure compressor (often decelerated via the diffuser) that is fed through the main channels in the dome of the combustor and the first set of liner holes. This air is mixed with fuel, and then combusted.\nIntermediate air is the air injected into the combustion zone through the second set of liner holes (primary air goes through the first set). This air completes the reaction processes, cooling the air down and diluting the high concentrations of carbon monoxide (CO) and hydrogen (H).\nDilution air is airflow injected through holes in the liner at the end of the combustion chamber to help cool the air to before it reaches the turbine stages. The air is carefully used to produce the uniform temperature profile desired in the combustor. However, as turbine blade technology improves, allowing them to withstand higher temperatures, dilution air is used less, allowing the use of more combustion air.\nCooling air is airflow that is injected through small holes in the liner to generate a layer (film) of cool air to protect the liner from the combustion temperatures. The implementation of cooling air has to be carefully designed so it does not directly interact with the combustion air and process. In some cases, as much as 50% of the inlet air is used as cooling air. There are several different methods of injecting this cooling air, and the method can influence the temperature profile that the liner is exposed to (see \"Liner\", above).\n\nCan combustors are self-contained cylindrical combustion chambers. Each \"can\" has its own fuel injector, igniter, liner, and casing. The primary air from the compressor is guided into each individual can, where it is decelerated, mixed with fuel, and then ignited. The secondary air also comes from the compressor, where it is fed outside of the liner (inside of which is where the combustion is taking place). The secondary air is then fed, usually through slits in the liner, into the combustion zone to cool the liner via thin film cooling.\n\nIn most applications, multiple cans are arranged around the central axis of the engine, and their shared exhaust is fed to the turbine(s). Can type combustors were most widely used in early gas turbine engines, owing to their ease of design and testing (one can test a single can, rather than have to test the whole system). Can type combustors are easy to maintain, as only a single can needs to be removed, rather than the whole combustion section. Most modern gas turbine engines (particularly for aircraft applications) do not use can combustors, as they often weigh more than alternatives. Additionally, the pressure drop across the can is generally higher than other combustors (on the order of 7%). Most modern engines that use can combustors are turboshafts featuring centrifugal compressors.\n\nThe next type of combustor is the \"cannular\" combustor; the term is a portmanteau of \"can annular\". Like the can type combustor, can annular combustors have discrete combustion zones contained in separate liners with their own fuel injectors. Unlike the can combustor, all the combustion zones share a common ring (annulus) casing. Each combustion zone no longer has to serve as a pressure vessel. The combustion zones can also \"communicate\" with each other via liner holes or connecting tubes that allow some air to flow circumferentially. The exit flow from the cannular combustor generally has a more uniform temperature profile, which is better for the turbine section. It also eliminates the need for each chamber to have its own igniter. Once the fire is lit in one or two cans, it can easily spread to and ignite the others. This type of combustor is also lighter than the can type, and has a lower pressure drop (on the order of 6%). However, a cannular combustor can be more difficult to maintain than a can combustor. An example of a gas turbine engine utilizing a cannular combustor is the General Electric J79 The Pratt & Whitney JT8D and the Rolls-Royce Tay turbofans use this type of combustor as well.\n\nThe final, and most commonly used type of combustor is the fully annular combustor. Annular combustors do away with the separate combustion zones and simply have a continuous liner and casing in a ring (the annulus). There are many advantages to annular combustors, including more uniform combustion, shorter size (therefore lighter), and less surface area. Additionally, annular combustors tend to have very uniform exit temperatures. They also have the lowest pressure drop of the three designs (on the order of 5%). The annular design is also simpler, although testing generally requires a full size test rig. An engine that uses an annular combustor is the CFM International CFM56. Almost all of the modern gas turbine engines use annular combustors; likewise, most combustor research and development focuses on improving this type. \nOne variation on the standard annular combustor is the \"double annular combustor\" (DAC). Like an annular combustor, the DAC is a continuous ring without separate combustion zones around the radius. The difference is that the combustor has two combustion zones around the ring; a pilot zone and a main zone. The pilot zone acts like that of a single annular combustor, and is the only zone operating at low power levels. At high power levels, the main zone is used as well, increasing air and mass flow through the combustor. GE's implementation of this type of combustor focuses on reducing NOx and CO2 emissions. A good diagram of a DAC is available from Purdue. Extending the same principles as the double annular combustor, triple annular and \"multiple annular\" combustors have been proposed and even patented.\n\nOne of the driving factors in modern gas turbine design is reducing emissions, and the combustor is the primary contributor to a gas turbine's emissions. Generally speaking, there are five major types of emissions from gas turbine engines: smoke, carbon dioxide (CO), carbon monoxide (CO), unburned hydrocarbons (UHC), and nitrogen oxides (NO).\n\nSmoke is primarily mitigated by more evenly mixing the fuel with air. As discussed in the fuel injector section above, modern fuel injectors (such as airblast fuel injectors) evenly atomize the fuel and eliminate local pockets of high fuel concentration. Most modern engines use these types of fuel injectors and are essentially smokeless.\n\nCarbon dioxide is a product of the combustion process, and it is primarily mitigated by reducing fuel usage. On average, 1 kg of jet fuel burned produces 3.2 kg of CO. Carbon dioxide emissions will continue to drop as manufacturers make gas turbine engines more efficient.\n\nUnburned hydrocarbon (UHC) and carbon monoxide (CO) emissions are highly related. UHCs are essentially fuel that was not completely combusted, and UHCs are mostly produced at low power levels (where the engine is not burning all the fuel). Much of the UHC content reacts and forms CO within the combustor, which is why the two types of emissions are heavily related. As a result of this close relation, a combustor that is well optimized for CO emissions is inherently well optimized for UHC emissions, so most design work focuses on CO emissions.\n\nCarbon monoxide is an intermediate product of combustion, and it is eliminated by oxidation. CO and OH react to form CO and H. This process, which consumes the CO, requires a relatively long time (\"relatively\" is used because the combustion process happens incredibly quickly), high temperatures, and high pressures. This fact means that a low CO combustor has a long \"residence time\" (essentially the amount of time the gases are in the combustion chamber).\n\nLike CO, Nitrogen oxides (NO) are produced in the combustion zone. However, unlike CO, it is most produced during the conditions that CO is most consumed (high temperature, high pressure, long residence time). This means that, in general, reducing CO emissions results in an increase in NO and vice versa. This fact means that most successful emission reductions require the combination of several methods. \n\nAn afterburner (or reheat) is an additional component added to some jet engines, primarily those on military supersonic aircraft. Its purpose is to provide a temporary increase in thrust, both for supersonic flight and for takeoff (as the high wing loading typical of supersonic aircraft designs means that take-off speed is very high). On military aircraft the extra thrust is also useful for combat situations. This is achieved by injecting additional fuel into the jet pipe downstream of (i.e. \"after\") the turbine and combusting it. The advantage of afterburning is significantly increased thrust; the disadvantage is its very high fuel consumption and inefficiency, though this is often regarded as acceptable for the short periods during which it is usually used.\n\nJet engines are referred to as operating \"wet\" when afterburning is being used and \"dry\" when the engine is used without afterburning. An engine producing maximum thrust wet is at \"maximum power\" or \"max reheat\" (this is the maximum power the engine can produce); an engine producing maximum thrust dry is at \"military power\" or \"max dry\".\n\nAs with the main combustor in a gas turbine, the afterburner has both a case and a liner, serving the same purpose as their main combustor counterparts. One major difference between a main combustor and an afterburner is that the temperature rise is not constrained by a turbine section, therefore afterburners tend to have a much higher temperature rise than main combustors. Another difference is that afterburners are not designed to mix fuel as well as primary combustors, so not all the fuel is burned within the afterburner section. Afterburners also often require the use of flameholders to keep the velocity of the air in the afterburner from blowing the flame out. These are often bluff bodies or \"vee-gutters\" directly behind the fuel injectors that create localized low speed flow in the same manner the dome does in the main combustor.\n\nRamjet engines differ in many ways from traditional gas turbine engines, but most of the same principles hold. One major difference is the lack of rotating machinery (a turbine) after the combustor. The combustor exhaust is directly fed to a nozzle. This allows ramjet combustors to burn at a higher temperature. Another difference is that many ramjet combustors do not use liners like gas turbine combustors do. Furthermore, some ramjet combustors are \"dump combustors\" rather than a more conventional type. Dump combustors inject fuel and rely on recirculation generated by a large change in area in the combustor (rather than swirlers in many gas turbine combustors). That said, many ramjet combustors are also similar to traditional gas turbine combustors, such as the combustor in the ramjet used by the RIM-8 Talos missile, which used a can-type combustor.\n\nScramjet (\"supersonic combustion ramjet\") engines present a much different situation for the combustor than conventional gas turbine engines (scramjets are not gas turbines, they generally have few or no moving parts). While scramjet combustors may be physically quite different from conventional combustors, they face many of the same design challenges, like fuel mixing and flame holding. However, as its name implies, a scramjet combustor must address these challenges in a supersonic flow environment. For example, for a scramjet flying at Mach 5, the air flow entering the combustor would nominally be Mach 2. One of the major challenges in a scramjet engine is preventing shock waves generated by combustor from traveling upstream into the inlet. If that were to happen, the engine may unstart, resulting in loss of thrust, amongst other problems. To prevent this, scramjet engines tend to have an isolator section (see image) immediately ahead of the combustion zone.\n\n"}
{"id": "1380891", "url": "https://en.wikipedia.org/wiki?curid=1380891", "title": "Concrete shell", "text": "Concrete shell\n\nA concrete shell, also commonly called \"thin shell concrete structure\", is a structure composed of a relatively thin shell of concrete, usually with no interior columns or exterior buttresses. The shells are most commonly flat plates and domes, but may also take the form of ellipsoids or cylindrical sections, or some combination thereof. The first concrete shell dates back to the 2nd century.\n\nMost concrete shell structures are buildings, including storage facilities, commercial buildings, and residential homes. Concrete shell construction techniques are well suited for complex curves and are also used to build boat hulls (called \"ferroconcrete\"). Historically, it was used by the British to create the Mulberry Harbours for the 1944 D-Day invasion of Normandy.\n\nLike the arch, the curved shapes often used for concrete shells are naturally strong structures, allowing wide areas to be spanned without the use of internal supports, giving an open, unobstructed interior. The use of concrete as a building material reduces both materials cost and construction costs, as concrete is relatively inexpensive and easily cast into compound curves. The resulting structure may be immensely strong and safe; modern monolithic dome houses, for example, have resisted hurricanes and fires, and are widely considered to be strong enough to withstand even F5 tornadoes.\n\nSince concrete is a porous material, concrete domes often have issues with sealing. If not treated, rainwater can seep through the roof and leak into the interior of the building. On the other hand, the seamless construction of concrete domes prevents air from escaping, and can lead to buildup of condensation on the inside of the shell. Shingling or sealants are common solutions to the problem of exterior moisture, and dehumidifiers or ventilation can address condensation.\n\nThe oldest known concrete shell, the Pantheon in Rome, was completed about AD 125, and is still standing. It has a massive concrete dome 43m in diameter, with an oculus at its centre. A monolithic structure, it appears to have been sculpted in place by applying thin layers on top of each other in decreasing diameter. Massively thick at the bottom and thinning (with aerated volcanic pumice as part of the concrete mix) at the top, the Pantheon is a remarkable feat of engineering.\n\nModern thin concrete shells, which began to appear in the 1920s, are made from thin steel reinforced concrete, and in many cases lack any ribs or additional reinforcing structures, relying wholly on the shell structure itself. \n\nShells may be cast in place, or pre-cast off site and moved into place and assembled. The strongest form of shell is the monolithic shell, which is cast as a single unit. The most common monolithic form is the dome, but ellipsoids and cylinders (resembling concrete Quonset huts / Nissen huts) are also possible using similar construction methods.\n\nGeodesic domes may be constructed from concrete sections, or may be constructed of a lightweight foam with a layer of concrete applied over the top. The advantage of this method is that each section of the dome is small and easily handled. The layer of concrete applied to the outside bonds the dome into a semi-monolithic structure.\n\nMonolithic domes are cast in one piece out of reinforced concrete and date back to the 1960s. Advocates of these domes consider them to be cost-effective and durable structures, especially suitable for areas prone to natural disasters. They also point out the ease of maintenance of these buildings. Monolithic domes can be built as homes, office buildings, or for other purposes.\n\nCompleted in 1963, the University of Illinois Assembly Hall, located in Champaign, Illinois was and is the first ever concrete-domed arena. The design of the new building, by Max Abramovitz, called for the construction of one of the world’s largest edge-supported structures. See Construction of Assembly Hall.\n\nThe Seattle Kingdome was the world's first (and only) concrete-domed multi-purpose stadium. It was completed in 1976 and demolished in 2000. The Kingdome was constructed of triangular segments of reinforced concrete that were cast in place. Thick ribs provide additional support.\n\n\n"}
{"id": "41782177", "url": "https://en.wikipedia.org/wiki?curid=41782177", "title": "D/Vision Pro", "text": "D/Vision Pro\n\nD/Vision Pro was one of the earliest Non-linear editing systems that was marketed. It was released by TouchVision Systems, Inc. in the mid-1990s.\n\nThe Program was DOS based and worked on either the 386 or 486 Intel processors. The system used the AVI compression system and worked on an Action Media II board.\n\nThe system allowed users to digitize video, audio and time code, create an editing EDL, instantly play back the edited program and out put the finished EDL in a wide variety of formats.\n\nThese systems were used as a cost-effective editing choice by numerous independent filmmakers and low budget productions during the mid-late 1990s.\n\nLow quality compression led TouchVision (later renamed D/Vision Systems) to abandon this system in favor of the D/Vision Online program, which later became Discreet Logic Edit*.\n\nThis was after it was purchased by Discreet. Eventually Discreet Killed off Edit* as they did not want it to interfere with Smoke* Sales which were more profitable. \n\nLater Discreet was purchased by Autodesk.\n"}
{"id": "18099775", "url": "https://en.wikipedia.org/wiki?curid=18099775", "title": "Desander", "text": "Desander\n\nDesanders and desilters are solid control equipment with a set of hydrocyclones that separate sand and silt from the drilling fluids in drilling rigs. Desanders are installed on top of the mud tank following the shale shaker and the degasser, but before the desilter. Desander removes the abrasive solids from the drilling fluids which cannot be removed by shakers. Normally the solids diameter for desander to be separated would be 45~74μm, and 15~44μm for desilter.\n\nA centrifugal pump is used to pump the drilling fluids from mud tank into the set of hydrocyclones.\n\nDesanders have no moving parts. The larger the internal diameter of the desander is, the greater the amount of drilling fluids it is able to process and the larger the size of the solids removed. A desander with a ( cone) is able to remove 50% of solids within the 40-50 μm (micrometre) range at a flow rate of , whilea desilter ( Cone) is able to remove 50% of solids within the 15-20 μm range at a flow rate of . Micro-fine separators are able to remove 50% of solids within the 10-15 μm range at a flow rate of . A desander is typically positioned next-to-last in the arrangement of solids control equipment, with a desander centrifuge as the subsequent processing unit. Desanders are preceded by gas busters, gumbo removal equipment (if utilized), shale shaker, mud cleaner (if utilized) and a vacuum degasser. Desanders are widely used in oilfield drilling.\nPractice has proved that hydrocyclone desanders are economic and effective equipment.\n"}
{"id": "22763302", "url": "https://en.wikipedia.org/wiki?curid=22763302", "title": "Digital locker", "text": "Digital locker\n\nA digital locker or cyberlocker is an online file or digital media storage service. Files stored include music, videos, movies, games and other media. The term was used by Microsoft as a part of its Windows Marketplace in 2004. By storing files in a digital locker, users are able to access them anywhere they can find internet connections. Most (but not all) digital locker services require a user to register. Prices range from free to paid, and various combinations thereof. \nDigital lockers, as opposed to simple file storage services, are typically associated with Digital distribution — a commercial store where you can buy content such as Steam, Google Play, Amazon, and iTunes.\n\nDownload / Play / Watch \nDigital locker services often come with integrated client software that allow users to play the movies or games or songs.\n\nUpload\n\nMany digital locker services enable users to upload their own content or provide 'sync' management software that will scan a user's computer and upload the appropriate media for them.\n\nMatching\n\nSome services like Google Play and iTunes will match songs users have to a digital signature, allowing them to skip the sometimes slow process of uploading the media file. Rather, once the song is matched, it will just be added to a user's library.\n\nDigital lockers are often used as a way of controlling access to media via Digital Rights Management. Services such as Steam, Origin, Blizzard, Vudu, and others offer to users the convenience of a digital locker in exchange for the control of DRM.\n\nSome digital locker services such as Hotfile and MegaUpload have been accused of being large contributors towards copyright infringement. The MPAA alleged that Hotfile and similar services promote copyright infringement by paying users referral fees, and thus encouraging them to upload popular copyrighted content.\n\n"}
{"id": "57934418", "url": "https://en.wikipedia.org/wiki?curid=57934418", "title": "Direct Fusion Drive", "text": "Direct Fusion Drive\n\nDirect Fusion Drive (DFD) is a conceptual low radioactivity, nuclear-fusion engine designed to produce both thrust and electric power for interplanetary spacecraft. The concept is based on the Princeton field-reversed configuration reactor invented in 2002 by Samuel A. Cohen, and is being modeled and experimentally tested at Princeton Plasma Physics Laboratory, a US Department of Energy facility, and modeled and evaluated by Princeton Satellite Systems. As of 2018, the concept has moved on to Phase II to further advance the design.\n\nThe Direct Fusion Drive (DFD) is a conceptual fusion-powered spacecraft engine, named for its ability to produce thrust from fusion without going through an intermediary electricity-generating step. The DFD uses a novel magnetic confinement and heating system, fueled with a mixture of helium-3 (He-3) and deuterium (D), to produce a high specific power, variable thrust and specific impulse, and a low-radiation spacecraft propulsion system. Fusion happens when atomic nuclei, comprising one species in a hot (100 keV or 1,120,000,00 K) plasma, a collection of electrically charged particles that includes electrons and ions, join (or fuse) together, releasing enormous amounts of energy. In the DFD system, the plasma is confined in a torus-like magnetic field inside of a linear solenoidal coil and is heated by a rotating magnetic field to fusion temperatures. Bremsstrahlung and synchrotron radiation emitted from the plasma are captured and converted to electricity for communications, spacecraft station-keeping, and maintaining the plasma's temperature. This design uses a specially shaped radio waves (RF) \"antenna\" to heat the plasma. The design also includes a rechargeable battery or a deuterium-oxygen auxiliary power unit to startup or restart DFD.\n\nThe captured radiated energy heats to a He-Xe fluid that flows outside the plasma in a boron-containing structure. That energy is put through a closed-loop Brayton cycle generator to transform it into electricity for use in energizing the coils,\npowering the RF heater, charging the battery, communications, and station-keeping functions. Adding propellant to the edge plasma flow results in a variable thrust and specific impulse when channeled and accelerated through a magnetic nozzle; this flow of momentum past the nozzle is predominantly carried by the ions as they expand through the magnetic nozzle and beyond, and thus, function as an ion thruster.\n\nThe construction of the experimental research device and most of its early operations were funded by the US Department of Energy. The recent studies —Phase I and Phase II— are funded by the NASA Institute for Advanced Concepts (NIAC) program. A series of articles on the concept were published between 2001 and 2008; the first experimental results were reported in 2007. Numerous studies of spacecraft missions (Phase I) were published, beginning in 2012. In 2017 the team reported that \"Studies of electron heating with this method have surpassed theoretical predictions, and experiments to measure ion heating in the second-generation machine are ongoing.\" As of 2018, the concept has moved on to Phase II to further advance the design. The full-size unit would measure approximately 2 m in diameter and 10 m long.\n\nStephanie Thomas is vice president of Princeton Satellite Systems and also the Principal Investigator for the Direct Fusion Drive.\n\nAnalyses predict that the Direct Fusion Drive would produce between 5-10 Newtons thrust per each MW of generated fusion power, with a specific impulse (I) of about 10,000 seconds and 200 kW available as electrical power. Approximately 35% of the fusion power goes to thrust, 30% to electric power, 25% lost to heat, and 10% is recirculated for the RF heating. \n\nModeling shows that this technology can potentially propel a spacecraft with a mass of about to Pluto in 4 years. Since DFD provides power as well as propulsion in one integrated device, it would also provide as much as 2 MW of power to the payloads upon arrival, expanding options for instrument selection, laser/optical communications, and even transfer up to 50 kW of power from the orbiter to the lander through a laser beam operating at 1080 nm wavelength.\n\nThe designers think that this technology can radically expand the science capability of planetary missions. This dual power/propulsion technology has been suggested to be used on a Pluto orbiter and lander mission, a well as integration on the Orion spacecraft to transport a crewed mission to Mars in a relatively short time (4 months instead of 9 with current technology).\n\n"}
{"id": "42968292", "url": "https://en.wikipedia.org/wiki?curid=42968292", "title": "Dropcam", "text": "Dropcam\n\nDropcam, Inc. was an American technology company headquartered in San Francisco, California. The company is known for its Wi-Fi video streaming cameras, Dropcam and Dropcam Pro, that allow people to view live feeds through Dropcam’s cloud-based service. On June 20, 2014, it was announced that Google's Nest Labs bought Dropcam for $555 million USD. In June 2015, the new owners (Nest) introduced the Nest Cam, a successor to the Dropcam Pro. Dropcam app users are also currently being transitioned to the Nest app.\n\nSoftware engineers Greg Duffy and Aamir Virani founded Dropcam in 2009. Duffy served as Dropcam’s CEO and Virani served as COO. They originally developed software for cameras made by Swedish company AXIS. Wanting to develop a less expensive camera, the two companies parted ways and Dropcam started producing its own cameras that primarily provided video monitoring for homes and small businesses. Duffy and Virani credit Duffy’s dad with at least part of the inspiration for Dropcam. He wanted to identify the neighbor who was letting their dog poop on his lawn but they were having trouble finding a security camera that made it easy to record, stream and monitor large amounts of data.\n\nDropcam received early funding from technology investor Mitch Kapor, and in June 2012, Dropcam secured $12 million in venture capital funding led by Menlo Ventures and previous investors, Accel Partners and Bay Partners. Dropcam has also received funding from Felicis Ventures and Kleiner Perkins Caufield & Byers. The following year, it received $30 million more in funding led by Institutional Venture Partners, bringing the total raised to $47.8 million. Duffy said Dropcam’s revenue grew 500 percent year over year.\n\nDropcam hosts cloud data through Amazon Web Services and Duffy says that Dropcam presently records more video than YouTube.\n\nDropcam has become popular in families watching their children, through monitoring pets at home, at pet stores and in adoption centers. Users have also reportedly caught home-burglaries in progress. Duffy has said, “Moms are using it to catch their babies' first steps when they're not around, checking that older kids have arrived home safely; contacting children who are ignoring their cell phones; and sharing footage from birthday parties.”\n\nDue to the success of Dropcam, several companies launched similar products and services in 2014 and 2015, such as SpotCam and simplicam.\n\nIn June 2015, the parent company Nest has introduced Nest Cam as a successor to Dropcam Pro.\n\nDropcam provides free live streaming video which is accessible through a web app and mobile apps for iOS and Android. Dropcam sends encrypted data to the cloud, which then securely streams the video. Video streams are private by default, but users can make their video streams public as well.\n\nDropcam currently offers two video monitoring camera models, the Dropcam and Dropcam Pro. Both models offer two-way audio, night vision, motion and sound detection, scheduling and location awareness capabilities. The standard Dropcam model offers 4x zoom capability and a 107-degree field of view.\n\nIn October 2013, Dropcam released a new version of its camera, the Dropcam Pro, which offers 8x zoom capability, an all-glass lens with an image sensor that is 2x larger than the one used in the standard Dropcam model and a widened 130-degree field of view. The Dropcam Pro also has a Bluetooth LE feature that allows users to set up the camera using an iOS device and select Android devices.\n\nDropcam sells its products online through its website and through other retailers. It is the top-selling security and surveillance product on Amazon.com, and has been made available in Apple retail stores, Target and Verizon. It has also been made available in Canada.\n\nIn May 2014, Dropcam announced Dropcam Tabs, a wireless movement sensor, but product pre-orders were canceled in summer 2014 after Dropcam was acquired by Nest.\n\nIn June 2014, Dropcam was acquired by Nest Labs and in September, Dropcam joined the Works with Nest program. When the Nest Protect alarm goes off, Dropcam records a clip of the smoke or carbon monoxide event and saves it, regardless of whether the customer pays for Cloud Recording. Additionally, Dropcam will automatically turn on motion alerts based on the Nest Thermostat being set to \"Away.\"\n\nDropcam provides optional encrypted digital video recording through the cloud. The Cloud Recording service automatically saves video on a rolling basis, so users can review the past week or month of footage, depending on their plan. All users, with or without the service, can still view the live feed. Dropcam allows users to download the video and create video clips while also allowing for the creation of a public stream. About 40% of Dropcam users sign up for the cloud service.\n\nAs part of Dropcam's Cloud Recording service, markers are placed on a user's video timeline when motion or audio is detected, so a user may go back and view those specific events rather than watch the whole feed to search for notable activities. Dropcam introduced a beta version of its Activity Recognition feature for Cloud Recording, which learns typical motion patterns in a user's video stream, allowing for customized motion alerts.\n"}
{"id": "3123996", "url": "https://en.wikipedia.org/wiki?curid=3123996", "title": "Eisenia fetida", "text": "Eisenia fetida\n\nEisenia fetida (older spelling: foetida), known under various common names such as redworm, brandling worm, panfish worm, trout worm, tiger worm, red wiggler worm, red Californian earthworm, etc., is a species of earthworm adapted to decaying organic material. These worms thrive in rotting vegetation, compost, and manure. They are epigean, rarely found in soil. In this trait, they resemble \"Lumbricus rubellus\".\n\nThey have groups of bristles (called setae) on each segment that move in and out to grip nearby surfaces as the worms stretch and contract their muscles to push themselves forward or backward.\n\n\"E. fetida\" worms are used for vermicomposting of both domestic and industrial organic waste. They are native to Europe, but have been introduced (both intentionally and unintentionally) to every other continent except Antarctica.\n\nWhen roughly handled, a redworm exudes a pungent liquid, thus the specific name \"foetida\" meaning \"foul-smelling\". This is presumably an antipredator adaptation.\n\n\"E. fetida\" is closely related to \"E. andrei\", also referred to as \"E. f. andrei\". The only simple way of distinguishing the two species is that \"E. fetida\" is sometimes lighter in colour. Molecular analyses have confirmed their identity as separate species, and breeding experiments have shown that they do not produce hybrids.\n\nAs with other earthworm species, \"E. fetida\" is hermaphroditic. However, two worms are still required for reproduction. The two worms join clitella, the large, lighter-colored bands which contain the worms' reproductive organs, and which are only prominent during the reproduction process. The two worms exchange sperm. Both worms then secrete cocoons which contain several eggs each. These cocoons are lemon-shaped and are pale yellow at first, becoming more brownish as the worms inside become mature. These cocoons are clearly visible to the naked eye.\n\n"}
{"id": "6830465", "url": "https://en.wikipedia.org/wiki?curid=6830465", "title": "Enhanced Telephone", "text": "Enhanced Telephone\n\nThe Enhanced Telephone is a telephone developed by Citibank in the late 1980s for customers to do banking and other financial transactions from their home. The official launch date was February 26-27, 1990.\n\nThe first version of the Enhanced Telephone, the 99A model, was beige and featured a monochrome CRT screen. Because of its chunky appearance, several developers dubbed it the \"sawed-off ski boot.\" The physical hardware was manufactured by Transaction Technologies Incorporated (TTI).\n\nThe second version of the Enhanced Telephone, the P100 model, was manufactured by Philips Electronics and featured an LCD screen and more sleek styling. The font was developed by Bitstream Inc..\n\nSoftware for the Enhanced Telephone was written in a proprietary language called HAL (Home Application Language).\n\nThe Enhanced Telephone ultimately failed to become a viable product because by the time it was introduced, home banking via PCs was becoming more common. As the World Wide Web became popular in the early 1990s, the Enhanced Telephone was rendered obsolete.\n\nThe Philips P100 phone lived on and to this day variations of it are used for other applications.\n"}
{"id": "12799096", "url": "https://en.wikipedia.org/wiki?curid=12799096", "title": "Enterprise information access", "text": "Enterprise information access\n\nEnterprise information access refers to information systems that allow for enterprise search; content classification; content clustering; information extraction; enterprise bookmarking; taxonomy creation and management; information presentation (for example, visualization) to support analysis and understanding; and desktop or personal knowledge search.\n\n\n"}
{"id": "64919", "url": "https://en.wikipedia.org/wiki?curid=64919", "title": "Environmental science", "text": "Environmental science\n\nEnvironmental science is an interdisciplinary academic field that integrates physical, biological and information sciences (including ecology, biology, physics, chemistry, plant science, zoology, mineralogy, oceanography, limnology, soil science, geology and physical geography (geodesy), and atmospheric science) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment. Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.\n\nRelated areas of study include environmental studies and environmental engineering. Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect.\n\nEnvironmental scientists work on subjects like the understanding of earth processes, evaluating alternative energy systems, pollution control and mitigation, natural resource management, and the effects of global climate change. Environmental issues almost always include an interaction of physical, chemical, and biological processes. Environmental scientists bring a systems approach to the analysis of environmental problems. Key elements of an effective environmental scientist include the ability to relate space, and time relationships as well as quantitative analysis.\n\nEnvironmental science came alive as a substantive, active field of scientific investigation in the 1960s and 1970s driven by (a) the need for a multi-disciplinary approach to analyze complex environmental problems, (b) the arrival of substantive environmental laws requiring specific environmental protocols of investigation and (c) the growing public awareness of a need for action in addressing environmental problems. Events that spurred this development included the publication of Rachel Carson's landmark environmental book \"Silent Spring\" along with major environmental issues becoming very public, such as the 1969 Santa Barbara oil spill, and the Cuyahoga River of Cleveland, Ohio, \"catching fire\" (also in 1969), and helped increase the visibility of environmental issues and create this new field of study.\n\nIn common usage, \"environmental science\" and \"ecology\" are often used interchangeably, but technically, ecology refers only to the study of organisms and their interactions with each other and their environment. Ecology could be considered a subset of environmental science, which also could involve purely chemical or public health issues (for example) ecologists would be unlikely to study. In practice, there is considerable overlap between the work of ecologists and other environmental scientists.\n\nThe National Center for Education Statistics in the United States defines an academic program in environmental science as follows:\nA program that focuses on the application of biological, chemical, and physical principles to the study of the physical environment and the solution of environmental problems, including subjects such as abating or controlling environmental pollution and degradation; the interaction between human society and the natural environment; and natural resources management. Includes instruction in biology, chemistry, physics, geosciences, climatology, statistics, and mathematical modeling.\n\nAtmospheric sciences focus on the Earth's atmosphere, with an emphasis upon its interrelation to other systems. Atmospheric sciences can include studies of meteorology, greenhouse gas phenomena, atmospheric dispersion modeling of airborne contaminants, sound propagation phenomena related to noise pollution, and even light pollution.\n\nTaking the example of the global warming phenomena, physicists create computer models of atmospheric circulation and infra-red radiation transmission, chemists examine the inventory of atmospheric chemicals and their reactions, biologists analyze the plant and animal contributions to carbon dioxide fluxes, and specialists such as meteorologists and oceanographers add additional breadth in understanding the atmospheric dynamics.\n\nEcology is the study of the interactions between organisms and their environment. Ecologists might investigate the relationship between a population of organisms and some physical characteristic of their environment, such as concentration of a chemical; or they might investigate the interaction between two populations of different organisms through some symbiotic or competitive relationship.\n\nFor example, an interdisciplinary analysis of an ecological system which is being impacted by one or more stressors might include several related environmental science fields. In an estuarine setting where a proposed industrial development could impact certain species by water and air pollution, biologists would describe the flora and fauna, chemists would analyze the transport of water pollutants to the marsh, physicists would calculate air pollution emissions and geologists would assist in understanding the marsh soils and bay muds.\n\nEnvironmental chemistry is the study of chemical alterations in the environment. Principal areas of study include soil contamination and water pollution. The topics of analysis include chemical degradation in the environment, multi-phase transport of chemicals (for example, evaporation of a solvent containing lake to yield solvent as an air pollutant), and chemical effects upon biota.\n\nAs an example study, consider the case of a leaking solvent tank which has entered the habitat soil of an endangered species of amphibian. As a method to resolve or understand the extent of soil contamination and subsurface transport of solvent, a computer model would be implemented. Chemists would then characterize the molecular bonding of the solvent to the specific soil type, and biologists would study the impacts upon soil arthropods, plants, and ultimately pond-dwelling organisms that are the food of the endangered amphibian.\n\nGeosciences include environmental geology, environmental soil science, volcanic phenomena and evolution of the Earth's crust. In some classification systems this can also include hydrology, including oceanography.\n\nAs an example study of soils erosion, calculations would be made of surface runoff by soil scientists. Fluvial geomorphologists would assist in examining sediment transport in overland flow. Physicists would contribute by assessing the changes in light transmission in the receiving waters. Biologists would analyze subsequent impacts to aquatic flora and fauna from increases in water turbidity.\n\nIn the U.S. the National Environmental Policy Act (NEPA) of 1969 set forth requirements for analysis of major projects in terms of specific environmental criteria. Numerous state laws have echoed these mandates, applying the principles to local-scale actions. The upshot has been an explosion of documentation and study of environmental consequences before the fact of development actions.\n\nOne can examine the specifics of environmental science by reading examples of Environmental Impact Statements prepared under NEPA such as: \"Wastewater treatment expansion options discharging into the San Diego/Tijuana Estuary\", \"Expansion of the San Francisco International Airport\", \"Development of the Houston, Metro Transportation system\", \"Expansion of the metropolitan Boston MBTA transit system\", and \"Construction of Interstate 66 through Arlington, Virginia\".\n\nIn England and Wales the Environment Agency (EA), formed in 1996, is a public body for protecting and improving the environment and enforces the regulations listed on the communities and local government site. (formerly the office of the deputy prime minister). The agency was set up under the Environment Act 1995 as an independent body and works closely with UK Government to enforce the regulations.\n\n"}
{"id": "36085842", "url": "https://en.wikipedia.org/wiki?curid=36085842", "title": "Free license", "text": "Free license\n\nA free license or open license is a license agreement which contains conditions permitted to the user from the holder on a specific list of uses for his work, which gives him four major freedoms.\n\nWithout a special license, these uses are normally prohibited by the laws of copyright.\n\nMost free licenses are worldwide, royalty-free, non-exclusive, and perpetual (see copyright durations).\n\nFree licenses are often the basis of crowdsourcing and crowdfunding projects.\n\nThe invention of the term \"free license\" and the focus on the rights of users were connected to the sharing traditions of the hacker culture of the 1970s public domain software ecosystem, the social and political free software movement (since 1980) and the Open source movement (since the 1990s).\n\nThese rights were codified by different groups and organizations for different domains in Free Software Definition, Open Source Definition, Debian Free Software Guidelines, Definition of Free Cultural Works and the Open definition. These definitions were then transformed into licenses, using the copyright as legal mechanism. Since then, ideas of free/open licenses spread into different spheres of society.\n\nOpen source, free culture (unified as Free and open-source movement), anticopyright, Wikimedia Foundation projects, Public Domain advocacy groups and pirate parties are connected with free and open licenses.\n\n\n\n\n\nCreative Commons has affiliates in more than 100 jurisdictions all over the world.\n\nEUPL was created in the European Union.\n\nHarald Welte created gpl-violations.org\n\n"}
{"id": "7018613", "url": "https://en.wikipedia.org/wiki?curid=7018613", "title": "Glass tile", "text": "Glass tile\n\nGlass tiles are pieces of glass formed into consistent shapes.\n\nGlass was used in mosaics as early as 2500 BC, but it took until the 3rd century BC before innovative artisans in Greece, Persia and India created glass tiles.\n\nWhereas clay tiles are dated as early as 8000 BC, there were significant barriers to the development of glass tiles, included the high temperatures required to melt glass, and the complexities of mastering various annealing curves for glass.\n\nIn recent years, glass tiles have become popular for both field and accent tiles. This trend can be attributed to recent technological breakthroughs, as well as the tiles’ inherent properties, in particular their potential to impart intense color and reflect light, and their imperviousness to water.\n\nGlass tile introduces complexities to the installer. Since glass is more rigid than ceramic or porcelain tile, glass tiles break more readily under the duress of substrate shifts.\n\n\"Smalti tile\", sometimes referred to as \"Byzantine glass mosaic tile\", are typically opaque glass tiles that were originally developed for use in mosaics created during the time of the Byzantine empire.\n\nSmalti is made by mixing molten glass with metal oxides for color in a furnace; the result is a cloudy mixture that is poured into flat slabs that are cooled and broken into individual pieces. The molten mixture can also be topped with gold leaf, followed by a thin glass film to protect against tarnishing. During the Byzantine era, Constantinople became the center of the mosaic craft, and the use of gold leaf glass mosaic reached perhaps its greatest artistic expression in the former seat of the Orthodox patriarch of Constantinople, the Hagia Sophia.\n\nTraditional smalti tiles are still found today in many European churches and ornamental objects; the method is also used by some present-day artisans, both in installations and fine art. In the 1920s, mass production methods were applied to Smalti tile manufacturing, which enabled these tiles to find their way into many middle-class homes. Instead of the old method of rolling the colored glass mixture out, cooling, and cutting, the new method called for molten liquid to be poured and cooled in trays, usually resulting in 3/4 inch chicklet-type pieces.\n\nSince the 1990s, a variety of modern glass tile technologies, including methods to take used glass and recreate it as ‘green’ tiles, has resulted in a resurgence of interest in glass tile as a floor and wall cladding. It is now most commonly used in pools, kitchens, spas, and bathrooms. And while smalti tiles are still popular, small and large format glass products are now commonly formed using cast and fused glass methods. The plasticity of these last two methods has resulted in a wide variety of looks and applications, including floor tiles.\n\nIn the late 1990s, special glass tiles have been coated on the back side with a receptive white coating. This has allowed impregnation of heat-transfer dyes by a printing process reproducing high resolution pictures and designs. Custom printed glass tile and glass tile murals exhibit the toughness of glass on the wearing surface with photo-like pictures. These are especially practical in kitchens and showers, where cleanser and moisture resistance are important.\n\n"}
{"id": "8701481", "url": "https://en.wikipedia.org/wiki?curid=8701481", "title": "GlobalSantaFe Corporation", "text": "GlobalSantaFe Corporation\n\nGlobalSantaFe Corporation, which traded on the NYSE as GSF was an offshore oil and gas drilling contractor, which owned or operating a fleet of 59 marine drilling rigs. With its principal executive offices in Houston, Texas, GlobalSantaFe was one of the largest international drilling contractors, providing offshore drilling services to the world's leading oil and gas companies.\n\nOn 23 July 2007, a merger between GlobalSantaFe and larger rival Transocean was announced. The merger was completed on 27 November 2007. The combined company is known as Transocean.\n\nGlobalSantaFe was the successor to two companies, Global Marine and Santa Fe International Corporation, which merged in November 2001. Both companies shared a common history of providing drilling services dating back to the Union Oil Company of California in 1946. As of June 18, 2007, the Company's fleet included 43 cantilevered jack-up rigs, 11 semi-submersible rigs, three Drillships and two additional Semi-submersible Platforms it operated for third parties under a joint venture agreement.\n\nGlobalSantaFe provided offshore oil and gas contract drilling services to the oil and gas industry worldwide on a daily rate basis. It also provided oil and gas drilling management services on either a day rate or completed-project, fixed-price basis, as well as drilling engineering and drilling project management services, and it also participated in oil and gas exploration and production activities.\n\nGlobalSantaFe provided a range of offshore drilling services to domestic and international oil and gas companies. The company's primary business was providing fully manned mobile drilling rigs to drill offshore. Known as contract drilling, this segment prices its services on a per-day basis, is capital intensive, fixed cost and highly cyclical. All of the company's rigs were designed for international operations and were deployed in most of the significant offshore rig markets in the world, including the U.S. Gulf of Mexico, West Africa, the North Sea, South America, the Middle East/Mediterranean and South East Asia.\n\n"}
{"id": "12169594", "url": "https://en.wikipedia.org/wiki?curid=12169594", "title": "Go-to-bed matchbox", "text": "Go-to-bed matchbox\n\nGo-to-bed or getting-into-bed matchboxes were a variety of match storage box popular in the mid-to-late 19th century. Relatively small, about 6 cm high, they were frequently made of metal of some kind, though sometimes of wood or ivory.\n\nMost incorporated a rough surface on which the match could be struck. All featured a small hole or finial, sometimes in ivory and always part of the design, into which the lighted match could be placed, rather like a miniature candle. The idea was that, rather than risk taking a lighted candle near to the voluminous fabric of a four poster bed, the lighted match on the mantelpiece would burn for some 30 seconds — just long enough for the person to snuff out the candle and get into bed.\n\nThere is huge variety in the designs of these little pieces. Some were relatively simple boxes or cylindrical containers, some were barrels in wood or metal, while others were cast metal figures in a wide variety of designs. Private collections include castle towers, a Napoleonic soldier of the Second Empire, a Gothic knight holding a torch, a little boy selling newspapers, a bear chained to a ragged staff, and so on. Other designs feature flower sellers and exotic ladies with a separate 'basket' in which the matches were stored.\n\nOne specific variety of go-to-bed worthy of mention is 'Prince Albert's Safety Vesta Box'. This was a decorated brass tub with an embossed top. Ribbed under base for striking matches, it had a small finial to take a single match on top, and was marked 'Prince Albert's Safety Box, 150 Patent Vesta Lights'.\n\n"}
{"id": "8714592", "url": "https://en.wikipedia.org/wiki?curid=8714592", "title": "Gray card", "text": "Gray card\n\nA gray card is a middle gray reference, typically used together with a reflective light meter, as a way to produce consistent image exposure and/or color in film and photography.\n\nA gray card is a flat object of a neutral gray color that derives from a flat reflectance spectrum. A typical example is the Kodak R-27 set, which contains two 8x10\" cards and one 4x5\" card which have 18% reflectance across the visible spectrum, and a white reverse side which has 90% reflectance. Note that flat spectral reflectance is a stronger condition than simply appearing neutral; this flatness ensures that the card appears neutral under any illuminant (see metamerism).\n\nA major use of gray cards is to provide a standard reference object for exposure determination in photography. A gray card is an (approximate) realization of a Lambertian scatterer; its apparent brightness (and exposure determination) therefore depends only on its orientation relative to the light source. By placing a gray card in the scene to be photographed, oriented toward the direction of the incident light, and taking a reading from it with a reflected light meter, the photographer can be assured of consistent exposures across their photographs. This technique is similar to using an incident meter, as it depends on the illuminance but not the reflectivity of the subject. (Of course taking photographs with side lighting or back lighting implies that the gray card should be oriented toward the camera instead.)\n\nIn addition to providing a means for measuring exposure, a gray card provides a convenient reference for white balance, or color balance, allowing the camera to compensate for the illuminant color in a scene.\n\nGray cards can be used for in-camera white balance or post-processing white balance. Many digital cameras have a custom white balance feature. A photo of the gray card is taken and used to set white balance for a sequence of photos. For post-processing white balance, a photo of the gray card in the scene is taken, and the image processing software uses the data from the pixels in the gray card area of the photo to set the white balance point for the whole image.\n\nMost digital cameras do a reasonable job of controlling color but they might get it wrong.\nFor the casual user, a gray card is mostly unnecessary. Many serious photographers or hobbyists consider gray cards an essential part of the digital photography process.\n\nGray cards are made of a variety of materials including plastic, paper, and foam. Some photographers hold that any neutral white or gray surface, such as a white piece of paper, a concrete or stone wall, or a white shirt are suitable substitutes for a gray card; however, since bright white papers and clothing washed in typical detergents contain fluorescent whitening agents, they tend to not be very spectrally neutral. Gray cards specially made to be spectrally flat are therefore more suitable to the purpose than surfaces that happen to be available.\n\nA gray card is useful for setting or correcting the balance of neutral colors, as well as for exposure. Other charts, such as various color charts, provide standard reference patterns with calibrated reflectance spectrum and color coordinates, for use in adjusting color rendering in a larger range of situations.\n"}
{"id": "7527388", "url": "https://en.wikipedia.org/wiki?curid=7527388", "title": "Hybrid vehicle drivetrain", "text": "Hybrid vehicle drivetrain\n\nHybrid vehicle drivetrains transmit power to the driving wheels for hybrid vehicles. A hybrid vehicle has multiple forms of motive power.\n\nHybrids come in many configurations. For example, a hybrid may receive its energy by burning petroleum, but switch between an electric motor and a combustion engine.\n\nElectrical vehicles have a long history combining internal combustion and electrical transmission –as in a diesel-electric powertrain–, although they have mostly been used for rail locomotives. A diesel-electric powertrain fails the definition of hybrid because the electrical drive transmission directly replaces the mechanical transmission rather than being a supplementary source of motive power. One of the earliest forms of hybrid land vehicle is the 'trackless' trolleybus of the 1930s, which normally used traction current delivered by wire. The trolleybus was commonly fitted with an internal combustion engine (ICE) either to directly power the bus or to independently generate electricity. This enabled the vehicle to manoeuvre around obstacles and broken overhead transmission wires.\n\nThe powertrain includes all of the components used to transform stored potential energy. Powertrains may either use chemical, solar, nuclear or kinetic and make them useful for propulsion. The oldest example is the galley that used sails and oars. A common modern example is the electric bicycle. Hybrid electric vehicles combine a battery or supercapacitor supplemented by an ICE that can recharge the batteries or power the vehicle. Other hybrid powertrains use flywheels to store energy.\n\nAmong the different types of hybrid vehicles, only the electric/ICE type was commercially available as of 2016. One variety operated in parallel to simultaneously provide power from both motors. Another operated in series with one source exclusively providing the power and the second providing electricity. Either source may provide the primary motive force, with the other augmenting the primary.\n\nOther combinations offer efficiency gains from superior energy management and regeneration that are offset by expense, complexity and the battery limitations. Combustion-electric (CE) hybrids have battery packs with far larger capacity than a combustion-only vehicle. A combustion-electric hybrid has batteries that are light that offer higher energy density that are far more costly. ICEs require only a battery large enough to operate the electrical system and ignite the engine.\n\nParallel hybrid systems have both an internal combustion engine and an electric motor that can both individually drive the car or both coupled up jointly giving drive. This is the most common hybrid system as of 2016.\n\nIf they are joined at an axis (in parallel)\",\" the speeds at this axis must be identical and the supplied torques add together. (Most electric bicycles are of this type.) When only one of the two sources is in use, the other must either also rotate (idle), be connected by a one-way clutch or freewheel.\n\nWith cars the two sources may be applied to the same shaft (for example with the electric motor connected between the engine and transmission), turning at equal speeds and the torques adding up with the electric motor adding or subtracting torque to the system as necessary. (The Honda Insight uses this system.)\n\nParallel hybrids can be further categorized by the balance between the different motors are at providing motive power: the ICE may be dominant (engaging the electric motor only in specific circumstances) or vice versa; while in others can run on the electric system alone but because current parallel hybrids are unable to provide electric-only or internal combustion-only modes they are often categorized as mild hybrids (see below).\n\nParallel hybrids rely more on regenerative braking and the ICE can also act as a generator for supplemental recharging. This makes them more efficient in urban 'stop-and-go' conditions. They use a smaller battery pack than other hybrids. Honda's Insight, Civic, and Accord hybrids are examples of production parallel hybrids. General Motors Parallel Hybrid Truck (PHT) and BAS Hybrids such as the Saturn VUE and Aura Greenline and Chevrolet Malibu hybrids also employ a parallel hybrid architecture.\n\nAn alternative parallel hybrid is the 'through the road' type. In this system a conventional drivetrain powers one axle, with an electric motor or motors driving another. This arrangement was used by the earliest 'off track' trolleybuses. It in effect provides a complete backup power train. In modern motors batteries can be recharged through regenerative braking or by loading the electrically driven wheels during cruise. This allows a simpler approach to power-management. This layout also has the advantage of providing four-wheel-drive in some conditions. (An example of this principle is a bicycle fitted with a front hub motor, which assists the cyclist's pedal power at the rear wheel.) Vehicles of this type include the Audi 100 Duo II, Subaru VIZIV and Peugeot 307 Hybrid HDi concept cars, the PSA Group vehicles Peugeot 3008, Peugeot 508, 508 RXH, Citroen DS5 all using the HYbrid4 system, the Volvo V60 plug-in hybrid, the BMW 2 Series Active Tourer, BMW i8, and the second generation Honda NSX.\n\nSeries hybrids are also referred to as extended-range electric vehicles (EREV) or range-extended electric vehicles (REEV). (Series hybrids with particular characteristics are classified as range-extended battery-electric vehicle (BEVx) by the California Air Resources Board.)\n\nElectric transmission has been available as an alternative to conventional mechanical transmissions since 1903. Typically mechanical transmissions impose many penalties, including weight, bulk, noise, cost, complexity and a drain on engine power with every gear-change, whether accomplished manually or automatically. Unlike ICEs, electric motors do not require a transmission.\n\nIn effect the entire mechanical transmission between the ICE and the wheels is removed and replaced by an electric generator, some cable and controls, and electric traction motors, with the benefit that the ICE is no longer directly connected to the demand.\n\nThis is a series-hybrid arrangement and is common in diesel-electric locomotives and ships (the Russian river ship Vandal, launched in 1903, was the world's first diesel-powered and diesel-electric powered vessel) and Ferdinand Porsche successfully used this arrangement in the early 20th century in racing cars, including the Lohner-Porsche Mixte Hybrid. Porsche named the system System Mixte, which had a wheel hub motor arrangement, with a motor in each of the two front wheels, setting speed records.\n\nThe arguments of greater flexibility, higher efficiency and less emissions at the point of use are achieved in a series-hybrid system for road vehicles when an intermediate electric battery, acting as an energy buffer, sits between the electric generator and the electric traction motors.\n\nThe ICE turns a generator and is not mechanically connected to the driving wheels. This isolates the engine from demand, allowing it to consistently operate at its most efficient speed. Since the primary motive power is generated by the battery, a smaller generator/engine can be fitted as compared to a conventional direct drive engine. Electric traction motors can receive electricity from the battery, or directly from the engine/generator or both. Traction motors frequently are powered only by the electric battery, which can be charged from external sources such as the electricity grid.\n\nThis allows a vehicle with an engine/generator that only operates when needed, such as when the battery is depleted, or to charge the batteries.\n\nElectric motors are more efficient than ICEs, with high power-to-weight ratios providing torque over a wide speed range. ICEs are most efficient when turning at a constant speed.\n\nICEs can run optimally when turning a generator. Series-hybrid systems offer smoother acceleration by avoiding gear changes. Series-hybrids incorporate:\n\n\nIn addition:\n\n\nThe electric motor may be entirely fed by electricity from the battery or via the generator turned by the ICE, or both. Such a vehicle conceptually resembles a diesel-electric locomotive with the addition of a battery that may power the vehicle without running the ICE and acting as an energy buffer that is used to accelerate and achieve greater speed; the generator may simultaneously charge the battery and power the electric motor that moves the vehicle.\n\nWhen the vehicle is stopped the ICE is switched off without idling, while the battery provides whatever power is needed at rest. Vehicles at traffic lights, or in slow moving stop-start traffic need not burn fuel when stationary or moving slowly, reducing emissions.\n\nSeries-hybrids can be fitted with a supercapacitor or a flywheel to store regenerative braking energy, which can improve efficiency by recovering energy otherwise lost as heat through the braking system. Because a series-hybrid has no mechanical link between the ICE and the wheels, the engine can run at a constant and efficient rate regardless of vehicle speed, achieving higher efficiency (37%, rather than the ICE average of 20%) and at low or mixed speeds this could result in ~50% increase in overall efficiency (19% vs 29%).\n\nLotus offered an engine/generator set design that runs at two speeds, giving 15 kW of electrical power at 1,500 rpm and 35 kW at 3,500 rpm via the integrated electrical generator, used in the Nissan concept Infiniti Emerg-e.\n\nThis operating profile allows greater scope for alternative engine designs, such as a microturbine, rotary Atkinson cycle engine or linear combustion engine.\n\nThe ICE is matched to the electric engine by comparing the output rates at cruising speed. Generally, output rates for combustion engines are provided for instantaneous (peak) output rates, but in practice these can't be used.)\n\nThe use of an electric motor driving a wheel directly eliminates the conventional mechanical transmission elements: gearbox, transmission shafts and differential, and can sometimes eliminate flexible couplings.\n\nIn 1997, Toyota released the first series-hybrid bus sold in Japan. Designline International of Ashburton, New Zealand produces city buses with a microturbine powered series-hybrid system. Wrightbus produces series hybrid buses including the Gemini 2 and New Routemaster. Supercapacitors combined with a lithium ion battery bank have been used by AFS Trinity in a converted Saturn Vue SUV vehicle. Using supercapacitors they claim up to 150 mpg in a series-hybrid arrangement.\n\nWell known automotive series hybrid models include the variant of the BMW i3 that is equipped with a range extender. Another example of a series hybrid automobile is the Fisker Karma. The Chevrolet Volt is almost a series hybrid, but also includes a mechanical link from the engine to the wheels above 70 mph.\n\nSeries-hybrids have been taken up by the aircraft industry. The DA36 E-Star, an aircraft designed by Siemens, Diamond Aircraft and EADS, employs a series hybrid powertrain with the propeller turned by a Siemens 70 kW (94 hp) electric motor. A power sapping propeller speed reduction unit is eliminated. The aim is to reduce fuel consumption and emissions by up to 25 percent. An onboard 40 hp (30 kW) Austro Engine Wankel rotary engine and generator provides the electricity.\n\nThe Wankel was chosen because of its small size, low weight and great power to weight ratio. (Wankel engines also run efficiently at a constant speed of approximately 2,000 RPM which is suited to generator operation. Keeping to a constant/narrow band offsets many of the perceived disadvantages of the Wankel engine in automotive applications.)\n\nThe electric propeller motor uses electricity stored in batteries, with the engines not operating, to take off and climb reducing sound emissions. The powertrain reduces the weight of the plane by 100 kilos relative to its predecessor. The DA36 E-Star first flew in June 2013, making this the first ever flight of a series hybrid powertrain. Diamond Aircraft state that the technology is scalable to a 100-seat aircraft.\n\nIf the motors are attached to the vehicle body, flexible couplings are required but not if the traction motors are integrated into the wheels. One disadvantage is that the unsprung mass increases and suspension responsiveness decreases, which impacts ride and potentially safety. However the impact should be minimal as electric motors in wheel hubs such as Hi-Pa Drive, may be very small and light having exceptionally high power-to-weight ratios and braking mechanisms can be lighter as the wheel motors brake the vehicle.\n\nAdvantages of individual wheel motors include simplified traction control, all wheel drive if required and a lower floor (useful for buses and other specialised vehicles (some 8x8 all-wheel drive military vehicles use individual wheel motors). Diesel-electric locomotives have used this concept (individual motors driving axles of each pair of wheels) for 70 years.\n\nOther measures include lightweight aluminium wheels to reduce the unsprung mass of the wheel assembly; vehicle designs may be optimized to lower the centre of gravity by locating heavier elements (including battery) at floor level; In a typical road vehicle the power-transmission setup may be smaller and lighter than the equivalent conventional mechanical power-transmission setup, liberating space; the combustion generator set only requires cables to the driving electric motors, increasing flexibility in major component layout spread across a vehicle giving superior weight distribution and maximizing vehicle cabin space and opening up the possibility of superior vehicle designs exploiting this flexibility.\n\nPower-split hybrid or series-parallel hybrid are parallel hybrids that incorporate power-split devices, allowing for power paths from the ICE to the wheels that can be either mechanical or electrical. The main principle is to decouple the power supplied by the primary source from the power demanded by the driver.\n\nICE torque output is minimal at lower RPMs and conventional vehicles increase engine size to meet market requirements for acceptable initial acceleration. The larger engine has more power than needed for cruising. Electric motors produce full torque at standstill and are well-suited to complement ICE torque deficiency at low RPMs. In a power-split hybrid, a smaller, less flexible, and more efficient engine can be used. The conventional Otto cycle (higher power density, more low-RPM torque, lower fuel efficiency) is often modified to an Atkinson cycle or Miller cycle (lower power density, less low-rpm torque, higher fuel efficiency; sometimes called an Atkinson-Miller cycle). The smaller engine, using a more efficient cycle and often operating in the favorable region of the brake specific fuel consumption map, significantly contributes to the higher overall efficiency of the vehicle.\n\nInteresting variations of the simple design (pictured at right) found, for example, in the well-known Toyota Prius are the:\n\n\n\nThe Toyota Hybrid System THS / Hybrid Synergy Drive has a single power-split device (incorporated as a single three-shaft planetary gearset) and can be classified as an Input-Split, since the power of the engine is split at the input to the transmission. This in turn makes this setup very simple in mechanical terms, but has drawbacks of its own. For example, in Generation 1 and Generation 2 HSDs maximum speed is mainly limited by the speed of the smaller electric motor (often functioning as a generator). The Generation 3 HSD separates the ICE-MG1 path from the MG2 path, each with its own, tailored gear ratio (1.1:1 and 2.5:1, respectively, for late Priuses, including the Prius c). The Generation 4 HSD eliminates the second planetary gear set, and places the electric motors on parallel axes, with a combining gear in between these axes, and transfers the combined result to the final drive differential. This is quite similar to Toyota-affiliated Aisin Seiki's hybrid system, and saves significant space.\nGeneral Motors, BMW, and DaimlerChrysler collaborated on a system named \"Two-Mode Hybrid\" as part of the Global Hybrid Cooperation. The technology was released in the fall of 2007 on the Chevrolet Tahoe Hybrid. The system was also featured on the GMC Graphite SUV concept vehicle at the 2005 North American International Auto Show in Detroit. BYD Auto's F3DM sedan is a series-parallel plug-in hybrid automobile, which went on sale in China in 2008.\n\nThe Two-Mode Hybrid name highlights the drive-train's ability to operate in all-electric (Mode 1, or \"Input-Split\") as well as hybrid (Mode 2, or \"Compound-Split\") modes. The design allows for operation in more than two modes. Two power-split modes are available, along with several fixed gear (essentially parallel hybrid) regimes. Such a design can be referred to as a multi-regime design. The Two-Mode Hybrid powertrain design can be classified as a compound-split design, since the addition of four clutches within the transmission allows for multiple configurations of engine power-splitting. In addition to the clutches, this transmission has a second planetary gearset. The objective of the design is to vary the percentage of mechanically vs. electrically transmitted power to cope both with low-speed and high-speed operating conditions. This enables smaller motors to do the job of larger motors when compared to single-mode systems, because the derived electrical peak power is proportional to the width of the continuous variation range. The four fixed gears enable the Two-Mode Hybrid to function like a conventional parallel hybrid under high continuous power regions such as sustained high speed cruising or trailer towing. Full electric boost is available in fixed gear modes.\n\nMicro hybrid is a general term given to vehicles that use some type of start-stop system to automatically shut off the engine when idling. Strictly speaking, micro hybrids are not real hybrid vehicles, because they do not rely on two different sources of power.\n\nMild hybrids are essentially conventional vehicles with some hybrid hardware, but with limited hybrid features. Typically, they are a parallel hybrid with start-stop only or possibly with modest levels of engine assist or regenerative braking. Mild hybrids generally cannot provide all-electric propulsion.\n\nMild hybrids like the General Motors 2004-07 Parallel Hybrid Truck (PHT) and the Honda Eco-Assist hybrids are equipped with a three-phase electric motor mounted within the bell-housing between the engine and transmission, allowing the engine to be turned off whenever the truck is coasting, braking, or stopped, yet restart quickly to provide power. Accessories can continue to run on electrical power while the engine is off, and as in other hybrid designs, regenerative braking recaptures energy. The large electric motor spins up the engine to operating speeds before injecting fuel.\n\nThe 2004–07 Chevrolet Silverado PHT was a full-size pickup truck. Chevrolet was able to get a 10% efficiency improvement by shutting down and restarting the engine on demand and using regenerative braking. The electrical energy was used only to drive accessories such as power steering. The GM PHT used a 42 volt system via three 12 volt vented lead acid batteries connected in series (36V total) to supply the power needed for the startup motor, as well as to power the electronic accessories.\n\nGeneral Motors then introduced their BAS Hybrid system, another mild hybrid implementation officially released on the 2007 Saturn Vue Green Line. Its \"start-stop\" functionality operates similarly to the Silverado, although via a belted connection to the motor/generator unit. However the GM BAS Hybrid System can also provide modest assist under acceleration and during steady driving, and captures energy during regenerative (blended) braking. BAS Hybrid offered as much as a 27% improvement in combined fuel efficiency in EPA testing of the 2009 Saturn VUE. The system can also be found on the 2008-09 Saturn Aura and the 2008-2010 Chevrolet Malibu hybrids.\n\nAnother way to offer start/stop is by employing a static start engine. Such an engine requires no starter motor, but employs sensors to determine the exact position of each piston, then precisely timing the injection and ignition of fuel to \"turn over\" the engine.\n\nMild hybrids are sometimes called \"Power assist hybrids\" as they use the ICE for primary power, with a torque-boosting electric motor connected to a (largely) conventional power train. The electric motor is mounted between the engine and transmission. It is essentially a large starter motor that operates when the engine needs to be turned over and when the driver \"steps on the gas\" and requires extra power. The electric motor may also restart the combustion engine and shutting down the main engine at idle, while the enhanced battery system is used to power accessories. GM announced Buick LaCrosse and Buick Regal mild hybrids dubbed Eassist.\n\nHonda's hybrids, including the Insight, use this design, leveraging their expertise in small, efficient gasoline engines; their system is dubbed Integrated Motor Assist (IMA). IMA hybrids cannot provide propulsion on electric power alone. However, since the amount of electrical power needed is much smaller, system size is reduced.\n\nAnother variation is the Saturn Vue Green Line BAS Hybrid system that uses a smaller electric motor (mounted to the side of the engine) and battery pack than the Honda IMA, but functions similarly.\n\nAnother variation on this type is Mazda's e-4WD system, offered on the Mazda Demio sold in Japan. This front-wheel drive vehicle has an electric motor that can drive the rear wheels when extra traction is needed. The system is disengaged in all other driving conditions, so it does not directly enhance performance or economy but allows the use of a smaller and more economical engine relative to total performance.\n\nFord has dubbed Honda's hybrids \"mild\" in their advertising for the Escape Hybrid, arguing that the Escape's full hybrid design is more efficient.\n\nA full hybrid, sometimes also called a strong hybrid, is a vehicle that can run on just the engine, the batteries, or a combination. The Toyota Prius, Toyota Camry Hybrid, Ford Escape Hybrid/Mercury Mariner Hybrid, Ford Fusion Hybrid/Lincoln MKZ Hybrid/Mercury Milan Hybrid, Ford C-Max Hybrid, Kia Optima Hybrid, as well as the General Motors 2-mode hybrid trucks and SUVs, are examples of this type of hybridization as they can operate on battery power alone. A large, high-capacity battery provides battery-only operation. These vehicles have a split power path that allows more flexibility in the drivetrain by inter-converting mechanical and electrical power. To balance the forces from each portion, the vehicles use a differential-style linkage between the engine and motor connected to the head end of the transmission.\n\nThe Toyota brand name for this technology is Hybrid Synergy Drive, which is used in the Prius, the Highlander Hybrid SUV and the Camry Hybrid. A computer oversees system operation, determining how to mix the power sources. The Prius operations can be divided into six distinct regimes.\n\nA plug-in hybrid electric vehicle (PHEV) has two defining characteristics. It:\n\n\nThey are full hybrids, able to run on battery power. They offer greater battery capacity and the ability to recharge from the grid. They can be either parallel or series designs. They are also called \"gas-optional\", or \"griddable\" hybrids. Their main benefit is that they can be gasoline-independent for significant distances, with the extended range of an ICE for longer trips. Electric Power Research Institute research found a lower total cost of ownership for PHEVs due to reduced service costs and gradually improving battery technology. The \"well-to-wheel\" efficiency and emissions of PHEVs compared to gasoline hybrids depends on the grid energy sources (the US grid is 30% coal; California's grid is primarily natural gas, hydroelectric power, and wind power). \n\nPrototypes of PHEVs, with larger battery packs that can be recharged from the power grid, were built in the U.S., notably at Andy Frank's Hybrid Center at University of California, Davis. One production PHEV, the Renault Kangoo, went on sale in France in 2003. DaimlerChrysler built PHEVs based on the Mercedes-Benz Sprinter van. Light Trucks are offered by Micro-Vett SPA the so-called Daily Bimodale.\n\nThe California Cars Initiative converted the 2004 and newer Toyota Prius to become a prototype of what it calls PRIUS+. With the addition of of lead-acid batteries, the PRIUS+ achieved roughly double the gasoline mileage of a standard Prius and could make trips of up to using only electric power.\n\nChinese battery manufacturer and automaker BYD Auto released the F3DM compact sedan to the Chinese fleet market on December 15, 2008, later replaced by the BYD Qin plug-in hybrid.\n\nGeneral Motors began deliveries of the Chevrolet Volt in the United States in December 2010, and its sibling, the Opel Ampera, was released in Europe by early 2012. , other plug-in hybrids available in several markets were the Fisker Karma, Toyota Prius Plug-in Hybrid and Ford C-Max Energi.\n\n, the best selling PHEV is the Volt, with more than 33,000 units of the Volt/Ampera family sold worldwide since December 2010, led by US sales of 27,306, followed by the Netherlands with 2,175 Amperas sold through October 2012. The Prius Plug-in Hybrid had sold 21,600 units sold worldwide through October 2012, with US sales of 9,623 units, followed by Japan with 9,500 units.\n\nThere are many ways to create an electric-Internal Combustion Engine (ICE) hybrid. The variety of electric-ICE designs can be differentiated by how the electric and combustion portions of the powertrain connect, at what times each portion is in operation, and what percent of the power is provided by each hybrid component. Two major categories are series hybrids and parallel hybrids, though parallel designs are most common today.\n\nMost hybrids, no matter the specific type, use regenerative braking to recover energy when slowing down the vehicle. This simply involves driving a motor so it acts as a generator.\n\nMany designs also shut off the internal combustion engine when it is not needed in order to save energy. That concept is not unique to hybrids; Subaru pioneered this feature in the early 1980s, and the Volkswagen Lupo 3L is one example of a conventional vehicle that shuts off its engine when at a stop. Some provision must be made, however, for accessories such as air conditioning which are normally driven by the engine. Furthermore, the lubrication systems of internal combustion engines are inherently least effective immediately after the engine starts; since it is upon startup that the majority of engine wear occurs, the frequent starting and stopping of such systems reduce the lifespan of the engine considerably. Also, start and stop cycles may reduce the engine's ability to operate at its optimum temperature, thus reducing the engine's efficiency.\n\nFuel cell vehicles are often fitted with a battery or supercapacitor to deliver peak acceleration power and to reduce the size and power constraints on the fuel cell (and thus its cost); this is effectively also a series hybrid configuration.\n\nA hydraulic hybrid vehicle uses hydraulic and mechanical components instead of electrical. A variable displacement pump replaces the electric motor/generator. A hydraulic accumulator stores energy. The vessel typically carries a flexible bladder of pre-charged pressurized nitrogen gas. Pumped hydraulic fluid is compressed against the bladder storing the energy in the compressed nitrogen gas. Some versions have a piston in a cylinder rather than a pressurized bladder. The hydraulic accumulator is potentially cheaper and more durable than batteries. Hydraulic hybrid technology was originally implemented in Germany in the 1930s. Volvo Flygmotor used petro-hydraulic hybrids experimentally in buses from the early 1980s.\n\nThe initial concept involved a giant flywheel (see Gyrobus) for storage connected to a hydrostatic transmission. The system is under development by Eaton and several other companies, primarily in heavy vehicles like buses, trucks and military vehicles. An example is the Ford F-350 Mighty Tonka concept truck shown in 2002. It features an Eaton system that can accelerate the truck to highway speeds.\n\nThe system components were expensive, which precluded installation in smaller trucks and cars. A drawback was that the power motors were not efficient enough at part load. Focus switched to smaller vehicles. A British company made a breakthrough by introducing an electronically controlled hydraulic motor/pump that is efficient at all ranges and loads, making small applications of petro-hydraulic hybrids feasible. The company converted a BMW car to prove viability. The BMW 530i gave double the MPG in city driving compared to the standard car. The test used the standard 3,000 cc engine. Petro-hydraulic hybrids allows downsizing an engine to average power usage, not peak power usage. Peak power is provided by the energy stored in the accumulator.\n\nThe kinetic braking energy recovery rate is higher and therefore the system is more efficient than 2013-era battery charged hybrids, demonstrating a 60% to 70% increase in economy in EPA testing. In EPA tests a hydraulic hybrid Ford Expedition returned in urban driving and on the highway.\n\nOne research company's goal was to create a fresh design to improve the packaging of gasoline-hydraulic hybrid components. All bulky hydraulic components were integrated into the chassis. One design claimed to reach 130mpg in tests by using a large hydraulic accumulator that is also the structural chassis. The hydraulic driving motors are incorporated within the wheel hubs and reversing to recover braking energy. The aim is 170 mpg in average driving conditions. Energy created by shock absorbers and kinetic braking energy that normally would be wasted assists in charging the accumulator. An ICE sized for average power use charges the accumulator. The accumulator is sized to run the car for 15 minutes when fully charged.\n\nIn January 2011, Chrysler announced a partnership with the EPA to design and develop an experimental gasoline-hydraulic hybrid powertrain suitable for use in passenger cars. Chrysler adapted an existing production minvan to the powertrain.\n\nNRG Dynamix of the U.S.A. claimed its approach reduced cost by one-third compared with electric hybrids and added only 300 lbs (136 kg) to vehicle weight vs. 1,000 lbs (454 kg) for electric hybrids. The company claimed a standard pickup vehicle powered by a 2.3 litre 4 cylinder engine achieved 14 MPG (16.8 l/100 km) in city driving. Using the petro-hydraulic setup fuel economy reached \"the mid 20s\".\n\nCompressed air can power a hybrid car with a gasoline compressor to provide the power. Motor Development International in France was developing such air-powered cars. A team led by Tsu-Chin Tsao, a UCLA mechanical and aerospace engineering professor, collaborated with engineers from Ford to get pneumatic hybrid technology up and running. The system is similar to that of a hybrid-electric vehicle in that braking energy is harnessed and stored to assist the engine as needed during acceleration.\n\nMany land and water vehicles use human power combined with a further power source. Common are parallel hybrids, e.g. a sailboat with oars, motorized bicycles or a human-electric hybrid vehicle such as the Twike. Some series hybrids exist. Such vehicles can be tribrid vehicles, combining three power sources e.g. on-board solar cells, grid-charged batteries and pedals.\n\nHybrid vehicles can be used in different modes. The figure shows some typical modes for a parallel hybrid configuration.\n\nConmarket/aftermarket powertrain can be added to a vehicle.\n\nThe conmarket solution is used when the user delivers glider (rolling chassis) and the hybrid (two engines) or all-electric (only an electric motor) powertrain kit to the automaker and receives the vehicle with the tech installed. An (electric or hybrid) powertrain can be added to a glider by an aftermarket installer.\n\nIn 2013 a University of Central Florida design team, \"On the Green\", worked to develop a bolt-on hybrid conversion kit to transform an older model vehicle into a gas-electric hybrid.\n\nA conversion of a 1966 Mustang was demonstrated by an engineer in California. The system replaced the alternator with a 12 kW (30 kW peak) brushless electric motor. Gas mileage and power improved.\n\n\n"}
{"id": "319560", "url": "https://en.wikipedia.org/wiki?curid=319560", "title": "Instrument amplifier", "text": "Instrument amplifier\n\nAn instrument amplifier is an electronic device that converts the often barely audible or purely electronic signal of a musical instrument into a larger electronic signal to feed to a loudspeaker. An instrument amplifier is used with musical instruments such as an electric guitar, an electric bass, electric organ, synthesizers and drum machine to convert the signal from the pickup (with guitars and other string instruments and some keyboards) or other sound source (e.g, a synthesizer's signal) into an electronic signal that has enough power, due to being routed through a power amplifier, capable of driving one or more loudspeaker that can be heard by the performers and audience. \n\nCombination (\"combo\") amplifiers include a preamplifier, a power amplifier, tone controls, and one or more speakers in a cabinet, a housing or box usually made of hardwood, plywood or particleboard (or, less commonly, moulded plastic). Instrument amplifiers for some instruments are also available without an internal speaker; these amplifiers, called \"heads\", must plug into one or more external speaker cabinets. Instrument amplifiers also have features that let the performer modify the signal's tone, such as changing the equalization (adjusting bass and treble tone) or adding electronic effects such as intentional distortion/overdrive, reverb or chorus effect.\n\nInstrument amplifiers are available for specific instruments, including the electric guitar, electric bass, electric/electronic keyboards, and acoustic instruments such as the mandolin and banjo. Some amplifiers are designed for specific styles of music, such as the \"traditional\"-style \"tweed\" guitar amplifiers, such as the Fender Bassman used by blues and country music musicians, and the Marshall amplifiers used by hard rock and heavy metal bands.\n\nUnlike home \"hi-fi\" amplifiers or public address systems, which are designed to accurately reproduce the source sound signals with as little harmonic distortion as possible and without changing the tone or equalization (at least not unless the hi-fi owner adjusts it themselves with a graphic equalizer), instrument amplifiers are often designed to add additional tonal coloration to the original signal, emphasize (or de-emphasize) certain frequencies (most electric guitar amps roll off the very high frequencies), and, in the case of guitar amplifiers designed for electric guitar or Hammond organ, offer the capability to intentionally add some degree of \"overdrive\" or distortion to the tone. The two exceptions are keyboard amplifiers designed for use with digital pianos and synthesizers and \"acoustic\" instrument amplifiers for use with acoustic guitar or fiddle in a folk music setting, which typically aim for a relatively flat frequency response (i.e., no added colouration of the sound) and little or no distortion of the signal.\n\nA guitar amplifier amplifies the electrical signal of an electric guitar (or, less commonly, with acoustic amplifiers, an acoustic guitar) so that it can drive a loudspeaker at sufficient volume for the performer and audience to hear. Most guitar amplifiers can also modify the instrument's with controls that emphasize or de-emphasize certain frequencies and add electronic effects. String vibrations are sensed by a suitable microphone or pickup, depending on the type of guitar. For electric guitars, strings are almost always made of metal, and the pickup works by electro-magnetic induction (these are called magnetic pickups; they are the most widely used type of pickup on electric guitars). Acoustic guitars do not usually have a built-in pickup or microphone, at least with entry-level and beginner instruments. Some acoustic guitars have a small condenser microphone mounted inside the body, which designed to convert acoustic vibrations into an electrical signal, but usually they do so from direct contact with the strings (replacing the guitar's bridge) or with the guitar's body, rather than having a membrane like general-purpose microphones. Acoustic guitars may also use a piezoelectric pickup, which converts the vibrations of the instrument into an electronic signal. More rarely, a magnetic pickup may be mounted in the sound hole of an acoustic guitar; while magnetic pickups do not have the same acoustic tone that microphones and piezo pickups can produce, magnetic pickups are more resistant to acoustic feedback.\n\nStandard amplifiers, such as the Fender \"tweed\"-style amps (e.g., the Fender Bassman) and Gibson amps, are often used by traditional rock, blues, and country musicians who wish to create a \"vintage\" 1950s-style sound. They are used by electric guitarists, pedal steel guitar players, and blues harmonica (\"harp\") players. Combo amplifiers such as the Fender Super Reverb have powerful, loud tube amplifiers, four 10\" speakers, and they often have built-in reverb and \"vibrato\" effects units. Smaller guitar amps are also available, which have fewer speakers (some have only one speaker) and lighter, less powerful amplifier units. Smaller guitar amps are easier to transport to gigs and sound recording sessions. Smaller amps are widely used in small venue shows (nightclubs) and in recordings, because players can obtain the tone they want without having to have an excessively loud volume. One of the challenge with the large, powerful 4x10 Fender Bassman-type amps is that to get the tone a player wants, they have to turn up the amp to a loud volume.\n\nThese amps are designed to produce a variety of sounds ranging from a clean, warm sound (when used in country and soft rock) to a growling, natural overdrive, when the volume is set near its maximum, (when used for blues, rockabilly, psychobilly, and roots rock). These amplifiers usually have a sharp treble roll-off at 5 kHz to reduce the extreme high frequencies, and a bass roll-off at 60–100 Hz to reduce unwanted boominess. The nickname \"tweed\" refers to the lacquered beige-light brown fabric covering used on these amplifiers. \n\nThe smallest \"combo\" amplifiers, which are mainly used for individual practice and warm-up purposes, may have only a single 8\" or 10\" speaker. Some harmonica players use these small combo amplifiers for concert performances, though, because it is easier to create natural overdrive with these lower-powered amplifiers. Larger combo amplifiers, with one 12 inch speaker or two or four 10 or 12 inch speakers are used for club performances and larger venues. For large concert venues such as stadiums, performers may also use an amplifier \"head\" with several separate speaker cabinets (which usually contain two or four 12\" speakers).\n\nElectric guitar amplifiers designed for heavy metal are used to add an aggressive \"drive\", intensity, and \"edge\" to the guitar sound with distortion effects, preamplification boost controls (sometimes with multiple stages of preamps), and tone filters. While many of the most expensive, high-end models use 1950s-style tube amplifiers (even in the 2000s), there are also many models that use transistor amplifiers, or a mixture of the two technologies (i.e., a tube preamplifier with a transistor power amplifier). Amplifiers of this type, such as Marshall amplifiers, are used in a range of the louder, heavier genres of rock, including hard rock, heavy metal, and hardcore punk. This type of amplifier is available in a range of formats, ranging from small, self contained combo amplifiers for rehearsal and warm-ups to heavy \"heads\" that are used with separate speaker cabinets—colloquially referred to as a \"stack.\"\n\nIn the late 1960s and early 1970s, public address systems at rock concerts were used mainly for the vocals. As a result, to get a loud electric guitar sound, early heavy metal and rock-blues bands often used \"stacks\" of 4x12\" Marshall speaker cabinets on the stage. In 1969, Jimi Hendrix used four stacks to create a powerful lead sound, and in the early 1970s by the band Blue Öyster Cult used an entire wall of Marshall Amplifiers to create a roaring wall of sound that projected massive volume and sonic power. In the 1980s, metal bands such as Slayer and Yngwie Malmsteen also used \"walls\" of over 20 Marshall cabinets. However, by the 1980s and 1990s, most of the sound at live concerts was produced by the sound reinforcement system rather than the onstage guitar amplifiers, so most of these cabinets were not connected to an amplifier. Instead, walls of speaker cabinets were used for aesthetic reasons. \n\nAmplifiers for harder, heavier genres often use valve amplifiers (known as \"tube amplifiers\" in North America) also. Valve amplifiers are perceived by musicians and fans to have a \"warmer\" tone than those of transistor amps, particularly when overdriven (turned up to the level that the amplifier starts to clip or shear off the wave forms). Instead of abruptly clipping off the signal at cut-off and saturation levels, the signal is rounded off more smoothly. Vacuum tubes also exhibit different harmonic effects than transistors. In contrast to the \"tweed\"-style amplifiers, which use speakers in an open-backed cabinet, companies such as Marshall tend to use 12\" speakers in a closed-back cabinet. These amplifiers usually allow users to switch between \"clean\" and distorted tones (or a rhythm guitar-style \"crunch\" tone and a sustained \"lead\" tone) with a foot-operated switch.\n\nBass amplifiers are designed for bass guitars or more rarely, for upright bass. They differ from amplifiers for the regular (and comparatively higher-pitched) electric guitar in several respects. They have extended low frequency response and tone controls optimised for bass instruments, which produce pitches of 41 Hz, in the case of a standard four-string electric bass or double bass, or even lower for five- or six-string electric basses.\n\nHigher-cost bass amplifiers sometimes include built-in bass effects, which are electronic effects units designed for electric bass or more rarely, for upright bass. Common built-in effects include audio compressor or limiter features, which help to keep the amplifier from doing unwanted distorting at high volume levels and potentially damaging the speakers; equalizers; and in some amps from the 1980s and more commonly in the 2000s, bass overdrive. Bass amps may provide an XLR DI output for plugging the bass amp signal directly into a mixing board or PA system. Larger, more powerful bass amplifiers (300 or more watts) are often provided with internal or external metal heat sinks and/or fans to help keep the amplifier cool. \n\nSpeaker cabinets designed for bass usually use larger loudspeakers (or more loudspeakers, in the case of the popular 4x10\" cabinets, which contain four 10\" speakers) than the cabinets used for other instruments, so that they can move the larger amounts of air needed to reproduce low frequencies. Bass players have to use more powerful amplifiers than the electric guitarists, because deep bass frequencies take more power to amplify. As such, in a band in which the electric guitar player uses a 50 watt guitar amp, the bass player typically uses a 200 watt to 300 watt bass amp. While the largest speakers commonly used for regular electric guitar are 12\" speakers, electric bass speaker cabinets often use 15\" speakers. Bass players who play styles of music that require an extended low-range response, such as death metal, sometimes use speaker cabinets with 18\" speakers or add a large subwoofer cabinet to their rig.\n\nSpeakers for bass instrument amplification tend to be heavier-duty than those for regular electric guitar, and the speaker cabinets are typically more rigidly constructed and heavily braced, to prevent unwanted buzzes and rattles. Bass cabinets often include bass reflex ports, vents or openings in the cabinet, which improve the bass response and low-end, especially at high volumes.\n\nA keyboard amplifier, used for the stage piano, synthesizer, clonewheel organs and similar instruments, is distinct from other types of amplification systems due to the particular challenges associated with keyboards; namely, to provide solid low-frequency sound reproduction \"and\" crisp high-frequency sound reproduction. It is typically a combination amplifier that contains a two, three, or four-channel mixer, a pre-amplifier for each channel, equalization controls, a power amplifier, a speaker, and a horn, all in a single cabinet.\n\nNotable exceptions include keyboard amplifiers for specific keyboard types. The vintage Leslie speaker cabinet and modern recreations, which are generally used for Hammond organs, use a tube amplifier that is often turned up to add a warm, \"growling\" overdrive. Some electric pianos have built-in amplifiers and speakers, in addition to outputs for external amplification.\n\nThese amplifiers are intended for acoustic instruments such as violin (\"fiddle\"), mandolin, and acoustic guitar—especially for the way musicians play these instruments in quieter genres such as folk and bluegrass. They are similar to keyboard amplifiers, in that they have a relatively flat frequency response and avoid tonal coloration. \n\nTo produce this relatively \"clean\" sound, these amplifiers often have very powerful amplifiers (up to 800 watts RMS), to provide additional \"headroom\" and prevent unwanted distortion. Since an 800 watt amplifier built with standard Class AB technology would be heavy, some acoustic amplifier manufacturers use lightweight Class D, \"switching amplifiers.\"\n\nAcoustic amplifier designs strive to produce a clean, transparent, \"acoustic\" sound that does not—except for reverb and other effects—alter the natural instrument sound, other than to make it louder. Amplifiers often come with a simple mixer to blend signals from a pickup and microphone. Since the early 2000s, it is increasingly common for acoustic amplifiers to provided digital effects, such as reverb and compression. Some also contain feedback-suppressing devices, such as notch filters or parametric equalizers.\n\nInstrument amplifiers have a different purpose than 'Hi-Fi' (high fidelity) stereo amplifiers in radios and home stereo systems. Hi-fi home stereo amplifiers strive to accurately reproduce signals from pre-recorded music, with as little harmonic distortion as possible. In contrast, instrument amplifiers are add additional tonal coloration to the original signal or emphasize certain frequencies. For electric instruments such as electric guitar, the amplifier helps to create the instrument's tone by boosting the input signal gain and distorting the signal, and by emphasizing frequencies deemed desirable (e.g., low frequencies) and de-emphasizing frequencies deemed undesirable (e.g., very high frequencies).\n\nIn the 1960s and 1970s, large, heavy, high output power amplifiers were preferred for instrument amplifiers, especially for large concerts, because public address systems were generally only used to amplify the vocals. Moreover, in the 1960s, PA systems typically did not use monitor speaker systems to amplify the music for the onstage musicians. Instead, the musicians were expected to have instrument amplifiers that were powerful enough to provide amplification for the stage and audience. In late 1960s and early 1970s rock concerts, bands often used large stacks of speaker cabinets powered by heavy tube amplifiers such as the Super Valve Technology (SVT) amplifier, which was often used with eight 10\" speakers.\n\nHowever, over subsequent decades, PA systems substantially improved, and used different approaches, such as horn-loaded \"bass bins\" (in the 1980s) and subwoofers (1990s and 2000s) to amplify bass frequencies. As well, in the 1980s and 1990s, monitor systems substantially improved, which helped sound engineers provide onstage musicians with a better reproduction of their instruments' sound. \n\nAs a result of improvements to PA and monitor systems, musicians in the 2000s no longer need huge, powerful amplifier systems. A small combo amplifier patched into the PA suffices. In the 2000s, virtually all sound reaching the audience in large venues comes from the PA system. Onstage instrument amplifiers are more likely to be at a low volume, because high volume levels onstage make it harder for the sound engineer to control the sound mix. \n\nAs a result, in many large venues much of the onstage sound reaching the musicians now comes from in-ear monitors, not from the instrument amplifiers. While stacks of huge speaker cabinets and amplifiers are still used in concerts (especially in heavy metal), this is often mainly for aesthetics or to create a more authentic tone. The switch to smaller instrument amplifiers makes it easier for musicians to transport their equipment to performances. As well, it makes concert stage management easier at large clubs and festivals where several bands are performing in sequence, because the bands can be moved on and off the stage more quickly.\n\nInstrument amplifiers may be based on thermionic (\"tube\" or \"valve\") or solid state (transistor) technology.\n\nVacuum tubes were the dominant active electronic components in amplifiers from the 1930s through the early 1970s, and tube amplifiers remain preferred by many musicians and producers. Some musicians feel that tube amplifiers produce a \"warmer\" or more \"natural\" sound than solid state units, and a more pleasing overdrive sound when overdriven. However, these subjective assessments of the attributes of tube amplifiers' sound qualities are the subject of ongoing debate. Tube amps are more fragile, require more maintenance, and are usually more expensive than solid state amps.\n\nTube amplifiers produce more heat than solid state amplifiers, but few manufacturers of these units include cooling fans in the chassis. While tube amplifiers do need to attain a proper operating temperature, if the temperature goes above this operating temperature, it may shorten the tubes' lifespan and lead to tonal inconsistencies.\n\nBy the 1960s and 1970s, semiconductor transistor-based amplifiers began to become more popular because they are less expensive, more resistant to bumps during transportation, lighter-weight, and require less maintenance. In some cases, tube and solid-state technologies are used together in amplifiers. A common setup is the use of a tube preamplifier with a solid-state power amplifier. There are also an increasing range of products that use digital signal processing and digital modeling technology to simulate many different combinations of amp and cabinets. \n\nThe output transistors of solid-state amplifiers can be passively cooled by using metal fins called heatsinks to radiate away the heat. For high-wattage amplifiers (over 800 watts), a fan is often used to move air across internal heatsinks.\nThe most common hybrid amp design is to use a tube preamp with a solid state power amplifier. This gives users the pleasing preamp and overdrive tone of a tube amp with the lowered cost, maintenance and weight of a solid state power amp.\n\n"}
{"id": "6471605", "url": "https://en.wikipedia.org/wiki?curid=6471605", "title": "International Centre for Integrated Mountain Development", "text": "International Centre for Integrated Mountain Development\n\nIS MY RIGHT\n\nThe International Centre for Integrated Mountain Development (ICIMOD) is a regional intergovernmental learning and knowledge sharing centre serving the eight regional member countries of the Hindu Kush Himalayas . ICIMOD serves eight regional member countries (RMCs) of the Hindu Kush Himalayan region – Afghanistan, Bangladesh, Bhutan, China, India, Myanmar, Nepal, and Pakistan – and the global mountain community. Founded in 1983, ICIMOD is based in Lalitpur, Nepal, and brings together a partnership of its regional member countries, partner institutions, and donors with a commitment for development action to secure a better future for the people and environment of the extended Himalayan region.\n\nMen, women, and children of the Hindu Kush Himalayas enjoy improved well being.\n\nTo enable sustainable and resilient mountain development for improved and equitable livelihoods through knowledge and regional cooperation.\n\nThe idea of creating an institution to promote the ecologically sound development of mountainous regions was first discussed at the International Workshop on the Development of Mountain Environment in December 1974 in Munich, Germany, but it was only five years later in 1979 during a United Nations Educational, Scientific and Cultural Organisation (UNESCO) Regional Meeting in Kathmandu, under the framework of the Man and the Biosphere Programme, that concrete commitments were made to establish the Centre. The Government of Nepal offered to host the new institution, and the Governments of Switzerland and the Federal Republic of Germany and UNESCO agreed to act as the founding sponsors. His Majesty’s Government of Nepal and UNESCO signed the agreement that provided the legal basis for establishing the Centre in September 1981 in Paris. The Centre was finally established and inaugurated on 5 December 1983 with its headquarters in Lalitpur, Nepal, and legitimised through an Act of Parliament in Nepal in the same year.\n\n"}
{"id": "19244404", "url": "https://en.wikipedia.org/wiki?curid=19244404", "title": "Joint replacement of the hand", "text": "Joint replacement of the hand\n\nJoint replacement of the hand is a procedure that was invented by the Scottish scientist, Dr. Mitchell McGuire. The procedure was considered a major breakthrough in the medical field at the time. However, it is now considered an almost standard operation. The first successful surgery of this kind was conducted on 21 December 1992, in New York City, USA.\n\nThis surgical option is reserved for patients with advanced arthritis or with a hand deformity.\n\nMerging of a joint involves removing the joint and surgically \"fusing\" the joint's end so that the two bones effectively form one solid bone. This surgery stops all movement at that joint and therefore eliminates the pain. The benefit of fusion is pain relief and the downside is elimination of motion at the fused joint, which can hinder function. Arthritic joint replacements are usually the most effective surgical option in more youthful and active patients. Younger patients may not be candidates for joint replacement because of the increased stress demand on the joints which accompany higher activity levels. This increased stress demand can quickly wear out an artificial joint.\n\nFor those with a hand deformity, the surgical procedure varies slightly. Instead of the joint being removed and replaced with a prosthetic hand, a hand from a donor is used.\n"}
{"id": "11470486", "url": "https://en.wikipedia.org/wiki?curid=11470486", "title": "Marvin L. Manheim Award", "text": "Marvin L. Manheim Award\n\nThe Marvin L. Manheim Award For Significant Contributions in the Field of Workflow is an industry recognition created by the Workflow Management Coalition in honor of the late Marvin L. Manheim. Manheim was a co-founder of the Workflow Management Coalition and was the William A. Patterson Distinguished Professor of Transportation at the Kellogg Graduate School of Management at Northwestern University from 1983 until his death in August 2000.\n\nBefore joining the Kellogg School, he held faculty positions at the Massachusetts Institute of Technology. Prof. Manheim's major area of interest was information technology and its uses strategically, competitively, and organizationally. It included strategy formulation and implementation processes; the management of globally competing organizations; and international transportation and logistics. He was also interested in computer assistance to human problem-solving and decision-making, including decision support systems (DSS) and artificial intelligence.\n\nThe workflow award is given annually, since 2002, to recognize an individual or a specific group for their \"influence, contribution, or distinguished use of workflow systems.\" The criteria for consideration is based on any significant contribution and not limited to members of the coalition. Both software developers and software end users have finalists and award recipients.\n\nManheim was a founder of Cambridge Systematics, who in 2001 established a separate awards program called the \"Marvin L. Manheim Award\" to honor his memory and promote innovation in the field of transportation. This is a separate program and not related to the workflow award.\n\n"}
{"id": "750842", "url": "https://en.wikipedia.org/wiki?curid=750842", "title": "Microchip revolution", "text": "Microchip revolution\n\nThe microchip revolution had its beginnings with the inventions of integrated circuit (IC) and the microprocessor, both of which were developed to increase computer efficiency. \n\nThis in turn caused the much broader digital revolution, one of the most significant occurrences in the history of humankind.\n"}
{"id": "1490247", "url": "https://en.wikipedia.org/wiki?curid=1490247", "title": "Midwater trawling", "text": "Midwater trawling\n\nMidwater trawling is trawling, or net fishing, at a depth that is higher in the water column than the bottom of the ocean. It is contrasted with bottom trawling. Midwater trawling is also known as pelagic trawling and bottom trawling as benthic trawling.\nIn midwater trawling, a cone-shaped net can be towed behind a single boat and spread by trawl doors, or it can be towed behind two boats (pair trawling) which act as the spreading device. Midwater trawling catches pelagic fish such as anchovies, shrimp, tuna and mackerel, whereas bottom trawling targets both bottom living fish (groundfish) and semi-pelagic fish such as: cod, squid, halibut and rockfish.\n\nWhereas bottom trawling can leave serious incidental damage to the sea bottom in its trail, midwater trawling by contrast is relatively benign.\n\n\n\n"}
{"id": "8441254", "url": "https://en.wikipedia.org/wiki?curid=8441254", "title": "Missile Row", "text": "Missile Row\n\nMissile Row was a nickname given in the 1960s to the US Air Force and NASA launch complexes at Cape Canaveral Air Force Station (CCAFS). Operated by the 45th Space Wing of the U.S. Air Force since 1949, it was the site of all pre-Apollo 8 manned launches, as well as many other early Department of Defense (DoD) and NASA launches. For the DoD, it plays a secondary role to Vandenberg AFB in California, but is the launch site for many NASA unmanned space probes, as those spacecraft are typically launched on Air Force launchers. Active launch vehicles are in bold.\n\nMuch of the support activity for CCAFS occurs at Patrick Air Force Base to the south, its reporting base.\n\nSome of the launch complexes have been recommissioned for modern space vehicle launches.\n"}
{"id": "1234347", "url": "https://en.wikipedia.org/wiki?curid=1234347", "title": "Monofilament fishing line", "text": "Monofilament fishing line\n\nMonofilament fishing line (shortened to just mono) is fishing line made from a single fiber of plastic. Most fishing lines are now monofilament because monofilament fibers are cheap to produce and are produced in a range of diameters which have different tensile strengths (called \"tests\" after the process of tensile testing). Monofilament line is also manufactured in different colors, such as clear, white, green, blue, red, and fluorescent.\n\nMonofilament is made by melting and mixing polymers and then extruding the mixture through tiny holes, forming strands of line, which is then spun into spools of various thicknesses. The extrusion process controls not only the thickness of the line but its test as well.\n\nDuPont made public in 1938 that their company had invented nylon. This new invention was the first synthetic fiber, fabrics that are commonly used in textiles today. In 1939, DuPont began marketing nylon monofilament fishing lines; however, braided Dacron lines remained the most used and popular fishing line for the next two decades, as early monofilament line was very stiff or \"wiry\", and difficult to handle and cast. Early monofilament did, however, have good knot strength and very low visibility to the fish, creating a small loyal following among fishermen. In 1959 DuPont introduced Stren, a thinner and much softer monofilament line that could be used in a large range of reels, including newly introduced spinning and spin casting tackle. Stren's monofilament lines soon became a favorite with many fishermen because of its overall ease of use and it spawned a whole host of imitators.\n\nNew materials, e.g., Spectra or Dyneema, are finding growing usage as fishing lines. Polyvinylidene fluoride sold as fluorocarbon is very much like nylon monofilament, but has several advantages. Optical density is lower, which makes the line less easily discernible. The surface is harder so it is more resistant to sharp fish teeth and wear. Furthermore, PVDF does not take up water and it is resistant to UV-light. It is denser than nylon, too, which makes it sink faster.\n\nDyneema is also becoming very popular and it is much stronger, but it is used mostly as a braided fishing line. Because the elastic stretching is only a fraction of that of nylon monofilament, the contact with fish or bait is more direct. It is often used for deep water fishing from boats because lower diameters are used, which give less resistance to currents, and the low stretch makes bites easily detectable.\n\nMonofilament fishing line is used in a huge variety of fishing applications.\n\nMonofilament is not advisable for deepwater fishing, since it can absorb water, resulting in loose knots, and its sensitivity can decrease when it is wet. Monofilament degrades with time and can weaken when exposed to heat, sunlight, and/or salt water. When stored on a spool for a long time, it may come off the fishing reel in coils or loops. It is advisable to change monofilament line at regular intervals to prevent degradation.\n\nMonofilament fishing line is also used sometimes in medicine to test the sense of touch. The transparency of monofilament fishing line makes it desirable for special effects where objects need to look like they are floating unsupported.\n\nIt has also been used for string trimmers, musical instrument strings, sewing thread and bent in the shape of a staple for use as a septum piercing retainer.\n\nDiscarded monofilament lines can present serious environmental problems. These lines are extremely difficult to spot when submerged in water, and fish, birds, and other marine life can easily become entangled, causing starvation, amputation, and death. Ingestion is also a serious threat to wildlife. Monofilament lines also present a risk to swimmers and scuba divers. The breakdown of lines, especially in string trimmers, leads to microplastics which may cause starvation or poisoning of organisms in soil or water.\n\nFor these reasons, programs have been started to recycle fishing line, to keep it out of the environment. Specialized containers have been designed to collect fishing line for recycling.\n\n\n"}
{"id": "39509232", "url": "https://en.wikipedia.org/wiki?curid=39509232", "title": "Museum of Technology, Helsinki", "text": "Museum of Technology, Helsinki\n\nThe Museum of Technology, Helsinki, is situated in Helsinki, Finland. It is the only general museum of technology in the country.\n\nThe Museum of Technology is operated by the private Museum of Technology Foundation which was established in 1969. The foundation was established by industrial and engineering organizations, museum professionals and the municipalities of the Helsinki metropolitan area.\n\n"}
{"id": "40871795", "url": "https://en.wikipedia.org/wiki?curid=40871795", "title": "Myriam Joire", "text": "Myriam Joire\n\nMyriam Joire (born 1968 or 1969) is a technology writer in the United States, best known for her editorial and podcasting work on the website \"Engadget\".\n\nOriginally a video game programmer she became a writer/reviewer in 2010 with her blog \"tnkgrl Mobile\", which led to her becoming a senior editor at \"Engadget\".\n\nShe was a technology evangelist for the smart watch company Pebble Technology until October 2014.\n\nJoire lives in San Francisco. She was born in Cannes, France, and moved to Canada in 1986, originally as an exchange student; she lived in London, Ontario and later moved to Vancouver. She was married from 1995 to 1997 and has since identified as trans and queer. She began transitioning in 1999.\n\n\n"}
{"id": "25143300", "url": "https://en.wikipedia.org/wiki?curid=25143300", "title": "Nearmap", "text": "Nearmap\n\nNearmap is an Australian aerial imagery technology and location data company that provides frequently-updated, high-resolution aerial imagery of 88% of Australia's population, 68% of the US population, and 72% of the New Zealand population.\n\nHistorically, aerial imagery was available mainly to government and large enterprises, as it was quite an expensive and long process. First, a surveying company had to be engaged to take photos of a certain area of interest. Then, the data would be manually processed and stitched together digitally to create aerial maps. Imagery was then delivered on a hard disk. The entire process could take months and was repeated once every few years at best.\n\nNearmap disrupted this model by creating a patented camera system and software pipeline that enables the company to capture aerial photos, stitch them together into seamless digital maps, and publish the content online within days of capture.\nWith the cost of capture significantly lower than anything else on the market, Nearmap can update surveys more frequently and at a much lower cost. Its subscription model has effectively made high-quality aerial imagery available to a far wider swath of businesses and government organisations. Nearmap captures aerial images frequently, up to six times a year in urban centres, at a resolution of 5.8 cm - 7.5 cm per pixel or better.\nBy capturing aerial images with airplanes (as opposed to with satellites), Nearmap is able to solve for potential weather or atmospheric obstructions. The images are quickly processed and streamed to the cloud in a matter of days, where they are available for viewing and analysis on desktop, tablet, and mobile devices.\n\nNearmap provides its high-resolution aerial imagery content as a subscription service to a broad base of customers from diverse industries including construction, engineering, urban planning, insurance, solar, telecom, utilities, and government entities across local, state, and federal agencies. Subscribers use Nearmap to remotely inspect locations for better informed planning and monitoring; evaluate properties and infrastructural assets; and understand historical context at their locations of interest.\n\nNearmap serves over 8,200 businesses and organisations globally.\n\nNearmap provides three types of aerial imagery: Vertical, Panorama, and Measurable Obliques.\n\nVertical imagery is also known as orthorectified, providing a top-down view of the ground captured at a 90° angle. Orthorectification is a process used to correct the terrain distortion in aerial or satellite images that results from variations in the surface of the Earth and tilt of the satellite or aerial sensor being used to collect the data. This process allows accurate information to be gathered from the images such as distances, angles, and positions.\n\nThe Nearmap Panorama product provides a seamless mosaic view of locations and features from all four cardinal directions in a single viewing experience. Circumventing the need to switch from one image to another to view content from multiple perspectives, Nearmap Panorama allows users to pan and zoom across a large footprint uninterrupted, allowing information to be reviewed much more quickly.\n\nNearmap's Measurable Obliques allow users to make accurate height measurements of buildings and other ground features. Users can then export these images with the measurements and annotations included, allowing multiple stakeholders to communicate with accurate detail to an entire project team.\n\nNearmap aerial imagery is delivered through an easy-to-use cloud-based interface called MapBrowser™ or through API integrations. The MapBrowser web application allows users to easily switch base layers from Vertical to Panorama or Measurable Obliques content. It also allows users to navigate and search Nearmap's library of current and historical content, and includes tools for measuring and analysing locations. Nearmap's APIs offer integration with Esri® ArcGIS, Autodesk, CAMA systems, and other leading GIS and CAD applications via standard industry protocols including WMS, WMTS, and TMS. Along with current imagery, Nearmap's online archive offers an extensive gallery of historical imagery; as of 2018, up to 10 years of photographic maps were available for some parts of Australia.\n\nNearmap was founded in Perth, Western Australia, by Stuart Nixon in 2008. Nearmap's first capture of aerial imagery dates back to 2007.\n\nIn 2012, the business was acquired by an ASX-listed company Ipernica and it became the primary entity. Nearmap then moved its headquarters from Perth to Sydney. In 2014, the company expanded into the United States market. In 2017, Nearmap expanded its capture program and surveyed New Zealand's top 13 most populated cities, plus Queenstown. The same year, Nearmap expanded its aerial imaging content to provide 3D models of Australia's major capital cities. The business now offers DSM and textured mesh data sets for bespoke download.\n\n"}
{"id": "40935351", "url": "https://en.wikipedia.org/wiki?curid=40935351", "title": "Over-the-counter data", "text": "Over-the-counter data\n\nOver-the-counter data (OTCD) is a design approach used in data systems, particularly educational technology data systems, in order to increase the accuracy of users' data analyses by better reporting data. The approach involves adhering to standards that are organized by five components: Label, Supplemental Documentation, Help System, Package/Display, and Content.\n\nOTCD was inspired by the varied ways over-the-counter medication supports those using its contents. Just as it would be negligent for over-the-counter medication to contain no labeling, documentation, or other supports helping people to use its contents safely, it is deemed negligent for data systems to display data for educators without providing them with the necessary supports to best ensure it is used correctly when educators use the data to treat students’ needs.\n\nInspired by the varied ways over-the-counter medication supports those using its contents, OTCD was created in 2010 and applied to the improvement of education data systems. Consider the way in which the Food and Drug Administration (FDA) requires over-the-counter medication to be accompanied by textual guidance proven to improve its use, deeming it negligent to do otherwise. With such guidance, patients may take over-the-counter medication with the goal of improving wellbeing while a doctor is not present to explain how to use the medication. No or poor medication labels have resulted in many errors and tragedy, as people are left with no way to know how to use the contents wisely.\n\nLabeling conventions can translate to improved understanding on non-medication products, as well. Thus, in the way over-the-counter medicine’s proper use is communicated with a thorough label and added documentation, a data system used to analyze student performance can include components to help users better comprehend the data it contains. Using an OTCD approach (i.e., following OTCD Standards) when communicating data involves following research-based recommendations likely to improve educators’ understanding, analysis, and use of the data being displayed.\n\nNonetheless, labeling and tools within data systems to assist analyses are uncommon, even though most educators analyze data alone. Essentially, data systems and reports do not commonly present data in an “over-the-counter” format for educators, whose primary option for using data to treat students is thus compared to ingesting medicine from an unmarked or marginally marked container.\nJust as it would be negligent for over-the-counter medicine to contain no labeling, documentation, or other supports helping people to use its contents safely, it is negligent for data systems and reports to display data for educators without providing necessary supports to best ensure the data is used appropriately and thus has a \"desirable\" impact on students.\n\nThe recommendations summarized by OTCD Standards (below) are based on research in education and edtech, as well as research in a variety of other fields (e.g., behavioral economics, design, business analytics, technology, and more). An OTCD approach is not meant to replace educators’ professional development or other interventions that improve data use, but it is an added solution that doesn’t cost educators more time, money, or stress.\n\nEducators have widely accepted the importance of using data to inform their treatment of students’ needs. This is a good thing, as research touts the benefits of effective data use. Unfortunately, educators’ widespread data use is not always a good thing. A significant portion – and some research claims most – of educators analyzing and using data are doing so incorrectly. For example in two U.S. Department of Education studies conducted in districts known for strong data use, teachers achieved only 48% accuracy when making data inferences involving basic statistical concepts.\nThus educators are using data to inform decisions, but they do not always understand the data they are using. Since their data-misinformed decisions impact the students such decisions are meant to impact, this is a significant problem. Edtech products that present data to educators in an over-the-counter format – as opposed to simply “showing the data” and requiring educators to dig up resources to aid analyses – play an active role in improving educators’ data use.\n\nThough numerous studies over the years have produced evidence on which the OTCD standards are based, one quantitative study in 2013 focused specifically on OTCD’s direct impact on data analysis accuracy (as opposed to merely determining which edtech aspects educators prefer). 211 educators of varied backgrounds at nine schools in six different California school districts participated in the \"Over-the-Counter Data’s Impact on Educators’ Data Analysis Accuracy\" study. The study’s premise was to determine the precise impact on analysis accuracy when data system reporting environments made data “over-the-counter,” giving educators embedded supports like the kind over-the-counter medication provides for users in the form of labeling and supplemental documentation. Key findings were significant and hold implications for educators, educational technology and/or data system vendors, and anyone else involved in communicating data to educators:\n\nRelating to primary research questions\n\n\nRelating to Secondary Research Questions\n\n\nOTCD Standards involve embedding data analysis supports directly within reporting environments and adhering to best practices concerning design. OTCD Standards were designed to be used by anyone communicating data to educators and to be reflected in the tool(s) through which the data is communicated (e.g., data report, data system, or other edtech product with a data component). Their purpose is to foster optimal educator (“user”) understanding, analysis, and use of the data being provided.\n\nOrganizations’ and publications’ mentions of OTCD include:\n"}
{"id": "40436041", "url": "https://en.wikipedia.org/wiki?curid=40436041", "title": "Portable Aqua Unit for Lifesaving", "text": "Portable Aqua Unit for Lifesaving\n\nThe Portable Aqua Unit for Lifesaving (short PAUL), also known as Water Backpack is a portable membrane water filter developed at the University of Kassel for humanitarian aid. It allows the decentralized supply of clean water in emergency and disaster situations.\n\nThe filter only needs water (e.g. from wells or rivers), to function. There are neither chemicals nor energy nor trained personnel required. The entire operation is shown in four pictograms, so that it can be operated without any prior knowledge, as a test with different population groups in India has shown.\n\nThe core of the device is a membrane filter unit. After it is set up at its destination, it is filled with about 100 litres of raw water from surface waters. After a waiting period of one to two minutes the filtered water flows out of the drain hose. During filtering raw water must be replenished continuously .\n\nAt about 1.15 metres of water pressure, the water is filtered through the membrane with a pore size of 20 to 100 nm. The device removes bacteria with an efficiency of 99.999% (measurement Institut Fresenius, E. coli and Coliform) and viruses to 99.9% (measured Bonn University, coliphages).\nA system based on ultrafiltration system (unlike Reverse osmosis based units) is not able to filter out solutes like salts or liquids like mineral oils. They pass through the membrane. Water contaminated with such substances therefore can not be cleaned.\n\nA device with an average supply of 1200 litres of raw water can, according to the Sphere standards (2011), supply clean, drinkable water for 400 people per day.\n\nThe water filter is designed for use in emergency and disaster situations. As a backpack it can, if necessary, be brought by walking to the locations. It first came in March 2010 to use in Chile. Since September 2010, the spread increased significantly so that in April 2012, about 700 copies in over 30 countries worldwide.\n\nAs the lifespan of the membrane is around ten years, aid agencies can leave the device after a disaster on site. Regular servicing or cleaning of the filter every few months is recommended, and depending on the degree of contamination of the raw water necessary. To clean it the backpack is to be filled once completely and then emptied through the bottom outlet to flush the sediments out.\n\nPAUL is beside the German Foreign Office used by many organizations in humanitarian relief.\n\nThe device was developed in the Department of Urban Water Management in the Department of Civil Engineering at the University of Kassel under the \"German Federal Environmental Foundation\" funded projects. The current optimization project will run until mid-2013 in a research project.\n\nThe project \"PAUL - Potable water at disasters\" in 2011 at the German competition \"365 Landmarks in the Land of Ideas\" as the national winner in the Category Society Competition.\n\n\n\n"}
{"id": "343276", "url": "https://en.wikipedia.org/wiki?curid=343276", "title": "Regenerative circuit", "text": "Regenerative circuit\n\nA regenerative circuit is an amplifier circuit that employs positive feedback (also known as regeneration or reaction). Some of the output of the amplifying device is applied back to its input so as to add to the input signal, increasing the amplification. One example is the Schmitt trigger (which is also known as a regenerative comparator), but the most common use of the term is in RF amplifiers, and especially regenerative receivers, to greatly increase the gain of a single amplifier stage.\n\nThe regenerative receiver was invented in 1912 and patented in 1914 by American electrical engineer Edwin Armstrong when he was an undergraduate at Columbia University. It was widely used between 1915 and World War II. Advantages of regenerative receivers include increased sensitivity with modest hardware requirements, and increased selectivity because the \"Q\" of the tuned circuit will be increased when the amplifying vacuum tube or transistor has its feedback loop around the tuned circuit (via a \"tickler\" winding or a tapping on the coil) because it introduces some negative resistance.\n\nDue partly to its tendency to radiate interference when oscillating, by the 1930s the regenerative receiver was largely superseded by other TRF receiver designs (for example \"reflex\" receivers) and especially by another Armstrong invention - superheterodyne receivers and is largely considered obsolete. Regeneration (now called positive feedback) is still widely used in other areas of electronics, such as in oscillators, active filters, and bootstrapped amplifiers.\n\nA receiver circuit that used larger amounts of regeneration in a more complicated way to achieve even higher amplification, the superregenerative receiver, was also invented by Armstrong in 1922. It was never widely used in general commercial receivers, but due to its small parts count it was used in specialized applications. One widespread use during WWII was IFF transceivers, where single tuned circuit completed the entire electronics system. It is still used in a few specialized low data rate applications, such as garage door openers, wireless networking devices, walkie-talkies and toys.\n\nThe gain of any amplifying device, such as a vacuum tube, transistor, or op amp, can be increased by feeding some of the energy from its output back into its input in phase with the original input signal. This is called positive feedback or \"regeneration\". Because of the large amplification possible with regeneration, regenerative receivers often use only a single amplifying element (tube or transistor). In a regenerative receiver the output of the tube or transistor is connected back to its own input through a tuned circuit (LC circuit). The tuned circuit allows positive feedback only at its resonant frequency. In regenerative receivers using only one active device, the same tuned circuit is coupled to the antenna and also serves to select the radio frequency to be received, usually by means of variable capacitance. In the regenerative circuit discussed here, the active device also functions as a detector; this circuit is also known as a \"regenerative detector\". A regeneration control is usually provided for adjusting the amount of feedback (the loop gain). It is desirable for the circuit design to provide regeneration control that can gradually increase feedback to the point of oscillation and that provides control of the oscillation from small to larger amplitude and back to no oscillation without jumps of amplitude or hysteresis in control.\n\nTwo important attributes of a radio receiver are \"sensitivity\" and \"selectivity\". The regenerative detector provides sensitivity and selectivity due to voltage amplification and the characteristics of a resonant circuit consisting of inductance and capacitance. The regenerative voltage amplification formula_1 is formula_2 where formula_3 is the non-regenerative amplification and formula_4 is the portion of the output signal fed back to the L2 C2 circuit. As formula_5 becomes smaller the amplification increases. The formula_6 of the tuned circuit (L2 C2) without regeneration is formula_7 where formula_8 is the reactance of the coil and formula_9 represents the total dissipative loss of the tuned circuit. The positive feedback compensates the energy loss caused by formula_9, so it may be viewed as introducing a negative resistance formula_11 to the tuned circuit. The formula_6 of the tuned circuit with regeneration is formula_13. The regeneration increases the formula_6. Oscillation begins when formula_15.\n\nRegeneration can increase the detection gain of a detector by a factor of 1,700 or more. This is quite an improvement, especially for the low-gain vacuum tubes of the 1920s and early 1930s. The type 36 screen-grid tube (obsolete since the mid-1930s) had a non-regenerative detection gain (audio frequency plate voltage divided by radio frequency input voltage) of only 9.2 at 7.2 MHz, but in a regenerative detector, had detection gain as high as 7,900 at critical regeneration (non-oscillating) and as high as 15,800 with regeneration just above critical. The \"... non-oscillating regenerative amplification is limited by the stability of the circuit elements, tube [or device] characteristics and [stability of] supply voltages which determine the maximum value of regeneration obtainable without self-oscillation\". Intrinsically, there is little or no difference in the gain and stability available from vacuum tubes, JFETs, MOSFETs or bipolar junction transistors (BJTs).\n\nA major improvement in stability and a small improvement in available gain for reception of CW radiotelegraphy is provided by the use of a separate oscillator, known as a \"heterodyne oscillator\" or \"beat oscillator\". Providing the oscillation separately from the detector allows the regenerative detector to be set for maximum gain and selectivity - which is always in the non-oscillating condition. Interaction between the detector and the beat oscillator can be minimized by operating the beat oscillator at half of the receiver operating frequency, using the second harmonic of the beat oscillator in the detector.\n\nFor AM reception, the gain of the loop is adjusted so it is just below the level required for oscillation (a loop gain of just less than one). The result of this is to greatly increase the gain of the amplifier at the bandpass frequency (resonant frequency), while not increasing it at other frequencies. So the incoming radio signal is amplified by a large factor, 10 - 10, increasing the receiver's sensitivity to weak signals. The high gain also has the effect of reducing the circuit's bandwidth (increasing the Q) by an equal factor, increasing the selectivity of the receiver.\n\nFor the reception of CW radiotelegraphy (Morse code), the feedback is increased just to the point of oscillation. The tuned circuit is adjusted to provide typically 400 to 1000 Hertz difference between the receiver oscillation frequency and the desired transmitting station's signal frequency. The two frequencies \"beat\" in the nonlinear amplifier, generating heterodyne or \"beat\" frequencies. The difference frequency, typically 400 to 1000 Hertz, is in the audio range; so it is heard as a tone in the receiver's speaker whenever the station's signal is present.\n\nDemodulation of a signal in this manner, by use of a single amplifying device as oscillator and mixer simultaneously, is known as \"autodyne\" reception. The term \"autodyne\" predates multigrid tubes and is not applied to use of tubes specifically designed for frequency conversion.\n\nFor the reception of single-sideband (SSB) signals, the circuit is also adjusted to oscillate as in CW reception. The tuning is adjusted until the demodulated voice is intelligible.\n\nRegenerative receivers require fewer components than other types of receiver circuit, such as the TRF and superheterodyne. The circuit's advantage was that it got much more amplification (gain) out of the expensive vacuum tubes, thus reducing the number of tubes required and therefore the cost of a receiver. Early vacuum tubes had low gain and tended to oscillate at radio frequencies (RF). TRF receivers often required 5 or 6 tubes; each stage requiring tuning and neutralization, making the receiver cumbersome, power hungry, and hard to adjust. A regenerative receiver, by contrast, could often provide adequate reception with the use of only one tube. In the 1930s the regenerative receiver was replaced by the superheterodyne circuit in commercial receivers due to the superheterodyne's superior performance and the falling cost of tubes. Since the advent of the transistor in 1946, the low cost of active devices has removed most of the advantage of the circuit. However, in recent years the regenerative circuit has seen a modest comeback in receivers for low cost digital radio applications such as garage door openers, keyless locks, RFID readers and some cell phone receivers.\n\nA disadvantage of this receiver, especially in designs that couple the detector tuned circuit to the antenna, is that the regeneration (feedback) level must be adjusted when the receiver is tuned to a different frequency. The antenna impedance varies with frequency, changing the loading of the input tuned circuit by the antenna, requiring the regeneration to be adjusted. In addition, the Q of the detector tuned circuit components vary with frequency, requiring adjustment of the regeneration control.\n\nA disadvantage of the single active device regenerative detector in autodyne operation is that the local oscillation causes the operating point to move significantly away from the ideal operating point, resulting in the detection gain being reduced..\n\nAnother drawback is that when the circuit is adjusted to oscillate it can radiate a signal from its antenna, so it can cause interference to other nearby receivers. Adding an RF amplifier stage between the antenna and the regenerative detector can reduce unwanted radiation, but would add expense and complexity.\n\nOther shortcomings of regenerative receivers are the sensitive and unstable tuning. These problems have the same cause: a regenerative receiver’s gain is greatest when it operates on the verge of oscillation, and in that condition, the circuit behaves chaotically. Simple regenerative receivers electrically couple the antenna to the detector tuned circuit, resulting in the electrical characteristics of the antenna influencing the resonant frequency of the detector tuned circuit. Any movement of the antenna or large objects near the antenna can change the tuning of the detector.\n\nThe inventor of FM radio, Edwin Armstrong, invented and patented the regenerative circuit while he was a junior in college, in 1914. He patented the superregenerative circuit in 1922, and the superheterodyne receiver in 1918.\n\nLee De Forest filed a patent in 1916 that became the cause of a contentious lawsuit with the prolific inventor Armstrong, whose patent for the regenerative circuit had been issued in 1914. The lawsuit lasted twelve years, winding its way through the appeals process and ending up at the Supreme Court. Armstrong won the first case, lost the second, stalemated at the third, and then lost the final round at the Supreme Court.\n\nAt the time the regenerative receiver was introduced, vacuum tubes were expensive and consumed lots of power, with the added expense and encumbrance of heavy batteries. So this design, getting most gain out of one tube, filled the needs of the growing radio community and immediately thrived. Although the superheterodyne receiver is the most common receiver in use today, the regenerative radio made the most out of very few parts.\n\nIn World War II the regenerative circuit was used in some military equipment. An example is the German field radio \"Torn.E.b\". Regenerative receivers needed far fewer tubes and less power consumption for nearly equivalent performance.\n\nA related circuit, the \"superregenerative detector\", found several highly important military uses in World War II in Friend or Foe identification equipment and in the top-secret proximity fuze. An example here is the miniature RK61 thyratron marketed in 1938, which was designed specifically to operate like a vacuum triode below its ignition voltage, allowing it to amplify analog signals as a self-quenching superregenerative detector in radio control receivers, and was the major technical development which led to the wartime development of radio-controlled weapons and the parallel development of radio controlled modelling as a hobby.\n\nIn the 1930s, the superheterodyne design began to gradually supplant the regenerative receiver, as tubes became far less expensive. In Germany the design was still used in the millions of mass-produced German \"peoples receivers\" (Volksempfänger) and \"German small receivers\" (DKE, Deutscher Kleinempfänger). Even after WWII, the regenerative design was still present in early after-war German minimal designs along the lines of the \"peoples receivers\" and \"small receivers\", dictated by lack of materials. Frequently German military tubes like the \"RV12P2000\" were employed in such designs. There were even superheterodyne designs, which used the regenerative receiver as a combined IF and demodulator with fixed regeneration. The superregenerative design was also present in early FM broadcast receivers around 1950. Later it was almost completely phased out of mass production, remaining only in hobby kits, and some special applications, like gate openers.\n\nThe superregenerative receiver uses a second lower-frequency oscillation (within the same stage or by using a second oscillator stage) to provide single-device circuit gains of around one million. This second oscillation periodically interrupts or \"quenches\" the main RF oscillation. Ultrasonic quench rates between 30 and 100 kHz are typical. After each quenching, RF oscillation grows exponentially, starting from the tiny energy picked up by the antenna plus circuit noise. The amplitude reached at the end of the quench cycle (linear mode) or the time taken to reach limiting amplitude (log mode) depends on the strength of the received signal from which exponential growth started. A low-pass filter in the audio amplifier filters the quench and RF frequencies from the output, leaving the AM modulation. This provides a crude but very effective automatic gain control (AGC).\n\nSuperregenerative detectors work well for wide-band signals such as FM, where they perform \"slope detection\". Regenerative detectors work well for narrow-band signals, especially for CW and SSB which need a heterodyne oscillator or BFO. A superregenerative detector does not have a usable heterodyne oscillator – even though the superregen always self-oscillates, so CW (Morse code)and SSB (single side band) signals can't be received properly.\n\nSuperregeneration is most valuable above 27 MHz, and for signals where broad tuning is desirable. The superregen uses many fewer components for nearly the same sensitivity as more complex designs. It is easily possible to build superregen receivers which operate at microwatt power levels, in the 30 to 6,000 MHz range. It removes the need for the operator to manually adjust regeneration level to just below the point of oscillation - the circuit automatically is taken out of oscillation periodically, but with the disadvantage that small amounts of interference may be a problem for others. These are ideal for remote-sensing applications or where long battery life is important. For many years, superregenerative circuits have been used for commercial products such as garage-door openers, radar detectors, microwatt RF data links, and very low cost walkie-talkies.\n\nBecause the superregenerative detectors tend to receive the strongest signal and ignore other signals in the nearby spectrum, the superregen works best with bands that are relatively free of interfering signals. Due to Nyquist's theorem, its quenching frequency must be at least twice the signal bandwidth. But quenching with overtones acts further as a heterodyne receiver mixing additional unneeded signals from those bands into the working frequency. Thus the overall bandwidth of superregenerator cannot be less than 4 times that of the quench frequency, assuming the quenching oscillator produces an ideal sine wave.\n\n\n\n\n"}
{"id": "29357629", "url": "https://en.wikipedia.org/wiki?curid=29357629", "title": "Remove before flight", "text": "Remove before flight\n\nRemove before flight is a safety warning often seen on removable aircraft and spacecraft components, typically in the form of a red ribbon, to indicate that a device, such as a protective cover or a pin to prevent the movement of mechanical parts, is only used when the aircraft is on the ground (parked or taxiing). On small general aviation aircraft, this may include a pitot tube cover or a control lock. The warning appears in English only. Similar ribbons labelled \"pull to arm\" or similar are found on missiles and other weapon systems that are not mounted on aircraft. \n\nRemove before flight components are often referred to as \"red tag items\". Typically, the ground crew will have a checklist of remove before flight items. Some checklists will require the ribbon or tag to be attached to the checklist to verify it has been removed. In some cases, non-removal of a labelled part has caused airplane crashes, like that of Aeroperú Flight 603.\n\nIt is common to see key rings, T-shirts, bag tags, belts and other such products with this ribbon, especially for people who work in aviation or aviation enthusiasts.\n\nA green tag is sometimes used to identify components that must be attached before flight in a similar way to remove before flight components.\n\n"}
{"id": "24900031", "url": "https://en.wikipedia.org/wiki?curid=24900031", "title": "River linking", "text": "River linking\n\nRiver Linking is project linking two or more rivers by creating a network of manually created canals, and providing land areas that otherwise does not have river water access and reducing the flow of water to sea using this means. It is based on the assumptions that surplus water in some rivers can be diverted to deficit rivers by creating a network of canals to interconnect the rivers.\n\nFor an instance, in India the rainfall over the country is primarily orographic, associated with tropical depressions originating in the Arabian Sea and the Bay of Bengal. The summer monsoon accounts for more than 85 per cent of the precipitation. The uncertainty of occurrence of rainfall marked by prolonged dry spells and fluctuations in seasonal and annual rainfall is a serious problem for the country. Large parts of Haryana, Maharashtra, Andhra Pradesh, Rajasthan, Gujarat, Madhya Pradesh, Karnataka and Tamil Nadu are not only in deficit in rainfall but also subject to large variations, resulting in frequent droughts and causing immense hardship to the population and enormous loss to the nation. The water availability even for drinking purposes becomes critical, particularly in the summer months as the rivers dry up and the ground water recedes. Regional variations in the rainfall lead to situations when some parts of the country do not have enough water even for raising a single crop. On the other hand, excess rainfall occurring in some parts of the country creates havoc due to floods.\nIrrigation using river water and ground water has been the prime factor for raising the food grain production in India from a mere 50 million tonnes in the 1950s to more than 200 million tonnes at present, leading India to attain self-sufficiency in food. Irrigated area has increased from 22 million hectares to 95 million hectares during this period. The population of India, which is around 1000 million at present, is expected to increase to 1500 to 1800 million in the year 2050 and that would require about 450 million tonnes of food grains. For meeting this requirement, it would be necessary to increase irrigation potential to 160 million hectares for all crops by 2050. India's maximum irrigation potential that could be created through conventional sources has been assessed to be about 140 million hectares. For attaining a potential of 160 million hectares, other strategies shall have to be evolved.\nFloods are a recurring feature, particularly by the Brahmaputra and Ganga rivers, in which almost 60 per cent of the river flows of India country occur. Flood damages, which were Rs. 52 crores in 1953, have gone up to Rs. 5,846 crores in 1998 with annual average being Rs. 1,343 crores affecting the States of Assam, Bihar, West Bengal and Uttar Pradesh along with untold human sufferings. On the other hand, large areas in the States of Rajasthan, Gujarat, Andhra Pradesh, Karnataka and Tamil Nadu face recurring droughts. As much as 85 percentage of drought prone area falls in these States. One of the most effective ways to increase the irrigation potential for increasing the food grain production, mitigating floods and droughts and reducing regional imbalance in the availability of water is the Inter Basin Water Transfer (IBWT) from the surplus rivers to deficit areas. Brahmaputra and Ganga particularly their northern tributaries, Mahanadi, Godavari and West Flowing Rivers originating from the Western Ghats are found to be surplus in water resources. If we can build storage reservoirs on these rivers and connect them to other parts of the country, regional imbalances could be reduced significantly and lot of benefits could be gained by way of additional irrigation, domestic and industrial water supply, hydropower generation, navigational facilities etc.\n\nBy linking the rivers, vast amount of land areas which will not otherwise be irrigated and are unusable for agriculture become fertile.\n\nDuring heavy rainy seasons some areas can experience heavy floods while other areas might be experiencing drought like situations. With network of rivers this problem can be greatly avoided by channeling excess water to areas that are not experiencing a flood or are dry.\n\nWith new canals built, feasibility of new dams to generate hydroelectric power becomes a possibility.\n\nNewly created network of canals opens up new routes and ways and routes of water navigation, which is generally more efficient and cheaper compared to road transport.\n\nThe National River Linking Project (NRLP) is designed to ease water shortages in western and southern India while mitigating the impacts of recurrent floods in the eastern parts of the Ganga basin. The NRLP, if and when implemented, will be one of the biggest interbasin water transfer projects in the world.\n\nOne of the major concerns is that rivers change their course in 70–100 years and thus once they are linked, future change of course could create huge practical problems for the project.\n\nA number of leading environmentalists are of the opinion that the project could be an ecological disaster. There would be a decrease in downstream flows resulting in reduction of fresh water inflows into the seas seriously jeopardizing aquatic life.\n\nCreation of canals would need large areas of land resulting large scale deforestation in certain areas.\n\nPossibility of new dams comes with the threat of large otherwise habitable or reserved land getting submerged under water or surface water.\n\nAs large strips of land might have to be converted to canals, a considerable population living in this areas must need to be rehabilitated to new areas.\n\n"}
{"id": "2206410", "url": "https://en.wikipedia.org/wiki?curid=2206410", "title": "Saran (plastic)", "text": "Saran (plastic)\n\nSaran is a trade name currently owned by S.C. Johnson & Son, Inc. for a polyethylene food wrap. The Saran trade name was once owned by Dow Chemical for polyvinylidene chloride (PVDC), along with other monomers. Since its accidental discovery in 1933, polyvinylidene chloride has been used for a number of commercial and industrial products. \n\nWhen formed into a thin plastic film, the principal advantages of polyvinylidene chloride, when compared to other plastics, are its ability to adhere to itself and its very low permeability to water vapor, flavor and aroma molecules, and oxygen. This oxygen barrier retards food spoilage, while the film barrier to flavor and aroma molecules helps food retain its flavor and aroma.\n\nPolyvinylidene chloride (PVDC) was discovered at Dow Chemical Company (Michigan, USA) in 1933 when a lab worker, Ralph Wiley, was having trouble washing beakers used in his process of developing a dry-cleaning product. It was initially developed into a spray that was used on US fighter planes and, later, automobile upholstery, to protect them from the elements. Dow Chemical later named the product Saran and eliminated its green hue and offensive odor.\n\nIn 1942, fused layers of original-specification PVDC were used to make woven mesh ventilating insoles for newly developed jungle or tropical combat boots made of rubber and canvas. These insoles were tested by experimental Army units in jungle exercises in Panama, Venezuela, and other countries, where they were found to increase the flow of dry outside air to the insole and base of the foot, reducing blisters and tropical ulcers. The PVDC ventilating mesh insole was later adopted by the United States Army for standard issue in its M-1945 and M-1966 Jungle Boots. In 1943, John Reilly (Ralph Wiley's boss) and Ralph Wiley of The Dow Chemical Co. completed the final work needed for introduction of PVDC, which had been invented in 1939. PVDC monofilaments were also extruded for the first time. The word \"Saran\" was coined by a combination of John Reilly's wife's and daughter's names, Sarah and Ann Reilly.\n\nIn 1949, Dow introduced \"Saran Wrap\", a thin, clingy plastic wrap that was sold in rolls and used primarily for wrapping food. It quickly became popular for preserving food items stored in the refrigerator. \"Saran Wrap\" was later acquired by S. C. Johnson & Son. However, today's \"Saran Wrap\" is no longer composed of PVDC in the United States, due to cost, processing difficulties, and perceived environmental concerns with halogenated materials, and is now made from polyethylene. However, polyethylene has a higher oxygen permeability, which in turn affects food spoilage prevention. For example, at 23 °C and 95% relative humidity polyvinylidene chloride has an oxygen permeability of 0.6 cm μm m d kPa while low-density polyethylene under the same conditions has an oxygen permeability of 2000 cm μm m d kPa. For that reason, packaging for the meat industry still may use PVDC containing films, as a barrier layer.\n\nAfter the end of the Vietnam War, the U.S. military phased out PVDC insoles in favor of Poron®, a microcellular urethane, for its jungle and combat boots. However, the British Army continues to use PVDC insoles in its combat boots, primarily because of its insulating properties.\n\nIn some jurisdictions, the name Saran is a registered trademark of the Dow Chemical Company, while in others, it has lost trademark status and become a generic term for these polymers. In Japan, Dow's trademark rights in Saran Wrap were assigned to Asahi Kasei, which uses polyvinylidene chloride.\n\n"}
{"id": "25384898", "url": "https://en.wikipedia.org/wiki?curid=25384898", "title": "Sevsat", "text": "Sevsat\n\nSEVSAT is an acronym for Ship Equip VSAT, a maritime satellite broadband system from the Ship Equip Group with its head office in Norway. Ship Equip is a subsidiary of Inmarsat, a mobile satellite services operator.\n\nSEVSAT was developed from 2000–2003, and was initially based on space segment allocations from Eutelsat and Stratos who had implemented the iDirect technology on some of their satellites. In 2004, Stratos closed down their maritime broadband division, and Ship Equip turned to the start-up space segment broker New Wave Broadband and satellite operator Intelsat. These two companies had independently decided to implement iDirect technology on the 907 satellite that covered central and northern Europe, where customers of Ship Equip were primarily located.\nFrom 2004–2006, while focusing primarily on its local markets in the offshore oil, gas and fisheries segments, the number of SEVSAT customers increased rapidly to over 100 installed systems early in 2006, and 200 systems by mid-2007. The rate of growth during this period caught the attention of the media and in April 2006, the leading financial newspaper in Norway, \"Dagens Næringsliv\", ran an article indicating that Ship Equip was challenging Telenor, which at that time was the market leader in maritime VSAT. (Shortly after, Telenor Satellite Services was purchased and merged with France Telecom Mobile Satellite Communications (FTMSC) under the Vizada brand). The article initiated further press coverage from regional newspaper Sunnmørsposten and national newspaper Finansavisen.\n\nIn 2009, the Comsys group's annual VSAT report quoted Ship Equip as having a 14.3% share in the maritime VSAT market. In 2011, Ship Equip was purchased by London-based Inmarsat for $159.5 million. According to Inmarsat, at the time of purchase, approximately 10% of vessels using Ku-band VSAT worldwide were using networks provided by Ship Equip.\n\nSEVSAT includes above deck equipment (ADE) and below deck equipment (BDE). The ADE consists of the antenna which, except for some early deliveries, are SeaTel antennas. Two types of SEVSAT systems exist, C-band and Ku-band, denoting the frequency range in which they are capable of receiving a satellite signal. The C-band antennas are generally large, from in dish diameter. The Ku-band antennas are smaller, from in dish diameter. The BDE most commonly consists of one electronic rack containing the following components: an iDirect modem, the SEVSAT global satellite switching unit, a DAC, Cisco Routers, telephone adapters and a UPS.\n"}
{"id": "10617707", "url": "https://en.wikipedia.org/wiki?curid=10617707", "title": "Sewer gas destructor lamp", "text": "Sewer gas destructor lamp\n\nThe main purpose of a sewer gas destructor lamp is to remove sewer gases and their hazards.\n\nBiogas forming in sewers via anaerobic digestion can be a potentially foul-smelling and explosive hazard (chiefly due to chemical spills). Unlike ordinary gas lamps for street lighting, the main purpose of sewer gas destructor lamps is to remove sewer gases and their hazards. Joseph Edmund Webb of Birmingham patented a sewer gas destructor lamp.\n\nMany of these lamps were installed in the UK in towns and cities including Sheffield, Winchester, Durham, Whitley Bay, Monkseaton and Blyth, Northumberland. With a flame generated by burning town gas, sewer gases were drawn from the sewer below and discharged above the heads of passers-by to dissipate odours. The flame in the lamp does not actually generate sufficient thermal energy to combust any of the odour compounds in the air.\n\nJE Webb addressed a number of problems of the lamps with further patents. His patent GB189408193, approved 2-March-1895, stated: \nIt has also been found that when the gases are drawn out from the sewer by the burning of ordinary gas a sudden flushing of the sewer might prevent any sewer gas from escaping, and thus momentarily cause the gas jets to be extinguished.\n\nIn order to solve this problem the patent specifies an arrangement of burners, air supply and heat reflection designed to produce an intense heat at the point of combustion. (Webb suggests ).\n\nThe lamps were installed at places where sewer gases were likely to collect, such as at the tops of hills. The city of Sheffield, being a hilly area, had many sewer gas destructor lamps and many remain.\n\n\"Sheffield on the Net\" has a section on the old gas lamps, which states: \n\nEighty-four of these street lamps were erected in Sheffield between 1914 and 1935, the largest number in any British town, due mainly to the many hills in the area where gas could be trapped.\nThe \"Sheffield Star\" newspaper reported a local survey of the lamps by W Jessop. This survey found 25 remaining lamps in Sheffield. Twenty of these are grade II listed. In 2016 Sheffield residents campaigned for the lamps to be restored when the city council's replacement of every lamppost began, as part of the 25-year Streets Ahead road improvement programme. Sheffield Council plans to repaint the lamps and convert them to solar power with LED lights to replicate the original lighting. Sheffield's four gas-powered lamps will remain so after their restoration. It is planned that the lamps will be restored by December 2017.\n\nOnly one working sewer gas destructor lamp remains in London; however due to a traffic accident the original lamp was damaged and has been replaced with a replica. This lamp is currently in use and can be found burning day and night down the side street of the Savoy Hotel in London. The story of this lamp has given rise to locals referring to Carting Lane as 'Farting Lane'.\n\nAlthough many of the existing lamps in Sheffield and elsewhere are now disused, the lamps still have a use today in reducing odours. They do not prevent explosions as the concentration of methane in sewer gas is below the lower explosion limit (LEL) for methane. If the methane concentration were over the explosive limit (≈ 50,000 ppmv) the open flames in the lamps would burn like flares.\n\n\n"}
{"id": "5260471", "url": "https://en.wikipedia.org/wiki?curid=5260471", "title": "Signal tracer", "text": "Signal tracer\n\nA signal tracer is a piece of electronic test equipment used to troubleshoot radio and other electronic circuitry.\n\nUsually a very simple device, it normally provides an amplifier, and a loudspeaker, often battery-powered and packaged into a small, hand-held test probe. An optional diode detector is usually also provided, allowing the detection of amplitude-modulated signals.\n\nThe technician injects a test signal into the device under test. Then, by using the signal tracer, the tech can follow the signal through the various circuits of the radio receiver. So long as the signal can be heard, the circuitry up to that point is (at least minimally) functional. If the signal disappears, however, a fault can be assumed to be present in the stage of the circuit just passed.\n\nThe diode detector is only sensitive to amplitude modulation but even circuits that are normally used for other modulation schemes (such as FM radios) can be tested by using an AM test signal for testing the radio frequency circuits, then switching to an FM test signal (and switching out the diode detector) for testing the audio circuits of the radio.\n\nMore sophisticated signal tracers may display digital levels using, for example, LEDs. For long pulse trains, a cyclic redundancy check may be calculated and displayed, giving the tech insight into the content of circuits that are switching rapidly.\n"}
{"id": "17347053", "url": "https://en.wikipedia.org/wiki?curid=17347053", "title": "Society of Mexican American Engineers and Scientists", "text": "Society of Mexican American Engineers and Scientists\n\nMAES: Latinos in Science and Engineering, Inc. (MAES), originally the \"Mexican American Engineering Society\" was founded in 1974. It organizes an annual symposium and career fair.\n\nMAES was founded in Los Angeles in 1974 to increase the number of Mexican Americans and other Hispanics in the technical and scientific fields.\n\nThe idea to establish a professional society for Mexican American engineers originated with Robert Von Hatten, an aerospace electronics engineer with TRW Defense Space Systems in Redondo Beach, California. Mr. Von Hatten had for several years served as volunteer for programs directed at combating the alarming number of high school dropouts. He envisioned a national organization that would serve as a source for role models, address the needs of its members, and become a resource for industry and students.\n\nIn mid–1974, Mr. Von Hatten contacted Manuel Castro to join him in the campaign to form the professional organization. During a subsequent series of meetings, a cohort of individuals banded together to lay out the foundation for the “Mexican American Engineering Society.” The founders, listed below, drafted the articles of incorporation and the first bylaws of the society.\n\nOscar Buttner – Rockwell International\nSam Buttner – Southern California Edison\"\nManuel Castro – Bechtel Power\nClifford Maldonado – Northrop Corporation\nSam Mendoza – California State University, Fullerton\nFrank Serna – Northrop Corporation\nRobert Von Hatten – TRW Defense Space Systems\n\nThe society filed incorporation papers as a nonprofit, tax exempt organization with the California Secretary of State in October 1974, and it received its charter on March 28, 1975. The Internal Revenue Service granted the society a federal tax–exemption letter and employer identification number on January 4, 1979. Ten years later, to reflect its broader technical membership, the organization filed to change its name to the “Society of Mexican American Engineers and Scientists, Inc.”. This change was granted on July 19, 1989.\n\nMAES is one of several membership–based organizations that represent Latinos in engineering and science. As a mature organization with over 30 years of experience addressing the concerns of Latinos, MAES is a source of expertise on barriers to and methods for improving educational access and attainment. The society recognizes the importance of encouraging more youth to pursue careers in science, technology, engineering, and mathematics as a means for economic advancement and workforce development.\n\nMany of its programs, with the financial help of members, companies, and government agencies are directed at increasing the number of students at all grade levels who will study, prepare, enter, and excel in the technical professions.\n\n\n"}
{"id": "100563", "url": "https://en.wikipedia.org/wiki?curid=100563", "title": "System on a chip", "text": "System on a chip\n\nA system on a chip or system on chip (, or ) is an integrated circuit (also known as a \"chip\") that integrates all components of a computer or other electronic system. These components typically include a central processing unit (CPU), memory, input/output ports and secondary storage – all on a single substrate. It may contain digital, analog, mixed-signal, and often radio frequency signal processing functions, depending on the application. As they are integrated on a single electronic substrate, SoCs consume much less power and take up much less area than multi-chip designs with equivalent functionality. Because of this, SoCs are very common in the mobile computing and edge computing markets. Systems on chip are commonly used in embedded systems and the Internet of Things.\n\nSystems on Chip are in contrast to the common traditional motherboard-based PC architecture, which separates components based on function and connects them through a central interfacing circuit board. Whereas a motherboard houses and connects detachable or replaceable components, SoCs integrate all of these components into a single integrated circuit, as if all these functions were built into the motherboard. An SoC will typically integrate a CPU, graphics and memory interfaces, hard-disk and USB connectivity, random-access and read-only memories and secondary storage on a single circuit die, whereas a motherboard would connect these modules as discrete components or expansion cards.\n\nMore tightly integrated computer system designs improve performance and reduce power consumption as well as semiconductor die area needed for an equivalent design composed of discrete modules, at the cost of reduced replaceability of components. By definition, SoC designs are fully or nearly fully integrated across different component modules. For these reasons, there has been a general trend towards tighter integration of components in the computer hardware industry, in part due to the influence of SoCs and lessons learned from the mobile and embedded computing markets. Systems-on-Chip can be viewed as part of a larger trend towards embedded computing and hardware acceleration.\n\nAn SoC integrates a microcontroller or microprocessor with advanced peripherals like graphics processing unit (GPU), Wi-Fi module, or one or more coprocessors. Similar to how a microcontroller integrates a microprocessor with peripheral circuits and memory, an SoC can be seen as integrating a microcontroller with even more advanced peripherals. \n\nIn general, there are four distinguishable types of SoCs: \n\n\nSystems-on-chip can be applied to any computing task. However, they are typically used in mobile computing such as tablets, smartphones, smartwatches and netbooks as well as embedded systems and in applications where previously microcontrollers would be used. \n\nWhere previously only microcontrollers could be used, SoCs are rising to prominence in the embedded systems market. Tighter system integration offers better reliability and mean time between failure, and SoCs offer more advanced functionality and computing power than microcontrollers. Applications include AI acceleration, embedded machine vision, data collection, telemetry, vector processing and ambient intelligence. Often embedded systems-on-chip target the internet of things, industrial internet of things and edge computing markets.\n\nMobile computing based SoCs typically bundle processors, memories, on-chip caches, wireless networking capabilities and often digital camera hardware and firmware. Some mobile computing SoCs include:\n\n\nSystems-on-chip are being applied to personal computers as of 2018. They are particularly applied to laptops and tablet PCs. Tablet and laptop manufacturers have learned lessons from embedded systems and smartphone markets including about reduced power consumption, better performance and reliability from tighter integration of hardware and firmware modules, and LTE and other wireless network communications integrated on chip (integrated network interface controllers).\n\nARM based: \n\n\nx86 based: \n\n\nAn SoC consists of hardware functional units, including microprocessors that run software code, as well as a communications subsystem to connect, control, direct and interface between these functional modules.\n\nAn SoC must have at least one processor core, but will typically have more. Processor cores can be a microcontroller, microprocessor (μP), digital signal processor (DSP) or application-specific instruction set processor (ASIP) core. ASIPs have instruction sets that are customized for an application domain and designed to be more efficient than general-purpose instructions for a specific type of workload. Multiprocessor SoCs have more than one processor core by definition. \n\nWhether single-core, multi-core or manycore, SoC processor cores typically use RISC instruction set architectures. RISC architectures are advantageous over CISC processors for systems-on-chip because they require less digital logic, and therefore less power and area on board, and in the embedded and mobile computing markets these are often highly constrained. In particular, SoC processor cores often use the ARM architecture because it is a soft processor specified as an IP core and more power efficient than x86.\n\nSystems-on-chip must have semiconductor memory blocks to perform their computation, as do microcontrollers and other embedded systems. Depending on the application, SoC memory may form a memory hierarchy and cache hierarchy. In the mobile computing market, this is common, but in many low-power embedded microcontrollers this is not necessary.\n\nMemory technologies for SoCs include read-only memory (ROM), random-access memory (RAM), electrically erasable programmable ROM (EEPROM) and flash memory. As in other computer systems, RAM can be subdivided into relatively faster but more expensive static RAM (SRAM) and the slower but cheaper dynamic RAM (DRAM). When a SoC has a cache hierarchy, SRAM will usually be used to implement processor registers and cores' L1 caches whereas DRAM will be used for lower levels of the cache hierarchy including main memory. \"Main memory\" may be specific to a single processor (which can be multi-core) when the SoC has multiple processors, in which case it is distributed memory and must be sent via on-chip to be accessed by a different processor. For further discussion of multi-processing memory issues, see cache coherence and memory latency.\n\nSoCs include external interfaces, typically for communication protocols. These are often based upon industry standards such as USB, FireWire, Ethernet, USART, SPI, HDMI, I²C, etc. These interfaces will differ according to the intended application. Wireless networking protocols such as Wi-Fi, Bluetooth, 6LoWPAN and near-field communication may also be supported.\n\nWhen needed, SoCs include analog interfaces including analog-to-digital and digital-to-analog converters, often for signal processing. These may be able to interface with different types of sensors or actuators, including smart transducers. They may interface with application-specific modules or shields. Or they may be internal to the SoC, such as if an analog sensor is built in to the SoC and its readings must be converted to digital signals for mathematical processing.\n\nDigital signal processor (DSP) cores are often included on systems-on-chip. They perform signal processing operations in systems-on-chip for sensors, actuators, data collection, data analysis and multimedia processing. DSP cores typically feature very long instruction word (VLIW) and single instruction, multiple data (SIMD) instruction set architectures, and are therefore highly amenable to exploiting instruction-level parallelism through parallel processing and superscalar execution. DSP cores most often feature application-specific instructions, and as such are typically application-specific instruction-set processors (ASIP). Such application-specific instructions correspond to dedicated hardware functional units that compute those instructions.\n\nTypical DSP instructions include multiply-accumulate, Fast Fourier transform, fused multiply-add, and convolutions.\n\nAs with other computer systems, SoCs require timing sources to generate clock signals, control execution of SoC functions and provide time context to signal processing applications of the SoC, if needed. Popular time sources are crystal oscillators and phase-locked loops. \n\nSystem-on-chip peripherals including counter-timers, real-time timers and power-on reset generators. SoCs also include voltage regulators and power management circuits.\n\nSystems-on-chip comprise many execution units. These units must often send data and instructions back and forth. Because of this, all but the most trivial SoCs require communications subsystems. Originally, as with other microcomputer technologies, data bus architectures were used, but recently designs based on sparse intercommunication networks known as networks-on-chip (NoC) have risen to prominence and are forecast to overtake bus architectures for SoC design in the near future.\n\nHistorically, a shared global computer bus typically connected the different components, also called \"blocks\" of the System-on-Chip. A very common bus for system-on-chip communications is ARM's royalty-free Advanced Microcontroller Bus Architecture (AMBA) standard. \n\nDirect memory access controllers route data directly between external interfaces and SoC memory, bypassing the CPU or control unit, thereby increasing the data throughput of the system-on-chip. This is similar to some device drivers of peripherals on component-based multi-chip module PC architectures. \n\nComputer buses are limited in scalability, supporting only up to tens of cores (multicore) on a single chip. Wire delay is not scalable due to continued miniaturization, system performance does not scale with the number of cores attached, the SoC's operating frequency must decrease with each additional core attached for power to be sustainable, and long wires consume large amounts of electrical power. These challenges are prohibitive to supporting manycore systems on chip.\n\nIn the late 2010s, a trend of systems-on-chip implementing communications subsystems in terms of a network-like topology instead of bus-based protocols has emerged. A trend towards more processor cores on SoCs has caused on-chip communication efficiency to become one of the key factors in determining the overall system performance and cost. This has led to the emergence of interconnection networks with router-based packet switching known as \"networks on chip\" (NoCs) to overcome the bottlenecks of bus-based networks. \n\nNetworks-on-chip have advantages including destination- and application-specific routing, greater power efficiency and reduced possibility of bus contention. Network-on-chip architectures take inspiration from networking protocols like TCP and the Internet protocol suite for on-chip communication, although they typically have fewer network layers. Optimal network-on-chip network architectures are an ongoing area of much research interest. NoC architectures range from traditional distributed computing network topologies such as torus, hypercube, meshes and tree networks to genetic algorithm scheduling to randomized algorithms such as random walks with branching and randomized time to live (TTL).\n\nMany SoC researchers consider NoC architectures to be the future of system-on-chip design because they have been shown to efficiently meet power and throughput needs of SoC designs. Current NoC architectures are two-dimensional. 2D IC design has limited floorplanning choices as the number of cores in SoCs increase, so as three-dimensional integrated circuits (3DICs) emerge, SoC designers look to build three-dimensional on-chip networks known as 3DNoCs.\n\nA system on chip consists of both the hardware, described in , and the software controlling the microcontroller, microprocessor or digital signal processor cores, peripherals and interfaces. The design flow for an SoC aims to develop this hardware and software at the same time, also known as architectural co-design. The design flow must also take into account optimizations () and constraints.\n\nMost SoCs are developed from pre-qualified hardware component IP core specifications for the hardware elements and execution units, collectively \"blocks\", described above, together with software device drivers that may control their operation. Of particular importance are the protocol stacks that drive industry-standard interfaces like USB. The hardware blocks are put together using computer-aided design tools, specifically electronic design automation tools; the software modules are integrated using a software integrated development environment.\n\nSystems-on-chip components are also often designed in high-level programming languages such as C++, MATLAB or SystemC and converted to RTL designs through high-level synthesis (HLS) tools such as C to HDL or flow to HDL. HLS products called \"algorithmic synthesis\" allow designers to use C++ to model and synthesize system, circuit, software and verification levels all in one high level language commonly known to computer engineers in a manner independent of time scales, which are typically specified in HDL. Other components can remain software and be compiled and embedded onto soft-core processors included in the SoC as modules in HDL as IP cores.\n\nOnce the architecture of the SoC has been defined, any new hardware elements are written in an abstract hardware description language termed register transfer level (RTL) which defines the circuit behavior, or synthesized into RTL from a high level language through high-level synthesis. These elements are connected together in a hardware description language to create the full SoC design. The logic specified to connect these components and convert between possibly different interfaces provided by different vendors is called glue logic.\n\nChips are verified for logical correctness before being sent to a semiconductor foundry. This process is called functional verification and it accounts for a significant portion of the time and energy expended in the chip design life cycle, often quoted as 70%. With the growing complexity of chips, hardware verification languages like SystemVerilog, SystemC, e, and OpenVera are being used. Bugs found in the verification stage are reported to the designer.\n\nTraditionally, engineers have employed simulation acceleration, emulation or prototyping on reprogrammable hardware to verify and debug hardware and software for SoC designs prior to the finalization of the design, known as tape-out. Field-programmable gate arrays (FPGAs) are favored for prototyping systems-on-chip because FPGA prototypes are reprogrammable, allow debugging and are more flexible than application-specific integrated circuits (ASICs). \n\nWith high capacity and fast compilation time, simulation acceleration and emulation are powerful technologies that provide wide visibility into systems. Both technologies, however, operate slowly, on the order of MHz, which may be significantly slower – up to 100 times slower – than the SoC's operating frequency. Acceleration and emulation boxes are also very large and expensive at over US$1 million. \n\nFPGA prototypes, in contrast, use FPGAs directly to enable engineers to validate and test at, or close to, a system’s full operating frequency with real-world stimuli. Tools such as Certus are used to insert probes in the FPGA RTL that make signals available for observation. This is used to debug hardware, firmware and software interactions across multiple FPGAs with capabilities similar to a logic analyzer.\n\nIn parallel, the hardware elements are grouped and passed through a process of logic synthesis, during which performance constraints, such as operational frequency and expected signal delays, are applied. This generates an output known as a netlist describing the design as a physical circuit and its interconnections. These netlists are combined with the glue logic connecting the components to produce the schematic description of the SoC as a circuit which can be printed onto a chip. This process is known as place and route and precedes tape-out in the event that the SoCs are produced as application-specific integrated circuits (ASIC).\n\nSystems-on-chip must optimize power use, area on die, communication, positioning for locality between modular units and other factors. Optimization is necessarily a design goal of systems-on-chip. If optimization was not necessary, the engineers would use a multi-chip module architecture without accounting for the area utilization, power consumption or performance of the system to the same extent. \n\nCommon optimization targets for system-on-chip designs follow, with explanations of each. In general, optimizing any of these quantities may be a hard combinatorial optimization problem, and can indeed be NP-hard fairly easily. Therefore, sophisticated optimization algorithms are often required and it may be practical to use approximation algorithms or heuristics in some cases. Additionally, most SoC designs contain multiple variables to optimize simultaneously, so Pareto efficient solutions are sought after in SoC design. Oftentimes the goals of optimizing some of these quantities are directly at odds, further adding complexity to design optimization of systems-on-chip and introducing trade-offs in system design.\n\nFor broader coverage of trade-offs and requirements analysis, see requirements engineering.\n\nSystems-on-chip are optimized to minimize the electrical power used to perform the SoC's functions. Most SoCs must use low power. SoC systems often require long battery life (such as smartphones), can potentially spending months or years without a power source needing to maintain autonomous function, and often are limited in power use by a high number of embedded SoCs being networked together in an area. Additionally, energy costs can be high and conserving energy will reduce the total cost of ownership of the SoC. Finally, waste heat from high energy consumption can damage other circuit components if too much heat is dissipated, giving another pragmatic reason to conserve energy. The amount of energy used in a circuit is the integral of power consumed with respect to time, and the average rate of power consumption is the product of current by voltage. Equivalently, by Ohm's law, power is current squared times resistance or voltage squared divided by resistance: \n\nformula_1Systems-on-chip are frequently embedded in portable devices such as smartphones, GPS navigation devices, digital watches (including smartwatches) and netbooks. Customers want long battery lives for mobile computing devices, another reason that power consumption must be minimized in systems-on-chip. Multimedia applications are often executed on these devices, including video games, video streaming, image processing; all of which have grown in computational complexity in recent years with user demands and expectations for higher-quality multimedia. Computation is more demanding as expectations move towards 3D video at high resolution with multiple standards, so SoCs performing multimedia tasks must be computationally capable platform while being low power to run off a standard mobile battery.\n\nSoCs are optimized to maximize power efficiency in performance per watt: maximize the performance of the SoC given a budget of power usage. Many applications such as edge computing, distributed processing and ambient intelligence require a certain level of computational performance, but power is limited in most SoC environments. The ARM architecture has greater performance per watt than x86 in embedded systems, so it is preferred over x86 for most SoC applications requiring an embedded processor.\n\nSoC designs are optimized to minimize waste heat output on the chip. As with other integrated circuits, heat generated due to high power density are the bottleneck to further miniaturization of components. The power densities of high speed integrated circuits, particularly microprocessors and including SoCs, have become highly uneven. Too much waste heat can damage circuits and erode reliability of the circuit over time. High temperatures and thermal stress negatively impact reliability, stress migration, decreased mean time between failures, electromigration, wire bonding, metastability and other performance degradation of the SoC over time.\n\nIn particular, most SoCs are in a small physical area or volume and therefore the effects of waste heat are compounded because there is little room for it to diffuse out of the system. Because of high transistor counts on modern devices due to Moore's law, oftentimes a layout of sufficient throughput and high transistor density is physically realizable from fabrication processes but would result in unacceptably high amounts of heat in the circuit's volume. \n\nThese thermal effects force SoC and other chip designers to apply conservative design margins, creating less performant devices to mitigate the risk of catastrophic failure. Due to increased transistor densities as length scales get smaller, each process generation produces more heat output than the last. Compounding this problem, system-on-chip architectures are usually heterogeneous, creating spatially inhomogeneous heat fluxes, which are cannot be effectively mitigated by uniform passive cooling.\n\nSoCs are optimized to maximize computational and communications throughput.\n\nSoCs are optimized to minimize latency for some or all of their functions. This can be accomplished by laying out elements with proper proximity and locality to each-other to minimize the interconnection delays and maximize the speed at which data is communicated between modules, functional units and memories. In general, optimizing to minimize latency is an NP-complete problem equivalent to the boolean satisfiability problem.\n\nFor tasks running on processor cores, latency and throughput can be improved with . Some tasks run in application-specific hardware units, however, and even task scheduling may not be sufficient to optimize all software-based tasks to meet timing and throughput constraints.\n\nSystems on chip are modeled with standard hardware verification and validation techniques, but additional techniques are used to model and optimize SoC design alternatives to make the system optimal with respect to multiple-criteria decision analysis on the above optimization targets.\n\nTask scheduling is an important activity in any computer system with multiple processes or threads sharing a single processor core. It is important to reduce and increase for embedded software running on an SoC's . Not every important computing activity in a system-on-chip is performed in software running on on-chip processors, but scheduling can drastically improve performance of software-based tasks and other tasks involving shared resources.\n\nSoCs often schedule tasks according to network scheduling and randomized scheduling algorithms.\n\nHardware and software tasks are often pipelined in processor design. Pipelining is an important principle for speedup in computer architecture. They are frequently used in GPUs (graphics pipeline) and RISC processors (evolutions of the classic RISC pipeline), but are also applied to application-specific tasks such as digital signal processing and multimedia manipulations in the context of systems-on-chip.\n\nSystems-on-chip are often analyzed though probabilistic models, and Markov chains. For instance, Little's law allows SoC states and NoC buffers to be modeled as arrival processes and analyzed through Poisson random variables and Poisson processes.\n\nSoCs are often modeled with Markov chains, both discrete time and continuous time variants. Markov chain modeling allows asymptotic analysis of the system-on-chip's steady state distribution of power, heat, latency and other factors to allow design decisions to be optimized for the common case.\n\nThe netlists described above are used as the basis for the physical design (place and route) flow to convert the designers' intent into the design of the SoC. Throughout this conversion process, the design is analyzed with static timing modeling, simulation and other tools to ensure that it meets the specified operational parameters such as frequency, power consumption and dissipation, functional integrity (as described in the register transfer level code) and electrical integrity.\n\nWhen all known bugs have been rectified and these have been re-verified and all physical design checks are done, the physical design files describing each layer of the chip are sent to the foundry's mask shop where a full set of glass lithographic masks will be etched. These are sent to a wafer fabrication plant to create the SoC dice before packaging and testing.\n\nSoCs can be fabricated by several technologies, including:\n\n\nASICs consume less power and are faster than FPGAs but cannot be reprogrammed and are expensive to manufacture. FPGA designs are more suitable for lower volume designs, but after enough units of production ASICs reduce the total cost of ownership. \n\nSoC designs consume less power and have a lower cost and higher reliability than the multi-chip systems that they replace. With fewer packages in the system, assembly costs are reduced as well.\n\nHowever, like most very-large-scale integration (VLSI) designs, the total cost is higher for one large chip than for the same functionality distributed over several smaller chips, because of lower yields and higher non-recurring engineering costs.\n\nWhen it is not feasible to construct an SoC for a particular application, an alternative is a system in package (SiP) comprising a number of chips in a single package. When produced in large volumes, SoC is more cost-effective than SiP because its packaging is simpler. Another reasons SiP may be preferred is waste heat may be too high in a system-on-chip for a given purpose because functional components are too close together, and in an SiP heat will dissipate better from different functional modules being physically further apart.\n\nSoC research and development often compares many options. Benchmarks, such as COSMIC, are developed to help such evaluations.\n\n\n\n"}
{"id": "24015660", "url": "https://en.wikipedia.org/wiki?curid=24015660", "title": "TechRadar", "text": "TechRadar\n\nTechRadar is an online publication focused on technology, with editorial teams in the US, UK, Australia and India. It provides news and reviews of tech products and first launched in 2008.\n\nTechRadar is owned by Future plc, the sixth-largest publisher in the United Kingdom. In Q4 2017, TechRadar entered the top 100 of Similarweb's US Media Publications Rankings as the 93rd biggest media site in the United States.\n\n, SimilarWeb reported TechRadar to be the 10th most popular technology site globally with over 73 million readers per month.\n"}
{"id": "28179060", "url": "https://en.wikipedia.org/wiki?curid=28179060", "title": "Thin Film Electronics ASA", "text": "Thin Film Electronics ASA\n\nThin Film Electronics ASA (Oslo Stock Exchange : THIN) is a Norwegian printed electronics company, headquartered in Oslo with its main R&D offices in Linköping, Sweden and San Jose, California, United States.\nThin Film Electronics ASA (\"Thinfilm\") maintains an NFC Innovation Center at its San Jose, California location, and also has sales offices in San Francisco, Scandinavia, United Kingdom, Shanghai, Singapore, and Hong Kong. Thinfilm produces rewriteable non-volatile memories based on ferroelectric polymers using roll-to-roll printing and labels based on Near Field Communication (NFC) technology.\n\nThinfilm offers several products that leverage printed electronics technology, including NFC SpeedTap(tm), NFC OpenSense(tm), EAS (electronic article surveillance) tags, and temperature-sensing smart labels. In February 2017, Thinfilm launched its CNECT Software Portal, a multi-tenant cloud-based platform that integrates with its NFC SpeedTap and OpenSense tags.\n\nThinfilm has been developing memories based on polymer materials since 1994; first as part of Opticom ASA and then as an independent company. For the first ten years, the focus was on hybrid memory devices with polymer-based memory and silicon-based control circuitry, as developed jointly with Intel. From 2006, Thinfilm has concentrated its efforts on printed electronics.\n\nThinfilm successfully demonstrated roll-to-roll printed organic memory in 2009, and was awarded IDTechEx Technical Development Manufacturing Award the same year.\n\nIn September 2012, Thinfilm was selected by web 2.0 blog GigaOM as one of its Top 15 Mobile companies that are changing or could potentially change the mobile landscape in a significant way.\n\nIn January 2014 Thinfilm acquired the assets of Kovio and opened the Thinfilm NFC Innovation Center in San Jose.\nThinfilm's printed NFC products feature \"PDPS\" (printed dopant polysilicon) technology based on a hybrid manufacturing process that leverages print methods in key process steps. By combining conventional fabrication techniques with functional printing, the process is optimized for high throughput, low-cost production without sacrificing device performance. The high-mobility silicon inks used in the manufacturing process ensure critical circuit performance at the RF frequencies used in Thinfilm's NFC products.\n\nFor Thinfilm Memory(tm), the ferroelectric polymer is sandwiched between two sets of electrodes in a passive matrix. Each crossing of metal lines is a ferroelectric capacitor and defines a memory cell. This gives a non-volatile memory comparable to ferroelectric RAM technologies and offer the same functionality as flash memory.\n\nThinfilm's patented passive matrix dispenses with the need of active circuitry within the memory cell. It allows the memory to be separate from the read/write electronics enabling stand alone application without integration with printed logic. The passive array memory architecture also enables high density memories as well as the possibility to stack memory layers on top of each other.\n\nThe printed memory technology was licensed to Xerox in December 2014 and marketed as of June 2016 as Xerox Printed Memory.\nIn October 2013, Thinfilm produced the first ever electronically printed stand-alone sensor system. The sensor was built in the form of a temperature tracking label and is expected to go into production in Q4 of 2015.\n\nIn February 2015, Thinfilm demonstrated a connected \"smart bottle\" at the Mobile World Congress in Barcelona. This announcement coincided with the launch of NFC OpenSense(tm), Thinfilm's patent-pending printed sensor tags that provide smartphone-centric readability before and after a product is opened.\n\nThinfilm is working with PARC, a subsidiary of Xerox, to combine its memory technology with PARC's printed transistor technology to enable fully printed memory systems. \nIn October 2011, Thinfilm together with PARC announced a working prototype of the world's first printed non-volatile memory device addressed with complementary organic circuits, the organic equivalent of CMOS circuitry.\n\nThinfilm and PARC announced in June 2012 that they will jointly manufacture a printed temperature sensor tag using Thinfilm's Addressable Memory technology. Both parties are also working together to develop electronic displays that would combine printed sensors, memory modules and batteries.\n\nOther printed systems currently in development with Thinfilm and its partners will be used in food manufacturing to monitor temperature of temperature-sensitive fresh produce. The first temperature sensor packaging project to be announced by Thinfilm is a partnership with U.S. packaging giant Bemis Company, the agreement will lead to the development of a flexible sensing platform for the packaging market.\n\nIn December 2012, Thinfilm announced that it has entered into a commercial agreement with the American toy and board game company Hasbro, to supply printed electronic components for their range of products.\nIn March/April 2014, Thinfilm announced key partnerships with go-to-market partners Temptime and Paksense to bring its temperature sensor smart label to the pharmaceutical and perishable foods industries, respectively.\n\nIn January 2015, the company announced a partnership with Xerox, which licensed Thinfilm's technology to manufacture Thinfilm Memory(tm) labels. Xerox is modifying a plant in Webster, NY to manage production. \n\nThinfilm Electronics has won a number of awards for innovation and technical development since first demonstrating roll-to-roll printing of electronics in 2009. In 2010 Frost & Sullivan awarded the New Product Innovation Prize in Printed Electronic Memories to Thinfilm for its unique non-volatile, printable, polymer-based memory products.\n\nIn 2012 Thinfilm Electronics was awarded both the IDTechEx Product Development Award and the FlexTech Alliance Innovation Award for its Addressable Memory technology built in association with PARC. Also in late 2012, Thinfilm was named as a runner-up in the Wall Street Journal Technology's Innovation Award.\n\nIn October 2012 it was announced that the company had won the 2012 World Technology Award for Visionary Contribution to Materials Science and Technology in the development of printed smart tags for the Internet of Things.\n"}
{"id": "2590900", "url": "https://en.wikipedia.org/wiki?curid=2590900", "title": "Treadwheel", "text": "Treadwheel\n\nA treadwheel, or treadmill, is a form of engine typically powered by humans. It may resemble a water wheel in appearance, and can be worked either by a human treading paddles set into its circumference (treadmill), or by a human or animal standing inside it (treadwheel). These devices are no longer used for power or punishment, and the term \"treadmill\" has come to mean an exercise machine for running or walking in place.\n\nUses of treadwheels included raising water, to power cranes, or grind grain. They were used extensively in the Greek and Roman world, such as in the reverse overshot water-wheel used for dewatering purposes. They were widely used in the Middle ages to lift the stones in the soaring Gothic cathedrals. There is a literary reference to one in 1225, and one treadwheel crane survives at Chesterfield, Derbyshire and is housed in the Museum. It has been dated to the early 14th century and was housed in the top of the church tower until its removal in 1947. They were used extensively in the Renaissance famously by Brunelleschi during the construction of Florence cathedral.\n\nPenal treadmills were used in prisons during the early-Victorian period in the UK as a form of punishment. According to \"The Times\" in 1827, and reprinted in William Hone's \"Table-Book\" in 1838, the amount prisoners walked per day on average varied, from the equivalent of 6,600 vertical feet at Lewes to as much as 17,000 vertical feet in ten hours during the summertime at Warwick gaol.In 1902, the British government banned the use of the treadwheel as a form of punishment. \n\n\nThe Use of Treadmills in Pre-Industrial Times\n\n"}
{"id": "860924", "url": "https://en.wikipedia.org/wiki?curid=860924", "title": "Wind farm", "text": "Wind farm\n\nA wind farm or wind park is a group of wind turbines in the same location used to produce electricity. A large wind farm may consist of several hundred individual wind turbines and cover an extended area of hundreds of square miles, but the land between the turbines may be used for agricultural or other purposes. A wind farm can also be located offshore.\n\nMany of the largest operational onshore wind farms are located in China, India, and the United States. For example, the largest wind farm in the world, Gansu Wind Farm in China has a capacity of over 6,000 MW as of 2012, with a goal of 20,000 MW by 2020. As of September 2018, the 659 MW Walney Wind Farm in the UK is the largest offshore wind farm in the world.\n\nIndividual wind turbine designs continue to increase in power, resulting in fewer turbines being needed for the same total output. See list of most powerful wind turbines.\n\nThe location is critical to the success of a wind farm. Conditions contributing to a successful wind farm location include: wind conditions, access to electric transmission, physical access, and local electric prices.\n\nThe faster the average windspeed the more electricity the wind turbine will generate, so faster winds are economically better for wind farm developers. The balancing factor is that strong gusts and high turbulence require stronger more expensive turbines, otherwise they risk damage. The ideal wind conditions would be strong steady winds with low turbulence coming from a single direction.\n\nUsually sites are screened on the basis of a wind atlas, and validated with wind measurements. Meteorological wind data alone is usually not sufficient for accurate siting of a large wind power project. Collection of site specific data for wind speed and direction is crucial to determining site potential in order to finance the project. Local winds are often monitored for a year or more, and detailed wind maps are constructed before wind generators are installed.\nThe wind blows faster at higher altitudes because of the reduced influence of drag. The increase in velocity with altitude is most dramatic near the surface and is affected by topography, surface roughness, and upwind obstacles such as trees or buildings.\n\nHow closely to space the turbines together is a major factor in wind farm design. The closer the turbines are together the more the upwind turbines block wind from their neighbors. However spacing turbines far apart increases the costs of roads and cables, and raises the amount of land needed to install a specific capacity of turbines. As a result of these factors, turbine spacing varies by site. Generally speaking manufacturers require 3.5 times the rotor diameter of the turbine between turbines as a minimum. Closer spacing is possible depending on the turbine model, the conditions at the site, and how the site will be operated.\n\nThe world's first wind farm was 0.6 MW, consisting of 20 wind turbines rated at 30 kilowatts each, installed on the shoulder of Crotched Mountain in southern New Hampshire in December 1980.\n\nOnshore turbine installations in hilly or mountainous regions tend to be on ridges generally three kilometres or more inland from the nearest shoreline. This is done to exploit the topographic acceleration as the wind accelerates over a ridge. The additional wind speeds gained in this way can increase energy produced because more wind goes through the turbines. The exact position of each turbine matters, because a difference of 30m could potentially double output. This careful placement is referred to as 'micro-siting'.\n\nEurope is the leader in offshore wind energy, with the first offshore wind farm (Vindeby) being installed in Denmark in 1991. As of 2010, there are 39 offshore wind farms in waters off Belgium, Denmark, Finland, Germany, Ireland, the Netherlands, Norway, Sweden and the United Kingdom, with a combined operating capacity of 2,396 MW. More than 100 GW (or 100,000 MW) of offshore projects are proposed or under development in Europe. The European Wind Energy Association has set a target of 40 GW installed by 2020 and 150 GW by 2030.\n\n, The Walney Wind Farm in the United Kingdom is the largest offshore wind farm in the world at 659 MW, followed by the London Array (630 MW) also in the UK.\n\nOffshore wind turbines are less obtrusive than turbines on land, as their apparent size and noise is mitigated by distance. Because water has less surface roughness than land (especially deeper water), the average wind speed is usually considerably higher over open water. Capacity factors (utilisation rates) are considerably higher than for onshore locations.\n\nThe province of Ontario in Canada is pursuing several proposed locations in the Great Lakes, including the suspended Trillium Power Wind 1 approximately 20 km from shore and over 400 MW in size. Other Canadian projects include one on the Pacific west coast.\n\nIn 2010, there were no offshore wind farms in the United States, but projects were under development in wind-rich areas of the East Coast, Great Lakes, and Pacific coast; and in late 2016 the Block Island Wind Farm was commissioned.\n\nInstallation and service / maintenance of off-shore wind farms are a specific challenge for technology and economic operation of a wind farm. , there are 20 jackup vessels for lifting components, but few can lift sizes above 5MW. Service vessels have to be operated nearly 24/7 (availability higher than 80% of time) to get sufficient amortisation from the wind turbines. Therefore, special fast service vehicles for installation (like Wind Turbine Shuttle) as well as for maintenance (including heave compensation and heave compensated working platforms to allow the service staff to enter the wind turbine also at difficult weather conditions) are required. So-called inertial and optical based Ship Stabilization and Motion Control systems (iSSMC) are used for that.\n\nThere exist also some wind farms which were mainly built for testing wind turbines. In such wind farms, there is usually from each type to be tested only a single wind turbine. Such farms have usually at least one meteorological tower. An example of an experimental wind farm is Østerild Wind Turbine Test Field.\n\nFor some time, airborne wind farms have been discussed. An airborne wind farm is a group of airborne wind energy systems near to each other, connected to the grid in the same point.\n\nIn just five years, China leapfrogged the rest of the world in wind energy production, going from 2,599 MW of capacity in 2006 to 62,733 MW at the end of 2011. However, the rapid growth outpaced China's infrastructure and new construction slowed significantly in 2012.\n\nAt the end of 2009, wind power in China accounted for 25.1 gigawatts (GW) of electricity generating capacity, and China has identified wind power as a key growth component of the country's economy. With its large land mass and long coastline, China has exceptional wind resources. Researchers from Harvard and Tsinghua University have found that China could meet all of their electricity demands from wind power by 2030.\n\nBy the end of 2008, at least 15 Chinese companies were commercially producing wind turbines and several dozen more were producing components. Turbine sizes of 1.5 MW to 3 MW became common. Leading wind power companies in China were Goldwind, Dongfang Electric, and Sinovel along with most major foreign wind turbine manufacturers. China also increased production of small-scale wind turbines to about 80,000 turbines (80 MW) in 2008. Through all these developments, the Chinese wind industry appeared unaffected by the global financial crisis, according to industry observers.\n\nAccording to the Global Wind Energy Council, the development of wind energy in China, in terms of scale and rhythm, is absolutely unparalleled in the world. The National People's Congress permanent committee passed a law that requires the Chinese energy companies to purchase all the electricity produced by the renewable energy sector.\n\nThe European Union has a total installed wind capacity of 93,957 MW. Germany has the third-largest capacity in the world (after China and the United States) with an installed capacity was 29,060 MW at the end of 2011, and Spain has 21,674 MW. Italy and France each had between 6,000 and 7,000 MW. By January 2014, the UK installed capacity was 10,495 MW. But energy production can be different from capacity – in 2010, Spain had the highest European wind power production with 43 TWh compared to Germany's 35 TWh.\n\nEurope's largest windfarm is the 'London Array', an off-shore wind farm in the Thames Estuary in the United Kingdom, with a current capacity of 630 MW (the world's largest off-shore wind farm). Other large wind farms in Europe include Fântânele-Cogealac Wind Farm near Constanța, Romania with 600 MW capacity, and Whitelee Wind Farm near Glasgow, Scotland which has a total capacity of 539 MW.\n\nAn important limiting factor of wind power is variable power generated by wind farms. In most locations the wind blows only part of the time, which means that there has to be back-up capacity of conventional generating capacity to cover periods that the wind is not blowing. To address this issue it has been proposed to create a \"supergrid\" to connect national grids together across western Europe, ranging from Denmark across the southern North Sea to England and the Celtic Sea to Ireland, and further south to France and Spain especially in Higueruela which was for some time the biggest wind farm in the world. The idea is that by the time a low pressure area has moved away from Denmark to the Baltic Sea the next low appears off the coast of Ireland. Therefore, while it is true that the wind is not blowing everywhere all of the time, it will always be blowing somewhere.\n\nIndia has the fifth largest installed wind power capacity in the world. As of 31 March 2014, the installed capacity of wind power was 21136.3 MW mainly spread across Tamil Nadu state (7253 MW). Wind power accounts nearly 8.5% of India's total installed power generation capacity, and it generates 1.6% of the country's power.\n\nThe 117 MW Tafila Wind Farm in Jordan was inaugurated in December 2015, and is the first large scale wind farm project in the region.\n\nMorocco has undertaken a vast wind energy program, to support the development of renewable energy and energy efficiency in the country. The Moroccan Integrated Wind Energy Project, spanning over a period of 10 years with a total investment estimated at $3.25 billion, will enable the country to bring the installed capacity, from wind energy, from 280 MW in 2010 to 2000 MW in 2020.\n\nPakistan has wind corridors in Jhimpir, Gharo and Keti Bundar in Sindh province and is currently developing wind power plants in Jhimpir and Mirpur Sakro (District Thatta). The government of Pakistan decided to develop wind power energy sources due to problems supplying energy to the southern coastal regions of Sindh and Balochistan.\nThe Zorlu Energy Putin Power Plant is the first wind power plant in Pakistan. The wind farm is being developed in Jhimpir, by Zorlu Energy Pakistan the local subsidiary of a Turkish company. The total cost of project is $136 million.[3] Completed in 2012, it has a total capacity of around 56MW.\nFauji Fertilizer Company Energy Limited, has build a 49.5 MW wind Energy Farm at Jhimpir. Contract of supply of mechanical design was awarded to Nordex and Descon Engineering Limited. Nordex a German wind turbine manufacturer. In the end of 2011 49.6 MW will be completed.Pakistani Govt. also has issued LOI of 100 MW Wind power plant to FFCEL. Pakistani Govt. has plans to achieve electric power up to 2500 MW by the end of 2015 from wind energy to bring down energy shortage.\n\nCurrently four wind farms are operational (Fauji Fertilizer 49.5 MW (subsidiary of Fauji Foundation), Three Gorges 49.5 MW, Zorlu Energy Pakistan 56 MW, Sapphire Wind Power Co Ltd 52.6 MW) and six are under construction phase ( Master Wind Energy Ltd 52.6 MW, Sachal Energy Development Ltd 49.5 MW, Yunus Energy Ltd 49.5 MW, Gul Energy 49.5 MW, Metro Energy 49.5 MW, Tapal Energy ) and expected to achieve COD in 2017.\n\nIn Gharo wind corridor, two wind farms (Foundation Energy 1 & II each 49.5 MW) are operational while two wind farms Tenaga Generasi Ltd 49.5 MW and HydroChina Dawood Power Pvt Ltd 49.5 are under construction and expected to achieve COD in 2017.\n\nAccording to a USAID report, Pakistan has the potential of producing 150,000 megawatts of wind energy, of which only the Sindh corridor can produce 40,000 megawatts.\n\nThe Philippines has the first windfarm in Southeast Asia. Located Northern part of the countries' biggest island Luzon, alongside the seashore of Bangui, Ilocos Norte.\n\nThe wind farm uses 20 units of 70-metre (230 ft) high Vestas V82 1.65 MW wind turbines, arranged on a single row stretching along a nine-kilometer shoreline off Bangui Bay, facing the West Philippine Sea.\n\nPhase I of the NorthWind power project in Bangui Bay consists of 15 wind turbines, each capable of producing electricity up to a maximum capacity of 1.65 MW, for a total of 24.75 MW. The 15 on-shore turbines are spaced 326 metres (1,070 ft) apart, each 70 metres (230 ft) high, with 41 metres (135 ft) long blades, with a rotor diameter of 82 metres (269 ft) and a wind swept area of 5,281 square metres (56,840 sq ft). Phase II, was completed on August 2008, and added 5 more wind turbines with the same capacity, and brought the total capacity to 33 MW. All 20 turbines describes a graceful arc reflecting the shoreline of Bangui Bay, facing the West Philippine Sea.\n\nAdjacent municipalities of Burgos and Pagudpud followed with 50 and 27 wind turbines with a capacity of 3 MW each for a Total of 150 MW and 81 MW respectively.\n\nTwo other wind farms were built outside of Ilocos Norte, the Pililla Wind Farm in Rizal and the Mindoro Wind Farm near Puerto Galera in Oriental Mindoro.\n\nSri Lanka has received funding from the Asian Development Bank amounting to $300 million to invest in renewable energies. From this funding as well as $80 million from the Sri Lankan Government and $60 million from France’s Agence Française de Développement, Sri Lanka is building two 100MW wind farms from 2017 due to be completed by late 2020 in Northern Sri Lanka.\n\n \nAs of September 2015 a number of sizable wind farms have been constructed in South Africa mostly in the Western Cape region. These include the 100 MW Sere Wind Farm and the 138 MW Gouda Wind Facility.\n\nMost future wind farms in South Africa are earmarked for locations along the Eastern Cape coastline. Eskom has constructed one small scale prototype windfarm at Klipheuwel in the Western Cape and another demonstrator site is near Darling with phase 1 completed. The first commercial wind farm, Coega Wind Farm in Port Elisabeth, was developed by the Belgian company Electrawinds.\n\nU.S. wind power installed capacity in 2012 exceeded 51,630 MW and supplies 3% of the nation's electricity.\n\nNew installations place the U.S. on a trajectory to generate 20% of the nation’s electricity by 2030 from wind energy. Growth in 2008 channeled some $17 billion into the economy, positioning wind power as one of the leading sources of new power generation in the country, along with natural gas. Wind projects completed in 2008 accounted for about 42% of the entire new power-producing capacity added in the U.S. during the year.\n\nAt the end of 2008, about 85,000 people were employed in the U.S. wind industry, and GE Energy was the largest domestic wind turbine manufacturer. Wind projects boosted local tax bases and revitalized the economy of rural communities by providing a steady income stream to farmers with wind turbines on their land. Wind power in the U.S. provides enough electricity to power the equivalent of nearly 9 million homes, avoiding the emissions of 57 million tons of carbon each year and reducing expected carbon emissions from the electricity sector by 2.5%.\n\nTexas, with 10,929 MW of capacity, has the most installed wind power capacity of any U.S. state, followed by California with 4,570 MW and Iowa with 4,536 MW. The Alta Wind Energy Center (1,020 MW) in California is the nation's largest wind farm in terms of capacity. Altamont Pass Wind Farm is the largest wind farm in the U.S. in terms of the number of individual turbines.\n\nPublic perception is that renewable energies such as wind, solar, biomass and geothermal are having a significant positive impact on global warming. All of these sources combined only supplied 1.3% of global energy in 2013 as 8 billion tonnes of coal was burned annually.\n\nOne of the biggest factors inhibiting wind farm construction is human opposition. A study has shown \"turbine placement close to residents may heighten their uncertainty and concern of the wind turbines and overshadow any positive inclinations towards the development.\"\n\nWind farm development is affected by the emphasis being primarily placed on the domain of landscape assessment and environmental impact when seeking farm sites. The viability and efficiency of the wind farm are barely touched upon, instead falling to the developer. For example, Sturge et al. of the University of Sheffield wrote that in many countries where wind energy is becoming popular, engineering aspects, specifically energy yield are not being taken into consideration, either by the public or in the process of planning consent for wind farm development. As energy is the main purpose of wind farms, a lack of attention given to the subject could be detrimental to the general acceptance of wind farms.\n\nCompared to the environmental impact of traditional energy sources, the environmental impact of wind power is relatively minor. Wind power consumes no fuel, and emits no air pollution, unlike fossil fuel power sources. The energy consumed to manufacture and transport the materials used to build a wind power plant is equal to the new energy produced by the plant within a few months. While a wind farm may cover a large area of land, many land uses such as agriculture are compatible, with only small areas of turbine foundations and infrastructure made unavailable for use.\n\nThere are reports of bird and bat mortality at wind turbines as there are around other artificial structures. The scale of the ecological impact may or may not be significant, depending on specific circumstances. The estimated number of bird deaths caused by wind turbines in the United States is between 140,000 and 328,000, whereas deaths caused by domestic cats in the United States are estimated to be between 1.3 and 4.0 billion birds each year and over 100 million birds are killed in the United States each year by impact with windows.\nPrevention and mitigation of wildlife fatalities, and protection of peat bogs, affect the siting and operation of wind turbines.\n\nThere have been multiple scientific, peer-reviewed studies into wind farm noise, which have concluded that infrasound from wind farms is not a hazard to human health and there is no verifiable evidence for 'Wind Turbine Syndrome' causing Vibroacoustic disease, although some suggest further research might still be useful.\n\nA 2007 report by the U.S. National Research Council noted that noise produced by wind turbines is generally not a major concern for humans beyond a half-mile or so. Low-frequency vibration and its effects on humans are not well understood and sensitivity to such vibration resulting from wind-turbine noise is highly variable among humans. There are opposing views on this subject, and more research needs to be done on the effects of low-frequency noise on humans.\n\nIn a 2009 report about \"Rural Wind Farms\", a Standing Committee of the Parliament of New South Wales, Australia, recommended a minimum setback of two kilometres between wind turbines and neighbouring houses (which can be waived by the affected neighbour) as a precautionary approach.\n\nA 2014 paper suggests that the 'Wind Turbine Syndrome' is mainly caused by the nocebo effect and other psychological mechanisms. Australian science magazine Cosmos states that although the symptoms are real for those who suffer from the condition, doctors need to first eliminate known causes (such as pre-existing cancers or thyroid disease) before reaching definitive conclusions with the caveat that new technologies often bring new, previously unknown health risks.\n\nUtility-scale wind farms must have access to transmission lines to transport energy. The wind farm developer may be obliged to install extra equipment or control systems in the wind farm to meet the technical standards set by the operator of a transmission line. The company or person that develops the wind farm can then sell the power on the grid through the transmission lines and ultimately chooses whether to hold on to the rights or sell the farm or parts of it to big business like GE, for example.\n\nWind farms can interfere with ground radar systems used for military, weather and air traffic control. The large, rapidly moving blades of the turbines can return signals to the radar that can be mistaken as an aircraft or weather pattern.\nActual aircraft and weather patterns around wind farms can be accurately detected, as there is no fundamental physical constraint preventing that. But aging radar infrastructure is significantly challenged with the task. The US military is using wind turbines on some bases, including Barstow near the radar test facility.\n\nThe level of interference is a function of the signal processors used within the radar, the speed of the aircraft and the relative orientation of wind turbines/aircraft with respect to the radar. An aircraft flying above the wind farm's turning blades could become impossible to detect because the blade tips can be moving at nearly aircraft velocity. Studies are currently being performed to determine the level of this interference and will be used in future site planning. Issues include masking (shadowing), clutter (noise), and signal alteration. Radar issues have stalled as much as 10,000 MW of projects in USA.\n\nSome very long range radars are not affected by wind farms.\n\nPermanent problem solving include a \"non-initiation window\" to hide the turbines while still tracking aircraft over the wind farm, and a similar method mitigates the false returns.\nEngland's Newcastle Airport is using a short-term mitigation; to \"blank\" the turbines on the radar map with a software patch. Wind turbine blades using stealth technology are being developed to mitigate radar reflection problems for aviation. As well as stealth windfarms, the future development of infill radar systems could filter out the turbine interference.\n\nA mobile radar system, the Lockheed Martin TPS-77, can distinguish between aircraft and wind turbines, and more than 170 TPS-77 radars are in use around the world. \n\nThere are also reports of negative effects on radio and television reception in wind farm communities. Potential solutions include predictive interference modelling as a component of site selection.\n\nWind turbines can often cause terrestrial television interference when the direct path between television transmitter and receiver is blocked by terrain. Interference effects become significant when the reflected signal from the turbine blades approaches the strength of the direct unreflected signal. Reflected signals from the turbine blades can cause loss of picture, pixellation and disrupted sound. There is a common misunderstanding that digital TV signals will not be affected by turbines — in practice they are.\n\nA 2010 study found that in the immediate vicinity of wind farms, the climate is cooler during the day and slightly warmer during the night than the surrounding areas due to the turbulence generated by the blades.\n\nIn another study an analysis carried out on corn and soybean crops in the central areas of the United States noted that the microclimate generated by wind turbines improves crops as it prevents the late spring and early autumn frosts, and also reduces the action of pathogenic fungi that grow on the leaves. Even at the height of summer heat, the lowering of 2.5–3 degrees above the crops due to turbulence caused by the blades, can make a difference for the cultivation of corn.\n\n\n"}
{"id": "399590", "url": "https://en.wikipedia.org/wiki?curid=399590", "title": "Yamaha DSP-1", "text": "Yamaha DSP-1\n\nThe Yamaha DSP-1 is a processor of early home theater surround sound equipment, produced in 1985. The DSP-1 (referred to by Yamaha as a Digital Soundfield Processor) allowed owners to synthesize up to 6-channels of surround sound from 2 channel stereo sound via a complex digital signal processor (DSP). Much like today's home theater receivers the DSP-1 offered sixteen \"sound fields\" created through the DSP including a jazz club, a cathedral, a concert hall, and a stadium. However, unlike today's integrated amps and receivers, these soundfield modes were highly editable, allowing the owner to customize the effect to his or her own personal taste. The DSP-1 also included an analog Dolby Surround decoder as well as other effects such as real-time echo and pitch change. \n\nMost of the DSP-1's controls are on the unit's remote control. The reason, as mentioned in the manual, being that it was felt that adjustments should be done at the listening position. This can make it difficult for collectors to find a complete functioning unit, although there is at least one provider of aftermarket remote controls with duplicate programming for the DSP-1 if needed. In Dolby Surround mode, only 4 channels are active, with just the front main channels and rear surround channels operating, the forward surround channels being muted. \n\nYamaha has kept the DSP prefix for many of its home DSP and audio amp/receiver products.\n\n\n"}
