{"id": "2293875", "url": "https://en.wikipedia.org/wiki?curid=2293875", "title": "ACARS", "text": "ACARS\n\nIn aviation, ACARS (; an acronym for aircraft communications addressing and reporting system) is a digital datalink system for transmission of short messages between aircraft and ground stations via airband radio or satellite. The protocol was designed by ARINC and deployed in 1978, using the Telex format. More ACARS radio stations were added subsequently by SITA.\n\nPrior to the introduction of datalink in aviation, all communication between the aircraft and ground personnel was performed by the flight crew using voice communication, using either VHF or HF voice radios. In many cases, the voice-relayed information involved dedicated radio operators and digital messages sent to an airline teletype system or successor systems.\n\nFurther, the hourly rates for flight and cabin crew salaries depended on whether the aircraft was airborne or not, and if on the ground whether it was at the gate or not. The flight crews reported these times by voice to geographically dispersed radio operators. Airlines wanted to eliminate self-reported times to preclude inaccuracies, whether accidental or deliberate. Doing so also reduced the need for human radio operators to receive the reports.\n\nIn an effort to reduce crew workload and improve data integrity, the engineering department at ARINC introduced the ACARS system in July 1978, as essentially an automated time clock system. Teledyne Controls produced the avionics and the launch customer was Piedmont Airlines. The original expansion of the abbreviation was \"Arinc Communications Addressing and Reporting System\". Later, it was changed to \"Aircraft Communications, Addressing and Reporting System\". The original avionics standard was ARINC 597, which defined an ACARS Management Unit consisting of discrete inputs for the doors, parking brake and weight on wheels sensors to automatically determine the flight phase and generate and send as telex messages. It also contained a MSK modem, which was used to transmit the reports over existing VHF voice radios. Global standards for ACARS were prepared by the Airlines Electronic Engineering Committee (AEEC). The first day of ACARS operations saw about 4,000 transactions, but it did not experience widespread use by the major airlines until the 1980s.\n\nEarly ACARS systems were extended over the years to support aircraft with digital data bus interfaces, flight management systems, and printers.\n\nACARS as a term refers to the complete air and ground system, consisting of equipment on board, equipment on the ground, and a service provider.\n\nOn-board ACARS equipment consists of end systems with a router, which routes messages through the air-ground subnetwork.\n\nGround equipment is made up of a network of radio transceivers managed by a central site computer called AFEPS (Arinc Front End Processor System), which handles and routes messages. Generally, ground ACARS units are either government agencies such as the Federal Aviation Administration, an airline operations headquarters, or, for small airlines or general aviation, a third-party subscription service. Usually government agencies are responsible for clearances, while airline operations handle gate assignments, maintenance, and passenger needs.\n\nGround system provision is the responsibility of either a participating ANSP or an aircraft operator. Aircraft operators often contract out the function to either DSP or to a separate service provider. Messages from aircraft, especially automatically generated ones, can be pre-configured according to message type so that they are automatically delivered to the appropriate recipient just as ground-originated messages can be configured to reach the correct aircraft.\n\nThe ACARS equipment on the aircraft is linked to that on the ground by the datalink service provider. Because the ACARS network is modeled after the point-to-point telex network, all messages come to a central processing location to be routed. ARINC and SITA are the two primary service providers, with smaller operations from others in some areas. Some areas have multiple service providers.\n\nACARS messages may be of three broad types:\n\nControl messages are used to communicate between the aircraft and its base, with messages either standardized according to ARINC Standard 633, or user-defined in accordance with ARINC Standard 618. The contents of such messages can be OOOI events, flight plans, weather information, equipment health, status of connecting flights, etc.\n\nA major function of ACARS is to automatically detect and report the start of each major flight phase, called OOOI events in the industry (out of the gate, off the ground, on the ground, and into the gate). These OOOI events are detected using input from aircraft sensors mounted on doors, parking brakes, and struts. At the start of each flight phase, an ACARS message is transmitted to the ground describing the flight phase, the time at which it occurred, and other related information such as the amount of fuel on board or the flight origin and destination. These messages are used to track the status of aircraft and crews.\n\nACARS interfaces with flight management systems (FMS), acting as the communication system for flight plans and weather information to be sent from the ground to the FMS. This enables the airline to update the FMS while in flight, and allows the flight crew to evaluate new weather conditions or alternative flight plans.\n\nACARS is used to send information from the aircraft to ground stations about the conditions of various aircraft systems and sensors in real-time. Maintenance faults and abnormal events are also transmitted to ground stations along with detailed messages, which are used by the airline for monitoring equipment health, and to better plan repair and maintenance activities.\n\nAutomated ping messages are used to test an aircraft's connection with the communication station. In the event that the aircraft ACARS unit has been silent for longer than a preset time interval, the ground station can ping the aircraft (directly or via satellite). A ping response indicates a healthy ACARS communication.\n\nACARS interfaces with interactive display units in the cockpit, which flight crews can use to send and receive technical messages and reports to or from ground stations, such as a request for weather information or clearances or the status of connecting flights. The response from the ground station is received on the aircraft via ACARS as well. Each airline customizes ACARS to this role to suit its needs.\n\nACARS messages may be sent using a choice of communication methods, such as VHF or HF, either direct to ground or via satellite, using minimum-shift keying (MSK) modulation.\n\nACARS can send messages over VHF if a VHF ground station network exists in the current area of the aircraft. VHF communication is line-of-sight propagation and the typical range is up to 200 nautical miles at high altitudes. Where VHF is absent, an HF network or satellite communication may be used if available. Satellite coverage may be limited at high latitudes (trans-polar flights).\n\nIn the wake of the crash of Air France Flight 447 in 2009, there was discussion about making ACARS an \"online-black-box\" to reduce the effects of the loss of a flight recorder. However no changes were made to the ACARS system.\n\nIn March 2014, ACARS messages and Doppler analysis of ACARS satellite communication data played a very significant role in efforts to trace Malaysia Airlines Flight 370 to an approximate location. While the primary ACARS system on board MH370 had been switched off, a second ACARS system called Classic Aero was active as long as the plane was powered up, and kept trying to establish a connection to an Inmarsat satellite every hour.\n\nThe ACARS on the Airbus A320 of EgyptAir Flight 804 reported \"irregularities\" to ground staff on three separate occasions, which led to three emergency landings, in the 24 hours prior to the aircraft's crash into the Mediterranean Sea on May 19, 2016, which killed all 66 persons on board. The specific nature of the irregularities was not explained, but at each instance the aircraft was given clearance to continue its flight.\n\nIn 2002, ACARS was added to the NOAA Observing System Architecture. Thus commercial aircraft can act as weather data providers for weather agencies to use in their forecast models, sending meteorological observations like winds and temperatures over the ACARS network. NOAA provides real-time weather maps.\n\n\n"}
{"id": "304673", "url": "https://en.wikipedia.org/wiki?curid=304673", "title": "Akai", "text": "Akai\n\nAkai (アカイ) is a consumer electronics brand name. The original company was founded in 1946 in Tokyo, Japan as Akai Electric Co., Ltd., developing electronics such as LED TVs and Air Conditioning systems.\n\nAkai was founded by Masukichi Akai and his son, Saburo Akai (who died in 1973) as , a Japanese manufacturer in 1929 or 1946.\n\nThe company's business eventually became troubling and it left the audio industry in 1991. At its peak in the late 1990s, Akai Holdings employed 100,000 workers and had annual sales of HK$40 billion (US$5.2 billion). The company filed for insolvency in November 2000, owing creditors US$1.1B. It emerged that ownership of Akai Holdings had somehow passed in 1999 to Grande Holdings, a company founded by Akai's chairman James Ting. The liquidators claimed that Ting had stolen over US$800m from the company with the assistance of accountants Ernst & Young who had tampered with audit documents going back to 1994. Ting was imprisoned for false accounting in 2005, and E&Y paid $200m to settle the negligence case out of court in September 2009. In a separate lawsuit, a former E&Y partner, Christopher Ho, made a \"substantial payment\" to Akai creditors in his role as chairman of Grande Holdings.\n\nAkai's products included reel-to-reel audiotape recorders (such as the GX series), tuners (top level AT, mid level TR and TT series), audio cassette decks (top level GX and TFL, mid level TC, HX and CS series), amplifiers (AM and TA series), microphones, receivers, turntables, video recorders and loudspeakers.\nMany Akai products were sold under the name Roberts in the US, as well as A&D in Japan (from 1987 after a partnership with Mitsubishi Electric), Tensai and \"Transonic Strato\" in Western Europe. During the late 1960s, Akai adopted Tandberg's cross-field recording technologies (using an extra tape head) to enhance high frequency recording and switched to the increasingly reliable Glass and crystal (X'tal) (GX) ferrite heads a few years later. The company's most popular products were the GX-630D, GX-635D, GX-747/GX-747DBX and GX-77 open-reel recorders (latter featuring an auto-loading function), the three-head, closed-loop GX-F95, GX-90, GX-F91, GX-R99 cassette decks, and the AM-U61, AM-U7 and AM-93 stereo amplifiers.\n\nAkai manufactured and badged most of its imported hi-fi products with the Tensai brand (named after the Swiss audio and electronics distributor Tensai International. Tensai International was Akai's exclusive distributor for the Swiss and Western European markets until 1988.\n\nAkai limited its consumer hi-fi product line in the United States and Europe towards the end of the 20th century.\n\nAkai produced consumer video cassette recorders (VCR) during the 1980s. The Akai VS-2 was the first VCR with an on-screen display, originally named the Interactive Monitor System. By displaying the information directly on the television screen, this innovation eliminated the need for the user to be physically near the VCR to program recording, read the tape counter, or perform other common features. Within a few years, all competing manufacturers had adopted on-screen display technology in their own products.\n\nIn 1984, a new division of the company was formed to focus on the manufacture and sale of electronic instruments, and was called Akai Professional.\nThe first product released by the new subsidiary was MG1212, a 12 channel, 12 track recorder in 1984. This innovative device used a special VHS-like cartridge (a MK-20), and was good for 10 minutes of continuous 12 track recording (19 cm per second) or 20 minutes at half speed (9.5 cm per second). One track (14) was permanently dedicated to recording absolute time, and another one for synchronization such as SMPTE or MTC. Each channel strip included dbx type-1 noise reduction and semi-parametric equalizers (with fixed bandwidths). The unit also had innovations like an electronic 2 bus system, a 12 stereo channel patch bay and auto punch in and out, among others. The unique transport design and noise reduction gave these units a recording quality rivaling that of more expensive 16 track machines using 1\" tape. The MG-1212 was later replaced by the MG-1214, which improved the transport mechanism and overall performance.\n\nOther early products included the Akai AX80 8-voice analog synthesizer in 1984, followed by AX60 and AX73 6-voice analog synthesizers ca.1986. The AX-60 borrowed many ideas from the Roland Juno series, but used voltage controlled analog oscillators (VCO) as a sound source as opposed to Roland's more common digitally controlled analog oscillators (DCO), and also allowed the performer to \"split\" the keyboard (using different timbres for different ranges of keys). The AX-60 also had the ability to interface with Akai's early samplers through a serial cable, using 12-bit samples as an additional oscillator.\n\nThe S612 12-bit digital sampler in 1985, was the first in a series of (relatively) affordable samplers already in 19-inch studio-rack format but in black color. It held only a single sample at a time, which was loaded into memory via a separate disk drive utilizing Quick Disk 2.8-inch floppy disks. The maximum sample time at the highest quality sampling rate (32 kHz) was one second.\n\nThe introduction of a \"professional\" range of digital samplers began with the 12-bit S900 in 1986, followed by the X7000 keyboard sampler in 1986, and the S700 rack-mount version in 1987. Unlike the single-sample S612, however, they allowed the use of six active samples at once, had a built-in disk drive and could be extended with six individual outputs via cable and a flash memory extension which added another six samples to the memory for multisample playback. The S700/X7000 sampler series were light-grey colored, which didn't change throughout the whole \"professional\" range of Akai samplers.\n\nThe 16-bit Akai S1000 followed in 1988. The latter was replaced by the S3000 series in 1992–1995, which notably featured a writeable CD-ROM (on S3000CD) and hard disk recording (on S3000i), and was followed by the S5000 and S6000. Additional releases of note were the Z4 and Z8 24-bit 96 kHz samplers.\n\nAkai also produced several Digital MIDI sequencers and digital synthesizers such as the MPC range, a line of integrated drum machines, MIDI sequencers, samplers and direct-to-disk recorders.\n\nIn December 1999, one year before the application of to Akai Electric Company Ltd., the brand of their musical instrument division, Akai Professional was acquired by a company of the United States. The new company “Akai Professional Musical Instrument Corporation” (\"AKAI professional M.I.\") was established in the same year, however it was bankrupted in 2005.\n\nIn 2004, following a US distribution deal, the Akai Professional Musical Instrument division was acquired by Jack O'Donnell, owner of Numark Industries and Alesis. Numark, including Akai Professional, was acquired in 2012 by inMusic Brands.\n\nAn Akai Professional product that is somewhat sought after in current times is the model DM13 microphone. This small, unidirectional unit was originally made for tape recorders, as well as CB radio equipment. Today, they can be found in the arsenal of many blues harmonica players due to its high gain and high impedance properties.\n\nIn early 2003, the consumer electronics company began undergoing a re-exposure by marketing various rebranded video products manufactured by Samsung. In the same year, Akai began to distribute home appliances such as HVAC units, vacuum cleaners, water filtration devices, and refrigerated store showcases.\n\nIn Canada, Akai portable DVD players were sold at 'The Source by Circuit City', and at Zellers, a division of the Hudson's Bay Company.\n\n\n\n\n\n\n\nAkai Professional, a division of Numark Industries (based in Rhode Island, United States) since 2004, is currently not affiliated with Akai (a consumer audio and television brand).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7548205", "url": "https://en.wikipedia.org/wiki?curid=7548205", "title": "Arid Lands Information Network", "text": "Arid Lands Information Network\n\nArid Lands Information Network (ALIN) is a Kenya-based non-governmental organisation that seeks to exchange ideas and experiences among \"grassroots change agents\". It sees its goal as enabling such grassroot change agents to learn from one another, through capacity-building and what it terms the \"innovative use of information and communication technologies (ICTs).\"\n\nKnown as the Arid Lands Information Network (ALIN), this organisation describes itself as \"a non-profit, non-political NGO\". Its focus of work is on the arid and semi-arid lands (also referred to as the ASALs) within the African Great Lakes region.\n\nALIN is a network of over 2,000 grassroots Community Development Workers (CDWs) drawn from non-governmental organisations and community- based organisations as well as government departments. The members of the network offer a form of extension service in their fields of expertise.\n\nTo promote local level networking, ALIN has clustered members in the same geographical regions into grassroots networking nodes known as focal groups. These groups form the entry points of ALIN into the community. The network works through 15 focal groups spread out across the African Great Lakes region, in Kenya, Tanzania and Uganda.\n\nIn collaboration with other partners, ALIN has established and equipped Maarifa Centres (MCs) with ICT equipment. The centres act as access and dissemination points for information, content creation and skills development among the rural communities. There are MCs in all three Great Lakes countries. Partners of the MCs include Oxfam Novib, Ford Foundation, Communication Commission of Kenya (CCK), the Royal Danish Embassy and the Embassy of Finland among others. The centres are manned by Communication Information Volunteers (CIVs) who are hired on a one-year basis as young and fresh graduates from reputable institutions of higher learning and are then posted to the various Maarifa centres.\n\nIt was earlier known as the Reseau d'Information des Terres Arides - Arid Lands Information Network (RITA-ALIN).\n\nALIN is involved in the production of several publications. The \"Baobab Journal\" is ALIN's flagship journal. It has been in circulation since 1998. Baobab acts a platform for CDWs to share information and experiences on sustainable livelihoods.\n\nALIN also produces the LEISA (Low External Input in Sustainable Agriculture) magazine for Eastern Africa which is named \"Kilimo Endelevu Afrika\" (Sustainable Agriculture in Africa). This magazine aims to identify promising technologies for small-scale farmers.\n\n\"Joto Afrika\" (loosely translated as \"Africa is feeling the heat\") is the third major publication and is produced in collaboration with the Institute of Development Studies (IDS). It is a new series of printed briefings and online resources. The publication focuses on climate change adaptation and developments in sub-Saharan Africa and is published in both English and French.\n\n"}
{"id": "5974443", "url": "https://en.wikipedia.org/wiki?curid=5974443", "title": "Arms embargo", "text": "Arms embargo\n\nAn arms embargo is an embargo that applies solely to weaponry, and may also apply to \"dual-use technology\". An arms embargo may serve one or more purposes:\n\nUnited States President Jimmy Carter imposed an arms embargo on the 1976 Argentinian \"Proceso de Reorganizacion Nacional\" (National Reorganization Process) military junta due to the Dirty War that took place from 1974 to 1983. The embargo was joined by the United Kingdom following the 1982 Falklands War. The ban was lifted in the 1990s after Argentina was named as a major non-NATO ally. During those years, Argentine armed forces shifted to Western European countries and Israel for supplies.\n\nThe United States government imposed an arms embargo against Indonesia in 1999 due to human rights violations in East Timor. The embargo was lifted in 2005.\n\nThe United States imposed economic sanctions against Iran following the Iranian Revolution in 1979. However, to secure the release of American hostages, several senior Reagan Administration officials secretly facilitated the sale of arms to Iran in the 1980s, in a scandal called the Iran–Contra affair. In 1995 the US expanded sanctions to include firms dealing with the Iranian government.\n\nIn March 2007, UN Security Council Resolution 1747 tightened the sanctions imposed on Iran in connection with the Iranian nuclear program. The UN sanctions were lifted on 16 January 2016.\n\nThe United States and the European Union stopped exporting arms to China after 1989, due to the Chinese government's violent suppression of protests in Tiananmen Square. In 2004-05, there was some debate in the EU over whether to lift the embargo.\nThe arms embargo of South Africa from 1977 extended to dual-use items. The embargo was lifted by Resolution 919 in 1994.\n\nThe countries included in the list are under arms embargo of the United Nations or another international organization (EU, OSCE and others) or country. In some cases the arms embargo is supplemented by a general trade embargo, other sanctions (financial) or travel ban for specific persons. In some cases the arms embargo applies to any entity residing or established in the country, but in others it is partial – the recognized government forces and international peacekeepers are exempted from the embargo.\n\n\n\n"}
{"id": "2524673", "url": "https://en.wikipedia.org/wiki?curid=2524673", "title": "Bedpan", "text": "Bedpan\n\nA bedpan or bed pan is a receptacle used for the toileting of a bedridden patient in a health care facility, and is usually made of metal, glass, ceramic, or plastic. A bedpan can be used for both urinary and fecal discharge. Many diseases can confine a patient to bed, necessitating the use of bedpans, including Alzheimer's disease, Parkinson's disease, stroke, and dementia. Additionally, many patients may be confined to a bed temporarily as a result of a temporary illness, injury, or surgery, thereby necessitating the use of a bedpan.\n\nBedpans are usually constructed of stainless steel, which is easy to clean and durable, but may be cold, hard, and uncomfortable to use. Also, the supporting area of some products is very small, and prolonged use can cause pressure ulcers. To solve these problems, new ergonomic bedpans have been developed, which support the patient with a larger area of warm plastic. Some designs completely cover the genitalia during use, offering protection and an extra measure of privacy. On the other hand, the material is more difficult to sterilize, and may become a reservoir for microorganisms.\n\nFracture bedpans are smaller than standard size bedpans, and have one flat end. These bedpans are designed specifically for patients who have had a hip fracture or are recovering from hip replacement. This type of bedpan may be used for those patients who cannot raise their hips high enough or roll over onto a regular size bedpan.\n\nIn recent years, a bedpan liner made of recycled wood pulp (molded pulp) is more popular in UK hospitals; it is for single use, decreasing the risk of cross-contamination diseases. An alternative to the recycled pulp liner is the plastic bedpan liner, which also creates a barrier between the waste and the bedpan. Some liners are made of biodegradable plastic and contain absorbent powder to eliminate splashing and spills. Liners are used in hospitals to decrease infection, and can also be purchased and used for home health care.\n\n"}
{"id": "35426988", "url": "https://en.wikipedia.org/wiki?curid=35426988", "title": "Biosynthesis Reactor For Food Industries", "text": "Biosynthesis Reactor For Food Industries\n\nThe DFRL/DRDO has developed a biosynthesis reactor which facilitates to control, measure and process different micro formulations of Biopolymers, bio preservative coatings and packing films for MAP to enhance the shelf life of the fresh produces.\n\nFood scientists and technologists, packaging scientists and technologists, protein chemists, and polymer chemists in research and developments can be benefited by this reactor.\n"}
{"id": "40817", "url": "https://en.wikipedia.org/wiki?curid=40817", "title": "Bridging loss", "text": "Bridging loss\n\nBridging loss is the loss, at a given frequency, that results when an impedance is connected across a transmission line. It is expressed as the ratio, in decibels, of the signal power delivered to a given point in a system downstream from the bridging point prior to bridging, to the signal power delivered to the given point after bridging. The term is introduced because return loss is not applicable to the high-impedance input conditions. The term is also used in telephone practice and synonymous with the insertion loss that result from bridging an impedance across a circuit.\n\nSource: from Federal Standard 1037C and from MIL-STD-188\n"}
{"id": "4756951", "url": "https://en.wikipedia.org/wiki?curid=4756951", "title": "Canadian Information Processing Society", "text": "Canadian Information Processing Society\n\nThe Canadian Information Processing Society (CIPS) is legally recognized as the association that advances and self-regulates the Information Technology profession in Canada.\nThe association certifies professionals against a standard of practice and competency, assigning the designation that recognizes the level of experience of the IT Professional holder in the eyes of the general public:\nThe society also performs accreditation of computer science, software engineering and Business Technology Management programs, as well as college/technical institute level programs in Computer Systems Technology, Applied Information Technology, and finally, Post-Diploma type programs at Canadian universities. CIPS is responsible for defining the Canadian IT Body of Knowledge, and for enforcing a Code of Ethics and Professional Conduct.\n\nCalvin Gotlieb helped found CIPS in 1958, serving as its president from 1960 to 1961. Calvin was elected as founding fellow in 2006. The first President of CIPS was Fred Thomas serving in 1958 to 1959.\n\nPrime Minister of Canada Stephen Harper provided a message to CIPS on the 50th anniversary.\n\nEach province has a provincial body that administers the legislation or regulation establishing the self-regulating professional body. This consists of the following bodies:\n\nCIPS is the founding member organisation of the International Federation for Information Processing (IFIP). IFIP works on establishing international standards for information technology and software engineering. CIPS is also a member of South East Asia Regional Computer Confederation (SEARCC) and a founding member of IFIP IP3.\nCIPS is also a constituent member of the ICCP, . which is the Institute for Certification of Computing Professionals, based out of the USA, and dedicated to the establishment of high professional standards for the computer industry across North America.\n\nCIPS is also a member organization of the Federation of Enterprise Architecture Professional Organizations (FEAPO), a worldwide association of professional organizations which have come together to provide a forum to standardize, professionalize, and otherwise advance the discipline of Enterprise Architecture.\n\n\n\n"}
{"id": "13968216", "url": "https://en.wikipedia.org/wiki?curid=13968216", "title": "Cattle chute", "text": "Cattle chute\n\nA cattle chute (North America) or cattle race (Australia, British Isles and New Zealand) also called a run or alley, is a narrow corridor built for cattle, sheep, pigs and other animals to travel through when being herded from one location to another that is nearby. A conventional race consists of parallel panels or fences with a space between them just wide enough for one animal to pass through comfortably without being able to turn around, thus forming the animals into a queue that only allows them to go forward. It is used for routine husbandry activities such as drafting (sorting) or loading animals via ramp or \"loading chute\" into a vehicle; placing them one at a time in a \"cattle crush\"\nfor examination, marking or veterinary treatment. They are also used at packing plants to move animals into a crush designed for slaughter.\n\nAn experimental humane design of cattle run, by Temple Grandin, gradually narrows so that cattle have ample time to form the queue, and curves to encourage cattle to move forward in a controlled manner (see photo).\n\nCalves (and other smaller animals such as sheep) can turn around in an adult cattle race, so a narrower race is required for proper handling. Thus the width of some races are adjustable to accommodate different sized animals.\nCattle races may be portable, or may be a permanent fixture with a concrete walkway. Portable races may be made of steel, iron or aluminium; but modern permanent ones are usually of steel or iron (sometimes timber or even concrete) which is usually set in concrete, with solid or railed sides and a non-slip floor. Anti-bruise races do not have sharp edges, and instead use pipe with rounded edges such as oval rails; alternatively sides with sheet iron or steel can be found or built onto the races, which improves livestock movement and also prevents injuries from animals getting their legs or heads caught between the rails. Races that have concrete floors have the flooring made wider than the race itself to prevent hooves catching between the bottom rail and the edge of the concrete. The concrete is also not smooth like that on city sidewalks but roughed out to give the animals more traction to prevent slipping and injury. Lower parts of the race have side panels that may be removable in the event of an animal becoming cast (fallen) or caught up in which the animal is needed to be freed to prevent further injury.\n\nThe length of the race is usually determined by the size of the herd – a longer one requires less penning-up of a larger herd. Longer races may be curved, to improve the movement of the animals. However races longer than tend to cause trouble with the flow of the animals into the loading chute or cattle crush. A walkway may be provided on the outside of the race, on one or both sides, to allow handlers easier handling, examination or treatment of animals from above. \n\nThere are gates at the start and end of the race to regulate the movement of animals. The entrance is from a small funnel-shaped or semi-circular \"forcing pen\" (or \"forcing yard\" or \"crowding tub\"), where a gate is used to move cattle into the race. The gates are usually arranged so the operator cannot become trapped or injured by the cattle. This is achieved in several ways:\n\nThe exit from the race may be through a \"drafting gate\" (or \"shedding gate\"), which swings to open one or another of several exits for separating animals into various groups.\n\nA calf race and cradle makes calf branding and castration much quicker and cleaner. The calf is forced into a crush, like that of a normal crush, except it is pushed to one side and held by steel bars as shown in the photo on the right. Note that the bars are nowhere near the belly region of the calf, only located on the neck and in front of the stifle. Then one side of the crush is tipped 90°, exposing the side of the calf to be branded or examined. Calf cradles are available in temporary or permanent styles like that described above. The steel transportable race and table cradle, as shown in the photo, are very popular in Australia and New Zealand, but are also found in North America.\n\n"}
{"id": "349632", "url": "https://en.wikipedia.org/wiki?curid=349632", "title": "Clothes iron", "text": "Clothes iron\n\nA clothes iron is a device that, when heated, is used to press clothes to remove creases. It is named for the metal of which the device was historically commonly made, and the use of it is generally called ironing. Ironing works by loosening the ties between the long chains of molecules that exist in polymer fiber materials. With the heat and the weight of the ironing plate, the fibers are stretched and the fabric maintains its new shape when cool. Some materials, such as cotton, require the use of water to loosen the intermolecular bonds. \n\nBefore the introduction of electricity, irons were heated by combustion, either in a fire or with some internal arrangement. An \"electric flatiron\" was invented by American Henry W. Seeley and patented on June 6, 1882. It weighed almost 15 pounds and took a long time to heat. The UK Electricity Association is reported to have said that an electric iron with a carbon arc appeared in France in 1880, but this is considered doubtful.\n\nMetal pans filled with hot coals were used for smoothing fabrics in China in the 1st century BC. From the 17th century, \"sadirons\" or \"sad irons\" (from Middle English \"sad\" meaning of \"solid\", used in modern English through the 1800s) began to be used. They were thick slabs of cast iron, delta-shaped and with a handle, heated in a fire or on a stove. These were also called flat irons. A later design consisted of an iron box which could be filled with hot coals, which had to be periodically aerated by attaching a bellows. In Kerala in India, burning coconut shells were used instead of charcoal, as they have a similar heating capacity. This method is still in use as a backup device, since power outages are frequent. Other box irons had heated metal inserts instead of hot coals.\n\nAnother solution was to employ a cluster of solid irons that were heated from a single source: As the iron currently in use cooled down, it could be quickly replaced by a hot one. In the late nineteenth and early twentieth centuries, there were many irons in use that were heated by fuels such as kerosene, ethanol, whale oil, natural gas, carbide gas (acetylene, as with carbide lamps), or even gasoline. Some houses were equipped with a system of pipes for distributing natural gas or carbide gas to different rooms in order to operate appliances such as irons, in addition to lights. Despite the risk of fire, liquid-fuel irons were sold in U.S. rural areas up through World War II.\n\nIn the industrialized world, these designs have been superseded by the electric iron, which uses resistive heating from an electric current. The hot plate, called the \"sole plate\", is made of aluminium or stainless steel polished to be as smooth as possible; it is sometimes coated with a low-friction heat-resistant plastic to reduce friction below that of the metal plate. The heating element is controlled by a thermostat that switches the current on and off to maintain the selected temperature. The invention of the resistively heated electric iron is credited to Henry W. Seeley of New York City in 1882. In the same year an iron heated by a carbon arc was introduced in France, but was too dangerous to be successful. The early electric irons had no easy way to control their temperature, and the first thermostatically controlled electric iron appeared in the 1920s. Later, steam was used to iron clothing. Credit for the invention of the steam iron goes to Thomas Sears. The first commercially available electric steam iron was introduced in 1926 by a New York drying and cleaning company, Eldec, but was not a commercial success. The patent for an electric steam iron and dampener was issued to Max Skolnik of Chicago in 1934. In 1938 Skolnik granted the Steam-O-Matic Corporation of New York the exclusive right to manufacture steam-electric irons. This was the first steam iron to achieve any degree of popularity, and led the way to more widespread use of the electric steam iron during the 1940s and 1950s.\n\nHistorically, irons have had several variations and have thus been called by many names:\n\n\nModern irons for home use can have the following features:\n\nOne of the world's larger collection of irons, comprising 1300 historical examples of irons from Germany and the rest of the world, is housed in Gochsheim Castle, near Karlsruhe, Germany.\n\nMany ethnographical museums around the world have collections of irons. In Ukraine, for example, about 150 irons are the part of the exhibition of the Radomysl Castle in Ukraine.\n\nAn ironing center or steam ironing station is a device consisting of a clothes iron and a separate steam generator-tank. By having a separate tank, the ironing unit can generate more steam than a conventional iron, making steam ironing faster. Such ironing facilities take longer to warm up than conventional irons, and cost more.\n\n\n"}
{"id": "18491306", "url": "https://en.wikipedia.org/wiki?curid=18491306", "title": "Constant-current diode", "text": "Constant-current diode\n\nConstant-current diode is an electronic device that limits current to a maximal specified value for the device. It is known as current-limiting diode (CLD), current-regulating diode (CRD).\nThese diodes consist of an n-channel JFET with the gate shorted to the source, which functions like a two-terminal current limiter or current source (analogous to a voltage-limiting Zener diode). They allow a current through them to rise to a certain value, and then level off at a specific value. Unlike Zener diodes, these diodes keep the current constant instead of the voltage constant. These devices keep the current flowing through them unchanged when the voltage changes. An example is the 1N5312. Note the negative \"V\" is required, as an example on the n-type junction-gate field-effect transistor 2N5457.\n\n"}
{"id": "8813325", "url": "https://en.wikipedia.org/wiki?curid=8813325", "title": "Cool Stuff: How It Works", "text": "Cool Stuff: How It Works\n\nCool Stuff: How It Works is a multi-part documentary television mini-series that premiered in 2007 on the Discovery Channel. The program is based on an existing book about how \"Modern Marvels\" actually work. The show is hosted by Steve Truitt.\n\nThe mini-series was produced by Beyond International Group, the creator of the successful Discovery Channel series \"MythBusters\" and \"Prototype This!\", amongst other production credits.\n\n\n"}
{"id": "22468550", "url": "https://en.wikipedia.org/wiki?curid=22468550", "title": "Dark slide (photography)", "text": "Dark slide (photography)\n\nIn photography, a dark slide is a wooden or metal plate that covers the sensitized emulsion side of a photographic plate. In use, a pair of plates joined back to back were used with both plates covered with a dark slide. When used, the dark slide is removed for the period of the exposure and then replaced. Modern dark slides are used in conjunction with a film holder, that either holds in place pieces of cut sheet film or, if modified, some piece of light sensitive material such as glass.\nIn place, the dark slide is in a film holder or magazine. Film holders usually refer to cut sheet film and magazines refer to roll film. Vintage film holders were made of wood and held in place either photo-sensitized plates or photo sensitive film. In either case, the construction must be such to reduce light leaks, or any unwanted light from striking the film until the holder is held in place by a camera and considered \"light tight\". Some holders held only one piece of light sensitive material, some are double sided and hold two pieces of film.\n\nOnce in the camera with a light tight back and light tight bellows, the dark slide may be pulled out for the exposure to be made from a lens. After the exposure, the dark slide is put back in place and the array is put away, usually in a light tight cloth bag or box until it can be taken into a darkroom for development. The term \"dark slide\" refers to the fact the slide is pulled or slid out of the frame either plastic or wooden. On sliding it back in, the same channels for holding the dark slide are used to cover the film. Ordinarily, in modern film holders, the opening to permit the dark slide to be removed is protected by a strip of black velour or other black baffling that permits the dark slide to move in and out of the holder while restraining the amount of light used by the dark slide while either in place or removed.\n\nA dark slide is used for magazine backs on medium format cameras such as Hasselblad and Mamiya, and plate or sheet film holders on view cameras, like the earlier Reisekamera.\n"}
{"id": "712377", "url": "https://en.wikipedia.org/wiki?curid=712377", "title": "Diathermy", "text": "Diathermy\n\nDiathermy is electrically induced heat or the use of high-frequency electromagnetic currents as a form of physical therapy and in surgical procedures. \nThe field was pioneered in 1907 by German physician Karl Franz Nagelschmidt, who coined the term \"diathermy\" from the Greek words \"dia\" and θέρμη \"therma\", literally meaning \"heating through\" (adj., diather´mal, diather´mic).\n\nDiathermy is commonly used for muscle relaxation, and to induce deep heating in tissue for therapeutic purposes in medicine. It is used in physical therapy to deliver moderate heat directly to pathologic lesions in the deeper tissues of the body.\n\nDiathermy is produced by three techniques: ultrasound (\"ultrasonic diathermy\"), short-wave radio frequencies in the range 1–100 MHz (\"shortwave diathermy\") or microwaves typically in the 915 MHz or 2.45 GHz bands (\"microwave diathermy\"), the methods differing mainly in their penetration capability. It exerts physical effects and elicits a spectrum of physiological responses.\n\nThe same techniques are also used to create higher tissue temperatures to destroy neoplasms (cancer and tumors), warts, and infected tissues; this is called hyperthermia treatment. In surgery diathermy is used to cauterize blood vessels to prevent excessive bleeding. The technique is particularly valuable in neurosurgery and surgery of the eye.\n\nThe idea that high-frequency electromagnetic currents could have therapeutic effects was explored independently around the same time (1890-91) by French physician and biophysicist Jacques Arsene d'Arsonval and Serbian American engineer Nikola Tesla. d'Arsonval had been studying medical applications for electricity in the 1880s and performed the first systematic studies in 1890 of the effect of alternating current on the body, and discovered that frequencies above 10 kHz did not cause the physiological reaction of electric shock, but warming. He also developed the three methods that have been used to apply high-frequency current to the body: contact electrodes, capacitive plates, and inductive coils. Nikola Tesla first noted around 1891 the ability of high-frequency currents to produce heat in the body and suggested its use in medicine.\n\nBy 1900 application of high-frequency current to the body was used experimentally to treat a wide variety of medical conditions in the new medical field of \"electrotherapy\". In 1899 Austrian chemist von Zaynek determined the rate of heat production in tissue as a function of frequency and current density, and first proposed using high-frequency currents for deep heating therapy. In 1908 German physician Karl Franz Nagelschmidt coined the term \"diathermy\", and performed the first extensive experiments on patients. Nagelschmidt is considered the founder of the field. He wrote the first textbook on diathermy in 1913, which revolutionized the field.\n\nUntil the 1920s noisy spark-discharge Tesla coil and Oudin coil machines were used. These were limited to frequencies of 0.1 - 2 MHz, called \"longwave\" diathermy. The current was applied directly to the body with contact electrodes, which could cause skin burns. In the 1920s the development of vacuum tube machines allowed frequencies to be increased to 10 - 300 MHz, called \"shortwave\" diathermy. The energy was applied to the body with inductive coils of wire or capacitive plates insulated from the body, which reduced the risk of burns. By the 1940s microwaves were being used experimentally.\n\nThe three forms of diathermy employed by physical therapists are ultrasound, short wave and microwave. The application of moderate heat by diathermy increases blood flow and speeds up metabolism and the rate of ion diffusion across cellular membranes. The fibrous tissues in tendons, joint capsules, and scars are more easily stretched when subjected to heat, thus facilitating the relief of stiffness of joints and promoting relaxation of the muscles and decrease of muscle spasms.\n\nUltrasound diathermy employs high-frequency acoustic vibrations which, when propelled through the tissues, are converted into heat. This type of diathermy is especially useful in the delivery of heat to selected musculatures and structures because there is a difference in the sensitivity of various fibers to the acoustic vibrations; some are more absorptive and some are more reflective. For example, in subcutaneous fat, relatively little energy is converted into heat, but in muscle tissues there is a much higher rate of conversion to heat.\n\nThe therapeutic ultrasound apparatus generates a high-frequency alternating current, which is then converted into acoustic vibrations. The apparatus is moved slowly across the surface of the part being treated. Ultrasound is a very effective agent for the application of heat, but it should be used only by a therapist who is fully aware of its potential hazards and the contraindications for its use.\n\nShort wave diathermy machines use two condenser plates that are placed on either side of the body part to be treated. Another mode of application is by induction coils that are pliable and can be molded to fit the part of the body under treatment. As the high-frequency waves travel through the body tissues between the condensers or the coils, they are converted into heat. The degree of heat and depth of penetration depend in part on the absorptive and resistance properties of the tissues that the waves encounter.\n\nShort wave diathermy operations use the ISM band frequencies of 13.56, 27.12, and 40.68 megahertz. Most commercial machines operate at a frequency of 27.12 MHz, a wavelength of approximately 11 meters.\n\nShort wave diathermy usually is prescribed for treatment of deep muscles and joints that are covered with a heavy soft-tissue mass, for example, the hip. In some instances short wave diathermy may be applied to localize deep inflammatory processes, as in pelvic inflammatory disease. Short wave diathermy can also be used for hyperthermia therapy, as an adjuvant to radiation in cancer treatment. Typically, hyperthermia would be added twice a week before radiation, as shown in the photograph from a 2010 clinical trial at Mahavir Cancer Sansthan in Patna, India. \n\nMicrowave diathermy uses microwaves, radio waves which are higher in frequency and shorter in wavelength than the short waves above. Microwaves, which are also used in radar, have a frequency above 300 MHz and a wavelength less than one meter. Most, if not all, of the therapeutic effects of microwave therapy are related to the conversion of energy into heat and its distribution throughout the body tissues. This mode of diathermy is considered to be the easiest to use, but the microwaves have a relatively poor depth of penetration.\n\nMicrowaves cannot be used in high dosage on edematous tissue, over wet dressings, or near metallic implants in the body because of the danger of local burns. Microwaves and short waves cannot be used on or near persons with implanted electronic cardiac pacemakers.\n\nHyperthermia induced by microwave diathermy raises the temperature of deep tissues from 41 °C to 45 °C using electromagnetic power. The biological mechanism that regulates the relationship between the thermal dose and the healing process of soft tissues with low or high water content or with low or high blood perfusion is still under study. Microwave diathermy treatment at 434 and 915 MHz can be effective in the short-term management of musculo-skeletal injuries.\n\nHyperthermia is safe if the temperature is kept under 45 °C or 113 °F. The absolute temperature is, however, not sufficient to predict the damage that it may produce.\n\nMicrowave diathermy-induced hyperthermia produced short-term pain relief in established supraspinatus tendinopathy.\n\nThe physical characteristics of most of the devices used clinically to heat tissues have been proved to be inefficient to reach the necessary therapeutic heating patterns in the range of depth of the damage tissue. The preliminary studies performed with new microwave devices working at 434 MHz have demonstrated encouraging results. Nevertheless, adequately designed prospective-controlled clinical studies need to be completed to confirm the therapeutic effectiveness of hyperthermia with large number of patients, longer-term follow-up and mixed populations.\n\nMicrowave diathermy is used in the management of superficial tumours with conventional radiotherapy and chemotherapy. Hyperthermia has been used in oncology for more than 35 years, in addition to radiotherapy, in the management of different tumours. In 1994, hyperthermia was introduced in several countries of the European Union as a modality for use in physical medicine and sports traumatology. Its use has been successfully extended to physical medicine and sports traumatology in Central and Southern Europe.\n\n\"Surgical diathermy\" is usually better known as \"electrosurgery\". (It is also referred to occasionally as \"electrocautery\", but see disambiguation below.) Electrosurgery and surgical diathermy involve the use of high-frequency A.C. electric current in surgery as either a cutting modality, or else to cauterize small blood vessels to stop bleeding. This technique induces localized tissue burning and damage, the zone of which is controlled by the frequency and power of the device.\n\nSome sources insist that electrosurgery be applied to surgery accomplished by high-frequency alternating current (AC) cutting, and that \"electrocautery\" be used only for the practice of cauterization with heated nichrome wires powered by direct current (DC), as in the handheld battery-operated portable cautery tools.\n\nDiathermy used in surgery is of typically two types. \n\nBurns from electrocautery generally arise from a faulty grounding pad or from an outbreak of a fire. Monopolar electrocautery works because radio frequency energy is concentrated by the surgical instrument's small surface area. The electrical circuit is completed by passing current through the patient's body to a conductive pad that is connected to the radio frequency generator. Because the pad's surface area is large relative to the instrument's tip, energy density across the pad is reliably low enough that no tissue injury occurs at the pad site. Electrical shocks and burns are possible, however, if the circuit is interrupted or energy is concentrated in some way. This can happen if the pad surface in contact is small, e.g. if the pad's electrolytic gel is dry, if the pad becomes disconnected from the radio frequency generator, or via a metal implant. Modern electrocautery systems are equipped with sensors to detect high resistance in the circuit that can prevent some injuries.\n\nAs with all forms of heat applications, care must be taken to avoid burns during diathermy treatments, especially in patients with decreased sensitivity to heat and cold. With electrocautery there have been reported cases of flash fires in the operating theatre related to heat generation meeting chemical flash points, especially in the presence of increased oxygen concentrations associated with anaesthetic.\n\nConcerns have also been raised regarding the toxicity of surgical smoke produced by electrocautery. This has been shown to contain chemicals which may cause harm to patients, surgeons and/or operating theatre staff.\n\nFor patients that have a surgically implanted Spinal Cord Stimulator (SCS) system, diathermy can cause tissue damage through energy that is transferred into the implanted SCS components resulting in severe injury or death.\n\nMedical diathermy devices were used to cause interference to German radio beams used for targeting nighttime bombing raids in World War II during the Battle of the Beams.\n\n"}
{"id": "1866533", "url": "https://en.wikipedia.org/wiki?curid=1866533", "title": "Electronic filter", "text": "Electronic filter\n\nElectronic filters are circuits which perform signal processing functions, specifically to remove unwanted frequency components from the signal, to enhance wanted ones, or both. Electronic filters can be:\n\n\nThe most common types of electronic filters are linear filters, regardless of other aspects of their design. See the article on linear filters for details on their design and analysis.\n\nThe oldest forms of electronic filters are passive analog linear filters, constructed using only resistors and capacitors or resistors and inductors. These are known as RC and RL single-pole filters respectively. However, these simple filters have very limited uses. Multipole LC filters provide greater control of response form, bandwidth and transition bands. The first of these filters was the constant k filter, invented by George Campbell in 1910. Campbell's filter was a ladder network based on transmission line theory. Together with improved filters by Otto Zobel and others, these filters are known as image parameter filters. A major step forward was taken by Wilhelm Cauer who founded the field of network synthesis around the time of World War II. Cauer's theory allowed filters to be constructed that precisely followed some prescribed frequency function.\n\nPassive implementations of linear filters are based on combinations of resistors (R), inductors (L) and capacitors (C). These types are collectively known as \"passive filters\", because they do not depend upon an external power supply and/or they do not contain active components such as transistors.\n\nInductors block high-frequency signals and conduct low-frequency signals, while capacitors do the reverse. A filter in which the signal passes through an inductor, or in which a capacitor provides a path to ground, presents less attenuation to low-frequency signals than high-frequency signals and is therefore a \"low-pass filter\". If the signal passes through a capacitor, or has a path to ground through an inductor, then the filter presents less attenuation to high-frequency signals than low-frequency signals and therefore is a \"high-pass filter\". Resistors on their own have no frequency-selective properties, but are added to inductors and capacitors to determine the \"time-constants\" of the circuit, and therefore the frequencies to which it responds.\n\nThe inductors and capacitors are the reactive elements of the filter. The number of elements determines the order of the filter. In this context, an LC tuned circuit being used in a band-pass or band-stop filter is considered a single element even though it consists of two components.\n\nAt high frequencies (above about 100 megahertz), sometimes the inductors consist of single loops or strips of sheet metal, and the capacitors consist of adjacent strips of metal. These inductive or capacitive pieces of metal are called stubs.\n\nThe simplest passive filters, RC and RL filters, include only one reactive element, except hybrid LC filter which is characterized by inductance and capacitance integrated in one element.\n\nAn L filter consists of two reactive elements, one in series and one in parallel.\n\nThree-element filters can have a 'T' or 'π' topology and in either geometries, a low-pass, high-pass, band-pass, or band-stop characteristic is possible. The components can be chosen symmetric or not, depending on the required frequency characteristics. The high-pass T filter in the illustration, has a very low impedance at high frequencies, and a very high impedance at low frequencies. That means that it can be inserted in a transmission line, resulting in the high frequencies being passed and low frequencies being reflected. Likewise, for the illustrated low-pass π filter, the circuit can be connected to a transmission line, transmitting low frequencies and reflecting high frequencies. Using m-derived filter sections with correct termination impedances, the input impedance can be reasonably constant in the pass band.\n\nMultiple element filters are usually constructed as a ladder network. These can be seen as a continuation of the L,T and π designs of filters. More elements are needed when it is desired to improve some parameter of the filter such as stop-band rejection or slope of transition from pass-band to stop-band.\n\nActive filters are implemented using a combination of passive and active (amplifying) components, and require an outside power source. Operational amplifiers are frequently used in active filter designs. These can have high Q factor, and can achieve resonance without the use of inductors. However, their upper frequency limit is limited by the bandwidth of the amplifiers.\n\nThere are many filter technologies other than lumped component electronics. These include digital filters, crystal filters, mechanical filters, surface acoustic wave (SAW) filters, bulk acoustic wave (BAW) filters, garnet filters, and atomic filters (used in atomic clocks).\n\nThe transfer function formula_1 of a filter is the ratio of the output signal formula_2 to that of the input signal formula_3 as a function of the complex frequency formula_4:\n\nwith formula_6.\n\nThe transfer function of all linear time-invariant filters, when constructed of discrete components, will be the ratio of two polynomials in formula_4, i.e. a rational function of formula_4. The order of the transfer function will be the highest power of formula_4 encountered in either the numerator or the denominator.\n\nElectronic filters can be classified by the technology used to implement them.\nFilters using passive filter and active filter technology can be further classified by the particular electronic filter topology used to implement them.\n\nAny given filter transfer function may be implemented in any electronic filter topology.\n\nSome common circuit topologies are:\n\n\nHistorically, linear analog filter design has evolved through three major approaches. The oldest designs are simple circuits where the main design criterion was the Q factor of the circuit. This reflected the radio receiver application of filtering as Q was a measure of the frequency selectivity of a tuning circuit. From the 1920s filters began to be designed from the image point of view, mostly being driven by the requirements of telecommunications. After World War II the dominant methodology was network synthesis. The higher mathematics used originally required extensive tables of polynomial coefficient values to be published but modern computer resources have made that unnecessary.\n\nLow order filters can be designed by directly applying basic circuit laws such as Kirchhoff's laws to obtain the transfer function. This kind of analysis is usually only carried out for simple filters of 1st or 2nd order.\n\nThis approach analyses the filter sections from the point of view of the filter being in an infinite chain of identical sections. It has the advantages of simplicity of approach and the ability to easily extend to higher orders. It has the disadvantage that accuracy of predicted responses relies on filter terminations in the image impedance, which is usually not the case.\n\nThe network synthesis approach starts with a required transfer function and then expresses that as a polynomial equation of the input impedance of the filter. The actual element values of the filter are obtained by continued-fraction or partial-fraction expansions of this polynomial. Unlike the image method, there is no need for impedance matching networks at the terminations as the effects of the terminating resistors are included in the analysis from the start.\n\nHere is an image comparing Butterworth, Chebyshev, and elliptic filters. The filters in this illustration are all fifth-order low-pass filters. The particular implementation – analog or digital, passive or active – makes no difference; their output would be the same.\n\nAs is clear from the image, elliptic filters are sharper than all the others, but they show ripples on the whole bandwidth.\n"}
{"id": "25865228", "url": "https://en.wikipedia.org/wiki?curid=25865228", "title": "EngineeringUK", "text": "EngineeringUK\n\nEngineeringUK is an independent, not-for-profit organisation whose purpose is to promote the contribution that engineers, and engineering and technology, make to society. Working within the United Kingdom, EngineeringUK aims to inspire people at all levels to pursue careers in engineering and technology.\n\nEngineeringUK is the lead organiser of annual The Big Bang UK Young Scientists & Engineers Fair.\n\n"}
{"id": "30440282", "url": "https://en.wikipedia.org/wiki?curid=30440282", "title": "Existing visitor optimisation", "text": "Existing visitor optimisation\n\nExisting visitor optimisation (EVO) is a discipline of optimising websites to maximise return from existing visitors rather than through search engine optimisation (SEO). While SEO is useful to gain more visitors, EVO looks at types of neuromarketing and psychology to optimise the language and layout elements of websites to maximise the benefits of the increased visitors.\n\nStrategies that may be employed include asking \"how many would you like\" rather than offering an \"add to cart\" button on e-commerce sites. EVO is a concept that works closely with existing marketing concepts such as the utilisation of social media. Initially investigated by a PhD student from Exeter University and The University College Plymouth St Mark & St John to provide a solid foundation to making the most of existing websites and building on the work of Jakob Nielsen, EVO was first presented at York St. John University as part of a post-graduate conference in 2008.\n\nIn 2010, during the Second International NLP Research Conference (hosted by Cardiff University and co-organised by Paul Tosey of The University of Surrey in association with The association of NLP, ANLP), EVO was expanded to include email optimisation using the same psychological models.\n\n"}
{"id": "41102160", "url": "https://en.wikipedia.org/wiki?curid=41102160", "title": "Far-Play", "text": "Far-Play\n\nFar-Play (stylized fAR-Play, from augmented reality) is a software platform developed at the University of Alberta, for creating location-based, scavenger-hunt style games which use the GPS and web-connectivity features of a player's smartphone. According to the development team, \"our long-term objective is to develop a general framework that supports the implementation of AARGs that are fun to play and also educational\". It utilizes Layar, an augmented reality smartphone application, QR codes located at particular real-world sites, or a phone's web browser, to facilitate games which require players to be in close physical proximity to predefined \"nodes\". A node, referred to by the developers as a Virtual Point of Interest (vPOI), is a point in space defined by a set of map coordinates; fAR-Play uses the GPS function of a player's smartphone—or, for indoor games, which are not easily tracked by GPS satellites, specially-created QR codes—to confirm that they are adequately near a given node. Once a player is within a node's proximity, Layar's various augmented reality features can be utilized to display a range of extra content overlaid upon the physical play-space or launch another application for extra functionality.\n\nfAR-Play began development in 2008, emerging from a collaborative project undertaken by a group of University of Alberta students from the Computer Science and Humanities Computing departments. fAR-Play is still under development, but a beta version is available for testing by request. fAR-Play's development is managed by a team of interdisciplinary professors and students at the University of Alberta. Currently, the developing team's roster includes Supervising Professors Geoffrey Rockwell and Eleni Stroulia, Developers Lucio Gutierrez and Matthew Delaney, and Website Developers Calen Henry and Garry Wong.\n\nfAR-Play relies on a number of open- and closed-source web technologies as tools to create, and enhance the users' experience. Layar is the recommended client-side frontend for delivering game content to the player; it is available on Android and iOS, which covers over 91% of smartphones. While Layar is not a requirement to play fAR-Play games, the application does supply additional augmented reality functionality; Layar also includes a built-in QR scanner. Depending on the design of the particular game, the player may instead use a dedicated QR code scanner; the developers recommend BeeTagg, but any such application will do. Layar or a QR code scanner are the maximum software requirements to play a fAR-Play game, making implementation of games on a wide variety of platforms relatively straightforward. fAR-Play games can also be designed for play strictly within a mobile phone's web browser. On the server side, fAR-Play's engine is composed of an Apache server which manages the system's web interface, including the mobile and desktop versions of the fAR-Play website, and a Java-based REST framework for managing the database of nodes.\n\nAs a platform for designing AR games, as opposed to an AR game itself, fAR-Play offers little in the way of explicit shapes or patterns for games to take; instead, these elements are left to the game designer or players to develop. However, the nonspecific nature of nodes, the many options they offer for content delivery, and the open design of the platform are such that these elements can be developed extensively. Functionally, fAR-Play is a tool for tracking arbitrary points in space and a given player's proximity to them; what it does beyond that is up to the developers' and players' discretion. However, the fAR-Play website contains a leaderboard which tracks registered user's total scores. Players are assigned levels based on their total score, ranging from Novice - Super Player. Player profiles will display nodes that the player has recently caught, and any achievements the player has gained. Additionally, players can share their adventure progress, achievements, and the capture of vPOIs on Facebook.\n\nIn order to participate in the locative aspects of fAR-Play games, users must have an Android or iOS mobile device and access to wireless internet. Players can participate in fAR-Play anonymously, or create and sign into a fAR-Play account. Those who choose to play anonymously will lose the ability to track their progress across multiple games. When signed in, the player is presented with a list of games that are currently available for play. Each game includes a brief description and the various \"adventures\" available to the player. Once the game has been started, the player has three different methods for capturing nodes: they may scan a QR in the physical space, discover a node through the Layar camera virtual view, or receive a link in their device's web browser.\n\nQR codes can only be used as a method for capturing nodes and initiating games when there is a physical code present. In order to scan a QR code, players are required to have an application which can capture and recognize QR codes. If the player is utilizing a QR scanning application that has a built in browser, they will be required to log into fAR-Play through the app. Layar is a free to download augmented reality app, containing a built in QR code scanner, which enables its users to participate in fAR-Play games.\n\nLayar permits the player to see nodes on their mobile device, guiding the player to their goal. Using this application, the player is able to navigate to their objective with map provided by Google Maps' API or by using their camera—Layar overlays a virtual image onto the real-world scene presented by the camera. The representations on screen expand in size as the player approaches the node destination, simulating relative distance. If the player taps any of the nodes that are presented on the screen, they will be provided additional information about that node, including the node's name and a brief description. Nodes can be captured by tapping the \"capture\" button.\n\nThe player can also play fAR-Play games within their mobile device's browser. By visiting http://farplay.ualberta.ca/far-play/ on a mobile device, players will be presented with a fully realized user interface, permitting full interaction with the games. The player can capture the in game vPOIs through their browser by tapping the \"nodes\" button. This will bring up a list of all the accessible nodes, complete with a brief description for each location. By clicking on one of the nodes, the player is shown to a screen with a mapped location of the vPOI, an in-depth description of it, and hints. At the top of the page, the player can tap \"CAPTURE THIS NODE\" and advance in the game.\nWhen attempting to capture a node, the developer may or may not associate a challenge with the node. For example, in the game \"Zombies ate my Campus\", when players are attempting to capture a node, they're presented with a multiple choice question associated with the current node.\n\nPlayers complete an adventure when they have captured all of the nodes within it. fAR-Play provides two game modes: in a Virtual Scavenger Hunt, nodes must be captured in a specific order; in a Virtual Treasure Hunt, the order is unimportant.\n\nGames currently available through fAR-Play include:\n\nfAR-Play's ultimate goal is to provide a simple, effective platform for the creation of locative augmented reality games, but the developer tools are still under active development and not openly available to the public. Access can be granted on a case-by-case basis, however, and a developer's manual is available. Users with development privileges can create new games or edit their existing games, in addition to playing their own or others' games.\n\nGames that are developed with fAR-Play are segmented into components called \"Adventures\". To progress through each game adventure, the player must reach and capture virtual points of interest, referred to in the game as vPOIs. In order to capture a vPOI, the player must travel to a physical location that is set by the developer. It is the developer's choice to include a challenge question to capture the vPOI, though it is not mandatory. A deduction of points can be implemented if the player submits an incorrect answer to a challenge question.\n\nEach of the nodes will reward the player with a predetermined number of points once they have been captured by the player. These points are added to the player's total points. Each of the adventures that are created require a predetermined number of vPOIs to be completed before the player can advance. fAR-Play has the ability to implement achievements, which can reward players with extra points for completing certain tasks. The two general classifications of achievements in the game are: \"Percentage of Adventure Completed\" and \"Percentage of Game Completed\".\n\nThe developer must decide if the vPOIs must be completed in a certain order, or if the order is inconsequential. Additionally, the developer can choose if the win condition of an adventure requires all vPOIs to be captured, or only a certain percentage of them. Game creators can set a \"Catch Limit\" for each vPOI, which restricts the number of times each node may be captured. This can be used to encourage races and competitive play.\n\nWhen developing a game for fAR-Play, the site requires that users submit a game name, game description, and game story. Each of the adventures within the game will also require a name, and type; Scavenger Hunt is currently the only available adventure type. For additional aesthetic customization, fAR-Play allows developers to utilize CSS to customize the appearance and layout of the player's interface.\n\nThe developer may implement a hint system within the game to guide players through the experience. These hints can take the form of an image, audio, or video file. To encourage the player to discover answers for themselves, the game can penalize players for the use of hints, reducing their points by a predetermined number.\n\n"}
{"id": "251874", "url": "https://en.wikipedia.org/wiki?curid=251874", "title": "Flexible electronics", "text": "Flexible electronics\n\nFlexible electronics, also known as \"flex circuits\", is a technology for assembling electronic circuits by mounting electronic devices on flexible plastic substrates, such as polyimide, PEEK or transparent conductive polyester film. Additionally, flex circuits can be screen printed silver circuits on polyester. Flexible electronic assemblies may be manufactured using identical components used for rigid printed circuit boards, allowing the board to conform to a desired shape, or to flex during its use. An alternative approach to flexible electronics suggests various etching techniques to thin down the traditional silicon substrate to few tens of micrometers to gain reasonable flexibility, referred to as flexible silicon (~ 5 mm bending radius).\n\nFlexible printed circuits (FPC) are made with a photolithographic technology. An alternative way of making flexible foil circuits or flexible flat cables (FFCs) is laminating very thin (0.07 mm) copper strips in between two layers of PET. These PET layers, typically 0.05 mm thick, are coated with an adhesive which is thermosetting, and will be activated during the lamination process. FPCs and FFCs have several advantages in many applications:\n\n\n\nFlex circuits are often used as connectors in various applications where flexibility, space savings, or production constraints limit the serviceability of rigid circuit boards or hand wiring. A common application of flex circuits is in computer keyboards; most keyboards use flex circuits for the switch matrix.\n\nIn LCD fabrication, glass is used as a substrate. If thin flexible plastic or metal foil is used as the substrate instead, the entire system can be flexible, as the film deposited on top of the substrate is usually very thin, on the order of a few micrometres.\n\nOrganic light-emitting diodes (OLEDs) are normally used instead of a back-light for flexible displays, making a flexible organic light-emitting diode display.\n\nMost flexible circuits are passive wiring structures that are used to interconnect electronic components such as integrated circuits, resistors, capacitors and the like, however some are used only for making interconnections between other electronic assemblies either directly or by means of connectors.\n\nIn the automotive field, flexible circuits are used in instrument panels, under-hood controls, circuits to be concealed within the headliner of the cabin, and in ABS systems. In computer peripherals flexible circuits are used on the moving print head of printers, and to connect signals to the moving arm carrying the read/write heads of disk drives. Consumer electronics devices make use of flexible circuits in cameras, personal entertainment devices, calculators, or exercise monitors.\n\nFlexible circuits are found in industrial and medical devices where many interconnections are required in a compact package. Cellular telephones are another widespread example of flexible circuits.\n\nFlexible solar cells have been developed for powering satellites. These cells are lightweight, can be rolled up for launch, and are easily deployable, making them a good match for the application. They can also be sewn into backpacks or outerwear.\n\nPatents issued at the turn of the 20th century show that early researchers were envisioning ways of making flat conductors sandwiched between layers of insulating material to layout electrical circuits to serve in early telephony switching applications. One of the earliest descriptions of what could be called a flex circuit was unearthed by Dr Ken Gilleo and disclosed in an English patent by Albert Hansen in 1903 where Hansen described a construction consisting of flat metal conductors on paraffin coated paper. Thomas Edison’s lab books from the same period also indicate that he was thinking to coat patterns cellulose gum applied to linen paper with graphite powder to create what would have clearly been flexible circuits, though there is no evidence that it was reduced to practice.\n\nIn the 1947 publication \"Printed Circuit Techniques\" by Cledo Brunetti and Roger W. Curtis a brief discussion of creating circuits on what would have been flexible insulating materials (e.g. paper) indicated that the idea was in place and in the 1950s Sanders Associates' inventors (Nashua, NH) Victor Dahlgren and company founder Royden Sanders made significant strides developing and patenting processes for printing and etching flat conductors on flexible base materials to replace wire harnesses. An advertisement from the 1950 placed by Photocircuits Corporation in New York demonstrated their active interest in flexible circuits also.\n\nToday, flexible circuits which are also variously known around the world variously as \"flexible printed wiring, flex print, flexi circuits,\" are used many products. Large credit is due to the efforts of Japanese electronics packaging engineers who have found countless new ways to employ flexible circuit technology. For the last decade, flexible circuits have remained one of the fastest growing of all interconnection product market segments. A more recent variation on flexible circuit technology is one called \"flexible electronics\" which commonly involves the integration of both active and passive functions in the processing.\n\nThere are a few basic constructions of flexible circuits but there is significant variation between the different types in terms of their construction. Following is a review of the most common types of flexible circuit constructions\n\nSingle-sided flexible circuits have a single conductor layer made of either a metal or conductive (metal filled) polymer on a flexible dielectric film. Component termination features are accessible only from one side. Holes may be formed in the base film to allow component leads to pass through for interconnection, normally by soldering. Single sided flex circuits can be fabricated with or without such protective coatings as cover layers or cover coats, however the use of a protective coating over circuits is the most common practice. The development of surface mounted devices on sputtered conductive films has enabled the production of transparent LED Films, which is used in LED Glass but also in flexible automotive lighting composites.\n\nDouble access flex, also known as back bared flex, are flexible circuits having a single conductor layer but which is processed so as to allow access to selected features of the conductor pattern from both sides. While this type of circuit has certain benefits, the specialized processing requirements for accessing the features limits its use.\n\nSculptured flex circuits are a novel subset of normal flexible circuit structures. The manufacturing process involves a special flex circuit multi-step etching method which yields a flexible circuit having finished copper conductors wherein the thickness of the conductor differs at various places along their length. (i.e., the conductors are thin in flexible areas and thick at interconnection points.).\n\nDouble-sided flex circuits are flex circuits having two conductor layers. These flex circuits can be fabricated with or without plated through holes, though the plated through hole variation is much more common. When constructed without plated through holes and connection features are accessed from one side only, the circuit is defined as a \"Type V (5)\" according to military specifications. It is not a common practice but it is an option. Because of the plated through hole, terminations for electronic components are provided for on both sides of the circuit, thus allowing components to be placed on either side. Depending on design requirements, double-sided flex circuits can be fabricated with protective coverlayers on one, both or neither side of the completed circuit but are most commonly produced with the protective layer on both sides. One major advantage of this type of substrate is that it allows crossover connections to be made very easy. Many single sided circuits are built on a double sided substrate just because they have one of two crossover connections. An example of this use is the circuit connecting a mousepad to the motherboard of a laptop. All connections on that circuit are located on only one side of the substrate, except a very small crossover connection which uses the second side of the substrate.\n\nFlex circuits having three or more layers of conductors are known as multilayer flex circuits. Commonly the layers are interconnected by means of plated through holes, though this is not a requirement of the definition for it is possible to provide openings to access lower circuit level features. The layers of the multilayer flex circuit may or may not be continuously laminated together throughout the construction with the obvious exception of the areas occupied by plated through-holes. The practice of discontinuous lamination is common in cases where maximum flexibility is required. This is accomplished by leaving unbonded the areas where flexing or bending is to occur.\n\nRigid-flex circuits are a hybrid construction flex circuit consisting of rigid and flexible substrates which are laminated together into a single structure. Rigid-flex circuits should not be confused with rigidized flex constructions, which are simply flex circuits to which a stiffener is attached to support the weight of the electronic components locally. A rigidized or stiffened flex circuit can have one or more conductor layers. Thus while the two terms may sound similar, they represent products that are quite different.\n\nThe layers of a rigid flex are also normally electrically interconnected by means of plated through holes. Over the years, rigid-flex circuits have enjoyed tremendous popularity among military product designer, however the technology has found increased use in commercial products. While often considered a specialty product for low volume applications because of the challenges, an impressive effort to use the technology was made by Compaq computer in the production of boards for a laptop computer in the 1990s. While the computer's main rigid-flex PCBA did not flex during use, subsequent designs by Compaq utilized rigid-flex circuits for the hinged display cable, passing 10s of 1000s of flexures during testing. By 2013, the use of rigid-flex circuits in consumer laptop computers is now common.\n\nRigid-flex boards are normally multilayer structures; however, two metal layer constructions are sometimes used.\n\nPolymer thick film (PTF) flex circuits are true printed circuits in that the conductors are actually printed onto a polymer base film. They are typically single conductor layer structures, however two or more metal layers can be printed sequentially with insulating layers printed between printed conductor layers, or on both sides. While lower in conductor conductivity and thus not suitable for all applications, PTF circuits have successfully served in a wide range of low-power applications at slightly higher voltages. Keyboards are a common application, however, there are a wide range of potential applications for this cost-effective approach to flex circuit manufacture.\n\nEach element of the flex circuit construction must be able to consistently meet the demands placed upon it for the life of the product. In addition, the material must work reliably in concert with the other elements of the flexible circuit construction to assure ease of manufacture and reliability. Following are brief descriptions of the basic elements of flex circuit construction and their functions.\n\nThe base material is the flexible polymer film which provides the foundation for the laminate. Under normal circumstances, the flex circuit base material provides most primary physical and electrical properties of the flexible circuit. In the case of adhesiveless circuit constructions, the base material provides all of the characteristic properties.\nWhile a wide range of thickness is possible, most flexible films are provided in a narrow range of relatively thin dimension from 12 µm to 125 µm (1/2 mil to 5 mils) but thinner and thicker material are possible. Thinner materials are of course more flexible and for most material, stiffness increase is proportional to the cube of thickness. Thus for example, means that if the thickness is doubled, the material becomes eight times stiffer and will only deflect 1/8 as much under the same load.\nThere are a number of different materials used as base films including: polyester (PET), polyimide (PI), polyethylene naphthalate (PEN), polyetherimide (PEI), along with various fluropolymers (FEP) and copolymers. Polyimide films are most prevalent owing to their blend of advantageous electrical, mechanical, chemical and thermal properties.\n\nAdhesives are used as the bonding medium for creating a laminate. When it comes to temperature resistance, the adhesive is typically the performance limiting element of a laminate especially when polyimide is the base material. Because of the earlier difficulties associated with polyimide adhesives, many polyimide flex circuits presently employ adhesive systems of different polymer families. However some newer thermoplastic polyimide adhesives are making important in-roads.\nAs with the base films, adhesives come in different thickness. Thickness selection is typically a function of the application. For example, different adhesive thickness is commonly used in the creation of cover layers in order to meet the fill demands of different copper foil thickness which may be encountered.\n\nA metal foil is most commonly used as the conductive element of a flexible laminate. The metal foil is the material from which the circuit paths are normally etched. A wide variety of metal foils of varying thickness are available from which to choose and create a flex circuit, however copper foils, serve the vast majority of all flexible circuit applications. Copper’s excellent balance of cost and physical and electrical performance attributes make it an excellent choice. There are actually many different types of copper foil. The IPC identifies eight different types of copper foil for printed circuits divided into two much broader categories, electrodeposited and wrought, each having four sub-types.) As a result, there are a number of different types of copper foil available for flex circuit applications to serve the varied purposes of different end products. With most copper foil, a thin surface treatment is commonly applied to one side of the foil to improve its adhesion to the base film. Copper foils are of two basic types: wrought (rolled) and electrodeposited and their properties are quite different. Rolled and annealed foils are the most common choice, however thinner films which are electroplated are becoming increasingly popular.\n\nIn certain non standard cases, the circuit manufacturer may be called upon to create a specialty laminate by using a specified alternative metal foil, such as a special copper alloy or other metal foil in the construction. This is accomplished by laminating the foil to a base film with or without an adhesive depending on the nature and properties of the base film.\n\nSpecifications are developed to provide a common ground of understanding of what a product should look like and how it should perform. Standards are developed directly by manufacturer's associations such as the Association Connecting Electronics Industries (IPC) and by users of flexible circuits.\n\n\n"}
{"id": "3270009", "url": "https://en.wikipedia.org/wiki?curid=3270009", "title": "Greg Pfountz", "text": "Greg Pfountz\n\nGreg Pfountz is an American computer programmer who created the Color64 BBS system. Color64 was a popular BBS system for Commodore 64 computers.\n\n"}
{"id": "59039705", "url": "https://en.wikipedia.org/wiki?curid=59039705", "title": "Gwern Branwen", "text": "Gwern Branwen\n\nGwern Branwen is a pseudonymous author and security researcher. He is known for gathering data and conducting research on dark web marketplaces, as well as for writing articles for \"Wired\". He is also a cryptocurrency advocate and was formerly a moderator of Reddit's /r/darknetmarkets forum until the summer of 2015, when he stepped down from the role.\n\n"}
{"id": "52997259", "url": "https://en.wikipedia.org/wiki?curid=52997259", "title": "Insureon", "text": "Insureon\n\nInsureon is an online insurance agency headquartered in Chicago. Launched in 2011, Insureon built a FinTech platform that gives small-business owners access to property and casualty insurance quotes from multiple carriers and allows them to manage their policies.\n\nInsureon has been cited as a company shaping the InsurTech market because its digital platform allows small-business owners direct access to insurance quotes, providing an alternative to traditional brick-and-mortar agencies.\n\nIn 2013, UFC Fighter Ronda Rousey appeared in a promotional video for Insureon, playing the character of the Insureon Protector. These video were quickly removed from all social media concerning Insureon.\n\nIn October 2015, Insureon received a $31 million private investment led by Oak HC/FT, a venture capitalist firm specializing in financial technology companies. The company was also backed by Accretive LLC as part of a Series A funding in 2010.\n\nIn 2015, Insureon placed $150 million in premium and was the third fastest growing private insurance company in the United States with a three-year growth of 1,742% and one of the fastest growing companies in Chicago.\n\nOn March 15, 2016, Insureon launched its first national television campaign during March Madness.\n\nOn June 17, 2016 Insureon laid off 8% of its staff, which was 20 employees at the time. The cuts come eight months after the company raised $31 million in funding. \n"}
{"id": "4389367", "url": "https://en.wikipedia.org/wiki?curid=4389367", "title": "Integrated manufacturing database", "text": "Integrated manufacturing database\n\nAn integrated database system can be used by small and large businesses as a means to incorporate IT in the manufacturing process. It updates, stores and records information, with a view to rapid retrieval.\n\nSome examples of could include:\n\nIt is capable of performing searches for a particular part that may be present in many different products.\n"}
{"id": "54506518", "url": "https://en.wikipedia.org/wiki?curid=54506518", "title": "JSFiddle", "text": "JSFiddle\n\nJSFiddle is an online community for testing and showcasing user-created and collaborational HTML, CSS and JavaScript code snippets, known as 'fiddles'. It allows for simulated AJAX calls. \n\nJSFiddle started life as an app in 2009 by Piotr Zalewa, originally called MooShell. In 2010, the platform was made freely available when Oskar Krawczyk joined.\n\nIn 2017, Michał Laskowski and Andrzej Kała joined the company.\n"}
{"id": "45637858", "url": "https://en.wikipedia.org/wiki?curid=45637858", "title": "Leverate", "text": "Leverate\n\nLeverate is a software as a service provider for foreign exchange brokers. It provides both electronic trading platforms and back-office software to manage a company's operations through its various subsidiaries.> It has offices in Cyprus, Hong Kong, Israel, Ukraine, China and Germany.\n\nLeverate was founded in 2008 by Ran Strauss, Doron Cohen, Itai Damti, and Doron Somech who had worked together on the project since 2006. The original idea was to create a trading algorithm for the purpose of buying and selling that was completely automated. It took two years to build the platform and the company was launched officially in 2008. Leverate received its first angel investment in 2008 from Jacques Beer, owner and chief executive officer of Tempo Beverages Ltd. The company saw a limit to the use of the algorithm and instead turned it into a price feed for brokers that helped reduce losses incurred by latency.\n\nLeverate discontinued the use of its algorithm in 2008 and began to market products to brokers. One of the features of the algorithm was a product feed that was described as \"fast\" and \"accurate.\" The company began using the feed to help brokers who had a lack of knowledge of market conditions. The feed became Leverate's initial product offering, referred to as LXFeed. Leverate moved from algorithm trading into the SaaS market with the product launch, leading to the development of additional products that include risk management tools and mobile trading platforms.\n\nLeverate expanded its operations in 2011 by adding 20 employees to its staff of 50. The company was able to do so after selling a 25% stake to Saxo Bank for $12.5 million (USD). It also expanded its presence by opening a financial services branch in Hong Kong in 2012 and an office in Ukraine in 2013. Leverate entered into an agreement with Saxo Bank in 2014 to buy back that company's 25% stake in the company. In November 2017,Leverate launched cyprus-based regulatory consulting and services operation Regyoul8, The finacial Certification services the Edu8, and The new generation traeding platforms.\n\nLeverate's first product was its LXFeed which launched in 2008. It released a risk management system in 2009. The same year it began to bundle its products and release them under the name LXSuite while also allowing companies to purchase stand alone products. In 2010 it launched a platform called LXAPI which allowed users the ability to customize platforms. It also released LXCRM as a customer management tool for brokers. Digital currencies such as Bitcoin and Litecoin were added to its LXFeed in 2014.\n\nLeverate ventured into the social trading marketing by launching Sirix Social Community and The new generation Activ8. The community allows members to view and copy other trader's portfolios, creating transparency in currency pairs, as well as integrating with MetaTrader 4.\n\n"}
{"id": "5492199", "url": "https://en.wikipedia.org/wiki?curid=5492199", "title": "List of solid waste treatment technologies", "text": "List of solid waste treatment technologies\n\nThe following page contains a list of different forms of solid waste treatment technologies and facilities employed in waste management infrastructure.\n\n\n\nIn the UK these are sometimes termed advanced waste treatment technologies, even though these technologies are not necessarily more complex than the established technologies.\n\n"}
{"id": "39261916", "url": "https://en.wikipedia.org/wiki?curid=39261916", "title": "Liver support systems", "text": "Liver support systems\n\nLiver support systems are therapeutic devices to assist in performing the functions of the liver in persons with liver damage.\n\nThe primary functions of the liver include removing toxic substances from the blood, manufacturing blood proteins, storing energy in the form of glycogen, and secreting bile. The hepatocytes that perform these tasks can be killed or impaired by disease, resulting in hepatic insufficiency. A sudden onset of life-threatening hepatic insufficiency is known as acute liver failure (ALF).\n\nIn hyperacute and acute liver failure the clinical picture develops rapidly with progressive encephalopathy and multiorgan dysfunction such as hyperdynamic circulation, coagulopathy, acute renal and respiratory insufficiency, severe metabolic alterations and cerebral edema that can lead to brain death. In these cases the mortality without liver transplantation (LTx) ranges between 40-80%. LTx is the only effective treatment for these patients although it requires a precise indication and timing to achieve good results. Nevertheless, due to the scarcity of organs to carry out liver transplantations, it is estimated that one third of patients with ALF die while waiting to be transplanted.\nOn the other hand, a patient with a chronic hepatic disease can suffer an acute decompensation of liver function following a precipitating event such as variceal bleeding, sepsis and excessive alcohol intake among others that can lead to a condition referred to as acute-on-chronic liver failure (ACLF).\nBoth types of hepatic insufficiency, ALF and ACLF, can potentially be reversible and liver functionality can return to a level similar to that prior to the insult or precipitating event.\nLTx is the only treatment that has shown an improvement in the prognosis and survival with most severe cases of ALF. Nevertheless, cost and donor scarcity have prompted researchers to look for new supportive treatments that can act as “bridge” to the transplant procedure. By stabilizing the patient’s clinical state, or by creating the right conditions that could allow the recovery of native liver functions, both detoxification and synthesis can improve, after an episode of ALF or ACLF.\nBasically, three different types of supportive therapies have been developed: bio-artificial, artificial and hybrid liver support systems (Table 2).\n\nBio-artificial liver support systems are experimental extracorporeal devices that use living cell lines to provide detoxification and synthesis support to the failing liver. Bio-artificial liver (BAL) Hepatassist 2000 uses porcine hepatocytes whereas ELAD system employs hepatocytes derived from human hepatoblastoma C3A cell lines. \nBoth techniques can produce, in fulminant hepatic failure (FHF), an improvement of hepatic encephalopathy grade and biochemical parameters. Nevertheless, they are therapies with high complexity that require a complex logistic approach for implementation; a very high cost and possible inducement of important side effects such as immunological issues (porcine endogenous retrovirus transmission), infectious complications and tumor transmigration have been documented. Other biological hepatic systems are Bioartificial Liver Support (BLSS) and Radial Flow Bioreactor (RFB). Detoxification capacity of these systems is poor and therefore they must be used combined with other systems to mitigate this deficiency. Today its use is limited to centers with high experience in their application.\n\nArtificial liver support systems are aimed to temporally replace native liver detoxification functions and they use albumin as scavenger molecule to clear the toxins involved in the physiopathology of the failing liver. Most of the toxins that accumulate in the plasma of patients with liver insufficiency are protein bound, and therefore conventional renal dialysis techniques, such as hemofiltration, hemodialysis or hemodiafiltration are not able to adequately eliminate them.\nBetween the different albumin dialysis modalities, single pass albumin dialysis (SPAD) has shown some positive results at a very high cost; it has been proposed that lowering the concentration of albumin in the dialysate does not seem to affect the detoxification capability of the procedure. Nevertheless, the most widely used systems today are based on hemodialysis and adsorption. These systems use conventional dialysis methods with an albumin containing dialysate that is latter regenerate by means of adsorption columns, filled with activated charcoal and ion exchange resins. \nAt present, there are two artificial extracorporeal liver support systems: the Molecular Adsorbents Recirculating System (MARS) from Gambro and Fractionated Plasma Separation and Adsorption (FPSA), commercialised as Prometheus (PROM) from Fresenius Medical Care. Of the two therapies, MARS is the most frequently studied, and clinically used system to date.\n\nMARS was developed by a group of researchers at the University of Rostock (Germany), in 1993 and later commercialized for its clinical use in 1999. The system is able to replace the detoxification function of the liver while minimizing the inconvenience and drawbacks of previously used devices.\n\n\"In vivo\" preliminary investigations indicated the ability of the system to effectively remove bilirubin, biliary salts, free fatty acids and tryptophan while important physiological proteins such as albumin, alpha-1-glicoproteine, alpha 1 antitrypsin, alpha-2-macroglobulin, transferrin, globulin tyrosine, and hormonal systems are unaffected. Also, MARS therapy in conjunction with CRRT/HDF can help clear cytokines acting as inflammatory and immunological mediators in hepatocellular damage, and therefore can create the right environment to favour hepatocellular regeneration and recovery of native liver function.\n\nMARS is an extracorporeal hemodialysis system composed of three different circuits: blood, albumin and low-flux dialysis. The blood circuit uses a double lumen catheter and a conventional hemodialysis device to pump the patient’s blood into the MARS FLUX, a biocompatible polysulfone high-flux dialyser. With a membrane surface area of 2.1 m, 100 nm of thickness and a cut-off of 50 KDa, the MARSFLUX is essential to retaining the albumin in the dialysate. Blood is dialysed against a human serum albumin (HSA) dialysate solution that allows blood detoxification of both water-soluble and protein-bound toxins, by means of the presence of albumin in the dialysate (albumin dialysis). The albumin dialysate is then regenerated in a close loop in the MARS circuit by passing through the fibres of the low-flux diaFLUX filter, to clear water-soluble toxins and provide electrolyte/acid-base balance, by a standard dialysis fluid. Next, the albumin dialysate passes through two different adsorption columns; protein-bound substances are removed by the diaMARS AC250, containing activated charcoal and anionic substances are removed by the diaMARS IE250, filled with cholestyramine, an anion-exchange resin. The albumin solution is then ready to initiate another detoxifying cycle of the patient's blood that can be sustained until both adsorption columns are saturated, eliminating the need to continuously infuse albumin into the system during treatment (Fig. 1).\n\nA systematic review of the literature from 1999 to June 2011 was performed in the following databases:\n\n\nThe LiverNet is a database dedicated to the liver diseases treated with the support of extracorporeal therapies. To date, the most currently used system is the Molecular Adsorbent Recirculating System (MARS), which is based on the selective removal of albumin bound molecules and toxins from the blood in patients with acute and acute-on-chronic liver failure. The purpose is to register prospectively all patients treated worldwide with the MARS system in order to:\nThe liverNet is an eCRF database (www.livernet.net) using a SAS platform that allows major advantages for the centres including the automatic calculations of most liver rand ICU scoring systems, instant queries online, instant export of all patients included in the database of each centre to an Excel file for direct statistical analysis and finally instant online statistical analysis of selective data decided by the scientific committee. Therefore, the LiverNet is an important tool to progress in the knowledge of liver support therapies.\n\nHepatic encephalopathy (HE) represents one of the more serious extrahepatic complications associated with liver dysfunction. Neuro-psychiatric manifestations of HE affect consciousness and behaviour.\n\nEvidence suggests that HE develops as some neurotoxins and neuro active substances, produced after hepatocellular breakdown, accumulates in the brain as a consequence of a portosystemic shunt and the limited detoxification capability of the liver. Substances involved are ammonia, manganese, aromatic aminoacids, mercaptans, phenols, medium chain fatty acids, bilirubin, endogenous benzodiazepines, etc.\nThe relationship between ammonia neurotoxicity and HE was first described in animal studies by Pavlov et al.\nSubsequently, several studies in either animals or humans have confirmed that, a ratio in ammonia concentration higher than 2 mM between the brain and blood stream, causes HE, and even a comatose state when the value is greater than 5 mM. Some investigators have also reported a decrease in serum ammonia following a MARS treatment (Table 3).\n\nManganese and copper serum levels are increased in patients with either acute or acute on chronic liver failure. Nevertheless, only in those patients with chronic hepatic dysfunction, a bilateral magnetic resonance alteration on Globos Pallidus is observed, probably because this type of patients selectively shows higher cerebral membrane permeability.\nImbalance between aromatic and branched chain aminoacids (Fischer index), traditionally involved in HE genesis, can be normalized following a MARS treatment. The effects are noticeable even after 3 hours of treatment and this reduction in the Fisher index is accompanied with an improvement in the HE.\n\nNovelli G \"et al.\" published their three years experience on MARS analyzing the impact of the treatment in the cerebral level for 63 patients reporting an improvement in Glasgow Coma Score (GCS) for all observed in all patients. In the last 22 patients, cerebral perfusion pressure was monitored by Doppler (mean flow velocity in middle cerebral artery), establishing a clear relationship between a clinical improvement (especially neurological) and an improvement in arterial cerebral perfusion. This study confirms other results showing similar increments in cerebral perfusion in patients treated with MARS.\n\nMore recently, several studies have shown a significant improvement of HE in patients treated with MARS. In the studies by Heemann \"et al.\" and Sen \"et al.\" an improvement in HE was considered when encephalopathy grade was reduced by one or more grades vs. basal values; for Hassenein et al., in their randomized controlled trial, improvement was considered when a decrease of two grades was observed. In the latter, 70 patients with acute on chronic liver failure and encephalopathy grade III and IV were included. Likewise, Kramer et al. estimated an HE improvement when an improvement in peak N70 latency in electroencephalograms was observed.\nSen \"et al.\"44 observed a significant reduction in Child-Pugh Score (p<0,01) at 7 days following a MARS treatment, without any significant change in the controls. Nevertheless, when they looked at the Model for End-Stage Liver Disease Score (MELD), a significant reduction in both groups, MARS and controls, was recorded (p<0,01 y p<0,05, respectively).\nLikewise, in several case series, an improvement in HE grade with MARS therapy is also reported.\n\nHemodynamic instability is often associated with acute liver insufficiency, as a consequence of endogenous accumulation of vasoactive agents in the blood. This is characterized by a systemic vasodilatation, a decrease of systemic vascular resistance, arterial hypotension, and an increase of cardiac output that gives rise to a hyperdynamic circulation.\nDuring MARS therapy, systemic vascular resistance index and mean arterial pressure have been shown to increase and show improvement.\nSchmidt et al. reported the treatment of 8 patients, diagnosed with acute hepatic failure, that were treated with MARS for 6 hours, and were compared with a control group of 5 patients to whom ice pads were applied to match the heat loss produced in the treatment group during the extracorporeal therapy. They analyzed hemodynamic parameters in both groups hourly. In the MARS group, a statistically significant increase of 46% on systemic vascular resistance was observed (1215 ± 437 to 1778 ± 710 dinas x s x cm x m) compared with a 6% increase in the controls. Mean arterial pressure also increased (69 ± 5 to 83 ± 11 mmHg, p< 0.0001) in the MARS group, whereas no difference was observed in the controls. Cardiac output and heart rate also decreased in the MARS group as a consequence of an improvement in the hyperdynamic circulation. Therefore, it was shown that a statistically significant improvement was obtained with MARS when compared with the SMT.\n\nCatalina et al. have also evaluated systemic and hepatic hemodynamic changes produced by MARS therapy. In 4 patients with acute decompensation of chronic liver disease, they observed after MARS therapy, an attenuation of hyperdynamic circulation and a reduction in the portal pressure gradient was measured. Results are summarized in table 4.\n\nThere are other studies also worth mentioning with similar results: Heemann \"et al\". and Parés \"et al\". among others. Dethloff T \"et al\". concluded that there is a statistically significant improvement favourable to MARS in comparison with Prometheus system (Table 5).\n\nHepatorenal syndrome is one of the more serious complications in patients with an acute decompensation of cirrhosis and increased portal hypertension. It is characterized by hemodynamic changes in splanchnic, systemic and renal circulation. Splanchnic vasodilatation triggers the production of endogenous vasoactive substances that produce renal vasoconstriction and low glomerular filtration rate, leading to oliguria with a concomitant reduction in creatinine clearance. Renal insufficiency is always progressive with a very poor prognosis, with survival at 1 and 2 months of 20 and 10% respectively.\n\nPierre Versin is one of the pioneers in the study of hepatorenal syndrome in patients with liver impairment. \nGreat efforts have been made trying to improve the prognosis of this type of patient; however, few have solved the problem. Orthotopic liver transplantation is the only treatment that has shown to improve acute and chronic complications derived from severe liver insufficiency. Today it is possible to combine albumin dialysis with continuous veno-venous hemodialfiltration, which provides a greater expectation for these patients by optimization of their clinical status.\n\nMARS treatment lowers serum urea and creatinine levels improving their clearance, and even favors resolution of hepatorenal syndrome. Results are confirmed in a randomized controlled trial published by Mitzner \"et al\". in which 13 patients diagnosed with hepatorenal syndrome type I were treated with MARS therapy. Mean survival was 25,2±34,6 days in the MARS group compared to 4,6±1,8 days observed in the controls in whom hemodiafiltration and standard care (SMT) was applied. This resulted in a statistically significance difference in survival at 7 and 30 days (p<0.05). Authors concluded that MARS therapy, applied to liver failure patients (Child-Pugh C and UNOS 2A scores) who develop hepatorenal syndrome type I, prolonged survival compared to patients treated with SMT.\n\nAlthough mechanisms explaining previous findings are not yet fully understood, it has been reported that there was a decrease in plasma rennin concentrations in patients diagnosed with acute on chronic liver failure with renal impairment that were treated with MARS.\nLikewise, other studies have suggested some efficacy for MARS in the treatment of hepatorenal syndrome.\nHowever, other references have been published that do not show efficacy in the treatment of these types of patients with MARS therapy. Khuroo \"et al\". published a metaanalysis based in 4 small RCT’s and 2 non RCT’s in patients diagnosed with ACLF, concluding that MARS therapy would not bring any significant increment on survival compared with SMT.\nAnother observational study in 6 patients with cirrhosis, refractory ascitis and hepatorenal syndrome type I, not responding to vasoconstrictor therapy, showed no impact on hemodynamics following MARS therapy; however authors concluded that MARS therapy could effectively serve as bridge to liver transplantation.\n\nTotal bilirubin was the only parameter analyzed in all trials that was always reduced in the groups of patients treated with MARS; Banayosy \"et al\". measured bilirubin levels 14 days after since MARS therapy was terminated and observed a consistent, significant decrease not only for bilirubin but also for creatinine and urea (Table 6).\n\nImpact of MARS therapy on plasma biliary acids levels was evaluated in 3 studies. In the study from Stadbauer \"et al\"., that was specifically addressing the topic, it is reported that MARS and Prometheus systems lower to the same extent biliary acids plasma concentration. Heemann \"et al\". and Laleman \"et al\". have also published a significant improvement for these organic ions.\n\nPruritus is one of the most common clinical manifestations in cholestasis liver diseases and one of the most distressing symptoms in patients with chronic liver disease caused by viral hepatitis C. Many hypothesis have been formulated to explain physio pathogenesis of such manifestation, including incremental plasma concentration of biliary acids, abnormalities in the bile ducts, increased central neurotransmitters coupling opioid receptors, etc.…. Despite the number of historical drugs used, individually or combined (exchange resins, hidrophilic biliary acids, antihistamines, antibiotics, anticonvulsants, opioid antagonists), there are reported cases of intractable or refractory pruritus with a dramatic reduction in patients’ quality of life (i.e. sleep disorders, depression, suicide attempts…). Intractable pruritus can be an indication for liver transplantation.\n\nThe MARS indication for intractable pruritus is therapeutically an option that has shown to be beneficial for patients in desperate cases, although at high cost. In several studies, it was confirmed that after MARS treatments, patients remain free from pruritus for a period of time ranging from 6 to 9 months. Nevertheless, some authors have concluded that besides the good results found in the literature, application of MARS therapy in refractory pruritus requires larger evidence.\n\nPharmacokinetics and pharmacodynamics for a majority of drugs can be significantly be modified with liver failure, affecting the therapeutic approach and potential toxicity of the drugs. In these type of patients, Child-Pugh score represents a poor prognostic factor to assess the metabolic capacity of the failing liver.\n\nIn patients with hepatic failure, drugs that are only metabolized in the liver, accumulate in the plasma right after they are administered, and therefore it is needed to modify drug dosing in both, concentration and time intervals, to lower the risk of toxicity. It is also necessary to adjust the dosing for those drugs that are exclusively metabolized by the liver, and have low affinity for proteins and high distribution volume, such as fluoroquinolones (Levofloxacin and Ciprofloxacin).\n\nExtracorporeal detoxification with albumin dialysis increases the clearance of drugs that are bound to plasmatic proteins (Table 7).\n\nIn the meta-analysis published by Khuroo \"et al\". which included 4 randomized trials an improvement in survival for the patients with liver failure treated with MARS, compared with SMT, was not observed.\n\nHowever, neither in the extracorporeal liver support systems review by the Cochrane (published in 2004), nor the meta-analysis by Kjaergard \"et al\". was a significance difference in survival found for patients diagnosed with ALF treated with extracorporeal liver support systems. \nNevertheless, these reviews included all kind of liver support systems and used a heterogeneous type of publication ( abstracts, clinical trials, cohort, etc.).\n\nThere is literature showing favorable results in survival for patients diagnosed with ALF, and treated with MARS., In a randomized controlled trial, Salibà \"et al\". studied the impact on survival of MARS therapy for patients with ALF, waiting on the liver transplant list. Forty-nine patients received SMT and 53 were treated with MARS. They observed that patients that received 3 or more MARS sessions showed a statistically significance increase in transplant-free survival compared with the others patients of the study. Notably, 75% of the patients underwent liver transplantation in the first 24 hours after inclusion in the waiting list, and besides the short exposure to MARS therapy, some patients showed a better survival trend compared to controls, when they were treated with MARS prior to the transplant.\n\nIn a case-controlled study by Montejo \"et al\". it was reported that MARS treatment do not decrease mortality directly; however, the treatment contributed to significantly improve survival in patients that were transplanted. In studies by Mitzner \"et al\". and Heemann \"et al\". they were able to show a significance statistical difference in 30-day survival for patients in the MARS group. However, El Banayosy \"et al\". and Hassanein \"et al\". noticed a non significant improvement in survival, probably because of the short number of patients included in the trials. In the majority of available MARS studies published with patients diagnosed with ALF, either transplanted or not, survival was greater in the MARS group with some variations according to the type of trial, ranging from 20-30%, and 60-80%. Data is summarized in Tables 8, 9 and 10.\n\n<br>\n<br>\n\nFor patients diagnosed with acute on chronic liver failure and treated with MARS therapy, clinical trial results showed a not statistically significant reduction in mortality (odds ratio [OR] =0,78; confident interval [CI] =95%: 0,58 – 1,03; p= 0,1059, Figure 3)\n\n<br>A non-statistically significant reduction of mortality was shown in patients with ALF treated with MARS (OR = 0,75 [CI= 95%, 0,42 – 1,35]; p= 0,3427). (Figure 4)\n\n<br>Combined results yielded a non-significant reduction on mortality in patients treated with MARS therapy. However, the low number of patients included in each of the studies may be responsible for not being able to achieve enough statistical power to show differences between both treatment groups. Moreover, heterogeneity for the number of MARS sessions and severity of liver disease of the patients included, make it very difficult for the evaluation of MARS impact on survival.\n\nRecently, a meta-analysis on survival in patients treated with an extra-hepatic therapy has been published. Searching strategies yielded 74 clinical trials: 17 randomized controlled trials, 5 case control and 52 cohort studies. Eight studies were included in the meta-analysis: three addressing acute liver failure, one with MARS therapy and five addressing acute on chronic, being four MARS related. Authors concluded that extra-hepatic detoxifying systems improve survival for acute liver insufficiency, whereas results for acute decompensation of chronic liver diseases suggested a non significant survival benefit. Also, due to an increased demand for liver transplantation together with an augmented risk of liver failure following large resections, development of detoxifying extrahepatic systems are necessary.\n\nSafety, defined as presence of adverse events, is evaluated in few trials. Adverse events in patients receiving MARS therapy are similar to those in the controls with the exception of thrombocytopenia and hemorrhage that seems to occur more frequently with the MARS system.\n\nHeemann et al. reported two adverse events most probably MARS related: fever and sepsis, presumably originated at the catheter.\n\nIn the study by Hassanein \"et al.\", two patients in the MARS group abandoned the study owing to hemodynamic instability, three patients required larger than average platelets transfusion and three more patients presented gastrointestinal bleeding.\n\nLaleman \"et al\". detected one patient with thrombocytopenia in both the MARS and Prometheus treatment groups, and an additional patient with clotting of the dialysis circuit and hypotension, only in the Prometheus group.\n\nKramer \"et al\". (Biologic-DT) wrote about 3 cases with disseminated intravascular coagulation in the interventional group, two of them with fatal outcomes.\n\nMitzner \"et al\". described, among patients treated with MARS, a thrombocytopenia case and a second patient with chronic hepatitis B, who underwent TIPS placement on day 44 after randomization and died on day 105 of multiorgan failure, as a consequence of complications related to the TIPS procedure.\n\nMontejo \"et al\". showed that MARS is an easy technique, without serious adverse events related to the procedure, and also easy to implement in ICU settings that are used to renal extracorporeal therapies.\n\nThe MARS International Registry, with data from more than 500 patients (although sponsored by the manufacturer), shows that the adverse effects observed are similar to the control group. However, in these severely ill patients it is difficult to distinguish between complications of the disease itself and side effects attributable to the technique.\n\nOnly three Studies addressing cost-effectivenenss of MARS therapy have been found.\nHassanein et al. analysed costs of randomized patients with ACLF receiving MARS therapy or standard medical care. They used the study published in 2001 by Kim et al. describing the impact of complications in hospitalization costs in patients diagnosed with alcoholic liver failure. Cost of 11 patients treated with standard medical care (SMT) were compared to those that received MARS, in addition to SMT (12 patients). In the MARS group, there was less in-hospital mortality and complications related to the disease, with a remarkable reduction in cost which compensated the MARS related expenditure (Table 11).\n\nThere were 5 survivors in the control group, with a cost per patient of $35.904, whereas in the MARS group, 11 patients out of 12 survived with a cost per patient of $32.036 which represents a $4000 savings per patient in favors of the MARS group.\nHessel et al. published a 3-year follow-up of a cohort of 79 patients with ACLF, of whom 33 received MARS treatments and 46 received SMT. Survival was 67% for the MARS group and 63% for the controls, that was reduced to 58 and 35% respectively at one year follow-up, and then 52 and 17% at three years.\n\nHospitalization costs for the MARS treated group were greater than that for the controls (€31.539 vs. €7.543) and similarly direct cost at 3-year follow-up (€8.493 vs. €5.194). Nevertheless, after adjusting mortality rate, the annual cost per patient was €12.092 for controls and €5.827 for MARS group; also in the latter, they found an incremental cost-effectiveness ratio of 31.448 € per life-year gained (LYG) and an incremental costs per QALY gained of 47171 €.\n\nTwo years later, same authors published the results of 149 patients diagnosed with ACLF. There were 67 patients (44,9%) treated with MARS and 82 patients (55,1%) were allocated to receive SMT. Mean survival time was 692 days in the MARS group (33% at 3 years) and 453 days in the controls (15% at 3 years); the results were significant (p=0,022). Differences in average cost was €19.853 (95% IC: 13.308-25.429): 35.639 € for MARS patients and 15.804 € for the control group. Incremental cost per LYG was 29.985 € (95% IC: 9.441-321.761) and €43.040 (95% IC: 13.551-461.856) per quality-adjusted life years (QALY).\n\nLiver support systems, such as MARS, are very important to stabilize patients with acute or acute on chronic liver failure and avoid organ dysfunction, as well as a bridge-to-transplant. Although initial in-hospital costs are high, they are worth for the favorable outcome.\n\nEtiology:\n\nGoals of MARS Therapy\n\n\nMARS Therapy Indication\n\n\nTreatment Schedule:\n\nEtiology:\n\n\nGoals of MARS Therapy\n\n\nMARS Therapy Indication\n\n\nTreatment Schedule:\n\n\nEtiology:\n\n\nGoals of MARS Therapy\n\n\nMARS Therapy Indication\n\n\nTreatment Schedule:\n\n\nEtiology:\n\nGoals of MARS Therapy\n\nMARS Therapy Indication\n\nTreatment Schedule:\n\nEtiology:\n\n\nGoals of MARS Therapy\n\nMARS Therapy Indication\n\nTreatment Schedule:\n\nSame contraindications as with any other extracorporeal treatment may be applied to MARS therapy. \n\nBlood Flow\n\nThe trend is to use high flow rates, although it is determined by the technical specifications of the combined machine and catheters’ size\n\nIntermittent treatments:\nContinuous treatments:\n\nDyalisate Flow Rate\n\nIntermittent treatments:\nContinuous treatments:\n\nReplacement Flow Rate\n\nHeparin Anticoagulation\n\nSimilarly to CVVHD, it depends of previous patient’s coagulation status. In many cases it will not be needed, unless the patient presents a PTT inferior to 160 seconds. In patients with normal values, a bolus of 5000 to 10000 IU of heparin could be administered at the commencement of the treatment, followed by a continuous perfusion, to keep PTT in ratios from 1,5 to 2,5 or 160 to 180 seconds.\n\nMonitoring\n\nA biochemical analysis is recommended (liver and kidney profile, ionic, glucose) together with a hemogram at the end of first session and before starting the following one.\n\nCoagulation analysis must be also performed before starting the session to adjusting heparin dose.\n\nIn case that medication susceptible to be eliminated by MARS is being administered, it is also recommended to monitor their levels in blood\n\nEnd of the Session\nand both catheter’s lumens heparinized \n\nFederal Drug Administration (FDA) cleared, in a document dated on May 27, 2005, MARS therapy for the treatment of drug overdose and poisoning. The only requirement is that the drug or poison must be susceptible to be dialysed and removed by activated charcoal or anionic exchange resins.\n\nMore recently, on December 17, 2012, MARS therapy has been cleared by the FDA for the treatment of hepatic encephalopathy due to a decompensation of a chronic liver disease Clinical trials conducted with MARS treatment in HE patients having a decompensation of chronic liver disease demonstrated a transient effect from MARS treatments to significantly decrease their hepatic encephalopathy scores by at least 2 grades compared to standard medical therapy (SMT).\n\nThe MARS is not indicated as a bridge to liver transplant. Safety and efficacy has not been demonstrated in controlled, randomized clinical trials.\n\nThe effectiveness of the MARS device in patients that are sedated could not be established in clinical studies and therefore cannot be predicted in sedated patients\n"}
{"id": "5234734", "url": "https://en.wikipedia.org/wiki?curid=5234734", "title": "Logic Trunked Radio", "text": "Logic Trunked Radio\n\nLogic Trunked Radio (LTR) is a system developed in the late 1970s by the E. F. Johnson Company.\n\nLTR is distinguished from some other common trunked radio systems in that it does not have a dedicated control channel. Each repeater has its own controller and all of these controllers are coordinated together. Even though each controller monitors its own channel, one of the channel controllers is assigned to be a master and all the other controllers report to it.\n\nTypically on LTR systems, each of these controllers periodically sends out a data burst (approximately every 10 seconds on LTR Standard systems) so that the subscriber units know that the system is there. The idle data burst can be turned off if desired by the system operator. Some systems will broadcast idle data bursts only on channels used as home channels and not on those used for \"overflow\" conversations. To a listener, the idle data burst will sound like a short blip of static like someone keyed up and unkeyed a radio within about 1/2 second. This data burst is not sent at the same time by all the channels but happen randomly throughout all the system channels.\n\n"}
{"id": "19991083", "url": "https://en.wikipedia.org/wiki?curid=19991083", "title": "Matrix molding", "text": "Matrix molding\n\nMatrix molding or matrix transfer molding is a technique often used during molding. The person doing the assembly will first create the rigid outer shell or flask, then introduce the softer and more fluid molding material between the shell and the prototype. This process is often used for complex shapes using composites such as with glass and glass/ceramic composites.\n\n"}
{"id": "454390", "url": "https://en.wikipedia.org/wiki?curid=454390", "title": "Menstrual cup", "text": "Menstrual cup\n\nA menstrual cup is a feminine hygiene product that is inserted into the vagina during menstruation. Its purpose is to prevent menstrual fluid (blood from uterine lining) from leaking onto clothes. Menstrual cups are usually made of flexible medical grade silicone and shaped like a bell with a stem. The stem is used for insertion and removal. The bell-shaped cup seals against the vaginal wall just below the cervix. Every 4–12 hours (depending on the amount of flow), the cup is removed, emptied, rinsed, and reinserted. After each period, the cup should be boiled for at least 5 minutes and stored for use the next month.\n\nUnlike tampons and pads, cups collect menstrual fluid rather than absorbing it. One cup is reusable for up to five years or more. This makes their long-term cost lower than that of disposable tampons or pads, though the initial cost is higher. Menstrual cups are also promoted as more practical and eco-friendly than pads and tampons. Given that the menstrual cup is reusable, its use greatly decreases the amount of waste generated from menstrual cycles, as there is no daily waste and the amount of discarded packaging decreases as well.\n\nMost menstrual cup brands sell a smaller and a larger size. Menstrual cups are sold colorless and translucent, but several brands also offer colored cups, such as pink or purple.\n\nThe use of menstrual cups is considered a safe option relative to other forms of menstrual hygiene. \n\nThe menstrual cup is first folded or pinched and then inserted into the vagina. It will normally unfold automatically and create a light seal against the vaginal walls. In some cases, the user may need to twist the cup or flex the vaginal muscles to ensure the cup is fully open. The cup should sit around the cervix. If correctly inserted, the cup shouldn't leak or cause any discomfort. In comparison with a tampon, the menstrual cup should be placed lower in the vaginal canal. The stem should be completely inside the vagina. There are various folding techniques for insertion; common folds include the c-fold, as well as the punch-down fold.\n\nIf lubrication is necessary for insertion, it should be water-based, as silicone lubricant can be damaging to the silicone.\n\nAfter 4–12 hours of use (depending on the amount of flow), the cup is removed by reaching up to its stem to find the base. Simply pulling on the stem is not recommended to remove the cup, as that can create suction. The base of the cup is pinched to release the seal, and the cup is removed. After emptying, a menstrual cup should be rinsed or wiped and reinserted. It can be washed with a mild soap, and sterilized in boiling water for a few minutes at the end of the cycle. Alternatively, sterilizing solutions (usually developed for baby bottles and breast pump equipment) may be used to soak the cup. Specific cleaning instructions vary by brand.\n\n\nA 2011 randomized controlled trial in Canada investigated whether silicone menstrual cups are a viable alternative to tampons and found that approximately 91% of women in the menstrual cup group said they would continue to use the cup and recommend it to others. In a 1991 clinical study involving 51 women, 23 of the participants (45%) found rubber menstrual cups to be an acceptable way of managing menstrual flow.\n\nIn a randomized controlled feasibility study in rural western Kenya, adolescent primary school girls were provided with menstrual cups or sanitary pads instead of traditional menstrual care items of cloth or tissue. Girls provided with menstrual cups had a lower prevalence of sexually transmitted infections than control groups. Also, the prevalence of bacterial vaginosis was lower among cup users compared with sanitary pad users or those continuing other usual practice. After six months, menstrual cup users were free from embarrassing leakage or odor, and could engage in class activities and sport without humiliation or being teased.\n\n\nWhen using a urine-diverting dry toilet, menstrual blood can be emptied into the part that receives the feces. If any menstrual blood falls into the funnel for urine, it can be rinsed away with water.\n\nMenstrual cups are safe when used as directed and no health risks related to their use have been found.\n\nNo medical research was conducted to ensure that menstrual cups were safe prior to introduction on the market. Early research in 1962 evaluated 50 women using a bell-shaped cup. The researchers obtained vaginal smears, gram stains, and basic aerobic cultures of vaginal secretions. Vaginal speculum examination was performed, and pH was measured. No significant changes were noted. This report was the first containing extensive information on the safety and acceptability of a widely used menstrual cup that included both preclinical and clinical testing and over 10 years of post-marketing surveillance.\n\nOne case report noted the development of endometriosis and adenomyosis in one menstrual cup user. Additionally, one survey with a small sample size indicated a possible link. Therefore, two organizations have issued a combined statement that urged further research. However, the U.S. Food and Drug Administration declined to remove menstrual cups from the market, saying that there was insufficient evidence of risk.\n\nA 2011 randomized controlled trial measured urovaginal infection in a comparison of menstrual cup and tampon use, and found no difference.\n\nNo differences in the growth of \"Staphylococcus aureus\", or health harms were identified among school girls provided with menstrual cups compared to those using sanitary pads, or continuing their usual practice in rural western Kenya.\n\nToxic shock syndrome (TSS) is a potentially fatal bacterial illness. Scientists have recognized an association between TSS and tampon use, although the exact connection remains unclear. TSS caused by menstrual cup use appears to be very rare to virtually nonexistent. The probable reason for this is that menstrual cups are not absorbent, do not irritate the vaginal mucosal tissue, and so do not change the vaginal flora in any measurable amount. Conversely, vaginal dryness and abrasions may occur if the tampon used is more absorbent than needed for the menstrual flow, and normal liquid that should line the vaginal wall is also absorbed. Research has shown that the cup has no impact on the vaginal flora, which means there is no effect on the presence of \"S. aureus\", the bacterium that can cause TSS. The risk of TSS associated with cervical caps used for contraception in the female barrier method is also very low. Cervical caps and menstrual cups both use mostly medical grade silicone or latex.\n\nA widely reported study showed that in vitro, bacteria associated with TSS are capable of growing on menstrual cups. However, there have only been two confirmed cases of toxic shock syndrome (TSS) associated with the use of the DivaCup menstrual cup. The case was a woman who had a history of Hashimoto’s thyroiditis and chronic menorrhagia. In the second case, the menstrual cup had been left in for 7 days. \n\nMenstrual cups are generally bell-shaped, with a few exceptions. Most brands use medical grade silicone as the material for the menstrual cup, although latex and thermoplastic elastomer are also options. Menstrual cups made from silicone are generally designed to last for 1–5 years.\n\nThe majority of menstrual cup brands on the market are selling reusable cups, rather than disposable cups, which is therefore the focus of this article.\n\nMost menstrual cup brands sell a smaller and a larger size. The smaller size is recommended for women under 30 who have not given birth vaginally. The larger size is recommended for women who are over 30, have given birth vaginally, or have a heavy flow. Cups with an even smaller size are recommended for teenagers, as well as women and girls who are more physically fit, as those with stronger pelvic floor muscles may find a larger cup uncomfortable. Length also needs to be considered: if a female's cervix sits particularly low, she may want to use a shorter cup. Capacity is important to women who have a heavier flow. The average menstrual cup holds around 20 ml. Some cups are designed to be larger and hold 37–42 ml. However, all cups currently available have a larger capacity than a regular tampon, which is 10–12 ml.\n\nA final consideration in selecting a menstrual cup is firmness or flexibility. Some companies offer a range of firmness levels in their cups. A firmer cup pops open more easily after insertion and may hold a more consistent seal against the vaginal wall (preventing leaks), but many women find softer cups more comfortable.\n\nThe silicone of which most brands of cups are produced is naturally colorless and translucent. Several brands offer colored cups as well as, or instead of the colorless ones. Translucent cups lose their initial appearance faster than colored – they tend to get yellowish stains with use. The shade of a colored cup may change over time, though stains are often not as obvious as on colored cups. Stains on any color of cup can often be removed or at least lightened by soaking the cup in diluted hydrogen peroxide and/or leaving it out in the sun for a few hours.\n\nMost cups produced do not have any other additives to them, except for the colored cups. The coloring used is reported to be safe and approved by the FDA for medical use and food coloring.\n\nA disposable cup (also called menstrual disc) is usually disc-shaped, like a diaphragm, with a flexible outer ring and a soft, collapsible center. It is designed to be disposed of after use. These tend to be made of a medical-grade polymer blend. It is placed at the base of the cervix in the vaginal fornix, and covers the cervix, like a diaphragm. Because of its placement, it can be worn during sexual intercourse, but it is not a contraceptive nor does it protect against sexually transmitted infections.\n\nDisposable cups are designed to fit most women, but may not be suitable for teenagers as the diameter of the rim may be uncomfortable. The disposable cup is pinched in half and inserted into the vaginal canal. Once inserted, a finger is used to push it back and down toward the cervix. The top rim of the disc rests behind the pubic bone. If correctly inserted, it should not be felt and should not leak.\n\nAfter about 12 hours of use (depending on flow volume), a disposable cup should be removed and disposed of. This is done by inserting a finger into the vagina, feeling for the top of the rim, hooking the finger beneath the rim and pulling straight out. Sitting down is recommended, to keep the disc parallel to the floor while removing. Removing a disc cleanly takes practice, and may be done while seated on the toilet to avoid spillage. During peak flow, most women use two disposable cups per day.\n\nReusable menstrual products (including menstrual cups, but not disposable menstrual cups) are more economical than disposable products. Money will be saved using a menstrual cup, compared with other options such as tampons. A woman in a developed country spends an average of US$60 per year on pads and tampons. If a woman menstruates for 40 years, the lifetime expense for pads and tampons is US$2,400. If the average silicon menstrual cup lasts between one and five years, then between eight and 40 would be needed in 40 years. If a menstrual cup costs US$30 (costs vary by manufacturer), the lifetime cost for a menstrual cup would be between US$240 and US$1,200.\n\nThe up-front cost of a menstrual cup may be expensive for women from low-income households, especially in developing countries. Buying pads or using rags monthly may seem more affordable than purchasing a menstrual cup, though the lifetime spend is higher.\n\nAn early version of a bullet-shaped menstrual cup was patented in 1932, by the midwifery group of McGlasson and Perkins. Leona Chalmers patented the first usable commercial cup in 1937. Later menstrual cups were patented in 1935, 1937, and 1950. The Tassaway brand of menstrual cups was introduced in the 1960s, but it was not a commercial success. Early menstrual cups were made of rubber.\n\nIn 1987, another latex rubber menstrual cup, The Keeper, was manufactured in the United States. This proved to be the first commercially viable menstrual cup and it is still available today. The first silicone menstrual cup was the UK-manufactured Mooncup in 2001. Most menstrual cups are now manufactured from medical grade silicone because of its durability and hypoallergenic properties, though there are also brands made of TPE (thermoplastic elastomer). Menstrual cups are becoming more popular worldwide, with many different brands, shapes and sizes on the market. Most are reusable, though there is at least one brand of disposable menstrual cups currently manufactured.\n\nSome non-governmental organizations (NGOs) and companies have begun to propose menstrual cups to females in developing countries since about 2010, for example in Kenya and South Africa. Menstrual cups are regarded as a low-cost and environmentally friendly alternative to sanitary cloth, expensive disposable pads or \"nothing\" – the reality for many females in developing countries.\n\nWhile numerous companies all over the world offer this product it was still not well known in around 2010. It may be difficult for companies to make profit from this product as one single menstrual cup can last a girl or woman five years or longer. Most women hear of menstrual cups through the internet or word of mouth, rather than through conventional advertizing on TV for example. As of 2018, menstrual cups are mentioned more and more often alongside tampons and pads in publications about menstrual hygiene management.\n\nMenstrual cups can be useful as a means of menstrual hygiene management for women in developing countries, such as Kenya, Uganda and India, where access to affordable sanitary products may be limited. A lack of affordable hygiene products means inadequate, unhygienic alternatives are often used, which can present a serious health risk. Menstrual cups offer a long-term solution compared to some other feminine hygiene products because they do not need to be replaced monthly.\n\nFeminine hygiene products that need to be inserted into the vagina can be unacceptable for cultural reasons. There are myths that they interfere with female reproductive organs and that females lose their virginity. Use of a menstrual cup could stretch or break the hymen. Since some cultures value preservation of the hymen as evidence of virginity, this can discourage young women from using cups.\n\nSince they are reusable, menstrual cups help to reduce solid waste. Some disposable sanitary napkins and plastic tampon applicators can take 25 years to break down in the ocean and can cause a significant environmental impact. Biodegradable sanitary options are also available, and these decompose in a short period of time, but they must be composted, and not disposed off in a landfill.\n\nEach year, an estimated 20 billion pads and tampons are discarded in North America. They typically end up in landfills or are incinerated, which can have a great impact on the environment. Most of the pads and tampons are made of cotton and plastic. Plastic takes about 50 or more years and cotton starts degrading after 90 days if it's composted.\n\nGiven that the menstrual cup is reusable, its use greatly decreases the amount of waste generated from menstrual cycles, as there is no daily waste and the amount of discarded packaging decreases as well. After their life span is over, the silicone cups are put in landfills or incinerated.\n\nMenstrual cups may be emptied into a small hole in the soil or in compost piles, since menstrual fluid is a valuable fertilizer for plants and any pathogens of sexually transmitted diseases will quickly be destroyed by soil microbes. The water used to rinse the cups can be disposed of in the same way. This reduces the amount of wastewater that needs to be treated.\n\nIn developing countries, solid waste management is often lacking. Here, menstrual cups have an advantage over disposable pads or tampons as they do not contribute to the solid waste issues in the communities or generate embarrassing refuse that others may see.\n\n"}
{"id": "19004505", "url": "https://en.wikipedia.org/wiki?curid=19004505", "title": "Nano-RK", "text": "Nano-RK\n\nNano-RK: A Wireless Sensor Networking Real-Time Operating System (RTOS) is a real-time operating system (RTOS) from Carnegie Mellon University designed to run on micro-controllers for use in sensor networks. Nano-RK supports a fixed-priority fully preemptive scheduler with fine-grained timing primitives to support real-time task sets. \"Nano\" implies that the RTOS is small, consuming 2 KB of RAM and using 18 KB of flash, while \"RK\" is short for \"resource kernel\". A resource kernel provides reservations on how often system resources can be consumed. For example, a task might only be allowed to execute 10 ms every 150 ms (CPU reservation), or a node might only be allowed to transmit 10 network packets per minute (network reservation). These reservations form a virtual energy budget to ensure a node meets its designed battery lifetime as well as protecting a failed node from generating excessive network traffic. Nano-RK is open source, is written in C and runs on the Atmel-based FireFly sensor networking platform, the MicaZ motes as well as the MSP430 processor.\n\nThe following article discusses some of the tradeoffs associated with using an RTOS in sensor networks.\n\nNanoRK takes advantage of priority-based preemptive scheduling to help honor the real-time factor of being deterministic thus ensuring task timeliness and synchronization. Due to the characteristic of limited battery power on the wireless node, Nano-RK provides CPU, network, and sensor efficiency through the use of virtual energy reservations, labeling this system as a resource kernel. These energy reservations can enforce energy and communication budgets to minimize the negative impact on the node’s operational lifetime from unintentional errors or malicious behavior by other nodes within the network. It supports packet forwarding, routing and other network scheduling protocols with the help of a light-weight wireless networking stack. Compared with other current sensor operating systems, Nano-RK provides rich functionality and timeliness scheduling with a small-footprint for its embedded resource kernel (RK).\n\nStatic Configuration - Nano-RK uses a static design-time approach for energy usage control. Dynamic task creation is disallowed by Nano-RK requiring application developers to set both task and reservation quotas/priorities in a static testbed design. This design allows the developers to create an energy budget for each task in order to maintain application requirements as well as energy efficiency throughout the system’s lifetime. Using a static configuration approach, all of the runtime configurations as well as the power requirements are predefined and verified by the designer before the system is deployed and executed in the real world. This approach also helps to guarantee the stability and small-footprint characteristics when compared with traditional RTOSs.\n\nWatchdog Timer support - Watchdog is a software timer that triggers a system reset action if the system hangs on crucial faults for an extended period of time. The watchdog mechanism can bring the system back from the nonresponsive state into normal operation by waiting until the timer goes off and subsequently rebooting the device. In Nano-RK, the watchdog timer is tied directly to the\nprocessor’s reset signal REBOOT ON ERROR. By default, it is enabled when the system boots and reset each time the scheduler executes. If the system fails to respond within the predefined time period, the system will reboot and run the initialization instruction sequence to hopefully regain control.\n\nDeep Sleep Mode - Another feature of Nano-RK is the deep sleep mode. For energy efficiency reasons, if there are no eligible tasks to run, the system can be powered down and given the option to enter deep sleep mode. When the system is in deep sleep mode, only the deep sleep timer can wake the system up with a predefined latency period. After waking up from the deep sleep mode, the next context swap time is set to guarantee the CPU wakes up in time. If a sensor node does not wish to perform deep sleep, it also is presented with the choice to go into a low energy consumption state while still managing its peripherals.\n\nNano-RK has implemented a double-linked list of ready queue nodes within a fixed-size array, termed the ready queue, that orders All ready tasks in decreasing order by whichever of the task’s priorities is higher. As the number of tasks running within the Nano-RK implementation is statically-configured in a testbed before deployment, the ready queue size is also fixed to this number of tasks that can be ready to run. A fixed-length array named nrk readyQ is found within the nrk defs.h file along with two pointers to reference the two most important cells within this array. The free node pointer (free node) and the head node pointer (head node) point to the next cell in the array to be allocated and the current highest priority task ready to run, respectively.\n\nThe core of Nano-RK is a static preemptive real-time scheduler which is priority-based and energy efficient. For priority-based preemptive scheduling, the scheduler always selects the highest priority task from the ready queue. To save energy, tasks do not poll for a resource but rather tasks will be blocked on certain events and can be unlocked when the events occur. When there is no task in the ready queue, the system can be powered down to save energy. When the system is working, one and only one task (current task), signified by the nrk cur task tcb, is running for a predefined period. So the most important job of the scheduler is to decide which task should be run next and for how long the next task should be run until the scheduler is triggered to run again.\n\n"}
{"id": "605287", "url": "https://en.wikipedia.org/wiki?curid=605287", "title": "OS-tan", "text": "OS-tan\n\nThe OS-\"tan\" is an Internet meme that originated within the Japanese Futaba Channel. The OS-\"tan\" are the \"moe\" anthropomorphism/personification of several operating systems by various amateur Japanese artists. The OS-\"tan\" are typically depicted as women, with the OS-\"tan\" representative of Microsoft Windows operating systems usually depicted as sisters of varying ages. The \"-tan\" element in the term is a hypocoristic suffix from Japanese.\n\nThough initially appearing only in fan work, the OS-\"tan\" proved popular enough that Microsoft branches in Singapore and Taiwan used the OS-\"tan\" concept as the basis for ad campaigns for Internet Explorer and Microsoft Silverlight, respectively.\n\nThe concept of the OS-\"tan\" is reported to have begun as a personification of the common perception of Windows Me as unstable and prone to frequent crashes. Discussions on Futaba Channel likened this to the stereotype of a fickle, troublesome girl and as this personification expanded Me-\"tan\" was created and followed by the other characters. One of the early works to predominantly feature the OS-\"tan\" was an interactive Flash animation showing a possible intro to an imaginary anime show known as \"Trouble Windows\". A fansub of this was eventually created and is partly responsible for the spread of the OS-\"tan\" to English language imageboards.\n\nThe OS-\"tan\" are not an original concept and are pre-dated by Toy's iMac Girl, who was featured on a series of desktops released between August 1998 and March 1999.\n\nOhzora Publishing produced one book based on OS-\"tan\" characters, titled . It includes illustrations by over 25 contributors. It also includes 95-\"tan\", ME-\"tan\", XP-\"tan\" figures, titled OS Girl 95, OS Girl me, OS Girl XP respectively, but include a molded space for 2k-\"tan\" (named OS Girl 2K).\n\nME-\"tan\", 2K-\"tan\", XP-\"tan\" were designed by GUHICO of Stranger Workshop, while 95-\"tan\" was designed by Fujisaki Shiro from H.B.Company.\n\nParthenon Production Limited,company had commercialized Pink Company's OS-\"tan\" products.\n\nMALINO from Deja Vu ArtWorks produced the Me Document and Shared Folder! trilogy, which were sold in digital format.\n\nJapanese version of Windows 7 Ultimate DSP Edition includes the unofficial Nanami Madobe mascot. This inspired Microsoft Taiwan to launch an official mascot for Microsoft Silverlight, Hikaru. This was followed up by giving Hikaru \"sisters\", Lei, Aoi, and Yu.\n\nA special package of the Japanese Windows 7 Ultimate DSP Edition, called the Touch Mouse Artist Edition or Touch Mouse Limited Edition Artist Series, came with an animated tutorial Windows theme (with custom sounds and three desktop backgrounds) featuring Madobe Nanami.\n\nIn 2009, an Ubuntu-based comic titled \"Ubunchu!\" was serialized in \"Kantan Ubuntu!\", a spinoff from \"Weekly ASCII\" magazine. It was authored by Hiroshi Seo, with English version translated by Fumihito Yoshida, Hajime Mizuno, Martin Owens, Arturo Silva, and Anton Ekblad.\n\nThe Japanese suffix is a mispronunciation of , an informal, intimate, and diminutive honorific suffix for a person, used for friends, family, and pets. In this case, the mispronunciation is used intentionally to achieve the contrived cute or charming effect that is commonly associated with its use by young children and is also sometimes added to the names of non-mascot characters. The personifications as a whole are commonly simply called mascots or mascot characters, and as such the \"-tan\" suffix itself means nothing outside its role as an honorific and its implications of cuteness. Normal suffixes, including \"-san\", \"-chan\", and \"-kun\" are also used in the name of some OS-\"tan\", depending on the character and the speaker's preference; or the suffix may be omitted entirely.\n\nThe name of Windows 10 mascot is officially introduced as Tōko (or Touko) Madobe on 31 July 2015. As confirmed on the character's official Facebook page, her name is a homonym for one of the readings for the Japanese word for 'ten': . Her name was chosen by fans through an online poll. According to her fictional profile, her origins are the Madobe family and she is set 100 years in the future. She likes online gaming and supporting others. Her personal traits are being an excellent student, and expanding her knowledge on technology. Her manager often worries since she’s a bit spontaneous. She also enjoys cheering on people who are working hard and doing their best. She has a part-time job at the Akibano Custom Computer Company where she is a rookie. This level of back story is rather unusual for OS-\"tan\".\n\nThe Japanese Windows 8.1 Pro DSP edition Madobe Family version by Windows Navi+ (Techno-Alliance Corp.) is a limited (1000 units) version of Windows 8.1 Pro 32/64-bit edition with three types of Madobe family picture password wallpapers, Madobe character voices (Nanami, Yū, Ai, Claudia), Madobe family complete edition Windows theme pack, previously unpublished Madobe family designs, Final Pasocom Data Hikkoshi 9+ licence key, Skype three-month free trial, historical Windows logo stickers (XP, Vista, 7, 8). Other editions include a Memorial Pack version without voice, theme pack, stickers (6191 units); a 64-bit Windows Memorial Pack version with a Sculpt Mobile Mouse with Nanami decor (810 units). These editions were available for preorder on 2013-10-04 with release date on 2013-10-18. As part of the market launch, a Facebook draw of 8 followers took place when follower count reaches 80001; and total Twitter follower count for Yū and Ai reach 8001, where winners receive Yū- and Ai-themed prizes.\n\nAdditional types of Windows 8.1 Pro DSP edition Madobe family theme packs were also sold by Ark (TowerHill), ZOA Corporation, Tsukumo (Project White), Dospara, Buy More (Unit.com), Big Camera (Sofmap), and PC One. These versions include two types of wallpapers (Christmas, New Year), theme pack with system voices.\n\nThe Japanese Windows 8 Pro DSP editions were released in Madobe Yū (or Yuu, ) and Madobe Ai () editions by Windows Navi+ (Techno-Alliance Corp.). Both versions (4,000 units per character, thus 8,000 total) include a Microsoft Wedge touch mouse with the Windows 8 logo, character-specific Windows theme (three theme pack wallpapers, event sounds in the respective character's voice), picture password images. In addition, Limited Akihabara Editions (444 units per character, 888 total), sold in Tokyo's Akihabara shopping district, include Madobe Ai/Yū edition of Microsoft Wedge Touch Mouse, an alternate character-specific event sound samples and theme pack and an alternate wallpaper for its respective character. Nipponbashi versions (500 units per character), sold in Nipponbashi in Osaka, include Microsoft Wedge touch mouse (with Ai and Yū decal), three theme pack wall papers (two common and one character-specific), and Yū or Ai event sounds. The Nipponbashi packages include different art. The 32/64-bit version availability depends on retailer.\n\nAsuka Nishi voices the short-haired Yū, while Nao Tamura voices the long-haired Ai.\n\nThe Windows 8 Can Edition from Unitcom (available for the first 2,888 copies) included notepad, T-shirt, two-way mouse pad, pocket media case, smart phone stand cleaning, two-way PC cleaner, Yū and Ai badges, and a freeze blanket.\n\nThe extended fictional Madobe family tree detailed that Yū is the older sister, and their parents are Eiichi () from the Netsu (根津) family and Shii () from the Madobe family. Yū and Ai were said to have a birthdate of 18 November 1996 (Windows CE's release date) with age 15, with height of 152 cm. This conflicts with other back-story materials suggesting that Ai is the younger sister.\n\nMasatakaP and Electrocutica produced a Windows 8 music video titled \"Through the Window\", featuring Madobe characters Nanami, Yū (in silhouette), and Claudia. The video was presented as the opening to Microsoft's keynote on the second day of Windows Developer Days in Japan.\n\nIn 2012 and 2013, Windows Navi+ (Techno-Alliance Corp.) also created separate Twitter accounts for Ai and Yū, respectively.\n\nTwo theme songs for Yū and Ai – \"Mir8cle Days\" () and \"Donna Mirai Demo\" () were unveiled on 15 June 2013, and sold as a CD bundled with Windows 8 Pro DSP Edition, sold at TwinBox Akihabara.\n\nAkiba PC reported that the first 7777 copies of Japanese Windows 7 Ultimate DSP editions include special wallpaper and sound sets for a character called , voiced by Nana Mizuki. The character was designed by Wakaba. The premium set includes a Windows 7 theme featuring 3 Nanami wallpapers, 19 event sound sets, CD with 5 extra Nanami sounds. Regular DSP edition includes a digest Windows 7 theme including a Nanami wallpaper, an event sound set; the preorder users can also download an extra Nanami wallpaper and 6 event sound sets. This makes it the first OS-\"tan\" marketed by the company producing the operating system. In addition, the character also got its own Twitter account.\n\nDuring the initial sales event of the Windows 7 DSP edition, the official profile of the character has also been revealed. It shows Madobe Nanami was born in 1992-04-06 (release date of Windows 3.1) 17 years of age(at the time of release), who lives in Chiyoda, Tokyo. Nanami is among an extended family of 16 members, and she has elder brother named Goichi (吾一), elder sister named Mutsumi (むつみ), mother named Mikaho (美佳穗) from Madobe (窓辺) family, father named Kyuuachi (究八) from Shirato (白戸) family. Nanami and her cousin Claudia Madobe (クロード(蔵人)) later appeared in Microsoft's Cloud Girl comic strip.\n\nWindows Vista's most distinguishing characteristic is usually her horn-shaped pigtails (some variants have up to four pigtails) and heterochromatic eyes. Silver or white hair appears to be the most frequent, although light blue and black are also seen. A common costume design was a white and red sailor fuku and stockings. Since the release of more details about Vista's interface, her look has changed slightly. A black maid's outfit is now emerging in popularity (which matches the new default Vista color scheme), as well as a circular Windows logo hair clip, identical to the new Start Menu button in Vista. There also seems to be a more finalized version who has a hair colour similar to that of Vista wallpapers, with a range from light blue, to yellow, to green. She also wears a type of long coat (which only covers her left and right sides) which are transparent to imitate that of the Aero glass effect.\n\nXP-\"tan\" is a dark-haired girl with ribbons in her hair and an \"XP\" hair ornament typically worn on the left side. As Windows XP is criticized for bloating a system and being very pretty without being as useful, XP-\"tan\" is commonly depicted wearing tight clothing with big breasts. Additionally, as a reference to the memory usage of Windows XP, she is often seen eating or holding an empty rice bowl labeled \"Memory\". Some variants include a version for XP Home known as \"Homeko\" who has green hair which she wears in a short ponytail with two large XP-shaped hairclips that cover her ears, as well as a less common variation representing Windows XP Media Center Edition. The outfits worn by the two main variants are based on the loading lines at the Windows splash screen during startup.\n\nAlthough a few variants exist, the most common operating system represented is Windows 2000 Professional. She is typically drawn as an intelligent, professional, reserved looking woman with short blue hair, glasses, and hair clips that resemble cat ears flanking a small white bonnet or ruffle, similar to a maid's bonnet, that shows the Windows logo. Her outfit resembles a swimsuit suggesting the Windows logo colors worn with long blue coat, alluding to the popular opinion that Windows 2000 is the most stable and dependable of the Windows operating systems. Due to the greater stability of Win2K compared with WinME, which was released near 2000, 2K-\"tan\" is often described as the guardian of ME-\"tan\". The particular shade of blue used in most drawings is similar to the default Windows 2000 desktop color.\n\nThe design of ME-\"tan\", the personification of Windows Me, is very much in line with the Japanese concept of kawaii or cuteness. Her design has changed little from the artist's original designs and is depicted with green hair in long pigtails wearing a maid outfit with a \"!\" badge on the front reminiscent of the Windows Me Active Desktop Recovery screen, often shown after rebooting from a system crash in Me. While she is considered to be a hard worker, webcomics often depict her failing at anything she tries to do, often literally crashing and irritating her sisters. When she is not frozen or out of control, she tends to do things showing a lack of common sense or knowledge, such as putting a soda can into a microwave oven or defending herself by swinging a Welsh onion.\nWhile many variations exist the most common depiction of the Windows 98 operating systems is a pair of young girls. The OS-\"tan\" representative of the original release of Windows 98 is shown in a white and blue uniform that includes the Windows logo as part of a neck tie, navy blue hair, and a \"98\" hair clip. The Windows 98 Second Edition OS-\"tan\" is similar in appearance, but wears a green sailor school uniform with the letters \"SE\" on the front. Two early representations that are also seen are a pair of stick-limbed Pocky boxes with a face and version number drawn in crayon. This is a reference to Vulcan 300, a character from the \"Zatch Bell!\" anime series. These early representations are still used as a mecha piloted by the girls, dolls carried by the girls, or sometimes even as hiding places for them.\n\nAs Windows 95 is considered to be the oldest of the modern 32-bit Windows operating systems, it is usually represented as a traditional lady from the early modern era of Japan. She is typically depicted as a gentle-looking brown haired woman in a kimono, with a hair ribbon showing the four Windows colors. Her outfit is a traditional kimono and a hakama of Japan and she wears thick sandals, or geta, on her feet. These were a woman college student's typical clothes as seen in the earliest period during the course of the modernization in Japan (from the Meiji period to the Taishō period) and is a reference to the modernization of Windows in comparison to the modernization of Japan. Additionally, the pattern of her kimono is based on the file \"hana256.bmp\", which was used as a desktop wallpaper pattern in the Japanese version of Windows. She is typically depicted as engaged in drinking tea, serving meals or doing other housework. One recurring theme in stories is her unfamiliarity with newer, post Win-95 technologies, such as USB devices (even though the OSR 2.x supported it) and broadband internet connections. She is also occasionally depicted wielding a katana in an aggressive manner, symbolizing that it was with her generation of operating systems that Microsoft finally achieved full dominance of the personal computer market.\n\nWindows 3.1 is a short girl with long silver hair, a long light purple dress, and a large purple bow on her head. She is often seen carrying a small, black cat on her head as well. She acts as a servant, or a maid of some sort, who serves and tends to DOS-\"tan\". This is a reference to the fact that Windows 3.1 is not a full operating system, but rather just a GUI for MS-DOS.\n\nThe Mac OS X girl is often portrayed as a catgirl, following with the Apple \"wild cat\" naming tradition (every Mac OS X release until OS X Mavericks had a codename like Jaguar, Panther, Tiger, Snow Leopard, etc.). Otherwise, she is shown as an older variation of the Mac OS 9 girl, wearing a white coat and wearing an AirPort wireless hub fashioned as a hat. She is occasionally shown holding a publication of some sort, as Macs are often used for desktop publishing.\n\nOriginally seen as a bearded penguin (a reference to Tux, the penguin mascot of the Linux kernel), an image of a girl with a helmet and flippers was chosen as a human alternative. Her helmet (most likely a metaphor for the Linux kernel's oft-claimed excellent security) usually has horns on it, likely a reference to the GNU software which comprises the common system programs present in nearly all Linux distributions. The gear teeth on the helmet are a reference to KDE, a common desktop environment used with GNU/Linux. The on her shirt is a reference to GNOME, another common desktop environment. She is often seen with a spear that has flags attached representing the GRUB, LILO and GCC tools for Linux.\n\nThe artist who drew Konqi, Tyson Tan, submitted a mascot for LibreOffice during a contest run by The Document Foundation in late 2017. The mascot was not officially accepted by the foundation but has developed a fandom on 8chan and reddit.\n\nThe MSX-DOS girl is often portrayed as a young but grey-haired girl carrying a large cartridge-shaped bag with an MSX-DOS logo on it. There is even a short game featuring this OS-\"tan\" as player character. The bag this OS-\"tan\" is often shown holding is cartridge-shaped, likely because MSXDOS2 required an cartridge with an extra 64 kB of ROM in order to work.\n\nSome Americanized versions of Windows related OS-\"tan\", named XP-USA, Me-USA, and 2K-USA, were published in the Ohzora's FanBook in a comic strip named \"Trouble Windows in USA\", by Saint Muscle.\n\nBecause of heavy associations between operating systems and their supporting programs, such as anti-virus clients and Web browsers, many supporting characters have been created to personify the of these applications. Some examples are:\n\n\"Wired News\" rated OS-\"tan\" among the \"Lamest Technology Mascots Ever\", yet \"strangely compelling\".\n\n\n"}
{"id": "6130974", "url": "https://en.wikipedia.org/wiki?curid=6130974", "title": "Optical Storage Technology Association", "text": "Optical Storage Technology Association\n\nThe Optical Storage Technology Association (OSTA) is an international trade association which promotes the use of recordable optical technologies and products, and most notably it is responsible for the creation and maintenance of the UDF specification. Representing more than 85 percent of worldwide writable optical product shipment's manufacturers and resellers, it was incorporated in 1992.\n\nIn the autumn of 2007, OSTA spearheaded a campaign to encourage families and photographers to back up their digital photographs on compact discs. Since it is estimated that one out of seven computer hard drives \"crash\" within the first year, OSTA believes it is dangerous to merely rely on storing irreplaceable pictures on a hard drive alone.\n\n"}
{"id": "2612652", "url": "https://en.wikipedia.org/wiki?curid=2612652", "title": "Pierre de Caters", "text": "Pierre de Caters\n\nBaron Pierre de Caters (Berchem, 25 December 1875 – Paris, 21 March 1944) was a Belgian adventurer, aviator and car and motorboat racer. In 1908, he was the first Belgian to fly an aircraft.\n\nHe was also the first Belgian to receive a pilot's license from the Belgian air club on 2 December 1909 and received a gold medal for the first kilometer in the same year. He was the first aircraft manufacturer in Belgium and the first instructor of military aviation. He also took part in car and motorboat races in Belgium and France. \n\nIn 1904, he briefly held the land speed record, driving a DMG Mercedes Simplex at on a beach course in Ostend, Belgium. \n\nIn World War I he joined Belgian military aviation, commanding the flying school of Étampes.\n\nOn 16 November 1910, de Caters embarked to India with two Aviator airplanes. He was accompanied by Jules Tyck, another Belgian pilot.\nThe city of Bombay refused the organization of an aviation meeting. Then the two airmen traveled to Calcutta with their aircraft crated. In Calcutta, de Caters flew several times from the Club of Tollygunge. On 21 December, he flew for 27 minutes with Mrs. Sen Beil, sister of the Maharaja of Cooch Behar as passenger. One of the Aviators was damaged by fire. On 2 February 1911, de Caters and Tyck flew in Bangalore. They were received by the Maharaja of Mysore. From 16–18 February, Pierre flew from Secunderabad in the Hyderabad state. The Indian tour was completed and Pierre de Caters returned to Europe. A little later Aviator was dissolved and the baron would not take part in competitions any more.\n\n\n"}
{"id": "1108330", "url": "https://en.wikipedia.org/wiki?curid=1108330", "title": "Project CHATTER", "text": "Project CHATTER\n\nProject CHATTER was a United States Navy program beginning in the fall of 1947 focusing on the identification and testing of drugs in interrogations and the recruitment of agents. Their search included laboratory experiments on both animal and human subjects. The program operated under the direction of Charles Savage of the Naval Medical Research Institute, Bethesda, Maryland, from 1947 to 1953. The project was geared to identifying agents both synthetic and natural that were effective during interrogation. The project was centered on, but not restricted to, the use of anabasine (an alkaloid), scopolamine and mescaline. The program ended shortly after the Korean War in 1953, presumably due to limited progress and the success of other projects.\n\n"}
{"id": "4390612", "url": "https://en.wikipedia.org/wiki?curid=4390612", "title": "Protonic ceramic fuel cell", "text": "Protonic ceramic fuel cell\n\nA protonic ceramic fuel cell or PCFC is a fuel cell based on a ceramic electrolyte material that exhibits high protonic conductivity at elevated temperatures.\n\nPCFCs share the thermal and kinetic advantages of high temperature operation at 700 degrees Celsius with molten carbonate and solid oxide fuel cells, while exhibiting all of the intrinsic benefits of proton conduction in proton exchange membrane fuel cells (PEMFC) and phosphoric acid fuel cells (PAFC). The high operating temperature is necessary to achieve very high electrical fuel efficiency with hydrocarbon fuels. PCFCs can operate at high temperatures and electrochemically oxidize fossil fuels directly to the anode. This eliminates the intermediate step of producing hydrogen through the costly reforming process. Gaseous molecules of the hydrocarbon fuel are absorbed on the surface of the anode in the presence of water vapor, and hydrogen atoms are efficiently stripped off to be absorbed into the electrolyte, with carbon dioxide as the primary reaction product. PCFCs have a solid electrolyte, so that the membrane cannot dry out as with PEM fuel cells, and liquid cannot leak out as with PAFCs.\n\nColorado School of Mines is primarily researching this type of fuel cell.\n\n"}
{"id": "47395801", "url": "https://en.wikipedia.org/wiki?curid=47395801", "title": "Readdle", "text": "Readdle\n\nReaddle is a Ukrainian mobile application development company. The company research and development is based in Odesa, Ukraine. The operation is mostly built around the App Store, cumulatively generating over 100 million downloads. The company's two main products are PDF Expert and Spark.\n"}
{"id": "2110428", "url": "https://en.wikipedia.org/wiki?curid=2110428", "title": "Ricing (cooking)", "text": "Ricing (cooking)\n\nRicing is a cooking term meaning to pass food through a food mill or \"ricer\", which comes in several forms. In the most basic, food is pushed or pressured through a metal or plastic plate with many small holes, producing a smoother result than mashing, but coarser than pureeing or passing through a sieve or tamis. The size of the product produced by ricing is about the same as grains of rice. \n\nPotatoes are the most commonly \"riced\" foods, as in the dish riced potatoes, essentially a smoother version of mashed potatoes. However, other vegetables can be 'riced' in order to provide low glycemic, nutrient-rich vegetable dishes, either cooked or raw, with or without sauces and herbs, or in salads. Cooking artfully 'riced' cauliflower, then mashing it before serving, provides a more health-supporting vegetable dish that looks like mashed potatoes but isn't.\n\nThe potato ricer tool forces vegetables, such as potatoes, through a sheet of small holes. It differs in function somewhat from a food mill, which is larger and not held in one hand.\n\n"}
{"id": "267753", "url": "https://en.wikipedia.org/wiki?curid=267753", "title": "RoboCup", "text": "RoboCup\n\nRoboCup is an annual international robotics competition proposed and founded in 1996 (Pre-RoboCup) by a group of university professors (among which Hiroaki Kitano, Manuela M. Veloso, and Minoru Asada). The aim of such a competition consists of promoting robotics and AI research, by offering a publicly appealing, but formidable challenge.\n\n“Competition pushes advances in technologies. What we learn from robots playing soccer or navigating a maze can be applied to industry and help us solve difficult real-world problems,” according to Professor Maurice Pagnucco, Head of the School of Computer Science and Engineering at UNSW.\n\nThe name \"RoboCup\" is a contraction of the competition's full name, \"Robot Soccer World Cup\", but there are many other stages of the competition such as \"RoboCupRescue\", \"RoboCup@Home\" and \"RoboCupJunior\". In 2016, the world's competition was held in Leipzig, Germany. RoboCup 2017 was held in Nagoya, Japan. Professor Itsuki Noda is the current president of RoboCup since 2014.\n\nThe official goal of the project:\n\nThe contest currently has six major competition domains, each with a number of leagues and subleagues:\n\nEach team is fully autonomous in all RoboCup leagues. Once the game starts, the only input from any human is from the referee.\n\nThe formal RoboCup was preceded by the (often unacknowledged) first International Micro Robot World Cup Soccer Tournament (MIROSOT) held by KAIST in Taejon, Korea, in November 1996. This was won by an American team from Newton Labs, and the competition was shown on CNN.\n\n\n12 December - 18 December\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "34771387", "url": "https://en.wikipedia.org/wiki?curid=34771387", "title": "Ross–Fahroo lemma", "text": "Ross–Fahroo lemma\n\nNamed after I. Michael Ross and F. Fahroo, the Ross–Fahroo lemma is a fundamental result in optimal control theory.\n\nIt states that dualization and discretization are, in general, non-commutative operations. The operations can be made commutative by an application of the covector mapping principle.\n\nA continuous-time optimal control problem is information rich. A number of interesting properties of a given problem can be derived by applying the Pontryagin's minimum principle or the Hamilton–Jacobi–Bellman equations. These theories implicitly use the continuity of time in their derivation. \nWhen an optimal control problem is discretized, the Ross–Fahroo lemma asserts that there is a fundamental loss of information. This loss of information can be in the primal variables as in the value of the control at one or both of the boundary points\n\nor in the dual variables as in the value of the Hamiltonian over the time horizon. To address the information loss, Ross and Fahroo introduced the concept of closure conditions which allow the known information loss to be put back in. This is done by an application of the covector mapping principle.\n\nWhen pseudospectral methods are applied to discretize optimal control problems, the implications of the Ross–Fahroo lemma \nappear in the form of the discrete covectors seemingly being discretized by the transpose of the differentiation matrix.\nWhen the covector mapping principle is applied, it reveals the proper transformation for the adjoints. Application of the transformation generates the Ross–Fahroo pseudospectral methods.\n\n"}
{"id": "56871287", "url": "https://en.wikipedia.org/wiki?curid=56871287", "title": "SAE J300", "text": "SAE J300\n\nSAE J300 is a standard that defines the viscometric properties of mono- and multigrade engine oils, maintained by SAE International. Key parameters for engine oil viscometrics are the oil's kinematic viscosity, its high temperature-high shear viscosity measured by the tapered bearing simulator, and low temperature properties measured by the cold-cranking simulator and mini-rotary viscometer. This standard is commonly used throughout the world, and standards organizations that do so include API and ILSAC, and ACEA.\n\nSAE viscosity grades include 0W through 25W in increments of 5W, and 4 through 16 in increments of 4, and 20 through 60 in increments of 10. The former grades denote a multigrade engine oil's low-temperature properties (W denoting \"winter\"), whereas the latter grades denote an engine oil's properties at the operating temperature of an engine. To illustrate, a lawnmower may require SAE 30 monograde engine oil that must meet the SAE 30 requirements. In comparison, a contemporary passenger car engine may require SAE 0W-20 multigrade engine oil that must meet both the SAE 0W and SAE 20 requirements. \n\nThe addition of the SAE 8 through SAE 16 viscosity grades permit improved fuel economy through reduced hydrodynamic friction.\n"}
{"id": "6587771", "url": "https://en.wikipedia.org/wiki?curid=6587771", "title": "Scandinavian design", "text": "Scandinavian design\n\nScandinavian design is a design movement characterized by simplicity, minimalism and functionality that emerged in the early 20th century, and which flourished in the 1950s, in the five Nordic countries of Denmark, Finland, Iceland, Norway, and Sweden.\n\nIn 1914, the Danish \"Selskabet for Dekorativ Kunst\" (Company for Decorative Arts) launched its \"\" (literally \"Graceful Work\") magazine. Its title became the name of a new Danish style of arts and crafts, to rival Art Nouveau and Jugendstil.\n\nFrom the 1930s, designers such as Alvar Aalto (furniture, textiles), Arne Jacobsen (chairs), Borge Mogensen (furniture), Hans J. Wegner (chairs), Verner Panton (plastic chairs), Poul Henningsen (lamps), and Maija Isola (printed textiles) helped to create a \"golden age of Scandinavian design\".\n\nThe Lunning Prize, awarded to outstanding Scandinavian designers between 1951 and 1970, was instrumental in making Scandinavian design a recognized commodity, and in defining its profile.\n\nIn 1954, the Brooklyn Museum held its \"Design in Scandinavia\" exhibition, and a fashion for \"Scandinavian Modern\" furniture began in America. Scandinavian design is by no means limited to furniture and household goods. It has been applied to industrial design, such as of consumer electronics, mobile phones, and cars.\n\nThe concept of Scandinavian design has been the subject of scholarly debate, exhibitions and marketing agendas since the 1950s. Many emphasize the democratic design ideals that were a central theme of the movement and are reflected in the rhetoric surrounding contemporary Scandinavian and international design. Others, however, have analyzed the reception of Scandinavian design abroad, seeing in it a form of myth-making and racial politics.\n\nDanish Design is a style of functionalistic design and architecture that was developed in mid-20th century. Influenced by the German Bauhaus school, many Danish designers used the new industrial technologies, combined with ideas of simplicity and functionalism to design buildings, furniture and household objects, many of which have become iconic and are still in use and production, such as Arne Jacobsen's 1958 Egg chair and Poul Henningsen's 1926 PH-lamps. After the Second World War, conditions in Denmark were ideally suited to success in design. The emphasis was on furniture but architecture, silver, ceramics, glass and textiles also benefitted from the trend. Denmark's late industrialisation combined with a tradition of high-quality craftsmanship formed the basis of gradual progress towards industrial production.\nFinnish design spans clothing, engineering design, furniture, glass, lighting, textiles, and household products. The \"Design from Finland\" mark was created in 2011. Finland's Design Museum (formerly called the Museum of Art and Design) has a collection founded in 1873, while Helsinki's University of Art and Design, established in 1871, now forms part of Aalto University.\n\nProminent Finnish designers include Alvar Aalto (vases, furniture), Aino Aalto (glassware), Kaj Frank (glass, tableware), Klaus Haapaniemi (fabric prints), Simo Heikkilä (furniture), Kristina Isola (textiles), Maija Isola (Marimekko prints), Harri Koskinen (glass, homeware), Mika Piirainen (clothing, accessories), Timo Sarpaneva (glass, homeware), Oiva Toikka (glass art), Tapio Wirkkala (glass art, glassware), Eero Aarnio (plastic furniture), Sanna Annukka (screenprints), Anu Penttinen (glass), Aino-Maija Metsola (textiles, homeware), and Maija Louekari (tableware, homeware).\n\nDesign in Iceland is a relatively young tradition, starting in the 1950s but now growing rapidly. The country's limited options for manufacturing and its constrained choice of materials have both forced designers to be innovative, though wool remains a staple material, whether felted or knitted. Iceland's Museum of Design and Applied Art, aiming to record Icelandic design from 1900 onwards, opened in 1998. The Iceland Academy of the Arts was also founded in 1998, soon followed by its Faculty of Architecture and Design, which has promoted a distinctively Icelandic character in the nation's design.\nNorwegian design has a strong minimalist aesthetic. Designed items include lamps and furniture. Qualities emphasised include durability, beauty, functionality, simplicity, and natural forms.\n\nThe Norwegian Centre for Design and Architecture, \"DogA\", is housed in a former transformer station in Oslo. Norway holds an annual design exhibition called \"100% Norway\" at the London Design Fair.\n\nProminent Norwegian designers include Hans Brattrud (\"Scandia Jr.\", \"Grorudstolen\", the table \"Fagott\", and the armchair \"Comet\"), Sven Ivar Dysthe (armchair \"1001\" from 1960, and the chair \"Laminette\" from 1964), Olav Eldøy (chair Peel, the chair Date & the chair Eight), Olav Haug, Fredrik A. Kayser (Chair “711”) and Ingmar Relling (Orbit and Siesta).\n\nSwedish design is considered minimalist, with an emphasis on functionality and simple clean lines. This has applied especially to furniture. Sweden is known for traditional crafts including glass and Sami handicrafts. Swedish design was pioneered by Anders Beckman (graphics), Bruno Mathsson (furniture), Märta Måås-Fjetterström and Astrid Sampe (textile), and Sixten Sason (industrial). Organisations that promote design in Sweden are Svensk Form, the Swedish society of crafts and design, founded in 1845; the Swedish Industrial Design Foundation, known as SVID; the Swedish Arts Council; and the Swedish Centre for Architecture and Design (known as ArkDes) on the island of Skeppsholmen in Stockholm, beside the modern art museum.\n\n\n"}
{"id": "37316", "url": "https://en.wikipedia.org/wiki?curid=37316", "title": "Scientific journal", "text": "Scientific journal\n\nIn academic publishing, a scientific journal is a periodical publication intended to further the progress of science, usually by reporting new research.\n\nArticles in scientific journals are mostly written by active scientists such as students, researchers and professors instead of professional journalists. There are thousands of scientific journals in publication, and many more have been published at various points in the past (see list of scientific journals). Most journals are highly specialized, although some of the oldest journals such as \"Nature\" publish articles and scientific papers across a wide range of scientific fields. Scientific journals contain articles that have been peer reviewed, in an attempt to ensure that articles meet the journal's standards of quality, and scientific validity. Although scientific journals are superficially similar to professional magazines, they are actually quite different. Issues of a scientific journal are rarely read casually, as one would read a magazine. The publication of the results of research is an essential part of the scientific method. If they are describing experiments or calculations, they must supply enough details that an independent researcher could repeat the experiment or calculation to verify the results. Each such journal article becomes part of the permanent scientific record.\n\nArticles in scientific journals can be used in research and higher education. Scientific articles allow researchers to keep up to date with the developments of their field and direct their own research. An essential part of a scientific article is citation of earlier work. The impact of articles and journals is often assessed by counting citations (citation impact). Some classes are partially devoted to the explication of classic articles, and seminar classes can consist of the presentation by each student of a classic or current paper. Schoolbooks and textbooks have been written usually only on established topics, while the latest research and more obscure topics are only accessible through scientific articles. In a scientific research group or academic department it is usual for the content of current scientific journals to be discussed in journal clubs. Public funding bodies often require the results to be published in scientific journals. Academic credentials for promotion into academic ranks are established in large part by the number and impact of scientific articles published. Many doctoral programs allow for thesis by publication, where the candidate is required to publish a certain number of scientific articles.\n\nArticles tend to be highly technical, representing the latest theoretical research and experimental results in the field of science covered by the journal. They are often incomprehensible to anyone except for researchers in the field and advanced students. In some subjects this is inevitable given the nature of the content. Usually, rigorous rules of scientific writing are enforced by the editors; however, these rules may vary from journal to journal, especially between journals from different publishers. Articles are usually either original articles reporting completely new results or reviews of current literature. There are also scientific publications that bridge the gap between articles and books by publishing thematic volumes of chapters from different authors. Many journals have a regional focus, specializing in publishing papers from a particular geographic region, like \"African Invertebrates\".\n\nThe history of scientific journals dates from 1665, when the French \"Journal des sçavans\" and the English \"Philosophical Transactions of the Royal Society\" first began systematically publishing research results. Over a thousand, mostly ephemeral, were founded in the 18th century, and the number has increased rapidly after that.\n\nPrior to mid-20th century, peer review was not always necessary, but gradually it became essentially compulsory.\n\nThe authors of scientific articles are active researchers instead of journalists; typically, a graduate student or a researcher writes a paper with a professor. As such, the authors are unpaid and receive no compensation from the journal. However, their funding bodies may require them to publish in scientific journals. The paper is submitted to the journal office, where the editor considers the paper for appropriateness, potential scientific impact and novelty. If the journal's editor considers the paper appropriate, the paper is submitted to scholarly peer review. Depending on the field, journal and paper, the paper is sent to 1–3 reviewers for evaluation before they can be granted permission to publish. Reviewers are expected to check the paper for soundness of its scientific argument, i.e. if the data collected or considered in the paper support the conclusion offered. Novelty is also key: existing work must be appropriately considered and referenced, and new results improving on the state of the art presented. Reviewers are usually unpaid and not a part of the journal staff—instead, they should be \"peers\", i.e. researchers in the same field as the paper in question.\n\nThe standards that a journal uses to determine publication can vary widely. Some journals, such as \"Nature\", \"Science\", \"PNAS\", and \"Physical Review Letters\", have a reputation of publishing articles that mark a fundamental breakthrough in their respective fields. In many fields, a formal or informal hierarchy of scientific journals exists; the most prestigious journal in a field tends to be the most selective in terms of the articles it will select for publication, and usually will also have the highest impact factor. In some countries, journal rankings can be utilized for funding decisions and even evaluation of individual researchers, although they are poorly suited for that purpose.\n\nFor scientific journal Reproducibility and Replicability are core concepts that allow other scientist to check and reproduce the results under the same conditions mentioned in the paper or at least similar conditions and produce similar results with similar measurements of the same measurand or carried out under changed conditions of measurement.\n\nThere are several types of journal articles; the exact terminology and definitions vary by field and specific journal, but often include:\n\nThe formats of journal articles vary, but many follow the general IMRAD scheme recommended by the International Committee of Medical Journal Editors. Such articles begin with an \"abstract\", which is a one-to-four-paragraph summary of the paper. The \"introduction\" describes the background for the research including a discussion of similar research. The \"materials and methods\" or \"experimental\" section provides specific details of how the research was conducted. The \"results and discussion\" section describes the outcome and implications of the research, and the \"conclusion\" section places the research in context and describes avenues for further exploration.\n\nIn addition to the above, some scientific journals such as \"Science\" will include a news section where scientific developments (often involving political issues) are described. These articles are often written by science journalists and not by scientists. In addition, some journals will include an editorial section and a section for letters to the editor. While these are articles published within a journal, in general they are not regarded as scientific journal articles because they have not been peer-reviewed.\n\nElectronic publishing is a new area of information dissemination. One definition of electronic publishing is in the context of the scientific journal. It is the presentation of scholarly scientific results in only an electronic (non-paper) form. This is from its first write-up, or creation, to its publication or dissemination. The electronic scientific journal is specifically designed to be presented on the internet. It is defined as not being previously printed material adapted, or retooled, and then delivered electronically.\n\nElectronic publishing will exist alongside paper publishing, because printed paper publishing is not expected to disappear in the future. Output to a screen is important for browsing and searching but is not well adapted for extensive reading. Paper copies of selected information will definitely be required. Therefore, the article has to be transmitted electronically to the reader's local printer. Formats suitable both for reading on paper, and for manipulation by the reader's computer will need to be integrated. Many journals are electronically available in formats readable on screen via web browsers, as well as in portable document format PDF, suitable for printing and storing on a local desktop or laptop computer. New tools such as JATS and Utopia Documents provide a 'bridge' to the 'web-versions' in that they connect the content in PDF versions directly to the WorldWideWeb via hyperlinks that are created 'on-the-fly'. The PDF version of an article is usually seen as the version of record, but the matter is subject to some debate.\n\nElectronic counterparts of established print journals already promote and deliver rapid dissemination of peer reviewed and edited, \"published\" articles. Other journals, whether spin-offs of established print journals, or created as electronic only, have come into existence promoting the rapid dissemination capability, and availability, on the Internet. In tandem with this is the speeding up of peer review, copyediting, page makeup, and other steps in the process to support rapid dissemination.\n\nOther improvements, benefits and unique values of electronically publishing the scientific journal are easy availability of supplementary materials (data, graphics and video), lower cost, and availability to more people, especially scientists from non-developed countries. Hence, research results from more developed nations are becoming more accessible to scientists from non-developed countries.\n\nMoreover, electronic publishing of scientific journals has been accomplished without compromising the standards of the refereed, peer review process.\n\nOne form is the online equivalent of the conventional paper journal. By 2006, almost all scientific journals have, while retaining their peer-review process, established electronic versions; a number have moved entirely to electronic publication. In similar manner, most academic libraries buy the electronic version, and purchase a paper copy only for the most important or most-used titles.\n\nThere is usually a delay of several months after an article is written before it is published in a journal, making paper journals not an ideal format for announcing the latest research. Many journals now publish the final papers in their electronic version as soon as they are ready, without waiting for the assembly of a complete issue, as is necessary with paper. In many fields in which even greater speed is wanted, such as physics, the role of the journal at disseminating the latest research has largely been replaced by preprint databases such as arXiv.org. Almost all such articles are eventually published in traditional journals, which still provide an important role in quality control, archiving papers, and establishing scientific credit.\n\nMany scientists and librarians have long protested the cost of journals, especially as they see these payments going to large for-profit publishing houses. To allow their researchers online access to journals, many universities purchase \"site licenses\", permitting access from anywhere in the university, and, with appropriate authorization, by university-affiliated users at home or elsewhere. These may be quite expensive, sometimes much more than the cost for a print subscription, although this may reflect the number of people who will be using the license—while a print subscription is the cost for one person to receive the journal; a site-license can allow thousands of people to gain access.\n\nPublications by scholarly societies, also known as not-for-profit-publishers, usually cost less than commercial publishers, but the prices of their scientific journals are still usually several thousand dollars a year. In general, this money is used to fund the activities of the scientific societies that run such journals, or is invested in providing further scholarly resources for scientists; thus, the money remains in and benefits the scientific sphere.\n\nDespite the transition to electronic publishing, the serials crisis persists.\n\nConcerns about cost and open access have led to the creation of free-access journals such as the Public Library of Science (PLoS) family and partly open or reduced-cost journals such as the \"Journal of High Energy Physics\". However, professional editors still have to be paid, and PLoS still relies heavily on donations from foundations to cover the majority of its operating costs; smaller journals do not often have access to such resources.\n\nBased on statistical arguments, it has been shown that electronic publishing online, and to some extent open access, both provide wider dissemination and increase the average number of citations an article receives.\n\nTraditionally, the author of an article was required to transfer the copyright to the journal publisher. Publishers claimed this was necessary in order to protect authors' rights, and to coordinate permissions for reprints or other use. However, many authors, especially those active in the open access movement, found this unsatisfactory, and have used their influence to effect a gradual move towards a license to publish instead. Under such a system, the publisher has permission to edit, print, and distribute the article commercially, but the authors retain the other rights themselves.\n\nEven if they retain the copyright to an article, most journals allow certain rights to their authors. These rights usually include the ability to reuse parts of the paper in the author's future work, and allow the author to distribute a limited number of copies. In the print format, such copies are called reprints; in the electronic format, they are called postprints. Some publishers, for example the American Physical Society, also grant the author the right to post and update the article on the author's or employer's website and on free e-print servers, to grant permission to others to use or reuse figures, and even to reprint the article as long as no fee is charged. The rise of open access journals, in which the author retains the copyright but must pay a publication charge, such as the Public Library of Science family of journals, is another recent response to copyright concerns.\n\n\n"}
{"id": "24380244", "url": "https://en.wikipedia.org/wiki?curid=24380244", "title": "Spar and membrane structure", "text": "Spar and membrane structure\n\nSpar and Membrane Structure (SMS) is a type of wall construction in which lightly reinforced 2\" - 3\" [5 cm - 8 cm] of shotcrete or gunite skins (membranes) are interconnected with extended “X” shaped light rebar (spars) through the interior of the wall. Typically, SMS construction uses straw bales as leave in formwork for the concrete skins with the spars placed at the head of each bale, however, many other types of formwork could be used in place of straw bales. The SMS system was invented and patented by R. Gary Black, an engineer and professor of architecture at the University of California, Berkeley.\n\n"}
{"id": "45569620", "url": "https://en.wikipedia.org/wiki?curid=45569620", "title": "Stanford DASH", "text": "Stanford DASH\n\nStanford DASH was a cache coherent multiprocessor developed in the late 1980s by a group led by Anoop Gupta, John L. Hennessy, Mark Horowitz, and Monica S. Lam at Stanford University. It was based on adding a pair of directory boards designed at Stanford to up to 16 SGI IRIS 4D Power Series machines and then cabling the systems in a mesh topology using a Stanford-modified version of the Torus Routing Chip. The boards designed at Stanford implemented a directory-based cache coherence protocol allowing Stanford DASH to support distributed shared memory for up to 64 processors. Stanford DASH was also notable for both supporting and helping to formalize weak memory consistency models, including release consistency. Because Stanford DASH was the first operational machine to include scalable cache coherence, it influenced subsequent computer science research as well as the commercially available SGI Origin 2000. Stanford DASH is included in the 25th anniversary retrospective of selected papers from the International Symposium on Computer Architecture and several computer science books, has been simulated by the University of Edinburgh, and is used as a case study in contemporary computer science classes.\n"}
{"id": "7760965", "url": "https://en.wikipedia.org/wiki?curid=7760965", "title": "Steel building", "text": "Steel building\n\nA steel building is a metal structure fabricated with steel for the internal support and for exterior cladding, as opposed to steel framed buildings which generally use other materials for floors, walls, and external envelope. Steel buildings are used for a variety of purposes including storage, work spaces and living accommodation. They are classified into specific types depending on how they are used.\n\nSteel buildings first gained popularity in the early 20th century. Their use became more widespread during World War II and significantly expanded after the war when steel became more available. Steel buildings have been widely accepted, in part due to cost efficiency. The range of application has expanded with improved materials, products and design capabilities with the availability of computer aided design software.\n\nSteel provides several advantages over other building materials, such as wood:\n\n\nSome common types of steel buildings are \"straight-walled\" and \"arch,\" or Nissen or Quonset hut. Further, the structural type may be classed as clear span or multiple span. A clear span building does not have structural supports (e.g. columns) in the interior occupied space.\n\nStraight-walled and arch type refer to the outside shape of the building. More generally, these are both structural arch forms if they rely on a rigid frame structure. However, curved roof structures are typically associated with the arch term.\n\nSteel arch buildings may be cost efficient for specific applications. They are commonly used in the agricultural industry. Straight-walled buildings provide more usable space when compared to arch buildings. They are also easier to blend into existing architecture. Straight-walled buildings are commonly used for commercial, industrial, and many other occupancy types. \n\nClear span refers to the internal construction. Clear span steel buildings utilize large overhead support beams, thus reducing the need for internal supporting columns. Clear span steel buildings tend to be less cost efficient than structures with interior columns. However, other practical considerations may influence the selection of framing style such as an occupancy where interior structural obstructions are undesirable (e.g. aircraft hangars or sport arenas).\n\nLong Bay buildings are designed for use in bay spans of over 35'. They use prefabricated metal frames combined with conventional joists to provide larger openings and clearances in buildings.\n\nBuilding portions that are shop assembled prior to shipment to site are commonly referenced as prefabricated. The smaller steel buildings tend to be prefabricated or simple enough to be constructed by anyone. Prefabrication offers the benefits of being less costly than traditional methods and is more environmentally friendly (since no waste is produced on-site). The larger steel buildings require skilled construction workers, such as ironworkers, to ensure proper and safe assembly.\n\nThere are five main types of structural components that make up a steel frame - tension members, compression members, bending members, combined force members and their connections. Tension members are usually found as web and chord members in trusses and open web steel joists. Ideally tension members carry tensile forces, or pulling forces, only and its end connections are assumed to be pinned. Pin connections prevent any moment(rotation) or shear forces from being applied to the member. \nCompression members are also considered as columns, struts, or posts. They are vertical members or web and chord members in trusses and joists that are in compression or being squished. \nBending members are also known as beams, girders, joists, spandrels, purlins, lintels, and girts. Each of these members have their own structural application, but typically bending members will carry bending moments and shear forces as primary loads and axial forces and torsion as secondary loads. \nCombined force members are commonly known as beam-columns and are subjected to bending and axial compression.\nConnections are what bring the entire building together. They join these members together and must ensure that they function together as one unit.\n\n"}
{"id": "49642031", "url": "https://en.wikipedia.org/wiki?curid=49642031", "title": "Surface activated bonding", "text": "Surface activated bonding\n\nSurface activated bonding (SAB) is a low temperature wafer bonding technology with atomically clean and activated surfaces. Surface activation prior to bonding by using fast atom bombardment is typically employed to clean the surfaces. High strength bonding of semiconductor, metal, and dielectric can be obtained even at room temperature.\n\nIn the standard SAB method, wafer surfaces are activated by argon fast atom bombardment in ultra-high vacuum (UHV) of 10–10 Pa. The bombardment removes adsorbed contaminants and native oxides on the surfaces. The activated surfaces are atomically clean and reactive for formation of direct bonds between wafers when they are brought into contact even at room temperature.\n\nThe SAB method has been studied for bonding of various materials, as shown in Table I. \n\nThe standard SAB, however, failed to bond some materials such as SiO and polymer films. The modified SAB was developed to solve this problem, by using a sputtering deposited Si intermediate layer to improve the bond strength.\n\nThe combined SAB has been developed for SiO-SiO and Cu/SiO hybrid bonding, without use of any intermediate layer.\n"}
{"id": "9062245", "url": "https://en.wikipedia.org/wiki?curid=9062245", "title": "T-criterion", "text": "T-criterion\n\nThe T-failure criterion is a set of material failure criteria that can be used to predict both brittle and ductile failure. \nThese criteria were designed as a replacement for the von Mises yield criterion which predicts the unphysical result that pure hydrostatic tensile loading of metals never leads to failure. The T-criteria use the volumetric stress in addition to the deviatoric stress used by the von Mises criterion and are similar to the Drucker Prager yield criterion. T-criteria have been designed on the basis of energy considerations and the observation that the reversible elastic energy density storage process has a limit which can be used to determine when a material has failed. \n\nThe strain energy density stored in the material and calculated by the area under the formula_1-formula_2 curve represents the total amount of energy stored in the material only in the case of pure shear. In all other cases, there is a divergence between the actual and calculated stored energy in the material, which is maximum in the case of pure hydrostatic loading, where, according to the von Mises criterion, no energy is stored. This paradox is resolved if a second constitutive equation is introduced, that relates hydrostatic pressure p with the volume change formula_3. These two curves, namely formula_4 and (p-formula_3) are essential for a complete description of material behaviour up to failure. Thus, two criteria must be accounted for when considering failure and two constitutive equations that describe material response. According to this criterion, an upper limit to allowable strains is set either by a critical value Τ of the elastic energy density due to volume change (dilatational energy) or by a critical value Τ of the elastic energy density due to change in shape (distortional energy). The volume of material is considered to have failed by extensive plastic flow when the distortional energy Τ reaches the critical value Τ or by extensive dilatation when the dilatational energy Τ reaches a critical value Τ. The two critical values Τ and Τ are considered material constants independent of the shape of the volume of material considered and the induced loading, but dependent on the strain rate and temperature.\n\nFor the development of the criterion, a continuum mechanics approach is adopted. The material volume is considered to be a continuous medium with no particular form or manufacturing defect. It is also considered to behave as a linear elastic isotropically hardening material, where stresses and strains are related by the generalised Hook’s law and by the incremental theory of plasticity with the von Mises flow rule. For such materials, the following assumptions are considered to hold:\n(a) The total increment of a strain component formula_6 is decomposed into the elastic and the plastic formula_7 increment and formula_8 respectively:\nformula_9 (1)\n(b) The elastic strain increment formula_7 is given by Hooke’s law:\nformula_11(2)\nwhere formula_12the shear modulus, formula_13 the Poisson’s ratio and formula_14 the Krönecker delta.\n(c) The plastic strain increment formula_8 is proportional to the respective deviatoric stress:\nformula_16(3)\nwhere formula_17 and formula_18 an infinitesimal scalar. (3) implies that the plastic strain increment:\n(d) The increment in plastic work per unit volume using (4.16) is:\nformula_19 (4)\nand the increment in strain energy, formula_20, equals to the total differential of the potential formula_21:\nformula_22(5)\nwhere\nformula_23, formula_24 and for metals following the von Mises yield law, by definition\nformula_25(6)\nformula_26(7)\nare the equivalent stress and strain respectively.\nIn (5) the first term of the right hand side, formula_27 is the increment in elastic energy for unit volume change due to hydrostatic pressure. Its integral over a load path is the total amount of dilatational strain energy density stored in the material. The second term formula_28 is the energy required for an infinitesimal distortion of the material. The integral of this quantity is the distortional strain energy density. The theory of plastic flow permits the evaluation of stresses, strains and strain energy densities along a path provided that formula_18 in (3) is known. In elasticity, linear or nonlinear, formula_18. In the case of strain hardening materials, formula_18 can be evaluated by recording the formula_32 curve in a pure shear experiment. The hardening function after point “y” in Figure 1 is then:\nformula_33(8)\nand the infinitesimal scalar formula_18 is:\nformula_35 (9)\nwhere formula_36is the infinitesimal increase in plastic work (see Figure 1). The elastic part of the total distortional strain energy density is:\nformula_37 (10)\nwhere formula_38 is the elastic part of the equivalent strain. When there is no nonlinear elastic behaviour, by integrating (4.22) the elastic distortional strain energy density is:\nformula_39 (11)\nSimilarly, by integrating the increment in elastic energy for unit volume change due to hydrostatic pressure, formula_40, the dilatational strain energy density is:\nformula_41 (12)\nassuming that the unit volume change\" formula_42 is the elastic straining, proportional to the hydrostatic pressure, p (Figure 2):formula_43 or formula_44 (13)\nwhere formula_23, formula_24 and formula_47 the bulk modulus of the material. \nIn summary, in order to use (12) and (13) to determine the failure of a material volume, the following assumptions hold:\n\nThe criterion will not predict any failure due to distortion for elastic-perfectly plastic, rigid-plastic, or strain softening materials. For the case of nonlinear elasticity, appropriate calculations for the integrals in and (12) and (13) accounting for the nonlinear elastic material properties must be performed. The two threshold values for the elastic strain energy formula_48 and formula_49 are derived from experimental data. A drawback of the criterion is that elastic strain energy densities are small and comparatively hard to derive. Nevertheless, example values are presented in the literature as well as applications where the T-criterion appears to perform quite well.\n"}
{"id": "913620", "url": "https://en.wikipedia.org/wiki?curid=913620", "title": "Touchdown polymerase chain reaction", "text": "Touchdown polymerase chain reaction\n\nThe touchdown polymerase chain reaction or touchdown style polymerase chain reaction is a method of polymerase chain reaction by which primers avoid amplifying nonspecific sequences. The annealing temperature during a polymerase chain reaction determines the specificity of primer annealing. The melting point of the primer sets the upper limit on annealing temperature. At temperatures just above this point, only very specific base pairing between the primer and the template will occur. At lower temperatures, the primers bind less specifically. Nonspecific primer binding obscures polymerase chain reaction results, as the nonspecific sequences to which primers anneal in early steps of amplification will \"swamp out\" any specific sequences because of the exponential nature of polymerase amplification.\n\nThe earliest steps of a touchdown polymerase chain reaction cycle have high annealing temperatures. The annealing temperature is decreased in increments for every subsequent set of cycles (the number of individual cycles and increments of temperature decrease is chosen by the experimenter). The primer will anneal at the highest temperature which is least-permissive of nonspecific binding that it is able to tolerate. Thus, the first sequence amplified is the one between the regions of greatest primer specificity; it is most likely that this is the sequence of interest. These fragments will be further amplified during subsequent rounds at lower temperatures, and will outcompete the nonspecific sequences to which the primers may bind at those lower temperatures. If the primer initially (during the higher-temperature phases) binds to the sequence of interest, subsequent rounds of polymerase chain reaction can be performed upon the product to further amplify those fragments. Touchdown increases specificity of the reaction at higher temperatures and increases the efficiency towards the end by lowering the annealing temperature.\n"}
{"id": "297066", "url": "https://en.wikipedia.org/wiki?curid=297066", "title": "Transistor radio", "text": "Transistor radio\n\nA transistor radio is a small portable radio receiver that uses transistor-based circuitry. Following their development in 1954, made possible by the invention of the transistor in 1947, they became the most popular electronic communication device in history, with billions manufactured during the 1960s and 1970s. Their pocket size sparked a change in popular music listening habits, allowing people to listen to music anywhere they went. Beginning in the 1980s, however, cheap AM transistor radios were superseded by devices with higher audio quality such as portable CD players, personal audio players, boomboxes, and (eventually) smartphones, some of which contain radios themselves.\n\nBefore the transistor was invented, radios used vacuum tubes. Although portable vacuum tube radios were produced, they were typically bulky and heavy. The need for a low voltage high current source to power the filaments of the tubes and high voltage for the anode potential typically required two batteries. Vacuum tubes were also inefficient and fragile compared to transistors, and had a limited lifetime.\n\nBell Laboratories demonstrated the first transistor on December 23, 1947. The scientific team at Bell Laboratories responsible for the solid-state amplifier included William Shockley, Walter Houser Brattain, and John Bardeen. After obtaining patent protection, the company held a news conference on June 30, 1948, at which a prototype transistor radio was demonstrated.\n\nThere are many claimants to the title of the first company to produce practical transistor radios, often incorrectly attributed to Sony (originally Tokyo Telecommunications Engineering Corporation). Texas Instruments had demonstrated all-transistor AM (amplitude modulation) radios as early as May 25, 1954, but their performance was well below that of equivalent vacuum tube models. A workable all-transistor radio was demonstrated in August 1953 at the Düsseldorf Radio Fair by the German firm Intermetall. It was built with four of Intermetall's hand-made transistors, based upon the 1948 invention of the \"Transistron\"-germanium point-contact transistor by Herbert Mataré and Heinrich Welker. However, as with the early Texas Instruments units (and others) only prototypes were ever built; it was never put into commercial production. RCA had demonstrated a prototype transistor radio as early as 1952, and it is likely that they and the other radio makers were planning transistor radios of their own, but Texas Instruments and Regency Division of I.D.E.A., were the first to offer a production model starting in October 1954.\nThe use of transistors instead of vacuum tubes as the amplifier elements meant that the device was much smaller, required far less power to operate than a tube radio, and was more shock-resistant. Since the transistor base draws current, its input impedance is low in contrast to the high input impedance of the vacuum tubes. It also allowed \"instant-on\" operation, since there were no filaments to heat up. The typical portable tube radio of the fifties was about the size and weight of a lunchbox, and contained several heavy, non-rechargeable batteries— one or more so-called \"A\" batteries to heat the tube filaments and a large 45- to 90-volt \"B\" battery to power the signal circuits. By comparison, the transistor radio could fit in a pocket and weighed half a pound or less, and was powered by standard flashlight batteries or a single compact 9-volt battery. The now-familiar 9-volt battery was introduced for powering transistor radios.\n\nListeners sometimes held an entire transistor radio directly against the side of the head, with the speaker against the ear, to minimize the \"tinny\" sound caused by the high resonant frequency of its small speaker. Most radios included earphone jacks and came with single earphones that provided only mediocre-quality sound reproduction. To consumers familiar with the earphone-listening experience of the transistor radio, the first Sony Walkman cassette player, with a pair of high-fidelity stereo earphones, would provide a greatly contrasting display of audio fidelity.\n\nTwo companies working together, Texas Instruments of Dallas, Texas and Industrial Development Engineering Associates (I.D.E.A.) of Indianapolis, Indiana, were behind the unveiling of the Regency TR-1, the world's first commercially produced transistor radio. Previously, Texas Instruments was producing instrumentation for the oil industry and locating devices for the U.S. Navy and I.D.E.A. built home television antenna boosters. The two companies worked together on the TR-1, looking to grow revenues for their respective companies by breaking into this new product area. In May 1954, Texas Instruments had designed and built a prototype and was looking for an established radio manufacturer to develop and market a radio using their transistors. (The Chief Project Engineer for the radio design at Texas Instruments' headquarters in Dallas, Texas was Paul D. Davis, Jr., who had a degree in Electrical Engineering from Southern Methodist University. He was assigned the project due to his experience with radio engineering in World War II.) None of the major radio makers including RCA, Philco, and Emerson were interested. The President of I.D.E.A. at the time, Ed Tudor, jumped at the opportunity to manufacture the TR-1, predicting sales of the transistor radios at \"20 million radios in three years\". The Regency TR-1 was announced on October 18, 1954 by the Regency Division of I.D.E.A., was put on sale in November 1954, and was the first practical transistor radio made in any significant numbers. \"Billboard\" reported in 1954 that \"the radio has only four transistors. One acts as a combination mixer-oscillator, one as an audio amplifier, and two as intermediate-frequency amplifiers.\" One year after the release of the TR-1 sales approached the 100,000 mark. The look and size of the TR-1 was well received, but the reviews of the TR-1's performance were typically adverse. The Regency TR-1 is patented by Richard C. Koch, , former Project Engineer of I.D.E.A.\n\nIn February 1955 the second transistor radio, the 8-TP-1, was introduced by Raytheon. It was a larger portable transistor radio, including an expansive four-inch speaker and four additional transistors (the TR-1 used only four). As a result, the sound quality was much better than the TR-1. An additional benefit of the 8-TP-1 was its efficient battery consumption. In July 1955, the first positive review of a transistor radio appeared in the Consumer Reports that said, \"The transistors in this set have not been used in an effort to build the smallest radio on the market, and good performance has not been sacrificed.\" Following the success of the 8-TP-1, Zenith, RCA, DeWald, and Crosley began flooding the market with additional transistor radio models.\n\nChrysler and Philco announced that they had developed and produced the world's first all-transistor car radio in the April 28th 1955 edition of the Wall Street Journal. Chrysler made the all-transistor car radio, Mopar model 914HR, available as an \"option\" in fall 1955 for its new line of 1956 Chrysler and Imperial cars, which hit the showroom floor on October 21, 1955. The all-transistor car radio was a $150 option.\n\nPrior to the Regency TR-1, transistors were difficult to produce. Only one in five transistors that were produced worked as expected (only a 20% yield) and as a result the price remained extremely high. When it was released in 1954, the Regency TR-1 cost $49.95 (equivalent to $ today) and sold about 150,000 units. Raytheon and Zenith Electronics transistor radios soon followed and were priced even higher. In 1955, Raytheon's 8-TR-1 was priced at $80 (equivalent to $ today). By November 1956 a transistor radio small enough to wear on the wrist and a claimed battery life of 100 hours cost $29.95. Sony's TR-63, released in December 1957 cost $39.95 (equivalent to $ today). Following the success of the TR-63 Sony continued to make their transistor radios smaller. Because of the extremely low labor costs in Japan, Japanese transistor radios began selling for as low as $25. In 1962 American manufacturers dropped prices of transistor radios to as low as $15 (equivalent to $ today).\n\nWhile on a trip to the United States in 1952, Masaru Ibuka, founder of Tokyo Telecommunications Engineering Corporation (now Sony), discovered that AT&T was about to make licensing available for the transistor. Ibuka and his partner, physicist Akio Morita, convinced the Japanese Ministry of International Trade and Industry (MITI) to finance the $25,000 licensing fee (equivalent to $ today). For several months Ibuka traveled around the United States borrowing ideas from the American transistor manufacturers. Improving upon the ideas, Tokyo Telecommunications Engineering Corporation made its first functional transistor radio in 1954. Within five years, Tokyo Telecommunications Engineering Corporation grew from seven employees to approximately five hundred.\n\nOther Japanese companies soon followed their entry into the American market and the grand total of electronic products exported from Japan in 1958 increased 2.5 times in comparison to 1957.\n\nIn August 1955, while still a small company, Tokyo Telecommunications Engineering Corporation introduced their TR-55 five-transistor radio under the new brand name Sony. With this radio, Sony became the first company to manufacture the transistors and other components they used to construct the radio. The TR-55 was also the first transistor radio to utilize all miniature components. It is estimated that only 5,000 to 10,000 units were produced.\n\nThe TR-63 was introduced by Sony to the United States in December 1957. The TR-63 was 1/4\" narrower and 1/2\" shorter than the original Regency TR-1. Like the TR-1 it was offered in four colors: lemon, green, red, and black. In addition to its smaller size, the TR-63 had a small tuning capacitor and required a new battery design to produce the proper voltage. It used the nine-volt battery, which would become the standard for transistor radios. Approximately 100,000 units of the TR-63 were imported in 1957. This \"pocketable\" (the term \"pocketable\" was a matter of some interpretation, as Sony allegedly had special shirts made with oversized pockets for their salesmen) model proved highly successful. With the visible success of the TR-63 Japanese competitors such as Toshiba and Sharp joined the market. By 1959, in the United States market, there were more than six million transistor radio sets produced by Japanese companies that represented $62 million in revenue.\n\nTransistor radios were extremely successful because of three social forces — a large number of young people due to the post–World War II baby boom, a public with disposable income amidst a period of prosperity, and the growing popularity of rock 'n' roll music. The influence of the transistor radio during this period is shown by its appearance in popular films, songs, and books of the time, such as the movie \"Lolita\". \nIn the late 1950s, transistor radios took on more elaborate designs as a result of heated competition. Eventually, transistor radios doubled as novelty items. The small components of transistor radios that became smaller over time were used to make anything from \"Jimmy Carter Peanut-shaped\" radios to \"Gun-shaped\" radios to \"Mork from Ork Eggship-shaped\" radios. Corporations used transistor radios to advertise their business. \"Charlie the Tuna-shaped\" radios could be purchased from Star-Kist for an insignificant amount of money giving their company visibility amongst the public. These novelty radios are now bought and sold as collectors' items amongst modern day collectors.\n\nSince 1980, the popularity of portable radios has declined with the rise of portable audio players and smartphones, which allow users to carry and listen to the music of their choosing and may also include a radio tuner. This began in the late 1970s with boom boxes and portable cassette players such as the Sony Walkman, followed by portable CD players. A common type now is the portable digital audio player. This type of device is a popular choice with listeners who are dissatisfied with terrestrial music radio because of a limited selection of music and reception problems. However, transistor radios are still popular for news, talk radio, weather, live sporting events and emergency alert applications.\n\n\n\n"}
