{"id": "58198099", "url": "https://en.wikipedia.org/wiki?curid=58198099", "title": "5G Automotive Association", "text": "5G Automotive Association\n\nThe 5G Automotive Association (5GAA) is an international, global, cross-industry organization of companies from the automotive, technology, and telecommunications industries. Its goal is, to develop end-to-end solutions for future mobility and transportation services, so avoiding incompatibility problems from the beginning.\n\nThe 5G Automotive Association was created on September 2016, the by AUDI AG, BMW Group, Daimler AG from the side of car makers, Ericsson, Huawei, Intel, Nokia, as producers of telecommunications equipment and Qualcomm as an firmware manufacturer\n\n. In 2017 the 5G Automotive Association signed a Letter of Intent with the European Automotive Telecom Alliance (EATA) for cooperation.\n\nThe 5GAA is a registered voluntary association\n. It has a hierarchical structure:\n\nthere are 12 top (platinium) members, including the founders, prime (gold) members, having the right to propose members for leading positions and general members\n\nIn 2018, the 5G Automotive Association has more than 80 member companies.\n\nThe 5G Automotive Association works for the standardization needed for the implementation of driverless, autonomous driving in cooperation with standards organizations such as ETSI, 3GPP and SAE\nA second task is the information of the public on the emerging technology\n\n\n"}
{"id": "6755397", "url": "https://en.wikipedia.org/wiki?curid=6755397", "title": "AN/MPN-14K Mobile Ground Approach System", "text": "AN/MPN-14K Mobile Ground Approach System\n\nThe AN/MPN-14K Mobile Ground Approach System can be configured as a complete Radar Approach Control (RAPCON) or Ground Controlled Approach (GCA) facility.\n\nThe radar unit is used by air traffic controllers to identify, sequence, and separate participating aircraft, provide final approach guidance through air defense corridors and zones, and coordinate ID and intent with local air defense units at assigned airports and air bases. These services can be provided in all types of weather.\n\nThe radar unit is capable of identifying aircraft using secondary radar up to a radius and primary radar coverage to . The PAR provides both azimuth and elevation information from 15 NM to touchdown. Both the PAR and ASR can be used as final approach aids. The unit has three ASR display indicators and one PAR indicator located in the operations shelter, and one each ASR and PAR indicator located in the maintenance shelter. Complete operations are conducted from the operations trailer. The system is limited to a single runway but has the capability of providing opposite direction runway operations with the aid of a transportable turntable.\n\n235th ATCS Equipment Accessed August 29, 2006.\n"}
{"id": "24445978", "url": "https://en.wikipedia.org/wiki?curid=24445978", "title": "AV.link", "text": "AV.link\n\nAV.link, also known under the trade names nexTViewLink, SmartLink, Q-Link, EasyLink, etc., is a protocol to carry control information between audio-visual devices connected via the SCART (EIA Multiport) connector.\n\nIt is standardised as CENELEC EN 50157-1.\n\nThe Consumer Electronics Control (CEC) communication channel in HDMI and PDMI is based on AV.link.\n\nAV.Link uses a single wire in an open collector configuration. It is passively pulled up to 3 or 3.3 V, and may be pulled down by any device on the bus. Total bus capacitance is a maximum of 7300 pF (ten devices at 100 pF each, plus nine cables at 700 pF), and signal transitions are correspondingly slow: 333 bit/s, with 50 μs fall time and 250 μs rise time.\n\nEach bit transferred begins with a falling edge. The duration of the low period determines the value.\n\nData bits are 2.4±0.35 ms long, with 1 bits having a low period of 0.6±0.2 ms, and 0 bits having a low period of 1.5±0.2 ms. Receivers observe the data line at 1.05±0.2 ms after the falling edge to determine the bit's value.\n\nEvery message begins with a special start bit, 4.5±0.2 ms long, with a low period of 3.7±0.2 ms.\n\nA transmitter must listen to the bus as it transmits; the receiver may hold it low, turning a transmitted 1 bit into a 0 bit. This is done, for example, to acknowledge a transmission.\n\nIf a receiver detects an error in the received data, it holds the bus low for 3.6±0.24 ms; this causes the transmitter to abort the message and retry from the beginning.\n\nA message consists of a start bit, followed by a series of data bytes. Each byte is actually transmitted as 10 bits:\n\nEach message begins with an address byte specifying the 4-bit initiator and recipient addresses. If two initiators begin transmitting at the same time, one of them will transmit a 0 bit while the other transmits a 1 bit, and the latter will observe the conflict and cease transmitting until the bus is idle again. (Note that it must be prepared for the case that the incoming message is addressed to it.)\n\nAn address byte sent with EOM=1 is a simple \"ping\" to check if the addressed device exists and is powered on. Otherwise, it is followed by an opcode byte, and parameters as required by the opcode.\n\nWhen a device is powered on, it chooses an address and sends a ping to see if that address is claimed by another device. If no acknowledge is received, the address is free and may be kept. Otherwise, the device tries another address.\n\n\n"}
{"id": "52294302", "url": "https://en.wikipedia.org/wiki?curid=52294302", "title": "Anna Boyksen", "text": "Anna Boyksen\n\nAnna Helene Boyksen is known as the first female undergraduate engineering student at Technical University of Munich (TUM) (German: Technische Universität München). Anna Boyksen was born on August 11, 1881, in Havendorfersand, in the Grand Duchy of Oldenberg, to Dietrich Anton Boyksen, a merchant, and his wife Mathilde, née Lubben. In her curriculum vitae, Boyksen claimed her nationality as Bavarian and religion as Evangelical.\n\nin 1906, Anna Boyksen was the first woman to enroll in TUM (at the time called THM) in electrical engineering, completing her intermediate exams in 1908. She then went on to pursue studies in economics and law: in 1911, she defended her dissertation, titled \"The German Stock Exchange Regulations. A Comparative Representation\" (German: \"Die deutschen Börsenordnungen. Eine vergleichende Darstellung.\"), at the University of Erlangen, under her married name, Anna Helene Koch.\n\nThe Anna Boyksen Diversity Research Center at TUM \"explores human diversity and the opportunities of diversity for society. Its work focuses on a question often overlooked in Germany: How can the natural, engineering and life sciences benefit from a more diverse community culture?\"\n\nThe prestigious Anna Boyksen Fellowship has been offered by the Technical University of Munich - Institute for Advanced Study (TUM-IAS) since 2014.The Fellowship is granted to outstanding international scholars and researchers who wish to probe gender / diversity-related problems in the Natural and Engineering Sciences, in collaboration with TUM researchers. The two-year Fellowship was created to help advance TUM's goal to become \"Germany's most attractive university for women\" and to foster a productive and durable exchange of ideas and solutions on an international level.\n"}
{"id": "35449083", "url": "https://en.wikipedia.org/wiki?curid=35449083", "title": "BlackBerry Porsche Design P'9981", "text": "BlackBerry Porsche Design P'9981\n\nThe BlackBerry Porsche Design P'9981 is a high-end luxurious smartphone by Research In Motion and Porsche Design. Released in December 2011, it resembles the latest model of the Bold 9900, even sharing most of its hardware components. They both share the same Qualcomm Snapdragon MSM8655 CPU clocked at 1.2 GHz, 768 MB of RAM, a TFT capacitive touchscreen (built on 88µm pixel) capable of multi-touch with a resolution of 640 x 480 pixels and the same 5.0-megapixel EDOF rear camera capable of 720p HD video recording with and an LED flash. The major visual differences, are with the styling which include a unibody full stainless steel frame and authentic leather rear door. Porsche Design changed the aesthetics of the device by adding a uniquely styled metal QWERTY keyboard laid across four straight rows that are set into the steel frame, each row of keys divided by a space, along with custom menu buttons.\n\nIt runs the BlackBerry OS 7 upgradeable to 7.1 and features a custom user interface, modified by Porsche Design. Aside from this, the software contained within the device itself is parallel to that of the Bold 9900.\n\n"}
{"id": "22280550", "url": "https://en.wikipedia.org/wiki?curid=22280550", "title": "Building energy rating", "text": "Building energy rating\n\nA building energy rating (BER) is similar to the energy rating label for a household electrical appliance (examples of which include the U.S. EnergyGuide, the European Union energy label, and the Australia/New Zealand energy rating label). The label has a scale of A-G, with A-rated buildings the most energy efficient and G the least. In relation to dwellings, the Sustainable Energy Authority of Ireland states that \"A BER is an indication of the energy performance of a home. It covers energy use for space heating, water heating, ventilation and lighting calculated on the basis of standard occupancy.\" A BER assessment and certificate may be compulsory to sell a building or shortly after its construction. Methods of calculations and legislations related to BER may be different from one country to another. BER was created as a tool to help monitoring and improving the overall building energy efficiency.\n\nIn Ireland, each new dwelling built from January 1, 2007 required a BER, unless planning application was lodged prior to December 31, 2006. From January 1, 2009, it became illegal to offer a dwelling for sale without BER.\n\nFrom July 1, 2008 a BER is necessary for new non-residential buildings where planning permission is required. From January 1, 2009 a BER is required for existing non-residential buildings, when offered for sale or to let.\n\nThe Statutory Instrument regulating the requirements for BER is \"S.I. 666 of 2006\". It is Ireland's implementation of the Directive on the energy performance of buildings\n"}
{"id": "192043", "url": "https://en.wikipedia.org/wiki?curid=192043", "title": "CD player", "text": "CD player\n\nA CD player is an electronic device that plays audio compact discs, which are a digital optical disc data storage format. CD players were first sold to consumers in 1982. CDs typically contain recordings of audio material such as music. CD players are often a part of home stereo systems, car audio systems, and personal computers. With the exception of CD boomboxes, most CD players do not produce sound by themselves. Most CD players only produce an output signal via a headphone jack or RCA jacks. To listen to music using a CD player with a headphone output jack, the user plugs headphones or earphones into the headphone jack. To use a CD player in a home stereo system, the user connects an RCA cable to the RCA jacks or other outputs and connects it to a hi-fi (or other amplifier) and loudspeakers for listening to music. They are also manufactured as portable devices, which are battery powered and typically used with headphones.\n\nModern units can play other formats in addition to PCM audio coding used in CDs, such as MP3, AAC and WMA. DJs playing dance music at clubs often use specialized players with an adjustable playback speed to alter the pitch and tempo of the music. Audio engineers using CD players to play music for an event through a sound reinforcement system use professional audio-grade CD players. CD playback functionality is also available on CD-ROM/DVD-ROM drive equipped computers as well as on DVD players and most optical disc-based game consoles.\n\nAmerican inventor James T. Russell has been credited with inventing the first system to record digital information on an optical transparent foil that is lit from behind by a high-power halogen lamp. Russell's patent application was first filed in 1966, and he was granted a patent in 1970. Following litigation, Sony and Philips licensed Russell's patents (then held by a Canadian company, Optical Recording Corp.) in the 1980s.\n\nThe Compact Disc is an evolution of LaserDisc technology, where a focused laser beam\nis used that enables the high information density required for high-quality digital audio signals.\nPrototypes were developed by Philips and Sony independently in the late 1970s. In 1979, Sony and Philips set up a joint task force of engineers to design a new digital audio disc. After a year of experimentation and discussion, the \"Red Book\" CD-DA standard was published in 1980. After their commercial release in 1982, compact discs and their players were extremely popular. Despite costing up to $1,000, over 400,000 CD players were sold in the United States between 1983 and 1984. The success of the compact disc has been credited to the cooperation between Philips and Sony, who came together to agree upon and develop compatible hardware. The unified design of the compact disc allowed consumers to purchase any disc or player from any company, and allowed the CD to dominate the at-home music market unchallenged.\n\nThe Sony CDP-101, released in 1982, was the world's first commercially released compact disc player.\n\nIn 1974, L. Ottens, director of the audio division of Philips, started a small group with the aim to develop an analog optical audio disc with a diameter of 20 cm and a sound quality superior to that of the vinyl record. However, due to the unsatisfactory performance of the analog format, two Philips research engineers recommended a digital format in March 1974. In 1977, Philips then established a laboratory with the mission of creating a digital audio disc. The diameter of Philips's prototype compact disc was set at 11.5 cm, the diagonal of an audio cassette.\n\nHeitaro Nakajima, who developed an early digital audio recorder within Japan's national public broadcasting organization NHK in 1970, became general manager of Sony's audio department in 1971. His team developed a digital PCM adaptor audio tape recorder using a Betamax video recorder in 1973. After this, in 1974 the leap to storing digital audio on an optical disc was easily made. Sony first publicly demonstrated an optical digital audio disc in September 1976. A year later, in September 1977, Sony showed the press a 30 cm disc that could play 60 minutes of digital audio (44,100 Hz sampling rate and 16-bit resolution) using MFM modulation. In September 1978, the company demonstrated an optical digital audio disc with a 150-minute playing time, 44,056 Hz sampling rate, 16-bit linear resolution, and cross-interleaved error correction code—specifications similar to those later settled upon for the standard Compact Disc format in 1980. Technical details of Sony's digital audio disc were presented during the 62nd AES Convention, held on 13–16 March 1979, in Brussels. Sony's AES technical paper was published on 1 March 1979. A week later, on 8 March, Philips publicly demonstrated a prototype of an optical digital audio disc at a press conference called \"Philips Introduce Compact Disc\" in Eindhoven, Netherlands.\n\nSony executive Norio Ohga, later CEO and chairman of Sony, and Heitaro Nakajima were convinced of the format's commercial potential and pushed further development despite widespread skepticism. As a result, in 1979, Sony and Philips set up a joint task force of engineers to design a new digital audio disc. Led by engineers Kees Schouhamer Immink and Toshitada Doi, the research pushed forward laser and optical disc technology. After a year of experimentation and discussion, the task force produced the \"Red Book\" CD-DA standard. First published in 1980, the standard was formally adopted by the IEC as an international standard in 1987, with various amendments becoming part of the standard in 1996.\n\nPhilips coined the term \"compact disc\" in line with another audio product, the Compact Cassette, and contributed the general manufacturing process, based on video LaserDisc technology. Philips also contributed eight-to-fourteen modulation (EFM), which offers a certain resilience to defects such as scratches and fingerprints, while Sony contributed the error-correction method, CIRC. The \"Compact Disc Story\", told by a former member of the task force, gives background information on the many technical decisions made, including the choice of the sampling frequency, playing time, and disc diameter. The task force consisted of around four to eight persons, though according to Philips, the Compact Disc was \"invented collectively by a large group of people working as a team.\"\n\n“Red Book” was the first standard in the Rainbow Books series of standards.\n\nPhilips established the Polydor Pressing Operations plant in Langenhagen near Hannover, Germany, and quickly passed a series of milestones.\n\n\nThe Japanese launch was followed in March 1983 by the introduction of CD players and discs to Europe and North America (where CBS Records released sixteen titles). This event is often seen as the \"Big Bang\" of the digital audio revolution. The new audio disc was enthusiastically received, especially in the early-adopting classical music and audiophile communities, and its handling quality received particular praise. As the price of players gradually came down, and with the introduction of the portable Walkman, the CD began to gain popularity in the larger popular and rock music markets. The first artist to sell a million copies on CD was Dire Straits, with their 1985 album \"Brothers in Arms\". The first major artist to have his entire catalogue converted to CD was David Bowie, whose 15 studio albums were made available by RCA Records in February 1985, along with four greatest hits albums. In 1988, 400 million CDs were manufactured by 50 pressing plants around the world.\nThe CD was planned to be the successor of the gramophone record for playing music, rather than primarily as a data storage medium; but from its origins as a musical format, CDs have grown to encompass other applications. In 1983, following the CD's introduction, Immink and Braat presented the first experiments with erasable compact discs during the 73rd AES Convention. In June 1985, the computer-readable CD-ROM (read-only memory) and, in 1990, CD-Recordable were introduced, also developed by both Sony and Philips. Recordable CDs were a new alternative to tape for recording music and copying music albums without defects introduced in compression used in other digital recording methods. Other newer video formats such as DVD and Blu-ray use the same physical geometry as CD, and most DVD and Blu-ray players are backward compatible with audio CD.\n\nBy the early 2000s, the CD player had largely replaced the audio cassette player as standard equipment in new automobiles, with 2010 being the final model year for any car in the US to have a factory-equipped cassette player. Currently, with the increasing popularity of portable digital audio players, such as mobile phones, and solid state music storage, CD players are being phased out of automobiles in favor of minijack auxiliary inputs and connections to USB devices.\n\nMeanwhile, with the advent and popularity of Internet-based distribution of files in lossily-compressed audio formats such as MP3, sales of CDs began to decline in the 2000s. For example, between 2000 and 2008, despite overall growth in music sales and one anomalous year of increase, major-label CD sales declined overall by 20% – although independent and DIY music sales may be tracking better (according to figures released 30 March 2009), and CDs still continue to sell greatly. As of 2012, CDs and DVDs made up only 34 percent of music sales in the United States. In Japan, however, over 80 percent of music was bought on CDs and other physical formats as of 2015..\n\nThe process of playing an audio CD, touted as a digital audio storage medium, starts with the plastic polycarbonate compact disc, a medium that contains the digitally encoded data. The disc is placed in a tray which either opens up (as with portable CD players) or slides out (the norm with in-home CD players, computer disc drives and game consoles). In some systems, the user slides the disc into a slot (e.g., car stereo CD players). Once the disc is loaded into the tray, the data is read out by a mechanism that scans the spiral data track using a laser beam. An electric motor spins the disc. The tracking control is done by analogue servoamplifiers and then the high frequency analogue signal read from the disc is digitized, processed and decoded into analogue audio and digital control data which is used by the player to position the playback mechanism on the correct track, do the skip and seek functions and display track, time, index and, on newer players in the 2010s, display title and artist information on a display placed in the front panel.\n\nTo read the data from the disc, a laser beam shines on the surface of the disc. Surface differences between discs being played, and tiny position differences once loaded, are handled by using a movable lens with a very close focal length to focus the light on the disc. A low mass lens coupled to an electromagnetic coil is in charge of keeping focused the beam on the 600 nm wide data track.\n\nWhen the player tries to read from a stop, it first does a focus seek program that moves the lens up and down from the surface of the disc until a reflection is detected; when there is a reflection, the servo electronics lock in place keeping the lens in perfect focus while the disc rotates and changes its relative height from the optical block.\n\nDifferent brands and models of optical assemblies use different methods of focus detection. On most players, the focus position detection is made using the difference in the current output of a block of four photodiodes. The photodiode block and the optics are arranged in such a way that a perfect focus projects a circular pattern on the block while a far or near focus projects an ellipse differing in the position of the long edge in north-south or west-southwest. That difference is the information that the servoamplifier uses to keep the lens at the proper reading distance during the playback operation, even if the disc is warped.\n\nAnother servo mechanism in the player is in charge of keeping the focused beam centered on the data track.\n\nTwo optical pick-up designs exists, the original CDM series from Philips use a magnetic actuator mounted on a swing-arm to do coarse and fine tracking. Using only one laser beam and the 4 photodiode block, the servo knows if the track is centred by measuring side-by-side movement of the light of beam hitting on the block and corrects to keep the light on the centre.\n\nThe other design by Sony uses a diffraction grating to part the laser light into one main beam and two sub-beams. When focused, the two peripheral beams cover the border of the adjacent tracks a few micrometers apart from the main beam and reflect back on two photodiodes separated from the main block of four. The servo detects the RF signal being received on the peripheral receivers and the difference in output between these two diodes conform the tracking error signal that the system uses to keep the optics in the proper track. The tracking signal is fed to two systems, one integrated in the focus lens assembly can do fine tracking correction and the other system can move the entire optical assembly side by side to do coarse track jumps.\n\nThe sum of the output from the four photodiodes makes the RF or high frequency signal which is an electronic mirror of the pits and lands recorded on the disc. The RF signal, when observed on an oscilloscope, has a characteristic \"fish-eye\" pattern and its usefulness in servicing the machine is paramount for detecting and diagnosing problems, and calibrating CD players for operation.\n\nThe first stage in the processing chain for the analog RF signal (from the photoreceptor device) is digitizing it. Using various circuits like a simple comparator or a data slicer, the analog signal becomes a chain of two binary digital values, 1 and 0. This signal carries all the information in a CD and is modulated using a system called EFM (Eight-to-fourteen modulation). The second stage is demodulating the EFM signal into a data frame that contains the audio samples, error correction parity bits, according with the CIRC error correction code, and control data for the player display and micro-computer.\nThe EFM demodulator also decodes part of the CD signal and routes it to the proper circuits, separating audio, parity and control (subcode) data.\n\nAfter demodulating, a CIRC error corrector takes each audio data frame, stores it in a SRAM memory and verifies that it has been read correctly, if it is not, it takes the parity and correction bits and fixes the data, then it moves it out to a DAC to be converted to an analog audio signal. If the data missing is enough to make recovery impossible, the correction is made by interpolating the data from subsequent frames so the missing part is not noticed. Each player has a different interpolation ability. If too many data frames are missing or unrecoverable, the audio signal may be impossible to fix by interpolation, so an audio mute flag is raised to mute the DAC to avoid invalid data to be played back.\n\nThe Redbook standard dictates that, if there is invalid, erroneous or missing audio data, it cannot be output to the speakers as digital noise, it has to be muted.\n\nThe Audio CD format requires every player to have enough processing power to decode the CD data; this is normally made by application-specific integrated circuits (ASICs). ASICs do not work by themselves, however; they require a main microcomputer or microcontroller to orchestrate the entire machine.\n\nSony released its CDP-101 CD player in 1982 with a slide-out tray design for the CD. As it was easy to manufacture and to use, most CD player manufacturers stayed with the tray style ever since. However, there have been some notable exceptions.\n\nDuring the launch of the first prototype \"Goronta\" CD player by Sony at the Japanese Audio Fair in 1982, Sony showcased the vertical loading design. Although the Sony prototype design was never put into volume production, the concept was for a time adopted for production by a number of early Japanese CD player manufacturers, including Alpine/Luxman, Matsushita under the Technics brand, Kenwood and Toshiba/Aurex. For the early vertical loading players, Alpine sourced their AD-7100 player designs for Luxman, Kenwood and Toshiba (using their Aurex brand). Kenwood added their \"Sigma Drive\" outputs to this design as a modification. A picture of this early design can be seen on the Panasonic Web site. The vertical loading is similar the one common in cassette decks, where the holder opens, and disc is dropped to it. The holder is closed manually by hand, by motor after pressing a button, or completely automatically. Some CD players combine vertical loading with slot loading due the disc being drawn further into the disc holder as it closes.\n\nIn 1983 Philips, at the US and European launch of the CD format, showcased the first top loading CD tray designs with their CD100 CD player. (Philips audio products were sold as Magnavox in the US at the time.) The design had a clamp on the lid which meant the user had to close this over the CD when it was placed inside the machine. Later, Meridian introduced their MCD \"high end\" CD player, with Meridian electronics in the Philips CD100 chassis.\n\nTop-loading was adopted on various equipment designs such as mini systems and portable CD players, but among stereo component CD players, only a handful of top-loading models have been made. Examples include Luxman's D-500 and D-500X series players and Denon's DP-S1, both launched in 1993.\nTop-loading is also common in players intended for broadcast and live sound \"DJ\" use, such as Technics' SL-P50 (1984–1985) and Technics SL-P1200 (1986–1992). They more closely mimic the physical arrangement and ergonomics of record turntables used in those applications.\n\nThe Philips CD303 of 1983-1984 was the first player to adopt tray loading with a sliding play mechanism. Basically as the tray came out to collect the CD, the entire player's transport system also came out as one unit. The Meridians 200 and 203 players were of this type. They were also the first to use a design in which the audio electronics were in a separate enclosure from the CD drive and pickup mechanism.\n\nSlot loading is the preferred loading mechanism for car audio players. There is no tray that pops out, and a motor is used to assist disc insertion and removal. Some slot-loading mechanisms and changers can load and play back Mini-CDs without the need of an adapter but they may work with limited functionality (A disc changer will refuse to operate the changer until the Mini CD is removed for example).\nNon-circular CDs cannot be used on such loaders because they cannot handle non-circular discs. When inserted, such discs may become stuck and damage the mechanism.\n\nTwo types of optical tracking mechanisms exist:\n\nThe swing-arm mechanism has a distinctive advantage over the other in that it does not \"skip\" when the rail becomes dirty. The swing arm mechanisms tend to have a much longer life than their radial counterparts. The main difference between the two mechanisms is the way they read the data from the disc. The swing-arm mechanism uses a magnetic coil wound over a permanent magnet to provide the tracking movement to the laser assembly in a similar way a hard drive moves its head across the data tracks. It also uses another magnetic movement mechanism attached to the focusing lens to focus the laser beam on the disc surface. By operating the tracking or the focus actuators, the laser beam can be positioned on any part of the disc.\nThis mechanism employs a single laser beam and a set of four photodiodes to read, focus and keep track of the data coming from the disc.\nThe linear tracking mechanism uses a motor and reduction gears to move the laser assembly radially across the tracks of the disc and it also has a set of six coils mounted in the focusing lens over a permanent magnetic field. One set of two coils moves the lens closer to the disc surface, providing the focusing motion, and the other set of coils moves the lens radially, providing a finer tracking motion. This mechanism uses the three-beam tracking method in which a main laser beam is used to read and focus the data track of the disc using three or four photodiodes, depending on the focus method, and two smaller beams read the adjacent tracks at each side to help the servo keep the tracking using two more \"helper\" photodiodes.\n\nA CD player has three major mechanical components : a drive motor, a lens system, and a tracking mechanism.\nThe drive motor (also called spindle) spins the disc to a scanning velocity of 1.2–1.4 m/s (constant linear velocity) – equivalent to approximately 500 RPM at the inside of the disc, and approximately 200 RPM at the outside edge. (A disc played from beginning to end slows its rotation rate during playback.) The tracking mechanism moves the lens system along the spiral tracks in which information is encoded, and the lens assembly reads the information using a laser beam, typically produced by a laser diode. The laser reads information by focusing a beam on the CD, which is reflected off the disc's mirrored surface back to a photodiode array sensor. The sensor detects changes in the beam, and a digital processing chain interprets these changes as binary data. The data are processed, and eventually converted to sound using a digital-to-analog converter (DAC).\n\nA TOC or Table of Contents is located after the \"lead-in\" area of the disc, which is located in an inner ring of the disc, and contains roughly five kilobytes of available space. It is the first information that the player reads when the disc is loaded in the player and contains information on the total number of audio tracks, the running time on the CD, the running time of each track, and other information such as ISRC and the format structure of the disc. The TOC is of such vital importance for the disc that if it is not read correctly by the player, the CD could not be played back. That is why it is repeated 3 times before the first music program starts. The \"lead out\" area in the end (the outer peripheral) of the disc tells the player that disc has come to an end.\n\nCD players can employ a number of ways to improve performance, or reduce component count or price. Features such as oversampling, one-bit DACs, dual DACs, interpolation (error correction), anti-skip buffering, digital and optical outputs are, or were, likely to be found. Other features improve functionality, such as track programming, random play and repeat, or direct track access. Yet others are related to the CD player's intended target, such as anti-skip for car and portable CD players, pitch control and queuing for a DJ's CD player, remote and system integration for household players. Description of some features follows:\n\n\nA portable CD player is a portable audio player used to play compact discs. Portable CD players are powered by batteries and they have a 1/8\" headphone jack into which the user plugs a pair of headphones. The first portable CD player released was the D-50 by Sony. The D-50 was made available on the market in 1984, and adopted for Sony's entire portable CD player line.\n\nA 2005 book stated that after Apple Computers entered the music industry, within ten years it became the dominant seller of portable digital audio music players (its iPod line), \"...while former giant Sony (maker of the [portable] Walkman and [CD] Discman [was] struggling.\" This market change was initiated in 1998, when the first portable digital audio player, the Rio personal digital music player, was introduced. Portable MP3 players like the Rio competed with portable CD players. The 64 MB version of the Rio MP3 player enabled users to store about 20 songs on the player. One of the benefits of the Rio over portable CD players was that since the Rio had no moving parts, it offered skip-free playback. Since 1998, the price of portable digital audio players has dropped and the storage capacity has increased significantly. In the 2000s, users can \"carry [their] entire music collection in a [digital audio] player the size of a cigarette package.\" The 4 GB iPod, for example, holds over 1,000 songs.\n\nA boombox is a common term for a portable cassette and AM/FM radio that consists of an amplifier, two or more loudspeakers and a carrying handle. Beginning in the 1990s, boomboxes typically included a CD player. The boombox CD player is the only type of CD player which produces sound audible by the listener independently, without the need for headphones or an additional amplifier or speaker system. Designed for portability, boomboxes can be powered by batteries as well as by line current. The boombox was introduced to the American market during the mid-1970s. The desire for louder and heavier bass led to bigger and heavier boxes; by the 1980s, some boomboxes had reached the size of a suitcase. Most boomboxes were battery-operated, leading to extremely heavy, bulky boxes.\n\nMost boomboxes from the 2010s typically include a CD player compatible with CD-R and CD-RW, which allows the user to carry their own music compilations on a higher fidelity medium. Many also permit iPod and similar devices to be plugged into them through one or more auxiliary ports. Some also support formats such as MP3 and WMA. Another modern variant is a DVD player/boombox with a top-loading CD/DVD drive and an LCD video screen in the position once occupied by a cassette deck. Many models of this type of boombox include inputs for external video (such as television broadcasts) and outputs to connect the DVD player to a full-sized television.\n\nDisc jockeys (DJs) who are playing a mix of songs at a dance club, rave, or nightclub create their dance mixes by having songs playing on two or more sound sources and using a DJ mixer to transition seamlessly between songs. In the 1970s disco era, DJs typically used two record players. From the 1980s to the 1990s, two compact cassette players became a popular sound source for DJs. In subsequent decades, DJs shifted to CDs and then to digital audio players. DJs who use CDs and CD players typically use specialized DJ CD players that have features not available on regular CD players.\n\nDJs who are performing \"scratching\"–the creation of rhythmic sounds and sound effects from sound recordings–traditionally used vinyl records and turntables. In the 2010s, some specialized DJ CD players can be used to create the same \"scratching\" effects using songs on CDs.\n\n"}
{"id": "54131420", "url": "https://en.wikipedia.org/wiki?curid=54131420", "title": "Cambarysu", "text": "Cambarysu\n\nThe cambarysu was a drum which Jose Bach said he saw the Catuquinaru tribe of Brazil use, when he visited them in 1896-97, to communicate between villages.\n\nIt consisted of a hollow palm-wood cylinder about 1 m deep and 40 cm in diameter, filled with a layer of fine sand at the bottom, then a layer of pieces of wood, pieces of bone, and pulverised mica, with a narrow empty space above it, capped by leather, then wood, and then rubber on top. It was half buried in a hole in the ground, surrounded by pieces of wood, bone and other debris (covered at ground level with rubber), and resting on a bed of packed coarse sand.\n\nBach reported that one was located in the house of the chief of each village, and that when the device was drummed, the vibrations (travelling through the earth) could be heard (only) on the devices in other villages, up to 1.5 km away. In the late 1890s and early 1900s, several journals reported on the device, although some scholars expressed scepticism that it existed and functioned as described, as no other Europeans reported it.\n"}
{"id": "266449", "url": "https://en.wikipedia.org/wiki?curid=266449", "title": "Coilgun", "text": "Coilgun\n\nA coilgun or Gauss rifle is a type of projectile accelerator consisting of one or more coils used as electromagnets in the configuration of a linear motor that accelerate a ferromagnetic or conducting projectile to high velocity. In almost all coilgun configurations, the coils and the gun barrel are arranged on a common axis. It is not a rifle as the barrel is not rifled. The name \"Gauss\" is in reference to Carl Friedrich Gauss, who formulated mathematical descriptions of the magnetic effect used by magnetic accelerator cannons.\n\nCoilguns generally consist of one or more coils arranged along a barrel, so the path of the accelerating projectile lies along the central axis of the coils. The coils are switched on and off in a precisely timed sequence, causing the projectile to be accelerated quickly along the barrel via magnetic forces. Coilguns are distinct from railguns, as the direction of acceleration in a railgun is at right angles to the central axis of the current loop formed by the conducting rails. In addition, railguns usually require the use of sliding contacts to pass a large current through the projectile or sabot but coilguns do not necessarily require sliding contacts. While some simple coilgun concepts can use ferromagnetic projectiles or even permanent magnet projectiles, most designs for high velocities actually incorporate a coupled coil as part of the projectile. Another form of Gauss Rifle is one which consists of a strong magnet on a rail. There are two metal balls on one end of the magnet. Another ball is placed next to the magnet, but not attracted to it. When the ball is pushed toward the magnet, it accelerates until it hits the magnet with some force and velocity. The momentum is transferred through the magnet to the last ball, which flies off the end with nearly as much force as it started with.\n\nThe first operational coilgun was developed and patented by Norwegian scientist Kristian Birkeland in 1904.\n\nIn 1933, Texan inventor Virgil Rigsby developed a stationary coilgun that was designed to be used like a machine gun. It was powered by a large electrical motor and generator. It appeared in many contemporary science publications, but never piqued the interest of any armed forces.\n\nThere are two main types or setups of a coilgun: single-stage and multistage. A single-stage coilgun uses one electromagnet to propel a projectile. A multistage coilgun uses several electromagnets in succession to progressively increase the speed of the projectile.\n\nFor ferromagnetic projectiles, a single stage coilgun can be formed by a coil of wire, an electromagnet, with a ferromagnetic projectile placed at one of its ends. This type of coilgun is formed like the solenoid used in an electromechanical relay, i.e. a current-carrying coil which will draw a ferromagnetic object through its center. A large current is pulsed through the coil of wire and a strong magnetic field forms, pulling the projectile to the center of the coil. When the projectile nears this point the electromagnet must be switched off, to prevent the projectile from becoming arrested at the center of the electromagnet.\n\nIn a multistage design, further electromagnets are then used to repeat this process, progressively accelerating the projectile. In common coilgun designs, the \"barrel\" of the gun is made up of a track that the projectile rides on, with the driver into the magnetic coils around the track. Power is supplied to the electromagnet from some sort of fast discharge storage device, typically a battery, or high-capacity high voltage capacitors (one per electromagnet), designed for fast energy discharge. A diode is used to protect polarity sensitive components (such as semiconductors or electrolytic capacitors) from damage due to inverse polarity of the voltage after turning off the coil.\n\nMany hobbyists use low-cost rudimentary designs to experiment with coilguns, for example using photoflash capacitors from a disposable camera, or a capacitor from a standard cathode-ray tube television as the energy source, and a low inductance coil to propel the projectile forward.\n\nSome designs have non-ferromagnetic projectiles, of materials such as aluminium or copper, with the armature of the projectile acting as an electromagnet with internal current induced by pulses of the acceleration coils. A superconducting coilgun called a \"quench gun\" could be created by successively quenching a line of adjacent coaxial superconducting coils forming a gun barrel, generating a wave of magnetic field gradient traveling at any desired speed. A traveling superconducting coil might be made to ride this wave like a surfboard. The device would be a mass driver or linear synchronous motor with the propulsion energy stored directly in the drive coils. Another method would have non-superconducting acceleration coils and propulsion energy stored outside them but a projectile with superconducting magnets.\n\nThough the cost of power switching and other factors can limit projectile energy, a notable benefit of some coilgun designs over simpler railguns is avoiding an intrinsic velocity limit from hypervelocity physical contact and erosion. By having the projectile pulled towards or levitated within the center of the coils as it is accelerated, no physical friction with the walls of the bore occurs. If the bore is a total vacuum (such as a tube with a plasma window), there is no friction at all, which helps prolong the period of reusability.\n\nOne main obstacle in coilgun design is switching the power through the coils. There are several common solutions—the simplest (and probably least effective) is the spark gap, which releases the stored energy through the coil when the voltage reaches a certain threshold. A better option is to use solid-state switches; these include IGBTs or power MOSFETs (which can be switched off mid-pulse) and SCRs (which release all stored energy before turning off).\n\nA quick-and-dirty method for switching, especially for those using a flash camera for the main components, is to use the flash tube itself as a switch. By wiring it in series with the coil, it can silently and non-destructively (assuming that the energy in the capacitor is kept below the tube's safe operating limits) allow a large amount of current to pass through to the coil. Like any flash tube, ionizing the gas in the tube with a high voltage triggers it. However, a large amount of the energy will be dissipated as heat and light, and, due to the tube being a spark gap, the tube will stop conducting once the voltage across it drops sufficiently, leaving some charge remaining on the capacitor.\n\nThe electrical resistance of the coils and the equivalent series resistance (ESR) of the current source dissipate considerable power.\n\nAt low speeds the heating of the coils dominates the percentage efficiency of the coilgun, giving exceptionally low efficiency. However, as speeds climb, mechanical power grows proportional to the speed, but, correctly switched, the resistive losses are largely unaffected, and thus these resistive losses become much smaller in percentage terms.\n\nIdeally, 100% of the magnetic flux generated by the coil would be delivered to and act on the projectile; in reality this is impossible due to energy losses always present in a real system, which cannot be eliminated entirely\n\nWith a simple air-cored solenoid, the majority of the magnetic flux is not coupled into the projectile because of the magnetic circuit's high reluctance. The uncoupled flux generates a magnetic field that stores energy in the surrounding air. The energy that is stored in this field does not simply disappear from the magnetic circuit once the capacitor finishes discharging, instead returning to the coilgun's electric circuit. Because the coilgun's electric circuit is inherently analogous to an LC oscillator, the unused energy returns in the reverse direction ('ringing'), which can seriously damage polarized capacitors such as electrolytic capacitors.\n\nReverse charging can be prevented by a diode connected in reverse-parallel across the capacitor terminals; as a result, the current keeps flowing until the diode and the coil's resistance dissipate the field energy as heat. While this is a simple and frequently utilized solution, it requires an additional expensive high-power diode and a well-designed coil with enough thermal mass and heat dissipation capability in order to prevent component failure.\n\nSome designs attempt to recover the energy stored in the magnetic field by using a pair of diodes. These diodes, instead of being forced to dissipate the remaining energy, recharge the capacitors with the right polarity for the next discharge cycle. This will also avoid the need to fully recharge the capacitors, thus significantly reducing charge times. However, the practicality of this solution is limited by the resulting high recharge current through the equivalent series resistance (ESR) of the capacitors; the ESR will dissipate some of the recharge current, generating heat within the capacitors and potentially shortening their lifetime.\n\nTo reduce component size, weight, durability requirements, and most importantly, cost, the magnetic circuit must be optimized to deliver more energy to the projectile for a given energy input. This has been addressed to some extent by the use of back iron and end iron, which are pieces of magnetic material that enclose the coil and create paths of lower reluctance in order to improve the amount of magnetic flux coupled into the projectile. Results can vary widely, depending on the materials used; hobbyist designs may use, for example, materials ranging anywhere from magnetic steel (more effective, lower reluctance) to video tape (little improvement in reluctance). Moreover, the additional pieces of magnetic material in the magnetic circuit can potentially exacerbate the possibility of flux saturation and other magnetic losses.\n\nAnother significant limitation of the coilgun is the occurrence of magnetic saturation in the ferromagnetic projectile. When the flux in the projectile lies in the linear portion of its material's B(H) curve, the force applied to the core is proportional to the square of coil current (I)—the field (H) is linearly dependent on I, B is linearly dependent on H and force is linearly dependent on the product BI. This relationship continues until the core is saturated; once this happens B will only increase marginally with H (and thus with I), so force gain is linear. Since losses are proportional to I, increasing current beyond this point eventually decreases efficiency although it may increase the force. This puts an absolute limit on how much a given projectile can be accelerated with a single stage at acceptable efficiency.\n\nApart from saturation, the B(H) dependency often contains a hysteresis loop and the reaction time of the projectile material may be significant. The hysteresis means that the projectile becomes permanently magnetized and some energy will be lost as a permanent magnetic field of the projectile. The projectile reaction time, on the other hand, makes the projectile reluctant to respond to abrupt B changes; the flux will not rise as fast as desired while current is applied and a B tail will occur after the coil field has disappeared. This delay decreases the force, which would be maximized if the H and B were in phase.\n\nMost of the work to develop coilguns as hyper-velocity launchers has used \"air-cored\" systems to get around the limitations associated with ferromagnetic projectiles. In these systems, the projectile is accelerated by a moving coil \"armature\". If the armature is configured as one or more \"shorted turns\" then induced currents will result as a consequence of the time variation of the current in the static launcher coil (or coils).\n\nIn principle, coilguns can also be constructed in which the moving coils are fed with current via sliding contacts. However, the practical construction of such arrangements requires the provision of reliable high speed sliding contacts. Although feeding current to a multi-turn coil armature might not require currents as large as those required in a railgun, the elimination of the need for high speed sliding contacts is an obvious potential advantage of the induction coilgun relative to the railgun.\n\nAir cored systems also introduce the penalty that much higher currents may be needed than in an \"iron cored\" system. Ultimately though, subject to the provision of appropriately rated power supplies, air cored systems can operate with much greater magnetic field strengths than \"iron cored\" systems, so that, ultimately, much higher accelerations and forces should be possible.\n\nSmall coilguns are recreationally made by hobbyists, typically up to several joules to tens of joules projectile energy (the latter comparable to a typical air gun and an order of magnitude less than a firearm) while ranging from under one percent to several percent efficiency.\n\nIn 2018, a Los Angeles-based company Arcflash Labs offered the first coilgun for sale to the general public. It fired 6-gram steel slugs at 45 m/s with a muzzle energy of approximately 5 joules.\n\nMuch higher efficiency and energy can be obtained with designs of greater expense and sophistication. In 1978, Bondaletov in the USSR achieved record acceleration with a single stage by sending a 2-gram ring to 5000 m/s in 1 cm of length, but the most efficient modern designs tend to involve many stages. Above 90% efficiency is estimated for some vastly larger superconducting concepts for space launch. An experimental 45-stage DARPA coilgun mortar design is 22% efficient, with 1.6 megajoules KE delivered to a round.\n\nThough facing the challenge of competitiveness versus conventional guns (and sometimes railgun alternatives), coilguns are being researched for weaponry.\n\nThe DARPA Electromagnetic Mortar program is an example of potential benefits, if practical challenges like sufficiently low weight can be managed. The coilgun would be relatively silent with no smoke giving away its position, though a coilgun projectile would still create a sonic boom if supersonic. Adjustable yet smooth acceleration of the projectile throughout the barrel can allow somewhat higher velocity, with a predicted range increase of 30% for a 120mm EM mortar over the conventional version of similar length. With no separate propellant charges to load, the researchers envision the firing rate to approximately double.\n\nIn 2006, a 120mm prototype was under construction for evaluation, though time before reaching field deployment, if such occurs, was estimated then as 5 to 10+ years by Sandia National Laboratories. In 2011, development was proposed of an 81mm coilgun mortar to operate with a hybrid-electric version of the future Joint Light Tactical Vehicle.\n\nElectromagnetic aircraft catapults are planned, including on board future U.S. Gerald R. Ford class aircraft carriers. An experimental induction coilgun version of an Electromagnetic Missile Launcher (EMML) has been tested for launching Tomahawk missiles. A coilgun-based active defense system for tanks is under development at HIT in China.\n\nCoilgun potential has been perceived as extending beyond military applications. Challenging and corresponding to a magnitude of capital investment that few entities could readily fund, gigantic coilguns with projectile mass and velocity on the scale of gigajoules of kinetic energy (as opposed to megajoules or less) have not been developed so far, but such have been proposed as launchers from the Moon or from Earth:\n\n\n"}
{"id": "6939566", "url": "https://en.wikipedia.org/wiki?curid=6939566", "title": "Coring", "text": "Coring\n\nCoring happens when a heated alloy, such as a Cu-Ni system, cools in non-equilibrium conditions. This causes the exterior of the material to solidify before the interior. Coring causes the centers of the grains to retain more of the higher melting temperature element. In this case, the dendrite arms formed from the exterior have a different composition than the alloy in the inner regions, resulting in a local compositional difference. Coring results in poor mechanical performance of the overall product.\n\nCoring is predominantly observed in alloys having a marked difference between liquidus and solidus temperatures. It is often being removed by subsequent annealing and/or hot-working. It is exploited in zone refining techniques to produce high purity metals.\n\n"}
{"id": "2457166", "url": "https://en.wikipedia.org/wiki?curid=2457166", "title": "Cypres", "text": "Cypres\n\nCYPRES is an acronym for Cybernetic Parachute Release System. It refers to a specific make and model of an automatic activation device (AAD), a device that automatically activate a parachute (typically as a reserve system for a skydiver) under certain circumstances. A CYPRES is designed to activate the reserve parachute at a preset altitude if the rate of descent is over a certain threshold. The manufacturer of the CYPRES is Airtec.\n\nThe CYPRES works by using a cutter to cut the reserve container closing loop. A spring-loaded pilot chute then leaves the container and breaks through the skydiver's slipstream to begin reserve deployment.\n\nThe CYPRES comes in five different models: Expert, Student, Tandem, Speed and Wingsuit. The basic operation of the units is the same; only the activation parameters are different, having been optimized for different types of skydiving.\n\n"}
{"id": "293035", "url": "https://en.wikipedia.org/wiki?curid=293035", "title": "Disintermediation", "text": "Disintermediation\n\nDisintermediation is the removal of intermediaries in economics from a supply chain, or cutting out the middlemen in connection with a transaction or a series of transactions. Instead of going through traditional distribution channels, which had some type of intermediary (such as a distributor, wholesaler, broker, or agent), companies may now deal with customers directly, for example via the Internet. Hence, the use of factory direct and direct from the factory to mean the same thing.\n\nDisintermediation may decrease the total cost of servicing customers and may allow the manufacturer to increase profit margins and/or reduce prices. Disintermediation initiated by consumers is often the result of high market transparency, in that buyers are aware of supply prices direct from the manufacturer. Buyers may choose to bypass the middlemen (wholesalers and retailers) to buy directly from the manufacturer, and pay less. Buyers can alternatively elect to purchase from wholesalers. Often, a business-to-consumer electronic commerce (B2C) company functions as the bridge between buyer and manufacturer.\n\nHowever manufacturers will still incur distribution costs, such as the physical transport of goods, packaging in small units, advertising, and customer helplines, some or all of which would previously have been borne by the intermediary. To illustrate, a typical B2C supply chain is composed of four or five entities. These are the supplier, manufacturer, wholesaler, retailer and buyer.\n\nThe term was originally applied to the banking industry in 1967; disintermediation occurred when consumers avoided the intermediation of banks by investing directly in securities (government and private bonds, insurance companies, hedge funds, mutual funds and stocks) rather than leaving their money in savings accounts. The original cause was a U.S. government regulation (Regulation Q) which limited the interest rate paid on interest bearing accounts that were insured by the Federal Deposit Insurance Corporation.\n\nIt was later applied more generally to \"cutting out the middleman\" in commerce, though the financial meaning remained predominant. Only in the late 1990s did it become widely popularized.\n\nIt has been argued that the Internet modifies the supply chain due to market transparency. Disintermediation has acquired a new meaning with the advent of the virtual marketplace. The virtual marketplace sellers like Amazon are edging out the middlemen. Direct sellers and buyers connect with each other because of the platform created by the virtual marketplace vendor. There is quid pro quo for the vendor for the use of the platform, else it would make no business sense to create such a platform. If the buyer, having connected with the seller, circumvents the platform and talks to the seller and does her deal directly with the seller, then the platform owner is unlikely to get her revenue share. This may be considered a new form of disintermediation.\n\nIn the non-Internet world, disintermediation has been an important strategy for many big box retailers like Walmart, which attempt to reduce prices by reducing the number of intermediaries between the supplier and the buyer. Disintermediation is also closely associated with the idea of just in time manufacturing, as the removal of the need for inventory removes one function of an intermediary. The existence of laws which discourage disintermediation has been cited as a reason for the poor economic performance of Japan and Germany in the 1990s.\n\nHowever, Internet-related disintermediation occurred less frequently than many expected during the dot com boom. Retailers and wholesalers provide essential functions such as the extension of credit, aggregation of products from different suppliers, and processing of returns. In addition, shipping goods to and from the manufacturer can in many cases be far less efficient than shipping them to a store where the consumer can pick them up (if the consumer's trip to the store is ignored). In response to the threat of disintermediation, some retailers have attempted to integrate a virtual presence and a physical presence in a strategy known as bricks and clicks.\n\nReintermediation can be defined as the reintroduction of an intermediary between end users (consumers) and a producer. This term applies especially to instances in which disintermediation has occurred first.\n\nAt the start of the Internet revolution, electronic commerce was seen as a tool of disintermediation for cutting operating costs. The concept was that by allowing consumers to purchase products directly from producers via the Internet, the product delivery chain would be drastically shortened, thereby \"disintermediating\" the standard supply model middlemen. However, what largely happened was that new intermediaries appeared in the digital landscape (e.g., Amazon.com and eBay).\n\nReintermediation occurred due to many new problems associated with the e-commerce disintermediation concept, largely centered on the issues associated with the direct-to-consumers model. The high cost of shipping many small orders, massive customer service issues, and confronting the wrath of disintermediated retailers and supply channel partners all presented real obstacles. Huge resources are required to accommodate presales and postsales issues of individual consumers. Before disintermediation, supply chain middlemen acted as salespeople for the producers. Without them, the producer itself would have to handle procuring those customers. Selling online has its own associated costs: developing quality websites, maintaining product information, and marketing expenses all add up. Finally, limiting a product's availability to Internet channels forces the producer to compete with the rest of the Internet for customers' attention, a space that is becoming increasingly crowded.\n\nNotable examples of disintermediation include Dell and Apple, which sell many of their systems direct to the consumer—thus bypassing traditional retail chains, having succeeded in creating brands well recognized by customers, profitable and with continuous growth.\n\n\n"}
{"id": "1894767", "url": "https://en.wikipedia.org/wiki?curid=1894767", "title": "Eidophor", "text": "Eidophor\n\nAn Eidophor was a television projector used to create theater-sized images. The name Eidophor is derived from the Greek word-roots ‘eido’ and ‘phor’ meaning 'image' and 'bearer' (carrier). Its basic technology was the use of electrostatic charges to deform an oil surface.\n\nThe idea for the original Eidophor was conceived in 1939 in Zurich by Dr. Fritz Fischer and the Swiss Federal Institute of Technology, with the first prototype being unveiled in 1943. A patent was granted by the United States Patent and Trademark Office (patent no. 2,391,451) to Friederich Ernst Fischer for the \"Process and appliance for projecting television pictures\" on 25 December 1945. \n\nFollowing the Second World War, Paramount Pictures and 20th Century Fox experimented with the concept of \"theatre television\", where television images would be broadcast onto cinema screens. Over 100 cinemas were set up for the project, which failed because of financial losses and the refusal of the U.S. Federal Communications Commission (FCC) to grant theatre owners their own UHF bands for presentation.\n\nAn original August 1952 magazine article in the Radio and Television News credits the development of the Eidophor to Dr. Edgar Gretener. It was not until around the time of this article that Gretener took a lead in commercialising the Eidophor, following Fischer’s death in 1947. \n\nEidophors used an optical system somewhat similar to a conventional movie projector, but substituted a slowly rotating mirrored disk or dish for the film. The disk was covered with a thin film of transparent high-viscosity oil, and through the use of a scanned electron beam, electrostatic charges could be deposited onto the oil, causing its surface to deform. Light was shone on the disc by a striped mirror consisting of strips of reflective material alternating with transparent non-reflective areas. Areas of the oil unaffected by the electron beam would allow the light to be reflected directly back to the mirror and towards the light source, whereas light passing through deformed areas would be displaced and would pass through the adjacent transparent areas and onwards through the projection system. As the disk rotated, a doctor blade discharged and smoothed the ripples in the oil, readying it for re-use on another television frame.\n\nThe Eidophor was a large and cumbersome device and not commonly used until there was a need for good-quality large-screen projection. This opportunity arose as part of the NASA space program, where the technology was deployed in mission control.\n\nEidophors were also used in stadiums by touring music groups for live event visual amplification.\n\nSimple Eidophors produced black-and-white images. Later units used a color wheel (equivalent to the color television standard CBS tried to bring to the market against RCA/NBC's FCC-approved NTSC system, and today's DLP projection system) to produce red, green, and blue fields. The last models produced used separate red, green, and blue units in a single case. The Eidophor was 80 times brighter than CRT projectors of the time. The last Eidophors were able to project colour images of up to 18 metres in width. Advances in projection television technology in the 1990s brought about the end of the Eidophor. The new devices were smaller and cheaper and produced comparable results. Current technologies include the liquid-crystal display (LCD) and digital light processing (DLP) projectors, both of which produce superior results from easily portable devices.\n\n\n\n"}
{"id": "10610500", "url": "https://en.wikipedia.org/wiki?curid=10610500", "title": "Electronic Industries Association of Japan", "text": "Electronic Industries Association of Japan\n\nFounded in 1948, the Electronic Industries Association of Japan (EIAJ) was one of two Japanese electronics trade organizations that were merged into the Japan Electronics and Information Technology Industries Association (JEITA).\n\nPrior to the merger, EIAJ created a number of electronics industry standards that have had some use outside Japan, including:\n\n\nAnother standard is the multi-channel TV sound system used with the NTSC-J analog TV system. It is often referred to simply as EIAJ, or sometimes as FM-FM audio.\n\nThe Japanese technical standard JIS-C-7102 provides a method of developing part numbers for transistor devices. The part number has up to five fields, for example in the number 2SC82DA:\n\n"}
{"id": "53642796", "url": "https://en.wikipedia.org/wiki?curid=53642796", "title": "Exar Corporation", "text": "Exar Corporation\n\nExar Corporation is an American semiconductor manufacturer which was acquired by MaxLinear in May 2017 and maintained as a wholly owned subsidiary. It was established in 1971 as a subsidiary of the Japanese firm Rohm.\n\nThe Japanese semiconductor company Rohm established Exar as a California-based American subsidiary in 1971. At that time, the integrated circuit boards designed and manufactured by Rohm were developed in the United States, and establishing an American subsidiary provided closer ties to the design process and provided a marketing foothold. In the early 1980s, when Exar was running sales figures of around annually, only about ⅓ of sales were through its parent, Rohm. Part of the success of Exar and Rohm in the early 1980s stemmed from proprietary technologies developed between the two.\n\nAs a result of pressure from the U.S. Government for Japan to reduce its involvement in American industry, in particular in the semiconductor industry, Rohm began slowly divesting itself of Exar. Following Exar's initial public offering in 1985, Rohm's share of ownership had dropped to under 70%.\n\nExar's acquisition of Exel Microelectronics in the mid-1980s contributed heavily to a financial downturn in the company. Excar purchased Exel for its technology, but also for its chip fabrication facilities; however, unbeknownst to Exar at the time of purchase, those plants were not run-ready and required large amounts of capital investment, which ate into Exar profits through the end of the decade. In response to these problems, Rohm reversed its position on Exar by purchasing outstanding stock, leading to its again fully owning the company. Reacquisition by Rohm freed Exar to fully capitalize assets acquired from Exel and turn up the investment in research and development, leading again to profitability based in large part of new introductions in the EEPROM and ASIC product spaces.\n\nAfter Rohm had rescued the company and supported its return to profitability, it changed course and began divesting again, leading in 1990 to holding less than 50 percent of Exar. From its inception, Exar's top executive staff and board had been picked by Rohm, and consisted largely of Japanese expatriates. This changed in 1992 when Rohm removed Exar's chief executive officer, Nabuo Hatta, who had served in an executive post since at least 1985, when he was president of the company. Exar's board subsequently hired George Wells, a Scotsman educated at the University of Glasgow, as the company's new chief executive; Wells had previously held positions at General Electric and LSI Logic.\n\nWells oversaw a reorganization and re-prioritization of product offerings and introduced a culture of quality, leading to major increases in sales and income. Armed with a large cash surplus, and following Rohm's final divestment of Exar assets in 1994, Wells went on an acquisition spree in 1994 and 1995 which led to the purchases of Origin Technology, Micro Power Systems, Startech Semiconductor and Silicon Microstructures; all in all, these purchases enriched the technologies available to Exar. Acquiring so many companies in such a short period had consequences for the company, and Exar lost its profitability by early 1995.\n\nOn April 3, 2009, Exar Corporation closed the acquisition of hi/fn, inc., a transaction which included stock and about in cash. On March 22, 2013, Exar acquired the assets of Altior in Eatontown, New Jersey. On July 5, 2013, it acquired Stretch, Inc. On June 3, 2014, it acquired most of Integrated Memory Logic Limited.\n\nOn March 29, 2017, MaxLinear Inc. announced it would buy Exar Corporation for about $661.6 million cash. The acquisition of Exar Corp for $687 million was completed in May 2017.\n\n\n"}
{"id": "2136049", "url": "https://en.wikipedia.org/wiki?curid=2136049", "title": "Explosimeter", "text": "Explosimeter\n\nAn explosimeter is a gas detector which is used to measure the amount of combustible gases present in a sample. When a percentage of the lower explosive limit (LEL) of an atmosphere is exceeded, an alarm signal on the instrument is activated. \"Explosimeter\" is a registered trademark of MSA.\n\nThe device, also called a combustible gas detector, operates on the principle of resistance proportional to heat—a wire is heated, and a sample of the gas is introduced to the hot wire. Combustible gases burn in the presence of the hot wire, thus increasing the resistance and disturbing a Wheatstone bridge, which gives the reading.\n\nA flashback arrestor is installed in the device to avoid the explosimeter igniting the sample external to the device.\n\nNote, that the detection readings of an explosimeter are only accurate if the gas being sampled has the same characteristics and response as the calibration gas. Most explosimeters are calibrated to methane or hydrogen. \n\n"}
{"id": "44037589", "url": "https://en.wikipedia.org/wiki?curid=44037589", "title": "FET (timetabling software)", "text": "FET (timetabling software)\n\nFET is a free and open-source time tabling app for automatically scheduling the timetable of a school, high-school or university. FET is written in C++ using the Qt cross-platform application framework. Initially, FET stood for \"Free Evolutionary Timetabling\"; as it is no longer evolutionary, the E in the middle can stand for anything the user prefers.\n\nCustom made versions of FET are available for Moroccan, Algerian and US high-schools.\n\n\n\n"}
{"id": "4503954", "url": "https://en.wikipedia.org/wiki?curid=4503954", "title": "FOUP", "text": "FOUP\n\nFOUP is an acronym for Front Opening Unified Pod or Front Opening Universal Pod.\n\nIt is a specialised plastic enclosure designed to hold silicon wafers securely and safely in a controlled environment, and to allow the wafers to be transferred between machines for processing or measurement.\n\nFOUPs began to appear along with the first 300mm wafer processing tools in the mid 1990s. The size of the wafers and their comparative lack of rigidity meant that SMIF was not a viable technology. FOUPs were designed with the constraints of 300mm in mind, with the removable cassette being replaced by fins in the FOUP which hold the wafers in place, and the bottom opening door being replaced by a front opening door to allow robot handling mechanisms to access the wafers directly from the FOUP. The weight of a fully loaded 25 wafer FOUP at around 9 kilograms means that automated material handling systems are essential for all but the smallest of fabrication plants. To allow this, each FOUP has various coupling plates, pins and holes to allow the FOUP to be located on a load port, and to be manipulated by the AMHS (Automated Material Handling System). FOUPs may also contain RF tags that allow them to be identified by readers on tools, in the AMHS etc. FOUPs are available in several colors, depending on the customers wish.\n\nFOSB is an acronym for Front Opening Shipping Box, is used for transferring wafers between manufacturing facilities.\n\n\n"}
{"id": "6156238", "url": "https://en.wikipedia.org/wiki?curid=6156238", "title": "Fusible link", "text": "Fusible link\n\nFusible links include mechanical and electrical devices. \n\nA mechanical fusible link is a device consisting of two strips of metal soldered together with a fusible alloy that is designed to melt at a specific temperature, thus allowing the two pieces to separate. Mechanical fusible links are utilized as the triggering device in fire sprinkler systems and mechanical automatic door release mechanisms that close fire doors in warehouses, etc. Some high-security safes also utilize fusible link-based relockers as a defense against torches and heat-producing tools. Mechanical fusible links come in a variety of designs and different temperature ratings.\n\nAn electrical fusible link is a type of electrical fuse that is constructed simply with a short piece of wire typically four American wire gauge sizes smaller than the wire that is being protected. For example, an AWG 16 fusible link might be used to protect AWG 12 wiring. Electrical fusible links are common in high-current automotive applications. The wire in an electrical fusible link is encased in high-temperature fire-resistant insulation to reduce hazards when the wire melts. \n\n"}
{"id": "10059597", "url": "https://en.wikipedia.org/wiki?curid=10059597", "title": "GPS signals", "text": "GPS signals\n\nGlobal Positioning System (GPS) satellites broadcast microwave signals to enable GPS receivers on or near the Earth's surface to determine location and time and derive velocity. The GPS system itself is operated by the U.S. Department of Defense (DoD) for use by both the military and the general public.\n\nGPS signals include ranging signals, used to measure the distance to the satellite, and navigation messages. The navigation messages include \"ephemeris\" data, used to calculate the position of each satellite in orbit, and information about the time and status of the entire satellite constellation, called the \"almanac\".\n\nThere are four signals available for civilian use. In order of date of introduction, these are: L1 C/A, L2C, L5 and L1C. L1 C/A is also called the \"legacy signal\" and is broadcast by all satellites. The other signals are called \"modernized signals\" and are not broadcast by all satellites. In addition, there are \"restricted signals\", also broadcast to the general public, but whose encoding is secret and are intended to be used only by authorized parties. Nonetheless, some limited use of restricted signals can be made by civilians without access to the secret encoding details; this is called \"codeless\" and \"semi-codeless\" access, and is officially supported.\n\nThe interface to the User Segment (GPS receivers) is described in the Interface Control Documents (ICD). The format of civilian signals is described in the Interface Specification (IS) which is a subset of the ICD.\n\nThe GPS satellites (called \"space vehicles\" in the GPS interface specification documents) transmit simultaneously several ranging codes and navigation data using binary phase-shift keying (BPSK).\nOnly a limited number of central frequencies are used; satellites using the same frequency are distinguished by using different ranging codes; in other words, GPS uses code division multiple access. The ranging codes are also called \"chipping codes\" (in reference to CDMA/DSSS), \"pseudorandom noise\" and \"pseudorandom binary sequences\" (in reference to the fact that it is predictable, but statistically it resembles noise).\n\nSome satellites transmit several BPSK streams at the same frequency in quadrature, in a form of quadrature amplitude modulation. However, unlike typical QAM systems where a single bit stream is split in two half-symbol-rate bit streams to improve spectral efficiency, in GPS signals the in-phase and quadrature components are modulated by separate (but functionally related) bit streams.\nSatellites are uniquely identified by a serial number called \"space vehicle number\" (SVN) which does not change during its lifetime. In addition, all operating satellites are numbered with a \"space vehicle identifier\" (SV ID) and \"pseudorandom noise number\" (PRN number) which uniquely identifies the ranging codes that a satellite uses. There is a fixed one-to-one correspondence between SV identifiers and PRN numbers described in the interface specification. Unlike SVNs, the SV ID/PRN number of a satellite may be changed (also changing the ranging codes it uses). At any point in time, any SV ID/PRN number is in use by at most a single satellite. A single SV ID/PRN number may have been used by several satellites at different points in time and a single satellite may have used different SV ID/PRN numbers at different points in time. The current SVNs and PRN numbers for the GPS constellation may be found at NAVCEN.\n\nThe original GPS design contains two ranging codes: the \"coarse/acquisition\" (C/A) code, which is freely available to the public, and the restricted \"precision\" (P) code, usually reserved for military applications.\n\nThe C/A PRN codes are Gold codes with a period of 1023 chips transmitted at 1.023 Mbit/s, causing the code to repeat every 1 millisecond. They are combined with a navigation message using exclusive or and the resulting bit stream is used for modulation as previously described. These codes only match up, or strongly autocorrelate when they are almost exactly aligned. Each satellite uses a unique PRN code, which does not correlate well with any other satellite's PRN code. In other words, the PRN codes are highly orthogonal to one another. The 1 ms period of the C/A code corresponds to a 299,8 km symbol length, and the alignment of the C/A bit stream resolves phase information to a precision of 293 m.\n\nThe C/A codes are generated by combining (using \"exclusive or\") 2-bit streams generated by maximal period 10 stage linear feedback shift registers (LFSR). Different codes are obtained by selectively delaying one of those bit streams. Thus:\nwhere:\nThe arguments of the functions therein are the number of \"bits\" or \"chips\" since their epochs, starting at 0. The epoch of the LFSRs is the point at which they are at the initial state; and for the overall C/A codes it is the start of any UTC second plus any integer number of milliseconds. The output of LFSRs at negative arguments is defined consistent with the period which is 1,023 chips (this provision is necessary because formula_6 may have a negative argument using the above equation).\n\nThe delay for PRN numbers 34 and 37 is the same; therefore their C/A codes are identical and are not transmitted at the same time (it may make one or both of those signals unusable due to mutual interference depending on the relative power levels received on each GPS receiver).\n\nThe P-code is also a PRN; however, each satellite's P-code PRN code is 6.187104 · 10 bits long (773,388 GByte) and only repeats once a week (it is transmitted at 10.23 Mbit/s). The extreme length of the P-code increases its correlation gain and eliminates any range ambiguity within the Solar System. However, the code is so long and complex it was believed that a receiver could not directly acquire and synchronize with this signal alone. It was expected that the receiver would first lock onto the relatively simple C/A code and then, after obtaining the current time and approximate position, synchronize with the P-code.\n\nWhereas the C/A PRNs are unique for each satellite, the P-code PRN is actually a small segment of a master P-code approximately 2.35 · 10 bits in length (235,000,000,000,000 bits, ~26.716 terabytes) and each satellite repeatedly transmits its assigned segment of the master code.\n\nTo prevent unauthorized users from using or potentially interfering with the military signal through spoofing, it was decided to encrypt the P-code. To that end the P-code was modulated with the \"W-code\", a special encryption sequence, to generate the \"Y-code\". The Y-code is what the satellites have been transmitting since the anti-spoofing module was set to the \"on\" state. The encrypted signal is referred to as the \"P(Y)-code\".\n\nThe details of the W-code are kept secret, but it is known that it is applied to the P-code at approximately 500 kHz, which is a slower rate than that of the P-code itself by a factor of approximately 20. This has allowed companies to develop semi-codeless approaches for tracking the P(Y) signal, without knowledge of the W-code itself.\n\nIn addition to the PRN ranging codes, a receiver needs to know detailed information about each satellite's position and the network. The GPS design has this information modulated on top of both the C/A and P(Y) ranging codes at 50 bit/s and calls it the \"navigation message\". The navigation message format described in this section is called LNAV data (for \"legacy navigation\").\n\nThe navigation message conveys information which can be classified in 3 broad areas:\n\nWhereas ephemeris information is highly detailed and considered valid for no more than four hours, almanac information is more general and is considered valid for up to 180 days. The almanac assists the receiver in determining which satellites to search for, and once the receiver picks up each satellite's signal in turn, it then downloads the ephemeris data directly from that satellite. A position fix using any satellite can not be calculated until the receiver has an accurate and complete copy of that satellite's ephemeris data. If the signal from a satellite is lost while its ephemeris data is being acquired, the receiver must discard that data and start again.\n\nThe navigation message consists of 1,500 bit long \"frames\". Each frame consists of 5 \"subframes\" of 300 bits, numbered 1 to 5. In turn each subframe consists of 10 \"words\" of 30 bit each and requires 6 seconds to transmit. Each subframe has the GPS time. Subframe 1 contains the GPS date (week number) and information to correct the satellite's time to GPS time, plus satellite status and health. Subframes 2 and 3 together contain the transmitting satellite's ephemeris data. Subframes 4 and 5 contain components of the almanac but each frame contains only 1/25th of the complete almanac; a receiver must process 25 whole frames worth of data to retrieve the entire 15,000 bit almanac message. At this rate, 12.5 minutes are required to receive the entire almanac from a single satellite. Each of the 25 versions of frames 4 and 5 is called a \"page\" and they are numbered 1 to 25.\n\nFrames begin and end at the start/end of week plus an integer multiple of 30 seconds. At start/end of week the cycling between pages is reset to page 1.\n\nThere are two navigation message types: LNAV-L is used by satellites with PRN numbers 1 to 32 (called \"lower PRN numbers\") and LNAV-U is used by satellites with PRN numbers 33 to 63 (called \"upper PRN numbers\"). The 2 types use very similar formats. Subframes 1 to 3 are the same while subframes 4 and 5 use almost the same format. Both message types contain almanac data for all satellites using the same navigation message type, but not the other type.\n\nEach subframe begins with a Telemetry Word (TLM), which enables the receiver to detect the beginning of a subframe and determine the receiver clock time at which the navigation subframe begins. The next word is the handover word (HOW), which gives the GPS time (actually the time when the first bit of the next subframe will be transmitted) and identifies the specific subframe within a complete frame. The remaining eight words of the subframe contain the actual data specific to that subframe. Each word includes 6 bits of parity generated using an algorithm based on Hamming codes, which take into account the 24 non-parity bits of that word and the last 2 bits of the previous word.\n\nAfter a subframe has been read and interpreted, the time the next subframe was sent can be calculated through the use of the clock correction data and the HOW. The receiver knows the receiver clock time of when the beginning of the next subframe was received from detection of the Telemetry Word thereby enabling computation of the transit time and thus the pseudorange. The receiver is potentially capable of getting a new pseudorange measurement at the beginning of each subframe or every 6 seconds. \n\nGPS time is expressed with a resolution of 1.5 seconds as a week number and a time of week count (TOW). Its zero point (week 0, TOW 0) is defined to be 1980-01-06T00:00Z. The TOW count is a value ranging from 0 to 403,199 whose meaning is the number of 1.5 second periods elapsed since the beginning of the GPS week. Expressing TOW count thus requires 19 bits (2 = 524,288). GPS time is a continuous time scale in that it does not include leap seconds; therefore the start/end of GPS weeks may differ from that of the corresponding UTC day by an integer number of seconds.\n\nIn each subframe, each hand-over word (HOW) contains the most significant 17 bits of the TOW count corresponding to the start of the next following subframe. Note that the 2 least significant bits can be safely omitted because one HOW occurs in the navigation message every 6 seconds, which is equal to the resolution of the truncated TOW count thereof. Equivalently, the truncated TOW count is the time duration since the last GPS week start/end to the beginning of the next frame in units of 6 seconds.\n\nEach frame contains (in subframe 1) the 10 least significant bits of the corresponding GPS week number. Note that each frame is entirely within one GPS week because GPS frames do not cross GPS week boundaries. Since rollover occurs every 1,024 GPS weeks (approximately every 19.6 years; 1,024 is 2), a receiver that computes current calendar dates needs to deduce the upper week number bits or obtain them from a different source. One possible method is for the receiver to save its current date in memory when shut down, and when powered on, assume that the newly decoded truncated week number corresponds to the period of 1,024 weeks that starts at the last saved date. This method correctly deduces the full week number if the receiver is never allowed to remain shut down (or without a time and position fix) for more than 1,024 weeks (~19.6 years).\n\nThe \"almanac\" consists of coarse orbit and status information for each satellite in the constellation, an ionospheric model, and information to relate GPS derived time to Coordinated Universal Time (UTC). Each frame contains a part of the almanac (in subframes 4 and 5) and the complete almanac is transmitted by each satellite in 25 frames total (requiring 12.5 minutes). The almanac serves several purposes. The first is to assist in the acquisition of satellites at power-up by allowing the receiver to generate a list of visible satellites based on stored position and time, while an ephemeris from each satellite is needed to compute position fixes using that satellite. In older hardware, lack of an almanac in a new receiver would cause long delays before providing a valid position, because the search for each satellite was a slow process. Advances in hardware have made the acquisition process much faster, so not having an almanac is no longer an issue. The second purpose is for relating time derived from the GPS (called GPS time) to the international time standard of UTC. Finally, the almanac allows a single-frequency receiver to correct for ionospheric delay error by using a global ionospheric model. The corrections are not as accurate as GNSS augmentation systems like WAAS or dual-frequency receivers. However, it is often better than no correction, since ionospheric error is the largest error source for a single-frequency GPS receiver.\n\nSatellite data is updated typically every 24 hours, with up to 60 days data loaded in case there is a disruption in the ability to make updates regularly. Typically the updates contain new ephemerides, with new almanacs uploaded less frequently. The Control Segment guarantees that during normal operations a new almanac will be uploaded at least every 6 days.\n\nSatellites broadcast a new ephemeris every two hours. The ephemeris is generally valid for 4 hours, with provisions for updates every 4 hours or longer in non-nominal conditions. The time needed to acquire the ephemeris is becoming a significant element of the delay to first position fix, because as the receiver hardware becomes more capable, the time to lock onto the satellite signals shrinks; however, the ephemeris data requires 18 to 36 seconds before it is received, due to the low data transmission rate.\n\nFor the ranging codes and navigation message to travel from the satellite to the receiver, they must be modulated onto a carrier wave. In the case of the original GPS design, two frequencies are utilized; one at 1575.42 MHz (10.23 MHz × 154) called L1; and a second at 1227.60 MHz (10.23 MHz × 120), called L2.\n\nThe C/A code is transmitted on the L1 frequency as a 1.023 MHz signal using a bi-phase shift keying (BPSK) modulation technique. The P(Y)-code is transmitted on both the L1 and L2 frequencies as a 10.23 MHz signal using the same BPSK modulation, however the P(Y)-code carrier is in quadrature with the C/A carrier (meaning it is 90° out of phase).\n\nBesides redundancy and increased resistance to jamming, a critical benefit of having two frequencies transmitted from one satellite is the ability to measure directly, and therefore remove, the ionospheric delay error for that satellite. Without such a measurement, a GPS receiver must use a generic model or receive ionospheric corrections from another source (such as the Wide Area Augmentation System or WAAS). Advances in the technology used on both the GPS satellites and the GPS receivers has made ionospheric delay the largest remaining source of error in the signal. A receiver capable of performing this measurement can be significantly more accurate and is typically referred to as a \"dual frequency receiver\".\n\nHaving reached full operational capability on July 17, 1995 the GPS system had completed its original design goals. However, additional advances in technology and new demands on the existing system led to the effort to \"modernize\" the GPS system. Announcements from the Vice President and the White House in 1998 heralded the beginning of these changes and in 2000, the U.S. Congress reaffirmed the effort, referred to as \"GPS III\".\n\nThe project involves new ground stations and new satellites, with additional navigation signals for both civilian and military users, and aims to improve the accuracy and availability for all users. A goal of 2013 has been established with incentives offered to the contractors if they can complete it by 2011.\n\nModernized GPS civilian signals have two general improvements over their legacy counterparts: a dataless acquisition aid and forward error correction (FEC) coding of the NAV message.\n\nA dataless acquisition aid is an additional signal, called a pilot carrier in some cases, broadcast alongside the data signal. This dataless signal is designed to be easier to acquire than the data encoded and, upon successful acquisition, can be used to acquire the data signal. This technique improves acquisition of the GPS signal and boosts power levels at the correlator.\n\nThe second advancement is to use forward error correction (FEC) coding on the NAV message itself. Due to the relatively slow transmission rate of NAV data (usually 50 bits per second), small interruptions can have potentially large impacts. Therefore, FEC on the NAV message is a significant improvement in overall signal robustness.\n\nOne of the first announcements was the addition of a new civilian-use signal, to be transmitted on a frequency other than the L1 frequency used for the coarse/acquisition (C/A) signal. Ultimately, this became the L2C signal, so called because it is broadcast on the L2 frequency. Because it requires new hardware on board the satellite, it is only transmitted by the so-called Block IIR-M and later design satellites. The L2C signal is tasked with improving accuracy of navigation, providing an easy to track signal, and acting as a redundant signal in case of localized interference.\n\nUnlike the C/A code, L2C contains two distinct PRN code sequences to provide ranging information; the \"civil-moderate\" code (called CM), and the \"civil-long\" length code (called CL). The CM code is 10,230 bits long, repeating every 20 ms. The CL code is 767,250 bits long, repeating every 1,500 ms. Each signal is transmitted at 511,500 bits per second (bit/s); however, they are multiplexed together to form a 1,023,000-bit/s signal.\n\nCM is modulated with the CNAV Navigation Message (see below), whereas CL does not contain any modulated data and is called a \"dataless sequence\". The long, dataless sequence provides for approximately 24 dB greater correlation (~250 times stronger) than L1 C/A-code.\n\nWhen compared to the C/A signal, L2C has 2.7 dB greater data recovery and 0.7 dB greater carrier-tracking, although its transmission power is 2.3 dB weaker.\n\nThe civil-moderate and civil-long ranging codes are generated by a modular LFSR which is reset periodically to a predetermined initial state. The period of the CM and CL is determined by this resetting and not by the natural period of the LFSR (as is the case with the C/A code). The initial states are designated in the interface specification and are different for different PRN numbers and for CM/CL. The feedback polynomial/mask is the same for CM and CL. The ranging codes are thus given by:\nwhere:\n\nThe initial states are described in the GPS interface specification as numbers expressed in octal following the convention that the LFSR state is interpreted as the binary representation of a number where the output bit is the least significant bit, and the bit where new bits are shifted in is the most significant bit. Using this convention, the LFSR shifts from most significant bit to least significant bit and when seen in big endian order, it shifts to the right. The states called \"final state\" in the IS are obtained after cycles for CM and after cycles for LM (just before reset in both cases).\n\nThe feedback bit mask is 100100101001001010100111100. Again with the convention that the least significant bit is the output bit of the LFSR and the most significant bit is the shift-in bit of the LFSR, 0 means no feedback \"into\" that position, and 1 means feedback \"into\" that position.\n\nThe CNAV data is an upgraded version of the original NAV navigation message. It contains higher precision representation and nominally more accurate data than the NAV data. The same type of information (time, status, ephemeris, and almanac) is still transmitted using the new CNAV format; however, instead of using a frame / subframe architecture, it uses a new pseudo-packetized format made of 12-second 300-bit \"messages\" analogous to LNAV frames. While LNAV frames have a fixed information content, CNAV messages may be of one of several defined types. The type of a frame determines its information content. Messages do not follow a fixed schedule regarding which message types will be used, allowing the Control Segment some versatility. However, for some message types there are lower bounds on how often they will be transmitted.\n\nIn CNAV, at least 1 out of every 4 packets are ephemeris data and the same lower bound applies for clock data packets. The design allows for a wide variety of packet types to be transmitted. With a 32-satellite constellation, and the current requirements of what needs to be sent, less than 75% of the bandwidth is used. Only a small fraction of the available packet types have been defined; this enables the system to grow and incorporate advances without breaking compatibility.\n\nThere are many important changes in the new CNAV message:\n\nCNAV messages begin and end at start/end of GPS week plus an integer multiple of 12 seconds. Specifically, the beginning of the first bit (with convolution encoding already applied) to contain information about a message matches the aforesaid synchronization. CNAV messages begin with an 8-bit preamble which is a fixed bit pattern and whose purpose is to enable the receiver to detect the beginning of a message.\n\nThe convolutional code used to encode CNAV is described by:\nwhere:\nSince the FEC encoded bit stream runs at 2 times the rate than the non FEC encoded bit as already described, then formula_32. FEC encoding is performed independently of navigation message boundaries; this follows from the above equations.\n\nAn immediate effect of having two civilian frequencies being transmitted is the civilian receivers can now directly measure the ionospheric error in the same way as dual frequency P(Y)-code receivers. However, users utilizing the L2C signal alone, can expect 65% more position uncertainty due to ionospheric error than with the L1 signal alone.\n\nA major component of the modernization process is a new military signal. Called the Military code, or M-code, it was designed to further improve the anti-jamming and secure access of the military GPS signals.\n\nVery little has been published about this new, restricted code. It contains a PRN code of unknown length transmitted at 5.115 MHz. Unlike the P(Y)-code, the M-code is designed to be autonomous, meaning that a user can calculate their position using only the M-code signal. From the P(Y)-code's original design, users had to first lock onto the C/A code and then transfer the lock to the P(Y)-code. Later, direct-acquisition techniques were developed that allowed some users to operate autonomously with the P(Y)-code.\n\nA little more is known about the new navigation message, which is called \"MNAV\". Similar to the new CNAV, this new MNAV is packeted instead of framed, allowing for very flexible data payloads. Also like CNAV it can utilize Forward Error Correction (FEC) and advanced error detection (such as a CRC).\n\nThe M-code is transmitted in the same L1 and L2 frequencies already in use by the previous military code, the P(Y)-code. The new signal is shaped to place most of its energy at the edges (away from the existing P(Y) and C/A carriers).\n\nIn a major departure from previous GPS designs, the M-code is intended to be broadcast from a high-gain directional antenna, in addition to a full-Earth antenna. This directional antenna's signal, called a spot beam, is intended to be aimed at a specific region (several hundred kilometers in diameter) and increase the local signal strength by 20 dB, or approximately 100 times stronger. A side effect of having two antennas is that the GPS satellite will appear to be two GPS satellites occupying the same position to those inside the spot beam. While the whole Earth M-code signal is available on the Block IIR-M satellites, the spot beam antennas will not be deployed until the Block III satellites are deployed, tentatively in 2018.\n\nAn interesting side effect of having each satellite transmit four separate signals is that the MNAV can potentially transmit four different data channels, offering increased data bandwidth.\n\nThe modulation method is binary offset carrier, using a 10.23 MHz subcarrier against the 5.115 MHz code. This signal will have an overall bandwidth of approximately 24 MHz, with significantly separated sideband lobes. The sidebands can be used to improve signal reception.\n\nThe L5 signal provides a means of radionavigation secure and robust enough for life critical applications, such as aircraft precision approach guidance. The signal is broadcast in a frequency band protected by the ITU for aeronautical radionavigation services. It was first demonstrated from satellite USA-203 (Block IIR-M), and is available on all satellites from GPS IIF. The L5 band provides additional robustness in the form of interference mitigation, the band being internationally protected, redundancy with existing bands, geostationary satellite augmentation, and ground-based augmentation. The added robustness of this band also benefits terrestrial applications.\n\nTwo PRN ranging codes are transmitted on L5 in quadrature: the in-phase code (called \"I5-code\") and the quadrature-phase code (called \"Q5-code\"). Both codes are 10,230 bits long, transmitted at 10.23 MHz (1 ms repetition period), and are generated identically (differing only in initial states). Then, I5 is modulated (by exclusive-or) with navigation data (called L5 CNAV) and a 10-bit Neuman-Hoffman code clocked at 1 kHz. Similarly, the Q5-code is then modulated but with only a 20-bit Neuman-Hoffman code that is also clocked at 1 kHz.\n\nCompared to L1 C/A and L2, these are some of the changes in L5:\n\nThe I5-code and Q5-code are generated using the same structure but with different parameters. These codes are the combination (by exclusive-or) of the output of 2 differing linear-feedback shift registers (LFSRs) which are selectively reset.\nwhere:\n\nformula_4 and formula_6 are maximal length LFSRs. The modulo operations correspond to resets. Note that both are reset each millisecond (synchronized with C/A code epochs). In addition, the extra modulo operation in the description of formula_4 is due to the fact it is reset 1 cycle before its natural period (which is 8,191) so that the next repetition becomes offset by 1 cycle with respect to formula_6 (otherwise, since both sequences would repeat, I5 and Q5 would repeat within any 1 ms period as well, degrading correlation characteristics).\n\nThe L5 CNAV data includes SV ephemerides, system time, SV clock behavior data, status messages and time information, etc. The 50 bit/s data is coded in a rate 1/2 convolution coder. The resulting 100 symbols per second (sps) symbol stream is modulo-2 added to the I5-code only; the resultant bit-train is used to modulate the L5 in-phase (I5) carrier. This combined signal is called the L5 Data signal. The L5 quadrature-phase (Q5) carrier has no data and is called the L5 Pilot signal. The format used for L5 CNAV is very similar to that of L2 CNAV. One difference is that it uses 2 times the data rate. The bit fields within each message, message types, and forward error correction code algorithm are the same as those of L2 CNAV. L5 CNAV messages begin and end at start/end of GPS week plus an integer multiple of 6 seconds (this applies to the beginning of the first bit to contain information about a message, as is the case for L2 CNAV).\n\nBroadcast on the L5 frequency (1176.45 MHz, 10.23 MHz × 115), which is an aeronautical navigation band. The frequency was chosen so that the aviation community can manage interference to L5 more effectively than L2.\n\nL1C is a civilian-use signal, to be broadcast on the L1 frequency (1575.42 MHz), which contains the C/A signal used by all current GPS users. The L1C will be available with the first Block III launch, tentatively scheduled for the first half of fiscal year 2017.\n\nL1C consists of a pilot (called L1C) and a data (called L1C) component. These components use carriers with the same phase (within a margin of error of 100 milliradians), instead of carriers in quadrature as with L5. The PRN codes are 10,230 bits long and transmitted at 1.023 Mbit/s. The pilot component is also modulated by an overlay code called L1C (a secondary code that has a lower rate than the ranging code and is also predefined, like the ranging code). Of the total L1C signal power, 25% is allocated to the data and 75% to the pilot. The modulation technique used is BOC(1,1) for the data signal and TMBOC for the pilot. The time multiplexed binary offset carrier (TMBOC) is BOC(1,1) for all except 4 of 33 cycles, when it switches to BOC(6,1).\n\n\nThe L1C pilot and data ranging codes are based on a Legendre sequence with length used to build an intermediate code (called a \"Weil code\") which is expanded with a fixed 7-bit sequence to the required 10,230 bits. This 10,230-bit sequence is the ranging code and varies between PRN numbers and between the pilot and data components. The ranging codes are described by:\nwhere:\n\nAccording to the formula above and the GPS IS, the first formula_84 bits (equivalently, up to the insertion point of formula_78) of formula_61 and formula_63 are the first bits the corresponding Weil code; the next 7 bits are formula_78; the remaining bits are the remaining bits of the Weil code.\n\nThe IS asserts that formula_92. For clarity, the formula for formula_61 does not account for the hypothetical case in which formula_94, which would cause the instance of formula_78 inserted into formula_61 to wrap from index to 0.\n\nThe overlay codes are 1,800 bits long and is transmitted at 100 bit/s, synchronized with the navigation message encoded in L1C.\n\nFor PRN numbers 1 to 63 they are the truncated outputs of maximal period LFSRs which vary in initial conditions and feedback polynomials.\n\nFor PRN numbers 64 to 210 they are truncated Gold codes generated by combining 2 LFSR outputs (formula_97 and formula_98, where formula_3 is the PRN number) whose initial state varies. formula_97 has one of the 4 feedback polynomials used overall (among PRN numbers 64–210). formula_98 has the same feedback polynomial for all PRN numbers in the range 64–210.\n\nThe L1C navigation data (called CNAV-2) is broadcast in 1,800 bits long (including FEC) frames and is transmitted at 100 bit/s.\n\nThe frames of L1C are analogous to the messages of L1C and L5. While L2 CNAV and L5 CNAV use a dedicated message type for ephemeris data, all CNAV-2 frames include that information.\n\nThe common structure of all messages consists of 3 frames, as listed in the adjacent table. The content of subframe 3 varies according to its page number which is analogous to the type number of L2 CNAV and L5 CNAV messages. Pages are broadcast in an arbitrary order.\n\nThe time of messages (not to be confused with clock correction parameters) is expressed in a different format than the format of the previous civilian signals. Instead it consists of 3 components:\n\nTOI is the only content of subframe 1. The week number and ITOW are contained in subframe 2 along with other information.\n\nSubframe 1 is encoded by a modified BCH code. Specifically, the 8 least significant bits are BCH encoded to generate 51 bits, then combined using exclusive or with the most significant bit and finally the most significant bit is appended as the most significant bit of the previous result to obtain the final 52 bits. Subframes 2 and 3 are individually expanded with a 24-bit CRC, then individually encoded using a low-density parity-check code, and then interleaved as a single unit using a block interleaver.\n\nAll satellites broadcast at the same two frequencies, 1.57542 GHz (L1 signal) and 1.2276 GHz (L2 signal). The satellite network uses a CDMA spread-spectrum technique where the low-bitrate message data is encoded with a high-rate pseudo-random (PRN) sequence that is different for each satellite. The receiver must be aware of the PRN codes for each satellite to reconstruct the actual message data. The C/A code, for civilian use, transmits data at 1.023 million chips per second, whereas the P code, for U.S. military use, transmits at 10.23 million chips per second. The L1 carrier is modulated by both the C/A and P codes, while the L2 carrier is only modulated by the P code. The P code can be encrypted as a so-called P(Y) code which is only available to military equipment with a proper decryption key. Both the C/A and P(Y) codes impart the precise time-of-day to the user.\n\nEach composite signal (in-phase and quadrature phase) becomes:\n\nwhere formula_103 and formula_104 represent signal powers; formula_105 and formula_106 represent codes with/without data formula_107. This is a formula for the ideal case (which is not attained in practice) as it does not model timing errors, noise, amplitude mismatch between components or quadrature error (when components are not exactly in quadrature).\n\nA GPS receiver processes the GPS signals received on its antenna to determine position, velocity and/or timing. The signal at antenna is amplified, down converted to baseband or intermediate frequency, filtered (to remove frequencies outside the intended frequency range for the digital signal that would alias into it) and digitalized; these steps may be chained in a different order. Note that aliasing is sometimes intentional (specifically, when undersampling is used) but filtering is still required to discard frequencies not intended to be present in the digital representation.\n\nFor each satellite used by the receiver, the receiver must first acquire the signal and then track it as long as that satellite is in use; both are performed in the digital domain in by far most (if not all) receivers.\n\nAcquiring a signal is the process of determining the frequency and code phase (both relative to receiver time) when it was previously unknown. Code phase must be determined within an accuracy that depends on the receiver design (especially the tracking loop); 0.5 times the duration of code chips (approx. 0.489 µs) is a representative value.\n\nTracking is the process of continuously adjusting the estimated frequency and phase to match the received signal as close as possible and therefore is a phase locked loop. Note that acquisition is performed to start using a particular satellite, but tracking is performed as long as that satellite is in use.\n\nIn this section, one possible procedure is described for L1 C/A acquisition and tracking, but the process is very similar for the other signals. The described procedure is based on computing the correlation of the received signal with a locally generated replica of the ranging code and detecting the highest peak or lowest valley. The offset of the highest peak or lowest valley contains information about the code phase relative to receiver time. The duration of the local replica is set by receiver design and is typically shorter than the duration of navigation data bits, which is 20 ms.\n\nAcquisition of a given PRN number can be conceptualized as searching for a signal in a bidimensional search space where the dimensions are (1) code phase, (2) frequency. In addition, a receiver may not know which PRN number to search for, and in that case a third dimension is added to the search space: (3) PRN number.\n\n\nIf the almanac information has previously been acquired, the receiver picks which satellites to listen for by their PRNs. If the almanac information is not in memory, the receiver enters a search mode and cycles through the PRN numbers until a lock is obtained on one of the satellites. To obtain a lock, it is necessary that there be an unobstructed line of sight from the receiver to the satellite. The receiver can then decode the almanac and determine the satellites it should listen for. As it detects each satellite's signal, it identifies it by its distinct C/A code pattern.\n\nThe simplest way to acquire the signal (not necessarily the most effective or least computationally expensive) is to compute the dot product of a window of the digitalized signal with a set of locally generated replicas. The locally generated replicas vary in carrier frequency and code phase to cover all the already mentioned search space which is the Cartesian product of the frequency search space and the code phase search space. The carrier is a complex number where real and imaginary components are both sinusoids as described by Euler's formula. The replica that generates the highest magnitude of dot product is likely the one that best matches the code phase and frequency of the signal; therefore, if that magnitude is above a threshold, the receiver proceeds to track the signal or further refine the estimated parameters before tracking. The threshold is used to minimize false positives (apparently detecting a signal when there is in fact no signal), but some may still occur occasionally.\n\nUsing a complex carrier allows the replicas to match the digitalized signal regardless of the signal's carrier phase and to detect that phase (the principle is the same used by the Fourier transform). The dot product is a complex number; its magnitude represents the level of similarity between the replica and the signal, as with an ordinary correlation of real-valued time series. The argument of the dot product is an approximation of the corresponding carrier in the digitalized signal.\n\nAs an example, assume that the granularity for the search in code phase is 0.5 chips and in frequency is 500 Hz, then there are 1,023/0.5=2,046 code phases and 10,000 Hz/500 Hz=20 frequencies to try for a total of 20×2,046=40,920 local replicas. Note that each frequency bin is centered on its interval and therefore covers 250 Hz in each direction; for example, the first bin has a carrier at −4.750 Hz and covers the interval −5,000 Hz to −4,500 Hz. Code phases are equivalent modulo 1,023 because the ranging code is periodic; for example, phase −0.5 is equivalent to phase 1,022.5.\n\nThe following table depicts the local replicas that would be compared against the digitalized signal in this example. \"•\" means a single local replica while \"...\" is used for elided local replicas:\nAs an improvement over the simple correlation method, it is possible to implement the computation of dot products more efficiently with a Fourier transform. Instead of performing one dot product for each element in the Cartesian product of code and frequency, a single operation involving FFT and covering all frequencies is performed for each code phase; each such operation is more computationally expensive, but it may still be faster overall than the previous method due to the efficiency of FFT algorithms, and it recovers carrier frequency with a higher accuracy, because the frequency bins are much closely spaced in a DFT.\n\nSpecifically, for all code phases in the search space, the digitalized signal window is multiplied element by element with a local replica of the code (with no carrier), then processed with a discrete Fourier transform.\n\nGiven the previous example to be processed with this method, assume real-valued data (as opposed to complex data, which would have in-phase and quadrature components), a sampling rate of 5 MHz, a signal window of 10 ms, and an intermediate frequency of 2.5 MHz. There will be 5 MHz×10 ms=50,000 samples in the digital signal, and therefore 25,001 frequency components ranging from 0 Hz to 2.5 MHz in steps of 100 Hz (note that the 0 Hz component is real because it is the average of a real-valued signal and the 2.5 MHz component is real as well because it is the critical frequency). Only the components (or bins) within 5 kHz of the central frequency are examined, which is the range from 2.495 MHz to 2.505 MHz, and it is covered by 51 frequency components. There are 2,046 code phases as in the previous case, thus in total 51×2,046=104,346 complex frequency components will be examined.\n\nLikewise, as an improvement over the simple correlation method, it is possible to perform a single operation covering all code phases for each frequency bin. The operation performed for each code phase bin involves forward FFT, element-wise multiplication in the frequency domain. inverse FFT, and extra processing so that overall, it computes circular correlation instead of circular convolution. This yields more accurate \"code phase determination\" than the simple correlation method in contrast with the previous method, which yields more accurate \"carrier frequency determination\" than the previous method.\n\nSince the carrier frequency received can vary due to Doppler shift, the points where received PRN sequences begin may not differ from O by an exact integral number of milliseconds. Because of this, carrier frequency tracking along with PRN code tracking are used to determine when the received satellite's PRN code begins. Unlike the earlier computation of offset in which trials of all 1,023 offsets could potentially be required, the tracking to maintain lock usually requires shifting of half a pulse width or less. To perform this tracking, the receiver observes two quantities, phase error and received frequency offset. The correlation of the received PRN code with respect to the receiver generated PRN code is computed to determine if the bits of the two signals are misaligned. Comparisons of the received PRN code with receiver generated PRN code shifted half a pulse width early and half a pulse width late are used to estimate adjustment required. The amount of adjustment required for maximum correlation is used in estimating phase error. Received frequency offset from the frequency generated by the receiver provides an estimate of phase rate error. The command for the frequency generator and any further PRN code shifting required are computed as a function of the phase error and the phase rate error in accordance with the control law used. The Doppler velocity is computed as a function of the frequency offset from the carrier nominal frequency. The Doppler velocity is the velocity component along the line of sight of the receiver relative to the satellite.\n\nAs the receiver continues to read successive PRN sequences, it will encounter a sudden change in the phase of the 1,023-bit received PRN signal. This indicates the beginning of a data bit of the navigation message. This enables the receiver to begin reading the 20 millisecond bits of the navigation message. The TLM word at the beginning of each subframe of a navigation frame enables the receiver to detect the beginning of a subframe and determine the receiver clock time at which the navigation subframe begins. The HOW word then enables the receiver to determine which specific subframe is being transmitted. There can be a delay of up to 30 seconds before the first estimate of position because of the need to read the ephemeris data before computing the intersections of sphere surfaces.\n\nAfter a subframe has been read and interpreted, the time the next subframe was sent can be calculated through the use of the clock correction data and the HOW. The receiver knows the receiver clock time of when the beginning of the next subframe was received from detection of the Telemetry Word thereby enabling computation of the transit time and thus the pseudorange. The receiver is potentially capable of getting a new pseudorange measurement at the beginning of each subframe or every 6 seconds. \n\nThen the orbital position data, or ephemeris, from the navigation message is used to calculate precisely where the satellite was at the start of the message. A more sensitive receiver will potentially acquire the ephemeris data more quickly than a less sensitive receiver, especially in a noisy environment.\n\n\nGPS Interface Specification\n"}
{"id": "952601", "url": "https://en.wikipedia.org/wiki?curid=952601", "title": "George Owen Squier", "text": "George Owen Squier\n\nMajor General George Owen Squier (March 21, 1865 – March 24, 1934) was born in Dryden, Michigan, United States. He graduated from the United States Military Academy in the Class of 1887 and received a Ph.D. from Johns Hopkins University in 1893.\n\nHe was famous both in the United States and in Europe as a soldier, a scientist and as an inventor. He is known for what today is called Muzak.\n\nGeorge Squier wrote and edited many books and articles on the subject of radio and electricity. An inventor, he and Dartmouth professor Albert Cushing Crehore developed a magneto-optical streak camera \"The Polarizing Photo-chronograph\" in 1896 to measure the speed of projectiles both inside a cannon and directly after they left the cannon barrel. This was one of the earliest photonic programs. They also worked to develop synchronous AC telegraphic systems. His biggest contribution was that of telephone carrier multiplexing in 1910 for which he was elected to the National Academy of Science in 1919.\n\nAs executive officer to the Chief Signal Officer, U.S. Signal Corps in 1907, Squier was instrumental in the establishment of the Aeronautical Division, U.S. Signal Corps, the first organizational ancestor of the US Air Force. He also was the first military passenger in an airplane on September 12, 1908 and, working with the Wright Brothers, was responsible for the purchase of the first airplanes by the US Army in 1909.\n\nFrom May 1916 to February 1917, he was Chief of the Aviation Section, U.S. Signal Corps, the first successor of the Aeronautical Division, before being promoted to major general and appointed Chief Signal Officer during World War I.\n\nIn 1922, he created Wired Radio, a service which piped music to businesses and subscribers over wires. In 1934, he changed the service's name to 'Muzak'.\n\nAsked how to say his name, he told \"The Literary Digest\" it was pronounced like the word \"square\".\n\nHe was a member of the Sons of the American Revolution.\n\nHe died in Washington, D.C., at George Washington Hospital on March 24, 1934 of pneumonia.\n\n\nIn 1943, the U.S. Navy named troopship in his honor. It was the lead ship of its class, which was known as of transport ships.\n\nGeneral Squier Park, a historic district and waterpark in his hometown of Dryden, Michigan, is named in his honor.\n\n\n"}
{"id": "1862422", "url": "https://en.wikipedia.org/wiki?curid=1862422", "title": "Hispano-Moresque ware", "text": "Hispano-Moresque ware\n\nHispano-Moresque ware is a style of initially Islamic pottery created in Al Andalus or Muslim Spain, which continued to be produced under Christian rule in styles blending Islamic and European elements. It was the most elaborate and luxurious pottery being produced in Europe until the Italian maiolica industry developed sophisticated styles in the 15th century, and was exported over most of Europe. The industry's most successful period was the 14th and 15th centuries.\n\nAround 711, the Moors conquered Spain. Over the following centuries, they introduced two ceramic techniques to Europe: glazing with an opaque white tin-glaze, and lustreware, which imitates metallic finishes with iridescent effects. Hispano-Moresque wares use both processes, applying the paint as an overglaze which is then fired again. Lustreware was a speciality of Islamic pottery, at least partly because the use of drinking and eating vessels in gold and silver, the ideal in ancient Rome and Persia as well as medieval Christian societies, is prohibited by the Hadiths, with the result that pottery and glass were used for tableware by Muslim elites, when Christian medieval elites still normally used metal for both dishes and cups.\n\nAt first centred on Málaga in the south, and using typical Islamic decoration, by the 15th century the largest production was around Valencia, which had long been reconquered by the Crown of Aragon. Wares from Manises and other Valencian towns were mainly for the Christian market, and exported very widely.\n\nThe earliest major centre of fine pottery in Al-Andalus was Málaga in southern Spain. This is the major centre whose best-known wares were produced in a Muslim kingdom, as opposed to by a workforce presumed to be largely Muslim, or Morisco, under Christian rule. It was already celebrated for its gold lustrewares in the 14th century, and remained under Muslim rule until 1487, shortly before the fall of Granada, the last Moorish kingdom. Murcia, Almería, and perhaps Granada itself were also early centres of production. This pottery stayed much closer to styles seen in other Islamic countries, although much of it was being exported to Christian markets, as can be seen by the coats of arms on many pieces.\n\nAt least one authority, Alan Caiger-Smith, excludes this pottery from the term \"Hispano-Moresque\", but most who use the term at all use it to include Malaga and other Andalusian wares from the Islamic period as well as the Valencian pottery. When Spanish medieval pottery was first studied in the 19th century, there was awareness of the Valencian centres but very little of the Al-Andalus ones, and there has been a steady re-attribution of types of pottery formerly attributed to Manises to Malaga and the south, which was still continuing in the 1980s, following archaeological discoveries in Malaga, and scientific analysis of the clays used.\n\nThough other types of painted pottry, not usually called Hispano-Moresque ware, were produced in Al-Andaluz earlier, firm evidence of lustreware production is not found before the early or mid-13th century, when it may have been begun by Egyptian potters escaping political disturbances. Already it was being exported, as some of the earliest evidence is bowls set as decoration into the facades of churches in Pisa when they were built. An import from Malaga through Sandwich, Kent in England for the Spanish-born Queen Eleanor of Castile was recorded in 1289, consisting of \"42 bowls, 10 dishes, and 4 earthenware jars of foreign colour (\"extranei coloris\")\". Malagan ware was also exported to the Islamic world, and has been found at Fustat (medieval Cairo) and elsewhere.\nThe best known and most impressive examples of Andalucian wares are the Alhambra vases, a number of very large vases made to stand in niches in the Alhambra in Granada, and perhaps elsewhere. These are very atypical in Islamic pottery in having only a decorative function, with no practical purpose, and are \"by far\" the largest pieces of lustreware known. They are based on traditional shapes descended from the ancient amphora, but at about 115 to 170 cm tall are close to the height of a human. They are thought to come from a range of dates covering the late 14th and the 15th centuries, and the decoration and precise shape of the body is different in each surviving example. According to Alan Caiger-Smith, \"few other pots in the world make such a strong physical impression\". \n\nAll are now in museums, five in Spain and others in St Petersburg, Berlin, Washington D.C., Stockholm and Palermo; various large fragments also survive. Lustre tiles are also still in place at the Alhambra. The \"Fortuny Tablet\", a unique plaque measuring 90 x 44 cm, has a garden-like design, inside a border with an inscription praising Yusuf III, Sultan of Granada (r. 1408-1417). Its design resembles that of some Spanish carpets.\n\nAfter Yusuf's throne was inherited by an eight-year-old in 1418, the Nasrid kingdom went into a decline before its final conquest, and the production of fine pottery seems to cease abruptly about 1450, even though the name \"obra de Malequa\" (\"Malaga work\") continued to be used in Valencia for lustreware long afterwards.\n\nValencia and its suburbs Manises and Paterna became important centres after potters migrated there from the south; the city had returned to Christian rule from 1238, and the immigration of skilled potters had been going on since at least the mid-14th century. In 1362 a cardinal commissioned floor-tiles in \"obra de Malicha\" (\"Malaga work\", probably meaning lustreware) for the Pope's Palais des Papes in Avignon from two masters in Manises, at least one with an Arabic name (though \"Juan\" as his forename). In 1484 a German traveller mentioned vessels \"which are made by the Moorish potters\".\n\nIt seems that the local lords of Manises, the family of Buyl, encouraged the immigration, and may have acted as distributors and agents for the product; certainly when Maria of Castile, Queen of Aragon, wanted to order a large service in 1454, she wrote to the Buyl lord for him to arrange it. Several Buyl's had served as ambassadors, to Granada as well as Christian courts, giving them contacts in many markets. They seem to have taken a 10% royalty on all sales of pottery, and enjoyed a very high income from these. The largest deposit of Manises ware found by archaeology, apart from Manises itself, comes from Sluis in the Netherlands, then part of the territories of the wealthy Duchy of Burgundy. Manises also had clay and a cave nearby where a special sand used as a raw material for glazes was extracted.\n\nBarcelona in Catalonia in northeastern Spain, which was under Muslim rule from 718 to 801, became a centre for pottery much later, probably receiving immigrant Christian potters from Al-Andalus, especially Valencia, during the later Reconquista period. It was important at first for wares resembling the brown and green decorated pottery of Paterna and in the 16th century for lustreware in a \"warm silvery-gold\", either reflecting different materials available, or a deliberate change in style. Several other towns began to produce lustreware in the same period.\n\nMuch, in Valencia most, of the pottery was clearly made for a Christian market, as it includes coats of arms and other Western elements in the decoration. As well as the Christian IHS monogram in the centre, the naturalistic vine-leaf decoration of the dish shown at top is derived from Gothic art, probably via the border decoration of illuminated manuscripts. No pieces have yet been found that are signed (as many pieces from other Islamic regions are), and hardly any dated, so heraldry, especially when pieces are assumed to have been commissioned to celebrate a wedding, is important evidence for dating. The pieces \"had to be spectacular and elegant, yet every category of vessel had a particular use\" and on grand occasions all might be used, even though the largest platters spent most of the time on display propped up vertically on sideboards, as is shown in some contemporary paintings.\n\nAndalucian designs use a repertoire of geometric motifs, many of which probably had a religious significance of which Christian buyers remained unaware. These are usually contained in painted compartments. Pseudo-Kufic script is used, as well as inscriptions in proper Arabic. The dominant colours of gold and blue perhaps represent the sun and sky; other colours available, such as brown, green and yellow, are much less used. From about 1400 some elements, including the depiction of animals, which were probably first used for export wares seem to have become popular among local Muslim buyers also; two of the later \"Alhambra vases\" described above have pairs of gazelles. By then the Nasrid kings of Granada had given themselves heraldic arms in the Christian way, which are also seen on pottery.\n\nMany large Valencian dishes with typical complicated designs centring on a coat of arms are also decorated on the underside with boldly-painted animal figures occupying the whole space, often also taken from heraldry. Of Manises ware, Alan Caiger-Smith has written, \"the sustained production of fine pieces at Manises during the years 1380–1430 is without parallel in the history of ceramics. Many of these vessels will keep their place among the world's finest pottery for ever; regardless of changes and outlook.\" \n\nHispano-Moresque shapes of the fifteenth century included the \"albarello\" (a tall jar), large serving dishes with coats of arms, made for wealthy people all over Europe, jugs (some on high feet, the \"citra\" and the \"grealet\"), a deep-sided dish (the \"lebrillo de alo\") and the eared bowl (\"cuenco de oreja\"). Hispano-Moresque wares had a considerable influence on early Italian maiolica, indeed two possible derivations of the name have connections with it. Towards the end of the century designs began to incorporate raised elements in imitation of European silverware shapes, such as gadrooning. Tiles were made in all centres, and the small ceramic tombstone of an Andalucian student who died in 1409 is one of the very few precisely datable pieces.\n\nAlan Caiger-Smith describes the Valencian industry as the victim of its own success; as the wares initially produced for the very top of society, usually as bespoke commissions with personalized heraldry, were demanded by the expanding lesser nobility and bourgeoisie, both the size of pieces and their quality of decoration declined, with painting becoming more routine repetitions of simple motifs. The Italian maiolica industry, largely developed in imitation of the Spanish, was developing in directions where Valencia could or would not follow. That the Italian figurative Renaissance painting was not attempted in Spain is perhaps not surprising, but Valencia only joined the Italians in copying simpler shapes from metalware, the Italians being more ambitious.\n\nThe Reconquista captured Valencia for the third and final time in 1238, and Malaga was one of the last cities to fall, after the Siege of Málaga (1487). The remaining Islamic Mudéjar and converted Morisco populations were expelled from Spain in 1496 and 1609 respectively, the latter Expulsion of the Moriscos involving a third of the population in the province of Valencia. But many of the craftsmen had long been Christians in any case, and the Hispano-Moresque style survived in the province of Valencia, although showing an immediate drop in quality. Later wares usually have a coarse reddish-buff body, dark blue decoration and luster; by now their position as the most prestigious European pottery had been lost to Italian and other producers.\n\nWares continued to be produced in a slow decline, now relying on relatively local demand for tiles and other decorated items, including votive offerings. There were still said to be thirty working kilns at Manises around 1800, by which time the first efforts to revive the industry's former glory had already been made. The secrets of the techniques for making high-quality wares were largely lost, and after Carlos III of Spain took a personal interest a report was commissioned in 1785 to record the methods then being used, lest more was lost. By the 1870s a market had developed for pieces as close to the early work as could be managed, and a number of new firms were set up, some of which continue today, although little original work in the tradition is done.\n\nThe term \"Hispano-Moresque\" is also used to describe figured silk textiles with geometric patterns woven in Al-Andalus, and sometimes to refer to Mudéjar or other work in other media, such as carpets, an industry which followed a similar pattern to pottery in Spain. The Metropolitan Museum of Art uses the term to describe a gilded parade helmet in its collection.\n\n\n\n"}
{"id": "520066", "url": "https://en.wikipedia.org/wiki?curid=520066", "title": "History of cryptography", "text": "History of cryptography\n\nCryptography, the use of codes and ciphers to protect secrets, began thousands of years ago. Until recent decades, it has been the story of what might be called classic cryptography — that is, of methods of encryption that use pen and paper, or perhaps simple mechanical aids. In the early 20th century, the invention of complex mechanical and electromechanical machines, such as the Enigma rotor machine, provided more sophisticated and efficient means of encryption; and the subsequent introduction of electronics and computing has allowed elaborate schemes of still greater complexity, most of which are entirely unsuited to pen and paper.\n\nThe development of cryptography has been paralleled by the development of cryptanalysis — the \"breaking\" of codes and ciphers. The discovery and application, early on, of frequency analysis to the reading of encrypted communications has, on occasion, altered the course of history. Thus the Zimmermann Telegram triggered the United States' entry into World War I; and Allied reading of Nazi Germany's ciphers shortened World War II, in some evaluations by as much as two years.\n\nUntil the 1970s, secure cryptography was largely the preserve of governments. Two events have since brought it squarely into the public domain: the creation of a public encryption standard (DES), and the invention of public-key cryptography.\n\nThe earliest known use of cryptography is found in non-standard hieroglyphs carved into the wall of a tomb from the Old Kingdom of Egypt circa 1900 BCE. These are not thought to be serious attempts at secret communications, however, but rather to have been attempts at mystery, intrigue, or even amusement for literate onlookers.\n\nSome clay tablets from Mesopotamia somewhat later are clearly meant to protect information—one dated near 1500 BCE was found to encrypt a craftsman's recipe for pottery glaze, presumably commercially valuable. Furthermore, Hebrew scholars made use of simple monoalphabetic substitution ciphers (such as the Atbash cipher) beginning perhaps around 500 to 600 BCE.\n\nIn India around 400 BCE to 200 CE, Mlecchita vikalpa or \"the art of understanding writing in cypher, and the writing of words in a peculiar way\" was documented in the Kama Sutra for the purpose of communication between lovers. This was also likely a simple substitution cipher. Parts of the Egyptian demotic Greek Magical Papyri were written in a cypher script.\n\nThe ancient Greeks are said to have known of ciphers. The scytale transposition cipher was used by the Spartan military, but it is not definitively known whether the scytale was for encryption, authentication, or avoiding bad omens in speech. Herodotus tells us of secret messages physically concealed beneath wax on wooden tablets or as a tattoo on a slave's head concealed by regrown hair, although these are not properly examples of cryptography \"per se\" as the message, once known, is directly readable; this is known as steganography. Another Greek method was developed by Polybius (now called the \"Polybius Square\"). The Romans knew something of cryptography (e.g., the Caesar cipher and its variations).\n\nDavid Kahn notes in \"The Codebreakers\" that modern cryptology originated among the Arabs, the first people to systematically document cryptanalytic methods. The invention of the frequency-analysis technique for breaking monoalphabetic substitution ciphers, by Al-Kindi, an Arab mathematician sometime around AD 800 proved to be the single most significant cryptanalytic advance until World War II. Al-Kindi wrote a book on cryptography entitled \"Risalah fi Istikhraj al-Mu'amma\" (\"Manuscript for the Deciphering Cryptographic Messages\"), in which he described the first cryptanalytic techniques, including some for polyalphabetic ciphers, cipher classification, Arabic phonetics and syntax, and most importantly, gave the first descriptions on frequency analysis. He also covered methods of encipherments, cryptanalysis of certain encipherments, and statistical analysis of letters and letter combinations in Arabic.\n\nIn early medieval England between the years 800-1100, substitution ciphers were frequently used by scribes as a playful and clever way encipher notes, solutions to riddles, and colophons. The ciphers tend to be fairly straightforward, but sometimes they deviate from an ordinary pattern, adding to their complexity and, possibly, to their sophistication as well. This period saw vital and significant cryptographic experimentation in the West. \n\nAhmad al-Qalqashandi (AD 1355–1418) wrote the \"Subh al-a 'sha\", a 14-volume encyclopedia which included a section on cryptology. This information was attributed to Ibn al-Durayhim who lived from AD 1312 to 1361, but whose writings on cryptography have been lost. The list of ciphers in this work included both substitution and transposition, and for the first time, a cipher with multiple substitutions for each plaintext letter. Also traced to Ibn al-Durayhim is an exposition on and worked example of cryptanalysis, including the use of tables of letter frequencies and sets of letters which cannot occur together in one word.\n\nThe earliest example of the homophonic substitution cipher is the one used by Duke of Mantua in the early 1400s. Homophonic cipher replaces each letter with multiple symbols depending on the letter frequency. The cipher is ahead of the time because it combines monoalphabetic and polyalphabetic features.\n\nEssentially all ciphers remained vulnerable to the cryptanalytic technique of frequency analysis until the development of the polyalphabetic cipher, and many remained so thereafter. The polyalphabetic cipher was most clearly explained by Leon Battista Alberti around the year AD 1467, for which he was called the \"father of Western cryptology\". Johannes Trithemius, in his work Poligraphia, invented the tabula recta, a critical component of the Vigenère cipher. Trithemius also wrote the Steganographia. The French cryptographer Blaise de Vigenère devised a practical polyalphabetic system which bears his name, the Vigenère cipher.\n\nIn Europe, cryptography became (secretly) more important as a consequence of political competition and religious revolution. For instance, in Europe during and after the Renaissance, citizens of the various Italian states—the Papal States and the Roman Catholic Church included—were responsible for rapid proliferation of cryptographic techniques, few of which reflect understanding (or even knowledge) of Alberti's polyalphabetic advance. 'Advanced ciphers', even after Alberti, weren't as advanced as their inventors / developers / users claimed (and probably even themselves believed). They were regularly broken. This over-optimism may be inherent in cryptography, for it was then - and remains today - fundamentally difficult to accurately know how vulnerable one's system actually is. In the absence of knowledge, guesses and hopes, predictably, are common.\n\nCryptography, cryptanalysis, and secret-agent/courier betrayal featured in the Babington plot during the reign of Queen Elizabeth I which led to the execution of Mary, Queen of Scots. Robert Hooke suggested in the chapter \"Of Dr. Dee's Book of Spirits\", that John Dee made use of Trithemian steganography, to conceal his communication with Queen Elizabeth I.\n\nThe chief cryptographer of King Louis XIV of France was Antoine Rossignol and he and his family created what is known as the Great Cipher because it remained unsolved from its initial use until 1890, when French military cryptanalyst, Étienne Bazeries solved it. An encrypted message from the time of the Man in the Iron Mask (decrypted just prior to 1900 by Étienne Bazeries) has shed some, regrettably non-definitive, light on the identity of that real, if legendary and unfortunate, prisoner.\n\nOutside of Europe, after the Mongols brought about the end of the Muslim Golden Age, cryptography remained comparatively undeveloped. Cryptography in Japan seems not to have been used until about 1510, and advanced techniques were not known until after the opening of the country to the West beginning in the 1860s.\n\nAlthough cryptography has a long and complex history, it wasn't until the 19th century that it developed anything more than ad hoc approaches to either encryption or cryptanalysis (the science of finding weaknesses in crypto systems). Examples of the latter include Charles Babbage's Crimean War era work on mathematical cryptanalysis of polyalphabetic ciphers, redeveloped and published somewhat later by the Prussian Friedrich Kasiski. Understanding of cryptography at this time typically consisted of hard-won rules of thumb; see, for example, Auguste Kerckhoffs' cryptographic writings in the latter 19th century. Edgar Allan Poe used systematic methods to solve ciphers in the 1840s. In particular he placed a notice of his abilities in the Philadelphia paper \"Alexander's Weekly (Express) Messenger\", inviting submissions of ciphers, of which he proceeded to solve almost all. His success created a public stir for some months. He later wrote an essay on methods of cryptography which proved useful as an introduction for novice British cryptanalysts attempting to break German codes and ciphers during World War I, and a famous story, \"The Gold-Bug\", in which cryptanalysis was a prominent element.\n\nCryptography, and its misuse, were involved in the execution of Mata Hari and in Dreyfus' conviction and imprisonment, both in the early 20th century. Cryptographers were also involved in exposing the machinations which had led to the Dreyfus affair; Mata Hari, in contrast, was shot.\n\nIn World War I the Admiralty's Room 40 broke German naval codes and played an important role in several naval engagements during the war, notably in detecting major German sorties into the North Sea that led to the battles of Dogger Bank and Jutland as the British fleet was sent out to intercept them. However its most important contribution was probably in decrypting the Zimmermann Telegram, a cable from the German Foreign Office sent via Washington to its ambassador Heinrich von Eckardt in Mexico which played a major part in bringing the United States into the war.\n\nIn 1917, Gilbert Vernam proposed a teleprinter cipher in which a previously prepared key, kept on paper tape, is combined character by character with the plaintext message to produce the cyphertext. This led to the development of electromechanical devices as cipher machines, and to the only unbreakable cipher, the one time pad.\n\nDuring the 1920s, Polish naval-officers assisted the Japanese military with code and cipher development.\n\nMathematical methods proliferated in the period prior to World War II (notably in William F. Friedman's application of statistical techniques to cryptanalysis and cipher development and in Marian Rejewski's initial break into the German Army's version of the Enigma system in 1932).\n\nBy World War II, mechanical and electromechanical cipher machines were in wide use, although—where such machines were impractical—manual systems continued in use. Great advances were made in both cipher design and cryptanalysis, all in secrecy. Information about this period has begun to be declassified as the official British 50-year secrecy period has come to an end, as US archives have slowly opened, and as assorted memoirs and articles have appeared.\n\nThe Germans made heavy use, in several variants, of an electromechanical rotor machine known as Enigma. Mathematician Marian Rejewski, at Poland's Cipher Bureau, in December 1932 deduced the detailed structure of the German Army Enigma, using mathematics and limited documentation supplied by Captain Gustave Bertrand of French military intelligence. This was the greatest breakthrough in cryptanalysis in a thousand years and more, according to historian David Kahn. Rejewski and his mathematical Cipher Bureau colleagues, Jerzy Różycki and Henryk Zygalski, continued reading Enigma and keeping pace with the evolution of the German Army machine's components and encipherment procedures. As the Poles' resources became strained by the changes being introduced by the Germans, and as war loomed, the Cipher Bureau, on the Polish General Staff's instructions, on 25 July 1939, at Warsaw, initiated French and British intelligence representatives into the secrets of Enigma decryption.\n\nSoon after the Invasion of Poland by Germany on 1 September 1939, key Cipher Bureau personnel were evacuated southeastward; on 17 September, as the Soviet Union attacked Poland from the East, they crossed into Romania. From there they reached Paris, France; at PC Bruno, near Paris, they continued breaking Enigma, collaborating with British cryptologists at Bletchley Park as the British got up to speed on breaking Enigma. In due course, the British cryptographers - whose ranks included many chess masters and mathematics dons such as Gordon Welchman, Max Newman, and Alan Turing (the conceptual founder of modern computing) - substantially advanced the scale and technology of Enigma decryption.\n\nGerman code breaking in World War II also had some success, most importantly by breaking the Naval Cypher No. 3. This enabled them to track and sink Atlantic convoys. It was only Ultra intelligence that finally persuaded the admiralty to change their codes in June 1943. This is surprising given the success of the British Room 40 code breakers in the previous world war.\nAt the end of the War, on 19 April 1945, Britain's top military officers were told that they could never reveal that the German Enigma cipher had been broken because it would give the defeated enemy the chance to say they \"were not well and fairly beaten\".\n\nUS Navy cryptographers (with cooperation from British and Dutch cryptographers after 1940) broke into several Japanese Navy crypto systems. The break into one of them, JN-25, famously led to the US victory in the Battle of Midway; and to the publication of that fact in the Chicago Tribune shortly after the battle, though the Japanese seem not to have noticed for they kept using the JN-25 system. A US Army group, the SIS, managed to break the highest security Japanese diplomatic cipher system (an electromechanical 'stepping switch' machine called Purple by the Americans) even before World War II began. The Americans referred to the intelligence resulting from cryptanalysis, perhaps especially that from the Purple machine, as 'Magic'. The British eventually settled on 'Ultra' for intelligence resulting from cryptanalysis, particularly that from message traffic protected by the various Enigmas. An earlier British term for Ultra had been 'Boniface' in an attempt to suggest, if betrayed, that it might have an individual agent as a source.\n\nThe German military also deployed several mechanical attempts at a one-time pad. Bletchley Park called them the Fish ciphers, and Max Newman and colleagues designed and deployed the Heath Robinson, and then the world's first programmable digital electronic computer, the Colossus, to help with their cryptanalysis. The German Foreign Office began to use the one-time pad in 1919; some of this traffic was read in World War II partly as the result of recovery of some key material in South America that was discarded without sufficient care by a German courier.\n\nThe Japanese Foreign Office used a locally developed electrical stepping switch based system (called Purple by the US), and also had used several similar machines for attaches in some Japanese embassies. One of the electrical stepping switch based systems referred to earlier as Purple was called the 'M-machine' by the US, another was referred to as 'Red'. All were broken, to one degree or another, by the Allies.\n\nAllied cipher machines used in World War II included the British TypeX and the American SIGABA; both were electromechanical rotor designs similar in spirit to the Enigma, albeit with major improvements. Neither is known to have been broken by anyone during the War. The Poles used the Lacida machine, but its security was found to be less than intended (by Polish Army cryptographers in the UK), and its use was discontinued. US troops in the field used the M-209 and the still less secure M-94 family machines. British SOE agents initially used 'poem ciphers' (memorized poems were the encryption/decryption keys), but later in the War, they began to switch to one-time pads.\n\nThe VIC cipher (used at least until 1957 in connection with Rudolf Abel's NY spy ring) was a very complex hand cipher, and is claimed to be the most complicated known to have been used by the Soviets, according to David Kahn in \"Kahn on Codes\". For the decrypting of Soviet ciphers (particularly when \"one-time pads\" were reused), see Venona project.\n\nEncryption in modern times is achieved by using algorithms that have a key to encrypt and decrypt information. These keys convert the messages and data into \"digital gibberish\" through encryption and then return them to the original form through decryption. In general, the longer the key is, the more difficult it is to crack the code. This holds true because deciphering an encrypted message by brute force would require the attacker to try every possible key. To put this in context, each binary unit of information, or bit, has a value of 0 or 1. An 8-bit key would then have 256 or 2^8 possible keys. A 56-bit key would have 2^56, or 72 quadrillion, possible keys to try and decipher the message. With modern technology, cyphers using keys with these lengths are becoming easier to decipher. DES, an early US Government approved cypher, has an effective key length of 56 bits, and test messages using that cypher have been broken by brute force key search. However, as technology advances, so does the quality of encryption. Since World War II, one of the most notable advances in the study of cryptography is the introduction of the asymmetric key cyphers (sometimes termed public-key cyphers). These are algorithms which use two mathematically related keys for encryption of the same message. Some of these algorithms permit publication of one of the keys, due to it being extremely difficult to determine one key simply from knowledge of the other.\n\nBeginning around 1990, the use of the Internet for commercial purposes and the introduction of commercial transactions over the Internet called for a widespread standard for encryption. Before the introduction of the Advanced Encryption Standard (AES), information sent over the Internet, such as financial data, was encrypted if at all, most commonly using the Data Encryption Standard (DES). This had been approved by NBS (a US Government agency) for its security, after public call for, and a comptetition among, candidates for such a cypher algorithm. DES was approved for a short period, but saw extended use due to complex wrangles over the use by the public of high quality encryption. DES was finally replaced by the AES after another public competition organized by the NBS successor agency, NIST. Around the late 1990s to early 2000s, the use of public-key algorithms became a more common approach for encryption, and soon a hybrid of the two schemes became the most accepted way for e-commerce operations to proceed. Additionally, the creation of a new protocol known as the Secure Socket Layer, or SSL, led the way for online transactions to take place. Transactions ranging from purchasing goods to online bill pay and banking used SSL. Furthermore, as wireless Internet connections became more common among households, the need for encryption grew, as a level of security was needed in these everyday situations.\n\nClaude E. Shannon is considered by many to be the father of mathematical cryptography. Shannon worked for several years at Bell Labs, and during his time there, he produced an article entitled \"A mathematical theory of cryptography\". This article was written in 1945 and eventually was published in the Bell System Technical Journal in 1949. It is commonly accepted that this paper was the starting point for development of modern cryptography. Shannon was inspired during the war to address \"[t]he problems of cryptography [because] secrecy systems furnish an interesting application of communication theory\". Shannon identified the two main goals of cryptography: secrecy and authenticity. His focus was on exploring secrecy and thirty-five years later, G.J. Simmons would address the issue of authenticity. Shannon wrote a further article entitled \"A mathematical theory of communication\" which highlights one of the most significant aspects of his work: cryptography's transition from art to science.\n\nIn his works, Shannon described the two basic types of systems for secrecy. The first are those designed with the intent to protect against hackers and attackers who have infinite resources with which to decode a message (theoretical secrecy, now unconditional security), and the second are those designed to protect against hackers and attacks with finite resources with which to decode a message (practical secrecy, now computational security). Most of Shannon's work focused around theoretical secrecy; here, Shannon introduced a definition for the \"unbreakability\" of a cipher. If a cipher was determined \"unbreakable\", it was considered to have \"perfect secrecy\". In proving \"perfect secrecy\", Shannon determined that this could only be obtained with a secret key whose length given in binary digits was greater than or equal to the number of bits contained in the information being encrypted. Furthermore, Shannon developed the \"unicity distance\", defined as the \"amount of plaintext that… determines the secret key.\"\n\nShannon's work influenced further cryptography research in the 1970s, as the public-key cryptography developers, M. E. Hellman and W. Diffie cited Shannon's research as a major influence. His work also impacted modern designs of secret-key ciphers. At the end of Shannon's work with cryptography, progress slowed until Hellman and Diffie introduced their paper involving \"public-key cryptography\".\n\nThe mid-1970s saw two major public (i.e., non-secret) advances. First was the publication of the draft Data Encryption Standard in the U.S. \"Federal Register\" on 17 March 1975. The proposed DES cipher was submitted by a research group at IBM, at the invitation of the National Bureau of Standards (now NIST), in an effort to develop secure electronic communication facilities for businesses such as banks and other large financial organizations. After advice and modification by the NSA, acting behind the scenes, it was adopted and published as a Federal Information Processing Standard Publication in 1977 (currently at FIPS 46-3). DES was the first publicly accessible cipher to be 'blessed' by a national agency such as the NSA. The release of its specification by NBS stimulated an explosion of public and academic interest in cryptography.\n\nThe aging DES was officially replaced by the Advanced Encryption Standard (AES) in 2001 when NIST announced FIPS 197. After an open competition, NIST selected Rijndael, submitted by two Belgian cryptographers, to be the AES. DES, and more secure variants of it (such as Triple DES), are still used today, having been incorporated into many national and organizational standards. However, its 56-bit key-size has been shown to be insufficient to guard against brute force attacks (one such attack, undertaken by the cyber civil-rights group Electronic Frontier Foundation in 1997, succeeded in 56 hours.) As a result, use of straight DES encryption is now without doubt insecure for use in new cryptosystem designs, and messages protected by older cryptosystems using DES, and indeed all messages sent since 1976 using DES, are also at risk. Regardless of DES' inherent quality, the DES key size (56-bits) was thought to be too small by some even in 1976, perhaps most publicly by Whitfield Diffie. There was suspicion that government organizations even then had sufficient computing power to break DES messages; clearly others have achieved this capability.\n\nThe second development, in 1976, was perhaps even more important, for it fundamentally changed the way cryptosystems might work. This was the publication of the paper New Directions in Cryptography by Whitfield Diffie and Martin Hellman. It introduced a radically new method of distributing cryptographic keys, which went far toward solving one of the fundamental problems of cryptography, key distribution, and has become known as Diffie–Hellman key exchange. The article also stimulated the almost immediate public development of a new class of enciphering algorithms, the asymmetric key algorithms.\n\nPrior to that time, all useful modern encryption algorithms had been symmetric key algorithms, in which the same cryptographic key is used with the underlying algorithm by both the sender and the recipient, who must both keep it secret. All of the electromechanical machines used in World War II were of this logical class, as were the Caesar and Atbash ciphers and essentially all cipher systems throughout history. The 'key' for a code is, of course, the codebook, which must likewise be distributed and kept secret, and so shares most of the same problems in practice.\n\nOf necessity, the key in every such system had to be exchanged between the communicating parties in some secure way prior to any use of the system (the term usually used is 'via a secure channel') such as a trustworthy courier with a briefcase handcuffed to a wrist, or face-to-face contact, or a loyal carrier pigeon. This requirement is never trivial and very rapidly becomes unmanageable as the number of participants increases, or when secure channels aren't available for key exchange, or when, as is sensible cryptographic practice, keys are frequently changed. In particular, if messages are meant to be secure from other users, a separate key is required for each possible pair of users. A system of this kind is known as a secret key, or symmetric key cryptosystem. D-H key exchange (and succeeding improvements and variants) made operation of these systems much easier, and more secure, than had ever been possible before in all of history.\n\nIn contrast, asymmetric key encryption uses a pair of mathematically related keys, each of which decrypts the encryption performed using the other. Some, but not all, of these algorithms have the additional property that one of the paired keys cannot be deduced from the other by any known method other than trial and error. An algorithm of this kind is known as a public key or asymmetric key system. Using such an algorithm, only one key pair is needed per user. By designating one key of the pair as private (always secret), and the other as public (often widely available), no secure channel is needed for key exchange. So long as the private key stays secret, the public key can be widely known for a very long time without compromising security, making it safe to reuse the same key pair indefinitely.\n\nFor two users of an asymmetric key algorithm to communicate securely over an insecure channel, each user will need to know their own public and private keys as well as the other user's public key. Take this basic scenario: Alice and Bob each have a pair of keys they've been using for years with many other users. At the start of their message, they exchange public keys, unencrypted over an insecure line. Alice then encrypts a message using her private key, and then re-encrypts that result using Bob's public key. The double-encrypted message is then sent as digital data over a wire from Alice to Bob. Bob receives the bit stream and decrypts it using his own private key, and then decrypts that bit stream using Alice's public key. If the final result is recognizable as a message, Bob can be confident that the message actually came from someone who knows Alice's private key (presumably actually her if she's been careful with her private key), and that anyone eavesdropping on the channel will need Bob's private key in order to understand the message.\n\nAsymmetric algorithms rely for their effectiveness on a class of problems in mathematics called one-way functions, which require relatively little computational power to execute, but vast amounts of power to reverse, if reversal is possible at all. A classic example of a one-way function is multiplication of very large prime numbers. It's fairly quick to multiply two large primes, but very difficult to find the factors of the product of two large primes. Because of the mathematics of one-way functions, most possible keys are bad choices as cryptographic keys; only a small fraction of the possible keys of a given length are suitable, and so asymmetric algorithms require very long keys to reach the same level of security provided by relatively shorter symmetric keys. The need to both generate the key pairs, and perform the encryption/decryption operations make asymmetric algorithms computationally expensive, compared to most symmetric algorithms. Since symmetric algorithms can often use any sequence of (random, or at least unpredictable) bits as a key, a disposable \"session key\" can be quickly generated for short-term use. Consequently, it is common practice to use a long asymmetric key to exchange a disposable, much shorter (but just as strong) symmetric key. The slower asymmetric algorithm securely sends a symmetric session key, and the faster symmetric algorithm takes over for the remainder of the message.\n\nAsymmetric key cryptography, Diffie–Hellman key exchange, and the best known of the public key / private key algorithms (i.e., what is usually called the RSA algorithm), all seem to have been independently developed at a UK intelligence agency before the public announcement by Diffie and Hellman in 1976. GCHQ has released documents claiming they had developed public key cryptography before the publication of Diffie and Hellman's paper. Various classified papers were written at GCHQ during the 1960s and 1970s which eventually led to schemes essentially identical to RSA encryption and to Diffie–Hellman key exchange in 1973 and 1974. Some of these have now been published, and the inventors (James H. Ellis, Clifford Cocks, and Malcolm Williamson) have made public (some of) their work.\n\nHashing is a common technique used in cryptography to encode information quickly using typical algorithms. Generally, an algorithm is applied to a string of text, and the resulting string becomes the \"hash value\". This creates a \"digital fingerprint\" of the message, as the specific hash value is used to identify a specific message. The output from the algorithm is also referred to as a \"message digest\" or a \"check sum\". Hashing is good for determining if information has been changed in transmission. If the hash value is different upon reception than upon sending, there is evidence the message has been altered. Once the algorithm has been applied to the data to be hashed, the hash function produces a fixed-length output. Essentially, anything passed through the hash function should resolve to the same length output as anything else passed through the same hash function. It is important to note that hashing is not the same as encrypting. Hashing is a one-way operation that is used to transform data into the compressed message digest. Additionally, the integrity of the message can be measured with hashing. Conversely, encryption is a two-way operation that is used to transform plaintext into cipher-text and then vice versa. In encryption, the confidentiality of a message is guaranteed.\n\nHash functions can be used to verify digital signatures, so that when signing documents via the Internet, the signature is applied to one particular individual. Much like a hand-written signature, these signatures are verified by assigning their exact hash code to a person. Furthermore, hashing is applied to passwords for computer systems. Hashing for passwords began with the UNIX operating system. A user on the system would first create a password. That password would be hashed, using an algorithm or key, and then stored in a password file. This is still prominent today, as web applications that require passwords will often hash user's passwords and store them in a database.\n\nThe public developments of the 1970s broke the near monopoly on high quality cryptography held by government organizations (see S Levy's \"Crypto\" for a journalistic account of some of the policy controversy of the time in the US). For the first time ever, those outside government organizations had access to cryptography not readily breakable by anyone (including governments). Considerable controversy, and conflict, both public and private, began more or less immediately, sometimes called the crypto wars. They have not yet subsided. In many countries, for example, export of cryptography is subject to restrictions. Until 1996 export from the U.S. of cryptography using keys longer than 40 bits (too small to be very secure against a knowledgeable attacker) was sharply limited. As recently as 2004, former FBI Director Louis Freeh, testifying before the 9/11 Commission, called for new laws against public use of encryption.\n\nOne of the most significant people favoring strong encryption for public use was Phil Zimmermann. He wrote and then in 1991 released PGP (Pretty Good Privacy), a very high quality crypto system. He distributed a freeware version of PGP when he felt threatened by legislation then under consideration by the US Government that would require backdoors to be included in all cryptographic products developed within the US. His system was released worldwide shortly after he released it in the US, and that began a long criminal investigation of him by the US Government Justice Department for the alleged violation of export restrictions. The Justice Department eventually dropped its case against Zimmermann, and the freeware distribution of PGP has continued around the world. PGP even eventually became an open Internet standard (RFC 2440 or OpenPGP).\n\nWhile modern ciphers like AES and the higher quality asymmetric ciphers are widely considered unbreakable, poor designs and implementations are still sometimes adopted and there have been important cryptanalytic breaks of deployed crypto systems in recent years. Notable examples of broken crypto designs include the first Wi-Fi encryption scheme WEP, the Content Scrambling System used for encrypting and controlling DVD use, the A5/1 and A5/2 ciphers used in GSM cell phones, and the CRYPTO1 cipher used in the widely deployed MIFARE Classic smart cards from NXP Semiconductors, a spun off division of Philips Electronics. All of these are symmetric ciphers. Thus far, not one of the mathematical ideas underlying public key cryptography has been proven to be 'unbreakable', and so some future mathematical analysis advance might render systems relying on them insecure. While few informed observers foresee such a breakthrough, the key size recommended for security as best practice keeps increasing as increased computing power required for breaking codes becomes cheaper and more available.\n\n\n"}
{"id": "185982", "url": "https://en.wikipedia.org/wiki?curid=185982", "title": "Home appliance", "text": "Home appliance\n\nHome appliances are electrical/mechanical machines which accomplish some household functions, such as cooking, cleaning, or food preservation.\n\nHome appliances can be divided into three classifications, which include:\n\nThis division is also noticeable in the maintenance and repair of these kinds of products. Brown goods usually require high technical knowledge and skills (which get more complex with time, such as going from a soldering iron to a hot-air soldering station), while white goods may need more practical skills and force to manipulate the devices and heavy tools required to repair them.\n\nGiven a broad usage, the domestic application attached to \"home appliance\" is tied to the definition of appliance as \"an instrument or device designed for a particular use or function\". More specifically, Collins dictionary defines \"home appliance\" as: \"devices or machines, usually electrical, that are in your home and which you use to do jobs such as cleaning or cooking.\" The broad usage, afforded to the definition allows for nearly any device intended for domestic use to be a home appliance, including consumer electronics as well as stoves, refrigerators, toasters and air conditioners to light bulbs and water well pumps.\n\nWhile many appliances have existed for centuries, the self-contained electric or gas powered appliances are a uniquely American innovation that emerged in the twentieth century. The development of these appliances is tied to the disappearance of full-time domestic servants and the desire to reduce the time-consuming activities in pursuit of more recreational time. In the early 1900s, electric and gas appliances included washing machines, water heaters, refrigerators and sewing machines. The invention of Earl Richardson's small electric clothes iron in 1903 gave a small initial boost to the home appliance industry. In the Post–World War II economic expansion, the domestic use of dishwashers, and clothes dryers were part of a shift for convenience. Increasing discretionary income was reflected by a rise in miscellaneous home appliances.\n\nIn America during the 1980s, the industry shipped $1.5 billion worth of goods each year and employed over 14,000 workers, with revenues doubling between 1982 and 1990 to $3.3 billion. Throughout this period companies merged and acquired one another to reduce research and production costs and eliminate competitors, resulting in anti-trust legislation.\n\nThe United States Department of Energy reviews compliance with the National Appliance Energy Conservation Act of 1987, which required manufacturers to reduce the energy consumption of the appliances by 25% every five years.\n\nIn the 1990s, the appliance industry was very consolidated, with over 90% of the products being sold by just five companies. For example, in 1991, dishwasher manufacturing market share was split between General Electric with 40% market share, Whirlpool with 31% market share, Electrolux with 20% market share, Maytag with 7% market share and Thermador with just 2% of market share.\n\nMajor appliances, also known as white goods, comprise major household appliances and may include: air conditioners, dishwashers, clothes dryers, drying cabinets, freezers, refrigerators, kitchen stoves, water heaters, washing machines, trash compactors, microwave ovens, and induction cookers. White goods were typically painted or enameled white, and many of them still are.\n\nSmall appliances are typically small household electrical machines, also very useful and easily carried and installed. Yet another category is used in the kitchen, including: juicers, electric mixers, meat grinders, coffee grinders, deep fryers, herb grinders, food processors, electric kettles, waffle irons, coffee makers, blenders and dough blenders, rice cookers, toasters and exhaust hoods.\n\nEntertainment and information appliances such as: home electronics, TV sets, CD, VCRs and DVD players, camcorders, still cameras, clocks, alarm clocks, computers, video game consoles, HiFi and home cinema, telephones and answering machines are classified as \"brown goods\". Some such appliances were traditionally finished with genuine or imitation wood, hence the name. This has become rare but the name has stuck, even for goods that are unlikely ever to have had a wooden case (e.g. camcorders).\n\nSee Life spans of home appliances\n\nThere is a trend of networking home appliances together, and combining their controls and key functions. For instance, energy distribution could be managed more evenly so that when a washing machine is on, an oven can go into a delayed start mode, or vice versa. Or, a washing machine and clothes dryer could share information about load characteristics (gentle/normal, light/full), and synchronize their finish times so the wet laundry does not have to wait before being put in the dryer.\n\nAdditionally, some manufacturers of home appliances are quickly beginning to place hardware that enables Internet connectivity in home appliances to allow for remote control, automation, communication with other home appliances, and more functionality enabling connected cooking. Internet-connected home appliances were especially prevalent during recent Consumer Electronic Show events.\n\nAppliance recycling consists of dismantling waste home appliances and scrapping their parts for reuse. The main types of appliances that are recycled are T.V.s, refrigerators, air conditioners, washing machines, and computers. It involves disassembly, removal of hazardous components and destruction of the equipment to recover materials, generally by shredding, sorting and grading.\n\n\n"}
{"id": "7421038", "url": "https://en.wikipedia.org/wiki?curid=7421038", "title": "K-CASH", "text": "K-CASH\n\nK-CASH is an electronic money system established by 'Korea Financial Telecommunication and Clearings Institute'.\n\nK-CASH card is easily obtained from issuing bank. Several banks are issuing K-CASH card, including two credit card companies as of 2011. Users can load or unload value from their own account at bank counter, store, ATMs and online. Unlike other market dominant \"transport cards\", unloading K-CASH value to owner's account is done real-time, as a law only permits real-time transaction to electronic money, not to prepaid card.\n\nSouth Korean Army, and Chuncheon .Hoengseong , Wonju buses adopt K-CASH as a major payment system. At first, K-CASH was mainly promoted as electronic fare collection system. Despite its advantage, such as bank guarantees its payment or real-time transaction, market dominant systems like T-money or Mybi/Cashbee system drive it out of the market. As of November 2011, following cities and railroad systems are using K-CASH as their public transport fare collecting system.\n\n\n"}
{"id": "15023349", "url": "https://en.wikipedia.org/wiki?curid=15023349", "title": "Lisa C. Klein", "text": "Lisa C. Klein\n\nLisa C. Klein (born 1951) is an American engineer, Graduate Director at Rutgers University and President of the American Association of University Professors (AAUP) at the University.\n\nShe has made many breakthroughs in engineering. Klein's research focuses on the area of sol-gel science, a low-temperature process for making ceramic coatings. Her most important contribution to science has been the development of electrochromic window coatings that can be lightened or darkened through the use of a dimmer attached to a battery. This window coating is unique because it is manually controlled and can reflect the heat away, while still transmitting light in the summer or permit the solar heat in winter. This new development is more efficient than blinds, curtains, or tinted windows and can save on heating and cooling costs in the home or office.\n\nKlein is also noteworthy for her status at Rutgers University: she was the first female faculty member in the Rutgers School of Engineering (1977), the first female faculty member in the department of Materials Science and Engineering, the first female faculty member elevated to the distinguished PII rank, and, as of January, 2008, the only female professor in the department. As President of the Rutgers AAUP chapter Klein also concluded groundbreaking negotiations that lead to novel methods for expanding the number of tenure track faculty at Rutgers that have been cited as potential models for faculty efforts to halt the attrition in numbers of tenure track faculty.\n"}
{"id": "42208759", "url": "https://en.wikipedia.org/wiki?curid=42208759", "title": "List of smoked foods", "text": "List of smoked foods\n\nThis is a list of smoked foods. Smoking is the process of flavoring, cooking, or preserving food by exposing it to smoke from burning or smoldering material, most often wood. Foods have been smoked by humans throughout history. Meats and fish are the most common smoked foods, though cheeses, vegetables, and ingredients used to make beverages such as whisky, smoked beer, and \"lapsang souchong\" tea are also smoked. Smoked beverages are also included in this list.\n\n\nSmoked cheese is any cheese that has been specially treated by smoke-curing. It typically has a yellowish-brown outer pellicle which is a result of this curing process.\n\n\n\nSmoked fish is fish that has been cured by smoking. This was originally done as a preservative.\n\nSmoked meat is a method of preparing red meat (and fish) which originates in prehistory. Its purpose is to preserve these protein-rich foods, which would otherwise spoil quickly, for long periods. There are two mechanisms for this preservation: dehydration and the antibacterial properties of absorbed smoke. In modern days, the enhanced flavor of smoked foods makes them a delicacy in many cultures.\n\n\nSausage is a food usually made from ground meat with a skin around it. Typically, a sausage is formed in a casing traditionally made from intestine, but sometimes synthetic. Sausage making is a traditional food preservation technique. Sausages may be preserved by curing, drying, or smoking. Many types and varieties of sausages are smoked to help preserve them and to add flavor.\n\n\n\n\n"}
{"id": "2805105", "url": "https://en.wikipedia.org/wiki?curid=2805105", "title": "Lyot stop", "text": "Lyot stop\n\nA Lyot stop is an optic stop, invented by French astronomer Bernard Lyot, that reduces the amount of flare caused by diffraction of other stops and baffles in optical systems.\n\nLyot Stop is located at the image of the objective formed by a field lens, with an aperture slightly smaller than the objective image. Example of applications can be found in Ref.\n\n\n"}
{"id": "2681904", "url": "https://en.wikipedia.org/wiki?curid=2681904", "title": "MEMS sensor generations", "text": "MEMS sensor generations\n\nMEMS sensor generations represent the progress made in micro sensor technology and can be categorized as follows:\n\n\n\n\n"}
{"id": "41200493", "url": "https://en.wikipedia.org/wiki?curid=41200493", "title": "Modular process skid", "text": "Modular process skid\n\nA modular process skid is a process system contained within a frame that allows the process system to be easily transported. Individual skids can contain complete process systems and multiple process skids can be combined to create larger process systems or entire portable plants. They are sometimes called “a system in a box.” An example of a multi-skid process system might include a raw materials skid, a utilities skid and a processing unit which work in tandem.\n\nProcess skids are considered an alternative to traditional stick-built construction where process system parts are shipped individually and installed incrementally at the manufacturing site. They provide the advantage of parallel construction, where process systems are built off-site in a fabrication facility while civil site upgrades are completed at the plant site simultaneously. Skids are not always appropriate. If individual process parts are large and cannot reasonably be contained within the frame of a modular process skid, traditional construction methods are preferred.\n\nProcess skids are designed to contain a complete process system, a complete unit of operations or to organize a manufacturing process into logical units. All skids have the following characteristics in common:\n\nModular process skids typically contain the following equipment:\n\n"}
{"id": "22730221", "url": "https://en.wikipedia.org/wiki?curid=22730221", "title": "Nanoarchitectures for lithium-ion batteries", "text": "Nanoarchitectures for lithium-ion batteries\n\nNanoarchitectures for lithium-ion batteries are attempts to employ nanotechnology to improve the design of lithium-ion batteries. Research in lithium-ion batteries focuses on improving energy density, power density, safety, durability and cost.\n\nIncreased energy density requires inserting/extracting more ions from the electrodes. Electrode capacities are compared through three different measures: capacity per unit of mass (known as \"specific energy\" or \"gravimetric capacity\"), capacity per unit volume (\"volumetric capacity\"), and area-normalized specific capacity (\"areal capacity\").\n\nSeparate efforts focus on improving power density (rate of charge/discharge). Power density is based upon mass and charge transport, electronic and ionic conductivity, and electron-transfer kinetics; easy transport through shorter distance and greater surface area improve the rates.\n\nCarbon anodes are traditionally used because of lithium's ability to intercalate without unacceptable volumetric expansion. The latter damages the battery and reduces the amount of lithium available for charging. Reduced intercalation limits capacity. Carbon based anodes have a gravimetric capacity of 372 mAh/g for LiC\n\nThe specific capacity of silicon is approximately ten times greater than carbon. The atomic radius of Si is 1.46 angstroms, while the atomic radius of Li is 2.05 angstroms. The formation of LiSi causes significant volumetric expansion, progressively destroying the anode. Reducing the anode architecture to the nanoscale offers advantages, including improved cycle life and reduced crack propagation and failure. Nanoscale particles are below the critical flaw size within a conductive binder film. Reducing transport lengths(the distance between the anode and cathode) reduces ohmic losses (resistance).\n\nNanostructuring increases the surface area to volume ratio, which improves both energy and power density due to an increase in the electrochemically active area and a reduction in transport lengths. However, the increase also increases side reactions between the electrode and the electrolyte, causing higher self-discharge, reduced charge/discharge cycles and lower calendar life. Some recent work focused on developing materials that are electrochemically active within the range where electrolyte decomposition or electrolyte/electrode reactions do not occur.\n\nA research concept has been proposed, in which the major parts of lithium-ion batteries, that is, anode, electrolyte and cathode are combined in one functional molecule. A layer of such functional molecules aligned by the use of Langmuir-Blodgett method than placed in between two current collectors. The feasibility is not confirmed yet.\n\nA significant majority of battery designs are two–dimensional and rely on layered construction. Recent research has taken the electrodes into three-dimensions. This allows for significant improvements in battery capacity; a significant increase in areal capacity occurs between a 2d thick film electrode and a 3d array electrode.\n\nSolid state batteries employ geometry most similar to traditional thin-film batteries. Three-dimensional thin-films use the third dimension to increase the electrochemically active area. Thin film two dimensional batteries are restricted to between 2-5 micrometres, limiting areal capacity to significantly less than that of three-dimensional geometries.\n\nDimensionality is increased by using a perforated substrate. One way to create perforations is through inductive coupled plasma etching on silicon.\n\nAnother approached used highly anisotropic etching of a silicon substrate through electrochemical or reactive ion etching to create deep trenches. The requisite layers, an anode, separator, and cathode, for a battery were then added by low-pressure chemical vapor deposition. The battery consists of a thin active silicon layer separated from a thin cathodic layer by a solid-state electrolyte. The electrochemically active area consists of 50 nm nanoparticles, smaller than the critical size for crack propagation.\n\nAnother architecture is a periodic grouping of anodic and cathodic poles. For this design power and energy density is maximized by minimizing electrode separation. An innate non-uniform current density occurs and lowers cell efficiencies, reduces stability and produces non-uniform heating within the cell. Relative to a two dimensional battery the length (L) over which transport must occur is decreased by two-thirds, which improves kinetics and reduces ohmic loses. Optimization of L can lead to significant improvement in areal capacity; an L on the size scale of 500 micrometres results in a 350% increase in capacity over a comparable two dimensional battery. However, ohmic losses increase with L, eventually offsetting the enhancement achieved through increasing L.\n\nFor this geometry, four main designs were proposed: rows of anodes and cathodes, alternating anodes and cathodes, hexagonally packed 1:2 anodes:cathodes, and alternating anodic and cathodic triangular poles where the nearest neighbors in the row are rotated 180 degrees.\n\nThe row design has a large, non-uniform current distribution. The alternating design exhibits better uniformity, given a high number of electrodes of opposite polarity. For systems with an anode or cathode that is sensitive to non-uniform current density, non-equal numbers of cathodes and anodes can be used; the 2:1 hexagonal design allows for a uniform current density at the anode but a non-uniform current distribution at the cathode. Performance can be increased through changing the shape of the poles. The triangular design improves cell capacity and power by sacrificing current uniformity. A similar system uses interdigitated plates instead of poles.\n\nIn 2013 researchers used additive manufacturing to create stacked, interdigitated electrodes. The battery was no larger than a grain of sand. The process placed anodes and cathodes closer to each other than before. The ink for the anode was nanoparticles of one lithium metal oxide compound, and the ink for the cathode from nanoparticles of another. The printer deposited the inks onto the teeth of two gold combs, forming an interlaced stack of anodes and cathodes.\n\nThe concentric cylinder design is similar to interdigitated poles. Instead of discrete anode and cathode poles, the anode or cathode is kept as a pole that is coated by electrolyte. The other electrode serves as the continuous phase in which the anode/cathode resides. The main advantage is that the amount of electrolyte is reduced, increasing energy density. This design maintains a short transport distance like the interdigitated system and thus has a similar benefit to charge and mass transport, while minimizing ohmic loses.\n\nA version of the concentric cylinder packed particles or close-packed polymer to create a three-dimensionally ordered macroporous (3DOM) carbon anode. This system is fabricated by using colloidal crystal templating, electrochemical thin-film growth, and soft sol–gel chemistry. 3DOM materials have a unique structure of nanometer thick walls that surround interconnected and closed-packed sub-micrometer voids. The 3DOM structure is coated with a thin polymer layer and then filled with second conducting phase. This method leads to a battery with short transport lengths, high ionic conductivity and reasonable electrical conductivity. It removes the need for additives that do not contribute to electrochemical performance. Performance can be improved by coating with tin oxide nanoparticles to enhance the initial capacity. The coating infiltrates the network formed by the 3DOM structure to produce uniform thickness.\n\nNanowire and nanotubes have been integrated with various battery components. The reason for this interest is because of shortened transport lengths, resistance to degradation and storage. For carbon nanotubes (CNT), lithium-ions can be stored on the exterior surface, in the interstitial sites between the nanotubes and on the tube's interior.\n\nNanowires have been incorporated into the anode/cathode matrix to provide a builtin conductive charge collector and enhancing capacity. The nanowires were incorporated through a solution-based method that allows the active material to be printed on a substrate.\n\nAnother approach uses a CNT-cellulose composite. CNTs were grown on a silicon substrate by thermal-CVD and then embedded in cellulose. Finally a lithium electrode is added on top of the cellulose across from the CNTs.\n\nIn 2007 Si nanowires were fabricated on a steel substrate by a vapor-liquid solid growth method. These nanowires exhibited close to the theoretical value for silicon and showed only minimal fading after a 20% drop between the first to second cycles. This performance is attributed to the facile strain relaxation that allows for accommodations of large strains, while maintaining good contact with the current collector and efficient 1D electron transport along the nanowire.\n\nPeriodic structures lead to non-uniform current densities that lower efficiency and decrease stability. The aperoidic structure is typically made of either aerogels or somewhat more dense ambigels that forms a porous aperiodic sponge. Aerogels and ambigels are formed from wet gels; aerogels are formed when wet gels are dried such that no capillary forces are established, while ambigels are wet gels dried under conditions that minimize capillary forces. Aerogels and ambigels are unique in that 75-99% of the material is ‘open’ but interpenetrated by a solid that is on the order of 10 nm, resulting in pores on the order of 10 to 100 nm. The solid is covalently networked and resistant to agglomeration and sintering. Beyond aperiodicity, these structures are used because the porous structure allows for rapid diffusion throughout the material, and the porous structure provides a large reaction surface. Fabrication is through coating the ambigel with a polymer electrolyte and then filling the void space with RuO colloids that act as an anode.\n\nMost designs were half-cell experiments; testing only the anode or cathode. As geometries become more complex, non-line-of-sight methods to in-fill the design with electrolyte materials supply the oppositely charged electrode is essential. These batteries can be coated with various materials to improve their performance and stability. However, chemical and physical heterogeneity leaves molecular-level control a significant challenge, especially since the electrochemistry for energy storage is not defect-tolerant.\n\nLbL approaches are used to coat 3d nanoarchitecture. Electrostatically binding a charged polymer to an oppositely charged surface coats the surface with polymer. Repeated steps of oppositely charged polymer build up a well-controlled thick layer. Polyelectrolyte films and ultrathin (less than 5 nm) of electroactive polymers have been deposited on planar substrates using this method. However, problems exist with the deposition of polymers within complex geometries, e.g. pores, on the size scale of 50-300 nm, resulting in defective coatings. One potential solution is to use self-limiting approaches.\n\nAnother approach to coating is ALD which coats the substrate layer-by-layer with atomic precision. The precision is because reactions are confined to the surface containing an active chemical moiety that reacts with a precursor; this limits thickness to one monolayer. This self-limiting growth is essential for complete coatings since deposition does not inhibit the access by other polymeric units to non-coated sites. Thicker samples can be produced by cycling gases in a similar manner to alternating with oppositely charged polymers in LbL. In practice ALD may require a few cycles in order to achieve the desired coverage and can result in varied morphologies such as islands, isolated crystallites, or nanoparticles. Morphology can alter electrochemical behavior and therefore must be carefully controlled.\n\nALD was also used to deposit iron oxide on 3DOM carbon to enhance reactivity between lithium and oxygen. The iron was then coatedwith palladium nanoparticles, which effectively reduced carbon's destructive reaction with oxygen and improved the discharge cycle. Wang said the findings show 3DOm carbon can meet new performance standards when it is stabilized.\n\nElectropolymerization supplies a thin polymer film, 10 to 100 nm. The electropolymerization of an insulating polymer results in self-limiting deposition as the active moiety is protected; the deposition can also be self-limiting if the polymer can block the solubilized monomer and prohibit continued growth. Through the control of electrochemical variables, polyaniline and polythiophene can be deposited in a controlled manner. Styrene, methyl methacrylate, phenols and other electrically insulating polymers have been deposited on the electrodes to act as a separator that allows ionic transport, but inhibits electrical transport to prevent shorts. Mesoporous manganese dioxide ambigels have been protected by 7-9 nm films of polymer such that dissolution of the manganese dioxide in aqueous acid was avoided. Uniform coatings require the architecture to be wetted by the monomer solution; this can be achieved through a solution that displays a similar surface energy to that of the porous solid. As the scale continuous to decrease and transport through the solid becomes more difficult, pre-equilibration is needed to ensure coating uniformity.\n"}
{"id": "26111009", "url": "https://en.wikipedia.org/wiki?curid=26111009", "title": "OpenSeaMap", "text": "OpenSeaMap\n\nOpenSeaMap is a software project collecting freely usable nautical information and geospatial data to create a worldwide nautical chart. This chart is available on the OpenSeaMap website, and can also be downloaded for use as an electronic chart for offline applications.\n\nThe project is part of OpenStreetMap. OpenSeaMap uses the same database, and complements the spatial data with nautical information. Such data may be used in accordance with the Open Database License. This ensures integration into printed materials, websites and applications is possible, without being limited by restrictive licenses, or having to pay fees.\n\nThe idea for the project was born at an OpenStreetMap developer conference in autumn 2008 at the Linux Hotel in Essen, Germany. A group of boaters and programmers decided to extend the coverage of OpenStreetMap to the seas and fresh water bodies. From the start the project has been worldwide and multilingual. By the end of 2009, the design and architecture of the project had been created, and a sample harbor \"Warnemünde\" was created to serve as an example chart. Since autumn 2009, a dedicated server has been available and the project is engaged in several collaborations with other free projects and organizations.\nIn January 2010 OpenSeaMap was given a stand at boot Düsseldorf, Europe's largest boat show, allowing volunteers to present the project to a large audience of specialists for the first time.\n\nCharts will show lighthouses, lateral buoy, cardinal marks and other navigational aids. In the ports, facilities will be mapped (port wall, pier, walkways, docks, fueling stations, loading cranes, access roads, railway lines, ferry lines). Similarly, public authorities, shipbuilders and repairers, as well as sanitation and utility facilities will be displayed. The navigational attributes correspond to the international standard IHO S-57.\n\nWater depths are not yet covered, because the database is not designed for three-dimensional coordinates. However, the plan is to eventually integrate a bathymetric model to describe the seabed.\n\nThe data are presented in multiple levels with OpenLayers on the base map of OpenStreetMap. The base map contains all the possible objects from OpenStreetMap. OpenSeaMap includes additional layers such as aids to navigation, ports and temporary racing events.\n\nThe chart is for planning sailing and boat trips. It will also be useful as a guide for tourists. It is not intended to replace official charts.\n\n\n\n\n\nThe site is translated into six languages: English, German, French, Italian, Spanish and Russian. The tools and the legends are available with a German, English and French interface. On the map the place names are always written in local language and script. The geographical coverage is worldwide. Depending on the region and the active cartographers, the coverage varies but is growing daily.\n\nData is stored in the database as soon as it has been entered, and is immediately available worldwide. Base map data is visible on the map after a few minutes. Navigational data is currently (2010) visible on the chart about two days after it has been entered.\n\n\nTo edit you must register with a verified email address. Registration is free and will encourage high data quality. Read access does not require registration.\n\nOther programs can access the XML-RPC interface to the data on the server in order to read and write OpenSeaMap data.\n\nOn the map are the world port, marina and anchorage. Via a pop-up window these are linked with a port guide containing detailed port information. This guide is organized as a Wiki and is shaped by the users. Included is the free wikiproject \"SkipperGuide\". 5000 ports are accessible worldwide, including detailed descriptions of 600 marinas.\n\nIn worldwide weather charts there are shown weather data like wind speed, air pressure (isobars), etc., each actualised two to three times per day, and with weather forecast up to three days.\n\nFor each harbor there is a Meteogram with detailed wind forecasts and weather information for eight days. Available weather data for wind direction, wind speed, air temperature, air pressure, relative humidity, cloudiness, precipitation. From this worldwide available data the captains can predict the marine weather. \n\nWorldwide, depths between 100 and 10,000 m are shown in a blue colour scale of 23 steps, together with depth shading. At higher zoom levels depth contours are also displayed. The data is given through cooperation with GEBCO and its bathymetric GEBCO_08-database.\n\nSee also OSMDCON. \n\nOpenSeaMap measures water depths by Crowdsourcing. Sailors, motor boaters, commercial shipping, divers, fishermen collect water depths between 0 and 100m with sounder and GPS. Divers work with a dive computer. Coastal water, lakes and rivers are surveyed. Result wil be an elevation model and derived depth lines. Uploaded tracks are shown in the chart. There is a discussion how crowd sourcing can support and enrich the systematic official measuring.\n\nBy \"View Wikipedia-Links\" OpenSeaMap displays all georeferenced Wikipedia articles worldwide as Icon, and optionally as gallery of all pictures from Commons. Until now 1.7 Mio. georeferenced articles are shown, in 41 languages, 260’000 in German and 740’000 in English (9.2011). By mouseover a popup is opened with title of the article and an informative picture. By click you will get the responding Wikipedia-article.\n\n"}
{"id": "17796821", "url": "https://en.wikipedia.org/wiki?curid=17796821", "title": "Rate of penetration", "text": "Rate of penetration\n\nIn the drilling industry, the rate of penetration (ROP), also known as penetration rate or drill rate, is the speed at which a drill bit breaks the rock under it to deepen the borehole. It is normally measured in feet per minute or meters per hour, but sometimes it is expressed in minutes per foot.\n\nGenerally, ROP increases in fast drilling formation such as sandstone (positive drill break) and decreases in slow drilling formations such as shale (reverse break). ROP decreases in shale due to diagenesis and overburden stresses. Over pressured zones can give twice of ROP as expected which is an indicative of a \"well kick\". Drillers need to stop and do the bottoms up.\n\n\n"}
{"id": "23326090", "url": "https://en.wikipedia.org/wiki?curid=23326090", "title": "SWREG", "text": "SWREG\n\nSWREG is one product in the suite of ecommerce solutions offered by Digital River MyCommerce.\n\nSWREG provides independent software and shareware business owners a method of selling products online, with features such as direct software download and electronic license delivery. SWREG offers multiple payment options, customization, flexibility, and distribution into international markets. Their checkout process allows users to purchase with their currency of preference using MasterCard, Eurocard, VISA, Delta, JCB, Switch, Solo, Discover, American Express, Diner’s Club, US personal check, International Money Orders, bank wires and PayPal.\n\nFounded in 1987, SWREG Inc is now part of the Digital River family of companies, attributed to their MyCommerce, Inc. brand and is headquartered in Minnetonka, Minnesota.\n\nOriginally part of Compuserve, SWREG was purchased by Steve Lee's Atlantic Coast PLC in England who employed Cyrus Maaghul (who had previously owned Digibuy) as managing director in order to grow and sell the business eventually to Digital River. Steve Lee then left. Digital River's SWREG acquisition was announced on 18 May 2005.\n\n\n"}
{"id": "24538623", "url": "https://en.wikipedia.org/wiki?curid=24538623", "title": "Sistema de Pagamentos em Moeda Local", "text": "Sistema de Pagamentos em Moeda Local\n\nSML, Sistema de Pagamentos em Moeda Local (en: \"Local Currency Payment System\")\n\nThe SML was established on October 3, 2008. It allows that payment orders are paid and received in local currency. Initially, the SML was used only in transactions between Brazil and Argentina, but recently Brazilian Central Bank is studying the expansion of the system to other countries like Uruguay and Paraguay. More countries, like Russia, India and China have shown interest in adopting the system in bilateral trade with Brazil.\n\nSenders and recipients have the option of paying or receiving payment in their own local currency. This supposedly makes life easier for small producers, which until then were used to sell and buy goods only in their own currencies. They don’t have to fix their prices in US-Dollars or calculate exchange-rates their own currency and the currency of the other country, intermediating with the US-dollar.\n<br>\nThe system includes by the central banks authorized financial institutions. Transactions are made between the financial institutions and the Central Banks. The importers and exporters fix their prices using the SML-rate published by the Central Banks.\n<br> \nIn the first year Brazilians and Argentines only used the SML in transactions involving goods, but central banks are considering enable operations of trade in services and payments of social benefits.\n"}
{"id": "56885478", "url": "https://en.wikipedia.org/wiki?curid=56885478", "title": "Soyuz carrier rocket monument", "text": "Soyuz carrier rocket monument\n\nThe Soyuz carrier rocket monument () is a monument in Samara (former Kuybyshev), Russia dedicated to Samara rocket builders. It is located at the center of Samara on Lenina Avenue between Novo-Sadovaya and Chelyuskintsev Streets next to Rossiyskaya metro station. The opening ceremony took place on 1 Oktober 2001 and was held in conjunction by the 50-year anniversary of the First Manned Spaceflight performed by Yuri Gagarin.\n\nGovernor of Samara Oblast Konstantin Titov, aerospace engineer, founder of the Progress State Research and Production Rocket Space Center Dmitri Ilyich Kozlov, members of Roscosmos and staff of Baikonur and Plesetsk Cosmodromes attended at the ceremony. The monument consists of the intact R-7 11A511 (Soyuz) which is fixed on the Space museum building. The rocket is 50 m height; mass is 20 tones. Facility which provides required support for the rocket weighs 53 tones. The rocket R-7 11A511 was built at the Kuybyshev Progress Rocket Space Centre in 1984. It was used for crew training at Plesetsk Cosmodrome. The rocket was ending one's lifespan in 1999 and then it was transferred to the production plant. The transfer was held in conjunction by the 40-year anniversary of the plant. The space center dismantled equipment and exhibited the carrier rocket. The missile body was painted with white and orange colours.\n\nThe Space museum was opened on 12 April 2007. The museum building was designed by architects Zhukov and Checherin. The museum is occupied by the collections of rocket-and-space equipment samples. Also this exhibition include: Yantar 4K1 and Resurs F1 landing sections, rocket engine mockups.\n"}
{"id": "654126", "url": "https://en.wikipedia.org/wiki?curid=654126", "title": "Supercomputer Systems", "text": "Supercomputer Systems\n\nThree firms have held, simultaneously, the name Supercomputer Systems or Supercomputing Systems.\n\nThe first Supercomputer Systems was founded in 1987 by Steve Chen, architect of the Cray X-MP and Cray Y-MP.\nThe second was based in San Diego, California, United States.\nThe third still exists in Zurich, Switzerland.\n\n\n"}
{"id": "27461244", "url": "https://en.wikipedia.org/wiki?curid=27461244", "title": "Sync.in", "text": "Sync.in\n\nSync.in is a web-based collaborative real-time editor, from Cynapse. It allows multiple people to edit the same text document simultaneously. Participants can see the changes in real-time, with each author's text in their own color. A chat box in the sidebar allows participants to communicate.\n\nSync.in is based on EtherPad that was acquired by Google in December 2009 and released as open-source later that month.\n\nSync.in uses the Freemium financial model. Free service requires no sign-up or registration and users can create unlimited public documents. Pro version is available in a Software as a service model and offers business-centric functions.\n\nAs of May 2013, creation of public notes has been disabled on the sync.in domain.\n\nAnyone can create a new collaborative text document, known as a \"note\". Each note has its own URL, and anyone who knows this URL can edit the note and participate in the document collaboration. Users can invite their friends or colleagues by sharing the note url using the \"share this note\" functionality provided in each note. Note URLs can be shared by Email or other social networking sites like Twitter, Facebook, Del.ici.ous, Digg, Linkedin, Ping.fm and Cyn.in\n\nThe software auto-saves the document at each keystroke while participants can permanently save specific versions at any time. A \"time slider\" feature allows anyone to explore the history of the note and how it evolved to its current state. It is a screenshot video of the creation of the note, keystroke-by-keystroke. The final document can be downloaded in HTML, plain text, or a bookmark file.\n\nSync.in Pro allows teams to create a secure site on a sync.in sub-domain with users for everyone in the team. Notes are private to the team members and individual notes can be password protected. There is also an option to selectively make notes available to the world.\n\nAn Adobe Air-based desktop client is available for Windows, Mac OS X and Linux and has enhanced features for Pro users. The desktop client lets users create new notes and launch notes directly from their desktop.\n\n\n"}
{"id": "50368438", "url": "https://en.wikipedia.org/wiki?curid=50368438", "title": "Sébastienne Guyot", "text": "Sébastienne Guyot\n\nSébastienne Guyot (26 April 1896 – 21 August 1941) was a French engineer who specialized in aerodynamic flying. She was born in Pont l'Abbé in the Finistère.\n\nA teacher, Guyot resigned in 1917 to prepare for the competition of the Ecole Centrale of the Paris Lycée Jules-Ferry when she learned that the school would accepts girls into its ranks. She graduated in 1921 from the first class of Central School of Paris to accept women. She ranked 45th of 243 graduates (425 students were received in the contest).\n\nAlso a top athlete, she participated in the 1928 Summer Olympics in Amsterdam in the 800 meters. She was also in 1928. Arrested by the Germans in 1940, she died in 1941 in Paris\n\nGuyot was awarded the medal of the resistance posthumously. She is the only woman whose name is on the war memorial of the Ecole Centrale. Since 2010, a scholarship in her name is given annually to five young Ecole Centrale students to fully fund their studies at the School. In 2015, her name was given to one of the eight streets in the new university area of Moulon in Gif-sur-Yvette, an area that will accommodate CentraleSupélec in 2017, following the merger and Centrale Paris and Supélec.\n"}
{"id": "32185806", "url": "https://en.wikipedia.org/wiki?curid=32185806", "title": "Terahertz nondestructive evaluation", "text": "Terahertz nondestructive evaluation\n\nTerahertz nondestructive evaluation pertains to devices, and techniques of analysis occurring in the terahertz domain of electromagnetic radiation. These devices and techniques evaluate the properties of a material, component or system without causing damage.\n\nTerahertz imaging is an emerging and significant nondestructive evaluation (NDE) technique used for dielectric (nonconducting, i.e., an insulator) materials analysis and quality control in the pharmaceutical, biomedical, security, materials characterization, and aerospace industries. It has proved to be effective in the inspection of layers in paints and coatings, \ndetecting structural defects in ceramic and composite materials\nand imaging the physical structure of paintings and manuscripts.\nThe use of THz waves for non-destructive evaluation enables inspection of multi-layered structures and can identify abnormalities from foreign material inclusions, disbond and delamination, mechanical impact damage, heat damage, and water or hydraulic fluid ingression.\nThis new method can play a significant role in a number of industries for materials characterization applications where precision thickness mapping (to assure product dimensional tolerances within product and from product-to-product) and density mapping (to assure product quality within product and from product-to-product) are required.\n\nSensors and instruments are employed in the 0.1 to the 10 THz range for nondestructive evaluation, which includes detection.\n\nThe Terahertz Density Thickness Imager is a nondestructive inspection method that employs terahertz energy for density and thickness mapping in dielectric, ceramic, and composite materials. This non-contact, single-sided terahertz electromagnetic measurement and imaging method characterizes micro-structure and thickness variation in dielectric (insulating) materials. This method was demonstrated for the Space Shuttle external tank sprayed-on foam insulation and has been designed for use as an inspection method for current and future NASA thermal protection systems and other dielectric material inspection applications where no contact can be made with the sample due to fragility and it is impractical to use ultrasonic methods.\n\nRotational spectroscopy uses electromagnetic radiation in the frequency range from 0.1 to 4 terahertz (THz). This range includes millimeter-range wavelengths and is particularly sensitive to chemical molecules. The resulting THz absorption produces a unique and reproducible spectral pattern that identifies the material. THz spectroscopy can detect trace amounts of explosives in less than one second. Because explosives continually emit trace amounts of vapor, it should be possible to use these methods to detect concealed explosives from a distance.\n\nTHz-wave radar can sense gas leaks, chemicals and nuclear materials. In field tests, THz-wave radar detected chemicals at the 10-ppm level from 60 meters away. This method can be used in a fence line or aircraft mounted system that works day or night in any weather. It can locate and track chemical and radioactive plumes. THz-wave radar that can sense radioactive plumes from nuclear plants have detected plumes several kilometers away based on radiation-induced ionization effects in air.\n\nTHz tomography techniques are nondestructive methods that can use THz pulsed beam or millimeter-range sources to locate objects in 3D. These techniques include tomography, tomosynthesis, synthetic aperture radar and time of flight. Such techniques can resolve details on scales of less than one millimeter in objects that are several tens of centimeters in size.\n\nSecurity imaging is currently being done by both active and passive methods. Active systems illuminate the subject with THz radiation whereas passive systems merely view the naturally occurring radiation from the subject.\n\nEvidently passive systems are inherently safe, whereas an argument can be made that any form of \"irradiation\" of a person is undesirable. In technical and scientific terms, however, the active illumination schemes are safe according to all current legislation and standards.\n\nThe purpose of using active illumination sources is primarily to make the signal-to-noise ratio better. This is analogous to using a flash on a standard optical light camera when the ambient lighting level is too low.\n\nFor security imaging purposes the operating frequencies are typically in the range 0.1 THz to 0.8 THz (100 GHz to 800 GHz). In this range skin is not transparent so the imaging systems can look through clothing and hair, but not inside the body. There are privacy issues associated with such activities, especially surrounding the active systems since the active systems, with their higher quality images, can show very detailed anatomical features.\n\nIt should be noted that active systems such as the L3 Provision™ and the Smiths eqo™ are actually mm-wave imaging systems rather than Terahertz imaging systems like Millitech™ systems. These widely deployed systems do not display images, avoiding any privacy issues. Instead they display generic \"mannequin\" outlines with any anomalous regions highlighted.\n\nSince security screening is looking for anomalous images, items like false legs, false arms, colostomy bags, body-worn urinals, body-worn insulin pumps, and external breast augmentations will show up. Note that breast implants, being under the skin, will not be revealed.\n\nActive imaging techniques can be used to perform medical imaging. Because THz radiation is biologically safe (non ionisant), it can be used in high resolution imaging to detect skin cancer.\n\nNASA Space Shuttle inspections are an example of this technology's application.\n\nAfter the Shuttle Columbia accident in 2003, Columbia Accident Investigation Board recommendation R3.2.1 stated “Initiate an aggressive program to eliminate all External Tank Thermal Protection System debris-shedding at the source….” To support this recommendation, inspection methods for flaws in foam are being evaluated, developed, and refined at NASA.\n\nSTS-114 employed Space Shuttle \"Discovery\", and was the first \"Return to Flight\" Space Shuttle mission following the Space Shuttle \"Columbia\" disaster. It launched at 10:39 EDT, 26 July 2005. During the STS-114 flight significant foam shedding was observed. Therefore, the ability to nondestructively detect and characterize crushed foam after that flight became a significant priority when it was believed that the staff processing the tank had crushed foam by walking on it or from hail damage when the shuttle was on the launch pad or during other preparations for launch.\n\nAdditionally, density variations in the foam were also potential points of flaw initiation causing foam shedding. The innovation described below answered the call to develop a nondestructive, totally non-contact, non-liquid-coupled method that could simultaneously and precisely characterize thickness variation (from crushed foam due to worker handling and hail damage) and density variation in foam materials. It was critical to have a method that did not require fluid (water) coupling; i.e.; ultrasonic testing methods require water coupling.\n\nThere are millions of dollars of ultrasonic equipment in the field and on the market that are used as thickness gauges and density meters. When \"terahertz nondestructive evaluation\" is fully commercialized into a more portable form, and becomes less expensive it will be able to replace the ultrasonic instruments for structural plastic, ceramic, and foam materials. The new instruments will not require liquid coupling thereby enhancing their usefulness in field applications and possibly for high-temperature in-situ applications where liquid coupling is not possible. A potential new market segment can be developed with this technology.\n\n"}
{"id": "1034724", "url": "https://en.wikipedia.org/wiki?curid=1034724", "title": "Theodore Ts'o", "text": "Theodore Ts'o\n\nTheodore Yue Tak Ts'o (曹子德) (born 1968) is a software engineer mainly known for his contributions to the Linux kernel, in particular his contributions to file systems. He is the primary developer and maintainer of e2fsprogs, the userspace utilities for the ext2, ext3 and ext4 filesystems, and is a maintainer for the ext4 file system.\n\nTs'o graduated from MIT with a degree in computer science in 1990, after which he worked in MIT's Information Systems (IS) department until 1999. During this time he was project leader of the Kerberos V5 team.\n\nIn 1994, Ts'o created the codice_1 Linux device node and the corresponding kernel driver, which was Linux's (and Unix's) first kernel interface that provided high quality cryptographic random numbers to user programs. codice_1 works without access to a hardware random number generator, allowing user programs to depend upon its existence. Separate daemons such as codice_3 take random numbers from such hardware and make them accessible via codice_1. Since its creation, codice_1 and codice_6 have become standard interfaces on Unix, Linux, BSD and macOS systems.\n\nAfter MIT IS, Ts'o went to work for VA Linux Systems for two years. In late 2001 he joined IBM, where he worked on improvements in the Linux kernel's performance and scalability. After working on a real-time kernel at IBM, Ts'o joined the Linux Foundation in late 2007 for a two-year fellowship. Initially he served as Chief Platform Strategist before becoming Chief Technology Officer in 2008. Ts'o also served as Treasurer for USENIX until 2008, and has chaired the annual Linux Kernel Developers Summit.\n\nIn 2010 Ts'o moved to Google, saying he would be working on \"kernel, file system, and storage stuff\".\n\nTs'o is a Debian Developer, maintaining several packages, mostly filesystem-related ones, including e2fsprogs since March 2003. He was a member of the Security Area Directorate for the Internet Engineering Task Force, and was one of the chairs for the IPsec working group. He was one of the founding board members for the Free Standards Group.\n\nTs'o was awarded the Free Software Foundation's 2006 Award for the Advancement of Free Software.\n\n"}
{"id": "1197767", "url": "https://en.wikipedia.org/wiki?curid=1197767", "title": "Thermal interface material", "text": "Thermal interface material\n\nThe term thermal interface material (shortened to TIM) describes any material that is inserted between two parts in order to enhance the thermal coupling between these two components. A large portion of those application are related to the heat dissipation and the TIM is inserted between the heat producing device (e.g. the heat source) and the heat dissipation device (e.g. the heat sink).\n\nThere are several kind of TIM with different target applications:\n\n"}
{"id": "1256909", "url": "https://en.wikipedia.org/wiki?curid=1256909", "title": "Thermalright", "text": "Thermalright\n\nThermalright Inc. is a Taiwan-based company headquartered in Taipei, established in 2001.\n\nIts products include aftermarket heat sinks for desktop computers, mouse and iPhone cases. It also produces components for other manufacturers, including AMD and Intel. Thermalright products are primarily marketed towards gamers and enthusiasts who commonly overclock different parts of their computer which can generate large amounts of heat.\n\nSince the release of AMD's Thunderbird CPU in 2002, Thermalright has been making high-performance heat sinks, not only for AMD processors, but Intel as well. Their current product line includes heatsinks for CPUs, GPUs, RAM, and Motherboard components. Thermalright's primary focus is fanless heatsinks which can be used passively, making quiet computers; however many enthusiasts choose to use the heat sinks with a fan for better heat dissipation.\n\nIn the year of 2012, Thermalright established the Leetgion brand, starting to extend its business to peripheral and game related products. At the same time, Thermalright set its oversea branch, 索魔乐 in Shanghai, China.\n\n\nThe SK-6 was Thermalright's first heatsink, sold as a standalone product with clips for users' fans (up to 80mm) instead of as an HSF (heatsink-fan) combo. Apart from being the first heatsink to offer a custom fan mounting system, it was also the first to be manufactured in 100% fin-tube copper instead of the die-cast aluminum that was common in the day. With copper having significantly superior thermal conductivity properties to aluminum, the product caught on with enthusiasts and spurred a new era of copper-based cooling. \n\nAX-7 is the second heat sink by Thermalright. With the size of 70mm and soldered by copper and aluminum, AX-7 is different from the mainstream of 60mm fan and heat sink at that time.\n\nSLK800 is covered and piled by fin-tube copper as well as SK-6. With the size of 80mm and weight of 400g, SLK800 was kept an eye on its thickness of 38mm fix fan.\nSP94 is the first heat sink manufacturing with the heat pipe as a medium of thermal conductivity. With the size of 90mm and capability to fix 80mm fan, SP94 is undoubtedly a cutting edge heat sink at that time. SP series have two kinds of product collocating different CPU.\nThe XP120 was the first heatsink to support a 120mm fan, and violated many motherboards' Keep-Out Zones (perimeter of a certain size set around the CPU die, in which no components above a certain height can be placed). Slowly the Keep-Out Zone standard began to be updated, and bigger coolers like the XP120 proliferated. \nAS the trend of overclocking becomes a global phenomenon and niche market, the design of heat sinks transformed from horizontal cooler to tower cooler. Ultra120, with the size of 160mm, is the first tower cooler by Thermalright. Even today, the computer case manufacturers set the height of widely used 160mm of tower cooler as the design standard.\nThe HR-01 was developed to satisfy the market segment that demanded complete silence, and it did so by being capable of operating without a fan while still offering fan ducts for attachment. In this way Thermalright was able to satisfy 2 segments of the market in one go by allowing the silence enthusiasts to run the cooler as boxed and allowing the performance enthusiasts to attach a fan for maximum cooling potential. \nUltra120 extreme (also called U120E) has six thermal conductors. From the appearance, we can tell Ultra120E is an updated version of old heatsink. However, not only is it a brand new one, but also, from the efficiency test report issued by Anandtech, the efficiency of U120E was proved to be a better version. Thermalright then declared that the copper-base heatsink is manufactured with protruding face, not flat one, which is different from the concept of thermal conduct as convention would have it. Afterwards, Thermalright manufactured the Ultra120E heatsink with protruding face, fascinating other heatsink manufacturers following with this concept.\nHR-02 is another product designed for the need of mute cooler in demand by Thermalright. The combination of fin-tube and thermal conductor is not centered and symmetrical. If HR-02 is installed in the case, the system fan is close to heat sink itself enough to fulfill the need of passive heat sink passively.\n\n"}
{"id": "538123", "url": "https://en.wikipedia.org/wiki?curid=538123", "title": "Ticker tape", "text": "Ticker tape\n\nTicker tape was the earliest digital electronic communications medium, transmitting stock price information over telegraph lines, in use between around 1870 through 1970. It consisted of a paper strip that ran through a machine called a stock ticker, which printed abbreviated company names as alphabetic symbols followed by numeric stock transaction price and volume information. The term \"ticker\" came from the sound made by the machine as it printed.\n\nPaper ticker tape became obsolete in the 1960s, as television and computers were increasingly used to transmit financial information. The concept of the stock ticker lives on, however, in the scrolling electronic tickers seen on brokerage walls and on news and financial television channels.\n\nTicker tape stock price telegraphs were invented in 1867 by Edward A. Calahan, an employee of the American Telegraph Company.\n\nAlthough telegraphic printing systems were first invented by Royal Earl House in 1846, early models were fragile, required hand-cranked power, frequently went out of synchronization between sender and receiver, and did not become popular in widespread commercial use. David E. Hughes improved the printing telegraph design with clockwork weight power in 1856, and his design was further improved and became viable for commercial use when George M. Phelps devised a resynchronization system in 1858. The first stock price ticker system using a telegraphic printer was invented by Edward A. Calahan in 1863; he unveiled his device in New York City on November 15, 1867. Early versions of stock tickers provided the first mechanical means of conveying stock prices (\"quotes\"), over a long distance over telegraph wiring. In its infancy, the ticker used the same symbols as Morse code as a medium for conveying messages. One of the earliest practical stock ticker machines, the Universal Stock Ticker developed by Thomas Edison in 1869, used alphanumeric characters with a printing speed of approximately one character per second.\n\nPreviously, stock prices had been hand-delivered via written or verbal messages. Since the useful time-span of individual quotes is very brief, they generally had not been sent long distances; aggregated summaries, typically for one day, were sent instead. The increase in speed provided by the ticker allowed for faster and more exact sales. Since the ticker ran continuously, updates to a stock's price whenever the price changed became effective much faster and trading became a more time-sensitive matter. For the first time, trades were being done in what is now thought of as near real-time.\n\nBy the 1880s, there were about a thousand stock tickers installed in the offices of New York bankers and brokers. In 1890, members of the exchange agreed to create the New York Quotation Co., buying up all other ticker companies to ensure accuracy of reporting of price and volume activity.\n\nStock ticker machines are an ancestor of the modern computer printer, being one of the first applications of transmitting text over a wire to a printing device, based on the printing telegraph. This used the technology of the then-recently invented telegraph machines, with the advantage that the output was readable text, instead of the dots and dashes of Morse code. A special typewriter designed for operation over telegraph wires was used at the opposite end of the telegraph wire connection to the ticker machine. Text typed on the typewriter was displayed on the ticker machine at the opposite end of the connection.\n\nThe machines printed a series of \"ticker symbols\" (usually shortened forms of a company's name), followed by brief information about the price of that company's stock; the thin strip of paper on which they were printed was called \"ticker tape\". The word \"ticker\" comes from the distinct tapping (or \"ticking\") noise the machines made while printing. Pulses on the telegraph line made a letter wheel turn step by step until the correct symbol was reached and then printed. A typical 32-symbol letter wheel had to turn on average 15 steps until the next letter could be printed resulting in a very slow printing speed of one character per second. In 1883, ticker transmitter keyboards resembled the keyboard of a piano with black keys indicating letters and the white keys indicating numbers and fractions, corresponding to two rotating type wheels in the connected ticker tape printers.\n\nNewer and more efficient tickers became available in the 1930s, but these newer and better tickers still had an approximate 15-to-20-minute delay. Ticker machines became obsolete in the 1960s, replaced by computer networks; none have been manufactured for use for decades. However, working reproductions of at least one model are now being manufactured for museums and collectors.\n\nSimulated ticker displays, named after the original machines, still exist as part of the display of television news channels and on some websites — see news ticker. One of the most famous outdoor displays is the simulated ticker scrolling marquee located at One Times Square in New York City.\n\nTicker tapes then and now contain generally the same information. The ticker symbol is a unique set of characters used to identify the company. The shares traded is the volume for the trade being quoted. Price traded refers to the price per share of a particular trade. Change direction is a visual cue showing whether the stock is trading higher or lower than the previous trade, hence the terms \"downtick\" and \"uptick\". Change amount refers to the difference in price from the previous day’s closing. Many today include color to indicate whether a stock is trading higher than the previous day’s (green), lower than previous (red), or has remained unchanged (blue or white).\n\nIn the early days of baseball, before electronic scoreboards, manual score turners used a ticker to get the latest scores from around the league. Today, computers and electronic scoreboards have replaced the manual scoreboard and the ticker.\nUsed ticker tape was cut into a form of confetti, to be thrown from the windows above parades, primarily in lower Manhattan; this became known as a ticker tape parade. Ticker tape parades generally celebrated some significant event, such as the end of World War I and World War II, or the safe return of one of the early astronauts. Ticker tape was also incorporated into some of the innovative weaver Dorothy Liebes' unusual art textiles.\n\nTicker tape parades are still held in New York City, specifically in the \"Canyon of Heroes\" in Manhattan, most often when local sports teams win a championship. However, actual ticker tape is not used during these parades any longer; often, pieces of paper from paper shredders are used as a convenient source of confetti.\n\n\n"}
{"id": "4646894", "url": "https://en.wikipedia.org/wiki?curid=4646894", "title": "Tide mill", "text": "Tide mill\n\nA tide mill is a water mill driven by tidal rise and fall. A dam with a sluice is created across a suitable tidal inlet, or a section of river estuary is made into a reservoir. As the tide comes in, it enters the mill pond through a one-way gate, and this gate closes automatically when the tide begins to fall. When the tide is low enough, the stored water can be released to turn a water wheel.\n\nTide mills are usually situated in river estuaries, away from the effects of waves but close enough to the sea to have a reasonable tidal range. Cultures have built such mills have existed since the Middle Ages, and some may date to the Roman period.\n\nA modern version of a tide mill is the electricity-generating tidal barrage.\n\nPossibly the earliest tide mill in the Roman world was located in London on the River Fleet, datingto Roman times.\n\nThe earliest \"recorded\" tide mills in England are listed in the Domesday Book (1086). Eight mills are recorded on the River Lea (the site at Three Mills remains, with Grade I listed buildings and a small museum), as well as a mill in Dover harbour. By the 18th century, there were about 76 tide mills in London, including two on London Bridge.\n\nWoodbridge Tide Mill, an excellent example, survives at Woodbridge, Suffolk, England. This mill, dating from 1170 and reconstructed in 1792, has been preserved and is open to the public. It was further restored in 2010 and re-opened in 2011 in full working order. It is the second working tide mill in the United Kingdom that is regularly producing flour. Carew Castle in Wales also has an intact tide mill, but it is not operating. The first tide mill to be restored to working order is Eling Tide Mill in Eling, Hampshire. Another example, now extant only in historic documents, is the mill in the hamlet of Tide Mills, East Sussex. Traces of a tide mill may be seen at Fife Ness, revealed through an archaeological survey.\n\nA mediæval tide mill still operates at Rupelmonde near Antwerp, and there are several that have survived in the Netherlands.\n\nAt one time there were 750 tide mills operating along the shores of the Atlantic Ocean: approximately 300 in North America, including many in colonial Boston over a 150-year span. In addition, 200 have been documented in the British Isles, and 100 in France. The Rance estuary in France was also home to some of these mills.\n\nBy the mid-20th century, the use of water mills had declined dramatically. In 1938, an investigation by Rex Wailes discovered that of the 23 extant tidal mills in England, only 10 were still working by their own motive power. Of one at Beaulieu, H. J. Massingham wrote in the 1940s, \n\"Part of the mill is built on piles into the river and is weatherboarded, while the rest of the building is a warm red brick roofed with lozenge-shaped and rounded tiles which I believe are called fish-tiles. All the interior is of wood - ladders, bins for the meal, floor-boarding, square pillars, beams, narrow passages, fittings, shaft rising to the first floor and all. So ramshackle is the arrangement of the props and supports that it is a wonder that the whole edifice does not tumble about the miller's ears like a pack of cards. The point is that it has stood in this way for something like six centuries, and that gives the explorer into its dusky depths a more penetrating notion of how the old builders could build, more than does a Gothic church or even a cathedral. The pulse and swing of the great wheel sets the whole building in an ague, but it will still be standing when all the flimsy excrescences of development between Beaulieu and Poole have fallen down.\"\n\nNewer types of tidal power often propose construction of a dam across a large river estuary. Although hydroelectric power represents a source of renewable energy, each proposal tends to come under local opposition because of its likely adverse effect on coastal habitats. One proposal, which was developed in 1966, is the Rance barrage, which generates 250MW. Unlike historical tide mills, which could operate only on an ebb tide, the Rance barrage can generate electricity on both flows of the tide, or it can be used for pumped storage, depending on demand. A less intrusive design is a 1MW free-standing turbine, constructed in 2007 at Strangford Lough Narrows; this site is close to an historic tide mill.\n\n\n\n\n\n"}
{"id": "38189634", "url": "https://en.wikipedia.org/wiki?curid=38189634", "title": "Timeline of the introduction of radio in countries", "text": "Timeline of the introduction of radio in countries\n\nThis is a list of when the first radio broadcasts to the public occurred in the mentioned countries and territories. Non-public field tests and closed circuit demonstrations are not referred to; neither are license dates or dates of the official opening. \n\nSources:\nNotes:\n\nBasis for each entry is the time of introduction.\n\nListed are\n\nEach entry comprises\nand in brackets\n\nSee also:\n"}
{"id": "3113840", "url": "https://en.wikipedia.org/wiki?curid=3113840", "title": "Traction (engineering)", "text": "Traction (engineering)\n\nTraction, or tractive force, is the force used to generate motion between a body and a tangential surface, through the use of dry friction, though the use of shear force of the surface is also commonly used.\n\nTraction can also refer to the \"maximum\" tractive force between a body and a surface, as limited by available friction; when this is the case, traction is often expressed as the ratio of the maximum tractive force to the normal force and is termed the \"coefficient of traction\" (similar to coefficient of friction).\n\nTraction can be defined as:\nIn vehicle dynamics, tractive force is closely related to the terms tractive effort and drawbar pull, though all three terms have different definitions.\n\nThe \"coefficient of traction\" is defined as the usable force for traction divided by the weight on the running gear (wheels, tracks etc.) i.e.:\n\nTraction between two surfaces depends on several factors:\n\nIn the design of wheeled or tracked vehicles, high traction between wheel and ground is more desirable than low traction, as it allows for higher acceleration (including cornering and braking) without wheel slippage. One notable exception is in the motorsport technique of drifting, in which rear-wheel traction is purposely lost during high speed cornering.\n\nOther designs dramatically increase surface area to provide more traction than wheels can, for example in continuous track and half-track vehicles. A tank or similar tracked vehicle uses tracks to reduce the pressure on the areas of contact. A 70-ton M1A2 would sink to the point of high centering if it used round tires. The tracks spread the 70 tons over a much larger area of contact than tires would and allow the tank to travel over much softer land.\n\nIn some applications, there is a complicated set of trade-offs in choosing materials. For example, soft rubbers often provide better traction but also wear faster and have higher losses when flexed—thus reducing efficiency. Choices in material selection may have a dramatic effect. For example: tires used for track racing cars may have a life of 200 km, while those used on heavy trucks may have a life approaching 100,000 km. The truck tires have less traction and also thicker rubber.\n\nTraction also varies with contaminants. A layer of water in the contact patch can cause a substantial loss of traction. This is one reason for grooves and siping of automotive tires.\n\nThe traction of trucks, agricultural tractors, wheeled military vehicles, etc. when driving on soft and/or slippery ground has been found to improve significantly by use of Tire Pressure Control Systems (TPCS). A TPCS makes it possible to reduce and later restore the tire pressure during continuous vehicle operation. Increasing traction by use of a TPCS also reduces tire wear and ride vibration.\n\n"}
{"id": "220460", "url": "https://en.wikipedia.org/wiki?curid=220460", "title": "Wax paper", "text": "Wax paper\n\nWax paper (also waxed paper or paraffin paper) is paper that has been made moisture-proof through the application of wax.\n\nThe practice of oiling parchment or paper in order to make it semi-translucent or moisture-proof goes back at least to the Middle Ages. Paper impregnated or coated with purified beeswax was widely used throughout the 19th century to retain or exclude moisture, or to wrap odorous products. Gustave Le Gray introduced the use of waxed paper for photographic negatives in 1851. Natural wax was largely replaced for the making of wax paper (or paraffine paper) after Herman Frasch developed ways of purifying paraffin and coating paper with it in 1876. Wax paper is commonly used in cooking for its non-stick properties, and wrapping food for storage, such as cookies, as it keeps water out or in. It is also used in arts and crafts.\n\nOven: wax paper is not recommended for baking use as it will smoke. Parchment paper is better for this use.\n\nMicrowave: wax paper can be used to prevent splatters by covering the food when microwave cooking. Since the paper is mostly unaffected by microwaves, it will not heat to the point of combustion under normal usage. This makes wax paper more functional than plastic wrap which will melt at higher temperatures, or aluminium foil which is not safe for use in most microwave ovens.\n\nSafety razor blades are traditionally wrapped in wax paper to make handling them less dangerous. Wax paper can also be used to make long lasting paper boats because of its high resistance to water.\n\nFrom the early 1950s to the mid-1990s, wax paper was used as a common wrapping for sports card packages (O-Pee-Chee, Topps, Donruss, etc.). It was notorious for leaving wax markings on the back card where the wax paper was heated to be sealed. Wax paper was used as a way to keep the enclosed piece of bubble gum protected.\n\nIn the mid-1990s, sports card manufacturers stopped including pieces of bubble gum in packs of sports cards, thus ending the need for wax paper packs. Plastic (mylar) or other plastic/paper blends were used from then on.\n\nWax paper is also commonly used to attach pattern pieces to fabric while cutting it for sewing. One presses an iron over the wax paper briefly and attaches it to the cloth, making it easier to trace while cutting.\n\nWhen children's playground slides were made of metal, it was common to sit on a piece of wax paper. This would not only lessen the heat, it would make the ride much faster.\n\nWax paper's particularly high dielectric strength makes it a practical electrical insulator, although modern materials have surpassed and mostly replaced it. Common applications are coil winding separators and capacitor dielectrics, and other applications requiring resilience against a potential difference up to the order of a few thousand volts per layer.\n\nTurntablists (DJs) commonly place one or multiple sheets of wax paper under their records to increase record slip and aid in scratch (where the record is rotated in a number of different ways by a finger to create special sound effects) routines.\n\nIn photography, wax paper can be used as a light diffuser.\n\nThere are multiple environmental issues concerned with wax paper. Though it is biodegradable in its unaltered form, oft-applied additives such as petroleum rid it of that quality. Wax paper also cannot be recycled.\n\n"}
{"id": "12308064", "url": "https://en.wikipedia.org/wiki?curid=12308064", "title": "Welltec", "text": "Welltec\n\nWelltec is an international provider of robotic well solutions for the oil and gas industry. The company's flagship is the “Well Tractor”, a remote-controlled device that is able to convey other intervention tools and perform operations on wireline. The Well Tractor enables operators to run operations on wireline in the entire length of horizontal and highly deviated wells. With the ability to reach the end of these well types, recoverable reserves are increased.\n\nServices range from logging and perforation to mechanical services such as plug pulling/setting, valve manipulation, clean-up, milling and many more. \nThe company was established in 1994; today the company has established over 45 offices worldwide and employs more than 1,000 people.\n\nJørgen Hallundbæk, the founder and CEO of Welltec, conceived the idea behind the Well Tractor while he was a graduate student at the Technical University of Denmark. The idea consisted of eliminating the large and expensive amounts of equipment needed for conveying tools and performing interventions in oil and gas wells. The Well Tractor enabled operators to reach the horizontal part of oil and gas wells and getting past the deviated sections using standard electrical wireline. Generally, wells are considered deviated when they reach an inclination higher than 60°. Previously, interventions in horizontal and deviated wells were considered a lengthy and costly process.\n\nNew technology in the oil and gas industry is not readily accepted due to the costs of failure. The multinational oil company Shell has estimated that new technology requires 20–30 years to achieve broad acceptance . The Well Tractor performed its first job in 1996, an operation that took place in the North Sea offshore Norway .\n\nIn 2003 the company went from being a sub contractor to other service companies to being a direct contractor to the operators.\n\nToday, Welltec has more than 45 offices and service facilities worldwide including Headquarters located in Allerød, Denmark.\n\nIn July 2007, Welltec partnered with Summit Partners, a growth capital private equity investor.\n\nWelltec provides a range of services to international oil and gas companies from the drilling & evaluation phase to the completion and production phases. Services are delivered on wireline. The proprietary technology is developed and manufactured at headquarters in Allerød.\n\nServices solve problems across different well types and environments. Welltec’s technology is applicable in vertical, horizontal, deviated and technologically advanced wells. Challenging conditions may be wells such as deepwater, Subsea, extended reach, high-yield, heavy oil, Arctic conditions and unconventional gas environments.\n\nWireline technology is by many people in the oil and gas industry viewed as a focus area with potential for development, and Welltec has expanded their technology portfolio during the years. Most services are performed with the Well Tractor that conveys logging tools or intervention tools to the desired point in the well where an intervention is needed. Originally, the Well Tractor was designed as a tool conveyor but today it is also able to perform interventions.\n\nInitially, the Well Tractor formed Welltec’s tool portfolio, but in 2003 the so-called mechanical down hole services were added to the portfolio of technology. These intervention services are intended for electric wireline operations such as valve manipulation, scale milling, setting and pulling plugs, milling plugs and sand bailing.\nTo date, Welltec has developed and produced seven tools with different areas of operation. Apart from the Wireline Well Tractor, a Coiled tubing Well Tractor has been manufactured, which instead of running on wireline is fluid-driven. The mechanical down hole services consist of the Well Stroker, the Well Key, the Well Cleaner, the Well Miller and the Welltec Release Device. All tool names are registered trademarks of Welltec.\n\n"}
