{"id": "23265531", "url": "https://en.wikipedia.org/wiki?curid=23265531", "title": "Aerospace Valley", "text": "Aerospace Valley\n\nAerospace Valley is a French cluster of aerospace engineering companies and research centres. The cluster is located in the regions of Occitanie and Nouvelle Aquitaine in the southwest of France and is mainly concentrated in and around the cities of Bordeaux and Toulouse.\n\nThe over 500 affiliated companies (including Airbus, Air France Industries and Dassault Aviation) are responsible for some 120,000 jobs in the aviation and space flight industries. In addition, some 8,500 researchers are active within the affiliated companies and institutions.\n\nThe cluster's stated aim is to create 40,000 - 45,000 new jobs by 2026. Since its inception in 2005, the cluster has initiated some 220 research projects with a total budget of 460 million euros, including 204 million euros in government funding.\n\nThe headquarters of Aerospace Valley is located in Toulouse. Chair of the cluster is Jean-Marc Thomas, who is also vicechair of Airbus France. Key locations of the cluster include:\n\nAerospace Valley is a member of the European Aviation Clusters Partnership and of Institut au service du spatial, de ses applications et technologies.\n\n\n"}
{"id": "36722187", "url": "https://en.wikipedia.org/wiki?curid=36722187", "title": "Ajka Crystal", "text": "Ajka Crystal\n\nAjka Crystal is a Hungarian crystal manufacturer. Created in 1878 by Bernard Neumann. The company, one of the biggest in Central Europe produces unique, handmade pieces of glass art. Ajka Crystal also goes under the name of \"The Romanov Collection\" in the United States. Ajka Crystal ships 90% of the factory's total production - both in tableware (stemware, tumblers etc...) and in giftware (vases, bowls) is exported for world-famous brands such as Wedgwood, Tiffany's, Rosenthal, Waterford Crystal, Polo Ralph Lauren, Christian Dior, Moser and other high end French Crystal Manufacturers.\n\nAjka Crystal is located in Ajka, Hungary.\n\nAjka Crystal's online webshop (CrystALaCarte.com) was established in 2003, with the widest selection of lead crystal products for both retail and wholesale.\n"}
{"id": "1435276", "url": "https://en.wikipedia.org/wiki?curid=1435276", "title": "Alfa (rocket)", "text": "Alfa (rocket)\n\nAlfa was the designation of an Italian ballistic missile program that started in 1971 under the control of the GRS (Gruppo di Realizzazione Speciale Interforze). Starting as a development effort for a study on efficient solid-propellant rockets, the Alfa rocket was planned as a two-stage rocket. Test launches with an upper stage mockup took place between 1973 and 1975, from Salto di Quirra.\n\nThe Alfa was long and had a diameter of . The first stage of the Alfa was long and contained 6 t of solid rocket fuel. It supplied a thrust of 232 kN for a duration of 57 seconds. It could carry one tonne warhead for a range of 1,600 kilometres (990 mi), placing European Russia and Moscow in range from the Adriatic Sea.\n\nItaly has been active in the space sector since 1957, conducting launch and control operations from the Luigi Broglio Space Centre. The advanced Scout and Vega launchers currently used by the European Space Agency (ESA) derive their technological basis partially from Alfa studies.\n\n"}
{"id": "20627732", "url": "https://en.wikipedia.org/wiki?curid=20627732", "title": "Alpha capture system", "text": "Alpha capture system\n\nAn alpha capture system is a computer system that enables investment banks and other organisations to submit \"trading ideas\" or \"trade ideas\" to clients in a written electronic format, for example TIM Group's TIM Ideas product or Bloomberg LP's Trade Ideas product. Financial Services Authority Markets Division: Newsletter on Market Conduct and Transaction Reporting Issues, Issue No. 17, September 2006, First used in 2001 by Marshall Wace they are an alternative to the traditional stockbrokering approach of communicating ideas and strategies to clients face-to-face or over the telephone.\n\nThe term \"alpha capture\" refers to the aim of such systems to help investors find alpha or market-beating returns on investments. Submitted trade ideas are accompanied by a rationale, timeframe and conviction level and enable investors to quantify and monitor the performance of different ideas.\n\n \n"}
{"id": "2644848", "url": "https://en.wikipedia.org/wiki?curid=2644848", "title": "Bassinet", "text": "Bassinet\n\nA bassinet, bassinette, or cradle is a bed specifically for babies from birth to about four months. Bassinets are generally designed to work with fixed legs or casters, while cradles are generally designed to provide a rocking or gliding motion. Bassinets and cradles are distinguished from Moses baskets and carry cots, which are designed to be carried and sit directly on the floor or furniture. After four months, babies are often transferred to a crib (North American usage) or cot (UK usage). In the United States, however, the bedside sleeper is the prevalent option, since they are generally bigger, recommended up to 6 months, and often used up to a year. \n\nA bassinet is typically a basket-like structure on free-standing legs, often with casters. A cradle is typically set in a fixed frame, but with the ability to rock or glide.\n\nBassinet usage in the United States nearly doubled to 20% from 1992-2006. Greater than 45% of babies up to two months used a bassinet. By 5–6 months, however, fewer than 10% of babies sleep in bassinets. In a hospital environment, a special form of sealed bassinet is used in a neonatal intensive care unit.\n\nOn many long-haul flights, most airlines provide a bassinet (which is attached to a bulkhead) to adults travelling with an infant, i.e., a child under the age of two. The use of the bassinet is restricted by the infant's size and weight. These need to be requested in advance with the airline. However, most USA and Canadian airlines have bassinet policies with mean they are only allocated at the airport gate.\n\nResearch has shown that the mattress influences SIDS outcomes; a firm mattress lowers SIDS risk.\n\nSome bassinets are designed to rock or swing freely, with many carers finding their child calmed by this action. The process of lulling the child to sleep may be accompanied by prerecorded or live performance of lullabies.\n\nAlthough there are many variations, they fall generally into two categories:\n\nIn both cases, they are generally designed to allow the resting baby to be carried from place to place. Within the home, they are often raised on a stand or other surface to reduce back strain when bending over to tend the baby. Wheeled frames to convert a bassinet into a pram or baby carriage are common.\n\nAt three or four months of age babies are able to roll over by themselves; this means they could tip the bassinet over, so for safety they must use an infant bed or toddler bed instead.\n\n"}
{"id": "10927936", "url": "https://en.wikipedia.org/wiki?curid=10927936", "title": "BioValley (Europe)", "text": "BioValley (Europe)\n\nBioValley is the leading life science cluster in Europe, founded 1996. It connects academia and companies of three nations in the Upper Rhine Valley, namely France, Germany and Switzerland. The main objective is the greater research cooperation between companies and academia involved in the life science sectors, including pharmacology, biotechnology, nanotechnology, medical technology, chemistry and agricultural biotechnology.\n\n\n\n"}
{"id": "57869968", "url": "https://en.wikipedia.org/wiki?curid=57869968", "title": "BrainScope Company, Inc.", "text": "BrainScope Company, Inc.\n\nBrainScope Company, Inc. (BrainScope) is a medical neuro-technology firm using artificial intelligence to assess a variety of neurological conditions, beginning with mild traumatic brain injury (mTBI), including concussion. BrainScope was founded in 2006 and is headquartered in Bethesda, Maryland. \n\nBrainScope's markets include the military, emergency departments, urgent care and occupational medicine centers, and sports at the collegiate and professional levels.\n\n\"BrainScope One\" uses artificial intelligence and multi-modality to help clinicians rapidly assess mild head-injured patients for structural and functional brain injuries, including concussions, with a non-invasive handheld device and a disposable headset. \n\nUsing electroencephalographic (EEG) data collected with a disposable headset, \"BrainScope One\" objective biomarkers both predict the likelihood that a patient's structural brain injury would be visible on a CT scan and assess the likelihood of brain functional impairment by comparing the patient's brain function to that of non-head injured iindividuals. \"BrainScope One\" also includes neurocognitive tests, performed by the patient on the device, and an extensive library of digitized concussion assessments.\n\nAn independent validation trial conducted using \"BrainScope One\" determined that the device's sensitivity to \"≥1 mL of blood was ... 98.6%\" and that its accurate CT-scan predictions can lead to a \"reduction in the number of false positives of 33%.\" \n\n\"BrainScope One's\" technology was created through 21 clinical studies at over 55 clinical sites spanning a decade, contributing to 23 investigator-initiated peer-review journal articles and resulting in 5 FDA clearances. \"BrainScope One\" is FDA cleared as an Rx-only device for use on patients 18-85 years of age who have suffered a mild head injury (Glasgow Coma Scale score of 13-15 and mTBI) within the previous 72 hours. \"BrainScope One\" was originally cleared by the FDA as the Ahead 300 under 510(k) K161068 in September 2016. Subsequent modifications to device Indications for Use regarding concussion/mTBI capabilities were FDA cleared under 510(k) K181179 in May 2018. \n\nThe company initiated a clinical research study on patients 13-25 years of age in May 2018 as part of its initiative to \"introduce a teenage-focused product in the near-term.\"\n\nBrainScope has raised over $60 million in private capital and has received $30 million in research contracts, including funding from the United States Department of Defense and General Electric and the National Football League through their Head Health Challenge competition. Among the company's private investors are Revolution LLC, an investment firm founded by AOL co-founder Steve Case; DBL Partners, a venture capital firm that also backs Tesla and Pandora; and the Maryland Venture Fund.\n\nBrainScope's other development partners include New York University School of Medicine and Johns Hopkins University and its clinical partners include Emory University, the University of Virginia and Washington University in St. Louis.\n\n"}
{"id": "1429428", "url": "https://en.wikipedia.org/wiki?curid=1429428", "title": "CKUA Radio Network", "text": "CKUA Radio Network\n\nThe CKUA Radio Network is a Canadian donor-funded community radio network based in Edmonton, Alberta. Originally located on the campus of the University of Alberta in Edmonton (hence the UA of the call letters), it was the first public broadcaster in Canada. It now broadcasts from studios in downtown Edmonton, and as of fall 2016 out of a studio in Calgary located at the National Music Centre. CKUA's primary station is CKUA-FM located on 94.9 FM in Edmonton, and the station operates fifteen rebroadcasters to serve the remainder of the province.\n\nCKUA was created in 1927 through a provincial grant which allowed the University of Alberta's Extension Department to purchase the licence of CFCK. CKUA was also the first radio station to offer educational radio programming, including music concerts, poetry readings, and university lectures. From 1930 to 1931 the station was an affiliate of the CNR Radio network. CKUA was operated from 1945 until 1974 by Alberta Government Telephones. The crown corporation, Alberta Educational Communications Corporation (later known as Access), assumed ownership of the station in 1974. In 1994, Access sold the CKUA network to the non-profit CKUA Radio Foundation for $10.\n\nIn 1994 the station won an Alberta Recording Industry Award of Excellence. \n\nOn March 20, 1997 the station went off the air for five weeks due to political squabbles, poor financial management, and attempts at privatization. The station restarted broadcasting on April 25, 1997 after control was given to the public from directors appointed by the provincial government. As of 2005, more than two-thirds of the station's funding came from its listeners in the form of donations.\n\nCKUA is considered a cultural icon by many musicians throughout Canada. The station's practice of supporting local, independent, and non-commercial artists has helped launch the careers of such renowned musicians as k.d. lang, Jann Arden, and Bruce Cockburn. In addition, CKUA has contributed to the careers of Arthur Hiller, Robert Goulet, and Tommy Banks, among others. Throughout the 1930s an early radio drama series, \"CKUA Players\", was produced out of the station and broadcast throughout Western Canada by a network of stations.\n\nCKUA schedules different programs throughout the week and thus can offer many different genres including blues, bluegrass, R&B, Celtic, country, classical, jazz, reggae, folk, hiphop, dance, funk, rock, roots, and world.\n\nCKUA's music library boasts one of the largest and most diverse music collections in Canada, with more than 250,000 CDs and LPs, including 10,000 78 rpm records, as well as a few aluminium transcription discs, 45s, and other various media formats.\n\nCKUA was headquartered in the Alberta Block building on Jasper Avenue in Edmonton starting in 1955. In October, 2012, CKUA moved into its current location in the Alberta Hotel building, with its first broadcast from the new location on October 15, 2012.\n\nThe station's original transmitter was located at 580 kHz in Edmonton. It operated at 10,000 watts. Due to its location near the bottom of the AM dial, as well as its transmitter power, it was powerful enough to cover nearly all of Alberta's densely populated area. It added an FM simulcast in 1947.\n\nStarting in the 1970s, CKUA built a network of 16 FM repeaters across Alberta. CKUA also broadcasts in western Canada on select cable and satellite providers (such as SaskTel, who carries CKUA across Saskatchewan as a Lloydminster station). As of February 29, 1996, CKUA became the first radio station in Canada to stream their broadcast online, and now has upgraded the service to carry an unlimited number of streams. The station currently has more than 250,000 weekly listeners.\n\nBecause of CKUA's extensive coverage, the station was one of only a handful of broadcasters (another being CTV Two Alberta, formerly Access) to carry the Alberta Emergency Public Warning System. The provincial government-funded programme provided the station with 12% of its annual income until the contract was lost to an Ottawa firm, Black Coral Inc., in January 2010.\n\nCKUA announced plans to shut down its legacy 580 AM signal, the longest continuously-used AM frequency in Canada, in the spring of 2013. It would have needed to invest as much as $5 million to upgrade the transmitter site to modern standards, an amount it could not afford. However, CKUA did not receive formal approval from the CRTC until September 12, 2013. AM 580 went off the air on November 21, 2013, the station's 86th anniversary.\n\nThe CKUA program lineup relies on a number of on-air personalities.\n"}
{"id": "13698942", "url": "https://en.wikipedia.org/wiki?curid=13698942", "title": "CNET", "text": "CNET\n\nCNET (stylized as c|net) is an American media website that publishes reviews, news, articles, blogs, podcasts and videos on technology and consumer electronics globally. Founded in 1994 by Halsey Minor and Shelby Bonnie, it was the flagship brand of CNET Networks and became a brand of CBS Interactive through CNET Networks' acquisition in 2008. CNET originally produced content for radio and television in addition to its website and now uses new media distribution methods through its Internet television network, CNET Video, and its podcast and blog networks.\n\nIn addition, CNET currently has region-specific and language-specific editions. These include the United Kingdom, Australia, China, Japan, French, German, Korean and Spanish. According to third-party web analytics providers, Alexa and SimilarWeb, CNET is the highest-read technology news source on the Web, with over 200 million readers per month, being among the 200 most visited websites globally, as of 2015.\n\nIn 1994, with the help from Fox Network co-founder Kevin Wendle and former Disney creative associate Dan Baker, CNET produced four pilot television programs about computers, technology, and the Internet. CNET TV was composed of \"CNET Central\", \"The Web\", and \"The New Edge\". \"CNET Central\" was created first and aired in syndication in the United States on the USA Network. Later, it began airing on USA's sister network Sci-Fi Channel along with \"The Web\" and \"The New Edge\". These were later followed by TV.com in 1996. Current \"American Idol\" host Ryan Seacrest first came to national prominence at CNET, as the host of \"The New Edge\" and doing various voice-over work for CNET.\n\nIn addition, CNET produced another television technology news program called \"News.com\" that aired on CNBC beginning in 1999.\n\nFrom 2001 to 2003, CNET operated CNET Radio on the Clear Channel-owned KNEW (910) in the San Francisco Bay Area, WBPS (890) in Boston and on XM Satellite Radio. CNET Radio offered technology-themed programming. After failing to attract a sufficient audience, CNET Radio ceased operating in January 2003 due to financial losses.\n\nAs CNET Networks, the site made various acquisitions to expand its reach across various web platforms, regions, and markets.\n\nIn July 1999, CNET acquired the Swiss-based company GDT. GDT was later renamed to CNET Channel.\n\nIn 1998, CNET granted the right to Asiacontent to set up CNET Asia and the operation was brought back in December 2000.\n\nIn January 2000, the same time CNET became CNET Networks, they acquired comparison shopping site mySimon for $736 million.\n\nIn October 2000, CNET Networks acquired ZDNet for approximately $1.6 billion. In January 2001, Ziff Davis Media, Inc. reached an agreement with CNET Networks, Inc. to regain the URLs lost in the 2000 sale of Ziff Davis, Inc. to SoftBank Corp. a publicly traded Japanese media and technology company. In April 2001, CNET acquired TechRepublic Inc., which provides content for IT professionals from Gartner, Inc., for $23 million in cash and stock.\n\nOn July 14, 2004, CNET announced that it would acquire Webshots, the leading photography website for $70 million ($60 million in cash, $10 million in deferred consideration), completing the acquisition that same month.\nIn October 2007, they sold Webshots to American Greetings for $45 million.\n\nIn December 2006, James Kim, an editor at CNET, died in the Oregon wilderness. CNET hosted a memorial show and podcasts dedicated to him.\n\nOn March 1, 2007, CNET announced the public launch of BNET, a website targeted towards business managers. BNET had been running under beta status since 2005.\n\nOn May 15, 2008 it was announced that CBS Corporation would buy CNET Networks for US$1.8 billion. On June 30, 2008, the acquisition was completed. Former CNET properties are now part of CBS Interactive. CBS Interactive now owns many domain names originally created by CNET Networks, including download.com, downloads.com, upload.com, news.com, search.com, TV.com, mp3.com, chat.com, computers.com, shopper.com, radio.com, com.com, and cnet.com.\n\nOn September 19, 2013 CBS Interactive launched a Spanish language sister site under the name CNET en Español. It focuses on topics of relevance primarily to Spanish-speaking technology enthusiasts. The site offered a \"new perspective\" on technology and is under the leadership of managing editor Gabriel Sama.\n\nIn March 2014, CNET refreshed its site by merging with CNET UK and vowing to merge all editions of the agency into a unified agency. This merge brought many changes, foremost of which would be a new user interface and the renaming of CNET TV as CNET Video.\n\nCNET launched a website to cover video games, CNET Gamecenter, in the middle of 1996. According to the \"San Francisco Chronicle\", it was \"one of the first Web sites devoted to computer gaming news\". It became a leading game-focused website; in 1999, \"PC Magazine\" named it one of the hundred-best websites in any field, alongside competitors IGN and GameSpot. According to Gamecenter head Michael Brown, the site received between 50,000 and 75,000 daily visitors by late 2000. In May 2000, CNET founded the Gamecenter Alliance network to bring Gamecenter and four partner websites, including Inside Mac Games, under one banner.\n\nOn July 19, 2000, CNET made public its plan to buy Ziff-Davis and its ZDNet Internet business for $1.6 billion. Because ZDNet had partnered with SpotMedia—parent company of GameSpot—in late 1996, the acquisition brought both GameSpot and Gamecenter under CNET's ownership. Later that year, \"The New York Times\" described the two publications as the \"\"Time\" and \"Newsweek\" of gaming sites\". The paper reported that Gamecenter \"seem[ed] to be thriving\" amid the dot-com crash, with its revenue distributed across online advertising and an affiliate sales program with CNET's Game Shopper website, launched in late 1999.\n\nFollowing an almost $400 million loss at CNET as a result of the dot-com crash, the company ended the Gamecenter Alliance network in January 2001. On February 7, Gamecenter itself was closed in a redundancy reduction effort, as GameSpot was the more successful of the two sites. Around 190 jobs were cut from CNET during this period, including \"at least 20\" at Gamecenter, according to the \"San Francisco Chronicle\". Discussing the situation, Tom Bramwell of Eurogamer reported, \"It is thought[...] that very few if any of the website's staff will move sideways into jobs at GameSpot, now the company's other gaming asset.\" \"The Washington Post\" later noted that Gamecenter was among the \"popular video-game news sites\" to close in 2001, alongside Daily Radar.\n\nWith a catalog of more than 400,000 titles, the Downloads section of the website allows users to download popular software. CNET's download.com provides Windows, Macintosh and mobile software for download. CNET claims that this software is free of spyware, but independent sources have confirmed that this is not the case. Download.com not only hosts software with malware, but their own download wrapper contains adware and bloatware.\n\nIn 1998, CNET was sued by Snap Technologies, operators of the education service CollegeEdge, for trademark infringement relating to CNET's ownership of the domain name Snap.com, due to Snap Technologies already owning a trademark on its name.\n\nIn 2005, Google representatives refused to be interviewed by all CNET reporters for a year after CNET published Google's CEO Eric Schmidt's salary, named the neighborhood where he lives, some of his hobbies and political donations. All the information had been gleaned from Google searches.\n\nOn October 10, 2006, Shelby Bonnie resigned as chairman and CEO, in addition to two other executives, as a result of a stock options backdating scandal that occurred between 1996 and 2003. This would also cause the firm to restate its financial earnings over 1996 through 2003 for over $105 million in resulting expenses. The Securities and Exchange Commission later dropped an investigation into the practice. Neil Ashe was named as the new CEO.\n\nIn 2011, CNET and CBS Interactive were sued by a coalition of artists (led by FilmOn founder Alki David) for copyright infringement by promoting the download of LimeWire, a popular peer to peer downloading software. Although the original suit was voluntarily dropped by Alki David, he vowed to sue at a later date to bring \"expanded\" action against CBS Interactive. In November 2011, another lawsuit against CBS Interactive was introduced, claiming that CNET and CBS Interactive knowingly distributed LimeWire, the file sharing software.\n\nIn January 2013, CNET named Dish Network's \"Hopper with Sling\" digital video recorder as a nominee for the CES \"Best in Show\" award (which is decided by CNET on behalf of its organizers), and named it the winner in a vote by the site's staff. However, CBS abruptly disqualified the Hopper, and vetoed the results because the company was in active litigation with Dish Network. CNET also announced that it could no longer review any product or service provided by companies that CBS are in litigation with (which also includes Aereo). The new vote subsequently gave the Best in Show award to the Razer Edge tablet instead.\n\nDish Network's CEO Joe Clayton said that the company was \"saddened that CNET’s staff is being denied its editorial independence because of CBS’ heavy-handed tactics.\" On January 14, 2013, editor-in-chief Lindsey Turrentine addressed the situation, stating that CNET's staff were in an \"impossible\" situation due to the conflict of interest posed by the situation, and promised that she would do everything within her power to prevent a similar incident from occurring again. The conflict also prompted one CNET senior writer, Greg Sandoval, to resign.\n\nThe decision also drew the ire of staff from the Consumer Electronics Association, the organizers of CES; CEO Gary Shapiro criticized the decision in a \"USA Today\" op-ed column and a statement by the CEA, stating that \"making television easier to watch is not against the law. It is simply pro-innovation and pro-consumer.\" Shapiro felt that the decision also hurt the confidence of CNET's readers and staff, \"destroying its reputation for editorial integrity in an attempt to eliminate a new market competitor.\" As a result of the controversy and fearing damage to the show's brand, the CEA announced on January 31, 2013 that CNET will no longer decide the CES Best in Show award winner due to the interference of CBS (the position has been offered to other technology publications), and the \"Best in Show\" award was jointly awarded to both the Hopper with Sling and Razer Edge.\n\n"}
{"id": "38063354", "url": "https://en.wikipedia.org/wiki?curid=38063354", "title": "COGEN Europe", "text": "COGEN Europe\n\nCOGEN Europe is a European advocacy group based in Belgium that promotes the practice of cogeneration in energy production. The group acts as a liaison between its member companies and European Union energy organizations and committees.\n\nThe managing director of COGEN Europe is also the coordinator of the ene.field project.\n\n"}
{"id": "25379184", "url": "https://en.wikipedia.org/wiki?curid=25379184", "title": "Carbon nanotube nanomotor", "text": "Carbon nanotube nanomotor\n\nA device generating linear or rotational motion using carbon nanotube(s) as the primary component, is termed a nanotube nanomotor. Nature already has some of the most efficient and powerful kinds of nanomotors. Some of these natural biological nanomotors have been re-engineered to serve desired purposes. However, such biological nanomotors are designed to work in specific environmental conditions (pH, liquid medium, sources of energy, etc.). Laboratory-made nanotube nanomotors on the other hand are significantly more robust and can operate in diverse environments including varied frequency, temperature, mediums and chemical environments. The vast differences in the dominant forces and criteria between macroscale and micro/nanoscale offer new avenues to construct tailor-made nanomotors. The various beneficial properties of carbon nanotubes makes them the most attractive material to base such nanomotors on.\nJust fifteen years after making the world's first micrometer-sized motor, Dr. Alex Zettl led his group at University of California at Berkeley to construct the first nanotube nanomotor in 2003. A few concepts and models have been spun off ever since including the nanoactuator driven by a thermal gradient as well as the conceptual electron windmill, both of which were revealed in 2008.\n\nCoulomb's law states that the electrostatic force between two objects is inversely proportional to the square of their distance. Hence, as the distance is reduced to less than a few micrometers, a large force can be generated from seemingly small charges on two bodies. However, electrostatic charge scales quadratically, thereby the electrostatic force also scales quadratically, as the following equations show:\n\nC = \n"}
{"id": "241047", "url": "https://en.wikipedia.org/wiki?curid=241047", "title": "Carbon tetrachloride", "text": "Carbon tetrachloride\n\nCarbon tetrachloride, also known by many other names (the most notable being tetrachloromethane, also recognized by the IUPAC, carbon tet in the cleaning industry, Halon-104 in firefighting, and Refrigerant-10 in HVACR) is an organic compound with the chemical formula CCl. It was formerly widely used in fire extinguishers, as a precursor to refrigerants and as a cleaning agent. It is a colourless liquid with a \"sweet\" smell that can be detected at low levels. It has practically no flammability at lower temperatures.\n\nCarbon tetrachloride was originally synthesized by the French chemist Henri Victor Regnault in 1839 by the reaction of chloroform with chlorine, but now it is mainly produced from methane:\n\nThe production often utilizes by-products of other chlorination reactions, such as from the syntheses of dichloromethane and chloroform. Higher chlorocarbons are also subjected to \"chlorinolysis\":\n\nPrior to the 1950s, carbon tetrachloride was manufactured by the chlorination of carbon disulfide at 105 to 130 °C:\n\nThe production of carbon tetrachloride has steeply declined since the 1980s due to environmental concerns and the decreased demand for CFCs, which were derived from carbon tetrachloride. In 1992, production in the U.S./Europe/Japan was estimated at 720,000 tonnes.\n\nIn the carbon tetrachloride molecule, four chlorine atoms are positioned symmetrically as corners in a tetrahedral configuration joined to a central carbon atom by single covalent bonds. Because of this symmetrical geometry, CCl is non-polar. Methane gas has the same structure, making carbon tetrachloride a halomethane. As a solvent, it is well suited to dissolving other non-polar compounds, fats, and oils. It can also dissolve iodine. It is somewhat volatile, giving off vapors with a smell characteristic of other chlorinated solvents, somewhat similar to the tetrachloroethylene smell reminiscent of dry cleaners' shops.\n\nSolid tetrachloromethane has two polymorphs: crystalline II below −47.5 °C (225.6 K) and crystalline I above −47.5 °C. At −47.3 °C it has monoclinic crystal structure with space group \"C2/c\" and lattice constants \"a\" = 20.3, \"b\" = 11.6, \"c\" = 19.9 (.10 nm), β = 111°.\n\nWith a specific gravity greater than 1, carbon tetrachloride will be present as a dense nonaqueous phase liquid if sufficient quantities are spilled in the environment.\n\nIn organic chemistry, carbon tetrachloride serves as a source of chlorine in the Appel reaction.\n\nPrior to the Montreal Protocol, large quantities of carbon tetrachloride were used to produce the chlorofluorocarbon refrigerants R-11 (trichlorofluoromethane) and R-12 (dichlorodifluoromethane). However, these refrigerants play a role in ozone depletion and have been phased out. Carbon tetrachloride is still used to manufacture less destructive refrigerants. Carbon tetrachloride made from heavy chlorine-37 has been used in the detection of neutrinos.\n\nCarbon tetrachloride is a key ingredient in lava lamps, as it adds weight to the otherwise buoyant wax.\n\nIt once was a popular solvent in organic chemistry, but, because of its adverse health effects, it is rarely used today. It is sometimes useful as a solvent for infrared spectroscopy, because there are no significant absorption bands > 1600 cm. Because carbon tetrachloride does not have any hydrogen atoms, it was historically used in proton NMR spectroscopy. In addition to being toxic, its dissolving power is low. Its use has been largely superseded by deuterated solvents. Use of carbon tetrachloride in determination of oil has been replaced by various other solvents, such as tetrachloroethylene. Because it has no C-H bonds, carbon tetrachloride does not easily undergo free-radical reactions. It is a useful solvent for halogenations either by the elemental halogen or by a halogenation reagent such as \"N\"-bromosuccinimide (these conditions are known as Wohl-Ziegler Bromination).\n\nIn 1910, the Pyrene Manufacturing Company of Delaware filed a patent to use carbon tetrachloride to extinguish fires. The liquid was vaporized by the heat of combustion and extinguished flames, an early form of gaseous fire suppression. At the time it was believed the gas simply displaced oxygen in the area near the fire, but later research found that the gas actually inhibits the chemical chain reaction of the combustion process.\n\nIn 1911, Pyrene patented a small, portable extinguisher that used the chemical. The extinguisher consisted of a brass bottle with an integrated handpump that was used to expel a jet of liquid toward the fire. As the container was unpressurized, it could easily be refilled after use. Carbon tetrachloride was suitable for liquid and electrical fires and the extinguishers were often carried on aircraft or motor vehicles.\n\nIn the first half of the 20th century, another common fire extinguisher was a single-use, sealed glass globe known as a \"fire grenade,\" filled with either carbon tetrachloride or salt water. The bulb could be thrown at the base of the flames to quench the fire. The carbon tetrachloride type could also be installed in a spring-loaded wall fixture with a solder-based restraint. When the solder melted by high heat, the spring would either break the globe or launch it out of the bracket, allowing the extinguishing agent to be automatically dispersed into the fire. A well-known brand was the \"Red Comet,\" which was variously manufactured with other fire-fighting equipment in the Denver, Colorado area by the Red Comet Manufacturing Company from its founding in 1919 until manufacturing operations were closed in the early 1980s.\nCarbon tetrachloride was widely used as a dry cleaning solvent, as a refrigerant, and in lava lamps.\n\nOne specialty use of carbon tetrachloride is in stamp collecting, to reveal watermarks on postage stamps without damaging them. A small amount of the liquid was placed on the back of a stamp, sitting in a black glass or obsidian tray. The letters or design of the watermark could then be clearly seen.\n\nCarbon tetrachloride is one of the most potent hepatotoxins (toxic to the liver), so much so that it is widely used in scientific research to evaluate hepatoprotective agents. Exposure to high concentrations of carbon tetrachloride (including vapor) can affect the central nervous system, degenerate the liver and kidneys, and prolonged exposure may lead to coma or death. Chronic exposure to carbon tetrachloride can cause liver and kidney damage and could result in cancer. See safety data sheets.\n\nThe effects of carbon tetrachloride on human health and the environment have been assessed under REACH in 2012 in the context of the substance evaluation by France. Thereafter, further information has been requested from the registrants. Later this decision was reversed.\n\nIn 2008, a study of common cleaning products found the presence of carbon tetrachloride in \"very high concentrations\" (up to 101 mg/m) as a result of manufacturers' mixing of surfactants or soap with sodium hypochlorite (bleach).\n\nCarbon tetrachloride is also both ozone-depleting and a greenhouse gas. However, since 1992 its atmospheric concentrations have been in decline for the reasons described above (see also the atmospheric time-series figure). CCl has an atmospheric lifetime of 85 years.\n\nUnder high temperatures in air, it forms poisonous phosgene.\n\n"}
{"id": "10189411", "url": "https://en.wikipedia.org/wiki?curid=10189411", "title": "Caroline Haslett", "text": "Caroline Haslett\n\nDame Caroline Harriet Haslett, DBE, JP, born in 1895 in Worth, Sussex, was an English electrical engineer, electricity industry administrator and champion of women's rights.\n\nShe was the first secretary of the Women's Engineering Society and the founder and editor of its journal, \"The Woman Engineer\". She was co-founder, alongside Laura Annie Willson and with the support of Margaret, Lady Moir, of the Electrical Association for Women, which pioneered such 'wonders', as they were described in contemporary magazines, as the All-Electric House in Bristol in 1935. She became the first director of the Electrical Association for Women in 1925. Her chief interest was in harnessing the benefits of electrical power to emancipate women from household chores, so that they could pursue their own ambitions outside the home. In the early 1920s, few houses had electric light or heating, let alone electrical appliances; the National Grid was not yet in existence.\n\nBorn in Worth (now part of Crawley, West Sussex), Caroline Haslett was the eldest daughter of Robert Haslett, a railway signal fitter and activist for the co-operative movement, and his wife, Caroline Sarah, formerly Holmes. After attending school in Haywards Heath, she was employed by the Cochran boiler Company as a clerk and joined the Women's Social and Political Union (WSPU). Transferring to the Cochran workshops during the First World War, she acquired a basic engineering training in London and in Annan, Dumfriesshire; from that time she became a pioneer for women in the electrical and professional world.\n\nIn 1919 Caroline Haslett left Cochran's to become the first secretary of the Women's Engineering Society (WES) and first editor of \"The Woman Engineer\" magazine, which she continued to edit until 1932. In June 1920 she helped to found Atalanta, an engineering firm for women. In November 1924 she co-founded and became the first director of the Electrical Association for Women, of which she remained a director until 1956, when she was obliged to retire because of ill health; from 1924 to 1956 she edited \"The Electrical Age\".\n\nIn 1925 WES came to national attention when it organised a special conference at Wembley, in association with the First International Conference of Women in Science, Industry and Commerce. The conference was opened by the Duchess of York (later Queen Elizabeth the Queen Mother) and was chaired by Nancy, Lady Astor, the first woman to take her seat in the House of Commons. This event also introduced Caroline Haslett to a wider public. She remained secretary of WES until 1929, when she became honorary secretary, and she was the society's president from 1940 to 1941.\n\nHaslett was the sole woman delegate to the World Power Conference in Berlin in 1930 and represented Britain at later power conferences. During the next 20 years her public activities were extraordinary, as described by her friend Margaret Partridge, another president of WES: 'She was a member of council of the British Institute of Management 1946–54, of the Industrial Welfare Society, of the National Industrial Alliance, of the Administrative Staff College, and of King's College of Household and Social Science; a governor of the London School of Economics, of Queen Elizabeth College, and of Bedford College for Women; a member of the Central Committee on Women's Training and Employment; a member of council and vice-president of the Royal Society of Arts 1941–55; and president of the British Federation of Business and Professional Women. She was a member of the Women's Consultative Committee and the Advisory Council of the Appointments Department, Ministry of Labour; a member of the Correspondence Committee on Women's Work of the International Labour Office; and the first woman to be made a Companion of the Institution of Electrical Engineers (IEE).'\n\nIn 1932 the National Safety First Association (the forerunner of the Royal Society for the Prevention of Accidents) extended its activities to home safety, and Caroline Haslett was appointed as chair of the Home Safety Committee, a post she held until 1936. She became the first woman vice-president of the association in 1937.\n\nDuring the Second World War she was the only woman member (and the only safety expert) on the 20-person committee convened by the IEE to examine the requirements for electrical installations in post-war Britain, part of a larger scheme of Post-War Building Studies. An important part of those recommendations was a new plug and socket standard, the first requirement for which was \"To ensure the safety of young children it is of considerable importance that the contacts of the socket-outlet should be protected by shutters or other like means, or by the inherent design of the socket outlet.\" The result was BS 1363.\n\nHaslett became vice-president of the International Federation of Business and Professional Women in 1936 and president of the organisation in 1950; and she was the first woman to chair a government working party – the Board of Trade's Hosiery Industry Working Party 1945–46. For many years she was a member of the Royal Institute of International Affairs and the Royal Institution. She was appointed to Crawley New Town Development Corporation 1947–56; and served as vice-president (1948) and first female chairman (1953–54) of the British Electricity Development Association. She represented the UK government on business missions in the USA, Canada and Scandinavia, and after the Second World War she took a leading role in conferences organised for women in Germany by the British and American authorities.\n\nIn Margaret Partridge's view, the crowning achievement of Haslett's multifaceted career occurred in 1947, when she was appointed a member of the British Electricity Authority (BEA), later the Central Electricity Authority, which was formed to run the industry under national ownership. The BEA named one of the ships in its collier fleet \"Dame Caroline Haslett\" in honour of its first woman member, and the authority set up the Caroline Haslett Trust to provide scholarships and travelling fellowships for its members.\n\nCaroline Haslett's publications include \"The Electrical Handbook for Women\" (1934); \"Teach Yourself Household Electricity\" (in collaboration with E. E. Edwards, 1939); \"Munitions Girl, A Handbook for the Women of the Industrial Army\" (1942); and \"Problems Have No Sex\" (1949). She was also the author of numerous journal articles and conference papers.\n\nIn recognition of Haslett's services to women she was made a Commander of the Order of the British Empire in 1931, and in 1947, in recognition of her work for the Board of Trade and the Ministry of Labour, she was created a Dame Commander of the Order of the British Empire. She was elected a Companion of the Institution of Electrical Engineers (IEE) in 1932. From 1950 until her death she was a Justice of the Peace for the County of London. A blue plaque has been erected to honour her memory by Crawley Arts Council and EDF Energy. It is located in a road named after her: Haslett Avenue East, in Three Bridges, Crawley, West Sussex. Caroline Haslett Primary School in Milton Keynes, Buckinghamshire, is also named after her.\n\nShe retired to live at the home of her sister (and biographer) Rosalind Messenger at Bungay, in Suffolk, where she died from a coronary thrombosis on 4 January 1957. Reportedly, her dying wish was that she be cremated by electricity.\n\n"}
{"id": "20506520", "url": "https://en.wikipedia.org/wiki?curid=20506520", "title": "Dann v. Johnston", "text": "Dann v. Johnston\n\nDann v. Johnston, 425 U.S. 219 (1976), is a decision of the United States Supreme Court on the patentability of a claim for a business method patent.\n\nThe business method at issue in \"Johnston\" was claimed as a “machine system for automatic record-keeping of bank checks and deposits.\" Although the advance was claimed as a system, the invention was a method of creating records of bank checks for expenditures in different categories, such as rent, wages, cost of materials, etc. so that income taxes could more readily be calculated. The system involved such steps as imprinting machine-readable numbers on the individual checks, corresponding to the categories into which the expenditures fell; then the computer would periodically provide a check tabulation, broken down by each category. However, the claims were written in the form of a series of means for performing the steps of the method (in functional language). Accordingly, the claimed subject matter could be argued to be a \"machine.\"\n\nThe Patent Office did not accept that argument and rejected the patent application. It said that Johnston wanted the Office to \"grant a monopoly ... on a method of conducting the banking business.\" Johnston then appealed the rejection to the United States Court of Customs and Patent Appeals (CCPA).\n\nThe CCPA reversed the ruling of the Patent Office (3-2). The majority said that Johnston was claiming a machine, not a process, so that there would be no monopoly on the banking business if other banks used a different machine. The majority also held the Supreme Court's decision in \"Gottschalk v. Benson\" inapplicable because that case involved a process patent while this case involved a machine.\n\nJudge Giles Rich dissented on the ground that the \"Benson\" case had held a computer program patent ineligible and the machine format was immaterial: \" 'Every competent patent draftsman' can draft claims to computer programs either as a process or a machine system.\" Another dissenting judge found the claimed invention obvious.\n\nThe CCPA reversed the Patent Office and the government sought review by the Supreme Court,\n\nThe government sought review on two questions: Whether the claimed business method was ineligible for patent protection, and whether the subject matter was obvious. The Court granted \"certiorari\" on both questions.\n\nThe great majority of the government's brief discussed reasons why the claimed invention was not patent eligible. A small portion addressed the obviousness issue.\n\nJustice Thurgood Marshall delivered the unanimous opinion of the seven-member Court. The Court took notice of the prevalence, indeed ubiquity, of computers in the banking industry. That made computerization an obvious approach to the banking activities involved here. More important, the Dirks patent (claiming a computer-operated system for tracking expenses by category within each department of a business\norganization) was too close in concept to Johnston's system. The Court explained:\n\n[T]he mere existence of differences between the prior art and an invention does not establish the invention's nonobviousness. The gap between the prior art and respondent's system is simply not so great as to render the system nonobvious to one reasonably skilled in the art.\n\nUntil \"Bilski v. Kappos\" and \"Alice Corp. v. CLS Bank International\" about four decades later, \"Johnston\" was the only business-method patent case that the Supreme Court had so far decided. But the decision turns on obviousness rather than patent eligibility. Despite the fact that most of the pages of the government’s brief on the merits were devoted to a discussion of why advances of the type claimed are not eligible to be considered for patentability, the Court declined to reach that question and instead simply found unanimously that the claimed system was obvious.\n"}
{"id": "50340185", "url": "https://en.wikipedia.org/wiki?curid=50340185", "title": "Dolby AC-4", "text": "Dolby AC-4\n\nDolby AC-4 is an audio compression technology developed by Dolby Laboratories. Dolby AC-4 decoders are required to decode 5.1 surround sound. Dolby AC-4 bitstreams can contain audio channels and/or audio objects. Dolby AC-4 has been adopted by the DVB project and standardized by the ETSI.\n\nOn March 10, 2015, the Advanced Television Systems Committee (ATSC) announced that Dolby AC-4 was one of the three standards proposed for the audio system of ATSC 3.0.\n\nOn July 14, 2015, Dolby Laboratories announced that Sony Visual Products and Vizio would support Dolby AC-4.\n\nOn April 14, 2016, Dolby Laboratories announced that Samsung would ship TVs with support for Dolby AC-4 in 2017.\n\nDolby AC-4 can have up to 5.1 core audio channels which all Dolby AC-4 decoders are required to decode. Additional audio channels may be encoded as side signals which Dolby AC-4 decoders can optionally support which would allow for the delivery of 7.1.4 channel audio. Side signals may also contain audio objects. Dolby AC-4 has two different channel based encoding tools with Advanced Joint Channel Coding (A-JCC) used for low bit rates and Advanced Coupling (A-CPL) used for high bit rates. A-JCC doesn't support side signals and is limited to 5.1 channel audio while A-CPL does support side signals. Dolby AC-4 supports up to 7 audio objects with a core decoder and can optionally support additional audio objects with a more advanced decoder. The use of different decoders allows Dolby AC-4 to support lower cost devices while also allowing for more advanced decoders for AV receivers.\n\nDolby states that Dolby AC-4 provides a 50% reduction in bit rate over Dolby Digital Plus. When Dolby AC-4 was tested by the DVB the MUSHRA score was 90 at 192 kbit/s for 5.1 channel audio. When tested for ATSC 3.0 the bit rates needed for the required audio score was 96 kbit/s for stereo audio, 192 kbit/s for 5.1 channel audio, and 288 kbit/s for 7.1.4 channel audio.\n\nDolby AC-4 is extensible and audio substreams allow for new features to be added to Dolby AC-4 while maintaining compatibility with older decoders.\n\nDolby AC-4 is covered by patents and requires a license from Dolby Laboratories. Dolby AC-4 has a consumer royalty rate of US$0.15 to US$1.20 depending on the type of device and volume of sales. Dolby only charges for one technology per device, which means that Dolby AC-4 effectively costs nothing in devices that include existing Dolby technologies such as Dolby Digital Plus. The professional royalty rate is up to US$50 for an eight channel transcoder.\n\n"}
{"id": "29688124", "url": "https://en.wikipedia.org/wiki?curid=29688124", "title": "Energy autarkic/autonomic habitats", "text": "Energy autarkic/autonomic habitats\n\nThe purpose of energy-autarkic habitats is to be independent of 3rd parties concerning energy consumption for living. This may be based and resource-efficiency respective material, living space and energy.\n\nAutarky can be defined as the quality of being self-sufficient. As well as Autonomy reflects to an autonomous person that acts morally solely for the sake of doing \"good\", independently of other incentives.\n\nIn former times nearly everyone had in some kind a form of autarkic living to a certain point of view, as people had to take care of satisfying' their basic needs (food, living - as it is still done in subsistence agriculture), until a network of proving these needs was established (craft, trade, profession).\n\nIn times of the industrialization also networks for energy have been set up, which led to a sort of dependency but made live \"easier\".\n\nNowadays there can be seen a trend \"back to the roots\" - being independent of coal, gas and oil using renewable energy (wind, sun, water, wood).\n\nThere are several historical examples of habitats that could be considered autarkic, including: stone age caves/shelters/huts, mountain cabins, Bedouin tents, and Inuit igloos.\n\nEven in antic times people thought of energy-efficient living and tried to optimise there conditions.\nBy orientation to the winter sun to get as much sunrays as possible for heating up the building and less exposure to the winter wind, protection from the summer sun to achieve the different effect and ventilation by the summer wind, compact forms with less surface.\n\nAn Igloo is a habitat with a compact form that has little surface (compare the low energy form of a drop of water; compare the optimised relation of volume to surface of a ball or globe). This results in less exposure to the wind and reduces hence the cooling of the building. The igloo contains an airtight shell which again assists to keep the temperature inside constant. Concerning heating there is relatively small volume to heat (as mentioned above; volume-surface-ratio).\n\nA peat house provides, similar to the igloo, a compact form with less surface and hence less exposure to the wind. It also shows a nearly airtight shell with good heat insulation (peat). An optimised orientation to the sun adds additional degrees during the day.\n\nLog houses have the same advantages as mentioned above. The material wood offers also a good heat insulation and a good airtightness can be reached by filling the gaps with moss and clay.\n\nMonte Rosa Cabins mark a milestone in high alpine building presenting a high degree of energy autarky of over 90%, meaning that 90% of needed energy is obtained locally from renewable sources. The technical systems are based on existing technology (combined heat and power unit, photovoltaics, thermal collectors), innovative wastewater treatment (the wastewater is going to be purified on-site to be reused as greywater) and foremost an ingenious energy management, which takes into account external conditions such as weather forecast and anticipated occupancy schedules to achieve the demanding goal of the high degree of autarky.\n\nThe basis for Mikrohaus is a 8,80 × 3,44 × 3,25 m cube that allows as kind of modular assembly system to combine several of them and hence to generate a flexibility in the size. The cube(s) don’t have any fundament (only screws of 1,40 m length) which allows some kind of mobile living. No cement and therefore time for drying is requisite. Additional floats allow also an installation on water. The building offers a high isolation up to passive house including e.g. photovoltaic (PV) energy supply.\nThe cubical form allows a good ratio from volume to surface.\nThe concept includes waste water treatment for greywater via wastewater treatment plant with an ultraviolet light (UV) treatment unit for sanitary water.\nAdditional planted walls (www.gruenwand.at) are used outside (for cooling/summer, isolation/winter, noise prevention) and inside (for climate and humidity aspects) which also are used as water-filters as kind of biological purification plant.\n\nAutarc homes provides the 1st swimming passive-house worldwide and was originated to provide clients autarkic buildings on the basis of a protective handling of our natural resources to present the following generations a working basis of life.\nThe swimming and rotatable passiv-house has its own on-board energy generation, water supply and disposal.\nThe idea is to provide a sustainable, energy-efficient, environmentally compatible and affordable living space. The building can be orientated at the sun to get the most possible effect out of it. As this building is located on water the rotation process is done very energy-efficient.\nThe tasks are decentralized energy supply and storage, decentralized effluent disposal and recycling/ reprocessing and drinking water supply.\n\nLife on our planet is a permanent energy flow between living things and the environment. Most of the currently used energy resources, such as oil, coal, natural gas and uranium are non-renewable. The supply of them buried in the earth is limited and we are using them up at a rapid pace.\nRenewable energy resources such as Solar energy, wind energy, small hydroelectricity (e.g. Swimming hydroelectric power plant), geothermal energy and biomass fuels are becoming increasingly attractive. Solar, Photovoltaics, water and wind energy do not send pollutants into the air as occurs with coal and petroleum energy.\n\nHydrogen storage (storage by hydrogen)\nThe objectives are to store H in solid metal hydrides from which it can be readily recovered by heating which is an alternative and safe, highly volume efficient storage method. The final aim is to provide a storage technology that is attractive both economically and environmentally.\n\n\n\n"}
{"id": "47880216", "url": "https://en.wikipedia.org/wiki?curid=47880216", "title": "Externalities of automobiles", "text": "Externalities of automobiles\n\nThe externalities of automobiles, as similarly other economic externalities, are the measurable costs for other parties except the car proprietor, such costs not being taken into account when the proprietor opts to drive their car. According to the Harvard University, the main externalities of driving are local and global pollution, oil dependence, traffic congestion\nand traffic accidents; while according to a meta-study conducted by the Delft University these externalities are congestion and scarcity costs, accident costs, air pollution costs, noise costs, climate change costs, costs for nature and landscape, costs for water pollution, costs for soil pollution and costs of energy dependency.\n\nThe negative externalities seem to be the most obvious to confirm, since the driver does not take into account, for example, the negative effects of air pollution on third parties, when they opt to drive their car. The legislators and the regulators shall, therefore, internalize those external costs, either by taxes on fuels for example, either by any kind of limitation to car usage, such as parking meters or urban tolls. Nevertheless, it seems the drivers in some countries, already pay some external costs with taxes. Road taxes in the Netherlands for instance, have a relatively high yearly value, which covers the maintenance of the infrastructures. Nevertheless, in the majority of western nations, the external costs of driving, are not covered totally either by taxes, or by any kind of car usage limitation.\n\nIncreased reliance on the automobile leads to increased road congestion. While expansions in road capacity are often touted as relieving congestion, induced demand often means that any reductions in congestion are temporary.\n\nCars are the leading cause of fatal traffic accidents in many countries, cars are the leading cause of death of youth and children. In addition to that, health care costs are largely borne by society and injuries to healthy productive citizens caused by car accidents have a cost of millions of disability adjusted life years every year. Vision zero is one approach to reduce road fatalities.\n\nCars produce numerous harmful air pollutants in their exhaust such as Nitrogen oxides, particulate matter, low atmospheric ozone (indirectly) and in the case of leaded fuel, lead. Those pollutants are known to cause various respiratory and other health issues and cars are among the leading cause of smog in modern developed world cities.\n\nCars significantly contribute to noise pollution. While on common perception the engine is the main cause for noise, at city speeds the noise produced by wheel and asphalt is commonly the dominant factor while at highway speeds air friction noises become a major factor.\n\nClimate change is significantly caused by human activity, particularly the production of greenhouse gasses and their release into the atmosphere. Cars produce more Carbon dioxide per passenger kilometer than any other form of land transport. In addition to that Nitrogen oxides are also greenhouse gasses.\n\nRoads, parking spaces but also suburban sprawl caused by cars need significant amount of space. Typically, once agricultural or uncultivated land is turned over into ever wider motorways and ever larger parking lots to accommodate the automobile but induced demand means any relief is temporary and more and more surfaces are sealed in the process.\n\nLubricants and fuels used by automobiles are harmful when they leak into the groundwater. Oil refineries and particularly the mining of unconventional oil like oil shales and oil sands can be extremely harmful for the surrounding water resources and bodies of water.\n\nIn addition to that runoff of impervious surfaces like roads or parking lots can be contaminated with all sorts of pollutants.\n\nIn addition to the fertile topsoil often \"buried\" under freeways and parking spaces, cars directly or indirectly release pollutants into the soil. Oil may leak into the groundwater and the common practice to clean cars in the front yard causes surfactants and other products in the cleaning products to pollute the ground. Similarly, salt is often used to keep roads and highways free of snow and ice and chlorides cause major damage to vegetation as well as being an aggressive substance linked to rust and corrosion.\n\nWhile trains and tramway often run on electricity which can be generated through renewable sources or locally available fuel, cars by and large run on petroleum derived fuels. Only a handful of countries are net exporters of petroleum. For developed countries this causes a political dependence on a reliable petroleum supply and has been cited as the reason for foreign policy decisions of the United States among others. For developing countries, petroleum products can be among the chief imports and reliance on automobiles can significantly impact the trade deficit and public debt of such nations.\n\nSome research indicates a correlation between urban sprawl and obesity. Car centric development and lack of walkability lead to less use of active modes of transportation such as utility cycling and walking which is linked to various health issues caused by a lack of exercise.\n\nWhile the existence of negative externalities seem consensual, the existence of positive externalities of the automobile does not have consensus amongst economists and experts in the transportation sector. The creation of jobs or the fact that the related industries pay taxes, cannot be considered, as such, as positive externalities, because any legal economic activity pays taxes, and the big majority also needs job demand. Time saving to the driver, and therefore eventually more personal production, cannot either be considered a positive externality, because the driver has already taken those factors into account when they opted to use their car, and therefore these factors cannot be considered, by many authors, a pure externality.\n\nNotwithstanding the above objections, some authors enumerate positive externalities for the automobile like accessibility and land value. Where land is expensive, it is developed more intensively. Where it is more intensively developed, there are more activities and destinations that can be reached in a given time. Where there are more activities, accessibility is higher and where accessibility is higher, land is more expensive.\n\nHowever observations show that less car dependent forms of development produce denser settlement patterns and higher land values.\n\nEconomists have sought to understand why cities grow and why large cities seem to be at an advantage relative to others. One explanation that has received much attention emphasizes the role of agglomeration economies in facilitating and sustaining city growth. The clustering of firms and workers in cities generates positive externalities by allowing for labor market pooling, input sharing, and knowledge spillovers.\n\nNevertheless some other economists mention urban decay and urban sprawl as a negative effect or cost of the automobile, when the city grows due to automobile dependency.\n\n"}
{"id": "11968699", "url": "https://en.wikipedia.org/wiki?curid=11968699", "title": "Fish factory", "text": "Fish factory\n\nA fish factory, also called a fish plant, fish processing facility, is a facility where fish processing is performed. Fish factories range in the size and range of species of fish they process. Some species of fish, such as mackerel and herring, and can be caught at sea by large pelagic trawlers and offloaded to the factory within a few days of being caught. Or the fish can be caught by freezer trawlers that freeze the fish before providing it to factories, or by factory ships which can do the processing themselves on board. Some fish factories have fishing vessels catching fish for them at a given times of the year. This is to do with quotas and seasons conflicting how much and when the fish can be landed.\n\n\n\n"}
{"id": "1905105", "url": "https://en.wikipedia.org/wiki?curid=1905105", "title": "Flexible-fuel vehicle", "text": "Flexible-fuel vehicle\n\nA flexible-fuel vehicle (FFV) or dual-fuel vehicle (colloquially called a flex-fuel vehicle) is an alternative fuel vehicle with an internal combustion engine designed to run on more than one fuel, usually gasoline blended with either ethanol or methanol fuel, and both fuels are stored in the same common tank. Modern flex-fuel engines are capable of burning any proportion of the resulting blend in the combustion chamber as fuel injection and spark timing are adjusted automatically according to the actual blend detected by a fuel composition sensor. Flex-fuel vehicles are distinguished from bi-fuel vehicles, where two fuels are stored in separate tanks and the engine runs on one fuel at a time, for example, compressed natural gas (CNG), liquefied petroleum gas (LPG), or hydrogen.\n\nThe most common commercially available FFV in the world market is the ethanol flexible-fuel vehicle, with about 50 million + automobiles, motorcycles and light duty trucks manufactured and sold worldwide by mid 2015, and concentrated in four markets, Brazil (29.0 million by 2017-YTD), the United States (17.4 million by the end of 2014), Canada (1.6 million by 2014), and Europe, led by Sweden (243,100). In addition to flex-fuel vehicles running with ethanol, in Europe and the US, mainly in California, there have been successful test programs with methanol flex-fuel vehicles, known as M85 flex-fuel vehicles. There have been also successful tests using P-series fuels with E85 flex fuel vehicles, but as of June 2008, this fuel is not yet available to the general public. These successful tests with P-series fuels were conducted on Ford Taurus and Dodge Caravan flexible-fuel vehicles.\n\nThough technology exists to allow ethanol FFVs to run on any mixture of gasoline and ethanol, from pure gasoline up to 100% ethanol (E100), North American and European flex-fuel vehicles are optimized to run on E85, a blend of 85% anhydrous ethanol fuel with 15% gasoline. This upper limit in the ethanol content is set to reduce ethanol emissions at low temperatures and to avoid cold starting problems during cold weather, at temperatures lower than . The alcohol content is reduced during the winter in regions where temperatures fall below to a winter blend of E70 in the U.S. or to E75 in Sweden from November until March. Brazilian flex fuel vehicles are optimized to run on any mix of E20-E25 gasoline and up to 100% hydrous ethanol fuel (E100). The Brazilian flex vehicles are built-in with a small gasoline reservoir for cold starting the engine when temperatures drop below . An improved flex motor generation was launched in 2009 which eliminated the need for the secondary gas tank.\n\nAs ethanol FFVs became commercially available during the late 1990s, the common use of the term \"flexible-fuel vehicle\" became synonymous with ethanol FFVs. In the United States flex-fuel vehicles are also known as \"E85 vehicles\". In Brazil, the FFVs are popularly known as \"total flex\" or simply \"flex\" cars. In Europe, FFVs are also known as \"flexifuel\" vehicles. Automakers, particularly in Brazil and the European market, use badging in their FFV models with the some variant of the word \"flex\", such as Volvo \"Flexifuel\", or Volkswagen \"Total Flex\", or Chevrolet \"FlexPower\" or Renault \"Hi-Flex\", and Ford sells its Focus model in Europe as \"Flexifuel\" and as \"Flex\" in Brazil. In the US, only since 2008 FFV models feature a yellow gas cap with the label \"E85/Gasoline\" written on the top of the cap to differentiate E85s from gasoline only models.\n\nFlexible-fuel vehicles (FFVs) are based on dual-fuel systems that supply both fuels into the combustion chamber at the same time in various calibrated proportions. The most common fuels used by FFVs today are unleaded gasoline and ethanol fuel. Ethanol FFVs can run on pure gasoline, pure ethanol (E100) or any combination of both. Methanol has also been blended with gasoline in flex-fuel vehicles known as M85 FFVs, but their use has been limited mainly to demonstration projects and small government fleets, particularly in California.\n\nThe Ford Model T, produced from 1908 through 1927, was fitted with a carburetor with adjustable jetting, allowing use of ethanol, gasoline or kerosene (each by itself), or a combination of the first two mentioned fuels. Other car manufactures also provided engines for ethanol fuel use. Henry Ford continued to advocate for ethanol as fuel even during Prohibition. However, cheaper oil caused gasoline to prevail, until the 1973 oil crisis resulted in gasoline shortages and awareness on the dangers of oil dependence. This crisis opened a new opportunity for ethanol and other alternative fuels, such as methanol, gaseous fuels such as CNG and LPG, and also hydrogen. Ethanol, methanol and natural gas CNG were the three alternative fuels that received more attention for research and development, and government support.\n\nSince 1975, and as a response to the shock caused by the first oil crisis, the Brazilian government implemented the National Alcohol Program -Pró-Álcool- (), a nationwide program financed by the government to phase out automotive fuels derived from fossil fuels in favor of ethanol made from sugar cane. It began with a low blend of anhydrous alcohol with regular gasoline in 1976, and since July 2007 the mandatory blend is 25% of alcohol or gasohol E25. In 1979, and as a response to the second oil crisis, the first vehicle capable of running with pure hydrous ethanol (E100) was launched to the market, the Fiat 147, after testing with several prototypes developed by Fiat, Volkswagen, GM and Ford. The Brazilian government provided three important initial drivers for the ethanol industry: guaranteed purchases by the state-owned oil company Petrobras, low-interest loans for agro-industrial ethanol firms, and fixed gasoline and ethanol prices. After reaching more than 4 million cars and light trucks running on pure ethanol by the late 1980s, the use of E100-only vehicles sharply declined after increases in sugar prices produced shortages of ethanol fuel.\n\nAfter extensive research that began in the 90s, a second push took place in March 2003, when the Brazilian subsidiary of Volkswagen launched to the market the first full flexible-fuel car, the Gol 1.6 Total Flex. Several months later was followed by other Brazilian automakers, and by 2010 General Motors, Fiat, Ford, Peugeot, Renault, Volkswagen, Honda, Mitsubishi, Toyota, Citroën, Nissan and Kia Motors were producing popular models of flex cars and light trucks. The adoption of ethanol flex fuel vehicles was so successful, that production of flex cars went from almost 40 thousand in 2003 to 1.7 million in 2007. This rapid adoption of the flex technology was facilitated by the fuel distribution infrastructure already in place, as around 27,000 filling stations countrywide were available by 1997 with at least one ethanol pump, a heritage of the \"Pró-Álcool\" program.\n\nIn the United States, initial support to develop alternative fuels by the government was also a response to the first oil crisis, and some time later, as a goal to improve air quality. Also, liquid fuels were preferred over gaseous fuels not only because they have a better volumetric energy density but also because they were the most compatible fuels with existing distribution systems and engines, thus avoiding a big departure from the existing technologies and taking advantage of the vehicle and the refueling infrastructure. California led the search of sustainable alternatives with interest focused in methanol. Ford Motor Company and other automakers responded to California's request for vehicles that run on methanol. In 1981, Ford delivered 40 dedicated methanol fuel (M100) Escorts to Los Angeles County, but only four refueling stations were installed. The biggest challenge in the development of alcohol vehicle technology was getting all of the fuel system materials compatible with the higher chemical reactivity of the fuel. Methanol was even more of a challenge than ethanol but much of the early experience gained with neat ethanol vehicle production in Brazil was transferable to methanol. The success of this small experimental fleet of M100s led California to request more of these vehicles, mainly for government fleets. In 1983, Ford built 582 M100 vehicles; 501 went to California, and the remaining to New Zealand, Sweden, Norway, United Kingdom, and Canada.\nAs an answer to the lack of refueling infrastructure, Ford began development of a flexible-fuel vehicle in 1982, and between 1985 and 1992, 705 experimental FFVs were built and delivered to California and Canada, including the 1.6L Ford Escort, the 3.0L Taurus, and the 5.0L LTD Crown Victoria. These vehicles could operate on either gasoline or methanol with only one fuel system. Legislation was passed to encourage the US auto industry to begin production, which started in 1993 for the M85 FFVs at Ford. In 1996, a new FFV Ford Taurus was developed, with models fully capable of running on either methanol or ethanol blended with gasoline. This ethanol version of the Taurus became the first commercial production of an E85 FFV. The momentum of the FFV production programs at the American car companies continued, although by the end of the 1990s, the emphasis shifted to the FFV E85 version, as it is today. Ethanol was preferred over methanol because there is a large support from the farming community, and thanks to the government's incentive programs and corn-based ethanol subsidies available at the time. Sweden also tested both the M85 and the E85 flexifuel vehicles, but due to agriculture policy, in the end emphasis was given to the ethanol flexifuel vehicles. Support for ethanol also comes from the fact that it is a biomass fuel, which addresses climate change concerns and greenhouse gas emissions, though nowadays these benefits are questioned and depend on the feedstock used for ethanol production and their indirect land use change impacts.\n\nThe demand for ethanol fuel produced from field corn in the United States was stimulated by the discovery in the late 90s that methyl tertiary butyl ether (MTBE), an oxygenate additive in gasoline, was contaminating groundwater. Due to the risks of widespread and costly litigation, and because MTBE use in gasoline was banned in almost 20 states by 2006, the substitution of MTBE opened a new market for ethanol fuel. This demand shift for ethanol as an oxygenate additive took place at a time when oil prices were already significantly rising. By 2006, about 50 percent of the gasoline used in the U.S. contained ethanol at different proportions, and ethanol production grew so fast that the US became the world's top ethanol producer, overtaking Brazil in 2005. This shift also contributed to a sharp increase in the production and sale of E85 flex vehicles since 2002.\n\nFlexible-fuel technology started being developed by Brazilian engineers near the end of the 1990s. The Brazilian flexible fuel car is built with an ethanol-ready engine and one fuel tank for both fuels. The small gasoline reservoir for starting the engine in cold weather, used in earlier neat ethanol vehicles, was kept to avoid start up problems in the central and southern regions, where winter temperatures normally drop below . An improved flex motor generation was launched in 2009 and allowed to eliminate the need for this secondary gas reservoir tank. Another improvement was the reduction of fuel consumption and tailpipe emissions, between 10% to 15% as compared to flex motors sold in 2008. In March 2009 Volkswagen do Brasil launched the Polo E-Flex, the first flex fuel model without an auxiliary tank for cold start.\n\nA key innovation in the Brazilian flex technology was avoiding the need for an additional dedicated sensor to monitor the ethanol-gasoline mix, which made the first American M85 flex fuel vehicles too expensive.\n\nBrazilian flex cars are capable of running on just hydrated ethanol (E100), or just on a blend of gasoline with 25 to 27% anhydrous ethanol (the mandatory blend), or on any arbitrary combination of both fuels.\n\nThe flexibility of Brazilian FFVs empowers the consumers to choose the fuel depending on current market prices. As ethanol fuel economy is lower than gasoline because of ethanol's energy content is close to 34% less per unit volume than gasoline, flex cars running on ethanol get a lower mileage than when running on pure gasoline. However, this effect is partially offset by the usually lower price per liter of ethanol fuel. As a rule of thumb, Brazilian consumers are frequently advised by the media to use more alcohol than gasoline in their mix only when ethanol prices are 30% lower or more than gasoline, as ethanol price fluctuates heavily depending on the result of seasonal sugar cane harvests.\n\nIn March 2003 Volkswagen do Brasil launched in the market the Gol 1.6 Total Flex, the first commercial flexible fuel vehicle capable of running on any blend of gasoline and ethanol. GM do Brasil followed three months later with the Chevrolet Corsa 1.8 Flexpower, using an engine developed by a joint-venture with Fiat called PowerTrain. Passenger flex-fuel vehicles became a commercial success in the country, and , a total of 15 car manufacturers produce flex-fuel engines for the Brazilian market, dominating all light vehicle segments except sports cars, off-road vehicles and minivans.\n\nThe production of flex-fuel cars and light commercial vehicles since 2003 reached the milestone of 10 million vehicles in March 2010. At the end of 2012 registrations of flex-fuel cars and light trucks represented 87% of all passenger and light duty vehicles sold in the country in 2012, and climbed to a 94% market share of all new passenger vehicles sales in 2013. Production passed the 20 million-unit mark in June 2013. By the end of 2014, flex-fuel cars represented 54% of the Brazilian registered stock of light-duty vehicles, while gasoline only vehicles represented 34.3%. , flex-fuel light-duty vehicle sales totaled 25.5 million units.\n\nThe rapid success of flex vehicles was made possible by the existence of 33,000 filling stations with at least one ethanol pump available by 2006, a heritage of the early \"Pró-Álcool\" ethanol program. These facts, together with the mandatory use of E25 blend of gasoline throughout the country, allowed Brazil in 2008 to achieve more than 50% of fuel consumption in the gasoline market from sugar cane-based ethanol.\n\nAccording to two separate research studies conducted in 2009, at the national level 65% of the flex-fuel registered vehicles regularly used ethanol fuel, and the usage climbed to 93% in São Paulo, the main ethanol producer state where local taxes are lower, and prices at the pump are more competitive than gasoline. However, as a result of higher ethanol prices caused by the Brazilian ethanol industry crisis that began in 2009, combined with government subsidies to keep gasoline price lower than the international market value, by November 2013 only 23% flex-fuel car owners were using ethanol, down from 66% in 2009.\n\nOne of the latest innovation within the Brazilian flexible-fuel technology is the development of flex-fuel motorcycles. The first flex-fuel motorcycle was launched by Honda in March 2009, the CG 150 Titan Mix. In September 2009, Honda launched a second flexible-fuel motorcycle, the on-off-road NXR 150 Bros Mix. By December 2012 the five available models of flexible-fuel motorcycles from Honda and Yamaha reached a cumulative production of 2,291,072 units, representing 31.8% of all motorcycles manufactured in Brazil since 2009, and 48.2% of motorcycle production in 2012. Flexible-fuel motorcycle production passed the 3 million-unit milestone in October 2013. The 4 million mark was reached in March 2015.\n\nFlexible-fuel vehicles were introduced in Sweden as a demonstration test in 1994, when three Ford Taurus were imported to show the technology existed. Because of the existing interest, a project was started in 1995 with 50 Ford Taurus E85 flexifuel in different parts of Sweden: Umeå, Örnsköldsvik, Härnösand, Stockholm, Karlstad, Linköping, and Växjö. From 1997 to 1998 an additional 300 Taurus were imported, and the number of E85 fueling grew to 40. Then in 1998 the city of Stockholm placed an order for 2,000 of FFVs for any car manufacturer willing to produce them. The objective was to jump-start the FFV industry in Sweden. The two domestic car makers Volvo Group and Saab AB refused to participate arguing there were not in place any ethanol filling stations. However, Ford Motor Company took the offer and began importing the flexifuel version of its Focus model, delivering the first cars in 2001, and selling more than 15,000 FFV Focus by 2005, then representing an 80% market share of the flexifuel market.\n\nIn 2005 both Volvo and Saab introduced to the Sweden market their flexifuel models. Saab began selling its 9-5 2.0 Biopower, joined in 2006 by its 9-5 2.3 Biopower. Volvo introduced its S40 and V50 with flexible-fuel engines, joined in late 2006 by the new C30. All Volvo models were initially restricted to the Sweden market, until 2007, when these three models were launched in eight new European markets. In 2007, Saab also started selling a BioPower version of its popular Saab 9-3 line. In 2008 the Saab-derived Cadillac BLS was introduced with E85 compatible engines, and Volvo launched the V70 with a 2.5-litre turbocharged Flexifuel engine.\n\nAll flexible-fuel vehicles in Sweden use an E75 winter blend instead of E85 to avoid engine starting problems during cold weather. This blend was introduced since the winter 2006-07 and E75 is used from November until March. For temperature below E85 flex vehicles require an engine block heater. The use of this device is also recommended for gasoline vehicles when temperatures drop below . Another option when extreme cold weather is expected is to add more pure gasoline in the tank, thus reducing the ethanol content below the E75 winter blend, or simply not to use E85 during extreme low temperature spells.\n\nSweden has achieved the largest E85 flexible-fuel vehicle fleet in Europe, with a sharp growth from 717 vehicles in 2001 to 243,136 through December 2014. As of 2008 a total of 70% of all flexifuel vehicles operating in the EU were registered in Sweden. The recent and accelerated growth of the Swedish fleet of E85 flexifuel vehicles is the result of the National Climate Policy in Global Cooperation Bill passed in 2005, which not only ratified the Kyoto Protocol but also sought to meet the 2003 EU Biofuels Directive regarding targets for use of biofuels, and also let to the 2006 government's commitment to eliminate oil imports by 2020.\n\nIn order to achieve these goals several government incentives were implemented. Ethanol, as the other biofuels, was exempted of both, the CO and energy taxes until 2009, resulting in a 30% price reduction at the pump of E85 fuel over gasoline. Furthermore, other demand side incentives for flexifuel vehicle owners include a bonus to buyers of FFVs, exemption from the Stockholm congestion tax, up to 20% discount on auto insurance, free parking spaces in most of the largest cities, owner annual registration taxes, and a 20% tax reduction for flexifuel company cars. Also, a part of the program, the Swedish Government ruled that 25% of their vehicle purchases (excluding police, fire and ambulance vehicles) must be alternative fuel vehicles. By the first months of 2008, this package of incentives resulted in sales of flexible-fuel cars representing 25% of new car sales.\n\nOn the supply side, since 2005 the gasoline fuelling stations selling more than 3 million liters of fuel a year are required to sell at least one type of biofuel, resulting in more than 1,200 gas stations selling E85 by August 2008. Despite all the sharp growth of E85 flexifuel cars, by 2007 they represented just two percent of the four million Swedish vehicles. In addition, this law also mandated all new filling stations to offer alternative fuels, and stations with an annual volume of more than one million liters are required to have an alternative fuel pump by December 2009. Therefore, the number of E85 pumps is expected to reach by 2009 nearly 60% of Sweden's 4,000 filling stations.\n\nThe Swedish-made Koenigsegg CCXR, a limited edition and version of the CCX, is currently the fastest and most powerful flexible fuel vehicle with its twin-supercharged V8 producing 1018 hp when running on biofuel, as compared to 806 hp on 91 octane US unleaded gasoline.\n\nFlexifuel vehicles are sold in 18 European countries, including Austria, Belgium, Czech Republic, Denmark, Estonia, Finland, France, Germany, Hungary, Ireland, Italy, the Netherlands, Norway, Poland, Spain, Sweden, Switzerland, and the United Kingdom. Ford, Volvo and Saab are the main automakers offering flexifuel autos in the region.\n\nBiofuel cars in general get strong tax incentives in France, including a 0 or 50% reduction on the tax on new vehicles, and a 40% reduction on CO tax for new cars. For company cars there is a corporate car tax free for 2 years and a recovery of 80% of the value added tax (VAT) on E85 vehicles. Also, E85 fuel price is set significantly lower than diesel or gasoline, resulting in E85 at € 0.80, diesel at €1.15, and gasoline at €1.30 per liter, as of April 2007. By May 2008, France had 211 pumps selling E85, even though the government made plans for the installation of up to 500 E85 pumps by year end 2007. French automakers Renault and PSA (Citroen & Peugeot) announced they will start selling FFV cars beginning in the summer 2007.\n\nBiofuel emphasis in Germany is on biodiesel, and no specific incentives have been granted for E85 flex-fuel cars, however there is complete exemption of taxes on all biofuels while there is a normal tax of €0.65 per liter of petroleum fuels. The distribution of E85 began in 2005, and with 219 stations as of September 2008, Germany ranks second after Sweden with the most E85 fueling stations in the EU.\nAs of July 2012 retail prices of E85 was €1.09 per liter, and gasoline was priced at €1.60 per liter (for gasoline RON 95), then providing enough margin to compensate for ethanol's lower fuel economy.\nFord has offered the Ford Focus since August 2005 in Germany. Ford is about to offer also the Mondeo and other models as FFV versions between 2008 and 2010. The Saab 9-5 and Saab 9-3 Biopower, the Peugeot 308 Bioflex, the Citroën C4 Bioflex, the Audi A5, two models of the Cadillac BLS, and five Volvo models are also available in the German market by 2008. Since 2011, Dacia offers the Logan MCV with a 1.6l 16v flexfuel engine.\n\nIreland is the third best seller European market of E85 flex-fuel vehicles, after Sweden and France. Bioethanol (E85) in Ireland is made from whey, a waste product of cheese manufacturing. The Irish government established several incentives, including a 50% discount in vehicle registration taxes (VRT), which can account for more than one third of the retail price of a new car in Ireland (around €6,500). The bioethanol element of the E85 fuel is excise-free for fuel companies, allowing retail prices to be low enough to offset the 25 per cent cut in fuel economy that E-85 cars offer, due to ethanol's lower energy content than gasoline. Also, the value added tax (VAT) on the fuel can also be claimed back. E-85 fuel is available across the country in more than 20 of Maxol service stations. In October 2005, the 1.8 Ford Focus FFV became the first flexible-fuel vehicle to be commercially sold in Ireland. Later Ford launched the C-max and the Mondeo flexifuel models. Saab and Volvo also have E85 models available.\nFrom 1 January 2011 E85 fuel is no longer excise-free in Ireland. Maxol has announced they will not provide E85 when their current supplies have run out.\n\nThe first flexifuel vehicles were introduced in Spain by late 2007, with the acquisition of 80 cars for use in the Spaniard official government fleet. At that time the country had only three gas stations selling E85, making necessary to deploy an official E85 fueling station in Madrid to attend these vehicles. Despite the introduction in the Spaniard market of several flexifuel models, by the end of 2008 still persists the problems of adequate E85 fueling infrastructure, as only 10 gas stations were selling E85 fuel to the public in the entire country.\n\nThe UK government established several incentives for E85 flex-fuel vehicles. These include a fuel duty rebate on E85 fuel of 20 p per liter, until 2010; a £ 10 to 15 reduction in the vehicle excise duty (VED); and a 2% annual company car tax discount for flex-fuel cars. Despite the small number of E85 pump stations available, limited to the Morrisons supermarket chain stations, most automakers offer the same models in the UK that are available in the European market. In 2005 the Ford Focus Flexi-Fuel became the first flexible-fuel car sold in the UK, though E85 pumps were not opened until 2006. Volvo now offers its flexifuel models S80, S40, C30, V50 and V70. Other models available in the UK are the Ford C-Max Flexi-Fuel, and the Saab models 9-5 and 9-3 Flex-Fuel Biopower, and the new Saab Aero X BioPower E100 bioethanol.\n\nSince 1998 a total of 17.7 million E85 flex-fuel vehicles have been sold or leased in the United States through the end of 2014. About 11 million flex-fuel cars and light trucks were still in operation as of early 2013, up from 7.3 million in 2008, 4.1 million in 2005, and 1.4 million on U.S roads in 2001. For the 2011 model year there are about 70 vehicles E85 capable, including sedans, vans, SUVs and pick-up trucks. Many of the models available in the market are trucks and sport-utility vehicles getting less than when filled with gasoline. Actual consumption of E85 among flex-fuel vehicle owners is limited. Nevertheless, the U.S. Department of Energy estimated that in 2011 only 862,837 flex-fuel fleet-operated vehicles were regularly fueled with E85. As a result, from all the ethanol fuel consumed in the country in 2009, only 1% was E85 consumed by flex-fuel vehicles.\n\nThe E85 blend is used in gasoline engines modified to accept such higher concentrations of ethanol, and the fuel injection is regulated through a dedicated sensor, which automatically detects the amount of ethanol in the fuel, allowing to adjust both fuel injection and spark timing accordingly to the actual blend available in the vehicle's tank. Because ethanol contains close to 34% less energy per unit volume than gasoline, E85 FFVs have a lower mileage per gallon than gasoline. Based on EPA tests for all 2006 E85 models, the average fuel economy for E85 vehicles was 25.56% lower than unleaded gasoline.\n\nThe American E85 flex-fuel vehicle was developed to run on any mixture of unleaded gasoline and ethanol, anywhere from 0% to 85% ethanol by volume. Both fuels are mixed in the same tank, and E85 is sold already blended. In order to reduce ethanol evaporative emissions and to avoid problems starting the engine during cold weather, the maximum blend of ethanol was set to 85%. There is also a seasonal reduction of the ethanol content to E70 (called winter E85 blend) in very cold regions, where temperatures fall below during the winter. In Wyoming for example, E70 is sold as E85 from October to May.\n\nE85 flex-fuel vehicles are becoming increasingly common in the Midwest, where corn is a major crop and is the primary feedstock for ethanol fuel production. Regional retail E85 prices vary widely across the US, with more favorable prices in the Midwest region, where most corn is grown and ethanol produced. Depending of the vehicle capabilities, the break-even price of E85 has to be between 25 and 30% lower than gasoline.\n\nA 2005 survey found that 68% of American flex-fuel car owners were not aware they owned an E85 flex. This was because the exteriors of flex and non-flex vehicles look exactly the same; there is no sale price difference between them; the lack of consumers' awareness about E85s; and also the initial decision of American automakers of not putting any kind of exterior labeling, so buyers could be unaware they are purchasing an E85 vehicle. Since 2008, all new FFV models in the US feature a bright yellow gas cap to remind drivers of the E85 capabilities and proper flex-fuel badging.\n\nSome critics have argued that American automakers have been producing E85 flex models motivated by a loophole in the Corporate Average Fuel Economy (CAFE) requirements, that allows for a fuel economy credit for every flex-fuel vehicle sold, whether or not in practice these vehicles are fueled with E85. This loophole might allow the car industry to meet the CAFE targets in fuel economy just by spending between and that it cost to turn a conventional vehicle into a flex-fuel, without investing in new technology to improve fuel economy, and saving them the potential fines for not achieving that standard in a given model year. The CAFE standards proposed in 2011 for the period 2017-2025 will allow flexible-fuel vehicles to receive extra credit but only when the carmakers present data proving how much E85 such vehicles have actually consumed.\n\nA major restriction hampering sales of E85 flex vehicles, or fueling with E85, is the limited infrastructure available to sell E85 to the public with only 2% of the motor fuel stations offering E85 by March 2014. , there were only 3,218 fueling stations selling E85 to the public in the entire U.S., while about 156,000 retail motor fuel outlets do not offer any ethanol blend. In addition, there has been a great concentration of E85 stations in the Corn Belt states. The main constraint for a more rapid expansion of E85 availability is that it requires dedicated storage tanks at filling stations, at an estimated cost of for each dedicated ethanol tank. The Obama Administration set the goal of installing 10,000 blender pumps nationwide until 2015, and to support this target the US Department of Agriculture (USDA) issued a rule in May 2011 to include flexible fuel pumps in the Rural Energy for America Program (REAP). This ruling will provide financial assistance to fuel station owners to install E85 and blender pumps.\n\nA flex fuel conversion kit is a kit that allows a conventional equipment manufactured vehicle to be altered to operate on propane, natural gas, methane gas, ethanol, or electricity are classified as aftermarket AFV conversions. All vehicle conversions, except those that are completed for a vehicle to run on electricity, must meet current applicable U.S. Environmental Protection Agency (EPA) standards.\n\nIn 2008, Ford delivered the first flex-fuel plug-in hybrid as part of a demonstration project, a Ford Escape Plug-in Hybrid capable of running on E85 or gasoline. General Motors announced that the new Chevrolet Volt plug-in hybrid, launched in the United States market in December 2010, would be flex-fuel-capable in 2013. General Motors do Brasil announced that it will import from five to ten Volts to Brazil during the first semester of 2011 as part of a demonstration and also to lobby the federal government to enact financial incentives for green cars. If successful, GM would adapt the Volt to operate on ethanol fuel, as most new cars sold in Brazil are flex-fuel.\n\nIn 2008, Chrysler, General Motors, and Ford pledged to manufacture 50 percent of their entire vehicle line as flexible fuel in model year 2012, if enough fueling infrastructure develops. The Open Fuel Standard Act (OFS), introduced to Congress in May 2011, is intended to promote a massive adoption of flex-fuel vehicles capable of running on ethanol or methanol. The bill requires that 50 percent of automobiles made in 2014, 80 percent in 2016, and 95 percent in 2017, would be manufactured and warranted to operate on non-petroleum-based fuels, which includes existing technologies such as flex-fuel, natural gas, hydrogen, biodiesel, plug-in electric and fuel cell.\n\n, almost half of new vehicles produced by Chrysler, Ford, and General Motors are flex-fuel, meaning roughly one-quarter of all new vehicles sold by 2015 are capable of using up to E85. However, obstacles to widespread use of E85 fuel remain. A 2014 analysis by the Renewable Fuels Association (RFA) found that oil companies prevent or discourage affiliated retailers from selling E85 through rigid franchise and branding agreements, restrictive supply contracts, and other tactics. The report showed independent retailers are five times more likely to offer E85 than retailers carrying an oil company brand.\n\nIn January 2007 GM brought UK-sourced Saab 9-5 Biopower E85 flex-fuel vehicles to Australia as a trial, in order to measure interest in ethanol-powered vehicles in the country. Saab Australia placed the vehicles with the fleets of the Queensland Government, the media, and some ethanol producers. E85 is not available widely in Australia, but the Manildra Group provided the E85 blend fuel for this trial.\n\nSaab Australia became the first car maker to produce an E85 flex-fuel car for the Australian market with the Saab 9-5 BioPower. One month later launched the new 9-3 BioPower, the first vehicle in Australia to give drivers a choice of three fuels, E85, diesel or gasoline, and both automobiles are sold for a small premium. Australia's largest independent fuel retailer, United Petroleum, announced plans to install Australia's first commercial E85 fuel pumps, one in Sydney and one in Melbourne.\n\nGM Holden, the Victorian state government, Coskata, Caltex, Veolia Environmental Services and Mitsui have announced a consortium with a co-ordinated plan to build a bio-ethanol plant from household waste for use as E85 fuel. In August 2010 Caltex launched the E85 ethanol fuel called Bio E-Flex, designed for use in the Holden Commodore VE Series II flex-fuel vehicles to be released later in 2010. Caltex Australia plans to begin selling Bio E-Flex in Melbourne from September and expects to have Bio E-Flex available in more than 30 service stations in Melbourne, Sydney, Brisbane, Adelaide and Canberra by the end of October, with plans to increase to 100 metropolitan and regional locations in 2011.\n\nAs part of the North American auto market, by 2007 Canada had available 51 models of E85 flex-vehicles, most from Chrysler, Ford and General Motors, including automobiles, pickup trucks, and SUVs. The country had around 1.6 million capable flex fuel E85s on the roads by 2014. However, most users are not aware they own an E85, as vehicles are not clearly labeled as such, and even though the newer models have a yellow cap in the fuel tank informing that the vehicle can handle E85, most users are still not aware because there are very few gas stations offering E85. Another major drawback to greater E85 fuel use is the fact that by June 2008 Canada had only three public E85 pumps, all located in Ontario, in the cities of Guelph, Chatham, and Woodstock. E85 fueling is available primarily for fleet vehicles, including 20 government refueling stations not available for the public. The main feedstocks for E85 production in Canada are corn and wheat, and there were several proposals being discussed to increase the actual use of E85 fuel in FFVs, such as creating an ethanol-friendly highway or ethanol corridor.\n\nIn March 2009 the Colombian government enacted a mandate to introduce E85 flexible-fuel cars. The executive decree applies to all gasoline-powered vehicles with engines smaller than 2.0 liters manufactured, imported, and commercialized in the country beginning in 2012, mandating that 60% of such vehicles must have flex-fuel engines capable of running with gasoline or E85, or any blend of both. By 2014 the mandatory quota is 80% and it will reach 100 percent by 2016. All vehicles with engines bigger than 2.0 liters must be E85 capable starting in 2013. The decree also mandates that by 2011 all gasoline stations must provide infrastructure to guarantee availability of E85 throughout the country. The mandatory introduction of E85 flex-fuels has caused controversy among carmakers, car dealers, gasoline station owners, and even some ethanol producers complained the industry is not ready to supply enough ethanol for the new E85 fleet.\n\nIn 2006 New Zealand began a pilot project with two E85 Ford Focus Flexi-Fuel evaluation cars. The main feedstock used in New Zealand for ethanol production is whey, a by-product of milk production.\n\nGovernment officials and businessmen from Paraguay began negotiations in 2007 with Brazilian automakers in order to import flex cars that run on any blend of gasoline and ethanol. If successful, Paraguay would become the first destination for Brazilian flex-fuel car exports. In May 2008, the Paraguayan government announced a plan to eliminate import taxes of flex-fuel vehicles and an incentive program for ethanol production. The plan also includes the purchase of 20,000 flex cars in 2009 for the government fleet.\n\nIn 2006, tax incentives were established in Thailand for the introduction of compressed natural gas (CNG) as an alternative fuel, by eliminating import duties and lowering excise taxes on CNG-compatible cars. Then in 2007, Thai authorities approved incentives for the production of \"eco-cars\", with the goal of the country to become a regional hub for the production of small, affordable and fuel-efficient cars. Seven automakers joint in the program, Toyota, Suzuki, Nissan, Mitsubishi, Honda, Tata and Volkswagen. In 2008 the government announced priority for E85, expecting these flex-fuel vehicles to become widely available in Thailand in 2009, three years ahead of schedule. The incentives include cuts in excise tax rates for E85-compatible cars and reduction of corporate taxes for ethanol producers to make sure E85 fuel supply will be met. This new plan however, brought confusion and protests by the automakers which sign-up for the \"eco-cars\", as competition with the E85 flex-fuel cars will negatively affect their ongoing plans and investments, and their production lines will have to be upgraded at a high cost for them to produce flex-fuel cars. They also complained that flex-fuel vehicles popular in a few countries around the world, limiting their export potential as compared with other engine technologies.\n\nDespite the controversy, the first E85 flexible fuel vehicles were introduced in November 2008. The first two models available in the Thai market were the Volvo S80 and the C30. The S80 is manufactured locally and the C30 is imported. By the time of the introduction of flex vehicles there were already two gas stations with E85 fuel available. During 2009 it was expected that 15 fueling stations in Bangkok will have E85 fuel available. In October 2009 the Mitsubishi Lancer Ex was launched becoming the first mass-production E85 flexi-fuel vehicle produced in Thailand.\n\n\n\n"}
{"id": "90021", "url": "https://en.wikipedia.org/wiki?curid=90021", "title": "Flocking (behavior)", "text": "Flocking (behavior)\n\nFlocking behavior is the behavior exhibited when a group of birds, called a flock, are foraging or in flight. There are parallels with the shoaling behavior of fish, the swarming behavior of insects, and herd behavior of land animals.\n\nComputer simulations and mathematical models which have been developed to emulate the flocking behaviors of birds can generally be applied also to the \"flocking\" behavior of other species. As a result, the term \"flocking\" is sometimes applied, in computer science, to species other than birds.\n\nThis article is about the modelling of flocking behavior. From the perspective of the mathematical modeller, \"flocking\" is the collective motion of a large number of self-propelled entities and is a collective animal behavior exhibited by many living beings such as birds, fish, bacteria, and insects. It is considered an emergent behavior arising from simple rules that are followed by individuals and does not involve any central coordination.\n\nFlocking behavior was simulated on a computer in 1987 by Craig Reynolds with his simulation program, Boids. This program simulates simple agents (boids) that are allowed to move according to a set of basic rules. The result is akin to a flock of birds, a school of fish, or a swarm of insects.\n\nBasic models of flocking behavior are controlled by three simple rules:\n\n\nWith these three simple rules, the flock moves in an extremely realistic way, creating complex motion and interaction that would be extremely hard to create otherwise.\n\nThe basic model has been extended in several different ways since Reynolds proposed it. For instance, Delgado-Mata et al.\n\nextended the basic model to incorporate the effects of fear. Olfaction was used to transmit emotion between animals, through pheromones modelled as particles in a free expansion gas. Hartman and Benes\n\nintroduced a complementary force to the alignment that they call the change of leadership. This steer defines the chance of the bird to become a leader and try to escape.\nHemelrijk and Hildenbrandt\n\nused attraction, alignment and avoidance and extended this with a number of traits of real starlings: first, birds fly according to fixed wing aerodynamics, while rolling when turning (thus losing lift); second, they coordinate with a limited number of interaction neighbours of 7 (like real starlings); third, they try to stay above a sleeping site (like starlings do at dawn), and when they happen to move outwards from the sleeping site, they return to it by turning; fourth, they move at relative fixed speed. The authors showed that the specifics of flying behaviour as well as large flocksize and low number of interaction partners were essential to the creation of the variable shape of flocks of starlings.\n\nMeasurements of bird flocking have been made using high-speed cameras, and a computer analysis has been made to test the simple rules of flocking mentioned above. It is found that they generally hold true in the case of bird flocking, but the long range attraction rule (cohesion) applies to the nearest 5-10 neighbors of the flocking bird and is independent of the distance of these neighbors from the bird. In addition, there is an anisotropy with regard to this cohesive tendency, with more cohesion being exhibited towards neighbors to the sides of the bird, rather than in front or behind. This is no doubt due to the field of vision of the flying bird being directed to the sides rather than directly forward or backward.\n\nAnother recent study is based on an analysis of high speed camera footage of flocks above Rome, and uses a computer model assuming minimal behavioural rules.\n\nIn flocking simulations, there is no central control; each bird behaves autonomously. In other words, each bird has to decide for itself which flocks to consider as its environment. Usually environment is defined as a circle (2D) or sphere (3D) with a certain radius (representing reach).\n\nA basic implementation of a flocking algorithm has complexity formula_1 - each bird searches through all other birds to find those which fall into its environment.\n\nPossible improvements:\n\nLee Spector, Jon Klein, Chris Perry and Mark Feinstein studied the emergence of collective behavior in evolutionary computation systems.\n\nBernard Chazelle proved that under the assumption that each bird adjusts its velocity and position to the other birds within a fixed radius, the time it takes to converge to a steady state is an iterated exponential of height logarithmic in the number of birds. This means that if the number of birds is large enough, the convergence time will be so great that it might as well be infinite. This result applies only to convergence to a steady state. For example, arrows fired into the air at the edge of a flock will cause the whole flock to react more rapidly than can be explained by interactions with neighbors, which are slowed down by the time delay in the bird's central nervous systems—bird-to-bird-to-bird.\n\nIn Cologne, Germany, two biologists from the University of Leeds demonstrated a flock-like behavior in humans. The group of people exhibited a very similar behavioral pattern to that of a flock, where if 5% of the flock would change direction the others would follow suit. When one person was designated as a predator and everyone else was to avoid him, the flock behaved very much like a school of fish.\n\nFlocking has also been considered as a means of controlling the behavior of Unmanned Air Vehicles (UAVs).\n\nFlocking is a common technology in screensavers, and has found its use in animation. Flocking has been used in many films to generate crowds which move more realistically. \nTim Burton's \"Batman Returns\" (1992) featured flocking bats, and Disney's \"The Lion King\" (1994) included a wildebeest stampede.\n\nFlocking behaviour has been used for other interesting applications. It has been applied to automatically program Internet multi-channel radio stations.\nIt has also been used for visualizing information\nand for optimization tasks.\n\n\n\n"}
{"id": "9398035", "url": "https://en.wikipedia.org/wiki?curid=9398035", "title": "Fulvic acid", "text": "Fulvic acid\n\nFulvic acids are a family of organic acids, natural compounds, and components of the humus (which is a fraction of soil organic matter). They are similar to humic acids, with differences being the carbon and oxygen contents, acidity, degree of polymerization, molecular weight, and color. Fulvic acid remain in solution after removal of humic acid from humin by acidification.\n\nFulvic acids are of relatively low molecular mass and less biologically active than Humic Acids. \nFulvic acid, one of two classes of natural acidic organic polymers that can be extracted from humus found in soil, sediment, or aquatic environments. Its name derives from Latin \"fulvus\", indicating its yellow colour. This organic matter is soluble in strong acid (pH = 1) and has the average chemical formula CHONS. A hydrogen-to-carbon ratio greater than 1:1 indicates less aromatic character (i.e., fewer benzene rings in the structure), while an oxygen-to-carbon ratio greater than 0.5:1 indicates more acidic character than in other organic fractions of humus (for example, humic acid, the other natural acidic organic polymer that can be extracted from humus). Its structure is best characterized as a loose assembly of aromatic organic polymers with many carboxyl groups (COOH) that release hydrogen ions, resulting in species that have electric charges at various sites on the ion. It is especially reactive with metals, forming strong complexes with Fe, Al, and Cu in particular and leading to their increased solubility in natural waters. Fulvic acid is believed to originate as a product of microbial metabolism, although it is not synthesized as a life-sustaining carbon or energy.\n\nFulvic acid is created in extremely small quantities under the influence of millions of useful microbes, working on the decay of plant matter in a soil environment with sufficient oxygen.\n\nFulvic acids cannot be readily synthesized because of their extremely complex nature, although Lignosulfonates from the paper industry can appear similar to Fulvic Acids in certain tests as discussed below.\n\nAt the same time, the main problem is not extraction, but subsequent purification, in particular, the breaking of the molecular bond with Cl, Fe, which together with FA form toxic dihaloacetonitriles and have the property of accumulating in the body before reaching the critical point.\n\nUntil recently, there has been no standardized analytical method that the scientific community could rely on for consistent accuracy to determine the quantity of fulvic acid in an extract. Without an industry standard, manufacturers and sellers of fulvic products used methods that resulted in various claims being made on labels, marketing literature and websites of commercial fulvic acid products. These claims have caused many scientists and consumers to question the validity and accuracy of these claims about fulvic acid content, which made the evaluation of fulvic products very difficult. \n\nAnalytical quantification methods in the past measured both humic and fulvic acid as one substance. This created analytical challenges and mass confusion for those products that are fulvic isolates, having no measurable or very low humic acid in them. It is also the primary reason that fulvic acid content claims were usually inaccurate and much higher than is being brought to light with the new standardized method. \n\nThe Lamar standardized method was developed by a team of scientists and individuals from various organizations involved in soil science, the Lamar method was recently accepted as the standardized method for fulvic acid quantification by Association of American Plant Food Control Officials, the Humic Products Trade Association), and the International Humic Substances Society. \n\nUntil the emergence of the standardized Lamar method in 2015, this has been the most accurate of all testing methods for fulvic acid and may still hold promise since the results produced, with regards to fulvic acid content, are somewhat similar to the Lamar method.\n\nFulvic acid is condensed tannin and can be absorbed by a resin whereby it can be quantified much more accurately by reading the vanillin conjugates of the sample. The one disadvantage to this method, as compared to the Lamar method, is that it does not purify or separate the lignin sulfates from the fulvic acid fraction leading to some inaccuracies in the final FA result.\n\nHumic acid is exposed to light; the amount of light absorbed is compared to the quantification value of a Sigma-Aldrich standardized sample taken from a mine in Germany. Although quick and easy, this method lumps the fulvic acid in with the humic acid producing poor quantification of fulvic isolates. \n\nThis test was developed by the California State Department of Agriculture. This method does separate the humic and the fulvic but it then discards the fulvic solution and only measures the remaining liquid also including the organic ash content as part of the quantification result with no purification steps performed to remove the ash. This of course leads to various analytical inaccuracies. This is the only method that the California departments of agriculture will accept when registering a product. California does not recognize fulvic acid as separate substance from humic acid and requires that all label registrations list the content as humic acid only. Until 2017, Oregon also required using this method but has recently switched to the Lamar Method of fulvic acid quantification and now allows the label registration of fulvic acid as a substance apart from humic acid.\n\nThe Verploegh and Brandvold method (V&B) quantifies both humic and fulvic acid and is a quick, cost effective, and easy test to perform. It does not go through purification of the chemical reagents used to separate the humic and fulvic acids. This results in various inaccuracies that can produce inflated fulvic acid content readings since amino acids, lipids, carbohydrates and lignin sulfates are all lumped in with the quantification.\n"}
{"id": "45353208", "url": "https://en.wikipedia.org/wiki?curid=45353208", "title": "Gas cabinet", "text": "Gas cabinet\n\nA gas cabinet is a metallic enclosure which is used to provide local exhaust ventilation system for virtually all of the gases used or generated in the Semiconductor, Solar, MEMS, NANO, Solar PV, Manufacturing and other advanced technologies.\n\nThe primary purpose of gas cabinets is to contain potential leaks in piping and fittings at the cylinder connection. The cabinet must be exhausted by a specifically designed fan and exhaust system. The cabinet exhaust system draws leaking hazardous gasses out of the cabinet. In the case of a flammable gas the cabinet will contain the flame for a period of time. One can use a newly reconditioned cabinet as well as non-reconditioned used gas cabinets depending on their requirements.\n\nThere are a variety of gas cabinets available in the market in different gas cylinder configurations, such as 1, 2 and 3 bottle designs. They can be either new, used, or reconditioned. A gas cylinder cabinet can have many features depending on the specific gas. These features include gas sensor, sprinkler head, excess flow sensor, automatic operation with automatic purging and excess pressure sensor.\n\nThey should meet the Uniform Fire Code Specifications and the National Fire Protection Association. They need to adhere to the Compressed Gas Association and Semiconductor Equipment & Materials Institute codes. It is also important to know whether the cabinet is OSHA compliant. OSHA offers additional safety precautions involving the use of gas cabinets. In the absence of finite specifications or unknowns the user or designer should seek the assistance of an impartial consultant.\n\nReconditioned gas cabinets can be more useful than non-reconditioned used gas cabinets because of the thorough process used in professionally reconditioning and testing to ensure all systems meet manufacturer’s specifications. Different types of gas cabinets can be used, depending on the gas type in the cylinder. Automatic gas cabinets with multiple sensors are useful and fulfill many other requirements.\n\nA gas cabinet can also be manufactured specifically for a company's needs and at lower cost. It is required that a gas cabinet is used for fire safety for gas cylinders. The requirements vary by state. Many states have no advanced regulations for hazardous industrial gasses since there is little or no use of such gasses in that state.\nThe gas cabinets are categorized into four popular types based on the type of gas. The categories are:\n\nThese gas cabinets are used for inert, non-reactive and non-toxic gases. There are automatic gas cabinets also available and thus these cabinets are less useful when compared to automatic cabinets.\n\nThese cabinets are available for corrosive, toxic and reactive gases. They provide safe and clean delivery of ultra-high purity gases. Such systems are designed to monitor a huge number of facility inputs and process sensors as per requirements.\n\nThese gas cabinets are useful when uninterrupted gas flow is required. Auto changeover gas cabinets can be defined by mass or pressure inputs. There is software also available for multiple cylinder scales on each joint or bank. Purge down and purge up processes can also be performed without any interruption of connected cylinders.\n\nThe port and valve specifications for gas cabinets and distribution systems are an important part of the selection process.\n\nGas cabinets may be distinguished by the types of valves used to control flow.\nManual valves. Valves are manually adjusted or deployed via a control knob, lever, or other manual device.\nSolenoid valves. Valves are opened and closed via a solenoid magnet deployed by an electrical signal.\nAir pilot valves. Valves are deployed via a pneumatic signal.\n\nPorts are openings in the manifold or distribution system where the inlet and outlet connections are produced. Each opening is either an inlet (supply) port or an outlet port. The number of each corresponds to the requirements of the application.\nThe quantity of supply ports specifies the number of independent fluid supplies that could be interfaced with the manifold or manifold system.\n\nThe number of outlet ports establishes the number of outlets in the system. This is frequently specified as number of ports or valves that are or can be attached to the manifold. For example, an 8-point manifold has 8 ports or valves.\nPorts are sized based roughly on tube or pipe size with some important choices. For industrial gases it is possible to purchase manifolds made of Stainless steel pipe or copper and brass although both these materials are giving way to Stainless steel tubing. (Note differences between Tubing and Pipe). Connection choices might be NPT, VCO, Flare or other\ntypes.\n\nThe specifications for a gas cabinet detail everything from gas flow rates to the physical size of the system.\n\nPressure describes the amount of force exerted on a system by the contained and pressurized gas. Most compressed gases will not exceed 2,000 to 2,640 pounds per square inch gage (psig), but some can reach pressures of 6,000 psig. The system's weakest point determines the pressure limit, so any parts weakened by heat, corrosion, or stress may potentially lower the maximum pressure of the system or cause the vessel to rupture. Many times this is at the point of welds.\n\nFlow rate details the maximum rate of flow of the gas through the operation, typically measured in standard cubic feet per minute (scfm).\n\nTemperature range is the full mandated range of safe ambient or fluid operating temperatures, given in degrees Fahrenheit or degrees Celsius.\n\nSize dimensions specify the physical size of the gas cabinet, distribution system, or its components.\nCabinet size - Indicates the physical size of the gas cabinet or the body of the distribution system.\nPort/tube size - Indicates the physical size of the tubing or exhaust port connections in the system, typically given in inches based on a sizing standard such as National Pipe Thread (NPT). Sizing is important, as an undersized tube line will result in high pressure drops, while an oversized line will be unnecessarily expensive to install.\n\nThe materials used to construct the gas cabinet are an important part of proper system selection. The materials used for the casing and outer parts must have adequate structural strength, while the materials for the gas handling components must be compatible with the media, temperature requirements, and pressure ratings to prevent leakage, rupture, or contamination.\n\nA light and fairly corrosion resistant metal which is most often anodized for increased corrosion and wear resistance. Aluminum is never used for tubing or fittings in modern industrial gas control & distribution systems since 316L stainless steel (optimum choice) is so available. Aluminum is not an ideal choice for purity. Aluminum in any form is never used for ultra pure industrial gas systems.\n\nA soft, ductile metal with low hardness and excellent corrosion resistance. Copper is used commonly in tubes and pipes for its inertness and resistance to corrosion. Copper can be used for low air, oxygen and other inert non-critical gases such as medical CFOS systems. For ultra pure gasses 316L Stainless steel remains the optimum choice for many reasons.\n\nAny of numerous thermoplastic or thermosetting polymers of high molecular weight. Different grades (such as nylon, acetal, and polycarbonate) have varying properties, but most have strong chemical and corrosion resistance.\n\nGeneral purpose industrial metal with high physical strength and hardness. Steel is typically coated or finished to increase its corrosion resistance properties. Steel is used in the petroleum and petro-chemical industries.\n\n316L Stainless steel became industry standard for fittings, piping and controls in gas cabinets and distribution systems in the early 1980s. The material is further improved by electropolishing to render wetted surfaces extremely impervious to the most reactive gasses creating a non-shedding surface. A large industry has grown around supplying these ultra-specialized components and materials.\n\n"}
{"id": "40379651", "url": "https://en.wikipedia.org/wiki?curid=40379651", "title": "IBM", "text": "IBM\n\nInternational Business Machines Corporation (IBM) is an American multinational information technology company headquartered in Armonk, New York, United States, with operations in over 170 countries. The company began in 1911 as the Computing-Tabulating-Recording Company (CTR) and was renamed \"International Business Machines\" in 1924.\n\nIBM manufactures and markets computer hardware, middleware and software, and provides hosting and consulting services in areas ranging from mainframe computers to nanotechnology. IBM is also a major research organization, holding the record for most U.S. patents generated by a business () for 25 consecutive years. Inventions by IBM include the automated teller machine (ATM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the SQL programming language, the UPC barcode, and dynamic random-access memory (DRAM). The IBM mainframe, exemplified by the System/360, was the dominant computing platform during the 1960s and 1970s.\n\nIBM has continually shifted its business mix by commoditizing markets focusing on higher-value, more profitable markets. This includes spinning off printer manufacturer Lexmark in 1991 and selling off its personal computer (ThinkPad/ThinkCentre) and x86-based server businesses to Lenovo (2005 and 2014, respectively), and acquiring companies such as PwC Consulting (2002), SPSS (2009), The Weather Company (2016), and Red Hat(2018). Also in 2014, IBM announced that it would go \"fabless\", continuing to design semiconductors, but offloading manufacturing to GlobalFoundries.\n\nNicknamed Big Blue, IBM is one of 30 companies included in the Dow Jones Industrial Average and one of the world's largest employers, with () over 380,000 employees. Known as \"IBMers\", IBM employees have been awarded five Nobel Prizes, six Turing Awards, ten National Medals of Technology and five National Medals of Science.\n\nIn the 1880s, technologies emerged that would ultimately form the core of International Business Machines (IBM). Julius E. Pitrap patented the computing scale in 1885; Alexander Dey invented the dial recorder (1888); Herman Hollerith (1860-1929) patented the Electric Tabulating Machine; and Willard Bundy invented a time clock to record a worker's arrival and departure time on a paper tape in 1889. On June 16, 1911, their four companies were amalgamated in New York State by Charles Ranlett Flint forming a fifth company, the Computing-Tabulating-Recording Company (CTR) based in Endicott, New York. The five companies had 1,300 employees and offices and plants in Endicott and Binghamton, New York; Dayton, Ohio; Detroit, Michigan; Washington, D.C.; and Toronto.\n\nThey manufactured machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called on Flint and, in 1914, was offered a position at CTR. Watson joined CTR as General Manager then, 11 months later, was made \"President\" when court cases relating to his time at NCR were resolved. Having learned Patterson's pioneering business practices, Watson proceeded to put the stamp of NCR onto CTR's companies. He implemented sales conventions, \"generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker\". His favorite slogan, \"THINK\", became a mantra for each company's employees. During Watson's first four years, revenues reached $9 million and the company's operations expanded to Europe, South America, Asia and Australia. Watson never liked the clumsy hyphenated name \"Computing-Tabulating-Recording Company\" and on February 14, 1924 chose to replace it with the more expansive title \"International Business Machines\". By 1933 most of the subsidiaries had been merged into one company, IBM.\nIn 1937, IBM's tabulating equipment enabled organizations to process unprecedented amounts of data, its clients including the U.S. Government, during its first effort to maintain the employment records for 26 million people pursuant to the Social Security Act, and the tracking of persecuted groups by Hitler's Third Reich, largely through the German subsidiary Dehomag.\n\nIn 1949, Thomas Watson, Sr., created IBM World Trade Corporation, a subsidiary of IBM focused on foreign operations. In 1952, he stepped down after almost 40 years at the company helm, and his son Thomas Watson, Jr. was named president. In 1956, the company demonstrated the first practical example of artificial intelligence when Arthur L. Samuel of IBM's Poughkeepsie, New York, laboratory programmed an IBM 704 not merely to play checkers but \"learn\" from its own experience. In 1957, the FORTRAN scientific programming language was developed. In 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter. In 1963, IBM employees and computers helped NASA track the orbital flight of the Mercury astronauts. A year later, it moved its corporate headquarters from New York City to Armonk, New York. The latter half of the 1960s saw IBM continue its support of space exploration, participating in the 1965 Gemini flights, 1966 Saturn flights and 1969 lunar mission.\n\nOn April 7, 1964, IBM announced the first computer system family, the IBM System/360. It spanned the complete range of commercial and scientific applications from large to small, allowing companies for the first time to upgrade to models with greater computing capability without having to rewrite their applications. It was followed by the IBM System/370 in 1970. Together the 360 and 370 made the IBM mainframe the dominant mainframe computer and the dominant computing platform in the industry throughout this period and into the early 1980s. They, and the operating systems that ran on them such as OS/VS1 and MVS, and the middleware built on top of those such as the CICS transaction processing monitor, had a near-monopoly-level hold on the computer industry and became almost synonymous with IBM products due to their marketshare.\n\nIn 1974, IBM engineer George J. Laurer developed the Universal Product Code. IBM and the World Bank first introduced financial swaps to the public in 1981 when they entered into a swap agreement. The IBM PC, originally designated IBM 5150, was introduced in 1981, and it soon became an industry standard. In 1991, IBM sold printer manufacturer Lexmark.\n\nIn 1993, IBM posted a US$8 billion loss - at the time the biggest in American corporate history. Lou Gerstner was hired as CEO from RJR Nabisco to turn the company around. In 2002, IBM acquired PwC consulting, and in 2003 it initiated a project to redefine company values, hosting a three-day online discussion of key business issues with 50,000 employees. The result was three values: \"Dedication to every client's success\", \"Innovation that matters—for our company and for the world\", and \"Trust and personal responsibility in all relationships\".\n\nIn 2005, the company sold its personal computer business to Chinese technology company Lenovo and, in 2009, it acquired software company SPSS Inc. Later in 2009, IBM's Blue Gene supercomputing program was awarded the National Medal of Technology and Innovation by U.S. President Barack Obama. In 2011, IBM gained worldwide attention for its artificial intelligence program Watson, which was exhibited on \"Jeopardy!\" where it won against game-show champions Ken Jennings and Brad Rutter. The company also celebrated its 100th anniversary on the same year on June 16. In 2012, IBM announced it has agreed to buy Kenexa, and a year later it also acquired SoftLayer Technologies, a web hosting service, in a deal worth around $2 billion.\n\nIn 2014, IBM announced it would sell its x86 server division to Lenovo for $2.1 billion. Also that year, IBM began announcing several major partnerships with other companies, including Apple Inc., Twitter, Facebook, Tencent, Cisco, UnderArmour, Box, Microsoft, VMware, CSC, Macy's, Sesame Workshop, the parent company of Sesame Street, and Salesforce.com.\n\nIn 2015, IBM announced three major acquisitions: Merge Healthcare for $1 billion, data storage vendor Cleversafe, and all digital assets from The Weather Company, including Weather.com and the Weather Channel mobile app. Also that year, IBMers created the film \"A Boy and His Atom\", which was the first molecule movie to tell a story. In 2016, IBM acquired video conferencing service Ustream and formed a new cloud video unit. In April 2016, it posted a 14-year low in quarterly sales. The following month, Groupon sued IBM accusing it of patent infringement, two months after IBM accused Groupon of patent infringement in a separate lawsuit.\n\nIn October 2018, IBM announced its intention to acquire Red Hat for $34 billion.\n\nIBM is headquartered in Armonk, New York, a community north of Midtown Manhattan. Its principal building, referred to as CHQ, is a glass and stone edifice on a parcel amid a 432-acre former apple orchard the company purchased in the mid-1950s. There are two other IBM buildings within walking distance of CHQ: the North Castle office, which previously served as IBM's headquarters; and the Louis V. Gerstner, Jr., Center for Learning (formerly known as IBM Learning Center (ILC)), a resort hotel and training center, which has 182 guest rooms, 31 meeting rooms, and various amenities.\n\nIBM operates in 174 countries , with mobility centers in smaller markets areas and major campuses in the larger ones. In New York City, IBM has several offices besides CHQ, including the IBM Watson headquarters at Astor Place in Manhattan. Outside of New York, major campuses in the United States include Austin, Texas; Research Triangle Park (Raleigh-Durham), North Carolina; Rochester, Minnesota; and Silicon Valley, California.\n\nIBM's real estate holdings are varied and globally diverse. Towers occupied by IBM include 1250 René-Lévesque (Montreal, Canada), Tour Descartes (Paris, France), and One Atlantic Center (Atlanta, Georgia, USA). In Beijing, China, IBM occupies Pangu Plaza, the city's seventh tallest building and overlooking Beijing National Stadium (\"Bird's Nest\"), home to the 2008 Summer Olympics.\n\nIBM India Private Limited is the Indian subsidiary of IBM, which is headquartered at Bengaluru, Karnataka. It has facilities in Bengaluru, Ahmedabad, Delhi, Kolkata, Mumbai, Chennai, Pune, Gurugram, Noida, Bhubaneshwar, Coimbatore, Visakhapatnam and Hyderabad.\n\nOther notable buildings include the IBM Rome Software Lab (Rome, Italy), the Hursley House (Winchester, UK), 330 North Wabash (Chicago, Illinois, United States), the Cambridge Scientific Center (Cambridge, Massachusetts, United States), the IBM Toronto Software Lab (Toronto, Canada), the IBM Building, Johannesburg (Johannesburg, South Africa), the IBM Building (Seattle) (Seattle, Washington, United States), the IBM Hakozaki Facility (Tokyo, Japan), the IBM Yamato Facility (Yamato, Japan), the IBM Canada Head Office Building (Ontario, Canada) and the Watson IoT Headquarters (Munich, Germany). Defunct IBM campuses include the IBM Somers Office Complex (Somers, New York). The company's contributions to industrial architecture and design include works by Eero Saarinen, Ludwig Mies van der Rohe and I.M. Pei. Van der Rohe's building in Chicago, the original center of the company's research division post-World War II, was recognized with the 1990 Honor Award from the National Building Museum.\n\nIBM was recognized as one of the Top 20 Best Workplaces for Commuters by the United States Environmental Protection Agency (EPA) in 2005, which recognized Fortune 500 companies that provided employees with excellent commuter benefits to help reduce traffic and air pollution. In 2004, concerns were raised related to IBM's contribution in its early days to pollution in its original location in Endicott, New York.\n\nFor the fiscal year 2017, IBM reported earnings of US$5.7 billion, with an annual revenue of US$79.1 billion, a decline of 1.0% over the previous fiscal cycle. IBM's shares traded at over $125 per share, and its market capitalization was valued at over US$113.9 billion in September 2018. IBM ranked No. 34 on the 2018 Fortune 500 rankings of the largest United States corporations by total revenue.\n\nIBM has a large and diverse portfolio of products and services. , these offerings fall into the categories of cloud computing, cognitive computing, commerce, data and analytics, Internet of Things (IoT), IT infrastructure, mobile, and security.\n\nIBM Cloud includes infrastructure as a service (IaaS), software as a service (SaaS) and platform as a service (PaaS) offered through public, private and hybrid cloud delivery models. For instance, the IBM Bluemix PaaS enables developers to quickly create complex websites on a pay-as-you-go model. IBM SoftLayer is a dedicated server, managed hosting and cloud computing provider, which in 2011 reported hosting more than 81,000 servers for more than 26,000 customers. IBM also provides Cloud Data Encryption Services (ICDES), using cryptographic splitting to secure customer data.\n\nIBM also hosts the industry-wide cloud computing and mobile technologies conference InterConnect each year.\n\nHardware designed by IBM for these categories include IBM's POWER microprocessors, which are employed inside many console gaming systems, including Xbox 360, PlayStation 3, and Nintendo's Wii U. IBM Secure Blue is encryption hardware that can be built into microprocessors, and in 2014, the company revealed TrueNorth, a neuromorphic CMOS integrated circuit and announced a $3 billion investment over the following five years to design a neural chip that mimics the human brain, with 10 billion neurons and 100 trillion synapses, but that uses just 1 kilowatt of power. In 2016, the company launched all-flash arrays designed for small and midsized companies, which includes software for data compression, provisioning, and snapshots across various systems.\n\nIT outsourcing also represents a major service provided by IBM, with more than 40 data centers worldwide. alphaWorks is IBM's source for emerging software technologies, and SPSS is a software package used for statistical analysis. IBM's Kenexa suite provides employment and retention solutions, and includes the BrassRing, an applicant tracking system used by thousands of companies for recruiting. IBM also owns The Weather Company, which provides weather forecasting and includes weather.com and Weather Underground.\n\nSmarter Planet is an initiative that seeks to achieve economic growth, near-term efficiency, sustainable development, and societal progress, targeting opportunities such as smart grids, water management systems, solutions to traffic congestion, and greener buildings.\n\nServices provisions include Redbooks, which are publicly available online books about best practices with IBM products, and developerWorks, a website for software developers and IT professionals with how-to articles and tutorials, as well as software downloads, code samples, discussion forums, podcasts, blogs, wikis, and other resources for developers and technical professionals.\n\nIBM Watson is a technology platform that uses natural language processing and machine learning to reveal insights from large amounts of unstructured data. Watson was debuted in 2011 on the American game-show \"Jeopardy!\", where it competed against champions Ken Jennings and Brad Rutter in a three-game tournament and won. Watson has since been applied to business, healthcare, developers, and universities. For example, IBM has partnered with Memorial Sloan Kettering Cancer Center to assist with considering treatment options for oncology patients and for doing melanoma screenings. Also, several companies have begun using Watson for call centers, either replacing or assisting customer service agents. \n\nResearch has been a part of IBM since its founding, and its organized efforts trace their roots back to 1945, when the Watson Scientific Computing Laboratory was founded at Columbia University in New York City, converting a renovated fraternity house on Manhattan's West Side into IBM's first laboratory. Now, IBM Research constitutes the largest industrial research organization in the world, with 12 labs on 6 continents. IBM Research is headquartered at the Thomas J. Watson Research Center in New York, and facilities include the Almaden lab in California, Austin lab in Texas, Australia lab in Melbourne, Brazil lab in São Paulo and Rio de Janeiro, China lab in Beijing and Shanghai, Ireland lab in Dublin, Haifa lab in Israel, India lab in Delhi and Bangalore, Tokyo lab, Zurich lab and Africa lab in Nairobi.\n\nIn terms of investment, IBM's R&D spend totals several billion dollars each year. In 2012, that expenditure was approximately US$6.9 billion. Recent allocations have included $1 billion to create a business unit for Watson in 2014, and $3 billion to create a next-gen semiconductor along with $4 billion towards growing the company's \"strategic imperatives\" (cloud, analytics, mobile, security, social) in 2015.\n\nIBM has been a leading proponent of the Open Source Initiative, and began supporting Linux in 1998. The company invests billions of dollars in services and software based on Linux through the IBM Linux Technology Center, which includes over 300 Linux kernel developers. IBM has also released code under different open source licenses, such as the platform-independent software framework Eclipse (worth approximately US$40 million at the time of the donation), the three-sentence International Components for Unicode (ICU) license, and the Java-based relational database management system (RDBMS) Apache Derby. IBM's open source involvement has not been trouble-free, however (see \"SCO v. IBM\").\n\nFamous inventions and developments by IBM include: the Automated teller machine (ATM), Dynamic random access memory (DRAM), the electronic keypunch, the financial swap, the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, RISC, the SABRE airline reservation system, SQL, the Universal Product Code (UPC) bar code, and the virtual machine. Additionally, in 1990 company scientists used a scanning tunneling microscope to arrange 35 individual xenon atoms to spell out the company acronym, marking the first structure assembled one atom at a time. A major part of IBM research is the generation of patents. Since its first patent for a traffic signaling device, IBM has been one of the world's most prolific patent sources. In 2018, the company holds the record for most patents generated by a business, marking 25 consecutive years for the achievement.\n\nFive IBMers have received the Nobel Prize: Leo Esaki, of the Thomas J. Watson Research Center in Yorktown Heights, N.Y., in 1973, for work in semiconductors; Gerd Binnig and Heinrich Rohrer, of the Zurich Research Center, in 1986, for the scanning tunneling microscope; and Georg Bednorz and Alex Müller, also of Zurich, in 1987, for research in superconductivity. Several IBMers have also won the Turing Award, including the first female recipient Frances E. Allen.\n\nCurrent research includes a collaboration with the University of Michigan to see computers act as an academic adviser for undergraduate computer science and engineering students at the university, and a partnership with AT&T, combining their cloud and Internet of Things (IoT) platforms to make them interoperable and to provide developers with easier tools.\n\nThe company is also involved in research into advanced algorithms and machine learning and their decision-making processes. To that end, the company recently released an analysis tool for how and why algorithms make decisions while scanning for biases in automated decision-making.\n\nIBM is nicknamed \"Big Blue\" in part due to its blue logo and color scheme, and also partially since IBM once had a de facto dress code of white shirts with blue suits. The company logo has undergone several changes over the years, with its current \"8-bar\" logo designed in 1972 by graphic designer Paul Rand. It was a general replacement for a 13-bar logo, since period photocopiers did not render large areas well. Aside from the logo, IBM used Helvetica as a corporate typeface for 50 years, until it was replaced in 2017 by the custom-designed IBM Plex.\n\nIBM has a valuable brand as a result of over 100 years of operations and marketing campaigns. Since 1996, IBM has been the exclusive technology partner for the Masters Tournament, one of the four major championships in professional golf, with IBM creating the first Masters.org (1996), the first course cam (1998), the first iPhone app with live streaming (2009), and first-ever live 4K Ultra High Definition feed in the United States for a major sporting event (2016). As a result, IBM CEO Ginni Rometty became the third female member of the Master's governing body, the Augusta National Golf Club. IBM is also a major sponsor in professional tennis, with engagements at the U.S. Open, Wimbledon, the Australian Open, and the French Open. The company also sponsored the Olympic Games from 1960–2000, and the National Football League from 2003–2012.\n\nIn 2012, IBM's brand was valued at $75.5 billion and ranked by \"Interbrand\" as the second-best brand worldwide. That same year, it was also ranked the top company for leaders (\"Fortune\"), the number two green company in the U.S. (\"Newsweek\"), the second-most respected company (\"Barron's\"), the fifth-most admired company (\"Fortune\"), the 18th-most innovative company (\"Fast Company\"), and the number one in technology consulting and number two in outsourcing (Vault). In 2015, Forbes ranked IBM the fifth-most valuable brand.\n\nIBM has one of the largest workforces in the world, and employees at Big Blue are referred to as \"IBMers\". The company was among the first corporations to provide group life insurance (1934), survivor benefits (1935), training for women (1935), paid vacations (1937), and training for disabled people (1942). IBM hired its first black salesperson in 1946, and in 1952, CEO Thomas J. Watson, Jr. published the company's first written equal opportunity policy letter, one year before the U.S. Supreme Court decision in Brown vs. Board of Education and 11 years before the Civil Rights Act of 1964. The Human Rights Campaign has rated IBM 100% on its index of gay-friendliness every year since 2003, with IBM providing same-sex partners of its employees with health benefits and an anti-discrimination clause. Additionally, in 2005, IBM became the first major company in the world to commit formally to not use genetic information in employment decisions; and in 2017, IBM was named to \"Working Mother\"s 100 Best Companies List for the 32nd consecutive year.\n\nIBM has several leadership development and recognition programs to recognize employee potential and achievements. For early-career high potential employees, IBM sponsors leadership development programs by discipline (e.g., general management (GMLDP), human resources (HRLDP), finance (FLDP)). Each year, the company also selects 500 IBMers for the IBM Corporate Service Corps (CSC), which has been described as the corporate equivalent of the Peace Corps and gives top employees a month to do humanitarian work abroad. For certain interns, IBM also has a program called Extreme Blue that partners top business and technical students to develop high-value technology and compete to present their business case to the company's CEO at internship's end.\n\nThe company also has various designations for exceptional individual contributors such as Senior Technical Staff Member (STSM), Research Staff Member (RSM), Distinguished Engineer (DE), and Distinguished Designer (DD). Prolific inventors can also achieve patent plateaus and earn the designation of Master Inventor. The company's most prestigious designation is that of IBM Fellow. Since 1963, the company names a handful of Fellows each year based on technical achievement. Other programs recognize years of service such as the Quarter Century Club established in 1924, and sellers are eligible to join the Hundred Percent Club, composed of IBM salesmen who meet their quotas, convened in Atlantic City, New Jersey. Each year, the company also selects 1,000 IBMers annually to award the Best of IBM Award, which includes an all-expenses paid trip to the awards ceremony in an exotic location.\n\nIBM's culture has evolved significantly over its century of operations. In its early days, a dark (or gray) suit, white shirt, and a \"sincere\" tie constituted the public uniform for IBM employees. During IBM's management transformation in the 1990s, CEO Louis V. Gerstner, Jr. relaxed these codes, normalizing the dress and behavior of IBM employees. The company's culture has also given to different plays on the company acronym (IBM), with some saying is stands for \"I've Been Moved\" due to relocations and layoffs, others saying it stands for \"I'm By Myself\" pursuant to a prevalent work-from-anywhere norm, and others saying it stands for \"I'm Being Mentored\" due to the company's open door policy and encouragement for mentoring at all levels. In terms of labor relations, the company has traditionally resisted labor union organizing, although unions represent some IBM workers outside the United States. In Japan, IBM employees also have an American football team complete with pro stadium, cheerleaders and televised games, competing in the Japanese X-League as the \"Big Blue\".\n\nIn 2015, IBM started giving employees the option of choosing either a PC or a Mac as their primary work device, resulting in IBM becoming the world's largest Mac shop. In 2016, IBM eliminated forced rankings and changed its annual performance review system to focus more on frequent feedback, coaching, and skills development.\n\nMany IBMers have also achieved notability outside of work and after leaving IBM. In business, former IBM employees include Apple Inc. CEO Tim Cook, former EDS CEO and politician Ross Perot, Microsoft chairman John W. Thompson, SAP co-founder Hasso Plattner, Advanced Micro Devices (AMD) CEO Lisa Su, former Citizens Financial Group CEO Ellen Alemany, former Yahoo! chairman Alfred Amoroso, former AT&T CEO C. Michael Armstrong, former Xerox Corporation CEOs David T. Kearns and G. Richard Thoman, former Fair Isaac Corporation CEO Mark N. Greene, Citrix Systems co-founder Ed Iacobucci, ASOS.com chairman Brian McBride, former Lenovo CEO Steve Ward, and former Teradata CEO Kenneth Simonds.\n\nIn government, alumna Patricia Roberts Harris served as United States Secretary of Housing and Urban Development, the first African American woman to serve in the United States Cabinet. Samuel K. Skinner served as U.S. Secretary of Transportation and as the White House Chief of Staff. Alumni also include U.S. Senators Mack Mattingly and Thom Tillis; Wisconsin governor Scott Walker; former U.S. Ambassadors Vincent Obsitnik (Slovakia), Arthur K. Watson (France), and Thomas Watson Jr. (Soviet Union); and former U.S. Representatives Todd Akin, Glenn Andrews, Robert Garcia, Katherine Harris, Amo Houghton, Jim Ross Lightfoot, Thomas J. Manton, Donald W. Riegle Jr., and Ed Zschau.\n\nOthers are NASA astronaut Michael J. Massimino, Canadian astronaut Julie Payette, Harvey Mudd College president Maria Klawe, Western Governors University president emeritus Robert Mendenhall, former University of Kentucky president Lee T. Todd Jr., NFL referee Bill Carollo, former Rangers F.C. chairman John McClelland, and recipient of the Nobel Prize in Literature J. M. Coetzee. Thomas Watson Jr. also served as the 11th national president of the Boy Scouts of America and Tucker Technologies Founder and CEO Mycah E. Tucker\n\nThe company's 14 member Board of Directors are responsible for overall corporate management and includes the CEOs of American Express, Ford Motor Company, Boeing, Dow Chemical, Johnson and Johnson, and Cemex.\n\nIn 2011, IBM became the first technology company Warren Buffett's holding company Berkshire Hathaway invested in. Initially he bought 64 million shares costing 10.5 billion dollars. Over the years he increased his IBM holdings however he reduced it by 94.5% to 2.05 million shares at the end of 2017. By May 2018 he was completely out of IBM.\n\n\n\n"}
{"id": "31386309", "url": "https://en.wikipedia.org/wiki?curid=31386309", "title": "IBM Cognos Business Intelligence", "text": "IBM Cognos Business Intelligence\n\nIBM Cognos Business Intelligence is a web-based integrated business intelligence suite by IBM. It provides a toolset for reporting, analytics, scorecarding, and monitoring of events and metrics. The software consists of several components designed to meet the different information requirements in a company. IBM Cognos has components such as IBM Cognos Framework Manager, IBM Cognos Cube Designer, IBM Cognos Transformer.\n\nThe elements described below are web-based components that can be accessed from most popular browsers (IBM Cognos specifically supports Mozilla Firefox, Google Chrome and Internet Explorer).\n\nCognos Connection is the Web portal for IBM Cognos BI. It is the starting point for access to all functions provided with the suite. Using this portal, content can be searched in the form of reports, scorecards and agents, it can be managed, structured and displayed. In addition, the portal is used for multiple functions, for example to schedule and distribute reports, for creating tasks, administering the server and the access permissions to content available to different users. You can also create shortcuts, URLs and pages.\n\nQuery Studio allows simple queries and self-service reports to answer basic business questions. The report layout can be customized and data can be filtered and sorted. Formatting and creation of diagrams is also supported.\n\nThe Report Studio is used to create management reports. It offers two different modes: The professional authoring mode enables users to access the full range of Report Studio functionality. In this mode, users can create any type of report, including charts, maps, lists, and repeat functions. In professional authoring mode all types of Data (relational or multidimensional) can be used, but dynamic data can not be displayed.\n\nThe express authoring mode has a more simple user interface, designed for non-technical users. It enables them to create traditional financial or management reports in a more focused user interface. In contrast to the professional authoring mode, the express authoring mode allows the use of dynamic data.\n\nUsers can create analyses of large data sources and search for background information about an event or action. Multidimensional analysis allows identifying trends and understanding of anomalies or deviations, which are not obvious in other types of reports. Drag-and-drop features, elements and key performance indicators can be included in the analysis, rows and columns can be switched, OLAP-functionalities like drill-up and drill-down can be used to get a deeper understanding about the sources of the information used in the analysis.\n\nThe Event Studio is a notification tool that informs about events within the enterprise in real time. Therefore, agents can be created to detect the occurrence of business events or exceptional circumstances, based on the change of specified event- or data conditions. A notification may be served by sending an e-mail, its publication in the portal, or by triggering reports. This can be used to handle failure with notification.It is very robust in nature.\n\nIBM Cognos Workspace (formerly introduced in version 10.1 as IBM Cognos Business Insight and renamed in version 10.2.0) is a web-based interface that allows business users to use existing IBM Cognos content (report objects) to build interactive workspaces for insight and collaboration.\n\nIBM Cognos Workspace Advanced (formerly introduced in version 10.1 as IBM Cognos Business Insight Advanced and renamed in version 10.2.0) is a web-based interface that allows business users to author/create reports and analyze information.\n\n\nThe Go! Office component lets users work with IBM Cognos content in their familiar Microsoft Office environment. The component provides access to all IBM Cognos Report contents, including data, metadata, headers and footers, and diagrams. Users can use predefined reports or create new content with Query Studio, Analysis Studio or Report Studio. By importing content into Microsoft Excel users can use the formatting, calculation and presentation features. The created documents can then be imported using Cognos Connection, published and made available for other users.\n\nIn Cognos Connection, you can do a full-text search for content contained in reports, analyses, dashboards, metric information and events. When searching, an index of the prompts, titles, headings, column names, row names, data elements and other important fields is used as base. The full text search in IBM Cognos Go! Search is related to the search in regular search engines such as Google. Users can search operators such as +, - or use \"\" (quotation marks) to change the default behavior of search queries with multiple words. Search terms are not case sensitive, word and spelling variants are included in the results. You can also search for a specific type of entry, such as an agent. The search results are sorted in descending order, the entry with the greatest amount of relevant metadata is displayed at the top of the list. In Analysis Studio, Query Studio and IBM Cognos Viewer, you can either perform a full text search, as also search for content related to the data of the current view.\n\nWith IBM Cognos Go! Dashboard, interactive dashboards containing IBM Cognos content and external data sources can be created to fit the information needs of an individual user.\n\nThe following items can be added to a dashboard:\nReport objects, they are displayed in a Cognos Viewer portlet. Report parts such as lists, crosstabs, and charts are displayed in interactive portlets. Lists or crosstabs can be displayed as a chart and vice versa. Content can be shown or hidden dynamically by the use of sliders and checkboxes. The Cognos Search portlet allows searching for published content. In addition, Web links, Web pages, RSS feeds, and images can be displayed on the dashboard.\nThe user interface has two modes: In the interactive mode, existing dashboards are viewed and interacted with, creating and editing of dashboards can be done in assembly mode.\n\n"}
{"id": "11921706", "url": "https://en.wikipedia.org/wiki?curid=11921706", "title": "Insecticidal soap", "text": "Insecticidal soap\n\nInsecticidal soap is based on potassium fatty acids and is used to control many plant pests. Because insecticidal soap works on only direct contact with the pests, it is sprayed on plants in way such that the entire plant is wetted. Soaps have a low mammalian toxicity and are therefore considered safe to be used around children and pets and may be used in organic farming.\n\nInsecticidal soap should be based on long-chain fatty acids (10–18 carbon atoms), because shorter-chain fatty acids tend to be damaging for the plant (phytotoxicity). Short (8-carbon) fatty-acid chains occur for example in coconut oil and palm oil and soaps based on those oils. Recommended concentrations are typically in the range 1–2 percent. One manufacturer recommends a concentration of 0.06% to 0.25% (pure soap equivalent) for most agricultural applications.; another one recommends concentrations of 0.5 to 1% pure soap equivalent. In the European Union, fatty acid potassium salts are registered and allowed as insecticide at a 2% concentration.\n\nInsectidal soap is most effective if it is dissolved in soft water, since the fatty acids in soap tend to precipitate in hard water, thereby reducing the effectivity.\n\nInsecticidal soap is sold commercially for aphid control. Labels on these products may not always use the word soap, but they will list \"potassium salts of fatty acids\" or \"potassium laurate\" as the active ingredient. Certain types of household soaps (not synthetic detergents) are also suitable, but it may be difficult to tell the composition and water content from the label. Potassium-based soaps are typically soft or liquid.\n\nThe mechanism of action is not exactly understood. Possible mechanisms are:\n\nInsecticidal soap works best on soft-bodied insects and arthropods such as aphids, adelgids,\nmealybugs, spider mites, thrips, jumping plant lice, scale insects, whiteflies, and sawfly larvae. It can also be used for caterpillars and leafhoppers, but these large-bodied insects can be more difficult to control with soaps alone. Many pollinators and predatory insects such as lady beetles, bumblebees, and hoverflies are relatively unaffected. However, soap will kill predatory mites that may help control spider mites. Also, the soft-bodied aphid-eating larvae of lady beetles, lacewing, and hoverflies may be affected negatively. According to one study a single soap application killed about 15% of lacewing and lady-beetle larvae, and about 65% of predatory mites (\"Amblyseius andersoni\").\n\nGreen peach aphids are difficult to control since they reproduce quickly (one adult female can deposit up to four nymphs per day) because they tend to reside under the leaves and in leaf axils (\"leaf armpits\"), where they may not be wetted by a soap spray. Manufacturers indeed state that their insecticidal soaps are only suitable for controlling green peach aphids if used in combination with another insecticide, whereas the same soaps can control other aphids on their own. Among green peach aphids that are in contact with a 2% soap solution, around 95% of the adults and 98% of nymphs die within 48 hours. At 0.75% concentration, the mortality rates are reduced to 75% and 90%, respectively. \n\nSince 2011, insecticidal soap has also been approved in the United States for use against powdery mildew. In the European pesticide registration, its use as an insecticide is listed for aphids, white fly, and spider mites. At different concentrations, it may also be used against algae and moss.\n\nInsecticidal soap solution will only kill pests on contact; it has no residual action against aphids that arrive after it has dried. Therefore, the infested plants must be thoroughly wetted. Repeated applications may be necessary to adequately control high populations of pests.\n\nSoap spray may damage plants, especially at higher concentrations or at temperatures above 32 °C (90 °F). Plant injury may not be apparent until two days after application. Some plant species are particularly sensitive to soap sprays. Highly sensitive plants include: horse chestnut, Japanese maple (\"Acer\"), \"Sorbus aucuparia\" (mountain ash), Cherimoya fruit, \"Lamprocapnos\" (bleeding heart), and sweet pea. Other sensitive plants are, for example: \"Portulaca\", some tomato varieties, \"Crataegus\" (hawthorn), cherries, plum, \"Adiantum\" (maidenhair fern), \"Euphorbia milii\" (crown of thorns), \"Lantana camara\", \"Tropaeolum\" (nasturtium), \"Gardenia jasminoides\", \"Lilium longiflorum\" (Easter lily). Conifers under (drought) stress or with tender new growth are sensitive as well.\n\nDamage may occur as yellow or brown spotting on the leaves, burned tips, or leaf scorch. Plants under drought stress, young transplants, unrooted cuttings and plants with soft young growth tend to be more sensitive. Sensitivity may be tested on a small portion of a plant or plot before a full-scale application.\n\nOne manufacturer recommends that applications are done with 7- to 14-day intervals, with a maximum of three applications, as repeated applications may aggravate phytotoxicity. In addition, water conditioning agents can increase phytotoxicity.\n\nThanks to its low mammalian toxicity, application of insecticidal soap is typically allowed up to the day of harvest.\n\n"}
{"id": "4033188", "url": "https://en.wikipedia.org/wiki?curid=4033188", "title": "Lagotek", "text": "Lagotek\n\nLagotek was a privately held company specializing in home automation. It was founded in 2005 by four former Microsoft employees and it is located in Bellevue, WA. Lagotek developed Home Intelligence Platform (HIP) technology. The 4 founders of Lagotek are Eugene Luskin, Alex Grach, David Kizhnerman and Lev Tcherkachine,\n\nLagotek officially ceased operations in 2011.\n\nSystems controlled by Lagotek\n\nCurrent software applications for the HIP platform from Lagotek and its partners include:\nApplications that provide integration and automation are:\n\n\nThe HIP100 touch screen panels from which the users control the functionality of their homes are elegant, but unobtrusive, easily blending with the walls when not used. Due to the wireless nature of the product there is no need to run new wires since the panels fit into the standard 2-gang box replacing the \"so 20th century \"dumb\" dimmers and switches.\n\nLagotek HIP is the distributed system (no single point of failure) that runs on HIP100 touch screen panels, PDAs, PC, UMPC, and Windows Vista MCE. Multiple instances of Lagotek HIP running on various devices throughout the house are fully synchronized and provide unique level of reliability compare to any other Home Automation system where there is only one central controlling device.\nThrough the support of SideShow technology Lagotek Modes can be controlled from any SideShow device, so no matter where you are and what device you have in your hands you will be able to monitor and control your home.\nFrom the cell phone:\nLagotek cell phone application (powered by Crayon Interface’s Moshi server) provides both information about the current state of the home and control of the home through Modes. It is possible to see live real time video from the cameras installed in the house, know how many lights are on and where and what is the temperature in the house. Every system in the house can send Alerts to the cell phone.\nFrom the Web browser:\nThe same functionality that is accessible on the phone is available through the Web interface due to the integration between Lagotek HIP and Microsoft Windows Home Server.\n\nThe target markets for Lagotek are new home construction and aftermarket remodeling. A network of Certified Installer/Dealers, who install the systems and provide aftermarket service both on an as-needed and post-installation service agreement basis, serves these markets. As a member of CEDIA (Custom Electronic Design & Installation Association), Lagotek is currently adding to its network of Certified Installer/Dealers, and is interviewing and training Certified Installers from the over 6,000 CEDIA installer/dealers\n\nLagotek touts its technology as an open platform, on top of which other vendors can provide extensions and customizations.\n\n"}
{"id": "56235559", "url": "https://en.wikipedia.org/wiki?curid=56235559", "title": "Link Motion", "text": "Link Motion\n\nLink Motion is an automotive software and hardware company developing embedded automotive systems that have been used in the Lamborghini Huracán. Their main product is the Motion T carputer which can implement a connected vehicle gateway as a separate unit or as a part of the cockpit solution (eCockpit). The Motion T carputer runs on NXP's i.MX8 multi-OS platform, supports four in-car HD displays and hosts connectivity features on Microsoft’s connected vehicle platform, a set of services built on the Microsoft Azure cloud, such as over-the-air software and firmware updates, telemetry and diagnostics data and secure remote access.\n\nLink Motion is a next generation on-board unit hardware and open source powered secure software platform that enables integration of features for connected and smart cars. Link Motion uses Linux and Qt to implement several previously discrete ECU with one computer and relies on ARM TrustZone and virtualization to protect critical system functions from vulnerabilities. An AUTOSAR based operating system in the heart of the carputer is responsible for controlling safety-critical functions and vehicle communication on the controller-area network (CAN bus). It supports central unit, instrument cluster and head-up displays, rear-view cameras, CANopen protocol and it offers connectivity through cellular, vehicle-to-everything (V2X), WiFi and Bluetooth connections as well as location services using GPS, GLONASS and BeiDou. Data is shared and stored in the cloud, uses internal blockchain technology and a platform that supports both custom connected car services and others such as Apple CarPlay, Baidu Carlife or Android Auto. Vehicle data can be captured with on-board diagnostics (OBD) which is available using standard OBD-II PIDs.\n\nLink Motion will further enhance its carputer into a “CarBrain”, using artificial intelligence (AI) and blockchain applications to make vehicles a more intelligent part of the traffic ecosystem. They have also integrated Keystone into the platform which is Irdeto’s new smartphone-based vehicle access solution that allows vehicle owners to create and control policies around multi-user vehicle access, settings and usage including ECU-side functionality and cloud services, as well as a complete back-end management system with analytics. \nFounded in 2001 to develop embedded Linux systems the company was originally called Nomovok. That led to developing mobile operating systems for Nokia and others.\n\nIn 2006 they started working on automotive software and embedded automotive systems. Then in 2014 the company was re-branded to Link Motion.\n\nIn October 2014, Tieto, Link Motion and Nomovok announced their collaboration on Link Motion.\n\nIn January 2015, Link Motion and MTA SpA announced their collaboration on Link Motion.\n\nIn March 2015, Link Motion and Red Bend announced strategic cooperation for an over-the-air (OTA) software update solution for Link Motion.\n\nIn the summer of 2015 NQ Mobile took a controlling stake in Link Motion.\n\nIn August 2016, Link Motion announced it will use Qt technology for the development of the user interface.\n\nIn September 2016, Link Motion became a member of the Linux Foundation and Automotive Grade Linux\n(AGL).\n\nIn November 2016, Finland's first autonomous passenger car is pilot licensed. Named Marilyn the car is being developed in the VTT Technical Research Centre of Finland.\n\nIn January 2017, Link Motion implemented Cloakware for Automotive by Irdeto into Link Motion’s connected carputer, Motion T.\n\nIn May 2017, Finland's first autonomous passenger car is allowed to drive in city traffic.\n\nIn November 2017, Link Motion joined the IETA3 APPSTACLE project which aims to include an open and secure car-to-cloud and cloud-to-car platform that interconnects a wide range of cars and transport vehicles to the cloud via open in-car and Internet connection, taking advantage of 5G opportunities.\n\nIn December 2017, a partnership with Chery was announced to develop a connected car platform.\n\nIn January 2018, Link Motion was selected as the platform vendor for pure electric buses in the city of Qingdao for public transportation vehicles by extending their existing passenger car platform.\n\nIn March 2018, Link Motion announced integration of Keystone into the Link Motion CarBrain platform. \n\n"}
{"id": "47270052", "url": "https://en.wikipedia.org/wiki?curid=47270052", "title": "Lithium–silicon battery", "text": "Lithium–silicon battery\n\nLithium–silicon batteries are a lithium-ion battery technology that employ a silicon anode and lithium ions as the charge carriers. Silicon has a much larger energy density (25 times as many lithium ions) than graphite. Silicon's large volume change when lithium is inserted is the main obstacle to commercializing this device. Commercial battery anodes may have small amounts of silicon, boosting their performance slightly. The amounts are closely held trade secrets, limited as of 2018 to at most 10% of the anode.\n\nThe first laboratory experiments with lithium-silicon batteries took place in the late 1990s.\n\nTest sample production of batches of batteries using a silicon-graphite composite electrode were started by Amprius in 2014. The same company claims to have sold several hundred thousand of these batteries as of 2014. In 2016, Stanford University researchers presented a method of encapsulating silicon microparticles in a graphene shell, which confines fractured particles and also acts as a stable solid electrolyte interface layer. These microparticles reached an energy density of 3,300 mAh/g.\n\nAlso in 2014, a company called Enevate presented a battery using an unknown monolithic silicon-composite anode with a low cell resistance. These batteries leave 25% of the capacity unused, most likely to reduce degradation. For this technology it was named an Innovation Award Honoree in three categories at 2016's Consumer Electronics Show (CES). Shortly thereafter CES 2016, Sonim Technologies (a company selling rugged mobile phones) would use Enevate's lithium-silicon batteries in its products.\n\nIn 2015, Tesla founder Elon Musk claimed that silicon in Model S batteries increased the car’s range by 6%.\n\nAs of 2018, products by startups Sila Nanotechnologies, Angstron Materials, Enovix, Enevate, and others were undergoing tests by the battery manufacturers, car companies, and consumer-electronics companies.\n\nChina-based Amperex is one of Sila's investors, Sila clients include BMW and Amperex Technology, battery supplier to companies including Apple and Samsung. BMW plans to incorporate Sila technology by 2023 and increase battery-pack capacity by 10-15%.\n\nEnevate produces anodes for vehicle manufacturers and claims to increase electric vehicle range by 30% versus conventional batteries.\n\nEnovix, whose investors include Intel and Qualcomm, is working on anodes that are almost pure silicon. The company claims its batteries would provide 30% to 50% more energy than conventional batteries.\n\nA crystalline silicon anode has a theoretical specific capacity of 4200 mAh/g, more than ten times that of anodes such as graphite (372 mAh/g). Each silicon atom can bind up to 4.4 lithium atoms in its fully lithiated state (), compared to one lithium atom per 6 carbon atoms for the fully lithiated graphite ().\n\nThe lattice distance between silicon atoms multiplies as it accommodates lithium ions (lithiation), reaching 320% of the original volume. The expansion causes large anisotropic stresses to occur within the electrode material, fracturing and crumbling the silicon material and detachment from the current collector. Prototypical lithium-silicon batteries lose most of their capacity in as little as 10 charge-discharge cycles. A solution to the capacity and stability issues posed by the significant volume expansion upon lithiation is critical to the success of silicon anodes.\n\nSilicon nanostructures are one potential solution. Researchers created silicon nanowires on a conductive substrate for an anode, and found that the nanowire morphology creates direct current pathways to help increase power density and decreases disruption from volume change. However, the large volume change of the nanowires can still pose a fading problem.\n\nOther studies examined the potential of silicon nanoparticles. Anodes that use silicon nanoparticles may overcome the price and scale barriers of nanowire batteries, while offering more mechanical stability over cycling compared to other silicon electrodes. Typically, these anodes add carbon as a conductive additive and a binder for increased mechanical stability. However, this geometry does not fully solve the issue of large volume expansion upon lithiation, exposing the battery to increased risk of capacity loss from inaccessible nanoparticles after cycle-induced cracking and stress.\n\nAnother nanoparticle approach is to use conducting polymers as both the binder and the additive for nanoparticle batteries. One study examined a three-dimensional conducting polymer and hydrogel network to carry silicon nanoparticles. The framework resulted in a marked improvement in electrode stability, with over 90% capacity retention after 5,000 cycles. However, the potential for inexpensive scale up has not been thoroughly investigated. Other researchers offered another potential solution, utilizing slurry coating techniques – which are currently employed at large scales for electrode production – with a conducting polymer binder. In general, the conducting polymer additive provides both mechanical stabilization and an avenue for conduction, replacing the conventional two-material system of a polymer stabilizer and carbon black particles. The substitution allows both better stabilization and better conduction.\n\nAnother issue is the destabilization of the solid electrolyte interface (SEI) layer consisting of decomposed electrolyte material.\n\nThe SEI layer normally forms a layer impenetratable to the electrolyte, which prevents further growth. However, due to the swelling of the silicon, the SEI layer cracks and become porous. Thus, it can thicken. A thick SEI layer results in a higher cell resistance, which decreases cell efficiency.\n\nThe SEI layer on silicon is composed of reduced electrolyte and lithium. At the operating voltage of the battery, the electrolyte is unstable and decomposes. The consumption of lithium in the formation of the SEI layer further decreases the battery capacity. Limiting growth of the SEI layer is therefore critical for commercial lithium-silicon batteries.\n\n"}
{"id": "51767373", "url": "https://en.wikipedia.org/wiki?curid=51767373", "title": "March of Intellect", "text": "March of Intellect\n\nThe March of Intellect, or the 'March of mind', was the subject of heated debate in early nineteenth-century England, one side welcoming the progress of society towards greater, and more widespread, knowledge and understanding, the other deprecating the modern mania for progress and for new-fangled ideas.\n\nThe 'March' debate was seen by Mary Dorothy George as a public reflection of the changes in British society associated with industrialisation, democracy and shifting social statuses – changes welcomed by some and not by others.\n\nThe roots of the controversy over the March of intellect can be traced back to the spread of education to two new groups in England after 1800 - children and the working-class. 1814 saw the first use of the term the 'march of Mind' as a poem written by Mary Russell Mitford for the Lancastrian Society, and the latter's work in bringing education to children was soon rivalled by the efforts of the Established Church.\n\nThe March of Intellect forms part of nineteenth-century debates over science communication, marking a peak in the development of the idea and possession of knowledge. The concept of knowledge as a result of the industrial revolution had changed from the end of the eighteenth century. ‘Polite learning’ practiced by the upper and middle classes through the study of ancient cultures was criticised for being ornamental in its uses by commentators such as Jeremy Bentham. The industrial revolution created a new focus on applied knowledge, particularly regarding natural philosophy (later science) and its various fields. ‘Useful knowledge’ was believed to be the way forward by liberal Whigs, but the definition of this term remained fluid. The increase in periodicals, encyclopaedias and printed literature from the late eighteenth century began to raise questions about the newfound availability of knowledge. Advances in the production of books further extended knowledge to the middle classes and owning printed literature became a desirable commodity. Where a volume would cost around 10 shillings at the beginning of the nineteenth century, by the 1820s a reprint of a volume could be half this value. At the same time, the spread of print culture, artisan coffee houses, and, from 1823 onwards, Mechanics' Institutes, as well as the growth of Literary and Philosophical Societies, meant something of a revolution in adult reading habits.\n\nThe working classes had limited access to knowledge owing to poor literacy rates and the expensive cost of printed materials relative to wages. The Spa Fields Riots and Peterloo Massacre raised concerns about revolution and the violent unrest created resistance among the elite towards educating the lower classes. Other conservative commentators supported educating the working class as a means of control. The \"Edinburgh Review\" commented in 1813 on the hopes of 'a universal system of education' that would 'encourage foresight and self-respect among the lower orders.' Through education, the working class would know their economic position in life and this would prevent further outbreaks of political unrest. Liberal Whig supporters of educating the working classes, such as Henry Brougham, believed in 'the greatest happiness of the greatest number' outlined by Bentham's utilitarian philosophy. The sciences were seen by these supporters as valuable knowledge for the working classes and debates on the best means of diffusing knowledge was debated.\n\nInterest in the so-called March of Intellect came to a peak in the 1820s. On the one hand, the Philosophic Whigs, spearheaded by Brougham, offered a new vision of a society progressing into the future: Thackeray would write of \"the three cant terms of the Radical spouters...'the March of Intellect', 'the intelligence of the working classes', and 'the schoolmaster abroad'\". Brougham's foundation of the Society for the Diffusion of Useful Knowledge, and of University College, London, seemed to epitomise the new progress of the age.\n\nBut the same phenomenon of the March of Intellect was equally hailed by conservatives as epitomising everything they rejected about the new age: liberalism, machinery, education, social unrest - all became the focus of a critique under the guise of the 'March'. The March of Intellect was repeatedly satirised in written print and visual media, such as cartoons. Cartoons were frequently used in the nineteenth century to explore current affairs and were becoming increasingly accessible during the peak of the March of Intellect. William Heath’s collection of prints published between 1825 and 1829 have become central representations of the debate. Heath used machines, steam-powered vehicles and other forms of technology in his work to mock liberal Whiggish ambitions that problems in society could be solved through widespread education. The scenes present futuristic visions of society whereby issues such as travel, emigration and postal delivery had been conquered by technological innovation through knowledge. They represent some of the advances in everyday life such as faster travel due to the extension of the railway and the rise in exchanging letters. These and other satirical works from the period recognised that a transformation within society was already in motion, but were ambiguous as to whether reform would be progressive or damaging. For example, Heath's cartoon entitled 'The March of Intellect' (c.1828) in which a giant automaton sweeps away quacks, delayed parliamentary bills and court cases, can be seen as apocalyptic in its attempt to improve society. The March of Intellect remained ambivalent throughout satire, but recurrently criticised the ambition of educating the working class. In Peacock's 1831 novel Crotchet Castle, one character, Dr. Folliott, satirised the \"Steam Intellect Society\" and linked the march explicitly to folly, rural protest and the rise in crime: \"the march of mind...marched in through my back-parlour shutters, and out again with my silver spoons\". Peacock had earlier parodied the Utilitarian take on the role of the modern poet: \"The march of his intellect is like that of a crab, backward\"\n\nThe March of Mind was used by the Whigs as one argument for the Great Reform Act; and after a decade of reform and railway, the idea of progress became something of a Victorian truism. Continuing concerns related more to ameliorating its effects than turning back the clock - philosophers fearing over-education would reduce moral and physical fibre, poets seeking to preserve individuality in the face of the utilitarian march.\n\nSee also Magee, D, 'Popular periodicals, common readers and the \"grand march of intellect\" in London, 1819-32' (DPhil, Oxon 2008).\n\n"}
{"id": "34966291", "url": "https://en.wikipedia.org/wiki?curid=34966291", "title": "Maritime Vsat", "text": "Maritime Vsat\n\nMaritime VSAT is the use of satellite communication through a Very-Small-Aperture Terminal (VSAT) on a moving ship at sea. \nSince a ship at sea moves with the water, the antenna needs to be stabilized with reference to the horizon and True north, so that the antenna is constantly pointing at the satellite it uses to transmit and receive signals.\n\nThe Maritime VSAT (Business) Industry was first created for the US Navy in September 1986, by the joint venture team of Richard A. Hadsall, President and CEO of Crescomm Transmission Services and Robert J. Matthews, President of SeaTel Inc.\nThe result of this business venture was the creation of the company Maritime Telecommunications Network, Inc. (MTN) MTN went on to commercialize the Maritime VSAT business by delivering services to Various Cruise Lines around the world as well as many Commercial Oil and Gas installations and vessels. MTN also pushed for global Maritime VSAT recognition with the petition for rule making from the FCC and the lobbying of the ITU’s World Radio Commission (WRC) for recognition to use the fixed satellite service (FSS) in C and Ku band by creating new rulings to recognize Earth Stations on Vessels ESV. This was accomplished by revision of the Radio Regulations, complementing the Constitution and the Convention of the International Telecommunication Union ITU, which incorporated the decisions of the World Radio Communication Conferences of 2003 (WRC-03)\n\nThere are many different options to build a maritime broadband network onboard of a vessels. \nEach option has its advantage (and disadvantage) in cost, in the coverage, the signal strength and requirements for the antenna size (and thus the requirements of installation).\n\nThere are major differences in capabilities, features, cost and performance between VSAT (Geostationary orbit satellites in Ku-band, C-band and Ka-band) and Low Earth orbit or Medium Earth Orbit satellites with L-band technologies in use.\n\nBoth L-band (LEO & MEO) and VSAT (GEO) systems are marketed with what appear to be a shared set of features and benefits.\n"}
{"id": "20433319", "url": "https://en.wikipedia.org/wiki?curid=20433319", "title": "Materiomics", "text": "Materiomics\n\nMateriomics is defined as the holistic study of material systems. Materiomics examines links between physiochemical material properties and material characteristics and function. The focus of materiomics is system functionality and behavior, rather than a piecewise collection of properties, a paradigm similar to systems biology. While typically applied to complex biological systems and biomaterials, materiomics is equally applicable to non-biological systems. Materiomics investigates the material properties of natural and synthetic materials by examining fundamental links between processes, structures and properties at multiple scales, from nano to macro, by using systematic experimental, theoretical or computational methods. \n\nThe term has been independently proposed with slightly different definitions by T. Akita et al. (AIST/Japan), Markus J. Buehler (MIT/USA), and Clemens van Blitterswijk, Jan de Boer and H. Unadkat (University of Twente/The Netherlands) in analogy to genomics, the study of an organism's entire genome. Similarly, materiomics refers to the study of the processes, structures and properties of materials from a fundamental, systematic perspective by incorporating all relevant scales, from nano to macro, in the synthesis and function of materials and structures. The integrated view of these interactions at all scales is referred to as a material's materiome.\n\nNew techniques for evaluating materials at the tissue level, such as reference point indentation (RPI) and raman spectroscopy are lending insight into the nature of these highly complex, functional relationships.\n\nMateriomics is related to proteomics, where the difference is the focus on material properties, stability, failure and mechanistic insight into multi-scale phenomena. \n\n\n"}
{"id": "30679561", "url": "https://en.wikipedia.org/wiki?curid=30679561", "title": "Moody's Analytics", "text": "Moody's Analytics\n\nMoody's Analytics is a subsidiary of Moody's Corporation established in 2007 to focus on non-rating activities, separate from Moody's Investors Service. It provides economic research regarding risk, performance and financial modeling, as well as consulting, training and software services. Moody's Analytics is composed of divisions such as Moody's KMV, Moody's Economy.com, Moody's Wall Street Analytics, the Institute of Risk Standards and Qualifications, and Canadian Securities Institute Global Education Inc.\n\nIn 1995, Moody's Corporation started a business unit providing quantitative analysis services, including credit risk assessment software and services after acquiring Financial Proformas, Inc., called Moody's Risk Management Service (MRMS).\n\nIn early 2000 Moody's acquired the Software Products Group of Crowe, Chizek & Co., then the eighth largest accounting and consulting firm in the U.S., which brought software used by banks to analyze the risk in taking on commercial loans. The same year, MRMS partnered with RiskMetrics to develop software that combined credit risk analysis with portfolio management.\n\nIn February 2002, Moody's purchased KMV (Kealhofer, McQuown and Vasicek), a San Francisco-based quantitative risk management firm, and merged it with MRMS to create Moody's KMV. The company acquired KMV's clients and its software tool for calculating the probability of credit default, EDF (Expected Default Frequency). Moody's KMV integrated financial modeling software from each former company and, in 2003, debuted its credit risk management system, Credit Monitor.\n\nIn 2005, Moody's acquired Economy.com, an economics research and analytics firm based in West Chester, Pennsylvania, adding services related to economic and demographic research, country analysis, and data on industrial, financial and regional markets.\n\nThe following year, in December 2006 the firm acquired Wall Street Analytics, a San Francisco-based financial analysis and monitoring software developer founded by Ron Unz, which then became Moody's Wall Street Analytics. The acquisition brought with it software for financial risk management, including CDOnet, a tool for collateralized debt obligation (CDO) valuation.\n\nIn August 2007, Moody's Corporation created a new division for its combined non-ratings businesses, Moody's Analytics, to operate separately from Moody's Investors Service. Subsidiary companies that make up Moody's Analytics today include Moody's KMV, Economy.com, Wall Street Analytics, Fermat International, Enb Consulting Ltd., and, most recently, CSI Global Education Inc. The division began operations with Moody's KMV, Economy.com and Wall Street Analytics, and other subsidiary companies were added to Moody's Analytics through later acquisitions.\n\nIn 2008, Moody's Analytics acquired Fermat International, a Brussels-based provider of software for financial risk and performance management in the banking sector, used by over 100 banks across 30 countries in Europe, the Middle East and Asia.\n\nIn December 2008, Moody's Analytics added Enb Consulting Ltd., a provider of professional training and career services in the financial sector based in Surrey, England, to Moody's Analytics Training Services. Its services include technical and soft skills training programs for banking and capital markets professionals.\n\nMoody's Analytics further added to its training services in November 2010, when it acquired Canadian Securities Institute Global Education Inc. (CSI), a provider of training and certification for the financial sector, best known for its introductory course to stocks and bonds, the Canadian Securities Course, which is mandatory for Canadian licensed investment advisers.\n\nIn June 2010, Moody's Analytics formed a strategic alliance with Experian to provide software for financial institutions to manage consumer loan portfolios. The first product provided by the companies was Moody's CreditCycle Plus, a tool to forecast potential losses and provide stress testing of loan portfolios.\n\nIn March 2011 Moody's Analytics announced the release of a software program developed by Moody's Research Labs, the Mortgage Portfolio Analyzer, to assist portfolio managers in managing credit risk.\n\nIn November 2011, Moody’s Corporation acquired a major stake in Copal Partners, providers of outsourced research and analytical services to institutional customers. This acquisition extends the research, data, software and training services offered by Moody’s Analytics.\n\nIn December 2011, Moody’s Corporation added Barrie & Hibbert Limited, a provider of risk management modeling tools for insurance companies, to Moody’s Analytics enterprise risk management services. The acquisition broadens Moody’s Analytics suite of software solutions for the insurance and pension sectors. Barrie & Hibbert’s Economic Scenario Generator (ESG) is widely recognized as an industry standard for valuing and projecting assets and liabilities and assessing risk and capital positions.\n\nIn December 2013, Moody's acquired Amba Investment Services, a provider of investment research and quantitative analytics for global financial institutions.\n\nIn October 2014, Moody's acquired Lewtan, a leading provider of a wide range of content and technology-based solutions to members of the global asset-securitization industry.\n\nIn December 2016, Copal Amba, formerly a Moody's Analytics subsidiary, was merged with Moody's Analytics to form Moody's Analytics' Knowledge Services unit.\n\nMark Almeida has been president of Moody's Analytics since January 2008. In October 2017, SBI collaborated with Moody's Analytics to train staffs.\n\nMoody's Analytics' products include Market Implied Ratings (MIR) and Expected Default Frequency (EDF) software packages. MIR applies Moody's ratings scale to credit and equity market price signals so users can identify investment opportunities; EDF estimates a company's credit default probability based on quantitative factors including market capitalization, equity, volatility and capital structure. The division also provides financial institutions with analytical and risk management software, including its RiskAnalyst credit risk management software, which is used to provide analysis of credit data for commercial loans and to calculate risk.\n\n"}
{"id": "2570241", "url": "https://en.wikipedia.org/wiki?curid=2570241", "title": "NESI", "text": "NESI\n\nNet-centric Enterprised Solutions for Interoperability (NESI) is a joint effort between the United States Navy’s Program Executive Office for C4I & Space and the United States Air Force’s Electronic Systems Center. It provides implementation guidance which facilitates the design, development, maintenance, evolution, and use of information systems for the Net-Centric Operations and Warfare (NCOW) environment. NESI has also been provided to other Department of Defense (DoD) services and agencies for potential adoption.\n\nNESI comprises six parts, each focusing on a specific area of guidance. NESI provides guidance, best practices, and examples for developing Net-Centric software. The overall goal is to provide common, cross-service guidance in basic terms for the program managers and developers of net-centric solutions. The objective is not to replace or repeat existing direction, but to help translate into concrete actions the plethora of mandated and sometimes contradictory guidance on the topic of net-centric compliance and standards.\n\nNESI subsumes two now obsolete references; in particular, the Air Force C2 Enterprise Technical Reference Architecture (C2ERA) and the Navy Reusable Applications Integration and Development Standards (RAPIDS). Initial authority for NESI is per the Memorandum of Agreement between Space and Naval Warfare Systems Command (SPAWAR), Navy PEO C4I & Space and the United States Air Force Electronic Systems Center, dated 22 December 2003, Subject: Cooperation Agreement for Net-Centric Solutions for Interoperability (NESI). This guidance will continue to evolve as direction and understanding of the requirements of net-centricity evolve. NESI will be updated to reflect changes to the guiding documents and new regulations.\n\nThe NESI documentation, a six-part information set, is available as PDF files. These parts consist of:\n\n\n\n\nIn order to evaluate programs/projects, NESI has developed a technical checklist. The current version of the checklist should be used to analyze the current status of a program/project. The technical checklist is produced from the guidance details and provides public, published, consistent interpretation of NESI guidance. Overall, the checklist will provide a uniform interpretation for all of participating organizations. \n\n"}
{"id": "30864591", "url": "https://en.wikipedia.org/wiki?curid=30864591", "title": "Networked control system", "text": "Networked control system\n\nA Networked Control System (NCS) is a control system wherein the control loops are closed through a communication network. The defining feature of an NCS is that control and feedback signals are exchanged among the system's components in the form of information packages through a network.\n\nThe functionality of a typical NCS is established by the use of four basic elements: \n\nThe most important feature of a NCS is that it connects cyberspace to physical space enabling the execution of several tasks from long distance. In addition, networked control systems eliminate unnecessary wiring reducing the complexity and the overall cost in designing and implementing the control systems. They can also be easily modified or upgraded by adding sensors, actuators and controllers to them with relatively low cost and no major changes in their structure. Moreover, featuring efficient sharing of data between their controllers, NCS are able to easily fuse global information to make intelligent decisions over large physical spaces. \nTheir potential applications are numerous and cover a wide range of industries such as: space and terrestrial exploration, access in hazardous environments, factory automation, remote diagnostics and troubleshooting, experimental facilities, domestic robots, aircraft, automobiles, manufacturing plant monitoring, nursing homes and tele-operations. While the potential applications of NCS are numerous, the proven applications are few, and the real opportunity in the area of NCS is in developing real-world applications that realize the area's potential.\n\n\nAdvent and development of the Internet combined with the advantages provided by NCS attracted the interest of researchers around the globe. Along with the advantages, several challenges also emerged giving rise to many important research topics. New control strategies, kinematics of the actuators in the systems, reliability and security of communications, bandwidth allocation, development of data communication protocols, corresponding fault detection and fault tolerant control strategies, real-time information collection and efficient processing of sensors data are some of the relative topics studied in depth.\n\nThe insertion of the communication network in the feedback control loop makes the analysis and design of an NCS complex, since it imposes additional time delays in control loops or possibility of packages loss. Depending on the application, time-delays could impose severe degradation on the system performance.\n\nTo alleviate the time-delay effect, Y. Tipsuwan and M-Y. Chow, in ADAC Lab at North Carolina State University, proposed the Gain Scheduler Middleware (GSM) methodology and applied it in iSpace. S. Munir and W.J. Book (Georgia Institute of Technology) used a Smith predictor, a Kalman filter and an energy regulator to perform teleoperation through the Internet.\n\nK.C. Lee, S. Lee and H.H. Lee used a genetic algorithm to design a controller used in a NCS. Many other researchers provided solutions using concepts from several control areas such as robust control, optimal stochastic control, model predictive control, fuzzy logic etc.\n\nMoreover, a most critical and important issue surrounding the design of distributed NCSs with the successively increasing complexity is to meet the requirements on system reliability and dependability, while guaranteeing a high system performance over a wide operating range. This makes network based fault detection and diagnosis techniques, which are essential to monitor the system performance, receive more and more attention.\n\n\n"}
{"id": "49483249", "url": "https://en.wikipedia.org/wiki?curid=49483249", "title": "Sam Labs", "text": "Sam Labs\n\nSAM Labs is a startup that makes app-enabled construction kits, designed for people of all ages to learn STEM, play, and create with technology and the Internet of Things. Founded by Belgian born CEO Joachim Horn, the company works out of their headquarters in East London. \n\nA month-long Kickstarter campaign initiated in October 2014 raised over $160,000, and helped launch the mass production of the kits. Backers included Jawbone co-founder Alexander Asseily, who pre-ordered the kit. The company has been featured in WIRED and The Telegraph.\n\nOn 29 September 2014, SAM Labs launched a month-long Kickstarter campaign to crowdfund the mass manufacture of the SAM kits for the spring of 2015, primarily through pre-orders of the kits. At the conclusion of the campaign on 29 October, SAM Labs had raised over $160,000 from 817 backers.\n\nSAM Labs produces app-enabled construction toys. \n\nThe SAM Science Museum Inventor Kit was produced in collaboration with the London Science Museum. Using the SAM Blocks and the free SAM Space app inside of the kit, kids can build awesome projects, games, inventions and hacks. Super quick. Super smart.\n\nThe company's hero kit, SAM's Curious Cars, launched in October of 2016. Available now at Barnes & Noble across the United States and John Lewis (department store) in the United Kingdom, the Curious Cars kit allows kids to build and program their own cars and games. Winning an IFA Markit Innovation Award, Consumer Electronics Show Award, and a KAPi award, the triple-award-winning kit also comes with its own app: Curious Cars.\n\nSAM Labs has created a habit of working with local design talent. SAM Labs first partnered with MAP Project Office, the London-based creative consultancy that specialises in industrial design, to deliver the Kickstarter campaign. Map also featured SAM Labs in a video collaboration with Honda, where SAM led modules were featured in this promotional video, covering \"Technology for Exploration.\" The second design partnership was with Pentagram Design Studio, to collaborate around the SAM Labs branding \"that never stands still\". Most recently, SAM collaborated with Hato Press and the London Science Museum to create the SAM Labs Science Museum \"Inventor Kit\".\n\nThe SAM Labs kits have received two Design Week Awards: the Best of Show Award and the Interactive Design Award, the judges said: “A really cohesive design; from the elegant packaging, to the use of the product itself. An amazing use of code and education. This really steps above and beyond expected design and opens up hundreds of possibilities.”\nAdditionally, the SAM Labs kit have received four Maker Faire awards by MAKE magazine: three \"Editor's Choice Awards\" and one \"Best in Class Award\".\nSAM Labs was a winner at the November 2014 Pitch@Palace UK Royal competition, which saw SAM being presented at the World Economic Forum in January 2015. In partnership with the Map Project Office, SAM Labs was awarded \"Best Interactive Design\" in the Design Week Awards 2015. SAM Labs also became a member of the Disrupt 100 most innovative companies for educational toys. \nWinner of a gold medal for the best Educational Gaming Toy in the 2016 Independent Toy Awards, SAM Labs swept up four more awards throughout Q4: a silver place for the UK Business awards for Disruptive Business model, an IFA Markit Innovation Award for Apps, Platforms & Ecosystems, Best Educational Toy in the Toy Testers awards, and a CES Innovation award in the Tech For a Better World category. \nAfter winning the Alphr + ITPro award for Best Startup of 2016, the company has landed a finalist position in the GESS Education Awards for 2017. \n"}
{"id": "50577184", "url": "https://en.wikipedia.org/wiki?curid=50577184", "title": "Sea Witch (lure)", "text": "Sea Witch (lure)\n\nThe Sea Witch is a record setting artificial fishing lure made by C&H Lures. It has been in production since 1925. It is recommended for trolling for billfish and dolphin.\n\n"}
{"id": "28675208", "url": "https://en.wikipedia.org/wiki?curid=28675208", "title": "Silicon Taiga", "text": "Silicon Taiga\n\nSilicon Taiga is a nickname for Akademgorodok, a Russian research and development center that is located near Novosibirsk. The nickname is a reference to the Silicon Valley, a renowned IT region found in Northern California. The term was first introduced to the Western press by Newsweek magazine in 1999.\n\nThe town is situated partly on the River Ob, and partly on the shore of a Novosibirsk artificial reservoir that was created by a dam on the river. Informally, this reservoir is called the Ob Sea. The dam contains a large hydroelectric power plant that was constructed in the 1950s.\n\nThe town itself has no skyscrapers, and only one remote speedway connecting it to the city of Novosibirsk. Office buildings and residences are connected by many intertwined, footworn paths.\n\nIn the 1950s the former Academy of Sciences of the USSR founded its Siberian Division, today known as the Siberian Division of the Russian Academy of Sciences, with a center of scientific research in Akademgorodok, where they established a dozen research institutes. To keep institutes supplied with fresh minds, Novosibirsk State University (NSU) was founded by the resolution of the Council of Ministers of the USSR on January 9, 1958.\n\nThe distinctive feature of NSU is its system of competitive selection and its training of talented youth. NSU is the only university in Siberia to have developed a multilevel model of continuing education. The University was built and developed simultaneously with the Novosibirsk scientific center and is focused on training highly qualified specialists for scientific work and tutoring. Since its foundation in 1958, more than fifty thousand specialists graduated from Novosibirsk State University; more than six thousand have defended their PhD dissertations, and over one thousand five hundred have received doctoral degrees. In 2009, NSU achieved the status of a national research university. This high honor was granted by the Russian Government on a competitive basis for a period of ten years.\n\nThe first IT companies were established in Akademgorodok in the 1990s. After the collapse of the Soviet Union, the government's investments into scientific activity were greatly reduced, and many scientists left long established institutions on a quest for better conditions. Some of these scientists decided to leave Russia in search of jobs in foreign scientific organizations all over the world. Others established their own private businesses, which were software-related high-tech IT companies. Many of these companies grew up into large, internationally recommended providers of software products and services. Their strategy was simple: the new IT companies would adopt the tried and true principles of the already established, famous IT corporations.\n\nAll IT companies in the Silicon Taiga can be divided into the two categories according to the offshore programming business model they employ: The firms that provide Offshore Development Center (ODC), and mostly develop new custom products (NPD), and the firms that are oriented to the SAAS model. Meantime the global giants in the field of software and high-tech products, such as IBM, Intel Corporation, and Schlumberger, saw this growing trend and established branch offices in the Silicon Taiga.\n\nThe summer of 2010 saw the launch of an innovative technology park in Akademgorodok, which brought together economic and intellectual power in the area, thus expanding innovation in Russia. Today, the Silicon Taiga is not just a collection of IT companies; it is a beneficial environment, in which the Russian national system of innovations can continue to flourish.\n\nThe following is a partial list of past and present notable companies founded in the Silicon Taiga or which have a major subsidiary located there:\n\n\n"}
{"id": "831609", "url": "https://en.wikipedia.org/wiki?curid=831609", "title": "Stepping switch", "text": "Stepping switch\n\nIn electrical controls, a stepping switch or stepping relay, also known as a uniselector, is an electromechanical device that switches an input signal path to one of several possible output paths, directed by a train of electrical pulses.\n\nThe major use of stepping switches was in early automatic telephone exchanges to route telephone calls. Later, they were often used in such equipment as industrial control systems. They were used in Japanese cypher machines during World War 2, known to the Americans as CORAL, JADE, and PURPLE. Code breakers at Bletchley Park employed uniselectors driven by a continuously rotating motor rather than a series of pulses in the Bombe machines to cryptanalyse the German Enigma ciphers.\n\nIn a uniselector, the stepping switch steps only on one axis, although there are often several sets of contacts in parallel. In other types, such as the Strowger switch, mechanical switching occurs on two directions, across a grid of contacts. The Strowger switch was invented by Almon Brown Strowger in 1888.\n\nStepping switches were widely used in telephony and industrial control systems (among related applications) when electromechanical technology was paramount.\n\nA basic stepping switch is an electrically operated rotary switch with a single (typically input) terminal, and multiple (typically output) terminals. Like other typical rotary switches, the single terminal connects to one of the multiple terminals by rotating a contact arm, sometimes called a wiper, to the desired position. Moving from one position to the next is called stepping, hence the name of the mechanism. Using traditional terminology, this is a single-pole, multi-position switch.\n\nWhile some stepping switches have only one pole (layer of contacts), a typical switch has more; in the latter case, all wipers are aligned and move together. Hence, one input with multiple wires could be connected to one of multiple outputs, based on the receipt of a single set of pulses. In this configuration, the rotating contacts resembled the head support arms in a modern hard disk drive. Multipole switches were common; some had perhaps as many as a dozen poles, but those were less common.\n\nMost switches have a bank of stationary contacts extending over half a cylinder, while some have only a third of a cylinder. The typical \"half-cylinder\" switch has two sets of wiper contacts opposite each other, while the \"third of a cylinder\" type has three sets, equally spaced. For any given level, both or all three wipers are connected, so it makes no difference which of the two (or three) is connecting. When access to more outlets was required, the rotor had two sets of wipers opposite each other but offset vertically: on the first half rotation one set of outlets was accessed; the second set of outlets was accessed on the second half rotation.\n\nAn electromagnet advances (steps) the wipers to the next position when fed with a pulse of DC. The magnet's armature (spring-loaded) operates a pawl that advances a ratchet. When the pawl reaches its full stroke, it blocks the ratchet so it and the wipers will not overshoot. When power to the coil disconnects, the spring retracts the pawl. Another pawl pivoted on the frame ensures that the wipers do not move backward; contact friction keeps them in place. Some uniselector designs step on application of the operate pulse; others step on its removal.\n\nIn most applications, such as telephony, it is desirable to be able to return the wipers to a \"home\" position; this is at the beginning of rotation, at one end of the array of fixed contacts. Some switches have a cam attached to the wiper shaft. This cam operates a set of contacts when the wiper is at home position, which is at the beginning of the span of rotation. Other circuit designs used one level (pole) of the contacts to home the wipers, so the separate homing contacts were not needed.\n\nTypical stepping switches have contacts directly operated by the stepping magnet's armature; these contacts can serve to make the magnet cycle (\"self-step\") and advance the wipers as long as power is applied. The external control circuits remove power when the wipers reach the desired position; that could be the home position.\n\nMost stepping switches rotate the wipers in only one direction, but some are bidirectional; the latter have a second magnet to rotate the wipers the other way. A third variety \"winds\" a spring as the wiper steps progressively, and a ratchet holds the wipers from returning to home position. When the circuit is no longer needed, another electromagnet releases the holding pawl; the spring then returns the wipers to their home position.\n\nStepping switches were quite noisy in operation (especially when self-stepping), because their mechanisms accelerated and stopped quickly to minimize operating time. One could compare their sound to that of some snap-action mechanisms. Nevertheless, they were engineered for long life, given periodic maintenance; they were quite reliable.\n\nSingle-axis stepping switches are sometimes known as uniselectors.\n\nSlightly more complicated was the \"two axis stepping switch\", (also called \"Strowger switch\" or \"two motion selector\" in Britain). Typically, a single compact group of wipers could connect to one of 100 (or 200) different fixed contacts, in ten levels. When the switch was idle, the wipers were disengaged from the fixed contacts. The wipers moved up and down on a vertical shaft, and rotated into the contact bank to make a connection. A spring, internal to the vertical shaft, returned the wipers to their home position at the bottom.\n\nThis type had two stepping coils with pawls and ratchets, one to raise the wipers to the desired banks of contacts, and one to rotate the wipers into the banks. These were commonly used in telephone switching with ten banks of ten contacts. The coils were typically driven by the electrical pulses derived from a rotary telephone dial. On a two-motion selector, as a digit was dialed, the wipers would step up the banks, then automatically rotate (self-step) into the selected bank until they found an \"unused\" outlet to the next switch stage. The last two digits dialed would operate the connector switch (\"final selector\" in Britain). The second to last digit would cause the wipers to move up and the last digit would cause them to rotate into the bank to the called customer's line outlet. If the line was idle then ringing voltage would be applied to the called line and ringing tone was sent to the calling line.\n\nAnother variant of the two-axis switch was the Stromberg-Carlson X-Y Switch which was quite common in telephone exchanges in the western USA. It was a flat mechanism, and the moving contacts moved both sidewise, as well as to and fro. It was quite reliable, and could be maintained by people with minimal training.\n\nStepping switches are used in a variety of applications, other than telephone systems. By connecting several in series with the highest output of one going to the stepping contact of the next, a counter could be constructed. Or by feeding the stepping contact with an endless pulse train via a relay, and controlling the relay from the switch's own output, it can be made to automatically hunt for the first unpowered line (or powered, depending on whether the relay is normally open or normally closed). They could also be used as a demultiplexer, so that two input lines could control a number of output devices. One input line steps the switch until the correct device is selected, and the other then powers that device. Many other applications are possible.\n\nSuch switches were used in a series of Japanese cypher machines during World War 2: CORAL, JADE, PURPLE (the names were American). Some of the equipment used to break the Enigma machine code also used many such switches, which some observers called the \"Machine Gun\" for the loud noise.\n\n"}
{"id": "773714", "url": "https://en.wikipedia.org/wiki?curid=773714", "title": "São José dos Campos", "text": "São José dos Campos\n\nSão José dos Campos (, meaning Saint Joseph of the Fields) is a major city and the seat of the municipality of the same name in the state of São Paulo, Brazil. One of the leading industrial and research centers with emphasis in aerospace sciences in Latin America, the city is located in the Paraíba Valley, between the two most active production and consumption regions in the country, São Paulo ( from the city) and Rio de Janeiro (). It is the main city of the Metropolitan Region of Vale do Paraíba e Litoral Norte. A native of São José dos Campos is called a \"joseense\" ().\n\nSource: City administration website (in Portuguese) \n\n\nThe municipality comprises three districts: São José dos Campos — the city itself, (also the seat), Eugênio de Melo and São Francisco Xavier. The last one is known for its natural sites and ecotourism.\n\nThe district of São José dos Campos is also subdivided into 2 subdistricts (São José dos Campos and Santana do Paraíba).\n\nHowever, for administrative purposes, the city is composed of 7 urban regions: Center, North, South, West, East, Southeast and São Francisco Xavier.\n\nHighlands predominate in the northern region of the municipality with altitudes ranging from . The northern border of the municipality lies over the Serra da Mantiqueira Mountains (\"Mantiqueira Range\"), with some peaks reaching over 2000 meters (6500 ft.). The highest point in the municipality is known as 'Pico do Selado' at an altitude of 2082 meters.\n\nIn the urban area, there are rolling plateaus and hills.\nThe lowest elevation in the city (and also in the municipality) is found in the \"Paraíba do Sul\" river, at an elevation of 550 m.\n\nThe municipality is bounded at the south by the 'Serra do Jambeiro' mountains, with an elevation of about 900m.\n\n\nThe municipality holds the São Francisco Xavier Environmental Protection Area, established in 2002.\nIt contains part of the Mananciais do Rio Paraíba do Sul Environmental Protection Area, created in 1982 to protect the sources of the Paraiba do Sul river.\n\nThe climate of the city can be, perhaps, best described as a mix between the subtropical climate of southern parts of the country, the tropical climate of most of the country and the subtropical highland climate (in Portuguese: clima tropical de altitude) of neighbouring mountainous regions.\n\nTechnically, the city has a humid subtropical climate with significant less precipitation during winters (transitioning between \"Cfa\" and \"Cwa\" climatic types in the Köppen classification). Winters are very mild, with average temperature in the coldest month of 17 °C. Summers are not excessively hot, with average temperature of the hotttest month of 24 °C. With global warming it is very likely that the city's climate will transition to a true tropical climate (type 'Aw' in the Köppen climate classification) in the near future.\n\nFrosts are very rare, happening on average once per decade. There's no record of snow precipitation in the city.\n\nThe peaks at the northern border of the municipality (some over 2000 m high) due to altitude have a colder Cwb/Cfb climates, with very occasional snowfalls (about once per decade).\n\nThe origins of São José dos Campos lie at the end of the 16th Century when Jesuits founded a cattle farm, \"Aldeia do Rio Comprido\". The farm was created through a concession of settlements around 1590 to the Society of Jesus. The farm was located on the banks of the Rio Comprido, natural division between São José and the city of Jacareí today.\n\nThe farm status was an artifice to hide a religious outpost, one of the several Jesuit Reductions in Brazil, known for their resistance to enslavement, from the Portuguese expedition leaders and indigenous people hunters, known as the Bandeirantes.\n\nOn September 10, 1611, the local was officially recognized and the farmers precluded from utilizing the Natives as slaves. However, a turmeric conflict between farmers and the religious led to the expulsion of the Jesuits in 1640 from the region and the consequent dispersion of the mission.\n\nNevertheless, the Jesuits returned and reestablished a new settlement, where the current city center is spotted. It was about northeast of the previous mission, on a higher plain with a privileged view above a geological depression, which guaranteed security against invasions and floods. Again, despite being a new mission, it was officially treated as a cattle farm.\n\nThe initial urbanization plan is attributed to the Jesuit priest Manoel de Leão, whose main occupation was really to be an administrator of the community.\n\nIn 1692, documents named the village as \"Residência do Paraíba do Sul\"; in 1696 as \"Residência de São José\".\n\nAt the beginning of the gold mining economic cycle in Brazil, the settlement goes through serious difficulties due to the exit of labor to the mines.\n\nAfter the definitive expulsion of Jesuits from the Portuguese Empire in 1759, all the religious order's assets, such as farms, colleges and villages were taken under the Portuguese Crown's custody. The governor, D. Luis Antonio Botelho Mourão, had as a priority to turn these new assets into productive units and increase tax collection. For that, Boutelho Mourão successfully requested authorization from the Viceroy to create civil parishes, known as \"freguesias\", and to change the fiscal status of villages to the category of Vila (town).\n\nThen, on July 27, 1767, São José reached the official status of town, with a hall and a pillory, passing over the status of civil parish; and the name Vila de São José do Paraíba was formalized. But for many years it maintained the same rural characteristics. The main difficulty was the fact that the \"Estrada Real\" (Royal Road) passed by its limits, far from the village.\n\nIn the middle of the 19th century, the village of São José do Paraíba had demonstrated some signs of economic growth through the development of agriculture. Cotton production evolved rapidly in the region, exported to the English textile industry. The production reached a peak in 1864.\n\nIn the same year, on April 22, the town became the seat of a municipality, acquiring finally, in 1871, the current name of São José dos Campos, followed by the creation of a judiciary district in 1872. Almost simultaneously, there was development of coffee crops in Paraíba Valley, which started to take off in 1870.\n\nIn 1886, after the opening of the Estrada de Ferro Central do Brasil railway (1877), the coffee production peaked. Then started to decay, running steady until the 1930s.\n\nThe call for the municipality of São José dos Campos for the treatment of pulmonary tuberculosis by sanatoriums became noticed at the\n\nbeginning of last century, due to its supposedly favorable climate conditions. The city became to be known as \"the Sanatorium City\". The country's then-largest hospital, the Vicentina Aranha Sanitarium, was opened in town in 1924, and in 1935 the municipality was officially recognized as a health retreat.\n\nWith the advent of antibiotics in the 1940s, tuberculosis begins to be treated anywhere, thus ending the healthcare advantage carried out by São José, whereas the establishment of industries was about just to start.\n\nThe industrialization process of the municipality takes hold from the installation of the \"Technological Institute of Aeronautics \" in 1950 and also with the opening of the Dutra Highway (BR-116), thus making possible a faster connection between Rio de Janeiro and São Paulo and cutting into the urban area of São José dos Campos. Altogether, these factors allowed the municipality to make strides towards fulfilling its scientific and technological potential.\n\nSão José dos Campos has a \"strong mayoral\" system in which the mayor is vested with extensive executive powers, as it is in all municipalities in Brazil. The mayor is elected to a four-year term by universal voting. The City Council is elected every four years with the mayor. The current mayor is Felicio Ramuth, from the political party PSDB and was elected in 2016.\n\nThe State of São Paulo is divided politically and administratively into 15 regions. São José dos Campos is the seat and the name of the 3rd Administrative Region, which includes the North Coast of São Paulo state and the Paraíba Valley.\n\nThe region comprises 39 municipalities with sharp contrasts. São José dos Campos is a densely populated city, with approximately 2,100 inhabitants/square kilometers in urban area, whereas the quiet municipality of São José do Barreiro has only seven inhabitants/square kilometer. There are both highly industrialized cities and the others in the region are focused on agriculture and tourism. São José dos Campos is well known as the \"Capital do Vale\" which means that São José dos Campos is the most important city of the Paraíba Valley.\n\nIt is one of the state's most dynamic areas, the fourth one in terms of population density, and covers 11.3% of the state's territory. The main municipalities are São José dos Campos, Taubaté, Jacareí, Guaratingueta, Caraguatatuba, Campos do Jordão, São Sebastião, Lorena, Pindamonhangaba, Ubatuba and Caçapava.\n\n\n\nAll the major hypermarkets, supermarket chains and discount and department stores are in city. The largest malls are:\n\n\nBesides those malls, the most important commercial centers include:\n\n\nAnd newer areas such as:\n\n\nThe Vila Ema district has sites for nightlife including bars and restaurants.\n\nSeveral fairs and expositions are done at the two pavilions located in the city.\n\nA list of notable sites:\n\n\n\nA Japanese garden is open for visits within the Santos-Dumont Park, celebrating the sister cities.\n\nAlthough São José is an industrial center, the city still preserves green areas and quiet town districts. Around 62% of the area from the municipality is characterized as an environmental preservation area. On the oustskirts of the urban area, Augusto Ruschi Ecology Reserve has many local plant species. The natural reserve has , being a government protect area for the local flora. São Francisco Xavier is a community the offers many of those attributes as well.\n\nFurthermore, there is easy access to the mountain cities (Campos do Jordão, Santo Antônio do Pinhal) and to the beaches of the Northern Coast of São Paulo.\n\nThe city has three parks and several sports and country clubs. Tenis Clube and Associação Esportiva São José have hosted the 35th Banana Bowl International Tennis Federation Juniors Circuit in 2005 and 2006. \n\nA soccer stadium, called Estádio Martins Pereira, is the home ground of São José Esporte Clube, a professional soccer team.\n\nNotable teams from the city:\n\nThere are 19 movie theaters and 2 theaters including one inside the Univap - University of Vale do Paraíba.\n\nThe Brazilian National Institute for Space Research (INPE) has its headquarters in São José dos Campos. It coordinates intensive research and development in areas such as Earth observation, space sciences and space technologies. Also the Brazilian General Command for Aerospace Technology (CTA) has its facilities in the city. There are 53 secondary schools, 54 primary schools and 109 preschools.\n\n\n\n\nThe city has two bus stations, having lines to cities in all regions of the country and international routes to Argentina, Paraguay, Uruguay and Chile.\n\nSão José dos Campos boasts an extensive bus system. Operated by three companies (Expresso Maringa, Julio Simões and Viação Capital do Vale), these lines serve nearly all areas in the city with 319 buses. São José dos Campos also has an alternative system with minivans to supplement the regular buses.\n\nThe city uses a ring road system, that interconnects it to important national and state highways:\n\n\n\nThe city is also served by a railway (the former Central do Brasil), administered by MRS Logística, which today only carries freight.\n\nThe ports of São Sebastião and Santos can be reached by the highways SP-099, SP-155, and BR-101. The transportation of cargo to the domestic and foreign markets is made through both ports.\n\nThe São José dos Campos Airport (IATA: SJK, ICAO: SBSJ) has a heavy passenger flow, mainly business trips during weekdays, and it is an important connection between São Paulo and Rio de Janeiro. With a runway, the airport also serves people who come to visit the tourist city of Campos de Jordão. The airlines Azul Brazilian Airlines and TRIP Linhas Aéreas fly from São José dos Campos to several destinations and presently are the only commercial airlines operating on São José dos Campos.\n\nIt is also used for the transportation of cargo from the several industries located in the so-called Cone Leste Paulista (\"São Paulo's East Cone\"). Infraero and the Federal Revenue Agency are also introducing a new concept called airport-industry, that will offer fiscal incentives and fast importation and exportation procedures. The municipality has also a Customs Station for the Hinterland (dry port), controlled by the Federal Revenue Agency.\n\nSão José dos Campos receives natural gas from two gas pipelines, and large companies such as General Motors, Kodak, Monsanto and Embraer are among the main users. The city is the third largest in the country referring to the distribution net of natural gas for residential use. \n\nIt has also a large network of fiber optics, with broadband services covering 75% of the city. There is 1 telephone for each 3 inhabitants and a vast service network of cellular telephones. \n\nThe municipality cultivates different crops: rice, tomato, potatoes, orange and many vegetables; cattle are raised for beef and milk supply. There are also farms for production of eggs and chicken. \n\nIn contrast to the rural town in 1950s, today São José is an important manufacturing center and holds a large array of industries. Over 1,251 industries are in the municipality and nearby 47,000 inhabitants work for industries. The three main industries are automotive, oil/petrochemical and aerospace. There are significant pharmaceutical, consumer durables, chemical, and telecommunication companies in the city.\n\nIt is also known as the \"Brazilian aeronautics capital\" because it is home of CTA and one of the biggest aircraft manufacturers in the world, Embraer.\n\nIn 2014 São José dos Campos ranked as the 5th largest exporter, by value, of all Brazilian municipalities exporting $4.6B (USD) worth of materials. In that year, between aircraft and aircraft parts categories, São José Dos Campos exported $3.57B (USD) or 81.9% of the total exports of the municipality.\n\nSince the 1990s, the local economy has been evolving in a different direction. The manufacturing economy has been downsized or replaced by tertiary and quaternary sectors of industries.\n\nFor instance, the Entrepreneurial District of Chacaras Reunidas concentrates companies of micro, small and medium size, which are mainly the result of downsizing from old large local industries. Yet even though most of these are industries, these companies provide service as well.\n\nTwo technological parks and five (one in project) business incubators have been created within universities or industrial facilities.\n\nThere are incubators with technological start-up companies installed at Univap and at Henrique Lage Refinery of Petrobrás. The CTA houses other incubator, Incubaero, specialized in the aeronautical field.\n\nUnivap features a technological park with capacity for around 40 small to medium-sized innovating companies in the areas of materials, electronics and telecommunications, information technology, aerospace, energy, environment control, biotechnology, bioinformatics, chemical engineering, and software among others. A new technological park, managed by the municipality and the state government of São Paulo, will house two new think tanks: the Institute for Technological Research (IPT) and the ItecBio (Instituto de Tecnologias Biomédicas).\n\nAs a result of its geographical location, the city became an important distribution center, having several logistics providers. Activities like purchasing, transport, planning and warehousing have employed many people recently.\n\nCommerce and real estate ventures have developed in the last years, reflecting the changes in the economy. For instance, the largest shopping mall in the region was an old manufacturing facility. Serving the region's population of approximately one million, the city is the regional hub for shopping and services for the Vale do Paraíba, the northern coast of São Paulo and southern Minas Gerais.\n\n"}
{"id": "1500407", "url": "https://en.wikipedia.org/wiki?curid=1500407", "title": "Tablecloth", "text": "Tablecloth\n\nA tablecloth is a cloth used to cover a table. Some are mainly ornamental coverings, which may also help protect the table from scratches and stains. Other tablecloths are designed to be spread on a dining table before laying out tableware and food.\n\nTablecloths can be made of almost any material, including delicate fabrics like embroidered silk. Dining cloths are typically made of cotton, a poly-cotton blend, or a PVC-coated material that can be wiped clean, but they can range from functional coverings to fine textiles, as long as they can be laundered. Some cloths are designed as part of an overall table setting, with coordinating napkins, placemats, or other decorative pieces. Special kinds of tablecloth include runners which overhang the table at two ends only and table protectors to provide a padded layer under a normal cloth.\n\nIn many European cultures a white, or mainly white, tablecloth used to be the standard covering for a dinner table. In the later medieval period spreading a high quality white linen or cotton cloth was an important part of preparing for a feast in a wealthy household. Over time the custom of arranging tableware on a cloth became common for most social classes except the very poorest. As eating habits changed in the 20th century, a much greater range of table-setting styles developed. Some formal dinners still use white tablecloths, often with a damask weave, but other colours and patterns are possible. \n\nPerugia tablecloths and napkins have been made since medieval times. White with characteristic woven blue stripes and patterns, the style is also associated with church linen.\n\nVictorian interiors were full of thick, fringed draperies in deep colours, including tablecloths reaching to the floor on any kind of table.\n\nA popular \"magic trick\" involved pulling a loaded tablecloth away from a table but leaving the plates behind. This trick relies on inertia.\n\n"}
{"id": "833690", "url": "https://en.wikipedia.org/wiki?curid=833690", "title": "Tolerance interval", "text": "Tolerance interval\n\nA tolerance interval is a statistical interval within which, with some confidence level, a specified proportion of a sampled population falls. \"More specifically, a 100×p%/100×(1−α) tolerance interval provides limits within which at least a certain proportion (p) of the population falls with a given level of confidence (1−α).\" \"A (p, 1−α) tolerance interval (TI) based on a sample is constructed so that it would include at least a proportion p of the sampled population with confidence 1−α; such a TI is usually referred to as p-content − (1−α) coverage TI.\" \"A (p, 1−α) upper tolerance limit (TL) is simply an 1−α upper confidence limit for the 100 p percentile of the population.\"\n\nA tolerance interval can be seen as a statistical version of a probability interval. \"In the parameters-known case, a 95% tolerance interval and a 95% prediction interval are the same.\" If we knew a population's exact parameters, we would be able to compute a range within which a certain proportion of the population falls. For example, if we know a population is normally distributed with mean formula_1 and standard deviation formula_2, then the interval formula_3 includes 95% of the population (1.96 is the z-score for 95% coverage of a normally distributed population).\n\nHowever, if we have only a sample from the population, we know only the sample mean formula_4 and sample standard deviation formula_5, which are only estimates of the true parameters. In that case, formula_6 will not necessarily include 95% of the population, due to variance in these estimates. A tolerance interval bounds this variance by introducing a confidence level formula_7, which is the confidence with which this interval actually includes the specified proportion of the population. For a normally distributed population, a z-score can be transformed into a \"\"k\" factor\" or tolerance factor for a given formula_7 via lookup tables or several approximation formulas. \"As the degrees of freedom approach infinity, the prediction and tolerance intervals become equal.\"\n\nThe tolerance interval is less widely known than the confidence interval and prediction interval, a situation some educators have lamented, as it can lead to misuse of the other intervals where a tolerance interval is more appropriate.\n\nThe tolerance interval differs from a confidence interval in that the confidence interval bounds a single-valued population parameter (the mean or the variance, for example) with some confidence, while the tolerance interval bounds the range of data values that includes a specific proportion of the population. Whereas a confidence interval's size is entirely due to sampling error, and will approach a zero-width interval at the true population parameter as sample size increases, a tolerance interval's size is due partly to sampling error and partly to actual variance in the population, and will approach the population's probability interval as sample size increases.\n\nThe tolerance interval is related to a prediction interval in that both put bounds on variation in future samples. The prediction interval only bounds a single future sample, however, whereas a tolerance interval bounds the entire population (equivalently, an arbitrary sequence of future samples). In other words, a prediction interval covers a specified proportion of a population \"on average\", whereas a tolerance interval covers it \"with a certain confidence level\", making the tolerance interval more appropriate if a single interval is intended to bound multiple future samples.\n\n gives the following example: So consider once again a proverbial EPA mileage test scenario, in which several nominally identical autos of a particular model are tested to produce mileage figures formula_9. If such data are processed to produce a 95% confidence interval for the mean mileage of the model, it is, for example, possible to use it to project the mean or total gasoline consumption for the manufactured fleet of such autos over their first 5,000 miles of use. Such an interval, would however, not be of much help to a person renting one of these cars and wondering whether the (full) 10-gallon tank of gas will suffice to carry him the 350 miles to his destination. For that job, a prediction interval would be much more useful. (Consider the differing implications of being \"95% sure\" that formula_10 as opposed to being \"95% sure\" that formula_11.) But neither a confidence interval for formula_1 nor a prediction interval for a single additional mileage is exactly what is needed by a design engineer charged with determining how large a gas tank the model really needs to guarantee that 99% of the autos produced will have a 400-mile cruising range. What the engineer really needs is a tolerance interval for a fraction formula_13 of mileages of such autos.\n\nAnother example is given by: The air lead levels were collected from formula_14 different areas within the facility. It was noted that the log-transformed lead levels fitted a normal distribution well (that is, the data are from a lognormal distribution. Let formula_1 and formula_16, respectively, denote the population mean and variance for the log-transformed data. If formula_17 denotes the corresponding random variable, we thus have formula_18. We note that exp(mu) is the median air lead level. A confidence interval for mu can be constructed the usual way, based on the \"t\"-distribution; this in turn will provide a confidence interval for the median air lead level. If formula_19 and S denote the sample mean and standard deviation of the log-transformed data for a sample of size n, a 95% confidence interval for mu is given by formula_20, where formula_21 denotes the 1-alpha quantile of a \"t\"-distribution with m degrees of freedom. It may also be of interest to derive a 95% upper confidence bound for the median air lead level. Such a bound for mu is given by formula_22. Consequently, a 95% upper confidence bound for the median air lead is given by formula_23. Now suppose we want to predict the air lead level at a particular area within the laboratory. A 95% upper prediction limit for the log-transformed lead level is given by formula_24. A two-sided prediction interval can be similarly computed. The meaning and interpretation of these intervals are well known. For example, if the confidence interval formula_25 is computed repeatedly from independent samples, 95% of the intervals so computed will include the true value of formula_1, in the long run. In other words, the interval is meant to provide information concerning the parameter formula_1 only. A prediction interval has a similar interpretation, and is meant to provide information concerning a single lead level only. Now suppose we want to use the sample to conclude whether or not at least 95% of the population lead levels are below a threshold. The confidence interval and prediction interval cannot answer this question, since the confidence interval is only for the median lead level, and the prediction interval is only for a single lead level. What is required is a tolerance interval; more specifically, an upper tolerance limit. The upper tolerance limit is to be computed subject to the condition that at least 95% of the population lead levels is below the limit, with a certain confidence level, say 99%.\n\nOne-sided normal tolerance intervals have an exact solution in terms of the sample mean and sample variance based on the noncentral \"t\"-distribution. \nTwo-sided normal tolerance intervals can be obtained based on the noncentral chi-squared distribution.\n\n\n"}
{"id": "10177213", "url": "https://en.wikipedia.org/wiki?curid=10177213", "title": "Trevor Blackwell", "text": "Trevor Blackwell\n\nTrevor Blackwell (born 4 November 1969 in Canada) is a computer programmer, engineer and entrepreneur based in Silicon Valley.\n\nBlackwell is a developer of humanoid robots. Dr. Blackwell is the founder and CEO of Anybots and a partner at Y Combinator.\n\nBlackwell grew up in Saskatoon, Saskatchewan, Canada. Blackwell studied engineering at Carleton University and received a Bachelor of Engineering in 1992, then studied Computer Science at Harvard University and received a PhD in 1998. His dissertation applied randomized methods to analyzing the performance of networks and compilers.\n\nDuring graduate school Blackwell joined Viaweb for which he wrote the image rendering, order processing and statistics software. The company was acquired by Yahoo in 1998, and Blackwell moved to Silicon Valley to lead the Yahoo Store development group.\n\nHe founded Anybots in 2001 to build teleoperated humanoid robots. In 2006, Anybots announced a humanoid robot that walks and balances like people do, without depending on large feet for stability.\n\nAs side projects, he has built two other balancing vehicles: a two-wheeled balancing scooter similar to the Segway but with different steering, and the self-balancing Eunicycle. Several hobbyists have built vehicles based on the open design of the machine.\n\nHe was a co-founder of Y Combinator in 2005.\n\n"}
{"id": "55871256", "url": "https://en.wikipedia.org/wiki?curid=55871256", "title": "UMANG", "text": "UMANG\n\numang app or Unified Mobile Application for New-age Governance, is a Government of India all-in-one single unified secure multi-channel multi-platform multi-lingual multi-service freeware mobile app for accessing over 1,200 central and state government services in multiple Indian languages over Android, iOS, Windows and services such as AADHAR, DigiLocker, Bharat Bill Payment System, PAN, EPFO services, PMKVY services, AICTE, CBSE, tax and fee or utilities bills payments, education, job search, tax, business, health, agriculture, travel, birth certificates, e-District, passport and much more. This a key component of Digital India government initiative to make all traditional offline government services available 24/7 online through single unified app. App is initially available in 13 language and will replace or compliment 1500 apps launched by the government so far. Progressively more services will be brought online. This e-Gov service can be accessed via multiple channels, such as mobile app, IVR, website, etc. User numbers will be boosted by the roll out of BharatNet. Users can Create and update Profile, Sort & Filter through categories and services, or search to access relevant services while on the move. \n\nSample usage of app includes locating CBSE exam centres, viewing school results, calculating crop insurance premium, accessing soil health cards, booking OPD appointments at government hospitals and viewing lab reports from there, registering and applying for the PMKY scheme, raising EPFO claims and more. \n\nIt has synergies with other key Government of India schemes, such as Digital India, National e-Governance Plan, BharatNet, Make in India, Startup India, Standup India, Industrial corridors, Bharatmala, Sagarmala, Dedicated Freight Corridors and UDAN-RCS.\n\nOn 23 November 2017 this app service was launched by the Prime Minister Narendra Modi at the fifth edition of the Global Conference on Cyberspace in New Delhi. Conceived in 2016, this e-governance app was developed by Ministry of Electronics and Information Technology (MeitY) and National e-Governance Division (NeGD), and went through several rounds of beta tests since November 2016 before its launch. At the time of launch the app had 162 services from 33 state and central government departments and four states, a number that will soon go up to 1,200 and more.\n\nThe app is for everyone who needs to interact with any government or semi-government depart at centre, state or local level in India. It makes available online a plethora of off-line services through a single unified app. Customer Support is available from 8 am to 8 pm all days of the year. Users can create or change Profile, browse Service Dictionary, check Transaction History, view their DigiLocker and change Settings of the app. App homepage shows recently viewed services, new services, recently updated services, trending services section, top rated services and suggested services, with ability to bookmark favorite services. \n\nThis multiplatform app is available on Android, iOS, Windows, USSD. The app can be downloaded from google playstore, by giving a miss call to 97183-97183, by scanning app QR code or by submitting own cell phone number on app homepage. It is available for via multiple channels, such as mobile application, web, IVR and SMS which can be accessed through smartphones, feature phones and computers. \n\n1.2 billion Indians and many more who interact with Indian government are potential users.\n\n\nUmang Mobile App\n\n"}
{"id": "19424800", "url": "https://en.wikipedia.org/wiki?curid=19424800", "title": "UniPro", "text": "UniPro\n\nUniPro (or Unified Protocol) is a high-speed interface technology for interconnecting integrated circuits in mobile and mobile-influenced electronics. The various versions of the UniPro protocol are created within the MIPI Alliance (Mobile Industry Processor Interface Alliance), an organization that defines specifications targeting mobile and mobile-influenced applications.\n\nThe UniPro technology and associated physical layers aim to provide high-speed data communication (gigabits/second), low-power operation (low swing signaling, standby modes), low pin count (serial signaling, multiplexing), small silicon area (small packet sizes), data reliability (differential signaling, error recovery) and robustness (proven networking concepts, including congestion management).\n\nUniPro version 1.6 concentrates on enabling high-speed point to point communication between chips in mobile electronics. UniPro has provisions for supporting networks consisting of up to 128 UniPro devices (integrated circuit, modules, etc.). Network features are planned in future UniPro releases. In such a networked environment, pairs of UniPro devices are interconnected via so-called links while data packets are routed toward their destination by UniPro switches. These switches are analogous to the routers used in wired LAN based on gigabit Ethernet. But unlike a LAN, the UniPro technology was designed to connect chips within a mobile terminal, rather than to connect computers within a building.\n\nThe initiative to develop the UniPro protocol came forth out of a pair of research projects at respectively Nokia Research Center and Philips Research. Both teams independently arrived at the conclusion that the complexity of mobile systems could be reduced by splitting the system design into well-defined functional modules interconnected by a network. \nThe key assumptions were thus that the networking paradigm gave modules well-structured, layered interfaces and that it was time to improve the system architecture of mobile systems to make their hardware- and software design more modular. In other words, the goals were to counteract the rising development costs, development risks and time-to-market impact of increasingly complex system integration.\n\nIn 2004, both companies jointly founded what is now MIPI's UniPro Working Group. Such multi-company collaboration was considered essential to achieve interoperability between components from different component vendors and to achieve the necessary scale to drive the new technology.\n\nThe name of both the working group and the standard, UniPro, reflects the need to support a wide range of modules and wide range of data traffic using a single protocol stack. Although other connectivity technologies (SPI, PCIe, USB) exist which also support a wide range of applications, it should be noted that the inter-chip interfaces used in mobile electronics are still diverse which differs significantly from the (in this respect more mature) computer industry.\n\nIn January 2011, UniPro Version 1.40 was completed. Its main purpose is full support for a new Physical Layer: M-PHY® including support for power modes change and peer device configuration. In July 2012 UniPro v1.40 has been upgraded to UniPro v1.41 to support the newer higher speed M-PHY v2.0. The UniPro v1.4x specifications have been released together with a formal specification model (SDL).\n\nThe final draft of Version 1.6 of the UniPro specification was completed in August 2013. Its acknowledgements list 19 engineers from 12 companies and organizations: Agilent, Cadence, IEEE-ISTO,Intel, nVidia, Nokia, Qualcomm, Samsung, STMicroelectronics, Synopsys, Texas Instruments and Toshiba.\nThe UniPro v1.6 Specification is an update to the UniPro v1.41.00 Specification, and consists solely of the UniPro specification document, SDL is no longer supported.\nThe UniPro v1.6 Specification references the following documents:\n\nTo date, several vendors have announced the availability of UniPro IP blocks and various chip suppliers have created implementations that are at various phases of development. In the meantime, the MIPI UniPro Working Group is setting up a conformance test suite and is preparing future extensions of the technology (see UniPro Versions and Roadmap).\n\nOn January 30, 2018, JEDEC published the UFS 3.0 standard which uses MIPI M-PHY v4.1 (with HS-Gear4) and MIPI UniProSM v1.8 for mobile memory with data rates up to 2900 MB/s (11,6 Gbit/s per lane, 2 lanes, 23,2 Gbit/s total).\n\n\nUniPro associated with its underlying PHY layer is a layered protocol stack that covers layers L1 to L4 of the OSI Reference Model for networking. UniPro introduces an extra layer L1.5 between L1 and L2 which can be regarded as a sub-layer of OSI's layer L1.\n\nUniPro's strict layering enables it to be used for a wide range of applications:\n\nUniPro's layered architecture also allows it to support multiple physical layer (L1, PHY) technologies even within a single network. This is analogous to TCP/IP which can run on a wide range of lower-layer technologies. In the case of UniPro, two PHY technologies are supported for off-chip use.\n\nThese PHY technologies are covered in separate MIPI specifications (which are referenced by the UniPro specification. Note that the term UniPort is used to represent the actual port on a chip which conforms to the UniPro specification for its upper layers (L1.5 to 4) and a MIPI PHY specification for L1. As there are two PHY technologies, these are respectively known as UniPort-D (UniPro with D-PHY) and UniPort-M (UniPro with M-PHY).\n\nThe UniPro 1.0 specification was approved by the MIPI Board of Directors on January 14, 2008. UniPro 1.1, that was completed in July 2009, aims to improve readability, provides a reference model (in SDL) for two of the four UniPro protocol layers, and provides features to facilitate automated conformance testing.\n\nThe architects designing UniPro intended from the start to release the technology as a step-wise roadmap with backward compatibility. UniPro 1.1 is designed to be fully backwards compatible with UniPro 1.0. The main purpose of UniPro 1.40 and UniPro v1.41 (UniPro v1.4x) is to support an additional physical layer, the M-PHY. Furthermore, UniPort-M features local and remote control of a peer UniPro device that can be used for example to control various supported power modes of the link.\nPlanned roadmap steps beyond UniPro v1.4x aim to provide specifications for network-capable endpoint and network switch devices.\n\nThe UniPro v1.6 Specification was designed to ensure interoperability with UniPro v1.41.00 when using the M-PHY physical layer. As D-PHY is no longer supported on v1.60, backwards compatibility for D-PHY operation cannot be maintained.\n\nUniPro and its underlying physical layer were designed to support low power operation needed for battery-operated systems. These features range from power-efficient high-speed operation to added low-power modes during idle or low bandwidth periods on the network. Actual power behavior is, however, highly dependent on system design choices and interface implementation.\n\nThe UniPro protocol can support a wide range of applications and associated traffic types. Example chip-to-chip interfaces encountered in mobile systems:\n\nNote that such applications require an application protocol layer on top of UniPro to define the structure and semantics of the byte streams transported by UniPro. These can be done by simply porting existing data formats (e.g. tracing, pixel streams, IP packets), introducing new proprietary formats (e.g. chip-specific software drivers) or defining new industry standards (e.g. UFS for memory-like transactions).\n\nApplications which are currently believed to be less suitable for UniPro are:\n\nThe UniPro protocol stack follows the classical OSI reference architecture (ref). For practical reasons, OSI's Physical Layer is split into two sub-layers: Layer 1 (the actual physical layer) and Layer 1.5 (the PHY Adapter layer) which abstracts from differences between alternative Layer 1 technologies.\nThe UniPro specification itself covers Layers 1.5, 2, 3, 4 and the DME (Device Management Entity). The Application Layer (LA) is out of scope because different uses of UniPro will require different LA protocols. The Physical Layer (L1) is covered in separate MIPI specifications in order to allow the PHY to be reused by other (less generic) protocols if needed(ref).\n\nOSI Layers 5 (Session) and 6 (Presentation) are, where applicable, counted as part of the Application Layer.\n\nUniPro is specifically targeted by MIPI to simplify the creation of increasingly complex products. This implies a relatively long-term vision about future handset architectures composed of modular subsystems interconnected via stable, standardized, but flexible network interfaces. It also implies a relatively long-term vision about the expected or desired structure of the mobile handset industry, whereby components can readily interoperate and components from competing suppliers are to some degree plug compatible.\n\nSimilar architectures have emerged in other domains (e.g. automotive networks, largely standardized PC architectures, IT industry around the Internet protocols) for similar reasons of interoperability and economy of scale. It is nevertheless too early to predict how rapidly UniPro will be adopted by the mobile phone industry.\n\nHigh speed interconnects like UniPro, USB or PCI Express typically cost more than low speed interconnects (e.g. I2C, SPI or simple CMOS interfaces). This is for example because of the silicon area occupied by the required mixed-signal circuitry (Layer 1), as well as due to the complexity and buffer space required to automatically correct bit errors. UniPro's cost and complexity may thus be an issue for certain low bandwidth UniPro devices.\n\nAs Metcalfe postulated, the value of a network technology scales with the square of the number of devices which use that technology. This makes any new cross-vendor interconnect technology only as valuable as the commitment of its proponents and the resulting likelihood that the technology will become self-sustaining. Although UniPro is backed by a number of major companies and that the UniPro incubation time is more or less in line with comparable technologies (USB, Internet Protocol, Bluetooth, in-vehicle networks), adoption rate is presumed to be main concern about the technology. This is especially true because the mobile industry has virtually no track record on hardware standards which pertain to the internals of the product.\n\nA key driver for UniPro adoption is JEDEC Universal Flash Storage (UFS) v2.0 which uses MIPI UniPro and M-PHY as the basis for the standard. There are several implementation of the standard which are expected to hit the market\n\nInteroperability requires more than just alignment between the peer UniPro devices on protocol layer L1-L4: it also means aligning on more application-specific data formats, commands and their meaning, and other protocol elements. This is a known intrinsically unsolvable problem in all design methodologies: you can agree on standard and reusable \"plumbing\" (lower hardware/software/network layers), but that doesn't automatically get you alignment on the detailed semantics of even a trivial command like ChangeVolume(value) or the format of a media stream.\n\nPractical approaches thus call for a mix of several approaches:\n\nThe Membership Agreement of the MIPI Alliance specifies the licensing conditions for MIPI specifications for member companies. Royalty-free licensing conditions apply within the main target domain of the MIPI Alliance, mobile phones and their peripherals, whereas RAND licensing conditions apply in all other domains.\n\n\n"}
{"id": "7854405", "url": "https://en.wikipedia.org/wiki?curid=7854405", "title": "Universal Powerline Association", "text": "Universal Powerline Association\n\nThe Universal Powerline Association (UPA) was a trade association that covered power line communication (PLC) markets and applications. The UPA promoted and certified power line communication technology from 2004 to 2010.\n\nAn interest group for the UPA was established by the founding members in May 2004. A memorandum of understanding was signed by the founding members in September 2004, and the UPA was officially announced in January 2005. It was a non-profit trade group registered in Belgium.\nDesign of Systems on Silicon (DS2), a supplier of integrated circuits and software for PLC, was among the founding members; UPA systems were based on its technology.\nOther members included: AcBel, Ambient Corporation, bpl, Corinex Communications, IBEC, Netgear, PCN Technology, Toshiba, Toyo Network Systems, and Junaid.\n\nThe Universal Powerline Association released specifications related to three aspects of powerline technology. \nThe UPA coexistence specification was published in June 2005.\nThe UPA Access specification (the European OPERA project endorsed specification for Internet access over power lines) was published in February 2006.\nIn-home systems and solutions, called \"triple play\" scenarios, were published in February 2006 as the UPA Digital Home Specification v1.0.\nThe UPA worked with international standardisation bodies such as IEEE and ETSI.\nCertification events (known as Plugtests) were held in January 2006 and 2007, with products demonstrated at the Consumer Electronics Show.\n\nDS2 had financial problems in 2010.\nMarvell Technology Group acquired the intellectual property of DS2 in August 2011, although it continued to provide UPA specifications compliant devices. With the exit of DS2 from the market, the UPA suspended activities in November 2010 and discontinued its website, although the UPA market continued through Marvell and its partners.\n\n"}
{"id": "25218908", "url": "https://en.wikipedia.org/wiki?curid=25218908", "title": "VAT Information Exchange System", "text": "VAT Information Exchange System\n\nThe VAT Information Exchange System (VIES) is an electronic means of transmitting information relating to VAT registration (i.e., validity of VAT numbers) of companies registered in the European Union. EU law requires that, where goods or services are procured within the EU, VAT must be paid only in the member state where the purchaser resides. For this reason, suppliers need an easy way to validate the VAT numbers presented by purchasers. This validation is performed through VIES.\n\nVIES does not itself maintain a VAT number database. Instead, it forwards the VAT number validation query to the database of the member state concerned and, upon reply, it transmits back to the inquirer the information provided by the member state. This information includes at least a \"YES/NO\" answer on the existence and validity of the supplied number. It may also include additional information, such as the holder's name and address, if this is provided by the member state.\n\nVIES optionally provides a unique reference number which can be used to prove to a tax authority that a particular VAT number was confirmed at time of purchase.\n\nGermany, Italy and Spain provide similar services at national level.\n\n"}
{"id": "25784888", "url": "https://en.wikipedia.org/wiki?curid=25784888", "title": "Veena Rawat", "text": "Veena Rawat\n\nVeena Rawat is an electrical engineer who specializes in telecommunications. In 2014 she was awarded the Officer of the Order of Canada (OC) by the Governor General of Canada, one of Canada's highest civilian awards, for her lifetime achievements and contributions at the national and international levels to wireless communications.\n\nBetween 2011-2013, Rawat was Vice President and Ambassador to the International Telecommunications Union, for Blackberry, Advanced Technology Division, Canada.\n\nUntil 2011, she was the President of the Communications Research Centre (CRC), an agency of Industry Canada that carries out research and development in communications technologies.\n\nIn 2010 Rawat was a candidate for the position of Director, Radiocommunication Bureau, International Telecommunication Union (ITU).\n\nRawat emigrated to Canada from India in 1968. She speaks English, French, Hindi and Spanish.\n\nIn 1973, Rawat was the first woman to graduate with a PhD in electrical engineering from Queen’s University in Kingston, Ontario, Canada.\n\nIn 1967 she graduated from Birla Institute of Technology and Science, Pilani, India, with M. Tech in Electronics.\n\nSince Jan 2013, Rawat has been providing advisory services in the capacity of an internationally acclaimed wireless communications expert to various global telecommunications companies and organizations.\n\nBetween 2011-2013, Rawat was Vice President and Ambassador to the International Telecommunications Union, for Blackberry, Advanced Technology Division, Canada. She was responsible for representing Blackberry at the ITU and various national and international fora at executive level in matters related to RF Spectrum planning, allocation, harmonization and coordination for wireless technologies and services.\n\nDuring 2004-2011, Rawat was President of the Communications Research Centre, Canada's centre of excellence for telecommunications R&D, with 400 staff and an annual budget of over $50 million. For over 40 years, CRC has made significant contributions to the information and communications technology sector in Canada and abroad. The CRC’s research encompasses the four main platforms for information delivery: terrestrial wireless, satellite, fibre optics and broadcasting. Rawat was responsible for Canada’s participation in bilateral and multilateral information and communications technologies (ICT) research partnerships with many countries around the world.\n\nDr. Rawat came to CRC after spending 28 years within the Canadian Government where she held executive positions managing programs related to radio frequency spectrum engineering for all wireless and space communication services. This included: leading negotiations at the International Telecommunication Union of United Nations (ITU), Organization of American States (OAS) and US Government (FCC, NTIA); chairing major national and international committees; and consultations with senior executives of the telecom and space industry at global level to develop policies and regulations. Rawat was the first woman to chair a World Radiocommunication Conference (WRC-03), for which she was awarded an ITU gold medal. Also, Rawat was chair of the ITU-R SG-4 for Satellite Services and chair of numerous working groups and technical committees at ITU-R and WRCs.\n\nAn internationally recognized expert in spectrum management and ICT technologies and trends, Rawat has been a keynote and invited speaker at over 100 domestic and international conferences and events since 1995.\n\n\n"}
{"id": "56007469", "url": "https://en.wikipedia.org/wiki?curid=56007469", "title": "Virus nanotechnology", "text": "Virus nanotechnology\n\nViruses consist of a genome and a capsid; and some viruses are enveloped. Most virus capsids measure between 20-500 nm in diameter. Because of their nanometer size dimensions, viruses have been considered as naturally occurring nanoparticles. Virus nanoparticles have been subject to the nanoscience and nanoengineering disciplines. Viruses can be regarded as prefabricated nanoparticles. Many different viruses have been studied for various applications in nanotechnology: for example, mammalian viruses are being developed as vectors for gene delivery, and bacteriophages and plant viruses have been used in drug delivery and imaging applications as well as in vaccines and immunotherapy intervention.\n\nVirus nanotechnology is one of the very promising and emerging disciplines in nanotechnology. A highly interdisciplinary field, viral nanotechnology occupies the interface between virology, biotechnology, chemistry, and materials science. The fields employs viral nanoparticles (VNPs) and its counterparts of virus-like nanoparticles (VLPs) for potential applications in the diverse fields of electronics, sensors, and most significantly at clinical field. VNPs and VLPs are attractive building blocks for several reasons. Both particles are on the nanometer-size scale; they are monodisperse with a high degree of symmetry and polyvalency; they can be produced with ease on large scale; they are exceptionally stable and robust, and they are biocompatible, and in some cases, orally bioavailable. They are \"programmable\" units that can be modified by either genetic modification or chemical bioconjugation methods.\n\nA virus is a biological infective agent that infect living hosts and reproduces inside the host cells. Viral infect all forms of life; can cause disease in humans, animals, plants and even insects. The science of studying viruses also called as Virology. Unlike most living things, viruses do not have cells that divide; new viruses are assembled in the infected host cells. Therefore, viruses are ultimately called as obligatory parasites. Viruses contain genetic materials (either DNA or RNA) which give them the ability to mutate and evolve. It also has a protective coat termed the capsid which is made up of multiple numbers of protein monomers called capsomer. Some viruses have additional structural features such as a lipid envelope over capsid or they may consist of separate head and tail structures. E.g. Bacteriophage. Every year novel viruses emerged with the potential to cause disease and death worldwide. Today, over 5000 species of viruses have been discovered and described.\n\nViruses spread in many ways. Just as many viruses are very specific as to which host species or tissue they attack, each species of virus relies on a particular method for propagation. Plant viruses are often spread from plant to plant by insects and other organisms, known as vectors. Some viruses of animals, including humans, are spread by exposure to infected bodily fluids. Viral infections can cause disease in humans, animals and even plants.\n\nNanotechnology is the manipulation or self-assembly of individual atoms, molecules, or, molecular clusters into structures to create materials and devices with new or vastly different properties. Nanotechnology can work from the top down (which means reducing the size of the smallest structures to the nanoscale) or bottom up (which involves manipulating individual atoms and molecules into nanostructures) .The definition of nanotechnology is based on the prefix \"nano\" which is from the Greek word meaning \"dwarf\". In more technical terms, the word \"nano\" means 10-9, or one billionth of something. For a meaningful comparison, a virus is roughly 100 nanometers (nm) in size. So that a virus can also call as a nanoparticle. The word nanotechnology is generally used when referring to materials with the size of 0.1 to 100 nanometres, however, it is also inherent that these materials should display different properties from bulk (or micrometric and larger) materials as a result of their size. These differences include physical strength, chemical reactivity, electrical conductance, magnetism and optical effects.\n\nNanotechnology has an almost limitless string of applications in biology, biotechnology, and biomedicine. Nanotechnology has engendered a growing sense of excitement due to the ability to produce and utilize materials, devices, and systems through the control of matter on the nanometer scale (1 to 50 nm). This bottom-up approach requires less material and causes less pollution. Nanotechnology has had several commercial applications in advanced laser technology, hard coatings, photography, pharmaceuticals, printing, chemical-mechanical polishing, and cosmetics. Soon, there will be lighter cars using nanoparticle reinforced polymers, orally applicable insulin, artificial joints made from nanoparticulate materials, and low-calorie foods with nanoparticulate taste enhancers.\n\nViruses have long been studied as deadly pathogens to cause disease in all living forms. By the 1950s, researchers had begun thinking of viruses as tools in addition of pathogens. Bacteriophage genomes and components of the protein expression machinery have been widely utilized as tools for understanding the fundamental cellular process. On the basis of these studies, several viruses have been exploited as expression systems in biotechnology. Later in the 1970s, viruses are used as a vector for the benefit of humans. Since that, often viruses are used as vectors for gene therapy, cancer control and control of harmful or damaging organisms, in both agriculture and medicine.\n\nRecently, a new approach to exploiting viruses and their capsids for biotechnology began to change toward using them for nanotechnology application. Researchers Douglas and Young (Montana State University, Bozeman, MT, USA) were the first to consider the utility of a virus capsid as a nanomaterial. They have taken plant virus Cowpea Chlorotic Mottle Virus (CCMV) for their study. CCMV showed a highly dynamic platform with pH and metal ion dependent structural transitions. Douglas and Young made use of these capsid dynamics and exchanged the natural cargo (nucleic acid) with synthetic materials. Since then many materials have been encapsulated into CCMV and other VNPs. At about the same time, the research team led by Mann (University of Bristol, UK) pioneered a new area using the rod-shaped particles of TMV (Tobacco Mosaic Virus). The particles were used as templates for the fabrication of a range of metallized nanotube structures using mineralization techniques. Following that invention, TMV particles have been utilized to generate various structures (nanotubes and nanowires) for use in batteries and data storage devices.\n\nViral capsids have attracted great interest in the field of nanobiology because of their nanoscale size, symmetrical structural organization, load capacity, controllable self-assembly, and ease of modification. viruses are essentially naturally occurring nanomaterials capable of self-assembly with a high degree of precision. Viral capsid- nanoparticle hybrid structures, which combine the bio-activities of virus capsids with the functions of nanoparticles, are a new class of bionanomaterials that have many potential applications as therapeutic and diagnostic vectors, imaging agents, and advanced nanomaterial synthesis reactors.\n\nPlant virus-based systems, in particular, are among the most advanced and exploited for their potential use as bioinspired structured nanomaterials and nano-vectors.Plant virus nanoparticles are non-infectious to mammalian cells also proved by Raja muthuramalingam et al 2018. Plant viruses have a size particularly suitable for nanoscale applications and can offer several advantages. In fact, they are structurally uniform, robust, biodegradable and easy to produce. Moreover, many are the examples regarding functionalization of plant virus-based nanoparticles by means of modification of their external surface and by loading cargo molecules into their internal cavity. This plasticity in terms of nanoparticles engineering is the ground on which multivalency, payload containment and targeted delivery can be fully exploited.\n\nGeorge P. Lomonossoff writing in \"Recent Advances in Plant Virology\",\nThe capsids of most plant viruses are simple and robust structures consisting of multiple copies of one or a few types of protein subunit arranged with either icosahedral or helical symmetry. The capsids can be produced in large quantities either by the infection of plants or by the expression of the subunit(s) in a variety of heterologous systems. In view of their relative simplicity and ease of production, plant virus particles or virus-like particles (VLPs) have attracted much interest over the past 20 years for applications in both bio- and nanotechnology [Lomonossoff, 2011]. As result, plant virus particles have been subjected to both genetic and chemical modification, have been used to encapsulate foreign material and have themselves, been incorporated into supramolecular structures. Significantly, plant viruses studied are not human pathogens, which have no natural tendency to interact with human cell surface receptors. Recently, a plant pathogenic virus are reported to used to synthesize a noble hybrid metal nanomaterials used as biosemiconductor.\n\nViruses cause several destructive plant diseases and are accountable for massive losses in crop production and quality in all parts of the world. Infected plants may show a range of symptoms depending on the disease but often there is severe leaf curling, stunting (abnormalities in the whole plant) and leaf yellowing (either of the whole leaf or in a pattern of stripes or blotches). Most plant viruses are therefore transmitted by a vector organism (insects, nematodes, plasmodiophorids and mites) that feeds on the plant or (in some diseases) are introduced through wounds made, for example during agriculture practices (e.g. pruning). Many plant viruses, for example, tobacco mosaic virus, have been used as model systems to provide a basic understanding of how viruses express genes and replicate. Others permitted the elucidation of the processes underlying RNA silencing, now recognised as a core epigenetic mechanism underpinning numerous areas of biology.\n\n\nManifold plant virus platform technologies are being developed and studied for many applications including:\n"}
{"id": "11456403", "url": "https://en.wikipedia.org/wiki?curid=11456403", "title": "Walter F. Buckley", "text": "Walter F. Buckley\n\nWalter Frederick Buckley (1922 – January 26, 2006) was an American sociologist, and professor of sociology at the University of New Hampshire. Buckley was among the first to apply concepts from general systems theory based on the work of Bertalanffy to sociology\n\nBorn and raised in Lynn, Massachusetts, Buckley studied sociology at the University of Wisconsin–Madison. In 1958 he received his Ph.D. with the doctoral dissertation \"Sociological theory and social stratification\", in which he outlined a non-functionalist theory of social stratification.\n\nBuckley started his academic career early 1960s as Assistant Professor of Sociology at the University of California, Santa Barbara in the department of sociology, which was chaired by David Gold PhD. From 1971 to 1985 he was professor of Sociology at the University of New Hampshire. In the 1970s he participated in the Uppsala Theory Circle at the Uppsala University in Sweden founded by Tom R. Burns. In 1998 he was awarded the honorary chair of the Socio-Cybernetics Research Committee of the International Sociological Association.\n\nBuckley has been described as a pioneer in social systems theory that challenged conventional views. In his personal life he appreciated jazz music and played tenor saxophone. He died in 2006 in Durham, New Hampshire.\nBooks, a selection: \n"}
{"id": "37161156", "url": "https://en.wikipedia.org/wiki?curid=37161156", "title": "Young Rewired State", "text": "Young Rewired State\n\nYoung Rewired State (often styled as YRS) was an organisation based in the United Kingdom, which ran events and schemes for technically gifted young people aged 18 and under. It brought together young developers, designers, and those with other technical skills to build projects (mainly phone and web applications) to attempt to solve real world problems. Most of the developers participating in Young Rewired State events taught themselves or learned coding skills outside the traditional school curriculum. \n\nYoung Rewired State was initially run as an annual event by its sister organisation Rewired State, and subsequently became its own separate entity. The former Managing Director of Young Rewired State was Ruth Nicholls, a law graduate from Cambridge University, specialising in criminal law, penal policy and human rights. The organisation is currently dormant.\n\nBetween 2009 and 2015, Young Rewired State held a national hackathon where attendees across the UK took part in a competition to make an application including at least one piece of open government data. This event was initially called \"Young Rewired State\", but was renamed in 2012 to the \"Festival of Code\". Young Rewired State also ran their \"Hyperlocal\" scheme from October 2014, which aimed to re-open the local Festival centres to provide support for local young \"digital makers\" throughout the year.\n\nAccording to an article by Emma Mulqueeny, one of the founders of Young Rewired State, around 5% of the participants are female as of the 2012 festival, although this rose to over 30% by March 2015.\n\nThe Festival of Code that would have occurred in 2016 was initially postponed until 2017, but ultimately did not occur.\n\nThroughout the year, Young Rewired State ran various \"Hyperlocal\" centres across the UK, which provide coding challenges across the year, rather than being focused on one week as the Festival of Code is. Hyperlocal generally had fewer centres operating than the Festival of Code, with 21 operating as of 25 July 2015, although it did have three operating outside of the UK.\n\n\nFestival of Code\n"}
