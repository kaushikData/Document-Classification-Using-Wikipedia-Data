{"id": "23242689", "url": "https://en.wikipedia.org/wiki?curid=23242689", "title": "AC-to-AC converter", "text": "AC-to-AC converter\n\nA solid-state AC-to-AC converter converts an AC waveform to another AC waveform, where the output voltage and frequency can be set arbitrarily.\n\nReferring to Fig 1, AC-AC converters can be categorized as follows: \n\nThere are two types of converters with DC link:\nAny dynamic braking operation required for the motor can be realized by means of braking DC chopper and resistor shunt connected across the rectifier. Alternatively, an anti-parallel thyristor bridge must be provided in the rectifier section to feed energy back into the AC line. Such phase-controlled thyristor-based rectifiers however have higher AC line distortion and lower power factor at low load than diode-based rectifiers.\n\nAn AC-AC converter with approximately sinusoidal input currents and bidirectional power flow can be realized by coupling a pulse-width modulation (PWM) rectifier and a PWM inverter to the DC-link. The DC-link quantity is then impressed by an energy storage element that is common to both stages, which is a capacitor C for the voltage DC-link or an inductor L for the current DC-link. The PWM rectifier is controlled in a way that a sinusoidal AC line current is drawn, which is in phase or anti-phase (for energy feedback) with the corresponding AC line phase voltage.\n\nDue to the DC-link storage element, there is the advantage that both converter stages are to a large extent decoupled for control purposes. Furthermore, a constant, AC line independent input quantity exists for the PWM inverter stage, which results in high utilization of the converter’s power capability. On the other hand, the DC-link energy storage element has a relatively large physical volume, and when electrolytic capacitors are used, in the case of a voltage DC-link, there is potentially a reduced system lifetime.\n\nA cycloconverter constructs an output, variable-frequency, approximately sinusoid waveform by switching segments of the input waveform to the output; there is no intermediate DC link. With switching elements such as SCRs, the output frequency must be lower than the input. Very large cycloconverters (on the order of 10 MW) are manufactured for compressor and wind-tunnel drives, or for variable-speed applications such as cement kilns.\n\nIn order to achieve higher power density and reliability, it makes sense to consider Matrix Converters that achieve three-phase AC-AC conversion without any intermediate energy storage element. Conventional Direct Matrix Converters (Fig. 4) perform voltage and current conversion in one single stage.\n\nThere is the alternative option of indirect energy conversion by employing the Indirect Matrix Converter (Fig. 5) or the Sparse matrix converter which was invented by Prof. Johann W. Kolar from the ETH Zurich. As with the DC-link based VSI and CSI controllers (Fig. 2 and Fig. 3), separate stages are provided for voltage and current conversion, but the DC-link has no intermediate storage element. Generally, by employing matrix converters, the storage element in the DC-link is eliminated at the cost of a larger number of semiconductors. Matrix converters are often seen as a future concept for variable speed drives technology, but despite intensive research over the decades they have until now only achieved low industrial penetration. However, citing recent availability of low-cost, high performance semiconductors, one larger drive manufacturer has over past few years been actively promoting matrix converters.\n\n"}
{"id": "13808007", "url": "https://en.wikipedia.org/wiki?curid=13808007", "title": "Acoustic rheometer", "text": "Acoustic rheometer\n\nAn acoustic rheometer employs a piezo-electric crystal that can easily launch a successive wave of extensions and contractions into the fluid. It applies an oscillating extensional stress to the system. System response can be interpreted in terms of extensional rheology.\n\nIt is well known that properties of viscoelastic fluid are characterised in shear rheology with a shear modulus \"G\", which links shear stress \"T\" and shear strain \"S\"\n\nThere is similar linear relationship in extensional rheology between extensional stress \"P\", extensional strain \"S\" and extensional modulus \"K\":\n\nDetail theoretical analysis indicates that propagation of sound or ultrasound through a viscoelastic fluid depends on both shear modulus \"G\" and extensional modulus \"K\". It is convenient to introduce a combined longitudinal modulus \"M\":\n\nThere are simple equations that express longitudinal modulus in terms of acoustic properties, sound speed \"V\" and attenuation α\n\nAcoustic rheometer measures sound speed and attenuation of ultrasound for a set of frequencies in the megahertz range. These measurable parameters can be converted into real and imaginary components of \"longitudinal modulus\". \n\nThis type of rheometer works at much higher frequencies than others. It is suitable for studying effects with much shorter relaxation times than any other rheometer.\n\n"}
{"id": "50603900", "url": "https://en.wikipedia.org/wiki?curid=50603900", "title": "Ambio 4", "text": "Ambio 4\n\nAmbio 4 was quadrophonic sound technology commercialised in the early 1970s that could reproduce the ambience or sound information of a room as well as play stereo. Ambiophony was an extension of stereo reproduction to enhance the sense of realism and it could be used with nearly all stereo programme materials.\n\nThe technology was included in receivers, amplifiers and music centres from manufacturers including Philips, Ferguson Electronics and Bang and Olufsen alongside mono and stereo playback. The electronics behind Ambiophony was based on, or similar to, the Hafler circuit.\n"}
{"id": "4558426", "url": "https://en.wikipedia.org/wiki?curid=4558426", "title": "Austrian Standards Institute", "text": "Austrian Standards Institute\n\nThe Austrian Standards Institute (formerly ), abbreviated ASI, is a standards organization and the ISO member body for Austria. \n\nIts predecessor organization, the \"Österreichischer Normenausschuss für Industrie und Gewerbe\" (Austrian Standards Committee for Industry and Trade), was founded in the First Republic of Austria on 23 September 1920, with 13 committees developing technical standards primarily for mechanical and electrical engineering. The first standard was published in 1921 on metric screw threads. In 1932, the committee's name was shortened to the \"Österreichischer Normenausschuss\" (Austrian Standards Committee). With the Austrian \"Anschluss\" to Nazi Germany in 1938, it became a branch office of the German \"Deutsches Institut für Normung\" (DIN) standards organization, but resumed operations in its own right after World War II, and was a founding ISO member in 1946.\n\nThe 1954 Federal Act on Standardization recognized the activities of the committee, and the Act, as amended, serves as its legal basis. The Austrian Standards Committee's name was changed to the Austrian Standards Institute in 1969.\n\nOn 1 March 2006, the Institute published ON rule ONR 168000, used to calculate the monetary value of a brand.\n\nThe Institute is located at Heinestrasse 38, 1020 Vienna.\n\n"}
{"id": "58293964", "url": "https://en.wikipedia.org/wiki?curid=58293964", "title": "Battlefield Combat Identification System", "text": "Battlefield Combat Identification System\n\nThe Battlefield Combat Identification System (BCIS) was a question and answer system, in which military combat vehicles were able to communicate via a 38-GHz electronic millimeter wave pulse. The platform was mounted on Abrams Tanks and Bradley Fighting Vehicles. BCIS was intended to reduce fratricide, which became a concern during the Persian Gulf War.\n\nThe development of BCIS began after the US Army awarded a contract to TRW Inc. of Cleveland in 1993. 1,200 systems were fielded to the 4th Infantry Division at Fort Hood, TX in June 1999. Production of the system ended in 2003 due to the platform’s expense, which was estimated at $100,000 for each installation.\n\nFratricide concerns began during the Persian Gulf War, when 35 American troops were killed by friendly fire. During this period, soldiers often used low-tech methods to communicate, such as painting inverted \"Vs\" on allied vehicles or waving red-lensed flashlights. Army requirements for a combat identification system were recognized by the DoD’s Joint Requirements Oversight Council in March 1992.\n\nCombat vehicles carried BCIS transmitters and receivers that sent pulses of energy to one another if the vehicles engaged. The encrypted signal identified vehicles as a friend or foe.\n\nDuring operation, a millimeter wave beam was transmitted from the gunner’s station, interrogating a potential target prior to firing.  A BCIS transponder automatically responded with a message, provided in the form of audio and visual signals to the interrogating gunner (i.e. Friend, Friend-at-Range, or Unknown). The BCIS platform included an interrogator subassembly, a transponder subassembly, an antenna, a processor, display unit, and sight ring indicators.\n\nIn 1995, Army Research Laboratory scientists conducted a study to improve the auditory signals of BCIS, changing notifications from pure tones to auditory icons, such as a doorbell for engaged allied vehicles. The study focused on ergonomic design of signals for quicker user guidance during emergency conditions.\n"}
{"id": "45342362", "url": "https://en.wikipedia.org/wiki?curid=45342362", "title": "Butler oscillator", "text": "Butler oscillator\n\nThe Butler oscillator is a crystal-controlled oscillator that uses the crystal near its series resonance point.\n\n\n\n"}
{"id": "3441969", "url": "https://en.wikipedia.org/wiki?curid=3441969", "title": "Christie (company)", "text": "Christie (company)\n\nThe Christie group of companies manufactures DLP projectors and various digital cinema devices, and offers a selection of LCD projectors, line array audio solutions, and collaboration and presentation solutions which are used in various settings. The Christie group includes Christie Digital Systems USA, Inc.; Christie Digital Systems Canada Inc.; Christie Medical Holdings, Inc.; Vista Systems Control, Corp.; Event Audio Visual Group, Inc.; Christie Digital Systems South America Ltda. (Brazil); Christie Digital Systems Mexico S. de R.L. de C.V. (Mexico); Christie Digital Systems (India); and Christie Digital Systems Australia Pty Ltd. (Queensland, Australia). They are all part of the Ushio group of companies, the ultimate parent of which is Ushio Inc. (Japanese: ), a publicly traded Japanese company.\n\nChristie Corporate Headquarters are located in Cypress (Orange County), California, United States, while its Research & Innovation (R&I), engineering teams are located in Kitchener, Ontario, Canada. In addition to the Kitchener facility, Christie has a second manufacturing facility in Shenzhen, China. Christie has over 1,500 employees worldwide.\n\nChristie was founded in 1929 and began as a manufacturer of 35mm film movie projectors, lamphouses, lamp consoles and film platter systems. Later, it began importing Japanese-made Xenon bulbs before acquiring the Kitchener, Ontario-based digital projection business of Electrohome Limited in 1999.\n\n"}
{"id": "1527098", "url": "https://en.wikipedia.org/wiki?curid=1527098", "title": "Clairaut's theorem", "text": "Clairaut's theorem\n\nClairaut's theorem is a general mathematical law giving the surface gravity on a viscous rotating ellipsoid in equilibrium under the action of its gravitational field and centrifugal force. It was published in 1743 by Alexis Claude Clairaut in his \"Théorie de la figure de la terre, tirée des principes de l'hydrostatique\" (\"Theory of the shape of the earth, drawn from the principles of hydrostatics\") which synthesized physical and geodetic evidence that the Earth is an oblate rotational ellipsoid. It was initially used to relate the gravity at any point on the Earth's surface to the position of that point, allowing the ellipticity of the Earth to be calculated from measurements of gravity at different latitudes. Today it has been largely supplanted by the Somigliana equation.\n\nAlthough it had been known since antiquity that the Earth was spherical, by the 17th century evidence was accumulating that it was not a perfect sphere. In 1672 Jean Richer found the first evidence that gravity was not constant over the Earth (as it would be if the Earth were a sphere); he took a pendulum clock to Cayenne, French Guiana and found that it lost minutes per day compared to its rate at Paris. This indicated the acceleration of gravity was less at Cayenne than at Paris. Pendulum gravimeters began to be taken on voyages to remote parts of the world, and it was slowly discovered that gravity increases smoothly with increasing latitude, gravitational acceleration being about 0.5% greater at the poles than at the equator.\n\nBritish physicist Isaac Newton explained this in his \"Principia Mathematica\" (1687) in which he outlined his theory and calculations on the shape of the Earth. Newton theorized correctly that the Earth was not precisely a sphere but had an oblate ellipsoidal shape, slightly flattened at the poles due to the centrifugal force of its rotation. Since the surface of the Earth is closer to its center at the poles than at the equator, gravity is stronger there. Using geometric calculations, he gave a concrete argument as to the hypothetical ellipsoid shape of the Earth.\n\nThe goal of \"Principia\" was not to provide exact answer for natural phenomena, but to theorize potential solutions to these unresolved factors in science. Newton pushed for scientists to look further into the unexplained variables. Two prominent researchers that he inspired were Alexis Clairaut and Pierre Louis Maupertuis. They both sought to prove the validity of Newton's theory on the shape of the Earth. In order to do so, they went on an expedition to Lapland in an attempt to accurately measure the meridian arc. From such measurements they could calculate the eccentricity of the Earth, its degree of departure from a perfect sphere. Clairaut confirmed that Newton's theory that the Earth was ellipsoidal was correct, but his calculations were in error, and wrote a letter to the Royal Society of London with his findings. The society published an article in Philosophical Transactions the following year in 1737 that revealed his discovery. Clairaut showed how Newton's equations were incorrect, and did not prove an ellipsoid shape to the Earth. However, he corrected problems with the theory, that in effect would prove Newton's theory correct. Clairaut believed that Newton had reasons for choosing the shape that he did, but he did not support it in \"Principia.\" Clairaut's article did not provide an valid equation to back up his argument as well. This created much controversy in the scientific community.\n\nIt was not until Clairaut wrote \"Théorie de la figure de la terre\" in 1743 that a proper answer was provided. In it, he promulgated what is more formally known today as Clairaut's theorem.\n\nClairaut's formula for the acceleration due to gravity \"g\" on the surface of a spheroid at latitude φ, was:\n\nwhere formula_2 is the value of the acceleration of gravity at the equator, \"m\" the ratio of the centrifugal force to gravity at the equator, and \"f\" the flattening of a meridian section of the earth, defined as:\n(where \"a\" = semimajor axis, \"b\" = semiminor axis).\n\nClairaut derived the formula under the assumption that the body was composed of concentric coaxial spheroidal layers of constant density. \nThis work was subsequently pursued by Laplace, who relaxed the initial assumption that surfaces of equal density were spheroids.\nStokes showed in 1849 that the theorem applied to any law of density so long as the external surface is a spheroid of equilibrium. A history of the subject, and more detailed equations for \"g\" can be found in Khan.\n\nThe above expression for \"g\" has been supplanted by the Somigliana equation (after Carlo Somigliana):\n\nwhere,\n\nFor Earth, formula_2 = 9.7803253359 ms; formula_9 = 9.8321849378 ms; \"k\" = 0.00193185265241 ; \"e\" = 0.00669437999013: \n\nThe spheroidal shape of the Earth is the result of the interplay between gravity and centrifugal force caused by the Earth's rotation about its axis. In his \"Principia\", Newton proposed the equilibrium shape of a homogeneous rotating Earth was a rotational ellipsoid with a flattening \"f\" given by 1/230. As a result, gravity increases from the equator to the poles. By applying Clairaut's theorem, Laplace found from 15 gravity values that \"f\" = 1/330. A modern estimate is 1/298.25642. See Figure of the Earth for more detail.\n\nFor a detailed account of the construction of the reference Earth model of geodesy, see Chatfield.\n"}
{"id": "17181237", "url": "https://en.wikipedia.org/wiki?curid=17181237", "title": "Compressed air filters", "text": "Compressed air filters\n\nCompressed air filters, often referred to as line filters, are used to remove contaminates from compressed air after compression has taken place.\n\nWhen the filer is combined with a regulator and an oiler, it is called an air set.\n\nAir leaving a standard screw or piston compressor will generally have a high water content, as well as a high concentration of oil and other contaminants. There are many different types of filters, suitable for different pneumatics applications.\n\nUnfiltered compressed air frequently contains dust, oil, rust, moisture and other harmful substances, and therefore requires filtration. In the first stage of filtration, the compressed air passes through a tube-shaped mesh filter, which creates a coalescence effect. Here bigger particles are adsorbed on the filter and the water will condense into larger droplets, which can then pass into the separation chamber. The compressed air is slowed down, which makes the particles condense on a honeycomb-like pad, allowing the water droplets to travel to the bottom of the drainage system and through an automatic or electric drain valve to the discharge. In the first filtration stage more than 95% of the water droplets, oil and large particles are removed. This practice is most common for removing water, but is also used for removing oil.\n\nIn the second filtration stage the air is passed through fiber made of cotton, generating thousands of small vortices and accelerating the air. \n\nParticulate compressed air filters are used to remove dust and particles from the air.\n\nActivated carbon filters utilize a composite carbon material to remove gases and odors from the air. They are used in factories where food is produced or for breathing gas.\n\nHigh oil compressed air coalescing filters remove water and oil aerosols by coalescing the aerosols into droplets. This happens partially because of tortuous path and pressure drop. Coalescers remove both water and oil aerosols from the air stream, and are rated at particulate contamination through direct interception. Filtration of oil, water aerosols, dust and dirt particles to 0.01 µm the best achievable in industry.\n\nCold coalescing filters are coalescing filters operated at around , allowing them to be more effective at removing moisture.\n\nIntake filters are the first line of defense in filtering. These filters can remove contaminates down to 0.3 µm and can remove chemical contaminants.\n\n\n"}
{"id": "27899041", "url": "https://en.wikipedia.org/wiki?curid=27899041", "title": "Contouring", "text": "Contouring\n\nContouring is a makeup technique that uses cosmetics to define, enhance and sculpt the structure of the face or other body parts, such as breasts.\n\nContouring is usually produced by placing a warm or cool toned colour that is one or two shades darker than the skin color in areas such as in the hollows of the cheeks, on the side of the nose, and on the temples to give a shadow and slimming effect. It can be complemented with a highlighter that is one or two shades lighter than the skin color on areas of the face that is more prominent such as on the apples on the cheeks and the tip of the nose or the t-zone.\n\nIn the 16th century, contouring was used by Elizabethan stage actors, who would apply chalk and soot to their faces to help audience members read their facial expressions.\n\nIn the late 1800s, when electricity was invented and lights were widely used, soot was no longer an option. Instead of soot, actors would use greasepaint to help audience members decipher their emotions. In 1800s-1900s, Queen Victoria deemed makeup as vulgar, as only stage actors and prostitutes wore makeup. Makeup could only be purchased in costume stores.\n\nIn the 1920s and 30s, contouring could be seen in the film world. German actress, Marlene Dietrich would contour her face for her films. She would accentuate the natural lines of her face with shading and sculpting.\n\nIn 1934, makeup artist Max Factor Sr. was famous for applying makeup for stage actors. He added shading to the face so that it wouldn't appear flat on film. In 1945, he released the first tutorial on how to contour the face, for different face shapes.\n\nIn 1944, Ben Nye, a famous makeup artist, did the makeup for characters in Gone with the Wind (film) and Planet of the Apes (1968 film). He then created his own makeup line, which is still popular today.\n\nIn the 1950s, a time of Old Hollywood glamour, features were subtly contoured and shaded. This method was used by such actresses as Audrey Hepburn, Marilyn Monroe, and Elizabeth Taylor.\n\nIn the 1990s, makeup artist Kevyn Aucoin was responsible for the sculpted, chiseled look of Gwyneth Paltrow, Cindy Crawford, and Janet Jackson.\n\nIn the 2000s, the practice of \"body contouring\" – the application of contouring to other parts of the body than the face, such as shinbones or breasts (\"boob contouring\") – became more widely noticed as a result of the increasing number of images of celebrities appearing in social media.\n"}
{"id": "33897723", "url": "https://en.wikipedia.org/wiki?curid=33897723", "title": "Coordinatograph", "text": "Coordinatograph\n\nA coordinatograph is an instrument which mechanically plots X and Y coordinates onto a surface, such as in compiling maps or in plotting control points such as in electronic circuit design.\n\nOne historic application of a coordinatograph was a machine that precisely placed and cut rubylith to create photomasks for early integrated circuits including some of the earliest generations of the modern PC microprocessor. The coordinatograph produced layout would then be photographically reduced 100:1 to create the production photomask.\n\n\n"}
{"id": "1191600", "url": "https://en.wikipedia.org/wiki?curid=1191600", "title": "Document processing", "text": "Document processing\n\nDocument processing involves the conversion of typed and handwritten text on paper-based & electronic documents (e.g., scanned image of a document) into electronic information utilising one of, or a combination of, intelligent character recognition (ICR), optical character recognition (OCR) and experienced data entry clerks.\n\n"}
{"id": "2376008", "url": "https://en.wikipedia.org/wiki?curid=2376008", "title": "Dwang", "text": "Dwang\n\nIn construction, a nogging or nogging piece (England, Australia), dwang (Scotland Central and lower North Island, New Zealand and South Island, New Zealand,), blocking (North America), noggin (Greater Auckland region North Island, New Zealand), or nogs (New Zealand and Australia ) are horizontal bracing pieces used between wall studs or floor joists to give rigidity to the wall or floor frames of a building. Noggings may be made of timber, steel, or aluminium. If made of timber they are cut slightly longer than the space they fit into, and are driven into place so they fit tightly or are rebated into the wall stud.\n\nThe interval between noggings is dictated by local building codes and by the type of timber used; a typical timber-framed house in a non-cyclonic area will have two or three noggings per storey between each pair of neighbouring studs. Additional noggings may be added as grounds for later fixings.\n\nNoggings between vertical studs generally brace the studs against buckling under load; noggings on floor joists prevent the joists from twisting or rotating under load (lateral-torsional buckling), and are often fixed at intervals, in pairs diagonally for that reason. In floors this type of bracing is also called herringbone strutting.\n\nNoggings provide no bracing effect in shear and are generally supplemented by diagonal bracing to prevent the frame from racking.\n\n"}
{"id": "24150036", "url": "https://en.wikipedia.org/wiki?curid=24150036", "title": "Economic Times Awards", "text": "Economic Times Awards\n\nThe Economic Times Awards for Corporate Excellence (also referred as ET Awards) are the awards conferred by The Economic Times in the field of business, corporate and government policies, economies in India. It is an annual awards, conferred in various categories. Since 2017, Deloitte Touche Tohmatsu India LLP (Deloitte) has been the title sponsor of the awards. The 2018 leg of the awards will be held on 22 November in Mumbai.\n\n\n\n\n\n\n\n\n\n\nDeloitte was the title sponsor for the event, held on 28 October 2017 in Mumbai.\nET AWARDS FOR CORPORATE EXCELLENCE\nThe Economic Times Awards for Corporate Excellence\n\n\nThe Economic Times Awards for Corporate Excellence\nThe 2018 leg of the awards will be held on 17 November 2018 in Mumbai. Deloitte continues to be the title sponsor of the event.\n\nThe jury for the awards met on 4 September 2018 in Mumbai. It comprised:\n\nThe jury members also participated in a panel discussion on the topic, “Spurring equitable growth: role of government, technology, and business”. They were joined in the panel discussion by N. Venkatram, Managing Partner and CEO, Deloitte India.\n\n2018 award winners:\n\n"}
{"id": "2023084", "url": "https://en.wikipedia.org/wiki?curid=2023084", "title": "Electric pen", "text": "Electric pen\n\nThomas Edison's electric pen, part of a complete outfit for duplicating handwritten documents and drawings, was the first relatively safe electric-motor-driven office appliance produced and sold in the United States.\n\nEdison recognized the possible demand for a high speed copying device after observing the incredible amount of document duplication required of merchants, lawyers, insurance companies, and those of similar occupations. To satisfy this demand, Edison invented the electric pen, which uses a perforating function inspired by the printing telegraph. Edison and his associate Charles Batchelor observed that as this device punctured the paper, a mark was left underneath by the chemical solution it utilized. Edison took advantage of this property and built the electric pen around it.\nDevelopment of the electric pen took place in the summer of 1875. US patent 1800,857 for autographic printing was issued to Thomas Edison in 1876, covering the pen, the duplication press, and accessories.\n\nThe electric pen was the key component of a complete duplicating system, which included the pen, a cast-iron holder with a wooden insert, a wet cell battery on a cast-iron stand, and a cast-iron flatbed duplicating press with ink roller. All the cast-iron parts were black japanned, with gold striping or decoration. The hand-held electric pen was powered by a wet cell battery, which was wired to an electric motor mounted on top of a pen-like shaft. The motor drove a reciprocating needle which, according to the manual, could make 50 punctures per second, or 3,000 per minute. The user was instructed to place the stencil on firm blotting paper on a flat surface, then use the pen to write or draw naturally to form words and designs as a series of minute perforations in the stencil.\n\nOnce the stencil was prepared, it was placed in the flatbed duplicating press with a blank sheet of paper below. An inked roller was passed over the stencil, leaving an impression of the image on the paper. Edison boasted that over 5,000 copies could be made from one stencil.\n\nEdison’s main target audience included firms that depended on the duplication of documents to run their business. To drive demand, Edison advertised in a circular that was written by the pen itself, in which the pen was called “the “Electro-Autographic Press” and was said to be “the only process yet invented whereby an unlimited number of impressions can be taken with rapidity from ordinary manuscript.” Another advertisement made by the pen read “Like Kissing--Every Succeeding Impression is as Good as the First--Endorsed By Every One Who Has Tried It!--Only a Gentle Pressure Used.” with the words floating around an embracing couple.\n\nAside from companies, the electric pen was also marketed to the general public, with other uses for the invention being personal letters, pamphlets, music, contracts, circulars, and architectural and mechanical drawings among other types of documents. In late 1875, the pen was at first sold only in the East Coast of the United States at the starting price of $30. It was further spread to the Midwest, British Columbia, and England after its rise in popularity, when more than 150 pens were being sold monthly. The market continued to expand to Cuba and South America, with Europe and Asia being added by 1877. However, by 1880, the business for the electric pen started to decline when other inventions that were more efficient soon overtook Edison’s product in the market, causing it to eventually fall into obscurity. It is said that roughly 60,000 pens were sold throughout its commercial lifespan in total; however, this number is likely to be made up by Edison in order to give the product more publicity.\n\nThe major drawback to Edison’s electric pen was its wet cell battery, which had to be taken care of and maintained by experienced telegraphists. Due to its messy nature, it was important for Edison to incorporate batteries that were more acceptable to clerks who had to take care of the pen and its underlying machinery. Otherwise, the bankers and insurance people may never take interest in it, as said by Mullarkey, an ex-telegraph operator and New York agent for Edison.\n\nThe need for batteries in the electric pen ultimately caused its steady decline, as mechanical pens that did not require batteries to operate took over the market by 1880. These pens, along with other cheaper and simpler stencil-making technologies quickly became more popular and widely used, until all were eventually overtaken by the typewriter by the late 1880s.\n\nEdison started selling the rights to manufacture and market the pens as early as the end of 1876, but it was not until the mid-1880s that the A.B. Dick Company finally ended up with the rights and patent to the invention. The Chicago manufacturer went on to create the mimeograph, an electric pen spin-off marketed specifically as \"Edison’s Mimeograph\" under his permission. Unlike the electric pen, the mimeograph sold with relative success, and the A.B. Dick Company remained in business until 2004.\n\nAfter its life as a writing implement had ended, the electric pen was adapted to fulfill an entirely different role. In 1891, a New York City tattoo artist Samuel O’Reilly repurposed the electric pen’s design to be used as the first electric tattoo needle. What was previously done by hand was now done at a much faster pace thanks to this revolutionary device. Around this time, tattoos were starting to rise as a cultural phenomenon thanks to their popularity among European nobility. O’Reilly took advantage of this and produced an electric tattoo needle to give him the edge in this new market. While O’Reilly enjoyed considerable success in this, his abrupt death in 1908 cut his lucrative life short. O’Reilly’s apprentice Charles Wagner inherited the business from his master.\n\nAn October 2015 episode of \"History\" program \"American Pickers\" finds a European version Electric Pen in a private Wisconsin collection of early electric devices. The owner says recent auctions have seen other examples sell for between $15,000 and $20,000 USD This particular Electric Pen also includes a very rare battery box. The owner sells the Pen to the Pickers for $12,000, which they expect to resell at a higher price.\n\n\n"}
{"id": "10943394", "url": "https://en.wikipedia.org/wiki?curid=10943394", "title": "Electronics Technician distance education program", "text": "Electronics Technician distance education program\n\nThe Electronics Technician (ET) Distance Education program provides flexible, skills-based training in electronics. It has been developed for adult learners pursuing electronics technician-level training through independent study, specifically students who cannot attend college full-time because of work or family commitments. The program was developed and launched in 1997 by Dr. Colin Simpson, a best-selling author and electronics professor at George Brown College in Toronto, Ontario, Canada. Since then, the award-winning program has grown to become the largest of its kind in the world.\n\nWith over 10,000 students studying electronics at a distance, the ET distance education program has effectively broken down the barriers that prevent students from accessing technical course material on-line. Of note is that the program has broken the gender barrier in the study of electronics. Typically, less than 2% of students who study electronics in Colleges and Universities are female. In the ET distance education program almost 20% of the student’s are female, which has been attributed to the accessibility of the learning material and the integrative multimedia courseware which is designed to scaffold student learning and accommodate learning style differences.\n\nIn 1995, Simpson approached Joe Koenig, President of Electronics Workbench (EWB), with the concept of integrating course material from Simpson’s Principles of Electronics textbook with laboratory simulation software developed by EWB. The Learning Management System was developed by Logic Design Inc which integrated the course material, multimedia and simulation software, and included real-time testing and assessment. At the time, there was considerable opposition among the electronics education community regarding the use of simulation software for the delivery of electronics curriculum. Many educators felt that a “hands on” methodology was the only valid method of learning electronics, and that simulation was a less-effective substitute.\n\nSimpson and Koenig embarked on a series of lectures, conference presentations and meetings with accrediting organizations throughout 1996, where they demonstrated that electronics simulation software could achieve identical results to laboratory experiments performed with real equipment. In January, 1997 the program received approval and accreditation from the Ministry of Training, Colleges and Universities and was launched in April, 1997. In its first year, the program enrolled over 500 students from 17 countries, with over 30 companies sponsoring employees.\n\nIn 1998, the program received the Program Excellence Award, from the Association of Canadian Community Colleges, a consortium of 155 Colleges. It was the first time a distance education program had earned this award and was noted by ACCC President, Gerald Brown, as a “landmark achievement in the field of distance education”.\nIn 2003, the program received a $1 million grant from the Government of Ontario for the development of a “virtual campus” to support students who were enrolled in 85 cities and towns throughout the province. The award was presented by TVOntario President, Isabel Bassett.\nIn 2007, the program underwent a significant revision when the laboratory software simulation was changed to CircuitLogix, which included both 2D and 3D simulation. The simulation capability of the new simulation software allowed for further integration of course theory and lab and greatly enhanced the program’s virtual learning environment \n\nIn recent years, the program has expanded through partnerships between Colleges across North America, enabling the program to be supported in local areas. In addition to the accreditation offered by Colleges delivering the program, graduates are also eligible for certification by external accrediting organizations such as the Electronics Technicians Association and the International Society of Certified Engineering Technicians. Colleges who are part of this consortium include: Allegany College of Maryland, Allen County Community College, Anne Arundel Community College, Brooklyn College, Thomas Edison State University, Hocking College, Horry-Georgetown Technical College, MiraCosta College, Mount Wachusett Community College, and Valencia Community College.\n\nThe ET distance education program covers an introductory curriculum in electronics equivalent to a two-year college associate degree. Contents of the program includes Current, Voltage, Resistance, Ohm's Law, Parallel and Series Circuits, Magnetism, AC/DC Circuits, Capacitance, Inductance, Transformers, Resonance, Filters, control relays, relay logic, Transistors, Semiconductors, Amplifiers, Integrated Circuits and Digital electronics. The interactive multimedia program presents twenty-three modules of interactive curriculum using text, video, 2D and 3D animations, photos, audio clips and interactive electronic circuit simulation using CircuitLogix. The simulation software provides more than 400 laboratory projects as part of the curriculum, allowing students to design circuitry, perform tests and work with electronic equipment as though in a real electronics workshop.\n\nThe program is delivered in an asynchronous, self-paced format and allows the student to determine the time required to complete the program. Due to the empirical nature of electronics curriculum, and the high level of integration of lab simulation software, students are able to successfully complete the course material with a minimum amount of interaction with faculty and staff at the College. When support is required, it is provided through the “virtual campus”.\n\n"}
{"id": "41818488", "url": "https://en.wikipedia.org/wiki?curid=41818488", "title": "Equalising beam", "text": "Equalising beam\n\nAn equalising beam, equalising lever or equalising bar ( or \"Ausgleichhebel\") links the suspension of two or more adjacent axles of a vehicle with more than two axles, especially railway locomotives. Its job is to provide 'compensated' springing, i.e. to ensure an even and statically determinate distribution of load to all the axles on uneven terrain or poorly laid track. The function of an equalising lever thus corresponds roughly to that of axle compensators or rockers (\"Achswippen\").\n"}
{"id": "46477688", "url": "https://en.wikipedia.org/wiki?curid=46477688", "title": "Floodway (road)", "text": "Floodway (road)\n\nA floodway is a flood plain crossing for a road, built at or close to the natural ground level.\n\nThey are designed to be submerged under water, but withstand such conditions. Typically floodways are used when the flood frequency or time span is minimal, traffic volumes are low, and the cost of a bridge is uneconomic – in most cases, in rural areas.\n\n\n"}
{"id": "5981219", "url": "https://en.wikipedia.org/wiki?curid=5981219", "title": "Food, Tobacco, Agricultural, and Allied Workers", "text": "Food, Tobacco, Agricultural, and Allied Workers\n\nThe United Cannery, Agricultural, Packing, and Allied Workers of America union (UCAPAWA) changed its name to Food, Tobacco, Agricultural, and Allied Workers (FTA) in 1944. The FTA sought to further organize cannery units and realized the best way to do this would be through organizing women and immigrant workers and in 1945 started finding success to these ends. The FTA started to experience problems when the International Brotherhood of Teamsters (IBT) began interfering in its organizing efforts. The IBT was affiliated with the American Federation of Labor (AFL) and the FTA was affiliated with its rival, the radical, Congress of Industrial Organizations (CIO). The IBT union was more conservative in regards to women and immigrant workers. It did not have much interest in integrating them into the union. It was far more concerned with making sweetheart deals and collecting union dues. This willingness to maintain the status quo made the IBT a favorite among California Processors and growers. This meant that they signed more contracts with processors and growers than the FTA, which ultimately undermined the more radical FTA union. The Taft-Hartley Act of 1947 further damaged the FTA. This act stated that American labor unions could not have communist ties and the FTA had many. Because of this act many of the organizers either left the union or were deported. It was at this time that the FTA as a whole was expelled from the CIO. This put the IBT in the center of the California cannery industry and it remained there for the next two decades.\n\nThe Food, Tobacco, Agricultural, and Allied Workers (FTA) have had constant ideological and political battles between the Congress of Industrial Organizations (CIO). A lot of what caused this constant friction was that the FTA had a lot of members who were involved with various communist parties and/or organizations. More importantly, \"The real active Communist Party leaders within the union.\" The most prominent active Communist Party members within the FTA included Theodosia Simpson, Velma Hopkins, Viola Brown, Moranda Smith, Christine Gardner, Robert Black, Clark Sheppard, John Henry Miller, Jethro Dunlap, and Vivian Bruce. The anti-communist sentiment of not only the more conservative/liberal labor unions, but the national community at large felt that the \"Communist Inspired\" militant party members prevented settlements at the time of negotiations. The general sentiment perceived that the tactics the Communist Party members of the FTA used would \"lead to trouble and possibly to race rioting.\" Phillip Murray President of the CIO had held strong anti-communist political and theoretical beliefs, but even he himself was convinced that \"anti-Communism played into the hands of labor’s enemies.\" Phillip Murray held and shared many of the core beliefs that members of the Communist Party in the FTA and the FTA in general agreed to. Some of those beliefs included, \"continued cooperation between the United States and the Soviet\nUnion, and he was committed to maintaining the CIO as an inclusive federation of politically diverse industrial unions.\" Despite having conflict with the FTA, Murray felt that organizing against labor’s enemies was more beneficial to workers as opposed to focusing energy fighting against the FTA whose political and tactics mirrored those of the Communist Party.\n\nAt a time when big labor unions were racially prejudiced, the Food, Tobacco, Agricultural, and Allied Workers (FTA) took a stance of organizing community members of racial backgrounds. African American and Black members of the FTA held high ethics of racial pride and had held a strong foundation of solidarity that transcended the boundaries of race. For African American/Black members of the FTA, the black church had played a huge influence on their politics of interracialism. The black church held the belief \"whites would cast off the sin of racism and embrace the brotherhood of all people, in part by the secular radicalism of a left-led union, and in part by popular anticolonialism.\" Organizers of FTA and local black and white leaders were diligent in seeking to break down any racial barriers that would prevent class solidarity from prospering. In 1944, the FTA had found themselves on the tail end of a controversial issue that had arisen in a court hearing that had to do with the contract negotiations between Reynolds and Local 22. White workers in the Employees Association were involved in a shouting match that broke out between union leaders, which ultimately resulted in William Deberry being charged with assaulting a white woman. This caused problems for the FTA because the public had predicted that FTA’s principles of \"race mixing\" would lead to violence and violation.\n\nBeginning in 1946, in an effort to organize the largely unorganized south, CIO officials began what is called Operation Dixie. This was done for multiple reasons, the first being it sought to organize the Southern textile industry in order to close the wage gap that existed between the North and South stopping the potential flight risk from the North due to the cheap, organized labor that was available in the South. Second, this effort was also an assault on the Bourbons who until then had been the ruling southern power. To start CIO flooded the streets with 200 organizes in what was called the \"Holy Crusade\" of organizing. This movement quickly inspired the AFL to engage in the same organizing efforts beginning a competition between the two groups. Out of fear that it could potentially lose some of its less radical unions the FTA joined the organizing efforts. The FTA was a vital part of Operation Dixie and its effects can be seen by what it accomplished in North Carolina where they won 25 elections against the AFL. This was despite the fact that this organizing was done in the midst of a Jim Crow south. In the end, it was the Jim Crow laws and the resulting racial strife that contributed to the defeat of this campaign.\n"}
{"id": "39689661", "url": "https://en.wikipedia.org/wiki?curid=39689661", "title": "Gamemaster's screen", "text": "Gamemaster's screen\n\nA gamemaster's (GM) screen, also called dungeon master's (DM) screen, is a gaming accessory, usually made out of cardboard, used by the gamemaster to hide all the relevant data related to a tabletop role-playing game session, hiding them from the players in order to not spoil the plot of the story.\n\nVery often the front side of the screen just displays an illustration, usually related to the game's subject. The reverse side is most of the times filled with tables, rules and other useful gaming data.\n\nSome role-playing games have an adapted name for their gamemaster's screen. \"Dungeon Master's Screen\" is the term used, for example, for the \"Dungeons & Dragons\" gamemaster's screen.\n\nIn addition to the official gamemaster's screens sold by professional gaming publishers, some customizable models are also available.\n\n\n"}
{"id": "24237178", "url": "https://en.wikipedia.org/wiki?curid=24237178", "title": "HAWAII MR1", "text": "HAWAII MR1\n\nThe HAWAII MR1 is a seafloor imaging system developed by the Hawaii Mapping Research Group (HMRG) in 1991. HAWAII MR1 is short for HIGP (Hawaii Institute of Geophysics and Planetology) Acoustic Wide Angle Imaging Instrument, Mapping Researcher 1. This system is the first to use all-digital signal processing. It has been used in the discovery of several objects and locations of note, examples being the USS \"Yorktown\" and the HMRG Deep.\n\nThe MR1 system is relatively small and stable, and is towed at a depth of 80 to 100 meters. Because it is not hull-mounted, this data collection system has the ability to withstand rough conditions.\n\nThe HAWAII MR1 also has the capability to measure and collect bathymetric and backscatter data simultaneously, and record raw acoustic data \nindependently of human supervision. After collecting data, the data can be processed with software available from HMRG.\n\nThe width of a scan produced by the system varies, depending upon whether it is producing a bathymetry swath or a sidescan swath; the swath while recording bathymetry data is 3.4 times the water depth and the swath while recording sidescan data is 7.5 times the water depth.\n\nThe HAWAII MR1 was chosen to be used on an expedition headed by Robert Ballard, the person who discovered the RMS \"Titanic\". The goal of the expedition was to find the USS \"Yorktown\", which they did successfully. Ballard said of the sonar, \"It was the [University of Hawaii mapping team's sonar] that enabled us to find it.\" Bruce Appelgate, at the time director of field operations at HMRG, noted that they were fortunate in finding the ship, as it was at the very edge of the MR1's resolution. The ship was 17,000 feet (5.2 km) underwater.\n\nThe mapping system was also used in the discovery of the HMRG Deep, the second deepest spot in the world (after the Challenger Deep).\n\n"}
{"id": "6410435", "url": "https://en.wikipedia.org/wiki?curid=6410435", "title": "ISO 13485", "text": "ISO 13485\n\nISO 13485 \"Medical devices -- Quality management systems -- Requirements for regulatory purposes\" is an International Organization for Standardization (ISO) standard published for the first time in 1996; it represents the requirements for a comprehensive quality management system for the design and manufacture of medical devices. This standard supersedes earlier documents such as EN 46001 and EN 46002 (both 1997), the previously published ISO 13485 (1996 and 2003), and ISO 13488 (also 1996). ISO 13485:2016 Certificates meets the requirement of IEC 60601-2-25 : 1993 + A1: 1999 safety of Electrocardiograms. \n\nThe current ISO 13485 effective edition was published on 1 March 2016.\n\nThough it is tailored to the industry's quality system expectations and regulatory requirements, an organization does not need to be actively manufacturing medical devices or their components to seek certification to this standard, in contrast to the automotive sector's ISO/TS 16949, where only firms with an active request for quotation, or on the bid list, of an International Automotive Task Force supply chain manufacturer can seek registration.\n\nWhile it remains a stand-alone document, ISO 13485 is generally harmonized with ISO 9001. A principal difference, however, is that ISO 9001 requires the organization to demonstrate continual improvement, whereas ISO 13485 requires only that the certified organization demonstrate the quality system is effectively implemented and maintained. Additionally, the ISO 9001 requirements regarding customer satisfaction are absent from the medical device standard.\n\nOther specific differences include:\n\n\nCompliance with ISO 13485 is often seen as the first step in achieving compliance with European regulatory requirements. The conformity of Medical Devices and In-vitro Diagnostic Medical Device according to European Union Directives 93/42/EEC, 90/385/EEC and 98/79/EEC must be assessed before sale is permitted. \nOne of the major requirements to prove conformity is the implementation of the Quality Management System according ISO 9001 and/or ISO 13485 and ISO 14971. Although the European Union Directives do not mandate certification to ISO 9001 and/or ISO 13485 the preferred method to prove compliance to such standards is to seek its official certification which is issued by certifying organizations known as \"Registrars\". Several registrars also act as Notified Body. \nFor those medical devices requiring the pre-market involvement of a Notified Body, the result of a positive assessment from the Notified Body is the certificate of conformity allowing the CE mark and the permission to sell the medical device in the European Union.\nA very careful assessment of the company Quality Management System by the Notified Body, together with the review of the required Technical Documentation, is a major element which the Notified Body takes into account to issue the certificate of conformity to the company product(s).\n\nThis standard adopted by CEN as EN ISO 13485:2003/AC:2007 is harmonized with respect to the European medical device directives 93/42/EEC, 90/385/EEC and 98/79/EC.\n\nISO 13485 is now considered to be inline standard and requirement for medical devices even with \"Global Harmonization Task Force Guidelines\" (GHTF). The GHTF guidelines are slowly becoming universal standards for design, manufacture, export and sales of various medical devices. The GHTF has been replaced in the last few years by the International Medical Device Regulatory Forum (IMDRF) and is structured differently from the GHTF as only the regulators, that are primary members of the group, get to make many of the decisions. The IMDRF main membership (the regulators) do want to have non-regulators involved without voting rights and in this way they are hoping to get the process and documents completed quicker than under the GHTF system (regulators & non-regulators were equal in voting rights) that worked reasonably well, but somewhat slow.\n\nThis standard adopted by CEN as EN ISO 13485:2012 is harmonized with respect to the European Medical Devices Directive 93/42/EEC.\n\nMexico has published in October 11, 2012 a national standard as a Norma Oficial Mexicana (NOM) to control manufacture of medical devices inside the country. NOM-241-SSA1-2012, Buenas Practicas de Fabricación para Establecimientos dedicados a la Fabricación de Dispositivos Médicos. The scope of application is mandatory in the national territory, for all establishments dedicated to the process of medical devices marketed in the country. The Cofepris is the body assigned to its control, verification and to grant the records of compliance to the companies that implement this Standard of Good Manufacturing Practices. This standard is partially in line with ISO 13485: 2003 and ISO 9001: 2008.\n\nIn 2017, The Farmacopea de los Estados Unidos Mexicanos (United Mexican States Pharmacopoeia), medical industrial sectors and Cofepris are working together for updating NOM-241 Standard, putting special attention on manage of risks during manufacture and regulating by manufacturing lines some of the most important medical devices manufacturing processes. This standard will be published in August 2018, and 180 days after publication it will become mandatory for the industry.\n\nIn Spain, medical devices are named in ISO-13485 as \"Sanitary Products\" as Castellano-language translation of ISO-13485, but in Mexico they are known as \"Medical Devices\" and correspond to those used in medical practice and that meet the definition established by NOM-241 as: Medical device, to the substance, mixture of substances, material, apparatus or instrument (including the computer program necessary for its proper use or application), used alone or in combination in the diagnosis, monitoring or prevention of human or auxiliary diseases in the treatment of the same and of the disability, as well as the employees in the replacement, correction, restoration or modification of the anatomy or human physiological processes. Medical devices include products of the following categories: medical equipment, prostheses, orthotics, functional aids, diagnostic agents, supplies for dental use, surgical, healing and hygiene products.\nISO 13485:2016 Certificates meets the requirement of IEC 60601-2-25 : 1993 + A1: 1999 safety of Electrocardiograms.\n\n\n"}
{"id": "1568613", "url": "https://en.wikipedia.org/wiki?curid=1568613", "title": "International Federation for Information Processing", "text": "International Federation for Information Processing\n\nThe International Federation for Information Processing (IFIP) is a global organisation for researchers and professionals working in the field of information and communication technologies (ICT) to conduct research, develop standards and promote information sharing.\n\nEstablished in 1960 under the auspices of UNESCO, IFIP is recognised by the United Nations and links some 50 national and international societies and academies of science with a total membership of over half a million professionals. IFIP is based in Laxenburg, Austria and is an international, non-governmental organisation that operates on a non-profit basis.\n\nIFIP activities are coordinated by 13 Technical Committees (TCs) which are organised into more than 100 Working Groups (WGs), bringing together over 3,500 ICT professionals and researchers from around the world to conduct research, develop standards and promote information sharing. Each TC covers a particular aspect of computing and related disciplines, as detailed below.\n\nIFIP actively promotes the principle of open access and proceedings for which IFIP holds the copyright are made available electronically via IFIP's Open Access Digital Library. Downloading articles from IFIP's Open DL is not only free of charge, but unlike commercial publishers and other professional organisations, IFIP does not charge authors of open access articles to publish in its Open DL.\n\nConference and workshop organizers who prefer printed proceedings can take advantage of the agreement between IFIP and Springer and publish their proceedings as part of IFIP's Advances in Information and Communication Technology (AICT) series or in the Lecture Notes in Computer Science (LNCS) series. Proceedings published by Springer in IFIP's LNCS and AICT series will be included within IFIP's Open DL after an embargo period of three years.\n\nAn important activity of the IFIP Technical Committees is to organise and sponsor high quality conferences and workshops in the field of ICT. Sponsoring is generally in the form of Best Paper Awards (BPA) and/or Student Travel Grants (STG). To assist conference and workshop organisers, IFIP has facilities to host conference websites and supports conference management systems such as JEMS, which include export functions that seamlessly integrate with IFIP's Open DL.\n\nIFIP was established in 1960 under the auspices of UNESCO, originally under the name of the \"International Federation of Information Processing Societies\" (IFIPS). In preparation, UNESCO had organised the first International Conference on Information Processing, which took place in June 1959 in Paris, and is now considered the first IFIP Congress.\n\nThe name was changed to IFIP in 1961. The organisation's original contribution was to define the ALGOL 60 programming language, in one of the first examples of truly international collaboration in computer science, leaving a durable mark on the entire field.\n\nThe founding president of IFIP was Isaac L. Auerbach (1960–1965).\n\nIn 2009, IFIP established the International Professional Practice Partnership (IFIP IP3) to lead the development of the global ICT profession.\"\n\nIFIP's activities are centered on its 13 Technical Committees, which are divided into Working Groups. These groups, (with names like \"WG 2.4 Software Implementation Technology\") organise conferences and workshops, distribute technical papers and promote discussion and research outcomes.\n\nA full list of IFIP Technical Committees is listed below:\n\nThe current IFIP TC1, which focuses on Foundations of Computer Science, was established in 1997. There was an earlier TC1, covering Terminology, which was IFIP’s first Technical Committee. Formed in 1961, it produced a multilingual dictionary of information-processing terminology but was later disbanded.\n\nThe working groups of the current TC1 are:\n\nEstablished in 1962, IFIP TC2 explores Software Theory and Practice with the aim of improving software quality by studying all aspects of the software development process to better understand and enhance programming concepts.\n\nThe working groups of IFIP TC2 are:\n\nThe formation of TC3, to deal with computers and education, was announced in 1962. Richard Buckingham of the University of London was appointed its first chairman and TC3 held its initial meeting in Paris in February 1964.\n\nThe working groups of IFIP TC3 are:\n\nEstablished in 1970, IFIP TC5 provides a focus for multi-disciplinary research into the application of information technologies and practices to facilitate information management. It encompasses work in product life-cycle management, digital modelling, virtual product creation, integrated manufacturing/production management and more.\n\nThe working groups of IFIP TC5 are:\n\nEstablished in 1971, IFIP TC6 (Communication Systems) is one of the largest TCs within IFIP in terms of activities and revenues. TC6 has nine Working Groups (WGs) as well as a number of Special Interest Groups (SIGs), the majority of which are concerned either with specific aspects of communications systems themselves or with the application of communications systems. In addition, one WG focuses on communications in developing countries. TC6 meets twice a year, in spring and fall, usually co-locating its meetings with a related conference. Examples of TC6 conferences include IFIP Networking, DisCoTec, Middleware, WiOpt, CNSM, Integrated Network Management (IM) and Wireless Days (WD).\n\nMembership of a TC6 WG or SIG is open to leading researchers within the field, independent of the national society within the country of origin. Well-known (past) TC6 members include: Vint Cerf, André Danthine, Donald Davies, Peter Kirstein, Robert (Bob) Metcalfe, Louis Pouzin, Otto Spaniol and Hubert Zimmermann. Each WG or SIG elects a chair and vice-chair for a period of three years. WG and SIG (vice-)chairs are, next to the national representatives and some key researchers, automatically members of TC6.\n\nTC6 is a strong proponent of open access and the driving force behind the IFIP Open Digital Library (DL). The IFIP Open DL is currently operated by TC6 and eventually will move to the INRIA HAL system. To ensure maximum accessibility of accepted papers, several TC6 conferences publish their proceedings not only in the IFIP Open DL, but also in other online systems, such as IEEE Xplore, ACM DL, ResearchGate and arXiv.\n\nTC6 supports conferences by providing Best Paper Awards (usually 500 Euro each) as well as Student Travel Grants (usually 750 Euro). Conference organisers who intend to obtain IFIP sponsorship are encouraged to fill-in the online Event Request Form (ERF). Depending on the category and type of event, IFIP may charge fees to conferences to cover the costs of (future) awards as well as the IFIP secretariat.\n\nThe working groups of IFIP TC6 are:\nIn November 2015, a new Special Interest Group on \"Internet of People\" (IoP) was created.\n\nIFIP TC7 was founded in 1972 by A.V. Balakrishnan, J.L. Lions and M. Marchuk.\nThe aims of this Technical Committee are\n\nThe working groups of IFIP TC7 are:\n\nIFIP TC8 was established in 1976 and focuses on Information Systems. This committee aims to promote and encourage the advancement of research and practice of concepts, methods, techniques and issues related to information systems in organisations. It currently includes the following working groups: \n\nIFIP TC9 on ICT and Society was formed in 1976 to develop greater understanding of how ICT innovation is associated with changes in society and to influence the shaping of socially responsible and ethical policies and professional practices. The main work of the TC9 is conducted through its working groups, which organise regular conferences and events, including the Human Choice and Computers (HCC) conference series. This is a well established forum for the study of ICT and Society - the first HCC conference took place in Vienna in 1974, while the last one took place in Finland in 2014.\n\nThe working groups of IFIP TC9 are:\n\n\nIFIP TC10 was founded in 1976 and revised in 1987.\nIt aims to promote State-of-the-Art concepts, methodologies and tools in the life cycle of computer systems and to coordinate the exchange of information around these practices.\n\nTC10 currently has four working groups: \n\nIFIP TC11 on Security and Privacy Protection in Information Processing Systems was founded in 1984 and revised in 2006 and 2009. It focuses on increasing the trustworthiness of, and general confidence in, information processing and providing a forum for security and privacy protection experts and others professionally active in the field to share information and advance standards.\n\nIFIP TC11 currently has the following working groups:\n\nIFIP TC12 on Artificial Intelligence was established in 1984 and revised in 1991 and 2004. It aims to foster the development and understanding of Artificial Intelligence (AI) and its applications worldwide and to promote interdisciplinary exchanges between AI and other fields of information processing.\n\nIFIP TC12 currently includes the following working groups:\n\nIFIP TC 13 on Human-Computer Interaction was founded in 1989. It aims to encourage empirical research (using valid and reliable methodology, with studies of the methods themselves where necessary); to promote the use of knowledge and methods from the human sciences in both design and evaluation of computer systems; to promote better understanding of the relation between formal design methods and system usability and acceptability; to develop guidelines, models and methods by which designers may be able to provide better human-oriented computer systems; and to co-operate with other groups, inside and outside IFIP, so as to promote user-orientation and \"humani-zation\" in system design.\n\nTC 13 currently has nine working groups:\n\n\nCreated in 2002 as SG16, on August 28, 2006, the General Assembly of IFIP decided to establish this new Technical Committee . To encourage computer applications for entertainment and to enhance computer utilization in the home, the technical committee will pursue the following aims: to enhance algorithmic research on board and card games; to promote a new type of entertainment using information technologies; to encourage hardware technology research and development to facilitate implementing entertainment systems, and; to encourage non-traditional human interface technologies for entertainment. \n\n\nList of full members as of 2018, November 20:\nList of associate members as of 2015, June 22:\n\n"}
{"id": "57001978", "url": "https://en.wikipedia.org/wiki?curid=57001978", "title": "Jean Dabry", "text": "Jean Dabry\n\nJean Dabry (8 December 1901, Avignon5 July 1990, Montmorency) was a French aviator of the Aéropostale, then an airline pilot for Air France. He is buried in the Montmorency cemetery.\n\nDabry gained his first officer-rank when he joined the Aéropostale in 1928 as navigator. Two years later on the 12th April 1930 with Jean Mermoz as pilot and Léopold Gimié as radio-operator they made a closed circuit length record by floatplane. The aircraft, a Latécoère 28 fitted with an 600 hp Hispano-Suiza engine flew 4 345 km for 30h25.\n\nOn 12 and 13 May 1930 the same crew performed the first crossing of the South Atlantic Ocean with the floatplane Laté 28 \"Compte de la Vault\" fitted with an Hispano-Suiza engine (600 hp). They flew between Saint Louis, Sénégal to Natal (Brazil). They delivered 130 kg of mail during this 3200 km journey. During the flight two records would be established namely, distance in a straight line for a seaplane and mail delivery time as they flew for 20 hours and 15 minutes.\n\nHe became a captain at Air France in 1936. During his career he made more than 540 crossings of the North Atlantic Ocean. He became one of the executive directors of the company. He had logged 16,000 hours of flight time when he left Air France in 1957.\n\n\n\n\n"}
{"id": "20174341", "url": "https://en.wikipedia.org/wiki?curid=20174341", "title": "Klismos", "text": "Klismos\n\nA klismos (Greek: κλισμός) or klismos chair is a type of ancient Greek chair, with curved backrest and tapering, outcurved legs.\n\nKlismoi are familiar from depictions of ancient furniture on painted pottery and in bas-reliefs from the mid-fifth century BCE onwards. In epic, \"klismos\" signifies an armchair, but no specific description is given of its form; in \"Iliad\" xxiv, after Priam's appeal, Achilles rises from his \"thronos\", raises the elder man to his feet, goes out to prepare Hector's body for decent funeral and returns, to take his place on his \"klismos\".\n\nA vase-painting of a satyr carrying a klismos chair on his shoulder shows how light such chairs were. The curved, tapered legs of the klismos chair sweep forward and rearward, offering stability. The rear legs sweep continuously upward to support a wide concave backrest like a curved tablet, which supports the sitter's shoulders, or which may be low enough to lean an elbow on.\n\nThe klismos fell from general favour during the Hellenistic period; nevertheless, the theatre of Dionysus at the foot of the Acropolis, Athens, of the first century CE, has carved representations of \"klismoi\". Where a klismos is represented in Roman portraits of seated individuals, the sculptures are copies of Greek works. The fall of the klismos might be due to a design flaw, the legs of the chair are bending outwards and without any further support the legs will spread out and break when sat upon.\nThe klismos was revived during the second, archaeological phase of European neoclassicism. Klismos chairs were first widely seen in Paris in the furniture made for the painter Jacques-Louis David by Georges Jacob in 1788, to be used as props in David's historical paintings, where the new sense of historicism required visual authenticity. It would be hard to find a French klismos chair earlier than the ones designed by the architect Jean-Jacques Lequeu in 1786 for a decor in the \"Etruscan style\" for the hôtel Montholon, boulevard Montmartre, and executed by Jacob; the furnishings have disappeared, but the watercolor designs are conserved in the Cabinet des Estampes. Simon Jervis has noted that Joseph Wright of Derby included the tablet of an approximation of a klismos chair in his \"Penelope Unraveling Her Web\", 1783–84 (J. Paul Getty Museum).\nIn London, early klismos chairs were designed by Thomas Hope for his house in Duchess Street, London, which George Beaumont had described as early as 1804 as \"more a \"Museum\" than anything else\"; klismos chairs were illustrated by Hope in several variations in \"Household Furniture and Interior Decoration\" (1807), the record of his semi-public house-museum. Klismos chairs in their purest form furnished Hope's Picture Gallery (pl. II) and the Second Room containing Greek Vases (pl. IV), but the swept legs featured in variations on the classical theme illustrated in other plates. Henry Moses' illustration of genteel company playing cards seated on klismos chairs appeared in Hope's \"Designs of Modern Costume\" (c. 1812).\n\nBy the presence of fashionable klismos chairs and a collection of Greek vases, a self-portrait of Adam Buck and his family, significantly, was formerly thought to represent Thomas Hope. Klismos chairs were designed for Packington Hall, Warwickshire, by Joseph Bonomi.\n\nIn Philadelphia, the architect Benjamin Henry Latrobe designed a set of klismos chairs for an interior in the most advanced neoclassical taste for William Waln's drawing-room, c. 1808. Latrobe's design, painted cream and red on a black background, the \"Etruscan\" color range, included a panel of caning beneath the tablet backrest and legs that splayed outwards to the sides as well as the front. For the White House, Latrobe's designs of 1809 for klismos chairs are cautiously reinforced with stretchers to render them more sturdy. A range of early 19th-century American klismos chairs were included in the exhibition \"Classical Taste in America, 1800–1840\", Baltimore Museum of Art, 1993.\n\nSuch severely academic revivals might be compromised by more familiar features of the chair-maker's usual practice: an early 19th-century klismos chair by J.E. Höglander, Stockholm has a padded backrest, supported on five slender colonettes, and the faces of the legs are lightly paneled.\nThe classicizing phase of Modernism allied with Art Deco found the simple lines of the klismos once again in favor: klismos chairs designed by the Danish Edvard Thomspon were illustrated in \"Architekten\", 1922. In 1960, T. H. Robsjohn-Gibbings met Greek cabinetmakers Susan and Eleftherios Saridis, and, together, they created the Klismos line of furniture, recreating ancient Greek furnishings with some accuracy, including klismos chairs.\n\nThe long and elegant curve was quite difficult to create; and may have been carved from a single piece of wood, or by using mortise and tenon joints, or by bending by steam, or by training the wood.\n\nThe seat was built of four wooden turned staves, morticed into the legs; a web of cording or leather strips supported a cushion or a pelt. The klismos was a specifically Greek invention, without detectable earlier inspiration.\n"}
{"id": "494018", "url": "https://en.wikipedia.org/wiki?curid=494018", "title": "Knole Settee", "text": "Knole Settee\n\nThe Knole settee (sometimes known as the Knole Sofa) was made in the 17th century. It is housed at Knole in Kent, a house owned by the Sackville-Wests since 1605 but now in the care of the National Trust. It was originally used not as comfortable sofa but as a formal throne on which the monarch would have sat to receive visitors. It features adjustable side arms and considerable depth of seating, it usually has exposed wooden finials at the rear corner tops, and some exposed wood may be present on the otherwise arms. The arms, more correctly sides, are of the same height. The side arms are tied to the sofa back by means of heavy decorative braid, often with an elaborate tassel.\n\nA number of references to the Knole sofa are found in literature; for example, Marlowe notes the usage in his book \"Memoirs of a Venus Lackey.\" In literature the Knole sofa is sometimes mentioned in the context of a room decorated with fine antique furniture \nsuch as in the novel \"In High Places\", in which the Knole sofa is positioned in a room with a fine Kerman antique carpet.\n\nThe spelling of Knole sofa is as shown in this sentence and not Knoll, as the name comes from Knole House, Sevenoaks, Kent UK, which is open to the public and run by the National Trust. On show is the original Knole Sofa.\n"}
{"id": "54514992", "url": "https://en.wikipedia.org/wiki?curid=54514992", "title": "Light-emitting transistor", "text": "Light-emitting transistor\n\nA light-emitting transistor or LET is a form of transistor that emits light. Higher efficiency than light-emitting diode (LED) is possible.\n\nReported in the January 5, 2004 issue of the journal Applied Physics Letters, Milton Feng and Nick Holonyak, the inventor of the first practical light-emitting diode (LED) and the first semiconductor laser to operate in the visible spectrum, made the world's first light-emitting transistor. This hybrid device, fabricated by Feng's graduate student Walid Hafez, had one electrical input and two outputs (electrical output and optical output) and operated at a frequency of 1 MHz. The device was made of indium gallium phosphide, indium gallium arsenide, and gallium arsenide, and emitted infrared photons from the base layer.\n\n"}
{"id": "47413702", "url": "https://en.wikipedia.org/wiki?curid=47413702", "title": "List of English Channel crossings by air", "text": "List of English Channel crossings by air\n\nThis is a list of notable flights across the English Channel.\n"}
{"id": "20356934", "url": "https://en.wikipedia.org/wiki?curid=20356934", "title": "Marine electronics", "text": "Marine electronics\n\nMarine electronics refers to electronics devices designed and classed for use in the marine environment on board ships and yachts where even small drops of salt water will destroy electronics devices. Therefore, the majority of these types of devices are either water resistant or waterproof.\n\nMarine electronics devices include chartplotter, marine VHF radio, autopilot and self-steering gear, fishfinder and sonar, marine radar, GPS, fibre optic gyrocompass, satellite television, and marine fuel management.\n\nThe electronics devices communicate by using a protocol defined by NMEA with two standards available, NMEA 0183 (serial communication network) and NMEA 2000 (controller-area network based technology). There is also Lightweight Ethernet (LWE).\n\nIn recent years, the International Electrotechnical Commission (IEC) has created a new standards suite for \"Digital interfaces for navigational equipment within a ship\". This is known as IEC 61162 and included NMEA 0183, NMEA 2000 and LWE.\n\nAdditionally, different suppliers of marine electronics have their own communications protocol.\n\n\nAnother important part of marine electronics is the navigation equipment. Here compasses, which includes both gyro compasses and magnetic compasses make up for equipment that is used by the entire shipping industry. \n\nSome manufacturers specialize more in equipment for commercial vessels such as tankers and general cargo vessels.\nThis industry is relatively small with worldwide sales of $3.2 billion in 2015. The top manufacturer was Japan-based Furuno with estimated sales of $325 million followed closely by Norway-based Navico, a holding company for several current and former industry brands (Simrad Yachting, Lowrance Electronics, B&G, Magellan, Northstar), with revenue of $308 million. Rounding out the top five are Japan Radio Company in third, Garmin (popular with recreational users) in fourth, and Sam Electronics (a subsidiary of Wärtsilä). The next five top manufacturers are Transas, Raymarine Marine Electronics (a subsidiary of FLIR Systems, Raytheon Anschütz, Sperry Marine, and Tokyo Keiki.\n\nOther companies outside of the industry's top ten that have a significant presence chartplotters include Samyung ENC, Hummingbird (Johnson Outdoors), Murphy (Enovation Controls), Naviop, SI-TEX Marine Electronics, and TwoNav.\n\n"}
{"id": "2015211", "url": "https://en.wikipedia.org/wiki?curid=2015211", "title": "Nelson rules", "text": "Nelson rules\n\nNelson rules are a method in process control of determining if some measured variable is out of control (unpredictable versus consistent). Rules, for detecting \"out-of-control\" or non-random conditions were first postulated by Walter A. Shewhart in the 1920s. The Nelson rules were first published in the October 1984 issue of the \"Journal of Quality Technology\" in an article by Lloyd S Nelson.\n\nThe rules are applied to a control chart on which the magnitude of some variable is plotted against time. The rules are based on the mean value and the standard deviation of the samples.\n\nThe above eight rules apply to a chart of a variable value.\n\nA second chart, the moving range chart, can also be used but only with rules 1, 2, 3 and 4. Such a chart plots a graph of the maximum value - minimum value of N adjacent points against the time sample of the range.\n\nAn example moving range: if N = 3 and values are 1, 3, 5, 3, 3, 2, 4, 5 then the sets of adjacent points are (1,3,5) (3,5,3) (5,3,3) (3,3,2) (3,2,4) (2,4,5) resulting in moving range values of (5-1) (5-3) (5-3) (3-2) (4-2) (5-2) = 4, 2, 2, 1, 2, 3.\n\nApplying these rules indicates when a potential \"out of control\" situation has arisen. However, there will always be some false alerts and the more rules applied the more will occur. For some processes, it may be beneficial to omit one or more rules. Equally there may be some missing alerts where some specific \"out of control\" situation is not detected. Empirically, the detection accuracy is good.\n\n\n"}
{"id": "3797703", "url": "https://en.wikipedia.org/wiki?curid=3797703", "title": "Non-speech audio input", "text": "Non-speech audio input\n\nNon-speech audio input is the use of non-speech sounds such as whistling, humming or hissing for entering data or controlling the user interface.\n\n"}
{"id": "17974754", "url": "https://en.wikipedia.org/wiki?curid=17974754", "title": "Orienteering map", "text": "Orienteering map\n\nAn orienteering map is a map specially prepared for use in orienteering competitions. It is a topographic map with extra details to help the competitor navigate through the competition area.\n\nThese maps are much more detailed than general-purpose topographic maps, and incorporate a standard symbology that is designed to be useful to anyone, regardless of native language. In addition to indicating the topography of the terrain with contour lines, orienteering maps also show forest density, water features, clearings, trails and roads, earthen banks and rock walls, ditches, wells and pits, fences and power lines, buildings, boulders, and other features of the terrain. Orienteering maps are 1:15 000 or 1:10 000 scale.\n\nThe International Orienteering Federation (IOF) publishes the standard for orienteering maps, including:\n\nAn orienteering map, and a compass, are the primary aids for the competitor to complete an orienteering course of control points as quickly as possible. A map that is reliable and accurate is essential so that a course can be provided which will test the navigational skills of the competitor. The map also needs to be relevant to the needs of the competitor showing the terrain in neither too much nor too little detail.\n\nBecause the competition must test the navigational skills of the competitor, areas are sought which have a terrain that is rich in usable features. In addition, the area should be attractive and interesting. Notable examples in the US include Pawtuckaway State Park, New Hampshire and Valles Caldera, New Mexico, both having many boulders and boulder fields, and a wide variety of other terrain types.\n\nOrienteering maps are produced by local orienteering clubs and are a valuable resource for the club. Orienteering maps are expensive to produce and the principal costs are: the fieldwork, drawing (cartography), and printing. Each of these can use up valuable resources of a club, be it in manpower or financial costs. Established clubs with good resources e.g. maps and manpower are usually able to host more events.\n\nIn the early days of orienteering, competitors used whatever maps were available; these were typically topographic maps from the national mapping agency. While national mapping agencies update their topographic maps on a regular basis, they are usually not sufficiently up to date for orienteering purposes. Gradually, specially drawn maps have been provided to meet the specific requirements of orienteering.\n\nMaps produced specifically for orienteering show a more detailed and up-to-date description of terrain features. For example, large rocks above the soil surface do not normally appear on topographic maps but can be important features on many orienteering maps. New features such as fence lines can be important navigational aids and may also affect route choice. Orienteering maps include these new features.\n\nCartographer Jan Martin Larsen was a pioneer in the development of the specialized orienteering map.\n\nThe map scale depends on the purpose of the competition and also the standard used, for example, a map used in a foot orienteering long distance event has scale of 1:15000. The map is printed in six base colours, which cover the main groups: Land forms, rock and boulders, water and marsh, vegetation, and man-made features, and an extra colour for overprinting symbols.\nLand forms are shown using contour lines. The contour interval is normally 5 metres, but other interval such as 2 or 2.5 metres may be used in sprint maps. Additional symbols are provided to show e.g. earth bank, knoll, depression, small depression, pit, broken ground etc.\n\nThis group covers cliffs, boulders, boulder fields, and boulder clusters etc.\n\nThis group covers lakes, ponds, rivers, water channels, marshes, and wells etc.\nThis group covers vegetation. \"White\" is typically open runnable forest. \"Green\" means a forest of low visibility with reduced running speed, being graded from slow running, through difficult running, to impassable. \"Yellow\" colour shows open areas. Green vertical stripes are used to indicate undergrowth (slow or difficult running) but otherwise with good visibility.\n\nMan-made features include roads, tracks, paths, power lines, stone walls, fences, buildings, etc.\n\nTwo technical symbols are required on all maps: Magnetic north lines printed in blue, and register crosses (these show that the printed colours are coincident).\n\nOther information is required to be on the printed map although the presentation is not specified, e.g. scale, contour interval and scale bar. Good practice requires information such as date of survey, survey scale, copyright information, and proper credit for the people who produced the map (surveyor, cartographer).\n\nSymbols are specified so that a course can be overprinted on the map. It includes symbols for the start, control points, control numbers, lines between control points, and finish. Extra symbols are available so that information relating to that event may be shown e.g. crossing points, forbidden route, first aid post, and refreshment point etc. These are not permanent features and cannot be included when the map is printed.\n\nThe International Specification for Orienteering Maps sets out the specifications for orienteering maps for use in foot orienteering, together with specifications for the other sports governed by the International Orienteering Federation (IOF) i.e. mountain bike orienteering, ski orienteering, and trail orienteering. The specifications are mostly the same but with a few sport specific symbols e.g. ski-o needs to distinguish snow-covered roads from cleared roads.\n\nThe mapping process has four main stages: Creation of the base map, field-work, drawing, and printing.\n\nThe base map can be a topographic map made for other purposes e.g. mapping from the National Mapping Agency, or a photogrammetric plot produced from an aerial survey.\n\nAs LIDAR-surveying advances, base maps consisting of 1 meter contours and other data derived from the LIDAR data get more common. As these base maps contain large amounts of information the cartographic generalization becomes important in creating a readable map.\n\nCartographers use a projection to project the curved surface of the earth onto a flat surface. This generates a grid that is used as a base for national topographic mapping. The projection introduces a distortion so that grid north differs from true north; magnetic north is a natural feature that differs from both. As an example: at 52° 35' N 1° 10' E (approx 7 km west of Norwich, England) true north is 2° 33' west of grid north, and magnetic north is about 7° west of grid north. Magnetic north varies continually and in this example (1986) was reducing by about ° in four years. Orienteering maps are printed using magnetic north and this requires an adjustment to be made to the base map.\n\nField-work is carried out using a small part of the base map fixed to a survey board, covered with a piece of draughting film, and drawn with pencils. The final map needs to be drawn with sufficient accuracy so that a feature shown on the map can be identified clearly on the ground by the competitor, thus, field-workers need to locate features with a high level of accuracy, to ensure consistency between map and terrain. Where the map and terrain are inconsistent, the feature becomes unusable: no control point can be placed there. Periodic corrections to the map may be necessary, typically vegetation changes in forested areas.\n\nThe earliest orienteering maps used existing topographic maps e.g. United Kingdom Ordnance Survey 1:25 000 plans. These were cut down to a suitable size, corrected by hand, and then copied.\n\nThese were initially drawn by hand on tracing paper using one sheet for each of the five colours; the various dot or line screens being added using dry transfer screens, for example Letratone manufactured by Letraset in the UK. The map was drawn at twice final map scale, and photographically reduced to produce the five film positives for printing. This was a simple process that required very few specialist tools. Draughting film has replaced tracing paper. This is a plastic waterproof material etched on one side so that the ink will hold.\n\nThis is the standard process used by National Mapping Agencies. It uses a plastic film, which is coated on one side with a photo-opaque film. The layer is removed with a scribing tool or scalpel to produce a negative image. One sheet of film is needed for each solid colour, and one for each screen, usually requiring about ten sheets of film altogether. The map is drawn at final map scale, and the negatives are printed with high quality dot screens to produce the five film positives for printing. The process makes it easy to produce high quality maps, but it does require a number of specialist tools.\n\nComputer software is available to aid in the drawing of digital maps. OCAD is the leading provider. Another one is opensource OpenOrienteering Mapper application, created by community as free alternative to OCAD. \nOther computer software is available that will link with OCAD, or with the digital map files, so that courses can be incorporated into the map ready for printing.\n\nColour maps were sent to commercial printers for printing in five colours, with the overprinting being added after the map had been printed. This process was chosen as it gave a higher quality for the fine line-work than the industry standard four-colour process (CMYK). As computer and software technology has advanced, and the cost reduced, many clubs are now in a position to print their own maps. This enables clubs to print the six colours together (map and overprinting symbols) using that same four-colour process, but with a reduction in quality over traditional printing. Printing costs can be minimised by using standard stock sizes of paper e.g. A4 or Letter. It is important to use the correct type of paper: both the weight and the coating affect the usability of the final map.\n\nMap accuracy refers to the work of the surveyor (field-worker) and relates not so much to the positional accuracy of the survey but rather to its utility for the competitor. Map quality refers to the quality of the artwork. Many national bodies have a competition in which awards are made to cartographers after assessment by a national panel.\n\n"}
{"id": "41561194", "url": "https://en.wikipedia.org/wiki?curid=41561194", "title": "Rail-Veyor", "text": "Rail-Veyor\n\nRail-Veyor is a remote controlled, electrically powered light-rail haulage solution for surface and underground applications in the mining and aggregate industries. Rail-Veyor Technologies Global Inc. is a private Sudbury, Canada-based industrial bulk material handling and material haulage company that manufactures and installs Rail-Veyor systems. \n\nRail-Veyor's light-rail system was first demonstrated by its inventor, Mike Dibble, in conjunction with the Florida Institute of Phosphate Research from 1999-2001. Since then it has been installed commercially by Harmony Gold at its Phakisa Gold Mine in Free State, South Africa. Canadian entrepreneur Risto Laamanen incorporated the business, secured the global distribution rights, and set up a second demonstration and test site with Vale S.A. at their Frood Stobie mine in Sudbury, Ontario, Canada in 2008. Following successful testing of the system at the Frood Stobie test site, a Rail-Veyor system was installed at Vale's Copper Cliff Mine 114 Ore Body Mine and became operational in 2012, with the intention of using the Rail-Veyor system as an enabling technology for rapid mine development and high speed production.\n\nRisto Laamanen died on July 7, 2009, but the Laamanen family continue to be large investors in the private company, Rail-Veyor Technologies Global Inc., along with investors from Canada and the United States of America.\n\nThe Rail-Veyor system incorporates a remotely operated electrically powered series of two wheeled railcars driven by power stations located along on a light-rail track. Because the cars are remotely operated and compact in size, they can be used as an enabling technology for rapid development and high speed production at the working face. The Rail-Veyor system can reduce capital costs and infrastructure, travelling below shafts and in spaces as small as 10 by 12 feet or 3.05 m by 3.66 m. Using multiple train systems in tandem optimizes continuous material haulage. The railcars can travel at variable speeds up to 18 mph, or 8 metres/second, and climb grades of 20%. The company claims that the system combines the best features of conveyors, rail, and truck haulage, including travelling on 20% inclines, increased capacity and availability, reduced installation time, a small profile, and a short turning radius of 95 feet or 30 m. The system is used for underground and surface applications in the mining and aggregate industries.\n\n"}
{"id": "652467", "url": "https://en.wikipedia.org/wiki?curid=652467", "title": "Room 40", "text": "Room 40\n\nRoom 40, also known as 40 O.B. (Old Building) (latterly NID25), was the cryptanalysis section of the British Admiralty during the First World War.\n\nThe group, which was formed in October 1914, began when Admiral Oliver, the Director of Naval Intelligence, gave intercepts from the German radio station at Nauen, near Berlin, to Director of Naval Education Alfred Ewing, who constructed ciphers as a hobby. Ewing recruited civilians such as William Montgomery, a translator of theological works from German, and Nigel de Grey, a publisher. It was estimated that during the war Room 40 decrypted around 15,000 intercepted German communications from wireless and telegraph traffic. Most notably the section intercepted and decoded the Zimmermann Telegram, a secret diplomatic communication issued from the German Foreign Office in January 1917 that proposed a military alliance between Germany and Mexico. Its decoding has been described as the most significant intelligence triumph for Britain during World War I because it played a significant role in drawing the then-Neutral United States into the conflict.\n\nRoom 40 operations evolved from a captured German naval codebook, the (SKM), and maps (containing coded squares) that Britain's Russian allies had passed on to the Admiralty. The Russians had seized this material from the German cruiser SMS \"Magdeburg\" after it ran aground off the Estonian coast on 26 August 1914. The Russians recovered three of the four copies that the warship had carried; they retained two and passed the other to the British. In October 1914 the British also obtained the Imperial German Navy's (HVB), a codebook used by German naval warships, merchantmen, naval zeppelins and U-Boats: the Royal Australian Navy seized a copy from the Australian-German steamer \"Hobart\" on 11 October. On 30 November a British trawler recovered a safe from the sunken German destroyer \"S-119\", in which was found the (VB), the code used by the Germans to communicate with naval attachés, embassies and warships overseas. In March 1915 a British detachment impounded the luggage of Wilhelm Wassmuss, a German agent in Persia and shipped it, unopened, to London, where the Director of Naval Intelligence, Admiral Sir William Reginald Hall discovered that it contained the German Diplomatic Code Book, Code No. 13040.\n\nThe section retained \"Room 40\" as its informal name even though it expanded during the war and moved into other offices. Alfred Ewing directed Room 40 until May 1917, when direct control passed to Captain (later Admiral) Reginald 'Blinker' Hall, assisted by William Milbourne James. Although Room 40 successfully decrypted Imperial German communications throughout the First World War, its function was compromised by the Admiralty's insistence that all decoded information would only be analysed by Naval specialists. This meant that, while Room 40 operators could decrypt the encoded messages, they were not permitted to understand or interpret the information themselves.\n\nIn 1911, a sub-committee of the Committee of Imperial Defence on cable communications concluded that in the event of war with Germany, German-owned submarine cables should be destroyed. In the early hours of 5 August 1914, the cable ship \"Alert\" located and cut Germany's five trans-Atlantic cables, which ran down the English Channel. Soon after, the six cables running between Britain and Germany were cut. As an immediate consequence, there was a significant increase in cable messages sent via cables belonging to other countries, and cables sent by wireless. These could now be intercepted, but codes and ciphers were naturally used to hide the meaning of the messages, and neither Britain nor Germany had any established organisations to decode and interpret the messages. At the start of the war, the navy had only one wireless station for intercepting messages, at Stockton. However, installations belonging to the Post Office and the Marconi Company, as well as private individuals who had access to radio equipment, began recording messages from Germany.\n\nIntercepted messages began to arrive at the Admiralty intelligence division, but no one knew what to do with them. Rear-Admiral Henry Oliver had been appointed Director of the Intelligence division in 1913. In August, 1914, his department was fully occupied with the war and no-one had experience of code breaking. Instead he turned to a friend, Sir Alfred Ewing, the Director of Naval Education (DNE), who previously had been a professor of engineering with a knowledge of radio communications and who he knew had an interest in ciphers. It was not felt that education would be a priority during the expected few months duration of the war, so Ewing was asked to set up a group for decoding messages. Ewing initially turned to staff of the naval colleges Osborne and Dartmouth, who were currently available, due both to the school holidays and to naval students having been sent on active duty. Alastair Denniston had been teaching German but later became second in charge of Room 40, then becoming Chief of its successor after the First World War, the Government Code and Cypher School (located at Bletchley Park during the Second World War). \n\nOthers from the schools worked temporarily for Room 40 until the start of the new term at the end of September. These included Charles Godfrey, the Headmaster of Osborne (whose brother became head of naval Intelligence during the Second World War), two Naval instructors, Parish and Curtiss, and scientist and mathematician Professor Henderson from Greenwich Naval College. Volunteers had to work at code breaking alongside their normal duties, the whole organisation operating from Ewing's ordinary office where code breakers had to hide in his secretary's room whenever there were visitors concerning the ordinary duties of the DNE. Two other early recruits were R. D. Norton, who had worked for the Foreign Office, and Richard Herschell, who was a linguist, an expert on Persia and an Oxford graduate. None of the recruits knew anything about code breaking but were chosen for knowledge of German and certainty they could keep the matter secret.\n\nA similar organisation had begun in the Military Intelligence department of the War Office, which become known as MI1b, and Colonel Macdonagh proposed that the two organisations should work together. Little success was achieved except to organise a system for collecting and filing messages until the French obtained copies of German military ciphers. The two organisations operated in parallel, decoding messages concerning the Western Front. A friend of Ewing's, a barrister by the name of Russell Clarke, plus a friend of his, Colonel Hippisley, approached Ewing to explain that they had been intercepting German messages. Ewing arranged for them to operate from the coastguard station at Hunstanton in Norfolk, where they were joined by another volunteer, Leslie Lambert (later becoming known as a BBC broadcaster under the name A. J. Alan). Hunstanton and Stockton formed the core of the interception service (known as 'Y' service), together with the Post Office and Marconi stations, which grew rapidly to the point it could intercept almost all official German messages. At the end of September, the volunteer schoolmasters returned to other duties, except for Denniston; but without a means to decode German naval messages there was little specifically naval work to do.\n\nThe first breakthrough for Room 40 came with the capture of the (SKM) from the German light cruiser SMS \"Magdeburg\". Two light cruisers, \"Magdeburg\" and SMS \"Augsburg\", and a group of destroyers all commanded by Rear-Admiral Behring were carrying out a reconnaissance of the Gulf of Finland, when the ships became separated in fog. \"Magdeburg\" ran aground on the island of Odensholm off the coast of Russian-controlled Estonia. The ship could not be re-floated so the crew was to be taken on board by the destroyer SMS \"V26\". The commander, Habenicht prepared to blow up the ship after it had been evacuated but the fog began to clear and two Russian cruisers \"Pallada\" and \"Bogatyr\" approached and opened fire. The demolition charges were set off prematurely, causing injuries amongst the crew still on board and before secret papers could be transferred to the destroyer or disposed of. Habenicht and fifty seven of his crew were captured by the Russians.\n\nExactly what happened to the papers is not clear. The ship carried more than one copy of the SKM codebook and copy number 151 was passed to the British. The German account is that most of the secret papers were thrown overboard, but the British copy was undamaged and was reportedly found in the charthouse. The current key was also needed in order to use the codebook. A gridded chart of the Baltic, the ship's log and war diaries were all also recovered. Copies numbered 145 and 974 of the SKM were retained by the Russians while was dispatched from Scapa Flow to Alexandrovosk in order to collect the copy offered to the British. Although she arrived on 7 September, due to mix-ups she did not depart until 30 September and returned to Scapa with Captain Kredoff, Commander Smirnoff and the documents on 10 October. The books were formally handed over to the First Lord, Winston Churchill, on 13 October.\n\nThe SKM by itself was incomplete as a means of decoding messages, since they were normally enciphered as well as coded and those that could be understood were mostly weather reports. Fleet paymaster C. J. E. Rotter, a German expert from the naval intelligence division, was tasked with using the SKM codebook to interpret intercepted messages, most of which decoded as nonsense since initially it was not appreciated that they were also enciphered. An entry into solving the problem was found from a series of messages transmitted from the German Norddeich transmitter, which were all numbered sequentially and then re-enciphered. The cipher was broken, in fact broken twice as it was changed a few days after it was first solved, and a general procedure for interpreting the messages determined. Enciphering was by a simple table, substituting one letter with another throughout all the messages. Rotter started work in mid October but was kept apart from the other code breakers until November, after he had broken the cipher.\n\nThe intercepted messages were found to be intelligence reports on the whereabouts of allied ships. This was interesting but not vital. Russel Clarke now observed that similar coded messages were being transmitted on short-wave, but were not being intercepted because of shortages of receiving equipment, in particular the aerial. Hunstanton was directed to stop listening to the military signals it had been intercepting and instead monitor short-wave for a test period of one weekend. The result was information about the movements of the High Seas Fleet and valuable naval intelligence. Hunstanton was permanently switched to the naval signals and as a result stopped receiving messages valuable to the military. Navy men who had been helping the military were withdrawn to work on the naval messages, without explanation, because the new code was kept entirely secret. The result was a bad feeling between the naval and military interception services and a cessation of cooperation between them, which continued into 1917.\n\nThe SKM (sometimes abbreviated SB in German documents) was the code normally used during important actions by the German fleet. It was derived from the ordinary fleet signal books used by both British and German navies, which had thousands of predetermined instructions which could be represented by simple combinations of signal flags or lamp flashes for transmission between ships. The SKM had 34,300 instructions, each represented by a different group of three letters. A number of these reflected old-fashioned naval operations, and did not mention modern inventions such as aircraft. The signals used four symbols not present in ordinary Morse code (given the names alpha, beta, gamma and rho), which caused some confusion until all those involved in interception learnt to recognise them and use a standardised way to write them. Ships were identified by a three-letter group beginning with a beta symbol. Messages not covered by the predetermined list could be spelled out using a substitution table for individual letters.\n\nThe sheer size of the book was one reason it could not easily be changed, and the code continued in use until summer 1916. Even then, ships at first refused to use the new codebook because the replacement was too complicated, so the (FFB) did not fully replace the SKB until May 1917. Doubts about the security of the SKB were initially raised by Behring, who reported that it was not definitely known whether \"Magdeburg's\" code books had been destroyed or not, and it was suggested at the court martial enquiry into the loss that books might anyway have been recovered by Russians from the clear shallow waters where the ship had grounded. Prince Heinrich of Prussia, commander in chief of Baltic operations, wrote to the C-in-C of the High Seas Fleet, that in his view it was a certainty that secret charts had fallen into the hands of the Russians, and a probability that the codebook and key had also. The German navy relied upon the re-enciphering process to ensure security, but the key used for this was not changed until 20 October and then not changed again for another three months. The actual substitution table used for enciphering was produced by a mechanical device with slides and compartments for the letters. Orders to change the key were sent out by wireless, and frequently confusion during the changeover period led to messages being sent out using the new cipher and then being repeated with the old. Key changes continued to occur infrequently, only 6 times during 1915 from March to the end of the year, but then more frequently from 1916.\n\nThere was no immediate capture of the FFB codebook to help the Admiralty understand it, but instead a careful study was made of new and old messages, particularly from the Baltic, which allowed a new book to be reconstructed. Now that the system was understood, Room 40 reckoned to crack a new key within three to four days, and to have reproduced the majority of a new codebook within two months. A German intelligence report on the matter was prepared in 1934 by Kleikamp which concluded that the loss of \"Magdeburg's\" codebook had been disastrous, not least because no steps were taken after the loss to introduce new secure codes.\n\nThe second important code used by the German navy was captured at the very start of the war in Australia, although it did not reach the Admiralty until the end of October. The German-Australian steamer \"Hobart\" was seized off Port Phillip Heads near Melbourne on 11 August 1914. \"Hobart\" had not received news that war had broken out, and Captain J. T. Richardson and party claimed to be a quarantine inspection team. \"Hobart's\" crew were allowed to go about the ship but the captain was closely observed, until in the middle of the night he attempted to dispose of hidden papers. The (HVB) codebook which was captured contained the code used by the German navy to communicate with its merchant ships and also within the High Seas Fleet. News of the capture was not passed to London until 9 September. A copy of the book was made and sent by the fastest available steamer, arriving at the end of October.\n\nThe HVB was originally issued in 1913 to all warships with wireless, to naval commands and coastal stations. It was also given to the head offices of eighteen German steamship companies to issue to their own ships with wireless. The code used 450,000 possible four-letter groups which allowed alternative representations of the same meaning, plus an alternate ten-letter grouping for use in cables. Re-ciphering was again used but for general purposes was more straightforward, although changed more frequently. The code was used particularly by light forces such as patrol boats, and for routine matters such as leaving and entering harbour. The code was used by U-boats, but with a more complex key. However, the complications of their being at sea for long periods meant that codes changed while they were away and often messages had to be repeated using the old key, giving immediate information about the new one. German intelligence were aware in November 1914 that the HVB code had fallen into enemy hands, as evidenced by wireless messages sent out warning that the code was compromised, but it was not replaced until 1916.\n\nThe HVB was replaced in 1916 by the (AFB) together with a new method of keying. The British obtained a good understanding of the new keying from test signals, before it was introduced for real messages. The new code was issued to even more organisations than the previous one, including those in Turkey, Bulgaria and Russia. It had more groups than its predecessor but now of only two letters. The first copy to be captured came from a shot-down Zeppelin but others were recovered from sunk U-boats.\n\nA third codebook was recovered following the sinking of German destroyer SMS \"S119\" in the Battle off Texel. In the middle of October 1914, the Battle of the Yser was fought for control of the coastal towns of Dixmude and Dunkirk. The British navy took part by bombarding German positions from the sea and German destroyers were ordered to attack the British ships. On 17 October Captain Cecil Fox commanding the light cruiser \"Undaunted\" together with four destroyers, , \"Lennox\", \"Legion\" and \"Loyal\", was ordered to intercept an anticipated German attack and met four German destroyers (\"S115\", \"S117\", \"S118\", and \"S119\") heading south from Texel with instructions to lay mines. The German ships were outclassed and all were sunk after a brief battle, whereupon the commander of \"S119\" threw overboard all secret papers in a lead-lined chest. The matter was dismissed by both sides, believing the papers had been destroyed along with the ships. However, on 30 November a British trawler dragged up the chest which was passed to Room 40 (Hall later claimed the vessel had been searching deliberately). It contained a copy of the (VB) codebook, normally used by flag officers of the German Navy. Thereafter the event was referred to by Room 40 as \"the miraculous draft of fishes\".\n\nThe code consisted of 100,000 groups of 5-digit numbers, each with a particular meaning. It had been intended for use in cables sent overseas to warships and naval attachés, embassies and consulates. It was used by senior naval officers with an alternative \"Lambda\" key, none of which failed to explain its presence on a small destroyer at the start of the war. Its greatest importance during the war was that it allowed access to communications between naval attachés in Berlin, Madrid, Washington, Buenos Aires, Peking, and Constantinople.\n\nIn 1917 naval officers switched to a new code with a new key \"Nordo\" for which only 70 messages were intercepted, but the code was also broken. For other purposes VB continued in use throughout the war. Re-ciphering of the code was accomplished using a key made up of a codeword transmitted as part of the message and its date written in German. These were written down in order and then the letters in this key were each numbered according to their order of appearance in the alphabet. This now produced a set of numbered columns in an apparently random order. The coded message would be written out below these boxes starting top left and continuing down the page once a row was filled. The final message was produced by taking the column numbered '1' and reading off its contents downward, then adding on the second column's digits, and so on. In 1918 the key was changed by using the keywords in a different order. This new cipher was broken within a few days by Professor Walter Horace Bruford, who had started working for Room 40 in 1917 and specialised in VB messages. Two messages were received of identical length, one in the new system and one in the old, allowing the changes to be compared.\n\nIn early November 1914 Captain William Hall, son of the first head of Naval Intelligence, was appointed as the new DID to replace Oliver, who had first been transferred to Naval Secretary to the First Lord and then Chief of the Admiralty War Staff. Hall had formerly been captain of the battlecruiser \"Queen Mary\" but had been forced to give up sea duties due to ill health. Hall was to prove an extremely successful DID, despite the accidental nature of his appointment.\n\nOnce the new organisation began to develop and show results it became necessary to place it on a more formal basis than squatting in Ewing's office. On 6 November 1914 the organisation moved to Room 40 in the Admiralty Old Building, which was by default to give it its name. Room 40 has since been renumbered, but still exists in the original Admiralty Building off Whitehall, London, on the first floor, with windows looking inwards to a courtyard wholly enclosed by Admiralty buildings. Previous occupants of the room had complained that no one was ever able to find it, but it was on the same corridor as the Admiralty boardroom and the office of the First Sea Lord, Sir John Fisher, who was one of the few people allowed to know of its existence. Adjacent was the First Lord's residence (then Winston Churchill), who was another of those people. Others permitted to know of the existence of a signals interception unit were the Second Sea Lord, the Secretary of the Admiralty, the Chief of Staff (Oliver), the Director of Operations Division (DOD) and the assistant director, the Director of Intelligence Division (DID, Captain William Hall) and three duty captains. Admiral Sir Arthur Wilson, a retired First Sea Lord, had returned to the admiralty to work with the staff and was also included in the secret. The Prime Minister may also have been informed.\n\nAll messages received and decoded were to be kept completely secret, with copies only being passed to the Chief of Staff and Director of Intelligence. It was decided that someone from the intelligence department needed to be appointed to review all the messages and interpret them from the perspective of other information. Rotter was initially suggested for the job, but it was felt preferable to retain him in code breaking and Commander Herbert Hope was chosen, who had previously been working on plotting the movements of enemy ships. Hope was initially placed in a small office in the west wing of the Admiralty in the intelligence section, and waited patiently for the few messages which were approved for him to see. Hope reports that he attempted to make sense of what he was given and make useful observations about them, but without access to the wider information being received his early remarks were generally unhelpful. He reported to Hall that he needed more information, but Hall was unable to help. On 16 November, after a chance meeting with Fisher where he explained his difficulties, Hope was granted full access to the information together with instructions to make twice daily reports to the First Sea Lord. Hope knew nothing of cryptanalysis or German, but working with the code breakers and translators he brought detailed knowledge of naval procedures to the process, enabling better translations and then interpretations of received messages. In the interests of secrecy the intention to give a separate copy of messages to the DID was dispensed with so that only the Chief of Staff received one, and he was to show it to the First Sea Lord and Arthur Wilson.\n\nAs the number of intercepted messages increased, it became part of Hope's duties to decide which were unimportant and should just be logged, and which should be passed on outside Room 40. The German fleet was in the habit each day of reporting by wireless the position of each ship, and giving regular position reports when at sea. It was possible to build up a precise picture of the normal operation of the High Seas Fleet, indeed to infer from the routes they chose where defensive minefields had been placed and where it was safe for ships to operate. Whenever a change to the normal pattern was seen, it signalled that some operation was about to take place and a warning could be given. Detailed information about submarine movements was available. Most of this information, however, was retained wholly within Room 40 although a few senior members of the Admiralty were kept informed, as a huge priority was placed by the Staff upon keeping secret the British ability to read German transmissions.\n\nJellicoe, commanding the Grand Fleet, on three occasions requested from the Admiralty that he should have copies of the codebook which his cruiser had brought back to Britain, so that he could make use of it intercepting German signals. Although he was aware that interception was taking place, little of the information ever got back to him, or it did so very slowly. No messages based upon Room 40 information were sent out except those approved by Oliver personally (except for a few authorised by the First Lord or First Sea Lord). Although it might have been impractical and unwise for code breaking to have taken place on board ship, members of Room 40 were of the view that full use was not being made of the information they had collected, because of the extreme secrecy and being forbidden to exchange information with the other intelligence departments or those planning operations.\n\nThe British and German interception services began to experiment with direction-finding radio equipment in the start of 1915. Captain Round, working for Marconi, had been carrying out experiments for the army in France and Hall instructed him to build a direction-finding system for the navy. At first this was sited at Chelmsford but the location proved a mistake and the equipment was moved to Lowestoft. Other stations were built at Lerwick, Aberdeen, York, Flamborough Head and Birchington and by May 1915 the Admiralty was able to track German submarines crossing the North Sea. Some of these stations also acted as 'Y' stations to collect German messages, but a new section was created within Room 40 to plot the positions of ships from the directional reports. A separate set of five stations was created in Ireland under the command of the Vice Admiral at Queenstown for plotting ships in the seas to the west of Britain and further stations both within Britain and overseas were operated by the Admiral commanding reserves.\n\nThe German navy knew of British direction-finding radio and in part this acted as a cover, when information about German ship positions was released for operational use. The two sources of information, from directional fixes and from German reports of their positions, complemented each other. Room 40 was able to observe, using intercepted wireless traffic from Zeppelins which were given position fixes by German directional stations to help their navigation, that the accuracy of British systems was better than their German counterparts. This was explainable by the wider baseline used in British equipment.\n\nRoom 40 had very accurate information on the positions of German ships but the Admiralty's priority remained to keep the existence of this knowledge secret. Hope was shown the regular reports created by the Intelligence Division about German ship whereabouts so that he might correct them. This practice was shortly discontinued, for fear of giving away their knowledge. From June 1915, the regular intelligence reports of ship positions were no longer passed to all flag officers, only to Jellicoe, who was the only person to receive accurate charts of German minefields prepared from Room 40 information. Some information was passed to Beatty (commanding the battlecruisers), Tyrwhitt (Harwich destroyers) and Keyes (submarines) but Jellicoe was unhappy with the arrangement. He requested that Beatty should be issued with the \"Cypher B\" (reserved for secret messages between the Admiralty and him) to communicate more freely and complained that he was not getting sufficient information. \n\nAll British ships were under instructions to use radio as sparingly as possible and to use the lowest practical transmission power. Room 40 had benefited greatly from the free chatter between German ships, which gave them many routine messages to compare and analyse, and from the German habit of transmitting at full power, making the messages easier to receive. Messages to Scapa were never to be sent by wireless, and when the fleet was at sea, messages might be sent using lower power and relay ships (including private vessels), to make German interception more difficult. No attempts were made by the German fleet to restrict its use of wireless until 1917 and then only in response to perceived British use of direction finding, not because it believed messages were being decoded.\n\nRoom 40 played an important role in several naval engagements during the war, notably in detecting major German sorties into the North Sea that led to the Battle of Dogger Bank in 1915 and the Battle of Jutland in 1916, as the British fleet was sent out to intercept them. Its most notable contribution was in decrypting the Zimmermann Telegram, a telegram from the German Foreign Office sent in January 1917 via Washington to its ambassador Heinrich von Eckardt in Mexico. It has been called the most significant intelligence triumph for Britain during World War I, and one of the earliest occasions on which a piece of signals intelligence influenced world events.\n\nIn the telegram's plaintext, Nigel de Grey and William Montgomery learned of German Foreign Minister Arthur Zimmermann's offer to Mexico of United States' territories of Arizona, New Mexico, and Texas as an enticement to join the war as a German ally. The telegram was passed to the U.S. by Captain Hall, and a scheme was devised (involving a still unknown agent in Mexico and a burglary) to conceal how its plaintext had become available and also how the U.S. had gained possession of a copy. The telegram was made public by the United States, which declared war on Germany on 6 April 1917, entering the war on the Allied side.\n\nOther staff of Room 40 included Frank Adcock, John Beazley, Francis Birch, Walter Horace Bruford, William 'Nobby' Clarke, Alastair Denniston, Frank Cyril Tiarks and Dilly Knox.\n\nIn 1919, Room 40 was deactivated and its function merged with the British Army's intelligence unit MI1b to form the Government Code and Cypher School (GC&CS). This unit was housed at Bletchley Park during the Second World War and subsequently renamed Government Communications Headquarters (GCHQ) and relocated to Cheltenham.\n\n"}
{"id": "18878206", "url": "https://en.wikipedia.org/wiki?curid=18878206", "title": "SAF Tehnika", "text": "SAF Tehnika\n\nSAF Tehnika () is a designer, producer and distributor of digital Microwave Data transmission equipment. SAF Tehnika products provide wireless backhaul solutions for digital voice and data transmission to mobile and fixed network operators, data service providers, governments and private companies. The Company’s product portfolio consists of microwave point-to-point radios for licensed and license free frequency bands as well as unique spectrum analyzer Spectrum Compact. SAF Tehnika also provides wide range of customized microwave solutions for various applications, such as Broadcasting and Low latency networks.\n\nIn 2004 SAF Tehnika acquired a Swedish company, – SAF Tehnika Sweden, a fully owned subsidiary, based in Gothenburg, however in 2008 it was bought out by its management, changing the name to \"Trebax AB\". In May 2004 the Company launched a successful IPO with initial market capitalization of more than €50 million, with substantial subscriptions from institutional investors. The Company is listed on the NASDAQ OMX Riga under the symbol SAF1R.\n\nDuring the 2000s SAF Tehnika has been taking next steps towards global expansion by developing a large network of authorized partners and sales representatives all over the globe, most notable being the opening of SAF Tehnika North America office and warehouse facilities in Denver in 2013.\n\n\nSAF Integra is one of the latest technological developments in the field of microwave data transmission from SAF Tehnika. This next generation microwave radio platform combines antenna, radio part, and the mounting brackets into a single unit. \nIntegra has a capacity of up to 1Gbit/s throughput with header compression in 1+0 configuration. The latest modem technology solution enables covering longer distances due to better system gain at 256QAM and with hitless ACM switching up to 1024QAM. With optional ETSI Class 4 antennas Integra may be deployed in a dense microwave environment. The bandwidth range goes from 3.5 MHz up to 60 MHz in single hardware design. Direct radio and antenna integration allows saving the time usually spent on radio-to-antenna assembly and sealing. \nIntegra is a light, energy efficient carrier-grade system. Integration of next generation microwave radio with high and super high performance antennas into a single unit translates into a lower total cost of ownership, as well as less time spent on the installation site, and better link reliability even in densely served areas. Its body is made from an EMC-compliant plastic material ensuring complete corrosion resistance surpassing that of the classic microwave antennas. \nThe mounting bracket is optimized for wind-load reduction, ensuring a considerably higher wind resistance, which keeps the radio link up even in rugged weather. Another significant addition is the software controlled LED which indicates whether the radio has been synchronized with the remote end and is operating properly.\nBuilt-in multi-core network packet processor enables Carrier Ethernet performance with features like Synchronous Ethernet, header compression, and RADIUS authentication. Three Gigabit Ethernet ports per radio allow using the built-in high performance Gigabit switch in all-outdoor environment and avoid additional cost of expensive rack-mount switches and facilitate shelter-free installations.\n\nSpectrum Compact is a handheld microwave spectrum analyzer. Spectrum Compact is a measurement solution for the 6 – 40 GHz licensed microwave frequency bands; E-band (70 - 87 GHz) and V-band (56 – 67 GHz) frequencies. Designed specifically for comfortable outdoor use by network engineers in a variety of challenging environments, this battery-powered device is designed for any microwave radio engineer performing equipment installation or gathering data for site planning purposes. The LCD touchscreen ensures smooth and intuitive onsite use, and the SMA connector allows the Spectrum Compact to integrate with any vendors antenna or waveguide system. \nThe Spectrum Compact makes it possible to detect existing interference on installed paths or available radio channels with high precision. Data logging of all spectrum scans is available inside of the Spectrum compact with enhanced data processing and analysis available via SAF Tehnika designed PC software. \n\nSG Compact is microwave signal generator, which is an essential tool for antenna alignment and testing and top choice for LoS verification applications. Device is perfect for different microwave system analysis and measurement applications.\n\nCFIP Lumina and Phoenix are two products by SAF Tehnika launched in late 2009. CFIP Lumina follows the success of CFIP 108, providing full IP traffic of up to 366Mbit/s and offering an outdoor solution, working in 20 MHz, 28 MHz, 30 MHz, 40 MHz, 50 MHz and 56 MHz channel bandwidths. It also offers such features as Automatic transmit power control and ACM. CFIP Lumina is a first Native Ethernet/IP power efficient all outdoor 360Mbit/s radio system with 256QAM and built-in advanced GigE switch (QoS, STPs, VLAN support).\n\nCFIP-106/108 is primarily designed for IP networks and provides Fast Ethernet interface with capacities from 8 Mbit/s up to full duplex 100 Mbit/s Fast Ethernet. In addition to that, CFIP-108 has a 4xE1 port for legacy equipment connectivity and for use in hybrid TDM/IP networks.\n\nCFIP PhoeniX split mount system is designed to fit in a classic telecom architecture with a radio located outdoor and a sheltered indoor unit. CFIP PhoeniX also enables transition from TDM networks to hybrid TDM/IP networks providing up to 20E1 + GigE. Total maximum capacity is up to 363 Mbit/s full duplex.\n\nCFIP PhoeniX M split mount system consists of PhoeniX M IDU and PhoeniX ODU (XPIC functionality supported). PhoeniX M IDU provides up to 63E1 and 2xSTM-1 bringing the total maximum capacity to 360 Mbit/s full duplex.\n\nCFIP PhoeniX C split mount system is designed to fit in a classic telecom architecture with a radio located outdoor and a sheltered indoor unit. CFIP PhoeniX C system has a compact indoor unit that enables transition from TDM networks to hybrid TDM/IP networks providing up to 64E1, Gigabit Ethernet ports, or up to 16 ASI ports for transferring video signal. Total maximum capacity is up to 366 Mbit/s full duplex.\n\nA solution for industrial applications where fiber is not available. CFIP Marathon 300 MHz / 1.4 GHz point-to-point long-haul microwave system is a solution for industrial applications where fiber is not available. Primarily designed for remote and rural areas Marathon Full Indoor Unit can establish a link in adverse weather conditions and over long distances, frequently exceeding 100 km. SAF Tehnika Marathon solution is particularly recommended for utility (electricity, water), oil & gas, public safety, and transport companies who are looking for reliable data & voice transmission.\n\nSAF FreeMile 5 GE MIMO All-outdoor system is a Licence-free 5 GHz gibabit radio for Ethernet packet data transmission. Making use of ground breaking 2x2 MIMO technology, the SAF FreeMile 5 GE MIMO delivers up to 210Mbit/s (105Mbit/s full-duplex) of real throughput combined with a high packets-per-second performance.\n\nSAF FreeMile FODU - a new generation license-free 17/24 GHz ISM band radio for Ethernet packet data and E1 voice transmission. The new radio system combines features traditionally associated with ISM radios – interference-free operation, high availability, Carrier grade full duplex capacity with no cost of licensing and quick installation. SAF FreeMile radio also offers a user-friendly Web browser based management interface and straightforward installation process.\n\nOutdoor Branching Unit was launched in June 2015. OBU is built by using high quality materials for all components and special narrow bandwidth channel filters that are temperature resistant and waveguide circulators for best electrical and mechanical characteristics. Outdoor Branching Unit is made in compact form factor for interconnecting up to 4 radios per OBU.\n\nTechnical solution and project design competence for microsecond class ultra low latency networks with CFIP Low Latency Repeater as the key product with the lowest latency on the market.\n\nSAF NMS is an integrated toolset consisting of hardware and software, used to monitor and administer a network, made of SAF designed microwave radio equipment, locally and remotely in a convenient and user-friendly manner. A new v4.1 version of SAF NMS is now available.\nSAF has created a smartphone application for monitoring microwave network environment – SAF Mobile NMS. It is an Android (ver. 2.2 or higher) based client application for SAF NMS and provides enhanced real-time overview of the actual equipment and network status via instant push-notification service.\n\nAranet is an environmental IoT monitoring solution for a variety of businesses. The wireless temperature and humidity monitoring systems offer innovative 3km/1.9mi line-of-sight range between the wireless sensors and gateway. Aranet is suitable for large industrial site applications as well as smaller compounds. Aranet product line consists of two solutions – Aranet MINI for smaller site size applications, and the Aranet PRO for industrial-grade deployment. \n\n"}
{"id": "690346", "url": "https://en.wikipedia.org/wiki?curid=690346", "title": "Signal strength in telecommunications", "text": "Signal strength in telecommunications\n\nIn telecommunications, particularly in radio frequency, signal strength (also referred to as field strength) refers to the transmitter power output as received by a reference antenna at a distance from the transmitting antenna. High-powered transmissions, such as those used in broadcasting, are expressed in dB-millivolts per metre (dBmV/m). For very low-power systems, such as mobile phones, signal strength is usually expressed in dB-microvolts per metre (dBµV/m) or in decibels above a reference level of one milliwatt (dBm). In broadcasting terminology, 1 mV/m is 1000 µV/m or 60 dBµ (often written dBu).\n\n\nThe electric field strength at a specific point can be determined from the power delivered to the transmitting antenna, its geometry and radiation resistance. Consider the case of a center-fed half-wave dipole antenna in free space, where the total length L is equal to one half wavelength (λ/2). If constructed from thin conductors, the current distribution is essentially sinusoidal and the radiating electric field is given by\n\nwhere formula_2 is the angle between the antenna axis and the vector to the observation point, formula_3 is the peak current at the feed-point, formula_4 is the permittivity of free-space, formula_5 is the speed of light in a vacuum, and formula_6 is the distance to the antenna in meters. When the antenna is viewed broadside (formula_7) the electric field is maximum and given by \n\nSolving this formula for the peak current yields\n\nThe average power to the antenna is\n\nwhere formula_11 is the center-fed half-wave antenna’s radiation resistance. Substituting the formula for formula_12 into the one for formula_13 and solving for the maximum electric field yields\n\nTherefore, if the average power to a half-wave dipole antenna is 1 mW, then the maximum electric field at 313 m (1027 ft) is 1 mV/m (60 dBµ).\n\nFor a short dipole (formula_15) the current distribution is nearly triangular. In this case, the electric field and radiation resistance are\n\nUsing a procedure similar to that above, the maximum electric field for a center-fed short dipole is\n\nAlthough there are cell phone base station tower networks across many nations globally, there are still many areas within those nations that do not have good reception. Some rural areas are unlikely to ever be covered effectively since the cost of erecting a cell tower is too high for only a few customers. Even in areas with high signal strength, basements and the interiors of large buildings often have poor reception.\n\nWeak signal strength can also be caused by destructive interference of the signals from local towers in urban areas, or by the construction materials used in some buildings causing significant attenuation of signal strength. Large buildings such as warehouses, hospitals and factories often have no usable signal further than a few metres from the outside walls.\n\nThis is particularly true for the networks which operate at higher frequency since these are attenuated more by intervening obstacles, although they are able to use reflection and diffraction to circumvent obstacles.\n\nThe estimated received signal strength in an active RFID tag can be estimated as follows:\n\nIn general, you can take the path loss exponent into account:\n\nThe effective path loss depends on frequency, topography, and environmental conditions.\n\nActually, one could use any known \"signal power\" dBm at any distance r as a reference:\n\nWhen we measure cell distance \"r\" and received power pairs,\nwe can estimate the mean cell radius as follows:\n\nSpecialized calculation models exist to plan the location of a new cell tower, taking into account local conditions and radio equipment parameters, as well as consideration that mobile radio signals have line-of-sight propagation, unless reflection occurs.\n\n\n"}
{"id": "1584835", "url": "https://en.wikipedia.org/wiki?curid=1584835", "title": "Smart key", "text": "Smart key\n\nA smart key is an electronic access and authorization system that is available either as standard equipment or as an option in several car models. It was first developed by Siemens in 1995 and introduced by Mercedes-Benz under the name \"Key-less Go\" in 1998 on the W220 S-Class, after the design patent was filed by Daimler-Benz on May 17, 1997.\n\nThe smart key allows the driver to keep the key fob pocketed when unlocking, locking and starting the vehicle. The key is identified via one of several antennas in the car's bodywork and a radio pulse generator in the key housing. Depending on the system, the vehicle is automatically unlocked when a button or sensor on the door handle or trunk release is pressed. Vehicles with a smart-key system have a mechanical backup, usually in the form of a spare key blade supplied with the vehicle. Some manufacturers hide the backup lock behind a cover for styling.\nVehicles with a smart-key system can disengage the immobilizer and activate the ignition without inserting a key in the ignition, provided the driver has the key inside the car. On most vehicles, this is done by pressing a starter button or twisting an ignition switch.\n\nWhen leaving a vehicle that is equipped with a smart-key system, the vehicle is locked by either pressing a button on a door handle, touching a capacitive area on a door handle, or simply walking away from the vehicle. The method of locking varies across models.\n\nSome vehicles automatically adjust settings based on the smart key used to unlock the car. User preferences such as seat positions, steering wheel position, exterior mirror settings, climate control (e.g. temperature) settings, and stereo presets are popular adjustments. Some models, such as the Ford Escape, even have settings to prevent the vehicle from exceeding a maximum speed if it has been started with a certain key.\n\nManufacturers use keyless authorization systems under different names:\n\n\nIn 2005, the UK motor insurance research expert Thatcham introduced a standard for keyless entry, requiring the device to be inoperable at a distance of more than 10 cm from the vehicle. In an independent test, the Nissan Micra's system was found to be the most secure, while certain BMW and Mercedes keys failed, being theoretically capable of allowing cars to be driven away while their owners were refueling. Despite these security vulnerabilities, auto theft rates have decreased 7 percent between 2009 and 2010, and the National Insurance Crime Bureau credits smart keys for this decrease.\n\nSmartKeys was developed by Siemens in the mid-1990s and introduced by Mercedes-Benz in 1997 to replace the infrared security system introduced in 1989. Daimler-Benz filed the first patents for SmartKey on February 28, 1997 in German patent offices, with multifunction switchblade key variants following on May 17, 1997. The device entailed a plastic key to be used in place of the traditional metal key. Electronics that control locking systems and the ignitions made it possible to replace the traditional key with a sophisticated computerized \"Key\". It is considered a step up from remote keyless entry. The SmartKey adopts the remote control buttons from keyless entry, and incorporates them into the SmartKey fob.\n\nOnce inside a Mercedes-Benz vehicle, the SmartKey fob, unlike keyless entry fobs, is placed in the ignition slot where a starter computer verifies the rolling code. Verified in milliseconds, it can then be turned as a traditional key to start the engine. The device was designed with cooperation of Siemens Automotive and Huf exclusively for Mercedes-Benz, but many luxury manufacturers have implemented similar technology based on the same idea. In addition to the SmartKey, Mercedes-Benz now integrates as an option Keyless Go; this feature allows the driver to keep the SmartKey in their pocket, yet giving them the ability to open the doors, trunk as well as starting the car without ever removing it from their pocket.\n\nThe SmartKey's electronics are embedded in a hollow, triangular piece of plastic, wide at the top, narrow at the bottom, squared-off at the tip with a half-inch-long insert piece. The side of the SmartKey also hides a traditional Mercedes-Benz key that can be pulled out from a release at top. The metal key is used for valet purposes such as locking the glovebox and/or trunk before the SmartKey is turned over to a parking attendant. Once locked manually, the trunk cannot be opened with the SmartKey or interior buttons. The key fob utilizes a radio-frequency transponder to communicate with the door locks, but it uses infrared to communicate with the engine immobilizer system. Original SmartKeys had a limited frequency and could have only been used in line-of-sight for safety purposes. The driver can also point the smart key at the front driver side door while pushing and holding the unlock button on the SmartKey and the windows and the sunroof will open in order to ventilate the cabin. Similarly, if the same procedure is completed while holding the lock button, the windows and sunroof will close. In cars equipped with the Active Ventilated Seats, the summer opening feature will activate the seat ventilation in addition to opening the windows and sunroof.\n\nKeyless Go (also: Keyless Entry / Go; Passive Entry / Go) is Mercedes' term for an automotive technology which allows a driver to lock and unlock a vehicle without using the corresponding SmartKey buttons. Once a driver enters a vehicle with an equipped Keyless Go SmartKey or Keyless Go wallet-size card, they have the ability to start and stop the engine, without inserting the SmartKey. A transponder built within the SmartKey allows the vehicle to identify a driver. An additional safety feature is integrated into the vehicle, making it impossible to lock a SmartKey with Keyless Go inside a vehicle.\nThe system works by having a series of LF (low frequency 125 kHz) transmitting antennas both inside and outside the vehicle. The external antennas are located in the door handles. When the vehicle is triggered, either by pulling the handle or touching the handle, an LF signal is transmitted from the antennas to the key. The key becomes activated if it is sufficiently close and it transmits its ID back to the vehicle via RF (Radio frequency >300 MHz) to a receiver located in the vehicle. If the key has the correct ID, the PASE module unlocks the vehicle.\n\nThe hardware blocks of a Keyless Entry / Go Electronic control unit ECU are based on its functionality:\n\nThe smart key determines if it is inside or outside the vehicle by measuring the strength of the LF fields. In order to start the vehicle, the smart key must be inside the vehicle.\n\nThe mark of a good passive entry system is that the user never hits the \"wall\". This happens when the user pulls the door handle to its full extent before the door is unlocked. The handle has to be released and pulled again to gain access. Good systems have an override feature that allows the doors to be opened more quickly.\n\nIt is important that the vehicle can't be started when the user and therefore the smart key is outside the vehicle. This is especially important at fueling stations where the user is very close to the vehicle. The internal LF field is allowed to overshoot by a maximum of 10 cm to help minimise this risk. Maximum overshoot is usually found on the side windows where there is very little attenuation of the signal.\nA second scenario exists under the name \"relay station attack\" (RSA). The RSA is based on the idea of reducing the long physical distance between the car and the regular car owner's SmartKey. Two relay stations will be needed for this: The first relay station is located nearby the car and the second is close to the SmartKey. So on first view, the Keyless Entry / Go ECU and the SmartKey could communicate together. A third person at the car could pull the door handle and the door would open. However, in every Keyless Entry / Go system provisions exist to avoid a successful two-way communication via RSA. Some of the most known are:\n\n\nFurthermore, Keyless Entry / Go communicates with other \"Control Units\" within the same vehicle. Depending on the electric car architecture, the following are some \"Control Systems\" that can be enabled or disabled:\n\nDead spots are a result of the maximum overshoot requirement from above. The power delivered to the internal LF antennas has to be tuned to provide the best performance i.e. minimum dead spots and maximum average overshoot. Dead spots are usually near the extremities of the vehicle e.g. the rear parcel shelf.\n\nIf the battery in the smart key becomes depleted, it is necessary for there to be a backup method of opening and starting the vehicle. Opening is achieved by an emergency (fully mechanical) key blade usually hidden in the smart key. On many cars emergency starting is achieved by use of an inductive coupling. The user either has to put the key in a slot or hold it near a special area on the cockpit, where there is an inductive coil hidden behind which transfers energy to a matching coil in the dead key fob using inductive charging.\n\nSlots have proven to be problematic, as they can go wrong and the key becomes locked in and cannot be removed. Another problem with the slot is it can't compensate for a fob battery below certain operating threshold. Most smart key batteries are temperature sensitive causing the fob to become intermittent, fully functional, or inoperative all in the same day.\n\nA Keyless Entry / Go system should be able to detect and handle most of the following cases:\n\n\nThe system is based on a technology invented by Siemens VDO called PASE: Passive Start and Entry System. It operates in the ISM band of radio frequencies. Keyless Entry / Go was introduced first by Mercedes-Benz in the S-Class car series in 1998.\n\nToday a Keyless Entry / Go system is a state-of-the art technology and still has a lot of potential to optimise. Here are some general trends of the advance (AD) and series development (SD):\n\n\nA test by ADAC revealed that 20 car models with Keyless Go could be entered and driven away without the key. In 2014, 6,000 cars (about 17 per day) were stolen using keyless entry in London.\n\n\n"}
{"id": "2319202", "url": "https://en.wikipedia.org/wiki?curid=2319202", "title": "Snow gauge", "text": "Snow gauge\n\nA snow gauge is a type of instrument used by meteorologists and hydrologists to gather and measure the amount of solid precipitation (as opposed to liquid precipitation that is measured by a rain gauge) over a set period of time.\n\nThe first use of snow gauges were precipitation gauges that was widely used in 1247 during the Southern Song dynasty to gather meteorological data. The Song Chinese mathematician and inventor Qin Jiushao records the use of gathering rain and snowfall measurements in the Song mathematical treatise \"Mathematical Treatise in Nine Sections\". The book discusses the use of large conical or barrel-shaped snow gauges made from bamboo situated in mountain passes and uplands which are speculated to be first referenced to snow measurement.\n\nThe snow gauge consists of two parts, a copper catchment container and the funnel shaped gauge itself. The actual gauge is mounted on a pipe outdoors and is approximately 1.5 m (4 ft 11 in) high, while the container is 51.5 cm (4 ft 2.25 in) long.\n\nWhen snow is collected, the container is removed and replaced with a spare one. The snow is then melted while it is still in the container, and then poured into a glass measuring graduate. While the depth of snow is normally measured in centimetres, the measurement of melted snow (water equivalent) is in millimetres.\n\nAn estimate of the snow depth can be obtained by multiplying the water equivalent by ten. However, this multiplier can vary over a wide range (many say the range is 5 to 30, but the National Snow and Ice Data Center has quoted a range as wide as 3 to 100), depending on the water content of the snow (how \"dry\" it is), so this only provides, at best, a rough estimate of snow depth.\n\nThe snow gauge suffers from the same problem as that of the rain gauge when conditions are windy. If the wind is strong enough, then the snow may be blown across the wind gauge and the amount of snow fallen will be under-reported. However, due to the shape and size of the funnel this is a minor problem.\n\nIf the wind is very strong and a blizzard occurs then extra snow may be blown into the gauge and the amount of snow fallen will be over-reported. In this case the observer must judge how much of the water is from snow blown into the container and how much is fallen snow.\n\nAnother problem occurs when both snow and rain fall before the observer has time to change the gauge. In all of these cases the observer must judge how much of the water is snow and how much is rain.\n\nRemote reading gauges, such as used by weather stations, work similarly to rain gauges. They have a large catch area (such as a drum sawn in half, top to bottom) which collects snow until a given weight is collected. When this critical weight is reached, it tips and empties the snow catch. This dumping trips a switch, sending a signal. The collection then repeats. If the catch container has a heater in it, it measures the snow weight accurately. It is also possible to tip based on volume instead of weight, with appropriate fill sensing.\n\nAnother snow sensor called a snow pillow looks like a round bag lying on the ground. Inside the pillow is a liquid such as an environmentally safe antifreeze. Usually the snow pillow will be connected to a manometer. The manometer reading will vary based on how much snow is sitting on the pillow. This type of sensor works well for many locations but is more difficult to use in areas of hard blowing snow.\n"}
{"id": "19554718", "url": "https://en.wikipedia.org/wiki?curid=19554718", "title": "Space elevator construction", "text": "Space elevator construction\n\nThree basic approaches for constructing a space elevator have been proposed: First, using in-space resources to manufacture the whole cable in space. Second, launching and deploying a first seed cable and successively reinforcing the seed cable by additional cables, transported by climbers. Third, spooling two cables down and then connecting the ends, forming a loop. \n\nThere are two approaches to constructing a space elevator. Either the cable is manufactured in space or it is launched into space and gradually reinforced by additional cables, transported by climbers into space. Manufacturing the cable in space could be done in principle by using an asteroid or Near-Earth object. \n\nOne early plan involved lifting the entire mass of the elevator into geostationary orbit, and lowering one cable downwards towards the Earth's surface while simultaneously another cable is deployed upwards directly away from the Earth's surface.\n\nTidal forces (gravity and centrifugal force) would naturally pull the cables directly towards and directly away from the Earth and keep the elevator balanced around geostationary orbit. As the cable is deployed, Coriolis forces would pull the upper portion of the cable somewhat to the West and the lower portion of the cable somewhat to the East; this effect can be controlled by varying the deployment speed.\n\nHowever, this approach requires lifting hundreds or even thousands of tons on conventional rockets, an expensive proposition.\n\nBradley C. Edwards, former Director of Research for the Institute for Scientific Research (ISR), based in Fairmont, West Virginia proposed that, if nanotubes with sufficient strength could be made in bulk, a space elevator could be built in little more than a decade, rather than the far future. He proposed that a single hair-like 20-ton 'seed' cable be deployed in the traditional way, giving a very lightweight elevator with very little lifting capacity. Then, progressively heavier cables would be pulled up from the ground along it, repeatedly strengthening it until the elevator reaches the required mass and strength. This is much the same technique used to build suspension bridges. The length of this cable is 35,786 km or 35,786,000 m. A 20-ton cable would weigh about 1.12 grams per m. \n\nThis is a less well developed design, but offers some other possibilities.\n\nIf the cable provides a useful tensile strength to density of about 48.1 GPa/(kg/m) or above, then a constant width cable can reach beyond geostationary orbit without breaking under its own weight. The far end can then be turned around and passed back down to the Earth forming a constant width loop, which would be kept spinning to avoid tangling. The two sides of the loop are naturally kept apart by coriolis forces due to the rotation of the Earth and the loop. By increasing the thickness of the cable from the ground a very quick (exponential) build-up of a new elevator may be performed (it helps that no active climbers are needed, and power is applied mechanically.) However, because the loop runs at constant speed, joining and leaving the loop may be somewhat challenging, and the carrying capacity of such a loop is lower than a conventional tapered design.\n\nCurrently, the cable seeding design and the space manufacturing design are considered. The space manufacturing design would use a carbonacious asteroid or near-Earth object for mining its material and producing a carbon nanotube cable. The cable would then be transported back to geostationary orbit and spooled down. Although this approach shifts the construction complexity away from the use of climbers in the cable seeding design, it increases the complexity of the required in-space infrastructure. \n\nThe cable seeding design could be rendered infeasible in case the material strength is considerably lower than was projected by Brad Edwards.\n\nCurrent technological status of the cable seeding design:\na) It would take 5 days to reach a geostationary altitude of 36,000 km with this speed.\n\n\n"}
{"id": "3721032", "url": "https://en.wikipedia.org/wiki?curid=3721032", "title": "Supersonic wind tunnel", "text": "Supersonic wind tunnel\n\nA supersonic wind tunnel is a wind tunnel that produces supersonic speeds (1.2<M<5)\nThe Mach number and flow are determined by the nozzle geometry. The Reynolds number is varied by changing the density level (pressure in the settling chamber). Therefore, a high pressure ratio is required (for a supersonic regime at M=4, this ratio is of the order of 10). Apart from that, condensation of moisture or even gas liquefaction can occur if the static temperature becomes cold enough. This means that a supersonic wind tunnel usually needs a drying or a pre-heating facility.\nA supersonic wind tunnel has a large power demand, so most are designed for intermittent instead of continuous operation.\n\nOptimistic estimate:\nPressure ratio formula_1 the total pressure ratio over normal shock at M in test section:\n\nformula_2\n\nExamples:\n\nTemperature in the test section:\n\nformula_3\n\nwith formula_4 = 330 K: formula_5 = 70 K at formula_6 = 4\n\nThe velocity range is limited by reservoir temperature\n\nThe power required to run a supersonic wind tunnel is enormous, of the order of 50 MW per square meter of test section cross-sectional area. For this reason most wind tunnels operate intermittently using energy stored in high-pressure tanks. These wind tunnels are also called intermittent supersonic blowdown wind tunnels (of which a schematic preview is given below). Another way of achieving the huge power output is with the use of a vacuum storage tank. These tunnels are called indraft supersonic wind tunnels, and are seldom used because they are restricted to low Reynolds numbers. Some large countries have built major supersonic tunnels that run continuously; one is shown in the photo. \nOther problems operating a supersonic wind tunnel include:\nTunnels such as a Ludwieg tube have short test times (usually less than one second), relatively high Reynolds number, and low power requirements.\n\n\n"}
{"id": "18037090", "url": "https://en.wikipedia.org/wiki?curid=18037090", "title": "Tax compliance software", "text": "Tax compliance software\n\nTax compliance software is software that assists tax compliance, and may cover income tax, corporate tax, VAT, service tax, customs, sales tax, use tax, or other taxes its users may be required to pay. The software automatically calculates a user's tax liabilities to the government, keeps track of all transactions (in case of indirect taxes), keeps track of eligible tax credits, etc. The software can also generate forms or filings needed for tax compliance. The software will have pre-defined tax rates and slabs and can allocate income or revenue in the right slab itself. The aim of the software is to provide the user with easy way to calculate tax payment and minimize any human error.\nTax compliance software has been present in developed countries for long in the form of tax calculators mainly for direct taxes, such as income tax and corporate tax. Gradually some more complex and customized tax compliance software has been designed and developed by organizations around the globe.\n\nTax compliance software can be divided into two main categories: direct and indirect tax compliance software.\n\nA direct tax is one paid directly to the government by the persons (juristic or natural) on whom it is imposed (often accompanied by a tax return filed by the taxpayer). Examples include income tax, corporate tax, and transfer tax such as estate tax and gift tax.\nBasic software for income tax in the form of a tax calculator, and are now widely used. For example, the Government of India provides an income tax calculator on their website.\nCorporate tax compliance software has also been in existence for years, more often than not within the company's Finance & Accounting software or financial module of ERPs. These suites have the facilities to maintain the company's General Ledger, Cash Management, Accounts Payable, Accounts Receivable, Fixed Assets along with some basic taxes.\n\nAn indirect tax (such as sales tax, value added tax (VAT), or goods and services tax (GST)) is a tax collected by an intermediary (such as a retail store) from the person who bears the ultimate economic burden of the tax (such as the customer). The intermediary later files a tax return and forwards the tax proceeds to government with the return.\nIndirect Tax compliance has always been much more complex as compared to the direct taxes. Many indirect tax compliance programs have separate modules for VAT, Service Tax, Customs etc.\n\nValue Added Tax (VAT) legislation has a common structure across countries (and states in case of India). Software must be customized to each country, however, because of differences in the some areas, such as handling of credit of capital goods, sale of scrap and second-hand goods, formats of mandatory submissions and audit exercises.\n\nService tax compliance software often include maintenance of credit registers, handling reverse charges, rebate claims on export of services along with payment of tax and filing of returns.\n\nSome popular tax preparation software include:\n\n"}
{"id": "12152911", "url": "https://en.wikipedia.org/wiki?curid=12152911", "title": "Tedder (machine)", "text": "Tedder (machine)\n\nA tedder (also called hay tedder) is a machine used in haymaking. It is used after cutting and before windrowing, and uses moving forks to aerate or \"wuffle\" the hay and thus speed up the process of hay-making. The use of a tedder allows the hay to dry (\"cure\") better, which results in improved aroma and color.\n\nThe tedder came into use in the second half of the nineteenth century. While Charles Wendel claims in his \"Encyclopedia of American farm implements & antiques\" that the machine wasn't introduced to the United States until the 1880s, there are enough indications that the tedder was in use in the 1860s—\"The New York Times\" reports on its efficacy in 1868, and in that same year the Annual Report of the Commissioner of Agriculture in Maine comments on the American-made Hubbard's hay tedder, which had been on the market since 1863; according to the Maine report, in 1859 the machine was \"an implement lately imported from England.\"\n\nThe action of the tedder is described, in the late 19th and early 20th century, as being used to \"stir\" or \"scatter\" cut hay in the field.\n\nThe original tedder is a farm tool on two wheels pulled by a horse; the rotation of the axle drives a gear which operates a \"number of arms with wire tines or fingers at the lower ends.\" The tines pick up the hay and disperse it; usually, the height at which the tines pick up the hay can be adjusted.\n\nIn an early, simple hay tedder described in 1852 and manufactured in Edinburgh by the company of Mr. Slight, the two wheels, via a spur wheel and a pinion, drive a set of light wheels, the \"rake wheels\"; on these two rake wheels are mounted eight rakes, which pick up and disperse the hay. A later \"English hay-tedder\" uses two separate cylinders with rotating forks that can be reversed to lay the hay down lightly for improved exposure to air.\n\nAmerican machines, such as those made by Garfield, by Mudgett, and by Bullard (Ezekiel W. Bullard of Barre, Massachusetts, is credited in one source with the invention of the machine, nicknamed \"the grasshopper\"), typically used a system with a revolving crank in the middle of the arm and a lever at the upper end, or a system whereby rotating wheels moved the forks up and down. The first tedder widely available on the American market was the already mentioned \"Bullard's Hay Tedder\", which had forks moving up and down on a compound crank, working in a motion described as \"the energetic scratching of a hen.\" The \"American Hay Tedder\", made by the Ames Plow Company of Boston and described in 1869 as a \"new machine, remarkable for its simplicity and perfection of working, was more like the British machine in its rotational operation.\n\nSome tedders have the rotating tines enclosed inside a solid structure to increase the force applied to the hay. Other similar machines included \"the wuffler\" and \"the acrobat\". The wuffler shuffles the hay in a manner similar to the tedder. The acrobat may be used also for turning, and for rowing hay up ready for baling.\n\nOn two opposing horizontal gyroscopes, which are pto-driven, are mounted obliquely downward standing tines. These refer to the green waste and throw it back. Due to the rear-mounted collecting baskets a windrowing is as possible with a Rake. Their distribution is low because of the limited job performance.\n\nIts development was of great importance to agriculture, since it saved labor and thus money: using a tedder, a man and a horse could do as much work as fifteen laborers. It also resulted in greater economy, since cut grass could be turned into hay the same day even if it had become wet or been trampled by horses and before its nutritional value could be reduced by repeated soaking from rain. Especially in humid areas (such as the Eastern United States), the invention of the tedder added greatly to improved hay production from such crops as alfalfa and clover, and allowed for haying while the grass was still green which produced hay of much higher value.\n\n"}
{"id": "5252160", "url": "https://en.wikipedia.org/wiki?curid=5252160", "title": "Test bench", "text": "Test bench\n\nA test bench or testing workbench is an environment used to verify the correctness or soundness of a design or model.\n\nThe term has its roots in the testing of electronic devices, where an engineer would sit at a lab bench with tools for measurement and manipulation, such as oscilloscopes, multimeters, soldering irons, wire cutters, and so on, and manually verify the correctness of the device under test (DUT).\n\nIn the context of software or firmware or hardware engineering, a test bench is an environment in which the product under development is tested with the aid of software and hardware tools. The software may need to be modified slightly in some cases to work with the test bench but careful coding can ensure that the changes can be undone easily and without introducing bugs. \n"}
{"id": "743547", "url": "https://en.wikipedia.org/wiki?curid=743547", "title": "Trench code", "text": "Trench code\n\nTrench Codes (a form of cryptography) were codes used for secrecy by field armies in World War I. Originally, the most commonly used codes were simple substitution codes, but due to the relative vulnerability of the classical cipher, trench codes came into existence. (Important messages generally used alternative encryption techniques for greater security.) The use of these codes required the distribution of codebooks to military personnel, which proved to be a security liability since these books could be stolen by enemy forces.\n\nBy the middle of World War I the conflict had settled down into a static battle of attrition, with the two sides sitting in huge lines of fixed earthwork fortifications. With armies generally immobile, distributing codebooks and protecting them was easier than it would have been for armies on the move. However, armies were still in danger of trench-raiding parties who would sneak into enemy lines and try to snatch codebooks. When this happened, an alarm could be raised and a code quickly changed. Trench codes were changed on a regular basis in an attempt to prevent code breakers from deciphering messages.\n\nThe French began to develop trench codes in early 1916. They started out as telephone codes, implemented at the request of a general whose forces had suffered devastating artillery bombardments due to indiscretions in telephone conversations between his men. The original telephone code featured a small set of two-letter codewords that were spelled out in voice communications. This grew into a three-letter code scheme, which was then adopted for wireless, with early one-part code implementations evolving into more secure two-part code implementations. The British began to adopt trench codes as well.\n\nThe Germans started using trench codes in the spring of 1917, evolving into a book of 4,000 codewords that were changed twice a month, with different codebooks used on different sectors of the front. The French codebreakers were extremely competent at cracking ciphers but were somewhat inexperienced at cracking codes, which require a slightly different mindset. It took them time to get to the point where they were able to crack the German codes in a timely fashion.\n\nThe Americans were relative newcomers to cryptography when they entered the war, but they did have their star players. One was Parker Hitt, b. 1878, who before the war had been an Army Signal Corps instructor. He was one of the first to try to bring US Army cryptology into the 20th century, publishing an influential short work on the subject in 1915 called the Manual for the solution of military ciphers, Publication date 1916, available at https://archive.org/details/manualforsolutio00hittrich. He was assigned to France in an administrative role, but his advice was eagerly sought by colleagues working in operational cryptology. Another Signal Corps officer who would make his mark on cryptology was Joseph Mauborgne, who in 1914, as a first lieutenant, had been the first to publish a solution to the Playfair cipher.\n\nWhen the Americans began moving up to the front in numbers in early 1918, they adopted trench codes and became very competent at their construction, with a Captain Howard R. Barnes eventually learning to produce them at a rate that surprised British colleagues. The Americans adopted a series of codes named after rivers, beginning with \"Potomac\". They learned to print the codebooks on paper that burned easily and degraded quickly after a few weeks, when the codes would presumably be obsolete, while using a font that was easy to read under trench conditions.\n\nAmerican code makers were often frustrated by the inability or refusal of combat units to use the codes—or worse, to use them properly. A soldier engaged in combat doesn't always feel the need to do things \"by the book\" even when there are very good reasons to do so, and generals on the front line felt that they had other things to worry about. One codemaker suggested that the best way to address the problem was to publicly hang a few offenders, but he lacked the authority to do so.\n\nThe British and French were already familiar with such problems in \"communications discipline\". They hadn't completely solved the problems either, but they had at least managed to get it through the heads of most of their signalmen that if they didn't have time to properly encrypt a message, they shouldn't bother trying; send the message unencrypted, or \"in the clear\". A partially or badly encrypted message could undermine a cipher or code system, sometimes completely, which made an unencrypted message far preferable.\n"}
{"id": "279585", "url": "https://en.wikipedia.org/wiki?curid=279585", "title": "Uptime", "text": "Uptime\n\nUptime is a measure of system reliability, expressed as the percentage of time a machine, typically a computer, has been working and available. Uptime is the opposite of downtime.\n\nIt is often used as a measure of computer operating system reliability or stability, in that this time represents the time a computer can be left unattended without crashing, or needing to be rebooted for administrative or maintenance purposes.\nConversely, long uptime may indicate negligence, because some critical updates can require reboots on some platforms.\n\nIn 2005, Novell reported a server with a 6-year uptime. Although that might sound unusual, that is actually common when servers are maintained under an industrial context and host critical applications such as banking systems.\n\nNetcraft maintains the uptime records for many thousands of web hosting computers.\n\nA server running Novell NetWare has been reported to have been shut down after 16 years of uptime due to a failing hard disk.\n\nUsers of Windows XP Professional, Windows Server 2003 and Windows Vista systems can type codice_1 at the Command Prompt to display all system information, including the \"System Up Time\".\n\n\"Note: Windows Vista Business 64-bit and Windows 7 do not return a \"System Up Time\" but \"System Boot Time\" instead. Also note that the exact text and date format is dependent of the language and locale Windows is running.\"\n\n\"Note: Windows 7's \"System Boot Time\" is not a reliable indicator of boot time. It does not take into account the time spent in sleep or hibernation mode. Hence, the boot time drifts forward every time the computer is left in sleep or hibernate mode.\"\n\nThe line that start with \"Statistics since ...\" provides the time that the server was up from. The command \"net stats srv\" is shorthand for \"net statistics server.\" The exact text and date format is dependent of the language and locale Windows is running.\n\nMicrosoft has also provided a downloadable utility:\n\n\"Note: The Windows 7 Uptime.exe utility is not a reliable indicator of total uptime. It gives the same wrong information and boot time as the Task Manager Uptime. It does not take into account the time spent in sleep or hibernation mode. An alternative to the Uptime.exe utility is \"net statistics workstation\" under \"Statistics\".\n\nUptime can also be determined via WMI from the command-line with codice_2:\n\nThe timestamp is in the format yyyymmddhhmmss.nnn, so this is a computer that last booted up on 8 May 2011 at 16:17:51.822. WMI can also be used to find the boot time of remote computers as well (Windows permissions allowing), for example with WMIC:\n\nThe text \"LastBootUpTime\" and the timestamp format are always the same regardless of the language and locale, Windows is running.\n\nWMI can also be used via a programming language such as VBScript or Powershell\n\nUsers of Windows Vista, Windows 7 and Windows 8 can see uptime in Windows Task Manager under the tab Performance. The uptime format is DD:HH:MM:SS\n\nThe codice_3 command is also available for FreeDOS. The version was developed by M. Aitchison.\n\nUsers of Linux systems can use the BSD uptime utility, which also displays the system load averages for the past 1, 5 and 15 minute intervals:\n\nShows how long the system has been on since it was last restarted:\n\nThe first number is the total number of seconds the system has been up. The second number is how much of that time the machine has spent idle, in seconds. On multi core systems (and some linux versions) the second number is the sum of the idle time accumulated by each CPU.\n\nBSD-based operating systems such as FreeBSD, Mac OS X and SySVr4 have the \"uptime\" command (See ).\nThere is also a method of using \"sysctl\" to call the system's last boot time:\n\nUsers of OpenVMS systems can type show system at the DCL command prompt; the header (first) line of the resulting display includes the system's uptime.\n\nThe uptime is displayed following the Uptime label as days followed by hours:minutes:seconds. In this example, the command qualifier /noprocess simply suppresses the display of per-process detail lines of information.\n\n"}
{"id": "16065959", "url": "https://en.wikipedia.org/wiki?curid=16065959", "title": "Visco Corporation", "text": "Visco Corporation\n\n"}
{"id": "614994", "url": "https://en.wikipedia.org/wiki?curid=614994", "title": "Wendy B. Lawrence", "text": "Wendy B. Lawrence\n\nWendy Barrien Lawrence (born July 2, 1959) is a retired United States Navy Captain, former helicopter pilot, an engineer, and a former NASA astronaut. She was the first female graduate of the United States Naval Academy to fly into space and she has also visited the Russian Space Station Mir. She was a mission specialist on STS-114, the first Space Shuttle flight after the Space Shuttle \"Columbia\" disaster.\n\nLawrence was born in Jacksonville, Florida. She is the daughter and the granddaughter of Naval Aviators; her grandfather was noted student athlete Fatty Lawrence and her father was the late Vice Admiral William P. Lawrence, a Mercury astronaut finalist and a former Vietnam Prisoner of War who was Superintendent of the U.S. Naval Academy during her last three years as a Midshipman and the person for whom the destroyer USS \"William P. Lawrence\" (DDG-110) is named.\n\nLawrence graduated from Fort Hunt High School in Alexandria, Virginia in 1977. She went on to attend the U.S. Naval Academy, graduating in 1981 with a Bachelor of Science in ocean engineering. She later earned a Master of Science in ocean engineering from the Massachusetts Institute of Technology and the Woods Hole Oceanographic Institution in 1988, as part of a joint program between the two schools.\n\n\nLawrence graduated from the U.S. Naval Academy in 1981. A distinguished flight school graduate, she was designated as a Naval Aviator in July 1982. Lawrence has more than 1,500 hours of flight time in six different types of helicopters and has made more than 800 shipboard landings. While assigned to Helicopter Combat Support Squadron 6 (HC-6), she was one of the first two female helicopter pilots to make a long deployment to the Indian Ocean as part of a carrier battle group. After completion of a master's degree program at MIT and Woods Hole in 1988, she was assigned to Helicopter Anti-Submarine Squadron Light 30 (HSL-30) as Officer-in-charge of Detachment Alpha. In October 1990, Lawrence reported to the U.S. Naval Academy where she served as a physics instructor and the novice women’s crew coach.\n\nSelected by NASA in March 1992, Lawrence reported to the Johnson Space Center in August 1992. She completed one year of training and was qualified for flight assignment as a Mission Specialist. Her technical assignments within the Astronaut Office have included flight software verification in the Shuttle Avionics Integration Laboratory (SAIL), Astronaut Office Assistant Training Officer, and Astronaut Office representative for Space Station training and crew support. She flew as the ascent/entry flight engineer and blue shift orbit pilot on STS-67 (March 2–18, 1995). She next served as Director of Operations for NASA at the Gagarin Cosmonaut Training Center in Star City, Russia, with responsibility for the coordination and implementation of mission operations activities in the Moscow region for the joint U.S./Russian Shuttle/Mir program. In September 1996 she began training as a crew member for a 4-month mission on the Russian Space Station Mir. However, she was replaced by David Wolf due to concerns over minimum size requirements for the Russian Orlan EVA suit. Because of her knowledge and experience with Mir systems and with crew transfer logistics for the Mir, she flew on STS-86 (September 25 to October 6, 1997) and STS-91 (June 2–12, 1998). A veteran of four space flights, she logged over 1,200 hours in space. Lawrence was a Mission Specialist on the crew of STS-114. She was in charge of the transfer of supplies and equipment and operated the Space Station robotic arm on the Return To Flight mission during which the crew tested and evaluated new procedures for the inspection and repair of the Space Shuttle thermal protection system. The mission launched on July 26, 2005 and landed on August 9, 2005.\n\nShe was part of the STR-26 mission, The next mission after the challenger accident. Her mission crew used the challenger's crew mission pass in their own.\nCaptain Lawrence retired from NASA in June 2006.\n\nSTS-67 \"Endeavour\" (March 2–18, 1995) was the second flight of the ASTRO observatory, a unique complement of three telescopes. During this 16-day mission, the crew conducted observations around the clock to study the far ultraviolet spectra of faint astronomical objects and the polarization of ultraviolet light coming from hot stars and distant galaxies. Mission duration was 399 hours and 9 minutes.\n\nSTS-86 \"Atlantis\" (September 25 to October 6, 1997) was the seventh mission to rendezvous and dock with the Russian Space Station Mir. Highlights included the exchange of U.S. crew members Mike Foale and David Wolf, a spacewalk by Scott Parazynski and Vladimir Titov to retrieve four experiments first deployed on Mir during the STS-76 docking mission, the transfer to Mir of 10,400 pounds of science and logistics, and the return of experiment hardware and results to Earth. Mission duration was 169 orbits in 259 hours and 21 minutes.\n\nSTS-91 \"Discovery\" (June 2–12, 1998) was the 9th and final Shuttle-Mir docking mission and marked the conclusion of the joint U.S./Russian Phase I Program. Mission duration was 235 hours, 54 minutes.\n\nSTS-114 \"Discovery\" (July 26 – August 9, 2005) was the first \"Return to Flight\" mission following the Space Shuttle \"Columbia\" disaster. Highlights included the first in-flight repair to the orbiter during a spacewalk. Mission duration was 333 hours, 33 minutes.\n"}
{"id": "4211531", "url": "https://en.wikipedia.org/wiki?curid=4211531", "title": "Zero-energy building", "text": "Zero-energy building\n\nA zero-energy building, also known as a zero net energy (ZNE) building, net-zero energy building (NZEB), net zero building or zero-carbon building is a building with zero net energy consumption, meaning the total amount of energy used by the building on an annual basis is roughly equal to the amount of renewable energy created on the site, or in other definitions by renewable energy sources elsewhere. These buildings consequently contribute less overall greenhouse gas to the atmosphere than similar non-ZNE buildings. They do at times consume non-renewable energy and produce greenhouse gases, but at other times reduce energy consumption and greenhouse gas production elsewhere by the same amount. A similar concept approved and implemented by the European Union and other agreeing countries is nearly Zero Energy Building (nZEB), with the goal of having all buildings in the region under nZEB standards by 2020.\n\nMost zero net energy buildings get half or more of their energy from the grid, and return the same amount at other times. Buildings that produce a surplus of energy over the year may be called \"energy-plus buildings\" and buildings that consume slightly more energy than they produce are called \"near-zero energy buildings\" or \"ultra-low energy houses\".\n\nTraditional buildings consume 40% of the total fossil fuel energy in the US and European Union and are significant contributors of greenhouse gases. The zero net energy consumption principle is viewed as a means to reduce carbon emissions and reduce dependence on fossil fuels and although zero-energy buildings remain uncommon even in developed countries, they are gaining importance and popularity. To read about recent examples of newly built houses with zero net energy use and examples of renovated existing houses with a zero net energy use see here. \n\nMost zero-energy buildings use the electrical grid for energy storage but some are independent of the grid. Energy is usually harvested on-site through energy producing technologies like solar and wind, while reducing the overall use of energy with highly efficient HVAC and lighting technologies. The zero-energy goal is becoming more practical as the costs of alternative energy technologies decrease and the costs of traditional fossil fuels increase.\n\nThe development of modern zero-energy buildings became possible largely through the progress made in new energy and construction technologies and techniques. These include highly insulating spray-foam insulation, high-efficiency solar panels, high-efficiency heat pumps and highly insulating low-E triple-glazed windows. These innovations have also been significantly improved by academic research, which collects precise energy performance data on traditional and experimental buildings and provides performance parameters for advanced computer models to predict the efficacy of engineering designs. \n\nZero-energy buildings can be part of a smart grid. Some advantages of these buildings are as follows:\n\n\nAlthough the net zero concept is applicable to a wide range of resources such as energy, water and waste. Energy is usually the first resource to be targeted because:\n\n\nDespite sharing the name \"zero net energy\", there are several definitions of what the term means in practice, with a particular difference in usage between North America and Europe.\n\n\n\n\n\n\n\nWithin this balance procedure several aspects and explicit choices have to be determined:\n\nThe information is based on the publications, and in which deeper information could be found.\n\nThe most cost-effective steps toward a reduction in a building's energy consumption usually occur during the design process. To achieve efficient energy use, zero energy design departs significantly from conventional construction practice. Successful zero energy building designers typically combine time tested passive solar, or artificial/fake conditioning, principles that work with the on-site assets. Sunlight and solar heat, prevailing breezes, and the cool of the earth below a building, can provide daylighting and stable indoor temperatures with minimum mechanical means. ZEBs are normally optimized to use passive solar heat gain and shading, combined with thermal mass to stabilize diurnal temperature variations throughout the day, and in most climates are superinsulated. All the technologies needed to create zero energy buildings are available off-the-shelf today.\n\nSophisticated 3-D building energy simulation tools are available to model how a building will perform with a range of design variables such as building orientation (relative to the daily and seasonal position of the sun), window and door type and placement, overhang depth, insulation type and values of the building elements, air tightness (weatherization), the efficiency of heating, cooling, lighting and other equipment, as well as local climate. These simulations help the designers predict how the building will perform before it is built, and enable them to model the economic and financial implications on building cost benefit analysis, or even more appropriate – life cycle assessment.\n\nZero-energy buildings are built with significant energy-saving features. The heating and cooling loads are lowered by using high-efficiency equipment (such as heat pumps rather than furnaces. Heat pumps are about four times as efficient as furnaces) added insulation (especially in the attic and in the basement of houses), high-efficiency windows (such as low-E triple-glazed windows), draft-proofing, high efficiency appliances (particularly modern high-efficiency refrigerators), high-efficiency LED lighting, passive solar gain in winter and passive shading in the summer, natural ventilation, and other techniques. These features vary depending on climate zones in which the construction occurs. Water heating loads can be lowered by using water conservation fixtures, heat recovery units on waste water, and by using solar water heating, and high-efficiency water heating equipment. In addition, daylighting with skylights or solartubes can provide 100% of daytime illumination within the home. Nighttime illumination is typically done with fluorescent and LED lighting that use 1/3 or less power than incandescent lights, without adding unwanted heat. And miscellaneous electric loads can be lessened by choosing efficient appliances and minimizing phantom loads or standby power. Other techniques to reach net zero (dependent on climate) are Earth sheltered building principles, superinsulation walls using straw-bale construction, Vitruvianbuilt pre-fabricated building panels and roof elements plus exterior landscaping for seasonal shading.\n\nOnce the energy use of the building has been minimized it can be possible to generate all that energy on site using roof-mounted solar panels. See examples of zero net energy houses here.\n\nZero-energy buildings are often designed to make dual use of energy including that from white goods. For example using refrigerator exhaust to heat domestic water, ventilation air and shower drain heat exchangers, office machines and computer servers, and body heat to heat the building. These buildings make use of heat energy that conventional buildings may exhaust outside. They may use heat recovery ventilation, hot water heat recycling, combined heat and power, and absorption chiller units.\n\nZEBs harvest available energy to meet their electricity and heating or cooling needs. By far the most common way to harvest energy is to use roof-mounted solar photovoltaic panels that turn the sun's light into electricity. Energy can also be harvested with solar thermal collectors (which use the sun's heat to heat water for the building). Heat pumps either ground-source (otherwise known as geothermal) or air-sourced can also harvest heat and cool from the air or ground near the building. Technically heat pumps move heat rather than harvest it, but the overall effect in terms of reduced energy use and reduced carbon footprint is similar. In the case of individual houses, various microgeneration technologies may be used to provide heat and electricity to the building, using solar cells or wind turbines for electricity, and biofuels or solar thermal collectors linked to a seasonal thermal energy storage (STES) for space heating. An STES can also be used for summer cooling by storing the cold of winter underground. To cope with fluctuations in demand, zero energy buildings are frequently connected to the electricity grid, export electricity to the grid when there is a surplus, and drawing electricity when not enough electricity is being produced. Other buildings may be fully autonomous.\n\nEnergy harvesting is most often more effective (in cost and resource utilization) when done on a local but combined scale, for example, a group of houses, cohousing, local district, village, etc. rather than an individual basis. An energy benefit of such localized energy harvesting is the virtual elimination of electrical transmission and electricity distribution losses. On-site energy harvesting such as with roof top mounted solar panels eliminates these transmission losses entirely. These losses amount to about 7.2%–7.4% of the energy transferred. Energy harvesting in commercial and industrial applications should benefit from the topography of each location. However, a site that is free of shade can generate large amounts of solar powered electricity from the building's roof and almost any site can use geothermal or air-sourced heat pumps. The production of goods under net zero fossil energy consumption requires locations of geothermal, microhydro, solar, and wind resources to sustain the concept.\n\nZero-energy neighborhoods, such as the BedZED development in the United Kingdom, and those that are spreading rapidly in California and China, may use distributed generation schemes. This may in some cases include district heating, community chilled water, shared wind turbines, etc. There are current plans to use ZEB technologies to build entire off-the-grid or net zero energy use cities.\n\nOne of the key areas of debate in zero energy building design is over the balance between energy conservation and the distributed point-of-use harvesting of renewable energy (solar energy, wind energy and thermal energy). Most zero energy homes use a combination of these strategies.\n\nAs a result of significant government subsidies for photovoltaic solar electric systems, wind turbines, etc., there are those who suggest that a ZEB is a conventional house with distributed renewable energy harvesting technologies. Entire additions of such homes have appeared in locations where photovoltaic (PV) subsidies are significant, but many so called \"Zero Energy Homes\" still have utility bills. This type of energy harvesting without added energy conservation may not be cost effective with the current price of electricity generated with photovoltaic equipment (depending on the local price of power company electricity). The cost, energy and carbon-footprint savings from conservation (e.g., added insulation, triple-glazed windows and heat pumps) compared to those from on-site energy generation (e.g., solar panels) have been published for an upgrade to an existing house here.\n\nSince the 1980s, passive solar building design and passive house have demonstrated heating energy consumption reductions of 70% to 90% in many locations, without active energy harvesting. For new builds, and with expert design, this can be accomplished with little additional construction cost for materials over a conventional building. Very few industry experts have the skills or experience to fully capture benefits of the passive design. Such passive solar designs are much more cost-effective than adding expensive photovoltaic panels on the roof of a conventional inefficient building. A few kilowatt-hours of photovoltaic panels (costing 2 to 3 dollars per annual kWh production, U.S. dollar equivalent) may only reduce external energy requirements by 15% to 30%. A high seasonal energy efficiency ratio 14 conventional air conditioner requires over 7 kW of photovoltaic electricity while it is operating, and that does not include enough for off-the-grid night-time operation. Passive cooling, and superior system engineering techniques, can reduce the air conditioning requirement by 70% to 90%. Photovoltaic-generated electricity becomes more cost-effective when the overall demand for electricity is lower.\n\nThe energy used in a building can vary greatly depending on the behavior of its occupants. The acceptance of what is considered comfortable varies widely. Studies of identical homes have shown dramatic differences in energy use in a variety of climates. An average widely accepted ratio of highest to lowest energy consumer in identical homes is about 3, with some identical homes using up to 20 times as much heating energy as the others . Occupant behavior can vary from differences in setting and programming thermostats, varying levels of illumination and hot water use, window and shading system operation and the amount of miscellaneous electric devices or plug loads used.\n\nUtility companies are typically legally responsible for maintaining the electrical infrastructure that brings power to our cities, neighborhoods, and individual buildings. Utility companies typically own this infrastructure up to the property line of an individual parcel, and in some cases own electrical infrastructure on private land as well. Utilities have expressed concern that the use of Net Metering for ZNE projects threatens the Utilities base revenue, which in turn impacts their ability to maintain and service the portion of the electrical grid that they are responsible for. Utilities have expressed concern that states that maintain Net Metering laws may saddle non-ZNE homes with higher utility costs, as those homeowners would be responsible for paying for grid maintenance while ZNE home owners would theoretically pay nothing if they do achieve ZNE status. This creates potential equity issues, as currently, the burden would appear to fall on lower-income households. A possible solution to this issue is to create a minimum base charge for all homes connected to the utility grid, which would force ZNE home owners to pay for grid services independently of their electrical use.\n\nAdditional concerns exist that local distribution as well as larger transmission grids have not been designed to convey electricity in two directions, which may be necessary as higher levels of distributed energy generation come on line. Overcoming this barrier could require extensive upgrades to the electrical grid, however this is not believed to be a major problem until renewable generation reaches much higher levels of penetration than currently realized.\n\nWide acceptance of zero-energy building technology may require more government incentives or building code regulations, the development of recognized standards, or significant increases in the cost of conventional energy.\n\nThe Google photovoltaic campus and the Microsoft 480-kilowatt photovoltaic campus relied on U.S. Federal, and especially California, subsidies and financial incentives. California is now providing US$3.2 billion in subsidies for residential-and-commercial near-zero-energy buildings. The details of other American states' renewable energy subsidies (up to US$5.00 per watt) can be found in the Database of State Incentives for Renewables and Efficiency. The Florida Solar Energy Center has a slide presentation on recent progress in this area.\n\nThe World Business Council for Sustainable Development has launched a major initiative to support the development of ZEB. Led by the CEO of United Technologies and the Chairman of Lafarge, the organization has both the support of large global companies and the expertise to mobilize the corporate world and governmental support to make ZEB a reality. Their first report, a survey of key players in real estate and construction, indicates that the costs of building green are overestimated by 300 percent. Survey respondents estimated that greenhouse gas emissions by buildings are 19 percent of the worldwide total, in contrast to the actual value of roughly 40 percent.\n\nThose who commissioned construction of passive houses and zero-energy homes (over the last three decades) were essential to iterative, incremental, cutting-edge, technology innovations. Much has been learned from many significant successes, and a few expensive failures.\n\nThe zero-energy building concept has been a progressive evolution from other low-energy building designs. Among these, the Canadian R-2000 and the German \"passive house\" standards have been internationally influential. Collaborative government demonstration projects, such as the superinsulated Saskatchewan House, and the International Energy Agency's \"Task 13\", have also played their part.\n\nThe U.S. National Renewable Energy Laboratory (NREL) published a groundbreaking report titled Net-Zero Energy Buildings: A Classification System Based on Renewable Energy Supply Options. This is the first report to lay out a full spectrum classification system for Net Zero/Renewable Energy buildings that includes the full spectrum of Clean Energy sources, both on site and off site. This classification system identifies the following 4 main categories of Net Zero Energy Buildings/Sites/Campuses:\nApplying this U.S. Government Net Zero classification system means that every building \"can\" become Net Zero with the right combination of the key Net Zero Technologies - PV (solar), GHP (geothermal heating and cooling, thermal batteries), EE (energy efficiency), sometimes Wind, and Electric Batteries. A graphical exposé of the scale of impact of applying these NREL guidelines for Net Zero can be seen in the graphic at Net Zero Foundation titled \"Net Zero Effect on U.S. Total Energy Use\" showing a possible 39% U.S. total fossil fuel use reduction by changing U.S. Residential and Commercial buildings to Net Zero, 37% savings if we still use Nat. Gas for cooking at the same level.\n\nMany well known universities have professed to want to completely convert their energy systems off of fossil fuels. The very idea that one could convert a whole campus off of fossil fuels has to date only been theoretical. Capitalizing on the continuing developments in both Photovoltaics and Geothermal heat pump technologies, and in the advancing Electric Battery field, complete conversion to a carbon free energy solution is now possible. An example of this is in the Net Zero Foundation's proposal at MIT to take that campus completely off fossil fuel use. This proposal shows the coming application of Net Zero Energy Buildings technologies at the District Energy scale.\n\n\n\nThe goal of green building and sustainable architecture is to use resources more efficiently and reduce a building's negative impact on the environment. Zero energy buildings achieve one key green-building goal of completely or very significantly reducing energy use and greenhouse gas emissions for the life of the building. Zero energy buildings may or may not be considered \"green\" in all areas, such as reducing waste, using recycled building materials, etc. However, zero energy, or net-zero buildings do tend to have a much lower ecological impact over the life of the building compared with other \"green\" buildings that require imported energy and/or fossil fuel to be habitable and meet the needs of occupants.\n\nBecause of the design challenges and sensitivity to a site that are required to efficiently meet the energy needs of a building and occupants with renewable energy (solar, wind, geothermal, etc.), designers must apply holistic design principles, and take advantage of the free naturally occurring assets available, such as passive solar orientation, natural ventilation, daylighting, thermal mass, and night time cooling.\n\nMany green building certification programs do not require a building to have net zero energy use, only to reduce energy use a few percentage points below the minimum required by law. Green Globes involves check lists that are measurement tools, not design tools. Inexperienced designers or architects may cherry-pick points to meet a target certification level, even though those points may not be the best design choices for a specific building or climate. In November, 2011, the International Living Future Institute (ILFI) developed the Net Zero Energy Building Certification. In 2017, the ILFI simplified the certification program and renamed it Zero Energy Building Certification.\n\nBetween 2008 and 2013, researchers from Australia, Austria, Belgium, Canada, Denmark, Finland, France, Germany, Italy, Republic of Korea, New Zealand, Norway, Portugal, Singapore, Spain, Sweden, Switzerland, United Kingdom and USA were working together in the joint research program “Towards Net Zero Energy Solar Buildings” under the umbrella of International Energy Agency (IEA) Solar Heating and Cooling Program (SHC) Task 40 / Energy in Buildings and Communities (EBC, formerly ECBCS) Annex 52 in order to bring the Net ZEB concept to market viability. The joint international research and demonstration activities are divided in subtasks. The objective is to develop a common understanding, a harmonized international applicable definition framework (Subtask A, see definitions methodology “Net Zero Energy Building” above), design process tools (Subtask B), advanced building design and technology solutions and industry guidelines for Net ZEBs (Subtask C). The scope encompasses new and existing residential and non-residential buildings located within the climatic zones of the participating countries.\n\nIn Australia, researchers have recently developed a new approach to the construction of visually-clear solar energy harvesting windows suitable for industrialization and applications in net-zero energy buildings. Industrial production of several prototype batches of solar windows has started in 2016.\n\nUp to the December 2017, the State of Queensland has more than 30% of households with rooftop solar photovoltaic (PV) system. The average size of Australian rooftop solar PV system has exceeded 3.5kW. In Brisbane (capital city of Queensland), households with 6kW rooftop PV system and reasonable energy rating (5~6 stars for Australian National House Energy Rating NatHERS) can achieve net zero total energy target or even positive energy.\n\nIn Belgium there is a project with the ambition to make the Belgian city Leuven climate-neutral in 2030.\n\nAfter April 2011 Fukushima earthquake follow up with Fukushima Daiichi nuclear disaster, Japan experienced severe　power crisis that led to the awareness of importance of energy conservation.\nIn 2012 Ministry of Economy, Trade and Industry, Ministry of Land, Infrastructure, Transport and Tourism and Ministry of the Environment (Japan) summarized the road map for Low-carbon Society which contains the goal of ZEH and ZEB to be standard of new construction in 2020.\n\n\n\n\n\nStrategic Research Centre on Zero Energy Buildings was in 2009 established at Aalborg University by a grant from the Danish Council for Strategic Research (DSF), the Programme Commission for Sustainable Energy and Environment, and in cooperation with the Technical University of Denmark, Danish Technological Institute, Danfoss A/S, Velux A/S, Saint Gobain Isover A/S, and The Danish Construction Association, the section of aluminium facades. The purpose of the centre is through development of integrated, intelligent technologies for the buildings, which ensure considerable energy conservations and optimal application of renewable energy, to develop zero energy building concepts. In cooperation with the industry, the centre will create the necessary basis for a long-term sustainable development in the building sector.\n\n\nIndia's first net zero building is Indira Paryavaran Bhawan, located in New Delhi. Features include passive solar building design and other green technologies.\n\nIn 2011, \"Payesh\" Energy House (PEH) or \"Khaneh Payesh Niroo\" by a collaboration of Fajr-e-Toseah Consultant Engineering Company and Vancouver Green Homes Ltd] under management of Payesh Energy Group (EPG) launched the first Net-Zero passive house in Iran. This concept makes the design and construction of PEH a sample model and standardized process for mass production by MAPSA.\n\nAlso an example of the new generation of zero energy office buildings is the 24-story OIIC Office Tower, which is started in 2011, as the OIIC Company headquarters. It uses both modest energy efficiency, and a big distributed renewable energy generation from both solar and wind. It is managed by Rahgostar Naft Company in Tehran, Iran. The tower is receiving economic support from government subsidies that are now funding many significant fossil-fuel-free efforts.\n\nIn 2005, \"Scandinavian Homes\" launched the world's first standardised passive house in Ireland, this concept makes the design and construction of passive house a standardised process.\nConventional low energy construction techniques have been refined and modelled on the PHPP (Passive House Design Package) to create the standardised passive house.\nBuilding offsite allows high precision techniques to be utilised and reduces the possibility of errors in construction.\nIn 2009 the same company started a project to use 23,000 liters of water in a seasonal storage tank, heated up by evacuated solar tubes throughout the year, with the aim to provide the house with enough heat throughout the winter months thus eliminating the need for any electrical heat to keep the house comfortably warm. The system is monitored and documented by a research team from The University of Ulster and the results will be included in part of a PhD thesis.\n\nIn 2012 Cork institute of Technology started renovation work on its 1974 building stock to develop a net zero energy building retrofit. The exemplar project will become Ireland's first zero energy testbed offering a post occupancy evaluation of actual building performance against design benchmarks.\n\nIn October 2007, the Malaysia Energy Centre (PTM) successfully completed the development and construction of the PTM Zero Energy Office (ZEO) Building. The building has been designed to be a super-energy-efficient building using only 286 kWh/day. The renewable energy – photovoltaic combination is expected to result in a net zero energy requirement from the grid. The building is currently undergoing a fine tuning process by the local energy management team. Findings are expected to be published in a year.\n\nIn 2016,The Sustainable Energy Development Authority Malaysia (SEDA Malaysia) has started a voluntary initiative called Low Carbon Building Facilitation Program. The purpose is to support the current low carbon cities program in Malaysia. Under the program, several project demonstration managed to reduce energy and carbon beyond 50% savings and some managed to save more than 75%. Continuous improvement of super energy efficient buildings with significant implementation of on-site renewable energy managed to made a few of them become nearly Zero Energy (nZEB) as well as Net Zero Energy Building (NZEB). In March 2018, SEDA Malaysia has started the Zero Energy Building Facilitation Program. \n\nMalaysia also have its own sustainable building tool special for Low Carbon and zero energy building, called GreenPASS that been developed by the Construction Industry Development Board Malaysia (CIDB) in 2012, and currently being administered and promoted by SEDA Malaysia. GreenPASS official be called the Construction Industry Standard (CIS) 20:2012. \n\nIn September 2006, the Dutch headquarters of the World Wildlife Fund (WWF) in Zeist was opened. This earth-friendly building gives back more energy than it uses. All materials in the building were tested against strict requirements laid down by the WWF and the architect.\n\nIn February 2009, the Research Council of Norway assigned The Faculty of Architecture and Fine Art at the Norwegian University of Science and Technology to host the Research Centre on Zero Emission Buildings (ZEB), which is one of eight new national Centres for Environment-friendly Energy Research (FME). The main objective of the FME-centres is to contribute to the development of good technologies for environmentally friendly energy and to raise the level of Norwegian expertise in this area. In addition, they should help to generate new industrial activity and new jobs. Over the next eight years, the FME-Centre ZEB will develop competitive products and solutions for existing and new buildings that will lead to market penetration of zero emission buildings related to their production, operation and demolition.\n\nSingapore's first zero-energy building was launched at the inaugural Singapore Green Building Week.\n\nThe Swiss MINERGIE-A-Eco label certifies zero energy buildings. The first building with this label, a single-family home, was completed in Mühleberg in 2011.\n\n\nIn December 2006, the government announced that by 2016 all new homes in England will be zero energy buildings. To encourage this, an exemption from Stamp Duty Land Tax is planned. In Wales the plan is for the standard to be met earlier in 2011, although it is looking more likely that the actual implementation date will be 2012. However, as a result of a unilateral change of policy published at the time of the March 2011 budget, a more limited policy is now planned which, it is estimated, will only mitigate two thirds of the emissions of a new home.\n\nIn the US, ZEB research is currently being supported by the US Department of Energy (DOE) Building America Program, including industry-based consortia and researcher organizations at the National Renewable Energy Laboratory (NREL), the Florida Solar Energy Center (FSEC), Lawrence Berkeley National Laboratory (LBNL), and Oak Ridge National Laboratory (ORNL). From fiscal year 2008 to 2012, DOE plans to award $40 million to four Building America teams, the Building Science Corporation; IBACOS; the Consortium of Advanced Residential Buildings; and the Building Industry Research Alliance, as well as a consortium of academic and building industry leaders. The funds will be used to develop net-zero-energy homes that consume 50% to 70% less energy than conventional homes.\n\nDOE is also awarding $4.1 million to two regional building technology application centers that will accelerate the adoption of new and developing energy-efficient technologies. The two centers, located at the University of Central Florida and Washington State University, will serve 17 states, providing information and training on commercially available energy-efficient technologies.\n\nThe U.S. Energy Independence and Security Act of 2007 created 2008 through 2012 funding for a new solar air conditioning research and development program, which should soon demonstrate multiple new technology innovations and mass production economies of scale.\n\nThe 2008 Solar America Initiative funded research and development into future development of cost-effective Zero Energy Homes in the amount of $148 million in 2008.\n\nThe Solar Energy Tax Credits have been extended until the end of 2016. Solar power in the United States\n\nBy Executive Order 13514, U.S. President Barack Obama mandated that by 2015, 15% of existing Federal buildings conform to new energy efficiency standards and 100% of all new Federal buildings be Zero-Net-Energy by 2030.\n\nIn 2007, the philanthropic Siebel Foundation created the Energy Free Home Foundation. The goal was to offer $20 million in global incentive prizes to design and build a 2,000 square foot (186 square meter) three-bedroom, two bathroom home with (1) net-zero annual utility bills that also has (2) high market appeal, and (3) costs no more than a conventional home to construct.\n\nThe plan included funding to build the top ten entries at $250,000 each, a $10 million first prize, and then a total of 100 such homes to be built and sold to the public.\n\nBeginning in 2009, Thomas Siebel made many presentations about his Energy Free Home Challenge. The Siebel Foundation Report stated that the Energy Free Home Challenge was \"Launching in late 2009\".\n\nThe Lawrence Berkeley National Laboratory at the University of California, Berkeley participated in writing the \"Feasibility of Achieving Zero-Net-Energy, Zero-Net-Cost Homes\" for the $20-million Energy Free Home Challenge.\n\nIf implemented, the Energy Free Home Challenge would have provided increased incentives for improved technology and consumer education about zero energy buildings coming in at the same cost as conventional housing.\n\nThe U.S. Department of Energy Solar Decathlon is an international competition that challenges collegiate teams to design, build, and operate the most attractive, effective, and energy-efficient solar-powered house. Achieving Zero Net Energy balance is a major focus of the competition.\n\n\nCalifornia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTennessee\n\n\n\n\n\n\n\n"}
