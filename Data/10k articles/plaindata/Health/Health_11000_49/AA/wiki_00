{"id": "710441", "url": "https://en.wikipedia.org/wiki?curid=710441", "title": "Abortion in Australia", "text": "Abortion in Australia\n\nAbortion in Australia is largely regulated by the states and territories rather than the Federal Government. The grounds on which abortion is permitted in Australia vary by jurisdiction. In every state, abortion is legal to protect the life and health of a woman, though each state has a different definition.\n\nNowhere in Australia is there a requirement that a woman's sexual partner be notified of a proposed abortion or to consent to the procedure. Australian courts will not grant an injunction to restrain a pregnant woman from terminating her pregnancy, even if the applicant is the putative father of the fetus. There is also no waiting period for an abortion. A minor does not need to notify a parent of a proposed abortion nor is parental consent required, except in Western Australia. In Western Australia, a proposed abortion by a minor under 16 years of age must be notified to one of the parents, except where permission has been granted by the Children's Court or the minor does not live with her parents. \n\nEarly-term surgical abortions are generally available around Australia for those women who seek them. The procedure is partially funded under Medicare, the government-funded public health scheme, or by private healthcare insurers. Prosecutions against medical practitioners for performing abortions have not occurred for decades, with one exception – a prosecution in 1998 in Western Australia that soon after led to the explicit legalisation of on-request abortions under certain circumstances in that state. RU-486, an abortifacient widely used overseas, has been available in Australia only since February 2006.\n\nIn the case of 'a child capable of being born alive' (usually taken to mean after 28 weeks of pregnancy), a termination may be subject to a separate crime of child destruction in some States and Territories.\n\nAbortion in Australia has always been regulated by state (previously colonial) law. Before the end of the 19th century, each colony had adopted the Imperial \"Offences Against the Person Act 1861\", which in turn was derived from English laws from 1837, 1828 and 1803, which made abortion illegal under any circumstance. Since then, abortion law has continued to evolve in each State by case law and changes in legislation.\n\nA legal precedent concerning the legality of abortion was set in Australia in 1969, by the \"Menhennitt ruling\" in the Victorian Supreme Court case \"R v Davidson\", which held that abortion was lawfully justified if \"necessary to preserve the physical or mental health of the woman concerned, provided that the danger involved in the abortion did not outweigh the danger which the abortion was designed to prevent.\" The ruling was later largely adopted by courts in New South Wales, and Queensland, and was influential in some other states. Over time this has come to be broadly defined so as to include the mental health of the woman, to which an unwanted pregnancy is interpreted as clinically injurious.\n\nA number of federal politicians have expressed anti-abortion views including Senator Ron Boswell (who was for a time National Party leader in the Senate), Barnaby Joyce and Tony Abbott (when he was federal Health Minister), who in 2004 described the scale of abortion as a \"national tragedy\".\n\nIn the mid-1990s the conservative Howard government was in power in Australia, with conservative independent Tasmanian Senator Brian Harradine holding the balance of power in the Senate. Howard brokered a deal with Harradine to ensure his support for proposed bills, including the privatisation of national telecommunications provider Telecom (now Telstra). In return, Harrdine received support for introducing restrictions on abortion ref></ref>. Abortifacient drugs were designated as \"restricted goods\" which required approval from the health minister before the drug could be assessed by the Therapeutic Goods Administration (TGA), which was required before drugs could be sold in Australia. This created a ministerial veto over such drugs. This ensured that RU-486, a drug widely used overseas as an abortifacient, was effectively banned in Australia until 2006. Health Minister Tony Abbott, and previous ministers, used this process to prohibit the sale of RU-486 in Australia, by refusing to allow the drug to be assessed by the TGA. \n\nIn late 2005, a private member's bill was introduced in the Senate and passed on 10 February 2006 to transfer the veto over abortifacients from the health minister to the TGA. The bill was approved by the Houses of Representatives in March 2006. Abbott responded to the vote by calling for the addition of a Medicare item number for funding of alternative counselling to pregnant women through church-affiliated groups to lower the national abortion rate, without success. In 2010, while seeking election as Liberal Party leader, Abbott pledged not to make any changes to abortion laws, to ban funding for abortions through Medicare, or to ban drug RU-486.\n\nIn 2017 Cory Bernardi moved a motion to ban abortion on gender grounds. Queensland LNP senator Barry O'Sullivan, Tasmanian Liberal Eric Abetz, frontbenchers Zed Seselja, Anne Ruston and Matt Canavan with One Nation's Fraser Anning, Brian Burston, Peter Georgiou and Pauline Hanson voted Yes on the Motion. It was voted down (10–36).\n\nThe violence seen in the United States against abortion providers has generally not occurred in Australia, with two exceptions. In 2001, Peter Knight forced his way into a Melbourne clinic carrying a rifle, kerosene, and equipment to lock the doors of the clinic. Three people attempted to disarm him after he pointed his rifle at a woman at the clinic. He shot and killed a security guard. Afterwards, Knight, described by the prosecution as a \"hermit obsessed with killing abortion doctors\" was convicted of murder. On 6 January 2009 A firebombing using Molotov cocktails was attempted at a medical clinic in Mosman Park, Western Australia. Faulty construction of the bombs limited damage to a single external burnt area, though if successful damage would have been severe. It is believed that the individuals who made the attack were responsible for graffiti \"baby killers\" on the site, indicating an anti-abortion reason for the attack. The site turned out to in fact not be an abortion clinic, though the attackers most likely were not aware of this.\n\nAnother tactic adopted by anti-abortion campaigners is to form picket lines outside premises where abortions are being performed. Several states and territories set \"exclusions zones\" for anti-abortion protesters.\n\nIn the Australian Capital Territory, reference to abortion as a criminal offence were repealed by the \"Crimes (Abolition of Offence of Abortion) Act 2002\". Before then abortion law was for many years governed by case law under sections 82–84 of the \"Crimes Act 1900\" of New South Wales.\n\nSince 2015, it is an offence to protest within 50 metres of an abortion clinic within the Australian Capital Territory (otherwise called \"protest free zones\").\n\nSection 9 of the \"Human Rights Act 2004 (ACT)\" confirms that the right to life applies to a person from the time of birth. Full-term abortions on demand are legal in the ACT as there are no gestational limits.\n\nSince 1 July 2018 within NSW it is illegal to protest within 150m of an abortion service.\n\nSince the year 1900, within New South Wales abortion is explicitly listed as a crime under sections 82–84 of the \"Crimes Act 1900\", but the interpretation of the law is subject to the \"Levine ruling\", from \"R v Wald\" of 1971, itself derived from the Victorian Menhennitt ruling, which held an abortion to be legal if a doctor had an honest and reasonable belief that, due to 'any economic, social or medical ground or reason', the abortion was necessary to 'preserve the woman involved from serious danger to her life or physical or mental health which the continuance of the pregnancy would entail'.\n\nThis was expanded by \"CES v Superclinics Australia Pty Ltd\" (1995), which extended the period during which health concerns might be considered from the duration of pregnancy to any period during the woman's life, even after the birth of the child. This arguably precludes any successful prosecutions for illegal abortions. Despite this, in 2006, a doctor, Suman Sood, was convicted of two counts of performing an illegal abortion where she failed to enquire as to whether a lawful reason for performing the abortion did exist.\n\nSince 1 July 2016 all NSW laws also apply to the approximately 2,000 residents on Norfolk Island, under both the \"Norfolk Island Legislation Amendment Act 2015\" and the \"Territories Legislation Amendment Act 2016\" – because the Norfolk Legislative Assembly was abolished on 1 July 2015.\n\nIn August 2016, Greens MP Dr Mehreen Faruqi released an exposure draft of the Abortion Law Reform (Miscellaneous Acts Amendment) Bill 2016 to \"Repeal sections 82–84 of the Crimes Act, relating to abortion offences; Provide for a 150m safe access zone around abortion clinics and service providers to ensure a patient's right to medical privacy; and Require doctors to disclose conscientious objection at the start of the consultation and refer patients to another doctor who does not have such an objection or to the local Women's Health NSW centre\".\n\nOn the eve of the introduction of the bill on 23 June 2016, it was removed from the order of business for the following day, despite being first in the order of precedence for months, scheduled and publicly announced. Dr Mehreen Faruqi \"introduced the first ever abortion law reform bill into NSW Parliament\" on 11 August 2016.\n\nIn the Northern Territory, abortion is legal for up to 24 weeks’ gestation, implementing a 150-metre \"safe access zone\", removing the requirement of parental approval for abortions and providing early medical abortions with tablets. The law went into effect on 1 July 2017.\n\nPrior to December 2018, abortion access in Queensland was determined by the 1986 \"McGuire ruling\", which declared abortion to be legal if necessary to preserve the woman from a serious danger to her life or health – beyond the normal dangers of pregnancy and childbirth – that would result if the pregnancy continued, and is not disproportionate to the danger being averted. Until 2008, abortion law in Queensland closely mirrored the law in Victoria. The \"McGuire ruling\" was affirmed in the 1994 case \"Veivers v. Connolly\", by a single judge of the Supreme Court of Queensland.\n\nIn October 2018, the Queensland parliament passed a bill by a vote of 50-41 that legalized abortion up to the twenty-second week of gestation. For abortion after the twenty-second week, two doctors must sign off on the procedure before a woman can access an abortion. The legislation gained royal assent a week later and went into effect on 3 December 2018.\n\nIn South Australia, legislation in 1969 legalised abortion when necessary to protect the life or physical or mental health of the woman – taking into account the current and reasonably foreseeable future – or in cases when the child was likely to be born with serious handicaps. Abortions must be performed before a time limit of 28 weeks of pregnancy. Abortions must be performed in a hospital and be approved by two physicians, and are also subject to a residency requirement.(Patient must be a resident of South Australia). The hospital, dual approval and residency requirement may be waived in an emergency. Abortions in South Australia are available for free or low cost at some of the public health facilities including The Pregnancy Advisory Centre. This Pregnancy Advisory Centre is a registered hospital with doctors available for approval. Both medical and surgical abortions are performed.\n\nIn Tasmania, since 21 November 2013, abortions are allowed on request up to 16 weeks of pregnancy. After 16 weeks abortions can be performed up to birth of the child requiring consent of two doctors on medical or psychological grounds. The law also criminalises filming, \"intimidation\" and \"protests\" (where handing out information, offering pregnant women help and support, praying, and so forth is deemed intimidating and offensive) within 150 metres of abortion clinics (\"protest free zones\").\n\nFrom 1925 until 2001, Tasmania's Criminal Code prohibited \"unlawful abortion\" without actually stating what was lawful or not. While it had never actually been prosecuted, it had been held that Victoria's Menhennit ruling of 1969 and New South Wales' Levine ruling applied in Tasmanian law. In late 2001, the Criminal Code was clarified to state that an abortion must be carried out under a set of criteria resembling those of the South Australian requirements.\n\nIn Victoria, since 2008, abortions are allowed on request up to 24 weeks of pregnancy, with abortions after that time, up until the child's birth, requiring two doctors to agree that it is appropriate, based on the woman's current and future physical, psychological and social circumstances.\n\nBefore 2008, abortion law was based on the Victorian Crimes Act as interpreted by the \"Menhennitt ruling\" of 1969, in the case of \"R v Davidson\". Under the ruling, abortions were legal if necessary to preserve the woman from a serious danger to her life or health – beyond the normal dangers of pregnancy and childbirth – that would result if the pregnancy continued, and is not disproportionate to the danger being averted. Menhennitt's ruling remained the basis for abortion law in Victoria for almost 40 years, until the \"Abortion Law Reform Act 2008\" (Vic) formally decriminalised abortion.\n\nIn Western Australia, since 20 May 1998, abortions are allowed on request up to 20 weeks of pregnancy – subject to counselling by a medical practitioner other than the one performing the abortion – or when serious personal, family or social consequences will result to the woman if an abortion is not performed, when the life or physical or mental health of the woman is endangered and when the pregnancy causes serious danger to the woman's mental health. After 20 weeks of pregnancy abortions may only be performed if the fetus is likely to be born with severe medical problems – which must be confirmed by two independently appointed doctors. In the event of the woman being under 16 years of age one of her parents must be notified, except where permission has been granted by the Children's Court or the woman does not live with her parents.\n\nUntil 1998, Western Australian law apparently mirrored that of Queensland, though it was never clarified by case law or legislation. Following the 1998 announcement of the prosecution of two Perth doctors for performing an illegal abortion – the first such prosecution in over 30 years – a private member's bill was introduced by Cheryl Davenport, a member of the Australian Labor Party in the Upper House of the Western Australian parliament to amend the law.\n\nDue to the lack of consistent data collection standards across States and the differences in definitions, it is difficult or impossible to accurately quantify the number of abortions performed in Australia each year. There were an average of 75,700 Medicare-funded procedures that could result in an \"abortive outcome\" performed each year from 1995 to 2004, but it should be noted that this figure includes miscarriages as well as terminations. On the other hand, many women who have medical abortions performed at private hospitals may not claim the Medicare rebate.\n\nSouth Australia is the only state which collects and publishes data on abortions. In 2002 there were 5,147 medical abortions performed in South Australia, or 17.2 per 1000 women aged 15–44. Projected nationally, this would suggest that about 73,300 abortions were performed nationwide. This does not take into account differences between states. For example, unpublished data from Western Australia estimates a rate of 19.4 terminations per 1000 women in the same age bracket, which would indicate about 82,700 abortions projected nationally.\n\nThe South Australian data also indicates that the vast majority of abortions performed 1994–2002 occurred before 14 weeks gestation. Less than 2% took place at or after 20 weeks.\n\nSince at least the 1980s, opinion polls have shown a majority of Australians support abortion rights.\n\n\n"}
{"id": "44594082", "url": "https://en.wikipedia.org/wiki?curid=44594082", "title": "Abortion in Botswana", "text": "Abortion in Botswana\n\nAbortion in Botswana is only legal if the abortion will save the woman's life, if the pregnancy gravely endangers the woman's physical or mental health, or if it is a result of rape or incest. In Botswana, abortions that meet these requirements must be performed within the first 16 weeks of pregnancy in a government hospital and must be approved by two physicians.\n\nThough women in Botswana are recognized as having some of the best access to abortions in Sub-Saharan Africa because of these exceptions, many women are still resorting to unsafe abortions and self-induced abortions, commonly leading to maternal death.\n\nIn Botswana, many families still follow the lobolo custom where men pay a woman's family in order to take her as a bride. This has established an expectation that husbands have paid for and own their wives' bodies, including their reproductive rights. Even though this sentiment may lead to pregnancy that is a result of rape, hospitals and clinics are unlikely to approve marital rape cases as justifying abortion, as cultural norms suggest husbands are entitled to their wives' bodies.\n"}
{"id": "3092471", "url": "https://en.wikipedia.org/wiki?curid=3092471", "title": "Alma Ata Declaration", "text": "Alma Ata Declaration\n\nThe Declaration of Alma-Ata was adopted at the International Conference on Primary Health Care (PHC), Almaty (formerly Alma-Ata), Kazakhstan (formerly Kazakh Soviet Socialist Republic), 6–12 September 1978. It expressed the need for urgent action by all governments, all health and development workers, and the world community to protect and promote the health of all people. It was the first international declaration underlining the importance of primary health care. The primary health care approach has since then been accepted by member countries of the World Health Organization (WHO) as the key to achieving the goal of \"Health For All\" but only in developing countries at first. This applied to all other countries five years later. The Alma-Ata Declaration of 1978 emerged as a major milestone of the twentieth century in the field of public health, and it identified primary health care as the key to the attainment of the goal of \"Health for All\" around the globe.\n\nThe conference called for urgent and effective national and international action to develop and implement primary health care throughout the world and particularly in developing countries in a spirit of technical cooperation and in keeping with a New International Economic Order. It urged governments, the WHO, UNICEF, and other international organizations, as well as multilateral and bilateral agencies, non-governmental organizations, funding agencies, all health workers and the world community to support national and international commitment to primary health care and to channel increased technical and financial support to it, particularly in developing countries. The conference called on the aforementioned to collaborate in introducing, developing and maintaining primary health care in accordance with the spirit and content of the declaration. The declaration has 10 points and is non-binding on member states.\n\nThe first section of the declaration reaffirms the WHO definition of health as \"a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity\". The definition seeks to include social and economic sectors within the scope of attaining health and reaffirms health as a human right.\n\nThe declaration highlighted the inequality of health status between the developed and the developing countries and termed it politically, socially and economically unacceptable.\n\nThe third section called for economic and social development as a pre-requisite to the attainment of health for all. It also declared positive effects on economic and social development and on world peace through promotion and protection of the health of the people.\n\nParticipation of people as a group or individually in planning and implementing their health care was declared as a human right and duty.\n\nThis section emphasized on the role of the state in providing adequate health and social measures. This section enunciated the call for \"Health For All\" which became a campaign of the WHO in the coming years. It defined Health for All as the attainment by all peoples of the world by the year 2000 of a level of health that will permit them to lead a socially and economically productive life. The declaration urged governments, international organizations and the whole world community to take this up as a main social target in the spirit of social justice.\n\nThis section defined primary health care and urged signatories to incorporate the concept of primary health care in their health systems. Primary health care has since been adopted by many member nations. More recently, Margaret Chan, the Director-General of the WHO has reaffirmed the primary health care approach as the most efficient and cost-effective way to organize a health system. She also pointed out that international evidence overwhelmingly demonstrates that health systems oriented toward primary health care produce better outcomes, at lower costs, and with higher user satisfaction.\n\nThe seventh section lists the components of primary health care. The next two sections called on all governments to incorporate primary health care approach in their health systems and urged international cooperation in better use of the world's resources.\n\nThe Alma-Ata Declaration generated numerous criticisms and reactions worldwide. Many argued that the slogan \"Health for All by 2000\" was not possible and that the declaration did not have clear targets. In his article \"The Origins of Primary Health Care and Selective Primary Health Care\", Marcos Cueto claims that the declaration was condemned as being unrealistic, idealistic, and too broad. As a result of these criticisms, the Rockefeller Foundation sponsored the Health and Population Development Conference held in Italy at the Bellagio Conference Center in 1979 (a year after Alma-Ata). The purpose of this conference was to specify the goals of PHC and to achieve more effective strategies.\n\nAs a result, Selective Primary Health Care (PHC) was introduced. As opposed to PHC of the Alma-Ata Declaration, Selective PHC presented the idea of obtaining low-cost solutions to very specific and common causes of death. The targets and effects of Selective PHC were clear, concise, measurable, and easy to observe. This is because Selective PHC had explicit areas of focus that were believed to be the most important. They were known as GOBI (growth monitoring, oral rehydration treatment, breastfeeding, and immunization), and later GOBI-FFF (adding food supplementation, female literacy, and family planning). Unlike the Alma-Ata Declaration, these aspects were very specific and concise, making global health as successful and attainable as possible. Nonetheless, there were still many supporters who preferred the comprehensive PHC introduced at Alma-Ata over Selective PHC, criticizing the latter as a misrepresentation of some core principles of the original declaration. The main critics are toward selective care as a restrictive approach to health. Therefore, such approach to primary care does not contribute toward integral care (globality) and does not address social determinants as a fundamental aspect of illness and thus essential to health care planning.\n\nThe World Health Organization, UNICEF and the Government of Kazakhstan co-hosted the Global Conference on Primary Health Care in Astana on 25-26 October, 2018. The conference marked the 40th anniversary of the Alma-Ata Declaration, and united world leaders to affirm that strong primary health care is essential to achieve universal health coverage.\n\n\n"}
{"id": "4397110", "url": "https://en.wikipedia.org/wiki?curid=4397110", "title": "Bangalore Water Supply and Sewerage Board", "text": "Bangalore Water Supply and Sewerage Board\n\nThe Bangalore Water Supply and Sewerage Board (BWSSB) is the premier governmental agency responsible for sewage disposal and water supply to the Indian city of Bangalore. It was formed in 1964.\n\nBWSSB currently supplies approximately 900 million liters (238 million gallons) of water to the city per day, despite a municipal demand of 1.3 billion liters. Water for the city (with a population of 10 million) comes from a number of sources, with 80% of it coming from the Cauvery River. Water is also drawn from the Arkavathy River, but the supply does not meet the demand.\n\nThe \"per capita\" water supply that BWSSB is able to provide averages 100 to 125 liters per capita per day. However, the actual availability of water to the poor areas of the city is limited by infrastructure, and so for these areas, the \"per capita\" supply can be as low as 40 to 45 liters per day. The \"per capita\" national standard for a city the size of Bangalore is 150 to 200 liters per day, From the month of March 2012, water supply in Whitefield has been stopped, with even rich neighborhoods left to fend for themselves.\n\nThe majority of the water for Bangalore is imported by the BWSSB from the Cauvery River, over south of the city. Cauvery water was originally drawn from a reservoir near the village of Thorekadanahalli. To meet the increasing demand, the \"Cauvery Water Supply Scheme\" was undertaken by the BWSSB, with Stages I - III completed. Stage IV is currently being built, with Phase I completed and bringing an additional 270 million liters to the city. Construction of Stage IV: Phase II is expected to bring an additional 510 million liters, and will be completed by 2010.\n\nThe energy required to transport the water this distance consumes 75% of the agency's revenues.\n\nUp to 20% of the normal water supply for Bangalore comes from the Arkavathy River, from two reservoirs built on the river, the Hesaraghatta (or Hesseraggatta) which was built in 1894 and the Tippagondanahalli Reservoir (or T G Halli), which was built in 1933.\n\nIn June 2007, T G Halli, from which BWSSB pumped 35 million liters per day into the western parts of the city, had effectively gone dry due to lack of rainfall over its watershed. BWSSB officials stated that they would address the issue initially by making water available for only one hour a day, and then by diverting water from other parts of the city, as well as bringing water in by truck. The quality of the water in the reservoir has also become compromised by the discharge of effluent into the reservoir.\n\nTo further address water supply issues, BWSSB has studied the rainwater harvesting (RWH) techniques used in Chennai, where such methods have greatly reduced water shortage issues. RWH methods are becoming mandatory in Bangalore. In addition, in April 2007, BWSSB issued a moratorium on new building hookups to the water system. The moratorium was lifted in July, but only on new buildings that have a RWH system installed.\n\nThe BWSSB is also considering implementing a mandatory water rationing program in order to evenly distribute what water supplies exist. A small trial rationing program was instituted in March 2007. Electronic water meters would shut off after the allotted amount of water was used, but the program was considered a technical failure, due in part to software issues.\n\nGroundwater extraction has caused the water table to drop variously from 90 to 300 meters (300 to 1000 feet) below ground level (as compared to an average water table depth of about 30 meters (90 feet) two decades ago), according to the Indian Institute of Science.\n\nThe situation affecting T G Halli are not isolated. Rapid urbanization in and around Bangalore has destroyed many wetlands areas (of the 51 lakes in the city in 1973, only 17 remain in 2007), which has also contributed to the decrease in the water table.\n\nBangalore's original sewerage system was built in 1922, a time when the city was much smaller than it is today; the original system served only the heart of the city. In 1950, with the city greatly expanding, a project was initiated to greatly expand the sewerage system. After the BWSSB was formed in the 1960s, programs were again implemented to expand the system to not sewer areas. The current sewer system utilizes stoneware pipes up to in diameter, and RCC pipes for the mains and outfalls up to in diameter.\n\nThere are three main sewage treatment plants, which are located in the Vrishabavathy, Koramangala-Chellaghatta and Hebbal valleys. Two additional mini-plants have been built near Madiwala and Kempambudi.\n\n\n"}
{"id": "44241154", "url": "https://en.wikipedia.org/wiki?curid=44241154", "title": "Bare (magazine)", "text": "Bare (magazine)\n\nBare was a British magazine developed and launched as a wellbeing brand by the John Brown Media company It was published from Sept/Oct 2000 to August 2001, with six issues per year. An early version of the magazine, then called \"Well\" was tested in market research groups in 1999 where Claudia Zeff, then art director of \"UK Gardens Illustrated\" commissioned designer, Kirsten Willey to produce a wellbeing magazine concept. It was Zeff who suggested the name change from \"Well\" to \"Bare\" after watching a BBC documentary about British architect, John Pawson.\n\nIn March 2000, Ilse Crawford – founder editor of British \"ELLE Decoration\" - was invited as editor of \"Bare\" for a summer launch. At this time the publication was a bimonthly magazine available on news-stands internationally. The advertising department led by publisher, Honor Riley, formerly of Condé Nast, secured a world first with Chanel advertising in the launch issue. Partnerships were garnered with Harvey Nichols and other brands with synergy. The magazine was popular amongst the design aficionado in Belgium and a copy of the Helena Christensen edition made an appearance on the \"Sex And The City\" episode, \"Time and Punishment\".\n\nThe magazine has been described as \"speak[ing] the earnest psychobabble of the Hampstead eco-hypochondriac\".\n\n"}
{"id": "49492650", "url": "https://en.wikipedia.org/wiki?curid=49492650", "title": "Blue cone monochromacy", "text": "Blue cone monochromacy\n\nBlue cone monochromacy (BCM) is an inherited eye disease that causes severely impaired color discrimination, low vision, nystagmus and photophobia due to the absence of functionality of red (L) and green (M) cone photoreceptor cells in the retina. This form of retinal disorder is a recessive X-linked disease and manifests its symptoms in early infancy.\n\nBlue cone monochromacy is considered a stationary (non-progressive) disease, although there is evidence of disease progression with macular degeneration in many patients.\n\nCone photoreceptor cells in the retina are responsible for day-light vision, color vision, visual acuity and sight in the central visual field. Blue cone monochromacy is a severe condition in which neither the cones sensible to red light, L-cones, nor the cones sensible to green light, M-cones, are fully functional, and only rods and cones sensible to blue light, S-cones, are functional.\n\nA variety of symptoms characterize blue cone monochromacy: affected individuals show, since after birth, low vision with a visual acuity between 20/60 and 20/200 and poor color discrimination. Phenomena such as photophobia (hemeralopia), which describes the event in which light is perceived as an intense glare, often manifest. Moreover, nystagmus is present since the age of 2 months and may slowly decrease with age. Families with BCM-affected individuals show a recessive X-linked inheritance pattern.\n\nThe majority of subjects with blue cone monochromacy are thought to have a stable condition, although there are cases of evidence of disease progression with macular changes.\n\nBlue cone monochromacy is inherited from the X chromosome.\n\nThe genes involved in the BCM are on chromosome X, at position Xq28, at the end of the q arm of the X chromosome. The genes’ names are:\n\nLCR is the Locus Control Region, and acts as a promoter of the expression of the two opsin genes thereafter. In the absence of this gene, none of the following two opsin genes are expressed in the human retina. In addition, it ensures that only one of the two opsin genes (red or green) is expressed exclusively in each cone. OPN1LW and OPN1MW\n\nThe gene responsible for the formation of the blue photopigment is in a position far away, on chromosome 7 and the gene responsible for the formation of rhodopsin (the rod photopigment) is located on chromosome 3.\n\nThere are many genetic mutations that can affect this group of genes, LCR, OPN1LW and OPN1MW that lead to the BCM: a deletion of the LCR\n, intragenic deletion of exons within the genes OPN1LW and OPN1MW and a 2 steps mechanism with an homologous recombination and a punctual inactivation.\n\nThe point mutation is the so-called C203R. The name of the point mutations indicates the position at which mutation has occurred, in this case the amino acid position 203 and which has been replaced, in this case a C = Cysteine with an R = Arginine. The C203R mutation causes the opsin protein once formed does not carry the folding, that is it doesn’t take the proper three-dimensional form. Other point mutations are the P307L and R247X. The last one replaces arginine with the Stop codon, prematurely stopping at position 247 the formation of the protein (nonsense mutation).\nOther mutations on genes OPN1LW and OPN1MW that lead to the blue cone monochromacy are constituted by a set of point mutations called for example LIAVA. Blue cone monochromacy will be caused by the production of new hybrid gene, like in the previous case, from the homologous recombination of OPN1LW and OPN1MW. Exon 3 of the hybrid gene contains the following amino acids in the positions indicated: 153 Leucine, 171 Isoleucine, 174 Alanine, 178 Valine and 180 Alanine. This genotype has the abbreviated name LIAVA.\n\nAnother disease of the retina that is associated with the position Xq28 is Bornholm Eye Disease (BED).\n\nFinally note there is also a particular mutation of the two genes OPN1LW and OPN1MW which causes a different disease from the blue cone monochromacy. This type of mutation is named W177R and is a misfolding mutation that, if present on both opsin genes cause cone dystrophy with evidence of degeneration and cell death of the cones.\n\nIn a male child, from 2 months upwards, an aversion to light and nystagmus may lead to the suspicion of a case of blue cone monochromacy, but it does not provide sufficient indications to establish the form of the condition. To identify a case of blue cone monochromacy, it is necessary to reconstruct the family history\n, with the condition linked to the transmission of the X chromosome, if there are other cases in the family.\n\nThe electroretinogram (ERG), can demonstrate the loss of cone function with retained rod function.\n\nIn adult individuals a color test like a Farnsworth D-15, a Farnsworth Munsell 100 Hue test can be part of the diagnosis tools and a Berson test makes it possible to distinguish blue cone monochromacy from other diseases. Visual acuity is usually tested in adults and is between 20/60 and 20/200.\n\nThere is no cure for blue cone monochromacy; however, the efficacy and safety of various prospective treatments are currently being evaluated. Gene therapy is actually the most promising one. The goal of gene therapy studies is to virally supplement retinal cells expressing mutant genes associated with the blue cone mMonochromacy phenotype with healthy forms of the gene; thus, allowing the repair and proper functioning of retinal photoreceptor cells in response to the instructions associated with the inserted healthy gene.\nCorrective visual aides and personalized vision therapy provided by Low Vision Specialists may help patients correct glare and optimize their remaining visual acuity. Tinted lenses for photophobia allow for greater visual comfort. A magenta (mixture of red and blue) tint allows for best visual acuity since it protects the rods from saturation while allowing the blue cones to be maximally stimulated.\n\nBCM is a cause of inherited low vision with approximately 1/100,000 individuals experiencing the disease within their lifetime.\nThe disease affects male recipients of the X-linked mutation, while females usually remain unaffected carriers of the BCM trait.\nThe disease is known since many years. The first detailed description of blue cone monochromacy is that given by Huddart (1777). The subject of that report 'could never do more than guess the name of any color; yet he could distinguish white from black, or black from any light or bright color...He had 2 brothers in the same circumstances as to sight; and 2 brothers and sisters who, as well as his parents, had nothing of this defect.' Sloan in 1954 studied several patients affected by blue cone monochromacy. Blackwell and Blackwell (1961) described patients who can distinguish blue and yellow signals and seems to have functional rods and S-cones cells. Information presented by Spivey (1965) indicated that affected persons can see small blue objects on a large yellow field and vice versa. The disease has been studied also by Alpern et al. (1960) and by Fleischman in 1981. The most important results have been obtained in 1989 and 1993 by Nathans et al. and by Reyniers et al. in 1991 who identified the genes causing blue cone monochromacy.\n\nFollowing previous important works several research groups worked on blue cone monochromacy with the aim to describe genotype and phenotype features of the disease.\n\nHigh-resolution imaging of the cone mosaic in the living human eye using Adaptive Optics has made it possible to address the question of how different genetic rearrangements affect the retinal phenotype at the cellular level.\nCone Mosaic studies performed with Adaptive Optics technology revealed a disrutpion of the normal pattern of photoreceptors human cone mosaic in presence of the most frequent causative blue cone monochromacy genetic mutations. Adaptive optics images show that the number of visible cones was significantly reduced and the regularity of the cone mosaic was disrupted compared to normals. These imaging data suggest that failure to express opsin results in the early degeneration of the associated cone photoreceptor.\n\nOnly in the last few years, blue cone monochromacy has been studied with the aim to find a treatment.\n\nFuture treatments may involve gene therapy. In fact it seems that in case of a genetic deletion of the human cone visual pigment there is a sufficient number of photoreceptors to warrant Gene Therapy. In 2015 scientists at the University of Pennsylvania evaluated possible outcoming measures of BCM gene therapy\n\nA previous important result shows that in adult primates it is possible to restore colour vision through an AAV gene therapy that introduced a new opsin in the primate retina. A mouse model of blue cone monochromacy has been treated with gene-based therapy.\n\n"}
{"id": "44260535", "url": "https://en.wikipedia.org/wiki?curid=44260535", "title": "Deborah Schofield", "text": "Deborah Schofield\n\nDeborah Schofield is the Professor and Chair of Health Economics at the School of Public Health at the University of Sydney.\n\nSchofield was born in 1965 in Wahroonga, Sydney. She has worked for the Australian Government, in academia and clinical practice and is notable for establishing the health microsimulation modelling program at the National Centre for Social and Economic Modelling. The methods for modelling public health expenditure which she established are now used by the Organisation for Economic Co-operation and Development. Prior to joining the University of Sydney, Schofield worked for a decade in senior government roles where she led the development of major new policies, continues to inform her academic career.\n\nProfessor Schofield began her career with a Bachelor of Speech Therapy from the University of Queensland in 1986, which she followed up with Graduate Diploma in Computing Science from the University of Canberra in 1993 and finally graduated with a PhD in Mathematics-economic modelling in 2000, also from the University of Canberra.\n\nSchofield's research includes assessments of the productivity impacts of illness and she currently leads a research program at the University of Sydney on the relationship between health and social policy where health has impacts across multiple government agencies. The new applications of microsimulation Schofield pioneered helps to ensure health care funding is sustainable and labour force participation is maximised. This assists governments providing the information needed by policymakers who impact health, the labour market, income and taxation policy. The purpose of her research is to ensure that the health and aged care systems meet the demands of an ageing population. It uses data drawn from her own work, other studies and publicly available datasets in areas as diverse as IQ deficits caused by anaemia in pregnancy, poverty and mental illness, genetic testing for highly disabling childhood genetic disorders, chronic pain and whole genome sequencing.\n"}
{"id": "11797534", "url": "https://en.wikipedia.org/wiki?curid=11797534", "title": "Density dependence", "text": "Density dependence\n\nIn population ecology, density-dependent processes occur when population growth rates are regulated by the density of a population. This article will focus on density-dependence in the context of macroparasite life cycles.\n\nPositive density-dependence, density-dependent facilitation, or the Allee effect describes a situation in which population growth is facilitated by increased population density.\n\nFor dioecious (separate sex) obligatory parasites, mated female worms are required to complete a transmission cycle. At low parasite densities, the probability of a female worm encountering a male worm and forming a mating pair can become so low that reproduction is restricted due to single sex infections. At higher parasite densities, the probability of mating pairs forming and successful reproduction increases. This has been observed in the population dynamics of \"Schistosomes\".\n\nPositive density-dependence processes occur in macroparasite life cycles that rely on vectors with a cibarial armature, such as \"Anopheles\" or \"Culex\" mosquitoes. For \"Wuchereria bancrofti\", a filarial nematode, well-developed cibarial armatures in vectors can damage ingested microfilariae and impede the development of infective L3 larvae. At low microfilariae densities, most microfilariae can be ruptured by teeth, preventing successful development of infective L3 larvae. As more larvae are ingested, the ones that become entangled in the teeth may protect the remaining larvae, which are then left undamaged during ingestion.\n\nPositive density-dependence processes may also occur in macroparasite infections that lead to immunosuppression. \"Onchocerca volvulus\" infection promotes immunosuppressive processes within the human host that suppress immunity against incoming infective L3 larvae. This suppression of anti-parasite immunity causes parasite establishment rates to increase with higher parasite burden.\n\nNegative density-dependence, or density-dependent restriction, describes a situation in which population growth is curtailed by crowding, predators and competition. In cell biology, it describes the reduction in cell division. When a cell population reaches a certain density, the amount of required growth factors and nutrients available to each cell becomes insufficient to allow continued cell growth.\n\nThis is also true for other organisms because an increased density means an increase in intraspecific competition. Greater competition means an individual has a decreased contribution to the next generation i.e. offspring. \nDensity-dependent mortality can be overcompensating, undercompensating or exactly compensating. \n\nThere also exists density-independent inhibition, where other factors such as weather or environmental conditions and disturbances may affect a population's carrying capacity.\n\nAn example of a density-dependent variable is crowding and competition.\n\n Density-dependent fecundity exists, where the birth rate falls as competition increases. In the context of gastrointestinal nematodes, the weight of female \"Ascaris lumbricoides\" and its rates of egg production decrease as host infection intensity increases. Thus, the per-capita contribution of each worm to transmission decreases as a function of infection intensity.\n\n In macroparasite life cycles, density-dependent processes can influence parasite fecundity, survival, and establishment. Density-dependent processes can act across multiple points of the macroparasite life cycle. For filarial worms, density-dependent processes can act at the host/vector interface or within the host/vector life-cycle stages. At the host/vector interface, density-dependence may influence the input of L3 larvae into the host’s skin and the ingestion of microfilariae by the vector. Within the life-cycle stages taking place in the vector, density-dependence may influence the development of L3 larvae in vectors and vector life expectancy. Within the life-cycle stages taking place in the host, density-dependence may influence the development of microfilariae and host life expectancy.\n\nIn reality, combinations of negative (restriction) and positive (facilitation) density-dependent processes occur in the life cycles of parasites. However, the extent to which one process predominates over the other vary widely according to the parasite, vector, and host involved. This is illustrated by the \"W. bancrofti\" life cycle. In \"Culex\" mosquitoes, which lack a well-developed cibarial armature, restriction processes predominate. Thus, the number of L3 larvae per mosquito declines as the number of ingested microfilariae increases. Conversely, in \"Aedes\" and \"Anopheles\" mosquitoes, which have well-developed cibarial armatures, facilitation processes predominate. Consequently, the number of L3 larvae per mosquito increases as the number of ingested microfilariae increases.\n\nNegative density-dependent (restriction) processes contribute to the resilience of macroparasite populations. At high parasite populations, restriction processes tend to restrict population growth rates and contribute to the stability of these populations. Interventions that lead to a reduction in parasite populations will cause a relaxation of density-dependent restrictions, increasing per-capita rates of reproduction or survival, thereby contributing to population persistence and resilience.\n\nContrariwise, positive density-dependent or facilitation processes make elimination of a parasite population more likely. Facilitation processes cause the reproductive success of the parasite to decrease with lower worm burden. Thus, control measures that reduce parasite burden will automatically reduce per-capita reproductive success and increase the likelihood of elimination when facilitation processes predominate.\n\nThe extinction threshold refers to minimum parasite density level for the parasite to persist in a population. Interventions that reduce parasite density to a level below this threshold will ultimately lead to the extinction of that parasite in that population. Facilitation processes increase the extinction threshold, making it easier to achieve using parasite control interventions. Conversely, restriction processes complicates control measures by decreasing the extinction threshold.\n\nAnderson and Gordon (1982) propose that the distribution of macroparasites in a host population is regulated by a combination of positive and negative density-dependent processes. In overdispersed distributions, a small proportion of hosts harbour most of the parasite population. Positive density-dependent processes contribute to overdispersion of parasite populations, whereas negative density-dependent processes contribute to underdispersion of parasite populations. As mean parasite burden increases, negative density-dependent processes become more prominent and the distribution of the parasite population tends to become less overdispersed.\n\nConsequently, interventions that lead to a reduction in parasite burden will tend to cause the parasite distribution to become overdispersed. For instance, time-series data for \"Onchocerciasis\" infection demonstrates that 10 years of vector control lead to reduced parasite burden with a more overdispersed distribution.\n\n"}
{"id": "6856978", "url": "https://en.wikipedia.org/wiki?curid=6856978", "title": "Drug Tariff", "text": "Drug Tariff\n\nThe Drug Tariff is a monthly publication used as a reference for the payment and repayment of NHS prescription costs in England and Wales by pharmacists or doctors dispensing in primary care. It covers such issues as the costs of prescription payments for patients, costs of appliances and blacklisted medicines.\n\n\n"}
{"id": "5004001", "url": "https://en.wikipedia.org/wiki?curid=5004001", "title": "Electrical muscle stimulation", "text": "Electrical muscle stimulation\n\nElectrical muscle stimulation (EMS), also known as neuromuscular electrical stimulation (NMES) or electromyostimulation, is the elicitation of muscle contraction using electric impulses. EMS has received an increasing amount of attention in the last few years for many reasons: it can be utilized as a strength training tool for healthy subjects and athletes; it could be used as a rehabilitation and preventive tool for partially or totally immobilized patients; it could be utilized as a testing tool for evaluating the neural and/or muscular function in vivo; it could be used as a post-exercise recovery tool for athletes. The impulses are generated by a device and are delivered through electrodes on the skin near to the muscles being stimulated. The electrodes are generally pads that adhere to the skin. The impulses mimic the action potential that comes from the central nervous system, causing the muscles to contract. The use of EMS has been cited by sports scientists as a complementary technique for sports training, and published research is available on the results obtained. In the United States, EMS devices are regulated by the U.S. Food and Drug Administration (FDA).\n\nA number of reviews have looked at the devices.\n\nElectrical muscle stimulation can be used as a training, therapeutic, or cosmetic tool.\n\nIn medicine, EMS is used for rehabilitation purposes, for instance in physical therapy in the prevention of disuse muscle atrophy which can occur for example after musculoskeletal injuries, such as damage to bones, joints, muscles, ligaments and tendons. This is distinct from transcutaneous electrical nerve stimulation (TENS), in which an electric current is used for pain therapy.\n\nIn EMS training few muscular groups are targeted at the same time, for specific training goals.\n\nThe FDA rejects certification of devices that claim weight reduction. EMS devices cause a calorie burning that is marginal at best: calories are burnt in significant amount only when most of the body is involved in physical exercise: several muscles, the heart and the respiratory system are all engaged at once. However, some authors imply that EMS can lead to exercise, since people toning their muscles with electrical stimulation are more likely afterwards\nto participate in sporting activities as the body becomes ready, fit, willing and able to take on physical activity.\n\n\"Strength training by NMES does promote neural and muscular adaptations that are complementary to the well-known effects of voluntary resistance training\". This statement is part of the editorial summary of a 2010 world congress of researchers on the subject. Additional studies on practical applications, which came after that congress, pointed out important factors that make the difference between effective and ineffective EMS. This in retrospect explains why in the past some researchers and practitioners obtained results that others could not reproduce. Also, as published by reputable universities, EMS causes adaptation, i.e. training, of muscle fibers. Because of the characteristics of skeletal muscle fibers, different types of fibers can be activated to differing degrees by different types of EMS, and the modifications induced depend on the pattern of EMS activity. These patterns, referred to as protocols or programs, will cause a different response from contraction of different fiber types. Some programs will improve fatigue resistance, i.e. endurance, others will increase force production.\n\nLuigi Galvani (1761) provided the first scientific evidence that current can activate muscle. During the 19th and 20th centuries, researchers studied and documented the exact electrical properties that generate muscle movement. It was discovered that the body functions induced by electrical stimulation caused long-term changes in the muscles. In the 1960s, Soviet sport scientists applied EMS in the training of elite athletes, claiming 40% force gains. In the 1970s, these studies were shared during conferences with the Western sport establishments. However, results were conflicting, perhaps because the mechanisms in which EMS acted were poorly understood. Recent medical physiology research pinpointed the mechanisms by which electrical stimulation causes adaptation of cells of muscles, blood vessels and nerves.\n\nThe U.S. Food and Drug Administration (FDA) certifies and releases EMS devices into two broad categories: over-the counter devices (OTC), and prescription devices. OTC devices are marketable only for muscle toning; prescription devices can be purchased only with a medical prescription for therapy. Prescription devices should be used under supervision of an authorized practitioner, for the following uses:\nThe FDA mandates that manuals prominently display contraindication, warnings, precautions and adverse reactions, including: no use for wearer of pacemaker; no use on vital parts, such as carotid sinus nerves, across the chest, or across the brain; caution in the use during pregnancy, menstruation, and other particular conditions that may be affected by muscle contractions; potential adverse effects include skin irritations and burns\n\nOnly FDA-certified devices can be lawfully sold in the US without medical prescription. These can be found at the corresponding FDA webpage for certified devices. The FTC has cracked down on consumer EMS devices that made unsubstantiated claims; many have been removed from the market, some have obtained FDA certification.\n\nNon-professional devices target home-market consumers with wearable units in which EMS circuitry is contained in belt-like garments (ab toning belts) or other clothing items.\n\nThe Relax-A-Cizor was one brand of device manufactured by the U.S. company Relaxacizor, Inc.\n\nFrom the 1950s, the company marketed the device for use in weight loss and fitness. Electrodes from the device were attached to the skin and caused muscle contractions by way of electrical currents. The device caused 40 muscular contractions per minute in the muscles affected by the motor nerve points in the area of each pad. The directions for use recommended use of the device at least 30 minutes daily for each figure placement area, and suggested that the user might use it for longer periods if they wished. The device was offered in a number of different models which were powered either by battery or household current.\n\nRelax-A-Cizors had from 1 to 6 channels. Two pads (or electrodes) were connected by wires to each channel. The user applied from 2 to 12 pads to various parts of their body. For each channel there was a dial which purported to control the intensity of the electrical current flowing into the user's body between the two pads connected to that channel.\n\nAs of 1970, the device was manufactured in Chicago, Illinois, by Eastwood Industries, Inc., a wholly owned subsidiary of Relaxacizor, Inc., and was then distributed throughout the country at the direction of Relaxacizor, Inc., or Relaxacizor Sales, Inc.\n\nThe device was banned by the United States Food and Drug Administration in 1970 as it was deemed to be potentially unhealthy and dangerous to the users. The case went to court, and the United States District Court for the Central District of California held that the Relax-A-Cizor was a \"device\" within the meaning of 21 U.S.C. § 321 (h) because it was intended to affect the structure and functions of the body as a girth reducer and exerciser, and upheld the FDA's assertions that the device was potentially hazardous to health.\n\nThe FDA informed owners of Relax-A-Cizors that second-hand sale of Relax-A-Cizors was illegal, and recommended that they should destroy the devices or render them inoperable.\n\nSlendertone is another brand name. the company's Slendertone Flex product had been approved by the U.S. Food and Drug Administration for over-the-counter sale for toning, strengthening and firming abdominal muscles.\n\n\n"}
{"id": "4334983", "url": "https://en.wikipedia.org/wiki?curid=4334983", "title": "Emotional competence", "text": "Emotional competence\n\nEmotional competence refers to one's ability to express or release one's inner feelings (emotions). It implies an ease around others and determines one's ability to effectively and successfully lead and express. It is described as the essential social skills to recognize, interpret, and respond constructively to emotions in yourself and others.\n\nThe concept of emotional competence is rooted in understanding emotions as normal, useful aspects of being human. Anger is a reaction to aggression and gives a person the strength to repel the aggression. Though anger is usually seen in a negative light, sometimes it can also serve the purpose of protection. Grief is a reaction to abandonment or feeling unloved and it has the effect of eliciting sympathetic responses from others. Fear is a response to danger and has a clear physiological effect of heightening our senses and speeding up our reactions.\n\nFrom this it can be seen that the suppression of emotion can be useful to avoid injury, embarrassment and arrest, but teaching people to suppress their inappropriate emotions is part of normal society. Suppressing other people's emotions to avoid conflict or discomfort in oneself can lead to controlling them, which may be unhealthy for all concerned. Emotionally competent people do express emotions appropriate to the situation, to their needs and to others, and they attempt not to suppress appropriate emotions, reactions and communications of feelings by others.\n\nSome psychologists believe that if appropriate emotions are not expressed on a regular basis, a misplaced or unresolved memory of them becomes stored. Alternatively, this may also lead to an inability to process emotional clues in others, or have emotionally appropriate behaviors in oneself. Events in the future may trigger old emotions resulting in inappropriate emotional responses, or may trigger nothing, leaving one with a lack of emotional competence. This often applies to emotions that children may be experiencing, or are prevented from expressing, when an adult simply wishes to avoid dealing with feelings that may be very real to the child, who has yet to learn that feelings and facts are not mutually exclusive, or that emotionalism can be misunderstood or misused. Releasing childhood emotions, or pent up adult emotions can be a useful tool in co-counselling.\n\nEmotional competence can lead to improved health through avoiding stress that would otherwise result from suppressing emotions. It can also lead to improved relationships since inappropriate emotions are less likely to be expressed and appropriate behaviour is not avoided through fear of triggering some emotion. It can be seen in an economics of human resource as a real capital.\n\nQuoted in her work in 2002, Prof. Dr. Benedicte Gendron defined the concept of emotional capital as the set of personal and social emotional competencies which constitute a resource inherent to the person, useful for the personal, professional and organizational development and takes part in social cohesion, to personal, social and economic success\" (Gendron, 2004, 2006).\n\"Furthermore, because of its impact on performance (at school as at work and for the organizations), on well-being (life satisfaction, health...) and on social cohesion and citizenship, emotional capital should be taken into account seriously by public and educational policy-makers and practicians and companies.\"\n\"To end, emotional capital is the set (resource) of emotional competencies which gives individuals and organizations the ability to use emotions to help individuals at solving problems and living a more effective life and the organization at facing economics and social changes and being successful and surviving in the new economics world. Emotional capital without capital (physical, human, social and cultural), or capital without emotional capital, is only part of a solution. With it, it is the head working with the heart and the hands. All of the three H need to be combined... taking into account the three H of each individual: Hands, Head and Heart.\" (Gendron, 2004, p. 31).\nThe concept is derived from the emotional intelligence work which looks at self-awareness, self-regulation, social awareness and social regulation as essential competencies helpful at coping stress regulation and using emotions in a positive way.\n\nHumanistic approaches to assertiveness, as, for instance, outlined by Anne Dickson emphasise the importance of working with emotions. In particular it recognises the need to address manipulative or passive (the person does not say what they want) – aggressive (they try to force the other person to do what they want) behaviour in which the manipulator exploits the feelings of the other to try to get what they want. Building up emotional competence is a way of learning to handle such behaviour.\n\nAnother aspect is learning to be assertive when feeling emotional. Assertiveness training involves learning a range of ways to handle any situation so that a person is able to choose a way which seems appropriate for them on each occasion. With respect to emotions, people are encouraged to notice and accept what they feel. They then have choices from handling the situation calmly through doing so and saying how they feel to letting the emotion out, all of which involve emotional competence.\n\nThis also would encompass the realm of where the emotionally competent response would have judicial consequences, e.g. competence under the law.\n\nSome researchers feel the role of emotion has been neglected, both in traditional accounts of decision-making and in assessments of adjudicative competence, and further attention and study.\n\n\n\n"}
{"id": "45700106", "url": "https://en.wikipedia.org/wiki?curid=45700106", "title": "Eric Stover", "text": "Eric Stover\n\nEric Stover is an American human rights researcher and advocate and faculty director of the Human Rights Center at the University of California at Berkeley.\n\nStover officially began his human rights work as a researcher at Amnesty International in London, England from 1977-1980. During this time, the organization won the Nobel Peace Prize for its “campaign against torture,” and the United Nations Prize in the Field of Human Rights. Following Amnesty International, Stover became the Director of the Science and Human Rights Program of the American Association for the Advancement of Science. In 1992, Stover served as the Executive Director of Physicians for Human Rights where he worked on forensic missions to examine mass gravesites for the International Criminal Tribunals for the former Yugoslavia and Rwanda. While at PHR, Stover performed research on the sociomedical consequences of land mines in war-torn countries such as Cambodia. His research helped launch the International Campaign to Ban Landmines, which, along with the organization’s director, Jody Williams, received the Nobel Peace Prize in 1997. He has published seven books and numerous reports and articles for press and scholarly publications.\n\nStover became the Faculty Director of the Human Rights Center (HRC) at the UC Berkeley School of Law in 1996, two years after the center was established.\n\nThe HRC is an interdisciplinary research center which uses science and law to pursue human rights issues. \nThe Human Rights center has conducted investigations or research focusing on sexual violence, human trafficking, torture, public health among vulnerable populations, accountability for war criminals, child soldiers, family reunification, and the applications of advanced technologies to human rights work. The Center's reports have examined human rights issues in sub-Saharan Africa, Central America and South America, Southeast Asia, the Balkans, the Middle East, and the United States. In February, 2015, the Human Rights Center was awarded a grant from the MacArthur foundation's program for Creative and Effective Institutions.\n\nFaculty Award for Civic Engagement, 2013. University of California, Berkeley\n\n\"Best Human Rights Book of 2005,\" for \"The Witness: War Crimes and the Promise of Justice in the Hague, \"American Political Science Association\n\n\"Notable Book of the Year for 1999,\" \"New York Times Book Review\" for \"Witnesses from the Grave: The Stories Bones Tell\"\n\n\n\n\nCo-Producer, \"Past Reckoning,\" Saybook Productions.\nPBS, In progress\n\nWriter and Associate Producer, \"Searching for Butch and Sundance\"\nNOVA/WGBH & Channel 4, London, 1992\n\nExecutive Producer, \"Crimes of War.\", 2001\n\nPhotographs have appeared in the \"New York Times\", \"Newsweek\", \"Parade\", \"Miami Herald\", \"The Boston Globe\", \"Science\", \"New Scientist\", \"TV Guide\", \"Visao\", \"The Scientist\", \"Technology Review\", and several reports and books, including in Gerald Posner and John Ware, \"Mengele: A Complete Story\" (New York: McGraw Hill, 1985)\n"}
{"id": "7364325", "url": "https://en.wikipedia.org/wiki?curid=7364325", "title": "Erotic lactation", "text": "Erotic lactation\n\nErotic lactation is sexual arousal by breastfeeding on a woman's breast. Depending on the context, the practice can also be referred to as adult suckling, adult nursing, and adult breastfeeding. Practitioners sometimes refer to themselves as being in an adult nursing relationship (ANR). Two persons in an exclusive relationship can be called a nursing couple.\n\n\"Milk fetishism\" and \"lactophilia\" are medical, diagnostic terms for paraphilias and are used for disorders according to the precise criteria of ICD-10 and DSM-IV.\n\nBreasts, and especially nipples, are highly erogenous zones, for both men and women. Nipple and breast stimulation of women are a near-universal aspect of human sexuality, though nipples in males are not as sexualized. Humans are the only primates whose female members have permanently enlarged breasts after the onset of puberty; the breasts of other primate species are enlarged only during pregnancy and nursing. One hypothesis postulates that the breasts grew as a frontal counterpart to the buttocks as primates became upright to attracting mates, a model first developed in 1967. Other hypotheses include that by chance breasts act as a cushion for infant heads, are a signal of fertility, or elevate the infant's head in breastfeeding to prevent suffocation. Paradoxically, there is even a school that believes that they are an evolutionary flaw, and can actually suffocate a nursing infant.\nThe association of pleasure and nutrition holds true as well for the lips, also erogenous zones, where pleasure may have led to \"kiss feeding\", in which mothers chew food before passing it on to the child.\n\nUnintended milk flow (galactorrhea) is often caused by nipple stimulation and it is possible to reach normal milk production exclusively by suckling on the breast. Nipple stimulation of any sort is noted in reducing the incidence of breast cancer.\n\nSome women lose the ability to be aroused while breastfeeding, and thus would not find lactation with a sexual partner to be erotic. This can be a result of physical reasons (soreness) or psychological reasons (conflicted about her breasts being used other than for an infant).\n\nBecause female breasts and nipples are generally regarded as an important part of sexual activity in most cultures, it is not uncommon that couples may proceed from oral stimulation of the nipples to actual breastfeeding. In lesbian partnerships, mutual breastfeeding has been regarded as a familiar expression of affection and tenderness.\n\nIn its issue of March 13, 2005, the London weekly \"The Sunday Times\" gave a report of a scientific survey (composed of 1690 British men) revealing that in 25 to 33% of all couples, the male partner had suckled his wife's breasts. Regularly, the men gave a genuine emotional need as their motive.\n\nThe breasts have two main roles in human society: nutritive and sexual. Breastfeeding in general is considered by some to be mildly exhibitionary, especially in Western societies (see breastfeeding in public). Breastfeeding mothers have faced legal ramifications for nursing their children into toddlerhood or in public, or for photographing themselves while nursing.\n\nResearcher Nikki Sullivan, in her book \"A Critical Introduction to Queer Theory\", calls erotic lactation a manifestation of \"Queer.\" She defines Queer as an ideology; that is, as a \"sort of vague and indefinable set of practices and (political) positions that has the potential to challenge normative knowledges and identities.\" Drawing on a statement of David Halperin, she continues \"since queer is a positionality rather than an identity in the humanist sense, it is not restricted to gays and lesbians but can be taken up by anyone who feels marginalised as a result of their sexual practices.\" The heteronormative profile of breastfeeding assumes certain norms:\nAdditionally, any relevant third party is assumed to be the mother's significant other and this person is regulated to a supportive role to maximise the breastfeeding mother's success.\n\nThe following are various methods people employ to practice erotic lactation. They are listed according to prevalence, in decreasing order:\n\nErotic lactation between partners or an adult nursing relationship may develop from natural breastfeeding of a baby. During the lactation period the partner starts to suckle on the female breast, and continues after the baby is weaned off. Milk production is continually stimulated and the milk flow continues. According to the book \"Body parts: critical explorations in corporeality\", adult nursing may occur when an \"individual, usually a mother, may choose to continue lactating after weaning a child, so that she avoids the significant physical challenge that inducing lactation can entail.\"\n\nHowever, milk production can be \"artificially\" and intentionally induced in the absence of any pregnancy in the woman. This is called induced lactation, while a woman who has lactated before and restarts is said to relactate. This can be done by regularly sucking on the nipples (several times a day), massaging and squeezing the female breasts, or with additional help from temporary use of milk-inducing drugs, such as the dopamine antagonist Domperidone. In principle—with considerable patience and perseverance—it is possible to induce lactation by sucking on the nipples alone.\n\nIt is not necessary that the woman has ever been pregnant, and she can be well in her post-menopausal period. Once established, lactation adjusts to demand. As long as there is regular breast stimulation, lactation is possible.\n\nThough birth is the beginning of the separation between mother and child, breastfeeding slows this process, making the mother and infant connect physically continually, sometimes for years. As a source of nourishment, the immediacy of this connection is intensified. Breastfeeding has a sexual element as a result of physiological factors. In a study conducted in 1999, approximately 33 to 50 percent of mothers found breast feeding erotic, and among them 25 percent felt guilty because of this. This study corroborated a study in 1949 that found that in a few cases where the arousal was strong enough to induce orgasm, some nursing mothers abandoned breastfeeding altogether. In a 1988 questionnaire on orgasm and pregnancy published in a Dutch magazine for women, when asked \"Did you experience, while breastfeeding, a sensation of sexual excitement?\", 34 percent (or 153 total) answered in the affirmative. An additional 71 percent answered in the affirmative when asked \"Did you experience, while breastfeeding, pleasurable contractions in the uterine region\"\n\nSince the European Middle Ages, a multitude of subliminally erotic, visionary experiences of saints have been passed on in which breastfeeding plays a major role. One prominent example is the Lactatio of Saint Bernard of Clairvaux.\n\nRoman Charity (or \"Caritas Romana\"). is a story of a woman, Pero, who secretly breastfeeds her father, Cimon, after he is incarcerated and sentenced to death by starvation. She is found out by a jailer, but her act of selflessness impresses officials and wins her father's release. The story comes from the Roman writer Valerius Maximus in the years AD 14–AD 37. In about AD 1362 the story was retold by the famous writer Giovanni Boccaccio. After Boccaccio, hundreds or possibly thousands of paintings were created, which tell the story. A variant of this story can be found at the conclusion of John Steinbeck's 1939 novel \"The Grapes of Wrath\". Primarily, the story tells of a conflict. An existing taboo (implied incest and adult breastfeeding of a woman's milk) \"or\" saving a life by breaking the taboo. In this aspect there is no erotic focus to the story.\n\nValerius Maximus tells another story about a woman breastfeeding her mother, which is followed by the very short story of a woman breastfeeding her father. The second, father-daughter story in fact consists of one sentence only. Thirteen hundred years later, Boccaccio retells the (first) mother-daughter story, and does not mention the father-daughter story, and the first is apparently forgotten, leading to nearly all \"caritas romana\" oil paintings and drawings showing only the father-daughter story.\n\nAdult suckling was used to treat ailing adults and treat illnesses including eye disease and pulmonary tuberculosis. The writer Thomas Moffat recorded one physician's use of a wet nurse in a tome first published in 1655.\n\nIn traditional Islamic law, someone who suckles the breast of a woman, who is less than 2 years old (besides many strict rules like that the suckling should be of such quantity that it could be said that the bones of the child were strengthened and the flesh allowed to grow. And if that cannot be ascertained, then if a child suckles for one full day and night, or if it suckles fifteen times to its fill, it will be sufficient), is that woman's child through a foster relationship (the woman is then called \"milk mother\"). However, according to the Jurist Abu's-Su`ud (c.1490–1574), this only applies to sucklings under the age of two and a half years. Also, according to Ayatollah Ali Sistani, a highly praised scholar for the Shia Muslims: \"The child should not have completed two years of his age\". The same latter source states at least 8 conditions that should apply before that child is considered a son/daughter of the feeding woman. A modern Saudi Jurist, in 1983, upheld that if a man suckles from his wife, their marriage is nullified. The query remains a popular one into the 21st century, and has come up in Saudi advice columns. A Sunni cleric Sheik Ezzat Atiya (عزت عطية), President of the Hadith Department of Egypt's al-Azhar University issued a fatwa in 2007 encouraging women to breastfeed their male business colleagues so that the man could become symbolically related to the woman, thereby precluding any sexual relations and the need for both sexes to observe modesty. \"Breast feeding an adult puts an end to the problem of the private meeting\" It was later denounced and declared defamatory to Islam.\n\nIn 2013 a domestic staff agency in China named Xinxinyu was reported to be providing wet nurses for the sick and other adults as well as for newborns. The agency's clients could choose to drink the breast milk directly from the breast or to drink it via a breast pump. The reports caused controversy in China, with one writer describing it as \"adding to China's problem of treating women as consumer goods and the moral degradation of China's rich.\" The agency was forced to suspend its operations by Chinese authorities for a number of reasons, one of which was for missing three years of annual checks.\n\nIn 1903, German philosopher Carl Buttenstedt published his marriage guidebook \"Die Glücksehe – Die Offenbarung im Weibe, eine Naturstudie\" (\"The Marriage of Happiness – The Revelation in the Woman, a study from nature\"), in which he described and recommended the lactational amenorrhea method (LAM) as a form of contraception and natural family planning that also deepens the relationship between wife and husband. He explicitly described erotic lactation as a source of great sexual pleasure for both partners, claiming that this is intended by nature especially on the part of the woman. This particular aspect of his broader general marriage philosophy gained a lot of attention and sparked wide debate. While some welcomed Buttenstedt's advice as inspirational for new ways to improve sexual satisfaction between marriage partners, others warned that this technique could \"pathologically increase sexual sensation of both partners.\" Consequently, the book was banned by the Nazis in 1938.\n\nThe \"Bonyu Bar\" (Mother’s Milk Bar), located in Tokyo’s entertainment and red-light district of Kabukicho, employs nursing women who provide customers with breast milk in a glass for 2,000 yen (about 15 euros) or directly from the nipple for 5000 yen (about 37.50 euros). In the latter case the women can run their fingers through the customers' hair, coo and say their name as they suckle.\n\n\n"}
{"id": "232917", "url": "https://en.wikipedia.org/wiki?curid=232917", "title": "Eyepatch", "text": "Eyepatch\n\nAn eyepatch is a small patch that is worn in front of one eye. It may be a cloth patch attached around the head by an elastic band or by a string, an adhesive bandage, or a plastic device which is clipped to a pair of glasses. It is often worn by people to cover a lost or injured eye, but it also has a therapeutic use in children for the treatment of amblyopia. \"(See orthoptics and vision therapy.)\" Eyepatches used to block light while sleeping are referred to as a sleep mask. Eyepatches associated with pirates are a stereotype originating from fiction.\n\nAn eyepad or eye pad is a soft medical dressing that can be applied over an eye to protect it. It is not necessarily the same as an eyepatch.\n\nIn the years before advanced medicine and surgery, eyepatches were common for people who had lost an eye. They were particularly prevalent among members of dangerous occupations, such as soldiers and sailors who could lose an eye in battle, as well as blacksmiths who used them to cover one eye for protection from sparks while working. While stereotypically associated with pirates, there is no evidence to suggest the historicity of eye patch wearing pirates before several popular novels of the 19th century (see Pirate Eyepatches below).\n\nEye patching is used in the orthoptic management of children at risk of lazy eye (amblyopia), especially strabismic or anisometropic amblyopia. These conditions can cause visual suppression of areas of the dissimilar images by the brain such as to avoid diplopia, resulting in a loss of visual acuity in the suppressed eye and in extreme cases in blindness in an otherwise functional eye. Patching the good eye forces the amblyopic eye to function, thereby causing vision in that eye to be retained. It is important to perform “near activities” (such as reading or handiwork) when patched, thereby exercising active, attentive vision.\n\nA study provided evidence that children treated for amblyopia with eye patching had lower self-perception of social acceptance. To prevent a child from being socially marginalized by his or her peers due to wearing an eye patch, atropine eye drops may be used instead. This induces temporary blurring in the treated eye.\n\nIt has been pointed out that the penalization of one eye by means of patching or atropine drops does not provide the conditions that are necessary in order to develop or improve binocular vision. Recently, efforts have been made to propose alternative treatments of amblyopia that do allow for the improvement of binocular sight, for example using binasal occlusion or partially frosted spectacles in place of any eye patch, using alternating occlusion goggles or using methods of perceptual learning based on video games or virtual reality games for enhancing binocular vision.\n\nA 2014 Cochrane Review sought to determine the effectiveness of occlusion treatment on patients with sensory deprivation amblyopia, however no trials were found eligible to be included in the review. However, it is suggested that good outcomes from occlusion treatment for sensory deprivation amblyopia rely on compliance with the treatment.\n\nTo initially relieve double vision (diplopia) caused by an extra-ocular muscle palsy, an eye care professional may recommend using an eyepatch. This can help to relieve the dizziness, vertigo and nausea that are associated with this form of double vision.\n\nRahmah ibn Jabir al-Jalahimah, once the most popular pirate in the Persian Gulf, was also the first to wear an eyepatch after losing an eye in battle. Although eyepatches have since become stereotypically associated with pirates, the source is unclear, and there is no historical evidence to suggest that their use was for any other reason than protecting and concealing the eye socket after the loss of an eye. Most historical depictions of seamen with eye patches are of ex-sailors, rather than pirates.\n\nMore recent medical texts have often referred to the eye patch as a \"pirate's patch\" and writing in the Minnesota Academy of Sciences Journal in 1934, Charles Sheard of the Mayo foundation, pointed out that by \"wearing a patch (The pirate's patch) over one eye, it will keep the covered eye in a state of readiness and adaptation for night vision\". This technique was explored during WWII by institutes such as the United States Navy.\n\nThe proposal that pirates may have worn an eyepatch so that one eye would be pre-adjusted to below-deck darkness was tested in an episode of Mythbusters in 2007 and found to be plausible, but without any recorded historical precedent.\n\nAircraft pilots used an eye patch, or close one eye to preserve night vision when there was disparity in the light intensity within or outside their aircraft, such as when flying at night over brightly lit cities, so that one eye could look out, and the other would be adjusted for the dim lighting of the cockpit to read unlit instruments and maps. Some military pilots have worn a lead-lined or gold-lined eyepatch, to protect against blindness in both eyes, in the event of a nuclear blast or laser weapon attack.\n\nEyepatches are not currently used by military personnel; modern technology has provided an array of other means to preserve and enhance night vision, including red-light and low-level white lights, and night vision devices.\n\n\n\n\n"}
{"id": "3668047", "url": "https://en.wikipedia.org/wiki?curid=3668047", "title": "Feingold diet", "text": "Feingold diet\n\nThe Feingold diet is an elimination diet initially devised by Dr. Benjamin Feingold following research in the 1970s which appeared to link food additives with hyperactivity; by eliminating these additives and various foods the diet was supposed to alleviate the condition.\n\nPopular in its day, the diet has since been referred to as an \"outmoded treatment\"; there is no good evidence that it is effective, and it is difficult for people to follow.\n\nThe diet was originally based on the elimination of salicylate, artificial food coloring, and artificial flavors; later on in the 1970s, the preservatives BHA, BHT, and (somewhat later) TBHQ were eliminated.\nBesides foods with the eliminated additives, aspirin- or additive-containing drugs and toiletries were to be avoided. Even today, parents are advised to limit their purchases of mouthwash, toothpaste, cough drops, perfume, and various other nonfood products to those published in the Feingold Association's annual \"Foodlist and Shopping Guide\". Some versions of the diet prohibit only artificial food coloring and additives. According to the Royal College of Psychiatrists the diet prohibited a number of foods which contain salicylic acid including apples, cucumbers and tomatoes.\n\nFeingold stressed that the diet must be followed strictly and for an entire lifetime, and that whole families – not just the subject being \"treated\" – must observe the diet's rules.\n\nAlthough the diet had a certain popular appeal, a 1983 meta-analysis found research on it to be of poor quality, and that overall there was no good evidence that it was effective in fulfilling its claims.\n\nIn common with other elimination diets, the Feingold diet can be expensive and boring, and so difficult for people to maintain.\n\nIn general, there is no evidence to support broad claims that food coloring causes food intolerance and ADHD-like behavior in children. It is possible that certain food coloring may act as a trigger in those who are genetically predisposed, but the evidence is weak.\n\nFor decades, the Feingold Program required a significant change in family lifestyle because families were limited to a narrow selection of foods. Such foods were sometimes expensive or had to be prepared \"from scratch,\" greatly increasing the amount of time and effort a family must put into preparing a meal. As more and more foods without the potentially offending additives are being produced and available in neighborhood supermarkets, this is much less a problem.\n\nWhile some fruits and a few vegetables are eliminated in the first weeks of the Program, they are replaced by others. Often, some or all of these items can be returned to the diet, once the level of tolerance is determined.\n\nFeingold was Chief of Pediatrics at Cedars of Lebanon Hospital in Los Angeles, CA, until 1951,\nwhen he became Chief of Allergy at Kaiser-Permanente Medical Center in San Francisco.\nHe\ncontinued his work with children and adults with hyperactivity and allergy until his death at the age of 82, in 1982.\n\nSince the 1940s, researchers worldwide had discussed cross-reactions of aspirin (a common salicylate) and tartrazine (FD&C Yellow #5).\nDr. Stephen Lockey at the Mayo Clinic and later Feingold at Kaiser, hypothesized that eliminating both salicylates and synthetic food additives from patients' diets not only eliminated allergic-type reactions such as asthma, eczema and hives, but also induced behavioral changes in some of their patients.\n\nFeingold presented his findings at the annual conference of the American Medical Association in June 1973. This led to a controlled double-blind crossover study published in the August 1976 issue of \"Pediatrics\".\n\nA two-week-long conference was arranged in January 1975, in Glen Cove, Long Island. There, the Nutrition Foundation attendees created what they called the National Advisory Committee. The committee widely published its preliminary report concluding that \"no controlled studies have demonstrated that hyperkinesis is related to the ingestion of food additives.\"\n\n"}
{"id": "5849200", "url": "https://en.wikipedia.org/wiki?curid=5849200", "title": "Frank Cotton", "text": "Frank Cotton\n\nFrank Stanley Cotton (30 April 1890 – 23 August 1955) was an Australian lecturer in physiology, specialising in the study of the effects of physical strain on the human body.\n\nFrank Stanley Cotton was born on 30 April 1890 at Camperdown, Sydney, New South Wales. He was the son of Australian politician Francis Cotton (1857–1942), and brother of Shackleton expeditioner and geology professor, Leo Arthur Cotton (1883–1963). Pioneer art photographer Olive Cotton was his niece. He attended Sydney Boys High School in 1904–08. In 1917, Frank married Catherine Drummond Smith, a geology demonstrator who taught at the University of Sydney.\n\nIn 1940, whilst at the University of Sydney, Professor Cotton invented the \"Cotton aerodynamic anti-G flying suit\" (G-suit), which prevented pilots from blacking out when making high speed turns or pulling out of a dive. This was used extensively by pilots in the Allied air forces during World War II.\n\nCotton was also responsible for the ergometer, a machine to test the athletic potential of sportsmen and women. Cotton claimed through this machine to have discovered the swimmers Jon Henricks and Judy-Joy Davies. The Australian swimming coach, Forbes Carlile, began his career as an assistant to Cotton.\n\nOn 23 August 1955, Frank Cotton died at Hornsby, New South Wales.\n\n\n"}
{"id": "3284724", "url": "https://en.wikipedia.org/wiki?curid=3284724", "title": "Gargling", "text": "Gargling\n\nGargling (same root as 'gurgle') is the act of bubbling liquid in the mouth. Vibration caused by the muscles in the throat and back of the mouth cause the liquid to bubble and flurry around inside the mouth cavity.\n\nA traditional home remedy of gargling warm saltwater is sometimes recommended to soothe a sore throat. \n\nA study in Japan has shown that gargling water a few times a day will lower the chance of upper respiratory infections such as colds, though some medical authorities are skeptical.\n\n "}
{"id": "7482808", "url": "https://en.wikipedia.org/wiki?curid=7482808", "title": "Good Clinical Practice Directive", "text": "Good Clinical Practice Directive\n\nThe Good Clinical Practice Directive (Directive 2005/28/EC of 8 April 2005 of the European Parliament and of the Council) lays down principles and detailed guidelines for good clinical practice as regards conducting clinical trials of medicinal products for human use, as well as the requirements for authorisation of the manufacturing or importation of such products.\n\nThe directive deals with the following items:\n\n\n\n"}
{"id": "20728871", "url": "https://en.wikipedia.org/wiki?curid=20728871", "title": "Group for the Advancement of Psychiatry", "text": "Group for the Advancement of Psychiatry\n\nThe Group for the Advancement of Psychiatry (GAP) is an American professional organization of psychiatrists dedicated to shaping psychiatric thinking, public programs and clinical practice in mental health. Its 29 committees meet semi-annually and choose their own topics for exploration. They explore issues and ideas on the frontiers of psychiatry and in applying psychiatric insights into general medical, social, and interpersonal problems.\n\nGAP was part of a larger move toward professionalization of the field. GAP was founded in May 1946 by a group of young psychiatrists who had served in World War II. They returned to the U.S. to find an inadequate system of civilian care and were impatient with the traditionalism of the American Psychiatric Association (which had originally been founded as an association of asylum superintendent). \nGAP was formed under the leadership of \nDr. William C. Menninger \nand the \"young turks\" in American psychiatry who were eager to professionalize the field. Menninger wrote:\n\nThe organization of GAP was not a revolution. With the deepest sincerity, the founding group was seeking a way in which American psychiatry could give more forceful leadership, both medically and socially. Although the name may sound presumptuous, it was chosen because of the sense of great urgency that psychiatry should advance, and the belief that by hard work, and teamwork, we could help it do so. Those early years of GAP were marked by the feeling on the part of its membership that much needed to be done, and quickly.\n\nGAP's first published report (by the Committee on Therapy) was on the \"promiscuous and indiscriminate use of electro-shock therapy.\" \nGAP's formulated policy to discuss controversial psychosocial issues was announced in 1950, in the Committee on Social Issues' Report, \"The Social Responsibility of Psychiatry, A Statement of Orientation\". In that Report, the Committee noted that two factors had been influential in causing diverse social problems in psychiatry: the role of prejudice in determining attitudes towards social problems and the sparse knowledge about the relationship between society and personality. In this pioneering document, the Committee on Social Issues emphasized the social responsibility of psychiatry. It made a number of suggestions for broadening the conceptual framework of psychiatry to include: \"redefinition of the concept of mental illness, emphasizing those dynamic principles which pertain to the person's interaction with society ... examination of the social factors which contribute to the causation of mental illness and also influence its course and outcome ... consideration of the specific group psychological phenomena which are relevant, in a positive sense, to community mental health ... the development of criteria for social action, relevant to the promotion of individual and community mental health.\" By 1955 the group was advocating an \"objective critical attitude should orient the field.\n\nGAP continued to produce position statements on relevant and controversial psychiatric issues such as abortion, \ndrug use, \nsex crimes,\nschool desegregation,\nloyalty oaths,\nnuclear energy,\nand euthanasia.\nIts \"Report on homosexuality with particular emphasis on this problem in governmental agencies\" (1955) criticized \"witch hunts\" against homosexuals working in the U.S. government.\nand the purging of homosexuals from the government.\nGAP reports were concise, published soon after they were written and widely respected and influential.\n\nGAP is composed of over 200 leaders in psychiatry who meet twice a year to debate and think through pertinent issues in psychiatry. \nIn over 50 years, GAP has shared Presidents with other national psychiatric organizations including: The American Psychiatric Association, the American College of Psychiatrists, and the American Academy of Child and Adolescent Psychiatry. GAP has researchers recognized nationally and internationally in the areas of addiction, geriatrics, child and adolescent psychiatry, terrorism and academics.\n\n"}
{"id": "42700586", "url": "https://en.wikipedia.org/wiki?curid=42700586", "title": "Gulf War Health Research Reform Act of 2014", "text": "Gulf War Health Research Reform Act of 2014\n\nThe Gulf War Health Research Reform Act of 2014 () is a bill that would alter the relationship between the Research Advisory Committee on Gulf War Illnesses (RAC) and the United States Department of Veterans Affairs (VA). The bill would make the RAC an independent organization within the VA, require that a majority of the RAC's members be appointed by Congress instead of the VA, and state that the RAC can release its reports without needing prior approval from the Secretary of Veterans Affairs. The RAC is responsible for investigating Gulf War syndrome, a chronic multisymptom disorder affecting returning military veterans and civilian workers of the Gulf War.\n\nThe bill was introduced into the United States House of Representatives during the 113th United States Congress.\n\nGulf War syndrome (GWS), also known as Gulf War illness (GWI), is a chronic multisymptom disorder affecting returning military veterans and civilian workers of the Gulf War. A wide range of acute and chronic symptoms have been linked to it, including fatigue, muscle pain, cognitive problems, rashes and diarrhea. Approximately 250,000 of the 697,000 U.S. veterans who served in the 1991 Gulf War are afflicted with enduring chronic multi-symptom illness, a condition with serious consequences. From 1995 to 2005, the health of combat veterans worsened in comparison with nondeployed veterans, with the onset of more new chronic diseases, functional impairment, repeated clinic visits and hospitalizations, chronic fatigue syndrome-like illness, posttraumatic stress disorder, and greater persistence of adverse health incidents. According to a report by the Iraq and Afghanistan Veterans of America, veterans of Iraq and Afghanistan may also suffer from the syndrome.\n\nSuggested causes have included depleted uranium, sarin gas, smoke from burning oil wells, vaccinations, combat stress and psychological factors.\n\nIn the year prior to the consideration of this bill, the VA and the RAC were at odds with one another. The VA replaced all but one of the members of the RAC, removed some of their supervisory tasks, tried to influence the board to decide that stress, rather than biology was the cause of Gulf War syndrome, and told the RAC that it could not publish reports without permission. The RAC was originally created in 1997, after Congress decided that the VA's research into the issue was flawed, and focused on psychological causes, while mostly ignoring biological ones.\n\nThe bill would make the Research Advisory Committee on Gulf War Illnesses (RAC) an independent committee within the VA.\n\nThe bill would require the majority of RAC members be appointed by the chairmen and ranking members of the United States House Committee on Veterans' Affairs and the United States Senate Committee on Veterans' Affairs. Three of the members would be required to be veterans, while a minimum of eight members \"must be scientists of physicians, with expertise in areas like epidemiology, immunology, neurology and toxicology.\"\n\nThe bill would also mandate that the condition be called \"Gulf War Illness\" instead of \"Gulf War Syndrome.\"\n\nThe bill also requests that the VA look at animal studies when investigating toxic exposure.\n\nThe bill states that \"reports, recommendations, publications, and other documents of the (RAC) committee shall not be subject to review or approval by the Secretary of Veterans Affairs.\"\n\nThe Gulf War Health Research Reform Act of 2014 was introduced into the United States House of Representatives on March 14, 2014 by Rep. Mike Coffman (R, CO-6). The bill was referred to the United States House Committee on Veterans' Affairs, the United States House Veterans' Affairs Subcommittee on Health, and the United States House Veterans' Affairs Subcommittee on Oversight and Investigations.\n\nAccording to Rep. Coffman, who sponsored the bill, the legislation is the result of an investigation by the House Veterans' Affairs Subcommittee on Oversight and Investigations which determined that the Department of Veterans Affairs was \"exercising too much control over the Research Advisory Committee on Gulf War Illnesses (RAC)\" and was \"denying their ability to effectively and independently carry out its Congressionally mandated role to improve the lives of Gulf War Veterans.\" The investigation found the misappropriation of funds, the placement of biased members in the RAC, and restrictions on RAC reports to keep them from circulating.\n\nRep. Ann Kirkpatrick, who supported the bill, said that it was our job to ensure \"the VA conducts objective research on chronic illnesses experienced by Gulf War veterans, in an effort to find treatments that can make a difference in their quality of life.\"\n\n\n"}
{"id": "31720936", "url": "https://en.wikipedia.org/wiki?curid=31720936", "title": "Hand Held (film)", "text": "Hand Held (film)\n\nHand Held is a 2010 documentary feature film about photojournalist Mike Carroll (\"Boston Globe\", \"People\", \"Rolling Stone\"), one of the first photographers to travel to Romania after the fall of the communist regime of Nicolae Ceaușescu in 1989. When he arrived, he walked into one of the most horrific scenes of the 20th century. His photographs and heart wrenching stories of the pediatric AIDS epidemic in Romania ran in the \"Boston Globe \"and\" New York Times\" and opened the eyes of the western world to the plight of Romanian children. The film chronicles Carroll's twenty-year journey to bring aid to children in a country he hardly knew, through the organization he founded, Romanian Children's Relief (RCR).\n\nThe film is produced and directed by Academy Award nominee Don Hahn, the man famous for producing \"Beauty and the Beast\"\" \"and\" The Lion King\" for the Walt Disney Animation Studios. After a charity premiere at WGBH in Boston, the film was selected for participation in the Heartland Film Festival, and has been the official selection of the Boulder International Film Festival, Newport Beach Film Festival, and the Rhode Island International Film Festival. \"Hand Held\"<nowiki>'s European premiere was in </nowiki>Bucharest in late 2010. The director Don Hahn and subject Mike Carroll were introduced by United States Ambassador Mark Gitenstein who was in attendance at the premiere.\n\n"}
{"id": "12975191", "url": "https://en.wikipedia.org/wiki?curid=12975191", "title": "High-stakes testing", "text": "High-stakes testing\n\nA high-stakes test is a test with important consequences for the test taker. Passing has important benefits, such as a high school diploma, a scholarship, or a license to practice a profession. Failing has important disadvantages, such as being forced to take remedial classes until the test can be passed, not being allowed to drive a car, or difficulty finding employment.\n\nThe use and misuse of high-stakes tests are a controversial topic in public education, especially in the United States and U.K. where they have become especially popular in recent years, used not only to assess students but in attempts to increase teacher accountability.\n\nIn common usage, a high-stakes test is any test that has major consequences or is the basis of a major decision.\n\nUnder a more precise definition, a high-stakes test is any test that:\n\nHigh-stakes testing is not synonymous with high-pressure testing. An American high school student might feel pressure to perform well on the SAT-I college aptitude exam. However, SAT scores do not directly determine admission to any college or university, and there is no clear line drawn between those who pass and those who fail, so it is not formally considered a high-stakes test. On the other hand, because the SAT-I scores are given significant weight in the admissions process at some schools, many people believe that it has consequences for doing well or poorly and is therefore a high-stakes test under the simpler, common definition.\n\nHigh stakes are not a characteristic of the test itself, but rather of the consequences placed on the outcome. For example, no matter what test is used—written multiple choice, oral examination, performance test—a medical licensing test must be passed to practice medicine.\n\nThe perception of the stakes may vary. For example, college students who wish to skip an introductory-level course are often given exams to see whether they have already mastered the material and can be passed to the next level. Passing the exam can reduce tuition costs and time spent at university. A student who is anxious to have these benefits may consider the test to be a high-stakes exam. Another student, who places no importance on the outcome, so long as he is placed in a class that is appropriate to his skill level, may consider the same exam to be a low-stakes test.\n\nThe phrase \"high stakes\" is derived directly from a gambling term. In gambling, a \"stake\" is the quantity of money or other goods that is risked on the outcome of some specific event. A high-stakes game is one in which, in the player's personal opinion, a large quantity of money is being risked. The term is meant to imply that implementing such a system introduces uncertainty and potential losses for test takers, who must pass the exam to \"win,\" instead of being able to obtain the goal through other means.\n\nExamples of high-stakes tests and their \"stakes\" include:\n\nA high-stakes system may be intended to benefit people other than the test-taker. For professional certification and licensure examinations, the purpose of the test is to protect the general public from incompetent practitioners. The individual stakes of the medical student and the medical school are, hopefully, balanced against the social stakes of possibly allowing an incompetent doctor to practice medicine.\n\nA test may be \"high-stakes\" based on consequences for others beyond the individual test-taker. For example, an individual medical student who fails a licensing exam will not be able to practice his or her profession. However, if enough students at the same school fail the exam, then the school's reputation and accreditation may be in jeopardy. Similarly, testing under the U.S.'s No Child Left Behind Act has no direct negative consequences for failing students, but potentially serious consequences for their schools, including loss of accreditation, funding, teacher pay, teacher employment, or changes to the school's management. The stakes are therefore high for the school, but low for the individual test-takers.\n\nAny form of assessment can be used as a high-stakes test. Many times, an inexpensive multiple-choice test is chosen for convenience. A high-stakes assessment may also involve answering open-ended questions or a practical, hands-on section. For example, a typical high-stakes licensing exam for a medical nurse determines whether the nurse can insert an I.V. line by watching the nurse actually do this task. These assessments are called \"authentic assessments\" or \"performance tests\".\n\nSome high-stakes tests may be standardized tests (in which all examinees take the same test under reasonably equal conditions), with the expectation that standardization affords all examinees a fair and equal opportunity to pass. Some high-stakes tests are non-standardized, such as a theater audition.\n\nAs with other tests, high-stakes tests may be criterion-referenced or norm-referenced. For example, a written driver's license examination typically is criterion-referenced, with an unlimited number of potential drivers able to pass if they correctly answer a certain percentage of questions. On the other hand, essay portions of some bar exams are often norm-referenced, with the worst essays failed and the best essays passed, without regard for the overall quality of the essays.\n\nThe \"clear line\" between passing and failing on an exam may be achieved through use of a cut score: for example, test takers correctly answering 75% or more of the questions pass the test; test takers correctly answering 74% or fewer fail. In large-scale high-stakes testing, rigorous and expensive standard-setting studies may be employed to determine the ideal cut score or to keep the test results consistent between groups taking the test at different times.\n\nHigh-stakes tests, despite their extensive usage for determination of academic and non-academic proficiency, are subject to criticism for various reasons. Example concerns include the following: \n\n"}
{"id": "14670762", "url": "https://en.wikipedia.org/wiki?curid=14670762", "title": "Hugo Gunckel Lüer", "text": "Hugo Gunckel Lüer\n\nHugo Gunckel Lüer (August 10, 1901 – July 17, 1997) was a Chilean pharmacist, botanist, and university professor.\n\nGunckel is the ICBN author citation corresponding to Hugo Gunckel.\n\nGunckel was born in Valdivia. His primary and secondary studies were at the Colegio Alemán (founded by Carlos Anwandter) and in the lyceum of Hombres, enterring the Universidad de Concepción in 1921, from which he got a degree in pharmacy.\n\nAs assistant of botany to the Prof. Alcibíades Santa Cruz, he demonstrated his interest in nature, stimulated by his parents who offered him frequent excursions that allowed him to observe nature, plants, and their development. Later, the study of plants, their properties, and life conditions became a passion.\n\nAfter graduating he worked as a professional in Talca, in the pharmacy of Guillermo Kuschel, the distinguished trade union and industrial director, one of the founding partners of the Laboratorio Geka. Later Gunckel returned to Valdivia to serve as the pharmaceutical head of the Railroad Zone IV, work which was later suppressed.\n\nGunckel moved to Corral, where he started a pharmacy, but the near-virgin landscape that surrounded this port, rich in forests, ferns, and grasses, made him a collector of plant specimens. In 1940 he moved to Temuco, where he became the director of the Museo Araucano (today the Museo Regional de la Araucanía). In 1943 he was elected the first president of the Temuco Regional Council of the College of Pharmacists of Chile (\"Consejo Regional en Temuco del Colegio de Farmacéuticos de Chile\"), which was led by Victor M. Cereceda. In 1946 he became the president of the School of Forestry Engineers (\"Escuela de Ingenieros Forestales\"), the first establishment of the specialty created in Chile. However, he continued studying, and wrote articles for the daily \"Austral\", other newspapers in the region, and scientific journals. He was founding member of the Academia de Ciencias Naturales, which he was president of later for 12 years, and for which he wrote articles they published in their journal over the course of 20 years. With Hans Niemeyer, he edited the \"Revista Universitaria-Universidad Católica\" or \"Anales de la Academia de Ciencias Naturales.\"\n\nOn May 1, 1950 he moved to Santiago to fill the botany chair in the Facultad de Farmacia, which was directed by the professor Juan Ibáñez, who performed duties for UNESCO and thus was frequently travelling. His mission was the formation of the herbarium of the School of Chemistry and Pharmacy. At the same time, Gunckel taught classes at the Instituto Pedagógico de la Universidad de Chile, today the Universidad Metropolitana de Ciencias de la Educación, where he continued to serve until he retired in 1968. This body's journal \"Academia\" published some of his works related to botany and history (Gunckel 1982). He became an honorary member of the Sociedad Chilena de Historia y Geografía and a member of the Academia Chilena de la Lengua.\n\nHugo Gunckel was also the founder and subdirector of the Third Company of Firefighters of Corral and served as mayor of the community.\n\nHe died in Santiago de Chile at the age of 96.\n\n\n\n\n"}
{"id": "25190904", "url": "https://en.wikipedia.org/wiki?curid=25190904", "title": "Hunger", "text": "Hunger\n\nHunger and satiety are sensations. Hunger represents the physiological need to eat food. Satiety is the absence of hunger; it is the sensation of feeling full.\n\nAppetite is another sensation experienced with eating; it is the desire to eat food. There are several theories about how the feeling of hunger arises. A healthy, well-nourished individual can survive for weeks without food intake, with claims ranging from three to ten weeks. The sensation of hunger typically manifests after only a few hours without eating and is generally considered to be unpleasant. Satiety occurs between 5 and 20 minutes after eating.\n\nHunger is also the most commonly used term to describe the condition of people who suffer from a chronic lack of sufficient food and constantly or frequently experience the sensation of hunger, and is discussed in malnutrition.\n\nWhen hunger contractions start to occur in the stomach, they are informally referred to as hunger pangs. Hunger pangs usually do not begin until 12 to 24 hours after the last ingestion of food. A single hunger contraction lasts about 30 seconds, and pangs continue for around 30 to 45 minutes, then hunger subsides for around 30 to 150 minutes. Individual contractions are separated at first, but are almost continuous after a certain amount of time. Emotional states (anger, joy etc.) may inhibit hunger contractions. Levels of hunger are increased by lower blood sugar levels, and are higher in diabetics. They reach their greatest intensity in three to four days and may weaken in the succeeding days, although research suggests that hunger never disappears. Hunger contractions are most intense in young, healthy people who have high degrees of gastrointestinal tonus. Periods between contractions increase with old age. \n\nShort-term regulation of hunger and food intake involves neural signals from the GI tract, blood levels of nutrients, GI tract hormones, and psychological factors.\n\nOne method that the brain uses to evaluate the contents of the gut is through vagal nerve fibers that carry signals between the brain and the gastrointestinal tract (GI tract). Stretch receptors work to inhibit appetite upon distention of the GI tract by sending signals along the vagus nerve afferent pathway and inhibiting the hunger center.\n\nBlood levels of glucose, amino acids, and fatty acids provide a constant flow of information to the brain that may be linked to regulating hunger and energy intake. Nutrient signals that indicate fullness, and therefore inhibit hunger include rising blood glucose levels, elevated blood levels of amino acids, and blood concentrations of fatty acids.\n\nThe hormones insulin and cholecystokinin (CCK) are released from the GI tract during food absorption and act to suppress feeling of hunger. CCK is key in suppressing hunger because of its role in inhibiting neuropeptide Y. Glucagon and epinephrine levels rise during fasting and stimulate hunger. Ghrelin, a hormone produced by the stomach, is a hunger stimulant.\n\nTwo psychological processes appear to be involved in regulating short-term food intake: liking and wanting. Liking refers to the palatability or taste of the food, which is reduced by repeated consumption. Wanting is the motivation to consume the food, which is also reduced by repeated consumption of a food and may be due to change in memory-related processes. Wanting can be triggered by a variety of psychological processes. Thoughts of a food may intrude on consciousness and be elaborated on, for instance, as when one sees a commercial or smells a desirable food.\n\nThe regulation of hunger and appetite (the appestat) has been the subject of much research; breakthroughs included the discovery, in 1994, of leptin, a hormone produced by the adipose tissue that appeared to provide negative feedback. Leptin is a peptide hormone that affects homeostasis and immune responses. Lowering food intake can lower leptin levels in the body, while increasing the intake of food can raise leptin levels. Later studies showed that appetite regulation is an immensely complex process involving the gastrointestinal tract, many hormones, and both the central and autonomic nervous systems. The circulating gut hormones that regulate many pathways in the body can either stimulate or suppress appetite. For example, ghrelin stimulates appetite, whereas cholecystokinin and glucagon-like peptide-1 (GLP-1) suppress appetite.\n\nThe arcuate nucleus of the hypothalamus, a part of the brain, is the main regulatory organ for the human appetite. Many brain neurotransmitters affect appetite, especially dopamine and serotonin. Dopamine acts primarily through the reward centers of the brain, whereas serotonin primarily acts through effects on neuropeptide Y (NPY)/agouti-related peptide (AgRP) [stimulate appetite] and proopiomelanocortin (POMC) [induce satiety] neurons located in the arcuate nucleus. Similarly, the hormones leptin and insulin suppress appetite through effects on AgRP and POMC neurons.\n\nHypothalamocortical and hypothalamolimbic projections contribute to the awareness of hunger, and the somatic processes controlled by the hypothalamus include vagal tone (the activity of the parasympathetic autonomic nervous system), stimulation of the thyroid (thyroxine regulates the metabolic rate), the hypothalamic-pituitary-adrenal axis and a large number of other mechanisms. Opioid receptor-related processes in the nucleus accumbens and ventral pallidum affect the palatability of foods.\n\nThe nucleus accumbens (NAc) is the area of the brain that coordinates neurotransmitter, opioid and endocannabinoid signals to control feeding behaviour. The few important signalling molecules inside the NAc shell modulate the motivation to eat and the affective reactions for food. These molecules include the DA, Ach, opioids and cannabinoids and their action receptors inside the brain, DA, muscarinic and MOR and CB1 receptors respectively.\n\nThe hypothalamus senses external stimuli mainly through a number of hormones such as leptin, ghrelin, PYY 3-36, orexin and cholecystokinin; all modify the hypothalamic response. They are produced by the digestive tract and by adipose tissue (leptin). Systemic mediators, such as tumor necrosis factor-alpha (TNFα), interleukins 1 and 6 and corticotropin-releasing hormone (CRH) influence appetite negatively; this mechanism explains why ill people often eat less.\n\nLeptin, a hormone secreted exclusively by adipose cells in response to an increase in body fat mass, is an important component in the regulation of long term hunger and food intake. Leptin serves as the brain's indicator of the body's total energy stores. When leptin levels rise in the bloodstream they bind to receptors in ARC. The functions of leptin are to:\n\nThough rising blood levels of leptin do promote weight loss to some extent, its main role is to protect the body against weight loss in times of nutritional deprivation. Other factors also have been shown to effect long-term hunger and food intake regulation including insulin.\n\nIn addition, the biological clock (which is regulated by the hypothalamus) stimulates hunger. Processes from other cerebral loci, such as from the limbic system and the cerebral cortex, project on the hypothalamus and modify appetite. This explains why in clinical depression and stress, energy intake can change quite drastically.\n\nThe set-point theories of hunger and eating are a group of theories developed in the 1940s and 1950s that operate under the assumption that hunger is the result of an energy deficit and that eating is a means by which energy resources are returned to their optimal level, or energy set-point. According to this assumption, a person's energy resources are thought to be at or near their set-point soon after eating, and are thought to decline after that. Once the person's energy levels fall below a certain threshold, the sensation of hunger is experienced, which is the body's way of motivating the person to eat again. The set-point assumption is a negative feedback mechanism. Two popular set-point theories include the glucostatic set-point theory and the lipostatic set-point theory.\n\nThe set-point theories of hunger and eating present a number of weaknesses. \n\n\nThe positive-incentive perspective is an umbrella term for a set of theories presented as an alternative to the set-point theories of hunger and eating. The central assertion to the positive-incentive perspective is the idea that humans and other animals are not normally motivated to eat by energy deficits, but are instead motivated to eat by the anticipated pleasure of eating, or the positive-incentive value. According to this perspective, eating is controlled in much the same way as sexual behavior. Humans engage in sexual behavior, not because of an internal deficit, but instead because they have evolved to crave it. Similarly, the evolutionary pressures of unexpected food shortages have shaped humans and all other warm blooded animals to take advantage of food when it is present. It is the presence of good food, or the mere anticipation of it that makes one hungry.\n\nPrior to consuming a meal, the body's energy reserves are in reasonable homeostatic balance. However, when a meal is consumed, there is a homeostasis-disturbing influx of fuels into the bloodstream. When the usual mealtime approaches, the body takes steps to soften the impact of the homeostasis-disturbing influx of fuels by releasing insulin into the blood, and lowering the blood glucose levels. It is this lowering of blood glucose levels that causes premeal hunger, and not necessarily an energy deficit.\n\nA food craving is an intense desire to consume a specific food, as opposed to general hunger. Similarly, thirst is the craving for water.\n\n"}
{"id": "43749048", "url": "https://en.wikipedia.org/wiki?curid=43749048", "title": "Ifmad", "text": "Ifmad\n\nThe International Forum on Mood and Anxiety Disorders or IFMAD is a professional organisation dedicated to raising awareness of the latest international thinking and innovations in mood and anxiety disorders and promoting the exchange of ideas across the global psychiatric community.\n\nIFMAD was founded in 2000 by and Professor Stuart Montgomery, supported by a scientific committee consisting of leading figures in mood and anxiety disorders from around the world.\n\nIFMAD organises a yearly congress to promote debate and highlight the latest research in mood and anxiety disorders. The annual event has become an important forum for the exchange of ideas and a key part of the congress calendar.\n\n"}
{"id": "19552284", "url": "https://en.wikipedia.org/wiki?curid=19552284", "title": "Immunization Alliance", "text": "Immunization Alliance\n\nThe Immunization Alliance is an American vaccine advocacy consortium, assembled under auspices of the American Academy of Pediatrics (AAP) in May 2008. The Immunization Alliance has called for a governmental information campaign, ongoing research into vaccine safety and efficacy, balanced media coverage, and restoration of confidence among parents due to the vaccine controversy and the related controversies in autism.\n\nCiting the largest measles outbreak in the US since 1966 (130 cases in fifteen states), Paul Offit, a member of the Alliance, asserted that this re-emergence of a common childhood disease was a warning about the dangers of \"what can happen when parents are misinformed about vaccine safety. \"We do not want to become a nation of people who are vulnerable to diseases that are deadly or that can have serious complications, especially if those diseases can be prevented,\" said Renee Jenkins, president of the AAP.\nThe Immunization Alliance was described in the July, 2008, issue of \"Pediatrics\" (published by the AAP), announcing the organization's debut and detailing its plans for improving vaccine schedule adherence and combating declining immunization rates.\n\nOver twenty organizations, representing a broad swath of the American medical community, have banded together with the AAP to form the Immunization Alliance, including:\n\n\n"}
{"id": "31058366", "url": "https://en.wikipedia.org/wiki?curid=31058366", "title": "Leona Baumgartner", "text": "Leona Baumgartner\n\nLeona Baumgartner (August 18, 1902 – January 15, 1991) was an American physician. She was the first woman to serve as Commissioner of New York City’s Department of Health (1954–1962). She was a strong advocate of health education and a pioneer in promoting health services among New York’s immigrant and poverty-stricken population.\n\nLeona Baumgartner was born in 1902 to Olga and William Baumgartner. She earned her B.A in Bacteriology and M.A in Immunology at the University of Kansas where her father was a professor of zoology. She was a member of the Kansas Alpha chapter of Pi Beta Phi, and was the 1933-34 winner of the Pi Beta Phi Graduate Fellowship. Moving onto Yale University, Baumgartner received her Ph.D. in Public Health in 1934 and received her M.D. the same year.\n\nFrom 1934–1936, she interned in Pediatrics at New York City Hospital. It was during this time, in depression-era New York, that Baumgartner began making home visits in the city’s poorest areas. In 1937, she joined New York’s Department of Health as a medical instructor in Child and School Hygiene. In 1939, Baumgartner was promoted to district health officer, where she managed a number of health services including school health programs, parenting classes and clinics on venereal disease.\n\nIn 1954, Baumgartner was appointed Commissioner of Health of New York City. In addition to revising the city’s health code, she also implemented routine inspections of the city’s many restaurant kitchens, slaughterhouses and day-care facilities. She was instrumental in garnering funding for public health research and a premature childcare facility. Following in the work of Sara Josephine Baker, Baumgartner sought to increase public knowledge of health issues through a series of radio and television broadcasts.\n\nOn October 28, 1956, she assisted Dr. Harold Fuerst in the inoculation of the then 21 year old Elvis Presley, an event witnessed by the entire world press with bureaus in New York City, carried live on all three networks and which resulted in the exponential increase in the polio immunization of all Americans from 0.6%, the prevailing rate on the previous day, to 80% by April 1957.\n\nAs the years went by, maternal and child health remained a constant concern throughout her career and informed her decision to promote family planning practices and birth control. In 1962, she was appointed by President John F. Kennedy to head the Office of Technical Cooperation and Research at the United States Agency for International Development. Under the Johnson administration she fought to \noverturn policies that prevented the inclusion of birth control in the agency’s health programs. She is credited with convincing President Lyndon B. Johnson to reverse policy on funding for international programs that provided birth control.\n\nIn 1965, Baumgartner accepted a position as a visiting professor at Harvard Medical School. She remained at this post until her retirement in 1972. During the same years, she also served as Executive Director of the Medical Care and Education Foundation.\n\nThroughout her career, Baumgartner was dedicated to health education as the cornerstone to the creation of a healthy community, beginning with her work as district health officer in planning classes and clinics. Baumgartner was also an early advocate of the vaccine developed by Jonas Salk as a method of immunization against polio and the fluoridation of water as a bulwark against dental disease.\n\nBaumgartner was elected a Fellow of the American Academy of Arts and Sciences in 1969. She was awarded the Public Welfare Medal from the National Academy of Sciences in 1977. Her other awards include the Sedgwick Medal, the Albert Lasker Public Service Award, the Elizabeth Blackwell Award, and the Samuel J. Crumbine Award. In 1942, Baumgartner married Nathaniel Elias, a chemical engineer. The marriage lasted until Elias’ death in 1964; in 1970, Baumgartner married Dr. Alexander Langmuir who survived her death in 1991 from polycythemia by two years.\n\nhttps://40.media.tumblr.com/1db34e5434271550bb562c335f4e63a7/tumblr_ngpuc712N61rjdad7o1_500.jpg\n\n"}
{"id": "1106055", "url": "https://en.wikipedia.org/wiki?curid=1106055", "title": "Linear no-threshold model", "text": "Linear no-threshold model\n\nThe linear no-threshold model (LNT) is a model used in radiation protection to quantify radiation exposure and set regulatory limits. It is most frequently used to calculate the probability of radiation induced cancer at both high doses where epidemiology studies support its application but, controversially, it likewise finds applications in calculating the effects of low doses, a dose region that is fraught with much less statistical confidence in its predictive power but that nonetheless has resulted in major personal and policy decisions in regards to public health. The model assumes that the long-term, biological damage caused by ionizing radiation (essentially the cancer risk) is directly proportional to the dose. This allows the summation by dosimeters of all radiation exposure, without taking into consideration dose levels or dose rates. In other words, radiation is always considered harmful with no safety threshold, and the sum of several very small exposures are considered to have the same effect as one larger exposure (response linearity).\n\nOne of the organizations for establishing recommendations on radiation protection guidelines internationally, the UNSCEAR, recommended in 2014 policies that do not agree with the Linear No-Threshold model at exposure levels below background levels of radiation to the UN General Assembly from the Fifty-Ninth Session of the Committee. Its recommendation states that \"the Scientific Committee does not recommend multiplying very low doses by large numbers of individuals to estimate numbers of radiation-induced health effects within a population exposed to incremental doses at levels equivalent to or lower than natural background levels.\" This is a reversal from previous recommendations by the same organization.\n\nThere are three active (2016) challenges to the LNT model currently being considered by the US Nuclear Regulatory Commission. One was filed by Nuclear Medicine Professor Carol Marcus of UCLA, who calls the LNT model scientific \"baloney\".\n\nWhether the model describes the reality for small-dose exposures is disputed. It opposes two competing schools of thought: the threshold model, which assumes that very small exposures are harmless, and the radiation hormesis model, which claims that radiation at very small doses can be beneficial. Because the current data are inconclusive, scientists disagree on which model should be used. Pending any definitive answer to these questions and the precautionary principle, the model is sometimes used to quantify the cancerous effect of collective doses of low-level radioactive contaminations, even though it estimates a positive number of excess deaths at levels that would have had zero deaths, or saved lives, in the two other models. Such practice has been condemned by the International Commission on Radiological Protection.\n\nThe LNT model is sometimes applied to other cancer hazards such as polychlorinated biphenyls in drinking water.\n\nThe association of exposure to radiation with cancer had been observed as early as 1902, six years after the discovery of X-ray by Wilhelm Röntgen and radioactivity by Henri Becquerel. In 1927, Hermann Muller demonstrated that radiation may cause genetic mutation and suggested that the mutation rate may be related to dose. He also suggested mutation as a cause of cancer. Muller, who received a Nobel Prize for his work on the mutagenic effect of radiation in 1946, asserted in his Nobel Lecture, \"The Production of Mutation\", that there is \"no threshold dose\".\n\nThe early studies were based on relatively high levels of radiation that made it hard to establish the safety of low level of radiation, and many scientists at that time believed that there may be a tolerance level, and that low doses of radiation may not be harmful. A later study in 1955 on mice exposed to low dose of radiation suggest that they may outlive control animals. The interest in the effect of radiation intensified after the dropping of atomic bombs on Hiroshima and Nagasaki, and studies were conducted on the survivors. Although compelling evidence on the effect of low dosage of radiation was hard to come by, by the late 1940s, the idea of LNT became more popular due to its mathematical simplicity. In 1954, the National Council on Radiation Protection and Measurements (NCRP) introduced the concept of maximum permissible dose. In 1958, United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR) assessed the LNT model and a threshold model, but noted the difficulty in acquiring \"reliable information about the correlation between small doses and their effects either in individuals or in large populations\". The United States Congress Joint Committee on Atomic Energy (JCAE) similarly could not establish if there is a threshold or \"safe\" level for exposure, nevertheless it introduced the concept of \"As Low As Reasonably Achievable\" (ALARA). ALARA would become a fundamental principle in radiation protection policy that implicitly accepts the validity of LNT. In 1959, United States Federal Radiation Council (FRC) supported the concept of the LNT extrapolation down to the low dose region in its first report. \n\nBy the 1970s, the LNT model had become accepted as the standard in radiation protection practice by a number of bodies. In 1972, the first report of National Academy of Sciences (NAS) Biological Effects of Ionizing Radiation (BEIR), an expert panel who reviewed available peer reviewed literature, supported the LNT model on pragmatic grounds, noting that while \"dose-effect relationship for x rays and gamma rays may not be a linear function\", the \"use of linear extrapolation . . . may be justified on pragmatic grounds as a basis for risk estimation.\" In its seventh report of 2006, NAS BEIR VII writes, \"the committee concludes that the preponderance of information indicates that there will be some risk, even at low doses\".\n\nRadiation precautions have led to sunlight being listed as a carcinogen at all sun exposure rates, due to the ultraviolet component of sunlight, with no safe level of sunlight exposure being suggested, following the precautionary LNT model. According to a 2007 study submitted by the University of Ottawa to the Department of Health and Human Services in Washington, D.C., there is not enough information to determine a safe level of sun exposure at this time.\n\nIf a particular dose of radiation is found to produce one extra case of a type of cancer in every thousand people exposed, LNT projects that one thousandth of this dose will produce one extra case in every million people so exposed, and that one millionth of the original dose will produce one extra case in every billion people exposed. The conclusion is that any given dose equivalent of radiation will produce the same number of cancers, no matter how thinly it is spread.\n\nThe model is simple to apply: a quantity of radiation can be translated into a number of deaths without any adjustment for the distribution of exposure, including the distribution of exposure within a single exposed individual. For example, a hot particle embedded in an organ (such as lung) results in a very high dose in the cells directly adjacent to the hot particle, but a much lower whole-organ and whole-body dose. Thus, even if a safe low dose threshold was found to exist at cellular level for radiation induced mutagenesis, the threshold would not exist for environmental pollution with hot particles, and could not be safely assumed to exist when the distribution of dose is unknown.\n\nThe linear no-threshold model is used to extrapolate the expected number of extra deaths caused by exposure to environmental radiation, and it therefore has a great impact on public policy. The model is used to translate any radiation release, like that from a \"dirty bomb\", into a number of lives lost, while any reduction in radiation exposure, for example as a consequence of radon detection, is translated into a number of lives saved. When the doses are very low, at natural background levels, in the absence of evidence, the model predicts via extrapolation, new cancers only in a very small fraction of the population, but for a large population, the number of lives is extrapolated into hundreds or thousands, and this can sway public policy.\n\nA linear model has long been used in health physics to set maximum acceptable radiation exposures.\n\nThe United States-based National Council on Radiation Protection and Measurements (NCRP), a body commissioned by the United States Congress, recently released a report written by the national experts in the field which states that, radiation's effects should be considered to be proportional to the dose an individual receives, regardless of how small the dose is.\n\nA 1958 analysis of two decades of research on the mutation rate of 1 million lab mice showed that six major hypotheses about ionizing radiation and gene mutation were not supported by data. Its data was used in 1972 by the Biological Effects of Ionizing Radiation I committee to support the LNT model. However, it has been claimed that the data contained a fundamental error that was not revealed to the committee, and would not support the LNT model on the issue of mutations and may suggest a threshold dose \"rate\" under which radiation does not produce any mutations. The acceptance of the LNT model has been challenged by a number of scientists, see controversy section below.\n\nThe LNT model and the alternatives to it each have plausible mechanisms that could bring them about, but definitive conclusions are hard to make given the difficulty of doing longitudinal studies involving large cohorts over long periods.\n\nA 2003 review of the various studies published in the authoritative \"Proceedings of the National Academy of Sciences\" concludes that \"given our current state of knowledge, the most reasonable assumption is that the cancer risks from low doses of x- or gamma-rays decrease linearly with decreasing dose.\"\n\nA 2005 study of Ramsar, Iran (a region with very high levels of natural background radiation) showed that lung cancer incidence was lower in the high-radiation area than in seven surrounding regions with lower levels of natural background radiation. A fuller epidemiological study of the same region showed no difference in mortality for males, and a statistically insignificant increase for females.\n\nA 2009 study by researchers that looks at Swedish children exposed to fallout from Chernobyl while they were fetuses between 8 and 25 weeks gestation concluded that the reduction in IQ at very low doses was greater than expected, given a simple LNT model for radiation damage, indicating that the LNT model may be too conservative when it comes to neurological damage. However, in medical journals, studies detail that in Sweden in the year of the Chernobyl accident, the birth rate, both increased and shifted to those of \"higher maternal age\" in 1986. More advanced maternal age in Swedish mothers was linked with a reduction in offspring IQ, in a paper published in 2013. Neurological damage has a different biology than cancer.\n\nIn a 2009 study cancer rates among UK radiation workers were found to increase with higher recorded occupational radiation doses. The doses examined varied between 0 and 500 mSv received over their working lives. These results exclude the possibilities of no increase in risk or that the risk is 2-3 times that for A-bomb survivors with a confidence level of 90%. The cancer risk for these radiation workers was still less than the average for persons in the UK due to the healthy worker effect.\n\nA 2009 study focusing on the naturally high background radiation region of Karunagappalli, India concluded: \"our cancer incidence study, together with previously reported cancer mortality studies in the HBR area of Yangjiang, China, suggests it is unlikely that estimates of risk at low doses are substantially greater than currently believed.\" A 2011 meta-analysis further concluded that the \"Total whole body radiation doses received over 70 years from the natural environment high background radiation areas in Kerala, India and Yanjiang, China are much smaller than [the non-tumour dose, \"defined as the highest dose of radiation at which no statistically significant tumour increase was observed above the control level\"] for the respective dose-rates in each district.\"\n\nIn 2011 an \"in vitro\" time-lapse study of the cellular response to low doses of radiation showed a strongly non-linear response of certain cellular repair mechanisms called radiation-induced foci (RIF). The study found that low doses of radiation prompted higher rates of RIF formation than high doses, and that after low-dose exposure RIF continued to form after the radiation had ended.\n\nIn 2012 a historical cohort study of >175 000 patients without previous cancer who were examined with CT head scans in UK between 1985 and 2002 was published. The study, which investigated leukaemia and brain cancer, indicated a linear dose response in the low dose region and had qualitative estimates of risk that were in agreement with the Life Span Study (Epidemiology data for low-linear energy transfer radiation).\n\nIn 2013 a data linkage study of 11 million Australians with >680 000 people exposed to CT scans between 1985 and 2005 was published. The study confirmed the results of the 2012 UK study for leukaemia and brain cancer but also investigated other cancer types. The authors conclude that their results were generally consistent with the linear no threshold theory.\n\nThe LNT model has been contested by a number of scientists. It is been claimed that the early proponent of the model Hermann Joseph Muller intentionally ignored an early study that did not support the LNT model when he gave his 1946 Nobel Prize address advocating the model.\n\nIt is also argued that LNT model had caused an irrational fear of radiation. In the wake of the 1986 Chernobyl accident in Ukraine, Europe-wide anxieties were formented in pregnant mothers over the perception enforced by the LNT model that their children would be born with a higher rate of mutations. As far afield as the country of Denmark, hundreds of excess induced abortions were performed on the healthy unborn, out of this no-threshold fear. Following the accident however, studies of data sets approaching a million births in the EUROCAT database, divided into \"exposed\" and control groups were assessed in 1999. As no Chernobyl impacts were detected, the researchers conclude \"in retrospect the widespread fear in the population about the possible effects of exposure on the unborn was not justified\". Despite studies from Germany and Turkey, the only robust evidence of negative pregnancy outcomes that transpired after the accident were these elective abortion indirect effects, in Greece, Denmark, Italy etc., due to the anxieties created.\n\nIn very high dose radiation therapy, it was known at the time that radiation can cause a physiological increase in the rate of pregnancy anomalies, however, human exposure data and animal testing suggests that the \"malformation of organs appears to be a deterministic effect with a threshold dose\" below which, no rate increase is observed. A review in 1999 on the link between the Chernobyl accident and teratology (birth defects) concludes that \"there is no substantive proof regarding radiation‐induced teratogenic effects from the Chernobyl accident\". It is argued that the human body has defense mechanisms, such as DNA repair and programmed cell death, that would protect it against carcinogenesis due to low-dose exposures of carcinogens.\n\nRamsar, located in Iran, is often quoted as being a counter example to LNT. Based on preliminary results, it was considered as having the highest natural background radiation levels on Earth, several times higher than the ICRP-recommended radiation dose limits for radiation workers, whilst the local population did not seem to suffer any ill effects. However, the population of the high-radiation districts is small (about 1800 inhabitants) and only receive an average of 6 millisieverts per year, so that cancer epidemiology data are too imprecise to draw any conclusions. On the other hand, there may be non-cancer effects from the background radiation such as \nchromosomal aberrations or female infertility.\n\nA 2011 research of the cellular repair mechanisms support the evidence against the linear no-threshold model. According to its authors, this study published in the Proceedings of the National Academy of Sciences of the United States of America \"casts considerable doubt on the general assumption that risk to ionizing radiation is proportional to dose\".\n\nHowever, a 2011 review of studies addressing childhood leukaemia following exposure to ionizing radiation, including both diagnostic exposure and natural background exposure, concluded that existing risk factors, excess relative risk per Sv (ERR/Sv), is \"broadly applicable\" to low dose or low dose-rate exposure.\n\nSeveral expert scientific panels have been convened on the accuracy of the LNT model at low dosage, and various organizations and bodies have stated their positions on this topic:\n\nThe scientific research base shows that there is no threshold of exposure below which low levels of ionizing radiation can be demonstrated to be harmless or beneficial.\n\n\nA number of organisations disagree with using the Linear no-threshold model to estimate risk from environmental and occupational low-level radiation exposure:\n\n\n\n\nThe US Nuclear Regulatory Commission takes the intermediate position that \"accepts the LNT hypothesis as a conservative model for estimating radiation risk\", but noting that \"public health data do not absolutely establish the occurrence of cancer following exposure to low doses and dose rates — below about 10,000 mrem (100 mSv). Studies of occupational workers who are chronically exposed to low levels of radiation above normal background have shown no adverse biological effects.\"\n\nThe consequences of low-level radiation are often more psychological than radiological. Because damage from very-low-level radiation cannot be detected, people exposed to it are left in anguished uncertainty about what will happen to them. Many believe they have been fundamentally contaminated for life and may refuse to have children for fear of birth defects. They may be shunned by others in their community who fear a sort of mysterious contagion.\n\nForced evacuation from a radiation or nuclear accident may lead to social isolation, anxiety, depression, psychosomatic medical problems, reckless behavior, even suicide. Such was the outcome of the 1986 Chernobyl nuclear disaster in the Ukraine. A comprehensive 2005 study concluded that \"the mental health impact of Chernobyl is the largest public health problem unleashed by the accident to date\". Frank N. von Hippel, a U.S. scientist, commented on the 2011 Fukushima nuclear disaster, saying that \"fear of ionizing radiation could have long-term psychological effects on a large portion of the population in the contaminated areas\".\n\nSuch great psychological danger does not accompany other materials that put people at risk of cancer and other deadly illness. Visceral fear is not widely aroused by, for example, the daily emissions from coal burning, although, as a National Academy of Sciences study found, this causes 10,000 premature deaths a year in the US. It is \"only nuclear radiation that bears a huge psychological burden — for it carries a unique historical legacy\".\n\n\n"}
{"id": "5918799", "url": "https://en.wikipedia.org/wiki?curid=5918799", "title": "List of Permanent Representatives of New Zealand to the United Nations in Vienna", "text": "List of Permanent Representatives of New Zealand to the United Nations in Vienna\n\nThe Permanent Representative of New Zealand to the United Nations in Vienna is New Zealand's foremost diplomatic representative at the offices of the United Nations in Vienna, and in charge of New Zealand's diplomatic mission to the United Nations in Vienna.\n\nThe Permanent Delegation is located at New Zealand's consulate-general in Vienna. New Zealand has maintained a resident Permanent Representative to the UN in Vienna since 1982. Until 1991, the Permanent Representative was concurrently accredited as the Ambassador to Austria. However, with the closing of the bilateral mission in that year, and its transfer to Bonn, in Germany, the position of Permanent Representative was separated.\n\n\n\n"}
{"id": "39624099", "url": "https://en.wikipedia.org/wiki?curid=39624099", "title": "Louise Bates Ames", "text": "Louise Bates Ames\n\nLouise Bates Ames (29 October 1908 – 31 October 1996) was an American psychologist specializing in child development. Ames was known as a pioneer of child development studies, introducing the theory of child development stages to popular discourse. Ames authored numerous internationally renowned books on the stages of child development, hosted a television show on child development, and co-founded the Gesell Institute of Child Development in New Haven, CT.\n\nAmes's work found that children go through clear, discrete developmental phases based on age. She demonstrated that various age groups feature unique behavioral patterns, to be considered by parents and doctors in monitoring children's development. Perhaps the best-known legacy of her work was the coining of the term \"Terrible Twos,\" to describe the rigid, conflict-laden behavioral patterns of two-year-olds.\n\nAmes (née Bates) was born on 29 October 1908 in Portland, Maine to Samuel Lewis Bates and Annie Earle Leach Bates. The oldest of three, Ames was the only daughter of the Bates family. Her father, a respected lawyer and judge, and her mother, a school teacher, valued education and fostered a stimulating educational environment for Ames and her two younger brothers.\n\nAmes attended Portland public schools throughout her primary and secondary education. Here she developed an interest in debating, history, and literature, and dreamed of becoming a lawyer like her father. After her 1926 high school graduation, Ames attended Wheaton College, an all-girls school in Norton, Massachusetts. Ames chafed against what she felt was Wheaton's elitist culture and transferred to the University of Maine in 1928. While she had intended to pursue law, an undergraduate course in psychology struck Ames's interest, and in 1930 she graduated from the University of Maine with a degree in psychology. That same year, she married fellow psychology student Smith Ames. They divorced in 1937.\n\nAmes earned a master's degree in psychology from the University of Maine in 1933. That same year, she joined the Yale Clinic of Child Development as research assistant to Arnold Gesell. Ames worked at the clinic from 1933 to 1948, co-authoring a number of works with Gesell, and received her Ph.D. from Yale University in 1936 in experimental psychology. Her dissertation, \"The sequential patterning of prone progression in the human infant,\" was published as a journal article in 1937. The paper explored infants' transition to crawling, or \"prone progression.\"\n\nWhile at the Yale Clinic of Child Development, Ames published a number of works exploring the stages of child development. Over the course of the 40's, she began translating that work into a trilogy of internationally-successful books. \"The First Five Years of Life\" (1940), \"Infant and Child in the Culture of Today\" (1943), and \"The Child From Five to Ten\" (1946) extended Gesell’s descriptions of children’s stages of growth from motor and cognitive development to that of personal-social activity.\n\nIn 1938, Ames made the first of a number of films about child development. Produced for Encyclopedia Films, \"How Behavior Grows: The Sequential Patterning of Prone Progression\" was an adaptation of her dissertation on prone progression in infants. In 1944, Ames accepted a position as curator of Yale Films of Child Development.\n\nAfter her retirement from the Yale Clinic of Child Development, Ames and her colleague Dr. Frances L. Ilg worked with Janet Learned to co-found the Gesell Institute of Child Development. The institute, founded in 1950, is located in New Haven, Connecticut, though operated independently of Yale. Ames was appointed director of research at the institute. At the time, she was also a member of International Council of Women Psychologists. In her work with the council, she conducted a large-scale survey with Harriet Fjeld of women psychologists' professional experiences. The survey was published in the \"Journal of Social Psychology\".\n\nIn 1951, Ames and Ilg began a newspaper column, \"Child Behavior\". The question-and-answer column featured write-in questions from parents and advised them with an eye towards child developmental psychology. The column ran for a number of years, leading to their popular book, also titled \"Child Behavior\" (1981). In 1962, Bates and Ilg changed the name of the column to \"Parents Ask\", which was the title of another widely selling book they published together that year. At the same time, Bates also worked on a half hour weekly TV show called \"Child Behavior\".\n\nIn 1952, Ames (et al.) published a monumental normative study on age and its relation to Rorschach responses. Her findings were published in the trilogy \"The Master Work Series.\" Prior to Bates’ research, clinicians had been using a single standard to judge the normality of Rorschach responses. Bates demonstrated that both children and the elderly have patterns of response that are different from other age groups. Her research further supported her existing claim that age must be considered when evaluating the significance of differences in behavior.\n\nBates was a prolific writer, publishing papers and books until her death in 1996, of thyroid cancer. She died at her granddaughter's home in Cincinnati. Her papers are held at the Library of Congress.\n\n\n\n\n"}
{"id": "149881", "url": "https://en.wikipedia.org/wiki?curid=149881", "title": "Medical uses of silver", "text": "Medical uses of silver\n\nThe medical uses of silver include its use in wound dressings, creams, and as an antibiotic coating on medical devices. Wound dressings containing silver sulfadiazine or silver nanomaterials may be used on external infections. There is tentative evidence that silver coatings on endotracheal breathing tubes may reduce the incidence of ventilator-associated pneumonia.\nSilver generally has low toxicity, and minimal risk is expected when silver is used in approved medical applications. Alternative medicine products such as colloidal silver are not safe or effective.\n\nSilver and most silver compounds have an oligodynamic effect and are toxic for bacteria, algae, and fungi \"in vitro\". The antibacterial action of silver is dependent on the silver ion. The effectiveness of silver compounds as an antiseptic is based on the ability of the biologically active silver ion () to irreversibly damage key enzyme systems in the cell membranes of pathogens. The antibacterial action of silver has long been known to be enhanced by the presence of an electric field. Applying an electric current across silver electrodes enhances antibiotic action at the anode, likely due to the release of silver into the bacterial culture. The antibacterial action of electrodes coated with silver nanostructures is greatly improved in the presence of an electric field.\n\nSilver, used as a topical antiseptic, is incorporated by bacteria it kills. Thus dead bacteria may be the source of silver which may kill additional bacteria.\n\nSystemic reviews in 2014, 2017 and 2018 concluded that more modern dressings, with and without silver, show better results for wound healing and infection prevention than silver sulfadiazine (SSD).\n\nThe US Food and Drug Administration has approved a number of topical preparations of silver sulfadiazine for treatment of second- and third-degree burns.\n\nA 2018 Cochrane review found that silver-containing dressings may increase the probability of healing for venous leg ulcers. A 2012 systematic review found that silver-containing dressings were no better than non-silver-containing dressings in treating burns. A 2012 Cochrane review found that silver-containing hydrocolloid dressings were no better than standard alginate dressings in treating diabetic foot ulcers. A 2010 Cochrane review found insufficient evidence to determine if dressings containing silver increase or decrease infection or affect healing rates. Another 2010 review found some evidence that silver-impregnated dressings improve the short-term healing of wounds and ulcers. The lead author of this paper is a speaker for one of the manufacturers of one of the silver dressings under study. A 2009 systematic review found that silver dressings improve both wound healing and quality of life when managing chronic non-healing wounds. Another 2009 review concluded that the evidence for silver-containing foam in chronic infected wounds is not clear, but found that silver-containing foam resulted in a greater reduction in wound size and more effective control of leakage and odor than non-silver dressings. A Cochrane review from 2013 found that all of the trials that assessed dressings on superficial and partial thickness burn wounds were at risk of bias and the data were poorly reported. Silver sulphadiazine had consistently poorer healing outcomes and delayed healing times compared with biosynthetic, silicon-coated and silver dressings. Another systematic review concluded that the evidence shows an overall positive effect of silver-releasing dressings in the management of infected chronic wounds, but expressed concern that the quality of the underlying trials was limited and potentially biased.\n\nA number of wound dressings containing silver as an anti-bacterial have been cleared by the U.S. Food and Drug Administration (FDA).\n\nLimited evidence suggests that endotracheal breathing tubes coated with silver may reduce the incidence of ventilator associated pneumonia (VAP) and delay its onset, although no benefit is seen in the duration of intubation, the duration of stay in intensive care or the mortality rate. Concerns have been raised surrounding the unblinded nature of some of the studies. It is unknown if they are cost effective, and more high quality scientific trials are needed.\n\nThe U.S. Food and Drug Administration in 2007 cleared an endotracheal tube with a fine coat of silver to reduce the risk of ventilator-associated pneumonia.\n\nEvidence does not support an important reduction in the risk of urinary tract infections when silver-alloy catheters are used. These catheters are associated with greater cost than other catheters.\n\nA combination of chlorhexidine & silver-sulfadiazine used in central venous catheters (CVC) reduces the rate of catheter-related bloodstream infections. However, it seems difficult to discern whether the addition of silver-sulfadizine is responsible for the antimicrobial action of chlorhexidine-silver-sulfadiazine central venous catheters. As chlorhexidine alone may be the main cause of the antimicrobial action and more studies are needed in order to resolve this. Although, it has been hypothesized silver-sulfadizine may still help reduce infection through another, not yet determined, mechanism.\n\nResearch in 2018 into the treatment of central nervous system infections caused by free-living amoebae such as \"Naegleria fowleri\" and \"Acanthamoeba castellanii\", tested the effectiveness of existing drugs as well as the effectiveness of the same drugs when they were conjugated with silver nanoparticles. In vitro tests demonstrated more potent amoebicidal effects for the drugs when conjugated with silver nanoparticles as compared to the same drugs when used alone. They also found that conjugating the drugs with silver nanoparticles enhanced their anti-acanthamoebic activity. \n\nSilver-halide imaging plates used with X-ray imaging were the standard before digital techniques arrived. Silver x-ray film remains popular for its accuracy, and cost effectiveness, particularly in developing countries, where digital X-ray technology is usually not available.\n\nSilver compounds have been used in external preparations as antiseptics, including both silver nitrate and silver proteinate, which can be used in dilute solution as eyedrops to prevent conjunctivitis in newborn babies. Silver nitrate is also sometimes used in dermatology in solid stick form as a caustic (\"lunar caustic\") to treat certain skin conditions, such as corns and warts.\n\nSilver nitrate is also used in certain laboratory procedures to stain cells. As it turns them permanently a dark-purple/black color, in doing so increasing individual cells' visibility under a microscope and allowing for differentiation between cells, or identification of irregularities. \nSilver is also used in bone prostheses, reconstructive orthopedic surgery, and cardiac devices. Silver diamine fluoride appears to be an effective intervention to reduce dental caries (tooth decay). Silver is also a component in dental amalgam.\n\nSilver acetate has been used as a potential aid to help stop smoking; a review of the literature in 2012, however, found no effect of silver acetate on smoking cessation at a six-month endpoint and if there is an effect it would be small. Silver has also been used in cosmetics, intended to enhance antimicrobial effects and the preservation of ingredients.\n\nIn the 1840s, founder of gynecology J. Marion Sims employed silver wire, which he had a jeweler fashion, as a suture in gynecological surgery. This produced very favorable results when compared with its predecessors, silk and catgut.\n\nThough toxicity of silver is low, the human body has no biological use for silver and when inhaled, ingested, injected, or applied topically, silver will accumulate irreversibly in the body, particularly in the skin, and chronic use combined with exposure to sunlight can result in a disfiguring condition known as argyria in which the skin becomes blue or blue-gray. Localized argyria can occur as a result of topical use of silver-containing creams and solutions, while the ingestion, inhalation, or injection can result in generalized argyria. Preliminary reports of treatment with laser therapy have been reported. These laser treatments are painful and general anesthesia is required. A similar laser treatment has been used to clear silver particles from the eye, a condition related to argyria called argyrosis. The Agency for Toxic Substances and Disease Registry (ATSDR) describes argyria as a \"cosmetic problem\".\n\nOne of the more publicized incidents of argyria came in 2008, when a man named Paul Karason, whose skin turned blue from using colloidal silver for over 10 years to treat dermatitis, appeared on NBC's \"Today\" show. Karason died in 2013 at the age of 62 after a heart attack.\n\nColloidal silver may interact with some prescription medications, reducing the absorption of some antibiotics and thyroxine, among others.\n\nSome people are allergic to silver, and the use of treatments and medical devices containing silver is contraindicated for such people. Although medical devices containing silver are widely used in hospitals, no thorough testing and standardization of these products has yet been undertaken.\n\nElectrolytically-dissolved silver has been used as a water disinfecting agent, for example, the drinking water supplies of the Russian Mir orbital station and the International Space Station. Many modern hospitals filter hot water through copper-silver filters to defeat MRSA and legionella infections. The World Health Organization includes silver in a colloidal state produced by electrolysis of silver electrodes in water, and colloidal silver in water filters as two of a number of water disinfection methods specified to provide safe drinking water in developing countries. Along these lines, a ceramic filtration system coated with silver particles has been created by Ron Rivera of Potters for Peace and used in developing countries for water disinfection (in this application the silver inhibits microbial growth on the filter substrate, to prevent clogging, and does not directly disinfect the filtered water).\n\nColloidal silver (a colloid consisting of silver particles suspended in liquid) and formulations containing silver salts were used by physicians in the early 20th century, but their use was largely discontinued in the 1940s following the development of safer and effective modern antibiotics. Since about 1990, there has been a resurgence of the promotion of colloidal silver as a dietary supplement, marketed with claims of it being an essential mineral supplement, or that it can prevent or treat numerous diseases, such as cancer, diabetes, arthritis, HIV/AIDS, herpes, and tuberculosis. No medical evidence supports the effectiveness of colloidal silver for any of these claimed indications. Silver is not an essential mineral in humans; there is no dietary requirement for silver, and hence, no such thing as a silver \"deficiency\". \nThere is no evidence that colloidal silver treats or prevents any medical condition, and it can cause serious and potentially irreversible side effects such as argyria.\n\nIn August 1999, the U.S. FDA banned colloidal silver sellers from claiming any therapeutic or preventive value for the product, although silver-containing products continue to be promoted as dietary supplements in the U.S. under the looser regulatory standards applied to supplements. The FDA has issued numerous Warning Letters to Internet sites that have continued to promote colloidal silver as an antibiotic or for other medical purposes. Despite the efforts of the FDA, silver products remain widely available on the market today. A review of websites promoting nasal sprays containing colloidal silver suggested that information about silver-containing nasal sprays on the internet is misleading and inaccurate.\n\nIn 2002, the Australian Therapeutic Goods Administration (TGA) found there were no legitimate medical uses for colloidal silver and no evidence to support its marketing claims. The U.S. National Center for Complementary and Integrative Health (NCCIH) warns that marketing claims about colloidal silver are scientifically unsupported, that the silver content of marketed supplements varies widely, and that colloidal silver products can have serious side effects such as argyria.\nIn 2009, the USFDA issued a \"Consumer Advisory\" warning about the potential adverse effects of colloidal silver, and said that \"...there are no legally marketed prescription or over-the-counter (OTC) drugs containing silver that are taken by mouth.\"\nQuackwatch states that colloidal silver dietary supplements have not been found safe or effective for the treatment of any condition. \"Consumer Reports\" lists colloidal silver as a \"supplement to avoid\", describing it as \"likely unsafe\". The \"Los Angeles Times\" stated that \"colloidal silver as a cure-all is a fraud with a long history, with quacks claiming it could cure cancer, AIDS, tuberculosis, diabetes, and numerous other diseases.\"\n\nIt may be illegal to market as preventing or treating cancer, and in some jurisdictions illegal to sell colloidal silver for consumption. In 2015 an English man was prosecuted and found guilty under the Cancer Act 1939 for selling colloidal silver with claims it could treat cancer.\n\nHippocrates in his writings discussed the use of silver in wound care. At the beginning of the twentieth century surgeons routinely used silver sutures to reduce the risk of infection. In the early 20th century, physicians used silver-containing eyedrops to treat ophthalmic problems, for various infections, and sometimes internally for diseases such as tropical sprue, epilepsy, gonorrhea, and the common cold. During World War I, soldiers used silver leaf to treat infected wounds.\n\nPrior to the introduction of modern antibiotics, colloidal silver was used as a germicide and disinfectant. With the development of modern antibiotics in the 1940s, the use of silver as an antimicrobial agent diminished. Silver sulfadiazine (SSD) is a compound containing silver and the antibiotic sodium sulfadiazine, which was developed in 1968.\n\nThe National Health Services in the UK spent about £25 million on silver-containing dressings in 2006. Silver-containing dressings represent about 14% of the total dressings used and about 25% of the overall wound dressing costs.\n\nConcerns have been expressed about the potential environmental cost of manufactured silver nanomaterials in consumer applications being released into the environment, for example that they may pose a threat to benign soil organisms.\n\n"}
{"id": "9538838", "url": "https://en.wikipedia.org/wiki?curid=9538838", "title": "National Research Act", "text": "National Research Act\n\nThe National Research Act was enacted by the 93rd United States Congress and signed into law by President Richard Nixon on July 12, 1974 after a series of congressional hearings on human-subjects research, directed by Senator Edward Kennedy. The National Research Act created the National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research to develop guidelines for human subject research and to oversee and regulate the use of human experimentation in medicine. The National Research Act issued Title 45, Part 46 of the Code of Federal Regulations: Protection of Human Subjects. The National Research Act is overseen by the Office of Human Research Protections. The Act also formalized a regulated IRB process through local institutional review boards, also overseen by the Office of Human Research Protections.\n\nThe National Research Act gained traction as a response to the infamous Tuskegee syphilis study.\n\n\n"}
{"id": "14458448", "url": "https://en.wikipedia.org/wiki?curid=14458448", "title": "Neonatal teeth", "text": "Neonatal teeth\n\nNatal teeth are teeth that are present above the gumline (have already erupted) at birth, and neonatal teeth are teeth that emerge through the gingiva during the first month of life (the neonatal period).\n\nThe incidence of neonatal teeth varies considerably, between 1:700 and 1:30,000 depending on the type of study; the highest prevalence is found in the only study that relies on personal examination of patients.\n\nNatal teeth, and neonatal teeth, can be the baby's normal deciduous teeth, sprouting prematurely. These should be preserved, if possible. Alternately, they could be supernumary teeth, extra teeth, not part of the normal allotment of teeth.\n\nMost often natal teeth are mandibular central incisors. They have little root structure and are attached to the end of the gum by soft tissue and are often mobile.\n\nMost of the time, natal teeth are not related to a medical condition. However, sometimes they may be associated with:\n\nNo intervention is usually recommended unless they are causing difficulty to the infant or mother.\n\nHowever some recommend that they be removed as the tooth can cut or amputate the tip of the tongue.\nThey should be left in the mouth as long as possible to decrease the likelihood of removing permanent tooth buds with the natal tooth. They should also not be removed if the infant has hypoprothrombinemia. In case of complications when the natal teeth need to be removed, dental radiographs should be obtained whenever possible, and evaluated and followed up with pediatric dentists.\n\n"}
{"id": "18890971", "url": "https://en.wikipedia.org/wiki?curid=18890971", "title": "Normative Aging Study", "text": "Normative Aging Study\n\nThe Normative Aging Study (NAS) is a longitudinal study which studies the effects of aging on various health issues. The ongoing study was established in 1963 by the United States Department of Veterans Affairs. The initial sample was 2,280 men now with an average age of 72 years (mean age at entry was 42 years). Most participants are veterans from World War II and the Korean War.\n\nParticipants in the study have undergone medical examinations every three to five years, also answering questions about behaviors affecting health.\n\nAmong the topics researchers have used the NAS for are stress, smoking, and cardiac health.\n\nOne of the findings based on the study was that subjective stress appeared to be a better prediction of mortality than the objective evaluation of stressful events.\n"}
{"id": "15959743", "url": "https://en.wikipedia.org/wiki?curid=15959743", "title": "O'Connor Hospital", "text": "O'Connor Hospital\n\nO'Connor Hospital is a hospital operated by the Verity Health System in San Jose, California. It was founded in 1889, and was the first hospital in Santa Clara County, California. O'Connor Hospital was built by Judge Myles P. O'Connor and his wife, Amanda. The original hospital was built on of property on the corner of Race Street and West San Carlos Street. \n\nThe O'Connors were very well known as being benefactors of many charitable organizations. The O'Connors' first thoughts were to build an elderly home, however, the demands of the community at that time was for a hospital. Patrick Riordan, Archbishop of San Francisco, suggested that a hospital should be built. The O'Connors then asked the Daughters of Charity to operate the hospital. Four Daughters of Charity moved into the facility on May 1, 1889.\n\nIn 1953, O'Connor Hospital moved to its present site on Forest Avenue.\n\nIn October 2014, Daughters of Charity announced they would be shedding the hospital for financial reasons.\n\n"}
{"id": "6497079", "url": "https://en.wikipedia.org/wiki?curid=6497079", "title": "Organ transplantation in China", "text": "Organ transplantation in China\n\nOrgan transplantation in China has taken place since the 1960s, and is one of the largest organ transplant programmes in the world, peaking at over 13,000 transplants a year in 2004. China is also involved in innovative transplant surgery such as face transplantation including bone.\n\nInvoluntary organ harvesting is illegal under Chinese law; though, under a 1984 regulation, it became legal to remove organs from executed criminals with the prior consent of the criminal or permission of relatives. Growing concerns about possible ethical abuses arising from coerced consent and corruption led medical groups and human rights organizations, by the 1990s, to start condemning the practice. These concerns resurfaced in 2001, when a Chinese asylum-seeking doctor testified that he had taken part in organ extraction operations.\n\nIn 2006, allegations emerged that a large number of Falun Gong practitioners had been killed to supply China's organ transplant industry. An initial investigation stated \"the source of 41,500 transplants for the six year period 2000 to 2005 is unexplained\" and concluded that \"there has been and continues today to be large scale organ seizures from unwilling Falun Gong practitioners\".\n\nIn December 2005, China's Deputy Health Minister acknowledged that the practice of removing organs from executed prisoners for transplants was widespread. In 2007, China issued regulations banning the commercial trading of organs, and the Chinese Medical Association agreed that the organs of prisoners should not be used for transplantation, except for members of the immediate family of the deceased. In 2008, a liver-transplant registry system was established in Shanghai, along with a nationwide proposal to incorporate information on individual driving permits for those wishing to donate their organs.\n\nDespite these initiatives, \"China Daily\" reported in August 2009 that approximately 65% of transplanted organs still came from death row prisoners. The condemned prisoners have been described as \"not a proper source for organ transplants\" by Vice-Health Minister Huang Jiefu, and in March 2010 he announced the trial of China's first organ donation program starting after death, jointly run by the Red Cross Society and the Ministry of Health, in 10 pilot regions. In 2013, Huang Jiefu altered his position on utilizing prisoners' organs, stating that death row prisoners should be allowed to donate organs and should be integrated into the new computer-based organ allocation system.\nIn October 2018 https://www.bbc.co.uk/programmes/w3csxyl4 BBC World Service Radio did an investigation into these allegations.\n\nGlobally, pioneering experimental studies in the surgical technique of human organ transplantation were made in the early 1900s by the French surgeon Alexis Carrel, and successful transplants starting spreading worldwide after the Second World War. China itself began organ transplantation in the 1960s, which grew to an annual peak of over 13,000 transplants in 2004; and, despite some deaths from infection and hepatitis, the transplant programme has been successful in saving many lives. Though the number of transplants fell to under 11,000 annually by 2005, China still has one of the largest transplant programmes in the world. China explores innovative surgery, such as the world's first flesh and bone face transplant, performed by Professor Guo Shuzhong. Organ donation, however, has met resistance, and involuntary organ donation is illegal under Chinese law, as it is against Chinese tradition and culture, which attach symbolic life affirming importance to the kidney and heart. China is not alone in encountering donation difficulties; demand outstrips supply in most countries. The world-wide shortage has encouraged some countries—such as India—to trade in human organs. Reports of organs being removed from executed prisoners in China for sale internationally had been circulating since the mid-1980s, when a 1984 regulation made it legal to harvest organs from convicted criminals with the consent of the family or if the body goes unclaimed. Development of an immunosuppressant drug, cyclosporine A, made transplants a more viable option for patients.\n\nThe first living related renal transplant was performed in China in 1972; the first allogeneic bone marrow transplantation was successfully executed in an acute leukaemia patient. The first recorded clinical liver transplant from a living donor in China took place in 1995, seven years after the world's first was performed in Sao Paulo, Brazil. Between January 2001 and October 2003, 45 patients received living donor liver transplantation (LDLT) at five different hospitals. In 2002, doctors at Xijing Hospital of the Fourth Military Medical University described three cases of living related liver transplantation. In 2003 a landmark brain-death case involving switched off ventilation came to the attention of the public and made a big impact on medical ethics and legislation. The first successful brain-death organ donation soon followed. From October 2003 to July 2006, 52 LDLT operations were conducted at the West China Hospital, West China Medical Center of Sichuan University. In October 2004, Peking University People's Hospital Liver Transplantation Center executed two cases of living related liver transplantation involving complex blood vessel anatomy. In 2002, the Chinese media reported surgeon Dr Zheng Wei successfully transplanted a whole ovary at the Zhejiang Medical Science University to a 34-year-old patient, Tang Fangfang, from her sister. In April 2006, the Xijing military hospital in Xian carried out a face transplant operation covering the cheek, upper lip, and nose of Li Guoxing, who was mauled by an Asiatic black bear while protecting his sheep.\n\nThe first successful penis transplant procedure was performed in September 2006, at a military hospital in Guangzhou. The patient, a 44-year-old male, had sustained the loss of most of his penis in an accident. The transplanted penis came from a brain-dead 22-year-old male. Although successful, the patient and his wife suffered psychological trauma as a result of the procedure, and had the surgery reversed fifteen days later. Following this, Jean-Michel Dubernard, famous for performing the world's first face transplant, wrote that the case \"raises many questions and has some critics\". He alluded to a double standard writing, \"I cannot imagine what would have been the reactions of the medical profession, ethics specialists, and the media if a European surgical team had performed the same operation.\"\n\nTransplantation first began in the early 1970s China, when organs were sourced from executed prisoners. Although other sources, such as brain-dead donors, had been tried, the lack of legal framework hampered efforts. Dr Klaus Chen said in 2007 that this was still the dominant pool. Concerns that some poorer countries were answering donor shortages by selling organs to richer countries led the World Medical Association (WMA) to condemn the purchase and sale of human organs for transplantation at Brussels in 1985, in 1987 and at Stockholm in 1994.\n\nIn Madrid in 1987, the World Health Organization (WHO) condemned the practice of extracting organs from executed prisoners due to the difficulty of knowing if they had given consent. Growing concern led other professional societies and human rights organisations to condemn the practice in the 1990s, and to question the way in which the organs were obtained. The WHO starting drafting an international guideline (WHA44.25) on human organ transplants in 1987 which resulted in the \"WHO Guiding Principles on Human Organ Transplantation\" being endorsed in 1991. However, the wording did not allow the international community to draw up any laws preventing China from continuing to trade in human organs.\n\nThe United States Senate Committee on Foreign Relations convened a hearing in 1995 on the trade in human body parts in China; receiving evidence from various sources including statements from Amnesty International, the BBC, and Chinese government documents produced by human rights activist Harry Wu.\n\nThe World Medical Association, the Korean Medical Association, and the Chinese Medical Association reached an agreement in 1998 that these practices were undesirable and that they would jointly investigate them with a view to stopping them; however, in 2000, the Chinese withdrew their cooperation. Amnesty International claimed to have strong evidence that the police, courts and hospitals were complicit in the organ trade, facilitated by the use of mobile execution chambers, or \"death vans\". Amnesty speculated that this profitable trade might explain China's refusal to consider abolishing the death penalty, which is used on between 1,770 (official figure) and 8,000 (Amnesty estimates) prisoners annually. Corpses are typically cremated before relatives or independent witnesses can view them, fuelling suspicions about the fate of internal organs.\n\nIn June 2001, Wang Guoqi, a Chinese doctor applying for political asylum, made contact with Harry Wu and his Laogai Research Foundation, who assisted Wang in testifying to the US Congress in writing that he had removed skin and corneas from more than 100 executed prisoners for the transplant market at the Tianjin Paramilitary Police General Brigade Hospital, and that during at least one such operation the prisoner was still breathing. Wang, a 'burns specialist', said that he had also seen other doctors remove vital organs from executed prisoners; and the hospital where he worked sold those organs to foreigners. Harry Wu said that he had gone to \"great lengths\" to verify Wang's identity and that both the foundation and congressional staff members found the doctor's statements \"highly credible.\"\n\nIn December 2005, China's Deputy Health Minister acknowledged that the practice of removing organs from executed prisoners for transplant was widespread – as many as 95% of all organ transplants in China derived from executions, and he promised steps to prevent abuse. In 2006, the WMA demanded that China cease using prisoners as organ donors. According to \"Time\", a transplant brokerage in Japan which organised 30–50 operations annually sourced its organs from executed prisoners in China. Edward McMillan-Scott, vice president of the European Parliament, said he believed that nearly 400 hospitals in China had been involved in the transplant organ trade, with websites advertising kidney transplants for $60,000.\n\nOn the eve of a state visit to the United States by President Hu Jintao, the 800-member British Transplantation Society also criticised China's use of death-row prisoners' organs in transplants, on the grounds that as it is impossible to verify that organs are indeed from prisoners who have given consent; the WMA once again condemned the practice on similar grounds. A BBC news report by Rupert Wingfield-Hayes in September 2006 showed negotiations with doctors in No 1 Central Hospital in Tianjin for a liver transplant.\n\nIn February 2017, CGTN quoted former vice health minister Huang Jiefu as saying \"\"From January 1, 2015, organ donation from voluntary civilian organ donors has become the only legitimate source of organ transplantations\", and Francis Delmonico interpreting this as a ban on \"the use of organs from executed prisoners\" in January 2015.\n\nIn 2006, allegations that Falun Gong practitioners had been killed to supply China's organ transplant industry prompted an investigation by former Canadian Secretary of State David Kilgour and human rights lawyer David Matas. In July 2006, the Kilgour-Matas report questioned \"the source of 41,500 transplants for the six year period 2000 to 2005\" and thereby inferred that \"the government of China and its agencies in numerous parts of the country, in particular hospitals but also detention centres and 'people's courts', since 1999 have put to death a large but unknown number of Falun Gong prisoners of conscience\".\n\nThe authors of the Kilgour-Matas report reached their conclusion via circumstantial evidence and inference from this evidence. \nIt included observations of the extremely short wait times for organs in China compared with other countries, indicating that organs were being procured on demand; the rise in the number of annual organ transplants in China corresponded with the onset of the persecution of Falun Gong. An updated version of their report was published as a book in 2009.\n\nIn 2014, investigative journalist Ethan Gutmann published the results of his own investigation. Gutmann conducted extensive interviews around with former detainees in Chinese labor camps and prisons, as well as former security officers and medical professionals with knowledge of China's transplant practices. He reported that organ harvesting from political prisoners likely began in Xinjiang province in the 1990s, and then spread nationwide. Gutmann estimates that some 64,000 Falun Gong prisoners may have been killed for their organs between 2000 and 2008.\n\nIn December 2006, after not getting assurances from the Chinese government about allegations relating to Chinese prisoners, the two major organ transplant hospitals in Queensland, Australia, stopped transplant training for Chinese surgeons and banned joint research programs into organ transplantation with China.\n\nIn July 2006 and April 2007, Chinese officials denied organ harvesting allegations, insisting that China abides by World Health Organization principles that prohibit the sale of human organs without written consent from donors.\n\nIn May 2008 two United Nations Special Rapporteurs reiterated their previous request for the Chinese authorities to adequately respond to the allegations, and to explain the source of organs which would account for the sudden increase in organ transplants in China since 2000.\n\nOn 12 September 2012, the United States House Committee on Foreign Affairs held a hearing on the topic of organ harvesting from prisoners of conscience in China. During the hearing, Gutmann described his interviews with former Chinese prisoners, surgeons and nurses with knowledge of organ harvesting practices. Gutmann found evidence that Falun Gong detainees in China were subject to medical exams designed to assess the health of their organs. Dr. Damon Noto's testimony concluded up to 60,000 Falun Gong prisoners of conscience had been killed for their organs, and noted that there was an \"exponential increase in transplantations\" in China from 2000 onward, corresponding with the onset of the suppression of Falun Gong.\n\nIn 2012, \"State Organs: Transplant Abuse in China\", edited by David Matas and Dr. Torsten Trey, was published with essays by Dr. Gabriel Danovitch, Professor of Medicine, Arthur Caplan, Professor of Bioethics, Dr. Jacob Lavee, cardiothoracic surgeon, Dr. Ghazali Ahmad, Professor Maria Fiatarone Singh, Dr. Torsten Trey, Gutmann and Matas.\n\nHarry Wu, a human rights activist, has questioned the Falun Gong's claims that Falun Gong members are specifically targeted for large-scale organ harvesting.\n\nInternational human rights lawyer David Matas argued Harry Wu's July 2006 article showed his views in his March 21 letter were formed before completing his investigation, so Wu's views were not based on his full investigation. Further, Harry Wu characterized the volume of organ harvesting Annie described as \"technically impossible\", but it is technically possible, according to medical expert.\n\nA Chinese government panel slammed the allegations in August 2016. Huang Jiefu, chairman of the National Organ Donation and Transplantation Committee, noted that there were 10,057 organ transplantation surgeries performed in China in 2015, accounting for 8.5 percent of global total, and 8 percent of drugs used globally, which matches China's national statistics. Michael Millis, professor of Surgery and chief of the Section of Transplantation of the University of Chicago Hospitals, corroborated that China is phasing out the organ transplantation of executed prisoners, and is moving towards a voluntary, donation-based system. José Nuñez, medical officer in charge of global organ transplantation at the World Health Organization, noted that China is reaching global standards in organ transplantation, and believed that in a few years, China will be leading the field.\n\nIn March 2006, the Ministry of Health issued the \"Interim Provisions on Clinical Application and Management of Human Organ Transplantation\", which stipulated that medical centres must meet new requirements for transplant services; the provinces were made responsible for plans for clinical applications. Establishments performing transplantation were thereby obliged to incorporate considerations for ethics, medical and surgical expertise, and intensive care. In April 2006, the Committee of Clinical Application of Human Organ Transplantation Technologies was created to standardise clinical practice; a national summit on clinical management took place in November 2006 which issued a declaration outlining regulatory steps. Professor Guo Shuzhong conducted a series of face transplant experiments in Xijing hospital, leading in April 2006 to the world's first face transplant that included bone. The donor had been declared brain-dead before the operation.\n\nIn May 2007 the Regulation on Human Organ Transplantation came into force, banning organ trading and the removal of a person's organs without their prior written consent, and this has been favourably received by the World Health Organization and The Transplantation Society. To curb illegal transplants, doctors involved in commercial trade of organs will face fines and suspensions; only a few hospitals will be certified to perform organ transplants. As a result of a systematic overhaul, the number of institutions approved for transplants has been reduced from more than 600 in 2007 to 87 as at October 2008; another 77 have received provisional approval from the Ministry of Health.\n\nTo further combat transplant tourism, the Health Ministry issued a notice in July 2007 in line with the Istanbul Declaration, giving Chinese citizens priority as organ recipients. In October 2007, after several years of discussions with the WHO, the Chinese Medical Association agreed to cease commercial organ collection from condemned prisoners, who would only be able to donate to their immediate relatives. Other safeguards implemented under the legislation include documentation of consent for organ removal from the donor, and review of all death sentences by the Supreme People's Court. Transplant professionals are not involved until death is declared. A symposium among legal and medical professionals was held in April 2008 to discuss the diagnostic criteria for brain death for donors of transplant organs.\n\nA liver-transplant registry system was established in Shanghai, in 2008, which allows the monitoring of the after-care of liver recipients; at the same time a nationwide proposal was announced that would allow people to note on their driving licence that they wish to donate their organs. Despite these initiatives the \"China Daily\" newspaper reported in August 2009 that approximately 65% of transplanted organs still came from death row prisoners, which has been described as \"not a proper source for organ transplants\" by Vice-Health Minister Huang Jiefu. China's first posthumous organ donation system was jointly launched in March 2010 by the Red Cross and the Ministry of Health. Huang Jiefu announced that the scheme, which will allow people to express their wishes on their driver's licences, would be trialled in 10 pilot regions including the cities of Tianjin, Wuhan and Shenzhen. Funds will be made available for the families of people who voluntarily donate their organs. Chinese authorities say they hope the pilot program's success will reduce the need to take organs from death row prisoners and stem the tide of black market organs. In 2012 China officials stated they plan to phase out organ harvesting of death-row inmates.\n\nIn September 2012, the report \"Organ Harvesting of Religious and Political Dissidents by the Chinese Communist Party\" presented to the members of a US Congress Subcommittee by Damon Noto, the spokesperson for the organization Doctors Against Forced Organ Harvesting, opined: \"Medical doctors outside China have confirmed that their patients have gone to China and received organs from Falun Gong practitioners\".\n\nThe Hangzhou resolution was promulgated in front of the 2013 China National Transplantation Congress on 31 October 2013 and was presented on 2 November 2013. The resolution vows for the cessation of the harvesting of organs from executed prisoners. While not all transplantation facilities have adopted the resolution, a campaign to eradicate inmate organ harvesting is underway.\n\nChina has by far the shortest wait times for organ transplants in the world, and there is evidence that the execution of prisoners for their organs is \"timed for the convenience of the waiting recipient.\" Organ tourists to China report receiving kidney transplants within days of arriving in China. A report produced by David Matas and David Kilgour cites the China International Transplantation Assistant Centre website as saying \"it may take only one week to find out the suitable (kidney) donor, the maximum time being one month...\"\n\nBy way of comparison, the median waiting time for an organ transplantation in Australia is 6 months to 4 years, in Canada, it is 6 years as of 2011. In UK it is 3 years.\n\n\n"}
{"id": "20774168", "url": "https://en.wikipedia.org/wiki?curid=20774168", "title": "Otofacial syndrome", "text": "Otofacial syndrome\n\nOtofacial syndrome is an extraordinarily rare congenital deformity in which a person is born without a mandible, and, consequently, without a chin.\n\nIn nearly all cases, the child does not survive because it is unable to breathe and eat properly. Even with reconstructive surgery, the tongue is extremely underdeveloped, making unaided breathing and swallowing impossible.\nThe first challenge to survival is assisted breathing and tubal feeding. This is a lifelong affair, generally requiring the patient to spend nearly all of the time under direct hospital care.\n\nAmerican surgeons successfully used bone from the hip of an Irish teenager named Alan Doherty to rebuild a jaw and chin. Surgeons began the procedures in June 2007 and completed the final of seven surgeries on 25 August 2008. Doherty is now able to smile, but is still unable to breathe, eat, or speak on his own.\n"}
{"id": "712576", "url": "https://en.wikipedia.org/wiki?curid=712576", "title": "Passive smoking", "text": "Passive smoking\n\nPassive smoking is the inhalation of smoke, called second-hand smoke (SHS), or environmental tobacco smoke (ETS), by persons other than the intended \"active\" smoker. It occurs when tobacco smoke permeates any environment, causing its inhalation by people within that environment. Exposure to second-hand tobacco smoke causes disease, disability, and death. The health risks of second-hand smoke are a matter of scientific consensus. These risks have been a major motivation for smoke-free laws in workplaces and indoor public places, including restaurants, bars and night clubs, as well as some open public spaces.\n\nConcerns around second-hand smoke have played a central role in the debate over the harms and regulation of tobacco products. Since the early 1970s, the tobacco industry has viewed public concern over second-hand smoke as a serious threat to its business interests. Harm to bystanders was perceived as a motivator for stricter regulation of tobacco products. Despite the industry's awareness of the harms of second-hand smoke as early as the 1980s, the tobacco industry coordinated a scientific controversy with the purpose of stopping regulation of their products.\n\nSecond-hand smoke causes many of the same diseases as direct smoking, including cardiovascular diseases, lung cancer, and respiratory diseases. These diseases include:\n\n\nEpidemiological studies show that non-smokers exposed to second-hand smoke are at risk for many of the health problems associated with direct smoking. Most of the research has come from studies of nonsmokers who are married to a smoker. Those conclusions are also backed up by further studies of workplace exposure to smoke.\n\nIn 1992, a review estimated that second-hand smoke exposure was responsible for 35,000 to 40,000 deaths per year in the United States in the early 1980s. The absolute risk increase of heart disease due to ETS was 2.2%, while the attributable risk percent was 23%. A 1997 meta-analysis found that second-hand smoke exposure increased the risk of heart disease by a quarter, and two 1999 meta-analyses reached similar conclusions.\n\nEvidence shows that inhaled sidestream smoke, the main component of second-hand smoke, is about four times more toxic than mainstream smoke. This fact has been known to the tobacco industry since the 1980s, though it kept its findings secret. Some scientists believe that the risk of passive smoking, in particular the risk of developing coronary heart diseases, may have been substantially underestimated.\n\nIn 1997, a meta-analysis on the relationship between secondhand smoke exposure and lung cancer concluded that such exposure caused lung cancer. The increase in risk was estimated to be 24% among non-smokers who lived with a smoker. In 2000, Copas and Shi reported that there was clear evidence of publication bias in the studies included in this meta-analysis. They further concluded that after correcting for publication bias, and assuming that 40% of all studies are unpublished, this increased risk decreased from 24% to 15%. This conclusion has been challenged on the basis that the assumption that 40% of all studies are unpublished was \"extreme\". In 2006, Takagi et al. reanalyzed the data from this meta-analysis to account for publication bias and estimated that the relative risk of lung cancer among those exposed to secondhand smoke was 1.19, slightly lower than the original estimate. A 2000 meta-analysis found a relative risk of 1.48 for lung cancer among men exposed to secondhand smoke, and a relative risk of 1.16 among those exposed to it at work. Another meta-analysis confirmed the finding of an increased risk of lung cancer among women with spousal exposure to secondhand smoke the following year. It found a relative risk of lung cancer of 1.29 for women exposed to secondhand smoke from their spouses. A 2014 meta-analysis noted that \"the association between exposure to secondhand smoke and lung cancer risk is well established.\"\n\nA minority of epidemiologists have found it hard to understand how second-hand smoke, which is more diluted than actively inhaled smoke, could have an effect that is such a large fraction of the added risk of coronary heart disease among active smokers. One proposed explanation is that second-hand smoke is not simply a diluted version of \"mainstream\" smoke, but has a different composition with more toxic substances per gram of total particulate matter. Passive smoking appears to be capable of precipitating the acute manifestations of cardio-vascular diseases (atherothrombosis) and may also have a negative impact on the outcome of patients who suffer acute coronary syndromes.\n\nIn 2004, the International Agency for Research on Cancer (IARC) of the World Health Organization (WHO) reviewed all significant published evidence related to tobacco smoking and cancer. It concluded:\n\nSubsequent meta-analyses have confirmed these findings.\n\nThe National Asthma Council of Australia cites studies showing that second-hand smoke is probably the most important indoor pollutant, especially around young children:\n\nIn France, exposure to second-hand smoke has been estimated to cause between 3,000 and 5,000 premature deaths per year, with the larger figure cited by Prime Minister Dominique de Villepin during his announcement of a nationwide smoke-free law: \"That makes more than 13 deaths a day. It is an unacceptable reality in our country in terms of public health.\"\n\nThere is good observational evidence that smoke-free legislation reduces the number of hospital admissions for heart disease.\n\nThe International Agency for Research on Cancer of the World Health Organization concluded in 2004 that there was sufficient evidence that second-hand smoke caused cancer in humans. Those who work in environments where smoke is not regulated are at higher risk. Workers particularly at risk of exposure include those in installation repair and maintenance, construction and extraction, and transportation.\n\nThe US Surgeon General, in his 2006 report, estimated that living or working in a place where smoking is permitted increases the non-smokers' risk of developing heart disease by 25–30% and lung cancer by 20–30%. In the U.S., smokers that have not quit successfully have a risk of lung cancer about 20 times higher than that of never smokers.\n\nEnvironmental tobacco smoke can be evaluated either by directly measuring tobacco smoke pollutants found in the air or by using biomarkers, an indirect measure of exposure. Carbon monoxide monitored through breath, nicotine, cotinine, thiocyanates, and proteins are the most specific biological markers of tobacco smoke exposure. Biochemical tests are a much more reliable biomarker of second-hand smoke exposure than surveys. Certain groups of people are reluctant to disclose their smoking status and exposure to tobacco smoke, especially pregnant women and parents of young children. This is due to their smoking being socially unacceptable. Also, it may be difficult for individuals to recall their exposure to tobacco smoke.\n\nA 2007 study in the \"Addictive Behaviors\" journal found a positive correlation between second-hand tobacco smoke exposure and concentrations of nicotine and/or biomarkers of nicotine in the body. Significant biological levels of nicotine from second-hand smoke exposure were equivalent to nicotine levels from active smoking and levels that are associated with behaviour changes due to nicotine consumption.\n\nCotinine, the metabolite of nicotine, is a biomarker of second-hand smoke exposure. Typically, cotinine is measured in the blood, saliva, and urine. Hair analysis has recently become a new, noninvasive measurement technique. Cotinine accumulates in hair during hair growth, which results in a measure of long-term, cumulative exposure to tobacco smoke. Urinary cotinine levels have been a reliable biomarker of tobacco exposure and have been used as a reference in many epidemiological studies. However, cotinine levels found in the urine reflect exposure only over the preceding 48 hours. Cotinine levels of the skin, such as the hair and nails, reflect tobacco exposure over the previous three months and are a more reliable biomarker.\n\nCarbon monoxide monitored via breath is also a reliable biomarker of second-hand smoke exposure as well as tobacco use. With high sensitivity and specificity, it not only provides an accurate measure, but the test is also non-invasive, highly reproducible, and low in cost. Breath CO monitoring measures the concentration of CO in an exhalation in parts per million, and this can be directly correlated to the blood CO concentration (carboxyhemoglobin). Breath CO monitors can also be used by emergency services to identify patients who are suspected of having CO poisoning.\n\nA 2004 study by the International Agency for Research on Cancer of the World Health Organization concluded that non-smokers are exposed to the same carcinogens as active smokers. Sidestream smoke contains more than 4,000 chemicals, including 69 known carcinogens. Of special concern are polynuclear aromatic hydrocarbons, tobacco-specific N-nitrosamines, and aromatic amines, such as 4-aminobiphenyl, all known to be highly carcinogenic. Mainstream smoke, sidestream smoke, and second-hand smoke contain largely the same components, however the concentration varies depending on type of smoke. Several well-established carcinogens have been shown by the tobacco companies' own research to be present at higher concentrations in sidestream smoke than in mainstream smoke.\n\nSecond-hand smoke has been shown to produce more particulate-matter (PM) pollution than an idling low-emission diesel engine. In an experiment conducted by the Italian National Cancer Institute, three cigarettes were left smoldering, one after the other, in a 60 m³ garage with a limited air exchange. The cigarettes produced PM pollution exceeding outdoor limits, as well as PM concentrations up to 10-fold that of the idling engine.\n\nSecond-hand tobacco smoke exposure has immediate and substantial effects on blood and blood vessels in a way that increases the risk of a heart attack, particularly in people already at risk. Exposure to tobacco smoke for 30 minutes significantly reduces coronary flow velocity reserve in healthy nonsmokers. Second-hand smoke is also associated with impaired vasodilation among adult nonsmokers. Second-hand smoke exposure also affects platelet function, vascular endothelium, and myocardial exercise tolerance at levels commonly found in the workplace.\n\nPulmonary emphysema can be induced in rats through acute exposure to sidestream tobacco smoke (30 cigarettes per day) over a period of 45 days. Degranulation of mast cells contributing to lung damage has also been observed.\n\nThe term \"third-hand smoke\" was recently coined to identify the residual tobacco smoke contamination that remains after the cigarette is extinguished and second-hand smoke has cleared from the air. Preliminary research suggests that by-products of third-hand smoke may pose a health risk,\nthough the magnitude of risk, if any, remains unknown. In October 2011, it was reported that Christus St. Frances Cabrini Hospital in Alexandria, Louisiana would seek to eliminate third-hand smoke beginning in July 2012, and that employees whose clothing smelled of smoke would not be allowed to work. This prohibition was enacted because third-hand smoke poses a special danger for the developing brains of infants and small children.\n\nIn 2008, there were more than 161,000 deaths attributed to lung cancer in the United States. Of these deaths, an estimated 10% to 15% were caused by factors other than first-hand smoking; equivalent to 16,000 to 24,000 deaths annually. Slightly more than half of the lung cancer deaths caused by factors other than first-hand smoking were found in nonsmokers. Lung cancer in non-smokers may well be considered one of the most common cancer mortalities in the United States. Clinical epidemiology of lung cancer has linked the primary factors closely tied to lung cancer in non-smokers as exposure to second-hand tobacco smoke, carcinogens including radon, and other indoor air pollutants.\n\nThere is widespread scientific consensus that exposure to second-hand smoke is harmful. The link between passive smoking and health risks is accepted by every major medical and scientific organisation, including:\n\nRecent major surveys conducted by the U.S. National Cancer Institute and Centers for Disease Control have found widespread public awareness that second-hand smoke is harmful. In both 1992 and 2000 surveys, more than 80% of respondents agreed with the statement that second-hand smoke was harmful. A 2001 study found that 95% of adults agreed that second-hand smoke was harmful to children, and 96% considered tobacco-industry claims that second-hand smoke was not harmful to be untruthful.\n\nA 2007 Gallup poll found that 56% of respondents felt that second-hand smoke was \"very harmful\", a number that has held relatively steady since 1997. Another 29% believe that second-hand smoke is \"somewhat harmful\"; 10% answered \"not too harmful\", while 5% said \"not at all harmful\".\n\nAs part of its attempt to prevent or delay tighter regulation of smoking, the tobacco industry funded a number of scientific studies and, where the results cast doubt on the risks associated with second-hand smoke, sought wide publicity for those results. The industry also funded libertarian and conservative think tanks, such as the Cato Institute in the United States and the Institute of Public Affairs in Australia which criticised both scientific research on passive smoking and policy proposals to restrict smoking. \"New Scientist\" and the \"European Journal of Public Health\" have identified these industry-wide coordinated activities as one of the earliest expressions of corporate denialism. Further, they state that the disinformation spread by the tobacco industry has created a \"tobacco denialism\" movement, sharing many characteristics of other forms of denialism, such as HIV-AIDS denialism.\n\nA 2003 study by James Enstrom and Geoffrey Kabat, published in the \"British Medical Journal\", argued that the harms of passive smoking had been overstated. Their analysis reported no statistically significant relationship between passive smoking and lung cancer, coronary heart disease (CHD), or chronic obstructive pulmonary disease, though the accompanying editorial noted that \"they may overemphasise the negative nature of their findings.\" This paper was widely promoted by the tobacco industry as evidence that the harms of passive smoking were unproven. The American Cancer Society (ACS), whose database Enstrom and Kabat used to compile their data, criticized the paper as \"neither reliable nor independent\", stating that scientists at the ACS had repeatedly pointed out serious flaws in Enstrom and Kabat's methodology prior to publication. Notably, the study had failed to identify a comparison group of \"unexposed\" persons.\n\nEnstrom's ties to the tobacco industry also drew scrutiny; in a 1997 letter to Philip Morris, Enstrom requested a \"substantial research commitment... in order for me to effectively compete against the large mountain of epidemiologic data and opinions that already exist regarding the health effects of ETS and active smoking.\" In a US racketeering lawsuit against tobacco companies, the Enstrom and Kabat paper was cited by the US District Court as \"a prime example of how nine tobacco companies engaged in criminal racketeering and fraud to hide the dangers of tobacco smoke.\" The Court found that the study had been funded and managed by the Center for Indoor Air Research, a tobacco industry front group tasked with \"offsetting\" damaging studies on passive smoking, as well as by Philip Morris who stated that Enstrom's work was \"clearly litigation-oriented.\" A 2005 paper in \"Tobacco Control\" argued that the disclosure section in the Enstrom and Kabat BMJ paper, although it met the journal's requirements, \"does not reveal the full extent of the relationship the authors had with the tobacco industry.\"\n\nIn 2006, Enstrom and Kabat published a meta-analysis of studies regarding passive smoking and coronary heart disease in which they reported a very weak association between passive smoking and heart disease mortality. They concluded that exposure to second-hand smoke increased the risk of death from CHD by only 5%, although this analysis has been criticized for including two previous industry-funded studies that suffered from widespread exposure misclassification.\n\nGio Batta Gori, a tobacco industry spokesman and consultant and an expert on risk utility and scientific research, wrote in the libertarian Cato Institute's magazine \"Regulation\" that \"...of the 75 published studies of ETS and lung cancer, some 70% did not report statistically significant differences of risk and are moot. Roughly 17% claim an increased risk and 13% imply a reduction of risk.\"\n\nSteven Milloy, the \"junk science\" commentator for Fox News and a former Philip Morris consultant, claimed that \"of the 19 studies\" on passive smoking \"only 8— slightly more than 42%— reported statistically significant increases in heart disease incidence..\"\n\nAnother component of criticism cited by Milloy focused on relative risk and epidemiological practices in studies of passive smoking. Milloy, who has a master's degree from the Johns Hopkins School of Hygiene and Public Health, argued that studies yielding relative risks of less than 2 were meaningless junk science. This approach to epidemiological analysis was criticized in the \"American Journal of Public Health\":\n\nThe tobacco industry and affiliated scientists also put forward a set of \"Good Epidemiology Practices\" which would have the practical effect of obscuring the link between secondhand smoke and lung cancer; the privately stated goal of these standards was to \"impede adverse legislation\". However, this effort was largely abandoned when it became clear that no independent epidemiological organization would agree to the standards proposed by Philip Morris et al.\n\nIn 1995, Levois and Layard, both tobacco industry consultants, published two analyses in the journal \"Regulatory Toxicology and Pharmacology\" regarding the association between spousal exposure to second-hand smoke and heart disease. Both of these papers reported no association between second-hand smoke and heart disease. These analyses have been criticized for failing to distinguish between current and former smokers, despite the fact that former smokers, unlike current ones, are not at a significantly increased risk of heart disease.\n\nA 1998 study by the International Agency for Research on Cancer (IARC) on environmental tobacco smoke (ETS) found \"weak evidence of a dose-response relationship between risk of lung cancer and exposure to spousal and workplace ETS.\"\n\nIn March 1998, before the study was published, reports appeared in the media alleging that the IARC and the World Health Organization (WHO) were suppressing information. The reports, appearing in the British \"Sunday Telegraph\" and \"The Economist\", among other sources, alleged that the WHO withheld from publication of its own report that supposedly failed to prove an association between passive smoking and a number of other diseases (lung cancer in particular).\n\nIn response, the WHO issued a press release stating that the results of the study had been \"completely misrepresented\" in the popular press and were in fact very much in line with similar studies demonstrating the harms of passive smoking. The study was published in the \"Journal of the National Cancer Institute\" in October of the same year, and concluded the authors found \"no association between childhood exposure to ETS and lung cancer risk\" but \"did find weak evidence of a dose–response relationship between risk of lung cancer and exposure to spousal and workplace ETS.\" An accompanying editorial summarized:\n\nWith the release of formerly classified tobacco industry documents through the Tobacco Master Settlement Agreement, it was found (by Elisa Ong and Stanton Glantz) that the controversy over the WHO's alleged suppression of data had been engineered by Philip Morris, British American Tobacco, and other tobacco companies in an effort to discredit scientific findings which would harm their business interests. A WHO inquiry, conducted after the release of the tobacco-industry documents, found that this controversy was generated by the tobacco industry as part of its larger campaign to cut the WHO's budget, distort the results of scientific studies on passive smoking, and discredit the WHO as an institution. This campaign was carried out using a network of ostensibly independent front organizations and international and scientific experts with hidden financial ties to the industry.\n\nIn 1993, the United States Environmental Protection Agency (EPA) issued a report estimating that 3,000 lung cancer related deaths in the United States were caused by passive smoking annually.\n\nPhilip Morris, R.J. Reynolds Tobacco Company, and groups representing growers, distributors and marketers of tobacco took legal action, claiming that the EPA had manipulated this study and ignored accepted scientific and statistical practices.\n\nThe United States District Court for the Middle District of North Carolina ruled in favor of the tobacco industry in 1998, finding that the EPA had failed to follow proper scientific and epidemiologic practices and had \"cherry picked\" evidence to support conclusions which they had committed to in advance. The court stated in part, \"EPA publicly committed to a conclusion before research had begun…adjusted established procedure and scientific norms to validate the Agency's public conclusion... In conducting the ETS Risk Assessment, disregarded information and made findings on selective information; did not disseminate significant epidemiologic information; deviated from its Risk Assessment Guidelines; failed to disclose important findings and reasoning…\"\n\nIn 2002, the EPA successfully appealed this decision to the United States Court of Appeals for the Fourth Circuit. The EPA's appeal was upheld on the preliminary grounds that their report had no regulatory weight, and the earlier finding was vacated.\n\nIn 1998, the U.S. Department of Health and Human Services, through the publication by its National Toxicology Program of the 9th Report on Carcinogens, listed environmental tobacco smoke among the known carcinogens, observing of the EPA assessment that \"The individual studies were carefully summarized and evaluated.\"\n\nThe tobacco industry's role in funding scientific research on second-hand smoke has been controversial. A review of published studies found that tobacco-industry affiliation was strongly correlated with findings exonerating second-hand smoke; researchers affiliated with the tobacco industry were 88 times more likely than independent researchers to conclude that second-hand smoke was not harmful. In a specific example which came to light with the release of tobacco-industry documents, Philip Morris executives successfully encouraged an author to revise his industry-funded review article to downplay the role of second-hand smoke in sudden infant death syndrome. The 2006 U.S. Surgeon General's report criticized the tobacco industry's role in the scientific debate:\n\nThis strategy was outlined at an international meeting of tobacco companies in 1988, at which Philip Morris proposed to set up a team of scientists, organized by company lawyers, to \"carry out work on ETS to keep the controversy alive.\" All scientific research was subject to oversight and \"filtering\" by tobacco-industry lawyers:\n\nPhilip Morris reported that it was putting \"...vast amounts of funding into these projects... in attempting to coordinate and pay so many scientists on an international basis to keep the ETS controversy alive.\"\n\nMeasures to tackle second-hand smoke pose a serious economic threat to the tobacco industry, having broadened the definition of smoking beyond a personal habit to something with a social impact. In a confidential 1978 report, the tobacco industry described increasing public concerns about second-hand smoke as \"the most dangerous development to the viability of the tobacco industry that has yet occurred.\" In \"United States of America v. Philip Morris et al.\", the District Court for the District of Columbia found that the tobacco industry \"... recognized from the mid-1970s forward that the health effects of passive smoking posed a profound threat to industry viability and cigarette profits,\" and that the industry responded with \"efforts to undermine and discredit the scientific consensus that ETS causes disease.\"\n\nAccordingly, the tobacco industry have developed several strategies to minimise the impact on their business:\n\nCiting the tobacco industry's production of biased research and efforts to undermine scientific findings, the 2006 U.S. Surgeon General's report concluded that the industry had \"attempted to sustain controversy even as the scientific community reached consensus... industry documents indicate that the tobacco industry has engaged in widespread activities... that have gone beyond the bounds of accepted scientific practice.\" The U.S. District Court, in \"U.S.A. v. Philip Morris et al.\", found that \"...despite their internal acknowledgment of the hazards of secondhand smoke, Defendants have fraudulently denied that ETS causes disease.\"\n\nThe positions of major tobacco companies on the issue of second-hand smoke is somewhat varied. In general, tobacco companies have continued to focus on questioning the methodology of studies showing that second-hand smoke is harmful. Some (such as British American Tobacco and Philip Morris) acknowledge the medical consensus that second-hand smoke carries health risks, while others continue to assert that the evidence is inconclusive. Several tobacco companies advocate the creation of smoke-free areas within public buildings as an alternative to comprehensive smoke-free laws.\n\nOn September 22, 1999, the U.S. Department of Justice filed a racketeering lawsuit against Philip Morris and other major cigarette manufacturers. Almost 7 years later, on August 17, 2006 U.S. District Court Judge Gladys Kessler found that the Government had proven its case and that the tobacco company defendants had violated the Racketeer Influenced Corrupt Organizations Act (RICO). In particular, Judge Kessler found that PM and other tobacco companies had:\nThe ruling found that tobacco companies undertook joint efforts to undermine and discredit the scientific consensus that second-hand smoke causes disease, notably by controlling research findings via paid consultants. The ruling also concluded that tobacco companies were fraudulently continuing to deny the health effects of ETS exposure.\n\nOn May 22, 2009, a three-judge panel of the U.S. Court of Appeals for the District of Columbia Circuit unanimously upheld the lower court's 2006 ruling.\n\nAs a consequence of the health risks associated with second-hand smoke, smoke-free regulations in indoor public places, including restaurants, cafés, and nightclubs have been introduced in a number of jurisdictions, at national or local level, as well as some outdoor open areas. Ireland was the first country in the world to institute a comprehensive national smoke-free law on smoking in all indoor workplaces on 29 March 2004. Since then, many others have followed suit. The countries which have ratified the WHO Framework Convention on Tobacco Control (FCTC) have a legal obligation to implement \"effective\" legislation \"for protection from exposure to tobacco smoke in indoor workplaces, public transport, indoor public places and, as appropriate, other public places.\" (Article 8 of the FCTC) The parties to the FCTC have further adopted \"Guidelines on the Protection from Exposure to Second-hand Smoke\" which state that \"effective measures to provide protection from exposure to tobacco smoke ... require the total elimination of smoking and tobacco smoke in a particular space or environment in order to create a 100% smoke-free environment.\"\n\nOpinion polls have shown considerable support for smoke-free laws. In June 2007, a survey of 15 countries found 80% approval for smoke-free laws. A survey in France, reputedly a nation of smokers, showed 70% support.\n\nSmoking bans by governments result in decreased harm from second hand smoke including decrease cardiovascular disease. In the first 18 months after the town of Pueblo, Colorado enacted a smoke-free law in 2003, hospital admissions for heart attacks dropped 27%. Admissions in neighbouring towns without smoke-free laws showed no change, and the decline in heart attacks in Pueblo was attributed to the resulting reduction in second-hand smoke exposure. A 2004 smoking ban instituted in Massachusetts workplaces decreased workers' secondhand smoke exposure from 8% of workers in 2003 to 5.4% of workers in 2010. A 2016 review also found benefits of decrease exposure to smoke from specific location policies.\n\nIn 2001, a systematic review for the Guide to Community Preventative Services acknowledged strong evidence of the effectiveness of smoke-free policies and restrictions in reducing expose to second-hand smoke. A follow up to this review, identified the evidence on which the effectiveness of smoking bans reduced the prevalence of tobacco use. Articles published until 2005, were examined to further support this evidence. The examined studies provided sufficient evidence that smoke-free policies reduce tobacco use among workers when implemented in worksites or by communities.\n\nWhile a number of studies funded by the tobacco industry have claimed a negative economic impact from smoke-free laws, no independently funded research has shown any such impact. A 2003 review reported that independently funded, methodologically sound research consistently found either no economic impact or a positive impact from smoke-free laws.\n\nAir nicotine levels were measured in Guatemalan bars and restaurants before and after an implemented smoke-free law in 2009. Nicotine concentrations significantly decreased in both the bars and restaurants measured. Also, the employees support for a smoke-free workplace substantially increased in the post-implementation survey compared to pre-implementation survey. The result of this smoke-free law provides a considerably more healthy work environment for the staff.\n\nRecent surveys taken by the Society for Research on Nicotine and Tobacco demonstrates supportive attitudes of the public, towards smoke-free policies in outdoor areas. A vast majority of the public supports restricting smoking in various outdoor settings. The respondents reasons for supporting the policies were for varying reasons such as, litter control, establishing positive smoke-free role models for youth, reducing youth opportunities to smoke, and avoiding exposure to secondhand smoke.\n\nAlternatives to smoke-free laws have also been proposed as a means of harm reduction, particularly in bars and restaurants. For example, critics of smoke-free laws cite studies suggesting ventilation as a means of reducing tobacco smoke pollutants and improving air quality. Ventilation has also been heavily promoted by the tobacco industry as an alternative to outright bans, via a network of ostensibly independent experts with often undisclosed ties to the industry. However, not all critics have connections to the industry.\n\nThe American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) officially concluded in 2005 that while completely isolated smoking rooms do eliminate the risk to nearby non-smoking areas, smoking bans are the only means of completely eliminating health risks associated with indoor exposure. They further concluded that no system of dilution or cleaning was effective at eliminating risk. The U.S. Surgeon General and the European Commission Joint Research Centre have reached similar conclusions. The implementation guidelines for the WHO Framework Convention on Tobacco Control states that engineering approaches, such as ventilation, are ineffective and do not protect against second-hand smoke exposure. However, this does \"not\" necessarily mean that such measures are useless in reducing harm, only that they fall short of the goal of reducing exposure completely to zero.\n\nOthers have suggested a system of tradable smoking pollution permits, similar to the cap-and-trade pollution permits systems used by the Environmental Protection Agency in recent decades to curb other types of pollution. This would guarantee that a portion of bars/restaurants in a jurisdiction will be smoke-free, while leaving the decision to the market.\n\nMultiple studies have been conducted to determine the carcinogenicity of environmental tobacco smoke to animals. These studies typically fall under the categories of simulated environmental tobacco smoke, administering condensates of sidestream smoke, or observational studies of cancer among pets.\n\nTo simulate environmental tobacco smoke, scientists expose animals to sidestream smoke, that which emanates from the cigarette's burning cone and through its paper, or a combination of mainstream and sidestream smoke. The IARC monographs conclude that mice with prolonged exposure to simulated environmental tobacco smoke, that is 6hrs a day, 5 days a week, for five months with a subsequent 4 month interval before dissection, will have significantly higher incidence and multiplicity of lung tumors than with control groups.\n\nThe IARC monographs concluded that sidestream smoke condensates had a significantly higher carcinogenic effect on mice than did mainstream smoke condensates.\n\nSecond-hand smoke is popularly recognised as a risk factor for cancer in pets. A study conducted by the Tufts University School of Veterinary Medicine and the University of Massachusetts Amherst linked the occurrence of feline oral cancer to exposure to environmental tobacco smoke through an overexpression of the p53 gene. Another study conducted at the same universities concluded that cats living with a smoker were more likely to get feline lymphoma; the risk increased with the duration of exposure to secondhand smoke and the number of smokers in the household. A study by Colorado State University researchers, looking at cases of canine lung cancer, was generally inconclusive, though the authors reported a weak relation for lung cancer in dogs exposed to environmental tobacco smoke. The number of smokers within the home, the number of packs smoked in the home per day, and the amount of time that the dog spent within the home had no effect on the dog's risk for lung cancer.\n\nAs of 2003, \"secondhand smoke\" was the term most used to refer to other people's smoke in the English-language media. Other terms used include \"environmental tobacco smoke\", while \"involuntary smoking\" and \"passive smoking\" are used to refer to exposure to secondhand smoke. The term \"environmental tobacco smoke\" can be traced back to a 1974 industry-sponsored meeting held in Bermuda, while the term \"passive smoking\" was first used in the title of a scientific paper in 1970. The Surgeon General of the United States prefers to use the phrase \"secondhand smoke\" rather than \"environmental tobacco smoke\", stating that \"The descriptor \"secondhand\" captures the involuntary nature of the exposure, while \"environmental\" does not.\" Most researchers consider the term \"passive smoking\" to be synonymous with \"secondhand smoke\". In contrast, a 2011 commentary in \"Environmental Health Perspectives\" argued that research into \"thirdhand smoke\" renders it inappropriate to refer to passive smoking with the term \"secondhand smoke\", which the authors stated constitutes a pars pro toto.\n\n\n\n\n\n\n"}
{"id": "19686261", "url": "https://en.wikipedia.org/wiki?curid=19686261", "title": "Pathway India", "text": "Pathway India\n\nPathway is a voluntary, charitable, nonprofit and non-governmental organisation, based in Chennai, India - which serves children and adults without any bias to religion, caste, creed, or any other consideration. Founded in 1975 by Dr. ADSN Prasad, now provides services to children with mental and learning disabilities, adult mental handicap, and orphaned and destitute children of rural south India.\n\nGuided by Dr. ADSN Prasad's philosophy - \"Every individual should be given the opportunity to utilise their potential in order to live with dignity and self-respect, regardless of mental or physical limitations\", Pathway seeks to serve the poorest of the poor by providing the best\n\nPathway began in 1975 with just two children in a small, rented house in Chennai, India. sought to fill the void existing in the care and rehabilitation of mentally ill children. From these humble beginnings grew an organisation that has served almost 22,000 children and adults in two city centers and a rural agricultural farm.\n\n1. Pathway Centre for Rehabilitation & Education of the Mentally Retarded - Chennai\n2. Pathway Centre for Vocational Training & Education - Koothavakkam\n\nPurpose of the above two facilities is to provide comprehensive care and educational opportunities for mentally ill children. Pathway is founded on the philosophy that every individual should be given the opportunity to utilise their potential in order to live with dignity and self-respect, regardless of mental or physical limitations. Today over 100 children receive full care at the live-in facility, while another 300 children are seen on daily basis for rehabilitation and educational programmes.\n\nIn Pathway, all children are served for free, without any charges. Prior to being admitted to the centre's Day or Residential program, each child is thoroughly screened and evaluated to assess their level of mental, neurological and physical development. These methods of testing and observation assist in designing a special education and vocational programme for each child.\n\nPathway's goal is to offer medical care, special education and vocational training to as many mentally and physically disabled children and adults as possible. During the last 27 years Pathway has served more than 14,000 children and adults with various problems and disabilities.\n\n3. Pamela M.Martinez/Pathway Agro-Farm For Children \nPamela Martinez/Pathway Agro Farm for Children named after its donor is located in a large agro farm in Agili, Madhuranthakam Taluk, Kancheepuram District. This farm houses a large orphanage, home to more than 200 children, a well organised and well run English medium Matriculation Residential school, recognised by the Directorate of Matriculation Schools, Govt. of Tamil Nadu. The children are offered high class facilities such as swimming, yoga, music, good recreational facilities, quality education, total medical care, etc. in an eco-friendly environment. A \"HOSPITAL ON WHEELS\" is also run from this facility serving a large number of poor villagers around the agro farm.\n\nThe organisation has great plans to help hundreds of needy children in all ways, we need your help in our humble endeavour. This facility will be the first one to use agriculture as a major medium of therapy and it can offer a rare opportunity to children to grow their own food and learn to use modern cultivating techniques.\n\nThis facility is planned to assist the disabled and disadvantaged children of India.\n\n\n\n\n"}
{"id": "27473637", "url": "https://en.wikipedia.org/wiki?curid=27473637", "title": "Perichondritis", "text": "Perichondritis\n\nPerichondritis is inflammation of the perichondrium, a layer of connective tissue which surrounds cartilage. A common form, auricular perichondritis (\"perichondritis auriculae\") involves infection of the pinna due to infection of traumatic or surgical wound or the spread of inflammation into depth. It may lead to severe deformation of the pinna if not treated vigorously with IV antibiotics. The causative organism is usually \"Pseudomonas aeruginosa\". A rare form is laryngeal perichondritis (\"perichondritis laryngis\"). It develops suddenly due to an injury, virulent organisms or compromised immune status of the host, and also affects cartilage of the larynx. This may result in deformations and stenoses.\n\nIntense pain in auricle. Auricles become red and thick. Body temperature rises. In serious cases pus appears between the perichondrium and cartillage. Purulent melting of auricular cartilage takes place. Dead tissue tears away, as a result auricle deforms strongly and becomes shrunken.\n"}
{"id": "14220244", "url": "https://en.wikipedia.org/wiki?curid=14220244", "title": "Potentiometric surface", "text": "Potentiometric surface\n\nA potentiometric surface is the imaginary plane where a given reservoir of fluid will \"equalize out to\" if allowed to flow. A potentiometric surface is based on hydraulic principles. For example, we know that two connected storage tanks with one full and one empty will gradually fill/drain to the same level. This is because of atmospheric pressure and gravity. This idea is heavily used in city water supplies - a tall water tower containing the water supply has a great enough potentiometric surface to provide flowing water at a decent pressure to the houses it supplies.\n\nFor groundwater \"potentiometric surface\" is a synonym of \"piezometric surface\" which is an imaginary surface that defines the level to which water in a confined aquifer would rise were it completely pierced with wells. If the potentiometric surface lies above the ground surface, a flowing artesian well results. Contour maps and profiles of the potentiometric surface can be prepared from the well data.\n\nHydraulic head\n\n"}
{"id": "709333", "url": "https://en.wikipedia.org/wiki?curid=709333", "title": "Riddick (character)", "text": "Riddick (character)\n\nRichard B. Riddick, more commonly known as Riddick, is a fictional character and the antiheroic protagonist of the \"Riddick\" series (\"Pitch Black\", \"The Chronicles of Riddick\", the animated movie \"\", and \"Riddick\"), as well as the two video games \"\" and \"\". Actor Vin Diesel has played the title role in all of the Riddick-based films and video games so far.\nWithin the canon of the series, Riddick is shown to be a highly skilled predator—he is extremely mobile and stealthy - especially for someone of his size, has a vast knowledge of how to kill almost any humanoid in a variety of ways, is an extreme survivalist, and is notoriously hard to contain. He is also self-admittedly a dangerous convict and murderer—yet despite this, he is sometimes shown to perform moral or even atypically heroic actions, usually against his own better judgment and survivalist nature.\n\nRiddick is a Human/Furyan hybrid, a member of a warrior race obliterated by a military campaign that left Furya desolate, and is one of the last of his kind. One of his most defining features are his eyes, a characteristic inherent in a certain caste of his species (the Alpha-Furyans), although he implies in \"Pitch Black\" that they were \"shined\" by a back-alley surgical operation. This allows him to see clearly in the dark, but also makes him vulnerable to strong light; he wears tinted welding goggles for protection.\n\nRiddick was once a mercenary, then part of a security force, and later a soldier. He is also an experienced pilot.\n\nRiddick was born on Furya. Before Riddick's birth, a Necromonger officer named Zhylaw (who would eventually become the Lord Marshall) consulted an Elemental seer, who told him that a Furyan male would be born that would be responsible for his downfall. This prompted Zhylaw to attack Furya and attempt to massacre all male children, going so far as to personally strangle newborn infants with their own umbilical cords. Riddick was among those presumably strangled, but he survived.\n\nIn later life, Riddick repressed his memories of Furya, including those of the massacre. He came to believe that when he was born, his mother attempted to strangle him and left him in a trash bin behind a liquor store. Despite this, he periodically received visions and messages from a Furyan survivor named Shirah, which he thought were signs of mental instability. \n\nLittle is known about Riddick's childhood, save that he had no formal education and seemed to have been in trouble with the law from an early age. He claims to have been \"educated\" in the penal system.\n\nIn the premiere installment in the franchise, Riddick is introduced as notorious convict en route to a penal institution with his captor, a bounty hunter named William J. Johns. By impersonating a cop and traveling with a group of settlers aboard the nondescript transport cruiser \"Hunter Gratzner\", Johns hopes to safely convey Riddick back to a prison colony and collect the outlaw's enormous bounty. However, the ship inadvertently passes through a comet's tail while on autopilot, forcing it to crash land on an unknown planet with three suns. Soon the survivors find themselves fighting for their lives against a predatory species called Bio-Raptors, and by the end of the film there are only three survivors: Riddick, a young girl called Jack, and Imam.\n\nFive years later, Riddick has remained in hiding on an Arctic planet called U.V. for its harsh Ultraviolet light. Meanwhile, Imam, learning that his own planet, Helion Prime, will be invaded by the twisted religious crusade of the Necromongers, recalls the story Riddick told him of his Furyan origins, and betrays Riddick's location to the Helion Prime leaders. Initially, Riddick is reluctant to become involved in the struggle between the Necromongers and the world they seek to conquer. But when Imam is killed, Riddick sets out on a mission to avenge him, eventually overthrowing the Lord Marshal (with help from Jack, who dies in the process) and inheriting his office.\n\nDissatisfied with his role as Lord Marshal, Riddick attempts to find Furya, but is instead abandoned on a desolate planet. After spending some time on this world, Riddick finds that he will soon be overtaken by a vicious storm that provides cover for countless predators. Using the communication system from a mercenary outpost, Riddick lures two ships to the planet, intending to commandeer one of them; but his plan is complicated when he finds that one of the arriving mercenaries is the father of his old enemy, Johns the bounty hunter. Riddick uses the approaching storm to force the mercenaries to cooperate, but he is left surrounded by enemies and nearly killed. To his surprise, the elder Johns rescues him and allows him to leave. Riddick praises Johns for being a better man than his son.\n\nAccording to Riddick, while in Butcher Bay he received eye surgery (a \"surgical shine job\", as he calls it) from a doctor who gave him permanent night vision in exchange for \"20 menthol KOOLs\". The surgery made his eyes more sensitive to normal light, requiring him to wear welding goggles to protect them (although he has also been shown without the goggles in a regularly illuminated room).\n\nIn the flash movie on the Pitchblack.com website, Riddick gets the eye-shine to give him more of an advantage following an encounter with humanoids called \"shiners\". Referring to Riddick as \"darkeye\", they have had the same operation to see in the dark bowels of the Ursa Luna prison where the guards do not tend to go. The eye-shine surgery seems to be performed by a bovine veterinarian on board a prison facility where Riddick arrived only a few hours before and from which he is already in the process of escaping. Riddick elects to have no anesthetic, despite the procedure involving cutting the cornea and drilling through the eye to inject a reflective substance behind the retina. The cost quoted to Riddick is 1000 creds. Having no cash, Riddick offers down payment of a pack of Kool cigarettes which he earlier took from a guard. He also takes some welder's goggles from the facility.\n\nThe video game, \"\" (which serves as a prequel to \"Pitch Black\"), clarifies that Riddick's eyeshine is more than just something he picked up in a prison. After helping a character called \"Pope Joe\" retrieve his \"blessed voice box\", a radio that picks up religious programming (including a Necromonger Talk Show), Riddick goes into a den to get stitches for an injury. After he is finished receiving stitches, Joe tells him how to escape, and, perhaps coincidentally, warns Riddick not to \"trust [his] eyes\"; at that moment, a ghostly voice informs Riddick that he has \"been blind for far too long...\" and that he is to receive a gift. It is at this moment that Riddick receives his eyeshine. The mysterious voice belongs to a character named \"Shirah\", who appears to serve as a sort of spiritual guide to Riddick, helping him awaken the Furyan abilities that lie dormant within him. In the \"Chronicles of Riddick\" film, Jack/Kyra angrily tells Riddick that when she was sent to prison, she found out that it is impossible to find anyone who can perform a \"surgical shine job\" at any price, and accuses him of lying about how he received his night vision.\n\nLight amplification surgery, somewhat similar in effect to Riddick's eyeshine, has been patented in real life.\n\nAside from helping Riddick unlock his eyeshine, the character Shirah also either allows or helps Riddick discover his ability to unleash a sort of energy wave. It is shown in the director's cut of \"The Chronicles of Riddick\" after she lays her hand on Riddick's chest, leaving a glowing blue hand print, and says, \"this mark carries the anger of an entire race... but it's going to hurt\". After either receiving a shot from Vaako's energy handgun or a fraction of a second before the discharge of said weapon, she immediately emits a large blast of blue energy from Riddick (or Riddick himself does so with her guidance), leaving the group of surrounding Necromongers dead. This ability is also displayed off screen in \"\".\n\nAside from his more supernatural skills, Riddick is in superlative physical condition and is an exceptional fighter with or without the use of his eyes. Whether as a result of his Furyan heritage or simply training, Riddick is stronger, faster, more agile, tougher, more resistant to damage and injury, possessing more acute senses, immense stamina, and superior healing when compared to most humans; he is shown on more than one occasion dislocating his limbs for brief moments with only slight signs of discomfort, although he was in more obvious pain when his leg was broken in a fall, and had to use a heated rock to cauterize a wound when he was impaled in the chest by a creature's sharpened tooth. In general, Riddick possesses an abnormally high threshold of pain and psychologically channels what pain he does feel into anger. His ability to cope with toxins is also heightened; when a merc team was attempting to take him out with horse tranquilizers, it took four darts just to make Riddick stop running, and he still remained conscious until he was hit in the head. His ability to cope with pain, stress and situational panic is most likely the main reason behind his success, which combined with his sharp intelligence gives Riddick the ability to escape from any situation.\n\nRiddick's abilities at hand-to-hand combat is a mix of Krav Maga, Hapkido, Eskrima, and Ninjutsu; he uses his formidable fighting skills while confronting one of the night-creatures of \"Pitch Black\" with only his bare hands and a shiv he had made, killing it with relative ease, as well as defeating one of the Lord Marshall's best Necromonger warriors in a matter of seconds in \"Chronicles\". Also, he was able to hold off the Lord Marshall and even injure him which the Lord Marshall admitted hadn't happened in a long time. While Riddick was no match for him in speed, he was able to hold off the Lord Marshall for several minutes.\n\nDespite his harsh upbringing and violent attitude, Riddick has been shown to have a certain knack for deduction, rapidly deducing what had happened to the original inhabitants of the mining base where he and the other crash survivors had been staying, as well as finding the creature's blind spot in \"Pitch Black\" and swiftly deducing what had caused the firefight between the Warden's and Toombs' men in \"The Chronicles of Riddick\". He has also been described as having \"a knack for escape\", surviving not only the Lord Marshall's purge of the Furyans when he was an infant but subsequently escaping from various prisons over the course of his life. He is also able to quickly judge the quality of, and find even seemingly minor flaws in most weaponry, down to very specific details. In one case, Riddick concluded that the prized dagger of a Necromonger was \"a half gram too heavy on the back end\" after an inspection lasting all of a couple of seconds, although the tone of voice in which he delivered this appraisal suggested pre-fight bravado rather than abstract fact. His only weakness (other than light sensitivity, leaving him in pain when in daylight without his goggles), as stated in the \"Pitch Black\" DVD, is his soft spot for children and anyone who really grows on him (those people become his friends), which resulted in his capture by the bounty hunter Johns (Cole Hauser). While he may be a ruthless killer when necessary, Riddick has never been shown to kill anyone who was not actively trying to kill him first, and has been known to help people in need only if they don't slow him down or make him vulnerable. The only exception to this was when, in \"Pitch Black\", he returned with Carolyn to help the others, after she pleaded with him. He also mentions on board the Dark Athena that people he helps often end up dead. In \"The Chronicles of Riddick\", he considered it an insult, when he discovered the bounty placed on him was around a million of the currency for most planets, and when Toombs has a small crew to capture him (it was originally four crew members, then it was five).\n\nIn addition to raw analytical power, Riddick is generally an astute judge of character, especially in the dark side of human nature. A killer himself and resident of several maximum security facilities, he is often able to predict an individual's negative impulses before they act on them. He has been known to be surprised on occasion, such as when Carolyn Fry in \"Pitch Black\" altruistically gave up a fast escape to save Jack and Imam from the alien creatures. Riddick was also surprised when he underestimated Jack's (Kyra's) affection toward him between \"Pitch Black\" and \"The Chronicles of Riddick\", only learning of it after she had become a mercenary and resident of a maximum security prison in order to see him once again. Otherwise, he correctly predicted which of the group in \"Pitch Black\" would turn on each other (in particular that Johns would try to double-cross Fry), and that the mercs and guards on Crematoria would turn on each other prior to his escape attempt. In \"Riddick\", he appeared pleasantly surprised to learn that Johns' father was a better man than his son, complimenting Johns' nerve when he came back to help Riddick when he didn't have to.\n\nOne major characteristic of Riddick is his indomitable will. Despite finding himself in situations where the odds of survival and/or escape would seem insurmountable, Riddick always pushes forward and never gives up. Faced with several creatures far more powerful than himself, or any human for that matter, he has shown himself to be capable of evading, killing, or even taming them. He has escaped out of prisons when most prisoners would resign themselves to captivity. When Riddick was mentally probed by the Quasi-Dead of the Necromongers, he not only resisted them, but also retaliated, the Quasi-Dead's containers actually exploding from the strain of trying to process Riddick. Given the Necromongers' apparent faith in and respect for the power of the Quasi-Dead, this is a very impressive feat. Perhaps even greater was his ability to resist the Lord Marshall's attempt to steal his soul. The one time his will seemed to waver was when Kyra was killed, sacrificing herself to save him and help him defeat the Lord Marshal, and even then he channeled his loss into new strength.\n\nRiddick is deadly with any weapon that he can get his hands on, but his weapon of choice is undoubtedly a knife. However, Riddick's most famous weapons are surely a pair of Ulak blades that he is seen using during his escape from Crematoria in \"The Chronicles of Riddick\".\n\nHe has also been known to use anything from Necromonger gravity rifles, teacups and anything in between. Riddick can also kill efficiently without using a weapon. In the second film, he used the knife he took from Irgun as his weapon during his fight with him and the Lord Marshall, killing both with it. The third film sees him predominately relying on a bone-sword he made from the remains of a creature he had killed.\n\nRiddick's first appearance outside \"Pitch Black\" was a guest appearance and potential recruit in \"Fallout Tactics\". He appears in the game during a random special encounter, aptly titled \"Pitch Black\". Like his movie origin, Riddick is a stealth focused melee fighter with advanced hand-to-hand fighting skills and the ability to see in the dark.\n\nRiddick has appeared as a playable character in the Xbox game \"Deathrow\". He is one of the final players available on the Convicts team and has the highest amount of strength and aggression of all recruitable Convict players, although he has the equal-lowest teamplay rating in the game.\n\nBehind the scenes Richard B. Riddick first appeared in \"Pitch Black: Slam City\", a Shockwave prequel comic on the official \"Pitch Black\" website, one month before the release of the movie. In the film, and in all subsequent appearances the character has been portrayed by Vin Diesel.\n\nIn Jim and Ken Wheat's original script for \"Pitch Black\", the Riddick character was a woman named Taras Krieg.\n"}
{"id": "30427940", "url": "https://en.wikipedia.org/wiki?curid=30427940", "title": "Rooftop water tower", "text": "Rooftop water tower\n\nA rooftop water tower is a variant of a water tower, consisting of a water container placed on the roof of a tall building.\n\nThis structure supplies water pressure to floors at higher elevation than public water towers.\n\nAs building height increases, the vertical height of its plumbing also increases. This produces a large water column and the weight of this water produces very high pressure at the bottom of the column. Normally, this would require very thick (heavy schedule) plumbing to survive the pressure. Fittings at the bottom of the column would need pressure reducing valves to operate normally, and municipal water pressure would need to be very high to supply pressure to the top of the column.\n\nInstead, the plumbing at various levels of the building are often sequestered, with pressure supplied from a rooftop water tower instead of the municipal supply. The tower itself is fed by a pump and a relatively high pressure line that carries water to the top of the building from the pipes below.\n\nMany have not been cleaned or inspected in years and regulations governing water tanks are rarely enforced. E. coli and coliform can be found inside tanks.\n"}
{"id": "37228954", "url": "https://en.wikipedia.org/wiki?curid=37228954", "title": "Sein Myint", "text": "Sein Myint\n\nSein Myint () is a Burmese politician and former political prisoner In the Burmese general election, 1990, he was elected as an Pyithu Hluttaw member of parliament, winning a majority of 28,259 (55% of the votes), but was never allowed to assume his seat.\n\nIn 1980, he obtained a medical degree (MBBS) from the Rangoon Institute of Medicine and ran a private clinic from 1981 to 1989.\n\nFrom 9 August to 30 October 1989, he was sentenced to the Bassein prison under the 1975 State Protection Act's Article 10a, for his involvement in organizing a trip by Aung San Suu Kyi to the Irrawaddy Division. From November 1991 to January 1992, he was arrested for allegedly participating in the Karen National Union's underground movement.\n"}
{"id": "35210340", "url": "https://en.wikipedia.org/wiki?curid=35210340", "title": "Sex cord tumour with annular tubules", "text": "Sex cord tumour with annular tubules\n\nSex cord tumour with annular tubules, commonly referred to by its abbreviation SCTAT, is a rare ovarian tumour in the sex cord group of gonadal tumours. It may be spelled sex cord tumor with annular tubules.\n\nThese tumours may be seen in the context of Peutz-Jeghers syndrome or be sporadic. Large tumours are more likely to be sporadic. Small incidental tumours are more likely to be syndromic.\n\nIt has a distinctive appearance under the microscope, from which it derives its name.\n\nGenerally, they are considered to benign; however, occasional malignant cases are reported.\n\n"}
{"id": "27577", "url": "https://en.wikipedia.org/wiki?curid=27577", "title": "Statistical inference", "text": "Statistical inference\n\nStatistical inference is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.\n\nInferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.\n\nStatistical inference makes propositions about a population, using data drawn from the population with some form of sampling. Given a hypothesis about a population, for which we wish to draw inferences, statistical inference consists of (first) selecting a statistical model of the process that generates the data and (second) deducing propositions from the model.\n\nKonishi & Kitagawa state, \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling\". Relatedly, Sir David Cox has said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\".\n\nThe conclusion of a statistical inference is a statistical proposition. Some common forms of statistical proposition are the following:\n\nAny statistical inference requires some assumptions. A statistical model is a set of assumptions concerning the generation of the observed data and similar data. Descriptions of statistical models usually emphasize the role of population quantities of interest, about which we wish to draw inference. Descriptive statistics are typically used as a preliminary step before more formal inferences are drawn.\n\nStatisticians distinguish between three levels of modeling assumptions;\n\nWhatever level of assumption is made, correctly calibrated inference in general requires these assumptions to be correct; i.e. that the data-generating mechanisms really have been correctly specified.\n\nIncorrect assumptions of 'simple' random sampling can invalidate statistical inference. More complex semi- and fully parametric assumptions are also cause for concern. For example, incorrectly assuming the Cox model can in some cases lead to faulty conclusions. Incorrect assumptions of Normality in the population also invalidates some forms of regression-based inference. The use of any parametric model is viewed skeptically by most experts in sampling human populations: \"most sampling statisticians, when they deal with confidence intervals at all, limit themselves to statements about [estimators] based on very large samples, where the central limit theorem ensures that these [estimators] will have distributions that are nearly normal.\" In particular, a normal distribution \"would be a totally unrealistic and catastrophically unwise assumption to make if we were dealing with any kind of economic population.\" Here, the central limit theorem states that the distribution of the sample mean \"for very large samples\" is approximately normally distributed, if the distribution is not heavy tailed.\n\nGiven the difficulty in specifying exact distributions of sample statistics, many methods have been developed for approximating these.\n\nWith finite samples, approximation results measure how close a limiting distribution approaches the statistic's sample distribution: For example, with 10,000 independent samples the normal distribution approximates (to two digits of accuracy) the distribution of the sample mean for many population distributions, by the Berry–Esseen theorem.\nYet for many practical purposes, the normal approximation provides a good approximation to the sample-mean's distribution when there are 10 (or more) independent samples, according to simulation studies and statisticians' experience. Following Kolmogorov's work in the 1950s, advanced statistics uses approximation theory and functional analysis to quantify the error of approximation. In this approach, the metric geometry of probability distributions is studied; this approach quantifies approximation error with, for example, the Kullback–Leibler divergence, Bregman divergence, and the Hellinger distance.\n\nWith indefinitely large samples, limiting results like the central limit theorem describe the sample statistic's limiting distribution, if one exists. Limiting results are not statements about finite samples, and indeed are irrelevant to finite samples. However, the asymptotic theory of limiting distributions is often invoked for work with finite samples. For example, limiting results are often invoked to justify the generalized method of moments and the use of generalized estimating equations, which are popular in econometrics and biostatistics. The magnitude of the difference between the limiting distribution and the true distribution (formally, the 'error' of the approximation) can be assessed using simulation. The heuristic application of limiting results to finite samples is common practice in many applications, especially with low-dimensional models with log-concave likelihoods (such as with one-parameter exponential families).\n\nFor a given dataset that was produced by a randomization design, the randomization distribution of a statistic (under the null-hypothesis) is defined by evaluating the test statistic for all of the plans that could have been generated by the randomization design. In frequentist inference, randomization allows inferences to be based on the randomization distribution rather than a subjective model, and this is important especially in survey sampling and design of experiments. Statistical inference from randomized studies is also more straightforward than many other situations. In Bayesian inference, randomization is also of importance: in survey sampling, use of sampling without replacement ensures the exchangeability of the sample with the population; in randomized experiments, randomization warrants a missing at random assumption for covariate information.\n\nObjective randomization allows properly inductive procedures.\nMany statisticians prefer randomization-based analysis of data that was generated by well-defined randomization procedures. (However, it is true that in fields of science with developed theoretical knowledge and experimental control, randomized experiments may increase the costs of experimentation without improving the quality of inferences.)\nSimilarly, results from randomized experiments are recommended by leading statistical authorities as allowing inferences with greater reliability than do observational studies of the same phenomena.\nHowever, a good observational study may be better than a bad randomized experiment.\n\nThe statistical analysis of a randomized experiment may be based on the randomization scheme stated in the experimental protocol and does not need a subjective model.\n\nHowever, at any time, some hypotheses cannot be tested using objective statistical models, which accurately describe randomized experiments or random samples. In some cases, such randomized studies are uneconomical or unethical.\n\nIt is standard practice to refer to a statistical model, often a linear model, when analyzing data from randomized experiments. However, the randomization scheme guides the choice of a statistical model. It is not possible to choose an appropriate model without knowing the randomization scheme. Seriously misleading results can be obtained analyzing data from randomized experiments while ignoring the experimental protocol; common mistakes include forgetting the blocking used in an experiment and confusing repeated measurements on the same experimental unit with independent replicates of the treatment applied to different experimental units.\n\nDifferent schools of statistical inference have become established. These schools—or \"paradigms\"—are not mutually exclusive, and methods that work well under one paradigm often have attractive interpretations under other paradigms.\n\nBandyopadhyay & Forster describe four paradigms: \"(i) classical statistics or error statistics, (ii) Bayesian statistics, (iii) likelihood-based statistics, and (iv) the Akaikean-Information Criterion-based statistics\". The classical (or frequentist) paradigm, the Bayesian paradigm, and the AIC-based paradigm are summarized below. The likelihood-based paradigm is essentially a sub-paradigm of the AIC-based paradigm.\n\nThis paradigm calibrates the plausibility of propositions by considering (notional) repeated sampling of a population distribution to produce datasets similar to the one at hand. By considering the dataset's characteristics under repeated sampling, the frequentist properties of a statistical proposition can be quantified—although in practice this quantification may be challenging.\n\n\nOne interpretation of frequentist inference (or classical inference) is that it is applicable only in terms of frequency probability; that is, in terms of repeated sampling from a population. However, the approach of Neyman develops these procedures in terms of pre-experiment probabilities. That is, before undertaking an experiment, one decides on a rule for coming to a conclusion such that the probability of being correct is controlled in a suitable way: such a probability need not have a frequentist or repeated sampling interpretation. In contrast, Bayesian inference works in terms of conditional probabilities (i.e. probabilities conditional on the observed data), compared to the marginal (but conditioned on unknown parameters) probabilities used in the frequentist approach.\n\nThe frequentist procedures of significance testing and confidence intervals can be constructed without regard to utility functions. However, some elements of frequentist statistics, such as statistical decision theory, do incorporate utility functions. In particular, frequentist developments of optimal inference (such as minimum-variance unbiased estimators, or uniformly most powerful testing) make use of loss functions, which play the role of (negative) utility functions. Loss functions need not be explicitly stated for statistical theorists to prove that a statistical procedure has an optimality property. However, loss-functions are often useful for stating optimality properties: for example, median-unbiased estimators are optimal under absolute value loss functions, in that they minimize expected loss, and least squares estimators are optimal under squared error loss functions, in that they minimize expected loss.\n\nWhile statisticians using frequentist inference must choose for themselves the parameters of interest, and the estimators/test statistic to be used, the absence of obviously explicit utilities and prior distributions has helped frequentist procedures to become widely viewed as 'objective'.\n\nThe Bayesian calculus describes degrees of belief using the 'language' of probability; beliefs are positive, integrate to one, and obey probability axioms. Bayesian inference uses the available posterior beliefs as the basis for making statistical propositions. There are several different justifications for using the Bayesian approach.\n\n\nMany informal Bayesian inferences are based on \"intuitively reasonable\" summaries of the posterior. For example, the posterior mean, median and mode, highest posterior density intervals, and Bayes Factors can all be motivated in this way. While a user's utility function need not be stated for this sort of inference, these summaries do all depend (to some extent) on stated prior beliefs, and are generally viewed as subjective conclusions. (Methods of prior construction which do not require external input have been proposed but not yet fully developed.)\n\nFormally, Bayesian inference is calibrated with reference to an explicitly stated utility, or loss function; the 'Bayes rule' is the one which maximizes expected utility, averaged over the posterior uncertainty. Formal Bayesian inference therefore automatically provides optimal decisions in a decision theoretic sense. Given assumptions, data and utility, Bayesian inference can be made for essentially any problem, although not every statistical inference need have a Bayesian interpretation. Analyses which are not formally Bayesian can be (logically) incoherent; a feature of Bayesian procedures which use proper priors (i.e. those integrable to one) is that they are guaranteed to be coherent. Some advocates of Bayesian inference assert that inference \"must\" take place in this decision-theoretic framework, and that Bayesian inference should not conclude with the evaluation and summarization of posterior beliefs.\n\nThe \"Akaike information criterion\" (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection.\n\nAIC is founded on information theory: it offers an estimate of the relative information lost when a given model is used to represent the process that generated the data. (In doing so, it deals with the trade-off between the goodness of fit of the model and the simplicity of the model.)\n\nThe minimum description length (MDL) principle has been developed from ideas in information theory and the theory of Kolmogorov complexity. The (MDL) principle selects statistical models that maximally compress the data; inference proceeds without assuming counterfactual or non-falsifiable \"data-generating mechanisms\" or probability models for the data, as might be done in frequentist or Bayesian approaches.\n\nHowever, if a \"data generating mechanism\" does exist in reality, then according to Shannon's source coding theorem it provides the MDL description of the data, on average and asymptotically. In minimizing description length (or descriptive complexity), MDL estimation is similar to maximum likelihood estimation and maximum a posteriori estimation (using maximum-entropy Bayesian priors). However, MDL avoids assuming that the underlying probability model is known; the MDL principle can also be applied without assumptions that e.g. the data arose from independent sampling.\n\nThe MDL principle has been applied in communication-coding theory in information theory, in linear regression, and in data mining.\n\nThe evaluation of MDL-based inferential procedures often uses techniques or criteria from computational complexity theory.\n\nFiducial inference was an approach to statistical inference based on fiducial probability, also known as a \"fiducial distribution\". In subsequent work, this approach has been called ill-defined, extremely limited in applicability, and even fallacious. However this argument is the same as that which shows that a so-called confidence distribution is not a valid probability distribution and, since this has not invalidated the application of confidence intervals, it does not necessarily invalidate conclusions drawn from fiducial arguments. An attempt was made to reinterpret the early work of Fisher's fiducial argument as a special case of an inference theory using Upper and lower probabilities.\n\nDeveloping ideas of Fisher and of Pitman from 1938 to 1939, George A. Barnard developed \"structural inference\" or \"pivotal inference\", an approach using invariant probabilities on group families. Barnard reformulated the arguments behind fiducial inference on a restricted class of models on which \"fiducial\" procedures would be well-defined and useful.\n\nThe topics below are usually included in the area of statistical inference.\n\n\n\n\n"}
{"id": "2994579", "url": "https://en.wikipedia.org/wiki?curid=2994579", "title": "Systematic review", "text": "Systematic review\n\nSystematic reviews are a type of literature review that uses systematic methods to collect secondary data, critically appraise research studies, and synthesize studies. Systematic reviews formulate research questions that are broad or narrow in scope, and identify and synthesize studies that directly relate to the systematic review question. They are designed to provide a complete, exhaustive summary of current evidence relevant to a research question. Systematic reviews of randomized controlled trials are key to the practice of evidence-based medicine, and a review of existing studies is often quicker and cheaper than embarking on a new study.\n\nAn understanding of systematic reviews, and how to implement them in practice, is highly recommended for professionals involved in the delivery of health care. Besides health interventions, systematic reviews may examine clinical tests, public health interventions, environmental interventions, social interventions, adverse effects, and economic evaluations. Systematic reviews are not limited to medicine and are quite common in all other sciences where data are collected, published in the literature, and an assessment of methodological quality for a precisely defined subject would be helpful.\n\nA systematic review aims to provide a complete, exhaustive summary of current literature relevant to a research question. The first step in conducting a systematic review is to create a structured question to guide the review. The second step is to perform a thorough search of the literature for relevant papers. The \"Methodology\" section of a systematic review will list all of the databases and citation indexes that were searched such as Web of Science, Embase, and PubMed and any individual journals that were searched. The titles and abstracts of identified articles are checked against pre-determined criteria for eligibility and relevance to form an inclusion set. This set will relate back to the research problem. Each included study may be assigned an objective assessment of methodological quality preferably by using methods conforming to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement (the current guideline) or the high quality standards of Cochrane.\n\nSystematic reviews often, but not always, use statistical techniques (meta-analysis) to combine results of eligible studies, or at least use scoring of the levels of evidence depending on the methodology used. An additional rater may be consulted to resolve any scoring differences between raters. Systematic review is often applied in the biomedical or healthcare context, but it can be applied in any field of research. Groups like the Campbell Collaboration are promoting the use of systematic reviews in policy-making beyond just healthcare.\n\nA systematic review uses an objective and transparent approach for research synthesis, with the aim of minimizing bias. While many systematic reviews are based on an explicit quantitative meta-analysis of available data, there are also qualitative reviews which adhere to standards for gathering, analyzing and reporting evidence. The EPPI-Centre has been influential in developing methods for combining both qualitative and quantitative research in systematic reviews. The PRISMA statement suggests a standardized way to ensure a transparent and complete reporting of systematic reviews, and is now required for this kind of research by more than 170 medical journals worldwide.\n\nDevelopments in systematic reviews during the 21st century included realist reviews and the meta-narrative approach, both of which addressed problems of methods and heterogeneity existing on some subjects.\n\nThe main stages of a systematic review are:\n\n\nOnce these stages are complete, the review may be published, disseminated and translated into practice after being adopted as evidence.\n\nThe Cochrane is a group of over 37,000 specialists in healthcare who systematically review randomised trials of the effects of prevention, treatments and rehabilitation as well as health systems interventions. When appropriate, they also include the results of other types of research. Cochrane Reviews are published in \"The Cochrane Database of Systematic Reviews\" section of the Cochrane Library. The 2015 impact factor for \"The Cochrane Database of Systematic Reviews\" was 6.103, and it was ranked 12th in the “Medicine, General & Internal” category. There are six types of Cochrane Review:\n\nThe Cochrane Collaboration provides a handbook for systematic reviewers of interventions which \"provides guidance to authors for the preparation of Cochrane Intervention reviews.\" The \"Cochrane Handbook\" outlines eight general steps for preparing a systematic review:\n\nThe Cochrane Handbook forms the basis of two sets of standards for the conduct and reporting of Cochrane Intervention Reviews (MECIR - Methodological Expectations of Cochrane Intervention Reviews)\n\nThe Cochrane Collaboration logo visually represents how results from some systematic reviews can be explained. The lines within illustrate the summary results from an iconic systematic review showing the benefit of corticosteroids, which 'has probably saved thousands of premature babies'.\n\nThe quasi-standard for systematic review in the social sciences is based on the procedures proposed by the Campbell Collaboration, which is one of a number of groups promoting evidence-based policy in the social sciences. The Campbell Collaboration \"helps people make well-informed decisions by preparing, maintaining and disseminating systematic reviews in education, crime and justice, social welfare and international development. It is a sister initiative of Cochrane. The Campbell Collaboration was created in 2000 and the inaugural meeting in Philadelphia, USA, attracted 85 participants from 13 countries.\n\nDue to the different nature of research fields outside of the natural sciences, the aforementioned methodological steps cannot easily be applied in business research. Early attempts to transfer the procedures from medicine to business research have been made by Tranfield et al. (2003). A step-by-step approach has been developed by Durach et al.: Based on the experiences they have made in their own discipline, these authors have adapted the methodological steps and developed a standard procedure for conducting systematic literature reviews in business and economics.\n\nWhile systematic reviews are regarded as the strongest form of medical evidence, a review of 300 studies found that not all systematic reviews were equally reliable, and that their reporting can be improved by a universally agreed upon set of standards and guidelines. A further study by the same group found that of 100 systematic reviews monitored, 7% needed updating at the time of publication, another 4% within a year, and another 11% within 2 years; this figure was higher in rapidly changing fields of medicine, especially cardiovascular medicine. A 2003 study suggested that extending searches beyond major databases, perhaps into grey literature, would increase the effectiveness of reviews.\n\nRoberts and colleagues highlighted the problems with systematic reviews, particularly those conducted by the Cochrane, noting that published reviews are often biased, out of date and excessively long. They criticized Cochrane reviews as not being sufficiently critical in the selection of trials and including too many of low quality. They proposed several solutions, including limiting studies in meta-analyses and reviews to registered clinical trials, requiring that original data be made available for statistical checking, paying greater attention to sample size estimates, and eliminating dependence on only published data.\n\nSome of these difficulties were noted early on as described by Altman: \"much poor research arises because researchers feel compelled for career reasons to carry out research that they are ill equipped to perform, and nobody stops them.\" Methodological limitations of meta-analysis have also been noted. Another concern is that the methods used to conduct a systematic review are sometimes changed once researchers see the available trials they are going to include. Bloggers have described retractions of systematic reviews and published reports of studies included in published systematic reviews.\n\nSystematic reviews are increasingly prevalent in other fields, such as international development research. Subsequently, a number of donors – most notably the UK Department for International Development (DFID) and AusAid – are focusing more attention and resources on testing the appropriateness of systematic reviews in assessing the impacts of development and humanitarian interventions.\n\n\n"}
{"id": "2555202", "url": "https://en.wikipedia.org/wiki?curid=2555202", "title": "Tarasoff v. Regents of the University of California", "text": "Tarasoff v. Regents of the University of California\n\nTarasoff v. Regents of the University of California, 17 Cal. 3d 425, 551 P.2d 334, 131 Cal. Rptr. 14 (Cal. 1976), was a case in which the Supreme Court of California held that mental health professionals have a duty to protect individuals who are being threatened with bodily harm by a patient. The original 1974 decision mandated warning the threatened individual, but a 1976 rehearing of the case by the California Supreme Court called for a \"duty to protect\" the intended victim. The professional may discharge the duty in several ways, including notifying police, warning the intended victim, and/or taking other reasonable steps to protect the threatened individual.\n\nProsenjit Poddar was a student from Bengal, India. He entered the University of California, Berkeley as a graduate student in September 1967 and resided at International House. In the fall of 1968, he attended folk dancing classes at the International House, and it was there that he met Tatiana Tarasoff. They dated, but apparently had different ideas about the relationship. He assumed their relationship was serious. This view was not shared by Tarasoff who, upon learning of his feelings, told him that she was involved with other men and that she was not interested in entering into an intimate relationship with him. This gave rise to feelings of resentment in Poddar. He began to stalk her.\n\nAfter this rebuff, Poddar underwent a severe emotional crisis. He became depressed and neglected his appearance, his studies, and his health. He kept to himself, speaking disjointedly and often weeping. This condition persisted, with steady deterioration, throughout the spring and into the summer of 1969. Poddar had occasional meetings with Tarasoff during this period and tape-recorded their various conversations to try to find out why she did not love him.\n\nDuring the summer of 1969, Tarasoff went to South America. After her departure, Poddar began to improve and at the suggestion of a friend sought psychological assistance. Prosenjit Poddar was a patient of Dr. Lawrence Moore, a psychologist at UC Berkeley's Cowell Memorial Hospital in 1969. Poddar confided his intent to kill Tarasoff. Dr. Moore requested that the campus police detain Poddar, writing that, in his opinion, Poddar was suffering from paranoid schizophrenia, acute and severe. The psychologist recommended that the defendant be civilly committed as a dangerous person. Poddar was detained but shortly thereafter released, as he appeared rational. Dr. Moore's supervisor, Dr. Harvey Powelson, then ordered that Poddar not be subject to further detention.\nIn October, after Tarasoff had returned, Poddar stopped seeing his psychologist. Neither Tarasoff nor her parents received any warning of the threat. Poddar then befriended Tarasoff's brother, even moving in with him. Several weeks later, on October 27, 1969, Poddar carried out the plan he had confided to his psychologist, stabbing and killing Tarasoff. Tarasoff's parents then sued Moore and various other employees of the university.\n\nPoddar was subsequently convicted of second-degree murder, but the conviction was later appealed and overturned on the grounds that the jury was inadequately instructed. A second trial was not held, and Poddar was released on the condition that he would return to India.\n\nThe California Supreme Court found that a mental health professional has a duty not only to a patient, but also to individuals who are specifically being threatened by a patient. This decision has since been adopted by most states in the U.S. and is widely influential in jurisdictions outside the U.S. as well.\n\nJustice Mathew O. Tobriner wrote the holding in the majority opinion. \"We conclude that the public policy favoring protection of the confidential character of patient-psychotherapist communications must yield to the extent to which disclosure is essential to avert danger to others. The protective privilege ends where the public peril begins.\"\n\nJustice Mosk wrote a partial dissent, arguing that (1) the rule in future cases should be one of the actual subjective prediction of violence on the part of the psychiatrist, which occurred in this case, not one based on objective professional standards, because predictions are inherently unreliable; and (2) the psychiatrists notified the police, who were presumably in a better position to protect Tarasoff than she would be to protect herself.\n\nJustice Clark dissented, quoting a law review article that stated, \"…the very practice of psychiatry depends upon the reputation in the community that the psychiatrist will not tell.\"\n\nAs of 2012, a duty to warn or protect is mandated and codified in legislative statutes of 23 states, while the duty is not codified in a statute but is present in the common law supported by precedent in 10 states. 11 states have a permissive duty, and six states are described as having no statutes or case law offering guidance.\n\nDespite initial commentators predictions of negative consequences for psychotherapy because of the \"Tarasoff\" ruling, court decisions show otherwise. An analysis of 70 cases that went to appellate courts between 1985 and 2006 found that only four of the six rulings in favor of the plaintiff cited \"Tarasoff\" statutes; courts ruled in favor of the defendant in 46 cases and sent 17 cases back to lower courts. However, courts do rule in victims' favor in clear-cut cases of failure to warn or protect, such as the case of a psychiatrist who committed rape during a child psychiatry fellowship, for which he was recommended even after telling his own psychiatrist about his sexual attraction to children.\n\nIn 2018, the Court held that universities should protect students in the Regents of University of California v. Superior Court of Los Angeles County.\n\n"}
{"id": "35842641", "url": "https://en.wikipedia.org/wiki?curid=35842641", "title": "Thomas Morison Legge", "text": "Thomas Morison Legge\n\nSir Thomas Morison Legge CBE (1863-1932) was the first Medical Inspector of Factories and Workshops in the United Kingdom, appointed in 1898 and resigning on 29 November 1926. He was appointed Commander of the Order of the British Empire in 1918 and knighted in 1925, in the New Year Honours. His work was especially concerned with anthrax and lead poisoning.\n\nLegge's axioms, which he expounded in 1929, are \"famous\". They include the following:\n"}
