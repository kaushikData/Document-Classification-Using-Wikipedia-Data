{"id": "15876936", "url": "https://en.wikipedia.org/wiki?curid=15876936", "title": "2008 Kousseri vaccination campaign", "text": "2008 Kousseri vaccination campaign\n\nThe Kousseri vaccination campaign took place between February 13 and 17 2008. During the campaign over 35,000 infants and children were vaccinated against measles, poliomyelitis or both in Cameroon's north-eastern district of Kousséri, in an operation led by the Cameroon Ministry of Health, the United Nations, and non-governmental organizations (NGOs), such as Médecins Sans Frontières – Switzerland (MSF-CH).\n\nThe UN Resident Coordinator, Sophie de Caen, stated that this was in response to the influx of Chadian refugees into the northeast Cameroon earlier in February 2008, in the aftermath of the Battle of N'Djamena, as the potential for epidemics substantially increases whenever there are population movements.\n\nFollowing fighting in the Chadian capital N'Djamena, which erupted on February 2, at least 30,000 Chadians crossed into Cameroon, reaching Kousséri in northeast Cameroon. Most found refuge in two temporary sites, while many others were hosted in schools, churches, and private homes, according to the United Nations.\n\nThe vaccination campaign targeted all children of applicable ages in Kousséri, ensuring protection for Cameroonian and refugee children alike. All children aged six months to 15 years were vaccinated against measles, and all children under 5, including newborns, were vaccinated against poliomyelitis.\n\nFigures released on 21 February 2008 show that a total of 35,615 children were vaccinated against poliomyelitis, while 32,624 were vaccinated against measles. Vitamin A supplement tablets were also provided to approximately 34,000 children along with the vaccinations.\n\n"}
{"id": "23077456", "url": "https://en.wikipedia.org/wiki?curid=23077456", "title": "2009 Shaanxi dog-free zone", "text": "2009 Shaanxi dog-free zone\n\nThe 2009 Shanxi dog-free zone is a goal by the government of Shanxi Province in north central People's Republic of China (PRC) to begin killing large number of dogs as part of a campaign to stop the spread of rabies in the region via stray dogs attacking humans. Link broken.\n\nIn 2006 a similar destruction of 50,000 dogs in Mouding county, Yunnan Province in south central China occurred after three deaths from rabies. This was followed a couple of weeks later by another mass slaughter in Jining, Shandong province in western China after 16 people died from rabies. China uses both the compulsory vaccination and registration of pets and the killing of strays as instruments in rabies control. Dogs, except in cages for sale as meat, are not allowed in a number of urban areas and strays are killed. World Health Organization reports attribute some part of a rise in rabies in China in the 1980s to economic reforms which saw an increase in dog populations in rural areas, and later in the 1990s to an increase in the demand for dog meat and to problems with rabies vaccine quality. The 1980s increase was dealt with primarily with a large-scale vaccination campaign in rural areas where dog rabies is endemic, but some dog slaughters also took place. Outbreaks in Beijing in the early to mid-1990s were also dealt with by ordering strike teams to capture and kill dogs, including pets, making this 2006 governmental killing of dogs a single incident among a number of similar mass killings. The World Health Organization also recommends a ban on bringing dog meat from rural to urban areas.\n\nIn 2009 Yang County is striving to be the first county nationally in the PRC to have no dogs. Links broken. The government ordered all dogs within a radius range of 3206 square kilometres to be killed, including registered and vaccinated dogs. In a statement to China Daily, Dang Zhengqing stated this was the only way they could stop the spread of rabies.\n\nPeople who are already pet owners were advised to register their dogs and vaccinate them. These vaccinations were offered for free. Stray and wild dogs within the region were also put down by the government. It was reported that over 20,000 dogs had been killed in the city of Hanzhong alone. Free leashes were also provided to allow citizens to control their dogs.\n\n"}
{"id": "19065640", "url": "https://en.wikipedia.org/wiki?curid=19065640", "title": "Adjustable-focus eyeglasses", "text": "Adjustable-focus eyeglasses\n\nAdjustable focus eyeglasses are eyeglasses with an adjustable focal length. They compensate for refractive errors (such as presbyopia) by providing variable focusing, allowing users to adjust them for desired distance or prescription, or both.\n\nCurrent bifocals and progressive lenses are static, in that the user has to change their eye position to look through the portion of the lens with the focal power corresponding to the distance of the object. This usually means looking through the top of the lens for distant objects and down through the bottom of the lens for near objects. Adjustable focus eyeglasses have one focal length, but it is variable without having to change where one is looking.\n\nPossible uses for such glasses are to provide inexpensive eyeglasses for people from low-income groups, developing countries, third world countries or to accommodate for presbyopia.\n\nThere are currently two basic methods to achieve variable focal length: electro-optical and opto-mechanical.\n\nElectro-optical often uses liquid crystals as the active medium. Applying an electric potential to the liquid changes the refraction of the liquid.\n\nEarly work on opto-mechanical methods was done by Martin Wright. Opto-mechanical spectacles allow focus control by the wearer via movement of a small slider located on top of the bridge. The user adjusts the lens for optical clarity at the desired distance. They are a combination of rigid and flexible lenses that can change prescription to enable sharp focus at different distances (from infinity up to 13\"). The appropriate addition range depends on the user’s level of refractive error. A tiny mechanism, actuated by the slider, simultaneously controls both flexible lenses to assure appropriate near vision tracking in both eyes.\n\nAnother type of opto-mechanical lens is the design of Joshua Silver, and uses liquid pressure against a diaphragm to control focus of a lens. These lenses were meant to provide improved vision without prescription by an optometrist, since these professionals are in short supply in many countries. Each eyepiece encloses a reservoir of fluid silicone and the user adjusts the level of fluid with a dial until they are satisfied with the result.\n\nStephen Kurtin also has a product based on what appears to be a related design called Superfocus (originally TruFocals).\n\nUnlike with bifocals, near-vision correction is achieved over the entire field of view, in any direction. Distance vision corrections are made by re-adjusting the lens for distance, instead of by tilting and/or rotating the head to view object through the best part of the lens for the distance. Adjustable focus lenses, like single-focus lenses, also reduce image-jump and spatial distortion in the field of view associated with traditional multi-focal lenses. Additionally, the ideal near-vision correction can be achieved with precision, because the variable lenses emulate the focusing action of the youthful (non-presbyopic) eye.\n\nThe focal distance is changed by a mechanism located on the glasses, requiring periodic adjustment as the user switches his gaze to nearer or farther objects.\n\n\n"}
{"id": "12565376", "url": "https://en.wikipedia.org/wiki?curid=12565376", "title": "Africa Myeloma Foundation", "text": "Africa Myeloma Foundation\n\nThe African Myeloma Foundation (AMF) was formed in 2006 to call attention to the plight of people with multiple myeloma in the African continent by a family who lost their mother after a protracted battle with multiple myeloma in Nigeria. Its head office is located in Wixom, Michigan, in the United States of America. The AMF has three regional locations in Africa - Eastern, Southern, and Western. All of the countries of Africa are subdivided into these three regions.\n\nThe original founders are Charles, Jenny, George, Rita, Ify, and Arthur Nwasor.\n\nThe AMF has identified certain factors as critical to making a significant impact on the treatment of myeloma in the African continent. To that end, it has adopted to work towards:\n\n\nIn the interim, the AMF would be a platform where myeloma patients, their caregivers, and medical professionals in the continent and from around the world, can converge to:\n\n\nAMF hopes to achieve these through partnership with host Universities, myeloma and cancer organizations, research centers from around the world, and the pharmaceutical industry.\n\n"}
{"id": "23715645", "url": "https://en.wikipedia.org/wiki?curid=23715645", "title": "African Journal of Health Sciences", "text": "African Journal of Health Sciences\n\nThe African Journal of Health Sciences is a peer-reviewed healthcare journal covering research and policy issues in the health sciences and related disciplines.\n\n"}
{"id": "18506917", "url": "https://en.wikipedia.org/wiki?curid=18506917", "title": "Agrarian system", "text": "Agrarian system\n\nAn agrarian system is the dynamic set of economic and technological factors that affect agricultural practices. It is premised on the idea that different systems have developed depending on the natural and social conditions specific to a particular region. Political factors also have a bearing on an agrarian system due to issues such as land ownership, labor organization, and forms of cultivation.\n\nAs food security has become more important, mostly due to the explosive population growth during the 20th century, the efficiency of agrarian systems has come under greater review.\n\nThe basis for a prevailing agrarian system may be derived from one of a number of major types, including agrarian social structure, for example, tribal or ethnic divisions, feudal classes or family based systems. Farming methods such as migratory herding of livestock are a common framework for which an agrarian system may evolve. Other important kinds of system are based on the dominant political ideology such as communism or agrarian socialism.\n\nEurope is dominated by mixed farming. This has meant careful management of tillage practices and good tools and implements were important. China developed an agrarian system based on labor-intensive wet rice cultivation where skill was paramount.\n\nThe Ottoman agrarian system was based around the tapu, which involved a permanent lease of state-owned arable land to a peasant family. In Haiti there was a social system based on collective labor teams, called kounbit, where farms were run by nuclear families and exchanges. This was replaced by smaller groups, called eskouad, who operated on a reciprocal basis or conducted collective labor to other peasants for a price.\n\nIn the 20th century the distribution of land ownership in rural Egypt had become grossly unequal. An overwhelming majority of land owners possessed small parcels of land while a small minority owned large farms. Many of the rural poor were landless. By the middle of the century the calls for agrarian reform grew. Tenancy reforms, including rent control and minimum wage legislation were enacted with mixed results.\n\nIn Nigeria, the Igbo people developed an agrarian system in which some farmers became traders. Their emphasis on small-scale, entrepreneurial capitalism was fundamental to Nigerian Independence.\n\n"}
{"id": "2676278", "url": "https://en.wikipedia.org/wiki?curid=2676278", "title": "Alaska Raptor Center", "text": "Alaska Raptor Center\n\nThe Alaska Raptor Center is a raptor rehabilitation center in Sitka in the U.S. state of Alaska. Located on a 17–acre campus bordering the Tongass National Forest and the Indian River, its primary mission is the rehabilitation of sick and injured eagles, hawks, falcons, owls, and other birds of prey which are brought in from all over Alaska. The Center (the largest of its type in the state, and one of the largest in North America) receives between 100–200 birds a year, with many suffering from gunshot wounds and traffic accident-related trauma. \n\nMost of the birds arriving at the center arrive in special containers, having been flown in via the baggage compartments of Alaska Airlines planes. The Center's goal is to introduce the birds back into the wild, once they are healed and retrained in \"raptor life skills\" (such as flying) in the enclosed, Bald Eagle Flight-Training Center. Many birds that are no longer able to live outside captivity are sent to zoos and wildlife centers located throughout the United States.\n\nA few of the animals that could not be returned to nature have become permanent guests. More than 40,000 visitors annually come to see the two dozen resident eagles, hawks, owls, and ravens, who assist in the Center’s secondary function, that of public education. The most well-known resident is \"Volta\", a bald eagle who suffered permanent damage after a 1992 collision with power lines (hence the name). Though since nursed back to health, \"Volta\" now regularly travels to the lower 48 states as an ambassador for the Center (out of the 100,000 or so bald eagles on Earth, half live in Alaska).\n\nThe Center is open to the public and offers daily tours.\n\nThe Alaska Raptor Center is a private, nonprofit organization.\n\n\n"}
{"id": "5595993", "url": "https://en.wikipedia.org/wiki?curid=5595993", "title": "American Horticultural Therapy Association", "text": "American Horticultural Therapy Association\n\nThe American Horticultural Therapy Association (AHTA) is an association based in Seattle, Washington, United States, which promotes horticultural therapy (HT) and the profession of horticultural therapist.\n\nThe association states that gardening work has been recognized as a beneficial form of rehabilitation for persons with mental illness or with developmental or other disabilities.\n\n"}
{"id": "16879763", "url": "https://en.wikipedia.org/wiki?curid=16879763", "title": "Aortopulmonary window", "text": "Aortopulmonary window\n\nAortopulmonary window refers to a congenital heart defect similar in some ways to persistent truncus arteriosus. Persistent truncus arteriosus involves a single valve; aortopulmonary window is a septal defect.\n\n"}
{"id": "4435948", "url": "https://en.wikipedia.org/wiki?curid=4435948", "title": "Asylums (book)", "text": "Asylums (book)\n\nAsylums: Essays on the Condition of the Social Situation of Mental Patients and Other Inmates is a 1961 collection of four essays by the sociologist Erving Goffman.\n\nBased on his participant observation field work (he was employed as a physical therapist's assistant under a grant from the National Institute of Mental Health at a mental institution in Washington, D.C.), Goffman details his theory of the \"total institution\" (principally in the example he gives, as the title of the book indicates, mental institutions) and the process by which it takes efforts to maintain predictable and regular behavior on the part of both \"guard\" and \"captor,\" suggesting that many of the features of such institutions serve the ritual function of ensuring that both classes of people know their function and social role, in other words of \"institutionalising\" them. Goffman concludes that adjusting the inmates to their role has at least as much importance as \"curing\" them. In the essay \"Notes on the Tinkering Trades,\" Goffman concluded that the \"medicalization\" of mental illness and the various treatment modalities are offshoots of the 19th century and the Industrial Revolution and that the so-called \"medical model\" for treating patients was a variation on the way trades- and craftsmen of the late 19th century repaired clocks and other mechanical objects: in the confines of a shop or store, contents and routine of which remained a mystery to the customer.\n\n\"Asylums\" brought Goffman immediate recognition when it was published in 1961, and by the 1970s had become required reading in some introductory sociology courses, according to socialist author Peter Sedgwick, who considered the book a \"powerful and compelling study\" and the recognition it brought to Goffman \"thoroughly deserved\".\n\n"}
{"id": "1374343", "url": "https://en.wikipedia.org/wiki?curid=1374343", "title": "Audiometry", "text": "Audiometry\n\nAudiometry (from , \"to hear\" and \"metria\", “to measure\") is a branch of audiology and the science of measuring hearing acuity for variations in sound intensity and pitch and for tonal purity, involving thresholds and differing frequencies. Typically, audiometric tests determine a subject's hearing levels with the help of an audiometer, but may also measure ability to discriminate between different sound intensities, recognize pitch, or distinguish speech from background noise. Acoustic reflex and otoacoustic emissions may also be measured. Results of audiometric tests are used to diagnose hearing loss or diseases of the ear, and often make use of an audiogram.\n\nThe basic requirements of the field were to be able to produce a repeating sound, some way to attenuate the amplitude, a way to transmit the sound to the subject, and a means to record and interpret the subject's responses to the test.\n\nFor many years there were a desultory use of various devices capable of producing sounds of controlled intensity. The first types were clock-like, giving off air-borne sound to the tubes of a stethoscope; the sound distributor head had a valve which could be gradually closed. Another model used a tripped hammer to strike a metal rod and produce the testing sound; in another a tuning fork was struck.\nThe first such measurement device for testing hearing was described by Wolke (1802)\n\nFollowing development of the induction coil in 1849 and audio transducers (telephone) in 1876, a variety of audiometers were invented in United States and overseas. These early audiometers were known as induction-coil audiometers due to...\nIn 1885, Arthur Hartmann designed an “Auditory Chart” which included left and right ear tuning fork representation on the abscissa and percent of hearing along the ordinate.\n\nIn 1899, Carl E. Seashore Prof. of Psychology at U. Iowa, United States, introduced the audiometer as an instrument to measure the “keenness of hearing” whether in the laboratory, schoolroom, or office of the psychologist or aurist. The instrument operated on a battery and presented a tone or a click; it had an attenuator set in a scale of 40 steps. His machine became the basis of the audiometers later manufactured at Western Electric.\n\n\nThe concept of a frequency versus sensitivity (amplitude) audiogram plot of human hearing sensitivity was conceived by German physicist Max Wien in 1903. The first vacuum tube implementations, November 1919, two groups of researchers — K.L. Schaefer and G. Gruschke, B. Griessmann and H. Schwarzkopf — demonstrated before the Berlin Oto-logical Society two instruments designed to test hearing acuity. Both were built with vacuum tubes. Their designs were characteristic of the two basic types of electronic circuits used in most electronic audio devices for the next two decades. Neither of the two devices was developed commercially for some time, although the second was to be manufactured under the name \"Otaudion.\"\nThe Western Electric 1A, developed by <who> was built in 1922 in the United States. It was not until 1922 that otolaryngologist Dr. Edmund P. Fowler, and physicists Dr. Harvey Fletcher and Robert Wegel of Western Electric Co. first employed frequency at octave intervals plotted along the abscissa and intensity downward along the ordinate as a degree of hearing loss. Fletcher et al. also coined the term “audiogram” at that time.\n\nWith further technologic advances, bone conduction testing capabilities became a standard component of all Western Electric audiometers by 1928.\n\nIn 1967, Sohmer and Feinmesser were the first to publish ABRs recorded with surface electrodes in humans which showed that cochlear potentials could be obtained non-invasively.\n\nIn 1978, David Kemp reported that sound energy produced by the ear could be detected in the ear canal. The first commercial system for detecting and measuring OAEs was produced in 1988.\n\nThe auditory system is composed of epithelial, osseous, vascular, neural and neocortical tissues. The anatomical divisions\nare external ear canal and tympanic membrane, middle ear, inner ear, VIII auditory nerve, and central auditory processing portions of the neocortex.\n\nSound waves enter the outer ear and travel through the external auditory canal until they reach the tympanic membrane, causing the membrane and the attached chain of auditory ossicles to vibrate. The motion of the stapes against the oval window sets up waves in the fluids of the cochlea, causing the basilar membrane to vibrate. This stimulates the sensory cells of the organ of Corti, atop the basilar membrane, to send nerve impulses to the central auditory processing areas of the brain, the auditory cortex, where sound is perceived and interpreted.\n\n\n\nSubjective audiometry requires the cooperation of the subject, and relies upon subjective responses which may both qualitative and quantitative, and involve attention (focus), reaction time, etc. \n\nSpeech audiometry may include:\n\nObjective audiometry is based on physical, acoustic or electrophysiologic measurements and does not depend on the cooperation or subjective responses of the subject.\n\nThe result of most audiometry is an audiogram plotting some measured dimension of hearing, either graphically or tabularly.\n\nThe most common type of audiogram is the result of a pure tone audiometry hearing test which plots frequency versus amplitude sensitivity thresholds for each ear along with bone conduction thresholds at 8 standard frequencies from 250 Hz to 8000 Hz. A PTA hearing test is the gold standard for evaluation of hearing loss/disability. Other types of hearing tests also generate graphs or tables of results that may be loosely called 'audiograms', but the term is universally used to refer to the result of a PTA hearing test.\n\nApart from testing hearing, part of the function of audiometry is in assessing or evaluating hearing from the test results. The most commonly used assessment of hearing is the determination of the threshold of audibility, i.e. the level of sound required to be just audible. This level can vary\nfor an individual over a range of up to 5 decibels from day to day and from determination to determination, but it provides an additional and useful tool in monitoring the potential ill effects of exposure to noise. Before carrying out a hearing test, it is important to obtain information about the person’s past medical history, not only concerning the ears but also other conditions which may have a bearing on possible hearing loss detected by an audiometric test. Hearing loss may be unilateral or bilateral, and bilateral hearing loss may not be symmetrical. The most common types of hearing loss, due to age and noise exposure, are usually bilateral and symmetrical. Wax in the ear can also cause hearing loss, so the ear should be examined to see if syringing is needed; also to determine if the eardrum has suffered any damage which may reduce the ability of sound to be transmitted to the middle ear.\n\nThe primary focus of audiometry is assessment of hearing status and hearing loss, including extent, type and configuration.\n\nHearing loss may be caused by a number of factors including heredity, congenital conditions, age-related (presbycusis) and acquired factors like noise-induced hearing loss, ototoxic chemicals and drugs, infections, and physical trauma.\n\nAudiometric testing may be performed by a general practitioner medical doctor, an otolaryngologist (a specialized MD also called an ENT), a CCC-A (Certificate of Clinical Competence in Audiology) audiologist, a certified school audiometrist (a practitioner analogous to an optometrist who tests eyes), and sometimes other trained practitioners. Practitioners are certified by American Board of Audiology (ABA). Practitioners are licensed by various state boards\nregulating workplace health & safety, occupational professions, or ...\n\nWorkplace and environmental noise is the most prevalent cause of hearing loss in the United States and elsewhere.\n\n\n"}
{"id": "26238163", "url": "https://en.wikipedia.org/wiki?curid=26238163", "title": "Bamforth–Lazarus syndrome", "text": "Bamforth–Lazarus syndrome\n\nBamforth–Lazarus syndrome is a genetic condition that results in thyroid dysgenesis. It is due to recessive mutations in forkhead/winged-helix domain transcription factor (\"FKLH15\" or \"TTF2\").\n\nIt is associated with \"FOXE1\".\n"}
{"id": "52356236", "url": "https://en.wikipedia.org/wiki?curid=52356236", "title": "Cannabis in Taiwan", "text": "Cannabis in Taiwan\n\nCannabis in Taiwan is illegal. Cannabis is listed as a category 2 narcotic by Narcotics Hazard Prevention Act in the ROC. Possession of a category 2 drug can result in up to three years of imprisonment; planting or trafficking can result in at least seven years of imprisonment.\n"}
{"id": "13876991", "url": "https://en.wikipedia.org/wiki?curid=13876991", "title": "Cardiac monitoring", "text": "Cardiac monitoring\n\nCardiac monitoring generally refers to continuous or intermittent monitoring of heart activity, generally by electrocardiography, with assessment of the patient's condition relative to their cardiac rhythm. It is different from hemodynamic monitoring, which monitors the pressure and flow of blood within the cardiovascular system. The two may be performed simultaneously on critical heart patients. Cardiac monitoring with a small device worn by an ambulatory patient (one well enough to walk around) is known as ambulatory electrocardiography (such as with a Holter monitor, wireless ambulatory ECG, or an implantable loop recorder). Transmitting data from a monitor to a distant monitoring station is known as telemetry or biotelemetry.\n\nIn the setting of out-of-hospital acute medical care, ambulance services and other emergency medical services providers utilize heart monitors to assess the patient's cardiac rhythm. Providers licensed or certified at the Intermediate or Paramedic level are qualified to interpret ECGs. The finding of a cardiac dysrhythmia (or for that matter, a normal sinus rhythm) may give additional information about the patient's condition or may be a sufficient diagnosis on its own to guide treatment. Treatment for specific cardiac rhythms is guided by ACLS. Basic EMTs are allowed to apply the electrodes and physically operate the monitor but not interpret the rhythm.\n\nIn the emergency department, cardiac monitoring is a part of the monitoring of vital signs in emergency medicine, and generally includes electrocardiography.\n\nSome digital patient monitors, especially those used EMS services, often incorporate a defibrillator into the patient monitor itself. These monitor/defibrillators usually have the normal capabilities of an ICU monitor, but have manual (and usually semi-automatic AED)defibrillation capability. This is particularly good for EMS services, who need a compact, easy to use monitor and defibrillator, as well as for inter- or intrafacility patient transport. Most monitor defibrillators also have transcutaneous pacing capability via large AED like adhesive pads (which often can be used for monitoring, defibrillation and pacing)that are applied to the patient in an anterior-posterior configuration. The monitor defibrillator units often have specialized monitoring parameters such as waveform capnography, invasive BP, and in some monitors, Masimo Rainbow SET pulse oximetry.\nExamples of monitor defibrillators are the Lifepak 12, 15 and 20 made by Physio Control, the Philips Heartstart MRx, and the E, R, and X Series by ZOLL Medical.\n\nThere are two broad classifications for cardiac event monitors: manual (or dumb) and automatic. Automatic ECG event monitors have the ability to monitor the patient's ECG and make recordings of abnormal events without requiring patient intervention. Manual ECG event recorders require the patient to be symptomatic and to activate the device to record an event; this makes these devices useless whilst, for example, the patient is sleeping. A third classification, the implantable loop recorder, provides both automatic and manual abilities.\n\nAn example of automatic monitoring is the transtelephonic cardiac event monitor. This monitor contacts ECG technicians, via telephone, on a regular basis transmitting ECG rhythms for ongoing monitoring. The transtelephonic cardiac event monitor can normally store approximately five \"cardiac events\" usually lasting 30–60 seconds.\n\nMonitoring of the heart rate can be performed as part of electrocardiography, but it can also be measured conveniently with specific heart rate monitors. Such heart rate monitors are largely used by performers of various types of physical exercise\n"}
{"id": "1198727", "url": "https://en.wikipedia.org/wiki?curid=1198727", "title": "David Ausubel", "text": "David Ausubel\n\nDavid Paul Ausubel (October 25, 1918 – July 9, 2008) was an American psychologist. His most significant contribution to the fields of educational psychology, cognitive science, and science education learning was on the development and research on advance organizers since 1960.\n\nHe was born on October 25, 1918 and grew up in Brooklyn, New York. He was nephew of the Jewish historian Nathan Ausubel.\n\nHe studied at the University of Pennsylvania where he graduated with honors in 1939, receiving a bachelor's degree majoring in psychology. Ausubel later graduated from medical school in 1943 at Middlesex University where he went on to complete a rotating internship at Gouverneur Hospital, located in the lower east side of Manhattan, New York. Following his military service with the US Public Health Service, Ausubel earned his M.A. and Ph.D. in developmental psychology from Columbia University in 1950. He continued to hold a series of professorships at several schools of education. \nIn 1973, Ausubel retired from academic life and devoted himself to his psychiatric practice. During his psychiatric practice, Ausubel published many books as well as articles in psychiatric and psychological journals. In 1976, he received the Thorndike Award from the American Psychological Association for \"Distinguished Psychological Contributions to Education\".\n\nIn 1994, at the age of 75, Ausubel retired from professional life to devote himself full-time to writing. He then published four books: \"Ego development and Psychopathology\" (1996), \" The Acquisition and Retention of Knowledge\" (2000), \"Theory and Problems of Adolescent Development\" (2002) and \"Death and the Human Condition\" (2002), in the last of which he wrote about the psychology of death and impressed his own personal psychological, theological and philosophical thoughts on the nature and implications of the afterlife. In this book, Ausubel conceptualized death from the perspective of both Christian believers and non-believers. He wrote that \"the relevance and value of faith should certainly not be derogated or treated pejoratively, as atheists, agnostics, and rationalists tend to do.\"\n\nHe died on July 9, 2008. Ausubel and his wife Pearl had two children.\n\nAusubel was influenced by the teachings of Jean Piaget. Similar to Piaget’s ideas of conceptual schemes, Ausubel related this to his explanation of how people acquire knowledge. “David Ausubel theorized that people acquire knowledge primarily by being exposed directly to it rather than through discovery” (Woolfolk et al., 2010, p. 288) In other words, Ausubel believed that understanding concepts, principles, and ideas are achieved through deductive reasoning.\n\nSimilarly, he believed in the idea of meaningful learning as opposed to rote memorization. In the preface to his book \"Educational Psychology: A Cognitive View\", he says that “If [he] had to reduce all of educational psychology to just one principle, [he] would say this: The most important single factor influencing learning is what the learner already knows. Ascertain this and teach him accordingly” (Ausubel, 1968, p. vi) Through his belief of meaningful learning, Ausubel developed his theory of advance organizers. However, Ausubel was a critic of discovery-based teaching techniques, stating:\nActual examination of the research literature allegedly supportive of learning by discovery reveals that valid evidence of this nature is virtually nonexistent. It appears that the various enthusiasts of the discovery method have been supporting each other research-wise by taking in each other's laundry, so to speak, that is, by citing each other's opinions and assertions as evidence and by generalizing wildly from equivocal and even negative findings.\nAn advance organizer is information presented by an instructor that helps the student organize new incoming information. This is achieved by directing attention to what is important in the coming material, highlighting relationships, and providing a reminder about relevant prior knowledge.\n\nAdvance organizers make it easier to learn new material of a complex or otherwise difficult nature, provided the following two conditions are met:\n\n1. The student must process and understand the information presented in the organizer—this increases the effectiveness of the organizer itself.\n\n2. The organizer must indicate the relations among the basic concepts and terms that will be used.\n\nAusubel distinguishes between two kinds of advance organizer: \"comparative\" and \"expository\".\n\n1. Comparative Organizers\n\nThe main goal of comparative organizers is to activate existing schemas. Similarly, they act as reminders to bring into the working memory of what you may not realize is relevant. By acting as reminders, the organizer points out explicitly “whether already established anchoring ideas are nonspecifically or specifically relevant to the learning material” (Ausubel & Robinson, 1969, p. 146). Similarly, a comparative organizer is used both to integrate as well as discriminate. It “integrate[s] new ideas with basically similar concepts in cognitive structure, as well as increase[s] discriminability between new and existing ideas which are essentially different but confusably similar” (Ausubel, 1968, p. 149).\n\nAn example of a comparative organizer would be one used for a history lesson on revolutions. This organizer “might be a statement that contrasts military uprisings with the physical and social changes involved in the Industrial Revolution” (Woolfolk et al., 2010, p. 289). Furthermore, you could also compare common aspects of other revolutions from different nations.\n\n2. Expository Organizers\n\n“In contrast, expository organizers provide new knowledge that students will need to understand the upcoming information” (Woolfolk et al., 2010, p. 289). Expository organizers are often used when the new learning material is unfamiliar to the learner. They often relate what the learner already knows with the new and unfamiliar material—this in turn is aimed to make the unfamiliar material more plausible to the learner.\n\nAn example which Ausubel and Floyd G. Robinson provides in their book \"School Learning: An Introduction To Educational Psychology\" is the concept of the Darwinian theory of evolution. To make the Darwinian theory of evolution more plausible, an expository organizer would have a combination of relatedness to general relevant knowledge that is already present, as well as relevance for the more detailed Darwinian theory.\n\nEssentially, expository organizers furnish an anchor in terms that are already familiar to the learner.\n\nAnother example would be the concept of a right angle in a mathematics class. A teacher could ask students to point out examples of right angles that they can find in the classroom. By asking students to do this, it helps relates the students present knowledge of familiar classroom objects with the unfamiliar concept of a 90 degree right angle.\n\n“The most persuasively voiced criticism of advance organizers is that their definition and construction are vague and, therefore, that different researchers have varying concepts of what an organizer is and can only rely on intuition in constructing one-- since nowhere, claim the critics, is it specified what their criteria are and how they can be constructed” (Ausubel, 1978, p. 251).\n\nIn a response to critics, Ausubel defends advance organizers by stating that there is no one specific example in constructing advance organizers as they “always depends on the nature of the learning material, the age of the learner, and his degree of prior familiarity with the learning passage” (Ausubel, 1978, p. 251).\n\nAnother criticism of Ausubel’s advance organizers is that the critics often compare the idea of advance organizers with overviews. However, Ausubel has addressed that issue in saying that advance organizers differ from overviews “in being relatable to presumed ideational content in the learner’s current cognitive structure” (Ausubel, 1978, p. 252).\n\nThirdly, critics also address the notion of advance organizers on whether they are intended to favour high ability or low ability students. However, Ausubel notes that “advance organizers are designed to favour meaningful learning..” (Ausubel, 1978, p. 255). Therefore, to question whether advance organizers are better suited for high or low ability students is unrelated as Ausubel argues that advance organizers can be catered to any student to aid them in bridging a gap between what they already know and what they are about to learn.\n\n\n"}
{"id": "5495959", "url": "https://en.wikipedia.org/wiki?curid=5495959", "title": "Disability pretender", "text": "Disability pretender\n\nA disability pretender is subculture term meaning a person who behaves as if he or she were disabled. It may be classified as a type of factitious disorder or as a medical fetishism.\n\nOne theory is that pretenders may be the \"missing link\" between devotees and wannabes, demonstrating an assumed continuum between those merely attracted to people with disabilities and those who actively wish to become disabled. Many wannabes use pretending as a way to appease the intense emotional pain related to having body integrity identity disorder.\n\nPretending takes a variety of forms. Some chatroom users on internet sites catering to devotees have complained that chat counterparts they assumed were female were revealed as male devotees. This form of pretending (where a devotee derives pleasure by pretending to be a disabled woman) may indicate a very broad predisposition to pretending among devotees.\n\nPretending includes dressing and acting in ways typical of disabled people, including making use of aids (Walking sticks, crutches, wheelchairs, mobility scooters, white canes, etc). Pretending may also take the form of a devotee persuading his or her sexual partner to play the role of a disabled person. Pretending may be practised in private, in intimacy, or in public, and may occupy surprisingly long periods. In the latter case, some pretenders hope that the disability may become permanent, such as through tissue necrosis caused by constricted blood supply.\n\nPeople with this condition may refer to themselves as \"transabled\".\n\n\n"}
{"id": "42972033", "url": "https://en.wikipedia.org/wiki?curid=42972033", "title": "Diseases and epidemics of the 19th century", "text": "Diseases and epidemics of the 19th century\n\nDiseases and epidemics of the 19th century reached epidemic proportions in the case of one emerging infectious disease: cholera. Other important diseases at that time in Europe and other regions included smallpox, typhus and yellow fewer.\n\nEpidemics of the 19th century were faced without the medical advances that made 20th-century epidemics much more rare and less lethal. Micro-organisms (viruses and bacteria) had been discovered in the 18th century, but it was not until the late 19th century that the experiments of Lazzaro Spallanzani and Louis Pasteur disproved spontaneous generation conclusively, allowing germ theory and Robert Koch's discovery of micro-organisms as the cause of disease transmission. Thus throughout the majority of the 19th century, there was only the most basic, common sense understanding of the causes, amelioration and treatment of epidemic disease.\n\nThe late 19th century was the beginning of widespread use of vaccines. The cholera bacterium was isolated in 1854 by Italian anatomist Filippo Pacini, and a vaccine, the first to immunize humans against a bacterial disease, was developed by Spanish physician Jaume Ferran i Clua in 1885, and by Russian-Jewish bacteriologist Waldemar Haffkine in July 1892.\n\nAntibiotic drugs did not appear until the middle of the 20th century. Sulfonamides did not appear until 1935, and penicillin, discovered in 1928, was not available as a treatment until 1950.\n\nDuring the second cholera pandemic of 1816–1837, the scientific community varied in its beliefs about its causes. In France doctors believed cholera was associated with the poverty of certain communities or poor environment. Russians believed the disease was contagious and quarantined their citizens. The United States believed that cholera was brought by recent immigrants, specifically the Irish. Lastly, some British thought the disease might rise from divine intervention. \n\nDuring the third pandemic, Tunisia, which had not been affected by the two previous pandemics, thought Europeans had brought the disease. They blamed their sanitation practices. The prevalence of the disease in the South in areas of black populations convinced United States scientists that cholera was associated with African Americans. Current researchers note they lived near the waterways by which travelers and ships carried the disease and their populations were underserved with sanitation infrastructure and health care.\n\nThe Soho outbreak in London in 1854 ended after the physician John Snow identified a neighborhood Broad Street pump as contaminated and convinced officials to remove its handle. Snow believed that germ-contaminated water was the source of cholera, rather than particles in the air (referred to as \"miasmata\"). His study proved contaminated water was the main agent spreading cholera, although he did not identify the contaminant. Though Filippo Pacini had isolated \"Vibrio cholerae\" as the causative agent for cholera that year, it would be many years before miasma theory would fall out of favor.\n\nIn London, in June 1866), a localized epidemic in the East End claimed 5,596 lives, just as the city was completing construction of its major sewage and water treatment systems. William Farr, using the work of John Snow, \"et al.\", as to contaminated drinking water being the likely source of the disease, relatively quickly identified the East London Water Company as the source of the contaminated water. Quick action prevented further deaths.\n\nDuring the fifth cholera pandemic, Robert Koch isolated \"Vibrio cholerae\" and proposed postulates to explain how bacteria caused disease. His work helped to establish the germ theory of disease. Prior to this time, many physicians believed that microorganisms were spontaneously generated, and disease was caused by direct exposure to filth and decay. Koch helped establish that the disease was more specifically contagious and was transmittable through contaminated water supply. The fifth was the last serious European cholera outbreak, as cities improved their sanitation and water systems.\n\nCholera is an infection of the small intestine caused by the bacterium \"Vibrio cholerae\". Cholera is transmitted primarily by drinking water or eating food that has been contaminated by the cholera bacterium. The bacteria multiply in the small intestine; the feces (waste product) of an infected person, including one with no apparent symptoms, can pass on the disease if it contacts the water supply by any means.\n\nHistory does not recount any incidents of cholera until the 19th century. Cholera came in seven waves, the last two of which occurred in the 20th century.\n\nThe first cholera pandemic started in 1816, spread across India by 1820, and extended to Southeast Asia and Central Europe, lasting until 1826.\n\nA second cholera pandemic began in 1829, reached Russia, causing the Cholera Riots. It spread to Hungary, Germany and Egypt in 1831, and London, Paris, Quebec, Ontario and New York City the following year. Cholera reached the Pacific coast of North America by 1834, reaching into the center of the country by steamboat and other river traffic.\n\nThe third cholera pandemic began in 1846 and lasted until 1860. It hit Russia hardest, with over one million deaths. In 1846, cholera struck Mecca, killing over 15,000. A two-year outbreak began in England and Wales in 1848, and claimed 52,000 lives. In 1849, outbreak occurred again in Paris, and in London, killing 14,137, over twice as many as the 1832 outbreak. Cholera hit Ireland in 1849 and killed many of the Irish Famine survivors, already weakened by starvation and fever. In 1849, cholera claimed 5,308 lives in the major port city of Liverpool, England, an embarkation point for immigrants to North America, and 1,834 in Hull, England. Cholera spread throughout the Mississippi river system. Thousands died in New York City, a major destination for Irish immigrants. Cholera claimed 200,000 victims in Mexico. That year, cholera was transmitted along the California, Mormon and Oregon Trails, killing people that are believed to have died on their way to the California Gold Rush, Utah and Oregon in the cholera years of 1849–1855. In 1851, a ship coming from Cuba carried the disease to Gran Canaria, killing up to 6,000 people.\n\nThe pandemic spread east to Indonesia by 1852, and China and Japan in 1854. The Philippines were infected in 1858 and Korea in 1859. In 1859, an outbreak in Bengal contributed to transmission of the disease by travelers and troops to Iran, Iraq, Arabia and Russia. Japan suffered at least seven major outbreaks of cholera between 1858 and 1902. The Ansei outbreak of 1858–60, for example, is believed to have killed between 100,000 and 200,000 people in Tokyo alone. An outbreak of cholera in Chicago in 1854 took the lives of 5.5% of the population (about 3,500 people). In 1853–4, London's epidemic claimed 10,738 lives. Throughout Spain, cholera caused more than 236,000 deaths in 1854–55. In 1854, it entered Venezuela; Brazil also suffered in 1855.\n\nThe fourth cholera pandemic (1863–1875) spread mostly in Europe and Africa. At least 30,000 of the 90,000 Mecca pilgrims died from the disease. Cholera ravaged northern Africa in 1865 and southeastward to Zanzibar, killing 70,000 in 1869–70. Cholera claimed 90,000 lives in Russia in 1866. The epidemic of cholera that spread with the Austro-Prussian War (1866) is estimated to have taken 165,000 lives in the Austrian Empire. In 1867, 113,000 lost their lives to cholera in Italy. and 80,000 in Algeria. Outbreaks in North America in 1866–1873 killed some 50,000 Americans. In 1866, localized epidemics occurred in the East End of London, in southern Wales, and Amsterdam. In the 1870s, cholera spread in the U.S. as an epidemic from New Orleans along the Mississippi River and to ports on its tributaries.\n\nIn the fifth cholera pandemic (1881–1896), according to Dr A. J. Wall, the 1883–1887 part of the epidemic cost 250,000 lives in Europe and at least 50,000 in the Americas. Cholera claimed 267,890 lives in Russia (1892); 120,000 in Spain; 90,000 in Japan and over 60,000 in Persia. In Egypt, cholera claimed more than 58,000 lives. The 1892 outbreak in Hamburg killed 8,600 people.\n\nSmallpox is caused by either of the two viruses, Variola major and Variola minor. Smallpox vaccine was available in Europe, the United States, and the Spanish Colonies during the last part of the century. The Latin names of this disease are Variola Vera. The words come from varius (spotted) or varus (pimple). In England this disease was first known as the \"pox\" or the \"red plague\". Smallpox settles itself in small blood vessels of the skin and in the mouth and throat. The symptoms of smallpox are rash on the skin and blisters filled with raised liquid.\n\nThe disease killed an estimated 400,000 Europeans annually during the 19th century and one third of all the blindness of that time was caused by smallpox. 20 to 60% of all the people that were infected died and 80% of all the children with the infection also died. It caused also many deaths in the 20th century, over 300–500 million. Wolfgang Amadeus Mozart also had Smallpox when he was only 11 years old. He survived the smallpox outbreak in Austria.\n\nEpidemic typhus is caused by the bacteria Rickettsia Prowazekii; it comes from lice. Murine Typhus is caused by the Rickettsia Typhi bacteria, from the fleas on rats. Scrub Typhus is caused by the Orientia Tsutsugamushi bacteria, from the harvest mites on humans and rodents. \nQueensland tick typhus is caused by the Rickettsia Australis bacteria, from ticks.\n\nDuring Napoleon's retreat from Moscow in 1812, more French soldiers died of typhus than were killed by the Russians. A major epidemic occurred in Ireland between 1816 and 1819, during the Year Without a Summer; an estimated 100,000 Irish perished. Typhus appeared again in the late 1830s, and between 1846 and 1849 during the Great Irish Famine. Spreading to England, and called \"Irish fever\", it was noted for its virulence. It killed people of all social classes, as lice were endemic and inescapable, but it hit particularly hard in the lower or \"unwashed\" social strata. In Canada alone, the typhus epidemic of 1847 killed more than 20,000 people from 1847 to 1848, mainly Irish immigrants in fever sheds and other forms of quarantine, who had contracted the disease aboard coffin ships. In the United States, epidemics occurred in Baltimore, Memphis and Washington DC between 1865 and 1873, and during the US Civil War.\n\nThis disease is transmitted by the bite of female mosquito; the higher prevalence of transmission by Aedes aegypti has led to it being known as the Yellow Fever Mosquito. The transmission of yellow fever is entirely a matter of available habitat for vector mosquito and prevention such as mosquito netting. They mostly infect other primates, but humans can be infected. The symptoms of the fever are: Headaches, back and muscle pain, chills and vomiting, bleeding in the eyes and mouth, and vomit containing blood.\n\nYellow fever accounted for the largest number of the 19th-century's individual epidemic outbreaks, and most of the recorded serious outbreaks of yellow fever occurred in the 19th century. It is most prevalent in tropical-like climates, but the United States was not exempted from the fever. New Orleans was plagued with major epidemics during the 19th century, most notably in 1833 and 1853. At least 25 major outbreaks took place in the Americas during the 18th and 19th centuries, including particularly serious ones in Santo Domingo in 1803 and Memphis in 1878. Major outbreaks occurred repeatedly in Gibraltar; outbreaks in 1804, 1814, and again in 1828. Barcelona suffered the loss of several thousand citizens during an outbreak in 1821. Urban epidemics continued in the United States until 1905, with the last outbreak affecting New Orleans.\n"}
{"id": "168915", "url": "https://en.wikipedia.org/wiki?curid=168915", "title": "Effects of cannabis", "text": "Effects of cannabis\n\nThe effects of cannabis are caused by the chemical compounds in the plant, including cannabinoids, such as tetrahydrocannabinol (THC), which is only one of more than 100 different cannabinoids present in the plant. Cannabis has various psychological and physiological effects on the human body.\n\nDifferent plants of the genus Cannabis contain different and often unpredictable concentrations of THC and other cannabinoids and hundreds of other molecules that have a pharmacological effect, so that the final net effect cannot reliably be foreseen.\n\nAcute effects while under the influence can include euphoria and anxiety. Cannabidiol (CBD), another cannabinoid found in cannabis in varying amounts, has been shown to alleviate the adverse effects of THC that some consumers experience. When ingested orally, THC can produce stronger psychotropic effects than when inhaled. At doses exceeding the psychotropic threshold, users may experience adverse side effects such as anxiety and panic attacks that can result in increased heart rate and changes in blood pressure.\n\nResearch about medical benefits of cannabis has been hindered by United States federal law. Smoking any substance could possibly carry similar risks as smoking tobacco due to carcinogens in all smoke,\n\nCannabis use disorder is defined as a medical diagnosis in the fifth revision of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5).\n\nThe most prevalent psychoactive substances in cannabis are cannabinoids, most notably THC. Some varieties, having undergone careful selection and growing techniques, can yield as much as 34% THC. Another psychoactive cannabinoid present in \"Cannabis sativa\" is tetrahydrocannabivarin (THCV), but it is only found in small amounts and is a cannabinoid antagonist.\n\nThere are also similar compounds contained in cannabis that do not exhibit any psychoactive response but are obligatory for functionality: cannabidiol (CBD), an isomer of THC; cannabivarin (CBV), an analog of cannabinol (CBN) with a different side chain, cannabidivarin (CBDV), an analog of CBD with a different side chain, and cannabinolic acid. How these other compounds interact with THC is not fully understood. Some clinical studies have proposed that CBD acts as a balancing force to regulate the strength of the psychoactive agent THC. CBD is also believed to regulate the body’s metabolism of THC by inactivating cytochrome P450, an important class of enzymes that metabolize drugs. Experiments in which babies were treated with CBD followed by THC showed that CBD treatment was associated with a substantial increase in brain concentrations of THC and its major metabolites, most likely because it decreased the rate of clearance of THC from the body. Cannabis cofactor compounds have also been linked to lowering body temperature, modulating immune functioning, and cell protection. The essential oil of cannabis contains many fragrant terpenoids which may synergize with the cannabinoids to produce their unique effects. THC is converted rapidly to 11-hydroxy-THC, which is also pharmacologically active, so the drug effect outlasts measurable THC levels in blood.\n\nTHC and cannabidiol are also neuroprotective antioxidants. Research in rats has indicated that THC prevented hydroperoxide-induced oxidative damage as well as or better than other antioxidants in a chemical (Fenton reaction) system and neuronal cultures. Cannabidiol was significantly more protective than either vitamin E or vitamin C.\n\nThe cannabinoid receptor is a typical member of the largest known family of receptors called a G protein-coupled receptor. A signature of this type of receptor is the distinct pattern of how the receptor molecule spans the cell membrane seven times. The location of cannabinoid receptors exists on the cell membrane, and both outside (extracellularly) and inside (intracellularly) the cell membrane. CB1 receptors, the bigger of the two, are extraordinarily abundant in the brain: 10 times more plentiful than μ-opioid receptors, the receptors responsible for the effects of morphine. CB2 receptors are structurally different (the sequence similarity between the two subtypes of receptors is 44%), found only on cells of the immune system, and seems to function similarly to its CB1 counterpart. CB2 receptors are most commonly prevalent on B-cells, natural killer cells, and monocytes, but can also be found on polymorphonuclear neutrophil cells, T8 cells, and T4 cells. In the tonsils the CB2 receptors appear to be restricted to B-lymphocyte-enriched areas.\n\nTHC and its endogenous equivalent anandamide additionally interact with glycine receptors.\n\nCannabinoids usually contain a 1,1'-di-methyl-pyran ring, a variedly derivatized aromatic ring and a variedly unsaturated cyclohexyl ring and their immediate chemical precursors, constituting a family of about 60 bi-cyclic and tri-cyclic compounds. Like most other neurological processes, the effects of cannabis on the brain follow the standard protocol of signal transduction, the electrochemical system of sending signals through neurons for a biological response. It is now understood that cannabinoid receptors appear in similar forms in most vertebrates and invertebrates and have a long evolutionary history of 500 million years. The binding of cannabinoids to cannabinoid receptors decrease adenylyl cyclase activity, inhibit calcium N channels, and disinhibit K channels. There are at least two types of cannabinoid receptors (CB1 and CB2).\n\nThe CB1 receptor is found primarily in the brain and mediates the psychological effects of THC. The CB2 receptor is most abundantly found on cells of the immune system. Cannabinoids act as immunomodulators at CB2 receptors, meaning they increase some immune responses and decrease others. For example, nonpsychotropic cannabinoids can be used as a very effective anti-inflammatory. The affinity of cannabinoids to bind to either receptor is about the same, with only a slight increase observed with the plant-derived compound CBD binding to CB2 receptors more frequently. Cannabinoids likely have a role in the brain’s control of movement and memory, as well as natural pain modulation. It is clear that cannabinoids can affect pain transmission and, specifically, that cannabinoids interact with the brain's endogenous opioid system and may affect dopamine transmission.\n\nMost cannabinoids are lipophilic (fat soluble) compounds that are easily stored in fat, thus yielding a long elimination half-life relative to other recreational drugs. The THC molecule, and related compounds, are usually detectable in drug tests from 3 days up to 10 days according to Redwood Laboratories; long-term users can produce positive tests for two to three months after ceasing cannabis use (see drug test).\n\nNo fatal overdoses with cannabis use have been reported as of 2006. A review published in the \"British Journal of Psychiatry\" in February 2008 said that \"no deaths directly due to acute cannabis use have ever been reported\".\n\nTHC, the principal psychoactive constituent of the cannabis plant, has an extremely low toxicity and the amount that can enter the body through the consumption of cannabis plants poses no threat of death. In dogs, the minimum lethal dose of THC is over 3 g/kg.\n\nAccording to the Merck Index, the of THC (the dose which causes the death of 50% of individuals) is 1270 mg/kg for male rats and 730 mg/kg for female rats from oral consumption in sesame oil, and 42 mg/kg for rats from inhalation.\nIt is important though to note that cannabinoids and other molecules present in cannabis can alter the metabolism of other drugs, especially due to competition for clearing metabolic pathways such as cytochromes CYP450, thus leading to drug toxicities by medications that the person consuming cannabis may be taking.\n\nA 2007 study found that while tobacco and cannabis smoke are quite similar, cannabis smoke contained higher amounts of ammonia, hydrogen cyanide, and nitrogen oxides, but lower levels of carcinogenic polycyclic aromatic hydrocarbons (PAHs). This study found that directly inhaled cannabis smoke contained as much as 20 times as much ammonia and 5 times as much hydrogen cyanide as tobacco smoke and compared the properties of both mainstream and sidestream (smoke emitted from a smouldering 'joint' or 'cone') smoke. Mainstream cannabis smoke was found to contain higher concentrations of selected polycyclic aromatic hydrocarbons (PAHs) than sidestream tobacco smoke. However, other studies have found much lower disparities in ammonia and hydrogen cyanide between cannabis and tobacco, and that some other constituents (such as polonium-210, lead, arsenic, nicotine, and tobacco-specific nitrosamines) are either lower or non-existent in cannabis smoke.\n\nCannabis smoke contains thousands of organic and inorganic chemical compounds. This tar is chemically similar to that found in tobacco smoke or cigars. Over fifty known carcinogens have been identified in cannabis smoke. These include nitrosamines, reactive aldehydes, and polycylic hydrocarbons, including benz[a]pyrene. Marijuana smoke was listed as a cancer agent in California in 2009. A study by the British Lung Foundation published in 2012 identifies cannabis smoke as a carcinogen and also finds awareness of the danger is low compared with the high awareness of the dangers of smoking tobacco particularly among younger users. Other observations include possible increased risk from each cigarette; lack of research on the effect of cannabis smoke alone; low rate of addiction compared to tobacco; and episodic nature of cannabis use compared to steady frequent smoking of tobacco. Professor David Nutt, a UK drug expert, points out that the study cited by the British Lung Foundation has been accused of both \"false reasoning\" and \"incorrect methodology\". Further, he notes that other studies have failed to connect cannabis with lung cancer, and accuses the BLF of \"scaremongering over cannabis\".\n\nWhen smoked, the short-term effects of cannabis manifest within seconds and are fully apparent within a few minutes, typically lasting for 1–3 hours, varying by the person and the strain of cannabis. After oral ingestion of cannabis, the onset of effect is delayed relative to smoking, taking 30 minutes to 2 hours, but the duration is prolonged due to continued slow absorption. The duration of noticeable effects has been observed to diminish due to prolonged, repeated use and the development of a tolerance to cannabinoids.\n\nThe psychoactive effects of cannabis, known as a \"high\", are subjective and can vary based on the person and the method of use.\n\nWhen THC enters the blood stream and reaches the brain, it binds to cannabinoid receptors. The endogenous ligand of these receptors is anandamide, the effects of which THC emulates. This agonism of the cannabinoid receptors results in changes in the levels of various neurotransmitters, especially dopamine and norepinephrine; neurotransmitters which are closely associated with the acute effects of cannabis ingestion, such as euphoria and anxiety.\nSome effects may include a general alteration of conscious perception, euphoria, feelings of well-being, relaxation or stress reduction, increased appreciation of the arts, including humor and music (especially discerning its various components/instruments), joviality, metacognition and introspection, enhanced recollection (episodic memory), increased sensuality, increased awareness of sensation, increased libido, and creativity. Abstract or philosophical thinking, disruption of linear memory and paranoia or anxiety are also typical. Anxiety is the most commonly reported side effect of smoking marijuana. Between 20 and 30 percent of recreational users experience intense anxiety and/or panic attacks after smoking cannabis, however, some report anxiety only after not smoking cannabis for a prolonged period of time. Inexperience and use in an unfamiliar environment are major contributing factors to this anxiety. Cannabidiol (CBD), another cannabinoid found in cannabis in varying amounts, has been shown to ameliorate the adverse effects of THC, including anxiety, that some consumers experience.\n\nCannabis also produces many other subjective and highly tangible effects, such as greater enjoyment of food taste and aroma, and marked distortions in the perception of time and space (where experiencing a \"rush\" of ideas from the bank of long-term memory can create the subjective impression of long elapsed time, while in reality only a short time has passed). At higher doses, effects can include altered body image, auditory and/or visual illusions, pseudohallucinations, and ataxia from selective impairment of polysynaptic reflexes. In some cases, cannabis can lead to dissociative states such as depersonalization and derealization.\n\nAny episode of acute psychosis that accompanies cannabis use usually abates after 6 hours, but in rare instances, heavy users may find the symptoms continuing for many days. If the episode is accompanied by aggression or sedation, physical restraint may be necessary.\n\nWhile many psychoactive drugs clearly fall into the category of either stimulant, depressant, or hallucinogen, cannabis exhibits a mix of all properties, perhaps leaning the most towards hallucinogenic or psychedelic properties, though with other effects quite pronounced as well. THC is typically considered the primary active component of the cannabis plant; various scientific studies have suggested that certain other cannabinoids like CBD may also play a significant role in its psychoactive effects.\n\nSome of the short-term physical effects of cannabis use include increased heart rate, dry mouth, reddening of the eyes (congestion of the conjunctival blood vessels), a reduction in intra-ocular pressure, muscle relaxation and a sensation of cold or hot hands and feet and / or flushed face.\n\nElectroencephalography or EEG shows somewhat more persistent alpha waves of slightly lower frequency than usual. Cannabinoids produce a \"marked depression of motor activity\" via activation of neuronal cannabinoid receptors belonging to the CB1 subtype.\n\nPeak levels of cannabis-associated intoxication occur approximately 30 minutes after smoking it and last for several hours.\n\nThe total short-term duration of cannabis use when smoked is based on the potency, method of smoking – e.g. whether pure or in conjunction with tobacco – and how much is smoked. Peak levels of intoxication typically last an average of three to four hours.\n\nWhen taken orally (in the form of capsules, food or drink), the psychoactive effects take longer to manifest and generally last longer, typically lasting for an average of four to ten hours after consumption. Very high doses may last even longer. Also, oral ingestion use eliminates the need to inhale toxic combustion products created by smoking and therefore negates the risk of respiratory harm associated with cannabis smoking.\n\nThe areas of the brain where cannabinoid receptors are most prevalently located are consistent with the behavioral effects produced by cannabinoids. Brain regions in which cannabinoid receptors are very abundant are the basal ganglia, associated with movement control; the cerebellum, associated with body movement coordination; the hippocampus, associated with learning, memory, and stress control; the cerebral cortex, associated with higher cognitive functions; and the nucleus accumbens, regarded as the reward center of the brain. Other regions where cannabinoid receptors are moderately concentrated are the hypothalamus, which regulates homeostatic functions; the amygdala, associated with emotional responses and fears; the spinal cord, associated with peripheral sensations like pain; the brain stem, associated with sleep, arousal, and motor control; and the nucleus of the solitary tract, associated with visceral sensations like nausea and vomiting.\n\nExperiments on animal and human tissue have demonstrated a disruption of short-term memory formation, which is consistent with the abundance of C receptors on the hippocampus, the region of the brain most closely associated with memory. Cannabinoids inhibit the release of several neurotransmitters in the hippocampus such as acetylcholine, norepinephrine, and glutamate, resulting in a major decrease in neuronal activity in that region. This decrease in activity resembles a \"temporary hippocampal lesion.\" \n\nIn \"in-vitro\" experiments THC at extremely high concentrations, which could not be reached with commonly consumed doses, caused competitive inhibition of the AChE enzyme and inhibition of β-amyloid peptide aggregation, implicated in the development of Alzheimer's disease. Compared to currently approved drugs prescribed for the treatment of Alzheimer's disease, THC is a considerably superior inhibitor of A aggregation, and this study provides a previously unrecognized molecular mechanism through which cannabinoid molecules may impact the progression of this debilitating disease.\n\nWhile several studies have shown increased risk associated with cannabis use by drivers, other studies have not found increased risk. Cannabis usage has been shown in some studies to have a negative effect on driving ability.\nThe British Medical Journal indicated that \"drivers who consume cannabis within three hours of driving are nearly twice as likely to cause a vehicle collision as those who are not under the influence of drugs or alcohol\".\n\nIn \"Cannabis and driving: a review of the literature and commentary\", the United Kingdom's Department for Transport reviewed data on cannabis and driving, finding although impaired, \"subjects under cannabis treatment appear to perceive that they are indeed impaired. Where they can compensate, they do...\". In a review of driving simulator studies, researchers note that \"even in those who learn to compensate for a drug's impairing effects, substantial impairment in performance can still be observed under conditions of general task performance (i.e. when no contingencies are present to maintain compensated performance).\"\n\nA 2012 meta-analysis found that acute cannabis use increased the risk of an automobile crash. An extensive 2013 review of 66 studies regarding crash risk and drug use found that cannabis was associated with minor, but not statistically significant increased odds of injury or fatal accident.\n\nIn the largest and most precisely controlled study of its kind carried out by the U.S. Department of Transportation’s National Highway Traffic Safety Administration, it was found that other \"studies that measure the presence of THC in the drivers' blood or oral fluid, rather than relying on self-report tend to have much lower (or no) elevated crash risk estimates. Likewise better controlled studies have found lower (or no) elevated crash risk estimates\". The study found that \"after adjusting for age, gender, race and alcohol use, drivers who tested positive for marijuana were no more likely to crash than those who had not used any drugs or alcohol prior to driving\".\n\nOn the other hand, a recent study of Journal of Transport & Health indicated that the numbers of fatal crashes involving marijuana after the recreational marijuana legalization or decriminalization have significantly increased in Colorado, Washington, and Massachusetts.\n\nShort-term (one to two hours) effects on the cardiovascular system can include increased heart rate, dilation of blood vessels, and fluctuations in blood pressure. There are medical reports of occasional heart attacks or myocardial infarction, stroke and other cardiovascular side effects. Marijuana's cardiovascular effects are not associated with serious health problems for most young, healthy users. Researchers reported in the \"International Journal of Cardiology\", \"Marijuana use by older people, particularly those with some degree of coronary artery or cerebrovascular disease, poses greater risks due to the resulting increase in catecholamines, cardiac workload, and carboxyhemoglobin levels, and concurrent episodes of profound postural hypotension. Indeed, marijuana may be a much more common cause of myocardial infarction than is generally recognized. In day-to-day practice, a history of marijuana use is often not sought by many practitioners, and even when sought, the patient's response is not always truthful\".\n\nA 2013 analysis of 3,886 myocardial infarction survivors over an 18-year period showed \"no statistically significant association between marijuana use and mortality\".\n\nA 2008 study by the National Institutes of Health Biomedical Research Centre in Baltimore found that heavy, chronic smoking of marijuana (138 joints per week) changed blood proteins associated with heart disease and stroke.\n\nA 2000 study by researchers at Boston's Beth Israel Deaconess Medical Center, Massachusetts General Hospital and Harvard School of Public Health found that a middle-age person's risk of heart attack rises nearly fivefold in the first hour after smoking marijuana, \"roughly the same risk seen within an hour of sexual activity\".\n\nCannabis arteritis is a very rare peripheral vascular disease similar to Buerger's disease. There were about 50 confirmed cases from 1960 to 2008, all of which occurred in Europe.\n\nA confounding factor in cannabis research is the prevalent usage of other recreational drugs, especially alcohol and nicotine. Such complications demonstrate the need for studies on cannabis that have stronger controls, and investigations into alleged symptoms of cannabis use that may also be caused by tobacco. Some critics question whether agencies doing the research make an honest effort to present an accurate, unbiased summary of the evidence, or whether they \"cherry-pick\" their data to please funding sources which may include the tobacco industry or governments dependent on cigarette tax revenue; others caution that the raw data, and not the final conclusions, are what should be examined.\n\nThe Australian National Household Survey of 2001 showed that cannabis in Australia is rarely used without other drugs. 95% of cannabis users also drank alcohol; 26% took amphetamines; 19% took ecstasy and only 2.7% reported not having used any other drug with cannabis. While research has been undertaken on the combined effects of alcohol and cannabis on performing certain tasks, little research has been conducted on the reasons why this combination is so popular. Evidence from a controlled experimental study undertaken by Lukas and Orozco suggests that alcohol causes THC to be absorbed more rapidly into the blood plasma of the user. Data from the Australian National Survey of Mental Health and Wellbeing found that three-quarters of recent cannabis users reported using alcohol when cannabis was not available, this suggests that the two are substitutes.\n\nStudies on cannabis and memory are hindered by small sample sizes, confounding drug use, and other factors. The strongest evidence regarding cannabis and memory focuses on its temporary negative effects on short-term and working memory.\n\nIn a 2001 study looking at neuropsychological performance in long-term cannabis users, researchers found \"some cognitive deficits appear detectable at least 7 days after heavy cannabis use but appear reversible and related to recent cannabis exposure rather than irreversible and related to cumulative lifetime use\". On his studies regarding cannabis use, lead researcher and Harvard professor Harrison Pope said he found marijuana is not dangerous over the long term, but there are short-term effects. From neuropsychological tests, Pope found that chronic cannabis users showed difficulties, with verbal memory in particular, for \"at least a week or two\" after they stopped smoking. Within 28 days, memory problems vanished and the subjects \"were no longer distinguishable from the comparison group\".\nResearchers from the University of California, San Diego School of Medicine failed to show substantial, systemic neurological effects from long-term recreational use of cannabis. Their findings were published in the July 2003 issue of the \"Journal of the International Neuropsychological Society\". The research team, headed by Dr Igor Grant, found that cannabis use did affect perception, but did not cause permanent brain damage. Researchers looked at data from 15 previously published controlled studies involving 704 long-term cannabis users and 484 nonusers. The results showed long-term cannabis use was only marginally harmful on the memory and learning. Other functions such as reaction time, attention, language, reasoning ability, perceptual and motor skills were unaffected. The observed effects on memory and learning, they said, showed long-term cannabis use caused \"selective memory defects\", but that the impact was \"of a very small magnitude\". A study at Johns Hopkins University School of Medicine showed that very heavy use of marijuana is associated with decrements in neurocognitive performance even after 28 days of abstinence.\n\nThe feeling of increased appetite following the use of cannabis has been documented for hundreds of years, and is known colloquially as \"the munchies\" in the English-speaking world. Clinical studies and survey data have found that cannabis increases food enjoyment and interest in food. A 2015 study suggests that cannabis triggers uncharacteristic behaviour in POMC neurons, which are usually associated with decreasing hunger. Rarely, chronic users experience a severe vomiting disorder, cannabinoid hyperemesis syndrome, after smoking and find relief by taking hot baths.\n\nEndogenous cannabinoids (\"endocannabinoids\") were discovered in cow's milk and soft cheeses. Endocannabinoids are also found in human breast milk. It is widely accepted that the neonatal survival of many species \"is largely dependent upon their suckling behavior, or appetite for breast milk\" and recent research has identified the endogenous cannabinoid system to be the first neural system to display complete control over milk ingestion and neonatal survival. It is possible that \"cannabinoid receptors in our body interact with the cannabinoids in milk to stimulate a suckling response in newborns so as to prevent growth failure\".\n\nMost microorganisms found in cannabis only affect plants and not humans, but some microorganisms, especially those that proliferate when the herb is not correctly dried and stored, can be harmful to humans. Some users may store marijuana in an airtight bag or jar in a refrigerator to prevent fungal and bacterial growth.\n\nThe fungi \"Aspergillus flavus\", \"Aspergillus fumigatus\", \"Aspergillus niger\", \"Aspergillus parasiticus\", \"Aspergillus tamarii\", \"Aspergillus sulphureus\", \"Aspergillus repens\", \"Mucor hiemalis\" (not a human pathogen), \"Penicillium chrysogenum\", \"Penicillium italicum\" and \"Rhizopus nigrans\" have been found in moldy cannabis. \"Aspergillus\" mold species can infect the lungs via smoking or handling of infected cannabis and cause opportunistic and sometimes deadly aspergillosis. Some of the microorganisms found create aflatoxins, which are toxic and carcinogenic. Researchers suggest that moldy cannabis should thus be discarded to avoid these serious risks.\n\nMold is also found in smoke from mold-infected cannabis, and the lungs and nasal passages are a major means of contracting fungal infections. Levitz and Diamond (1991) suggested baking marijuana in home ovens at 150 °C [302 °F], for five minutes before smoking. Oven treatment killed conidia of \"A. fumigatus\", \"A. flavus\" and \"A. niger\", and did not degrade the active component of marijuana, tetrahydrocannabinol (THC).\"\n\nCannabis contaminated with \"Salmonella muenchen\" was positively correlated with dozens of cases of salmonellosis in 1981. \"Thermophilic actinomycetes\" were also found in cannabis.\n\nExposure to marijuana may have biologically-based physical, mental, behavioral and social health consequences and is \"associated with diseases of the liver (particularly with co-existing hepatitis C), lungs, heart, eyesight and vasculature\" according to a 2013 literature review by Gordon and colleagues. The association with these diseases has only been reported in cases where people have smoked cannabis. The authors cautioned that \"evidence is needed, and further research should be considered, to prove causal associations of marijuana with many physical health conditions\".\n\nCannabis use disorder is defined in the fifth revision of the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM-5) as a condition requiring treatment. Several drugs have been investigated in an attempt to ameliorate the symptoms of stopping cannabis use. Such drugs include bupropion, divalproex, nefazodone, lofexidine, and dronabinol. Of these, dronabinol has proven the most effective.\n\nCannabis consumption in pregnancy might be associated with restrictions in growth of the fetus, miscarriage, and cognitive deficits in offspring based on animal studies, although there is limited evidence for this in humans at this time. A 2012 systematic review found although it was difficult to draw firm conclusions, there was some evidence that prenatal exposure to cannabis was associated with \"deficits in language, attention, areas of cognitive performance, and delinquent behavior in adolescence\". A report prepared for the Australian National Council on Drugs concluded cannabis and other cannabinoids are contraindicated in pregnancy as it may interact with the endocannabinoid system.\n\n\n\n"}
{"id": "17348660", "url": "https://en.wikipedia.org/wiki?curid=17348660", "title": "Elimination diet", "text": "Elimination diet\n\nAn elimination diet, also known as exclusion diet is a diagnostic procedure used to identify foods that an individual cannot consume without adverse effects. Adverse effects may be due to food allergy, food intolerance, other physiological mechanisms (such as metabolic or toxins), or a combination of these. Elimination diets typically involve entirely removing a suspected food from the diet for a period of time from two weeks to two months, and waiting to determine whether symptoms resolve during that time period. In rare cases, a health professional may wish to use an elimination diet, also referred to as an oligoantigenic diet, to relieve a patient of symptoms they are experiencing.\n\nCommon reasons for undertaking an elimination diet include suspected food allergies and suspected food intolerances. An elimination diet might remove one or more common foods, such as eggs or milk, or it might remove one or more minor or non-nutritive substances, such as artificial food colorings.\n\nAn elimination diet relies on trial and error to identify specific allergies and intolerances. Typically, if symptoms resolve after the removal of a food from the diet, then the food is reintroduced to see whether the symptoms reappear. This challenge–dechallenge–rechallenge approach has been claimed to be particularly useful in cases with intermittent or vague symptoms.\n\nThe exclusion diet can be a diagnostic tool or method used temporarily to determine whether a patient’s symptoms are food-related. The term elimination diet is also used to describe a \"treatment diet\", which eliminates certain foods for a patient.\nAdverse reactions to food can be due to several mechanisms. Correct identification of the type of reaction in an individual is important, as different approaches to management may be required. The area of food allergies and intolerances has been controversial and is currently a topic that is heavily researched. It has been characterised in the past by lack of universal acceptance of definitions, diagnosis and treatment.\n\nThe concept of the elimination diet was first proposed by Dr. Albert Rowe in 1926 and expounded upon in his book, \"Elimination Diets and the Patient's Allergies\", published in 1941.\n\nIn 1978 Australian researchers published details of an 'exclusion diet' to exclude specific food chemicals from the diet of patients. This provided a basis for challenge with these additives and natural chemicals. Using this approach, the role played by dietary chemical factors in the pathogenesis of chronic idiopathic urticaria (CIU) was first established and set the stage for future DBPCT trials of such substances in food intolerance studies.\n\n'Food hypersensitivity' is an umbrella term which includes food allergy and food intolerance.\n\nFood allergy is defined as an immunological hypersensitivity which occurs most commonly to food proteins such as egg, milk, seafood, shellfish, tree nuts, soya, wheat and peanuts. Its biological response mechanism is characterized by an increased production of IgE (immunoglobulin E) antibodies.\n\nA food intolerance on the other hand does not activate the individual's immune response system. A food intolerance differs from a food allergy or chemical sensitivity because it generally requires a normal serving size to produce symptoms similar to an IgE immunologic response. While food intolerances may be mistaken for a food allergy, they are thought to originate in the gastrointestinal system. Food intolerances are usually caused by the individual’s inability to digest or absorb foods or food components in the intestinal tract. One common example of food intolerance is lactose intolerance.\n\n\nElimination diets are useful to assist in the diagnosis of food allergy and pharmacological food intolerance. Metabolic, toxic and psychological reactions should be diagnosed by other means.\n\nFood allergy is principally diagnosed by careful history and examination. When reactions occur immediately after certain food ingestion then diagnosis is straight forward and can be documented by using carefully performed tests such as the skin prick test (SPT) and the radioallergosorbent test RAST to detect specific IgE antibodies to specific food proteins and aero-allergens. However false positive results occur when using the SPT when diagnosis of a particular food allergen is hard to determine. This can be confirmed by exclusion of the suspected food or allergen from the patient's diet. It is then followed by an appropriately timed challenge under careful medical supervision. If there is no change of symptoms after 2 to 4 weeks of avoidance of the protein then food allergy is unlikely to be the cause and other causes such as food intolerance should be investigated. This method of exclusion-challenge testing is the premise by which the Elimination Diet is built upon, as explained in the sections below.\n\nVega machine testing, a bioelectric test, is a controversial method that attempt to measure allergies or food or environmental intolerances. Currently this test has not been shown to be an effective measure of an allergy or intolerance.\n\nFood intolerance due to pharmacological reaction is more common than food allergy and has been estimated to occur in 10% of the population. Unlike a food allergy, a food intolerance can occur in non-atopic individuals. Food intolerances are more difficult to diagnose since individual food chemicals are widespread and can occur across a range of foods. Elimination of these foods one at a time would be unhelpful in diagnosing the sensitivity. Natural chemicals such as benzoates and salicylates found in food are identical to artificial additives in food processing and can provoke the same response. Since a specific component is not readily known and the reactions are often delayed up to 48 hours after ingestion, it can be difficult to identify suspect foods. In addition, chemicals often exhibit dose-response relationships and so the food may not trigger the same response each time. There is currently no skin or blood test available to identify the offending chemical(s), and consequently, elimination diets aimed at identifying food intolerances need to be carefully designed. All patients with suspected food intolerance should consult a physician first to eliminate other possible causes.\n\nThe elimination diet must be comprehensive and should contain only those foods unlikely to provoke a reaction in a patient. They also need to be able to provide complete nutrition and energy for the weeks it will be conducted. Professional nutritional advice from a dietitian or nutritionist is strongly recommended. Thorough education about the elimination diet is essential to ensure patients and the parents of children with suspected food intolerance understand the importance of complete adherence to the diet, as inadvertent consumption of an offending chemical can prevent resolution of symptoms and render challenge results useless.\n\nWhile on the elimination diet, records are kept of all foods eaten, medications taken, and symptoms that the patient may be experiencing. Patients are advised that withdrawal symptoms can occur in the first weeks on the elimination diet and some patients may experience symptoms that are worse initially before settling down.\n\nWhile on the diet some patients become sensitive to fumes and odours, which may also cause symptoms. They are advised to avoid such exposures as this can complicate the elimination and challenge procedures. Petroleum products, paints, cleaning agents, perfumes, smoke and pressure pack sprays are particular chemicals to avoid when participating in an elimination diet. Once the procedure is complete this sensitivity becomes less of a problem.\n\nClinical improvement usually occurs over a 2 to 4 week period; if there is no change after a strict adherence to the elimination diet and precipitating factors, then food intolerance is unlikely to be the cause. A normal diet can then be resumed by gradually introducing suspected and eliminated foods or chemical group of foods one at a time. Gradually increasing the amount up to high doses over 3 to 7 days to see if exacerbated reactions are provoked before permanently reintroducing that food to the diet. A strict elimination diet is not usually recommended during pregnancy, although a reduction in suspected foods that reduce symptoms can be helpful.\n\nChallenge testing is not carried out until all symptoms have cleared or improved significantly for five days after a minimum period of two weeks on the elimination diet. The restrictions of the elimination diet is maintained throughout the challenge period. Open food challenges on wheat and milk can be carried out first, then followed by challenge periods with natural food chemicals, then with food additives. Challenges can take the form of purified food chemicals or with foods grouped according to food chemical. Purified food chemicals are used in double blind placebo controlled testing, and food challenges involve foods containing only one suspect food chemical eaten several times a day over 3 to 7 days. If a reaction occurs patients must wait until all symptoms subside completely and then wait a further 3 days (to overcome a refractory period) before recommencing challenges. Patients with a history of asthma, laryngeal oedema or anaphylaxis may be hospitalised as inpatients or attended in specialist clinics where resuscitation facilities are available for the testing.\n\nIf any results are doubtful the testing is repeated, only when all tests are completed is a treatment diet determined for the patient. The diet restricts only those compounds to which the patient has reacted and over time liberalisation is attempted.\nIn some patients food allergy and food intolerance can coexist, with symptoms such as asthma, eczema and rhinitis. In such cases the elimination diet for food intolerance is used for dietary investigation. Any foods identified by SPT or RAST as suspect should not be included in the elimination diet.\n\n"}
{"id": "18685571", "url": "https://en.wikipedia.org/wiki?curid=18685571", "title": "Frasier syndrome", "text": "Frasier syndrome\n\nFrasier syndrome is a urogenital anomaly associated with the \"WT1\" (Wilms tumor 1 gene) gene.\n\nIt was first characterized in 1964.\n\nFrasier syndrome presents at birth with male pseudohermaphroditism (the external genitalia have a female appearance despite an XY genotype), streak gonads and progressive glomerulonephropathy (focal segmental glomerulosclerosis). Patients are also at increased risk of genito-urinary tumors (usually gonadoblastoma).\n\nThe glomerulonephropathy presents later than in Denys-Drash syndrome, and the tumour risk phenotype is different; whilst Denys-Drash syndrome is associated with Wilms' tumour, Frasier syndrome is associated with gonadoblastoma. Differentiating between the two syndromes can be challenging.\n\nThe \"WT1\" gene exists on chromosome 11 (at 11p13), and codes for a four zinc finger transcription factor. Its role as a transcription factor is related to proper kidney and gonadal development. The link between kidney and gonadal development and \"WT1\" was highlighted in past studies looking at the related Denys-Drash syndrome. Results of various investigations identified the loss of function of \"WT1\" to be a prerequisite of Wilms' tumour development, and also a key trait of individuals with genital abnormalities.\n\nMutations responsible for Frasier syndrome predominantly occur in intron 9 of the \"WT1\" gene, specifically nucleotide substitutions that influence an intron splice site. Mutations in this region proved for the absence of three amino acids—KTS—between the third and fourth \"WT1\" zinc fingers. Referring to the autosomal dominant expressive nature of this disease, it is only necessary for an individual to have one compliment of the mutated intronic sequence to appear affected. Differing from the similar Denys-Drash syndrome, where a mutated form of the WT1 protein exists, Frasier syndrome expression works solely on the existence of a changed ratio of KTS isoforms: normal WT1 proteins including the KTS site (+KTS), and mutated, shortened proteins lacking the KTS site (–KTS). Through alternative splicing, a specific ratio of the two isoforms normally exists, though the mutation in the intron 9 splice site severely lowers levels of the +KTS isoform; this leads to Frasier syndrome.\n\nFrasier syndrome is inherited in an autosomal dominant fashion, indicating the need for only one mutated allele in a cell to lead to expression of the disease. Mutations predominantly occur \"de novo\", allowing for expression in an individual that has no family history of it. The mutations occur during gamete formation or early in embryogenesis.\n\nReconstructive surgery.\n"}
{"id": "374298", "url": "https://en.wikipedia.org/wiki?curid=374298", "title": "G factor (psychometrics)", "text": "G factor (psychometrics)\n\nThe g\" factor (also known as general intelligence, general mental ability or general intelligence factor) is a construct developed in psychometric investigations of cognitive abilities and human intelligence; it is often said to be the most important construct to intelligence, even as no-one in the field says it is all there is to it. It's a variable that summarizes positive correlations among different cognitive tasks, reflecting the fact that an individual's performance on one type of cognitive task tends to be comparable to that person's performance on other kinds of cognitive tasks. The \"g\" factor typically accounts for 40 to 50 percent of the between-individual performance differences on a given cognitive test, and composite scores (\"IQ scores\") based on many tests are frequently regarded as estimates of individuals' standing on the \"g\" factor. The terms IQ, general intelligence, general cognitive ability, general mental ability, or simply intelligence are often used interchangeably to refer to this common core shared by cognitive tests. The g\" factor targets a particular measure of \"general intelligence\". \n\nThe existence of the \"g\" factor was originally proposed by the English psychologist Charles Spearman in the early years of the 20th century. He observed that children's performance ratings, across seemingly unrelated school subjects, were positively correlated, and reasoned that these correlations reflected the influence of an underlying general mental ability that entered into performance on all kinds of mental tests. Spearman suggested that all mental performance could be conceptualized in terms of a single general ability factor, which he labeled \"g\", and a large number of narrow task-specific ability factors. Soon after Spearman proposed the existence of \"g\", its existence was challenged by Godfrey Thomson, who presented evidence that Spearman's finding of intercorrelations among test results was neither inconsistent with the existence of \"g\" nor with its nonexistence. Today's factor models of intelligence typically represent cognitive abilities as a three-level hierarchy, where there are a large number of narrow factors at the bottom of the hierarchy, a handful of broad, more general factors at the intermediate level, and at the apex a single factor, referred to as the \"g\" factor, which represents the variance common to all cognitive tasks.\n\nTraditionally, research on \"g\" has concentrated on psychometric investigations of test data, with a special emphasis on factor analytic approaches. However, empirical research on the nature of \"g\" has also drawn upon experimental cognitive psychology and mental chronometry, brain anatomy and physiology, quantitative and molecular genetics, and primate evolution. While the existence of \"g\" as a statistical regularity is well-established and uncontroversial, there is no consensus as to what causes the positive correlations between tests.\n\nResearch in the field of behavioral genetics has established that the construct of \"g\" is highly heritable. It has a number of other biological correlates, including brain size. It is also a significant predictor of individual differences in many social outcomes, particularly in education and employment. The most widely accepted contemporary theories of intelligence incorporate the \"g\" factor. However, critics of \"g\" have contended that an emphasis on \"g\" is misplaced and entails a devaluation of other important abilities, as well as supporting an unrealistic reified view of human intelligence. Some critics have gone so far as to argue that \"g\" \"...is to the psychometricians what Huygens' ether was to early physicists: a nonentity taken as an article of faith instead of one in need of verification by real data.\"\n\nCognitive ability tests are designed to measure different aspects of cognition. Specific domains assessed by tests include mathematical skill, verbal fluency, spatial visualization, and memory, among others. However, individuals who excel at one type of test tend to excel at other kinds of tests, too, while those who do poorly on one test tend to do so on all tests, regardless of the tests' contents. The English psychologist Charles Spearman was the first to describe this phenomenon. In a famous research paper published in 1904, he observed that children's performance measures across seemingly unrelated school subjects were positively correlated. This finding has since been replicated numerous times. The consistent finding of universally positive correlation matrices of mental test results (or the \"positive manifold\"), despite large differences in tests' contents, has been described as \"arguably the most replicated result in all psychology\". Zero or negative correlations between tests suggest the presence of sampling error or restriction of the range of ability in the sample studied.\n\nUsing factor analysis or related statistical methods, it is possible to compute a single common factor that can be regarded as a summary variable characterizing the correlations between all the different tests in a test battery. Spearman referred to this common factor as the \"general factor\", or simply \"g\". (By convention, \"g\" is always printed as a lower case italic.) Mathematically, the \"g\" factor is \"a source of variance among individuals\", which entails that one cannot meaningfully speak of any one individual's mental abilities consisting of \"g\" or other factors to any specified degrees. One can only speak of an individual's standing on \"g\" (or other factors) compared to other individuals in a relevant population.\n\nDifferent tests in a test battery may correlate with (or \"load onto\") the \"g\" factor of the battery to different degrees. These correlations are known as \"g\" loadings. An individual test taker's \"g\" factor score, representing his or her relative standing on the \"g\" factor in the total group of individuals, can be estimated using the \"g\" loadings. Full-scale IQ scores from a test battery will usually be highly correlated with \"g\" factor scores, and they are often regarded as estimates of \"g\". For example, the correlations between \"g\" factor scores and full-scale IQ scores from David Wechsler's tests have been found to be greater than .95. The terms IQ, general intelligence, general cognitive ability, general mental ability, or simply intelligence are frequently used interchangeably to refer to the common core shared by cognitive tests.\n\nThe \"g\" loadings of mental tests are always positive and usually range between .10 and .90, with a mean of about .60 and a standard deviation of about .15. Raven's Progressive Matrices is among the tests with the highest \"g\" loadings, around .80. Tests of vocabulary and general information are also typically found to have high \"g\" loadings. However, the \"g\" loading of the same test may vary somewhat depending on the composition of the test battery.\n\nThe complexity of tests and the demands they place on mental manipulation are related to the tests' \"g\" loadings. For example, in the forward digit span test the subject is asked to repeat a sequence of digits in the order of their presentation after hearing them once at a rate of one digit per second. The backward digit span test is otherwise the same except that the subject is asked to repeat the digits in the reverse order to that in which they were presented. The backward digit span test is more complex than the forward digit span test, and it has a significantly higher \"g\" loading. Similarly, the \"g\" loadings of arithmetic computation, spelling, and word reading tests are lower than those of arithmetic problem solving, text composition, and reading comprehension tests, respectively.\n\nTest difficulty and \"g\" loadings are distinct concepts that may or may not be empirically related in any specific situation. Tests that have the same difficulty level, as indexed by the proportion of test items that are failed by test takers, may exhibit a wide range of \"g\" loadings. For example, tests of rote memory have been shown to have the same level of difficulty but considerably lower \"g\" loadings than many tests that involve reasoning.\n\nWhile the existence of \"g\" as a statistical regularity is well-established and uncontroversial among experts, there is no consensus as to what causes the positive intercorrelations. Several explanations have been proposed.\n\nCharles Spearman reasoned that correlations between tests reflected the influence of a common causal factor, a general mental ability that enters into performance on all kinds of mental tasks. However, he thought that the best indicators of \"g\" were those tests that reflected what he called \"the eduction of relations and correlates\", which included abilities such as deduction, induction, problem solving, grasping relationships, inferring rules, and spotting differences and similarities. Spearman hypothesized that \"g\" was equivalent with \"mental energy\". However, this was more of a metaphorical explanation, and he remained agnostic about the physical basis of this energy, expecting that future research would uncover the exact physiological nature of \"g\".\n\nFollowing Spearman, Arthur Jensen maintained that all mental tasks tap into \"g\" to some degree. According to Jensen, the \"g\" factor represents a \"distillate\" of scores on different tests rather than a summation or an average of such scores, with factor analysis acting as the distillation procedure. He argued that \"g\" cannot be described in terms of the item characteristics or information content of tests, pointing out that very dissimilar mental tasks may have nearly equal \"g\" loadings. Wechsler similarly contended that \"g\" is not an ability at all but rather some general property of the brain. Jensen hypothesized that \"g\" corresponds to individual differences in the speed or efficiency of the neural processes associated with mental abilities. He also suggested that given the associations between \"g\" and elementary cognitive tasks, it should be possible to construct a ratio scale test of \"g\" that uses time as the unit of measurement.\n\nThe so-called sampling theory of \"g\", originally developed by E.L. Thorndike and Godfrey Thomson, proposes that the existence of the positive manifold can be explained without reference to a unitary underlying capacity. According to this theory, there are a number of uncorrelated mental processes, and all tests draw upon different samples of these processes. The intercorrelations between tests are caused by an overlap between processes tapped by the tests. Thus, the positive manifold arises due to a measurement problem, an inability to measure more fine-grained, presumably uncorrelated mental processes.\n\nIt has been shown that it is not possible to distinguish statistically between Spearman's model of \"g\" and the sampling model; both are equally able to account for intercorrelations among tests. The sampling theory is also consistent with the observation that more complex mental tasks have higher \"g\" loadings, because more complex tasks are expected to involve a larger sampling of neural elements and therefore have more of them in common with other tasks.\n\nSome researchers have argued that the sampling model invalidates \"g\" as a psychological concept, because the model suggests that \"g\" factors derived from different test batteries simply reflect the shared elements of the particular tests contained in each battery rather than a \"g\" that is common to all tests. Similarly, high correlations between different batteries could be due to them measuring the same set of abilities rather than \"the\" same ability.\n\nCritics have argued that the sampling theory is incongruent with certain empirical findings. Based on the sampling theory, one might expect that related cognitive tests share many elements and thus be highly correlated. However, some closely related tests, such as forward and backward digit span, are only modestly correlated, while some seemingly completely dissimilar tests, such as vocabulary tests and Raven's matrices, are consistently highly correlated. Another problematic finding is that brain damage frequently leads to specific cognitive impairments rather than a general impairment one might expect based on the sampling theory.\n\nThe \"mutualism\" model of \"g\" proposes that cognitive processes are initially uncorrelated, but that the positive manifold arises during individual development due to mutual beneficial relations between cognitive processes. Thus there is no single process or capacity underlying the positive correlations between tests. During the course of development, the theory holds, any one particularly efficient process will benefit other processes, with the result that the processes will end up being correlated with one another. Thus similarly high IQs in different persons may stem from quite different initial advantages that they had. Critics have argued that the observed correlations between the \"g\" loadings and the heritability coefficients of subtests are problematic for the mutualism theory.\n\nFactor analysis is a family of mathematical techniques that can be used to represent correlations between intelligence tests in terms of a smaller number of variables known as factors. The purpose is to simplify the correlation matrix by using hypothetical underlying factors to explain the patterns in it. When all correlations in a matrix are positive, as they are in the case of IQ, factor analysis will yield a general factor common to all tests. The general factor of IQ tests is referred to as the \"g\" factor, and it typically accounts for 40 to 50 percent of the variance in IQ test batteries. The presence of correlations between many widely varying cognitive tests has often been taken as evidence for the existence of \"g\", but McFarland (2012) showed that such correlations do not provide any more or less support for the existence of \"g\" than for the existence of multiple factors of intelligence.\n\nCharles Spearman developed factor analysis in order to study correlations between tests. Initially, he developed a model of intelligence in which variations in all intelligence test scores are explained by only two kinds of variables: first, factors that are specific to each test (denoted \"s\"); and second, a \"g\" factor that accounts for the positive correlations across tests. This is known as Spearman's two-factor theory. Later research based on more diverse test batteries than those used by Spearman demonstrated that \"g\" alone could not account for all correlations between tests. Specifically, it was found that even after controlling for \"g\", some tests were still correlated with each other. This led to the postulation of \"group factors\" that represent variance that groups of tests with similar task demands (e.g., verbal, spatial, or numerical) have in common in addition to the shared \"g\" variance.\n\nThrough factor rotation, it is, in principle, possible to produce an infinite number of different factor solutions that are mathematically equivalent in their ability to account for the intercorrelations among cognitive tests. These include solutions that do not contain a \"g\" factor. Thus factor analysis alone cannot establish what the underlying structure of intelligence is. In choosing between different factor solutions, researchers have to examine the results of factor analysis together with other information about the structure of cognitive abilities.\n\nThere are many psychologically relevant reasons for preferring factor solutions that contain a \"g\" factor. These include the existence of the positive manifold, the fact that certain kinds of tests (generally the more complex ones) have consistently larger \"g\" loadings, the substantial invariance of \"g\" factors across different test batteries, the impossibility of constructing test batteries that do not yield a \"g\" factor, and the widespread practical validity of \"g\" as a predictor of individual outcomes. The \"g\" factor, together with group factors, best represents the empirically established fact that, on average, overall ability differences \"between\" individuals are greater than differences among abilities \"within\" individuals, while a factor solution with orthogonal factors without \"g\" obscures this fact. Moreover, \"g\" appears to be the most heritable component of intelligence. Research utilizing the techniques of confirmatory factor analysis has also provided support for the existence of \"g\".\n\nA \"g\" factor can be computed from a correlation matrix of test results using several different methods. These include exploratory factor analysis, principal components analysis (PCA), and confirmatory factor analysis. Different factor-extraction methods produce highly consistent results, although PCA has sometimes been found to produce inflated estimates of the influence of \"g\" on test scores.\n\nThere is a broad contemporary consensus that cognitive variance between people can be conceptualized at three hierarchical levels, distinguished by their degree of generality. At the lowest, least general level there are a large number of narrow first-order factors; at a higher level, there are a relatively small number – somewhere between five and ten – of broad (i.e., more general) second-order factors (or group factors); and at the apex, there is a single third-order factor, \"g\", the general factor common to all tests. The \"g\" factor usually accounts for the majority of the total common factor variance of IQ test batteries. Contemporary hierarchical models of intelligence include the three stratum theory and the Cattell–Horn–Carroll theory.\n\nSpearman proposed the principle of the \"indifference of the indicator\", according to which the precise content of intelligence tests is unimportant for the purposes of identifying \"g\", because \"g\" enters into performance on all kinds of tests. Any test can therefore be used as an indicator of \"g\". Following Spearman, Arthur Jensen more recently argued that a \"g\" factor extracted from one test battery will always be the same, within the limits of measurement error, as that extracted from another battery, provided that the batteries are large and diverse. According to this view, every mental test, no matter how distinctive, calls on \"g\" to some extent. Thus a composite score of a number of different tests will load onto \"g\" more strongly than any of the individual test scores, because the \"g\" components cumulate into the composite score, while the uncorrelated non-\"g\" components will cancel each other out. Theoretically, the composite score of an infinitely large, diverse test battery would, then, be a perfect measure of \"g\".\n\nIn contrast, L.L. Thurstone argued that a \"g\" factor extracted from a test battery reflects the average of all the abilities called for by the particular battery, and that \"g\" therefore varies from one battery to another and \"has no fundamental psychological significance.\" Along similar lines, John Horn argued that \"g\" factors are meaningless because they are not invariant across test batteries, maintaining that correlations between different ability measures arise because it is difficult to define a human action that depends on just one ability.\n\nTo show that different batteries reflect the same \"g\", one must administer several test batteries to the same individuals, extract \"g\" factors from each battery, and show that the factors are highly correlated. This can be done within a confirmatory factor analysis framework. Wendy Johnson and colleagues have published two such studies. The first found that the correlations between \"g\" factors extracted from three different batteries were .99, .99, and 1.00, supporting the hypothesis that \"g\" factors from different batteries are the same and that the identification of \"g\" is not dependent on the specific abilities assessed. The second study found that \"g\" factors derived from four of five test batteries correlated at between .95–1.00, while the correlations ranged from .79 to .96 for the fifth battery, the Cattell Culture Fair Intelligence Test (the CFIT). They attributed the somewhat lower correlations with the CFIT battery to its lack of content diversity for it contains only matrix-type items, and interpreted the findings as supporting the contention that \"g\" factors derived from different test batteries are the same provided that the batteries are diverse enough. The results suggest that the same \"g\" can be consistently identified from different test batteries.\n\nThe form of the population distribution of \"g\" is unknown, because \"g\" cannot be measured on a ratio scale. (The distributions of scores on typical IQ tests are roughly normal, but this is achieved by construction, i.e., by normalizing the raw scores.) It has been argued that there are nevertheless good reasons for supposing that \"g\" is normally distributed in the general population, at least within a range of ±2 standard deviations from the mean. In particular, \"g\" can be thought of as a composite variable that reflects the additive effects of a large number of independent genetic and environmental influences, and such a variable should, according to the central limit theorem, follow a normal distribution.\n\nA number of researchers have suggested that the proportion of variation accounted for by \"g\" may not be uniform across all subgroups within a population. \"Spearman's law of diminishing returns\" (SLODR), also termed the \"cognitive ability differentiation hypothesis\", predicts that the positive correlations among different cognitive abilities are weaker among more intelligent subgroups of individuals. More specifically, (SLODR) predicts that the \"g\" factor will account for a smaller proportion of individual differences in cognitive tests scores at higher scores on the \"g\" factor.\n\n(SLODR) was originally proposed by Charles Spearman, who reported that the average correlation between 12 cognitive ability tests was .466 in 78 normal children, and .782 in 22 \"defective\" children. Detterman and Daniel rediscovered this phenomenon in 1989. They reported that for subtests of both the WAIS and the WISC, subtest intercorrelations decreased monotonically with ability group, ranging from approximately an average intercorrelation of .7 among individuals with IQs less than 78 to .4 among individuals with IQs greater than 122.\n\n(SLODR) has been replicated in a variety of child and adult samples who have been measured using broad arrays of cognitive tests. The most common approach has been to divide individuals into multiple ability groups using an observable proxy for their general intellectual ability, and then to either compare the average interrelation among the subtests across the different groups, or to compare the proportion of variation accounted for by a single common factor, in the different groups. However, as both Deary et al. (1996). and Tucker-Drob (2009) have pointed out, dividing the continuous distribution of intelligence into an arbitrary number of discrete ability groups is less than ideal for examining (SLODR). Tucker-Drob (2009) extensively reviewed the literature on (SLODR) and the various methods by which it had been previously tested, and proposed that (SLODR) could be most appropriately captured by fitting a common factor model that allows the relations between the factor and its indicators to be nonlinear in nature. He applied such a factor model to a nationally representative data of children and adults in the United States and found consistent evidence for (SLODR). For example, Tucker-Drob (2009) found that a general factor accounted for approximately 75% of the variation in seven different cognitive abilities among very low IQ adults, but only accounted for approximately 30% of the variation in the abilities among very high IQ adults.\n\nA recent meta-analytic study by Blum and Holling also provided support for the differentiation hypothesis. As opposed to most research on the topic, this work made it possible to study ability and age variables as continuous predictors of the \"g\" saturation, and not just to compare lower- vs. higher-skilled or younger vs. older groups of testees. Results demonstrate that the mean correlation and \"g\" loadings of cognitive ability tests decrease with increasing ability, yet increase with respondent age. (SLODR), as described by Charles Spearman, could be confirmed by a \"g\"-saturation decrease as a function of IQ as well as a \"g\"-saturation increase from middle age to senescence. Specifically speaking, for samples with a mean intelligence that is two standard deviations (i.e., 30 IQ-points) higher, the mean correlation to be expected is decreased by approximately .15 points. The question remains whether a difference of this magnitude could result in a greater apparent factorial complexity when cognitive data are factored for the higher-ability sample, as opposed to the lower-ability sample. It seems likely that greater factor dimensionality should tend to be observed for the case of higher ability, but the magnitude of this effect (i.e., how much more likely and how many more factors) remains uncertain.\n\nThe practical validity of \"g\" as a predictor of educational, economic, and social outcomes is more far-ranging and universal than that of any other known psychological variable. The validity of \"g\" is greater the greater the complexity of the task.\n\nA test's practical validity is measured by its correlation with performance on some criterion external to the test, such as college grade-point average, or a rating of job performance. The correlation between test scores and a measure of some criterion is called the \"validity coefficient\". One way to interpret a validity coefficient is to square it to obtain the variance accounted by the test. For example, a validity coefficient of .30 corresponds to 9 percent of variance explained. This approach has, however, been criticized as misleading and uninformative, and several alternatives have been proposed. One arguably more interpretable approach is to look at the percentage of test takers in each test score quintile who meet some agreed-upon standard of success. For example, if the correlation between test scores and performance is .30, the expectation is that 67 percent of those in the top quintile will be above-average performers, compared to 33 percent of those in the bottom quintile.\n\nThe predictive validity of \"g\" is most conspicuous in the domain of scholastic performance. This is apparently because \"g\" is closely linked to the ability to learn novel material and understand concepts and meanings.\n\nIn elementary school, the correlation between IQ and grades and achievement scores is between .60 and .70. At more advanced educational levels, more students from the lower end of the IQ distribution drop out, which restricts the range of IQs and results in lower validity coefficients. In high school, college, and graduate school the validity coefficients are .50–.60, .40–.50, and .30–.40, respectively. The \"g\" loadings of IQ scores are high, but it is possible that some of the validity of IQ in predicting scholastic achievement is attributable to factors measured by IQ independent of \"g\". According to research by Robert L. Thorndike, 80 to 90 percent of the \"predictable\" variance in scholastic performance is due to \"g\", with the rest attributed to non-\"g\" factors measured by IQ and other tests.\n\nAchievement test scores are more highly correlated with IQ than school grades. This may be because grades are more influenced by the teacher's idiosyncratic perceptions of the student. In a longitudinal English study, \"g\" scores measured at age 11 correlated with all the 25 subject tests of the national GCSE examination taken at age 16. The correlations ranged from .77 for the mathematics test to .42 for the art test. The correlation between \"g\" and a general educational factor computed from the GCSE tests was .81.\n\nResearch suggests that the SAT, widely used in college admissions, is primarily a measure of \"g\". A correlation of .82 has been found between \"g\" scores computed from an IQ test battery and SAT scores. In a study of 165,000 students at 41 U.S. colleges, SAT scores were found to be correlated at .47 with first-year college grade-point average after correcting for range restriction in SAT scores (the correlation rises to .55 when course difficulty is held constant, i.e., if all students attended the same set of classes).\n\nThere is a high correlation of .90 to .95 between the prestige rankings of occupations, as rated by the general population, and the \"average\" general intelligence scores of people employed in each occupation. At the level of individual employees, the association between job prestige and \"g\" is lower – one large U.S. study reported a correlation of .65 (.72 corrected for attenuation). Mean level of \"g\" thus increases with perceived job prestige. It has also been found that the dispersion of general intelligence scores is smaller in more prestigious occupations than in lower level occupations, suggesting that higher level occupations have minimum \"g\" requirements.\n\nResearch indicates that tests of \"g\" are the best single predictors of job performance, with an average validity coefficient of .55 across several meta-analyses of studies based on supervisor ratings and job samples. The average meta-analytic validity coefficient for performance in job \"training\" is .63. The validity of \"g\" in the highest complexity jobs (professional, scientific, and upper management jobs) has been found to be greater than in the lowest complexity jobs, but \"g\" has predictive validity even for the simplest jobs. Research also shows that specific aptitude tests tailored for each job provide little or no increase in predictive validity over tests of general intelligence. It is believed that \"g\" affects job performance mainly by facilitating the acquisition of job-related knowledge. The predictive validity of \"g\" is greater than that of work experience, and increased experience on the job does not decrease the validity of \"g\".\n\nIn a 2011 meta-analysis, researchers found that general cognitive ability (GCA) predicted job performance better than personality (Five factor model) and three streams of emotional intelligence. They examined the relative importance of these constructs on predicting job performance and found that cognitive ability explained most of the variance in job performance. Other studies suggested that GCA and emotional intelligence have a linear independent and complementary contribution to job performance. Côté and Miners (2015) found that these constructs are interrelated when assessing their relationship with two aspects of job performance: organisational citizenship behaviour (OCB) and task performance. Emotional intelligence is a better predictor of task performance and OCB when GCA is low and vice versa. For instance, an employee with low GCA will compensate his/her task performance and OCB, if emotional intelligence is high.\n\nAlthough these compensatory effects favour emotional intelligence, GCA still remains as the best predictor of job performance. Several researchers have studied the correlation between GCA and job performance among different job positions. For instance, Ghiselli (1973) found that salespersons had a higher correlation than sales clerk. The former obtained a correlation of 0.61 for GCA, 0.40 for perceptual ability and 0.29 for psychomotor abilities; whereas sales clerk obtained a correlation of 0.27 for GCA, 0.22 for perceptual ability and 0.17 for psychomotor abilities. Other studies compared GCA – job performance correlation between jobs of different complexity. Hunter and Hunter (1984) developed a meta-analysis with over 400 studies and found that this correlation was higher for jobs of high complexity (0.57). Followed by jobs of medium complexity (0.51) and low complexity (0.38).\n\nJob performance is measured by objective rating performance and subjective ratings. Although the former is better than subjective ratings, most of studies in job performance and GCA have been based on supervisor performance ratings. This rating criteria is considered problematic and unreliable, mainly because of its difficulty to define what is a good and bad performance. Rating of supervisors tends to be subjective and inconsistent among employees. Additionally, supervisor rating of job performance is influenced by different factors, such as halo effect, facial attractiveness, racial or ethnic bias, and height of employees. However, Vinchur, Schippmann, Switzer and Roth (1998) found in their study with sales employees that objective sales performance had a correlation of 0.04 with GCA, while supervisor performance rating got a correlation of 0.40. These findings were surprising, considering that the main criteria for assessing these employees would be the objective sales.\n\nIn understanding how GCA is associated job performance, several researchers concluded that GCA affects acquisition of job knowledge, which in turn improves job performance. In other words, people high in GCA are capable to learn faster and acquire more job knowledge easily, which allow them to perform better. Conversely, lack of ability to acquire job knowledge will directly affect job performance. This is due to low levels of GCA. Also, GCA has a direct effect on job performance. In a daily basis, employees are exposed constantly to challenges and problem solving tasks, which success depends solely on their GCA. These findings are discouraging for governmental entities in charge of protecting rights of workers. Because of the high correlation of GCA on job performance, companies are hiring employees based on GCA tests scores. Inevitably, this practice is denying the opportunity to work to many people with low GCA. Previous researchers have found significant differences in GCA between race / ethnicity groups. For instance, there is a debate whether studies were biased against Afro-Americans, who scored significantly lower than white Americans in GCA tests. However, findings on GCA-job performance correlation must be taken carefully. Some researchers have warned the existence of statistical artifacts related to measures of job performance and GCA test scores. For example, Viswesvaran, Ones and Schmidt (1996) argued that is quite impossible to obtain perfect measures of job performance without incurring in any methodological error. Moreover, studies on GCA and job performance are always susceptible to range restriction, because data is gathered mostly from current employees, neglecting those that were not hired. Hence, sample comes from employees who successfully passed hiring process, including measures of GCA.\n\nThe correlation between income and \"g\", as measured by IQ scores, averages about .40 across studies. The correlation is higher at higher levels of education and it increases with age, stabilizing when people reach their highest career potential in middle age. Even when education, occupation and socioeconomic background are held constant, the correlation does not vanish.\n\nThe \"g\" factor is reflected in many social outcomes. Many social behavior problems, such as dropping out of school, chronic welfare dependency, accident proneness, and crime, are negatively correlated with \"g\" independent of social class of origin. Health and mortality outcomes are also linked to \"g\", with higher childhood test scores predicting better health and mortality outcomes in adulthood (see Cognitive epidemiology).\n\nHeritability is the proportion of phenotypic variance in a trait in a population that can be attributed to genetic factors. The heritability of \"g\" has been estimated to fall between 40 and 80 percent using twin, adoption, and other family study designs as well as molecular genetic methods. Estimates based on the totality of evidence place the heritability of \"g\" at about 50%. It has been found to increase linearly with age. For example, a large study involving more than 11,000 pairs of twins from four countries reported the heritability of \"g\" to be 41 percent at age nine, 55 percent at age twelve, and 66 percent at age seventeen. Other studies have estimated that the heritability is as high as 80 percent in adulthood, although it may decline in old age. Most of the research on the heritability of \"g\" has been conducted in the United States and Western Europe, but studies in Russia (Moscow), the former East Germany, Japan, and rural India have yielded similar estimates of heritability as Western studies.\n\nBehavioral genetic research has also established that the shared (or between-family) environmental effects on \"g\" are strong in childhood, but decline thereafter and are negligible in adulthood. This indicates that the environmental effects that are important to the development of \"g\" are unique and not shared between members of the same family.\n\nThe genetic correlation is a statistic that indicates the extent to which the same genetic effects influence two different traits. If the genetic correlation between two traits is zero, the genetic effects on them are independent, whereas a correlation of 1.0 means that the same set of genes explains the heritability of both traits (regardless of how high or low the heritability of each is). Genetic correlations between specific mental abilities (such as verbal ability and spatial ability) have been consistently found to be very high, close to 1.0. This indicates that genetic variation in cognitive abilities is almost entirely due to genetic variation in whatever \"g\" is. It also suggests that what is common among cognitive abilities is largely caused by genes, and that independence among abilities is largely due to environmental effects. Thus it has been argued that when genes for intelligence are identified, they will be \"generalist genes\", each affecting many different cognitive abilities.\n\nThe \"g\" loadings of mental tests have been found to correlate with their heritabilities, with correlations ranging from moderate to perfect in various studies. Thus the heritability of a mental test is usually higher the larger its \"g\" loading is.\n\nMuch research points to \"g\" being a highly polygenic trait influenced by a large number of common genetic variants, each having only small effects. Another possibility is that heritable differences in \"g\" are due to individuals having different \"loads\" of rare, deleterious mutations, with genetic variation among individuals persisting due to mutation–selection balance.\n\nA number of candidate genes have been reported to be associated with intelligence differences, but the effect sizes have been small and almost none of the findings have been replicated. No individual genetic variants have been conclusively linked to intelligence in the normal range so far. Many researchers believe that very large samples will be needed to reliably detect individual genetic polymorphisms associated with \"g\". However, while genes influencing variation in \"g\" in the normal range have proven difficult to find, a large number of single-gene disorders with mental retardation among their symptoms have been discovered.\n\nSeveral studies suggest that tests with larger \"g\" loadings are more affected by inbreeding depression lowering test scores. There is also evidence that tests with larger \"g\" loadings are associated with larger positive heterotic effects on test scores. Inbreeding depression and heterosis suggest the presence of genetic dominance effects for \"g\".\n\n\"g\" has a number of correlates in the brain. Studies using magnetic resonance imaging (MRI) have established that \"g\" and total brain volume are moderately correlated (r~.3–.4). External head size has a correlation of ~.2 with \"g\". MRI research on brain regions indicates that the volumes of frontal, parietal and temporal cortices, and the hippocampus are also correlated with \"g\", generally at .25 or more, while the correlations, averaged over many studies, with overall grey matter and overall white matter have been found to be .31 and .27, respectively. Some but not all studies have also found positive correlations between \"g\" and cortical thickness. However, the underlying reasons for these associations between the quantity of brain tissue and differences in cognitive abilities remain largely unknown.\n\nMost researchers believe that intelligence cannot be localized to a single brain region, such as the frontal lobe. Brain lesion studies have found small but consistent associations indicating that people with more white matter lesions tend to have lower cognitive ability. Research utilizing NMR spectroscopy has discovered somewhat inconsistent but generally positive correlations between intelligence and white matter integrity, supporting the notion that white matter is important for intelligence.\n\nSome research suggests that aside from the integrity of white matter, also its organizational efficiency is related to intelligence. The hypothesis that brain efficiency has a role in intelligence is supported by functional MRI research showing that more intelligent people generally process information more efficiently, i.e., they use fewer brain resources for the same task than less intelligent people.\n\nSmall but relatively consistent associations with intelligence test scores include also brain activity, as measured by EEG records or event-related potentials, and nerve conduction velocity.\n\nEvidence of a general factor of intelligence has also been observed in non-human animals. Studies have shown that \"g\" is responsible for 47% of the individual variance in primates and between 55% and 60% in mice. While not able to be assessed using the same intelligence measures used in humans, cognitive ability can be measured with a variety of interactive and observational tools focusing on innovation, habit reversal, social learning, and responses to novelty.\n\nNon-human models of \"g\" such as mice are used to study genetic influences on intelligence and neurological developmental research into the mechanisms behind and biological correlates of \"g\".\n\nSimilar to \"g\" for individuals, a new research path aims to extract a general collective intelligence factor \"c\" for groups displaying a group’s general ability to perform a wide range of tasks. Definition, operationalization and statistical approach for this \"c\" factor are derived from and similar to \"g\". Causes, predictive validity as well as additional parallels to \"g\" are investigated.\n\nHeight is correlated with intelligence (r~.2), but this correlation has not generally been found within families (i.e., among siblings), suggesting that it results from cross-assortative mating for height and intelligence, or from another factor that correlates with both (e.g. nutrition). Myopia is known to be associated with intelligence, with a correlation of around .2 to .25, and this association has been found within families, too.\n\nCross-cultural studies indicate that the \"g\" factor can be observed whenever a battery of diverse, complex cognitive tests is administered to a human sample. The factor structure of IQ tests has also been found to be consistent across sexes and ethnic groups in the U.S. and elsewhere. The \"g\" factor has been found to be the most invariant of all factors in cross-cultural comparisons. For example, when the \"g\" factors computed from an American standardization sample of Wechsler's IQ battery and from large samples who completed the Japanese translation of the same battery were compared, the congruence coefficient was .99, indicating virtual identity. Similarly, the congruence coefficient between the \"g\" factors obtained from white and black standardization samples of the WISC battery in the U.S. was .995, and the variance in test scores accounted for by \"g\" was highly similar for both groups.\n\nMost studies suggest that there are negligible differences in the mean level of \"g\" between the sexes, and that sex differences in cognitive abilities are to be found in more narrow domains. For example, males generally outperform females in spatial tasks, while females generally outperform males in verbal tasks. Another difference that has been found in many studies is that males show more variability in both general and specific abilities than females, with proportionately more males at both the low end and the high end of the test score distribution.\n\nConsistent differences between racial and ethnic groups in \"g\" have been found, particularly in the U.S. A 2001 meta-analysis of millions of subjects indicated that there is a 1.1 standard deviation gap in the mean level of \"g\" between white and black Americans, favoring the former. The mean score of Hispanic Americans was found to be .72 standard deviations below that of non-Hispanic whites. In contrast, Americans of East Asian descent generally slightly outscore white Americans. Several researchers have suggested that the magnitude of the black-white gap in cognitive ability tests is dependent on the magnitude of the test's \"g\" loading, with tests showing higher \"g\" loadings producing larger gaps (see Spearman's hypothesis). It has also been claimed that racial and ethnic differences similar to those found in the U.S. can be observed globally.\n\nElementary cognitive tasks (ECTs) also correlate strongly with \"g\". ECTs are, as the name suggests, simple tasks that apparently require very little intelligence, but still correlate strongly with more exhaustive intelligence tests. Determining whether a light is red or blue and determining whether there are four or five squares drawn on a computer screen are two examples of ECTs. The answers to such questions are usually provided by quickly pressing buttons. Often, in addition to buttons for the two options provided, a third button is held down from the start of the test. When the stimulus is given to the subject, they remove their hand from the starting button to the button of the correct answer. This allows the examiner to determine how much time was spent thinking about the answer to the question (reaction time, usually measured in small fractions of second), and how much time was spent on physical hand movement to the correct button (movement time). Reaction time correlates strongly with \"g\", while movement time correlates less strongly.\nECT testing has allowed quantitative examination of hypotheses concerning test bias, subject motivation, and group differences. By virtue of their simplicity, ECTs provide a link between classical IQ testing and biological inquiries such as fMRI studies.\n\nOne theory holds that \"g\" is identical or nearly identical to working memory capacity. Among other evidence for this view, some studies have found factors representing \"g\" and working memory to be perfectly correlated. However, in a meta-analysis the correlation was found to be considerably lower. One criticism that has been made of studies that identify \"g\" with working memory is that \"we do not advance understanding by showing that one mysterious concept is linked to another.\"\n\nPsychometric theories of intelligence aim at quantifying intellectual growth and identifying ability differences between individuals and groups. In contrast, Jean Piaget's theory of cognitive development seeks to understand qualitative changes in children's intellectual development. Piaget designed a number of tasks to verify hypotheses arising from his theory. The tasks were not intended to measure individual differences, and they have no equivalent in psychometric intelligence tests. For example, in one of the best-known Piagetian conservation tasks a child is asked if the amount of water in two identical glasses is the same. After the child agrees that the amount is the same, the investigator pours the water from one of the glasses into a glass of different shape so that the amount appears different although it remains the same. The child is then asked if the amount of water in the two glasses is the same or different.\n\nNotwithstanding the different research traditions in which psychometric tests and Piagetian tasks were developed, the correlations between the two types of measures have been found to be consistently positive and generally moderate in magnitude. A common general factor underlies them. It has been shown that it is possible to construct a battery consisting of Piagetian tasks that is as good a measure of \"g\" as standard IQ tests.\n\nThe traditional view in psychology is that there is no meaningful relationship between personality and intelligence, and that the two should be studied separately. Intelligence can be understood in terms of what an individual \"can\" do, or what his or her \"maximal\" performance is, while personality can be thought of in terms of what an individual \"will typically\" do, or what his or her general tendencies of behavior are. Research has indicated that correlations between measures of intelligence and personality are small, and it has thus been argued that \"g\" is a purely cognitive variable that is independent of personality traits. In a 2007 meta-analysis the correlations between \"g\" and the \"Big Five\" personality traits were found to be as follows:\nThe same meta-analysis found a correlation of .20 between self-efficacy and \"g\".\n\nSome researchers have argued that the associations between intelligence and personality, albeit modest, are consistent. They have interpreted correlations between intelligence and personality measures in two main ways. The first perspective is that personality traits influence performance on intelligence \"tests\". For example, a person may fail to perform at a maximal level on an IQ test due to his or her anxiety and stress-proneness. The second perspective considers intelligence and personality to be \"conceptually\" related, with personality traits determining how people apply and invest their cognitive abilities, leading to knowledge expansion and greater cognitive differentiation.\n\nSome researchers believe that there is a threshold level of \"g\" below which socially significant creativity is rare, but that otherwise there is no relationship between the two. It has been suggested that this threshold is at least one standard deviation above the population mean. Above the threshold, personality differences are believed to be important determinants of individual variation in creativity.\n\nOthers have challenged the threshold theory. While not disputing that opportunity and personal attributes other than intelligence, such as energy and commitment, are important for creativity, they argue that \"g\" is positively associated with creativity even at the high end of the ability distribution. The longitudinal Study of Mathematically Precocious Youth has provided evidence for this contention. It has showed that individuals identified by standardized tests as intellectually gifted in early adolescence accomplish creative achievements (for example, securing patents or publishing literary or scientific works) at several times the rate of the general population, and that even within the top 1 percent of cognitive ability, those with higher ability are more likely to make outstanding achievements. The study has also suggested that the level of \"g\" acts as a predictor of the \"level\" of achievement, while specific cognitive ability patterns predict the \"realm\" of achievement.\n\nRaymond Cattell, a student of Charles Spearman's, rejected the unitary \"g\" factor model and divided \"g\" into two broad, relatively independent domains: fluid intelligence (G\"f\") and crystallized intelligence (G\"c\"). G\"f\" is conceptualized as a capacity to figure out novel problems, and it is best assessed with tests with little cultural or scholastic content, such as Raven's matrices. G\"c\" can be thought of as consolidated knowledge, reflecting the skills and information that an individual acquires and retains throughout his or her life. G\"c\" is dependent on education and other forms of acculturation, and it is best assessed with tests that emphasize scholastic and cultural knowledge. G\"f\" can be thought to primarily consist of \"current\" reasoning and problem solving capabilities, while G\"c\" reflects the outcome of \"previously\" executed cognitive processes.\n\nThe rationale for the separation of G\"f\" and G\"c\" was to explain individuals' cognitive development over time. While G\"f\" and G\"c\" have been found to be highly correlated, they differ in the way they change over a lifetime. G\"f\" tends to peak at around age 20, slowly declining thereafter. In contrast, G\"c\" is stable or increases across adulthood. A single general factor has been criticized as obscuring this bifurcated pattern of development. Cattell argued that G\"f\" reflected individual differences in the efficiency of the central nervous system. G\"c\" was, in Cattell's thinking, the result of a person \"investing\" his or her G\"f\" in learning experiences throughout life.\n\nCattell, together with John Horn, later expanded the G\"f\"-G\"c\" model to include a number of other broad abilities, such as G\"q\" (quantitative reasoning) and G\"v\" (visual-spatial reasoning). While all the broad ability factors in the extended G\"f\"-G\"c\" model are positively correlated and thus would enable the extraction of a higher order \"g\" factor, Cattell and Horn maintained that it would be erroneous to posit that a general factor underlies these broad abilities. They argued that \"g\" factors computed from different test batteries are not invariant and would give different values of \"g\", and that the correlations among tests arise because it is difficult to test just one ability at a time.\n\nHowever, several researchers have suggested that the G\"f\"-G\"c\" model is compatible with a \"g\"-centered understanding of cognitive abilities. For example, John B. Carroll's three-stratum model of intelligence includes both G\"f\" and G\"c\" together with a higher-order \"g\" factor. Based on factor analyses of many data sets, some researchers have also argued that G\"f\" and \"g\" are one and the same factor and that \"g\" factors from different test batteries are substantially invariant provided that the batteries are large and diverse.\n\nSeveral theorists have proposed that there are intellectual abilities that are uncorrelated with each other. Among the earliest was L.L. Thurstone who created a model of \"primary mental abilities\" representing supposedly independent domains of intelligence. However, Thurstone's tests of these abilities were found to produce a strong general factor. He argued that the lack of independence among his tests reflected the difficulty of constructing \"factorially pure\" tests that measured just one ability. Similarly, J.P. Guilford proposed a model of intelligence that comprised up to 180 distinct, uncorrelated abilities, and claimed to be able to test all of them. Later analyses have shown that the factorial procedures Guilford presented as evidence for his theory did not provide support for it, and that the test data that he claimed provided evidence against \"g\" did in fact exhibit the usual pattern of intercorrelations after correction for statistical artifacts.\n\nMore recently, Howard Gardner has developed the theory of multiple intelligences. He posits the existence of nine different and independent domains of intelligence, such as mathematical, linguistic, spatial, musical, bodily-kinesthetic, meta-cognitive, and existential intelligences, and contends that individuals who fail in some of them may excel in others. According to Gardner, tests and schools traditionally emphasize only linguistic and logical abilities while neglecting other forms of intelligence. While popular among educationalists, Gardner's theory has been much criticized by psychologists and psychometricians. One criticism is that the theory does violence to both scientific and everyday usages of the word \"intelligence.\" Several researchers have argued that not all of Gardner's intelligences fall within the cognitive sphere. For example, Gardner contends that a successful career in professional sports or popular music reflects bodily-kinesthetic \"intelligence\" and musical \"intelligence\", respectively, even though one might usually talk of athletic and musical \"skills\", \"talents\", or \"abilities\" instead. Another criticism of Gardner's theory is that many of his purportedly independent domains of intelligence are in fact correlated with each other. Responding to empirical analyses showing correlations between the domains, Gardner has argued that the correlations exist because of the common format of tests and because all tests require linguistic and logical skills. His critics have in turn pointed out that not all IQ tests are administered in the paper-and-pencil format, that aside from linguistic and logical abilities, IQ test batteries contain also measures of, for example, spatial abilities, and that elementary cognitive tasks (for example, inspection time and reaction time) that do not involve linguistic or logical reasoning correlate with conventional IQ batteries, too.\n\nRobert Sternberg, working with various colleagues, has also suggested that intelligence has dimensions independent of \"g\". He argues that there are three classes of intelligence: analytic, practical, and creative. According to Sternberg, traditional psychometric tests measure only analytic intelligence, and should be augmented to test creative and practical intelligence as well. He has devised several tests to this effect. Sternberg equates analytic intelligence with academic intelligence, and contrasts it with practical intelligence, defined as an ability to deal with ill-defined real-life problems. Tacit intelligence is an important component of practical intelligence, consisting of knowledge that is not explicitly taught but is required in many real-life situations. Assessing creativity independent of intelligence tests has traditionally proved difficult, but Sternberg and colleagues have claimed to have created valid tests of creativity, too. The validation of Sternberg's theory requires that the three abilities tested are substantially uncorrelated and have independent predictive validity. Sternberg has conducted many experiments which he claims confirm the validity of his theory, but several researchers have disputed this conclusion. For example, in his reanalysis of a validation study of Sternberg's STAT test, Nathan Brody showed that the predictive validity of the STAT, a test of three allegedly independent abilities, was almost solely due to a single general factor underlying the tests, which Brody equated with the \"g\" factor.\n\nJames Flynn has argued that intelligence should be conceptualized at three different levels: brain physiology, cognitive differences between individuals, and social trends in intelligence over time. According to this model, the \"g\" factor is a useful concept with respect to individual differences but its explanatory power is limited when the focus of investigation is either brain physiology, or, especially, the effect of social trends on intelligence. Flynn has criticized the notion that cognitive gains over time, or the Flynn effect, are \"hollow\" if they cannot be shown to be increases in \"g\". He argues that the Flynn effect reflects shifting social priorities and individuals' adaptation to them. To apply the individual differences concept of \"g\" to the Flynn effect is to confuse different levels of analysis. On the other hand, according to Flynn, it is also fallacious to deny, by referring to trends in intelligence over time, that some individuals have \"better brains and minds\" to cope with the cognitive demands of their particular time. At the level of brain physiology, Flynn has emphasized both that localized neural clusters can be affected differently by cognitive exercise, and that there are important factors that affect all neural clusters.\n\nPerhaps the most famous critique of the construct of \"g\" is that of the paleontologist and biologist Stephen Jay Gould, presented in his 1981 book \"The Mismeasure of Man\". He argued that psychometricians have fallaciously reified the \"g\" factor as a physical thing in the brain, even though it is simply the product of statistical calculations (i.e., factor analysis). He further noted that it is possible to produce factor solutions of cognitive test data that do not contain a \"g\" factor yet explain the same amount of information as solutions that yield a \"g\". According to Gould, there is no rationale for preferring one factor solution to another, and factor analysis therefore does not lend support to the existence of an entity like \"g\". More generally, Gould criticized the \"g\" theory for abstracting intelligence as a single entity and for ranking people \"in a single series of worthiness\", arguing that such rankings are used to justify the oppression of disadvantaged groups.\n\nMany researchers have criticized Gould's arguments. For example, they have rejected the accusation of reification, maintaining that the use of extracted factors such as \"g\" as potential causal variables whose reality can be supported or rejected by further investigations constitutes a normal scientific practice that in no way distinguishes psychometrics from other sciences. Critics have also suggested that Gould did not understand the purpose of factor analysis, and that he was ignorant of relevant methodological advances in the field. While different factor solutions may be mathematically equivalent in their ability to account for intercorrelations among tests, solutions that yield a \"g\" factor are psychologically preferable for several reasons extrinsic to factor analysis, including the phenomenon of the positive manifold, the fact that the same \"g\" can emerge from quite different test batteries, the widespread practical validity of \"g\", and the linkage of \"g\" to many biological variables.\n\nJohn Horn and John McArdle have argued that the modern \"g\" theory, as espoused by, for example, Arthur Jensen, is unfalsifiable, because the existence of a common factor like \"g\" follows tautologically from positive correlations among tests. They contrasted the modern hierarchical theory of \"g\" with Spearman's original two-factor theory which was readily falsifiable (and indeed was falsified).\n\n\n\n"}
{"id": "30446253", "url": "https://en.wikipedia.org/wiki?curid=30446253", "title": "General Pharmaceutical Council", "text": "General Pharmaceutical Council\n\nThe General Pharmaceutical Council (GPhC) is the body responsible for the independent regulation of the pharmacy profession within England, Scotland and Wales, responsible for the regulation of pharmacists, pharmacy technicians and pharmacy premises. It was created, along with the Royal Pharmaceutical Society, in September 2010 when the previous Royal Pharmaceutical Society of Great Britain was split so that representative and regulatory functions of the pharmacy profession could be separated.\n\nThe Health and Social Care Act 2008 and the subsequent Pharmacy Order 2010 allowed for regulatory functions of the RPSGB to be transferred to the new pharmacy regulator, the GPhC. The GPhC is therefore responsible for the update and maintenance of the registers of pharmacists, pharmacy technicians, pharmacy premises and pharmacy training premises. These registers can be accessed electronically by any member of the public online at the GPhC's website.\n\nIn addition the GPhC states that the Health and Social Care Act 2008 has sufficient provisions to allow for the transfer of regulatory powers from the Pharmaceutical Society of Northern Ireland to the GPhC in the future, subject to approval of Northern Ireland Ministers.\n\nThe principal functions of the GPhC, as stated within Pharmacy Order 2010 are;\n\nFurthermore, under statute, the GPhC must have the following committees;\n\nThe Pharmacy Order 2010 requires not only that the GPhC sets acceptable standards of continuing professional development (CPD), but that it ensures that all registrants meet those required standards, and that there are processes in place for various remedial measures, including removal of a registrant from the register if they either fail to meet these standards, or make a false statement in relation to their CPD.\n\nThe GPhC is governed by a fourteen-member GPhC Council, with equal numbers of lay and registrant members, which is independent from the government, the professionals it regulates and any other interest groups. The GPhC state that to reinforce this independence, all members of the GPhC Council, including the Chair are appointed by the Privy Council, rather than elected.\n\nThe first Chair of the GPhC Council was Bob Nicholls CBE, a lay member with extensive experience in the National Health Service, who has previously been a lay member of the General Medical Council, among other regulatory appointments. The current Chair is Nigel Clarke, a lay member with experience of the General Osteopathic Council and chairing the Future Professional Body for Pharmacy and then the Transitional Committee, which created the prospectus for the reformed Royal Pharmaceutical Society\n\nThe GPhC Exam or GPhC Registration Assessment exam was set by the Royal Pharmaceutical Society of Great Britain, but since September 2010 has been the responsibility of the GPhC. The examination takes place on two occasions each year: the summer (the last Friday in June) and the autumn (the last Friday in September).\n\nThe Professional Standards Authority for Health and Social Care (PSA), is an independent body accountable to the UK Parliament, which promotes the health and wellbeing of the public and oversees the nine UK healthcare regulators, including General Pharmaceutical Council.\n\n\n"}
{"id": "8331953", "url": "https://en.wikipedia.org/wiki?curid=8331953", "title": "HealthBridge", "text": "HealthBridge\n\nHealthBridge is a service line of The Health Collaborative, a not-for-profit corporation located in Cincinnati, Ohio. HealthBridge supports health information technology (HIT) adoption, health information exchange (HIE), and innovative use of information for improved health care outcomes. HealthBridge is recognized as one of the nation’s largest, most advanced, and most financially sustainable health information exchange (HIE) organizations.\n\nHealthBridge was founded in 1997 as a community effort to develop a common technology infrastructure for sharing health information electronically in the Greater Cincinnati tri-state area. Its mission is to positively impact health status, experience, outcomes, and affordability by fostering a connected system of health care and community health through innovation, integration and informatics.\n\nAs a result of HealthBridge and its community partners’ efforts, more than 30 million clinical tests, images, and other clinical results are transmitted each year to authorized physicians through HealthBridge’s secure electronic network. HealthBridge serves more than 30 hospitals, 7500 physicians, 800 practices, as well as local health departments, nursing homes, independent labs, radiology centers and other health care entities across multiple communities in four states.\n\n"}
{"id": "19452043", "url": "https://en.wikipedia.org/wiki?curid=19452043", "title": "Health facility", "text": "Health facility\n\nA health facility is, in general, any location where healthcare is provided. Health facilities range from small clinics and doctor's offices to urgent care centers and large hospitals with elaborate emergency rooms and trauma centers. The number and quality of health facilities in a country or region is one common measure of that area's prosperity and quality of life. In many countries, health facilities are regulated to some extent by law; licensing by a regulatory agency is often required before a facility may open for business. Health facilities may be owned and operated by for-profit businesses, non-profit organizations, governments, and in some cases by individuals, with proportions varying by country. See also the recent review paper, which provides a comprehensive classification of health facilities from the location analysis perspective.\n\nThe workload of a health facility is often used to indicate its size. Large health facilities are those with a greater patient load.\n\nIn Australia the workload of a health facility is used to determine the level of government funding provided to that facility. The government measures a facility (or health practice) in terms of its standard whole patient equivalent (SWPE). The SWPE calculation is determined by analysis of the patients that attend that facility. The calculation takes into account the proportion of health services (in dollars) rendered at that facility relative to others that each patient attends. It includes a weighting factor based on each patients demography to account for the varied levels of services required by patients depending on their gender and age. The premise of weighting is that patients require different levels of health services depending on their age and gender. For example, the average male patient requires fewer consultations than his older and infant counterparts. \nThe table shows the weighting factors used in the standardization of workloads.\n\nTable: Age by Sex Weights for SWPE Standardisation\n\nA hospital is an institution for healthcare typically providing specialized treatment for inpatient (or overnight) stays. Some hospitals primarily admit patients suffering from a specific disease or affliction, or are reserved for the diagnosis and treatment of conditions affecting a specific age group. Others have a mandate that expands beyond offering dominantly curative and rehabilitative care services to include promotional, preventive and educational roles as part of a primary healthcare approach. Today, hospitals are usually funded by the state, health organizations (for profit or non-profit), by health insurances or by charities and by donations. Historically, however, they were often founded and funded by religious orders or charitable individuals and leaders. Hospitals are nowadays staffed by professionally trained doctors, nurses, paramedical clinicians, etc., whereas historically, this work was usually done by the founding religious orders or by volunteers.\n\nHealthcare centres, including clinics, doctor's offices, urgent care centers and ambulatory surgery centers, serve as first point of contact with a health professional and provide outpatient medical, nursing, dental, and other types of care services.\n\nMedical nursing homes, including residential treatment centers and geriatric care facilities, are health care institutions which have accommodation facilities and which engage in providing short-term or long-term medical treatment of a general or specialized nature not performed by hospitals to inpatients with any of a wide variety of medical conditions.\n\nPharmacies and drug stores comprise establishments engaged in retailing prescription or nonprescription drugs and medicines, and other types of medical and orthopaedic goods. Regulated pharmacies may be based in a hospital or clinic or they may be privately operated, and are usually staffed by pharmacists, pharmacy technicians, and pharmacy aides.\n\nA medical laboratory or clinical laboratory is a laboratory where tests are done on biological specimens in order to get information about the health of a patient. Such laboratories may be divided into categorical departments such as microbiology, hematology, clinical biochemistry, immunology, serology, histology, cytology, cytogenetics, or virology. In many countries, there are two main types of labs that process the majority of medical specimens. Hospital laboratories are attached to a hospital, and perform tests on these patients. Private or community laboratories receive samples from general practitioners, insurance companies, and other health clinics for analysis.\n\nA biomedical research facility is where basic research or applied research is conducted to aid the body of knowledge in the field of medicine. Medical research can be divided into two general categories: the evaluation of new treatments for both safety and efficacy in what are termed clinical trials, and all other research that contributes to the development of new treatments. The latter is termed preclinical research if its goal is specifically to elaborate knowledge for the development of new therapeutic strategies.\n\n"}
{"id": "7963692", "url": "https://en.wikipedia.org/wiki?curid=7963692", "title": "Hispanic paradox", "text": "Hispanic paradox\n\nThe Hispanic paradox, or Latino paradox, also known as the \"epidemiologic paradox,\" refers to the epidemiological finding that Hispanic and Latino Americans tend to have health outcomes that \"paradoxically\" are comparable to, or in some cases better than, those of their U.S. non-Hispanic White counterparts, even though Hispanics have lower average income and education. (Low socioeconomic status is almost universally associated with worse population health and higher death rates everywhere in the world.) The paradox usually refers in particular to low mortality among Latinos in the United States relative to non-Hispanic Whites. First coined the \"Hispanic Epidemiological Paradox\" in 1986 by Kyriakos Markides, the phenomenon is also known as the \"Latino Epidemiological Paradox\". According to Markides, a professor of sociomedical sciences at the University of Texas Medical Branch in Galveston, this paradox was ignored by past generations, but is now \"the leading theme in the health of the Hispanic population in the United States.\"\n\nThe specific cause of the phenomenon is poorly understood, although the decisive factor appears to be place of birth, raising the possibility that differing birthing or neonatal practices might be involved via a lack of breastfeeding combined with birth trauma imprinting (both common in American obstetrics) and consequent mental and physical illness, the latter compounded by the impact of psychological problems on the capacity for social networking. It appears that the Hispanic Paradox cannot be explained by either the \"salmon bias hypothesis\" or the \"healthy migrant effect,\" two theories that posit low mortality among immigrants due to, respectively, a possible tendency for sick immigrants to return to their home country before death and a possible tendency for new immigrants to be unusually healthy compared to the rest of their home-country population. Historical differences in smoking habits by ethnicity and place of birth may explain much of the paradox, at least at adult ages.\nOthers have proposed that the lower mortality of Hispanics could reflect a slower biological aging rate of Hispanics. However, some believe that there is no Hispanic Paradox, and that inaccurate counting of Hispanic deaths in the United States leads to an underestimate of Hispanic or Latino mortality.\n\nThough they are often at lower socioeconomic standing, most Hispanic groups, excepting Puerto Ricans, demonstrate lower or equal levels of mortality to their non-Hispanic White counterparts. The Center for Disease Control reported in 2003 that Hispanic’s mortality rate was 25 percent lower than non-Hispanic whites and 43 percent lower than African Americans. This mortality advantage most commonly found among middle-aged and elderly Hispanics. The death rates of Hispanics to non-Hispanic whites was found to exceed 1.00 in the twenties, decreases by age 45, then is severely reduced to 0.75-.90 by at age 65, persisting until death. When controlling for socioeconomic factors, the health advantage gap for Mexican Americans, the largest Hispanic population in the US, increases noticeably.\n\nHispanics do not have a mortality advantage over non-Hispanic Whites in all mortality rates; they have higher rates for mortality from liver disease, cervical cancer, AIDS, homicide (males), and diabetes.\n\nAnother important indicator of health is the infant mortality rate, which is also either equal or better in Hispanic Americans than in non-Hispanic Americans. A study by Hummer, et al. found that infants born to Mexican Immigrant women in the United States have about a 10% lower mortality in the first hour, first day, and first week than that of infants born to non-Hispanic white, U.S.-born women. In 2003, the national Hispanic infant mortality rate was found to be 5.7, nearly equal to that of non-Hispanic Americans and 58 percent lower than that of African Americans. Hispanic immigrants also have a 20% lower infant mortality rate than that of U.S.-born Hispanics, though the latter population usually has a higher income and education, and are much more likely to have health insurance.\n\nAccording to Alder and Estrove (2006), the more socioeconomically advantaged individuals are, the better their health. Access to health insurance and preventative medical services are one of the main reasons for socioeconomic health disparities. Economic hardship within the household can cause distress and affect parenting, causing health problems among children leading to depression, substance abuse, and behavior problems. Low socioeconomic status is correlated with increased rates of morbidity and mortality. Mental health disorders are an important health problem for those of low socioeconomic status; they are two to five times more likely to suffer from a diagnosable disorder than those of high socioeconomic status, and are more likely to face barriers to getting treatment. Furthermore, this lack of treatment for mental disorders can affect educational and employment opportunities and achievement.\n\nImportant to the understanding of migrant community health is the increasingly stratified American society, manifested in residential segregation. Beginning in the 1970s, the low to moderate levels of income segregation in the United States began to degrade. As the rich became richer, so did their neighborhoods. This trend was inversely reflected in the poor, as their neighborhoods became poorer. As sociologist Douglas Massey explains, “As a result, poverty and affluence both became more concentrated geographically.” Professor of public administration and economics John Yinger writes that “one way for poor people to win the spatial competition for housing is to rent small or low-quality housing.” However, he continues, low-quality housing often features serious health risks such as lead paint and animal pests. Though lead-based paint was deemed illegal in 1978, it remains on the walls of older apartments and houses, posing a serious neurological risk to children. Asthma, a possible serious health risk, also has a clear link to poverty. Moreover, asthma attacks have been associated with certain aspects of poor housing quality such as the presence of cockroaches, mice, dust, dust mites, mold, and mildew. The 1997 American Housing Survey found that signs of rats or mice are almost twice as likely to be detected in poor households as in non-poor households.\n\nOne hypothesis for the Hispanic Paradox proposes that living in the same neighborhood as people with similar ethnic backgrounds confers significant advantages to one’s health. In a study of elderly Mexican-Americans, those living in areas with a higher percentage of Mexican-Americans had lower seven-year mortality as well as a decreased prevalence of medical conditions, including stroke, cancer, and hip fracture. Despite these neighborhoods' relatively high rates of poverty due to lack of formal education and a preponderance of low paying service sector jobs, residents do not suffer from the same mortality and morbidity levels seen in similarly disadvantaged socioeconomic neighborhoods. These neighborhoods do have intact family structures, community institutions, and kinship structures that span households, all of which are thought to provide significant benefits to an individual’s health. These social network support structures are especially important to the health of the elderly population as they deal with declining physical function. Another reason for this phenomenon could be that those Hispanic-Americans that live among those of similar cultural and social backgrounds are shielded from some of the negative effects of assimilation to American culture.\n\nThe extent of a Hispanic American’s acculturation in the United States, or their assimilation to mainstream American culture, is relative to his or her health. One of the main negative effects of acculturation on health has been on substance abuse. More assimilated Latinos have higher rates of illicit drug use, alcohol consumption, and smoking, especially among women. Another negative effect of acculturation is changes in diet and nutrition. More acculturated Latinos eat less fruits, vegetables, vitamins, fiber and protein and consume more fat than their less acculturated counterparts. One of the most significant impacts of acculturation on Latino health is birth outcomes. Studies have found that more acculturated Latinas have higher rates of low birthweight, premature births, teenage pregnancy and undesirable prenatal and postnatal behaviors such as smoking or drinking during pregnancy, and lower rates of breastfeeding. Acculturation and greater time in the United States has also been associated with negative mental health impacts. US-born Latinos or long term residents of the United States had higher rates of mental illness than recent Latino immigrants. In addition, foreign-born Mexican Americans are at significantly lower risk of suicide and depression than those born in the United States. The increased rates of mental illness is thought to be due to increased distress associated with alienation, discrimination and Mexican Americans attempting to advance themselves economically and socially stripping themselves of traditional resources and ethnically-based social support.\n\nThe “healthy migrant effect” hypothesizes that the selection of healthy Hispanic immigrants into the United States is reason for the paradox. International immigration statistics demonstrate that the mortality rate of immigrants is lower than in their country of origin. In the United States, foreign-born individuals have better self-reported health than American-born respondents. Furthermore, Hispanic immigrants have better health than those living in the US for a long amount of time.\n\nA second popular hypothesis, called the “Salmon Bias”, attempts to factor in the occurrence of returning home. This hypothesis purports that many Hispanic people return home after temporary employment, retirement, or severe illness, meaning that their deaths occur in their native land and are not taken into account by mortality reports in the United States. This hypothesis considers those people as “statistically immortal” because they artificially lower the Hispanic mortality rate. Certain studies hint that it could be reasonable. These studies report that though return migration, both temporary and permanent, depend upon specific economic and social situations in communities, up to 75% of household in immigrant neighborhoods do some kind of return migration from the U.S. However, Abraido-Lanza, \"et al.\" found in 1999 that the “Salmon Hypothesis” cannot account for the lower mortality of Hispanics in the US because, according to their findings, the Hispanic paradox is still present when non-returning migrants are observed (e.g. Cubans).\n\nHorvath et al. (2013) have proposed that the lower mortality of Hispanics could reflect a slower biological aging rate of Hispanics. This hypothesis is based on the finding that blood and saliva from Hispanics ages more slowly than that of non-Hispanic whites, African Americans, and other populations according to a biomarker of tissue age known as epigenetic clock.\n\nOne of the most important aspects of this phenomenon is the comparison of Hispanics' health to non-Hispanic African Americans' health. Both the current and historical poverty rates for Hispanic and non-Hispanic African American populations in the United States are consistently starkly higher than that of non-Hispanic White and non-Hispanic Asian Americans. Dr. Hector Flores explains that “You can predict in the African–American population, for example, a high infant-mortality rate, so we would think a [similar] poor minority would have the same health outcomes.” However, he said, the health poor outcomes are not present in the Hispanic population. For example, the age-adjusted mortality rate for Hispanics living in Los Angeles County was 52 percent less than the blacks living in the same county.\n\nSome public health researchers have argued that the Hispanic paradox is not actually a national phenomenon in the United States. In 2006, Smith and Bradshaw argued that no Hispanic paradox exists. They maintain that life expectancies were nearly equal for non-Hispanic White and Hispanic females, but less close for non-Hispanic White and Hispanic Males. Turra and Goldman argue that the paradox is concentrated among the foreign born from specific national origins, and is only present in those of middle to older ages. At younger ages, they explain, deaths are highly related to environmental factors such as homicides and accidents. Deaths at older ages, they maintain, are more related to detrimental health-related behaviors and health status at younger ages. Therefore, immigration-related processes only offer survival protection to those at middle and older ages; the negative impact of assimilation into poor neighborhoods is higher on the mortality of immigrants at a younger age. In contrast, Palloni and Arias hypothesize that this phenomenon is most likely caused by across-the-board bias in underestimating mortality rates, caused by ethnic misidentification or an overstatement of ages. These errors could also be related to mistakes in matching death records to the National Health Interview Survey, missing security numbers, or complex surnames.\n\n"}
{"id": "46720456", "url": "https://en.wikipedia.org/wiki?curid=46720456", "title": "Hunting, fishing and animals in ancient Egypt", "text": "Hunting, fishing and animals in ancient Egypt\n\nThe ancient Egyptian culture is full of rich traditions and practices that until today we keep on learning about. Wildlife in ancient Egypt used to be very different compared to the wildlife currently present in Egypt for several factors and variables. Animals like elephants, rhinoceros, crocodiles and hippopotamus used to live in different parts of Egypt, however these animals do not exist in Egypt today. Animals were very much appreciated and played a big deal in Egyptian history; even some gods were represented as animals; as Hathor the goddess of fertility, love and beauty was represented as a cow.\n\nIn ancient Egypt, there existed a ceremony for slaughtering animals. However, there is no one common ritual, but several different ceremonies the most important ceremony is represented in the Re temple, the dramatically texts of Ramessuem and in the Book of Opening The Mouth. The pictures often display the same scene, where a bull is lying on the ground, with its legs tethered together; on the other side of the bull a woman; with the butcher ready to cut off its forelegs, a sem priest standing behind the butcher and a lector priest. The woman is identified as Isis; the sem priest gives the signal for the butcher to slaughter the bull and the lector priest reciting the ritual.\n\nSwamp hunting was a social event in which upper class hunting society families practiced. Swamp hunting included fowling with sticks and spear fishing. According to the narratives of the poorly preserved \"The Pleasures of Fishing and Fowling\" and \"The Sporting King\" which were edited by R. A. Caminos. T. These narratives described how the upper class enjoyed hunting as recreational sport. \"The Pleasures of Fishing and Fowling\" narrates King Amenemhet II's swamp hunts, where the royal hunting party travels to a lake in Fayum. The group included women of the harem and the king's children.\n\nEgypt's geographic location played a major role in the variety and population of birds in Egypt. Migrating Eurasian birds exhausted from their long journey come to rest in the wetlands of the Nile delta. Ancient Egyptians capitalized from the large flocks of birds and hunted them either for food, offerings to the dead and gods. Bird hunting through fowling with sticks was considered to be a sport practiced by royalty in ancient Egypt. Fowling with sticks was practiced by throwing a stick at flying birds. Initially, fowling with sticks was considered as a hobby practiced by the elite, fowling with sticks became a common practice to commoners and not specific to royalty after the Fifth Dynasty.\n\nA more efficient and effective technique practiced by ancient Egyptians to fowl birds was clap net, however it required teamwork, skilled fowlers, someone to coordinate and oversee, a clap net and sometimes a decoy bird which was usually a grey heron to attract the prey.\n\nFish were very abundant in Egypt, as Egypt is located on both the Mediterranean and Red Seas, along with the river Nile. Fishing was typically practiced on the river Nile, either by nets from a boat, using dragnets from shore or using bow nets in narrow banks of the river. On the other hand, fishing was also practiced as a sport for pleasure. Spear fishing and angle fishing were two types of fishing as a sport that required a lot of patience and skill.\n\nA demanding and challenging method of fishing, spear fishing requires certain attributes in the hunter, as patience to decoy the fish and a certain amount of accuracy to end up with a well-aimed throw. Spear fishing in ancient Egypt had greater value as a sport than angling did. Originally, in pre-historic and early times, spear fishing only served to provide food, and then it evolved into a recreation for the upper class.\n\nAccording to archeological evidence, spears used in sports could be divided into three types; spears with a single head, two headed spears and harpoons. It is not clear whether harpoons were used to fish for fish only or for crocodiles and hippopotamus also; this is because of the relative small size of the harpoon to the size of the hippopotamus and crocodiles.\n\nSimilar to modern fishing, angle fishing was a very common fishing technique, which requires a hook, however, no fishing rods were used at the time, instead, thick hand lines. Angling was mostly practiced among commoners and not upper class Egyptians. Unlike spear fishing, angling was not practiced as a sport but it was an important means of obtaining food. The picture evidence available does not show upper-class people practicing angling. However, usually the pictures display commoners using angling to fish from a boat, with their masters watching. Evidence of the first fishing rod appears in the Middle Kingdom period, in the tomb of Beni Hasan. Later on in tombs of 18th and 19th dynasty officials, do we see evidence of upper class Egyptians practicing fishing by angling with their wives, which indicates that by that time, fishing by angling had become an upper class recreational sport.\n\nA common hobby in our time, hunting was practiced as a way to gather food or for self-defense against wild animals in ancient Egypt. Once people started domesticating animals and depended on the reproduction of animals for food, that hunting lost its importance as a source of nutrition. As a result of the loss of dependency on hunting as a food source, hunting then became a recreational sport. Hunting was practiced by royalty to signify power and ability to protect their people from danger.\n\nThe hippopotamus often signifies chaos and evil in ancient Egypt, as the hippopotamus was believed to be the incarnation of the god Seth, the opponent of the good gods Osiris and Horus. Horus then avenged his father Osiris by killing Seth, who is incarnated as a hippopotamus. The king then takes the role of Horus whenever he kills the hippopotamus. From the First Dynasty onwards, some pictures have been found with scenes in which the king hunts alone, as the hippopotamus became the symbol of chaos and evil. Hunting the hippopotamus displayed the king's unmatched power, as depicted in King Dewen's cylindrical seal, where he wrestles and pins down the hippopotamus weaponless. Other pictures have been found of tomb owners in the New Kingdom killing the hippopotamus, these pictures are believed to have an exclusive religious significance.\n\nWild bulls were usually hunted by kings, this is evident in the story of king Amenophis III; where a man informed the king that there are wild bulls in the desert in the area of Fayum. The king then traveled north to Fayum accompanied by his army and ordered the soldiers to observe the wild bulls and confine them with fences and ditches. King Amenophis III spent four days in the hunt without resting his horses and had a tally of ninety-six wild bulls out of a total of one hundred and seventy bulls observed. Moreover, drawings of bull hunting have been represented on the walls of Ramesses II's funerary temple in Madinet Habu, where he stabs the last breath out of an injured bull.\n\nOften identified as the king of the jungle and a symbol of power in the animal kingdom. Earliest pictures of lion hunting came from late pre-historic or early historic times and in the beginning it was not intended to be as a sport, but to rid the country of a plague, which was threatening people. Later pictures emerged of the king taking hold of the lion to stab it to death as was displayed in Ramesses III's temple at Medinet Habu. Moreover, Tuthmosis III bragged about his ability to hunt lions, claiming that he killed seven lions in one second using his arrow shot. Amenophis III, a fan of big game hunting had a list of the animals he hunted, with one hundred and two wild lions in his first decade as ruler.\n\nIn prehistoric times, elephants were despised and initially driven out by Egyptians because of their consumption of the crops and damaging the agriculture. It is not until the Egyptians push into Asia in the 18th dynasty that the Egyptian came into contact with elephants. Hunting elephants was treasured by Egyptian kings because of their ivory, which was a remains a valuable, moreover, hunting elephants displayed the power of the king because of their immense size. Tuthmosis III reported that he killed one hundred and twenty elephants.\n"}
{"id": "28351679", "url": "https://en.wikipedia.org/wiki?curid=28351679", "title": "Illegal drug trade in Aruba", "text": "Illegal drug trade in Aruba\n\nThe illegal drug trade in Aruba involves trans-shipment of cocaine and other drugs through Aruba to the United States.\n\nCorruption in Aruba is so widespread that \"Claire Sterling, widely acclaimed for her works on drugs and crime, said of it: 'the world's first independent mafia state emerged in 1993.'\" The Italian daily \"Corriere della Sera\" described Aruba as \"the first state to be bought by the bosses Cosa Nostra.\" Between 1988 and 1992 the Cuntrera-Caruana Mafia clan was said to have acquired 60% of Aruba through investments in hotels, casinos and the election-campaign of a Prime Minister. As a result of the public outcry, Aruba's independence from the Netherlands, planned for 1996, was cancelled.\n\nWhile drug trafficking through Aruba used to be a major issue, it is much less so today. Aruba was removed from the US State Department’s list of major drug producing and transit countries in 1999. The reason for this decline is unclear. Case Study\n\n"}
{"id": "9024667", "url": "https://en.wikipedia.org/wiki?curid=9024667", "title": "List of UN numbers 1601 to 1700", "text": "List of UN numbers 1601 to 1700\n\nThe UN numbers from UN1601 to UN1700 as assigned by the United Nations Committee of Experts on the Transport of Dangerous Goods.\n\n"}
{"id": "2243764", "url": "https://en.wikipedia.org/wiki?curid=2243764", "title": "Medical assistant", "text": "Medical assistant\n\nA Medical Assistant is an allied health professional that supports the work of physicians and other health professionals, usually in a clinic setting. Medical assistants also referred as \"Clinical Assistant\" can become certified through an accredited program usually offered through a junior or community college. Medical Assistants perform routine tasks and procedures such as rooming and preparing patients, documenting patient medical history and current complaint or reason they are seeking medical attention. They measure patients' vital signs, assist healthcare provider with minor surgery prep, perform EKGs, and many other in office tests based on the specialty of the medical practice. Lab testing can be done in house, but most frequently, the specimens are collected, blood, urine, tissue are examples, they are prepared and packaged for outside lab testing. vaccines and therapeutic injections, keep accurate record of patient visit, phone and other communications, quite often, using Electronic Medical Record (EMR)Software. Often they are required to have current American Heart Association CPR and First Aid Certifications. \nMedical Assistants access information in medical recordkeeping systems, discuss and handle sensitive patient data so they must be familiar with HIPPA laws, Healthcare Information Privacy Protection Act. \n\nThe term \"medical assistant\" where they can be certified or registered, whereas elsewhere they may be a loosely defined group (covering related occupational titles such as ‘medical office assistant’, ‘clinical assistant’, 'assistant medical officer', or ‘ophthalmic assistant’). The occupation should not be confused with physician assistants, who are licensed professionals trained to practice medicine and perform surgical procedures in collaboration with a physician.\n\nIn military settings, occupations that provide primary medical care may go under similar titles, while other occupations may have different titles with similar responsibilities, such as Medical Assistant in the U.K. Royal Navy or Hospital Corpsman in the U.S. Navy.\n\nMedical assistants perform routine clinical and administrative duties under the direct supervision of a physician or other health care professional. Medical assistants perform many administrative duties, including answering telephones, greeting patients, updating and filing patients’ medical records, filling out insurance forms, handling correspondence, scheduling appointments, arranging for hospital admission and laboratory services, and handling billing and book keeping. Duties vary according to laws of the jurisdiction and may include taking medical histories and recording vital signs, explaining treatment procedures to patients, preparing patients for examination, and assisting during diagnostic examinations. Medical assistants collect and prepare laboratory specimens or perform basic laboratory tests on the premises, dispose of contaminated supplies, and sterilize medical instruments. They instruct patients about medications and special diets, prepare and administer medications as directed, authorize drug refills as directed, telephone prescriptions to a pharmacy, draw blood, prepare patients for X-rays, take electrocardiograms, remove sutures, and change dressings. They also facilitate communication between the patient and other health care professionals.\n\nSome jurisdictions allow medical assistants to perform more advanced procedures, such as giving injections or taking X-rays, after passing a test or taking a course.\n\nAccording to the International Standard Classification of Occupations, medical assistants normally require formal training in health services provision for competent performance in their jobs. Formal education usually occurs in post secondary institutions such as vocational schools, technical institutes, community colleges, proprietary colleges, online educational programs or junior colleges. Medical assistant training programs most commonly lead to a certificate or a diploma, which take around one year to complete, or an associate degree, which takes around two years. Study topics include medical terminology, anatomy and physiology, and programs may include a clinical internship, sometimes referred to as \"externship\", wherein the student works as a medical assistant in a medical clinic.\n\nIn Canada, medical assistants typically complete an educational program that prepares them to perform special assisting and secretarial duties for physicians, dentists, nurses, health care facilities, and other health service providers. Instructional programs include courses in business and medical communications, medical terminology, principles of health care operations, public relations and interpersonal communications, software applications, record-keeping and filing systems, scheduling and meeting planning, policies and regulations, and professional standards and ethics.\n\nMedical assistant job responsibilities vary depending on the nature and size of the health care facility where the individual works, but typically involve multiple administrative duties such as scheduling appointments, handling private medical documents, and assisting patients with the admissions process.\n\nIn Malaysia, Medical Assistants are known as Assistant Medical Officers (AMO). They complete a three and half year Diploma in Medical Assistant (DMA) undergraduate program recognized by the Malaysian Qualifications Agency. They work independently or with limited supervision of a physician to provide healthcare services to largely underserved populations. The occupation is more similar to that of clinical officers in Tanzania and elsewhere.\nIn Bangladesh, Medical Assistants are known as Sub Assistant Community Medical Officer (SACMO). Medical assistants (now to be designated as sub-assistant community medical officer) assist the medical officers posted at health facilities at the upazila health complex level and below. Medical assistants are produced by Medical Assistants Training School (MATS). They get registration from Bangladesh Medical and Dental Council as a medical assistant practitioner.\n\nIn the United States, medical assistants have traditionally held jobs almost exclusively in ambulatory care centers, urgent care facilities, and clinics, but this is now changing. Medical assistants now find employment in both private and public hospitals, inpatient and outpatient facilities, as well as assisted living facilities, Administrative and Clinical settings, or \nGeneral practice and Specialty Doctor’s offices. According to the U.S. Department of Labor, Occupational Outlook Handbook, 2014-15 Edition, employment of medical assistants is expected to grow by 29%, much faster than the average for all occupations through 2022. \n\nThe New America Foundation has criticized medical assistant programs, particularly those run by profit-making schools like Kaplan and Everest Institute. Many graduates of the school can't find full-time work, or can't find work at all, can't make enough to pay their loans, and go into default. According to the Department of Labor, median annual salary for medical assistants in 2011 was $29,100, but students with medical-assistant certificates typically earned less than $20,000. In some programs, graduates earned less than $15,080, the minimum wage, which means they were working part-time. For example, Drake College of Business, Elizabeth, NJ, charges $18,000, but 31% of graduates defaulted on loans. A few public community colleges have successful programs where graduates make more than $25,000 a year.\n\nIn the U.S., an institution's medical assisting program may be accredited by the Commission on Accreditation of Allied Health Education Programs (CAAHEP) or the Accrediting Bureau of Health Education Schools (ABHES) if its graduates plan to become certified or registered. Accreditation is a requirement of certification agencies such as the American Association of Medical Assistants (AAMA), the American Medical Technologists (AMT) and the National Health Career Association (NHA). Currently there are in excess of 600 CAAHEP accredited programs in can than 500 institutions, and more than 200 accredited by ABHES. Accreditation by CAAHEP, ABHES or other accreditation associations requires that the institution's medical assisting program meets specific educational standards and provides sufficient classroom, lecture, and laboratory time.\n\nProfessional certification is a way to measure competency of a medical assistant at an entry-level job. Certification for medical assistants is voluntary and optional, though encouraged by the American Association of Medical Assistants (AAMA) and a number of other certification bodies. Employers increasingly prefer or even require that the medical assistants they hire be certified.\n\nIn the United States, different organizations certify medical assistants. For one, the American Association of Medical Assistants (AAMA) was founded in 1956. Certification may be achieved by taking the CMA (AAMA) Certification Examination offered by the AAMA Certifying Board in consultation with the National Board of Medical Examiners, which also administers many national exams for physicians. The CMA (AAMA) exam is offered throughout the year at computer-based testing centers across the country. Only individuals who have successfully completed a CAAHEP or ABHES accredited medical assisting program are eligible for the CMA (AAMA) Certification Examination. Those who successfully complete the CMA (AAMA) Certification Examination earn the CMA (AAMA) credential, a title which then follows postnominally. A CMA (AAMA) must re-certify every 60 months by continuing education or re-examination in order to maintain certification.\n\nOther credential options include becoming a Registered Medical Assistant (RMA). Credentialing is voluntary. The American Medical Technologists (AMT) agency is responsible for certifying MAs who choose this course. The AMT first began offering this certification in 1972. AMT has its own conventions and committees, bylaws, state chapters, officers, registrations, and re validation examinations. To become eligible to hold the title of RMA, a student must either pass a medical assisting curriculum at a school that accredited by the National Commission for Certifying Agencies (NCCA), or possess a minimum of 5 years experience.\n\nThe National Center for Competency Testing (NCCT) is an independent credentialing organization that has administered more than 400,000 certification exams across the United States since 1989. Its National Certified Medical Assistant certification program has earned accreditation by the National Commission for Certifying Agencies (NCCA). Candidates who meet all medical assistant eligibility requirements and pass the NCCT national certification examination earn the credential NCMA(NCCT). NCCT accepts candidates from approved medical assistant programs in colleges/universities and provides additional experiential-based qualifying routes. Once certified, the NCMA(NCCT) must complete 14 clock hours of continuing education annually to maintain the credential. NCMA Handbook The NCCT also certifies medical office assistants, ECG technicians, phlebotomists, patient care technicians, insurance and coding specialists, and surgical technologists.\n\n\n\nhuehrytuno"}
{"id": "1585648", "url": "https://en.wikipedia.org/wiki?curid=1585648", "title": "Metabolic engineering", "text": "Metabolic engineering\n\nMetabolic engineering is the practice of optimizing genetic and regulatory processes within cells to increase the cells' production of a certain substance. These processes are chemical networks that use a series of biochemical reactions and enzymes that allow cells to convert raw materials into molecules necessary for the cell’s survival. Metabolic engineering specifically seeks to mathematically model these networks, calculate a yield of useful products, and pin point parts of the network that constrain the production of these products. Genetic engineering techniques can then be used to modify the network in order to relieve these constraints. Once again this modified network can be modeled to calculate the new product yield.\n\nThe ultimate goal of metabolic engineering is to be able to use these organisms to produce valuable substances on an industrial scale in a cost effective manner. Current examples include producing beer, wine, cheese, pharmaceuticals, and other biotechnology products. Some of the common strategies used for metabolic engineering are (1) overexpressing the gene encoding the rate-limiting enzyme of the biosynthetic pathway, (2) blocking the competing metabolic pathways, (3) heterologous gene expression, and (4) enzyme engineering.\n\nSince cells use these metabolic networks for their survival, changes can have drastic effects on the cells' viability. Therefore, trade-offs in metabolic engineering arise between the cells ability to produce the desired substance and its natural survival needs. Therefore, instead of directly deleting and/or overexpressing the genes that encode for metabolic enzymes, the current focus is to target the regulatory networks in a cell to efficiently engineer the metabolism.\n\nIn the past, to increase the productivity of a desired metabolite, a microorganism was genetically modified by chemically induced mutation, and the mutant strain that overexpressed the desired metabolite was then chosen. However, one of the main problems with this technique was that the metabolic pathway for the production of that metabolite was not analyzed, and as a result, the constraints to production and relevant pathway enzymes to be modified were unknown.\n\nIn 1990s, a new technique called metabolic engineering emerged. This technique analyzes the metabolic pathway of a microorganism, and determines the constraints and their effects on the production of desired compounds. It then uses genetic engineering to relieve these constraints. Some examples of successful metabolic engineering are the following: (i) Identification of constraints to lysine production in \"Corynebacterium\" \"glutamicum\" and insertion of new genes to relieve these constraints to improve production (ii) Engineering of a new fatty acid biosynthesis pathway, called reversed beta oxidation pathway, that is more efficient than the native pathway in producing fatty acids and alcohols which can potentially be catalytically converted to chemicals and fuels (iii) Improved production of DAHP an aromatic metabolite produced by \"E. coli\" that is an intermediate in the production of aromatic amino acids. It was determined through metabolic flux analysis that the theoretical maximal yield of DAHP per glucose molecule utilized, was 3/7. This is because some of the carbon from glucose is lost as carbon dioxide, instead of being utilized to produce DAHP. Also, one of the metabolites (PEP, or phosphoenolpyruvate) that are used to produce DAHP, was being converted to pyruvate (PYR) to transport glucose into the cell, and therefore, was no longer available to produce DAHP. In order to relieve the shortage of PEP and increase yield, Patnaik et al. used genetic engineering on \"E. coli\" to introduce a reaction that converts PYR back to PEP. Thus, the PEP used to transport glucose into the cell is regenerated, and can be used to make DAHP. This resulted in a new theoretical maximal yield of 6/7 – double that of the native \"E. coli\" system.\n\nAt the industrial scale, metabolic engineering is becoming more convenient and cost effective. According to the Biotechnology Industry Organization, \"more than 50 biorefinery facilities are being built across North America to apply metabolic engineering to produce biofuels and chemicals from renewable biomass which can help reduce greenhouse gas emissions\". Potential biofuels include short-chain alcohols and alkanes (to replace gasoline), fatty acid methyl esters and fatty alcohols (to replace diesel), and fatty acid-and isoprenoid-based biofuels (to replace diesel).\n\nMetabolic engineering continues to evolve in efficiency and processes aided by breakthroughs in the field of synthetic biology and progress in understanding metabolite damage and its repair or preemption. Early metabolic engineering experiments showed that accumulation of reactive intermediates can limit flux in engineered pathways and be deleterious to host cells if matching damage control systems are missing or inadequate. Researchers in synthetic biology optimize genetic pathways, which in turn influence cellular metabolic outputs. Recent decreases in cost of synthesized DNA and developments in genetic circuits help to influence the ability of metabolic engineering to produce desired outputs.\n\nAn analysis of metabolic flux can be found at \"Flux balance analysis\"\n\nThe first step in the process is to identify a desired goal to achieve through the improvement or modification of an organism's metabolism. Reference books and online databases are used to research reactions and metabolic pathways that are able to produce this product or result. These databases contain copious genomic and chemical information including pathways for metabolism and other cellular processes. Using this research, an organism is chosen that will be used to create the desired product or result. Considerations that are taken into account when making this decision are how close the organism's metabolic pathway is to the desired pathway, the maintenance costs associated with the organism, and how easy it is to modify the pathway of the organism. \"Escherichia coli\" (\"E. coli\") is widely used in metabolic engineering to synthesize a wide variety of products such as amino acids because it is relatively easy to maintain and modify. If the organism does not contain the complete pathway for the desired product or result, then genes that produce the missing enzymes must be incorporated into the organism.\n\nThe completed metabolic pathway is modeled mathematically to find the theoretical yield of the product or the reaction fluxes in the cell. A flux is the rate at which a given reaction in the network occurs. Simple metabolic pathway analysis can be done by hand, but most require the use of software to perform the computations. These programs use complex linear algebra algorithms to solve these models. To solve a network using the equation for determined systems shown below, one must input the necessary information about the relevant reactions and their fluxes. Information about the reaction (such as the reactants and stoichiometry) are contained in the matrices G and G. Matrices V and V contain the fluxes of the relevant reactions. When solved, the equation yields the values of all the unknown fluxes (contained in V).\n\nAfter solving for the fluxes of reactions in the network, it is necessary to determine which reactions may be altered in order to maximize the yield of the desired product. To determine what specific genetic manipulations to perform, it is necessary to use computational algorithms, such as OptGene or OptFlux. They provide recommendations for which genes should be overexpressed, knocked out, or introduced in a cell to allow increased production of the desired product. For example, if a given reaction has particularly low flux and is limiting the amount of product, the software may recommend that the enzyme catalyzing this reaction should be overexpressed in the cell to increase the reaction flux. The necessary genetic manipulations can be performed using standard molecular biology techniques. Genes may be overexpressed or knocked out from an organism, depending on their effect on the pathway and the ultimate goal.\n\nIn order to create a solvable model, it is often necessary to have certain fluxes already known or experimentally measured. In addition, in order to verify the effect of genetic manipulations on the metabolic network (to ensure they align with the model), it is necessary to experimentally measure the fluxes in the network. To measure reaction fluxes, carbon flux measurements are made using carbon-13 isotopic labeling. The organism is fed a mixture that contains molecules where specific carbons are engineered to be carbon-13 atoms, instead of carbon-12. After these molecules are used in the network, downstream metabolites also become labeled with carbon-13, as they incorporate those atoms in their structures. The specific labeling pattern of the various metabolites is determined by the reaction fluxes in the network. Labeling patterns may be measured using techniques such as gas chromatography-mass spectrometry (GC-MS) along with computational algorithms to determine reaction fluxes.\n\n\nBiotechnology Industry Organization(BIO) website:\n"}
{"id": "26964055", "url": "https://en.wikipedia.org/wiki?curid=26964055", "title": "Obstetrics &amp; Gynecology (journal)", "text": "Obstetrics &amp; Gynecology (journal)\n\nObstetrics & Gynecology is a peer-reviewed medical journal in the field of obstetrics and gynecology. It is the official publication of the American College of Obstetricians and Gynecologists. It is popularly known as the \"Green Journal\".\n\n\"Obstetrics & Gynecology\" has approximately 45,000 subscribers. According to the 2014 Journal Citation Reports, it had an impact factor of 5.175, ranking it 2nd among 79 reproductive medicine journals.\n"}
{"id": "53627834", "url": "https://en.wikipedia.org/wiki?curid=53627834", "title": "Oncology Care Model", "text": "Oncology Care Model\n\nThe Oncology Care Model (OCM) is an episode-based payment system developed by the Center for Medicare and Medicaid Innovation. The multipayer model is designed for discrete instances of care, especially those involving chemotherapy, which triggers the six-month episode. The program combines fee-for-service (FFS) payments for established services, monthly payments for additional care under a structured guideline, and performance-based payments weighed against quality metrics and benchmarks.\n\nOCM is part of a general move away from the FFS model, \"which pays doctors and hospitals according to the number of procedures they do, toward value-based care, which pays based on what helps patients get better.\" This idea was advanced by the Affordable Care Act (ACA), which was signed into law on March 23, 2010. , OCM is being utilized by 190 healthcare provider groups, which include over 3,000 physicians in the United States. Along with Centers for Medicare and Medicaid Services, the payment system is accepted by 16 other health care coverage programs in the US. The payment model went into operation in July 2016, and barring changes to the Affordable Care Act, is slated to run until 2021. Over this five-year period, it is estimated that the model will be used for $6 billion spent on medical care to 155,000 patients.\n\nThe program is a move by the CMS to shift its focus to include specialized care. The bundled design has been the source of praise and criticism for the payment system. The program has been criticized for not going far enough; that is not eliminating FFS altogether. Other criticisms include the lack of flexibility in allowing primary care physicians to conduct care as they see fit, the arbitrary nature of the time period or episode, the cumbersome burden of the reporting standards and how it penalizes practices for outcomes out of their control.\n\nThe Affordable Care Act mandated the creation of the Center for Medicare and Medicaid Innovation (CMMI) as part of the Centers for Medicare and Medicaid Services (CMS). It was created to test new \"payment and delivery system models\" to be used by \"Medicare, Medicaid, and the Children’s Health Insurance Program.\" The legislation also created the accountable care organizations (ACO) model, which holds voluntarily-enrolled health care practitioners accountable to patients and third-party payers for the quality, appropriateness, and efficiency of its services. ACO introduced the concept of rewards based on savings or \"shared savings,\" which would later be applied to OCM. However, the results were mixed, with \"only 31 percent of the nearly 400 ACOs\" being successful in seeing returns.\n\nA related program is the Community Oncology Medical Home or COME HOME—a program to develop medical homes as a part of several oncology centers across the US. OCM was based on this earlier model developed by Barbara McAneny and Innovative Oncology Business Solutions through a $20 million grant from the CMMI. COME HOME included provisions which would later be emulated by OCM such as: requirement for use of electronic health records (EHR), patient education, access to 24/7 telephone support and same-day appointments. The program has reported lower rate of hospitalization, use of emergency services and lower cost of care.\n\nIn September 2014, while OCM was still in development, the American Society of Clinical Oncology (ASCO) commented on the program, \"urging CMS to explore more substantial reforms,\" and also offered its own alternative payment model. In February 2015, CMMI launched a demonstration program that would include a potential 100 oncology practices and invited other insurance payers to participate. OCM was officially launched on July 1, 2016. It was announced on June 28, as part of Vice President Joe Biden's Cancer Moonshot Summit.\n\nPhysicians and Hospitals can earn $160 per patient per month for an entire 6 months which begins at the initiation of chemotherapy treatment. In order to qualify for these payments, the practice has to continually meet the following six care-standards:\nIn order to receive performance-based payments, the practices must demonstrate a lowered spending per treatment episode when compared to benchmark standards. The benchmark is determined through the use of a risk-adjustment of expenditures compared to \"a historical baseline period trended forward to the current performance period.\" The program requires that practices report the outcome of their treatments and compares that to quality metrics to determine the level of reimbursement to the practice. A number of healthcare provider groups have announced that they have developed systems of reporting specifically designed for oncology to meet the demands of OCM and its EHR requirements. Some of these include third-parties solutions like Archway Health, Cota Healthcare, Flatiron Health, McKesson Specialty Health, and Navigating Cancer.\n\nIn their response to the model, Blase Polite and Harold Miller of the University of Chicago criticized OCM for not going far enough. In their view, the failure to eliminate FFS all together is a primary shortcoming of the payment model. A superficial monthly payment does little to prevent the unnecessary clinic visits doctors are forced to schedule in order to justify fees for actual care provided: \n\nThey argue that the $160 per-beneficiary-per-month payments are insufficient to alleviate this sort of lose of revenue and the possibility of return-on-savings rarely succeeded in previous ACO models in testing.\n\nThere is little scientific basis for the six months length of an episode of care that begins at the initiation of chemotherapy. While some oncology treatments take only mere weeks to complete, others can take the full six months of the episode to complete. However, the physician is paid the full bundle rate for both cases. In fact, if the care of the patient takes a little over six months, the physician is required to maintain care for a second episode and entitled to payments for two bundled episodes. According to Polite and Miller, this creates a \"perverse incentive\" to the physician to delay ending treatment or delay a portion of the treatment until the second episode in order to demonstrate lowered spending in the first episode, which is a requirement of performance-based payment.\n\n, the following insurers currently accept the OCM payment system:\n"}
{"id": "19663248", "url": "https://en.wikipedia.org/wiki?curid=19663248", "title": "Osamu Shimomura", "text": "Osamu Shimomura\n\nBorn in Fukuchiyama, Kyoto in 1928, Shimomura was brought up in Manchukuo (Manchuria, China) and Osaka, Japan while his father served as an officer in the Imperial Japanese Army. Later, his family moved to Isahaya, Nagasaki, 25 km from the epicenter of the August 1945 atomic bombing of the city. He recalls hearing, as a 16-year-old boy, the bomber plane Bockscar before the atom bomb exploded. The explosion flash blinded Shimomura for about thirty seconds, and he was later drenched by the \"black rain\" bomb fallout. He overcame great odds in the following 11 years to earn an education and achieve academic success.\n\nShimomura's education opportunities were starkly limited in devastated, post-war Japan. Although he later recalled having no interest in the subject, he enrolled in the College of Pharmaceutical Sciences of Nagasaki Medical College (now Nagasaki University School of Pharmaceutical Sciences). The Medical College campus had been entirely destroyed by the atomic bomb blast, forcing the pharmacy school to relocate to a temporary campus near Shimomura's home. This proximity was the fortuitous reason he embarked upon the studies and career which would ultimately lead to unanticipated rewards. Shimomura was awarded a BS degree in pharmacy in 1951, and he stayed on as a lab assistant through 1955.\n\nShimomura's mentor at Nagasaki helped him find employment as an assistant to Professor Yoshimasa Hirata at Nagoya University in 1956. While working for Professor Hirata, he received a MS degree in organic chemistry in 1958 and, before leaving Japan for an appointment at Princeton University, a Ph.D. in organic chemistry in 1960 at Nagoya University. At Nagoya, Hirata assigned Shimomura the challenging task of determining what made the crushed remains of a type of crustacean (Jp. \"umi-hotaru\", lit. \"sea-firefly\", \"Vargula hilgendorfii\") glow when moistened with water. This assignment led Shimomura to the successful identification of the protein causing the phenomenon, and he published the preliminary findings in the \"Bulletin of the Chemical Society of Japan\" in a paper titled \"Crystalline Cypridina luciferin.\" The article caught the attention of Professor Frank Johnson at Princeton University, and Johnson successfully recruited Shimomura to work with him in 1960.\n\nShimomura worked in the Department of Biology at Princeton for Professor Johnson to study the jellyfish \"Aequorea victoria\", which they collected during many summers at the Friday Harbor Laboratories of the University of Washington. In 1962, their work culminated in the discovery of the proteins aequorin and green fluorescent protein (GFP) in the small bioluminescent jellyfish \"Aequorea victoria\"; for this work, he was awarded a third of the Nobel Prize in Chemistry in 2008.\n\nHis wife, Akemi, whom Shimomura met at Nagasaki University, is also an organic chemist and a partner in his research activities. Their son, Tsutomu Shimomura, is a computer security expert who was involved in the arrest of Kevin Mitnick. Their daughter, Sachi Shimomura, is director of Undergraduate Studies for the English Department at Virginia Commonwealth University and the author of \"Odd Bodies and Visible Ends in Medieval Literature\".\n\n\n\n\n"}
{"id": "639222", "url": "https://en.wikipedia.org/wiki?curid=639222", "title": "Parvovirus B19", "text": "Parvovirus B19\n\nPrimate erythroparvovirus 1, generally referred to as B19 virus, parvovirus B19 or sometimes erythrovirus B19, was the first (and until 2005 the only) known human virus in the family \"Parvoviridae\", genus \"Erythroparvovirus\"; it measures only 23–26 nm in diameter. The name is derived from Latin, parvum meaning small, reflecting the fact that B19 ranks among the smallest DNA viruses. B19 virus is most known for causing disease in the pediatric population; however, it can also affect adults. It is the classic cause of the childhood rash called fifth disease or erythema infectiosum, or \"slapped cheek syndrome\".\n\nThe virus was discovered by chance in 1975 by Australian virologist Yvonne Cossart. It gained its name because it was discovered in well B19 of a large series of microtiter plates.\n\nErythroviruses belong to the \"Parvoviridae\" family of small DNA viruses. It is a non-enveloped, icosahedral virus that contains a single-stranded linear DNA genome. The infectious particles may contain either positive or negative strands of DNA. The icosahedral capsid consists of two structural proteins, VP1 (83 kDa) and VP2 (58 kDa), which are identical except for 227 amino acids at the amino-terminal of the VP1-protein, the so-called VP1-unique region. Each capsid consists of a total of 60 capsomers: VP2 is the major capsid protein, and comprises approximately 95% of the total virus particle. VP1-proteins are incorporated into the capsid structure in a non-stochiometrical relation (based on antibody-binding analysis and X-ray structural analysis the VP1-unique region is assumed to be exposed at the surface of the virus particle. At each end of the DNA molecule there are palindromic sequences which form \"hairpin\" loops. The hairpin at the 3' end serves as a primer for the DNA polymerase. It is classified as erythrovirus because of its capability to invade red blood cell precursors in the bone marrow. Three genotypes (with subtypes) have been recognised.\n\nThe Nucleotide substitution rate for total coding DNA has been estimated to be 1.03 (0.6-1.27) x 10 substitutions/site/year. This rate is similar to that of other single stranded DNA viruses. VP2 codons were found to be under purifying selection. In contrast VP1 codons in the unique part of the gene were found to be under diversifying selection. This diversifying selection is consistent with persistent infection as this part of the VP1 protein contains epitopes recognised by the immnune system.\n\nLike other nonenveloped DNA viruses, pathogenicity of parvovirus B19 involves binding to host cell receptors, internalization, translocation of the genome to the host nucleus, DNA replication, RNA transcription, assembly of capsids and packaging of the genome, and finally cell lysis with release of the mature virions. In humans the P antigen (also known as globoside) is the cellular receptor for parvovirus B19 virus that causes Erythema infectiosum (fifth disease) in children. This infection is sometimes complicated by severe aplastic anemia caused by lysis of early erythroid precursors.\n\nThe most recent common ancestor of the extant strains has been dated to about 12,600 years ago. Three genotypes - 1,2 and 3 - are recocogised. A recombination between types 1 and 3 gave rise to genotype 2 between 5,000 and 6,800 years ago.\n\nThe virus is primarily spread by infected respiratory droplets; blood-borne transmission, however, has been reported. The secondary attack risk for exposed household persons is about 50%, and about half of that for classroom contacts.\n\nSymptoms begin some six days after exposure (between 4 and 28 days, with the average being 16 to 17 days) and last about a week. Infected patients with normal immune systems are contagious before becoming symptomatic, but probably not after then. Individuals with B19 IgG antibodies are generally considered immune to recurrent infection, but reinfection is possible in a minority of cases. About half of adults are B19-immune due to a past infection.\n\nA significant increase in the number of cases is seen every three to four years; the last epidemic year was 1998. Outbreaks can arise especially in nurseries and schools.\n\nParvovirus B19 causes an infection in humans only. Cat and dog parvoviruses do not infect humans. There is no vaccine available for human parvovirus B19, though attempts have been made to develop one.\n\nFifth disease or \"erythema infectiosum\" is only one of several expressions of Parvovirus B19. The associated bright red rash of the cheeks gives it the nickname \"slapped cheek syndrome\". Any age may be affected, although it is most common in children aged six to ten years. It is so named because it was the fifth most common cause of a pink-red infection associated rash to be described by physicians (many of the others, such as measles and rubella, are rare now) .\n\nOnce infected, patients usually develop the illness after an incubation period of four to fourteen days. The disease commences with high fever and malaise, when the virus is most abundant in the bloodstream, and patients are usually no longer infectious once the characteristic rash of this disease has appeared. The following symptoms are characteristic:\nTeenagers or young adults may develop the so-called \"Papular Purpuric Gloves and Socks Syndrome\".\nParvovirus B19 is a cause of chronic anemia in individuals who have AIDS. It is frequently overlooked. Treatment with intravenous immunoglobulin usually resolves the anemia although relapse can occur. The parvovirus infection may trigger an inflammatory reaction in AIDS patients who have just begun antiretroviral therapy.\n\nArthralgias and arthritis are commonly reported in association with parvovirus B19 infection in adults whereas erythema infectiosum is the main symptom observed in children. The occurrence of arthralgia coincides with the initial detection of circulating IgM- and IgG-antibodies against the viral structural proteins VP1 and VP2. Parvovirus B19 infection may affect the development of arthritis. In adults (and perhaps some children), parvovirus B19 can lead to a seronegative arthritis which is usually easily controlled with analgesics. Women are approximately twice as likely as men to experience arthritis after parvovirus infection. Possibly up to 15% of all new cases of arthritis are due to parvovirus, and a history of recent contact with a patient and positive serology generally confirms the diagnosis. This arthritis does not progress to other forms of arthritis. Typically joint symptoms last 1–3 weeks, but in 10–20% of those affected, it may last weeks to months.\n\nAlthough most patients have a decrease of erythropoiesis (production of red blood cells) during parvovirus infection, it is most dangerous in patients with pre-existing bone marrow stress, for example sickle cell anemia or hereditary spherocytosis, and are therefore heavily dependent on erythropoiesis due to the reduced lifespan of the red cells. This is termed \"aplastic crisis\" (also called reticulocytopenia). It is treated with blood transfusion.\n\nParvovirus infection in pregnant women is associated with hydrops fetalis due to severe fetal anemia, sometimes leading to miscarriage or stillbirth. This is due to a combination of hemolysis of the red blood cells, as well as the virus directly negatively affecting the red blood cell precursors in the bone marrow. The risk of fetal loss is about 10% if infection occurs before pregnancy week 20 (especially between weeks 14 and 20), but minimal after then. Routine screening of the antenatal sample would enable the pregnant mother to determine the risk of infection. Knowledge of her status would allow the mother to avoid contact with individuals suspected or known to have an ongoing infection, however, at the present time, antenatal testing for immunity is not recommended, since there is no good means to prevent the infection, there is no specific therapy and there are no vaccines available. It may increase maternal anxiety and fear without proven benefit. The best approach would be to recommend all pregnant women to avoid contact with children with current symptoms of infection, as described above. The risk to the fetus will be reduced with correct diagnosis of the anemia (by ultrasound scans) and treatment (by blood transfusions). There is some evidence that intrauterine Parvovirus B19 infection leads to developmental abnormalities in childhood.\n\nAt the moment, there are no treatments that directly target Parvovirus B19 virus. Intravenous immunoglobulin therapy (IVIG) therapy has been a popular alternative because doctors can administer it without stopping chemotherapy drugs like MEL-ASCT. Also, the treatment's side effects are rare as only 4 out of 133 patients had complications (2 had acute renal failure and 2 had pulmonary edema) even though 69 of the patients had organ transplants and 39 of them were HIV positive. This is a large improvement over administering Rituximab . The monoclonal antibody against the CD20 protein has been shown to cause acute hepatitis, neutropenia via Parvovirus B19 reactivations, and even persistent Parvovirus B19 infection. However, it is important to note that IVIG therapy is not perfect as 34% of treated patients will have a relapse after 4 months.\n\nAs of 2017, no approved human vaccine existed against Parvovirus B19.\n\n\n"}
{"id": "57222276", "url": "https://en.wikipedia.org/wiki?curid=57222276", "title": "Pascual-Castroviejo syndrome", "text": "Pascual-Castroviejo syndrome\n\nPascual-Castroviejo syndrome is a rare autosomal recessive condition characterized by facial dysmorphism, cognitive impairment and skeletal anomalies. \n\nThese can be divided into four areas\n\n\n\n\n\nThis disease is caused by mutations in the transmembrane and coiled-coil domain-containing protein 1 (TMCO1) on the long arm of chromosome 1.\n\nPascual-Castroviejo syndrome is rare. About 20 cases have been reported worldwide.\n\nThe diagnosis may be provisionally made on clinical grounds. Further diagnostic tests include serum and urine analysis for lactic acid, a chest X ray (or cardiac CT or MRI) and echocardiography. Biopsies from cardiac and skeletal muscle will show the presence of lipid and glycogen. Testing for mitochondrial abnormalities including adenosine nucleotide transporter deficiency and decreases in the respiratory chain complexes I and IV can also be done. \n\nThis condition forms part of the spectrum of TMCO1 defects. There may be some overlap in features.\n\nThere is no known treatment for this condition. Surgery may be helpful in treating the cleft lip and palate. \n\nAll cases to date have been reported in children. Long term prognosis is not known. \n\nThis condition was first described in 1975. \n"}
{"id": "90320", "url": "https://en.wikipedia.org/wiki?curid=90320", "title": "Patecatl", "text": "Patecatl\n\nIn Aztec mythology, Patecatl is a god of healing and fertility, and the discoverer of peyote as well as the \"lord of the root of pulque \". With Mayahuel, he was the father of the Centzon Totochtin.\n\nIn the Aztec calendar, Patecatl is the lord of the thirteen days from 1 Monkey to 13 House. The preceding thirteen days are ruled over by Mictlantecuhtli, and the following thirteen by Itztlacoliuhqui.\n"}
{"id": "5116656", "url": "https://en.wikipedia.org/wiki?curid=5116656", "title": "Personal Track Safety", "text": "Personal Track Safety\n\nPersonal Track Safety (PTS) is a system of safer working practices employed within the United Kingdom designed to ensure the safety of railway workers who have to work on or near the line.\n\nThe principal hazards include collisions between a rail vehicle and a track worker, electrocution from traction power sources (third rail, fourth rail, OHLE) and trips and falls. The last could compound the other two (e.g. a worker could fall onto an electrified third rail). PTS ensures that rail workers are aware of their surroundings so that they do not enter situations where the aforementioned accidents are likely to occur, are able to move around the lineside safely and are able to react appropriately to circumstances (e.g. the approach of a train).\n\nCompared to road vehicles, trains have a much greater stopping distance at the same speed, but often travel much faster than road vehicles. Unlike road vehicles, they cannot swerve out of the way of obstructions. Trains cannot be relied upon to stop for rail workers. Hence it is the duty of the track worker to remain or retire to a safe location on the approach of a train. It is important that a lookout is kept (often working as a team). In order that trains can indicate their presence to workers, orange high visibility clothing must be worn. Clothing that is yellow, green or red is disallowed because those colours are the colours of signal flags.\n\nA Sentinel Card is required before anybody is allowed to work within the boundary of Network Rail tracks in the UK (on or near the line). It is also a requirement in the Republic of Ireland. Any potential employee must undergo a medical and a drug and alcohol test before attending a \"personal track safety\" course.\n\nThe Sentinel Card is a smartcard which links to an online database, giving details of the holders competencies and other details:\n- this may include\n\nDeregulation in the national railway service in the UK has meant that increasing numbers of subcontractors are being employed within the track environment. Contractors are often used for specific limited projects and are often from companies that have not previously been considered as having \"railway pedigrees or culture\".\n\nTo work on or near the track involves certification for the individual employee of which there are several levels of competence and responsibility which are assessed.\n\nThe basic level for track access for Network Rail is the Personal Track Safety certificate (PTS) as defined by the railway safety standards NR/L2/CTM/021. These levels of certification only assess the individual’s ability to work safely in the track environment. This means that the individual is able to respond and correctly react to circumstances which may arise in this potentially dangerous environment. Persons therefore should not have any condition or be taking any medication that may lead to sudden incapacity, loss of consciousness, dizziness, impairment of awareness, concentration, balance, coordination, or any significant limitation of mobility or impairment of hearing and vision. Persons with minor medical conditions (e.g. colour blind) may still be issued a PTS – though the card will have an indicator, a blue circle, to inform the Controller of Site Safety (COSS) to that person's ailment. Similarly, those who have only recently got their PTS certificate will have a green square on their online record to indicate their 'new' status, as well as a blue hard hat instead of the usual white hard hat.\n\nThe medical assessment undertaken for PTS certification does not address the medical specificity for tasks that an individual may be expected to undertake in their normal duties when actually working on the track. i.e. operating machinery or responsibility for other staff requires a higher level of assessment for the job specification and procedures.\n\nThe medical assessment for these certificates must be undertaken by an accredited medical provider. In the case of PTS this is authorised byRISQS under the authority of Network Rail.\n\nProviders of this service are registered appropriately and regular audit is undertaken to ensure that they meet the quality control levels for the specific standards of performance and training needed to reliably undertake the medical assessments.\n\nSince 2006 the standards for a PTS have been regulated by the Office of Rail and Road\n\nBasic medical assessment for national rail PTS involves the following medical modalities and components.\n\n\nWhere an individual fails to meet the specific medical standards it is, sometimes possible with the written agreement and co-operation of management and occupational health to implement a system of formal \"safe working practices\". This is so as not to discriminate unnecessarily against individuals with certain medical problems.\n\nThe object of the medical assessment is to ensure that employers meet their duty of care so that individuals working in this potentially dangerous environment are not subject to increased risk of harm to themselves or their colleagues, or in some cases members of the public, due to any foreseeable underlying medical condition..\n\nThe work environment is in continual flux, and changes in acceptable risk are therefore variable. For this reason a provider of these services must be up to date with the policies and procedures within the industry and flexible to meet the changing demands required with transparency.\n\nPreserved railways often have their own volunteer PTS courses. Network Rail will not accept these PTS cards, and often vice versa.\n\n"}
{"id": "32690302", "url": "https://en.wikipedia.org/wiki?curid=32690302", "title": "Pharmaceutical fraud", "text": "Pharmaceutical fraud\n\nPharmaceutical fraud involves activities that result in false claims to insurers or programs such as Medicare in the United States or equivalent state programs for financial gain to a pharmaceutical company. There are several different schemes used to defraud the health care system which are particular to the pharmaceutical industry. These include: Good Manufacturing Practice (GMP) Violations, Off Label Marketing, Best Price Fraud, CME Fraud, Medicaid Price Reporting, and Manufactured Compound Drugs. Examples of fraud cases include the GlaxoSmithKline $3 billion settlement, Pfizer $2.3 billion settlement, and Merck $650 million settlement. Damages from fraud can be recovered by use of the False Claims Act, most commonly under the \"qui tam\" provisions which rewards an individual for being a \"whistleblower\", or relator (law).\n\nThere are several different schemes used to defraud the health care system which are particular to the pharmaceutical industry.\n\nInvolve fraud with the Good Manufacturing Practice (GMP) Regulations which require manufacturers to have adequately equipped manufacturing facilities, adequately trained personnel, stringent control over the manufacturing process, appropriate laboratory controls, complete and accurate records, reports, appropriate finished product examination, and so on. Certain violations of the Good Manufacturing Practice Regulations may be the basis for a False Claims Act lawsuit.\n\nThough physicians may prescribe drugs for off-label usage known as off-label marketing, the Food and Drug Administration (FDA) prohibits drug manufacturers from marketing or promoting a drug for a use that the FDA has not approved. A manufacturer illegally “misbrands” a drug if the drug’s labeling includes information about its unapproved uses. A drug is deemed misbranded unless its labeling bears adequate directions for use. The courts have agreed with the FDA that the Food, Drug, and Cosmetic Act (FDCA) requires information not only on how a product is to be used (e.g., dosage and administration), but also on all the intended uses of the product. In 2004, whistleblower David Franklin prevailed in a suit under the False Claims Act against Warner-Lambert, resulting in a $430 million settlement in the Franklin v. Parke-Davis case. It was the first off-label promotion case successfully brought under the False Claims Act in U.S. history. Oral statements and materials presented at industry-support scientific and educational activities may provide evidence of a product’s intended use. If these statements or materials promote a use that is inconsistent with the product’s approved labeling, the product is misbranded under the FDCA for failure to bear labeling with adequate directions for all intended uses.\n\nA figure reported by the manufacturer to CMS in quarterly reports under the Medicaid Rebate Program, it is used to calculate the Medicaid reimbursement rate. It is defined as the lowest price available to any wholesaler, retailer, provider, health maintenance organization (HMO), nonprofit entity, or the government. BP excludes prices to the Indian Health Service (IHS), Department of Veterans Affairs (DVA), Department of Defense (DOD), the Public Health Service (PHS), 340B covered entities, Federal Supply Schedule (FSS), state pharmaceutical assistance programs, depot prices, and nominal pricing. BP includes cash discounts and free goods that are contingent upon purchase, volume discounts, and rebates. The fraud occurs as the manufacturer falsely self-reports its Best Price.\n\nIn order to decrease the amounts owed to states under the Medicaid Drug Rebate Program, some companies misrepresented material facts regarding the regulatory origin/status of their brand name drugs, the AMP, and/or the best price. Despite the Government’s good faith reliance to charge manufacturers a unit rebate amount based upon the manufacturer’s own representation of drug status, and price, some manufacturers have deceptively and fraudulently, breached their duty to deal honestly with the Government.\n\nFDA guidelines authorize pharmacists to “compound” or mix medications only in response to a physician’s valid prescription. This assumes, of course, that the physician intends that the medication be compounded. The regulations further require that the mixed or compounded medications are medically necessary and not commercially available. Illegal compounding includes compounding of ingredients such that the compounded drug is tantamount to commercially available medications, involving mass manufacturing of drugs under the guise of compounding.\n\nFederal law, including the Centers for Medicare and Medicaid Services (CMS) guidelines and the regulations of other Government Healthcare Programs, prohibit coverage of claims for “compounded” medications when the claims are submitted by a company that is mass manufacturing large amounts of unapproved drugs in violation of the Federal Food, Drug and Cosmetic Act (FFDCA), under the guise of “compounding.”\n\nKickbacks are rewards such as cash, jewelry, free vacations, corporate sponsored retreats, or other lavish gifts used to entice medical professionals into using specific medical services. This could be a small cash kickback for the use of an MRI when not required, or a lavish doctor/patient retreat that is funded by a pharmaceutical company to entice the prescription and use of a particular drug.\n\nPeople engaging in this type of fraud are also subject to the federal Anti-Kickback statute.\n\n\n\n"}
{"id": "39155261", "url": "https://en.wikipedia.org/wiki?curid=39155261", "title": "Prader scale", "text": "Prader scale\n\nThe Prader scale or Prader staging, named after Dr. Andrea Prader, is a coarse rating system for the measurement of the degree of virilization of the genitalia of the human body and is similar to the Quigley scale. It primarily relates to virilization of the female genitalia in cases of congenital adrenal hyperplasia (CAH) and identifies five distinct stages, but in recent times has been used to describe the range of differentiation of genitalia, with normal infant presentation being shown on either end of the scale, female on the left (0) and male on the right (6).\n\n\nWhile the scale has been defined as a grading system for \"abnormal\" genitalia, the concept that atypical genitals are necessarily abnormal is contested. An opinion paper by the Swiss National Advisory Centre for Biomedical Ethics advises that \"not infrequently\" variations from sex norms may not be pathological or require medical treatment. Similarly, an Australian Senate Committee report on involuntary sterilization determined that research \"regarding 'adequate' or 'normal' genitals, particularly for women, raises some disturbing questions\", including preferences influenced by doctors' specialism and gender.\n\nNumerous clinical scales and measurement systems exist to define genitals as normal male or female, or \"abnormal\", including the orchidometer, Quigley scale and the satirical Phall-O-Meter.\n\n\n"}
{"id": "24767927", "url": "https://en.wikipedia.org/wiki?curid=24767927", "title": "Publicly owned treatment works", "text": "Publicly owned treatment works\n\nA publicly owned treatment works (POTW) is a term used in the United States for a sewage treatment plant that is owned, and usually operated, by a government agency. In the U.S., POTWs are typically owned by local government agencies, and are usually designed to treat domestic sewage and not industrial wastewater.\n\nThe term is used extensively in U.S. water pollution law (i.e. the Clean Water Act), regulations and programs. Many POTWs were established or expanded with grants or low-interest loans from the U.S. Environmental Protection Agency (EPA).\n\nThere are over 16,000 POTWs in the U.S., serving 75 percent of the total population. The remainder of the population is served by decentralized or private septic systems. The POTWs treat of wastewater every day. Most POTWs are required to meet national secondary treatment standards.\n\n"}
{"id": "33103141", "url": "https://en.wikipedia.org/wiki?curid=33103141", "title": "Raffles Medical Group", "text": "Raffles Medical Group\n\nRaffles Medical Group (RMG), (Chinese: 莱佛士医疗集团 : Lái fú shì yīliáo jítuán) is a private healthcare provider in Asia, operating medical facilities in thirteen cities in Singapore, China, Japan, Vietnam and Cambodia. \n\nRMG has a network of clinics with family physicians, specialists and dental surgeons and owns Raffles Hospital, a tertiary care hospital in Singapore, which accommodates surgical centres, medical laboratories and 24 specialist centres in various areas like Obstetrics & Gynaecology, Cardiology, Oncology and Orthopaedics. \n\nRMG’s medical practice is based on the Group Practice Model. \n\nRMG has its own consumer healthcare division which develops and distributes nutraceuticals, supplements, vitamins and medical diagnostic equipment. \n\nRMG is a member of the Mayo Clinic Care Network.\n\nIn 1976, the group's founders, Dr Loo Choon Yong and Dr Alfred Loh, opened their first two clinics in Singapore's Central Business District with the aim of providing medical services to corporate clients. By 1989, this had grown to five clinics and it was then that the two friends decided to incorporate their clinics into a medical practice group. Expanding after its incorporation, RMG moved into Singapore's HDB heartlands with their first neighbourhood clinic in 1993. The first of these areas included Telok Blangah, Bishan, Ang Mo Kio, Siglap, Tampines, Pasir Ris and Bedok.\n\nIn 1990, RMG tendered and obtained a contract with the Civil Aviation Authority of Singapore to provide medical services to the passengers transiting through Changi International Airport as well as airport workers. This also marked RMG's first 24-hour clinic.\n\nPatients of Raffles Medical Clinics that required specialist care were initially referred to the public hospitals or private specialists. In 1991, RMG appointed specialists in its medical staff. The Group consolidated its specialist service in 1993 with the opening of Raffles SurgiCentre at No. 182 Clemenceau Avenue – the first free-standing day surgery centre at Southeast Asia. It had four operating theatres, 28 recovery beds and two beds in intensive care unit.\n\nBy 1996, the network of clinics had grown to 30 branches covering most parts of Singapore. When Raffles SurgiCentre saw a lack of space for further expansion, Dr Loo began looking for a site to build a hospital. They eventually settled on Blanco Court, a commercial building at the intersection of North Bridge Road and Ophir Road. Construction works to convert it into a hospital began in 1999. This culminated in the opening of the 380-bed Raffles Hospital on 31 March 2001. It consists of 24 different specialist centres which provides specialist services such as obstetrics and gynaecology, cardiology, oncology and orthopaedics.\n\nToday, the Group is present in Singapore, China, Vietnam, Cambodia and Japan. The Group runs a network of 106 multi-disciplinary clinics across Singapore and medical centres in Hong Kong, Shanghai and Osaka. Raffles Medical Group also has representative offices in Indonesia, Vietnam, Cambodia, Brunei and Bangladesh, as well as associates throughout the Asia-Pacific region. Airport clinics in Singapore’s Changi International Airport and Hong Kong’s Chek Lap Kok International Airport are also managed by them.\n\n\nThese clinics offer primary healthcare services in various locations in Singapore.\n\n\nRMG operates three medical centres in Hong Kong and medical centres in Shanghai and Osaka.\n\nRMG has its own consumer healthcare division, Raffles Health, which develops and distributes nutraceuticals, supplements, vitamins and medical diagnostic equipment. \n\nThe first ‘Raffles Baby’ was born on 19 July 2001 at 7:20pm, delivered by Consultant Obstetrician and Gynaecologist, Dr Joan Thong Pao-Wen. The healthy baby girl weighed 3250g at birth. Raffles Hospital’s first triplets were delivered a few days before Christmas in 2004.\n\nRaffles Hospital undertook the surgical separation of a pair of adult craniopagus twins, Laleh Bijani and Ladan Bijani of Iran. The surgery was led by Dr Pierre Lasjaunias, a French neuro-radiologist. Separation was achieved. However, both twins died due to significant blood loss in the blood vessel repairing process.\n\nRaffles hospital separated another set of conjoined twins Ji Hye and Sa Rang. They underwent a successful surgery on 22 July 2003. On 16 August, both twins were discharged almost a month after their operation.\n\nAmerican Ryan Boarman was bitten by a shark on his right elbow on 25 April 2016. After spending some time in Balinese hospitals, he was transferred to Singapore’s Raffles Hospital on 29 April 2016, where he went under the knife of orthopaedic surgeon Dr Lim Yeow Wai. The American had suffered a 360-degree laceration around the elbow, with the shark biting, pulling off and shearing away at least eight muscles and tendons and injuring one nerve and one ligament.\n\nRMG started out as a two-clinic practice in 1976 under its founders. In 2009, their revenue grew 8.9% to S$218 million, while profit after tax increased by 20% to S$38 million. The Group’s profits continue to grow through the financial year of 2010, reaching S$311 million in 2012.\n\nRMG is an integrated private healthcare provider in Singapore based on the Group Practice Model. Full-time doctors practice exclusively and adhere to protocols and fee schedules set by the hospital.\n\nRMG's humanitarian arm, Asian Medical Foundation (AMF), is a non-profit organisation was started in 2003 to offer medical expertise in areas with poor access to health care services. AMF sent its first relief mission to Aceh on 26 December to assist in the 2004 Asian tsunami crisis. AMF also sent medical aid to the earthquake victims in Nias, Indonesia and Pakistan in 2005.\n\n"}
{"id": "18926098", "url": "https://en.wikipedia.org/wiki?curid=18926098", "title": "Rejuvelac", "text": "Rejuvelac\n\nRejuvelac is kind of grain water invented and promoted by Ann Wigmore.\n\nRejuvelac is a raw food made by sprouting a grain, soaking the sprouted grain in water for about two days at room temperature, and then reserving the liquid. A second batch can be made from the same sprouts, this time requiring only about one day to ferment. A third batch is possible but the flavor may be disagreeable. The spent sprouted grains are usually discarded.\n\nThe drink is good for individuals who have yeast sensitivities that would flare up upon ingesting kombucha, another fermented product. \n"}
{"id": "47221694", "url": "https://en.wikipedia.org/wiki?curid=47221694", "title": "Scottish Patient Safety Programme", "text": "Scottish Patient Safety Programme\n\nThe Scottish Patient Safety Programme (SPSP) is national initiative to improve the reliability of healthcare and reduce the different types of harm that can be associated. The programme is co-ordinated by Healthcare Improvement Scotland and is the first example of a country introducing a national patient safety programme across the whole healthcare system. From an initial focus on acute hospitals, the SPSP now includes safety improvment programmes including SPSP Primary care, SPSP Medicines, Maternity and Children Quality Improvement Collaborative (MCQIC) and Mental Health. \n\nThe programme was launched in January 2008. The first stage had a focus on activities in acute hospitals in Scotland to reduce mortality and adverse events by the end of 2012. Shortly after the programme began, there were improvements reported in several areas of care. This included reductions in the number of cases of bloodstream infections associated with central lines, ventilator-acquired pneumonia and the length of time patients were staying in intensive care. As the end of the first phase of the programme was reached in 2012, it was clear that good progress had been made towards the overall aim of reducing mortality by 15 per cent and adverse events by 30 per cent. By March 2015, the programme was running in GP surgeries, hospitals, mental health and maternity services.\n\nNHS Greater Glasgow and Clyde is pioneering the mental health arm of the Scottish Patient Safety Programme. The SPSP Mental Health is working with Scottish Government and Partners to deliver the \"Mental Health Strategy: 2017- 2027\". \n\nThe Maternity and Children Quality Improvement Collaborative brings together SPSP's Maternity, Neonatal and Paediatric care communities. \n\nIn community settings there was a focus on three main workstreams: leadership and culture; safer use of medicines; safe and effective patient care across the interface. £450,000 of funding was put towards work to reduce prescribing errors, through better communication between general practitioners and community pharmacists.\n"}
{"id": "39174052", "url": "https://en.wikipedia.org/wiki?curid=39174052", "title": "Social Progress Index", "text": "Social Progress Index\n\nThe Social Progress Index (SPI) measures the extent to which countries provide for the social and environmental needs of their citizens. Fifty-four indicators in the areas of basic human needs, foundations of well-being, and opportunity to progress show the relative performance of nations. The index is published by the nonprofit Social Progress Imperative, and is based on the writings of Amartya Sen, Douglass North, and Joseph Stiglitz. The SPI measures the well-being of a society by observing social and environmental outcomes directly rather than the economic factors. The social and environmental factors include wellness (including health, shelter and sanitation), equality, inclusion, sustainability and personal freedom and safety.\n\nThe index combines three dimensions\n\nEach dimension includes four components, which are each composed of between three and five specific outcome indicators. The included indicators are selected because they are measured appropriately, with a consistent methodology, by the same organization across all (or essentially all) of the countries in the sample. Together, this framework aims to capture a broad range of interrelated factors revealed by the scholarly literature and practitioner experience as underpinning social progress. \n\nTwo key features of the \"Social Progress Index\" are:\n\nSocial Progress Imperative evaluated hundreds of possible indicators while developing the \"Social Progress Index\", including engaging researchers at MIT to determine what indicators best differentiated the performance of nations. The index uses outcome measures when there are sufficient data available or the closest possible proxies.\n\nIn 2010, a group of global leaders from the social sector sought to develop a better measure of a country's level of development and, by extension, better understand its development priorities. Funded by private foundations and under the technical guidance of Professors Michael Porter from Harvard Business School and Scott Stern from the Massachusetts Institute of Technology, the group formed Social Progress Imperative and launched a beta version of the \"Social Progress Index\" for 50 countries in 2013 to measure a comprehensive array of components of social and environmental performance and aggregate them into an overall framework.\n\nThis work was influenced by the contributions of Amartya Sen on social development, as well as by the recent call for action in the report \"Mismeasuring Our Lives\" by the Commission on the Measurement of Economic Performance and Social Progress. The \"Social Progress Index\" was released in 2014 for 133 countries with a second version in 2015.\n\nOn 11 July 2013, Social Progress Imperative's chairman and professor at Harvard Business School, Michael Porter, addressed the United Nations 6th Ministerial Forum for Development and discussed the Social Progress Index.\n\nIn addition to the global \"Social Progress Index\", the methodology used to create it has been adapted to measure social and environmental performance in smaller areas, such as the Amazon region of Brazil. Other projects include a \"Social Progress Index\" for the Municipality of Guatemala City. Fundacion Paraguaya has integrated elements of the \"Social Progress Index\" into its Poverty Stoplight tool. The national government of Paraguay is setting a target for \"Social Progress Index\" performance alongside GDP targets.\n\n\"The Guardian\" reported that the European Commission had agreed to partner with Social Progress Imperative to create a social progress index for the European Union.The EU Social Progress Index was published in October, 2016.\n\nA similar index, although with some differences compared to the nation list (and therefore not directly comparable), has been published for the individual U.S. states.\n\nColor key:\n\nColor key:\n\nFrom an econometric stand point, the Index appears to be similar to other efforts aimed at overcoming the limitation of traditional economic measure such as the gross domestic product (GDP). One major criticism is that although the Social Progress Index can be seen as a superset of indicators used by earlier econometric models such as Gross National Well-being Index 2005, Bhutan Gross National Happiness Index of 2012, and World Happiness Report of 2012, Yet, unlike them, it ignores measures of subjective life satisfaction and psychological well-being. Other critics point out that \"there remain certain dimensions that are currently not included in the SPI. These are the concentration of wealth in the top 1 percent of the population, efficiency of the judicial system, and quality of the transportation infrastructure.\"\n\nSome critics argue that \"we must be wary. Though words such as “inclusive capitalism” are bandied around increasingly these days to signal a new age, free from ideological battlegrounds between public and private, much of what the organization’s founders say about it confirms that the index is more about being “business inclusive” than “inclusive capitalism.”\n\n"}
{"id": "52596904", "url": "https://en.wikipedia.org/wiki?curid=52596904", "title": "The Canton Hospital", "text": "The Canton Hospital\n\nThe Canton Hospital (廣州博濟醫院) or Ophthalmic Hospital in Canton, also known as the Canton Pok Tsai Hospital, was founded by Protestant medical missionary Peter Parker (1804-1888) in Canton, China on November 4, 1835. The hospital treated thousands of patients in need, became the center for the Medical Missionary Society in China, and still exists today as one of the most prestigious ophthalmic institutes in the world.\n\nCanton, now Guangzhou, was the center of foreign and international trade in China during the period of the late Qing dynasty. Canton was the only city in China where foreigners were allowed to set foot, thereby making it the only place where European and Chinese merchants could trade. Its location next to the Pearl River also made it ideal for international trade, as all ships used for trade were forced to travel along this river in order to arrive at the Port of Canton. Additionally, it was a major stop along the Silk Road, and is still a major port and city for transportation today. Canton was also surrounded by a major wall, which isolated it from the rest of the general population. Overall, because Canton was the only city in China that was open to foreign trade, medical mission work was made possible here and spread more easily.\nPeter Parker was an American Protestant medical missionary. Before travelling to China, he attended Yale University for postgraduate work in theology and medicine. However, his main goal in travelling to China was not to practice medicine; instead, it was to save the Chinese from idol-worshipping and introduce them to Protestantism and Christianity. He first began work in the hospital by treating diseases in the eye because he was told it would be the fastest way to gain the trust of the Chinese. Because many of the natives regarded all foreigners as barbarians, it was important for Parker to gain the natives' trust in order to perform medicine on them and introduce them to Christianity. \n\nEventually, Parker found so much joy in treating patients that he ultimately committed himself to becoming a full-time doctor, instead of a part-time evangelist and part-time doctor like he had originally planned. During his twenty years of work in China, Parker treated over 50,000 patients. In the later years of his life, he became a fervent promoter of medical missions and played an instrumental role in increasing their importance and popularity.\n\nOn November 4, 1835, Peter Parker opened the Canton Hospital, which was the first Western-style hospital in China. Parker, the first full-time Protestant medical missionary, opened the hospital in connection with the mission of the American Board after Dr. Thomas Richardson Colledge, a Christian surgeon of the East India Company, convinced existing Protestant mission societies of the need for a hospital in China. Colledge strongly believed that Christians were required to help the sick in China, and as a result, pushed Parker, his mentee, to open a hospital in Canton.\n\nUnder the support of the American Board and Canton businessmen, the hospital was the first and most famous charitable missionary hospital in South China at the time. When Parker opened the hospital, it was first only intended for the treatment of eye illnesses and was opened as an “Eye Infirmary.” This helped Parker gain trust with the Chinese, but it was also practical because of patients' needs at the time. In many areas, Chinese medicine was not extremely far behind western medicine, but because they were not as advanced in eye medicine, there was a high prevalence of eye diseases during this time. Throughout the first three months of the hospital being open, 1061 patients were treated, and 96.1% of those had ocular illnesses. The hospital quickly proved to be very successful, and after the first year, 2,910 patients were treated. Soon after the hospital opened, Parker was asked to treat ear illnesses in addition to eye illnesses. This led to him finding a number of patients with tumors, and he was forced to perform surgery on these patients, which were almost always successful. As soon as others heard of Parker’s success, patients with all different types of illnesses and diseases wanted to come to the hospital for treatment. As a result, Parker soon found it impractical for the hospital to only treat eye illnesses, and it then opened up to all different types of diseases.\n\nEventually, the hospital became so well known and in demand that it had to turn away patients because it became too much for only one physician. The hospital’s Chinese name, Pok Tsai, means Universal Helpfulness, which shows that everyone knew that the hospital would serve all classes in the community, Chinese and foreign. In order to stay open, Parker depended on support from missionary colleagues and local business firms and merchants (especially Chinese merchant How-Qua). Because he didn’t charge for his services, he relied on their money to stay open. In addition, this hospital also led to Western-style medical education in China, when Parker and Dr. E.C. Bridgman trained three young Chinese men to help out in the hospital. However, it was not until 1866 that the first western-style medical school, the Boji Medical School, was established in the hospital. This medical school was run by missionaries, and is now part of the Sun-yatsen University of Medical Sciences.\n\nIn 1840, the First Opium War led to hostilities between England and China. During the war, the port of Canton was blockaded and all foreigners were forced to leave, causing the Canton Hospital to be temporarily shut down. Near the end of the war, China was forced to accept treaties that required them to open their borders to foreigners. As a result, missionaries were now allowed to do work in all of China (not only in Canton like before the war). In 1842, Parker returned to Canton with his wife, Harriet Webster (who was the first Western woman to be granted residence in China), and reopened the hospital. It remained under the control of the Medical Missionary Society until 1930, when it became part of Lingnan University.\n\nThe hospital’s main goals included the diagnosis and treatment of diseases, the distribution of free vaccines, and plague treatments. Because the hospital was built by Parker in conjunction with other Protestant medical missionaries, philanthropic-minded physicians were very prominent at the hospital. As a result, it was also involved in areas outside of the treatment of diseases, including medical education, research, social service work, and promoting public health to the surrounding community. It cooperated with the Chinese government and many social organizations to develop school health, maternal and child health, and communicable disease control. The hospital was also built on a three-level medical system, which provided a model for the establishment of the Chinese rural medical system.\n\nThe Canton Hospital is one of the most influential missionary hospitals in South China. Not only was it the first hospital that brought the concept of public health to the city of Canton, but it is also sustainable, as it still exists today as one of the most prestigious ophthalmic institutes in the world. Throughout its time open, the hospital set many records in Chinese medical history, including being the first western medicine school in China, and producing the first Chinese medicine magazine and the first x-ray film. It also produced the hospital's best student, Sun Yat Sen, who became the first president of China. \nIn addition, the Canton Hospital gave explicit expression to the concept of medical missions for the first time. In 1838, it led to the creation of the Medical Missionary Society in China, a Protestant medical missionary society established in Canton, which was dedicated to promoting religious missions and gaining the trust of the Chinese through medical care, instead of through preaching. In 1898, Parker’s successor, John Glasgow Kerr, founded The Asylum for the Insane in Canton, which was the first institution in China dedicated to the mentally ill. This was a direct result of the hospital’s success, and shows the impact that it had on public health and medicine in China.\n\nToday it is the Second Affiliated Hospital of Sun Yat-sen University. It is a tertiary referral hospital.\n\n\n"}
{"id": "12271465", "url": "https://en.wikipedia.org/wiki?curid=12271465", "title": "Togolese Red Cross", "text": "Togolese Red Cross\n\nTogolese Red Cross was established in 1959. It has its headquarters in Lomé.\n\n"}
{"id": "174247", "url": "https://en.wikipedia.org/wiki?curid=174247", "title": "Traceability", "text": "Traceability\n\nTraceability is the capability to trace something. In some cases, it is interpreted as the ability to verify the history, location, or application of an item by means of documented recorded identification.\n\nOther common definitions include the capability (and implementation) of keeping track of a given set or type of information to a given degree, or the ability to chronologically interrelate uniquely identifiable entities in a way that is verifiable.\n\nTraceability is applicable to measurement, supply chain, software development, healthcare and security.\n\nThe term \"measurement traceability\" is used to refer to an unbroken chain of comparisons relating an instrument's measurements to a known standard. Calibration to a traceable standard can be used to determine an instrument's bias, precision, and accuracy. It may also be used to show a chain of custody - from current interpretation of evidence to the actual evidence in a legal context, or history of handling of any information.\n\nIn many countries, national standards for weights and measures are maintained by a National Measurement Institute (NMI) which provides the highest level of standards for the calibration / measurement traceability infrastructure in that country. Examples of government agencies include the National Physical Laboratory, UK (NPL) the National Institute of Standards and Technology (NIST) in the USA, the Physikalisch-Technische Bundesanstalt (PTB) in Germany, and the Istituto Nazionale di Ricerca Metrologica (INRiM) in Italy. As defined by NIST, \"Traceability of measurement requires the establishment of an unbroken chain of comparisons to stated references each with a stated uncertainty.\"\n\nA clock providing is traceable to a time standard such as Coordinated Universal Time or International Atomic Time. The Global Positioning System is a source a traceable time.\n\nIn the supply chain, traceability may be both a regulatory and an ethical or environmental issue. Environmentally friendly retailers may choose to make information regarding their supply chain freely available to customers, illustrating the fact that the products they sell are manufactured in factories with safe working conditions, by workers that earn a fair wage, using methods that do not damage the environment.\n\nIn regard to materials, traceability refers to the capability to associate a finished part with destructive test results performed on material from the same ingot with the same heat treatment, or to associate a finished part with results of a test performed on a sample from the same melt identified by the unique lot number of the material. Destructive tests typically include chemical composition and mechanical strength tests. A heat number is usually marked on the part or raw material which identifies the ingot it came from, and a lot number may identify the group of parts that experienced the same heat treatment (i.e., were in the same oven at the same time). Material traceability is important to the aerospace, nuclear, and process industry because they frequently make use of high strength materials that look identical to commercial low strength versions. In these industries, a part made of the wrong material is called \"counterfeit,\" even if the substitution was accidental.\n\nThis same practice extends throughout industries using military hardware, including the fastener industry.\n\nIn logistics, traceability refers to the capability for tracing goods along the distribution chain on a batch number or series number basis. Traceability is an important aspect for example in the automotive industry, where it makes recalls possible, or in the food industry where it contributes to food safety.\n\nThe international standards organization EPCglobal under GS1 has ratified the EPCglobal Network standards (especially the EPC Information Services EPCIS standard) which codify the syntax and semantics for supply chain events and the secure method for selectively sharing supply chain events with trading partners. These standards for traceability have been used in successful deployments in many industries and there are now a wide range of products that are certified as being compatible with these standards.\n\nIn food processing (meat processing, fresh produce processing), the term traceability refers to the recording through means of barcodes or RFID tags & other tracking media, all movement of product and steps within the production process. One of the key reasons this is such a critical point is in instances where an issue of contamination arises, and a recall is required. Where traceability has been closely adhered to, it is possible to identify, by precise date/time & exact location which goods must be recalled, and which are safe, potentially saving millions of dollars in the recall process. Traceability within the food processing industry is also utilised to identify key high production & quality areas of a business, versus those of low return, and where points in the production process may be improved.\n\nIn \"food processing software\", traceability systems imply the use of a unique piece of data (e.g., order date/time or a serialized sequence number, generally through the use of a barcode / RFID) which can be traced through the entire production flow, linking all sections of the business, including suppliers & future sales through the supply chain. Messages and files at any point in the system can then be audited for correctness and completeness, using the traceability software to find the particular transaction and/or product within the supply chain.\n\nThe European Union's General Food Law came into force in 2002, making traceability compulsory for food and feed operators and requiring those businesses to implement traceability systems. The EU introduced its Trade Control and Expert System, or TRACES, in April 2004. The system provides a central database to track movement of animals within the EU and from third countries. Australia has its National Livestock Identification System to keep track of livestock from birth to slaughterhouse.\n\nIndia has started taking initiatives for setting up traceability systems at Government and Corporate levels. Grapenet, an initiative by Agriculture and Processed Food Products Export Development Authority (APEDA), Ministry of Commerce, Government of India is an example in this direction. GrapeNet is an internet based traceability software system for monitoring fresh grapes exported from India to the European Union. GrapeNet is a first of its kind initiative in India that has put in place an end-to-end system for monitoring pesticide residue, achieve product standardization and facilitate tracing back from pallets to the farm of the Indian grower, through the various stages of sampling, testing, certification and packing. Grapenet won the National Award (Gold), in the winners announced for the best e-Governance initiatives undertaken in India in 2007. Grapenet was designed and developed by Logicsoft, award-winning traceability solutions company, based in New Delhi, India.\n\nThe Directorate Generate Foreign Trade (DGFT), Government of India, through its notification \n\nUruguay has also designed a system called \"Traceability & Electronic Information System of the Beef Industry\".\n\nWithin the context of supporting legal and sustainable forest supply chains, traceability has emerged in the last decade as a new tool to verify claims and assure buyers about the source of their materials. Mostly led out of Europe, and targeting countries where illegal logging has been a key problem (FLEGT countries), timber tracking is now part of daily business for many enterprises and jurisdictions. Full traceability offers advantages for multiple partners along the supply chain beyond certification systems, including:\n\n\nA number of timber tracking companies are in operation to service global demand.\n\nEnhanced traceability ensures that the supply chain data is 100% accurate from the forest to the point of export. Nowadays, there are techniques to predict geographical provenance of wood and contribute to the fight against illegal logging .\n\nIn systems and software development, the term traceability (or Requirements Traceability) refers to the ability to link product requirements back to stakeholders' rationales and forward to corresponding design artifacts, code, and test cases. Traceability supports numerous software engineering activities such as change impact analysis, compliance verification or traceback of code, regression test selection, and requirements validation. It is usually accomplished in the form of a matrix created for the verification and validation of the project. Unfortunately, the practice of constructing and maintaining a requirements trace matrix (RTM) can be very arduous and over time the traces tend to erode into an inaccurate state unless date/time stamped. Alternate automated approaches for generating traces using information retrieval methods have been developed.\n\nIn transaction processing software, traceability implies use of a unique piece of data (e.g., order date/time or a serialized sequence number) which can be traced through the entire software flow of all relevant application programs. Messages and files at any point in the system can then be audited for correctness and completeness, using the traceability key to find the particular transaction. This is also sometimes referred to as the transaction footprint.\n\nPatient safety during healthcare service plays an important role in preventing delayed recovery or even mortality, by increasing and improving the quality of life of citizens, and is considered an indicator of the quality status of health services Maintaining patient safety is a complex task and involves factors inherent to the environment and human actions. New technologies facilitate the traceability tools of patients and medications. This is particularly relevant for drugs that are considered high risk and cost.\n\nThe World Health Organization has recognized the importance of traceability for Medical Products of Human Origin (MPHO) and urged member states \"to encourage the implementation of globally consistent coding systems to facilitate national and international traceability\".\n\nTo prevent theft, and assist in locating stolen objects, goods may be marked indelibly or undetectably so that they may be determined to be stolen, and in some cases identified. For example, it is sometimes arranged that stolen banknotes are marked with indelible dye to show that they are stolen; they can be identified by their unique serial numbers. Announcing that cash machines were fitted with sprayers of SmartWater, an invisible gel detectable for years, to mark thieves and their clothing when breaking into or tampering with the machine was found in a 2016 pilot scheme to reduce theft by 90%.\n\n"}
{"id": "3575924", "url": "https://en.wikipedia.org/wiki?curid=3575924", "title": "Traditional Mongolian medicine", "text": "Traditional Mongolian medicine\n\nTraditional Mongolian medicine developed over many years among the Mongolian people. Mongolian medical practice spread across their empire and became an ingrained part of many other people's medical systems.\n\nThe Mongols were part of a wider network of Eurasian people who had developed a medical system of their own, including the Chinese, Korean, Tibetan, Indian, Uighur, Islamic, and Nestorian Christians. They took the medical knowledge of these people, adapted it to develop their own medical system and at the same time organized an exchange of knowledge between the different people in their empire. On their journeys throughout Asia, the Mongols brought with them a team of doctors. Usually foreign, these doctors themselves had brought medical knowledge from other people in Asia to the Mongol court. They serve three purposes on the journeys on which the accompanied Mongol princes. Their first purpose was to be the personal physicians of the princes in case they required medical attention. The second was to observe and obtain any new medical knowledge from the various groups of people that they encounter. Finally, they were to also spread the medical knowledge that the Mongols had put together to the peoples they encountered. The Mongols were also able to contribute new or more advanced knowledge on topics such as bone setting and treatments of war wounds because of their nomadic lifestyle. The Mongols were the first people to establish a link between diet and health.\n\nTraditional Mongolian doctors were known as shaman, or holy men. They relied on magic and spiritual powers to cure illness. They were called on to determine whether the illness was caused by natural means or because of malicious wishes. Though they were often used as healers, their main strength was in prophecy readings. Foreign physicians who used herbs to treat illness were distinguished from the shamans by their name, otochi, which meant herb user or physician. It was borrowed from the Uighur word for physician, which was otachi. When Mongolian medicine began to transition to using herbs and other drugs and had the service of foreign doctors, the importance of shamans as medical healers began to decline.\n\nHu Sihui (1314–1330) was a Mongol court therapist and dietitian during Mongol Yuan Dynasty reign in China. He is known for his book \"Yinshan Zhengyao\" (\"Important Principles of Food and Drink\"), that became a classic in Chinese medicine and Chinese cuisine. He was the first to empirically discover and clearly describe deficiency diseases.\n\nAnimal blood was used to treat a variety of illness, from gout to blood loss. Recorded in the Yuan Shih, are many incident where the blood of a freshly killed animal, usually a cow or an ox, was used to treat illness. Gout, which was a common affliction of the Mongol people, was treated by immersing the afflicted body part into the belly of a freshly killed cow. Placing a person in the stomach of an animal was also used as a method of blood transfusion. On the battlefield, when a soldier became unconscious due to massive amount of blood loss, he would be stripped and placed into the stomach of a freshly killed animal until he became conscious again. In less severe cases, the skin of a freshly killed ox was combined with the masticated grass found in a cow's stomach to form a sort of bandage and ointment to heal battle wounds. It was believed that the stomach and fat of the freshly killed animal could absorb the bad blood and restore the wounded to health.\n\nMongolian medical literature mentions the use of minerals in medicine, usually in the form of powdered metals or stones. From the Chinese, Mongolians also used cinnabar or mercury sulfide as treatment options, despite the high number of casualties it caused. Both the Chinese and the Mongols believed that cinnabar and mercury sulfide were the elixir of life.\n\nHerbs were the mainstay of Mongolian medicine; legend had it that any plant could be used as a medicine. An \"emchi\" is quoted as saying:\n\nThe Mongolian adopted the practice of acupuncture from the Chinese. They adapted this tradition and made it a Mongolian form of treatment when they burned herbs over the various meridian points rather than used a needle. The tradition of Moxibustion (burning mugwort over acupuncture points) was developed in Mongolia and later incorporated into Tibetan medicine.\n\nOne unusual aspect of Mongolian medicine is the use of water as a medicine. Water was collected from any source, including the sea, and stored for many years until ready for use. Acidity and other stomach upsets were said to be amenable to water treatments.\n\nBone setting is a branch of Mongolian medicine carried out by \"Bariachis\", specialist bone setters. They work without medicines, as anesthetics or instruments. Instead they rely on physiotherapy to manipulate bones back to their proper position. This was done without any pain to the patient. \"Bariachis\" are laypeople, without medical training, and are born into the job, following the family tradition. They had the ability to fix any bone problem, no matter how severe or difficult. When Chinese physicians were brought into the Mongolian empire, Wei Yilin, a famous Yuan orthopedic surgeon established particular methods for setting fractures and treating shoulder, hip, and knee dislocations. He also pioneered the suspension method for joint reduction. He was not only an orthopedic surgeon but also an anesthesiologist who used various folk medicine for anesthetics during his operations. It appears that this traditional practice is in decline, and that no scientific research has been carried out into it.\n\nPulse diagnosis is very popular in Western Asia and especially Iran, and its introduction to the Islamic West can be traced back to the Mongols. The Mongol word for pulse, mai, has Chinese etymology. In China, pulse diagnosis was related to the balance between the yin and yang. Irregular pulses were believed to be caused by an imbalance of the yin and the yang. However, when the Mongol adopted this medical practice, they believed that the pulse was directly related to moral order and that when the moral order was chaotic, so the pulse would be chaotic and irregular as well. This belief is highlighted in a story recounted in the Yuan Shih. In 1214, Ogodei Qa’an had an irregular pulse, and was very ill. His most trusted physician ordered that a general amnesty be declared all across the empire. Shortly afterwards, Ogodei Qa’an was restored to health and his pulse regular once again. For the Mongol, this account gives evidence to the direct relationship between pulse and moral order. Pulse diagnosis soon became the primary diagnosis’ tool and became the cornerstone of Mongolian medicine. Qubilai decreed that Chinese manuals on pulse-based medicine be translated to Mongolian. His successor, Temür, in 1305, ordered that pulse diagnosis be one of the ten compulsory subjects in which Imperial Academy of Medicine medical students be tested. In pulse diagnosis, there was a distinction between measuring a child's pulse versus and adult's pulse, and this distinction was greatly emphasized in the Chinese texts that were translated, and later in the Mongolian texts.\n\nIn 1330, Hu Sihui, a Mongolian physician published Yinshan Zhengyo (Important Principles of Food and Drink). It was the first book of its kind. In this textbook, Hu Sihui preached the importance of a balanced diet with a focus on moderation, especially in drinking. He also listed beneficial properties of various common foods, including fish, shellfish, meat, fruit, vegetables, and 230 cereals. Grapes were recommended for character strengthening and boosting one's energy levels. However, eating too many apples could cause distension and indulging in too many oranges lead to liver damage. A common menu item, dog meat, was very beneficial because it calmed the liver, spleen, heart, lungs, kidneys, and pericardium. This link between diet and health was spread far and wide by the Mongols on their journeys across the Eurasian steppe lands.\n\nDom is the tradition of household cures, many based simply on superstition - one instance being that a picture of a fox hung over a child's bed will help it sleep. Counting the frequency of breathing is also stated to be a relief for psychological problems and distress.\n\nA printing stock found in eastern Mongolia in the 1920s documents a historical custom of eating a piece of paper with words printed on it, in order to prevent or heal maladies. On fields of about 24x29 mm magical incantations in Tibetan are printed, along with use instructions in Mongolian. The practise apparently was part of lamaist popular medicine.\n\nToday Mongolia is one of the few countries which officially supports its traditional system of medicine.\n\nSince 1949, the Chinese government has steadily promoted advances in Mongolian medical care, research and education. In 1958 the Department of Traditional Chinese and Mongolian Medicine at the Inner Mongolia Medical College opened its doors to students. In 2007 it expanded, opening a state of the art campus just outside Hohhot City. The Chinese government has also established scores of Mongolian medicine hospitals since 1999, including 41 in Inner Mongolia, 3 in Xinjiang, and 1 each in Liaoning, Heilongjiang, Gansu and Qinghai.\n\n\n"}
{"id": "9080596", "url": "https://en.wikipedia.org/wiki?curid=9080596", "title": "Train to End Stroke", "text": "Train to End Stroke\n\nTrain To End Stroke is an endurance training and fund-raising program, benefiting the American Stroke Association, a division of the American Heart Association, in which participants train to run or walk a full or half marathon. \n\n\n"}
{"id": "414259", "url": "https://en.wikipedia.org/wiki?curid=414259", "title": "Trichomonas vaginalis", "text": "Trichomonas vaginalis\n\nTrichomonas vaginalis is an anaerobic, flagellated protozoan parasite and the causative agent of trichomoniasis. It is the most common pathogenic protozoan infection of humans in industrialized countries. Infection rates between men and women are similar with women being symptomatic, while infections in men are usually asymptomatic. Transmission usually occurs via direct, skin-to-skin contact with an infected individual, most often through vaginal intercourse. The WHO has estimated that 160 million cases of infection are acquired annually worldwide. The estimates for North America alone are between 5 and 8 million new infections each year, with an estimated rate of asymptomatic cases as high as 50%. Usually treatment consists of metronidazole and tinidazole.\n\nAlfred Francois Donné (1801–1878) was the first to describe a procedure to diagnose trichomoniasis through \"the microscopic observation of motile protozoa in vaginal or cervical secretions\" in 1836. He published this in the article entitled, \"Animalcules observés dans les matières purulentes et le produit des sécrétions des organes génitaux de l'homme et de la femme\" in the journal, \"Comptes rendus de l'Académie des sciences\". As a result, the official binomial name of the parasite is \"Trichomonas vaginalis\" D.\n\n\"Trichomonas vaginalis\", a parasitic protozoan, is the etiologic agent of trichomoniasis, and is a sexually transmitted infection. More than 160 million people worldwide are annually infected by this protozoan.\n\n Trichomoniasis, a sexually transmitted infection of the urogenital tract, is a common cause of vaginitis in women, while men with this infection can display symptoms of urethritis. 'Frothy', greenish vaginal discharge with a 'musty' malodorous smell is characteristic.\n\nOnly 2% of women with the infection will have a \"strawberry\" cervix (\"colpitis macularis\", an erythematous cervix with pinpoint areas of exudation) or vagina on examination. This is due to capillary dilation as a result of the inflammatory response.\n\nSome of the complications of \"T. vaginalis\" in women include: preterm delivery, low birth weight, and increased mortality as well as predisposing to HIV infection, AIDS, and cervical cancer. \"T. vaginalis\" has also been reported in the urinary tract, fallopian tubes, and pelvis and can cause pneumonia, bronchitis, and oral lesions. Condoms are effective at reducing, but not wholly preventing, transmission.\n\n\"Trichomonas vaginalis\" infection in males has been found to cause asymptomatic urethritis and prostatitis. It has been proposed that it may increase the risk of prostate cancer; however, evidence is insufficient to support this association as of 2014.\n\nClassically, with a cervical smear, infected women have a transparent \"halo\" around their superficial cell nucleus. It is unreliably detected by studying a genital discharge or with a cervical smear because of their low sensitivity. \"T. vaginalis\" was traditionally diagnosed via a wet mount, in which \"corkscrew\" motility was observed. Currently, the most common method of diagnosis is via overnight culture, with a sensitivity range of 75–95%. Newer methods, such as rapid antigen testing and transcription-mediated amplification, have even greater sensitivity, but are not in widespread use. The presence of \"T. vaginalis\" can also be diagnosed by PCR, using primers specific for GENBANK/L23861.\n\nInfection is treated and cured with metronidazole or tinidazole. The CDC recommends a one time dose of 2 grams of either metronidazole or tinidazole as the first-line treatment; the alternative treatment recommended is 500 milligrams of metronidazole, twice daily, for seven days if there is failure of the single-dose regimen. Medication should be prescribed to any sexual partner(s) as well because they may be asymptomatic carriers.\n\nUnlike other parasitic protozoa (\"Giardia lamblia\", \"Entamoeba histolytica\" etc.), \"Trichomonas vaginalis\" exists in only one morphological stage, a trophozoite, and cannot encyst.\nThe \"T. vaginalis\" trophozoite is oval as well as flagellated, or \"pear\" shaped as seen on a wet-mount. It is slightly larger than a white blood cell, measuring 9 × 7 μm. Five flagella arise near the cytostome; four of these immediately extend outside the cell together, while the fifth flagellum wraps backwards along the surface of the organism. The functionality of the fifth flagellum is not known. In addition, a conspicuous barb-like axostyle projects opposite the four-flagella bundle. The axostyle may be used for attachment to surfaces and may also cause the tissue damage seen in trichomoniasis infections.\n\nWhile \"T. vaginalis\" does not have a cyst form, organisms can survive for up to 24 hours in urine, semen, or even water samples.\n\n\"Trichomonas vaginalis\" lacks mitochondria and therefore necessary enzymes and cytochromes to conduct oxidative phosphorylation. \" T. vaginalis\" obtains nutrients by transport through the cell membrane and by phagocytosis. The organism is able to maintain energy requirements by the use of a small amount of enzymes to provide energy via glycolysis of glucose to glycerol and succinate in the cytoplasm, followed by further conversion of pyruvate and malate to hydrogen and acetate in an organelle called the hydrogenosome.\n\nOne of the hallmark features of \"Trichomonas vaginalis\" is the adherence factors that allow cervicovaginal epithelium colonization in women. The adherence that this organism illustrates is specific to vaginal epithelial cells (VECs) being pH, time and temperature dependent. A variety of virulence factors mediate this process some of which are the microtubules, microfilaments, adhesins (4), and cysteine proteinases. The adhesins are four trichomonad enzymes called AP65, AP51, AP33, and AP23 that mediate the interaction of the parasite to the receptor molecules on VECs. Cysteine proteinases may be another virulence factor because not only do these 30 kDa proteins bind to host cell surfaces but also may degrade extracellular matrix proteins like hemoglobin, fibronectin or collagen IV.\n\nThe \"T. vaginalis\" genome was found to be approximately 160 megabases in size – ten times larger than predicted from earlier gel-based chromosome sizing. (The human genome is ~3.5 gigabases by comparison.) As much as two-thirds of the \"T. vaginalis\" sequence consists of repetitive and transposable elements, reflecting a massive, evolutionarily recent expansion of the genome. The total number of predicted protein-coding genes is ~98,000, which includes ~38,000 'repeat' genes (virus-like, transposon-like, retrotransposon-like, and unclassified repeats, all with high copy number and low polymorphism). Approximately 26,000 of the protein-coding genes have been classed as 'evidence-supported' (similar either to known proteins, or to ESTs), while the remainder have no known function. These extraordinary genome statistics are likely to change downward as the genome sequence, currently very fragmented due to the difficulty of ordering repetitive DNA, is assembled into chromosomes, and as more transcription data (ESTs, microarrays) accumulate. But it appears that the gene number of the single-celled parasite \"T. vaginalis\" is, at minimum, on par with that of its host \"H. sapiens\".\n\nIn late 2007 TrichDB.org was launched as a free, public genomic data repository and retrieval service devoted to genome-scale trichomonad data. The site currently contains all of the \"T. vaginalis\" sequence project data, several EST libraries, and tools for data mining and display. TrichDB is part of the NIH/NIAID-funded EupathDB functional genomics database project.\n\nRecent studies into the genetic diversity of \"T.vaginalis\" has shown that there are two distinct lineages of the parasite found worldwide; both lineages are represented evenly in field isolates. The two lineages differ in whether or not \"T.vaginalis\" virus (TVV) infection is present. TVV infection in \"T.vaginalis\" is clinically relevant in that, when present, TVV has an effect on parasite resistance to metronidazole, a first line drug treatment for human trichomoniasis.\n\nThe damage caused by \"Trichomonas vaginalis\" to the vaginal epithelium increases a woman's susceptibility to an HIV infection. In addition to inflammation, the parasite also causes lysis of epithelial cells and RBCs in the area leading to more inflammation and disruption of the protective barrier usually provided by the epithelium. Having \"Trichomonas vaginalis\" also may increase the chances of the infected woman transmitting HIV to her sexual partner(s).\n\nThe biology of \"T. vaginalis\" has implications for understanding the origin of sexual reproduction in eukaryotes. \"T. vaginalis\" is not known to undergo meiosis, a key stage of the eukaryotic sexual cycle. However, when Malik et al. examined \"T. vaginalis\" for the presence of 29 genes known to function in meiosis, they found 27 such genes, including eight of nine genes that are specific to meiosis in model organisms. These findings suggest that the capability for meiosis, and hence sexual reproduction, was present in recent ancestors of \"T. vaginalis\". Twenty-one of the 27 meiosis genes were also found in another parasite \"Giardia lamblia\" (also called \"Giardia intestinalis\"), indicating that these meiotic genes were present in a common ancestor of \"T. vaginalis\" and \"G. intestinalis\". Since these two species are descendants of lineages that are highly divergent among eukaryotes, Malik et al. noted that these meiotic genes were likely present in a common ancestor of all eukaryotes.\n\n\n"}
