{"id": "15037222", "url": "https://en.wikipedia.org/wiki?curid=15037222", "title": "Adolescent health", "text": "Adolescent health\n\nAdolescent health, or youth health, is the range of approaches to preventing, detecting or treating young people’s health and well being . \n\nThe term adolescent and young people are often used interchangeably, as are the terms Adolescent Health and Youth Health. Young people's health is often complex and requires a comprehensive, biopsychosocial approach.\n\nSome young people engage in risky behaviours that affect their health and therefore the majority of health problems are psychosocial. Many young people experience multiple problems. These behaviours are established as a young person and go on to become the lifestyles of adults leading to chronic health problems. Social, cultural and environmental factors are all important.\nYoung people have specific health problems and developmental needs that differ from those of children or adults: The causes of ill-health in adolescents are mostly psychosocial rather than biological. Young people often engage in health risk behaviours that reflect the processes of adolescent development: experimentation and exploration, including using drugs and alcohol, sexual behaviour, and other risk taking that affect their physical and mental health. Adolescent health also encompasses children's and young people's sexual and reproductive health (SRH). \n\nThe World Health Organisation describes the leading health related problems in the age group 10 – 19 years to include:\n\nYoung people often lack awareness of the harm associated with risk behaviours, and the skills to protect themselves as well as the lack knowledge about how and where to seek help for their health concerns . By intervening at this early life stage, many chronic conditions later in life can be prevented.\n\nEvidence-based practices include harm reduction and health promotion to intervene early in the life course and illness trajectory. Youth health is founded on collaborative approaches that address social justice. Youth development approaches include youth empowerment and youth participation. Their aim is to promote youth rights, youth voice and youth engagement.\n\nStudies about young people's access to healthcare have identified major barriers including concerns about confidentiality, practitioners attitudes and communication style, environment, availability of services, cost and the developmental characteristics of young people. Marginalised young people can have greater difficulty accessing health services and need support to navigate the health system.\n\nThe World Health Organisation 'Global standards for quality health-care services for adolescents' include: \n\nYouth Health includes adolescent medicine as a speciality, along with other primary and tertiary care services. Health services for young people include mental health services, child protection, drug and alcohol services, sexual health services. General Practitioners work alongside multidisciplinary health practitioners including psychology, social Work and Youth health nursing and school health services. Youth work and youth development services support and engage young people. Web based supports, such as Reach Out!, provide early intervention.\n\nYouth health services ('one-stop-shops' for young people) are specialist services providing multi-disciplinary, primary health care to young people. Focusing on engaging disadvantaged young people, they deliver flexible and unique services to young people in relaxed and comfortable youth-friendly environments. Youth health services work in partnership with other government and non-government services. Youth health services provide a range of entry-points and non-threatening services (such as creative arts, basic services such as showers and laundries, a drop in service, sports and recreational facilities), which encourage young people to connect with the service on their own terms. They also provide informal links to other support services and sectors including education, housing, financial support and legal services, offering support to young people who are dealing with complex issues. Youth health services understand the need to respond immediately to young people’s requests for support and assistance and they share a common operating philosophy, which values social justice, equity, and a holistic view of young people’s health and well being.\n\nCapacity building organisations support the Youth Health sector by providing access to information and resources, conducting research and providing training.\n\nThe Egyptian Society for Adolescent Medicine\nThe Arab Coalition for Adolescent Medicine\n\n"}
{"id": "3341757", "url": "https://en.wikipedia.org/wiki?curid=3341757", "title": "Africa@home", "text": "Africa@home\n\nAfrica@home is a website that allow users to use their home computers to contribute for humanitarian causes at Africa. This project first went public on 13 July 2006. It partners with Swiss Tropical Institute, the University of Geneva, CERN, and International Conference Volunteers (ICV). It is sponsored by the Geneva International Academic Network (GIAN).\n\nAfrica@home together with ICVolunteers, recruited volunteers across Africa to help with the project. The Malaria Control Project (MCP) was the first and the only grid computing project ran by Africa@home. MCP ran for 10 years and became inactive since 21 June 2016. \n"}
{"id": "33492530", "url": "https://en.wikipedia.org/wiki?curid=33492530", "title": "Andrew Fyfe (chemist)", "text": "Andrew Fyfe (chemist)\n\nProfessor Andrew Fyfe FRSE FRCSE PRSSA PRMS (18 January 1792 – 31 December 1861) was a Scottish surgeon and chemist. Following early studies on Fox Talbot's newly created photographic techniques he was one of the first (1839) to work out the theory behind positive rather than negative prints. He had an amateur interest in photography but appears not to have pursued his own theories (later very important in the creation of moving images) and limited his experiments to ferns lying on chemical papers.\n\nFyfe was born in Edinburgh on 18 January 1792, the son of Agnes Ord Williamson and Andrew Fyfe. He graduated with an MD at the University of Edinburgh in 1814, and became Fellow of the Edinburgh College of Surgeons in 1818, and was its president in 1842-43.\n\nHe lectured privately on chemistry and pharmacy at Edinburgh for many years, having been assistant to Professor Thomas Charles Hope.\n\nHe was elected a Fellow of the Royal Society of Edinburgh in 1823, his proposer being Thomas Charles Hope.\n\nHe was an unsuccessful candidate in 1832 for the Chair of Materia Medica at the University of Edinburgh, and again in 1844 for the Chair of Chemistry, which was filled by Professor William Gregory. He successfully applied for Gregory's vacated post as Professor of Chemistry at the University of Aberdeen. He retained this professorship till his death on 31 December 1861 in Edinburgh, however he stopped lecturing in the summer of 1860 due to ill health. His knowledge of inflammable substances was reputed, and he gave evidence in official inquiries on such subjects.\n\nIn the 1830s his address appears as 11 Teviot Row in Edinburgh. The building is now demolished. In the 1840s he is living at 38 George Square, Edinburgh.\n\nIn 1840–41 he was elected President of the Royal Scottish Society of Arts. In 1842 he succeeded Dr Richard Huie as President of the Royal College of Surgeons of Edinburgh.\n\nHe died at home at 4 Windsor Street, in east Edinburgh on 31 December 1861. He is buried in a family vault in New Calton Cemetery, with his second wife. The grave lies midway along the northern boundary wheel.\n\n\nHe was twice married: firstly to Eliza Charles, secondly to Margaret Johnstone. He had a daughter by his first marriage. His son by the second marriage, was also named Andrew Fyfe, and was a London physician. \nA further son, John Fyfe (1891–1917) was also a surgeon, but died young.\n\n"}
{"id": "1683306", "url": "https://en.wikipedia.org/wiki?curid=1683306", "title": "Atonic seizure", "text": "Atonic seizure\n\nAn atonic seizure (also called drop seizure, akinetic seizure or drop attack) is a type of seizure that consists of partial or complete loss of muscle tone that is caused by temporary alterations in brain function. These seizures are brief – usually less than fifteen seconds. They begin in childhood and may persist into adulthood. The seizure itself causes no injury, but the loss of control, predominantly in trunk muscles, can result in direct injury from falling. Electroencephalography can be used to confirm diagnosis. It is rare and can be indicative of Lennox-Gastaut syndrome (\"see\" Henri Gastaut).\n\nAtonic seizures can occur while standing, walking, or sitting, and are often noticeable by a head drop (relaxing of the neck muscles). Fall injuries may result in impact to the face or head. As with common epileptic occurrences, no first aid is needed post-seizure, except in the instances where falling injuries have occurred. In some cases, a person may become temporarily paralyzed in part of his or her body. This usually does not last longer than 3 minutes.\n\nThere is no general treatment for patients with a seizure disorder. Each treatment plan is specifically tailored to the individual patient based on their diagnosis and symptoms. Treatment options may include medical therapy, nerve stimulation, dietary therapy, or surgery, as appropriate. Clinical trials may also be a valuable treatment alternative.\nUsually, anticonvulsants are given based on other symptoms and / or associated problems.\nBecause the areas of the cerebellum which determine increases and decreases in muscle tone are close together, people experiencing atonic seizures are most likely experiencing myoclonic ones too, at some point. This may play a role in therapy and diagnostic.\n\nOne surgical approach, selective posterior callostomy, can greatly decrease instances of drop attacks and improve function and behavior in patients with intellectual disability.\n\n"}
{"id": "4726434", "url": "https://en.wikipedia.org/wiki?curid=4726434", "title": "Attention deficit hyperactivity disorder controversies", "text": "Attention deficit hyperactivity disorder controversies\n\nAttention deficit hyperactivity disorder (ADHD) controversies include concerns about its existence, causes, perceived overdiagnosis, and methods of treatment, especially with the use of stimulant medications in children. These controversies have surrounded the subject since at least the 1970s.\nAccording to the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), the leading authority in the US on clinical diagnosis and psychological behavior published by the APA in 2013, ADHD is a neurodevelopmental disorder with a prevalence rate in most cultures of about 5% in children and 2.5% in adults. Today, the existence of ADHD is widely accepted, but controversy around the disorder has existed since at least the 1970s. Adult ADHD continues to be a source of debate. According to the DSM-5, symptoms must be present before age 12, but it's not uncommon for ADHD to continue into adulthood. Parents and educators sometimes still question a perceived overdiagnosis in children and the effectiveness of treatment options, especially stimulant medications. However, according to sociology professor Vincent Parrillo, \"Parent and consumer groups, such as CHADD (Children and Adults with Attention Deficit Hyperactivity Disorder), tend to support the medical perspective of ADHD.\"\n\nIn 2009, Dr. Leon Eisenberg, who had coined the term ADDD in the 1950s, said \"ADHD is a prime example of a fictitious disease\". Snopes, a popular fact checking website found this to be mostly true.\n\nThe pathophysiology of ADHD is unclear and there are a number of competing theories.\n\nFrequently observed differences in the brain between ADHD and non-ADHD patients have been discovered, but it is uncertain if or how these differences give rise to the symptoms of ADHD. Results from various types of neuroimaging techniques suggest there are differences in the brain, such as thinner regions of the cortex, between individuals with and without ADHD.\n\nADHD is said to be highly heritable: twin studies suggest that genetics explain 70-80% in the variation of ADHD. However, interest in the potential role of gene-environment interactions in ADHD is also increasing; maternal alcohol or tobacco use during pregnancy may be one contributor. It has also been argued that ADHD is a heterogeneous disorder with multiple genetic and environmental factors converging on similar neurological changes. Authors of a review of ADHD etiology in 2004 noted: \"Although several genome-wide searches have identified chromosomal regions that are predicted to contain genes that contribute to ADHD susceptibility, to date no single gene with a major contribution to ADHD has been identified.\" However, many further studies have occurred since, and the same is true for many other heritable human traits (e.g., schizophrenia). The Online Mendelian Inheritance in Man (OMIM) database has a listing for ADHD under autosomal dominant heritable conditions, claiming that multiple genes contribute to the disorder. As of 2014, OMIM listed 6 genes with variants that have been associated with ADHD.\n\nIt has been argued that even if it is a social construct, this does not mean it is not a valid condition; for example obesity has different cultural constructs but yet has demonstrable adverse effects associated with it. A minority of these critics maintain that ADHD was \"invented and not discovered\". They believe that the disorder does not exist and that the behavior observed is not abnormal and can be better explained by environmental causes or just the personality of the \"patient\".\n\nThere is no blood test or brain scan for ADHD. Diagnosis is based on a clinical interview with the child and parents.\n\nOver the past two decades more research on the functioning of the brain is being done to help support the idea that Attention Deficit Hyperactivity Disorder is an executive dysfunction issue. The brains of males and females are showing differences, which could potentially help to explain why ADHD presents differently in boys and girls. Studies conducted using EEGs between boys and girls suggest that we can no longer ignore sex difference between boys and girls when identifying ADHD. There are EEG differences between girls and boys in their maturational pattern and this suggests that more studies regarding sex differences in ADHD should be conducted.\n\nOverdiagnosis typically refers to children who are diagnosed with ADHD but should not be. These instances are termed as \"false positives\". However, the \"presence of false positives alone does not indicate overdiagnosis\". There may be evidence of overdiagnosis if inaccuracies are shown consistently in the accepted prevalence rates or in the diagnostic process itself. \"For ADHD to be overdiagnosed, the rate of false positives (i.e., children inappropriately diagnosed with ADHD) must substantially exceed the number of false negatives (children with ADHD who are not identified or diagnosed).\" Children aged 8 to 15 years living in the community, indicated an ADHD prevalence rate of 7.8%. However, only 48% of the ADHD sample had received any mental health care over the past 12 months.\n\nEvidence also exists of possible differences of race and ethnicity in the prevalence of ADHD. The prevalence of ADHD dramatically varies across cultures despite the fact that the same methodology has been used. Some believe this may be due to different perceptions of what qualifies as disruptive behavior, inattention and hyperactivity.\n\nIt is argued that over-diagnosis occurs more in well-off or more homogeneous communities, whereas under-diagnosis occurs more frequently in poorer and minority communities due to lack of resources and lack of financial access. Those without health insurance are less likely to be diagnosed with ADHD. It is further believed that the \"distribution of ADHD diagnosis falls along socioeconomic lines\", according to the amount of wealth within a neighborhood. Therefore, the difficulty of applying national, general guidelines to localized and specific contexts, such as where referral is unavailable, resources are lacking or the patient is uninsured, may assist in the establishment of a misdiagnosis of ADHD.\n\nDevelopment can also influence perception of relevant ADHD symptoms. ADHD is viewed as a chronic disorder that develops in childhood and continues into adulthood. However, some research shows a decline in the symptoms of ADHD as children grow up and mature into adulthood. As children move into the stage of adolescence, the most common reporters of ADHD symptoms, parents and teachers, tend to focus on behaviors affecting academic performance. Some research has shown that the primary symptoms of ADHD were strong discriminators in parent ratings, but differed for specific age groups. Hyperactivity was a stronger discriminator of ADHD in children, while inattentiveness was a stronger discriminator in adolescents.\n\nIssues with comorbidity is another possible explanation in favor of the argument of overdiagnosis. As many as 75% of diagnosed children with ADHD meet criteria for some other psychiatric diagnosis. Among children diagnosed with ADHD, about 25% to 30% have anxiety disorders, 9% to 32% have depression, 45% to 84% have oppositional defiant disorder, and 44% to 55% of adolescents have conduct disorder. Learning disorders are found in 20% to 40% of children with ADHD.\n\nAnother possible explanation of over-diagnosis of ADHD is the \"relative-age effect\", which applies to children of both sexes. Younger children are more likely to be inappropriately diagnosed with ADHD and treated with prescription medication than their older peers in the same grade. Children who are almost a year younger tend to appear more immature than their classmates, which influences both their academic and athletic performance.\n\nThe debate of underdiagnosis, or giving a \"false negative\", has also been discussed, specifically in literature concerning ADHD among adults, girls and underprivileged communities. It is estimated that in the adult population, rates of ADHD are somewhere between 4% and 6%. However, as little as 11% of these adults with ADHD actually receive assessment, and furthermore, any form of treatment. Between 30% and 70% of children with ADHD report at least one impairing symptom of ADHD in adulthood, and 30% to 50% still meet diagnostic criteria for an ADHD diagnosis.\n\nResearch on gender differences also reveals an argument for underdiagnosis of ADHD among girls. The ratio for male-to-female is 4:1 with 92% of girls with ADHD receiving a primarily inattentive subtype diagnosis. This difference in gender can be explained, for the majority, by the different ways boys and girls express symptoms of this particular disorder. Typically, females with ADHD exhibit less disruptive behaviors and more internalizing behaviors. Girls tend to show fewer behavioral problems, show fewer aggressive behaviors, are less impulsive, and are less hyperactive than boys diagnosed with ADHD. These patterns of behavior are less likely to disrupt the classroom or home setting, therefore allowing parents and teachers to easily overlook or neglect the presence of a potential problem. The current diagnostic criteria appear to be more geared towards males than females, and the ADHD characteristics of men have been over-represented. This leaves many women and girls with ADHD neglected. Studies have shown that girls with ADHD, especially those with signs of impulsivity, were three to four times more likely to attempt suicide when compared with female controls. Additionally, these girls were two to three times more likely to engage in self-harming behaviors.\n\nAs stated previously, underdiagnosis is also believed to be seen in more underprivileged communities. These communities tend to be poorer and inhabit more minorities. More than 50% of children with mental health needs do not receive assessment or treatment. Access to mental health services and resources differs on a wide range of factors, such as \"gender, age, race or ethnicity and health insurance\". Therefore, children deserving of an ADHD diagnosis may never receive this confirmation and are not identified or represented in prevalence rates.\n\nIn 2005, 82 percent of teachers in the United States considered ADHD to be over diagnosed while three percent considered it to be under diagnosed. In China 19 percent of teachers considered ADHD to be over diagnosed while 57 percent considered it to be under diagnosed.\n\nADHD management recommendations vary by country and usually involves some combination of counseling, lifestyle changes, and medications. The British guideline only recommends medications as a first-line treatment in children who have severe symptoms and for them to be considered in those with moderate symptoms who either refuse or fail to improve with counseling. Canadian and American guidelines recommend that medications and behavioral therapy be used together as a first-line therapy, except in preschool-aged children.\n\nThe National Institute of Mental Health recommends stimulants for the treatment of ADHD, and states that, \"under medical supervision, stimulant medications are considered safe\". A 2007 drug class review found no evidence of any differences in efficacy or side effects in the stimulants commonly prescribed.\n\nBetween 1993 and 2003 the worldwide use of medications that treat ADHD increased almost threefold. Most ADHD medications are prescribed in the United States. In the 1990s, the US accounted for 90% of global use of stimulants such as methylphenidate and dextroamphetamine. By the early 2000s, this had fallen to 80% due to increased usage in other countries. In 2003, doctors in the UK were prescribing about a 10th of the amount per capita of methylphenidate used in the US, while France and Italy accounted for approximately one twentieth of US stimulant consumption. These assertions appear to contradict the 2006 World Drug Report published by the United Nations Office on Drugs and Crime, which indicate the US constituted merely 17% of the world market for dextroamphetamine. They assert that in the early 2000s amphetamine use was \"widespread in Europe.\"\n\nIn 1999, a study constructed with 1,285 children and their parents across four U.S. communities has shown 12.5% of children that met ADHD criteria had been treated with stimulants during the previous 12 months.\nIn May 2000, the testimony of DEA Deputy Director Terrance Woodworth has shown that the Ritalin quota increased from 1,768 kg in 1990 to 14,957 kg in 2000. In addition, IMS Health also revealed the numerous use of Adderall prescription have increased from 1.3 million in 1996 to nearly 6 million in 1999.\n\nSome parents and professionals have raised questions about the side effects of drugs and their long-term use. Magnetic resonance imaging studies suggest that long-term treatment with amphetamine or methylphenidate decreases abnormalities in brain structure and function found in subjects with ADHD, and improves function of the right caudate nucleus.\n\nOn February 9, 2006, the U.S. Food and Drug Administration voted to recommend a \"black-box\" warning describing the cardiovascular risks of stimulant drugs used to treat ADHD. Subsequently, the USFDA commissioned studies which found that, in children, young adults, and adults, there is no association between serious adverse cardiovascular events (sudden death, myocardial infarction, and stroke) and the medical use of amphetamine or other ADHD stimulants.\n\nThe effects of amphetamine and methylphenidate on gene regulation are both dose- and route-dependent. Most of the research on gene regulation and addiction is based upon animal studies with intravenous amphetamine administration at very high doses. The few studies that have used equivalent (weight-adjusted) human therapeutic doses and oral administration show that these changes, if they occur, are relatively minor. The long-term effects on the developing brain and on mental health disorders in later life of chronic use of methylphenidate is unknown. Despite this, between 0.51% to 1.23% of children between the ages of 2 and 6 years take stimulants in the US. Stimulant drugs are not approved for this age group.\n\nIn individuals who experience sub-normal height and weight gains during stimulant therapy, a rebound to normal levels is expected to occur if stimulant therapy is briefly interrupted. The average reduction in final adult height from continuous stimulant therapy over a 3 year period is 2 cm.\n\nReviews of clinical stimulant research have established the safety and effectiveness of long-term amphetamine use for ADHD. An evidence review noted the findings of a randomized controlled trial of amphetamine treatment for ADHD in Swedish children following 9 months of amphetamine use. During treatment, the children experienced improvements in attention, disruptive behaviors, and hyperactivity, and an average change of +4.5 in IQ. It noted that the population in the study had a high rate of comorbid disorders associated with ADHD and suggested that other long-term amphetamine trials in people with less associated disorders could find greater functional improvements.\n\nA 2008 review found that the use of stimulants improved teachers' and parents' ratings of behavior; however, it did not improve academic achievement. The same review also indicates growth retardation for children consistently medicated over three years, compared to unmedicated children in the study. Intensive treatment for 14 months has no effect on long-term outcomes 8 years later. No significant differences between the various drugs in terms of efficacy or side effects have been found.\n\nSome schools have attempted to require treatment with medications before allowing a child to attend school. The United States has passed a bill against this practice.\n\nStimulants used to treat ADHD are classified as Schedule II controlled substances in the United States.\n\nControversy has surrounded whether methylphenidate is as commonly abused as other stimulants with many proposing that its rate of abuse is much lower than other stimulants. However, the majority of studies assessing its abuse potential scores have determined that it has an abuse potential similar to that of cocaine and d-amphetamine.\n\nBoth children with and without ADHD abuse stimulants, with ADHD individuals being at the highest risk of abusing or diverting their stimulant prescriptions. Between 16 and 29 percent of students who are prescribed stimulants report diverting their prescriptions. Between 5 and 9 percent of grade/primary and high school children and between 5 and 35 percent of college students have used nonprescribed stimulants. Most often their motivation is to concentrate, improve alertness, \"get high,\" or to experiment.\n\nStimulant medications may be resold by patients as recreational drugs, and methylphenidate (Ritalin) is used as a study aid by some students without ADHD.\n\nNon-medical prescription stimulant use is high. A 2003 study found that non prescription use within the last year by college students in the US was 4.1%. A 2008 meta analysis found even higher rates of non prescribed stimulant use. It found 5% to 9% of grade school and high school children and 5% to 35% of college students used a nonprescribed stimulant in the last year.\n\n, 8% of all United States Major League Baseball players had been diagnosed with ADHD, making the disorder common among this population. The increase coincided with the League's 2006 ban on stimulants, which has raised concern that some players are mimicking or falsifying the symptoms or history of ADHD to get around the ban on the use of stimulants in sport.\n\nIn 2008 five pharmaceutical companies received warning from the FDA regarding false advertising and inappropriate professional slide decks related to ADHD medication. In September 2008 the FDA sent notices to Novartis Pharmaceuticals and Johnson & Johnson regarding advertisings of Focalin XR and Concerta in which they overstated products' efficacies. A similar warning was sent to Shire plc with respect to Adderall XR.\n\nRussell Barkley, a well-known ADHD researcher who has published diagnostic guidelines, has been criticized for his works because he received payment from pharmaceutical companies for speaking and consultancy fees.\n\nIn 2008, it was revealed that Joseph Biederman of Harvard, a frequently cited ADHD expert, failed to report to Harvard that he had received $1.6 million from pharmaceutical companies between 2000 and 2007. E. Fuller Torrey, executive director of the Stanley Medical Research Institute which finances psychiatric studies, said \"In the area of child psychiatry in particular, we know much less than we should, and we desperately need research that is not influenced by industry money.\"\n\nChildren and Adults with Attention-Deficit/Hyperactivity Disorder, CHADD, an ADHD advocacy group based in Landover, MD received a total of $1,169,000 in 2007 from pharmaceutical companies. These donations made up 26 percent of their budget.\n\nRussell Barkley believes labeling is a double-edged sword; there are many pitfalls to labeling but by using a precise label, services can be accessed. He also believes that labeling can help the individual understand and make an informed decision how best to deal with the diagnoses using evidence-based knowledge. Furthermore studies also show that the education of the siblings and parents has at least a short-term impact on the outcome of treatment. Barkley states this about ADHD rights: \"... because of various legislation that has been passed to protect them. There are special education laws with the Americans with Disabilities Act, for example, mentioning ADHD as an eligible condition. If you change the label, and again refer to it as just some variation in normal temperament, these people will lose access to these services, and will lose these hard-won protections that keep them from being discriminated against. ...\" Psychiatrist Harvey Parker, who founded CHADD, states, \"we should be celebrating the fact that school districts across the country are beginning to understand and recognize kids with ADHD, and are finding ways of treating them. We should celebrate the fact that the general public doesn't look at ADHD kids as \"bad\" kids, as brats, but as kids who have a problem that they can overcome\". However, children may be ridiculed at school by their peers for using psychiatric medications including those for ADHD.\n\nIn 2009, the British Psychological Society and the Royal College of Psychiatrists, in collaboration with the National Institute for Clinical Excellence (NICE), released a set of diagnosis and treatment guidelines for ADHD. These guidelines reviewed studies by Ford et al. that found that 3.6 percent of boys and 0.85 percent of girls in Britain qualified for a diagnosis of ADHD using the American DSM-IV criteria. The guidelines go on to state that the prevalence drops to 1.5% when using the stricter criteria for the ICD-10 diagnosis of hyperkinetic disorder used mainly in Europe.\n\nA systematic review of the literature in 2007 found that the worldwide prevalence of ADHD was 5.29 percent, and that there were no significant differences in prevalence rates between North America and Europe. The review did find differences between prevalence rates in North America and those in Africa and the Middle East, but cautioned that this may be due to the small number of studies available from those regions.\n\nNorwegian National Broadcasting (NRK) broadcast a short television series in early 2005 on the increase in the use of Ritalin and Concerta for children. Sales were six times higher in 2004 than in 2002. The series included the announcement of a successful group therapy program for 127 unmedicated children aged four to eight, some with ADHD and some with oppositional defiant disorder.\n\nThe validity of the work of many of the ADHD experts (including Biederman) has been called into question by Marcia Angell, former editor in chief of the \"New England Journal of Medicine\", in her book review, \"Drug Companies & Doctors: A Story of Corruption.\" Newspaper columnists such as Benedict Carey, science and medical writer for \"The New York Times\", have also written controversial articles on ADHD.\n\nIn 1998, the US National Institutes of Health (NIH) released a consensus statement on the diagnosis and treatment of ADHD. The statement, while recognizing that stimulant treatment is controversial, supports the validity of the ADHD diagnosis and the efficacy of stimulant treatment. It found controversy only in the lack of sufficient data on long-term use of medications and in the need for more research in many areas.\n\nIn 2014, a preliminary retrospective analysis on the effect of increased use of methylphenidate among children in Quebec due to a policy change found little evidence of positive effects and limited evidence of negative effects.\n\nThe National Institute for Health and Care Excellence (NICE) concluded that while it is important to acknowledge the body of academic literature which raises controversies and criticisms surrounding ADHD for the purpose of developing clinical guidelines, it is not possible to offer alternative methods of assessment (i.e. ICD 10 and DSM IV) or therapeutic treatment recommendations. NICE stated that this is because the current therapeutic treatment interventions and methods of diagnosis for ADHD are based on the dominant view of the academic literature. NICE further concluded that despite such criticism, ADHD represented a valid clinical condition, with genetic, environmental, neurobiological, and demographic factors. The diagnosis has a high level of support from clinicians and medical authorities.\n\nBaroness Susan Greenfield, a leading neuroscientist, wanted a wide-ranging inquiry in the House of Lords into the dramatic increase in the diagnosis of ADHD in the UK and its possible causes. This followed a BBC Panorama programme in 2007 which highlighted US research (The Multimodal Treatment Study of Children with ADHD by the University of Buffalo showing treatment results of 600) suggesting drugs are no better than therapy for ADHD in the long-term. In the UK medication use is increasing dramatically. Other notable individuals have made controversial statements about ADHD. Terence Kealey, a clinical biochemist and vice-chancellor of University of Buckingham, has stated his belief that ADHD medication is used to control unruly boys and girls behavior.\n\nThe British Psychological Society said in a 1997 report that physicians and psychiatrists should not follow the American example of applying medical labels to such a wide variety of attention-related disorders: \"The idea that children who don't attend or who don't sit still in school have a mental disorder is not entertained by most British clinicians.\" The National Institute for Health and Care Excellence (NICE), in collaboration with others, release guidelines for the diagnosis and treatment of ADHD. They are currently devising an update for 2018.\n\nAn article in the \"Los Angeles Times\" stated that \"the uproar over Ritalin was triggered almost single-handedly by the Scientology movement.\" The Citizens Commission on Human Rights, an anti-psychiatry group formed by Scientologists in 1969, conducted a major campaign against Ritalin in the 1980s and lobbied Congress for an investigation of Ritalin. Scientology publications claimed the \"real target of the campaign\" as \"the psychiatric profession itself\" and said that the campaign \"brought wide acceptance of the fact that (the commission) and the Scientologists are the ones effectively doing something about ... psychiatric drugging\".\n\nTom Cruise has described the medications Ritalin (methylphenidate) and Adderall (a mixed-salt amphetamine formulation) as \"street drugs\". Ushma S. Neill criticized this view, stating that the doses of stimulants used in the treatment of ADHD do not cause addiction and that there is some evidence of a reduced risk of later substance addiction in children treated with stimulants.\n\nIn the UK, Susan Greenfield spoke out publicly in 2007 in the House of Lords about the need for a wide-ranging inquiry into the dramatic increase in the diagnosis of ADHD, and possible causes. Her comments followed a BBC \"Panorama\" program that highlighted research that suggested medications are no better than other forms of therapy in the long term. In 2010, the BBC Trust criticized the 2007 \"Panorama\" program for summarizing the research as showing \"no demonstrable improvement in children's behaviour after staying on ADHD medication for three years\" when in actuality \"the study found that medication did offer a significant improvement over time\" although the long-term benefits of medication were found to be \"no better than children who were treated with behavior therapy.\" In 2017, Senator Johnny Isakson was criticized by his constituents when he stated that ADD is not a learning disability but a \"parental deficit disorder\", and that it is a result of parents not \"raising their kids like they should\".\n\n\n"}
{"id": "15720819", "url": "https://en.wikipedia.org/wiki?curid=15720819", "title": "Canadian Association for Adolescent Health", "text": "Canadian Association for Adolescent Health\n\nThe Canadian Association for Adolescent Health (also known in French as l'Association Canadienne pour la Santé des Adolescents) is a multidisciplinary, non-profit advocacy organization based in Montreal, Quebec which promotes interest in health issues having to do with adolescents between 10 and 19 years of age. The organization publishes a journal and sponsors conferences for the purpose of setting standards in adolescent healthcare across Canada. Founded in 1993 by Jean-Yves Frappier, a pediatrician at Sainte-Justine University Health Center, the CAAH was incorporated in 1996.\n\n"}
{"id": "2408605", "url": "https://en.wikipedia.org/wiki?curid=2408605", "title": "Canadian Centre on Substance Abuse", "text": "Canadian Centre on Substance Abuse\n\nThe Canadian Centre on Substance Use and Addiction (CCSA) is a national non-governmental charity formed in 1988, with a legislated mandate to reduce alcohol- and other drug-related harm. The organization provides leadership that allows for the development, implementation and monitoring of national priorities, and fosters a knowledge-translation environment where evidence shapes policy, practice and action. It also creates, sustains and leverages partnerships that maximize individual and collective efforts.\n\nCCSA has 55 staff and a number of research associates, and is located in Ottawa, Ontario, Canada. The organization receives funding from Health Canada.\nKey priorities include youth drug prevention; reducing alcohol and substance abuse, impaired driving, and misusee of pharmaceuticals; supporting the addictions workforce; international drug policy; and improving Canada's treatment system.\n\n\n"}
{"id": "44382398", "url": "https://en.wikipedia.org/wiki?curid=44382398", "title": "Cheek teeth", "text": "Cheek teeth\n\nCheek teeth or post-canines comprise the molar and premolar teeth in mammals. Cheek teeth are multicuspidate (having many folds or tubercles). Mammals have multicuspidate molars (three in placentals, four in marsupials, in each jaw quadrant) and premolars situated between canines and molars whose shape and number varies considerably among particular groups. Cheek teeth are sometimes separated from the incisors by a gap called a diastema.\n\nCheek teeth in reptiles are much simpler as compared to mammals.\n\nApart from helping grind the food to properly reduce the size of substrates for stomach enzymes, their minor role is in giving shape and definition to the animals' jaws. The shape of cheek teeth are directly related to their function, and morphological differences between species can be attributed to their dietary variations. Additionally, the shape a cheek tooth can be mechanically worn down based on diet, which is used to provide insights into the consumption habits of fossilized animals. Proper cleaning of cheek teeth is vital for all species of organisms and many species including humans and ruminants keep it on top of their crucial priority list. Dental caries may result from improper care of cheek teeth which is a prominent problem across the globe.\n\n \n"}
{"id": "180811", "url": "https://en.wikipedia.org/wiki?curid=180811", "title": "Clayton Alderfer", "text": "Clayton Alderfer\n\nClayton Paul Alderfer (September 1, 1940 - October 30, 2015) was an American psychologist, and consultant, known for further developing Maslow's hierarchy of needs.\n\nBorn in Sellersville, Pennsylvania, Alderfer obtained his BA in psychology in 1962 at Yale University, where he also obtained his PhD in psychology 1966. In 1977 he also obtained certification by the American Board of Professional Psychology (ABPP).\n\nAfter graduation Alderfer started his academic career at Cornell University in 1966. In 1968 he returned to Yale University, where he was researcher, lecturer and program director in the Department of Administrative Sciences until 1992. In 1992 he moved to Rutgers University, where he acted as the program director for the Organizational Psychology department at the Graduate School of Applied and Professional Psychology for 12 years. In the new millennium he started his own consultancy firm.\n\nAlderfer further developed Maslow's hierarchy of needs by categorizing the hierarchy into his ERG theory (Existence, Relatedness and Growth). \n\n\n"}
{"id": "3121925", "url": "https://en.wikipedia.org/wiki?curid=3121925", "title": "Delay reduction hypothesis", "text": "Delay reduction hypothesis\n\nIn classical conditioning, the delay reduction hypothesis states that certain discriminative stimuli (DS) are more effective as conditioned reinforcers (CR) if they signal a decrease in time to a positive reinforcer or an increase in time to an aversive stimulus or punishment. This is often applied in chain link schedules, with the final link being the aversive stimulus or positive (unconditioned) reinforcer.\n\nThe delay reduction hypothesis was developed in 1969 by Edmund Fantino. As a hypothesis, delay reduction proposes that delays are aversive to organisms and that choices will be made by the organism to reduce delay.\nWhen an organism was rewarded for an act it would repeat that action and hope for the same outcome. This would make that organism conditioned to either act or not act on the specific stimulius.\n\n"}
{"id": "149355", "url": "https://en.wikipedia.org/wiki?curid=149355", "title": "Dorothea Dix", "text": "Dorothea Dix\n\nDorothea Lynde Dix (April 4, 1802July 17, 1887) was an American advocate on behalf of the indigent mentally ill who, through a vigorous and sustained program of lobbying state legislatures and the United States Congress, created the first generation of American mental asylums. During the Civil War, she served as a Superintendent of Army Nurses.\n\nBorn in the town of Hampden, Maine, she grew up in Worcester, Massachusetts among her parents' relatives. She was the first child of three born to Joseph Dix and Mary Bigelow, who had deep ancestral roots in Massachusetts Bay Colony. Her father was an itinerant bookseller and Methodist preacher. At the age of twelve, she sought refuge with her wealthy grandmother, Dorothea Lynde (wife of Dr. Elijah Dix) in Boston to get away from her alcoholic parents and abusive father. About 1821 Dix opened a school in Boston, which was patronized by well-to-do families. Soon afterward she also began teaching poor and neglected children out of the barn of her grandmother's house, but she suffered poor health. It has been suggested that Dorothea suffered from major depressive episodes, which contributed to her poor health. From 1824 to 1830, she wrote mainly devotional books and stories for children. Her \"Conversations on Common Things\" (1824) reached its sixtieth edition by 1869. Her book \"The Garland of Flora\" (1829) was, along with Elizabeth Wirt's \"Flora's Dictionary\", one of the first two dictionaries of flowers published in the United States. Other books of Dix's include \"Private Hours, Alice and Ruth,\" and \"Prisons and Prison Discipline.\"\n\nAfter Dix's health forced her to relinquish her school, she began working as a governess on Beacon Hill for the family of William Ellery Channing, a leading Unitarian intellectual. It was while working with his family that Dix traveled to St. Croix, where she first witnessed slavery at first hand, though her experience did not dispose her sympathies toward abolitionism. In 1831, she established a model school for girls in Boston, operating it until 1836, when she suffered a breakdown. Dix was encouraged to take a trip to Europe to improve her health. While she was there she met British social reformers who inspired her. These reformers included Elizabeth Fry, Samuel Tuke and William Rathbone with whom she lived during the duration of her trip in Europe. In hopes of a cure, in 1836 she traveled to England, where she met the Rathbone family. They invited her as a guest to Greenbank, their ancestral mansion in Liverpool. The Rathbones were Quakers and prominent social reformers. At Greenbank, Dix met their circle of men and women who believed that government should play a direct, active role in social welfare. She was also introduced to the reform movement for care of the mentally ill in Great Britain, known as lunacy reform. Its members were making deep investigations of madhouses and asylums, publishing their studies in reports to the House of Commons.\n\nReform movements for treatment of the mentally ill were related in this period to other progressive causes: abolitionism, temperance, and voter reforms. After returning to America, in 1840-41 Dix conducted a statewide investigation of care for the mentally ill poor in Massachusetts. In most cases, towns contracted with local individuals to care for mentally ill people who could not care for themselves and lacked family/friends to do so. Unregulated and underfunded, this system resulted in widespread abuse. Dix published the results in a fiery report, a \"Memorial\", to the state legislature. \"I proceed, Gentlemen, briefly to call your attention to the present state of Insane Persons confined within this Commonwealth, in cages, stalls, pens! Chained, naked, beaten with rods, and lashed into obedience.\" Her lobbying resulted in a bill to expand the state's mental hospital in Worcester.\n\nDuring the year 1844 Dix visited all the counties, jails and almshouses in New Jersey in a similar investigation. She prepared a memorial for the New Jersey Legislature, giving a detailed account of her observations and facts. Dix urgently appealed to the legislature to act and appropriate funds to construct a facility for the care and treatment of the mentally ill. She cited a number of cases to emphasize the importance of the state taking responsibility for this class of unfortunates. Dix's plea was to provide moral treatment for the mentally ill, which consisted of three values: modesty, chastity, and delicacy.\n\nShe gave as an example a man formerly respected as a legislator and jurist, who, suffering from mental decline, fell into hard times in old age. Dix discovered him lying on a small bed in a basement room of the county almshouse, berefet of even necessary comforts. She wrote: \"This feeble and depressed old man, a pauper, helpless, lonely, and yet conscious of surrounding circumstances, and not now wholly oblivious of the past — this feeble old man, who was he?\" Many members of the legislature knew her pauper jurist. Joseph S. Dodd introduced her report to the Senate on January 23, 1845.\n\nDodd's resolution to authorize an asylum passed the following day. The first committee made their report February 25, appealing to the New Jersey legislature to act at once. Some politicians secretly opposed it due to taxes needed to support it. Dix continued to lobby for a facility, writing letters and editorials to build support. During the session, she met with legislators and held group meetings in the evening at home. The act of authorization was taken up March 14, 1845, and read for the last time. On March 25, 1845, the bill was passed for the establishment of a state facility.\n\nDix traveled from New Hampshire to Louisiana, documenting the condition of the poor mentally ill, making reports to state legislatures, and working with committees to draft the enabling legislation and appropriations bills needed. In 1846, Dix traveled to Illinois to study mental illness. While there, she fell ill and spent the winter in Springfield recovering. She submitted a report to the January 1847 legislative session, which adopted legislation to establish Illinois' first state mental hospital. \n\nIn 1848, Dix visited North Carolina, where she again called for reform in the care of mentally ill patients. Her first attempt to bring reform to North Carolina was denied. However, after a board member's wife requested, as a dying wish, that Dix's plea be reconsidered, the bill for reform was approved. In 1849, when the (North Carolina) State Medical Society was formed, the legislature authorized construction of an institution in the capital, Raleigh, for the care of mentally ill patients. Dix Hill Asylum, named in honor of Dorothea Dix's father, was eventually opened in 1856. One hundred years later, the Dix Hill Asylum was renamed the Dorothea Dix Hospital, in honor of her legacy. A second state hospital for the mentally ill was authorized in 1875, Broughton State Hospital in Morganton, North Carolina; and ultimately, the Goldsboro Hospital for the Negro Insane was also built in the Piedmont area of the segregated state. Dix had a biased view that mental illness was related to conditions of educated whites, not minorities (Dix, 1847).\n\nShe was instrumental in the founding of the first public mental hospital in Pennsylvania, the Harrisburg State Hospital. In 1853, she established its library and reading room.\n\nThe high point of her work in Washington was the Bill for the Benefit of the Indigent Insane, legislation to set aside of Federal land ( to be used for the benefit of the mentally ill and the remainder for the \"blind, deaf, and dumb\". Proceeds from its sale would be distributed to the states to build and maintain asylums. Dix's land bill passed both houses of the United States Congress; but in 1854, President Franklin Pierce vetoed it, arguing that social welfare was the responsibility of the states. Stung by the defeat of her land bill, in 1854 and 1855 Dix traveled to England and Europe. She reconnected with the Rathbone family and, encouraged by British politicians who wished to increase Whitehall's reach into Scotland, conducted investigations of Scotland's madhouses. This work resulted in the formation of the Scottish Lunacy Commission to oversee reforms.\n\nDix visited the British colony of Nova Scotia in 1853 to study its care of the mentally ill. During her visit, she traveled to Sable Island to investigate reports of mentally ill patients being abandoned there. Such reports were largely unfounded. While on Sable Island, Dix assisted in a shipwreck rescue. Upon her return to Boston, she led a successful campaign to send upgraded life-saving equipment to the island. The day after supplies arrived, a ship was wrecked on the island. Thankfully, because of Dix's work, 180 people were saved.\n\nIn 1854, Dix investigated the conditions of mental hospitals in Scotland, and found them to be in similarly poor conditions. In 1857, after years of work and opposition, reform laws were finally passed. Dix took up a similar project in the Channel Islands, finally managing the building of an asylum after thirteen years of agitation. Extending her work throughout Europe, Dix continued on to Rome. Once again finding disrepair and maltreatment, Dix sought an audience with Pope Pius IX. His Holiness was receptive to Dix's findings and visited the asylums himself, shocked at their conditions. He thanked Dix for her work, saying in a second audience with her that \"a woman and a Protestant, had crossed the seas to call his attention to these cruelly ill-treated members of his flock.\"\n\nDuring the American Civil War, Dix, on June 10, 1861 was appointed Superintendent of Army Nurses by the Union Army, beating out Dr. Elizabeth Blackwell.\n\nDix set guidelines for nurse candidates. Volunteers were to be aged 35 to 50 and plain-looking. They were required to wear unhooped black or brown dresses, with no jewelry or cosmetics. Dix wanted to avoid sending vulnerable, attractive young women into the hospitals, where she feared they would be exploited by the men (doctors as well as patients). Dix often fired volunteer nurses she hadn't personally trained or hired (earning the ire of supporting groups like the United States Sanitary Commission).\n\nAt odds with Army doctors, Dix feuded with them over control of medical facilities and the hiring and firing of nurses. Many doctors and surgeons did not want any female nurses in their hospitals. To solve the impasse, the War Department introduced Order No.351 in October 1863. It granted both the Surgeon General (Joseph K. Barnes) and the Superintendent of Army Nurses (Dix) the power to appoint female nurses. However, it gave doctors the power of assigning employees and volunteers to hospitals. This relieved Dix of direct operational responsibility. As superintendent, Dix implemented the Federal army nursing program, in which over 3,000 women would eventually serve. Meanwhile, her influence was being eclipsed by other prominent women such as Dr. Mary Edwards Walker and Clara Barton. She resigned in August 1865 and later considered this \"episode\" in her career a failure. Although thousands of Catholic nuns successfully served as Army nurses, Dix distrusted them; her anti-Catholicism undermined her ability to work with Irish and German nuns.\nBut her even-handed caring for Union and Confederate wounded alike, assured her memory in the South. Her nurses provided what was often the only care available in the field to Confederate wounded. Georgeanna Woolsey, a Dix nurse, said, \"The surgeon in charge of our camp...looked after all their wounds, which were often in a most shocking state, particularly among the rebels. Every evening and morning they were dressed.\" Another Dix nurse, Julia Susan Wheelock, said, \"Many of these were Rebels. I could not pass them by neglected. Though enemies, they were nevertheless helpless, suffering human beings.\"\n\nWhen Confederate forces retreated from Gettysburg, they left behind 5,000 wounded soldiers. These were treated by many of Dix's nurses. Union nurse Cornelia Hancock wrote about the experience: \"There are no words in the English language to express the suffering I witnessed today...\"\n\nShe was well respected for her work throughout the war because of her dedication. This stemmed from her putting aside her previous work to focus completely on the war at hand. With the conclusion of the war her service was recognized formally. She was awarded with two national flags, these flags being for \"the Care, Succor, and Relief of the Sick and wounded Soldiers of the United States on the Battle-Field, in Camps and Hospitals during the recent war.\" Dix ultimately founded thirty-two hospitals, and influenced the creation of two others in Japan.\n\nAt the end of the war, Dix helped raise funds for the national monument to deceased soldiers at Fortress Monroe. Following the war, she resumed her crusade to improve the care of prisoners, the disabled, and the mentally ill. Her first step was to review the asylums and prisons in the South to evaluate the war damage to their facilities.\n\nIn 1881, Dix moved into the New Jersey State Hospital, formerly known as Trenton State Hospital, that she built years prior. The state legislature had designated a suite for her private use as long as she lived. Although in poor health, she carried on correspondence with people from England, Japan, and elsewhere. Dix died on July 17, 1887. She was buried in Mount Auburn Cemetery in Cambridge, Massachusetts.\n\n\nNumerous locations are commemorated to Dix, including the Dix Ward in McLean Asylum at Somerville, Dixmont Hospital in Pennsylvania, and the Dorothea L. Dix House.\n\nShe wrote a variety of other tracts on prisoners. She is also the author of many memorials to legislative bodies on the subject of lunatic asylums and reports on philanthropic subjects.\n\nand other books.\n\n\na. Internet Archive currently lists seven copies of Francis Tiffany's book, of varying replication quality. The book was reprinted a number of times, and publishers may vary. However, the text is identical. Unfortunately, two of the easier to read versions uploaded to Internet Archive, namely this and this (the two bottom listings), are missing the title page, so were not utilised for the citation in this article. The information provided in the Internet Archive listings should never be used for citation, as they can contain inaccuracies (as can Google book listings). The uploaded, visible text itself should always be relied upon.\n\n\n"}
{"id": "18470082", "url": "https://en.wikipedia.org/wiki?curid=18470082", "title": "Essence (Electronic Surveillance System for the Early Notification of Community-based Epidemics)", "text": "Essence (Electronic Surveillance System for the Early Notification of Community-based Epidemics)\n\nEssence is an abbreviation/acronym for the United States Department of Defense's Electronic Surveillance System for the Early Notification of Community-based Epidemics. Essence's goal is to monitor health data as it becomes available and discover epidemics and similar health concerns before they get out of control. The program was created and developed in 1999 by Dr. Michael Lewis, MD, MPH, when he was a resident in the Preventive Medicine residency training program at the Walter Reed Army Institute of Research in Silver Spring, Maryland.\n\nThough the program was originally intended for early detection of bioterrorism attacks in the Washington, DC, area in the wake of the attacks of 9/11/2001, the U.S. Army Surgeon General, LTG James Peake, MD, ordered Jay Mansfield, the information technology specialist responsible for the IT development of ESSENCE, to expand ESSENCE to look globally at the entire DoD Military Healthcare System as designed. Subsequently, ESSENCE has been adopted and adapted by the Centers for Disease Control and Prevention, Johns Hopkins University, and numerous health departments around the United States and other countries.\n"}
{"id": "1785007", "url": "https://en.wikipedia.org/wiki?curid=1785007", "title": "Gerda Alexander", "text": "Gerda Alexander\n\nGerda Alexander (February 15, 1908 – February 21, 1994) was a Danish teacher who devised a method of self-development called Eutony. She was born in Wuppertal, Germany, but moved to Denmark in 1929.\n\nAlexander's parents were believers in eurythmy, passing on to her a similar interest in movement. Alexander as a young woman contracted rheumatic fever and endocarditis, suffering several crises. This inspired her to find ways to move that did not exacerbate her symptoms. Long periods of rest stimulated her to look within herself looking for a \"more economic\" and more spontaneous form of movement, starting with learning regulation of muscle tone.\n\nBy means of observation and reflection on her students, their own ailments and difficulties in mobility, and the investigation of the neuro-psychological bases of human movement, she molded her own method. She postulated that \"it is important, in treatment, not to give and do more than is necessary, so that the other can rely on himself. It is not that I am the great master who gives you help. Rather, I can introduce you to my work for your own self discovery.\"\n\nQuackwatch describes Alexander's invention, Eutony, as \"form of body-centered psychotherapy\" which \"posits 'blocked energy' and a collective unconscious\".\n"}
{"id": "51596223", "url": "https://en.wikipedia.org/wiki?curid=51596223", "title": "HealthWise Wales", "text": "HealthWise Wales\n\nHealthWise Wales is a project surrounding the health of the population of Wales. It is led by Cardiff University and was launched on 29 February 2016. It hopes to receive over 260,000 participants of the age of sixteen or over. The project is the first of its kind in Europe and hopes to \"build a picture\" of the future health requirements in Wales.\n"}
{"id": "30551758", "url": "https://en.wikipedia.org/wiki?curid=30551758", "title": "Health network surveillance", "text": "Health network surveillance\n\nHealth network surveillance is a practice of health information management involving a combination of security, privacy and regulatory compliance with patient health information (PHI). Health network surveillance addresses the rapidly increasing trend of electronic health records (EHR) and its incompatibility with information security practices that ‘lock down’ access through methods such as: modern firewalls, intrusion detection and prevention devices, and anti-virus and end-point protections.\n\nIn contrast to restrictive security measures, health network surveillance runs in the background of networks through a combination of hardware and software devices that allow for real time monitoring that do not impede the day-to-day health care operations that make up healthcare systems and deliver essential services to patients and clients. Surveillance, in this context, means tracking the connections that are made between computers. These connections can be between computers within a health network or from a computer outside the health network. Effectively, this approach has the capacity to provide additional assurance that standard protective devices and approaches are working.\n\nGovernments at all levels have increased legislation and regulation of the ways health information should be handled, for both public and private health organizations in many countries. Major regulatory bodies and legislation in Canada and the United States include but are not limited to: the Health Insurance Portability and Accountability Act (HIPAA), the Personal Information and Electronic Documents Act (PIPEDA), the Personal Health Information Protection Act (PHIPA), International Organization for Standardization (ISO), PCI Security Standards Council, and Canada Health Infoway. Health network surveillance is able to address the increasingly complex legislation, regulations and policies imposed on health organizations in a way that restrictive security measures can only reduce the service levels of these organizations.\n\nHealth network surveillance also has a proactive impact by providing business intelligence and network monitoring that can improve a health organization’s efficiency and effectiveness through real time information that can support decision making about network architecture, business processes and resource allocation. Two approaches enable the development of health network surveillance tools. Commonly used flow measures based on a number of flow protocols available on the market use the capacity of routers and switches to provide data regarding the functioning of networks. The use of connection tracking works to record every connection between devices in a monitored network. There may be advantages in connection tracking techniques as they avoid sampling, produce more data in real time and put less load on the functioning of networks.\n"}
{"id": "15268", "url": "https://en.wikipedia.org/wiki?curid=15268", "title": "Inquests in England and Wales", "text": "Inquests in England and Wales\n\nInquests in England and Wales are held into sudden and unexplained deaths and also into the circumstances of discovery of a certain class of valuable artefacts known as \"treasure trove\". In England and Wales, inquests are the responsibility of a coroner, who operates under the jurisdiction of the Coroners and Justice Act 2009.\n\nThere is a general duty upon every person to report a death to the coroner if an inquest is likely to be required. However, this duty is largely unenforceable in practice and the duty falls on the responsible registrar. The registrar must report a death where:\n\nThe coroner must hold an inquest where the death was:\n\nWhere the cause of death is unknown, the coroner may order a post mortem examination in order to determine whether the death was violent. If the death is found to be non-violent, an inquest is unnecessary.\n\nIn 2004 in England and Wales, there were 514,000 deaths of which 225,500 were referred to the coroner. Of those, 115,800 resulted in post-mortem examinations and there were 28,300 inquests, 570 with a jury. In 2014 the Royal College of Pathologists claimed that up to 10,000 deaths a year recorded as being from natural causes should have been investigated by inquests. They were particularly concerned about people whose death occurred as a result of medical errors. \"We believe a medical examiner would have been alerted to what was going on in Mid-Staffordshire long before this long list of avoidable deaths reached the total it did,\" said Archie Prentice, the pathologists' president.\n\nA coroner must summon a jury for an inquest if the death was not a result of natural causes and occurred when the deceased was in state custody (for example in prison, police custody, or whilst detained under the Mental Health Act 1983); or if it was the result of an act or omission of a police officer; or if it was a result of a notifiable accident, poisoning or disease. The senior coroner can also call a jury at his or her own discretion. This discretion has been heavily litigated in light of the Human Rights Act 1998, which means that juries are required now in a broader range of situations than expressly required by statute.\n\nThe purpose of the inquest is to answer four questions:\n\nEvidence must be solely for the purpose of answering these questions and no other evidence is admitted. It is not for the inquest to ascertain \"how the deceased died\" or \"in what broad circumstances\", but \"how the deceased came by his death\", a more limited question. Moreover, it is not the purpose of the inquest to determine, or appear to determine, criminal or civil liability, to apportion guilt or attribute blame. For example, where a prisoner hanged himself in a cell, he came by his death by hanging and it was not the role of the inquest to enquire into the broader circumstances such as the alleged neglect of the prison authorities that might have contributed to his state of mind or given him the opportunity. However, the inquest should set out as many of the facts as the public interest requires.\n\nUnder the terms of article 2 of the European Convention of Human Rights, governments are required to \"establish a framework of laws, precautions, procedures and means of enforcement which will, to the greatest extent reasonably practicable, protect life\". The European Court of Human Rights has interpreted this as mandating independent official investigation of any death where public servants may be implicated. Since the Human Rights Act 1998 came into force, in those cases alone, the inquest is now to consider the broader question \"by what means and in what circumstances\".\n\nIn disasters, such as the 1987 King's Cross fire, a single inquest may be held into several deaths. However, when several protesters were shot and killed by police in Mitchelstown in 1887, the findings of a common inquest were quashed because the killings had taken place at different times and in different places.\n\nInquests are governed by the Rules. The coroner gives notice to near relatives, those entitled to examine witnesses and those whose conduct is likely to be scrutinised. Inquests are held in public except where there are real issues of national security.\n\nIndividuals with an interest in the proceedings, such as relatives of the deceased, individuals appearing as witnesses, and organisations or individuals who may face some responsibility in the death of the individual, may be represented by lawyers at the discretion of the coroner. Witnesses may be compelled to testify subject to the privilege against self-incrimination.\n\nThe following verdicts are not mandatory but are strongly recommended:\n\nIn 2004, 37% of inquests recorded an outcome of death by accident / misadventure, 21% by natural causes, 13% suicide, 10% open verdicts, and 19% other outcomes.\n\nSince 2004 it has been possible for the coroner to record a narrative verdict, recording the circumstances of a death without apportioning blame or liability. Since 2009, other possible verdicts have included \"alcohol/drug related death\" and \"road traffic collision\". The civil standard of proof, on the balance of probabilities, is needed for most verdicts, except unlawful killing and suicide where the criminal standard of beyond reasonable doubt is required.\n\nOwing in particular to the failures to notice the serial murder committed by Harold Shipman, the Coroners and Justice Act 2009 modernised the system with:\n\n\n\n"}
{"id": "31271621", "url": "https://en.wikipedia.org/wiki?curid=31271621", "title": "Institut Marques", "text": "Institut Marques\n\nInstitut Marquès is a private Spanish medical institution specialized in gynecology and assisted reproduction. It was founded in 1941, in Barcelona, and it counts with a team of more than one hundred medical and sanitary professionals. Institut Marquès is internationally distinguished for its pioneer study of the male factor and the genetic analysis of sperm cells.\n\nIn 1941 Dr. Vicens Marquès i Bertran one of the founding members of the Spanish Society of fertility founded the clinic known as Sanatorio Maternal, where more than 7000 successful labors and 2000 gynecological interventions were accomplished.\n\nIn 1952 his son Dr. Leonardo Marquès Giraut, head of the clinic Instituto Municipal of Maternology and the department of Gynecology at the Hospital Ntra. Sra. Del Mar until 1969, incorporated into the team. He also directed the infertility department for the Cruz Roja Hospital at Barcelona.\nDr. leonardo Marquès Giraut carried out various studies on the permeability of fallopian tubes achieving the “Fargas de la Academia de Ciencias médicas de Barcelona”.\n\nThe current directors Dr. Leonardo Marquès Amorós and Dr.Marisa López-Teijón joined in 1987.\n\nIn 1989 Institut Marquès set up the first FIV lab which was renovated and expanded in 2009 with top of the line equipment in Reproductive Biology.\n\nIn 2005 the clinic created a Preimplantation Genetic Diagnosis (DGP) lab at CIMA clinic, instigating in the genetic analysis of embryos, ovules and spermatozoids.\n\nThe same year the first baby is born after the adoption of an embryo frozen for thirteen years, nine months after the set up of the Adoption Program of Embryos.\n\n"}
{"id": "1458330", "url": "https://en.wikipedia.org/wiki?curid=1458330", "title": "Institute of Public Affairs", "text": "Institute of Public Affairs\n\nThe Institute of Public Affairs (IPA) is a conservative public policy think tank based in Melbourne, Victoria, Australia. It advocates free market economic policies such as privatisation and deregulation of state-owned enterprises, trade liberalisation and deregulated workplaces, climate change skepticism, the abolition of the minimum wage, and the repeal of parts of the Racial Discrimination Act 1975.\n\nHistorian Michael Bertram, writing in 1989, identified three distinct periods for the Institute of Public Affairs:\n\n\nThe Institute of Public Affairs was founded in 1943 as the Institute of Public Affairs Victoria, with Charles Denton (\"CD\") Kemp as its inaugural director and George Coles as its inaugural chair. The founders were prominent businessmen, and current executive director John Roskam says of the occasion: “Big business created the IPA”. The idea to form the Institute of Public Affairs was first floated in the Victorian Chamber of Manufactures.\n\nThe IPA’s formation was prompted by the collapse of Australia’s main right-wing party, the United Australia Party. The IPA’s initial purpose was to influence Australia’s post-war reconstruction, with business interests concerned that popular sentiment supported a Labor-led, collectivist post-war construction, a “prevailing clamour for a new kind of society”.\n\nThroughout 1943, branches were set up in NSW (May), SA (June) and Queensland (August), although the state branches remained administratively and ideologically distinct (the SA and Queensland branches closed in the 1950s). There seems to have been a pre-existing body called the Institute of Public Affairs in WA, which operated between 1941 and 1942. The IPA NSW engaged in “party political activism”, while at the IPA Victoria’s first annual meeting in 1944 chair GJ Coles said that they “did not wish to be directly involved in politics”.\n\nIn March 1943, the head of the Commonwealth Security Service (CSS), Brigadier William Simpson requested a report into whether the newly-formed IPA had sympathies with \"fascism\", \"counter-revolution\" or the powers that Australia was at war with, but his deputy director said that its committee and sponsors were \"beyond reproach\".\n\nThe CSS was restructured in late 1943 and it again investigated the IPA's state branches. The IPA Queensland's radio play The Harris Family was required to be submitted to and approved by the Chief-Inspector (Wireless). The second review was completed in 1944. The CSS reported that nothing could be found to suggest that the IPA was subversive, and the war record of its supporters was \"very fine\", although two of the IPA NSW's council members were members of the Japan-Australia Society and one was associated with the Old Guard. The National Archives of Australia preserve the CSS' reports into each branch, as well as material collected in the course of their investigation.\n\nIn October 1944, the IPA printed 50,000 copies of \"Looking Forward\", an 80-page booklet which set out the possibilities of post-war reconstruction. Robert Menzies described \"Looking Forward\" as “the finest statement of basic political and academic problems made in Australia for many years”.\n\nThe IPA had no formal association with the formation of the Liberal Party of Australia by Menzies in 1945. Political scientist Marian Simms says that the IPA’s role was to act as “an interim finance collector for non-Labor political interests”, initiate “the unification of the non-Labor organizations in Victoria … and then [mediate] among them” and provide “much of the content of the federal platform of the LPA and propaganda for political campaigns”. \"Looking Forward\" was influential in the Liberal Party’s inaugural platform.\n\nNorman Abjorensen credits the IPA in this period with the collapse in ALP support, saying that the IPA was “the architect of a stream of propaganda that sought, successfully, to discredit Australia’s very moderate Labor Party as a socialist tiger waiting to pounce once the war had ended.”\nDuring the 1950s and 1960s, the IPA \"came to wholeheartedly support\" Keynesian economics, with director C.D. Kemp writing \"we are all socialists now\". Over this period, the Institute argued for Australia's migration rate to be halved, which drew criticism from the Australian Industries Development Association and The Age. The Institute also identified inflation as a major issue, and opposed the abolition of the means test, called for lower taxes, criticised the introduction of the Trade Practices Act, advocated for fewer restraints on foreign investment and celebrated Britain joining the European Economic Community.\n\nIn 1962, the IPA dropped “Victoria” from its name, an act that caused relations between it and the IPA NSW to “deteriorate further”.\n\nIn the 1970s, the IPA and IPA NSW cooperated to establish Enterprise Australia. This organisation had as “an immediate target … the removal of the present Labor Government in Canberra”, while the IPA ostensibly stayed at arm’s length in an attempt to be perceived as above party politics.\n\nFrom its founding to the late 1970s, the IPA had been associated with anti-socialist Keynesian economics and protectionist industry. The appointment of Rod Kemp (CD Kemp’s son) as executive director in 1982, along with other administrative changes that had occurred in the late 1970s and early 1980s, marked a shift to neo-liberal ideology that continues to this day.\n\nIn June 1987 the IPA was incorporated as a company limited by guarantee.\n\nIn 1989, the IPA NSW – which had always been administratively and ideologically distinct – changed name to the Sydney Institute, and transitioned from neo-liberal think tank to discussion forum. The IPA NSW had a budget of $120,000 in 1985, compared to the IPA Victoria’s $300,000.\n\nRod Kemp left his position as executive director in 1989 as he had been elected to Parliament.\n\nIn 1991, the IPA amalgamated with the Perth-based Australian Institute of Public Policy and John Hyde moved from executive director of the Australian Institute of Public Policy to executive director of the IPA. The AIPP had been founded by Hyde in 1983 as a neo-liberal think tank, and the merger brought its annual revenue of about $300,000 or $400,000 to the IPA. Hyde described the merger as “joining forces with old friends”.\n\nThe IPA cooperated with the Tasman Institute on Project Victoria, which provided a blueprint for the privatisation and deregulation of the Victorian economy when Jeff Kennett became premier in 1992. The research was done with the assistance of Westpac staff seconded to work on the project.\n\nJohn Roskam replaced Mike Nahan as executive director in 2005, although he had worked at the IPA for a number of years before that.\n\nBetween 2009 and 2013, the IPA’s annual revenue doubled to $3.2 million a year, an increase attributed by Roskam to the IPA’s campaign against parts of the Racial Discrimination Act and the Gillard Government's media regulation proposals.\n\nIn 2008, former executive director of the IPA Rod Kemp was appointed chair of the IPA.\n\nIn 2013 the IPA celebrated its 70th anniversary. Notable in attendance at the celebrations were:\n\n\nThe IPA is governed by 54-55 members, also known as the IPA Council or the Council of the Institute, who are eligible to vote at the annual general meeting and to stand for board positions. These voting members are chosen by staff and the IPA board. The IPA 2003 annual report lists all 54 voting members, but subsequent annual reports do not.\n\nThe voting members are distinct from the general membership, who numbered 4,559 in 2017. Non-voting membership is open to the public, with membership fees ranging between $22 and $249 as of July 2018. Executive Director John Roskam noted in 2007 of this arrangement that: \"We market it as membership but technically they are not and it is something we are aware of’\". Membership has increased since 2010, when there were 826 members.\n\nThe IPA is funded by its membership, which include both private individuals and businesses. It has a dual structure, with the IPA as a whole reporting revenues of $6.1 million in financial year 2017 while its charitable arm, the Institute Of Public Affairs Research Trust, reported revenue of $0.6 million.\n\nThe IPA has been significantly funded by Hancock Prospecting, of which Gina Rinehart is the Executive Chair. Hancock Prospecting paid the IPA $2.3 million in financial year 2016 and $2.2 million in financial year 2017, which represents one-third to a half of the IPA's total revenue in those years. These payments were not disclosed in IPA annual reports, and Rinehart's daughter Bianca Hope Heyward submitted in court that the Hancock Prospecting payments were credited to Rinehart in an individual capacity. Gina Rinehart was made a life member of the IPA in November 2016.\n\nOther businesses who fund or have funded the IPA include ExxonMobil, Telstra, WMC Resources, BHP Billiton, Philip Morris, Murray Irrigation Limited, Visy Industries, Clough Engineering, Caltex, Shell, Esso and British American Tobacco (BAT).\n\nFunders are able to \"earmark\" their payments to support the work of particular units within the IPA.\n\nIn 2003, the Australian Government paid $50,000 to the Institute of Public Affairs to review the accountability of NGOs.\n\nThe IPA Victoria was founded during World War II by businessmen in response to the feared growing power of the Labor Party and international socialism, with founder C. D. Kemp putting the case to the Victorian Chamber of Manufactures sub-committee as such:\n\nThe IPA Victoria was founded as an apolitical organisation, and rejected the IPA NSW's strategy of \"direct short term political action to defeat the Labor Party with an emphasis on propaganda\". However, the IPA Victoria acted as a finance committee for non-Labor parties in its first year, and the IPA Treasurer at the time reportedly said that much of the IPA Victoria's funding was conditional on it being spent to \"fight socialism at the coming election\" (the Australian federal election, 1943). Its Publicity and Research Bureau wrote political broadcasters, provided speakers' notes to all endorsed United Australia Party and United Country Party candidates and producing advertisements.\n\nThe IPA Victoria's direct involvement in federal politics was reappraised after the 1943 election, and the organisation handed over responsibility for fundraising to the extra-parliamentary wing of the United Australia Party. The IPA Victoria remained involved in non-Labor politics, including financing by-election candidates and participating in the foundation of the Liberal Party of Australia.\n\nThe IPA Victoria was less involved in the 1949 federal election than the 1943 election, but the \"IPA Review\" did publish articles arguing against socialism and with tactical advice for the Liberal Party.\n\nDuring Charles Kemp's time as Director, the IPA Victoria focused its political engagement on the non-Labor parties, and did not \"seriously attempt\" to influence Labor politicians. Academic and public servant Finlay Crisp described it as a \"satellite\" of the Liberal Party during this time, and the \"IPA Review\" had a policy of not approaching Labor figures for submissions and of muting criticism of the Liberal Government.\n\nIn 1978, the IPA and the Australian Council of Trade Unions prepared a booklet on partnership in industry, but the ACTU baulked at the association and its name was not on the final publication. By the 1980s, the IPA had changed its policy and made space in the \"IPA Review\" for Labor politicians and \"others not of the free enterprise persuasion\".\n\nToday, the Institute still has close ideological and political affinities with the Liberal Party in Australia. For example, IPA Executive Director John Roskam worked on the Liberal Party's election campaign during the 2001 federal election and has run for Liberal Party preselection. Tony Abbott delivered the 57th C D Kemp lecture in 2001 on the Coalition Government's Work for the Dole program and Prime Minister John Howard delivered the 60th C D Kemp lecture in 2004, titled \"Iraq: The Importance of Seeing it Through\".\n\nThe IPA has affiliations with think tanks in the U.S., Canada, UK and Asia. It has a close relationship with the American Enterprise Institute, an American think-tank.\n\nFollowing the 2013 federal election of the Abbott Coalition Government, the IPA released a list of 75 policy initiatives (later adding another 25) to \"transform Australia\" which encapsulated the present direction of the IPA.\n\nThe IPA Victoria's ideological position was initially \"an amalgam of Keynesianism control and Hayekian regulation\", with IPA President Eric Lampe in 1961 saying that the IPA considered government responsibility for full employment, social security, the speed of development, living standards and financial stability \"all very necessary\".\n\nThis changed during the late 1970s and 1980s, when the IPA adopted an economic rationalist or neo-liberal position, with the IPA saying in 1988 that:\n\nRecent economic positions of the IPA include:\n\n\nThe IPA has made the following criticisms of proposals by the Australian government to introduce plain packaging of tobacco products:\n\nThe IPA adopts a position of doubt about climate change and finances several Australian climate change science doubters.\n\nIn 2008, the institute facilitated a donation of $350,000 by Dr G. Bryant Macfie, a climate change sceptic, to the University of Queensland for environmental research. The money is to fund three environmental doctoral projects, with the IPA suggesting two of the three agreed topics.\n\nIn 2010, the IPA published a compilation of essays by prominent climate change skeptics titled \"Climate Change: The Facts\" and edited by John Roskam and Alan Moran. An expanded version with 22 essays was published in 2015 through Stockade Books and a follow-up edited by Jennifer Marohasy was published in 2017, both in Kindle format.\n\nIn 2017, Marohasy and IPA colleague John Abbot publisher a paper on climate change in the journal \"GeoResJ\", also discussing the work on the IPA website, in \"The Spectator Australia\", and in Marohasy's blog. The research concludes that much of recent warming could be attributable to natural variations, and that the \"world was about as warm in 1980 as it was during the Middle Ages.\" This conclusion was welcomed by conservative media outlets but heavily criticised by climate scientists who pointed to methodological flaws in the research and declared it unworthy of publication. Gavin Schmidt, the Director of NASA's Goddard Institute for Space Studies, has pointed out that some data were shifted in time by approximately 35 years, leading to the omission of warming that has occurred since 1965. Schmidt described the research as \"worthless\" and an example of \"what happens when people have their conclusions fixed before they start the work.\"\n\nThe following individuals are or were associated with the Institute of Public Affairs.\n\nWhen the IPA Victoria was founded in 1943, it had a chairman (Sir George Coles); a chairman of the Executive Committee was chosen in 1944 (G. H. Grimwade) and a director (Charles Denton Kemp) in 1948. The Executive Committee was also referred to as the Industrial Committee or the Executive & Editorial Committee, and the role of chairman was subsequently renamed to president. From July 1983, the roles of president and chairman of the Executive Committee were combined.\n\nThe \"IPA Review\" is published quarterly.\n\n\n"}
{"id": "21858106", "url": "https://en.wikipedia.org/wiki?curid=21858106", "title": "International College of Dentists", "text": "International College of Dentists\n\nThe International College of Dentists was conceived at a farewell party for Dr. Louis Ottofy when he was returning home to the United States after practicing dentistry in the Philippines and Japan for 23 years. A colleague, Dr. Tsurukichi Okumura, a Japanese dentist, also urged Dr. Ottofy to form an international organization.\n\nSix years later, at an International Dental Congress in Philadelphia, U.S.. the group of dentists met again to finalize the concept of the ICD then on New Year's Eve of 1927 the College was announced with Drs. Ottofy and Okumura as the Co-Founders.\n\nOriginally, 250 dentists from 162 countries accepted the Fellowship oath. The group was selected based on an international reputation and participation in the FDI World Dental Federation. Each Fellow was given the task of nominating other dentists for membership based with the following instructions.\n\nIn the year following the initial formation of the ICD membership grew such that autonomous regions were required. Dentists who are inducted into the organization place the post-nominals FICD with their name. In the 1960s the Philippines section grew then the Middle East section. In the 1980s the South American section was formed. Today, the organization claims a membership of approximately 11,000 (2009) from hundreds of countries.\n\nIn 2008, fifteen ICD Fellows of Myanmar Region 34 with eight assistants went to\ncyclone-affected townships in the Delta and Dadeyae areas, providing free dental care and making some donations to the people in the camps under the name of the ICD organization. Over 235 patients were treated. Similar dental missions occur throughout the year and around the world to underserviced areas under the name of the ICD. Other projects are sponsored by ICD around the world., For a more detailed list of humanitarian projects see the list at ICD USA Projects\n\nIn Vietnam and Cambodia training for public health dentistry has been limited because of funds and experts in the field. Through an ICD sponsored program, roughly 40% of the public health dentists were given additional training. Another 20% of them went on to receive higher education (graduate) degrees in public health dentistry. While these programs do not directly provide care to patients in either country, they develop practical skills for dentists in designing programs for their populations. The organization has also worked to increase fluoridation in developing countries with high decay rates.\n\nTo keep members and the public up to date about the activities of the ICD publications are published by each of the autonomous sections and most maintain their own web sites. The ICD Headquarters also publishes an annual journal called The Globe and a quarterly newsletter called The College Today \n"}
{"id": "12501304", "url": "https://en.wikipedia.org/wiki?curid=12501304", "title": "Jean-Pierre Nuel", "text": "Jean-Pierre Nuel\n\nJean-Pierre Nuel (February 27, 1847 – August 21, 1920) was a Luxembourgian-Belgian ophthalmologist and physiologist who was a native of Tétange.\n\nIn 1870 he earned his doctorate from the University of Ghent, and became licensed to practice surgery and gynecology in Luxembourg. Subsequently, he opened a private practice in the town of Eich (today part of Luxembourg City) and furthered his education in Vienna, Bonn and Utrecht. During this time period, his interest shifted to ophthalmology, and he was particularly inspired by the work of Franciscus Donders (1818–1889) at Utrecht.\n\nIn 1877 Nuel became a professor of ophthalmology in Louvain, and later a professor of physiology in Ghent (1880). In 1885 he attained the chair of ophthalmology and physiology of sensory organs at Liège. In the field of otology, he is credited with the discovery of the eponymous \"space of Nuel\", which is an interval between the outer rods of Corti and the adjacent hair cells.\n\n\n"}
{"id": "18417892", "url": "https://en.wikipedia.org/wiki?curid=18417892", "title": "Lac de Coiselet", "text": "Lac de Coiselet\n\nLac de Coiselet is a reservoir on the border between the Ain and Jura departments in France. Its surface area is 3.8 km². The lake formed in 1970 after the Barrage de Coiselet was built at the confluence of the Ain and Bienne rivers.\n"}
{"id": "1146168", "url": "https://en.wikipedia.org/wiki?curid=1146168", "title": "Lancet surveys of Iraq War casualties", "text": "Lancet surveys of Iraq War casualties\n\n\"The Lancet\", one of the oldest scientific medical journals in the world, published two peer-reviewed studies on the effect of the 2003 invasion of Iraq and subsequent occupation on the Iraqi mortality rate. The first was published in 2004; the second (by many of the same authors) in 2006. The studies estimate the number of excess deaths caused by the occupation, both direct (combatants plus non-combatants) and indirect (due to increased lawlessness, degraded infrastructure, poor healthcare, etc.).\n\nThe first survey published on 29 October 2004, estimated 98,000 excess Iraqi deaths (with a range of 8,000 to 194,000, using a 95% confidence interval (CI)) from the 2003 invasion and subsequent occupation of Iraq to that time, or about 50% higher than the death rate prior to the invasion. The authors described this as a conservative estimate, because it excluded the extreme statistical outlier data from Fallujah. If the Fallujah cluster were included, the mortality estimate would increase to 150% over pre-invasion rates (95% CI: 1.6 to 4.2).\n\nThe second survey published on 11 October 2006, estimated 654,965 excess deaths related to the war, or 2.5% of the population, through the end of June 2006. The new study applied similar methods and involved surveys between May 20 and July 10, 2006. More households were surveyed, allowing for a 95% confidence interval of 392,979 to 942,636 excess Iraqi deaths. 601,027 deaths (range of 426,369 to 793,663 using a 95% confidence interval) were due to violence. 31% (186,318) of those were attributed to the US-led Coalition, 24% (144,246) to others, and 46% (276,472) unknown. The causes of violent deaths were gunshot (56% or 336,575), car bomb (13% or 78,133), other explosion/ordnance (14%), air strike (13% or 78,133), accident (2% or 12,020), and unknown (2%).\n\nThe \"Lancet\" surveys are controversial because the mortality figures are higher than in several other reports, including those of the Iraqi Health Ministry and the United Nations, as well as other household surveys such as the Iraq Living Conditions Survey and the Iraq Family Health Survey. The 2007 ORB survey of Iraq War casualties estimated more deaths than the Lancet, though it covered a longer period of the conflict. The \"Lancet\" surveys have triggered criticism and disbelief from some journalists, governments, the Iraq Body Count project, some epidemiologists and statisticians and others, but have also been supported by some journalists, governments, epidemiologists and statisticians.\n\nThe survey was sponsored by the Center for International Emergency Disaster and Refugee Studies, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD, United States (authors L Roberts PhD, G Burnham MD) and the Department of Community Medicine, College of Medicine, Al-Mustansiriya University, Baghdad, Iraq. Roberts' team was chosen for their experience in estimating total mortality in war zones, for example his estimate of 1.7 million deaths due to the war in the Congo which not only met with widespread acceptance and no challenge when published in 2000, but resulted and was cited in a U.N. Security Council resolution that all foreign armies must leave Congo, a United Nations request for $140 million in aid, and the US State Department pledging an additional $10 million in aid. Similar studies have been accepted uncritically as estimates of wartime mortality in Darfur and Bosnia.\n\nRoberts' regular technique is to estimate total mortality by personal surveys of a sample of the households in the area under study; this method being chosen in order to avoid the under-counting inherent in using only reported deaths in areas so chaotic that many deaths are unreported, and to include those deaths not directly attributable to violence but nevertheless the result of the conflict through indirect means, such as contamination of water supply or unavailability of medical care. The baseline mortality rate calculated from the interviewees' reports for the period prior to the conflict is subtracted from that reported during the conflict, to estimate the excess mortality which may be attributed to the presence of the conflict, directly or indirectly. This technique has been accepted uncritically in the previous mortality surveys discussed above.\n\nBecause of the impracticality of carrying out an evenly distributed survey, particularly during a war, Roberts' surveys use \"cluster sampling\", dividing the area into a number of randomly selected, approximately equally populated regions; a random point is chosen within each region, and a fixed number of the households closest to that point are surveyed as a \"cluster\". While not as accurate as an evenly distributed survey of the same number of households, this technique is more accurate than merely surveying one household for each selected point.\n\nIn his study of Iraq, Roberts divided the country into 33 regions, attempting to sample 30 households for each cluster, and selecting 988 households, with 7868 residents. In September 2004, each surveyed household was interviewed about household composition, births, and deaths since January, 2002. Of 78 households where members were asked to show documentation to confirm their claims after the interview was finished, 63 were able to present death certificates. According to the authors, 5 (0.5%) of the 988 households that were randomly chosen to be surveyed refused to be interviewed.\n\nThe relative risk of death due to the 2003 invasion and occupation was estimated by comparing mortality in the 17.8 months after the invasion with the 14.6 months preceding it. The authors stated, \"Making conservative assumptions, we think that about 100,000 excess deaths, or more have happened since the 2003 invasion of Iraq.\" Among such \"conservative assumptions\" is the exclusion of data from Fallujah in many of its findings. Since interpreting the results of the study would be complicated by the inclusion of an outlier cluster in Fallujah, where heavy fighting caused far more casualties than elsewhere in Iraq, the study focused mainly on the results that excluded the Fallujah cluster. While the authors argued that the Fallujah cluster's inclusion could be justified as a normal part of the sampling strategy (the authors noted that other \"hotspots\" like Najaf had not ended up being surveyed), and the authors presented two sets of results in some cases (one set including the Fallujah data and one not), the article, and most press coverage of the article, stresses the data that excluded the Fallujah cluster.\n\nThe main debate in the media in the U.S. and UK focused on whether 98,000 (95% CI 8000–194,000) more Iraqis died as a result of coalition intervention, calculated from their estimate of an increased mortality of 1.5 times (95% CI 1.1-2.3) the prewar rate (excluding the Fallujah data). Had the Fallujah sample been included, the survey's estimate that mortality rates had increased about 2.5 times since the invasion (with a 95% CI 1.6-4.2) including the Fallujah data would have resulted in an excess of about 298,000 deaths (95% CI ?-?), with 200,000 concentrated in the 3% of Iraq around Fallujah (Roberts et al. p. 5).\n\nAccording to the article, violence was responsible for most of the extra deaths whether or not the Fallujah data was excluded. Coalition airstrikes would be the main cause of these violent deaths if Fallujah data were included. The study makes the controversial conclusion that: \"Violent deaths were widespread, reported in 15 of 33 clusters, and were mainly attributed to coalition forces.\" and \"Violence accounted for most of the excess deaths and air strikes from coalition forces accounted for most violent deaths.\"\nThe study estimates that the risk of death specifically from violence in Iraq during the period after the invasion was approximately 58 times higher than in the period before the war, with the CI95 being 8.1-419, meaning that there is a 97.5% chance that the risk of death from violence after the invasion is at least 8.1 times higher than it was before.\nNewsday reported:\n\nThe most common causes of death before the invasion of Iraq were heart attacks, strokes and other chronic diseases. However, after the invasion, violence was recorded as the primary cause of death and was mainly attributed to coalition forces—with about 95 percent of those deaths caused by bombs or fire from helicopter gunships.\n\nSome criticisms have focused on the relatively broad 95% confidence intervals (CI95), resulting from the difficulty and scarcity of reliable sources.\n\nLila Guterman, after writing a long article in January 2005 in \"The Chronicle of Higher Education\", wrote a short article in the \"Columbia Journalism Review\" that stated: \"I called about ten biostatisticians and mortality experts. Not one of them took issue with the study's methods or its conclusions. If anything, the scientists told me, the authors had been cautious in their estimates. With a quick call to a statistician, reporters would have found that the probability forms a bell curve — the likelihood is very small that the number of deaths fell at either extreme of the range. It was very likely to fall near the middle.\"\n\nA Ministerial Statement written 17 November 2004, by the UK government stated \"the Government does not accept its [the study's] central conclusion\", because they were apparently inconsistent with figures published by the Iraq Ministry of Health, based on figures collected by hospitals, which said that \"between 5 April 2004 and 5 October 2004, 3,853 civilians were killed and 15,517 were injured\".\"\n\nSome critics have said that \"The Lancet\" study authors were unable to visit certain randomly selected sample areas. In an interview on the radio program \"This American Life\" however, the authors of the study say that they never substituted different, more accessible, areas, and that every place that was randomly selected at the beginning of the study was surveyed in full, despite the risk of death to the surveyors.\n\nCritics of the \"Lancet\" study have pointed out other difficulties in obtaining accurate statistics in a war zone. The authors of the study readily acknowledge this point and note the problems in the paper; for example they state that \"there can be a dramatic clustering of deaths in wars where many die from bombings\". They also said that the data their projections were based on were of \"limited precision\" because the quality of the information depended on the accuracy of the household interviews used for the study.\n\nThe results of the study were politically sensitive, since a heavy death toll could raise questions regarding the humanitarian justifications on the eve of a contested US presidential election. Critics objected to the timing of the report, claiming it was hastily prepared and published despite what they perceived as its poor quality in order to sway the U.S. electorate. On this topic, Les Roberts stated \"I emailed it in on Sept. 30 under the condition that it came out before the election. My motive in doing that was not to skew the election. My motive was that if this came out during the campaign, both candidates would be forced to pledge to protect civilian lives in Iraq. I was opposed to the war and I still think that the war was a bad idea, but I think that our science has transcended our perspectives.\"\n\n\n\"The Chronicle of Higher Education\" also wrote an article discussing the differences in the survey's reception in the popular press over how it was received in the scientific community.\n\nEpidemiologist Klim McPherson writes in the March 12, 2005 \"British Medical Journal\": \"The government rejected this survey and its estimates as unreliable; in part absurdly because statistical extrapolation from samples was thought invalid. Imprecise they are, but to a known extent. These are unique estimates from a dispassionate survey conducted in the most dangerous of epidemiological conditions. Hence the estimates, as far as they can go, are unlikely to be biased, even allowing for the reinstatement of Falluja. To confuse imprecision with bias is unjustified.\"\n\nA second study by some of the same authors was published in October, 2006, in \"The Lancet\".\n\nWe estimate that between March 18, 2003, and June, 2006, an additional 654,965 (392,979–942,636) Iraqis have died above what would have been expected on the basis of the pre-invasion crude mortality rate as a consequence of the coalition invasion. Of these deaths, we estimate that 601,027 (426,369–793,663) were due to violence.\n\nIf accurate, these figures would imply the death of an average 500 people per day, or 2.5% of Iraq's population during the period.\n\nAn October 11, 2006 \"Washington Post\" article reports:\n\nThe survey was conducted between May 20 and July 10 [2006] by eight Iraqi physicians organized through Mustansiriya University in Baghdad. They visited 1,849 randomly selected households that had an average of seven members each. One person in each household was asked about deaths in the 14 months before the invasion and in the period after. The interviewers asked for death certificates 87 percent of the time; when they did, more than 90 percent of households produced certificates.\n\nLancet: \n\nOnly 47 of the sought 50 clusters were included in this analysis. On two occasions, miscommunication resulted in clusters not being visited in Muthanna and Dahuk, and instead being included in other Governorates. In Wassit, insecurity caused the team to choose the next nearest population area, in accordance with the study protocol. Later it was discovered that this second site was actually across the boundary in Baghdad Governorate. These three misattributed clusters were therefore excluded, leaving a final sample of 1849 households in 47 randomly selected clusters.\n\nThe \"Lancet\" authors based their calculations on an overall, post-invasion, excess mortality rate of 7.8/1000/year. \"Pre-invasion mortality rates were 5.5 per 1000 people per year (95% CI 4.3–7.1), compared with 13.3 per 1000 people per year (10.9–16.1) in the 40 months post-invasion.\" See Table 3 in the \"Lancet\" article. The population number used in the calculation is reported in the \"Lancet\" supplement: \"Mortality projections were applied to the 2004 mid-year population estimates (26,112,353) of the surveyed areas (which exclude the governorates of Muthanna and Dahuk, which had been omitted through misattribution) to establish the mortality projections.\"\n\nOf 629 deaths verified and recorded among a sample of 1,849 households incorporating some 12,801 people at the time of the survey, 13% took place in the 14 months before the invasion and 87% in the 40 months afterwards. \"The study population at the beginning of the recall period (January 1, 2002) was calculated to be 11 956, and a total of 1474 births and 629 deaths were reported during the study period.\"\n\nThe study concluded that the mortality rate per 1,000 population per year in the pre-invasion period was 5.5 (range of 4.3-7.1, using a 95% CI, confidence interval) and in the post-invasion period was 13.3 (95% CI, 10.9-16.1). Excess mortality rate over the pre-invasion period was therefore 7.8 per 1,000 population per year, with violent death accounting for 92% of the increased mortality rate.\n\n\"Washington Post\": \"Gunshot wounds caused 56 percent of violent deaths, with car bombs and other explosions causing 14 percent, according to the survey results. Of the violent deaths that occurred after the invasion, 31 percent were caused by coalition forces or airstrikes, the respondents said.\"\n\nThe study results show an increasing mortality rate throughout the post-invasion periods, with the excess mortality rate for June 2005-June 2006 of 14.2 (95% CI, 8.6-21.5) being nearly 5.5 times the excess mortality rate for March 2003-April 2004 of 2.6 (95% CI, 0.6-4.7). The 2006 study also provides an estimate for the 18-month period following the invasion (March 2003 through September 2004) of 112,000 deaths (95% CI, 69,000-155,000). The authors conclude, \"Thus, the data presented here validates our 2004 study, which conservatively estimated an excess mortality of nearly 100,000 as of September, 2004.\"\n\nThe authors described the fact that their estimate is over ten times higher than other estimates, such as the Iraq Body Count project (IBC) estimate and U.S. Department of Defense estimates, as \"not unexpected\", stating that this is a common occurrence in conflict situations. They stated, \"Aside from Bosnia, we can find no conflict situation where passive surveillance recorded more than 20% of the deaths measured by population-based methods. In several outbreaks, disease and death recorded by facility-based methods underestimated events by a factor of ten or more when compared with population-based estimates. Between 1960 and 1990, newspaper accounts of political deaths in Guatemala correctly reported over 50% of deaths in years of low violence but less than 5% in years of highest violence.\"\n\nAn October 12, 2006 \"San Francisco Chronicle\" article reported:\n\n\"Six hundred thousand or whatever they guessed at is just, it's not credible,\" Bush said, and he dismissed the methodology as \"pretty well discredited.\" In December [2005], Bush estimated that 30,000 Iraqis had died in the war. Asked at the news conference what he thinks the number is now, Bush said: \"I stand by the figure a lot of innocent people have lost their life.\" At a separate Pentagon briefing, Gen. George Casey, the top U.S. commander in Iraq, said that the figure \"seems way, way beyond any number that I have seen. I've not seen a number higher than 50,000. And so I don't give it that much credibility at all.\"\n\nThe UK government, too, rejected the researchers' conclusions. In doing so, it did not mention the advice of the Ministry of Defence's Chief Scientific Adviser, Sir Roy Anderson, who had called the study \"robust\" and its claimed methods \"close to 'best practice' in this area, given the difficulties of data collection and verification in the present circumstances in Iraq\", in an internal memo on the day the study was published, dated 13 October 2006.\n\nThe Iraq Body Count project (IBC), who compiles a database of reported civilian deaths, has criticised the Lancet's estimate of 601,000 violent deaths out of the Lancet estimate of 654,965 total excess deaths related to the war. An October 2006 article by IBC argues that the \"Lancet\" estimate is suspect \"because of a very different conclusion reached by another random household survey, the Iraq Living Conditions Survey 2004 (ILCS), using a comparable method but a considerably better-distributed and much larger sample.\" IBC also enumerates several \"shocking implications\" which would be true if the \"Lancet\" report were accurate, e.g. \"Half a million death certificates were received by families which were never officially recorded as having been issued\" and claims that these \"extreme and improbable implications\" and \"utter failure of local or external agencies to notice and respond to a decimation of the adult male population in key urban areas\" are some of several reasons why they doubt the study's estimates. IBC states that these consequences would constitute \"extreme notions\". Later statements in a 2010 article by IBC say that the \"hugely exaggerated death toll figures\" from the 2006 Lancet report have \"been comprehensively discredited\" by recently published research.\n\nJon Pedersen of the Fafo Institute and research director for the ILCS survey, which estimated approximately 24,000 (95% CI 18,000-29,000) war-related deaths in Iraq up to April 2004, expressed reservations about the low pre-war mortality rate used in the Lancet study and about the ability of its authors to oversee the interviews properly as they were conducted throughout Iraq. Pedersen has been quoted saying he thinks the Lancet numbers are \"high, and probably way too high. I would accept something in the vicinity of 100,000 but 600,000 is too much.\"\n\nDebarati Guha-Sapir, director of the Centre for Research on the Epidemiology of Disasters in Brussels, was quoted in an interview for Nature.com saying that Burnham's team have published \"inflated\" numbers that \"discredit\" the process of estimating death counts. \"Why are they doing this?\" she asks. \"It's because of the elections.\". However, another interviewer a week later paints a more measured picture of her criticisms: \"She has some methodological concerns about the paper, including the use of local people — who might have opposed the occupation — as interviewers. She also points out that the result does not fit with any she has recorded in 15 years of studying conflict zones. Even in Darfur, where armed groups have wiped out whole villages, she says that researchers have not recorded the 500 violent deaths per day that the Johns Hopkins team estimates are occurring in Iraq. But overall Guha-Sapir says the paper contains the best data yet on the mortality rate in Iraq.\" A subsequent article co-authored by Guha-Sapir and Olivier Degomme for CRED reviews the \"Lancet\" data in detail. It concludes that \"The Lancet\" overestimated deaths and that the war-related death toll was most likely to be around 125,000 for the period covered by the \"Lancet\" study, reaching its conclusions by correcting errors in the 2006 \"Lancet\" estimate and triangulating with data from IBC and ILCS.\n\nBeth Osborne Daponte, a demographer known for producing death estimates for the first Gulf War, evaluates the Lancet survey and other sources in a paper for the International Review of the Red Cross. Among other criticisms, Daponte questions the reliability of pre-war estimates used in the Lancet study to derive its \"excess deaths\" estimate, and the ethical approval for the survey. She concludes that the most reliable information available to date is provided by the Iraq Family Health Survey, the Iraq Living Conditions Survey and Iraq Body Count.\n\nBorzou Daragahi Iraq correspondent for the \"Los Angeles Times\", in an interview with PBS, questioned the study based on their earlier research in Iraq, saying, \"Well, we think—the \"Los Angeles Times\" thinks these numbers are too large, depending on the extensive research we've done. Earlier this year, around June, the report was published at least in June, but the reporting was done over weeks earlier. We went to morgues, cemeteries, hospitals, health officials, and we gathered as many statistics as we could on the actual dead bodies, and the number we came up with around June was about at least 50,000. And that kind of jibed with some of the news report that were out there, the accumulation of news reports, in terms of the numbers killed. The U.N. says that there's about 3,000 a month being killed; that also fits in with our numbers and with morgue numbers. This number of 600,000 or more killed since the beginning of the war, it's way off our charts.\"\n\nThe October 2006 \"Lancet\" estimate also drew criticism from the Iraqi government. Government spokesman Ali Debbagh said, \"This figure, which in reality has no basis, is exaggerated\". Iraq's Health Minister Ali al-Shemari gave a similar view in November 2006: \"Since three and a half years, since the change of the Saddam regime, some people say we have 600,000 are killed. This is an exaggerated number. I think 150 is OK.\"\n\nA 2010 paper by Professor Michael Spagat entitled \"Ethical and Data-Integrity Problems in the Second Lancet Survey of Mortality in Iraq\" was published in the peer reviewed journal \"Defense & Peace Economics\". This paper argues that there were several \"ethical violations to the survey's respondents\", faults the study authors for \"non-disclosure of the survey's questionnaire, data-entry form, data matching anonymised interviewer identifications with households and sample design\", and presents \"evidence relating to data fabrication and falsification, which falls into nine broad categories.\" The paper concludes that the Lancet survey, \"cannot be considered a reliable or valid contribution towards knowledge about the extent of mortality in Iraq since 2003.\"\n\nOn February 3, 2009, the Executive Council of the American Association for Public Opinion Research (AAPOR) announced that an 8-month investigation found the author of the 2006 \"Lancet\" survey, Dr. Gilbert Burnham, had violated the Association's Code of Professional Ethics & Practices for repeatedly refusing to disclose essential facts about his research. \"Dr. Burnham provided only partial information and explicitly refused to provide complete information about the basic elements of his research,\" said Mary Losch, chair of the association's Standards Committee.\nAAPOR's President, Richard A. Kulka, added:\n\nWhen researchers draw important conclusions and make public statements and arguments based on survey research data, then subsequently refuse to answer even basic questions about how their research was conducted, this violates the fundamental standards of science, seriously undermines open public debate on critical issues, and undermines the credibility of all survey and public opinion research. These concerns have been at the foundation of AAPOR's standards and professional code throughout our history, and when these principles have clearly been violated, making the public aware of these violations is in integral part of our mission and values as a professional organization.\n\nAAPOR subsequently released a more detailed list of eight specific pieces of information Burnham failed to disclose after repeated requests. These include a copy of the survey questionnaire in all languages into which it was translated, the consent statement, information of sample selection methodology and a summary of the disposition of all sample cases.\n\nNeither Dr. Burnham nor the Johns Hopkins Bloomberg School of Public Health are members of AAPOR. Tim Parsons, public affairs director of the Bloomberg School wrote in an official statement that the school was \"not in a position to comment\" on AAPOR's findings because the school is not a member of the organization and \"does not know what procedures or standards were followed in reaching the decision regarding this study.\" Parsons also noted that the school was nearing completion of its own investigation into the study.\n\nAt least one article has been written critical of AAPOR's decision to censure Burnham. Debora MacKenzie, writing in \"New Scientist\", said \"There is no direct evidence that the latest attack on Burnham is politically motivated,\" but the APPOR's stated purpose, \"to ensure survey-based research meets high standards,\" has itself \"been questioned by experts.\" which MacKnenzie does not name.\n\nAccording to \"New Scientist's\" investigation... Burnham has sent his data and methods to other researchers, who found it sufficient. A spokesman for the Bloomberg School of Public Health at Johns Hopkins, where Burnham works, says the school advised him not to send his data to AAPOR, as the group has no authority to judge the research. The \"correct forum\", it says, is the scientific literature.\n\nAccording to MacKenzie, \"Burnham's complete data, including details of households, is available to bona fide researchers on request.\" She further noted that the AAPOR's own journal, \"Public Opinion Quarterly\", \"published an analysis of Burnham's Iraq survey by David Marker of Westat, a consultancy in Maryland that designs surveys.\"\n\nThe American Statistical Association has subsequently written in support of the actions taken by AAPOR, saying: \n\nWe are aware that, in taking this action, you have subjected yourselves to some criticism. On behalf of the American Statistical Association, we wish to recognize AAPOR for following procedure and acting professionally on such a difficult and divisive matter. In so doing, you eloquently express by your actions the goals stated in your Code.\n\nOn February 1, 2010, The Bloomberg School and Dr. Burnham were named for the \n\nSTONEWALLING/COVERUP\" award in iMediaEthics' 2010 Top Ten \"Dubious Polling\" Awards, based largely on the AAPOR censure. The authors David W. Moore and George F. Bishop, write that Bloomberg and Burnham received the award, \"for stonewalling in the face of serious questions about a flawed survey project, which reported more than 600,000 Iraqi deaths from 2003 to 2006,\" saying, \"AAPOR asked for the kind of information that any scientist doing this type of work should release ... The Bloomberg School will not attempt to evaluate what experts believe is almost certainly a faulty methodology, saying the scientific community should make the evaluation. But then the school advises Burnham not to release details about his methods, so the scientific community can't have the information it needs for a definitive assessment. Sounds like a cop-out and a Catch 22, all rolled into one!\n\nIn February 2009 Johns Hopkins Bloomberg School of Public Health published the results of an internal review of the study. The review found that researchers in the field used data collection forms that were different from those approved in the original protocol. The forms used in the field contained spaces for names of respondents or householders and many such names were collected, in violation of the protocol. The press release said the review did not find evidence that any individual was harmed as a result of these violations, and that no identifiable info was ever out of the possession of the researchers. As a result of their investigation, Hopkins suspended Dr. Burnham's privileges to serve as a principal investigator on projects involving human subjects research.\n\nThe press release also discussed an examination of all the original data collection forms:\n\nAn examination was conducted of all the original data collection forms, numbering over 1,800 forms, which included review by a translator. The original forms have the appearance of authenticity in variation of handwriting, language and manner of completion. The information contained on the forms was validated against the two numerical databases used in the study analyses. These numerical databases have been available to outside researchers and provided to them upon request since April 2007. Some minor, ordinary errors in transcription were detected, but they were not of variables that affected the study's primary mortality analysis or causes of death. The review concluded that the data files used in the study accurately reflect the information collected on the original field surveys.\n\nSteven E. Moore, who conducted survey research in Iraq for the Coalition Provisional Authority and was an advisor to Paul Bremer for the International Republican Institute, ridiculed the \"Lancet\" study in an October 18, 2006 editorial in the \"Wall Street Journal.\" In a piece entitled, \"655,000 War Dead? A bogus study on Iraq casualties\", Moore wrote, \"I wouldn't survey a junior high school, no less an entire country, using only 47 cluster points. Neither would anyone else...\"\n\nGilbert Burnham replied on October 20, 2006:\n\nMr. Moore did not question our methodology, but rather the number of clusters we used to develop a representative sample. Our study used 47 randomly selected clusters of 40 households each. In his critique, Mr. Moore did not note that our survey sample included 12,801 people living in 47 clusters, which is the equivalent to a survey of 3,700 randomly selected individuals. As a comparison, a 3,700-person survey is nearly 3 times larger than the average U.S. political survey that reports a margin of error of +/-3%.\n\nFred Kaplan, writing for \"Slate\", has criticized the pre-invasion death rate used in both the 2004 and 2006 Lancet surveys.\n\nIn an October 29, 2004 article in \"Slate\" he wrote:\n\nBut there are two problems with this calculation. First, Daponte (who has studied Iraqi population figures for many years) questions the finding that prewar mortality was 5 deaths per 1,000. According to quite comprehensive data collected by the United Nations, Iraq's mortality rate from 1980–85 was 8.1 per 1,000. From 1985–90, the years leading up to the 1991 Gulf War, the rate declined to 6.8 per 1,000. After '91, the numbers are murkier, but clearly they went up. Whatever they were in 2002, they were almost certainly higher than 5 per 1,000.\n\nSee also a related article about Beth Daponte:\n\nIn an October 20, 2006 \"Slate\" article Fred Kaplan wrote that the pre-invasion death rate calculated by the 2006 Lancet report authors was also too low. This he said would cause the \"Lancet\" estimate of excess deaths since the invasion to be too high. Fred Kaplan wrote:\n\nBased on the household surveys, the report estimates that, just before the war, Iraq's mortality rate was 5.5 per 1,000. (That is, for every 1,000 people, 5.5 die each year.) The results also show that, in the three and a half years since the war began, this rate has shot up to 13.3 per 1,000. So, the 'excess deaths' amount to 7.8 (13.3 minus 5.5) per 1,000. They extrapolate from this figure to reach their estimate of 655,000 deaths. However, according to data from the United Nations, based on surveys taken at the time, Iraq's preinvasion mortality rate was 10 per 1,000.\n\nIn a November 20, 2006 \"Slate\" article, 2 of the \"Lancet\" study authors, Gilbert Burnham and Les Roberts, write:\n\nKaplan claims that the rate was really 10, according to U.N. figures. He wrote, '[I]f Iraq's pre-invasion rate really was 5.5 per 1,000, it was lower than almost every country in the Middle East, and many countries in Europe.' This is just wrong! If Kaplan had checked the U.N. death-rate figures, most Middle Eastern nations really do have lower death rates than most European countries, and in fact have lower death rates than 5.5. Jordan's death rate is 4.2, Iran's 5.3, and Syria's 3.5. The reason for the lower rate is simple: Most Middle Eastern nations have much younger populations compared to most Western nations.\n\nFrom an October 19, 2006 Washington Post article there is this:\n\nIn a telephone interview, Jon Pedersen, research director for the 2004 [UNDP] study, said several factors probably account for researchers' different findings. One key issue is how researchers extrapolate from the deaths identified in their field research to a death toll for the whole country. Pedersen noted that the Lancet study is based on a pre-invasion mortality rate of 5.5 deaths per thousand people [per year]. The U.N., he said, used the figure of 9 deaths per thousand. Extrapolating from the lower pre-invasion mortality rate would yield a greater increase in post-invasion deaths, he noted.\n\nThe above-mentioned U.N. \"pre-invasion mortality rate\" of 9 deaths/1,000/year is more than either the 2002 or 2003 mortality rates measured by both Lancet studies.\n\nEven though the 2004 and 2006 Lancet studies interviewed different sets of households across Iraq, they came up with the same 2002 pre-war mortality rate. From the 2006 \"Lancet\" article: \"The striking similarity between the 2004 and 2006 estimates of pre-war mortality diminishes concerns about people's ability to recall deaths accurately over a 4-year period.\"\n\nHere is an excerpt from the supplement to the 2006 \"Lancet\" study:\n\nFor the purpose of analysis, the 40 months of survey data were divided into three equal periods—March 2003 to April 2004; May 2004 to May 2005, and June 2005 to June 2006. Following the invasion the death rate rose each year.\n\nThe difference between the pre-invasion mortality rate and the different mortality rates after the invasion are the excess mortality rates for each period. Table 3 in the \"Lancet\" article lists those rates as 2.6, 5.6, and 14.2. Why the excess mortality rate for June 2005 to June 2006 is listed as 14.2 instead of 14.3 may be due to how rounding was done. The overall excess mortality rate for the whole post-invasion survey period is listed as 7.8 deaths/1000/year in Table 3.\n\nThe difference between the \"Lancet\" and U.N. pre-invasion mortality rates is 3.5 deaths/1,000/year. The Lancet study used the number of 26,112,353 (from Lancet supplement) as the population of Iraq. 3.5 times 26,112 equals 91,392. So 3.5 deaths/1,000/year means around 91,400 deaths in one year in a population of 26.1 million.\n\nIn a March 5, 2007 article in \"The Times\", economist Michael Spagat says there is a perplexing finding in the 2006 Lancet report that child deaths have fallen.\n\nA May 25, 2000 BBC article reported that before Iraq sanctions were imposed by the UN in 1990, infant mortality had \"fallen to 47 per 1,000 live births between 1984 and 1989. This compares to approximately 7 per 1,000 in the UK.\" The BBC article was reporting from a study of the London School of Hygiene & Tropical Medicine, titled \"Sanctions and childhood mortality in Iraq\", that was published in the May 2000 \"Lancet\" medical journal.\n\nThe 2000 BBC article reported that after the UN sanctions were imposed after Iraq's 1990 invasion of Kuwait, \"They found that in south and central Iraq, infant mortality had risen to 108 per 1,000 between 1994 and 1999, while child mortality — covering those between one and five years — rocketed from 56 to 131 per 1,000.\"\n\nThe 2000 BBC article also reported, \"However, it found that infant and child mortality in the autonomous, mainly Kurd region in the North of the country, has actually fallen, perhaps reflecting the more favourable distribution of aid in that area.\"\n\nUN-sponsored studies taken after 2003 revealed that the previous childhood mortality figures for South/Central Iraq (supplied by Saddam's government) were inflated by more than a factor of two and that the childhood mortality rate in those regions was even lower than the rate in northern Iraq.\n\nThe UN sanctions ended on May 22, 2003 (with certain arms-related exceptions).\n\nMadelyn Hicks, a psychiatrist and public health researcher at King's College London in the U.K., says she \"simply cannot believe\" the paper's claim that 40 consecutive houses were surveyed in a single day. \"There is simply not enough time in the day,\" she says, \"so I have to conclude that something else is going on for at least some of these interviews.\" Households may have been \"prepared by someone, made ready for rapid reporting,\" she says, which \"raises the issue of bias being introduced.\"\n\nAn October 24, 2006 \"The Guardian\" article reports this response from \"Lancet\" study author Gilbert Burnham:\n\nOthers had suggested that it was impossible for 40 households to be surveyed in one day — but in fact the researchers were split into two teams and conducted 20 household interviews each, he said.\n\nAn October 30, 2006 BBC article reports this response from \"Lancet\" study author Les Roberts:\n\nIn Iraq in 2004, the surveys took about twice as long and it usually took a two-person team about three hours to interview a 30-house cluster. I remember one rural cluster that took about six hours and we got back after dark. Nonetheless, Dr. Hicks' concerns are not valid as many days one team interviewed two clusters in 2004.\n\nOf the 1849 households that completed the survey there were reports of 629 deaths during the study period from January 1, 2002 through June 2006.\n\nThe \"Lancet\" study claims that, \"Survey teams asked for death certificates in 545 (87%) reported deaths and these were present in 501 cases. The pattern of deaths in households without death certificates was no different from those with certificates.\"\n\nSo, 92% of those asked for death certificates produced them.\n\nIn an interview in April 2007 \"Lancet\" study author Les Roberts reported that, \"90 percent of the people we interviewed had death certificates. We're quite sure they didn't make these deaths up.\"\n\nThe Iraq Body Count project questioned the \"Lancet\" study's death certificate findings saying the \"Lancet\" study authors \"would imply that officials in Iraq have issued approximately 550,000 death certificates for violent deaths (92% of 601,000). Yet in June 2006, the total figure of post-war violent deaths known to the Iraqi Ministry of Health (MoH), combined with the Baghdad morgue, was approximately 50,000.\"\n\nThe August 2006 Basrah Governorate Assessment Report of the United Nations High Commissioner for Refugees described death certificate procedures of the Ministry of Health (MoH) as follows:\n\nDeath certificates, which are needed in order to obtain retirement benefits for a person's surviving spouse or children, as well as for inheritance purposes, are issued by the MoH Births/Deaths Administrative Offices which are located in Public Hospitals. Death certificates are usually issued the same day. The following documents are required:\n\nIn a November 20, 2006 \"Slate\" article, 2 of the \"Lancet\" study authors, Gilbert Burnham and Les Roberts, write:\n\nIn July [2006], for example, the Ministry of Health reported exactly zero violent deaths in Anbar Province, in spite of the contradictory evidence we saw on our televisions. Is that a surveillance network on which our understanding of what is going on in Iraq can depend?\n\nIn October 2006 Middle East Professor Juan Cole supported the \"Lancet\" findings, noting that Iraqis often bury their dead on the same day, and thus don't require a death certificate, and also may not report it for fear of reprisals by militias:\n\nAlthough there are benefits to registering with the government for a death certificate, there are also disadvantages. Many families who have had someone killed believe that the government or the Americans were involved, and will have wanted to avoid drawing further attention to themselves by filling out state forms and giving their address.\n\nIn a peer-reviewed paper on the Lancet survey, economist Michael Spagat examined the death certificate data. He noted that the very high reported rate of death certificates by the survey \"implies that the official death certificate system has issued, but failed to record the issuance of, about 500,000 death certificates\", and notes that the rate of confirmations claimed by the second survey is substantially higher than the rate found in the first survey, despite covering a longer period, and calculates the odds against this to be very high. Spagat further notes several \"unlikely patterns in the confirmations of violent deaths through the viewing of death certificates and in the patterns of when death certificates were requested and when they were not requested.\" His analysis concludes that \"there is likely fabrication in the death-certificate data\" and that \"these data do not give reliable support to [the Lancet survey's] very high estimated death rate.\"\n\nThe research team of Professors Neil Johnson, Sean Gourley and J.P. Onella of the physics department at Oxford University, Professor Michael Spagat of the economics department of Royal Holloway, University of London, and Professor Gesine Reinert of the statistics department at Oxford University, claimed the methodology of the study was fundamentally flawed by what they term \"main street bias\". They claimed the sampling methods used \"will result in an over-estimation of the death toll in Iraq\" because \"by sampling only cross streets which are more accessible, you get an over-estimation of deaths.\"\n\nThese professors have published a detailed paper discussing this bias and the Lancet study called \"Conflict Mortality Surveys\".\n\nAn October 24, 2006 \"The Guardian\" article reported this response from a \"Lancet\" study author:\n\nBut Prof Burnham said the researchers penetrated much further into residential areas than was clear from the \"Lancet\" paper. The notion 'that we avoided back alleys was totally untrue'. He added that 28% of households were in rural areas — which matches the population spread.\n\nAn article in \"Science\" magazine by John Bohannon describes some of the criticisms, as well as some responses from the Lancet report's lead author Gilbert Burnham. According to Bohannon and Johnson, the \"Lancet\" paper indicates that the survey team avoided small back alleys for safety reasons. But this could bias the data because deaths from car bombs, street-market explosions, and shootings from vehicles should be more likely on larger streets. Burnham counters that such streets were included and that the methods section of the published \"Lancet\" paper is oversimplified.\n\nBohannon also alleged that Burnham told \"Science\" that he does not know exactly how the Iraqi team conducted its survey; the details about neighborhoods surveyed were destroyed \"in case they fell into the wrong hands and could increase the risks to residents.\" These explanations have infuriated the study's critics. Michael Spagat, who specializes in civil conflicts, says the scientific community should call for an in-depth investigation into the researchers' procedures. \"It is almost a crime to let it go unchallenged,\" adds Johnson.\n\nIn a 24 November 2006 letter to \"Science\", the authors of the \"Lancet\" report claimed that Bohannon misquoted Burnham, stating that \"in no place does our \"Lancet\" paper say that the survey team avoided small back alleys\", and that \"The methods section of the paper was modified with the suggestions of peer reviewers and the editorial staff. At no time did Burnham describe it to Bohannon as 'oversimplified'.\"\n\nBohannon defended his comments as accurate, citing Burnham saying, in response to questions about why details of selecting \"residential streets that did not cross the main avenues\", that \"in trying to shorten the paper from its original very large size, this bit got chopped, unfortunately.\" In addition, the details which were destroyed refer to the \"scraps\" of paper on which streets and addresses were written to \"randomly\" choose households. The data set is now being selectively released.\n\nThe authors of the main street bias critique published a formal paper on this idea in the Journal of Peace Research. This paper subsequently won the journal's 2008 Article of the Year award. The jury states that the article \"provides an important advance in the methodology for estimating the number of casualties in civil wars,\" and that, \"the authors show convincingly that previous studies which are based on a cross-street cluster-sampling algorithm (CSSA) have significantly overestimated the number of casualties in Iraq.\"\n\nThe authors have also published a follow-up paper in \"Europhysics Letters\" which provides a generic framework than can be used to assess sampling bias in certain social and biological systems. A special case of the framework can be used to derive the results presented in their Journal of Peace Research paper. The authors also investigate the sensitivity of their results to the underlying model parameter values. They reiterate their view that a more precise determination of the model parameters and, hence, the extent of sampling bias, is possible only if the actual micro-level data of the \"Lancet\" study are released.\n\nFigure 4 from the October 2006 \"Lancet\" survey of Iraq War mortality, showing a comparison of 3 mortality estimates. Two letters subsequently published in the Lancet journal challenged this graph.\n\nThe purpose of the graph in the \"Lancet\" article is in \"monitoring trends over time,\" which show increased deaths from 3 mortality different mortality estimates. Results from other studies track results from the Lancet surveys. The graph states, \"the similar patterns of mortality over time documented in our survey and by other sources corroborate our findings about the trends in mortality over time.\" The graph shows that the IBC and DoD data document the rise in cumulative deaths over time (plotted along the \"Deaths\" axis on the left). Rates for the Lancet are plotted independently using the \"Deaths per 1,000 per year\" axis on the right.\n\nA letter by Debarati Guha-Sapir, Olivier Degomme and Jon Pedersen argues: \"Burnham and colleagues' figure 4, in which cumulated Iraq Body Count deaths parallel their study's mortality rates, is misleading. Rates cannot be compared with numbers, much less with cumulative numbers.\" A second letter by Josh Dougherty argues that the DoD figure is misrepresented: \"Burnham and colleagues' assertion that the DoD 'estimated the civilian casualty rate at 117 deaths per day' is mistaken, as is their figure 4, which repeats this error in graphic form. These data refer to Iraqi civilians and security-force personnel, not just to civilians, and to casualties (ie, deaths or injuries), not just deaths.\"\n\nThe \"Lancet\" authors replied, \"Josh Dougherty and Debarati Guha-Sapir and colleagues all point out that figure 4 of our report mixes rates and counts, creating a confusing image. We find this criticism valid and accept this as an error on our part. Moreover, Dougherty rightly points out that the data in the US Department of Defense source were casualties, not deaths alone... We wanted to show that the three sources all similarly pointed to an escalating conflict.\"\n\nIn a \"Democracy Now!\" interview, study co-author Les Roberts defended the methodology by noting that the method is the standard used in poor countries. He also said that the same method was used by the US government following wars in Kosovo and Afghanistan. Roberts also said that the US government's Smart Initiative program is spending millions of dollars per year teaching NGOs and UN workers how to use the same cluster method for estimating mortality rates.\n\nThe article's authors defended their research, claiming that their work was the only active study of the death toll, and that this is more accurate than passively counting reported deaths. They cited a number of factors that could lead to smaller figures from other sources; for example, the Islamic requirement that bodies be buried within 24 hours of death. They claim that the sources of bias in their study push the figure down.\n\nAn October 11, 2006 \"Washington Post\" article reports:\n\nIn a letter to \"The Age\", published on 21 October 2006, 27 epidemiologists and health professionals defended the methods of the study, writing that the study's \"methodology is sound and its conclusions should be taken seriously.\"\n\nA Reuters article reports on other researchers, epidemiologists, professors, and physicians who have defended the study. For example; this quote from the article;\n\n\"Over the last 25 years, this sort of methodology has been used more and more often, especially by relief agencies in times of emergency,\" said Dr. David Rush, a professor and epidemiologist at Tufts University in Boston.\n\nSir Richard Peto, Professor of Medical Statistics and Epidemiology in the University of Oxford, described the 2006 report as \"statistically valid\" in an interview on BBC television.\n\nDr. Ben Coghlan, an epidemiologist in Melbourne Australia, writes: \n\nThe US Congress should agree: in June this year [2006] they unanimously passed a bill outlining financial and political measures to promote relief, security and democracy in the Democratic Republic of Congo. The bill was based in part on the veracity of a survey conducted by the Burnet Institute (Melbourne) and the International Rescue Committee (New York) that found 3.9 million Congolese had perished because of the conflict. This survey used the same methodology as Burnham and his associates. It also passed the scrutiny of a UK parliamentary delegation and the European Union. \n\nBurnham is one of the authors of both of the \"Lancet\" studies.\n\nOctober 19, 2006 \"Washington Post\" article reports:\n\nA review of a variety of mortality estimates for Iraq was made by a group of scientists and published in 2008 in \"Conflict and Health\", a peer-reviewed journal; their conclusion is that the Lancet \"studies provided the most rigorous methodology as their primary outcome was mortality. \n\nUNDP ILCS stands for the 2004 United Nations Development Programme Iraq Living Conditions Survey\n\nThe Iraq Body Count project (IBC) records civilian deaths reported by English-language media, including all civilian deaths due to coalition military action, the insurgency or increased criminal violence. The IBC site states: \"it should be noted that many deaths will likely go unreported or unrecorded by officials and media.\"\n\nThe IBC death count at the time of the October 2006 \"Lancet\" study was released was between 43,546 and 48,343, or roughly 7% of the estimate in the \"Lancet\" study. Besides the admitted IBC undercount due to its media reliance, some of the difference between the Lancet and IBC estimates is explained by the fact that the Lancet study was estimating all \"excess\" deaths from any and all violent and nonviolent causes, and includes combatants and civilians alike.\n\nHowever, IBC believes some of it may also be explained by the \"Lancet\" having overestimated, citing the lower estimate from the UNDP's 2004 Iraq Living Conditions Survey (ILCS).\n\nIBC illustrated several of what it calls \"the main data that are relevant to a comparative assessment of\" the ILCS study and the 2004 \"Lancet\" study. It points to, for example, a much larger number of clusters (2,200 for ILCS vs. 33 for \"Lancet\"), and a more accurate sampling rate (1 in 200 for ILCS vs. 1 in 3,000 for Lancet). The 2006 \"Lancet\" study is somewhat larger than the first (it used 47 clusters instead of 33, and had a lower sampling rate). The 2004 Lancet study surveyed 988 households, and the 2006 \"Lancet\" study surveyed 1849 households. The ILCS study surveyed 22,000 households.\n\nLancet authors draw a different kind of comparison. From Appendix C of the 2006 Lancet study supplement there is this concerning the ILCS study:\n\nWorking for the U.N. Development Program [UNDP], the highly regarded Norwegian researcher Jon Pederson led a survey that recorded between 18,000 and 29,000 violent deaths during the first year of occupation. The survey was not focused on deaths, but asked about them over the course of lengthy interviews that focused on access to services. While this was more than twice the rate recorded by IBC [Iraq Body Count project] at the time, Pederson expressed concern for the completeness and quality of the data in a newspaper interview last year. The surveys reported in \"The Lancet\" were focused solely on recording deaths and count about two and a half times as many excess deaths from all causes over the same period.\n\nIn an October 30, 2006 BBC article \"Lancet\" study author Les Roberts compares the number of violent deaths found in the UNDP survey and in the 2 Lancet surveys through the first year after the invasion (by April 2004):\n\nThis UNDP survey covered about 13 months after the invasion. Our first survey recorded almost twice as many violent deaths from the 13th to the 18th months after the invasion as it did during the first 12. The second survey found an excess rate of 2.6/1000/year over the same period corresponding to approximately 70,000 deaths by April 2004. Thus, the rates of violent death recorded in the two survey groups are not so divergent.\n\nThe ILCS asked about deaths during the course of a lengthy interview on the household's living conditions. In the 3 main ILCS documents (in pdf form) all the war-related deaths info is in 6 paragraphs on page 54 of the analytical report. It states:\n\nThe ILCS data has been derived from a question posed to households concerning missing and dead persons during the two years prior to the survey. Although the date was not asked for, it is reasonable to suppose that the vast majority of deaths due to warfare occurred after the beginning of 2003.\n\nBesides the comparisons made in various publications, and in previous sections here, there are also more comparisons and criticisms of both studies in the relevant sections of the above-linked articles. In particular see the \"Undercounting\" section at Casualties of the conflict in Iraq since 2003 which lists many examples of how the media, hospitals, morgues, government, etc. miss some of the deaths caused by the war.\n\nOn September 14, 2007, ORB (Opinion Research Business), an independent UK based polling agency, published an estimate of the total casualties of the Iraq war. The figure suggested by ORB, which was based on survey responses from 1,499 adults, stands at 1,220,580 deaths, with a margin of error of 2.5%. This estimate, although conducted independently, and using a different polling methodology, is consistent with the Lancet findings if accounting for the additional 14 months covered by the ORB poll.\n\nOn 28 January 2008, ORB published an update based on additional work carried out in rural areas of Iraq. Some 600 additional interviews were undertaken and as a result of this the death estimate was revised to 1,033,000 with a given range of 946,000 to 1,120,000.\n\nThis ORB poll estimate came under criticism in a peer reviewed paper called \"Conflict Deaths in Iraq: A Methodological Critique of the ORB Survey Estimate\", published in the journal Survey Research Methods. This paper \"finds fundamental flaws in the data underpinning ORB's estimate\", and concludes that the ORB data \"are not suitable for deriving any credible estimate but, given proper scrutiny, it is clear that ORB has overestimated by a wide margin.\n\nThe \"Iraq Family Health Survey\" published in the \"New England Journal of Medicine\" surveyed 9,345 households across Iraq and estimated 151,000 deaths due to violence (95% uncertainty range, 104,000 to 223,000) over the same period covered in the second \"Lancet\" survey by Burnham et al. The NEJM article stated that the second Lancet survey \"considerably overestimated the number of violent deaths and said the \"Lancet\" results were, \"highly improbable, given the internal and external consistency of the data and the much larger sample size and quality-control measures taken in the implementation of the IFHS.\"\n\nThe figures provided by this survey on the total violent deaths in Iraq, are lower than \"Lancet\"s estimate by a factor of roughly 4. However, despite the differences, \"Lancet\" co-author Les Roberts said there were a few underlying similarities as well, such as a doubling of mortality rate after the invasion of Iraq in the study, compared to the 2.4-fold increase reported by \"Lancet\". It has been estimated by Roberts that the \"excess death\" toll in the IFHS survey would be about 400,000, which he says, puts these figures in league with \"Lancet\"'s. Roberts says the discrepancy between the two studies arise with \"Lancet\" attributing most of the post-war excess deaths to violence, while only one-third of the excess deaths would be due to violence in the IFHS. See: Iraq Family Health Survey#400,000\n\nThe authors of the IFHS report have disputed this conclusion, saying, \"The excess deaths reported by Burnham \"et al.\" included only 8.2% of deaths from nonviolent causes, so inclusion of these deaths will not increase the agreement between the estimates from the IFHS and Burnham \"et al.\"\" They defended the results of their survey saying, \"It is unlikely that a small survey with only 47 clusters has provided a more accurate estimate of violence-related mortality than a much larger survey sampling of 971 clusters.\"\n\n\n\n"}
{"id": "36190823", "url": "https://en.wikipedia.org/wiki?curid=36190823", "title": "List of United Nations resolutions concerning Syria", "text": "List of United Nations resolutions concerning Syria\n\nThe United Nations resolutions concerning Syria have mainly dealt with the Arab–Israeli conflict, Syrian occupation of Lebanon and the Syrian Civil War.\n\n"}
{"id": "32337267", "url": "https://en.wikipedia.org/wiki?curid=32337267", "title": "List of countries by future population (United Nations, low fertility variant)", "text": "List of countries by future population (United Nations, low fertility variant)\n\nThis is a List of countries by future population using the low variant, ranging from 2020 to 2100 in decades or ten-year periods, as estimated by the 2015 revision of the World Population Prospects database by the United Nations Population Division. All figures are rounded and given in thousands.\n\n\n"}
{"id": "42870895", "url": "https://en.wikipedia.org/wiki?curid=42870895", "title": "List of countries by quality of healthcare", "text": "List of countries by quality of healthcare\n\nThis is a list of countries by quality of healthcare as published by the Organisation for Economic Co-operation and Development (OECD).\n\nThe 5-year observed survival rate refers to the percentage of patients who live at least 5 years after being diagnosed with cancer. Many of these patients live much longer than 5 years after diagnosis.\n\nColorectal cancer 5-year survival rate\n\nBreast cancer 5-year survival rate\nCervical cancer 5-year survival rate\n\nHeart attack 30 day in-hospital mortality per 100 hospital discharges\nHemorrhagic stroke 30 day in-hospital mortality per 100 hospital discharges\nIschemic stroke 30 day in-hospital mortality per 100 hospital discharges\n\n\n2. http://www.businessinsider.com/best-healthcare-systems-in-the-world-2012-6?op=1\n"}
{"id": "4289981", "url": "https://en.wikipedia.org/wiki?curid=4289981", "title": "Mymensingh Medical College", "text": "Mymensingh Medical College\n\nMymensingh Medical College () is a government medical school in Bangladesh, established in 1924. It is located in the Mymensingh District of the Mymensingh Division. \"Mymensingh Medical Journal\", which is Index Medicus/MEDLINE listed, is the official journal of Mymensingh Medical College. Brigadier General Md. Nasir Uddin Ahmed is the present director of the medical college hospital.\n\nDuring the later part of British India, it was felt that the people of the northeastern region needed the services of qualified medical personnel. The then Campbell Medical School of Calcutta and Mitford Medical School of Dhaka were unable to cater the needs of the growing population. With this background the third medical school for the region, was established in 1924 in Mymensingh, by the Earl of Lytton, the then governor of Bengal. \"The Lytton Medical School\" was to run a four-year course of Licentiate of Medical Faculty (LMF). This course of LMF continued till 1962, when it was upgraded to a five-year undergraduate medical course under Dhaka University and the school was renamed as \"Mymensingh Medical College\".\n\nThis is the 2nd established Hospital in Bangladesh.\n\n\n\n"}
{"id": "7785210", "url": "https://en.wikipedia.org/wiki?curid=7785210", "title": "Myron Sharaf", "text": "Myron Sharaf\n\nMyron Russcol Sharaf (July 7, 1926 – May 13, 1997) was an American writer and psychotherapist. He was a lecturer in psychiatry at Harvard Medical School, the director of the Center for Sociopsychological Research and Education at Boston State Hospital, and assistant clinical professor of psychology in the Department of Psychiatry at Tufts University School of Medicine.\n\nSharaf was a student, patient, and colleague of Wilhelm Reich's from 1948 to 1954, and the author of what is widely regarded as the definitive biography of Reich, \"Fury On Earth\" (1983). He died of a heart attack in Berlin in 1997, after addressing a conference in Vienna marking Reich's centennial.\n\nSharaf was born in Miami, but grew up in Brookline, Massachusetts, the son of Nathan Sharaf and Anne Russcol Sharaf. His father founded the Steaming Kettle Coffee Shop chain. His paternal great-grandparents,\noriginally named Sharafsky, were Jewish emigrants from the Russian Empire. He obtained his first degree in psychology from Harvard College in 1949, an M.Ed. from Tufts University in 1953, and a Ph.D. in psychology and education from Harvard University in 1960.\n\nA \"New York Times\" review of \"Fury on Earth: A Biography of Wilhelm Reich\" describes Sharaf as \"intimate for more than 10 years as student, disciple, patient and colleague\" of Reich. Paul Roazen wrote in \"The Psychoanalytic Review\", \"Myron Sharaf's \"Fury on Earth\" is far and away the finest book both on Reich's work and his life. It is a work of scholarship that may well, until the Reich Archives are finally opened, remain definitive on the subject.\"\n\n\n\n"}
{"id": "44002623", "url": "https://en.wikipedia.org/wiki?curid=44002623", "title": "NPIW Sindh", "text": "NPIW Sindh\n\nNational Programme for Improvement of Watercourses (NPIW) is project of construction of watercourses in Pakistan to solve the problem of scarcity of irrigation water.This project is based on different conservation strategies. The programme was launched simultaneously in all the provinces of the Pakistan in 2004. The programme aims at lining of 33000 watercourses in Sindh province.It is the biggest engineering project in which more than 2300 engineers are employed.\n"}
{"id": "6144956", "url": "https://en.wikipedia.org/wiki?curid=6144956", "title": "Ontario Clean Water Agency", "text": "Ontario Clean Water Agency\n\nThe Ontario Clean Water Agency (OCWA) is a Crown agency of the Province of Ontario that provides operation, maintenance and management services for more than 450 water and wastewater treatment facilities in the province.\n\nOCWA was created in 1993 by the NDP government of Premier Bob Rae under the Ontario \"Capital Investment Plan Act\" and initially took over provincial ownership of 153 water-treatment plants and 77 sewage-treatment facilities. It also operated 116 municipally owned water and sewage facilities.\n\nBy 1996, it had 800 employees and held contracts to operate 429 facilities in the province, comprising 25 per cent of Ontario's water-treatment plants and 57 per cent of the wastewater-treatment plants. In October of that year, the Progressive Conservative government under Premier Mike Harris, which came to power in 1995, announced its plans to turn ownership of the facilities over to the municipalities and privatize OCWA as an environmental consulting firm.\n\nOwnership of the facilities was transferred to the municipalities, but OCWA was not sold. It was transformed into a management services organization and in 1998 won what was then Canada's largest water and wastewater operations and maintenance contract—a 10-year, $213 million deal to operate the South Peel system. By 2000, OCWA operated and maintained more than 300 municipally-owned water and sewage treatment facilities on behalf of about 200 Ontario municipalities.\n\nIn May 2000, an outbreak of \"E. coli\" contamination occurred in the water system of Walkerton, Ontario. In the aftermath of the disaster and the ensuing reforms to water treatment, the OCWA was put in charge of the cleanup of the water supply system of the town. This included a complete flushing of all the pipes in Walkerton, including those located in every building in the town.\n\n\n"}
{"id": "2106011", "url": "https://en.wikipedia.org/wiki?curid=2106011", "title": "Pathosystem", "text": "Pathosystem\n\nA pathosystem is a subsystem of an ecosystem and is defined by the phenomenon of parasitism. A plant pathosystem is one in which the host species is a plant. The parasite is any species in which the individual spends a significant part of its lifespan inhabiting one host individual and obtaining nutrients from it. The parasite may thus be an insect, mite, nematode, parasitic Angiosperm, fungus, bacterium, mycoplasma, virus or viroid. Other consumers, however, such as mammalian and avian herbivores, which graze populations of plants, are normally considered to be outside the conceptual boundaries of the plant pathosystem.\n\nA host has the property of resistance to a parasite. And a parasite has the property of parasitic ability on a host. Parasitism is the interaction of these two properties. The main feature of the pathosystem concept is that it concerns parasitism, and it is not concerned with the study of either the host or parasite on its own. Another feature of the pathosystem concept is that the parasitism is studied in terms of populations, at the higher levels and in ecologic aspects of the system. The pathosystem concept is also multidisciplinary. It brings together various crop science disciplines such as entomology, nematology, plant pathology, and plant breeding. It also applies to wild populations and to agricultural, horticultural, and forest crops, and to tropical, subtropical, as well as both subsistence and commercial farming.\n\nIn a wild plant pathosystem, both the host and the parasite populations exhibit genetic diversity and genetic flexibility. Conversely, in a crop pathosystem, the host population normally exhibits genetic uniformity and genetic inflexibility (i.e., clones, pure lines, hybrid varieties), and the parasite population assumes a comparable uniformity. This distinction means that a wild pathosystem can respond to selection pressures, but that a crop pathosystem does not. It also means that a system of locking (see below) can function in a wild plant pathosystem but not in a crop pathosystem.\n\nPathosystem balance means that the parasite does not endanger the survival of the host; and that the resistance in the host does not endanger the survival of the parasite. This is self-evident from the evolutionary survival of wild plant pathosystems, as systems, during periods of geological time.\n\nThe gene-for-gene relationship is an approximate botanical equivalent of antigens and antibodies in mammals. For each resistance gene in the host, there is a corresponding, or matching, gene in the parasite. When the genes of the parasite match those of the host, the resistance does not operate.\n\nThere are two kinds of resistance to parasites in plants:\n\n\nInfection is the contact made by one parasite individual with one host individual for the purposes of parasitism. There are two kinds of infection:\n\n\nAn epidemic is the growth of a parasite population which is made at the expense of the host population. There are two kinds of epidemic:\n\n\nThe n/2 model (pronounced either ‘en over two’ or 'half en') suggests the mode of operation of the gene-for-gene relationship in a wild plant pathosystem. It apparently functions as a system of locking in which every host and parasite individual has half of the genes in the gene-for-gene relationship (i.e., n/2 genes, where n is the total number of pairs of genes in that relationship). Each gene in the host is the equivalent of a tumbler in a mechanical lock, and each gene in the parasite is the equivalent of a notch on a mechanical key. Provided that each n/2 combination of genes occurs with an equal frequency, and with a random distribution, in both the host and parasite populations, the frequency of matching allo-infections will be reduced to the minimum. For example, with six pairs of genes, each host and parasite individual would have three genes, and there would be twenty different locks and keys; with a twelve-gene system, there would be 924 six-gene locks and keys. Given an equal frequency and a random distribution of every lock and key, the frequency of matching allo-infection would be 1/20 and 1/924, respectively. These figures are obtained from the binomial expansion illustrated by Pascal's triangle.\n\nThis system of locking cannot function in a crop pathosystem in which the host population has genetic uniformity. A crop pathosystem is usually the equivalent of every door in the town having the same lock, and every householder having the same key which fits every lock. A system of locking is ruined by uniformity, and this is exactly what we have achieved when protecting our genetically uniform crops with vertical resistance. It also explains why vertical resistance is temporary resistance in agriculture. This type of error is called sub-optimization and it results from working at too low a systems level. The system of locking is an emergent property that is observable only at the systems level of the pathosystem. Comparable biological emergents are the schooling of fish, and the flocking of birds, which cannot be observed at any systems level below that of the population. The n/2 model is also the most important hypothesis to emanate from the concept of the pathosystem. It can also be argued that the gene-for-gene relationship must function on a basis of heterogeneity in the wild pathosystem because the gross instability of the 'boom and bust' of modern plant breeding would have no evolutionary survival value.\n\nA gene-for-gene relationship can evolve only in a discontinuous pathosystem. This is because it functions as a system of locking. A matching allo-infection is the equivalent of a lock being unlocked. With the end of the season, all matched (i.e., unlocked) host tissues disappear. With the onset of a new growing season, all discontinuous host tissue (e.g., new leaves of a deciduous tree, newly germinated annual seedlings, or newly emerged tissue of a perennial herb) is unmatched and each host individual has a vertical resistance that is functioning. This is the equivalent of re-locking. This alternation of matching and non-matching (or unlocking and re-locking) is an essential feature of any system of locking, and it is possible only in a discontinuous pathosystem. Conversely, in a continuous pathosystem just one matching allo-infection on each host individual is required for that individual to be parasitised for the rest of its life which, in the case of some evergreen trees, may endure for centuries. A gene-for-gene relationship is useless in such a pathosystem and, consequently, it will not evolve.\n\nCrops that are derived from a continuous wild pathosystem (e.g., aroids, banana, cassava, citrus, cocoa, coconut, date palm, ginger, mango, oil palm, olive, papaya, pineapple, pyrethrum, sisal, sugarcane, sweet potato, tea, turmeric, vanilla, yams) have no gene-for-gene relationships, not withstanding a few erroneous reports to the contrary.\n\nHorizontal resistance is the resistance that invariably remains after a matching allo-infection has occurred. To postulate that horizontal resistance does not occur would be to postulate an absolute susceptibility. Such a level of susceptibility is experimentally unproved, and is theoretically impossible. Horizontal resistance is polygenically inherited and it can be exhibited at any level between its minimum and its maximum. Its maximum level should provide a virtually complete control of a parasite under conditions of maximum epidemiological competence. Breeding for comprehensive horizontal resistance will require simultaneous quantitative improvements and will eventually control all the parasites that have epidemiological competence in a particular agro-ecosystem. (5). However, because epidemiological competence is so variable, a cultivar that is in balance with one agro-ecosystem, is likely to be unbalanced in another agro-ecosystem, having too much resistance to some parasites and too little to others.\n\nOf particular importance is the concept of parasite interference, first defined by Vanderplank, who called it the cryptic error in field trials. Parasite interference does not affect the demonstration of vertical resistance, but it can totally destroy the evidence for high levels of horizontal resistance. This factor, which has only recently been recognised, largely explains the almost total neglect of horizontal resistance during the twentieth century.\n\nThe greater the area of a uniform host population with a single vertical resistance, the more dangerous that resistance becomes. This is because of an increased selection pressure for the matching parasite, and an increased loss when the matching does occur. The greater the area of uniformity of vertical resistance, therefore, the greater the danger. Conversely, the greater the area of a uniform host population with high horizontal resistances, the more effective the horizontal resistance becomes. This is because parasite interference declines as the area of a horizontally resistant host population increases, and it is least when the entire crop of a region has a high level of horizontal resistance in all of its cultivars. The greater the area of uniformity of horizontal resistance, therefore, the greater the security.\n\nIn breeding crop plants for horizontal resistance to their parasites, the disciplines of plant breeding, plant pathology, and crop entomology should be regarded as being amalgamated into a single discipline.\n"}
{"id": "14126826", "url": "https://en.wikipedia.org/wiki?curid=14126826", "title": "Pediatric plastic surgery", "text": "Pediatric plastic surgery\n\nPediatric plastic surgery is plastic surgery performed on children. Its procedures are most often conducted for reconstructive or cosmetic purposes. In children, this line is often blurred, as many congenital deformities impair physical function as well as aesthetics.\n\nSurgery is defined as treating injuries or conditions with operative instrumental treatment. Plastic is a derivative of the Greek word \"plastikos\", which means \"to build up\" or \"to take form\". This is a logical prefix, as parts of the body are remade or reformed during most reconstructive and cosmetic surgical procedures. Children make up roughly 3% of all plastic surgery procedures, and the majority of these procedures correct a congenital deformity.\n\nReconstructive plastic surgery is performed on abnormal structures of the body that are the result of congenital defects, developmental abnormalities, trauma, infection, tumors or disease. While reconstructive surgery is most often undertaken to regain normal motor function or prevent current or future health problems, aesthetics is also considered by the surgical team.\n\nCosmetic plastic surgery is defined as a surgical procedure undertaken to improve the physical appearance and self-esteem of a patient. These procedures are usually elective.\n\nSeveral of the most common congenital birth defects can be treated by a plastic surgeon operating as an individual, or as a part of a multi-disciplinary team. The most common pediatric birth defects requiring plastic surgeon involvement include:\n\nWhile the majority of pediatric plastic surgery procedures done are reconstructive; there are those performed for cosmetic purposes. The most common procedures done for cosmetic benefit in children include:\n\nOut of all procedures, nose reshaping generally has the most cases on an annual basis (4,313 procedures in 1996). However, children make up only 9% of the total caseload for all nose reshaping. On the opposite end of the spectrum, children requiring ear surgery accounted for 2,470 procedures in 1996, a total of 34% of all total ear surgeries.\nWhile many of these procedures are done for purely cosmetic benefit, many plastic surgeons work on these features (giving them a more normal appearance), while performing a surgery to improve function as the result of a congenital deformity.\n\nWith the unique challenges created in the field of plastic surgery, an increasingly popular trend has been to utilize the multi-disciplinary team approach in treatment.\n\nCommon conditions involving team treatment include:\n"}
{"id": "3991823", "url": "https://en.wikipedia.org/wiki?curid=3991823", "title": "Person-centred planning", "text": "Person-centred planning\n\nPerson-centred planning (PCP) is a set of approaches designed to assist an individual to plan their life and supports. It is most often used for life planning with people with learning and developmental disabilities, though recently it has been advocated as a method of planning personalised support with many other sections of society who find themselves disempowered by traditional methods of service delivery, including children, people with physical disabilities, people with mental health issues and older people. PCP is accepted as evidence based practice in many countries throughout the world.\n\nPerson-centred planning was adopted as government social policy in the United Kingdom through the 'Valuing People' White Paper in 2001, and as part of 'Valuing People Now', a 3-year plan, in 2009. It is promoted as a key method for delivering the personalisation objectives of the UK government's Putting People First programme for social care. The coalition government continued this commitment through 'Capable Communities and Active Citizens' (2010), and in 2011 over 30 health and social care organisations set up a sector-wide agreement 'Think Local, Act Personal' (2011) to transform adult social care.\n\n\"Person Centred Planning discovers and acts on what is important to a person. It is a process for continual listening and learning, focussing on what are important to someone now and in the future, and acting on this in alliance with their family and their friends\"\nPerson-centred planning was created in response to some specific problems with the way in which society responds to people with disabilities. Those who first described the processes were responding to the effects that 'services' can have on people's lives. In this context 'services' refers to the organisations which are set up to help people in relation to their disability (or at least in relation to how other people have responded to that disability). It would include health and social care services funded by government or local authorities, but also privately funded or voluntary sector projects of many kinds.\n\nPerson-centered planning has similarities to other processes and ideas, but was first named and described more definitely by a group of people in the US, including the Center on Human Policy's Rehabilitation Research and Training Center (RRTC) on Community Integration e.g., Julie Ann Racino, Zana Lutfiyya, Steve Taylor, John O'Brien, Beth Mount, Connie Lyle O'Brien, technical assistance \"partners\" of the RRTC (e.g., Michael Smull, Wade Hitzing, Karen Green-McGowen, Nick Arambarri) and person-centred planning in Canada by Jack Pearpoint, Judith Snow and Marsha Forest. Whilst it was developed because of the social and service response to disability, it was quickly recognised to be as useful for many other individuals and groups of people.\n\nDisabled people in the UK and USA developed the Social model of disability, arguing for a shift in the balance of power between people and the services on which they rely. Person centred planning is based in the social model of disability because it places the emphasis on transforming the options available to the person, rather than on 'fixing' or changing the person. Specifically person-centred planning was based diversely on principles of community integration/inclusion/ normalisation/social role valorization. Prior to its inception, these principles were crystallised by John O'Brien and Connie Lyle O'Brien in the 'Framework for Accomplishment' which listed five key areas important in shaping people's quality of life, and asserting that services should be judged by the extent to which they enable people to:\n\nThe title 'person-centred' is used because those who developed it and used it initially shared a belief that services tend to work in a 'service-centred' way. This 'service-centred' behaviour appears in many forms, but an example is that a person who is isolated would be offered different groups to attend (each run by a service specifically for people sharing a specific label), rather than being helped to make friends in ordinary society.\n\nThe person-centered concept grew out of the critique of the \"facility-based services\" approach in the US (and worldwide) that was central to the development of \"support approaches\" in the US The nationwide technical assistance funded by the National Institute on Disability Research and Rehabilitation (NIDRR), which included the person-centered approaches, is reported in the \"Journal of Vocational Rehabilitation\"\n\nA central idea behind person-centred planning, is that services which are set up to respond to problems of social exclusion, disempowerment, and devaluation, can unintentionally make the situation of individual people worse (i.e. further disempower, devalue and exclude people). Person-centred planning is designed specifically to 'empower' people, to directly support their social inclusion, and to directly challenge their devaluation. One of the benefits of person-centered planning is that it can address the perennial \"service problems\" of ethnicity, gender, culture and age by starting with planning by or with the \"whole person\".\n\nPerson-centred planning is not one clearly defined process, but a range of processes sharing a general philosophical background, and aiming at similar outcomes. As it has become more well known further processes and procedures have also been given the title 'person-centred planning'. Some of these have little in common with person-centred planning as originally envisaged. Person-centered planning through the Rehabilitation Research and Training Center on Community Integration in the US was, in part, an agency and systems change process as opposed to only an \"individual planning\" process moving to an \"individual budgeting process\"\n\nPerson-centred planning involves the individual receiving the service, with family members, neighbors, employers, community members, and friends, and professionals (such as physician/ doctors, psychiatrists, nurses, support workers, care managers, therapists, and social workers) developing a plan on community participation and quality of life with the individual. In contrast, traditional models of planning have focussed on the person's deficits and negative behaviours, labelling the person and creating a disempowering mindset from the start.\n\nPerson-centred planning offers an alternative to traditional models, striving to place the individual at the centre of decision-making, treating family members as partners. The process focusses on discovering the person's gifts, skills and capacities, and on listening for what is really important to the person (e.g., Snow, O'Brien & Mount). It is based on the values of human rights, interdependence, choice and social inclusion, and can be designed to enable people to direct their own services and supports, in a personalised way.\n\nPerson-centered planning utilises a number of techniques, with the central premise that any methods used must be reflective of the individual's personal communication mechanisms and assist them to outline their needs, wishes and goals. There is no differentiation between the process used and the output and outcomes of the PCP; instead, it pursues social inclusion through means such as community participation, employment and recreation.\nBeth Mount characterised the key similarities or 'family resemblances' of the different person centred methods and approaches into four themes:\n\nPerson centred thinking skills, total communication techniques, graphic facilitation of meetings and problem solving skills are some methods commonly used in the development of a person centred plan, as are PATH (Planning Alternative Tomorrows With Hope), circles of support (Canada), MAPS (Canada), personal futures planning (O'Brien & Mount, US), Essential Lifestyle Planning (Maryland, US), person centred reviews, Getting to Know You (Wisconsin, USA), and most recently the use of Person centred thinking tools to build from one page profiles into person centred descriptions/collections of person centred Information and on into full scale plans.\n\nThe resultant plan may be in any format that is accessible to the individual, such as a document, a drawing or an oral plan recorded onto a tape or compact disc. Multimedia techniques are becoming more popular for this type of planning as development costs decrease and the technology used becomes more readily available. Plans are updated as and when the individual wishes to make changes, or when a goal or aspiration is achieved. If part of a regular planning process in the US, regular plan updates are usually required by regulatory agencies (e.g., state offices in the USA through local agencies).\n\nPerson-centred planning can have many effects that go beyond the making of plans. It can create a space during which someone who is not usually listened to has central stage. It can insist that discussion is centred on what the person is telling us is important to them, with their words and behaviours, as well as what others feel is important for the person. It can engage participants personally by allowing them to hear of deeply felt hopes and fears. It can assist people in a circle of support to re-frame their views of the person it is focused on. It can help a group to solve difficult problems. In the US, person-centered planning can help to create new lifestyles, new homes and jobs, diverse kinds of support (informal and formal)and new social relationships.\n\nMany of the limitations discussed below reflect challenges and limitations in the implementation of Person-Centered Planning approaches in the context of formal human service systems.\n\nAnother approach to this question is to envision Person-Centered Planning as an approach that is anchored in the person's \"natural community\" and \"personal relationship network\". In this view, the Person-Centered Plan (PCP) offers a platform for the person and their trusted allies to identify and express their vision and commitments without limiting that expression to what can or will be provided by the service system.\n\nSome time later, the formal system can develop a plan for service delivery that may be based on and consistent with the person's plan, that recognizes and supports the contributions of the person, family and community, and that clearly acknowledges the limitations of what the system is prepared to provide.\n\nJohn O'Brien sums up the problem of trying to deliver person centredness through formal service systems that have a very different culture thus:\nMany human service settings are zones of compliance in which relationships are subordinated to and constrained by complex and detailed rules. In those environments, unless staff commit themselves to be people's allies and treat the rules and boundaries and structures as constraints to be creatively engaged as opposed to simply conforming, person centred work will be limited to improving the conditions of people's confinement in services. He calls for leadership to challenge these boundaries: Most service organisations have the social function of putting people to sleep, keeping them from seeing the social reality that faces people with disabilities...People go to sleep when the slogan that \"we are doing the best that is possible for 'them'\" distracts from noticing and taking responsibility for the uncountable losses imposed by service activities that keep people idle, disconnected and alienated from their own purposes in life. One way to understand leadership is to see it as waking up to people's capacities and the organisational and systemic practices that devalue and demean those capacities.\n\nA key obstacle to people achieving better lives has been the risk averse culture that has been prevalent in human services for a variety of reasons. Advocates of person centred thinking argue that applying person centred thinking tools to the risk decision making process, and finding strategies that are based on who the person is, can enable a more positive approach to risk that doesn't use risk as an excuse to trap people in boring and unproductive lives.\n\nThe key advocates of PCP and associated Person Centered Approaches warn of the danger of adopting the model in a bureaucratic way – adopting the 'form' of PCP, without the philosophical content. By changing it to fit existing practices rather than using it in its original form, most or all of its effects are lost. The hope of funding it in the USA was to influence the processes, such as planning through the Medicaid home and community-based waiver services for people moving from institutions to the community.\n\nThe philosophical content expects services to be responsive to the needs of people that use the service, rather than prescriptive in the types of services offered. These principles are reliant on mechanisms such as individualised funding packages and the organisational capacity to design and deliver \"support\" services. It is essential that organisations and agencies providing services make a commitment to strive for person-centredness in all of their activities, which can result in major changes in areas of practice such as recruitment, staff training, and business planning and management.\n\nWhile secondary users may debate the use of person-centered approaches to achieve the myriad goals it attempts to achieve, i.e., increased inclusion (Schwartz, Jacobson and Holburn, 2000) and \"Defining Person-centeredness\", others point to recent research such as \"The Impact of Person Centred Planning\", which suggests that Person Centred Planning can make a considerable difference to people's quality of life and explores the optimum conditions for Person Centred Approaches. 'Valuing People Now' says\n\n\"Person centred planning has been shown to work. The world's largest study into person centred planning described how it helps people get improvements in important parts of their lives and indicated that this was at no additional cost\"\n\nHowever it continues: \n\n\"too few people have access to proper person centred planning... In too many local authorities, person centred planning is not at the centre of how things are done. The challenge of the next three years is to take all this innovative work and make sure that more – and eventually all – people have real choice and control over their lives and services\"\n\nPerson-centered planning in the USA has continued to be investigated at the secondary research level and validated for more general use (e.g., ).\n\nLocal Authorities in Britain are now being challenged by government to change their model to one that is founded on Person Centred Approaches\n\nIn New York State (USA), the Office for People with Developmental Disabilities (OPWDD), has mandated the use of person-centered planning in all new service development for people with intellectual disabilities. Person-centered planning is central to the new approaches to person-directed supports with are based on stronger self-determination than traditional person-centered approaches.\n\nPerson centred thinking and planning is founded on the premise that genuine listening contains an implied promise to take action. Unless what is learned about how the person wishes to live, and where they wish to go in their lives is recorded and acted upon, any planning will have been a waste of time, and more importantly a betrayal of the person and the trust they have placed in those who have planned with them.\n\nIn the UK initiatives such as individual budgets and self-directed supports using models like In Control mean that Person Centred Planning can now be used to directly influence a person's Support Planning, giving them direct control over who delivers their support, and how it is delivered.\n\n\n\n"}
{"id": "43335986", "url": "https://en.wikipedia.org/wiki?curid=43335986", "title": "PharmAccess Foundation", "text": "PharmAccess Foundation\n\nPharmAccess Foundation is part of the PharmAccess Group. PharmAccess is an international non-profit organization with a digital agenda dedicated to connecting more people in sub-Saharan Africa to better healthcare. By making use of public-private partnerships, they leverage donor contributions, which they believe will pave the way for private investments hereby contributing to healthier populations and social and economic development. Currently PharmAccess employs a multidisciplinary team of professionals in Tanzania, Kenya, Nigeria, Ghana and the Netherlands. \n\nPharmAccess was founded in 2001 by HIV/AIDS researcher Prof. Joep Lange. He took an important first step for the organization by distributing life-saving medicines against HIV/AIDS in Africa in cooperation with multinationals. In 2007, PharmAccess was one of two organizations that won a competition for a World Bank funding partnership.\n\nOther leading individuals in the organization are Onno Schellekens, MsC, the managing director of the Investment Fund for Health in Africa, and Professor Tobias Rinke de Wit of the Amsterdam Institute for Global Health and Development.\n\nAll of the activities are co-funded by the Health Insurance Fund (HIF). In October 2006, the Health Insurance Fund signed a contract with the Dutch Ministry of Foreign Affairs to finance programs that provide access to affordable and quality healthcare among low income populations in sub-Saharan Africa through the introduction of financing mechanisms (including health insurance) and the improvement of healthcare quality.\n\nProf. Joep Lange of the Academic Medical Center (AMC) initiated PharmAccess in 2001, with support from the Dutch Aids Fonds, to bring life-saving antiretroviral therapy for HIV/AIDS treatment to Africa. Highly active antiretroviral therapy (HAART) was introduced in the developed world in the mid-nineties and immediately led to dramatic reductions in AIDS-related morbidity and mortality. In Africa, for the great majority of the population, adults and children, treatment was not available and many of the HIV/AIDS patients died of the disease.\nInitially, PharmAccess focused on the introduction of HIV/AIDS treatment programs for employees and dependents of Heineken in sub-Saharan Africa, which was revolutionary at the time. Soon they realized that these programs could be broadened to lay a foundation for the development of functional general health systems. The Dutch government, Dutch multinationals and PharmAccess joined forces to launch an alternative mechanism to the traditional development approach: a donor fund established to pilot health insurance for the currently uninsured in sub-Saharan Africa through the local private sector.\n\nIn order to enable the private companies in sub-Saharan Africa to grow and deliver the necessary services, a private investment fund (Investment Fund for Health in Africa, IFHA) was endorsed in 2007. In the years after this led to additional offshoots which all contribute to general health system strengthening and support to the health insurance schemes.\n\nThe PharmAccess Group employs an integrated approach in order to improve access to quality health care. The group mobilizes public and private resources for the benefit of doctors and patients through insurance, loans to doctors, clinical standards and impact research. The PharmAccess Group state they have a different approach to development cooperation, by making use of public-private partnerships. PharmAccess works with various different African and international partners, donors and investors from both the private and the public sector.\n\nThe PharmAccess Group headquarters is located in the Netherlands. The group currently works from five country offices located in Tanzania, Kenya, Namibia, Nigeria and Ghana.\n\nMembers of the group that each play a crucial role in the funding and implementation of this approach are Health Insurance Fund, Medical Credit Fund, SafeCare and our independent research partner the Amsterdam Institute for Global Health and Development. The PharmAccess Group works with different business lines:\n\nThey work with local partners to make health insurance available for low income groups (such as cooperatives of farmers, organizations of market women or company employees). Members pay an insurance premium according to their means. The health plans and related initiatives are financed with public grants from the Dutch Government and World Bank, with increasing local government involvement.\n\nMedical Credit Fund provides loans to doctors and clinics in the private sector, enabling them to improve the quality of their services, develop their business potential and service more low-income patients. Medical Credit Fund aims to contribute to a healthier investment climate and increase the bankability and thus scalability of the private healthcare sector. \n\nSub-Saharan Africa has a shortage of institutions and standards that can ensure objective measurement of quality. SafeCare is a quality improvement program designed for resource-restricted healthcare facilities. SafeCare’s internationally recognized set of standards aims to create a transparent improvement path that offers clinics positive incentives to move steadily upwards in quality. \n\nThe PharmAccess mHealth Program manages healthcare payments from both public and private donors to healthcare providers for providing quality healthcare services to patients.\n\nPharmAccess Foundation acts as a coordinating, implementing and advising (consultancy) partner for each of its organizations. In addition, PharmAccess aims to regularly develop new initiatives to strengthen the integrated approach.\n\nAll of these activities are co-funded by the Health Insurance Fund. In October 2006, the Health Insurance Fund signed a contract with the Dutch Ministry of Foreign Affairs to finance programs that provide access to affordable and quality healthcare among low income populations in sub-Saharan Africa through the introduction of innovative financing mechanisms (including health insurance) and the improvement of healthcare quality.\n\nThe programs of the PharmAccess Group and its partners have had impact on different communities, for example in the Kwara State in Nigeria, where the Kwara State Community Health Program was launched in 2009. The Kwara State Government subsidizes 60% of the premiums and has committed to extend the program to 600,000 people over the next five years. The program is also funded by the Dutch Health Insurance Fund with support from the Dutch Ministry of Foreign Affairs, and implemented by PharmAccess and the Nigerian health maintenance organization Hygeia. During a visit to Nigeria, UN Secretary General Ban Ki-moon spoke about the program’s unique character: ‘The groundbreaking Community Health Insurance of the Kwara State Government is exactly the kind of innovative partnership that we should replicate – here in Nigeria and beyond.’ This program represents the first time that a state government in Nigeria has partnered with the private sector and an NGO to provide statewide health insurance for its citizens. \n\nThe organization’s work has attracted considerable international attention including a G20 prize (G20 Small Medium Enterprise Finance Challenge 2010) that President Obama presented for their healthcare financing model.\n\n"}
{"id": "39671195", "url": "https://en.wikipedia.org/wiki?curid=39671195", "title": "Prancercise", "text": "Prancercise\n\nPrancercise is a holistic fitness method based on \"a springy, rhythmic way of moving forward, similar to a horse's gait and ideally induced by elation\" created by Joanna Rohrback.\n\nIt has been compared to the low-impact aerobics that were popularized by 1980s workout videos.\n\nMany parodies were created in reaction to Rohrback's original video, which themselves have accumulated hundreds of thousands of views.\n\nJoanna Rohrback graduated from Florida Atlantic University with a bachelor's degree in Health Services in 1978. Around 1989 she became a \"committed exercise devotee\", regularly working out on the Boardwalk in Hollywood Beach, FL. It was on that Boardwalk where Joanna had an experience she claims was the inspiration for Prancercise:\n\nThat same year she created the first video to feature the routine entitled “Funky Punky’s Prancercise Program.”\n\nJoanna quit her job and decided to focus on her new discovery. She practiced it every day, “taking years to hone its every gesture and kick.” Rohrback coined Prancercise as a means to vividly illustrate her comprehensive diet and fitness program, modeling it after the “strength and beauty of a horse.” From this she developed four distinct routines: the Prancercise Walk, the Prancercise Trot, the Prancercise Gallop, and the Prancercise Box.\n\nHowever, 1994 started a near-decade of personal setbacks for Joanna Rohrback. She wrote a book about Prancercise but couldn’t find a publisher. Her mother then began to experience complications from Parkinson's disease – so Joanna became her live-in caregiver. In 2004, after her mother died, Joanna also began to suffer from severe health complications. She was unable to Prancercise for almost nine years.\n\nDuring this time she maintained a natural healing regimen; by her 60th birthday, Joanna Rohrback was able to Prancercise once again. She decided to focus on her technique and started increasing her daily workouts. On Thanksgiving Day 2012, Rohrback successfully completed a 5k entirely in Prancercise. After this event, she decided to package her idea for the public sphere.\n\nIn December of that year she self-published her manuscript titled, “Prancercise: The Art of Physical and Spiritual Excellence.” Then, on Christmas Day, she uploaded a YouTube video featuring her fitness routine. The video went viral with over 10 million views and has attracted national and international attention.\n\nPrancercise has also been the subject of negative attention, much of it mocking the regimen.\n\nIn May 2013, Joanna Rohrback demonstrated Prancercise to Al Roker and Natalie Morales on The Today Show.\n\nIn July 2013, she starred in the official music video for John Mayer’s hit single Paper Doll.\n\nIn September 2013, she appeared in a promotional video for Wonderful Pistachios and e-cigarette brand Bull Smoke. She also appeared on the Comedy Central show Tosh.0 with Daniel Tosh and The Steve Harvey Show.\n\nIn October 2013, she appeared on The Dr. Oz show. In November 2013, she appeared on South Beach Tow.\n\nIn December 2013, she was named the Surprise Star of the Year by CNBC and appeared in YouTube's 2013 Rewind Mashup video, which has garnered over 15 million views in less than 48 hours.\n\nIn 2013 Joanna Rohrback and prancercise appeared on South Beach Tow where Joanna's car was repossessed and she taught prancercising to Tremont Towing dispatcher Dave Kosgrove who used it to help repossess a car.\n\nIn March 2014 Joanna Rohrback appeared in the popular TV show Glee as one of the judges at Nationals.\n\n"}
{"id": "58528658", "url": "https://en.wikipedia.org/wiki?curid=58528658", "title": "Preventable fraction for the population", "text": "Preventable fraction for the population\n\nIn epidemiology, preventable fraction for the population (PF), is the proportion of incidents in the population that could be prevented by exposing the whole population. It is calculated as formula_1, where formula_2 is the incidence in the exposed group, formula_3 is the incidence in the population. \n\nIt is used when an exposure reduces the risk, as opposed to increasing it, in which case its symmetrical notion is attributable fraction for the population.\n\n"}
{"id": "44356461", "url": "https://en.wikipedia.org/wiki?curid=44356461", "title": "Projekt Sex", "text": "Projekt Sex\n\nProjekt Sex (P6) is an independent organization working for the students of Lund University to promote sexual health on a physical, emotional and social level. It was founded in 1991 in Lund in Sweden. and belongs to the umbrella organization of the student life in Lund, Studentlund, and the national umbrella organization Students for Sexual Health (SfSH). The method which is used by P6 is called Peer Education, which means that students educate other students.\n\nTo promote sexual health on a physical, emotional and social level among students of Lund University, all students are welcome to pick up free condoms, lube and more information, for example about sexuality or where to get tested, from its office on the 4th floor of the AF Building in Lund. \nTo reach students outside of the P6 office, Projekt Sex organizes many different events: Most well-known are the condom raids happening during student parties and in nation clubs, where condoms are handed out and peer education takes place. Every second Thursday, P6 has its own radio show airing on Radio AF. Additionally, the organization includes an active LGBTQ-group which hosts movie nights, discussion evenings and monthly brunches. Every fourth year, Projekt Sex is also active in the student carnival Lundakarnevalen. The membership in P6 is free of charge, but students need to be members of the student umbrella organization Studentlund. \nP6 is also present at Campus Helsingborg, an additional campus of Lund University.\n\nProjekt Sex (P6) started as a peer education project under the Studenthealth and Skin Clinic in 1991. The project was launched after survey results had shown that years of frightening propaganda only had a short-term effect on the improvement of student's sexual health and that the numbers of sexually transmitted diseases among students was reaching extreme levels. The project P6 turned out to be successful; on three different occasions during the 1990s, large studies were conducted which all showed a consistent positive effect. \nThe project received a lot of attention internationally and several projects with P6 as a role model have been launched around the world until today, for example in India, China and Uganda. \nAround 2003, several similar projects were launched in Sweden and today there are more than 10 Swedish student organizations working with sexual health among students. In the fall of 2009, these organizations, with support from the already established organization P6, formed the national umbrella organization Students for Sexual Health (SfSH). Nowadays, P6 in Lund has around 700 members, of which 40-50 are active members, and keeps growing.\n\n"}
{"id": "3871677", "url": "https://en.wikipedia.org/wiki?curid=3871677", "title": "Protothecosis", "text": "Protothecosis\n\nProtothecosis is a disease found in dogs, cats, cattle, and humans caused by a type of green alga known as \"Prototheca\" that lacks chlorophyll. It and its close relative \"Helicosporidium\" are unusual in that they are actually green algae that have become parasites. The two most common species are \"Prototheca wickerhamii\" and \"Prototheca zopfii\". Both are known to cause disease in dogs, while most human cases are caused by \"P. wickerhami\". \"Prototheca\" is found worldwide in sewage and soil. Infection is rare despite high exposure, and can be related to a defective immune system. In dogs, females and Collies are most commonly affected.\n\nThe first human case was identified in 1964 in Sierra Leone.\n\nTreatment with amphotericin B has been reported.\n\n\"Prototheca\" has been thought to be a mutant of \"Chlorella\", a type of single-celled green alga. However, while \"Chlorella\" contains galactose and galactosamine in the cell wall, \"Prototheca\" lacks these. Also, \"Chlorella\" obtains its energy through photosynthesis, while \"Prototheca\" is saprotrophic, feeding on dead and decaying organic matter. When \"Prototheca\" was first isolated from slime flux of trees in 1894, it was thought to be a type of fungus. Its size varies from 2 to 15 micrometres.\n\nCattle can be affected by protothecal enteritis and mastitis. Protothecal mastitis is endemic worldwide, although most cases of infected herds have been reported in Germany, the United States, and Brazil.\n\nDisseminated protothecosis is most commonly seen in dogs. The algae enters the body through the mouth or nose and causes infection in the intestines. From there it can spread to the eye, brain, and kidneys. Symptoms can include diarrhea, weight loss, weakness, inflammation of the eye (uveitis), retinal detachment, ataxia, and seizures.\n\nDogs with acute blindness and diarrhea that develop exudative retinal detachment should be assessed for protothecosis. Diagnosis is through culture or finding the organism in a biopsy, cerebrospinal fluid, vitreous humour, or urine. Treatment of the disseminated form in dogs is very difficult, although use of antifungal medication has been successful in a few cases. Prognosis for cutaneous protothecosis is guarded and depends on the surgical options. Prognosis for the disseminated form is grave. This may be due to delayed recognition and treatment.\n\n"}
{"id": "29764286", "url": "https://en.wikipedia.org/wiki?curid=29764286", "title": "Real-time outbreak and disease surveillance", "text": "Real-time outbreak and disease surveillance\n\nReal-time outbreak and disease surveillance system (RODS) is a syndromic surveillance system developed by the University of Pittsburgh, Department of Biomedical Informatics. It is \"prototype developed at the University of Pittsburgh where real-time clinical data from emergency departments within a geographic region can be integrated to provide an instantaneous picture of symptom patterns and early detection of epidemic events.\"\n\nRODS uses a combination of various monitoring tools.\n"}
{"id": "578436", "url": "https://en.wikipedia.org/wiki?curid=578436", "title": "Self-medication", "text": "Self-medication\n\nSelf-medication is a human behavior in which an individual uses a substance or any exogenous influence to self-administer treatment for physical or psychological ailments.\n\nThe most widely self-medicated substances are over-the-counter drugs used to treat common health issues at home, as well as dietary supplements. These do not require a doctor's prescription to obtain and, in some countries, are available in supermarkets and convenience stores. \n\nThe psychology of self-medicating with psychoactive drugs is typically within the specific context of using recreational drugs, alcohol, comfort food, and other forms of behavior to alleviate symptoms of mental distress, stress and anxiety, including mental illnesses and/or psychological trauma, is particularly unique and can serve as a serious detriment to physical and mental health if motivated by addictive mechanisms. In postsecondary (university/college) students, the use of self-medicating of study-drugs such as Adderall, Ritalin, and Concerta has been widely reported and discussed in literature.\n\nProducts are marketed by manufacturers as useful for self-medication, sometimes on the basis of questionable evidence. Claims that nicotine has medicinal value have been used to market cigarettes as self-administered medicines. These claims have been criticized as inaccurate by independent researchers. Unverified and unregulated third-party health claims are used to market dietary supplements.\n\nSelf-medication is often seen as gaining personal independence from established medicine, and it can be seen as a human right, implicit in, or closely related to the right to refuse professional medical treatment. Self-medication can cause unintentional self-harm.\n\nGenerally speaking, self-medication is defined as \"the use of drugs to treat self-diagnosed disorders or symptoms, or the intermittent or continued use of a prescribed drug for chronic or recurrent disease or symptoms\"<ref name='WHO/EDM/QSM/00.1'> </ref>\n\nAs different drugs have different effects, they may be used for different reasons. According to the self-medication hypothesis (SMH), the individuals' choice of a particular drug is not accidental or coincidental, but instead, a result of the individuals' psychological condition, as the drug of choice provides relief to the user specific to his or her condition. Specifically, addiction is hypothesized to function as a compensatory means to modulate effects and treat distressful psychological states, whereby individuals choose the drug that will most appropriately manage their specific type of psychiatric distress and help them achieve emotional stability.\n\nThe self-medication hypothesis (SMH) originated in papers by Edward Khantzian, Mack and Schatzberg, David F. Duncan, and a response to Khantzian by Duncan. The SMH initially focused on heroin use, but a follow-up paper added cocaine. The SMH was later expanded to include alcohol, and finally all drugs of addiction.\n\nAccording to Khantzian's view of addiction, drug users compensate for deficient ego function by using a drug as an \"ego solvent\", which acts on parts of the self that are cut off from consciousness by defense mechanisms. According to Khantzian, drug dependent individuals generally experience more psychiatric distress than non-drug dependent individuals, and the development of drug dependence involves the gradual incorporation of the drug effects and the need to sustain these effects into the defensive structure-building activity of the ego itself. The addict's choice of drug is a result of the interaction between the psychopharmacologic properties of the drug and the affective states from which the addict was seeking relief. The drug's effects substitute for defective or non-existent ego mechanisms of defense. The addict's drug of choice, therefore, is not random.\n\nWhile Khantzian takes a psychodynamic approach to self-medication, Duncan's model focuses on behavioral factors. Duncan described the nature of positive reinforcement (e.g., the \"high feeling\", approval from peers), negative reinforcement (e.g. reduction of negative affect) and avoidance of withdrawal symptoms, all of which are seen in those who develop problematic drug use, but are not all found in all recreational drug users. While earlier behavioral formulations of drug dependence using operant conditioning maintained that positive and negative reinforcement were necessary for drug dependence, Duncan maintained that drug dependence was not maintained by positive reinforcement, but rather by negative reinforcement. Duncan applied a public health model to drug dependence, where the agent (the drug of choice) infects the host (the drug user) through a vector (e.g., peers), while the environment supports the disease process, through stressors and lack of support.\n\nKhantzian revisited the SMH, suggesting there is more evidence that psychiatric symptoms, rather than personality styles, lie at the heart of drug use disorders. Khantzian specified that the two crucial aspects of the SMH were that (1) drugs of abuse produce a relief from psychological suffering and (2) the individual's preference for a particular drug is based on its psychopharmacological properties. The individual's drug of choice is determined through experimentation, whereby the interaction of the main effects of the drug, the individual's inner psychological turmoil, and underlying personality traits identify the drug that produces the desired effects.\n\nMeanwhile, Duncan's work focuses on the difference between recreational and problematic drug use. Data obtained in the Epidemiologic Catchment Area Study demonstrated that only 20% of drug users ever experience an episode of drug abuse (Anthony & Helzer, 1991), while data obtained from the National Comorbidity Study demonstrated that only 15% of alcohol users and 15% of illicit drug users ever become dependent. A crucial determinant of whether a drug user develops drug abuse is the presence or absence of negative reinforcement, which is experienced by problematic users, but not by recreational users. According to Duncan, drug dependence is an avoidance behavior, where an individual finds a drug that produces a temporary escape from a problem, and taking the drug is reinforced as an operant behavior.\n\nSome mental illness sufferers attempt to correct their illnesses by use of certain drugs. Depression is often self-medicated with alcohol, tobacco, cannabis, or other mind-altering drug use. While this may provide immediate relief of some symptoms such as anxiety, it may evoke and/or exacerbate some symptoms of several kinds of mental illnesses that are already latently present, and may lead to addiction/dependence, among other side effects of long-term use of the drug.\n\nSufferers of posttraumatic stress disorder have been known to self-medicate, as well as many individuals without this diagnosis who have suffered from (mental) trauma.\n\nDue to the different effects of the different classes of drugs, the SMH postulates that the appeal of a specific class of drugs differs from person to person. In fact, some drugs may be aversive for individuals for whom the effects could worsen affective deficits.\n\nAlcohol and sedative/hypnotic drugs, such as barbiturates and benzodiazepines, are central nervous system (CNS) depressants that lower inhibitions via anxiolysis. Depressants produce feelings of relaxation and sedation, while relieving feelings of depression and anxiety. Though they are generally ineffective antidepressants, as most are short-acting, the rapid onset of alcohol and sedative/hypnotics softens rigid defenses and, in low to moderate doses, provides relief from depressive affect and anxiety. As alcohol also lowers inhibitions, alcohol is also hypothesized to be used by those who normally constrain emotions by attenuating intense emotions in high or obliterating doses, which allows them to express feelings of affection, aggression and closeness. People with social anxiety disorder commonly use these drugs to overcome their highly set inhibitions.\n\nPsychostimulants, such as cocaine, amphetamines, methylphenidate, caffeine, and nicotine, produce improvements in physical and mental functioning, including increased energy and feelings of euphoria. Stimulants tend to be used by individuals who experience depression, to reduce anhedonia and increase self-esteem. The SMH also hypothesizes that hyperactive and hypomanic individuals use stimulants to maintain their restlessness and heighten euphoria. Additionally, stimulants are useful to individuals with social anxiety by helping individuals break through their inhibitions. Some reviews suggest that students use psychostimulants recreationally to medicate for underlying deeper issues, such as depression or anxiety, and that their chance of self-medicating of these drugs can be loosely predicted using a variety of risk factors including childhood parental monitoring, participating in a sports team, or through the DAST-10 (screening test).\n\nOpiates, such as heroin and morphine, function as an analgesic by binding to opioid receptors in the brain and gastrointestinal tract. This binding reduces the perception of and reaction to pain, while also increasing pain tolerance. Opiates are hypothesized to be used as self-medication for aggression and rage. Opiates are effective anxiolytics, mood stabilizers, and anti-depressants, however, people tend to self-medicate anxiety and depression with depressants and stimulants respectively, though this is by no means an absolute analysis.\n\nCannabis is paradoxical in that it simultaneously produces stimulating, sedating and mildly psychedelic properties and both anxiolytic or anxiogenic properties, depending on the individual and circumstances of use. Depressant properties are more obvious in occasional users, and stimulating properties are more common in chronic users. Khantzian noted that research had not sufficiently addressed a theoretical mechanism for cannabis, and therefore did not include it in the SMH.\n\nSelf-medicating excessively for prolonged periods of time with benzodiazepines or alcohol often makes the symptoms of anxiety or depression worse. This is believed to occur as a result of the changes in brain chemistry from long-term use. Of those who seek help from mental health services for conditions including anxiety disorders such as panic disorder or social phobia, approximately half have alcohol or benzodiazepine dependence issues.\n\nSometimes anxiety precedes alcohol or benzodiazepine dependence but the alcohol or benzodiazepine dependence acts to keep the anxiety disorders going, often progressively making them worse. However, some people addicted to alcohol or benzodiazepines, when it is explained to them that they have a choice between ongoing poor mental health or quitting and recovering from their symptoms, decide on quitting alcohol or benzodiazepines or both. It has been noted that every individual has an individual sensitivity level to alcohol or sedative hypnotic drugs, and what one person can tolerate without ill health, may cause another to suffer very ill health, and even moderate drinking can cause rebound anxiety syndrome and sleep disorders. A person suffering the toxic effects of alcohol will not benefit from other therapies or medications, as these do not address the root cause of the symptoms.\n\nNicotine addiction seems to worsen mental health problems. Nicotine withdrawal depresses mood, increases anxiety and stress, and disrupts sleep. Although nicotine products temporarily relieve there nicotine withdrawal symptoms, an addiction causes stress and mood to be worse on average, due to mild withdrawal symptoms between hits. Nicotine addicts need the nicotine to temporarily feel normal. Nicotine industry marketing has claimed that nicotine is both less harmful and therapeutic for people with mental illness, and is a form of self-medication. This claim has been criticised by independent researchers.\n\nSelf medicating is a very common precursor to full addictions and the habitual use of any addictive drug has been demonstrated to greatly increase the risk of addiction to additional substances due to long-term neuronal changes. Addiction to any/every drug of abuse tested so far has been correlated with an enduring reduction in the expression of GLT1 (EAAT2) in the nucleus accumbens and is implicated in the drug-seeking behavior expressed nearly universally across all documented addiction syndromes. This long-term dysregulation of glutamate transmission is associated with an increase in vulnerability to both relapse-events after re-exposure to drug-use triggers as well as an overall increase in the likelihood of developing addiction to other reinforcing drugs. Drugs which help to re-stabilize the glutamate system such as N-acetylcysteine have been proposed for the treatment of addiction to cocaine, nicotine, and alcohol.\n\nSelf-medication with antibiotics is commonplace in some countries, such as Greece. Such use is cited as a potential factor in the incidence of certain antibiotic resistant bacterial infections in places like Nigeria.\n\nIn a questionnaire designed to evaluate self-medication rates amongst the population of Khartoum, Sudan, 48.1% of respondents reported self-medicating with antibiotics within the past 30 days, 43.4% reported self-medicating with antimalarials, and 17.5% reported self-medicating with both. Overall, the total prevalence of reported self-medication with one or both classes of anti-infective agents within the past month was 73.9%. Furthermore, according to the associated study, data indicated that self-medication \"varies significantly with a number of socio-economic characteristics\" and the \"main reason that was indicated for the self-medication was financial constraints\".\n\nSimilarly, in a survey of university students in Southern China, 47.8% of respondents reported self-medicating with antibiotics.\n\nIn a survey of West Bengal, India undergraduate medical school students, 57% reported self-medicating. The type of drugs most frequently used for self-medication were antibiotics (31%), analgesics (23%), antipyretics (18%), antiulcerics (9%), cough suppressants (8%), multivitamins (6%), and anthelmintics (4%).\n\nAnother study indicated that 53% of physicians in Karnataka, India reported self-administration of antibiotics.\n\nA study of Luo children in western Kenya found that 19% reported engaging in self-treatment with either herbal or pharmaceutical medicine. Proportionally, boys were much more likely to self-medicate using conventional medicine than herbal medicine as compared with girls, a phenomenon which was theorized to be influenced by their relative earning potential.\n\nSelf-medication is highly regulated in much of the world and many classes of drugs are available for administration only upon prescription by licensed medical personnel. Safety, social order, commercialization, and religion have historically been among the prevailing factors that lead to such prohibition.\n\n\n"}
{"id": "31936151", "url": "https://en.wikipedia.org/wiki?curid=31936151", "title": "Shows red card to abuser", "text": "Shows red card to abuser\n\nShow a red card to abusers () is a campaign against domestic violence launched by the Spanish Ministry of Equality on 18 March 2010 that has the support of many famous artists, journalists and athletes. It is considered very effective in helping \"to abandon complicity and take a step in favour of justice.\"\n\nThe initiative encourages every citizen to show a red card as a symbol to condemn any form of gender violence and to actively fight against abuse. It is asserted during this initiative that violence has no place in society.\n\nFor this campaign, the authors explain that \"the objective is to generate a social movement so that everyone, personally, welcomes this symbol and makes it their own.\" A series of television spots has been made in order to raise awareness on the issue of violence. Several professionals from different areas have participated (for free) with a red card saying firmly NO to any aggression against women.\n\n"}
{"id": "8158181", "url": "https://en.wikipedia.org/wiki?curid=8158181", "title": "Society of Cannabis Clinicians", "text": "Society of Cannabis Clinicians\n\nThe Society of Cannabis Clinicians (SCC) is a 501(c)3 non-profit organization dedicated to educating physicians about the medical use of cannabis. Its mission is to unite into one association members of the various medical specialties and allied professionals with this common purpose. The goals of the Society are as follows:\n\n\nThe group was established in 2004 by Tod Mikuriya, MD as a project of the California Cannabis Research Medical Group to facilitate voluntary medical standards for physician-approved cannabis under California law (HSC §11362.5).\n\nSCC publishes their research findings, such as president Jeffrey Hergenrather MD's survey of patients with Crohn's disease, in the Journal of Cannabis in Clinical Practice \"O'Shaughnessy\" and is in the process of developing an online research archive. They hold quarterly meetings of physicians and allied professionals featuring presentations by leading clinicians, researchers, and legal experts in the medical cannabis field.\n\nIn 2015, SCC launched the first online Medical Cannabis Continuing Education program, worth 12 CME credits, which in sequential order, a series of 12 courses designed to take a practicing clinician from the basics of the plant, its history and the underlying physiologic (endocannabinoid) system to the pharmacology and clinical practice of medical cannabis.\n\n\n"}
{"id": "22051851", "url": "https://en.wikipedia.org/wiki?curid=22051851", "title": "Study of Mathematically Precocious Youth", "text": "Study of Mathematically Precocious Youth\n\nThe Study of Mathematically Precocious Youth (SMPY) is a prospective longitudinal survey study of persons (mostly in the United States) identified by scores of 700 or higher on a section of the SAT Reasoning Test before age 13 years. It is one of the longest-running longitudinal studies of gifted youth in world history. Study scholars have used survey data from study participants to advance hypotheses about talent development and occupational preferences.\n\nThe Study of Mathematically Precocious Youth was founded by Julian Stanley in 1971 at Johns Hopkins University, with funding from the recently established Spencer Foundation. In 1986, the study headquarters moved to Iowa State University, where Camilla Benbow led the study until 1990. Since that year, the study has been led by Benbow and David Lubinski. In 1998, the study headquarters moved again, this time to Vanderbilt University.\n\nSMPY is the longest-running current longitudinal study of gifted children in the United States. Subjects are identified by high scores on the SAT Reasoning Test, which they take at or before the age of 13 years. Eligibility for the study is contingent on scoring at least 700 out of a possible 800 standard score points on the SAT by age 13 years (with prorated eligibility for higher scores up to age 13 years and 10 months). Participation in the Talent Search testing that involves children of that age sitting the SAT is voluntary.\n\nParticipation in SMPY is voluntary for students who attain eligible scores. Although after the first year, Stanley decided to include students with exceptional scores in either the mathematics or verbal sections of the SAT test, for inclusion in what is called the Study of Exceptional Talent, the name SMPY has been retained for the ongoing follow-up surveys. Follow-up surveys were sent to study participants after five, ten, twenty, and thirty-five years. Benbow and Lubinski and their colleagues have used the survey responses to explore individual differences among intellectually able individuals. The study population is analyzed by division into several subgroups.\n\nThe survey responses suggest that the profoundly gifted have different educational needs and accomplish much more in school and work than moderately gifted.\nTalented males and females also have differing abilities, interests, and lifestyle preferences,\nalthough they often express similar levels of intellectual satisfaction and achieve advanced educational credentials at similar rates. The sex differences other investigators have found on the things-people dimension in normative populations have been manifested in education and work, among the adolescents Benbow, Lubinski, and their associates have studied. SMPY has found that talented individuals with marked tilts tend to pursue careers that draw upon their cognitive strengths. Highly able youth with notably stronger mathematical than verbal ability often study and work in science and engineering, whereas adolescents with better scores on the verbal section than the mathematical one frequently went into the humanities, arts, social science, or law. Individuals with comparable mathematical and verbal ability did not follow such clear-cut trajectories, although many males with the \"high-flat\" ability profile pursued educational and vocational pursuits in science.\n\n\n\n"}
{"id": "2710167", "url": "https://en.wikipedia.org/wiki?curid=2710167", "title": "Sukhna Lake", "text": "Sukhna Lake\n\nSukhna Lake in Chandigarh, India, is a reservoir at the foothills (Shivalik hills) of the Himalayas. This 3 km² rainfed lake was created in 1958 by damming the Sukhna Choe, a seasonal stream coming down from the Shivalik Hills. Originally the seasonal flow entered the lake directly causing heavy siltation. To check the inflow of silt, 25.42 km² of land was acquired in the catchment area and put under vegetation. In 1974, the Choe was diverted and made to bypass the lake completely, the lake being fed by three siltation pots, minimising the entry of silt into the lake itself.\n\nThe lake was created by Le Corbusier and the Chief Engineer P L Verma. To preserve its tranquility, Corbusier insisted on two things: that it be forbidden for motor boats to circulate in the water, and for vehicular traffic to be prohibited on top of the dam (promenade). The lake is fringed by a golf course to the south, and Nek Chand's famous Rock Garden of Chandigarh to its west. Earlier some crocodiles were also found in this lake.\n\nSukhna is an inseparable part of the city of Chandigarh. Le Corbusier had foreseen that the residents of the city would be drawn it for the 'care of the body and spirit'. The city planners were deeply attached to the lake. So much so that Pierre Jeanneret's ashes were immersed in the lake in 1970 at his niece's request.\n\nThe roof of the 'bandh' or dam has become a favorite promenade. Serious walkers pursue an exercise regime, families enjoy an evening stroll and nature lovers mingle with children on roller skates. Photographers and painters love to capture its scenic beauty of the setting sun, or the heavily clouded monsoon sky, or the early morning mist in winter set amidst the tranquility of the lake. Even anglers do not leave unrewarded.\n\nSukhna has a membership-based Lake Club with lawns, a gym, indoor games, swimming pool and tennis courts with both synthetic and grass courts. Boating, rowing, sculling, sailing, kayaking and water skiing can be enjoyed throughout the year.\n\nThe lake, which was the venue for the Asian Rowing Championships, has the longest channel for rowing and yachting events in Asia. It also has facilities for other water sports like water surfing, skiing and sculling.\n\nSukhna is a sanctuary for many exotic migratory birds like the Siberian duck, storks and cranes, during the winter months. The lake has been declared as a protected national wetland by the Government of India.\n\nDuring summers, there are streams of men, women and children from all walks of life offering voluntary service to desilt the lake bed for about three months. This annual ritual has been a regular feature since long ago.\n\nSukhna Lake is the venue for many festive celebrations, too. The most popular is the Mango Festival held during the monsoons when scores of varieties of mangoes are on display. From time-to-time, other food festivals featuring specialties from different Indian States are also held here, along with cultural performances.\n\nThe Mera Chandigarh administration has made a decision not to allow fish more than 30 cm in size in the Sukhna Lake \n\nChandigarh Administration has finalized a new plan for Sukhna Lake and New Lake in Sector 42 with Rs 2.73 crore which has also been received from Union Government.B.A.\n\nThe lake is facing serious issues like weed overgrowth, catchment adequacy and silting that are significantly shrinking its size and depth. A project team, under Parasu Ram Mishra, was deployed to address the issue and take remedial measures, which halted the sedimentation, for a while. Additionally, it has become the subject of litigation between the Chandigarh and Punjab.\n\nSilting has taken its toll and the volume of the lake has been reduced to 56% of its original. The lake is shrinking rapidly due to siltation and lack of inflow. It was initially hoped that the work of desilting could be undertaken in summers at a war footing and dry dredging could be undertaken at a fraction of cost to save Sukhna in the coming years. Unfortunately, the ground realities seem to be different. Due to heavy rain in August and September Sukhna was filled up again and flood gates were being opened.\n\nIn December 2014, there was an Avian Influenza--or commonly known as Bird Flu scare-- that led to a temporary ban on the entry to the lake premises. The scare started after some migrant geese were found dead in the lake.\n\nHowever, the administration took great precautions and the domesticated geese were culled to check an infection. A score of workers who culled the geese went for a check to be sure that they were safe. The reason behind the death of the geese-- whether there is bird flu scare-- remains unknown.\n\n"}
{"id": "56213295", "url": "https://en.wikipedia.org/wiki?curid=56213295", "title": "Taryn Young", "text": "Taryn Young\n\nTaryn Young is the Director of the Centre for Evidence-based Health Care and Head of the Division of Epidemiology and Biostatistics at Stellenbosch University. She is a member of the Academy of Science of South Africa. Professor Young has co-authored over 100 peer-reviewed scholarly articles. Her research has focused on summarising and interpreting medical research. \n\n"}
{"id": "198064", "url": "https://en.wikipedia.org/wiki?curid=198064", "title": "Tuskegee syphilis experiment", "text": "Tuskegee syphilis experiment\n\nThe Tuskegee Study of Untreated Syphilis in the Negro Male was an infamous and unethical clinical study conducted between 1932 and 1972 by the U.S. Public Health Service. The purpose of this study was to observe the natural history of untreated syphilis; the African-American men in the study were told they were receiving free health care from the United States government.\n\nThe Public Health Service started working on this study in 1932 in collaboration with Tuskegee University, a historically black college in Alabama. Investigators enrolled in the study a total of 600 impoverished, African-American sharecroppers from Macon County, Alabama. Of these men, 399 had previously contracted syphilis before the study began, and 201 did not have the disease. The men were given free medical care, meals, and free burial insurance for participating in the study. The men were told that the study was only going to last six months, but it actually lasted 40 years. After funding for treatment was lost, the study was continued without informing the men that they would never be treated. None of the men infected were ever told that they had the disease, and none were treated with penicillin even after the antibiotic was proven to successfully treat syphilis. According to the Centers for Disease Control, the men were told that they were being treated for \"bad blood\", a colloquialism that described various conditions such as syphilis, anemia, and fatigue. \"Bad blood\"—specifically the collection of illnesses the term included—was a leading cause of death within the southern African-American community.\n\nThe 40-year study was controversial for reasons related to ethical standards. Researchers knowingly failed to treat patients appropriately after the 1940s validation of penicillin was found as an effective cure for the disease that they were studying. The revelation in 1972 of study failures by a whistleblower, Peter Buxtun, led to major changes in U.S. law and regulation on the protection of participants in clinical studies. Now studies require informed consent, communication of diagnosis, and accurate reporting of test results.\n\nBy 1947, penicillin had become the standard treatment for syphilis. Choices available to the doctors involved in the study might have included treating all syphilitic subjects and closing the study, or splitting off a control group for testing with penicillin. Instead, the Tuskegee scientists continued the study without treating any participants; they withheld penicillin and information about it from the patients. In addition, scientists prevented participants from accessing syphilis treatment programs available to other residents in the area. The study continued, under numerous US Public Health Service supervisors, until 1972, when a leak to the press resulted in its termination on November 16 of that year. The victims of the study, all African American, included numerous men who died of syphilis, 40 wives who contracted the disease, and 19 children born with congenital syphilis.\n\nThe Tuskegee Syphilis Study, cited as \"arguably the most infamous biomedical research study in U.S. history\", led to the 1979 Belmont Report and to the establishment of the Office for Human Research Protections (OHRP). It also led to federal laws and regulations requiring Institutional Review Boards for the protection of human subjects in studies involving them. The Office for Human Research Protections (OHRP) manages this responsibility within the US Department of Health and Human Services (HHS).\n\nOn May 16, 1997, President Bill Clinton formally apologized on behalf of the United States to victims of the experiment.\n\nThe venereal disease section of the U.S. Public Health Service (PHS) formed a study group in 1932 at its national headquarters. Taliaferro Clark was credited with founding it. His initial goal was to follow untreated syphilis in a group of black men for 6 to 9 months, and then follow up with a treatment phase. When he understood the intention of other study members to use deceptive practices, Clark disagreed with the plan to conduct an extended study. He retired the year after the study began.\n\nAlthough Clark is usually assigned blame for conceiving the Tuskegee Study, Thomas Parran Jr. is equally, if not more, deserving of originating the notion of a non-treatment experiment in Macon County, Alabama. As the Health Commissioner of New York State (and former head of the PHS Venereal Disease Division), Parran was asked by the Rosenwald Fund to make an assessment of their survey and demonstration projects in six Southern states. Among his conclusions was the recommendation that, \"If one wished to study the natural history of syphilis in the Negro race uninfluenced by treatment, this county (Macon) would be an ideal location for such a study.\"\n\nAn official committee at the University of Pittsburgh reported the following on Thomas Parran, M.D., who was a founder of the University’s Graduate School of Public Health: “Dr. Parran’s role, and the extent of his influence in approving, funding, and providing oversight of the Tuskegee and Guatemalan studies, is not entirely clear. Based upon the evidence available today, it might not be possible to determine with certainty Dr. Parran’s level of knowledge and involvement in the studies.” Office of Diversity and Inclusion Review Committee on Parran Hall, University of Pittsburgh, Report and Recommendations (June 11, 2018) p. 4. \n\nRepresenting the PHS, Clark had solicited the participation of the Tuskegee Institute (a well-known historically black college in Alabama, now known as Tuskegee University) and of the Arkansas regional Public Health Service office. Eugene Heriot Dibble, Jr., an African-American doctor, was head of the John Andrew Hospital at the Tuskegee Institute. (From 1936–1946, he served as director of the Tuskegee Veterans Administration Medical Center, established in 1923 in the city by the federal government on land donated by the Institute.)\n\nOliver C. Wenger was the director of the regional PHS Venereal Disease Clinic in Hot Springs, Arkansas. He and his staff took the lead in developing study procedures. Wenger and his staff played a critical role in developing early study protocols. Wenger continued to advise and assist the Tuskegee Study when it was adapted as a long-term, no-treatment observational study after funding for treatment was lost.\n\nRaymond A. Vonderlehr was appointed on-site director of the research program and developed the policies that shaped the long-term follow-up section of the project. His method of gaining the \"consent\" of the subjects for spinal taps (to look for signs of neurosyphilis) was by portraying this diagnostic test as a \"special free treatment\". Participants were not told their diagnosis. Vonderlehr retired as head of the venereal disease section in 1943, shortly after the antibiotic penicillin had first been shown to be a cure for syphilis.\n\nSeveral African American health workers and educators associated with Tuskegee Institute helped the PHS to carry out its experimentation and played a critical role in the progress of the study. The extent to which they knew about the full scope of the study is not clear in all cases. Robert Russa Moton, then president of Tuskegee Institute, and Eugene Dibble, head of the Institute's John Andrews Hospital, both lent their endorsement and institutional resources to the government study. Registered nurse Eunice Rivers, who had trained at Tuskegee Institute and worked at its affiliated John Andrew Hospital, was recruited at the start of the study to be the main contact with the participants in the study.\n\nVonderlehr advocated for Rivers' participation, as the direct link to the regional African-American community. During the Great Depression of the 1930s, the Tuskegee Study recruited poor lower-class African Americans, who often could not afford health care, by offering them the chance to join \"Miss Rivers' Lodge\". Patients were told they would receive free physical examinations at Tuskegee University, free rides to and from the clinic, hot meals on examination days, and free treatment for minor ailments.\n\nBased on the available health care resources, Rivers believed that the benefits of the study to the men outweighed the risks. As the study became long term, Rivers became the chief person with continuity. Unlike the national, regional and on-site PHS administrators, doctors, and researchers, some of whom were political appointees with short tenure and others who changed jobs, Rivers continued at Tuskegee University. She was the only study staff person to work with participants for the full 40 years. By the 1950s, Nurse Rivers had become pivotal to the study: her personal knowledge of the subjects enabled maintenance of long-term follow up.\n\nIn 1943, Congress passed the Henderson Act, a public health law requiring testing and treatment for venereal disease. By the late 1940s, doctors, hospitals and public health centers throughout the country routinely treated diagnosed syphilis with penicillin. However, the Tuskegee experiment continued to avoid treating the men who had the disease.\n\nIn the period following World War II, the revelation of the Holocaust and related Nazi medical abuses brought about changes in international law. Western allies formulated the Nuremberg Code to protect the rights of research subjects. In 1964 the World Health Organization's Declaration of Helsinki specified that experiments involving human beings needed the \"informed consent\" of participants. But no one appeared to have reevaluated the protocols of the Tuskegee Study according to the new standards and in light of treatment available for the disease which is fatal 8–58% of the time. On July 25, 1972, word of the Tuskegee Study was reported by Jean Heller of the Associated Press; the next day \"The New York Times\" carried it on its front page, and the story captured national attention. Peter Buxtun, a whistleblower who was a former PHS interviewer for venereal disease, had leaked information after failing to get a response to his protests about the study within the department. He gave information to the \"Washington Star\" and \"The New York Times\". John R. Heller Jr. of PHS, who in later years of the study led the national division, still defended the ethics of the study, stating, \"The longer the study, the better the ultimate information we would derive.\" Author James Jones editorialized about Heller, suggesting that his opinion was, \"The men's status did not warrant ethical debate. They were subjects, not patients; clinical material, not sick people.\"\n\nA Norwegian study in 1928 had reported on the pathologic manifestations of untreated syphilis in several hundred white males. This study is known as a retrospective study, since investigators pieced together information from the histories of patients who had already contracted syphilis but remained untreated for some time.\nThe Tuskegee study group decided to build on the Oslo work and perform a prospective study to complement it. Researchers could study the natural progression of the disease as long as they did not harm their subjects. The researchers involved with the Tuskegee experiment reasoned that they were not harming the black men involved in the study because they were unlikely to get treatment for their syphilis and further education would not diminish their inherent sex drive. Even at the beginning of the study, major medical textbooks had recommended that all syphilis be treated, as the consequences were quite severe. At that time, treatment included arsenic therapy and the \"606\" formula. The researchers reasoned that the knowledge gained would benefit humankind; however, it was determined afterward that the doctors did harm their subjects by depriving them of appropriate treatment once it had been discovered. The study was characterized as \"the longest non-therapeutic experiment on human beings in medical history.\"\n\nThe US Public Health Study of Syphilis at Tuskegee began as a 6-month descriptive epidemiological study of the range of pathology associated with syphilis in the Macon County population. At that time, it was believed that the effects of syphilis depended on the race of those affected. For African Americans, physicians believed that their cardiovascular system was more affected than the central nervous system. Initially, subjects were studied for six to eight months and then treated with contemporary methods, including Salvarsan (\"606,\") mercurial ointments, and bismuth. These methods were, at best, mildly effective. The disadvantage was that these treatments were all highly toxic. The Tuskegee Institute participated in the study, as its representatives understood the intent was to benefit public health in the local poor population. The Tuskegee University-affiliated hospital effectively loaned the PHS its medical facilities, and other predominantly black institutions and local black doctors participated as well.\n\nThe Rosenwald Fund, a major Chicago-based philanthropy devoted to black education and community development in the South, provided financial support to pay for the eventual treatment of the patients. They had previously collaborated with Public Health Services in a study of syphilis prevalence in over 2,000 black workers in Mississippi's Delta Pine and Land Company in 1928, and helped provide treatment for 25% of the workers who had tested positive for the disease. Study researchers initially recruited 399 syphilitic Black men, and 201 healthy Black men as controls. \n\nContinuing effects of the Stock Market Crash of 1929 and the beginning of the Great Depression led the Rosenwald Fund to withdraw its offer of funding. Study directors issued a final report as they thought this might mean the end of the study once funding to buy medication for the treatment phase of the study was withdrawn.\n\nMedical ethics considerations were limited from the start and rapidly deteriorated. The PHS asked black Tuskegee Institute physicians to participate in the study by offering funds, employment, and interns to encourage the ongoing participation of the patients. Additionally, the study intentionally employed Eunice Rivers, a black nurse from Macon County, to be primary source of contact and build personal, trusting relationships with patients to promote their participation. To ensure that the men would show up for the possibly dangerous, painful, diagnostic, and non-therapeutic spinal taps, the doctors sent the 400 patients a misleading letter titled \"Last Chance for Special Free Treatment\". The study also required all participants to undergo an autopsy after death in order to receive funeral benefits. After penicillin was discovered as a cure, researchers continued to deny such treatment to many study participants. Many patients were lied to and given placebo treatments so that researchers could observe the full, long-term progression of the fatal disease.\nThe Tuskegee Study published its first clinical data in 1934 and issued its first major report in 1936. This was prior to the discovery of penicillin as a safe and effective treatment for syphilis. The study was not secret since reports and data sets were published to the medical community throughout its duration.\n\nDuring World War II, 250 of the subject men registered for the draft. These men were consequently diagnosed as having syphilis at military induction centers and ordered to obtain treatment for syphilis before they could be taken into the armed services. PHS researchers attempted to prevent these men from getting treatment, thus depriving them of chances for a cure. A PHS representative was quoted at the time saying: \"So far, we are keeping the known positive patients from getting treatment.\" Despite this, 96% of the 90 original test subjects reexamined in 1963 had received either arsenical or penicillin treatments from another health provider.\n\nBy 1947 penicillin had become standard therapy for syphilis. The US government sponsored several public health programs to form \"rapid treatment centers\" to eradicate the disease. When campaigns to eradicate venereal disease came to Macon County, study researchers prevented their patients from participating.\n\nBy the end of the study in 1972, only 74 of the test subjects were alive. Of the original 399 men, 28 had died of syphilis, 100 were dead of related complications, 40 of their wives had been infected, and 19 of their children were born with congenital syphilis. The Tuskegee University Legacy Museum has on display a check issued by the United States government on behalf of Dan Carlis to Lloyd Clements, Jr., a descendant of one of the Tuskegee Syphilis Study participants. Lloyd Clements, Jr.'s great-grandfather Dan Carlis and two of his uncles, Ludie Clements and Sylvester Carlis, were in the study. Original legal paper work for Sylvester Carlis related to the Tuskegee Syphilis Study is on display at the museum as well. Lloyd Clements, Jr. has worked with noted historian Susan Reverby concerning his family's involvement with the Tuskegee Syphilis Study.\n\nThe first dissent against the Tuskegee study was Irwin Schatz, a young Chicago doctor only four years out of medical school. In 1965, Schatz read an article about the study in a medical journal, and wrote a letter directly to the study's authors confronting them with a declaration of brazen unethical practice. His letter, read by Anne R. Yobs (one of the study's authors), was immediately ignored and filed away with a brief memo that no reply would be sent.\n\nIn 1966 Peter Buxtun, a PHS venereal-disease investigator in San Francisco, sent a letter to the national director of the Division of Venereal Diseases to express his concerns about the ethics and morality of the extended Tuskegee Study. The Center for Disease Control (CDC), which by then controlled the study, reaffirmed the need to continue the study until completion; i.e., until all subjects had died and been autopsied. To bolster its position, the CDC received unequivocal support for the continuation of the study, both from local chapters of the National Medical Association (representing African-American physicians) and the American Medical Association (AMA).\n\nIn 1968 William Carter Jenkins, an African-American statistician in the PHS, part of the Department of Health, Education, and Welfare (HEW), founded and edited \"The Drum\", a newsletter devoted to ending racial discrimination in HEW. The cabinet-level department included the CDC. In \"The Drum\", Jenkins called for an end to the Tuskegee Study. He did not succeed; it is not clear who read his work.\n\nBuxtun finally went to the press in the early 1970s. The story broke first in the \"Washington Star\" on July 25, 1972. It became front-page news in the \"New York Times\" the following day. Senator Edward Kennedy called Congressional hearings, at which Buxtun and HEW officials testified. As a result of public outcry, the CDC and PHS appointed an \"ad hoc\" advisory panel to review the study. The panel found that the men agreed to certain terms of the experiment, such as examination and treatment. However, they were not informed of the study's actual purpose. The panel then determined the study was medically unjustified and ordered its termination.\n\nAs part of the settlement of a class action lawsuit subsequently filed by the NAACP on behalf of study participants and their descendants, the U.S. government paid $10 million ($ in ) and agreed to provide free medical treatment to surviving participants and to surviving family members infected as a consequence of the study; Congress created a commission empowered to write regulations to deter such abuses from occurring in the future.\n\nA collection of materials compiled to investigate the study is held at the National Library of Medicine in Bethesda, Maryland.\n\nIn 1974 Congress passed the National Research Act and created a commission to study and write regulations governing studies involving human participants. Within the US Department of Health and Human Services, the Office for Human Research Protections (OHRP) was established to oversee clinical trials. Now studies require informed consent, communication of diagnosis, and accurate reporting of test results. Institutional review boards (IRBs), including laypeople, are established in scientific research groups and hospitals to review study protocols and protect patient interests, to ensure that participants are fully informed.\n\nIn 1994, a multi-disciplinary symposium was held on the Tuskegee study: \"Doing Bad in the Name of Good?: The Tuskegee Syphilis Study and Its Legacy\" at the University of Virginia. Following that, interested parties formed the Tuskegee Syphilis Study Legacy Committee to develop ideas that had arisen at the symposium. It issued its final report in May 1996. The Committee had two related goals: (1) President Bill Clinton should publicly apologize for past government wrongdoing related to the study and (2) the Committee and relevant federal agencies should develop a strategy to redress the damages.\n\nA year later on May 16, 1997, President Bill Clinton formally apologized and held a ceremony at the White House for surviving Tuskegee study participants. He said: \n\nWhat was done cannot be undone. But we can end the silence. We can stop turning our heads away. We can look at you in the eye and finally say on behalf of the American people, what the United States government did was shameful, and I am sorry ... To our African American citizens, I am sorry that your federal government orchestrated a study so clearly racist.\n\nFive of the eight study survivors attended the White House ceremony.\n\nThe presidential apology led to progress in addressing the second goal of the Legacy Committee. The federal government contributed to establishing the National Center for Bioethics in Research and Health Care at Tuskegee, which officially opened in 1999 to explore issues that underlie research and medical care of African Americans and other under-served people.\n\nIn 2009 the Legacy Museum opened in the Bioethics Center, to honor the hundreds of participants of the \"Tuskegee Study of Untreated Syphilis in the Negro Male.\"\n\nThe revelations of mistreatment under the Tuskegee Syphilis Study are believed to have significantly damaged the trust of the black community toward public health efforts in the United States. Observers believe that the abuses of the study may have contributed to the reluctance of many poor black people to seek routine preventive care. A 1999 survey showed that 80% of African American men believe the men in the Tuskegee Syphilis Experiment had been injected with syphilis. A 2016 paper published by the National Bureau of Economic Research finds \"that the historical disclosure of the [Tuskegee experiment] in 1972 is correlated with increases in medical mistrust and mortality and decreases in both outpatient and inpatient physician interactions for older black men. Our estimates imply life expectancy at age 45 for black men fell by up to 1.4 years in response to the disclosure, accounting for approximately 35% of the 1980 life expectancy gap between black and white men.\" However, other studies, such as the Tuskegee Legacy Project Questionnaire, have challenged the degree to which knowledge of the Tuskegee experiments have kept black Americans from participating in medical research. This study shows that, even though black Americans are four times more likely to know about the syphilis trials than are whites, they are two to three times more willing to participate in biomedical studies.\nOther studies concluded that the Tuskegee Syphilis trial has played a minor role in the decisions of black Americans to decline participation as research subjects. Of the studies that have investigated the willingness of black Americans to participate in medical studies, they have not drawn consistent conclusions related to the willingness and participation in studies by racial minorities. Some of the factors that continue to limit the credibility of these few studies is how awareness differs significantly across studies. For instance, it appears that the rates of awareness differ as a function of method of assessment, study participants who reported awareness of the Tuskegee Syphilis Trials are often misinformed about the results and issues, and awareness of the study is not reliably associated with unwillingness to participate in scientific research.\n\nDistrust of the government, in part formed through the study, contributed to persistent rumors during the 1980s in the black community that the government was responsible for the HIV/AIDS crisis by having deliberately introduced the virus to the black community as some kind of experiment. In February 1992 on ABC's \"Prime Time Live\", journalist Jay Schadler interviewed Dr. Sidney Olansky, Public Health Services director of the study from 1950 to 1957. When asked about the lies that were told to the study subjects, Olansky said, \"The fact that they were\nilliterate was helpful, too, because they couldn't read the newspapers. If they were not, as things moved on they might have been reading newspapers and seen what was going on.\"\n\nTuskegee highlighted issues in race and science. The aftershocks of this study, and other human experiments in the United States, led to the establishment of the National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research and the National Research Act. The latter requires the establishment of institutional review boards (IRBs) at institutions receiving federal support (such as grants, cooperative agreements, or contracts). Foreign consent procedures can be substituted which offer similar protections and must be submitted to the Federal Register unless a statute or Executive Order requires otherwise.\n\nWriter James Jones said that physicians were fixated on African American sexuality, and believing that African Americans willingly had sexual relations with those who were infected (although none had been told his diagnosis) resulted in their believing that individuals were solely responsible for contracting the disease. One researcher critiqued how the study was administered and its change in purpose. He said that it was \"the economic exploitation of humans as a natural resource of a disease that could not be cultivated or animals in order to establish and sustain U.S. superiority in patented commercial biotechnology\".\n\nDue to the lack of information, the participants were manipulated into continuing the study without full knowledge of their role or their choices. Since the late 20th century, IRBs established in association with clinical studies require that all involved in study be willing and voluntary participants.\n\n\n\n\n\n\n"}
{"id": "4212753", "url": "https://en.wikipedia.org/wiki?curid=4212753", "title": "University of Washington Department of Global Health", "text": "University of Washington Department of Global Health\n\nThe University of Washington Department of Global Health is a department jointly run by the schools of Medicine and Public Health at the University of Washington in Seattle, Washington. Its aim is to provide a multidisciplinary venue to address issues of global health at the university.\n\nThe department was begun with funding supplied by the Bill & Melinda Gates Foundation.\n\nThe Department of Global Health was launched in January 2007 with support from the Bill & Melinda Gates Foundation, the state of Washington, and the University of Washington, with a mandate to harness the extraordinary expertise, energy, and creativity of faculty across all 17 UW schools and colleges to create one of the most comprehensive academic global health programs in the world.[1]\n\nThe pioneering work of UW researchers in sexually transmitted diseases in the 1970s and 1980s paved the way for the University's leading role in HIV/AID research and training, and, now, global health.\n\nThe Department is housed in both the School of Medicine and School of Public Health and has formed linkages across campus and throughout the world to help address not only infectious diseases but a host of pressing global health issues, including health metrics and evaluation; the health of women, children, and adolescents; health system strengthening and implementation science; climate change and health; global trauma and violence prevention; and global medicines safety with a cross-cutting focus on social justice and equity. The Center for Integrated Health of Women, Children, and Adolescents, for example, includes at least 22 collaborations (13 on campus and partnerships in Kenya, Ethiopia, Mexico, and Peru). And the initiative on Climate Change and Global Health involves more than 25 collaborations on campus and beyond.\n\nIts closely affiliated centers also include the Institute for Health Metrics and Evaluation (IHME), the International Training and Education Center for Health (I-TECH), the International Clinical Research Center (ICRFC), the Center for AIDS Research (CFAR), Health Alliance International (HaI), and the Global Health Resource Center (GHRC).\n\nThe Department has strong ties in Kenya, Peru, Mozambique, and Ethiopia. But worldwide, the Department works with nearly 250 collaborating organizations, including universities and hospitals, NGOs, government agencies, and ministries of health.\n\nQuote from Howard Frumkin MD, MPH, Dean of the UW School of Public Health: \"Global health represents the best of academic health sciences -- transdisciplinary systems thinking, cross-cultural sensitivity, rigorous scientific research, hands-on participatory training, effective service delivery with impact empirically measured, and sustainable collaboration. This Department is outstanding. \"\n\nMaster's Level: The Department offers several global health tracks for a master of Public Health degree: General; Leadership, Policy, and Management; Health Metrics and Evaluation; Peace Corps; Epidemiology; and concurrent degrees.\n\nDoctoral Programs: The Department offers a doctoral program in Pathobiology. A doctoral program in global health with emphases on health metrics and evaluation and implementation science is in development.\n\nFellowship Programs: The Department offers post-bachelor and post-graduate fellowship programs with the Institute for Health Metrics and Evaluation.\n\nCertificate Programs: The Department has a certificate program in Global Health and a program in AIDS and STIs. Medical students also can take a Global Health Pathway.\n\nUndergraduate Programs: An undergraduate minor in global health was launched in January 2011.\n\nJim Yong Kim, formerly of Partners in Health and the WHO HIV/AIDS program, was originally a candidate for director of the department, but was not selected. A controversial second selection process involving three new candidates took place in late 2005 and early 2006. The process was criticized for not being open, and there was concern among the student body and faculty about the chosen chair. Some feared that the department would be too heavily oriented towards biomedical research and biotechnology (e.g. vaccine development) and would neglect the broader issues of public health, such as social justice, health disparities, prevention, promotion, human resources in health, and public policy. Some also feared that the areas of education and service would be sacrificed for a research agenda, and pointed to the fact that one of the first steps in implementing the department was the leasing of a large facility off campus in Seattle's South Lake Union neighborhood - an area being developed as a biotechnology hub.\n\nThose within the process argued that planning was open and that the department would be multidisciplinary and would live up to its stated vision of taking a broad approach to global health. They also noted that while the department would have some facilities off campus, it would be primarily based at the university; the deans of the schools committed to finding on-campus space to house the department's administrative offices, although much of the lab space was planned to be located in a building in the Eastlake neighborhood.\n\nIn Spring, 2006, Michael Merson of Yale University was offered the position of director. However, in July of that year it was announced that Merson had been appointed direct of Duke University's Global Health Institute.\n\nOn September 8, University of Washington announced King K. Holmes, MD, PhD, a world leader in AIDS and infectious disease research and training, to become the first chair of the University of Washington's new Department of Global Health. His leadership in global health research and training, and experience as a public-health practitioner, will serve Holmes well in leading the Department of Global Health, according to Paul G. Ramsey, dean of the School of Medicine.\n\n\n"}
{"id": "25439126", "url": "https://en.wikipedia.org/wiki?curid=25439126", "title": "Vulva", "text": "Vulva\n\nThe vulva (plural vulvas or vulvae; derived from Latin for wrapper or covering) consists of the external female sex organs. The vulva includes the mons pubis, labia majora, labia minora, clitoris, vestibular bulbs, vulval vestibule, urinary meatus, the vaginal opening, and Bartholin's and Skene's vestibular glands. The urinary meatus is also included as it opens into the vulval vestibule. Other features of the vulva include the pudendal cleft, sebaceous glands, the urogenital triangle (anterior part of the perineum), and pubic hair. The vulva includes the entrance to the vagina, which leads to the uterus, and provides a double layer of protection for this by the folds of the outer and inner labia. Pelvic floor muscles support the structures of the vulva. Other muscles of the urogenital triangle also give support.\n\nBlood supply to the vulva comes from the three pudendal arteries. The internal pudendal veins give drainage. Afferent lymph vessels carry lymph away from the vulva to the inguinal lymph nodes. The nerves that supply the vulva are the pudendal nerve, perineal nerve, ilioinguinal nerve and their branches. Blood and nerve supply to the vulva contribute to the stages of sexual arousal that are helpful in the reproduction process.\n\nFollowing the development of the vulva, changes take place at birth, childhood, puberty, menopause and post-menopause. There is a great deal of variation in the appearance of the vulva particularly in relation to the labia minora. The vulva can be affected by many disorders which may often result in irritation. Vulvovaginal health measures can prevent many of these. Other disorders include a number of infections and cancers. There are several vulval restorative surgeries known as genitoplasties, and some of these are also used as cosmetic surgery procedures.\n\nDifferent cultures have held different views of the vulva. Some ancient religions and societies have worshipped the vulva and revered the female as a goddess. Major traditions in Hinduism continue this. In western societies there has been a largely negative attitude typified by the medical terminology of , meaning parts to be ashamed of. There has been an artistic reaction to this in various attempts to bring about a more positive and natural outlook, such as work from British, American, and Japanese artists. While the vagina is a separate part of the anatomy, it has often been used synonymously with vulva.\n\nThe main structures of the vulva are: the mons pubis, the labia majora and labia minora, the external parts of the clitoris – the clitoral hood and the glans, the urinary meatus, the vaginal opening and hymen, and Bartholin's and Skene's vestibular glands. Other features include the pudendal cleft, pubic hair, sebaceous glands, the vulval vestibule, and the urogenital triangle.\n\nThe mons pubis is the soft mound of fatty tissue at the front of the vulva, in the pubic region covering the pubic bone. is Latin for \"pubic mound\" and is present in both sexes to act as a cushion during sexual intercourse, and is more pronounced in the female. A variant term called the mons veneris ('mound of Venus') is sometimes used specifically for women. The lower part of the mons pubis is divided by a fissure – the pudendal cleft – which separates the mons pubis into the labia majora. After puberty, the clitoral hood and the labia minora can protrude into the pudendal cleft in a variable degree. The mons and labia majora become covered in pubic hair at puberty.\n\nThe labia majora and the labia minora cover the vulval vestibule. The outer pair of folds, divided by the pudendal cleft, are the labia majora, (New Latin for \"larger lips\"). They contain and protect the other structures of the vulva. The labia majora meet at the front at the mons pubis, and meet posteriorly at the urogenital triangle (the anterior part of the perineum) between the pudendal cleft and the anus. The labia minora are often pink or brownish black, relevant to the person's skin color.\n\nThe grooves between the labia majora and labia minora are called the interlabial sulci, or interlabial folds. The labia minora (smaller lips) are the inner two soft folds, within the labia majora. They have more color than the labia majora and contain numerous sebaceous glands. They meet posteriorly at the frenulum of the labia minora, a fold of restrictive tissue. The labia minora meet again at the front of the vulva to form the clitoral hood, also known as the prepuce.\n\nThe visible portion of the clitoris is the clitoral glans. Typically, this is roughly the size and shape of a pea, and can vary in size from about 6 mm to 25 mm. The size can also vary when it is erect. The clitoral glans contains as many nerve endings as the much larger homologous glans penis in the male, which makes it highly sensitive. The only known function of the clitoris is to focus sexual feelings. The clitoral hood is a protective fold of skin which varies in shape and size, and it may partially or completely cover the clitoris. The clitoris is the homologue of the penis, and the clitoral hood is the female equivalent of the male foreskin, and may be partially or completely hidden within the pudendal cleft.\n\nThere is a great deal of variation in the appearance of female genitals. Much of this variation lies in the significant differences in the size, shape, and colour of the labia minora. Though called the smaller lips they can often be of considerable size and may protrude outside the vagina or labia majora. This variation has also been evidenced in a large display of 400 vulval casts called the \"Great Wall of Vagina\" created by Jamie McCartney to fill the lack of information of what a normal vulva looks like. The casts taken from a large and varied group of women showed clearly that there is much variation. Pubic hair also varies in its colour, texture, and amount of curl.\n\nThe area between the labia minora where the vaginal opening and the urinary meatus are located is called the vulval vestibule, or vestibule of the vagina. The urinary meatus is below the clitoris and just in front of the vaginal opening which is near to the perineum. The term \"introitus\" is more technically correct than \"opening\", since the vagina is usually collapsed, with the opening closed. The introitus is sometimes partly covered by a membrane called the hymen. The hymen will usually rupture during the first episode of vigorous sex, and the blood produced by this rupture has been seen to signify virginity. However, the hymen may also rupture spontaneously during exercise or be stretched by normal activities such as the use of tampons and menstrual cups, or be so minor as to be unnoticeable, or be absent. In some rare cases, the hymen may completely cover the vaginal opening, requiring a surgical procedure called a hymenotomy. On either side of the back part of the vaginal opening are the two greater vestibular glands known as Bartholin's glands. These glands secrete mucus and a vaginal and vulval lubricant. They are homologous to the bulbourethral glands in the male. The lesser vestibular glands known as Skene's glands, are found on the anterior wall of the vagina. They are homologues of the male prostate gland and are also referred to as the female prostate.\n\nPelvic floor muscles help to support the vulvar structures. The voluntary, pubococcygeus muscle, part of the levator ani muscle partially constricts the vaginal opening. Other muscles of the urogenital triangle support the vulvar area and they include the transverse perineal muscles, the bulbospongiosus, and the ischiocavernosus muscles. The bulbospongiosus muscle decreases the vaginal opening. Their contractions play a role in the vaginal contractions of orgasm by causing the vestibular bulbs to contract.\n\nThe tissues of the vulva are highly vascularised and blood supply is provided by the three pudendal arteries. Venous return is via the external and internal pudendal veins.\nThe organs and tissues of the vulva are drained by a chain of superficial inguinal lymph nodes located along the blood vessels.\n\nThe ilioinguinal nerve originates from the first lumbar nerve and gives branches that include the anterior labial nerves which supply the skin of the mons pubis and the labia majora. The perineal nerve is one of the terminal branches of the pudendal nerve and this branches into the posterior labial nerves to supply the labia. The pudendal nerve branches include the dorsal nerve of clitoris which gives sensation to the clitoris. The clitoral glans is seen to be populated by a large number of small nerves, a number that decreases as the tissue changes towards the urether. The density of nerves at the glans indicates that it is the center of heightened sensation. Cavernous nerves from the uterovaginal plexus supply the erectile tissue of the clitoris. These are joined underneath the pubic arch by the dorsal nerve of the clitoris.\nThe pudendal nerve enters the pelvis through the lesser sciatic foramen and continues medial to the internal pudendal artery. The point where the nerve circles the ischial spine is the location where a pudendal block of local anesthetic can be administered to inhibit sensation to the vulva. A number of smaller nerves split off from the pudendal nerve. The deep branch of the perineal nerve supplies the muscles of the perineum and a branch of this supplies the bulb of the vestibule.\n\nIn week three of the development of the embryo, mesenchyme cells from the primitive streak migrate around the cloacal membrane. Early in the fifth week the cells form two swellings called the cloacal folds. The cloacal folds meet in front of the cloacal membrane and form a raised area known as the genital tubercle The urorectal septum fuses with the cloacal membrane to form the perineum. This division creates two areas one surrounded by the urethral folds and the other by the anal folds. These areas become the urogenital triangle and the anal triangle. The area between the vagina and the anus is known as the clinical perineum.\n\nAt the same time a pair of swellings on either side of the urethral folds known as the genital swellings develop into the labioscrotal swellings. Sexual differentiation takes place, and at the end of week 6 in the female, hormones stimulate further development and the genital tubercle bends and forms the clitoris. The urethral folds form the labia minora and the labioscrotal swellings form the labia majora. At this time the sexes still cannot be distinguished. The appearance of the external genitalia is similar in male and female embryos until the twelfth week and even then is difficult to distinguish.\n\nThe uterovaginal canal or genital canal, forms in the third month of the development of the urogenital system. The lower part of the canal is blocked off by a plate of tissue, the vaginal plate. This tissue develops and lengthens during the third to fifth months and the lower part of the vaginal canal is formed by a process of desquamation or cell shedding. The end of the vaginal canal is blocked off by an endodermal membrane which separates the opening from the vestibule. In the fifth month the membrane degenerates but leaves a remnant called the hymen.\n\nOrgans in the male and female with a shared common ancestry are said to be homologous. The clitoral glans is homologous to the male glans penis, and the clitoral body and the clitoral crura are homologous to the corpora cavernosa of the penis. The labia majora is homologous to the scrotum; the clitoral hood is homologous to the foreskin, and the labia minora is homologous to the spongy urethra. The vestibular bulbs beneath the skin of the labia minora are homologous to the corpus spongiosum, the tissue of the penis surrounding the urethra, and to the bulb of the penis. Bartholin's glands are homologous to the bulbourethral glands in males.\n\nThe newborn's vulva may be swollen or enlarged as a result of having been exposed, via the placenta, to her mother's increased levels of hormones. The labia majora are closed. These changes disappear over the first few months. During childhood before puberty, the lack of estrogen can cause the labia to become sticky and to ultimately join firmly together. This condition is known as labial fusion and is rarely found after puberty when oestrogen production has increased.\n\nPuberty is the onset of the ability to reproduce, and takes place over two to three years, producing a number of changes. The structures of the vulva become proportionately larger and may become more pronounced. Pubarche, the first appearance of pubic hair develops, firstly on the labia majora, and later spreads to the mons pubis, and sometimes to the inner thighs and perineum. Pubic hair is much coarser than other body hair, and is considered a secondary sex characteristic. Pubarche can occur independently of puberty. \"Premature pubarche\" may sometimes indicate a later metabolic-endocrine disorder seen at adolescence. The disorder sometimes known as a \"polyendocrine disorder\" is marked by elevated levels of androgen, insulin, and lipids, and may originate in the fetus. Instead of being seen as a normal variant it is proposed that premature pubarche may be seen as a marker for these later endocrine disorders.\n\nApocrine sweat glands secrete sweat into the pubic hair follicles. This is broken down by bacteria on the skin and produces an odor, which some consider to act as an attractant sex pheromone. The labia minora may grow more prominent and undergo changes in color. At puberty the first monthly period known as menarche marks the onset of menstruation.\nIn prepubertal girls the skin of the vulva is thin and delicate, and its neutral pH makes it prone to irritation. The production of the female sex hormone estradiol (an estrogen) at puberty, causes the perineal skin to thicken by keratinising, and this reduces the risk of infection. Estrogen also causes the laying down of fat in the development of the secondary sex characteristics. This contributes to the maturation of the vulva with increases in the size of the mons pubis, and the labia majora and the enlargement of the labia minora.\n\nIn pregnancy the vulva and vagina take on a bluish colouring due to venous congestion. This appears between the eighth and twelfth week and continues to darken as the pregnancy continues. Estrogen is produced in large quantities during pregnancy and this causes the external genitals to become enlarged. The vaginal opening and the vagina are also enlarged. After childbirth a vaginal discharge known as lochia is produced and continues for about ten days.\n\nDuring menopause, hormone levels decrease, which causes changes in the vulva known as vulvovaginal atrophy. The decreased estrogen affects the mons, the labia, and the vaginal opening and can cause pale, itchy, and sore skin. Other visible changes are a thinning of the pubic hair, a loss of fat from the labia majora, a thinning of the labia minora, and a narrowing of the vaginal opening. This condition has been renamed by some bodies as the \"genitourinary syndrome of menopause\" as a more comprehensive term.\n\nThe vulva has a major role to play in the reproductive system. It provides entry to, and protection for the uterus, and the right conditions in terms of warmth and moisture that aids in its sexual and reproductive functions. The external organs of the vulva are richly innervated and provide pleasure when properly stimulated. The mons pubis provides cushioning against the pubic bone during intercourse.\n\nA number of different secretions are associated with the vulva, including urine (from the urethral opening), sweat (from the apocrine glands), menses (leaving from the vagina), sebum (from the sebaceous glands), alkaline fluid (from the Bartholin's glands), mucus (from the Skene's glands), vaginal lubrication from the vaginal wall and smegma. Smegma is a white substance formed from a combination of dead cells, skin oils, moisture and naturally occurring bacteria, that forms in the genitalia. In females this thickened secretion collects around the clitoris and labial folds. It can cause discomfort during sexual activity as it can cause the clitoral glans to stick to the hood, and is easily removed by bathing. Aliphatic acids known as copulins are also secreted in the vagina. These are believed to act as pheromones. Their fatty acid composition, and consequently their odor changes in relation to the stages of the menstrual cycle.\n\nThe clitoris and the labia minora are both erogenous areas in the vulva. Local stimulation can involve the clitoris, vagina and other perineal regions. The clitoris is the most sensitive. Sexual stimulation of the clitoris (by a number of means) can result in widespread sexual arousal, and if maintained can result in an orgasm. Stimulation to orgasm is optimally achieved by a massaging sensation.\n\nSexual arousal results in a number of physical changes in the vulva. During arousal vaginal lubrication increases. Vulva tissue is highly vascularised; arterioles dilate in response to sexual arousal and the smaller veins will compress after arousal, so that the clitoris and labia minora increase in size. Increased vasocongestion in the vagina causes it to swell, decreasing the size of the vaginal opening by about 30%. The clitoris becomes increasingly erect, and the glans moves towards the pubic bone, becoming concealed by the hood. The labia minora increase considerably in thickness. The labia minora sometimes change considerably in color, going from pink to red in lighter skinned women who have not borne a child, or red to dark red in those that have. Immediately prior to an orgasm, the clitoris becomes exceptionally engorged, causing the glans to appear to retract into the clitoral hood. Rhythmic muscle contractions occur in the outer third of the vagina, as well as the uterus and anus. Contractions become less intense and more randomly spaced as the orgasm continues. The number of contractions that accompany an orgasm vary depending on its intensity. An orgasm may be accompanied by female ejaculation, causing liquid from either the Skene's gland or bladder to be expelled through the urethra. The pooled blood begins to dissipate, although at a much slower rate if an orgasm has not occurred. The vagina and vaginal opening return to their normal relaxed state, and the rest of the vulva returns to its normal size, position and color.\n\nIrritation and itching of the vulva is called pruritus vulvae. This can be a symptom of many disorders, some of which may be determined by a patch test. The most common cause of irritation is thrush, a fungal infection. Vulvovaginal health measures can help to prevent many disorders including thrush. Infections of the vagina such as vaginosis and of the uterus may produce vaginal discharge which can be an irritant when it comes into contact with the vulvar tissue. Inflammation as vaginitis, and vulvovaginitis can result from this causing irritation and pain. Ingrown hairs resulting from pubic hair shaving can cause folliculitis where the hair follicle becomes infected; or give rise to an inflammatory response known as pseudofolliculitis pubis. A less common cause of irritation is genital lichen planus another inflammatory disorder. A severe variant of this is \"vulvovaginal-gingival syndrome\" which can lead to narrowing of the vagina, or vulva destruction. Many types of infection and other diseases including some cancers may cause irritation.\n\nVulvar organs and tissues can become affected by different infectious agents such as bacteria and viruses, or infested by parasites such as lice and mites. Over thirty types of pathogen can be sexually transmitted, and many of these affect the genitals. Most STIs do not produce symptoms or symptoms may be mild and not be indicative of an STI. The practice of safe sex can greatly reduce the risk of infection from many sexually transmitted pathogens. The use of condoms (either male or female condoms) is one of the most effective methods of protection.\n\nBacterial infections include: chancroid – characterised by genital ulcers known as chancres; granuloma inguinale showing as inflammatory granulomas often described as nodules; syphilis –the primary stage classically presents with a single chancre, a firm, painless, non-itchy ulcer, but there may be multiple sores; and gonorrhea that very often presents no symptoms but can result in discharge.\n\nViral infections include human papillomavirus infection (HPV) – this is the most common STI and has many types. Genital HPV can cause genital warts. There have been links made between HPV and vulvar cancer, though HPV most often causes cervical cancer. Genital herpes is mostly asymptomatic but can present with small blisters that break open into ulcers. HIV/AIDS is mostly transmitted through sexual activity, and the vulva in some cases can be affected by sores. \nA highly contagious viral infection is molluscum contagiosum which is transmissible on close contact and causes water warts.\n\nParasitic infections include trichomoniasis, pediculosis pubis, and scabies. Trichomoniasis is transmitted by a parasitic protozoan and is the most common non-viral STI. Most cases are asymptomatic but may present symptoms of irritation and a discharge of unusual odor. Pediculosis pubis commonly called \"crabs\", is a disease caused by the crab louse an ectoparasite. When the pubic hair is infested the irritation produced can be intense. Scabies, also known as the \"seven year itch\", is caused by another ectoparasite, the mite \"Sarcoptes scabiei\", giving intense irritation.\n\nMany malignancies can develop in vulvar structures. Most vulvar cancers are squamous cell carcinomas and are usually found in the labia particularly the labia majora. The second most common vulval cancer (though not very common) is vulval melanoma. Signs and symptoms can include: itching, or bleeding; skin changes including rashes, sores, lumps or ulcers, and changes in vulval skin coloration. Pelvic pain might also occur especially during urinating and sex. Vulval melanoma usually affects women over the age of fifty, and affects white women more than black women. A vulvectomy may need to be performed in order to remove some or all of the vulva. This procedure is usually performed as a last resort in certain cases of cancer, vulvar dysplasia or vulvar intraepithelial neoplasia.\n\nLabial fusion, also called \"labial adhesion\", is the fusion of the labia minora. This affects a number of young girls and is not considered unduly problematic. The condition can usually be treated using creams, or it may right itself with the release of hormones at the onset of puberty.\n\nVulvodynia is chronic pain in the vulvar region. There is no single identifiable cause. A subtype of this is vulvar vestibulitis but since this is not thought to be an inflammatory condition it is more usually referred to as \"vestibulodynia\". Vulvar vestibulitis usually affects pre-menopausal women.\n\nA number of skin disorders such as lichen sclerosus, and lichen simplex chronicus can affect the vulva. Crohn's disease of the vulva is an uncommon form of metastatic Crohn's disease which manifests as a skin condition showing as hypertrophic lesions or vulvar abscesses. Papillary hidradenomas are nodules that can ulcerate and are mostly found on the skin of the labia or of the interlabial folds. Another more complex ulcerative condition is hidradenitis suppurativa which is characterised by painful cysts that can ulcerate, and recur, and can become chronic lasting for many years. Chronic cases can develop into squamous cell carcinomas. An asymptomatic skin disorder of the vulval vestibule is vestibular papillomatosis which is characterised by fine, pink projections from either the epithelium of the vulva or from the labia minora. Dermatoscopy can distinguish this condition from genital warts. A subtype of psoriasis, an autoimmune disease, is inverse psoriasis in which red patches can appear in the skin folds of the labia.\n\nThe vulvar region is at risk for trauma during childbirth.\nDuring childbirth, the vagina and vulva must stretch to accommodate the baby's head (approximately ). This can result in tears known as perineal tears in the vaginal opening, and other structures within the perineum. An episiotomy (a pre-emptive surgical cutting of the perineum) is sometimes performed to facilitate delivery and limit tearing. A tear takes longer to heal than an incision. Tears and incisions may be repaired using sutures that may be layered. Among the methods of hair removal evaluated for pre-surgeries, pubic hair shaving known as \"prepping\", was seen to increase the risk of surgical site infections. No advantages have been demonstrated in the routine shaving of pubic hair prior to childbirth.\n\nGenitoplasties are plastic surgeries that can be carried out to repair, restore or alter vulvar tissues, particularly following damage caused by injury or cancer treatment. These procedures include vaginoplasty which can also be performed as a cosmetic surgery. Other cosmetic surgeries to change the appearance of external structures include labiaplasties. Some of these procedures, vaginoplasties and labiaplasties, are also carried out as sex reassignment surgeries.\n\nThe use of cosmetic surgeries has been criticized by clinicians. The American College of Obstetricians and Gynecologists recommends that women be informed of the risks of these surgeries. They refer to the lack of data relevant to their safety and effectiveness and to the potential associated risks such as infection, altered sensation, dyspareunia, adhesions, and scarring. There is also a percentage of people seeking cosmetic surgery who may be suffering from body dysmorphic disorder and surgery in these cases can be counterproductive.\n\nIn some cultural practices, particularly in the African Khoikhoi and Rwanda cultures, the labia minora are purposefully stretched by repeated pulling on them and sometimes by using attached weights. Labia stretching is a recognised, familial cultural practice in parts of Eastern and Southern Africa. This is a desired and encouraged practice by the women (starting at puberty) in order to promote better sexual satisfaction for both parties. The achieved extensions can hang down below the labia majora for up to seven inches.\n\nIn some cultures, including modern Western culture, women have shaved or otherwise removed the hair from part or all of the vulva. When high-cut swimsuits became fashionable, women who wished to wear them would remove the hair on either side of their pubic triangles, to avoid exhibiting pubic hair. Other women relish the beauty of seeing their vulva with hair, or completely hairless, and find one or the other more comfortable. The removal of hair from the vulva is a fairly recent phenomenon in the United States, Canada, and Western Europe, usually in the form of bikini waxing or Brazilian waxing, but has been prevalent in many Eastern European and Middle Eastern cultures for centuries, usually due to the idea that it may be more hygienic, or originating in prostitution and pornography. Hair removal may include all, most, or some of the hair. French waxing leaves a small amount of hair on either side of the labia or a strip directly above and in line with the pudendal cleft called a \"landing strip\". Islam teaching includes Muslim hygienical jurisprudence a practice of which is the removal of pubic hair.\nSeveral forms of genital piercings can be made in the female genital area, and include the Christina piercing, the Nefertiti piercing, the fourchette piercing, and labia piercings. Piercings are usually performed for aesthetic purposes, but some forms like the clitoral hood piercing might also enhance pleasure during sex. Though they are common in traditional cultures, intimate piercings are a fairly recent trend in Western society.\n\nFemale genital surgery includes laser resurfacing of the labia to remove wrinkles, labiaplasty (reducing the size of the labia) and vaginoplasty. In September 2007, the American College of Obstetricians and Gynecologists (ACOG) issued a committee opinion on these and other female genital surgeries, including \"vaginal rejuvenation\", \"designer vaginoplasty\", \"revirgination\", and \"G-spot amplification\". This opinion states that the safety of these procedures has not been documented. The ACOG and the ISSVD recommend that women seeking these surgeries need to be informed about the lack of data supporting these procedures and the potential associated risks such as infection, altered sensation, dyspareunia, adhesions, and scarring.\n\nWith the growing popularity of female cosmetic genital surgeries, the practice increasingly draws criticism from an opposition movement of cyberfeminist activist groups and platforms, called the labia pride movement. The major point of contention is that heavy advertising for these procedures, in combination with a lack of public education, fosters body insecurities in women with larger labia in spite of the fact that there is normal and pronounced individual variation in the size of labia. The preference for smaller labia is a matter of a fashion fad and is without clinical or functional significance.\nThe most prevalent form of non-consensual genital alteration is that of female genital mutilation. This mostly involves the partial or complete removal of genital organs. Female genital mutilation is carried out in thirty countries in Africa, and Asia with more than 200 million girls being affected, and some women (as of 2018). Nearly all of the procedures are carried out on young girls. The practices are also carried out globally among migrants from these areas. Female genital mutilation is claimed to be mostly carried out for cultural traditional reasons.\n\nThe word \"vulva\" is Latin for \"womb\". It derives from the 1540s in referring to the womb and female sexual organs, from the earlier \"volvere\" meaning to turn, roll or revolve, with further derivatives such as used in volvox, and volvulus (twisted bowel). The naming of the female (and male) genitals as , meaning parts to be ashamed of, dates from the mid-17th century. The naming influenced the general perception of the vulva and this is shown in depicted gynaecological procedures. The examiner shown in the \"Obstetrical examination\" dated 1822, is adopting the compromise procedure where the woman's genitals cannot be seen.\n\nThere are many sexual slang terms used for the vulva. Cunt, a medieval word for the vulva and once the standard term, has become a vulgarism, and in other uses one of the strongest offensive and abusive swearwords in English-speaking cultures. The word has been replaced in normal usage by a few euphemisms including pussy (vulgar slang) and fanny (UK) which used to be a common pet name. In the UK these terms have other non-sexual meanings that lend themselves to \"double entendres\", such as pussy which is used as a term of endearment for a pet cat – pussy cat. In North American informal use the term pussy can also refer to a weak or effeminate man, and fanny is a term used for the buttocks. Other slang terms are muff, snatch, twat, and crotch. Vagina is often used as a synonym for vulva even though it is a separate part of the anatomy.\n\nSome cultures have long celebrated and even worshipped the vulva. During the Uruk period ( 4000–3100 BC), the ancient Sumerians regarded the vulva as sacred and a vast number of Sumerian poems praising the vulva of Inanna, the goddess of love, sex, and fertility, have survived. In Sumerian religion, the goddess Nin-imma is the divine personification of female genitalia. Vaginal fluid is always described in Sumerian texts as tasting \"sweet\" and, in a Sumerian bridal hymn, a young maiden rejoices that her vulva has grown hair. Clay models of vulvas were discovered in the temple of Inanna at Ashur.\nSome major Hindu traditions such as Shaktism, a goddess-centred tradition, revere the vulva and vagina under the name yoni. The goddess as Devi is worshipped as the supreme deity. The yoni is a representation of the female deity and is found in many temples as a focus for prayer and offerings. It is also represented symbolically as a mudra in spiritual practices, including yoga.\n\nSheela na gigs are figurative carvings of naked women displaying an exaggerated vulva. They are found in ancient and medieval European contexts. They are displayed on many churches, but their origin and significance is debatable. A main line of thinking is that they were used to ward off evil spirits. Another view is that the sheela na gig was a divine assistant in childbirth. Starr Goode explores the image and possible meanings of the Sheela na gig and Baubo images in particular, but writes also about the recurring image worldwide. Through hundreds of photographs, she demonstrates that the image of a female displaying her vulva is not specific to European religious art or architecture, but that similar images are found in the visual arts and in mythical narratives of goddesses and heroines parting their thighs to reveal what she calls, \"sacred powers.\" Her theory is that \"the image is so rooted in our psyches that it seems as if the icon is the original cosmological center of the human imagination.\"\n\" (\"Origin of the world\") painted by Gustave Courbet in 1866 was an early Realist painting of a vulva that only became exhibited many years later.\n\nJapanese sculptor and manga artist Megumi Igarashi has focused much of her work on painting and modelling vulvas and vulva-themed works. She has used molds to create dioramas – three-dimensional models of her vulva with the hope of demystifying the female genitals.\n\nAn art installation called \"The Dinner Party\" by feminist artist, Judy Chicago, portrays a symbolic history of famous women. The dinner plates each depict an elaborate vulval form and they are arranged in a triangular vulva shape. Another installation was made by British artist Jamie McCartney who used the casts of four hundred vulvas to create \"The Great Wall of Vagina\" in 2011. The vagina casts are life-size. Explanations written by the project's sexual health adviser accompany these. The purpose of the artist was to \"address some of the stigmas and misconceptions that are commonplace\".\n\n"}
{"id": "7532434", "url": "https://en.wikipedia.org/wiki?curid=7532434", "title": "Walk-in clinic", "text": "Walk-in clinic\n\nA walk-in clinic in the United States describes a very broad category of medical facilities loosely defined as those that accept patients on a walk-in basis and with no appointment required. A number of healthcare service providers fall under the walk-in clinic umbrella including urgent care centers, retail clinics and even many free clinics or community health clinics. Walk-in clinics offer the advantages of being accessible and often inexpensive. \nIt is estimated that there are nearly 11,000 walk-in clinics in America, although it is impossible to calculate an exact number given the variable and ill-defined nature of the category. Urgent care centers make up the largest percentage of walk-in clinics in America with an estimated 9,000 locations nationwide. In fact, consumers often erroneously refer to all walk-in clinics as urgent care centers, and vice versa. Retail clinics are the next most prevalent in the industry with 1,443 locations as of July 1, 2013.\n\nUrgent care clinics are usually led by physicians. The much smaller category of retail clinics, which are stand-alone clinics located inside large retail stores or shopping malls, tend to be headed by nurse practitioners. The significantly higher price for an urgent care visit compared to a retail clinic visit is largely attributed to this difference in staffing.\n\nAll types of walk-in clinics provide basic medical services, such as routine vaccinations, evaluation of cold and flu symptoms, and treatment for less severe physical injuries. Urgent care centers normally provide more services, such as X-ray testing for suspected pneumonia or broken bones.\n\nAccess to the patient's regular medical records depends on the agreements that the clinic has with other organizations. For example, a walk-in clinic that is part of or affiliated with a hospital or larger clinic may have full access to all the medical records belonging to the larger institution, while an independent walk-in clinic may not have access any patient records except those related to previous visits to that walk-in clinic. This lack of access can prevent healthcare providers from recognizing chronic problems.\n\nAccording to the brand tracker published by Urgent Care Locations from Solv (as of April 2018), the following are the top twenty walk-in clinic brands:\n\nThe existence of walk-in clinics has been controversial. Doctors acknowledge that Minute Clinics and other retail-based clinics are convenient. They admit that the clinics are cutting into their own income. However, doctors say they are trying to build a relationship with their patients, meet them regularly, and follow up on problems. The clinics interfere with that relationship and fragment health care. Furthermore, said pediatrician Claire McCarthy, \"Sometimes a minor thing isn't so minor.\" The clinics don't have the patient's medical record, and don't know the history. A swollen knee, if it is part of a pattern, might be a sign of arthritis. The American Academy of Pediatrics has recommended that parents do not use retail-based clinics for their children.\n\n"}
{"id": "8766023", "url": "https://en.wikipedia.org/wiki?curid=8766023", "title": "Water supply and sanitation in Germany", "text": "Water supply and sanitation in Germany\n\nPublic water supply and sanitation in Germany is universal and of good quality. Some salient features of the sector compared to other developed countries are its very low per capita water use, the high share of advanced wastewater treatment and very low distribution losses. Responsibility for water supply and sanitation provision lies with municipalities, which are regulated by the states. Professional associations and utility associations play an important role in the sector. As in other EU countries, most of the standards applicable to the sector are set in Brussels (see EU water policy). Recent developments include a trend to create commercial public utilities under private law and an effort to modernize the sector, including through more systematic benchmarking.\n\n\"Source\": Joint Monitoring Program WHO/UNICEF(JMP/2006). Data for water and sanitation based on Health for All database, WHO Regional Office for Europe (1990).\n\nAccess to safe water and adequate sanitation in Germany is universal. More than 99 percent of users are connected to a public water supply system. The remainder is served by private wells. 93 percent of users are connected to sewers. The remainder is connected to various types of on-site sanitation systems.\n\nAbout 80 percent of public water use is accounted for by residential and small commercial users. The remainder is accounted for by industries supplied from public water systems (14 percent) and other users (6 percent).\n\nResidential and small commercial water use is the second lowest among 14 European countries and only a fraction of what it is in North America. Despite forecasts about increasing per capita water use, use actually declined from 145 liter/capita/day in 1990 to only 121 liter/capita/day in 2010.\n\nLow water consumption has had some negative operational, health and even environmental impacts. On the operational side, sewers have to be flushed occasionally with injected drinking water in order to prevent stagnation of raw sewage. On the health side, there are concerns about potable water contamination due to low flows. On the environmental side, in some cities such as Berlin water tables are rising and cause damage to the foundations of buildings because of decreased pumping of groundwater by utilities.\n\nWater is not scarce in Germany, except for occasional localized droughts. Public water utilities abstract only 3 percent of total renewable water resources in Germany, or 5.4 billion cubic metres out of 182 billion cubic metres annually.\n\nThe sources of public water supply are as follows:\n\n\nWater supply in Germany is continuous, at good pressure, and drinking water quality is excellent, as evidenced by the universal compliance with the EU drinking water directive. Wastewater treatment is universal. 94 percent of municipal wastewater is treated according to the highest EU standards including nutrient elimination, a much higher percentage than in France (36 percent) or in England and Wales (39 percent).\n\nAccording to a 2007 national survey for the business association BDEW (BDEW customer barometer) 92% of customers were satisfied or very satisfied with the quality of their drinking water. 82% were satisfied or very satisfied with the service provided by their drinking water provider. 79% were satisfied or very satisfied with the service provided by their wastewater utility. The survey also showed that customers significantly overestimate the price of water and wastewater services compared to the actual price.\n\nIt is estimated that the total number of those directly employed in German water and sanitation utilities is far more than 100,000.\n\nThe length of the drinking water network in Germany is estimated to be more than 500,000 km. The length of the sewer network in 2004 was estimated by the Federal Statistical Office to be 515,000 km, divided as follows:\n\n\nThere were 9,994 wastewater treatment plants in Germany in 2004.\n\nPublic water supply and sanitation in Germany are responsibilities of municipalities, of which there were more than 12,000 in 2008. Smaller municipalities often associate in municipal associations to provide water and/or sanitation services. Municipalities or municipal associations in turn can delegate these responsibilities to municipal companies, private companies or public-private partnerships.\n\nThere are about 6,400 public water service providers and about 6,900 sanitation service providers in Germany. With a few exceptions, water and sanitation services are typically provided by different entities in the same locality, with sanitation bills being collected by the water utility on behalf of the entity in charge of sanitation.\n\nAmong the 1,266 larger water service providers about 15 percent are municipal utilities under public law (Eigenbetriebe); 16 percent are inter-municipal utilities (Zweckverbände); 63 percent are utilities under private or mixed law either under private, public or mixed ownership.; and 6 percent are water and land associations (Wasser- und Bodenverbände). Only 3.5 percent of service providers were entirely privately owned (no figures are available on companies with mixed ownership, an increasingly prevalent form of ownership).\n\nUnlike public water supply, sanitation is considered a sovereign core responsibility (hoheitliche Kernaufgabe) of municipalities in Germany. This implies that, unlike water supply, it is exempt from VAT and corporate taxes. It also implies that companies under private law cannot directly provide sanitation services. The great majority of municipalities thus provide sanitation directly through a municipal sanitation department (Regiebetrieb). Less than 10 percent of the 6,000 sanitation providers are utilities under public law, and none are utilities under private law. However, municipalities or municipal utilities can sign operating contracts (Betreiberverträge) with private companies. Out of the 900 largest sanitation service providers, about 10 percent have signed such contracts for sewerage services and 12 percent for wastewater treatment services.\n\nThe largest privately owned public water company is Gelsenwasser AG, although 92,9% of it are still owned by various municipalities, which is a multi-utility company (water, sanitation and natural gas distribution) serving 3.2 million inhabitants in North Rhine-Westphalia, under concession agreements with 39 municipalities, and many other localities throughout Germany and internationally.\n\nAn example of a publicly owned large multi-utility (water, electricity generation and distribution, natural gas distribution) is the Mainova AG in Frankfurt.\n\nThe Berliner Wasserbetriebe, an Institution under Public Law (\"Anstalt des öffentlichen Rechts\"), is the largest communal water service provider after its remunicipalisation in 2013, serving 3.5 million people with water and 3.9 million people with sanitation services.\n\nResponsibility for policy setting in public water supply and sanitation in Germany is shared between the EU, the federal government and state governments (Länder). (For more details on the role of the Länder and municipalities see States of Germany) The EU sets the framework legislation for water quality and water resources management (see EU water policy). The organization of public water supply and sanitation, however, remains a prerogative of EU member states. The German states (Länder) play a key role in the sector by setting, among other things, the legal framework for tariff approvals. Municipalities, legally entrusted with service provision, play an indirect role in influencing policy positions related to water and sanitation through their influential municipal associations (the Deutsche Städtetag representing the largest cities and towns and the Deutscher Städte- und Gemeindebund representing smaller cities and towns).\n\nThere are no autonomous regulatory agencies for water and sanitation in Germany at the state or federal level. The recently created \"federal\" regulatory agency for network industries (Bundesnetzagentur) covers telecommunications, postal services, electricity, gas and rail. It does not cover water supply and sanitation, since it is a responsibility of the states. Water and sanitation tariffs are approved through different procedures in each state, usually by a department in the state Ministry of Economy after a review of the tariff increase request by an independent auditor. In city-states (Berlin, Hamburg, Bremen) this means that the Minister (called Senator) of Economy both requests the tariff increase in his capacity as chairman of the board of the utility and also approves it, which constitutes a conflict of interest. In the case of some private utilities, tariffs are set by a mutually agreed arbitrator based on the professional opinion of an auditor.\n\nDrinking water quality is monitored by the public health departments of municipalities and counties (Landkreise). Environmental monitoring is largely based on self-monitoring, which has proven to be reliable, and occasional samples by environmental Ministries of the states.\n\nIndustry associations and professional associations also play an important role in self-regulating the water and sanitation sector (verbandliche Selbstverwaltung). In early 2007 there were six associations in the sector. They include two industry associations, the Association of Electricity and Water Utilities BDEW and the VKU (association of municipal utilities); two professional associations, the DWA (professional association for water and sanitation), BVGW (professional association for gas and water); and two associations specialized on sub-sectors, the ATT (working group of dam operators providing drinking water) and DBVW (association of land and water associations). In particular the two professional associations play an important role assisting in the development of technical norms and, more recently, in performance benchmarking.\n\nA study commissioned in 2000 by the Ministry of Economy suggested to liberalize the German water sector, allowing competition similar to the telecommunications and electricity sectors. The proposal met with harsh criticism, including from the Federal Environment Agency (UBA) and the associations of municipalities, which alleged that liberalization could entail setbacks for the protection of health and the environment.\nThe liberalization proposal was not further pursued. However, public-private partnerships continued to become more widespread and the trend towards the creation of private law water utilities (commercialization) continued.\n\nIn reaction to the liberalization debate the German Federal Parliament (Bundestag) passed a decision sponsored by the Green party and the Social-democrats (SPD) on sustainable water supply and sanitation (nachhaltige Wasserwirtschaft) in 2001. The decision rejected the liberalization of the water sector, but also called for the merging of smaller service providers, higher competitiveness and the general modernization of the sector, including through systematic performance benchmarking. In 2005, the six professional associations signed a declaration promoting benchmarking, based on a methodology developed by the International Water Association.\n\nWater losses in the distribution network have been estimated at only 7 percent in 2001, down from 11 percent in 1991. According to a study commissioned by the BGW losses are 19 percent in England/Wales, 26 percent in France and 29 percent in Italy. These would not only be the lowest water losses in the four countries, but also in the world. The study states that its methodology allows for an accurate comparison, including water used to flush pipes and for firefighting. This is consistent with the International Water Association's definition of non-revenue water, which includes authorized non-metered consumption such as for flushing and firefighting.\n\nBenchmarking has been undertaken for a long period by German utilities, but not in a comprehensive and systematic manner. In 1998 the Federal Ministry of Education and Research initiated a competition of ideas to reduce the costs of water supply together with the economic research institute RWI and 14 water utilities. It developed a set of criteria to assess strengths and weaknesses in the industry. Participating utilities say that they reduced their operating costs by about 5 percent after two to three years. The professional associations DVGW and DWA have jointly established a voluntary benchmarking system, which keeps individual company data confidential. The associations consider the system as being highly successful.\n\nBy law (Kommunalabgabengesetze or Betriebsgesetze der Länder) tariffs must cover the full costs of water supply and sanitation, including capital replacement and the remuneration of equity. The various state laws do not foresee a review of the level of the efficiency of investments and operations as part of the tariff approval procedure. Some states also levy a resource charge for groundwater abstractions which is passed on by utilities to the consumers. There is no such a charge for surface water abstraction, however.\n\nUtilities also pay a wastewater discharge fee which depends on the degree of pollution of the discharged treated wastewater. The discharge fee is supposed to provide an incentive to treat water beyond what is legally required (Abwasserabgabengesetz). It accounts for about 3 percent of total sanitation costs.\n\nIn 2004 water tariffs averaged 1.81 euro per cubic meter including VAT, and sanitation tariffs averaged 2.14 euro per cubic meter.\n\nAccording to NUS consulting water tariffs in Germany (without sanitation) were the highest of 16 mainly OECD countries at the equivalent of US$2.25 per cubic meter, about on par with tariffs in Denmark.\n\nHowever, according to a study commissioned by the German industry association BGW in 2006, the average household water bill was only 82 euro per year in Germany, lower than in France or in England and Wales, but higher than in Italy. The study shows that subsidies are more prevalent in the three comparator countries and service levels are lower. Taking into account these differences, the cost of supplying water at an equalized service level would be 84 euros in Germany, 106 euro in both France and England/Wales, and 74 euro in Italy. The apparent discrepancy between higher unit tariffs and lower bills is due to the lower water consumption in Germany. Water tariffs have remained stable in real terms over the past ten years.\n\nComparison of annual water and sanitation bills per capita in four EU countries\n\nSource: Metropolitan Consulting Group: Vergleich europäischer Wasser- und Abwasserpreise, 2006 \n\nConcerning sanitation, unequalized tariffs are by far the highest in Germany at 111 euro per year. Equalized costs net of subsidies are, however, highest in England and Wales with 138 euro, followed by France (122 euro), Germany (119 euro) and Italy (85 euro).\n\nMetering is widespread in Germany and almost universal for single family homes. However, many apartments do not have their own meter, so that households living in apartments where only the consumption of the entire house is metered have little financial incentive to conserve water.\n\nIn 2005 investments stood at 7.8 billion euros, including 5.5 billion euros for sanitation and 2.3 billion euros for water supply. Financing is predominantly through debt and ultimately through user fees. Commercial debt is issued directly by the municipalities in the form of municipal bonds (Kommunalanleihen) or by utilities. The development Bank KfW also provides long-term credit for up to 30 years (Kommunalkredit) for municipal investments, including water supply and sanitation.\n\nAccording to the professional associations of the sector there is no investment backlog (Investitionsstau).\n\n\n\n"}
{"id": "11527495", "url": "https://en.wikipedia.org/wiki?curid=11527495", "title": "Zhelin Reservoir", "text": "Zhelin Reservoir\n\nZhelin Reservoir (, Gan: Chā-līm fîkhú) is a reservoir located between Yongxiu (永修), Wuning (武宁) and Xiushui (修水) county. All three counties belong to Jiujiang, a metropolis of Jiangxi province, China. It's 40 kilometers to the north of Nanchang, and 90 kilometers to the south of Mount Lushan. This reservoir is the largest in Jiangxi province with a storage capacity of 7,920,000,000 cubic metres.\n\nAlthough the dam is located in an area of low seismic activity, after the impounding of the Zhelin Reservoir the seismic activity increased markedly.\n\nThe dam of this reservoir is the biggest earth-filled embankment dam in Asia.\n\nThe American reality program \"Survivor\" filmed its fifteenth season, \"\", on islands in the reservoir between June and August 2007. \"Survivor\" host Jeff Probst claimed that this was the first American television series to be filmed entirely within China.\n"}
