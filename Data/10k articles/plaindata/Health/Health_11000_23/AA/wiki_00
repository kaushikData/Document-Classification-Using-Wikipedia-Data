{"id": "33020214", "url": "https://en.wikipedia.org/wiki?curid=33020214", "title": "Alwaleed Philanthropies", "text": "Alwaleed Philanthropies\n\nAlwaleed Philanthropies (formerly the Al Waleed bin Talal Foundation\") is a charitable and philanthropic organization founded by Al-Waleed bin Talal and Princess Ameerah with a mission to help alleviate suffering and transcend international borders globally. The foundation has established centers and programs at institutions of higher education around the world.\n\nThe foundation has a network of partners and NGO’s and has local, regional and international activities to improve global cultural understanding, community development, women’s empowerment and rapid aid for natural disasters including Saudi Arabia and Syria and other countries.\n\nAlwaleed Philanthropies has established centers and programs at institutions of higher education around the world, all devoted to the promotion of better mutual understanding between the Islamic World and the West and has established centers for study at Harvard University, Georgetown University, University of Edinburgh, University of Cambridge, American University of Beirut and the American University in Cairo.\n\nThe goals of the foundation-Saudi Arabia are residential projects, social empowerment of women through education' medical, health and social assistance;humanitarian assistance, and national charities. Princess Ameerah Al Taweel serves as vice-chairwoman of the board and Nadia Bakhurji is the secretary general.\n\nThe goals of the foundation in Lebanon include medical and health assistance, social assistance (orphanages, rest houses), educational assistance (supporting and building schools, universities), development assistance (economic infrastructure including agricultural). Leila Al Solh is the vice-chairwoman of the foundation in Lebanon since its inception in 2003.\n\nThe foundation aspires to improve global cultural understanding through establishing and supporting academic centers, Alleviating poverty around the world through sustainable long-term philanthropic investments, aid to the victims of natural disasters and empowering girls and women.\n"}
{"id": "424015", "url": "https://en.wikipedia.org/wiki?curid=424015", "title": "Asbestosis", "text": "Asbestosis\n\nAsbestosis is long term inflammation and scarring of the lungs due to asbestos. Symptoms may include shortness of breath, cough, wheezing, and chest pain. Complications may include lung cancer, mesothelioma, and pulmonary heart disease.\nAsbestosis is caused by breathing in asbestos fibers. Generally it requires a relatively large exposure over a long period of time. Such levels of exposure typically only occur in those who work with the material. All types of asbestos fibers are associated with concerns. It is generally recommended that currently existing asbestos be left undisturbed. Diagnosis is based upon a history of exposure together with medical imaging. Asbestosis is a type of interstitial pulmonary fibrosis.\nThere is no specific treatment. Recommendations may include influenza vaccination, pneumococcal vaccination, oxygen therapy, and stopping smoking. Asbestosis affected about 157,000 people and resulted in 3,600 deaths in 2015. Asbestos use has been banned in a number of countries in an effort to prevent disease.\n\nThe signs and symptoms of asbestosis typically manifest after a significant amount of time has passed following asbestos exposure, often several decades under current conditions in the US. The primary symptom of asbestosis is generally the slow onset of shortness of breath, especially with physical activity. Clinically advanced cases of asbestosis may lead to respiratory failure. When a physician listens with a stethoscope to the lungs of a person with asbestosis, they may hear inspiratory crackles.\n\nThe characteristic pulmonary function finding in asbestosis is a restrictive ventilatory defect. This manifests as a reduction in lung volumes, particularly the vital capacity (VC) and total lung capacity (TLC). The TLC may be reduced through alveolar wall thickening; however, this is not always the case. Large airway function, as reflected by FEV/FVC, is generally well preserved. In severe cases, the drastic reduction in lung function due to the stiffening of the lungs and reduced TLC may induce right-sided heart failure (cor pulmonale). In addition to a restrictive defect, asbestosis may produce reduction in diffusion capacity and a low amount of oxygen in the blood of the arteries.\n\nThe cause of asbestosis is the inhalation of microscopic asbestos mineral fibers suspended in the air. In the 1930s, E. R. A. Merewether found that greater exposure resulted in greater risk.\n\nAsbestosis is the scarring of lung tissue (beginning around terminal bronchioles and alveolar ducts and extending into the alveolar walls) resulting from the inhalation of asbestos fibers. There are two types of fibers: amphibole (thin and straight) and serpentine (curly). All forms of asbestos fibers are responsible for human disease as they are able to penetrate deeply into the lungs. When such fibers reach the alveoli (air sacs) in the lung, where oxygen is transferred into the blood, the foreign bodies (asbestos fibers) cause the activation of the lungs' local immune system and provoke an inflammatory reaction dominated by lung macrophages that respond to chemotactic factors activated by the fibers. This inflammatory reaction can be described as chronic rather than acute, with a slow ongoing progression of the immune system attempting to eliminate the foreign fibers. Macrophages phagocytose (ingest) the fibers and stimulate fibroblasts to deposit connective tissue. Due to the asbestos fibers' natural resistance to digestion, some macrophages are killed and others release inflammatory chemical signals, attracting further lung macrophages and fibrolastic cells that synthesize fibrous scar tissue, which eventually becomes diffuse and can progress in heavily exposed individuals. This tissue can be seen microscopically soon after exposure in animal models. Some asbestos fibers become layered by an iron-containing proteinaceous material (ferruginous body) in cases of heavy exposure where about 10% of the fibers become coated. Most inhaled asbestos fibers remain uncoated. About 20% of the inhaled fibers are transported by cytoskeletal components of the alveolar epithelium to the interstitial compartment of the lung where they interact with macrophages and mesenchymal cells. The cytokines, transforming growth factor beta and tumor necrosis factor alpha, appear to play major roles in the development of scarring inasmuch as the process can be blocked in animal models by preventing the expression of the growth factors. The result is fibrosis in the interstitial space, thus asbestosis. This fibrotic scarring causes alveolar walls to thicken, which reduces elasticity and gas diffusion, reducing oxygen transfer to the blood as well as the removal of carbon dioxide. This can result in shortness of breath, a common symptom exhibited by individuals with asbestosis.\n\nAccording to the American Thoracic Society (ATS), the general diagnostic criteria for asbestosis are:\n\nThe abnormal chest x-ray and its interpretation remain the most important factors in establishing the presence of pulmonary fibrosis. The findings usually appear as small, irregular parenchymal opacities, primarily in the lung bases. Using the ILO Classification system, \"s\", \"t\", and/or \"u\" opacities predominate. CT or high-resolution CT (HRCT) are more sensitive than plain radiography at detecting pulmonary fibrosis (as well as any underlying pleural changes). More than 50% of people affected with asbestosis develop plaques in the parietal pleura, the space between the chest wall and lungs. Once apparent, the radiographic findings in asbestosis may slowly progress or remain static, even in the absence of further asbestos exposure. Rapid progression suggests an alternative diagnosis.\n\nAsbestosis resembles many other diffuse interstitial lung diseases, including other pneumoconiosis. The differential diagnosis includes idiopathic pulmonary fibrosis (IPF), hypersensitivity pneumonitis, sarcoidosis, and others. The presence of pleural plaquing may provide supportive evidence of causation by asbestos. Although lung biopsy is usually not necessary, the presence of asbestos bodies in association with pulmonary fibrosis establishes the diagnosis. Conversely, interstitial pulmonary fibrosis in the absence of asbestos bodies is most likely not asbestosis. Asbestos bodies in the absence of fibrosis indicate exposure, not disease.\n\nThere is no cure available for asbestosis. Oxygen therapy at home is often necessary to relieve the shortness of breath and correct underlying low blood oxygen levels. Supportive treatment of symptoms includes respiratory physiotherapy to remove secretions from the lungs by postural drainage, chest percussion, and vibration. Nebulized medications may be prescribed in order to loosen secretions or treat underlying chronic obstructive pulmonary disease. Immunization against pneumococcal pneumonia and annual influenza vaccination is administered due to increased sensitivity to the diseases. Those with asbestosis are at increased risk for certain cancers. If the person smokes, quitting the habit reduces further damage. Periodic pulmonary function tests, chest x-rays, and clinical evaluations, including cancer screening/evaluations, are given to detect additional hazards.\n\nThe death of English textile worker Nellie Kershaw in 1924 from pulmonary asbestosis was the first case to be described in medical literature, and the first published account of disease attributed to occupational asbestos exposure. However, her former employers (Turner Brothers Asbestos) denied that asbestosis even existed because the medical condition was not officially recognised at the time. As a result, they accepted no liability for her injuries and paid no compensation, either to Kershaw during her final illness or to her bereaved family after she had died. Even so, the findings of the inquest into her death were highly influential insofar as they led to a parliamentary enquiry by the British Parliament. The enquiry formally acknowledged the existence of asbestosis, recognised that it was hazardous to health and concluded that it was irrefutably linked to the prolonged inhalation of asbestos dust. Having established the existence of asbestosis on a medical and judicial basis, the report resulted in the first Asbestos Industry Regulations being published in 1931, which came into effect on 1 March 1932.\n\nThe first lawsuits against asbestos manufacturers occurred in 1929. Since then, many lawsuits have been filed against asbestos manufacturers and employers, for neglecting to implement safety measures after the link between asbestos, asbestosis and mesothelioma became known (some reports seem to place this as early as 1898 in modern times). The liability resulting from the sheer number of lawsuits and people affected has reached billions of dollars. The amounts and method of allocating compensation have been the source of many court cases, and government attempts at resolution of existing and future cases.\n\nTo date, about 100 companies have declared bankruptcy at least partially due to asbestos-related liability. In accordance with Chapter 11 and § 524(g) of the federal bankruptcy code, a company may transfer its liabilities and certain assets to an asbestos personal injury trust, which is then responsible for compensating present and future claimants. Since 1988, 60 trusts have been established to pay claims with about $37 billion in total assets. From 1988 through 2010, analysis from the United States Government Accountability Office indicates that trusts have paid about 3.3 million claims valued at about $17.5 billion.\n\nSome notable persons who have died from lung fibrosis associated with asbestos include:\n\n\n"}
{"id": "42437494", "url": "https://en.wikipedia.org/wiki?curid=42437494", "title": "Australian paradox", "text": "Australian paradox\n\nThe Australian Paradox is a term coined in 2011 to describe what its proponents claim are diverging trends in sugar consumption and obesity rates in Australia. The term was first used in a 2011 study published in \"Nutrients\" by Professor Jennie Brand-Miller, in which she and co-author Dr Alan Barclay reported that, in Australia, \"a substantial decline in refined sugars intake occurred over the same timeframe that obesity has increased.\" \n\nThe \"paradox\" in its name refers to the fact that sugar consumption is often considered (for example by Robert Lustig) to be a significant contributor to rising obesity rates, and because ecological studies in the United States have found a positive relationship over certain time periods between sugar consumption and obesity prevalence , although added sugars consumption is now also declining in the United States.\n\nSome people have criticized Brand-Miller's 2011 study, such as economist Rory Robertson, who argued that \"[Brand-Miller's study's] regular claim – \"In Australia sugar consumption has dropped 23 per cent since 1980\" – is woefully misleading, based as it is on a series that was abandoned by the Australian Bureau of Statistics (ABS) as unreliable a decade ago.\" Robertson has also argued that while the paper claims that consumption of sugary soft drinks in Australia declined by 10% between 1994 and 2006, it actually increased by 30%. He cites these and other data to support calling the research \"a menace to public health\". \n\nIn February 2014, the Australian Broadcasting Corporation (ABC) aired a program criticizing the 2011 study proposing the existence of the paradox, based in part on Robertson's research. The CEO of the Australian Beverages Council, Geoff Parker, has responded that his industry cites other studies besides Brand-Miller's 2011 study to support their view that sugar is not uniquely linked to obesity. In response to Robertson's allegations, Sydney University, Brand-Miller's employer, launched an investigation to determine if she is guilty of research misconduct. A spokesperson for the university said there were \"...no substantiated claims against the work of any academic at the university, nor indeed has there been any finding that the complaints warrant any further investigation\". \n\nIn July 2014, Brand-Miller and Barclay were cleared of misconduct by a six-month investigation conducted by Robert Clark of the University of New South Wales. Following an investigation prompted by the Australian economist, two minor arithmetical errors were identified in the original manuscript of The Australian Paradox which were promptly corrected. This was the only allegation out of 8 others that was substantiated.\n\nAnother study on the same topic was published in 2013 by researchers (Rikkers et al.) from the University of Western Australia. The study concluded that \"The Australian Paradox assertion is based on incomplete data, as it excludes sugar contained in imported processed foods, which have increased markedly.\" The study argued that the claim that sugar consumption had been declining in Australia relied only on production data, and that Australia gets back much of the raw sugar it exports in the form of processed foods.. Tom McNeill demonstrated that Rikkers paper was significantly flawed: \"Rikkers et al.'s biggest source of error is the inclusion of incorrect products in the category of \"moderate to high sugar content\", in violation of their study inclusion criteria. Fruit juices and fruit drinks have been added to the analysis by the authors without consideration of their actual sugar content, or the very definition of these products which must be adhered to by food manufacturers under the control of Food Standards Australia and New Zealand (FSANZ)\". A narrative review of eye disease published the following year argued that the claim of the existence of an Australian paradox \"is flawed as it assumes declining sugar intake, without taking into account imported foods containing sugar.\", quoting Rikkers et al's flawed analysis as evidence \n\nBrand-Miller and Barclay have responded that Rikkers et al. are wrong and that, in fact, the sugar consumption data they used (compiled by the United Nations Food and Agriculture Organization, the Australian Bureau of Statistics and Australian beverage industry) \"all incorporated data on imported products\". Recent research by Levy and Shrapnel (\"Quenching Australia's thirst: A trend analysis of water-based beverage sales from 1997 to 2011\") has concluded that added sugar from soft drinks has continued to decline.\n\nBrand-Miller's stated that per capita sales of sugar-sweetened beverages had decreased by 10%, in an interview with ABC Radio in 2014, \"it might be that a key word came out. It's possible that this should be, 'While nutritively sweetened beverages ... 10 per cent sweetened beverages decreased by 10 per cent.' So I'll double-check it.\" Barclay, the 2011 study's other author, also said, in an email to the program, that \"the 10 per cent decline could not possibly refer to per capita sales of nutritively sweetened soft drinks\". As mentioned previously, Brand-Miller and Barclay published a correction to their original 2011 study addressing this. According to Esther Han, this correction invalidates the study's claim that soft drink consumption decreased from 1994 to 2006.. Brand-Miller and Barclay disagree. \n\nComplaints about the scientific journal \"Nutrients\" over its publication of The Australian Paradox paper led to the Open Access Scholarly Publishers Association (OASPA) investigating \"Nutrients\" publisher, MDPI. In 2014, OASPA's investigation concluded that MDPI continued to meet its membership criteria.\n\nIn April 2017, an update of all available Australian added sugars consumption data titled \"Declining consumption of added sugars and sugar-sweetened beverages in Australia: a challenge for obesity prevention\" was published in the American Journal Of Clinical Nutrition. The analysis concluded \"In Australia, 4 independent data sets confirmed shorter- and longer-term declines in the availability and intake of added sugars, including those contributed by SSBs (Sugar Sweetened Beverages).\"\n\nIndependent analyses by Australian researchers including Ridoutt and colleagues at the Commonwealth Scientific and Industrial Research Organisation (CSIRO) and Lei and colleagues also concluded that Australians consumed less added sugars in the years 2011-12 than they did in 1995.\n\nIn December 2017, the Australian Bureau of Statistics published a comparison of free sugars consumption using Australia's 1995 National Nutrition Survey and 2011/2 Australian Health Survey titled \"CONSUMPTION OF ADDED SUGARS - A COMPARISON OF 1995 TO 2011-12\". Its main conclusion was \"Between 1995 and 2011-12, Australians had a relative decrease in their consumption of free sugars, with the average proportion of dietary energy from free sugars declining from 12.5% to 10.9%.\"\n\nOverall, the weight of evidence (Ridoutt and colleagues, Lei and colleagues, Australian Bureau of Statistics) supports Brand-Miller and Barclay's hypothesis that free and added sugars consumption has declined in Australia over the time-frame that overweight and obesity have increased.\n\n"}
{"id": "56379488", "url": "https://en.wikipedia.org/wiki?curid=56379488", "title": "Biological effects of radiation on the epigenome", "text": "Biological effects of radiation on the epigenome\n\nIonizing radiation can cause biological effects which are passed on to offspring through the epigenome. The effects of radiation on cells has been found to be dependent on the dosage of the radiation, the location of the cell in regards to tissue, and whether the cell is a somatic or germ line cell. Generally, ionizing radiation appears to reduce methylation of DNA in cells.\n\nIonizing radiation has been known to cause damage to cellular components such as proteins, lipids, and nucleic acids. It has also been known to cause DNA double-strand breaks. Accumulation of DNA double strand breaks can lead to cell cycle arrest in somatic cells and cause cell death. Due to its ability to induce cell cycle arrest, ionizing radiation is used on abnormal growths in the human body such as cancer cells, in radiation therapy. Most cancer cells are fully treated with some type of radiotherapy, however some cells such as stem cell cancer cells show a reoccurrence when treated by this type of therapy.\n\nNon-ionising radiations, electromagnetic fields (EMF) such as radiofrequency (RF), or power frequency radiation have become very common in everyday life. All of these exist as low frequency radiation which can come from wireless cellular devices or through electrical appliances which induce extremely low frequency radiation (ELF). Exposure to these radioactive frequencies has shown negative affects on the fertility of men by impacting the DNA of the sperm and deteriorating the testes as well as an increased risk of tumor formation in salivary glands. The International Agency for Research on Cancer considers RF electromagnetic fields to be possibly carcinogenic to humans, however the evidence is limited.\n\nAdvances in medical imaging has resulted in increased exposure of humans to low doses of ionizing radiation. Radiation exposure in pediatrics has been shown to have a greater impact as children's cells are still developing. The radiation obtained from medical imaging techniques is only harmful if consistently targeted multiple times in a short space of time. Safety measures have been introduced in order to limit the exposure of harmful ionizing radiation such as the usage of protective material during the use of these imaging tools. A lower dosage is also used in order to fully rid the possibility of a harmful effect from the medical imaging tools. The National Council on Radiation Protection and Measurements along with many other scientific committees have ruled in favor of continued use of medical imaging as the reward far outweighs the minimal risk obtained from these imaging techniques. If the safety protocols are not followed there is a potential increase in the risk of developing cancer. This is primarily due to the decreased methylation of cell cycle genes, such as those relating to apoptosis and DNA repair. The ionizing radiation from these techniques can cause many other detrimental effects in cells including changes in gene expression and halting the cell cycle. However, these results are extremely unlikely if the proper protocols are followed.\n\nTarget theory concerns the models of how radiation kills biological cells and is based around two main postulates:\n\n\nSeveral models have been based around the above two points. From the various proposed models three main conclusions were found:\n\n\nRadiation exposure through ionizing radiation (IR) affects a variety of processes inside of an exposed cell. IR can cause changes in gene expression, disruption of cell cycle arrest, and apoptotic cell death. The extent of how radiation effects cells depends on the type of cell and the dosage of the radiation. Some irradiated cancer cells have been shown to exhibit DNA methylation patterns due to epigenetic mechanisms in the cell. In medicine, medical diagnostic methods such as CT scans and radiation therapy expose the individual to ionizing radiation. Irradiated cells can also induce genomic instability in neighboring un-radiated cells via the bystander effect. Radiation exposure could also occur via many other channels than just ionizing radiation.\n\nIn this model a single hit on a target is sufficient to kill a cell The equation used for this model is as follows:\n\nformula_1\n\nWhere k represents a hit on the cell and m represents the mass of the cell.\n\nIn this model the cell has a number of targets n. A single hit on one target is not sufficient to kill the cell but does disable the target. An accumulation of successful hits on various targets leads to cell death. The equation used for this model is as follows:\n\nformula_2\n\nWhere n represents number of the targets in the cell.\n\nThe equation used for this model is as follows:\n\nformula_3\n\nwhere αD represents a hit made by a one particle track and βD represents a hit made by a two particle track and S(D) represents the probability of survival of the cell.\n\nThis model showed the accuracy of survival description for higher or repeated doses.\n\nThe equation used for this model is as follows:\n\nformula_4\n\nThe equation used for this model is as follows:\n\nformula_5\n\nThis model shows the mean number of lesions before any repair activations in a cell.\n\nThe equation used for this model is as follows:\n\nformula_6\n\nwhere U represents the yield of initially induced lesions, with λ being the linear self-repair coefficient, and T equaling time\n\nThis equation explores the hypothesis of a lesion becoming fatal within a given of time if it is not repair by repair enzymes.\n\nThe equation used for this model is as follows:\n\nformula_7\n\nT is the radiation duration and t is the available repair time.\n\nThis model illustrates the efficiency of the repair system decreasing as the dosage of radiation increases. This is due to the repair kinetics becoming increasingly saturated with the increase in radiation dosage.\n\nThe equation used for this model is as follows:\n\nformula_8\n\nn(t) is the number of unrepaired lesions, c(t) is the number of repair molecules or enzymes, k is the proportionality coefficient, and T is the time available for repair.\n\nHormesis is the hypothesis that low levels of disrupting stimulus can cause beneficial adaptations in an organism. The ionizing radiation stimulates repair proteins that are usually not active. Cells use this new stimuli to adapt to the stressors they are being exposed to.\n\nIn biology, the bystander effect is described as changes to nearby non-targeted cells in response to changes in an initially targeted cell by some disrupting agent. In the case of Radiation-Induced Bystander Effect, the stress on the cell is caused by ionizing radiation.\n\nThe bystander effect can be broken down into two catagories, long range bystander effect and short range bystander effect. In long range bystander effect, the effects of stress are seen further away from the initially targeted cell. In short range bystander, the effects of stress are seen in cells adjacent to the target cell.\n\nBoth low linear energy transfer and high linear energy transfer photons have been shown to produce RIBE. Low linear energy transfer photons were reported to cause increases in mutagenesis and a reduction in the survival of cells in clonogenic assays. X-rays and gamma rays were reported to cause increases in DNA double strand break, methylation, and apoptosis. Further studies are needed to reach a conclusive explanation of any epigenetic impact of the bystander effect.\n\nIonizing radiation produces fast moving particles which have the ability to damage DNA, and produce highly reactive free radicals known as reactive oxygen species (ROS). The production of ROS in cells radiated by LDIR (Low-Dose Ionizing Radiation) occur in two ways, by the radiolysis of water molecules or the promotion of nitric oxide synthesis (NOS) activity. The resulting nitric oxide formation reacts with superoxide radicals. This generates peroxynitrite which is toxic to biomolecules. Cellular ROS is also produced with the help of a mechanism involving nicotinamide adenosine dinucleotide phosphate (NADPH) oxidase. NADPH oxidase helps with the formation of ROS by generating a superoxide anion by transferring electrons from cytosolic NADPH across the cell membrane to the extracellular molecular oxygen. This process increases the potential for leakage of electrons and free radicals from the mitochondria. The exposure to the LDIR induces electron release from the mitochondria resulting in more electrons contributing to the superoxide formation in the cells.\n\nThe production of ROS in high quantity in cells results in the degradation of biomolecules such as proteins, DNA, and RNA. In one such instance the ROS are known to create double stranded and single stranded breaks in the DNA. This causes the DNA repair mechanisms to try to adapt to the increase in DNA strand breaks. Heritable changes to the DNA sequence have been seen although the DNA nucleotide sequence seems the same after the exposure with LDIR.\n\nThe formation of ROS is coupled with the formation of nitric oxide synthase activity (NOS). NO reacts with O generating peroxynitrite. The increase in the NOS activity causes the production of peroxynitrite (ONOO-). Peroxynitrite is a strong oxidant radical and it reacts with a wide array of biomolecules such as DNA bases, proteins and lipids. Peroxynitrite affects biomolecules function and structure and therefore effectively destabilizes the cell.\n\nIonizing radiation causes the cell to generate increased ROS and the increase of this species damages biological macromolecules. In order to compensate for this increased radical species, cells adapt to IR induced oxidative effects by modifying the mechanisms of epigenetic gene regulation. There are 4 epigenetic modifications that can take place:\n\n\nROS generated by ionizing radiation chemically modify histones which can cause a change in transcription. Oxidation of cellular lipid components result in an electrophilic molecule formation. The electrophilic molecule binds to the lysine residues of histones causing a ketoamide adduct formation. The ketoamide adduct formation blocks the lysine residues of histones from binding to acetylation proteins thus decreasing gene transcription.\n\nDNA hypermethylation is seen in the genome with DNA breaks at a gene-specific basis, such as promoters of regulatory genes, but the global methylation of the genome shows a hypomethylation pattern during the period of reactive oxygen species stress.\n\nDNA damage induced by reactive oxygen species results in increased gene methylation and ultimately gene silencing. Reactive oxygen species modify the mechanism of epigenetic methylation by inducing DNA breaks which are later repaired and then methylated by DNMTs. DNA damage response genes, such as GADD45A, recruit nuclear proteins Np95 to direct histone methyltransferase's towards the damaged DNA site. The breaks in DNA caused by the ionizing radiation then recruit the DNMTs in order to repair and further methylate the repair site.\n\nGenome wide hypomethylation occurs due to reactive oxygen species hydroxylating methylcytosines to 5-hydroxymethylcytosine (5hmC). The production of 5hmC serves as an epigenetic marker for DNA damage which is recognizable by DNA repair enzymes. The DNA repair enzymes attracted by the marker convert 5hmC to an unmethylated cytosine base resulting in the hypomethylation of the genome.\n\nAnother mechanism that induces hypomethylation is the depletion of S-adenosyl methionine synthetase (SAM). The prevalence of super oxide species causes the oxidization of reduced glutathione (GSH) to GSSG. Due to this, synthesis of the cosubstrate SAM is stopped. SAM is an essential cosubtrate for the normal functioning of DNMTs and histone methyltrasnferase proteins.\n\nDouble stranded DNA breaks caused by exposure to ionizing radiation are known to alter chromatin structure. Double stranded breaks are primarily repaired by poly ADP (PAR) polymerases which accumulate at the site of the break leading to activation of the chromatin remodeling protein ALC1. ALC1 causes the nucleosome to relax resulting in the epigenetic up-regulation of genes. A similar mechanism involves the ataxia telangiectasia mutated (ATM) serine/threonine kinase which is an enzyme involved in the repair of double stranded breaks caused by ionizing radiation. ATM phosphorylates KAP1 which causes the heterochromatin to relax, allowing increased transcription to occur.\n\nThe DNA mismatch repair gene (MSH2) promoter has shown a hypermethylation pattern when exposed to ionizing radiation. Reactive oxygen species induce the oxidization of deoxyguanosine into 8-hydroxydeoxyguanosine (8-OHdG) causing a change in chromatin structure. Gene promoters that contain 8-OHdG deactivate the chromatin by inducing trimethyl-H3K27 in the genome. Other enzymes such as transglutaminases (TGs) control chromatin remodeling through proteins such as sirtuin1 (SIRT1). TGs cause transcriptional repression during reactive oxygen species stress by binding to the chromatin and inhibiting the sirtuin 1 histone deacetylase from performing its function.\n\nEpigenetic imprinting is lost during reactive oxygen species stress. This type of oxidative stress causes a loss of NF- κB signaling. Enhancer blocking element CCCTC-binding factor (CTCF) binds to the imprint control region of insulin-like growth factor 2 (IGF2) preventing the enhancers from allowing the transcription of the gene. The NF- κB proteins interact with IκB inhibitory proteins, but during oxidative stress IκB proteins are degraded in the cell. The loss of IκB proteins for NF- κB proteins to bind to results in NF- κB proteins entering the nucleus to bind to specific response elements to counter the oxidative stress. The binding of NF- κB and corepressor HDAC1 to response elements such as the CCCTC-binding factor causes a decrease in expression of the enhancer blocking element. This decrease in expression hinders the binding to the IGF2 imprint control region therefore causing the loss of imprinting and biallelic IGF2 expression.\n\nAfter the initial exposure to ionizing radiation, cellular changes are prevalent in the unexposed offspring of irradiated cells for many cell divisions. One way this non-Mendelian mode of inheritance can be explained is through epigenetic mechanisms.\n\nIonizing radiation exposure affects patterns of DNA methylation. Breast cancer cells treated with fractionated doses of ionizing radiation showed DNA hypomethylation at the various gene loci; dose fractionation refers to breaking down one dose of radiation into separate, smaller doses. Hypomethylation of these genes correlated with decreased expression of various DNMTs and methyl CpG binding proteins. LINE1 transposable elements have been identified as targets for ionizing radiation. The hypomethylation of LINE1 elements results in activation of the elements and thus an increase in LINE1 protein levels. Increased transcription of LINE1 transposable elements results in greater mobilization of the LINE1 loci and therefore increases genomic instability.\n\nIrradiated cells can be linked to a variety of histone modifications. Ionizing radiation in breast cancer cell inhibits H4 lysine tri-methylation. Mouse models exposed to high levels of X-ray irradiation exhibited a decrease in both the tri-methylation of H4-Lys20 and the compaction of the chromatin. With the loss of tri-methylation of H4-Lys20, DNA hypomethylation increased resulting in DNA damage and increased genomic instability.\n\nBreaks in DNA due to ionizing radiation can be repaired. New DNA synthesis by DNA polymerases is one of the ways radiation induced DNA damage can be repaired. However, DNA polymerases do not insert methylated bases which leads to a decrease in methylation of the newly synthesized strand. Reactive oxygen species also inhibit DNMT activity which would normally add the missing methyl groups. This increases the chance that the demethylated state of DNA will eventually become permanent.\n\nChronic exposure to these types of radiation can have an effect on children from as early as when they are fetuses. There have been multiple cases reported of hindrance in the development of the brain, behavioral changes such as anxiety, and the disruption of proper learning and language processing. An Increase in the cases of ADHD behavior and autism behavior has been shown to be directly correlated with the exposure of EMF waves. The World Health Organization has classified RFR as a possible carcinogen for its epigenetic effects on DNA expression. The exposure to EMF waves on a consistent 24hr basis has shown to lower the activity of miRNA in the brain affecting developmental and neuronal activity. This epigenetic change causes the silencing of necessary genes along with the change in expression of other genes integral for the normal development of the brain.\n\nDNA methylation influences tissue responses to ionizing radiation. Modulation of methylation in the gene MGMT or in transposable elements such as LINE1 could be used to alter tissue responses to ionizing radiation and potentially opening new areas for cancer treatment.\n\nMGMT serves as a prognostic marker in glioblastoma. Hypermethylation of MGMT is associated with the regression of tumors. Hypermethylation of MGMT silences its transcription inhibiting alkylating agents in tumor killing cells. Studies have shown patients who received radiotherapy, but no chemotherapy after tumor extraction, had an improved response to radiotherapy due to the methylation of the MGMT promoter.\n\nAlmost all human cancers include hypomethylation of LINE1 elements. Various studies depict that the hypomethylation of LINE1 correlates with a decrease in survival after both chemotherapy and radiotheraphy.\n\nDMNT inhibitors are being explored in the treatment of malignant tumors. Recent in-vitro studies show that DNMT inhibitors can increase the effects of other anti-cancer drugs. Knowledge of in-vivo effect of DNMT inhibitors are still being investigated. The long term effects of the use of DNMT inhibitors are still unknown.\n"}
{"id": "30639359", "url": "https://en.wikipedia.org/wiki?curid=30639359", "title": "Burn center", "text": "Burn center\n\nA burn center, burn unit or burns unit is a hospital specializing in the treatment of burns. Burn centers are often used for the treatment and recovery of patients with more severe burns.\n\nThe severity of a burn, and therefore whether a referral will be made after the patient is treated and stabilized, differs depending upon many factors, among them: the age of the victim (burns to infants and toddlers or to those over age 65 are generally more serious, particularly if the face, head, respiratory system, chest, abdomen, groin, or extremities are burned; those who are not in these age groups can be more affected if they are or were already ill, injured, or immunocompromised), the total body surface area that is burned (the rule of nines), if proper treatment and referrals are delayed or the wrong treatments are given, if the burns are of the 2nd, 3rd, or 4th degree (the bigger and deeper, the worse it is), the source (if it was due to a chemical, or from a scald, or fire, or radiation, the treatment regimen has to be modified appropriately), or if skin grafting is not feasible and/or important organs are harmed. \n\nBurn centers needs a team approach for the management of critically burnt patient. Usually the burns management team consists of plastic surgeon, intensivist, chest physician, general surgeon, pediatrician, nurses and technicians, microbiologist, psychiatrist, nutritionist, physiotherapist and a social worker. Early burn wound excision and immediate wound cover can improve the chances of survival in major burn cases.\n\n"}
{"id": "52355912", "url": "https://en.wikipedia.org/wiki?curid=52355912", "title": "Cannabis in Croatia", "text": "Cannabis in Croatia\n\nCannabis in Croatia is decriminalized for personal use and legalized for limited medical uses.\n\nFrom 2013, there is a distinction in the Croatian penal code between various illegal substances, they are now separated on heavy drugs and light drugs like cannabis. According to the law, growing or selling cannabis is considered a felony punishable by a mandatory prison sentence (three years minimum). From 2013, the possession of small amount of marijuana and other light drugs is a minor offence which leads to a fine of 5000–20000kn (US$800–3500) depending on the case in question. \n\nAs of 15 October 2015, the Ministry of Health officially legalized the use of cannabis-based drugs for medical purposes for patients with illnesses such as cancer, multiple sclerosis, or AIDS.\n"}
{"id": "45272133", "url": "https://en.wikipedia.org/wiki?curid=45272133", "title": "Cartel of the Suns", "text": "Cartel of the Suns\n\nThe term Cartel of the Suns () describes a Venezuelan organization allegedly headed by high-ranking members of the Armed Forces of Venezuela who are involved in international drug trade. According to Héctor Landaeta, journalist and author of \"Chavismo, Narco-trafficking and the Military\", the phenomenon began when Colombian drugs began to enter into Venezuela from corrupt border units and the \"rot moved its way up the ranks\".\n\nReports that members of the Venezuelan military were involved in drug trafficking began in the 1990s, though it was limited to taking payments and ignoring drug traffickers. It was alleged that officers of Hugo Chávez's Revolutionary Bolivarian Movement-200 that planned the 1992 Venezuelan coup d'état attempts had created a group that participated in drug trafficking that was known as the \"Cartel Bolivariano\" or \"Bolivarian Cartel\". Following the 1992 coup attempts, \"The Los Angeles Times\" noted that Venezuelan officers may have sought to take over the government since there was \"money to be made from corruption, particularly in drugs\". In 1993, the term \"Cartel de los Soles\" or \"Cartel of the Suns\" was first used when allegations of two National Guard generals, who wore emblems that looked like suns on their uniforms, were investigated for drug trafficking crimes.\n\nThe \"Cartel of the Suns\" name returned in 2004 by reporter and city council member Mauro Marcano. Shortly before he was murdered, Marcano alleged that Alexis Maneiro, head of the National Guard and the Dirección General de Inteligencia Militar, was involved in drug trafficking.\n\nAccording to \"Vice News\", the Venezuelan government under Hugo Chavez expanded corruption to \"unprecedented levels\" in an already corrupt military. Chavez gave military officials millions of dollars for social programs that allegedly disappeared, also giving legal immunity to drug trafficking officials to maintain power and loyalty. When Chavez ousted the United States Drug Enforcement Administration in 2005, Venezuela became a more attractive route for drug trade. According to Colombian intelligence, an arrested drug vigilante stated that \"senior figures in President Hugo Chavez's security forces to arrange drug shipments through Venezuela\". It has been alleged that the National Guard had worked with the FARC with drug trade. British officials alleged that planes from Colombia would also be sheltered by Venezuelan Air Force bases.\n\nIn September 2013, an incident allegedly linked to the Cartel of the Suns and involving men from the Venezuelan National Guard who placed 31 suitcases containing 1.3 tons of cocaine on an Air France flight astonished Charles de Gaulle Airport authorities as it was the largest seizure of cocaine recorded in mainland France. On 15 February 2014, a commander for the Venezuelan National Guard was stopped while driving to Valencia with his family and was arrested for having 554 kilos of cocaine in his possession. On 11 November 2015, DEA agents arrested two relatives (an adopted son and a nephew) of Venezuelan president Nicolás Maduro while trying to move 800 kilograms of cocaine from Venezuela to the USA, a source from DEA stated unofficially that there is no way that the huge amount of cocaine that passes through Venezuela does it without high levels of corruption in the government.\n\nThere are groups within the branches of the Armed Forces of Venezuela such as the Venezuelan Army, Venezuelan Navy, Venezuelan Air Force, and Venezuelan National Guard; from the lowest to the highest levels of personnel.\n\nAllegedly, lower ranking National Guardsmen compete for positions at border checkpoints so they can be paid bribes for \"illicit trade\", though a large portion of bribes go to their superiors. Allegedly, the corrupt officials of the Cartel of the Suns traffic drugs from Colombia to Venezuela where they are shipped internationally.\n\nPresident of Venezuela Nicolás Maduro has personally promoted individuals accused of drug trafficking to high positions of the Venezuelan government. In May 2018, he was said to have received drug trafficking profits from Diosdado Cabello.\n\nIn January 2015, the former security chief of both Hugo Chavez and Diosdado Cabello, Leamsy Salazar, made accusations that Cabello was head of the Cartel of the Suns. Salazar was placed in witness protection, fleeing to the United States with assistance of the Drug Enforcement Administration's Special Operations Division after cooperating with the administration and providing possible details on Cabello's involvement with international drug trade. Salazar stated that he saw Cabello give orders on transporting tons of cocaine. The shipments of drugs were reportedly sent from the FARC in Colombia to the United States and Europe, with the possible assistance of Cuba. The alleged international drug operation had possibly involved other senior members of Venezuela's government as well, such as Tarek El Aissami and José David Cabello, Diosdado's brother.\n\nOn 18 May 2018, the Office of Foreign Assets Control (OFAC) of the United States Department of the Treasury placed sanctions in effect against Cabello, his wife, his brother and his \"testaferro\" Rafael Sarria. OFAC stated that Cabello and others used their power within the Bolivarian government \"to personally profit from extortion, money laundering, and embezzlement\", with Cabello allegedly directing drug trafficking activities with Vice President of Venezuela, Tareck El Aissami while dividing profits with President Nicolás Maduro. The Office also stated that Cabello would use public information to track wealth individuals who were potentially drug trafficking and steal their drugs and property in order to get rid of potential competition.\n\nTareck El Aissami was sanctioned by the US Treasury Department on 13 February 2017 under the Foreign Narcotics Kingpin Designation Actafter being accused of facilitating drug shipments from Venezuela to Mexico and the United States, freezing tens of millions of dollars of assets purportedly under El Aissami's control.\n\nHead of the Bolivarian National Guard, Néstor Reverol, has been indicted by the United States government of assisting with drug trafficking in Venezuela. Reverol allegedly tipped off traffickers, cancelled investigations and released those involved in drug shipments.\n\nHugo Carvajal is allegedly one of the leaders of the Cartel of the Suns. On July 22, 2014, Hugo Carvajal, former head of Venezuelan military-intelligence, was detained in Aruba, despite having been admitted on a diplomatic passport and being named consul general to Aruba in January. The arrest was carried out following a formal request by the U.S. government, which accuses Carvajal of ties to drug trafficking and to the FARC guerrilla group. On 27 July 2014, Carvajal was released after authorities decided he had diplomatic immunity, but was also considered persona non grata.\n\nYazenky Lamas, former pilot to First Lady Cilia Flores, was extradited to the United States from Colombia, having allegedly provided air traffic codes to allow planes carrying cocaine to impersonate commercial flights. Venezuela President Nicolás Maduro reportedly asked Colombian Defense Minister Luis Carlos Villegas Echeverri to reject the request for extradition. Lamas has been linked to hundreds of drug flights operated in Venezuela.\n\nOther officials that are possibly involved with the Cartel of the Suns include:\n\nIn 2005, all branches of the National Bolivarian Armed Forces of Venezuela were given the duty to combat drug trafficking in Venezuela, granting data once held only by the Bolivarian National Guard to the army, navy and air force. Mildred Camero, former anti-drug official of the Chávez government, stated that this data created competition within the ranks of the military who fought to make deals with the FARC to actively partake in drug trafficking.\n\nAuthorities in Colombia stated that through laptops they had seized on a raid against Raul Reyes in 2007, they found documents purporting to show that Hugo Chávez offered payments of as much as $300 million to the FARC \"among other financial and political ties that date back years\" and documents showing the FARC rebels sought Venezuelan assistance in acquiring surface-to-air missiles, alleging that Chavez met personally with FARC rebel leaders. According to Interpol, the files found by Colombian forces were considered to be authentic. In 2008, the United States Department of Treasury accused two senior Venezuelan government officials and one former official of providing material assistance for drug-trafficking operations carried out by the FARC guerrilla group in Colombia.\n\nHowever, independent analyses of the documents by some U.S. academics and journalists have challenged the Colombian interpretation of the documents, accusing the Colombian government of exaggerating their contents. In 2008, the Secretary General of the Organization of American States, Jose Miguel Insulza, testified before the U.S. Congress that \"there are no evidences\" that Venezuela is supporting \"terrorist groups\", including the FARC. Three years later in 2011, the International Institute for Strategic Studies (IISS) concluded that Chavez's government funded FARC's Caracas office and granted it access to intelligence services. Venezuelan diplomats denounced the IISS' findings saying that they had \"basic inaccuracies\". \n\nNevertheless, as of 2018, FARC dissidents who left FARC when it disbanded in 2017 still operate within Venezuela with virtual impunity. These dissident forces, with armed personnel numbering up to 2,500 individuals, allegedly still cooperate with the Cartel of the Suns.\n\n"}
{"id": "25658287", "url": "https://en.wikipedia.org/wiki?curid=25658287", "title": "Cool store", "text": "Cool store\n\nA cool store or cold store is a large refrigerated room or building designed for storage of goods in an environment below the outdoor temperature. Products needing refrigeration include fruit, vegetables, seafood and meat. Cold stores are often located near shipping ports used for import/export of produce.\n\nCool stores have been an essential part of the shipping industry since the late 19th century. Christian Salvesen expanded from a small Scottish whaling company when they established a cold store in Grimsby, then a major fishing port. Nine Elms Cold Store, constructed in 1965, could hold 16,000 tons of meat, cheese and butter. It closed in 1979 and was used by squatters and various illegal activities before being demolished towards the end of the century.\n\nThe stores themselves vary in size. A thirty hectare (74 acre) cold store with 200,000 tonne capacity was planned for Wuhan, China.\n"}
{"id": "265128", "url": "https://en.wikipedia.org/wiki?curid=265128", "title": "Cost-effectiveness analysis", "text": "Cost-effectiveness analysis\n\nCost-effectiveness analysis (CEA) is a form of economic analysis that compares the relative costs and outcomes (effects) of different courses of action. Cost-effectiveness analysis is distinct from cost–benefit analysis, which assigns a monetary value to the measure of effect. Cost-effectiveness analysis is often used in the field of health services, where it may be inappropriate to monetize health effect. Typically the CEA is expressed in terms of a ratio where the denominator is a gain in health from a measure (years of life, premature births averted, sight-years gained) and the numerator is the cost associated with the health gain. The most commonly used outcome measure is quality-adjusted life years (QALY).\n\nCost-utility analysis is similar to cost-effectiveness analysis. Cost-effectiveness analyses are often visualized on a plane consisting of four-quadrants, the cost represented on one axis and the effectiveness on the other axis. Cost-effectiveness analysis focuses on maximising the average level of an outcome, distributional cost-effectiveness analysis extends the core methods of CEA to incorporate concerns for the distribution of outcomes as well as their average level and make trade-offs between equity and efficiency, these more sophisticated methods are of particular interest when analysing interventions to tackle health inequality.\n\nThe concept of cost-effectiveness is applied to the planning and management of many types of organized activity. It is widely used in many aspects of life. In the acquisition of military tanks, for example, competing designs are compared not only for purchase price, but also for such factors as their operating radius, top speed, rate of fire, armor protection, and caliber and armor penetration of their guns. If a tank's performance in these areas is equal or even slightly inferior to its competitor, but substantially less expensive and easier to produce, military planners may select it as more cost effective than the competitor.\n\nConversely, if the difference in price is near zero, but the more costly competitor would convey an enormous battlefield advantage through special ammunition, radar fire control and laser range finding, enabling it to destroy enemy tanks accurately at extreme ranges, military planners may choose it instead—based on the same cost-effectiveness principle.\n\nIn the context of pharmacoeconomics, the cost-effectiveness of a therapeutic or preventive intervention is the ratio of the cost of the intervention to a relevant measure of its effect. Cost refers to the resource expended for the intervention, usually measured in monetary terms such as dollars or pounds. The measure of effects depends on the intervention being considered. Examples include the number of people cured of a disease, the mm Hg reduction in diastolic blood pressure and the number of symptom-free days experienced by a patient. The selection of the appropriate effect measure should be based on clinical judgment in the context of the intervention being considered.\n\nA special case of CEA is cost–utility analysis, where the effects are measured in terms of years of full health lived, using a measure such as quality-adjusted life years or disability-adjusted life years. Cost-effectiveness is typically expressed as an incremental cost-effectiveness ratio (ICER), the ratio of change in costs to the change in effects. A complete compilation of cost-utility analyses in the peer reviewed medical literature is available from the Cost-Effectiveness Analysis Registry website.\n\nA 1995 study of the cost-effectiveness of over 500 life-saving medical interventions found that the median cost per intervention was $42,000 per life-year saved. A 2006 systematic review found that industry-funded studies often concluded with cost effective ratios below $20,000 per QALY and low quality studies and those conducted outside the US and EU were less likely to be below this threshold. While the two conclusions of this article may indicate that industry-funded ICER measures are lower methodological quality than those published by non-industry sources, there is also a possibility that, due to the nature of retrospective or other non-public work, publication bias may exist rather than methodology biases. There may be incentive for an organization not to develop or publish an analysis that does not demonstrate the value of their product. Additionally, peer reviewed journal articles should have a strong and defendable methodology, as that is the expectation of the peer-review process.\n\nCEA has been applied to energy efficiency investments in buildings to calculate the value of energy saved in $/kWh. The energy in such a calculation is virtual in the sense that it was never consumed but rather saved due to some energy efficiency investment being made. Such savings are sometimes called \"negawatts\". The benefit of the CEA approach in energy systems is that it avoids the need to guess future energy prices for the purposes of the calculation, thus removing the major source of uncertainty in the appraisal of energy efficiency investments.\n\n\n"}
{"id": "52830327", "url": "https://en.wikipedia.org/wiki?curid=52830327", "title": "Dimitri Venediktov", "text": "Dimitri Venediktov\n\nDimitri Venediktov was the Deputy Health Minister of the USSR from 1965 to 1981 under Ministers of Health Boris Petrovsky and Sergei Burenkov. In this role, he was instrumental in the campaign to eradicate smallpox and supplying vaccines for the program He was also involved in organizing the Conference of Alma-Ata which was foundational in the field of public health. He was elected to the Congress of People's Deputies of the Soviet Union from 1989 to its dissolution. He currently serves on the Russian Academy of Medical Sciences, focusing on information storage in healthcare.\n\nVenediktov received two Orders of the Red Banner of Labour and one Order of the Badge of Honour amongst other awards.\n"}
{"id": "44305878", "url": "https://en.wikipedia.org/wiki?curid=44305878", "title": "Directed differentiation", "text": "Directed differentiation\n\nDirected differentiation is a bioengineering methodology at the interface of stem cell biology, developmental biology and tissue engineering. It is essentially harnessing the potential of stem cells by constraining their differentiation in vitro toward a specific cell type or tissue of interest. Stem cells are by definition pluripotent, able to differentiate into several cell types such as neurons, cardiomyocytes, hepatocytes, etc. Efficient \"directed differentiation\" requires a detailed understanding of the lineage and cell fate decision, often provided by developmental biology.\n\nDuring differentiation, pluripotent cells make a number of developmental decisions to generate first the three germ layers (ectoderm, mesoderm and endoderm) of the embryo and intermediate progenitors, followed by subsequent decisions or check points, giving rise to all the body's mature tissues. The differentiation process can be modeled as sequence of binary decisions based on probabilistic or stochastic models. Developmental biology and embryology provides the basic knowledge of the cell types' differentiation through mutation analysis, lineage tracing, embryo micro-manipulation and gene expression studies. Cell differentiation and tissue organogenesis involve a limited set of developmental signaling pathways. It is thus possible to direct cell fate by controlling cell decisions through extracellular signaling, mimicking developmental signals.\n\n\"Directed differentiation\" is primarily applied to pluripotent stem cells (PSCs) of mammalian origin, in particular mouse and human cells for biomedical research applications. Since the discovery of embryonic stem (ES) cells (1981) and induced pluripotent stem (iPS) cells (2006), source material is potentially unlimited.\nHistorically, embryonic carcinoma (EC) cells have also been used. Fibroblasts or other differentiated cell types have been used for direct reprogramming strategies.\n\nCell differentiation involves a transition from a proliferative mode toward differentiation mode. \"Directed differentiation\" consists in mimicking developmental (embryo's development) decisions in vitro using the stem cells as source material. For this purpose, pluripotent stem cells (PSCs) are cultured in controlled conditions involving specific substrate or extracellular matrices promoting cell adhesion and differentiation, and define culture media compositions. A limited number of signaling factors such as growth factors or small molecules, controlling cell differentiation, is applied sequentially or in a combinatorial manner, at varying dosage and exposure time. Proper differentiation of the cell type of interest is verified by analyzing cell type specific markers, gene expression profile, and functional assays.\n\nsupport cells and matrices provide developmental-like environmental signals.\n\nThis method consists in exposing the cells to specific signaling pathways modulators and manipulating cell culture conditions (environmental or exogenous) to mimick the natural sequence of developmental decisions to produce a given cell type/tissue. A drawback of this approach is the necessity to have a good understanding of how the\ncell type of interest is formed.\n\nThis method, also known as transdifferentiation or direct conversion, consists in overexpressing one or several factors, usually transcription factors, introduced in the cells. The starting material can be either pluripotent stem cells (PSCs), or either differentiated cell type such as fibroblasts. The principle was first demonstrated in 1987 with the myogenic factors MyoD.\nA drawback of this approach is the introduction of foreign nucleic acid in the cells and the forced expression of transcription factors which effects are not fully understood.\n\nThis methods consists in selecting the cell type of interest, usually with antibiotic resistance. For this purpose, the source material cells are modified to contain antibiotic resistance cassette under a target cell type specific promoter. Only cells committed to the lineage of interest is surviving the selection.\n\n\"Directed differentiation\" provides a potentially unlimited and manipulable source of cell and tissues.\nSome applications are impaired by the immature phenotype of the pluripotent stem cells (PSCs)-derived cell type, which limits the physiological and functional studies possible.\nSeveral application domains emerged:\n\nFor basic science, notably developmental biology and cell biology, PSC-derived cells allow to study at the molecular and cellular levels fundamental questions in vitro, that would have been otherwise extremely difficult or impossible to study for technical and ethical reasons in vivo such as embryonic development of human. In particular, differentiating cells are amenable for quantitative and qualitative studies.\nMore complex processes can also be studied in vitro and formation of organoids, including cerebroids, optic cup and kidney have been described.\n\nCell types differentiated from pluripotent stem cells (PSCs) are being evaluated as preclinical in vitro models of Human diseases. Human cell types in a dish provide an alternative to traditional preclinical assays using animal, human immortalized cells or primary cultures from biopsies, which their limitations. Clinically-relevant cell types i.e. cell type affected in diseases are a major focus of research, this includes hepatocytes, Langerhans islet beta-cells, cardiomyocytes and neurons. Drug screen are performed on miniaturized cell culture in multiwell-plates or on a chip.\n\nPSCs-derived cells from patients are used in vitro to recreate specific pathologies. The specific cell type affected in the pathology is at the base of the model. For example, motoneurons are used to study spinal muscular atrophy (SMA) and cardiomyocytes are used to study arrythmia. This can allow for a better understanding of the pathogenesis and the development of new treatments through drug discovery. Immature PSC-derived cell types can be matured in vitro by various strategies, such as in vitro ageing, to modelize age-related disease in vitro.\nMajor diseases being modelized with PSCs-derived cells are amyotrophic lateral sclerosis (ALS), Alzheimer's (AD), Parkinson's (PD), fragile X syndrome (FXS), Huntington disease (HD), Down syndrome, Spinal muscular atrophy (SMA), muscular dystrophies, cystic fibrosis, Long QT syndrome, and Type I diabetes.\n\nThe potentially unlimited source of cell and tissues may have direct application for tissue engineering, cell replacement and transplantation following acute injuries and reconstructive surgery. These applications are limited to the cell types that can be differentiated efficiently and safely from human PSCs with the proper organogenesis. Decellularized organs are also being used as tissue scaffold for organogenesis. Source material can be normal healthy cells from another donor (heterologous transplantation) or genetically corrected from the same patient (autologous).\nConcerns on patient safety have been raised due to the possibility of contaminating undifferentiated cells. The first clinical trial using hESC-derived cells was in 2011. The first clinical trial using hiPSC-derived cells started in 2014 in Japan.\n"}
{"id": "48610566", "url": "https://en.wikipedia.org/wiki?curid=48610566", "title": "Disability in Indonesia", "text": "Disability in Indonesia\n\nEstimates for the prevalence of disability in Indonesia vary widely based on criteria. The 2010 Indonesian census reports that only 4.29% of Indonesians have disabilities, with a rate of 3.94% among men and 4.64% among women. Data from the 2007 Riskesdas household survey, by contrast, based on a definition of having a lot of difficulty in at least one functional domain, gives a rate of disability of 11.05%, with 9.40% for males and 12.57% for females. Both sets of data show rates of disability rising significantly with age.\n\nIndonesia is a party to the United Nations Convention on the Rights of Persons with Disabilities, having signed the treaty on 30 March 2007 and ratified it on 30 November 2011.\n\n"}
{"id": "19150842", "url": "https://en.wikipedia.org/wiki?curid=19150842", "title": "Dynamic treatment regime", "text": "Dynamic treatment regime\n\nIn medical research, a dynamic treatment regime (DTR), adaptive intervention, or adaptive treatment strategy is a set of rules for choosing effective treatments for individual patients. Historically, medical research and the practice of medicine tended to rely on an acute care model for the treatment of all medical problems, including chronic illness. Treatment choices made for a particular patient under a dynamic regime are based on that individual's characteristics and history, with the goal of optimizing his or her long-term clinical outcome. A dynamic treatment regime is analogous to a \"policy\" in the field of reinforcement learning, and analogous to a controller in control theory. While most work on dynamic treatment regimes has been done in the context of medicine, the same ideas apply to time-varying policies in other fields, such as education, marketing, and economics.\n\n\n"}
{"id": "15588468", "url": "https://en.wikipedia.org/wiki?curid=15588468", "title": "Environmental impact of meat production", "text": "Environmental impact of meat production\n\nThe environmental impact of meat production varies because of the wide variety of agricultural practices employed around the world. All agricultural practices have been found to have a variety of effects on the environment. Some of the environmental effects that have been associated with meat production are pollution through fossil fuel usage, animal methane, effluent waste, and water and land consumption. Meat is obtained through a variety of methods, including organic farming, free range farming, intensive livestock production, subsistence agriculture, hunting, and fishing.\n\nThe 2006 report \"Livestock's Long Shadow\", released by the Food and Agriculture Organization (FAO) of the United Nations, states that \"the livestock sector is a major stressor on many ecosystems and on the planet as a whole. Globally it is one of the largest sources of greenhouse gases (GHG) and one of the leading causal factors in the loss of biodiversity, while in developed and emerging countries it is perhaps the leading source of water pollution.\" Removing all US agricultural animals would reduce US greenhouse gas emissions by 2.6%. (In this and much other FAO usage, but not always elsewhere, poultry are included as \"livestock\".) A 2017 study published in the journal \"Carbon Balance and Management\" found animal agriculture's global methane emissions are 11% higher than previous estimates based on data from the Intergovernmental Panel on Climate Change. Some fraction of these effects is assignable to non-meat components of the livestock sector such as the wool, egg and dairy industries, and to the livestock used for tillage. Livestock have been estimated to provide power for tillage of as much as half of the world's cropland. According to production data compiled by the FAO, 74 percent of global livestock product tonnage in 2011 was accounted for by non-meat products such as wool, eggs and milk. Meat is also considered one of the prime factors contributing to the current sixth mass extinction. A July 2018 study in \"Science\" asserts that meat consumption will increase as the result of human population growth and rising individual incomes, which will increase carbon emissions and further reduce biodiversity.\n\nIn November 2017, 15,364 world scientists signed a Warning to Humanity calling for, among other things, drastically diminishing our per capita consumption of meat.\n\nChanges in demand for meat may change the environmental impact of meat production by influencing how much meat is produced. It has been estimated that global meat consumption may double from 2000 to 2050, mostly as a consequence of increasing world population, but also partly because of increased per capita meat consumption (with much of the per capita consumption increase occurring in the developing world). Global production and consumption of poultry meat have recently been growing at more than 5 percent annually. Trends vary among livestock sectors. For example, global per capita consumption of pork has increased recently (almost entirely due to changes in consumption within China), while global per capita consumption of ruminant meats has been declining.\n\nIn comparison with grazing, intensive livestock production requires large quantities of harvested feed, this overproduction of feed can also hold negative effects. The growing of cereals for feed in turn requires substantial areas of land. However, where grain is fed, less feed is required for meat production. This is due not only to the higher concentration of metabolizable energy in grain than in roughages, but also to the higher ratio of net energy of gain to net energy of maintenance where metabolizable energy intake is higher. It takes seven pounds of feed to produce a pound of beef (live weight), compared to more than three pounds for a pound of pork and less than two pounds for a pound of chicken. However, assumptions about feed quality are implicit in such generalizations. For example, production of a pound of beef cattle live weight may require between 4 and 5 pounds of feed high in protein and metabolizable energy content, or more than 20 pounds of feed of much lower quality.\n\nFree-range animal production requires land for grazing, which in some places has led to land use change. According to FAO, \"Ranching-induced deforestation is one of the main causes of loss of some unique plant and animal species in the tropical rainforests of Central and South America as well as carbon release in the atmosphere.\" This has implications for—among other things—meat consumption in Europe, which imports significant amounts of feed from Brazil.\n\nRaising animals for human consumption accounts for approximately 40% of the total amount of agricultural output in industrialized countries. Grazing occupies 26% of the earth's ice-free terrestrial surface, and feed crop production uses about one third of all arable land.\n\nLand quality decline is sometimes associated with overgrazing, as these animals are removing much needed nutrients from the soil without the land having time to recover. Rangeland health classification reflects soil and site stability, hydrologic function, and biotic integrity. By the end of 2002, the US Bureau of Land Management (BLM) had evaluated rangeland health on 7,437 grazing allotments (i.e., 35 percent of its grazing allotments or 36 percent of the land area contained in its grazing allotments) and found that 16 percent of these failed to meet rangeland health standards due to existing grazing practices or levels of grazing use. This led the BLM to infer that a similar percentage would be obtained when such evaluations were completed. Soil erosion associated with overgrazing is an important issue in many dry regions of the world. However, on US farmland, much less soil erosion is associated with pastureland used for livestock grazing than with land used for production of crops. Sheet and rill erosion is within estimated soil loss tolerance on 95.1 percent, and wind erosion is within estimated soil loss tolerance on 99.4 percent of US pastureland inventoried by the US Natural Resources Conservation Service.\n\nEnvironmental effects of grazing can be positive or negative, depending on the quality of management, and grazing can have different effects on different soils and different plant communities. Grazing can sometimes reduce, and other times increase, biodiversity of grassland ecosystems. A study comparing virgin grasslands under some grazed and nongrazed management systems in the US indicated somewhat lower soil organic carbon but higher soil nitrogen content with grazing. In contrast, at the High Plains Grasslands Research Station in Wyoming, the top 30 cm of soil contained more organic carbon as well as more nitrogen on grazed pastures than on grasslands where livestock were excluded. Similarly, on previously eroded soil in the Piedmont region of the US, pasture establishment with well-managed grazing of livestock resulted in high rates of both carbon and nitrogen sequestration relative to results obtained where grass was grown without grazing. Such increases in carbon and nitrogen sequestration can help mitigate greenhouse gas emission effects. In some cases, ecosystem productivity may be increased due to grazing effects on nutrient cycling.\n\nRuminants have a four-compartment stomach that contains microbes. Microbes aid in the digestion of food. Some of these microbes (methanogenic archaea) produce methane as a metabolic byproduct. When the bovine ingests the food, the food travels to the rumen where microbes begin breaking down the roughage. The bovine then belches; this is when methane is first introduced to the atmosphere during this process. The food belched up is also known as cud. The cud is then swallowed where it is digested once more in the rumen before it enters the reticulum, omasum, abomasum, small intestine, and large intestine respectively. The remains exit where approximately 5% of the methane produced from cattle is emitted. This process is known as enteric fermentation. Enteric Fermentation occurs when methane is produced as cows' rumens digest carbohydrates through microbial fermentation. \n\nMethane makes up approximately 27% of rumen gases, carbon dioxide makes up approximately 66% of rumen gases, nitrogen makes up approximately 7% of rumen gases, and oxygen and hydrogen make up the remaining percentages. Animal waste contributes to 5% of methane sources available in the atmosphere while enteric fermentation makes up to 16% of all methane sources currently in the atmosphere. Together, that makes up 21% of the methane released into the atmosphere. Compare this percentage to the methane contribution of natural wetlands, which make up 22% of the methane released in the atmosphere. \n\nMore methane is produced from cows' belching than from flatulence; approximately 95% of methane produced by bovines is from belching. Methane is 84 times more potent than carbon dioxide and speeds up the Greenhouse Effect. The Greenhouse Effect is a process that warms Earth's surface and keeps the global temperature stable at 33°C by retaining a portion of greenhouse gases on Earth while releasing the rest back into space. As more methane and other greenhouse gases are introduced and held in the atmosphere or on Earth's surface, the global temperature will rise due to the fact that greenhouse gases absorb infrared radiation, also known as heat. The methane concentration has been growing exponentially since 1984 and is projected to continue to do so. \n\nAny deviation from the global temperature of 33°C will result in drastic effects in climate change such as a loss of biodiversity and more intense and unpredictable weather patterns. Coastal erosion is also another effect of climate change which leads to loss of fertile land as the sea levels rise. The sporadic weather patterns and change in seasons will also lead to unpredictable farming patterns. Pests and vector borne diseases will become more common and available as the global temperature rises. Growing seasons will become longer in cooler areas. \n\nAmericans are one of the largest contributing groups to the excessive amounts of beef consumption worldwide. The United States is the fourth largest consumer of beef and the 16th largest consumer of dairy worldwide. Americans consume four times the world average of beef consumption. On average, each American consumes around 600 pounds of beef and cow dairy products annually. The average American eats about 50 pounds of beef annually. \n\nThere are some controllable ways to reduce the amount of methane released into the atmosphere. Improving the digestion of bovine will decrease the bovine's tendency to belch and release digestive gases through the anus, which emit methane into the atmosphere. One way is to grind the cattle feed to make it finer which leads the cow to take less time and energy to digest it, and as a result, less methane is produced in the process. Scientists have introduced garlic into cattle's diets; garlic inhibits the microorganisms in the intestines from producing methane. Researchers at Penn State introduced 3-nitrooxypropanol to the cows' diets which suppresses the cows' ability to release methane but leads the cattle to gain weight since they are using less energy to digest their food. Studies have been conducted in adding plants high in tannin to ruminants' diets which in turn effectively reduces their methane emissions. All potential solutions in reducing bovine's methane emissions have proved to not be cost efficient which inhibits current farmers and ranchers from adopting them.\n\nAnother way to reduce methane released in the atmosphere is to monitor dietary practices. If the demand for cattle decreases, then the supply of cattle will also decrease as a result. Reducing beef and dairy intake in one's diet decreases one's risk in developing diseases such as lung cancer, breast cancer, ovarian cancer, prostate cancer, diabetes, Alzheimers, and heart disease. Dairy contains high levels of saturated fats. For example, cheese is 70% saturated fat. Overall, the overconsumption of beef and dairy shortens one's lifespan. Chicken, seafood, quinoa, tofu, mushrooms, lentils, nuts, and many other protein-rich foods are healthier alternative to beef. Alternatives that contain less saturated fats than cow milk include: almond milk, coconut milk, soy milk, rice milk, and hemp milk. Being environmentally conscious when making dietary decisions as well as altering the cow's feed will effectively decrease bovine's methane emissions into the atmosphere. \n\nVirtual water use for livestock production includes water used in producing feed.\nHowever, virtual water use data, such as those shown in the table, are often unrelated to environmental impacts of water use. For example, in a high-rainfall area, if similar soil infiltration capacity is maintained across different land uses, mm of groundwater recharge and hence sustainability of water use tends to be about the same for food crop production, meat-yielding livestock production, and saddle horse production, although virtual water use per kg of food produced may be several hundred L, several thousand L, and an infinite number of L, respectively. In contrast, in some low-rainfall areas, some livestock production is more sustainable than food crop production, from a water use standpoint, despite higher virtual water use per kg of food produced. Unirrigated land in many water-short areas may support grassland ecosystems in perpetuity, and thus may be able to support well-managed production of grazing cattle or sheep with a sustainable level of water use, whereas more water-demanding food crops would be unsustainable in the long run due to inadequate surface water supplies and groundwater recharge to sustain a high level of irrigation. Such considerations are important on much rangeland in western North America and elsewhere that can support cow-calf operations, backgrounding of stocker cattle, and sheep flocks. In the US, withdrawn surface water and groundwater use for crop irrigation exceeds that for livestock by about a ratio of 60:1.\n\nAlso, the high virtual water use figures associated with meat production do not necessarily imply reduction of water use if food crops are produced, instead of livestock. For example, some grazing lands are unsuitable for food crop production, so that evapotranspirational water use would continue on land vacated by livestock, while additional water would be needed for crops to provide substituting food from lands elsewhere, and additional water would also be needed to produce substitutes for the non-food products of livestock. (In the US, Land Capability Classes V, VI and VII contain soils unsuited for cultivation, much of which is suitable for grazing. Of non-federal land in the US, about 43 percent is classed as unsuitable for cultivation.)\n\nIrrigation accounts for about 37 percent of US withdrawn freshwater use, and groundwater provides about 42 percent of US irrigation water. Irrigation water applied in production of livestock feed and forage has been estimated to account for about 9 percent of withdrawn freshwater use in the United States. Groundwater depletion is a concern in some areas because of sustainability issues (and in some cases, land subsidence and/or saltwater intrusion). A particularly important North American example where depletion is occurring involves the High Plains (Ogallala) Aquifer, which underlies about 174,000 square miles in parts of eight states, and supplies 30 percent of the groundwater withdrawn for irrigation in the US. Some irrigated livestock feed production is not hydrologically sustainable in the long run because of aquifer depletion. However, rainfed agriculture, which cannot deplete its water source, produces much of the livestock feed in North America. Corn (maize) is of particular interest, accounting for about 91.8 percent of the grain fed to US livestock and poultry in 2010. About 14 percent of US corn-for grain land is irrigated, accounting for about 17 percent of US corn-for-grain production, and about 13 percent of US irrigation water use, but only about 40 percent of US corn grain is fed to US livestock and poultry. Together, these figures indicate that most production of grain used for US livestock and poultry feed does not deplete water resources and that irrigated production of grain for livestock feed accounts for a small fraction of US irrigation water use. However, where production relies on irrigation from groundwater reserves, water table monitoring is appropriate to provide timely warning if groundwater depletion occurs.\n\nIn the Western United States, many stream and riparian habitats have been negatively affected by livestock grazing. This has resulted in increased phosphates, nitrates, decreased dissolved oxygen, increased temperature, turbidity, and eutrophication events, and reduced species diversity. Livestock management options for riparian protection include salt and mineral placement, limiting seasonal access, use of alternative water sources, provision of \"hardened\" stream crossings, herding, and fencing. In the Eastern United States, waste release from pork farms have also been shown to cause large-scale eutrophication of bodies of water, including the Mississippi River and Atlantic Ocean (Palmquist, et al., 1997). However, in North Carolina, where Palmquist's study was done, measures have since been taken to reduce the risk of accidental discharges from manure lagoons; also, since then there is evidence of improved environmental management in US hog production. Implementation of manure and wastewater management planning can help assure low risk of problematic discharge into aquatic systems. (See Animal Waste section, below.)\n\nAt a global scale, the FAO has recently estimated that livestock (including poultry) accounts for about 14.5 percent of anthropogenic greenhouse gas emissions estimated as 100-year CO equivalents. A previous widely cited FAO report using somewhat more comprehensive analysis had estimated 18 percent. Because this emission percentage includes contributions associated with livestock used for the production of draft power, eggs, wool and dairy products, the percentage attributable to meat production alone is significantly lower, as indicated by the report's data. The indirect effects contributing to the percentage include emissions associated with the production of feed consumed by livestock and carbon dioxide emission from deforestation in Central and South America, attributed to livestock production. Using a different sectoral assignment of emissions, the IPCC (Intergovernmental Panel on Climate Change) has estimated that agriculture (including not only livestock, but also food crop, biofuel and other production) accounted for about 10 to 12 percent of global anthropogenic greenhouse gas emissions (expressed as 100-year carbon dioxide equivalents) in 2005 and in 2010.\n\nA PNAS model showed that even if animals were completely removed from US agriculture and diets, US GHG emissions would be decreased by 2.6%(or 28% of agricultural GHG emissions). This is because of the need to replace animal manures by fertilizers and to replace also other animal coproducts, and because livestock now use human-inedible food and fiber processing byproducts.\n\nIn the US, methane emissions associated with ruminant livestock (6.4 Mt in 2013) are estimated to have declined by about 17 percent from 1980 through 2012. Globally, enteric fermentation (mostly in ruminant livestock) accounts for about 27 percent of anthropogenic methane emissions, and methane accounts for about 32 to 40 percent of agriculture’s greenhouse gas emissions (estimated as 100-year carbon dioxide equivalents) as tabulated by the IPCC. Methane has a global warming potential recently estimated as 35 times that of an equivalent mass of carbon dioxide. However, despite the magnitude of methane emissions (recently about 330 to 350 Tg per year from all anthropogenic sources), methane’s current effect on global warming is quite small. This is because degradation of methane nearly keeps pace with emissions, resulting in a relatively little increase in atmospheric methane content (average of 6 Tg per year from 2000 through 2009), whereas atmospheric carbon dioxide content has been increasing greatly (average of nearly 15,000 Tg per year from 2000 through 2009).\n\nMitigation options for reducing methane emission from ruminant enteric fermentation include genetic selection, immunization, rumen defaunation, outcompetition of methanogenic archaea with acetogens, introduction of methanotrophic bacteria into the rumen, diet modification and grazing management, among others. The principal mitigation strategies identified for reduction of agricultural nitrous oxide emission are avoiding over-application of nitrogen fertilizers and adopting suitable manure management practices. Mitigation strategies for reducing carbon dioxide emissions in the livestock sector include adopting more efficient production practices to reduce agricultural pressure for deforestation (notably in Latin America), reducing fossil fuel consumption, and increasing carbon sequestration in soils. Australian scientists discovered that adding the seaweed Asparagopsis taxiformis to the cattle's diet can reduce methane by up to 99%, and reported a 3% seaweed diet resulted in an 80% reduction in methane.\n\nIn New Zealand, nearly half of [anthropogenic] greenhouse gas emission is associated with agriculture, which plays a major role in the nation’s economy, and a large fraction of this is assignable to the livestock industry. Some fraction of this is assignable to meat production: FAO data indicate that meat accounted for about 7 percent of product tonnage from New Zealand's livestock (including poultry) in 2010. Livestock sources (including enteric fermentation and manure) account for about 3.1 percent of US anthropogenic greenhouse gas emissions expressed as carbon dioxide equivalents, according to US EPA figures compiled using UNFCCC methodologies. Not all forms of meat and animal–based foods affect the environment equally. One study estimates that red meats are 150% more greenhouse gas intensive than chicken or fish. According to another research group, the ranking of some food products in relation to greenhouse gas emissions is lamb (#1), beef (#2), cheese (#3), and pork (#4). However, such ranking may not be broadly representative. Among sheep production systems, for example, there are very large differences in both energy use and prolificacy; both factors strongly influence emissions per kg of lamb production.\n\nAccording to a 2018 study in the journal \"Nature\", a significant reduction in meat consumption will be \"essential\" to mitigate climate change, especially as the human population increases by a projected 2.3 billion by the middle of the century.\n\nMeat production is one of the leading causes of greenhouse gas emissions and other particulate matter pollution in the atmosphere. This type of production chain produces copious byproducts; endotoxin, hydrogen sulfide, ammonia, and particulate matter (PM), such as dust, are all released along with the aforementioned methane and CO2. Furthermore, elevated greenhouse gas emissions have been associated with respiratory diseases like asthma, bronchitis, and COPD, as well as increased chances of acquiring pneumonia from bacterial infections.\n\nIn addition, exposure to PM10 (particulate matter 10 micrometers in diameter) may produce diseases that impact the upper and proximal airways. However, farmers aren’t the only ones at risk for exposure to these harmful byproducts. In fact, concentrated animal feeding operations (CAFOs) in proximity to residential areas adversely affect these individuals' respiratory health similarly seen in the farmers. Concentrated hog feeding operations release air pollutants from confinement buildings, manure holding pits, and land application of waste. Air pollutants from these operations have caused acute physical symptoms, such as respiratory illnesses, wheezing, increased breath rate, and irritation of the eyes and nose. That prolonged exposure to airborne animal particulate, such as swine dust, induces a large influx of inflammatory cells into the airways. Those in close proximity to CAFOs could be exposed to elevated levels of these byproducts, which may lead to poor health and respiratory outcomes.\n\nData of a USDA study indicate that about 0.9 percent of energy use in the United States is accounted for by raising food-producing livestock and poultry. In this context, energy use includes energy from fossil, nuclear, hydroelectric, biomass, geothermal, technological solar, and wind sources. (It excludes solar energy captured by photosynthesis, used in hay drying, etc.) The estimated energy use in agricultural production includes embodied energy in purchased inputs.\n\nAn important aspect of energy use of livestock production is the energy consumption that the animals contribute. Feed Conversion Ratio is an animal’s ability to covert feed into meat. The Feed Conversion Ratio (FCR) is calculated by the taking the energy, protein or mass input of the feed divided by the output of meat provided by the animal. A lower FCR corresponds with a smaller requirement of feed per meat out-put, therefore the animal contributes less GHG emissions. Chickens and pigs usually have a lower FCR compared to ruminants.\n\nIntensification and other changes in the livestock industries influence energy use, emissions and other environmental effects of meat production. For example, in the US beef production system, practices prevailing in 2007 are estimated to have involved 8.6 percent less fossil fuel use, 16 percent less greenhouse gas emissions, 12 percent less water use and 33 percent less land use, per unit mass of beef produced, than in 1977. These figures are based on analysis taking into account feed production, feedlot practices, forage-based cow-calf operations, backgrounding before cattle enter a feedlot, and production of culled dairy cows.\n\nWater pollution due to animal waste is a common problem in both developed and developing nations. The USA, Canada, India, Greece, Switzerland and several other countries are experiencing major environmental degradation due to water pollution via animal waste. Concerns about such problems are particularly acute in the case of CAFOs (concentrated animal feeding operations). In the US, a permit for a CAFO requires implementation of a plan for management of manure nutrients, contaminants, wastewater, etc., as applicable, to meet requirements under the Clean Water Act. There were about 19,000 CAFOs in the US as of 2008. In fiscal 2014, the United States Environmental Protection Agency (EPA) concluded 26 enforcement actions for various violations by CAFOs. Environmental performance of the US livestock industry can be compared with several other industries. The EPA has published 5-year and 1-year data for 32 industries on their ratios of enforcement orders to inspections, a measure of non-compliance with environmental regulations: principally, those under Clean Water Act and Clean Air Act. For the livestock industry, inspections focused primarily on CAFOs. Of the 31 other industries, 4 (including crop production) had a better 5-year environmental record than the livestock industry, 2 had a similar record, and 25 had a worse record in this respect. For the most recent year of the five-year compilation, livestock production and dry cleaning had the best environmental records of the 32 industries, each with an enforcement order/inspection ratio of 0.01. For crop production, the ratio was 0.02. Of the 32 industries, oil and gas extraction and the livestock industry had the lowest percentages of facilities with violations.\n\nWith good management, manure has environmental benefits. Manure deposited on pastures by grazing animals themselves is applied efficiently for maintaining soil fertility. Animal manures are also commonly collected from barns and concentrated feeding areas for efficient re-use of many nutrients in crop production, sometimes after composting. For many areas with high livestock density, manure application substantially replaces application of synthetic fertilizers on surrounding cropland. Manure was spread as a fertilizer on about 15.8 million acres of US cropland in 2006. Manure is also spread on forage-producing land that is grazed, rather than cropped. Altogether, in 2007, manure was applied on about 22.1 million acres in the United States. Substitution of animal manure for synthetic fertilizer has important implications for energy use and greenhouse gas emissions, considering that between about 43 and 88 MJ (i.e. between about 10 and 21 Mcal) of fossil fuel energy are used per kg of N in the production of synthetic nitrogenous fertilizers.\n\nManure can also have environmental benefit as a renewable energy source, in digester systems yielding biogas for heating and/or electricity generation. Manure biogas operations can be found in Asia, Europe, North America, and elsewhere. The US EPA estimates that as of July 2010, 157 manure digester systems for biogas energy were in operation on commercial-scale US livestock facilities. System cost is substantial, relative to US energy values, which may be a deterrent to more widespread use, although additional factors, such as odor control and carbon credits, may improve benefit /cost ratios.\n\nGrazing (especially overgrazing) may detrimentally affect certain wildlife species, e.g. by altering cover and food supplies. However, habitat modification by livestock grazing can also benefit some wildlife species. For example, in North America, various studies have found that grazing sometimes improves habitat for\nelk,\nblacktailed prairie dogs,\nsage grouse,\nmule deer,\nand numerous other species. A survey of refuge managers on 123 National Wildlife Refuges in the US tallied 86 species of wildlife considered positively affected and 82 considered negatively affected by refuge cattle grazing or haying. Such mixed effects suggest that wildlife diversity may be enhanced and maintained by grazing livestock in some places while excluding livestock in some places. The kind of grazing system employed (e.g. rest-rotation, deferred grazing, HILF grazing) is often important in achieving grazing benefits for particular wildlife species.\n\nSome scientists claim that the growing demand for meat is contributing to significant biodiversity loss as it is a significant driver of deforestation and habitat destruction; species-rich habitats, such as significant portions of the Amazon region, are being converted to agriculture for meat production. Nearly 40% of global land surface is being used for livestock farming.\n\nApproximately 90% of the total use of antimicrobials in the United States was for non-therapeutic purposes in agricultural production. Livestock production has been associated with increased antibiotic resistance in bacteria, and has been associated with the emergence of microbes which are resistant to multiple antimicrobials (often referred to as superbugs).\n\nAmong other environmental benefits of meat production, is the conversion of materials that might otherwise be wasted, to produce high-protein food. For example, Elferink et al. state that \"Currently, 70 % of the feedstock used in the Dutch feed industry originates from the food processing industry.\" US examples of \"waste\" conversion with regard to grain include feeding livestock the distillers grains (with solubles) remaining from ethanol production. For the marketing year 2009-2010, dried distillers grains used as livestock feed (and residual) in the US was estimated at 25.5 million metric tons. Examples with regard to roughages include straw from barley and wheat crops (feedable especially to large-ruminant breeding stock when on maintenance diets), and corn stover. Also, small-ruminant flocks in North America (and elsewhere) are sometimes used on fields for removal of various crop residues inedible by humans, converting them to food.\n\nThere are environmental benefits of meat-producing small ruminants for control of specific invasive or noxious weeds (such as spotted knapweed, tansy ragwort, leafy spurge, yellow starthistle, tall larkspur, etc.) on rangeland . Small ruminants are also useful for vegetation management in forest plantations, and for clearing brush on rights-of-way. These represent food-producing alternatives to herbicide use.\n\n\n"}
{"id": "56166567", "url": "https://en.wikipedia.org/wiki?curid=56166567", "title": "Estonia at the Deaflympics", "text": "Estonia at the Deaflympics\n\nEstonia has been participating at the Deaflympics since 1993 and has earned a total of 32 medals.\n\n\n"}
{"id": "28539183", "url": "https://en.wikipedia.org/wiki?curid=28539183", "title": "EuResist", "text": "EuResist\n\nEuResist is an international project designed to improve the treatment of HIV patients by developing a computerized system that can recommend optimal treatment based on the patient’s clinical and genomic data.\n\nThe project is part of the Virtual Physiological Human framework, funded by the European Commission. It started in 2006 with the formation of a consortium of several research institutes and hospitals in Europe and Israel. The consortium completed its commitment to the European Commission near the end of 2008, at which time the system became available online. A non-profit organization was consequently established by the main partners to maintain and improve the system.\n\nIn 2009, the EuResist project was named as a Computerworld honors program laureate.\n\nAIDS is a disease caused by the HIV retrovirus, which progressively reduces the effectiveness of the immune system, leading to infections and ultimately death.\n\nMore than 30 different drugs exist for treating HIV patients. Antiretroviral drugs can disrupt the virus’s replication process causing its numbers to decrease dramatically. While the virus cannot be eradicated completely, in small numbers it is harmless. Usually a patient is given a combination of three or four drugs, a treatment known as highly active antiretroviral therapy, or HAART. The main reason such a treatment might fail is the development of mutated strands of the virus, resistant to one or more of the prescribed drugs.\n\nThus an important consideration when choosing treatment for a patient is to prescribe those drugs to which the particular patient’s virus strands are most susceptible. One way to achieve that is to extract virus samples from the patient’s blood and test them against all possible drugs. Since this process is lengthy and costly, computerized systems have been developed to predict virus resistance based on its genotype. The treating physician samples virus genotype sequences from the patient’s blood and provides this data to a computerized system. The system then responds with drug recommendations.\n\nSuch systems are limited in accuracy, depending on the amount of data used for their creation, its quality and the richness of mathematical models used for the actual prediction. Prior to EuResist, such systems had several common characteristics that negatively impacted their accuracy:\n\nEuResist sought to create a more accurate HIV treatment prediction system by collecting a large database of in vivo data (clinical and genomic records of real treatments of HIV patients and their consequences), and by using an array of prediction models instead of just one.\nThe database was created by merging local databases of various clinics across Europe. This database is thought to be the largest of its kind in the world.\nFor each patient, it includes various personal and demographic details such as gender, age, country of origin, genomic sequencing of HIV found in the patient’s blood, records of the drugs prescribed, and the changes in the amount of virus in the blood following these treatments.\n\nThis data was used to train an array of prediction models, created by using various contemporary machine learning techniques, among them Bayesian networks, logistic regression, and others.\n\nA web interface allows physicians to specify patients' clinical and genomic data. This data is sent to the prediction engines, and the combined response, which is displayed to the physician, includes various suggested treatments and a prediction of their effect on the amount of HIV in the blood.\n\nThe EuResist system was tested and compared with its predecessors by feeding it with historical data on patients for which treatment results are known. The developers of EuResist, who conducted this test, reported an improved performance over the previous state-of-the-art system.\n\nEuResist started in 2006 as a consortium funded by the European Union as part of the Virtual Physiological Human FP-6 framework. The partners of this consortium were:\nThe consortium completed its commitment to the European Union in late 2008, at which time the EuResist system became available on line.\nThe first five partners mentioned above continued to form a non-profit organization that maintains the system, expands the database with new clinical and genomical records and updates the prediction engines accordingly. As of mid-2010, an average of 600 queries are submitted to the EuResist system every quarter.\n\nOn June 1, 2009, EuResist received a Computerworld honors program laureate award, a global program honoring individuals and organizations that use information technology to benefit society.\n\n"}
{"id": "8163006", "url": "https://en.wikipedia.org/wiki?curid=8163006", "title": "European Federation of Biotechnology", "text": "European Federation of Biotechnology\n\nThe European Federation of Biotechnology (EFB) was established by European scientists in 1978. It is a non-profit federation of national biotechnology associations, learned societies, universities, scientific institutes, biotechnology companies and individual biotechnologists working to promote biotechnology throughout Europe and beyond.\n\nThe mission of the EFB is to promote the safe, sustainable and beneficial use of the life sciences, to promote research and innovation at the cutting edge of biotechnology, to provide a forum for interdisciplinary and international cooperation, to improve scientific education and to facilitate an informed dialogue between scientists, the biotechnology industries and the public.\n\nThe EFB Central Office (ECO) is located in Barcelona, Spain. Membership administration, organization of Executive Board meetings and General Assemblies, website management and organization of scientific meetings for EFB Sections and promotion of the European Congresses on Biotechnology are some of the main responsibilities of ECO.\n\nThe European Congress on Biotechnology (ECB) is a conference for academic and industrial biotechnologists in Europe, organised by the European Federation of Biotechnology.\n\nThe first Congress in the series was held in 1978 (Interlaken, Switzerland). As in all of the earlier congresses, the principal aim of the current ECB2018 is bring together the international scientific community and many segments of the biotech industry. The Congress will present the latest developments in front-line research and provide a platform for exchanging ideas in biotechnology and its applications. Participants will be able to interact with business leaders, investors, trade exhibitors and policy makers to discuss regulatory issues and the bioeconomy.\n\n\"New Biotechnology\" is the official journal of the European Federation of Biotechnology (EFB) and is published bimonthly. It covers both the science of biotechnology and its surrounding political, business and financial milieu. The journal publishes peer-reviewed basic research papers, authoritative reviews, feature articles and opinions in all areas of biotechnology. It reflects the full diversity of current biotechnology science, particularly those advances in research and practice that open opportunities for exploitation of knowledge, commercially or otherwise, together with news, discussion and comment on broader issues of general interest and concern. The outlook is fully international.\n\n\n"}
{"id": "1632775", "url": "https://en.wikipedia.org/wiki?curid=1632775", "title": "European Health Insurance Card", "text": "European Health Insurance Card\n\nThe European Health Insurance Card (or EHIC) is issued free of charge and allows anyone who is insured by or covered by a statutory social security scheme of the EEA countries and Switzerland to receive medical treatment in another member state free or at a reduced cost, if that treatment becomes necessary during their visit (for example, due to illness or an accident), or if they have a chronic pre-existing condition which requires care such as kidney dialysis. The term of validity of the card varies according to the issuing country.\n\nThe intention of the scheme is to allow people to continue their stay in a country without having to return home for medical care; as such, it does not cover people who have visited a country for the purpose of obtaining medical care, nor does it cover care, such as many types of dental treatment, which can be delayed until the individual returns to his or her home country. The costs not covered by self-liability fees are paid by the issuing country, which is usually the country of residence but may also be the country where one receives the most pension from.\n\nIt only covers healthcare which is normally covered by a statutory health care system in the visited country, so it does not render travel insurance obsolete.\n\nThe card was phased in from 1 June 2004 and throughout 2005, becoming the sole healthcare entitlement document on 1 January 2006. The card is applicable in all French overseas departments (Martinique, Guadeloupe, Réunion, and French Guiana) as they are part of the EEA, but not non-EEA dependent territories such as Jersey, the Isle of Man, Aruba, or French Polynesia. However, there are agreements for the use of the EHIC in the Faroe Islands and Greenland, even though they are not in the EEA.\n\nThe reason for the existence of this card, is that the right to health care in Europe is based on the country of legal residence, not the country of citizenship. Therefore, a passport is not enough to receive health care. It is however possible that a photo ID document is asked for, since the European Health Insurance Card does not contain a photo.\n\nIn some cases, even if a person is covered by the health insurance of an EU country, one is not eligible for a European Health Insurance Card. For instance, in Romania, a person who is currently insured has to have been insured for the previous five years to be eligible. Romania is also the only participating country where not all permanent residents are covered by a health insurance. Due to these reasons the Romanian Roma people typically neither have European insurance cards nor is their costs paid by the country of residence.\n\nIt replaced the following medical forms:\n\nEuropean Health Insurance cards are provided free to all legal residents of participating countries. There are however various businesses who act as non-official agents on behalf of individuals, arranging supply of the cards in return for payment, often offering additional services such as the checking of applications for errors and general advice or assistance. This has proved extremely controversial. In 2010 the British government moved against companies that invited people to pay for the free EHIC, falsely implying that through payment the applicant could speed up the process. Despite this, the practice continues.\n\nAs of 2013, 32 countries in Europe participate: the 31 member states of the European Economic Area (EEA) plus Switzerland. This includes the 28 member states of the European Union (EU) and 4 member states of the European Free Trade Association (EFTA).\n\n\n\nThe Channel Islands and Isle of Man do not supply coverage under the EHIC, and their residents are not eligible for EHICs.\n\nIn August 2015 the \"Daily Mail\" ran a story about abuse of the EHIC system in which a card was issued to its undercover Hungarian reporter who “obtained the card after visiting the UK for less than one day” after another journalist posed as her landlord and presented a GP with the tenancy agreement of a property that neither occupied in order to get an NHS number. It claimed that \"foreigners were charging the NHS for care in their own country.\" As \"The Guardian\" pointed out, the NHS issued a card to an individual that wasn’t eligible to receive the card because a GP was duped into issuing an NHS number, and it was unclear what benefit would accrue as a result.\n\nWith no active investigation branch, the likelihood of NHS authorities discovering fraud is extremely low. The \"Huffington Post\" reported that only nine instances of low-level fraud involving the EHIC in the UK had been discovered in five years with a combined cost of £712.56.\n\n\n"}
{"id": "4644932", "url": "https://en.wikipedia.org/wiki?curid=4644932", "title": "Federation of European Biochemical Societies", "text": "Federation of European Biochemical Societies\n\nThe Federation of the European Biochemical Societies, frequently abbreviated FEBS is an international scientific society promoting activities in biochemistry, molecular biology and molecular biophysics in Europe. Since it was founded in 1964 it has grown to include almost 40,000 members from 36 member societies and 7 associated societies from 43 countries .\n\nFEBS sponsors advanced courses for Ph.D.-students and postdocs, arranges conferences and awards fellowships, awards and medals. FEBS distributes surplus scientific equipment in the poorer member countries as a part of the Scientific Apparatus Recycling Scheme (SARS). In addition, FEBS gives young scientists from Eastern and Central Europe a possibility to visit and work in labs in Western Europe. FEBS collaborates with related scientific societies such as the European Molecular Biology Organization (EMBO), European Life Scientist Organisation (ELSO), and the European Molecular Biology Laboratory (EMBL). FEBS is also founding member of the Initiative for Science in Europe.\n\nThe Sir Hans Krebs Medal is awarded annually for outstanding achievements in Biochemistry and Molecular Biology or related sciences. It was endowed by the Lord Rank Centre for Research and named after the German-born British biochemist Hans Adolf Krebs, well known for identifying the urea and citric acid cycles. The awardee receives a silver medal and presents one of the plenary lectures at the FEBS Congress.\n\nThe Datta Lectureship award is awarded for outstanding achievements in the field of Biochemistry and Molecular Biology or related sciences. The award is endowed by capital gifts from Elsevier Science Publishers and is named after S. Prakash Datta, the first Managing Editor of FEBS Letters (1968–1985) and Treasurer of FEBS. The awardee, who should normally be from a FEBS country, receives a medal provided by Elsevier Science Publishers and presents one of the plenary lectures at the FEBS Congress.\n\nThe Theodor Bücher Lecture and Medal is awarded for outstanding achievements in Biochemistry and Molecular Biology or related sciences. It was endowed by a capital gift from Frau Ingrid Bücher to the Gesellschaft für Biochemie und Molekularbiologie (GBM) and is named after the German biochemist, Theodor Bücher. The awardee receives a silver medal and presents one of the plenary lectures at the FEBS Congress.\n\nFEBS publishes four scientific journals \"FEBS Journal, FEBS Letters\", and \"Molecular Oncology\" and \"FEBS Open Bio\". \"FEBS Journal\" was previously known as the \"European Journal of Biochemistry\". \"FEBS Journal\" publishes full length research reports, whereas \"FEBS Letters\" is dedicated to brief and rapid reports. \"FEBS Open Bio\" is an open access journal.\n\n"}
{"id": "57559475", "url": "https://en.wikipedia.org/wiki?curid=57559475", "title": "Gummy smile", "text": "Gummy smile\n\nGummy smile also known as excessive gingival display is a smile that shows an excessive amount of gum under the upper lip. It is a common unaesthetic clinical condition, which can be caused by an abnormal dental eruption (delayed passive eruption), or hyperfunction of the upper lip elevator muscle or by an excessive vertical growth of the bone of maxilla or over-eruption of the maxillary anterior teeth, or a combinations to one of the above described factors together. Several treatment options have been proposed to enhance the smile display and to reduce the gingival exposure.\n\nTreatment option include orthodontics, surgery, botulinum toxin A injections, and micro-autologous fat transplantation (MAFT).\n\nBotox (BTX-A) has been successful in the treatment of gummy smiles, however the results are not permanent, they last for an average of 6 months. The material is injected into the hyperactive muscles of upper lip, which causes a reduction in the upward movement of lip thus resulting in a smile with a less exposure of gingiva. Botox is usually injected in the three lip elevator muscles that converge on the lateral side of the ala of the nose; the levator labii superioris (LLS), the levator labii superioris alaeque nasi muscle (LLSAN), and the zygomaticus minor (ZMi).\n"}
{"id": "57365707", "url": "https://en.wikipedia.org/wiki?curid=57365707", "title": "Guwahati Food Awards", "text": "Guwahati Food Awards\n\nGuwahati Food Awards (GFA) is an annual food award in India, which was instituted in 2015. It aims to recognize superior services and achievements in the Food and Beverage industry of Guwahati, India.\n\nThe winners gets selected through a multi-phase process that involves public voting, food tasting and the jury’s visits.\nIt was established in 2015 by G Plus.\nThis edition received over 157 nominations across 23 categories. The jury of GFA 2018 includes Chef Faruk Ahmed, Dr. Geeta Dutta and Aabhishek Bedi Varma, a former chef of Taj Group of Hotels and headed by Suresh Hinduja, the CEO of Gourmet India. Kunal Vijaykar was a chief guest for the event.\n\nGFA received 186 nominations across 25 categories in 2017. The special jury included Food critic and author Ashish Chopra and Suresh Hinduja, CEO of Gourmet India. Riyaaz Amlani, Restaurateur and President of National Restaurants Association of India (NRAI) was the Chief Guest for the event.\nThe second edition of Guwahati Food Awards was held at Vivanta by Taj, Guwahati with Chief Guest Chef Manjit Gill, Corporate Chef, ITC Hotels and President of Indian Federation of Culinary Associations (IFCA).\nThe first edition of Guwahati Food Awards was held at Radisson, Guwahati. The Chief Guest of the event was Sanjeev Pahwa - Senior Vice President – Asia for Carlson Rezidor Hotels.\n"}
{"id": "10008306", "url": "https://en.wikipedia.org/wiki?curid=10008306", "title": "Halachic Organ Donor Society", "text": "Halachic Organ Donor Society\n\nThe Halachic Organ Donor Society, also known as the HOD Society, was started in December 2001 by Robert Berman. Its mission is to save lives by increasing organ donation from Jews to the general public (including gentiles).\n\nThe organization recognizes the legitimate debate in Orthodox Jewish law surrounding brain stem death and offers a unique organ donor card that allows people to choose between donating organs at brain stem death or alternatively at cessation of heart beat (asystole). It currently has thousands of members, including more than 200 Orthodox Rabbis and several Chief Rabbis. It has delivered educational lectures that have encouraged more than 34,000 Jews to donate organs.\n\n"}
{"id": "17837671", "url": "https://en.wikipedia.org/wiki?curid=17837671", "title": "History of Indian cuisine", "text": "History of Indian cuisine\n\nThe history of Indian cuisine consists of cuisine from the Indian subcontinent, which is rich and diverse. As a land that has experienced extensive immigration and intermingling through many millennia, the Indian subcontinent has benefited from numerous food influences. The diverse climate in the region, ranging from deep tropical to alpine, has also helped considerably broaden the set of ingredients readily available to the many schools of cookery in India. In many cases, food has become a marker of religious and social identity, with varying taboos and preferences (for instance, a segment of the Jain population consume no roots or subterranean vegetable; see Jain vegetarianism) which has also driven these groups to innovate extensively with the food sources that are deemed acceptable.\n\nOne strong influence over Indian foods is the longstanding vegetarianism within sections of India's Hindu and Jain communities. At 31%, slightly less than a third of Indians are vegetarians.\n\nAround 7000 BCE, agriculture spread from the Fertile Crescent to the Indus Valley, and wheat and barley began to be grown. Sesame, and humped cattle were domesticated in the local farming communities. By 3000 BCE, turmeric, cardamom, black pepper and mustard were harvested in India. The ancient Hindu text \"Mahabharata\" mentions rice and meat cooked together, and the word \"pulao\" or \"pallao\" is used to refer to the dish in ancient Sanskrit works, such as Yājñavalkya Smṛti. Vegetarianism was facilitated by the advent of Buddhism and a cooperative climate where a variety of fruits, vegetables, and grains could easily be grown throughout the year. A food classification system that categorized any item as saatvic, raajsic or taamsic developed in Ayurveda. \n\nLater, arrivals from Arabia, Central Asia, and others had a deep and fundamental effect on Indian cooking. Islamic rule introduced rich gravies, biryani and non-vegetarian fare such as kebabs, resulting in Mughlai cuisine (Mughal in origin), as well as such fruits as apricots, melons, peaches, and plums. The Mughals were great patrons of cooking. Lavish dishes were prepared during the reigns of Jahangir and Shah Jahan. The Muslim meat dishes includes bovine, ovine, poultry and seafood dishes. The Nizams of Hyderabad state (1700s-1947) perfected their own style of cooking with the most notable dish being the Hyderabad biryani, often considered by many connoisseurs to be the finest of the main dishes in India.\n\nThe Portuguese and British during their rule introduced cooking techniques such as baking, and foods from the New World. The new-world vegetables popular in cuisine from the Indian subcontinent include tomato, potato, sweet potatoes, peanuts, squash, and chilli. Most New world vegetables such as sweet potatoes, potatoes, Amaranth, peanuts and cassava based Sago are allowed on Hindu fasting days.\n"}
{"id": "34955617", "url": "https://en.wikipedia.org/wiki?curid=34955617", "title": "International Maternal Pediatric Adolescent AIDS Clinical Trials Group", "text": "International Maternal Pediatric Adolescent AIDS Clinical Trials Group\n\nThe International Maternal Pediatric Adolescent AIDS Clinical Trials Group, known as IMPAACT, is a United States-based research network which studies HIV and AIDS in infant, pediatric, adolescent and pregnant women populations. It is a member of the Office of HIV/AIDS Network Coordination research group.\n\nIn 2012 IMPAACT published a study which found no link between the use of HIV medication and psychiatric illness in adolescents.\n\nAs of 2010, IMPAACT consisted of 39 domestic and 34 international clinical research sites.\n"}
{"id": "7236000", "url": "https://en.wikipedia.org/wiki?curid=7236000", "title": "Ishinpō", "text": "Ishinpō\n\nThe structural organization of the text is as follows:\n\nThe \"Ishinpō\" preserved more than 200 important medical documents that were all Chinese in origin and no Japanese sources. The medical knowledge in the tome covered clinical treatments that drew from the ancient Chinese traditional medicine and influenced by Indian medical theories found in Buddhist scriptures as well as Taoist references (e.g. Taoist drugs). For instance, there was the so-called \"Scripture on Pregnancy\", which outlined the physical developments and fetal movements. Scholars cite its similarity with a prescription from the old Chinese medical text called \"Taichan shu,\" which contained doctrines about the development of embryo and fetus as well as proper hygiene for pregnant women. \n\nThe \"Ishinpō\" is also notable for preserving some of the Taoist sexual manuals from the Han to the Tang dynasty. The twenty-eighth section of the \"Ishinpō\" contains a complete transcription of a Daoist text known as \"The Classic of Sunu\" which is a dialogue between the Dark Maiden and the Yellow Emperor, with the former providing advice on sexual practices to the latter.\n\nWhile the text is written in kanbun, Japanese terms are written to the side in Man'yōgana for plants, animals, and minerals.\n\n"}
{"id": "56913905", "url": "https://en.wikipedia.org/wiki?curid=56913905", "title": "John A. Kenney Sr.", "text": "John A. Kenney Sr.\n\nJohn Andrew Kenney Sr. (June 11, 1874January 29, 1950) was an African-American surgeon who was the medical director and chief surgeon of the John A. Andrew Memorial Hospital at the Tuskegee Institute in Tuskegee, Alabama, from 1902 to 1922. He was also the co-founder of the National Medical Association (NMA), of which he also served as secretary from 1904 to 1912. He was elected president of the NMA in 1912. He was the editor-in-chief of its journal, the \"Journal of the National Medical Association\", from 1916 to 1948. After leaving Tuskegee in 1924, he founded the Kenney Memorial Hospital in Newark, New Jersey. This hospital was later renamed the Booker T. Washington Community Hospital in 1935, and closed down in 1953. He also served as the personal physician of both Booker T. Washington and George Washington Carver. \n"}
{"id": "15255176", "url": "https://en.wikipedia.org/wiki?curid=15255176", "title": "List of fatal snake bites in the United States", "text": "List of fatal snake bites in the United States\n\nThis is a list of some people who received a fatal snake bite in the United States by decade in reverse chronological order. There is no evidence it is a comprehensive list.\n\nThe United States has about 20 species of venomous snakes, which include 16 species of rattlesnakes, two species of coral snakes, one species of cottonmouth (or water moccasin), and one species of copperhead. At least one species of venomous snake is found in every state except Alaska and Hawaii.\n\nIt has been estimated that 7,000–8,000 people per year receive venomous snake bites in the United States, and about five of those people die. Most fatal bites are attributed to the eastern diamondback rattlesnake and the western diamondback rattlesnake. The copperhead accounts for more cases of venomous snake bite than any other North American species; however, its venom is the least toxic, so its bite is seldom fatal.\n\nVenomous snakes are distributed unevenly throughout the United States — the vast majority of snake bites occur in warm weather states. States like Florida and Texas have a wide variety and large population of venomous snakes. Bites from venomous snakes are extremely rare in the states near the Canada–US border. Maine, for example, has only one species (timber rattlesnake); it is rarely seen, and then only in the southern part of the state, but the species is likely extirpated in Maine, with the last sighting in 1901.\n\nSnakes:\n\nOther animals:\n\n"}
{"id": "1370590", "url": "https://en.wikipedia.org/wiki?curid=1370590", "title": "Medical slang", "text": "Medical slang\n\nMedical slang is the use of acronyms and informal terminology to describe patients, other healthcare personnel and medical concepts. Some terms are pejorative. In English, medical slang has entered popular culture via television hospital and forensic science dramas such as \"ER\", \"House M.D.\", \"NCIS\", \"Scrubs\", and \"Grey's Anatomy\".\n\nExamples of pejorative language include \"bagged and tagged\" for a corpse, a reference to the intake process at a mortuary, \"donorcycle\" for \"motorcycle or PFO\" for \"pissed [drunk] and fell over\". Less offensive are the terms \"blue pipes\" for veins; \"cabbage\" for a heart bypass (\"coronary artery bypass graft\" or CABG), and \"champagne tap\" for a flawless lumbar puncture, that is, one where erythrocyte count is zero.\n\nIn many countries, facetious or insulting acronyms are now considered unethical and unacceptable, and patients can access their medical records. Medical facilities risk being sued by patients offended by the descriptions. Another reason for the decline is that facetious acronyms could be confused with genuine medical terms and the wrong treatment administered.\n\nIn one of his annual reports (related by the BBC), medical slang collector Dr. Adam Fox cited an example where a practitioner had entered “TTFO”, meaning “told to fuck off”, on a patient’s chart. When questioned about the chart entry, the practitioner said that the initials stood for “to take fluids orally.”\n\nAs a result, medical slang tends to be restricted to oral use and to informal notes or E-mails which do not form part of a patient’s formal records. It may also be used among medical staff outside of the hospital. It is not found on patients’ charts and, due to growing awareness of medical slang, often not used in front of patients themselves.\n\nAlthough online medical slang dictionaries are primarily from English-speaking countries, non-English medical slang has been collected by Fox from elsewhere. Brazilian medical slang includes \"PIMBA\" (\"Pé Inchado Mulambo Bêbado Atropelado\" meaning \"swollen-footed, drunk, run-over beggar\"), \"Poliesculhambado\" (multi-messed-up patient) and \"Trambiclínica\" (a \"fraudulent clinic\" staffed cheaply by medical students).\n\nThere is an annual round-up of the usage of medical slang by British physician Dr. Adam Fox of St Mary's Hospital, London. Fox has spent five years charting more than 200 examples, regional and national terms and the general decline of medical slang. He believes that doctors have become more respectful of patients, which has contributed to the decline. While its use may be declining in the medical profession, several dictionaries of the slang have been compiled on the internet.\n\n\n\n"}
{"id": "16743975", "url": "https://en.wikipedia.org/wiki?curid=16743975", "title": "Membrane bioreactor", "text": "Membrane bioreactor\n\nMembrane bioreactor (MBR) is the combination of a membrane process like microfiltration or ultrafiltration with a biological wastewater treatment process, the activated sludge process. It is now widely used for municipal and industrial wastewater treatment.\n\nWhen used with domestic wastewater, MBR processes can produce effluent of high quality enough to be discharged to coastal, surface or brackish waterways or to be reclaimed for urban irrigation. Other advantages of MBRs over conventional processes include small footprint, easy retrofit and upgrade of old wastewater treatment plants.\n\nIt is possible to operate MBR processes at higher mixed liquor suspended solids (MLSS) concentrations compared to conventional settlement separation systems, thus reducing the reactor volume to achieve the same loading rate.\n\nTwo MBR configurations exist: internal/submerged, where the membranes are immersed in and integral to the biological reactor; and external/sidestream, where membranes are a separate unit process requiring an intermediate pumping step.\n\nRecent technical innovation and significant membrane cost reduction have enabled MBRs to become an established process option to treat wastewaters. As a result, the MBR process has now become an attractive option for the treatment and reuse of industrial and municipal wastewaters, as evidenced by their constantly rising numbers and capacity. The current MBR market has been estimated to value around US$216 million in 2006 and to rise to US$363 million by 2010.\n\nMembrane bioreactors can be used to reduce the footprint of an activated sludge sewage treatment system by removing some of the liquid component of the mixed liquor. This leaves a concentrated waste product that is then treated using the activated sludge process.\n\nThe MBR process was introduced by the late 1960s, as soon as commercial scale ultrafiltration (UF) and microfiltration (MF) membranes were available. The original process was introduced by Dorr-Oliver Inc. and combined the use of an activated sludge bioreactor with a crossflow membrane filtration loop. The flat sheet membranes used in this process were polymeric and featured pore sizes ranging from 0.003 to 0.01 μm. Although the idea of replacing the settling tank of the conventional activated sludge process was attractive, it was difficult to justify the use of such a process because of the high cost of membranes, low economic value of the product (tertiary effluent) and the potential rapid loss of performance due to membrane fouling. As a result, the focus was on the attainment of high fluxes, and it was therefore necessary to pump the MLSS at high crossflow velocity at significant energy penalty (of the order 10 kWh/m product) to reduce fouling. Because of the poor economics of the first generation MBRs, they only found applications in niche areas with special needs, such as isolated trailer parks or ski resorts.\n\nThe breakthrough for the MBR came in 1989 with Yamamoto and co-workers idea of submerging the membranes in the bioreactor. Until then, MBRs were designed with the separation device located external to the reactor (sidestream MBR) and relied on high transmembrane pressure (TMP) to maintain filtration. With the membrane directly immersed in the bioreactor, submerged MBR systems are usually preferred to sidestream configuration, especially for domestic wastewater treatment. The submerged configuration relies on coarse bubble aeration to produce mixing and limit fouling. The energy demand of the submerged system can be up to 2 orders of magnitude lower than that of the sidestream systems and submerged systems operate at a lower flux, demanding more membrane area. In submerged configurations, aeration is considered as one of the major parameters in process performance both hydraulic and biological. Aeration maintains solids in suspension, scours the membrane surface and provides oxygen to the biomass, leading to a better biodegradability and cell synthesis.\n\nThe other key steps in the recent MBR development were the acceptance of modest fluxes (25 percent or less of those in the first generation), and the idea to use two-phase bubbly flow to control fouling. The lower operating cost obtained with the submerged configuration along with the steady decrease in the membrane cost encouraged an exponential increase in MBR plant installations from the mid 90s. Since then, further improvements in the MBR design and operation have been introduced and incorporated into larger plants. While early MBRs were operated at solid retention times (SRT) as high as 100 days with MLSS up to 30 g/L, the recent trend is to apply lower solid retention times (around 10–20 days), resulting in more manageable MLSS levels (10 to 15 g/L). Thanks to these new operating conditions, the oxygen transfer and the pumping cost in the MBR have tended to decrease and overall maintenance has been simplified. There is now a range of MBR systems commercially available, most of which use submerged membranes although some external modules are available; these external systems also use two-phase flow for fouling control. Typical hydraulic retention times (HRT) range between 3 and 10 hours. In terms of membrane configurations, mainly hollow fiber and flat sheet membranes are applied for MBR applications.\nDespite the more favourable energy usage of submerged membranes, there continued to be a market for the side stream configuration, particularly in smaller flow industrial applications. For ease of maintenance the side stream configuration can be installed on a lower level in a plant building. Membrane replacement can be undertaken without specialised lifting equipment. As a result, research continued with the side stream configuration, during which time it was found that full scale plants could be operated with higher fluxes. This has culminated in recent years with the development of low energy systems which incorporate more sophisticated control of the operating parameters coupled with periodic back washes, which enable sustainable operation at energy usage as low as 0.3 kWh/m of product.\n\nThe filtration element is installed in either the main bioreactor vessel or in a separate tank. The membranes can be flat sheet or tubular or combination of both, and can incorporate an online backwash system which reduces membrane surface fouling by pumping membrane permeate back through the membrane. In systems where the membranes are in a separate tank to the bioreactor, individual trains of membranes can be isolated to undertake cleaning regimes incorporating membrane soaks, however the biomass must be continuously pumped back to the main reactor to limit MLSS concentration increase. Additional aeration is also required to provide air scour to reduce fouling. Where the membranes are installed in the main reactor, membrane modules are removed from the vessel and transferred to an offline cleaning tank.\n\nThe filtration elements are installed externally to the reactor, often in a plant room. The biomass is either pumped directly through a number of membrane modules in series and back to the bioreactor, or the biomass is pumped to a bank of modules, from which a second pump circulates the biomass through the modules in series. Cleaning and soaking of the membranes can be undertaken in place with use of an installed cleaning tank, pump and pipework.\n\nThe MBR filtration performance inevitably decreases with filtration time. This is due to the deposition of soluble and particulate materials onto and into the membrane, attributed to the interactions between activated sludge components and the membrane. This major drawback and process limitation has been under investigation since the early MBRs, and remains one of the most challenging issues facing further MBR development.\n\nIn recent reviews covering membrane applications to bioreactors, it has been shown that, as with other membrane separation processes, membrane fouling is the most serious problem affecting system performance. Fouling leads to a significant increase in hydraulic resistance, manifested as permeate flux decline or transmembrane pressure (TMP) increase when the process is operated under constant-TMP or constant-flux conditions respectively. In systems where flux is maintained by increasing TMP, the energy required to achieve filtration increases. Alternatively frequent membrane cleaning is therefore required, increasing significantly the operating costs as a result of cleaning agents and production downtime. More frequent membrane replacement is also expected.\n\nMembrane fouling results from interaction between the membrane material and the components of the activated sludge liquor, which include biological flocs formed by a large range of living or dead microorganisms along with soluble and colloidal compounds. The suspended biomass has no fixed composition and varies both with feed water composition and MBR operating conditions employed. Thus though many investigations of membrane fouling have been published, the diverse range of operating conditions and feedwater matrices employed, the different analytical methods used and the limited information reported in most studies on the suspended biomass composition, has made it difficult to establish any generic behaviour pertaining to membrane fouling in MBRs specifically.\nThe air-induced cross flow obtained in submerged MBR can efficiently remove or at least reduce the fouling layer on the membrane surface. A recent review reports the latest findings on applications of aeration in submerged membrane configuration and describes the enhancement of performances offered by gas bubbling. As an optimal air flow-rate has been identified behind which further increases in aeration have no effect on fouling removal, the choice of aeration rate is a key parameter in MBR design.\n\nMany other anti-fouling strategies can be applied to MBR applications. They comprise, for example:\n\nIn addition, different types/intensities of chemical cleaning may also be recommended:\n\nIntensive cleaning is also carried out when further filtration cannot be sustained because of an elevated transmembrane pressure (TMP). Each of the four main MBR suppliers (Kubota, Evoqua, Mitsubishi and GE Water) have their own chemical cleaning recipes, which differ mainly in terms of concentration and methods (see Table 1). Under normal conditions, the prevalent cleaning agents remain NaOCl (sodium hypochlorite) and citric acid. It is common for MBR suppliers to adapt specific protocols for chemical cleanings (i.e. chemical concentrations and cleaning frequencies) for individual facilities.\n\nSimply due to the high number of microorganism in MBRs, the pollutants uptake rate can be increased. This leads to better degradation in a given time span or to smaller required reactor volumes. In comparison to the conventional activated sludge process (ASP) which typically achieves 95 percent, COD removal can be increased to 96 to 99 percent in MBRs (see table,). COD and BOD5 removal are found to increase with MLSS concentration. Above 15 g/L COD removal becomes almost independent of biomass concentration at >96 percent. Arbitrary high MLSS concentrations are not employed, however, as oxygen transfer is impeded due to higher and non-Newtonian fluid viscosity. Kinetics may also differ due to easier substrate access. In ASP, flocs may reach several 100 μm in size. This means that the substrate can reach the active sites only by diffusion which causes an additional resistance and limits the overall reaction rate (diffusion controlled). Hydrodynamic stress in MBRs reduces floc size (to 3.5 μm in sidestream MBRs) and thereby increases the apparent reaction rate. Like in the conventional ASP, sludge yield is decreased at higher SRT or biomass concentration. Little or no sludge is produced at sludge loading rates of 0.01 kgCOD/(kgMLSS d). Because of the imposed biomass concentration limit, such low loading rates would result in enormous tank sizes or long HRTs in conventional ASP.\n\nNutrient removal is one of the main concerns in modern wastewater treatment especially in areas that are sensitive to eutrophication. Like in the conventional ASP, currently, the most widely applied technology for N-removal from municipal wastewater is nitrification combined with denitrification. Besides phosphorus precipitation, enhanced biological phosphorus removal (EBPR) can be implemented which requires an additional anaerobic process step. Some characteristics of MBR technology render EBPR in combination with post-denitrification an attractive alternative that achieves very low nutrient effluent concentrations.\n\nAnaerobic MBRs (sometimes abbreviated AnMBR) were introduced in the 1980s in South Africa and currently see a renaissance in research. However, anaerobic processes are normally used when a low cost treatment is required that enables energy recovery but does not achieve advanced treatment (low carbon removal, no nutrients removal). In contrast, membrane-based technologies enable advanced treatment (disinfection), but at high energy cost. Therefore, the combination of both can only be economically viable if a compact process for energy recovery is desired, or when disinfection is required after anaerobic treatment (cases of water reuse with nutrients). If maximal energy recovery is desired, a single anaerobic process will be always superior to a combination with a membrane process.\n\nRecently, anaerobic MBRs have seen successful full-scale application to the treatment of some types of industrial wastewaters—typically high-strength wastes. Example applications include the treatment of alcohol stillage wastewater in Japan and the treatment of salad dressing/barbecue sauce wastewater in the United States.\n\nLike in any other reactors, the hydrodynamics (or mixing) within an MBR plays an important role in determining the pollutant removal and fouling control within an MBR. It has a substantial effect on the energy usage and size requirements of an MBR, therefore the whole life cost of an MBR is high.\n\nThe removal of pollutants is greatly influenced by the length of time fluid elements spend in the MBR (i.e. the residence time distribution or RTD). The residence time distribution is a description of the hydrodynamics/mixing in the system and is determined by the design of the MBR (e.g. MBR size, inlet/recycle flowrates, wall/baffle/mixer/aerator positioning, mixing energy input). An example of the effect of mixing is that a continuous stirred-tank reactor will not have as high pollutant conversion per unit volume of reactor as a plug flow reactor.\n\nThe control of fouling, as previously mentioned, is primarily undertaken using coarse bubble aeration. The distribution of bubbles around the membranes, the shear at the membrane surface for cake removal and the size of the bubble are greatly influenced by the mixing/hydrodynamics of the system. The mixing within the system can also influence the production of possible foulants. For example, vessels not completely mixed (i.e. plug flow reactors) are more susceptible to the effects of shock loads which may cause cell lysis and release of soluble microbial products.\n\nMany factors affect the hydrodynamics of wastewater processes and hence MBRs. These range from physical properties (e.g. mixture rheology and gas/liquid/solid density etc.) to the fluid boundary conditions (e.g. inlet/outlet/recycle flowrates, baffle/mixer position etc.). However, many factors are peculiar to MBRs, these cover the filtration tank design (e.g. membrane type, multiple outlets attributed to membranes, membrane packing density, membrane orientation etc.) and its operation (e.g. membrane relaxation, membrane back flush etc.).\n\nThe mixing modelling and design techniques applied to MBRs are very similar to those used for conventional activated sludge systems. They include the relatively quick and easy compartmental modelling technique which will only derive the RTD of a process (e.g. the MBR) or the process unit (e.g. membrane filtration vessel) and relies on broad assumptions of the mixing properties of each sub-unit. Computational fluid dynamics modelling (CFD) on the other hand does not rely on broad assumptions of the mixing characteristics and attempts to predict the hydrodynamics from a fundamental level. It is applicable to all scales of fluid flow and can reveal much information about the mixing in a process, ranging from the RTD to the shear profile on a membrane surface. Visualisation of MBR CFD modelling results is shown in the image.\n\nInvestigations of MBR hydrodynamics have occurred at many different scales, ranging from examination of shear stress at the membrane surface to RTD analysis of the whole MBR. Cui et al. (2003) investigated the movement of Taylor bubbles through tubular membranes. Khosravi, M. (2007) examined the entire membrane filtration vessel using CFD and velocity measurements, while Brannock et al. (2007) examined the entire MBR using tracer study experiments and RTD analysis.\n"}
{"id": "49611", "url": "https://en.wikipedia.org/wiki?curid=49611", "title": "Menopause", "text": "Menopause\n\nMenopause, also known as the climacteric, is the time in most women's lives when menstrual periods stop permanently, and they are no longer able to bear children. Menopause typically occurs between 49 and 52 years of age. Medical professionals often define menopause as having occurred when a woman has not had any vaginal bleeding for a year. It may also be defined by a decrease in hormone production by the ovaries. In those who have had surgery to remove their uterus but still have ovaries, menopause may be viewed to have occurred at the time of the surgery or when their hormone levels fell. Following the removal of the uterus, symptoms typically occur earlier, at an average of 45 years of age.\nIn the years before menopause, a woman's periods typically become irregular, which means that periods may be longer or shorter in duration or be lighter or heavier in the amount of flow. During this time, women often experience hot flashes; these typically last from 30 seconds to ten minutes and may be associated with shivering, sweating, and reddening of the skin. Hot flashes often stop occurring after a year or two. Other symptoms may include vaginal dryness, trouble sleeping, and mood changes. The severity of symptoms varies between women. While menopause is often thought to be linked to an increase in heart disease, this primarily occurs due to increasing age and does not have a direct relationship with menopause. In some women, problems that were present like endometriosis or painful periods will improve after menopause.\nMenopause is usually a natural change. It can occur earlier in those who smoke tobacco. Other causes include surgery that removes both ovaries or some types of chemotherapy. At the physiological level, menopause happens because of a decrease in the ovaries' production of the hormones estrogen and progesterone. While typically not needed, a diagnosis of menopause can be confirmed by measuring hormone levels in the blood or urine. Menopause is the opposite of menarche, the time when a girl's periods start.\nSpecific treatment is not usually needed. Some symptoms, however, may be improved with treatment. With respect to hot flashes, avoiding smoking, caffeine, and alcohol is often recommended. Sleeping in a cool room and using a fan may help. The following medications may help: menopausal hormone therapy (MHT), clonidine, gabapentin, or selective serotonin reuptake inhibitors. Exercise may help with sleeping problems. While MHT was once routinely prescribed, it is now only recommended in those with significant symptoms, as there are concerns about side effects. High-quality evidence for the effectiveness of alternative medicine has not been found. There is tentative evidence for phytoestrogens.\n\nDuring early menopause transition, the menstrual cycles remain regular but the interval between cycles begins to lengthen. Hormone levels begin to fluctuate. Ovulation may not occur with each cycle.\n\nThe date of the final menstrual period is usually taken as the point when menopause has occurred. During the menopausal transition and after menopause, women can experience a wide range of symptoms.\n\nDuring the transition to menopause, menstrual patterns can show shorter cycling (by 2–7 days); longer cycles remain possible. There may be irregular bleeding (lighter, heavier, spotting). Dysfunctional uterine bleeding is often experienced by women approaching menopause due to the hormonal changes that accompany the menopause transition. Spotting or bleeding may simply be related to vaginal atrophy, a benign sore (polyp or lesion), or may be a functional endometrial response. The European Menopause and Andropause Society has released guidelines for assessment of the endometrium, which is usually the main source of spotting or bleeding.\n\nIn post-menopausal women, however, any genital bleeding is an alarming symptom that requires an appropriate study to rule out the possibility of malignant diseases.\n\nSymptoms that may appear during menopause and continue through postmenopause include:\n\nOther physical symptoms of menopause include lack of energy, joint soreness, stiffness, back pain, breast enlargement, breast pain,\nheart palpitations, headache, dizziness, dry, itchy skin, thinning, tingling skin, weight gain, urinary incontinence,\nurinary urgency,\ninterrupted sleeping patterns, heavy night sweats, and hot flashes.\n\nPsychological symptoms include anxiety,\npoor memory,\ninability to concentrate,\ndepressive mood,\nirritability,\nmood swings, less interest in sexual activity.\n\nMenopause confers:\nWomen who experience menopause before 45 years of age have an increased risk of heart disease, death, and impaired lung function.\n\nMenopause can be induced or occur naturally. Induced menopause occurs as a result of medical treatment such as chemotherapy, radiotherapy, oophorectomy, or complications of tubal ligation, hysterectomy, unilateral salpingo-oophorectomy or leuprorelin usage.\n\nMenopause typically occurs between 49 and 52 years of age. The majority women have their last period between the ages of 48 and 55. The average age of the last period in the United States is 51 years, in the United Kingdom is 52 years, in Ireland is 50 years and in Australia is 51 years. In India and the Philippines, the median age of natural menopause is considerably earlier, at 44 years. The menopausal transition or perimenopause leading up to menopause usually lasts 7 years (sometimes as long as 14 years).\n\nIn rare cases, a woman's ovaries stop working at a very early age, ranging anywhere from the age of puberty to age 40. This is known as premature ovarian failure and affects 1 to 2% of women by age 40.\n\nUndiagnosed and untreated coeliac disease is a risk factor for early menopause. Coeliac disease can present with several non-gastrointestinal symptoms, in the absence of gastrointestinal symptoms, and most cases escape timely recognition and go undiagnosed, leading to a risk of long-term complications. A strict gluten-free diet reduces the risk. Women with early diagnosis and treatment of coeliac disease present a normal duration of fertile life span.\n\nWomen who have undergone hysterectomy with ovary conservation go through menopause on average 3.7 years earlier than the expected age. Other factors that can promote an earlier onset of menopause (usually 1 to 3 years early) are smoking cigarettes or being extremely thin.\n\nPremature ovarian failure (POF) is the cessation of the ovarian function before the age of 40 years. It is diagnosed or confirmed by high blood levels of follicle stimulating hormone (FSH) and luteinizing hormone (LH) on at least three occasions at least four weeks apart.\n\nKnown causes of premature ovarian failure include autoimmune disorders, thyroid disease, diabetes mellitus, chemotherapy, being a carrier of the fragile X syndrome gene, and radiotherapy. However, in about 50–80% of spontaneous cases of premature ovarian failure, the cause is unknown, i.e., it is generally idiopathic.\n\nWomen who have a functional disorder affecting the reproductive system (e.g., endometriosis, polycystic ovary syndrome, cancer of the reproductive organs) can go into menopause at a younger age than the normal timeframe. The functional disorders often significantly speed up the menopausal process.\n\nAn early menopause can be related to cigarette smoking, higher body mass index, racial and ethnic factors, illnesses, and the surgical removal of the ovaries, with or without the removal of the uterus.\n\nRates of premature menopause have been found to be significantly higher in fraternal and identical twins; approximately 5% of twins reach menopause before the age of 40. The reasons for this are not completely understood. Transplants of ovarian tissue between identical twins have been successful in restoring fertility.\n\nMenopause can be surgically induced by bilateral oophorectomy (removal of ovaries), which is often, but not always, done in conjunction with removal of the Fallopian tubes (salpingo-oophorectomy) and uterus (hysterectomy). Cessation of menses as a result of removal of the ovaries is called \"surgical menopause\". Surgical treatments, such as the removal of ovaries, might cause periods to stop altogether. The sudden and complete drop in hormone levels usually produces extreme withdrawal symptoms such as hot flashes, etc. The symptoms of early menopause may be more severe.\n\nRemoval of the uterus \"without\" removal of the ovaries does \"not\" directly cause menopause, although pelvic surgery of this type can often precipitate a somewhat earlier menopause, perhaps because of a compromised blood supply to the ovaries.. The time between surgery and possible early menopause is due to the fact that ovaries are still producing hormones.\n\nThe menopausal transition, and postmenopause itself, is a natural change, not usually a disease state or a disorder. The main cause of this transition is the natural depletion and aging of the finite amount of oocytes (ovarian reserve). This process is sometimes accelerated by other conditions and is known to occur earlier after a wide range of gynecologic procedures such as hysterectomy (with and without ovariectomy), endometrial ablation and uterine artery embolisation. The depletion of the ovarian reserve causes an increase in circulating follicle-stimulating hormone (FSH) and luteinizing hormone (LH) levels because there are fewer oocytes and follicles responding to these hormones and producing estrogen.\n\nThe transition has a variable degree of effects.\n\nThe stages of the menopause transition have been classified according to a woman's reported bleeding pattern, supported by changes in the pituitary follicle-stimulating hormone (FSH) levels.\n\nIn younger women, during a normal menstrual cycle the ovaries produce estradiol, testosterone and progesterone in a cyclical pattern under the control of FSH and luteinising hormone (LH) which are both produced by the pituitary gland. During perimenopause (approaching menopause), estradiol levels and patterns of production remain relatively unchanged or may increase compared to young women, but the cycles become frequently shorter or irregular. The often observed increase in estrogen is presumed to be in response to elevated FSH levels that, in turn, is hypothesized to be caused by decreased feedback by inhibin. Similarly, decreased inhibin feedback after hysterectomy is hypothesized to contribute to increased ovarian stimulation and earlier menopause.\n\nThe menopausal transition is characterized by marked, and often dramatic, variations in FSH and estradiol levels. Because of this, measurements of these hormones are \"not\" considered to be reliable guides to a woman's exact menopausal status.\n\nMenopause occurs because of the sharp decrease of estradiol and progesterone production by the ovaries. After menopause, estrogen continues to be produced mostly by aromatase in fat tissues and is produced in small amounts in many other tissues such as ovaries, bone, blood vessels, and the brain where it acts locally. The substantial fall in circulating estradiol levels at menopause impacts many tissues, from brain to skin.\n\nIn contrast to the sudden fall in estradiol during menopause, the levels of total and free testosterone, as well as dehydroepiandrosterone sulfate (DHEAS) and androstenedione appear to decline more or less steadily with age. An effect of natural menopause on circulating androgen levels has not been observed. Thus specific tissue effects of natural menopause cannot be attributed to loss of androgenic hormone production.\n\nHot flashes and other vasomotor symptoms accompany the menopausal transition. While many sources continue to claim that hot flashes during the menopausal transition are caused by low estrogen levels, this assertion was shown incorrect in 1935 and, in most cases, hot flashes are observed despite elevated estrogen levels. The exact cause of these symptoms is not yet understood, possible factors considered are higher and erratic variation of estradiol level during the cycle, elevated FSH levels which may indicate hypothalamic dysregulation perhaps caused by missing feedback by inhibin. It has been also observed that the vasomotor symptoms differ during early perimenopause and late menopausal transition and it is possible that they are caused by a different mechanism.\n\nLong-term effects of menopause may include osteoporosis, vaginal atrophy as well as changed metabolic profile resulting in cardiac risks.\n\nDecreased inhibin feedback after hysterectomy is hypothesized to contribute to increased ovarian stimulation and earlier menopause. Hastened ovarian aging has been observed after endometrial ablation. While it is difficult to prove that these surgeries are causative, it has been hypothesized that the endometrium may be producing endocrine factors contributing to the endocrine feedback and regulation of the ovarian stimulation. Elimination of this factors contributes to faster depletion of the ovarian reserve. Reduced blood supply to the ovaries that may occur as a consequence of hysterectomy and uterine artery embolisation has been hypothesized to contribute to this effect.\n\nImpaired DNA repair mechanisms may contribute to earlier depletion of the ovarian reserve during aging. As women age, double-strand breaks accumulate in the DNA of their primordial follicles. Primordial follicles are immature primary oocytes surrounded by a single layer of granulosa cells. An enzyme system is present in oocytes that ordinarily accurately repairs DNA double-strand breaks. This repair system is called \"homologous recombinational repair\", and it is especially effective during meiosis. Meiosis is the general process by which germ cells are formed in all sexual eukaryotes; it appears to be an adaptation for efficiently removing damages in germ line DNA. (See Meiosis.)\n\nHuman primary oocytes are present at an intermediate stage of meiosis, termed prophase I (see Oogenesis). Expression of four key DNA repair genes that are necessary for homologous recombinational repair during meiosis (BRCA1, MRE11, Rad51, and ATM) decline with age in oocytes. This age-related decline in ability to repair DNA double-strand damages can account for the accumulation of these damages, that then likely contributes to the depletion of the ovarian reserve.\n\nWays of assessing the impact on women of some of these menopause effects, include the Greene climacteric scale questionnaire, the Cervantes scale and the Menopause rating scale.\n\nPremenopause is a term used to mean the years leading up to the last period, when the levels of reproductive hormones are becoming more variable and lower, and the effects of hormone withdrawal are present. Premenopause starts some time before the monthly cycles become noticeably irregular in timing.\n\nThe term \"perimenopause\", which literally means \"around the menopause\", refers to the menopause transition years before the date of the final episode of flow. According to the North American Menopause Society, this transition can last for four to eight years. The Centre for Menstrual Cycle and Ovulation Research describes it as a six- to ten-year phase ending 12 months after the last menstrual period.\n\nDuring perimenopause, estrogen levels average about 20–30% higher than during premenopause, often with wide fluctuations. These fluctuations cause many of the physical changes during perimenopause as well as menopause, especially during the last 1–2 years of perimenopause (before menopause). Some of these changes are hot flashes, night sweats, difficulty sleeping, mood swings, vaginal dryness or atrophy, incontinence, osteoporosis, and heart disease. During this period, fertility diminishes but is not considered to reach zero until the official date of menopause. The official date is determined retroactively, once 12 months have passed after the last appearance of menstrual blood.\n\nThe menopause transition typically begins between 40 and 50 years of age (average 47.5). The duration of perimenopause may be for up to eight years. Women will often, but not always, start these transitions (perimenopause and menopause) about the same time as their mother did.\n\nIn some women, menopause may bring about a sense of loss related to the end of fertility. In addition, this change often occurs when other stressors may be present in a woman's life:\n\n\nSome research appears to show that melatonin supplementation in perimenopausal women can improve thyroid function and gonadotropin levels, as well as restoring fertility and menstruation and preventing depression associated with menopause.\n\nThe term \"postmenopausal\" describes women who have not experienced any menstrual flow for a minimum of 12 months, assuming that they have a uterus and are not pregnant or lactating. In women without a uterus, menopause or postmenopause can be identified by a blood test showing a very high FSH level. Thus postmenopause is the time in a woman's life that takes place after her last period or, more accurately, after the point when her ovaries become inactive.\n\nThe reason for this delay in declaring postmenopause is because periods are usually erratic at this time of life. Therefore, a reasonably long stretch of time is necessary to be sure that the cycling has ceased. At this point a woman is considered infertile; however, the possibility of becoming pregnant has usually been very low (but not quite zero) for a number of years before this point is reached.\n\nA woman's reproductive hormone levels continue to drop and fluctuate for some time into post-menopause, so hormone withdrawal effects such as hot flashes may take several years to disappear.\n\nA period-like flow during postmenopause, even spotting, may be a sign of endometrial cancer.\n\nPerimenopause is a natural stage of life. It is not a disease or a disorder. Therefore, it does not automatically require any kind of medical treatment. However, in those cases where the physical, mental, and emotional effects of perimenopause are strong enough that they significantly disrupt the life of the woman experiencing them, palliative medical therapy may sometimes be appropriate.\n\nIn the context of the menopause, hormone replacement therapy (HRT) is the use of estrogen in women without a uterus and estrogen plus progestin in women who have an intact uterus.\n\nHRT may be reasonable for the treatment of menopausal symptoms, such as hot flashes. It is the most effective treatment option, especially when delivered as a skin patch. Its use, however, appears to increase the risk of strokes and blood clots. When used for menopausal symptoms some recommend it be used for the shortest time possible and at the lowest dose possible. Evidence to support long term use however is poor.\n\nIt also appears effective for preventing bone loss and osteoporotic fracture, but it is generally recommended only for women at significant risk for whom other therapies are unsuitable.\n\nHRT may be unsuitable for some women, including those at increased risk of cardiovascular disease, increased risk of thromboembolic disease (such as those with obesity or a history of venous thrombosis) or increased risk of some types of cancer. There is some concern that this treatment increases the risk of breast cancer.\n\nAdding testosterone to hormone therapy has a positive effect on sexual function in postmenopausal women, although it may be accompanied by hair growth, acne and a reduction in high-density lipoprotein (HDL) cholesterol. These side effects diverge depending on the doses and methods of using testosterone.\n\nSERMs are a category of drugs, either synthetically produced or derived from a botanical source, that act selectively as agonists or antagonists on the estrogen receptors throughout the body. The most commonly prescribed SERMs are raloxifene and tamoxifen. Raloxifene exhibits oestrogen agonist activity on bone and lipids, and antagonist activity on breast and the endometrium. Tamoxifen is in widespread use for treatment of hormone sensitive breast cancer. Raloxifene prevents vertebral fractures in postmenopausal, osteoporotic women and reduces the risk of invasive breast cancer.\n\nSome of the SSRIs and SNRIs appear to provide some relief. Low dose paroxetine has been FDA-approved for hot moderate-to-severe vasomotor symptoms associated with menopause. They may, however, be associated with sleeping problems.\n\nGabapentin or clonidine may help but does not work as well as hormone therapy. Clonidine may be associated with constipation and sleeping problems.\n\nThere is no evidence of consistent benefit of alternative therapies for menopausal symptoms despite their popularity. The effect of soy isoflavones on menopausal symptoms is promising for reduction of hot flashes and vaginal dryness. Evidence does not support a benefit from phytoestrogens such as coumestrol, femarelle, or the non-phytoestrogen black cohosh. There is no evidence to support the efficacy of acupuncture as a management for menopausal symptoms. As of 2011 there is no support for herbal or dietary supplements in the prevention or treatment of the mental changes that occur around menopause. A 2016 Cochrane review found not enough evidence to show a difference between Chinese herbal medicine and placebo for the vasomotor symptoms.\n\n\nThe cultural context within which a woman lives can have a significant impact on the way she experiences the menopausal transition. Menopause has been described as a subjective experience, with social and cultural factors playing a prominent role in the way menopause is experienced and perceived.\n\nWithin the United States, social location affects the way women perceive menopause and its related biological effects. Research indicates that whether a woman views menopause as a medical issue or an expected life change is correlated with her socio-economic status. The paradigm within which a woman considers menopause influences the way she views it: Women who understand menopause as a medical condition rate it significantly more negatively than those who view it as a life transition or a symbol of aging.\n\nEthnicity and geography play roles in the experience of menopause. American women of different ethnicities report significantly different types of menopausal effects. One major study found Caucasian women most likely to report what are sometimes described as psychosomatic symptoms, while African-American women were more likely to report vasomotor symptoms.\n\nIt seems that Japanese women experience menopause effects, or \"konenki\", in a different way from American women. Japanese women report lower rates of hot flashes and night sweats; this can be attributed to a variety of factors, both biological and social. Historically, konenki was associated with wealthy middle-class housewives in Japan, i.e., it was a \"luxury disease\" that women from traditional, inter-generational rural households did not report. Menopause in Japan was viewed as a symptom of the inevitable process of aging, rather than a \"revolutionary transition\", or a \"deficiency disease\" in need of management.\n\nIn Japanese culture, reporting of vasomotor symptoms has been on the increase, with research conducted by Melissa Melby in 2005 finding that of 140 Japanese participants, hot flashes were prevalent in 22.1%. This was almost double that of 20 years prior. Whilst the exact cause for this is unknown, possible contributing factors include significant dietary changes, increased medicalisation of middle-aged women and increased media attention on the subject. However, reporting of vasomotor symptoms is still significantly lower than North America.\n\nAdditionally, while most women in the United States apparently have a negative view of menopause as a time of deterioration or decline, some studies seem to indicate that women from some Asian cultures have an understanding of menopause that focuses on a sense of liberation and celebrates the freedom from the risk of pregnancy. Postmenopausal Indian women can enter Hindu temples and participate in rituals, marking it as a celebration for reaching an age of wisdom and experience.\n\nDiverging from these conclusions, one study appeared to show that many American women \"experience this time as one of liberation and self-actualization\".\n\nMenopause literally means the \"end of monthly cycles\" (the end of monthly periods or menstruation), from the Greek word \"pausis\" (\"pause\") and \"mēn\" (\"month\"). This is a medical calque; the Greek word for menses is actually different. In Ancient Greek, the menses were described in the plural, \"ta emmēnia\", (\"the monthlies\"), and its modern descendant has been clipped to \"ta emmēna\". The Modern Greek medical term is \"emmenopausis\" in Katharevousa or \"emmenopausi\" in Demotic Greek.\n\nThe word \"menopause\" was coined specifically for human females, where the end of fertility is traditionally indicated by the permanent stopping of monthly menstruations. However, menopause exists in some other animals, many of which do not have monthly menstruation; in this case, the term means a natural end to fertility that occurs before the end of the natural lifespan.\n\nVarious theories have been suggested that attempt to suggest evolutionary benefits to the human species stemming from the cessation of women's reproductive capability before the end of their natural lifespan. Explanations can be categorized as adaptive and non-adaptive:\n\nThe high cost of female investment in offspring may lead to physiological deteriorations that amplify susceptibility to becoming infertile. This hypothesis suggests the reproductive lifespan in humans has been optimized, but it has proven more difficult in females and thus their reproductive span is shorter. If this hypothesis were true, however, age at menopause should be negatively correlated with reproductive effort and the available data do not support this.\n\nA recent increase in female longevity due to improvements in the standard of living and social care has also been suggested. It is difficult for selection, however, to favour aid to offspring from parents and grandparents. Irrespective of living standards, adaptive responses are limited by physiological mechanisms. In other words, senescence is programmed and regulated by specific genes.\n\nWhile it is fairly common for extant hunter-gatherers to live past age 50 provided that they survive childhood, fossil evidence shows that mortality in adults have decreased over the last 30000 to 50000 years (possibly due to innovations relating to defense from large carnivores and/or decrease in the ratio of large carnivores to humans) and that it was extremely unusual for early Homo sapiens to live to age 50. This discovery have led some biologists to argue that there was no selection for or against menopause at the time at which the ancestor of all modern humans lived in Africa and had not split into different ethnicities, suggesting that menopause is instead a random evolutionary effect of a selection shadow regarding ageing in early Homo sapiens. It is also argued that since the population fraction of post-menopausal women in early Homo sapiens was so low, menopause had no evolutionary effect on mate selection or social behaviors related to mate selection.\n\nThis hypothesis suggests that younger mothers and offspring under their care will fare better in a difficult and predatory environment because a younger mother will be stronger and more agile in providing protection and sustenance for herself and a nursing baby. The various biological factors associated with menopause had the effect of male members of the species investing their effort with the most viable of potential female mates.\nOne problem with this hypothesis is that we would expect to see menopause exhibited in the animal kingdom and another problem is that in the case of extended child development, even a female who was relatively young and still agile and attractive (but not very young) when producing a child would lose future support from her male partner due to him seeking out fertile mates when she reaches menopause while the child is still not independent. That would be counterproductive to the supposed adaptation of getting male support as a fertile female and ruin survival for children produced over much of the female's fertile and agile life, unless children were raised in ways that did not rely on support from a male partner which would eliminate that type of resource diverting selection anyway.\n\nThe young female preference hypothesis proposes that changes in male preferences for younger mates allowed late-age acting fertility mutations to accumulate in females without any evolutionary penalty, giving rise to menopause. A computer model was constructed to test this hypothesis, and showed that it was feasible. However, in order for deleterious mutations that affect fertility past roughly age fifty to accumulate, human maximum lifespan had to first be extended to about its present value, and as of 2016 it was unclear if there has been sufficient time since that happened, for such an evolutionary process to occur.\n\nThe male-biased philopatry theory proposes that male-biased philopatry in social species leads to increased relatedness to the group in relation to female age, making inclusive fitness benefits older females receive from helping the group greater than what they would receive from continued reproduction, which eventually lead to the evolution of menopause. In a pattern of male-biased dispersal and local mating, the relatedness of the individuals in the group decreases with female age, leading to a decrease in kin selection with female age. This occurs because a female will stay with her father in her natal group throughout life, initially being closely related to the males and females. Females are born and stay in the group, so relatedness to the females stays about the same. However, throughout time, the older male relatives will die and any sons she gives birth to will disperse, so that local relatedness to males, and therefore the whole group, declines. The situation is reversed in species where males are philopatric and either females disperse, or mating is non-local. Under these conditions, a female’s reproductive life begins away from her father and paternal relatives because she was either born into a new group from non-local mating or because she dispersed. In the case of female-biased dispersal, the female is initially equally unrelated with every individual in the group, and with non-local mating, the female is closely related to the females of the group, but not the males since her paternal relatives are in another group. As she gives birth, her sons will stay with her, increasing her relatedness to males in the group overtime and thus her relatedness with the overall group. The common feature that connects these two otherwise different behaviors is male-biased philopatry, which leads to an increase in kin selection with female age.\n\nWhile not conclusive, evidence does exist to support the idea that female-biased dispersal existed in pre-modern humans. The closest living relatives to humans, chimpanzees, bonobos, and both mountain gorillas and western lowland gorillas, are female-biased dispersers. Analysis of sex specific genetic material, the non-recombining portions of the Y chromosome and mitochondrial DNA, show evidence of a prevalence of female-biased dispersal as well; however, these results could also be affected by the effective breeding numbers of males and females in local populations. Evidence of female-biased dispersion in hunter-gatherers is not definitive, with some studies supporting the idea, and others suggesting there is no strong bias towards either sex. In killer whales, both sexes mate non-locally with members of a different pod but return to the pod after copulation. Demographic data shows that a female’s mean relatedness to the group does increase over time due to increasing relatedness to males. While less well-studied, there is evidence that short-finned pilot whales, another menopausal species, also display this behavior. However, mating behavior that increases local relatedness with female age is prevalent in non-menopausal species, making it unlikely that it is the only factor that determines if menopause will evolve in a species.\n\nThe mother hypothesis suggests that menopause was selected for humans because of the extended development period of human offspring and high costs of reproduction so that mothers gain an advantage in reproductive fitness by redirecting their effort from new offspring with a low survival chance to existing children with a higher survival chance.\n\nThe grandmother hypothesis suggests that menopause was selected for humans because it promotes the survival of grandchildren. According to this hypothesis, post-reproductive women feed and care for children, adult nursing daughters, and grandchildren whose mothers have weaned them. Human babies require large and steady supplies of glucose to feed the growing brain. In infants in the first year of life, the brain consumes 60% of all calories, so both babies and their mothers require a dependable food supply. Some evidence suggests that hunters contribute less than half the total food budget of most hunter-gatherer societies, and often much less than half, so that foraging grandmothers can contribute substantially to the survival of grandchildren at times when mothers and fathers are unable to gather enough food for all of their children. In general, selection operates most powerfully during times of famine or other privation. So although grandmothers might not be necessary during good times, many grandchildren cannot survive without them during times of famine. Arguably, however, there is no firm consensus on the supposed evolutionary advantages (or simply neutrality) of menopause to the survival of the species in the evolutionary past.\n\nIndeed, analysis of historical data found that the length of a female's post-reproductive lifespan was reflected in the reproductive success of her offspring and the survival of her grandchildren. Another study found comparative effects but only in the maternal grandmother—paternal grandmothers had a detrimental effect on infant mortality (probably due to paternity uncertainty). Differing assistance strategies for maternal and paternal grandmothers have also been demonstrated. Maternal grandmothers concentrate on offspring survival, whereas paternal grandmothers increase birth rates.\n\nSome believe a problem concerning the grandmother hypothesis is that it requires a history of female philopatry, while in the present day the majority of hunter-gatherer societies are patrilocal. However, there is disagreement split along ideological lines about whether patrilineality would have existed before modern times. Some believe variations on the mother, or grandmother effect fail to explain longevity with continued spermatogenesis in males (oldest verified paternity is 94 years, 35 years beyond the oldest documented birth attributed to females). Notably, the survival time past menopause is roughly the same as the maturation time for a human child. That a mother's presence could aid in the survival of a developing child, while an unidentified father's absence might not have affected survival, could explain the paternal fertility near the end of the father's lifespan. A man with no certainty of which children are his may merely attempt to father additional children, with support of existing children present but small. Note the existence of partible paternity supporting this. Some argue that the mother and grandmother hypotheses fail to explain the detrimental effects of losing ovarian follicular activity, such as osteoporosis, osteoarthritis, Alzheimer's disease and coronary artery disease.\n\nThe theories discussed above assume that evolution directly selected for menopause. Another theory states that menopause is the byproduct of the evolutionary selection for follicular atresia, a factor that causes menopause. Menopause results from having too few ovarian follicles to produce enough estrogen to maintain the ovarian-pituitary-hypothalamic loop, which results in the cessation of menses and the beginning of menopause. Human females are born with approximately a million oocytes, and approximately 400 oocytes are lost to ovulation throughout life.\n\nIn social vertebrates, the sharing of resources among the group places limits on how many offspring can be produced and supported by members of the group. This creates a situation in which each female must compete with others of the group to ensure they are the one that reproduces. The reproductive conflict hypothesis proposes that this female reproductive conflict favors the cessation of female reproductive potential in older age to avoid reproductive conflict, increasing the older female’s fitness through inclusive benefits. Female-biased dispersal or non-local mating leads to an increase in relatedness to the social group with female age. In the human case of female-biased dispersal, when a young female enters a new group, she is not related to any individual and she reproduces to produce an offspring with a relatedness of 0.5. An older female could also choose to reproduce, producing an offspring with a relatedness of 0.5, or she could refrain from reproducing and allow another pair to reproduce. Because her relatedness to males in the group is high, there is a fair probability that the offspring will be her grandchild with a relatedness of 0.25. The younger female experiences no cost to her inclusive fitness from using the resources necessary to successfully rear offspring since she is not related to members of the group, but there is a cost for the older female. As a result, the younger female has the advantage in reproductive competition. Although a female killer whale born into a social group is related to some members of the group, the whale case of non-local mating leads to similar outcomes because the younger female relatedness to the group as a whole is less than the relatedness of the older female. This behavior makes more likely the cessation of reproduction late in life to avoid reproductive conflict with younger females.\n\nResearch using both human and killer whale demographic data has been published that supports the role of reproductive conflict in the evolution of menopause. Analysis of demographic data from pre-industrial Finnish populations found significant reductions in offspring survivorship when mothers-in-laws and daughters-in-laws had overlapping births, supporting the idea that avoiding reproductive conflict is beneficial to offspring survivorship. Humans, more so than other primates, rely on food sharing for survival, so the large survivorship reduction values could be caused by a straining of community resources. Avoiding such straining is a possible explanation for why the reproductive overlap seen in humans is much lower than other primates. Food sharing is also prevalent among another menopausal species, killer whales. Reproductive conflict has also been observed in killer whales, with increased calf mortality seen when reproductive overlap between a younger and older generational female occurred.\n\nMenopause in the animal kingdom appears to be uncommon, but the presence of this phenomenon in different species has not been thoroughly researched. Life histories show a varying degree of senescence; rapid senescing organisms (e.g., Pacific salmon and annual plants) do not have a post-reproductive life-stage. Gradual senescence is exhibited by all placental mammalian life histories.\n\nMenopause has been observed in several species of nonhuman primates, including rhesus monkeys and chimpanzees. Menopause also has been reported in a variety of other vertebrate species including elephants, short-finned pilot whales, killer whales, narwhals, beluga whales, the guppy, the platyfish, the budgerigar, the laboratory rat and mouse, and the opossum. However, with the exception of the short-finned pilot whale, killer whale, narwhals, and beluga whales, such examples tend to be from captive individuals, and thus they are not necessarily representative of what happens in natural populations in the wild.\n\nDogs do not experience menopause; the canine estrus cycle simply becomes irregular and infrequent. Although older female dogs are not considered good candidates for breeding, offspring have been produced by older animals. Similar observations have been made in cats.\n\n\n"}
{"id": "18247233", "url": "https://en.wikipedia.org/wiki?curid=18247233", "title": "Million Women Study", "text": "Million Women Study\n\nThe Million Women Study is a study of women’s health analysing data from more than one million women aged 50 and over, led by Dame Valerie Beral and a team of researchers at the Cancer Epidemiology Unit, University of Oxford. It is a collaborative project between Cancer Research UK and the National Health Service (NHS), with additional funding from the Medical Research Council (UK).\n\nOne key focus of the study relates to the effects of hormone replacement therapy use on women's health. The study has confirmed the findings in the Women's Health Initiative (WHI) that women currently using HRT are more likely to develop breast cancer than those who are not using HRT.\n\nResults from the Million Women Study, together with those of the WHI trial from the USA, have influenced national policy, including recent recommendations on the prescribing and use of hormone replacement therapy from the Royal College of Obstetricians and Gynaecologists and from the Commission on Human Medicines.\n\nThe Million Women Study is a multi-centre, population-based prospective cohort study of women aged 50 and over invited to routine breast cancer screening in the UK. Between 1996 and 2001, women were invited to join the Million Women Study when they received their invitation to attend breast screening at one of 66 participating NHS Breast Screening Centres in the UK. At these centres, women received a study questionnaire with their invitation, which they were asked to complete and return at the time of screening. Around 70% of those attending the programme returned questionnaires and agreed to take part in the study, over 1 in 4 women in the UK in the target age group. The Million Women Study is the largest study of its kind in the world.\n\nThe Million Women Study was set up with the aim of recruiting 1,000,000 women in the UK into a cohort study, to provide answers to the following questions: \n\nInitial analysis of the results from over 1 million women in the Million Women Study appeared to confirm preliminary findings from other studies at the turn of the century finding that women currently using progestin-estrogen HRT were more likely to develop breast cancer than those who are not using HRT. This initial analysis received extensive press coverage. The initial reading of results showed that this effect is substantially greater for combined (estrogen-progestogen) HRT than for estrogen-only HRT; and that the effects were similar for all specific types and doses of estrogen and progestogen, for oral, transdermal and implanted HRT, and for continuous and sequential patterns of use. Current users of estrogen-progestogen HRT were at 2 fold increased risk of developing breast cancer, and current users of estrogen-only HRT appeared to have a 1.3 fold risk. Use of HRT by women aged 50–64 in the UK in the decade from 1993-2003 was estimated to have caused 20,000 extra breast cancers. Past users were not seen as having increased risk.\n\nUpon reanalysis by the authors, the initial conclusion that HRT increased the risk of breast cancer was retracted in a 2012 paper in the \"Journal of Family Planning and Reproductive Health Care\". The paper's authors, led by Samuel Shapiro of the University of Cape Town, revealed that the study had not in fact establish a causal relationship between HRT and breast cancer, and admitted that their original analysis had been flawed.\n\nIt is well known that post-menopausal women who have not had a hysterectomy are at increased risk of cancer of the endometrium (the lining of the womb) if they take estrogen-only HRT. Follow up of over 700 000 women in the Million Women Study confirmed this and showed that the risk of endometrial cancer is also increased in women who take tibolone; but is not altered, or may even be reduced, in women taking combined estrogen-progestogen HRT. These effects depend also on a woman’s body mass index (BMI, a measure of obesity) such that adverse effects of tibolone and estrogen-only HRT are greatest in thinner women, and the beneficial effects of combined HRT are greatest in fatter women.\n\nResults of the study show a small increase in risk of ovarian cancer in women taking HRT. Such an increased risk had been suspected from some previous studies, and has now been confirmed with the larger numbers available in this study. The findings come from analyses on 948,576 post-menopausal women in the study, followed up for about 5 years. Women currently taking HRT were at higher risk of developing and of dying from ovarian cancer than women not using HRT. Past users were not at increased risk. The risk in current users was increased about 1.2 fold; for every 1000 women using HRT, 2.6 developed ovarian cancer over 5 years, compared with 2.2 in those not taking HRT. The risk was the same for estrogen-only, combined estrogen-progestogen and other types of HRT (including tibolone) and did not vary by specific type of estrogen or progestogen, or between oral and transdermal (patch) administration.\n\nThese results are equivalent to one extra case of ovarian cancer for every 2500 women taking HRT, and one extra death from ovarian cancer per 3300 women taking HRT, over 5 years. Publishers of these studies say that the results need to be looked at in the context of the other risks and benefits of HRT. In particular, an estimation of the overall effect of HRT use on three common cancers in women: breast cancer, endometrial (womb) cancer and ovarian cancer. Together, these cancers account for about 4 in 10 cancers in women in the UK. According to the findings, in women aged 50–69, about 19 of these cancers will develop over 5 years in every 1000 women not taking HRT. In women taking HRT the estimate is for the number of cancers to be increased to about 31. The overall increased risk is higher in women using combined estrogen-progestogen HRT than in women using estrogen-only HRT because most of the overall increase is due to an increase in breast cancer, and users of combined HRT have a higher risk of breast cancer than users of estrogen-only HRT.\n\nThe study has also found that low to moderate alcohol consumption increases the risk of a variety of types of cancer in women, including breast cancer.\n\nResults from the Million Women Study, together with those from other studies such as the Women’s Health Initiative trial from the USA, have influenced national policy, including recent recommendations on the prescribing and use of hormone replacement therapy from the Royal College of Obstetricians and Gynaecologists and from the Commission on Human Medicines.\n\nPublic awareness of the study and its findings has led to significant behavioural changes, predominately resulting in the swift decline of HRT prescriptions throughout Europe and the US from 2003. In contrast to the increase in HRT prescriptions between 1991 and 1996, which remained stable through to 2001, sales of HRT fell by 50% between 2002 and 2005 following the publication of the Million Women Study and the Women's Health Initiative study.\n\nA number of recent studies have shown that the Million Women Study continues to impact women’s health and behavioural patterns in Europe. Research examining breast cancer incidence trends in Sweden between 1997 and 2007, showed that the prevalence of HRT use in women aged 50–59 years decreased from a peak of 36% in 1999 to 9% in 2007, a parallel decrease in incidence of breast cancer was also reported between 2003 and 2007. A recent report assessing breast cancer incidence in Belgium between 2007 and 2008 also showed a significant drop in breast cancer incidence attributed to the decrease in HRT use in Belgium during and leading up 2008.\n\n"}
{"id": "48982395", "url": "https://en.wikipedia.org/wiki?curid=48982395", "title": "NHS Employers", "text": "NHS Employers\n\nNHS Employers is an organisation which acts on behalf of NHS trusts in the National Health Service in England and Wales. It was formed in 2004, is part of the NHS Confederation, and negotiates contracts with healthcare staff on behalf of the government.\n\nIn January 2004 the Department of Health announced the responsibility for negotiating staff terms and conditions was to be devolved by them to the NHS Confederation. In November 2004 NHS Employers was formed, and became the body that negotiates healthcare staff contracts on behalf of the government. They regularly collect and analyse the views of employers.\n\nIn September 2014, Danny Mortimer was named Chief Executive of NHS Employers, succeeding Dean Royles.\n\nIn 2005, most NHS trusts estimated that around half of their staff were suffering from workplace stress, but less than a third of health service managers that responded were able to say that their trusts had a stress management policy at the time.\n\nIn 2012 they launched a \"Speaking Up Charter\", asking NHS trusts to commit to actions that would help staff to raise concerns.\n\nIn June 2014 they published statistics highlighting the proportion of women in various sections of the NHS workforce.\n"}
{"id": "45679400", "url": "https://en.wikipedia.org/wiki?curid=45679400", "title": "NanoHealth", "text": "NanoHealth\n\nNanoHealth is a social enterprise that focuses on managing chronic diseases such as diabetes, asthma, and hypertension in Indian urban slums and low income communities. It was the 2014 recipient of the Hult Prize.\n\nNanoHealth was founded in 2014 by a group of five alumni from the Indian School of Business in Hyderabad— Dr Ashish Bondia, Manish Ranjan, Ramanathan Lakshmanan, Aditi Vaish, and Pranav Kumar Maranganty. Its primary focus is the creation of a network of local community health workers called \"Saathis\". These workers, who are trained and certified by the company, use the \"Doc-in-a-Bag\", a low-cost diagnostic tool for chronic disease management. In 2014 the company was awarded the Hult Prize, the first ever Indian team to receive the award. Two years after the founding, Pagitipati family bought over most of the non-active co-founders. Manish Ranjan continues to be the leading face of the organization and steered the company towards other business lines. Currently, NanoHealth's solution offering includes\n\n\n\n"}
{"id": "31675695", "url": "https://en.wikipedia.org/wiki?curid=31675695", "title": "Norman B. Anderson", "text": "Norman B. Anderson\n\nNorman Bruce Anderson (born October 16, 1955) is an American scientist who was a tenured professor studying health disparities and mind/body health, and later an executive in government, non-profit, university sectors. \n\nAnderson is Assistant Vice President for Research and Academic Affairs, and Research Professor of Social Work and Nursing at Florida State University. He has served as Chief Executive Officer of the American Psychological Association (APA), the largest scientific and professional association for psychologists in the United States. Anderson became the APA's first African-American CEO when he was named to the post in 2003. He was the editor for the APA journal \"American Psychologist\". Prior to joining APA Anderson was an Associate Director of the National Institutes of Health (NIH), and held other roles in academia.\n\nAnderson was born October 16, 1955, in Greensboro, North Carolina, to Rev. Dr. Charles W. and Rev. Dr. Lois J. Anderson. A graduate of the North Carolina Central University in Durham, N.C., Anderson earned master's and doctoral degrees in clinical psychology from the University of North Carolina at Greensboro. He received additional clinical and research training at the schools of medicine at Brown and Duke Universities, including postdoctoral fellowships in psychophysiology and aging at Duke. Anderson also received training in Mindfulness Facilitation from the UCLA Mindful Awareness Research Center at the Semel Institute for Neuroscience and Human Behavior, University of California at Los Angeles. In addition, Dr. Anderson is trained as a Certified Executive and Professional Coach through the College of Executive Coaching.\n\nAnderson was the founding Associate Director of the National Institutes of Health (NIH) in charge of social and behavioral science, and was the first Director of the NIH Office of Behavioral and Social Sciences Research (OBSSR). At NIH, he facilitated behavioral and social sciences research across all of the Institutes and Centers of the NIH. Research in the behavioral and social research was under his purview in areas such as cancer, heart disease, diabetes, children's health, mental health, minority health, aging, and oral health. His special focus at NIH was in sociocultural determinants of health, and in advancing an integrated, trans-disciplinary, bio-psycho-social approach to health science, health promotion, prevention, and health care.\n\nIn addition to his formal leadership roles, Anderson served as a tenured associate professor of medical psychology and of psychology at Duke University and as a professor of health and social behavior at the Harvard School of Public Health. \n\nIn 2012, he was elected to the Institute of Medicine (now the National Academy of Medicine), which is part of the National Academy of Sciences.\n\nIn addition to publishing dozens of scientific articles, Anderson is the author or editor of several books. He served as editor-in-chief of the two-volume \"Encyclopedia of Health and Behavior\" (2003) and as co-editor of \"Interdisciplinary research: Case studies from health and social science\" (2008). For over 12 years he was editor-in-chief of APA's flagship journal, \"American Psychologist\".\n\nWith his wife, P. Elizabeth Anderson, he wrote a health book for the general public, \"Emotional Longevity: What Really Determines How Long You Live,\" released in 2003.\n\nAnderson retired from APA in July 14, 2015, following an APA authorized independent review report relating to ethics guidelines conducted by former assistant Assistant U.S. Attorney David H. Hoffman. Prior to the report's release, Anderson had informed the board that he would be retiring at the end of 2016.\n\n\n\n"}
{"id": "1965360", "url": "https://en.wikipedia.org/wiki?curid=1965360", "title": "Obstructed labour", "text": "Obstructed labour\n\nObstructed labour, also known as labour dystocia, is when, even though the uterus is contracting normally, the baby does not exit the pelvis during childbirth due to being physically blocked. Complications for the baby include not getting enough oxygen which may result in death. It increases the risk of the mother getting an infection, having uterine rupture, or having post-partum bleeding. Long term complications for the mother include obstetrical fistula. Obstructed labour is said to result in prolonged labour, when the active phase of labour is longer than twelve hours.\nThe main causes of obstructed labour include: a large or abnormally positioned baby, a small pelvis, and problems with the birth canal. Abnormal positioning includes shoulder dystocia where the anterior shoulder does not pass easily below the pubic bone. Risk factors for a small pelvis include malnutrition and a lack of exposure to sunlight causing vitamin D deficiency. It is also more common in adolescence as the pelvis may not have finished growing. Problems with the birth canal include a narrow vagina and perineum which may be due to female genital mutilation or tumors. A partograph is often used to track labour progression and diagnose problems. This combined with physical examination may identify obstructed labour.\nThe treatment of obstructed labour may require cesarean section or vacuum extraction with possible surgical opening of the symphysis pubis. Other measures include: keeping the women hydrated and antibiotics if the membranes have been ruptured for more than 18 hours. In Africa and Asia obstructed labor affects between two and five percent of deliveries. In 2015 about 6.5 million cases of obstructed labour or uterine rupture occurred. This resulted in 23,000 maternal deaths down from 29,000 deaths in 1990 (about 8% of all deaths related to pregnancy). It is also one of the leading causes of stillbirth. Most deaths due to this condition occur in the developing world.\n\nThe main causes of obstructed labour include: a large or abnormally positioned baby, a small pelvis, and problems with the birth canal. Both the size and the position of the fetus can lead to obstructed labor. Abnormal positioning includes shoulder dystocia where the anterior shoulder does not pass easily below the pubic bone. A small pelvis of the mother can be a result of many factors. Risk factors for a small pelvis include malnutrition and a lack of exposure to sunlight causing vitamin D deficiency. A deficiency in calcium can also result in a small pelvis as the structures of the pelvic bones will be weak due to the lack of calcium. A relationship between maternal height and pelvis size is present and can be used to predict the possibility of obstructed labor. This relationship is a result of the mother's nutritional health throughout her life leading up to childbirth.Younger mothers are also at more risk for obstructed labor due to growth of the pelvis not being completed. Problems with the birth canal include a narrow vagina and perineum which may be due to female genital mutilation or tumors. All of these mechanical factors lead to a failure to progress in labor.\n\nObstructed labor is unique to humans compared to other primates. The evolution of humans to become obligate bipedal and increase in brain size create the problems associated with obstructed labor. In order for bipedal locomotion to be possible, many changes had to occur to the skeletal structure of humans, especially in the pelvis. Both the shape and orientation of the pelvis changed. Other primates have straighter and wider pelvises compared to humans. A narrow pelvis is better for bipedal locomotion but makes childbirth more difficult. The pelvis is sexually dimorphic, with females having a wider pelvis to be better suited for childbirth. However, the female pelvis still must accommodate for bipedal locomotion which is what creates the challenges for obstructed labor. The brain size of humans have also increased as the species has evolved. This creates a larger head of the fetus that must exit the womb. This requires human infant to be born less developed when compared to other species. The bones of the skull are not fused when a human infant is born in order to prevent the head from becoming too large to exit the womb. The head of the fetus is still large and poses the possibility for obstructed labor.\n\nObstructed labour is usually diagnosed based on physical examination. Ultrasound can be used to predict malpresentation of the fetus. In examination of the cervix once labor has begun, all examinations are compared to regular cervical assessments. The comparison between the average cervical assessment and the current state of the mother allows for a diagnosis of obstructed labor. An increasingly long time in labor also indicates a mechanical issue that is preventing the fetus from exiting the womb.\n\nAccess to proper health services can reduce the prevalence of obstructed labor. Less developed areas have inadequate health services to attend to obstructed labor, resulting in a higher prevalence among less developed area. Improving nutrition of female, both before and during pregnancy, is important for reducing the risk of obstructive labor. Creating education programs about reproduction and increasing access to reproductive services such as contraception and family planning in developing areas can also reduce the prevalence of obstructed labor.\n\nBefore considering surgical options, changing the posture of the mother during labor can help to progress labor. The treatment of obstructed labour may require cesarean section or vacuum extraction with possible surgical opening of the symphysis pubis. Caesarean section is an invasive method but is often the only method that will save the lives of both the mother and the infant. Symphysiotomy is the surgical opening of the symphysis pubis. This procedure can be completed more rapidly than Caesarean sections and does not require anesthesia, making it a more accessible option in places with less advanced medical technology. This procedure also leaves no scars on the uterus which makes further pregnancies and births safer for the mother. Another important factor in treating obstructed labor is monitoring the energy and hydration of the mother. Contractions of the uterus require energy, so the longer the mother is in labor the more energy she expends. When the mother is depleted of energy, the contractions become weaker and labor will become increasingly longer. Antibiotics are also an important treatment as infection is a possible result of obstructed labor.\n\nIf cesarean section is obtained in a timely manner, prognosis is good. Prolonged obstructed labour can lead to stillbirth, obstetric fistula, and maternal death. Fetal death can be caused by asphyxia. Obstructed labor is the leading cause of uterine rupture worldwide. Maternal death can result from uterine rupture, complications during caesarean section, or sepsis.\n\nIn 2013 it resulted in 19,000 maternal deaths down from 29,000 deaths in 1990. Globally, obstructed labor accounts for 8% of maternal deaths.\n\nThe word dystocia means difficult labour. Its antonym is eutocia () or easy labour.\n\nOther terms for obstructed labour include: difficult labour, abnormal labour, difficult childbirth, abnormal childbirth, and dysfunctional labour.\n\nThe term can also be used in the context of various animals. Dystocia pertaining to birds and reptiles is also called egg binding.\n\nIn part due to extensive selective breeding, miniature horse mares experience dystocias more frequently than other breeds.\n"}
{"id": "432986", "url": "https://en.wikipedia.org/wiki?curid=432986", "title": "Physical fitness", "text": "Physical fitness\n\nPhysical fitness is a state of health and well-being and, more specifically, the ability to perform aspects of sports, occupations and daily activities. Physical fitness is generally achieved through proper nutrition, moderate-vigorous physical exercise, and sufficient rest.\n\nBefore the industrial revolution, \"fitness\" was defined as the capacity to carry out the day’s activities without undue fatigue. However, with automation and changes in lifestyles \"physical fitness\" is now considered a measure of the body's ability to function efficiently and effectively in work and leisure activities, to be healthy, to resist hypokinetic diseases, and to meet emergency situations.\n\nFitness is defined as the quality or state of being fit. Around 1950, perhaps consistent with the Industrial Revolution and the treatise of World War II, the term \"fitness\" increased in western vernacular by a factor of ten. The modern definition of fitness describes either a person or machine's ability to perform a specific function or a holistic definition of human adaptability to cope with various situations. This has led to an interrelation of human fitness and attractiveness that has mobilized global fitness and fitness equipment industries. Regarding specific function, fitness is attributed to persons who possess significant aerobic or anaerobic ability, i.e. endurance or strength. A well-rounded fitness program improves a person in all aspects of fitness compared to practicing only one, such as only cardio/respiratory endurance or only weight training.\n\nA comprehensive fitness program tailored to an individual typically focuses on one or more specific skills, and on age- or health-related needs such as bone health. Many sources also cite mental, social and emotional health as an important part of overall fitness. This is often presented in textbooks as a triangle made up of three points, which represent physical, emotional, and mental fitness. Physical fitness can also prevent or treat many chronic health conditions brought on by unhealthy lifestyle or aging. Working out can also help some people sleep better and possibly alleviate some mood disorders in certain individuals.\n\nDeveloping research has demonstrated that many of the benefits of exercise are mediated through the role of skeletal muscle as an endocrine organ. That is, contracting muscles release multiple substances known as myokines, which promote the growth of new tissue, tissue repair, and various anti-inflammatory functions, which in turn reduce the risk of developing various inflammatory diseases.\n\nThe Physical Activity Guidelines for Americans was created by the Office of Disease Prevention and Health Promotion. This publication suggests that all adults should avoid inactivity to promote good health mentally and physically. For substantial health benefits, adults should participate in at least 150 minutes a week of moderate-intensity, or 75 minutes a week of vigorous-intensity aerobic physical activity, or an equivalent combination of moderate- and vigorous-intensity aerobic activity. Aerobic activity should be performed in episodes of at least 10 minutes, and preferably, it should be spread throughout the week.\n\nFor additional and more extensive health benefits, adults should increase their aerobic physical activity to 300 minutes (5 hours) a week of moderate-intensity, or 150 minutes a week of vigorous-intensity aerobic physical activity, or an equivalent combination of moderate- and vigorous-intensity activity. Additional health benefits are gained by engaging in physical activity beyond this amount. Adults should also do muscle-strengthening activities that are moderate or high intensity and involve all major muscle groups on 2 or more days a week, as these activities provide additional health benefits.\n\nCardiorespiratory fitness can be measured using VO2 max, a measure of the amount of oxygen the body can uptake and utilize. Aerobic exercise, which improves cardiorespiratory fitness, involves movement that increases the heart rate to improve the body's oxygen consumption. This form of exercise is an important part of all training regiments ranging from professional athletes to the everyday person. Also, it helps increase stamina.\nExamples are:\n\n\nSpecific or task-oriented fitness is a person's ability to perform in a specific activity with a reasonable efficiency: for example, sports or military service. Specific training prepares athletes to perform well in their sport.\n\nExamples are:\n\n\nFor physical fitness activity to benefit an individual, the exertion triggers a response called a stimulus. Exercise with the correct amount of intensity, duration, and frequency can produce a significant amount of improvement. The person may overall feel better, but the physical effects on the human body take weeks or months to notice and possibly years for full development. For training purposes, exercise must provide a stress or demand on either a function or tissue. To continue improvements, this demand must eventually increase little over an extended period of time. This sort of exercise training has three basic principles: overload, specificity, and progression. These principles are related to health but also enhancement of physical working capacity.\n\nHigh intensity interval training (HIIT) consists of repeated, short bursts of exercise, completed at a high level of intensity. These sets of intense activity are followed by a predetermined time of rest or low intensity activity. Studies have shown that exercising at a higher intensity has increased cardiac benefits for humans, compared to when exercising at a low or moderate level. When your workout consists of an HIIT session, your body has to work harder to replace the oxygen it lost. Research into the benefits of HIIT have revealed that it can be very successful for reducing fat, especially around the abdominal region. Furthermore, when compared to continuous moderate exercise, HIIT proves to burn more calories and increase the amount of fat burned post- HIIT session. Lack of time is one of the main reasons stated for not exercising; HIIT is a great alternative for those people because the duration of an HIIT session can be as short as 10 minutes, making it much quicker than conventional workouts.\n\nPhysical fitness has proven to result in positive effects on the body's blood pressure because staying active and exercising regularly builds up a stronger heart. The heart is the main organ in charge of systolic blood pressure and diastolic blood pressure. Engaging in a physical activity raises blood pressure. Once the subject stops the activity, the blood pressure returns to normal. The more physical activity that one engages in, the easier this process becomes, resulting in a more ‘fit’ individual. Through regular physical fitness, the heart does not have to work as hard to create a rise in blood pressure, which lowers the force on the arteries, and lowers the overall blood pressure.\n\nCenters for disease control and prevention provide lifestyle guidelines of maintaining a balanced diet and engaging in physical activity to reduce the risk of disease. The WCRF/ American Institute for Cancer Research (AICR) published a list of recommendations that reflect the evidence they have found through consistency in fitness and dietary factors that directly relate to cancer prevention.\n\nThe WCRF/AICR recommendations include the following: \n\nThese recommendations are also widely supported by the American Cancer Society. The guidelines have been evaluated and individuals that have higher guideline adherence scores substantially reduce cancer risk as well as help towards control with a multitude of chronic health problems. Regular physical activity is a factor that helps reduce an individual’s blood pressure and improves cholesterol levels, two key components that correlate with heart disease and Type 2 Diabetes. The American Cancer Society encourages the public to \"adopt a physically active lifestyle\" by meeting the criteria in a variety of physical activities such as hiking, swimming, circuit training, resistance training, lifting, etc. It is understood that cancer is not a disease that can be cured by physical fitness alone, however, because it is a multifactorial disease, physical fitness is a controllable prevention. The large associations tied with being physically fit and reduced cancer risk are enough to provide a strategy to reduce cancer risk.\nThe American Cancer Society asserts different levels of activity ranging from moderate to vigorous to clarify the recommended time spent on a physical activity. These classifications of physical activity consider the intentional exercise and basic activities are done on a daily basis and give the public a greater understanding of what fitness levels suffice as future disease prevention.\n\nStudies have shown an association between increased physical activity and reduced inflammation. It produces both a short-term inflammatory response and a long-term anti-inflammatory effect. Physical activity reduces inflammation in conjunction with or independent of changes in body weight. However, the mechanisms linking physical activity to inflammation are unknown.\n\nPhysical activity boosts the immune system. This is dependent on the concentration of endogenous factors (such as sex hormones, metabolic hormones and growth hormones), body temperature, blood flow, hydration status and body position. Physical activity has shown to increase the levels of natural killer (NK) cells, NK T cells, macrophages, neutrophils and eosinophils, complements, cytokines, antibodies and T cytotoxic cells. However, the mechanism linking physical activity to immune system is not fully understood.\n\nAchieving resilience through physical fitness promotes a vast and complex range of health-related benefits. Individuals who keep up physical fitness levels generally regulate their distribution of body fat and stay away from obesity. Abdominal fat, specifically visceral fat, is most directly affected by engaging in aerobic exercise. Strength training has been known to increase the amount of muscle in the body, however, it can also reduce body fat. Sex steroid hormones, insulin, and an appropriate immune response are factors that mediate metabolism in relation to the abdominal fat. Therefore, physical fitness provides weight control through regulation of these bodily functions.\n\nMenopause is often said to have occurred when a woman has had no vaginal bleeding for over a year since her last menstrual cycle. There are a number of symptoms connected to menopause, most of which can affect the quality of life of a woman involved in this stage of her life. One way to reduce the severity of the symptoms is to exercise and keep a healthy level of fitness. Prior to and during menopause, as the female body changes, there can be physical, physiological or internal changes to the body. These changes can be reduced or even prevented with regular exercise. These changes include:\n\n\nThe Melbourne Women's Midlife Health Project provided evidence that showed over an eight-year time period 438 were followed. Even though the physical activity was not associated with VMS in this cohort at the beginning. Women who reported they were physically active every day at the beginning were 49% less likely to have reported bothersome hot flushes. This is in contrast to women whose level of activity decreased and were more likely to experience bothersome hot flushes.\n\nStudies have shown that physical activity can improve mental health and well-being.This improvement is due to an increase in blood flow to the brain and the release of hormones. Being physically fit and working out on a consistent and constant basis can positively impact one's mental health and bring about several other benefits, such as the following.\n\n\nTo achieve some of these benefits, the Centers for Disease Control and Prevention suggests at least 30-60 minutes of exercise 3-5 times a week.\n\n\n"}
{"id": "147494", "url": "https://en.wikipedia.org/wiki?curid=147494", "title": "Pre-ejaculate", "text": "Pre-ejaculate\n\nPre-ejaculate (also known as pre-ejaculatory fluid, pre-seminal fluid or Cowper's fluid, and colloquially as \"pre-cum\") is a clear, colorless, viscous fluid that is emitted from the urethra of the penis during sexual arousal. It is similar in composition to semen but has distinct chemical differences. The presence of sperm in the fluid is variable from low to absent. Pre-ejaculate functions as a lubricant and an acid neutralizer.\n\nThe fluid is discharged from the urethra of the penis during arousal, masturbation, foreplay or at an early stage during sexual intercourse, some time before the individual fully reaches orgasm and semen is ejaculated. It is primarily produced by the bulbourethral glands (Cowper's glands), with the glands of Littre (the mucus-secreting urethral glands) also contributing. The amount of fluid that is issued varies widely among individuals. Some individuals do not produce any pre-ejaculate fluid, while others emit as much as .\n\nPre-ejaculate contains some chemicals associated with semen, such as acid phosphatase. However, other semen markers, such as gamma-glutamyltransferase, are completely absent from pre-ejaculate fluid.\n\nPre-ejaculate neutralizes residual acidity in the urethra caused by urine, creating a more favorable environment for the passage of sperm. The vagina is normally acidic, so the deposit of pre-ejaculate before the emission of semen may change the vaginal environment to promote sperm survival. Pre-ejaculate also acts as a lubricant during sexual activity, and plays a role in semen coagulation.\n\nLow levels or no sperm exists in pre-ejaculate, although studies examined small samples of men. Two contrary studies found mixed evidence, including individual cases of a high sperm concentration. Popular belief – dating to a 1966 Masters and Johnson study – stated that pre-ejaculate may contain sperm that can cause pregnancy, which is a common basis of argument against the use of coitus interruptus (withdrawal) as a contraceptive method. However, pre-ejaculate is ineffectual at causing pregnancy.\n\nStudies have demonstrated the presence of HIV in most pre-ejaculate samples from infected men.\n\nIn rare cases, an individual may produce an excessive amount of pre-ejaculate fluid that can be a cause of embarrassment or irritation. A few case reports have indicated satisfactory results when such individuals are treated with a 5-alpha-reductase inhibitor, such as finasteride.\n\n"}
{"id": "49988299", "url": "https://en.wikipedia.org/wiki?curid=49988299", "title": "Prem Nath Wahi", "text": "Prem Nath Wahi\n\nPrem Nath Wahi (1908–1991) was an Indian pathologist, writer, medical academic and the director general of the Indian Council of Medical Research. He was a fellow of the Royal College of Physicians of London, a founder fellow of the National Academy of Medical Sciences and a recipient of Dr. B. C. Roy Award and the Padma Bhushan.\n\nBorn on 10 April 1908 at Moradabad in Uttar Pradesh, Wahi did his schooling at the local schools and secured his graduate degree in medicine from King George Medical College in 1932. His advanced studies in pathology was at Sarojini Naidu Medical College where he started his career as a member of faculty in 1935 and rose to position of the professor and the head of the department of pathology in 1941. In 1960, he became the principal of the college and worked there till his superannuation in 1968, to be appointed as the vice chancellor of Agra University. A year later, he took up the position of the director general of the Indian Council of Medical Research.\n\nAfter retiring from ICMR in 1974, he worked at Sir Ganga Ram Hospital as a consultant pathologist till 1984. He wrote over 300 medical articles and books, including \"Histological Typing of Oral and Oropharyngeal Tumours\", a book written for the World Health Organization, and has mentored several research scholars in their doctoral studies. The Government of India awarded him the third highest civilian honour of the Padma Bhushan, in 1970, for his contributions to medicine. He received the Platinum Jubilee Award of the Indian Medical Association in 1974 and a decade later, the Medical Council of India honoured him with Dr. B. C. Roy Award, the highest Indian medical award, in 1984. He was a fellow of the Royal College of Physicians of London and a founder fellow of the National Academy of Medical Sciences.\n\nWahi, who was married to Krishan Kumari, died on 8 July 1991, survived by their son and three daughters. The Indian Council of Medical Research has since instituted an annual award, \"Dr. Prem Nath Wahi Award\", to recognize excellence in the fields of cytology and preventive oncology, in honour of its former director general. The Indian Academy of Cytologists conducts an annual oration under the name, \"Dr P. N. Wahi Academy Oration\".\n"}
{"id": "17968317", "url": "https://en.wikipedia.org/wiki?curid=17968317", "title": "Presentation (obstetrics)", "text": "Presentation (obstetrics)\n\nIn obstetrics, the presentation of a fetus about to be born refers to which anatomical part of the fetus is leading, that is, is closest to the pelvic inlet of the birth canal. According to the leading part, this is identified as a cephalic, breech, or shoulder presentation. A malpresentation is any presentation other than a vertex presentation (with the top of the head first).\n\nThus the various presentations are:\n\n\n\n\n\n"}
{"id": "40363468", "url": "https://en.wikipedia.org/wiki?curid=40363468", "title": "Rooming-in", "text": "Rooming-in\n\nRooming-in is the practice followed in hospitals and nursing homes where the baby's crib is kept by the side of the mother's bed. This arrangement gives an opportunity for the parents to know their baby. The bond between the parent and the child is well established in roomed-in babies. There is a better chance of success with breast-feeding in roomed-in babies. Parents do not have the fear of baby-switching while roomed-in. Rooming-in is one of the components of baby-friendly hospitals, devised by WHO.\n"}
{"id": "2871075", "url": "https://en.wikipedia.org/wiki?curid=2871075", "title": "Rosemary Sage", "text": "Rosemary Sage\n\nProfessor Rosemary Sage is a British academic who specialises in the field of education, with particular focus on communication and special educational needs. Since 2008, she is a professor of education at the College of Teachers and was dean of academic affairs until 2014. She previously worked at Liverpool Hope University, where she was made a professor of communication in education in 2007, and at the University of Leicester, where she was a senior lecturer. She has also been a visiting professor at Nara Women's University in Japan and at the University of Havana in Cuba. Sage has been an outside examiner at several universities and has given key speeches at international conferences across the world. She has published 15 books and over 200 articles in international academic journals. Her communication opportunity group strategy has earned her national and international awards. \n\nSage is a qualified speech and language pathologist, neuro-psychologist, psycholinguist and teacher (English and maths). She has worked in health, education and social services and been a director of health services in Leicester/Leicestershire. She has also been a senior language advisor in LEAs. Her research has been in the area of communication, language, education and employment. Her communicative model of teaching has been translated into Japanese, French and German and has been taught in Japan, Cuba and Latin America, the Czech Republic, Poland, Finland, Latvia and Bulgaria. It has been reviewed by academics as one of the best researched models of teaching and learning with many attesting to the personal and academic development gained from the approach. (see Evaluation of The Communication Opportunity Group scheme (COGS) by Nelson and Burchell (1998) and an Evaluation of COGS by Cooper (2004).\n\nSage has served as the president of Human Communication International, as a member of Sir Michael Rutter's Advisory Committee on Language Research, as a member of the Research Committee of the British Stammering Association, and as an educational advisor to the Royal College of Speech and Language Therapists. She has also served as a trustee of the Association for Speech Impaired Children and of the Independent Panel for Education Advice. She is a member of the judiciary and served on the Lord Chancellor's training committee. \n\n\n"}
{"id": "49266299", "url": "https://en.wikipedia.org/wiki?curid=49266299", "title": "Sam Chachoua", "text": "Sam Chachoua\n\nSamir \"Sam\" Chachoua is an Australian alternative medicine practitioner, trained as a medical doctor. He is not actively licensed to practice medicine in Australia or the United States. Chachoua offers treatments in Mexico that he claims to be effective alternative medicine vaccine therapies for cancer and HIV, among other diseases. His claims lack scientific support, and are disputed by medical doctors.\n\nChachoua's treatments depend on theories he has named, including \"Induced Remission Therapy\" and \"The Nemesis Theory\", \"i.e.\" \"for every disease there is an anti-disease organism capable of destroying it and restoring health\".\n\nFor treatment of HIV, Chachoua vaccinates patients with Caprine arthritis encephalitis virus, which is known to cross-react immunologically with HIV. He claims to have eradicated HIV from the nation of Comoros in 2006. This claim has been refuted by Savlator Niyonzima, the UNAIDS country director of Madagascar, Comoros, Mauritius, and Seychelles. As of 2012, the adult HIV prevalence in Comoros was 2.1%, higher than the global prevalence of HIV in adults at 0.8%.\n\nIn 1997, Chachoua filed a lawsuit in the Los Angeles US District Court against Cedars-Sinai Medical Center. Chachoua stated that he had shared his cell cultures with them, but after the results had been published, the medical center claimed both the material and the credit. The only issues that made it to trial were Chachoa's claims of breach of contract and of a conspiracy to defame him. Although Chachoua prevailed on his breach of contract claim, the trial court reduced the verdict to $11,250, the amount Chachoua had paid for the testing of his samples, and gave Chachoua the choice of accepting that amount or a new trial on the breach of contract claim. Chachoua requested a new trial. The lawsuit was dismissed by Judge Margaret M. Morrow on November 13, 2001, due to Chachoua's “history of repeatedly disobeying court orders and resort to other dilatory tactics” and his “pattern of misconduct [that] spanned the tenures of several different attorneys.” Chachoua was later successfully sued by one of his lawyers for unpaid legal fees and costs, losing a counterclaim in which he accused her of legal malpractice.\n\nIn an episode of \"The Dr. Oz Show\" taped in late 2015 and aired January 12, 2016, Charlie Sheen said he had been receiving alternative treatment for HIV in Mexico from Chachoua, stating \"I'm [sic] been off my meds for about a week now\"; according to his manager, however, after the episode was taped he resumed taking his medications.\n\nIn the course of his treatment of Sheen, Chachoua claims that he injected himself with Sheen's blood. Sheen has stated that Chachoua may have switched the needle before injecting himself with blood. Chachoua has claimed that his treatments rendered Sheen HIV negative, despite the fact that Sheen continues to be HIV positive. Sheen has claimed that Chachoua unlawfully administered treatments in the United States, where he is not licensed to practice medicine.\n\n"}
{"id": "25147777", "url": "https://en.wikipedia.org/wiki?curid=25147777", "title": "Sankurathri Foundation", "text": "Sankurathri Foundation\n\nSankurathri Foundation (SF)' was established in 1989 by Dr. Chandra Sekhar Sankurathri in memory of his wife Manjari, son Kiran and daughter Sarada, who died in the bombing of Air India Flight 182 off the coast of Ireland on 23 June 1985.\n\nChandrasekhar Sankurathri was born to Appala Narasayya Naidu Sankurathri and Ramayamma Sankurathri on 20 November 1943 at Singarayakonda, Andhra Pradesh, India. He is the youngest of eight. He attended Municipal High School at Rajahmundry, East Godavari District, Andhra University, Waltair, Memorial University of Newfoundland, Canada and University of Alberta, Canada.\nChandrasekhar Sankurathri worked as a visiting scientist for the Ministry of Fisheries, in Canada and a scientific evaluator for Canada's Ministry of Health.\n\nChandrasekhar Sankurathri married Manjari on 13-05-1975 at Kakinada, India. Together they had a son, Srikiran, and a daughter, Sarada. Manjari, Srikiran and Sarada were killed in the Air India Flight 182 bombing on 23 June 1985 off the coast of Ireland.\n\nFollowing the bombing, he resigned his job in Canada and returned to India in 1988. He established the Manjari Sankurathri Memorial Foundation in 1989 in memory of his wife, which is a registered charity in Canada. He established the Sankurathri Foundation in India in memory of his family in 1989. The Foundations' goals are to improve the quality of life of needy people in the rural and remote areas of Andhra Pradesh. Established Sarada Vidyalayam in 1992 on his daughter’s name, which is a High school to provide free education for rural poor children. Sankurathri established Srikiran Institute of Ophthalmology in 1993, named after his son.\n\nSankurathri Foundation is working in collaboration with the Manjari Sankurathri Memorial Foundation (MSMF), established in year 1989 at Ottawa, Ontario, Canada. A board of volunteers, manage and oversee the distribution of all donations for humanitarian projects in India. In 1989, the active implementation of MSMF objectives fused the onset of The Sankurathri Foundation(SF). SF is currently managed by three volunteer trustees. Dr. Chandrasekhar Sankurathri remains the president of the MSMF and Executive Trustee of the SF.\n\nThe Sankurathri Foundation was registered as a non-profit organization in India with the purpose of improving the quality of life for destitute and downtrodden in the society. This is planned through education, health care and disaster relief programs.\nAll the activities are being organized from Kakinada, in the East Godavari District of Andhra pradesh, serving a population of over five million. Chandrasekhar lives on site and supervises all internal programs and outreach campaigns.\n\nSankurathri Foundation is implementing educational programs through Sarada Vidyalayam,health care programs through SriKiran Institute of Ophthalmology and disaster relief programs through Spandana.\n\nSarada Vidyalayam consists of three schools- Primary, High, and Vocational. While the primary school provides instruction in grades one to five, the High school covers grades six to ten. The vocational School will provide much needed job skills to unemployed youth in the region. Sarada vidyalayam opened its doors in 1992 with 25 children enrolled in grade one. The current enrollment is 158. The school follows the Andhra pradesh State Government syllabus. In addition to the academic subjects, children are encouraged to participate in activities such as gardening, drawing, painting, sewing, embroidery, yoga, dance, drama, and crafts. The children of the school have done very well academically as well as in extra curricular activities. The school has been formally recognized for its achievements and designated as a model school in the district. The primary school is free for all children. The children receive lunch, milk, uniforms, shoes, books, schoolbags, transportation, medical checkups, and medicines free of cost.\nSarada Vidyalayam is supported by MSMF, Asha for Education, St.Isidore School, St.Gregory Catholic School and other donors.He was named as Paul Harris Fellow by the Rotary Foundation, USA for his humanitarian work.\n\nSriKiran Institute of Ophthalmology was inaugurated in January, 1993 with a mission to \"provide quality eye care with compassion which is accessible, affordable and equitable to all\". Its outreach program with a strong awareness component has brought increased demand on SriKiran's resources and services. Today, SriKiran is a very well equipped and provider of modern eye care in the region. The new hospital building has ample waiting areas for out patients, five fully equipped air-conditioned operating theatres, speciality clinics for Cornea, Glaucoma, retina, Pediatric Ophthalmology Microbiology laboratory, Library, low Vision Rehabilitation Center and Auditorium.\n\nSriKiran offers access to affordable eye care to all regardless of their socio-economic status. To those who attend eye screening camps, SriKiran provides free eye examinations, and if a cataract surgery is recommended, provides free transportation, free surgery with an intraocular lens (IOL), free medications, free accommodation and food while they are in the hospital. The other strengths of SriKiran are its training Programs and International Volunteer program. SriKiran has been a leader in setting standards for eye care in the region, and providing training that emphasizes the importance of 'quality care' in delivering eye care services.\n\nSriKiran conducts continuing medical education programs to disseminate current concepts and techniques to local Ophthalmologists. Those programs are organized regularly through visits of volunteer specialists from Canada and the USA.\n\nSriKiran is supported by Canadian International Development Agency, National Program for Control of Blindness(NPCB), Christoffel-Blindenmission(CBM), Sensor Technology Limited], Eye care for the Adirondacks, Eye Foundation of America, District Blindness Control Society(DBCS), Aravind Eye Hospital, University of Ottawa Eye Institute, Wildrose Foundation, Help the Aged Canada, Rotary International, Orbis International, Infosys.\n\nSpandana is a disaster relief program started in 1998. As the region is situated on the coast of Bay of Bengal, disasters are frequent due to cyclones and monsoon rains resulting in floods and considerable damage to property. The program provides to displaced persons basic necessities such as food, drinking water, medications while they are in temporary shelters, and utensils,food, clothing and other needs after they return to their homes.\n\n\nCNN's special video on Dr. Sankurathri\n\n"}
{"id": "868983", "url": "https://en.wikipedia.org/wiki?curid=868983", "title": "Stretching", "text": "Stretching\n\nStretching is a form of physical exercise in which a specific muscle or tendon (or muscle group) is deliberately flexed or stretched in order to improve the muscle's felt elasticity and achieve comfortable muscle tone. The result is a feeling of increased muscle control, flexibility, and range of motion. Stretching is also used therapeutically to alleviate cramps.\n\nIn its most basic form, stretching is a natural and instinctive activity; it is performed by humans and many other animals. It can be accompanied by yawning. Stretching often occurs instinctively after waking from sleep, after long periods of inactivity, or after exiting confined spaces and areas.\n\nIncreasing flexibility through stretching is one of the basic tenets of physical fitness. It is common for athletes to stretch before (for warming up) and after exercise in an attempt to reduce risk of injury and increase performance.\n\nStretching can be dangerous when performed incorrectly. There are many techniques for stretching in general, but depending on which muscle group is being stretched, some techniques may be ineffective or detrimental, even to the point of causing hypermobility, instability, or permanent damage to the tendons, ligaments, and muscle fiber. The physiological nature of stretching and theories about the effect of various techniques are therefore subject to heavy inquiry.\n\nAlthough static stretching (see image on the right for an example) is part of some warm-up routines, a study in 2013 indicated that it weakens muscles. For this reason, an active dynamic warm-up (movement of the muscle groups with lights weights for example) is recommended before exercise in place of static stretching.\n\nStudies have shed light on the function, in stretching, of a large protein within the myofibrils of skeletal muscles named titin. A study performed by Magid and Law demonstrated that the origin of passive muscle tension (which occurs during stretching) is actually within the myofibrils, not extracellularly as had previously been supposed.\nDue to neurological safeguards against injury, it is normally impossible for adults to stretch most muscle groups to their fullest length without training due to the activation of muscle antagonists as the muscle reaches the limit of its normal range of motion.\n\nThere are three kinds of stretching: static, dynamic (bouncing), and Proprioceptive neuromuscular facilitation (PNF), where the muscle is passively stretched, then the muscle is contracted, then stretched further.\n\nAlthough many people engage in stretching before or after exercise, the medical evidence has shown this has no meaningful benefit in preventing muscle soreness.\n\nStretching does not appear to reduce the risk of injury during exercise, except perhaps for runners. There is some evidence that pre-exercise stretching may increase athletes' range of movement.\n\n"}
{"id": "2809489", "url": "https://en.wikipedia.org/wiki?curid=2809489", "title": "Sure Start", "text": "Sure Start\n\nSure Start is a UK Government area-based initiative, announced in 1998 by the then Chancellor of the Exchequer, Gordon Brown, applying primarily in England with slightly different versions in Wales, Scotland and Northern Ireland. The initiative originated from HM Treasury, with the aim of \"giving children the best possible start in life\" through improvement of childcare, early education, health and family support, with an emphasis on outreach and community development.\n\nLaunched in 1998 by Tessa Jowell, Sure Start had similarities to the much older, and similarly named, Head Start programme in the United States and is also comparable to Australia Head Start and Ontario's Early Years Plan. The initiatives were subsequently bound together to form \"Sure Start Children's Centres\", and responsibility for them was transferred to local government. Tessa Jowell subsequently commented in 2015, \"I am very proud of setting up Sure Start, because the first three years of a child's life are absolutely critical in determining the chances they have subsequently.\"\n\nThe National Evaluation of Sure Start (NESS) project ran from 2001 until 2012. Initial research findings from NESS, published in 2005, suggested the impact of (the then termed) Sure Start Local Programmes (SSLPs) was not as great as had been hoped. However, by 2010, NESS could identify a significant impact on some of the outcomes set for Sure Start.\n\nThe Evaluation of Children's Centres in England (ECCE) project ran from 2009 until 2015. Results concerning the impact of (the subsequently termed) Sure Start Children's Centres (SSCCs) concluded that, \"\"Children's Centres set up to support parents of young children can improve the mental health of mothers and functioning of families but that these benefits are being eroded by cuts\".\".\n\nInitial funding was substantial, with £540m allocated for expenditure between 1999 and 2002, £452m of it within England, to set up 250 Sure Start Local Programmes (SSLPs) reaching up to 150,000 children in areas of deprivation.\n\nThe UK Government initially pledged to fund Sure Start for 10 years, but in 2003, Chancellor Gordon Brown announced the Government's long-term plan to transfer Sure Start into the control of local government by 2005, and create a Sure Start Children's Centre in every community.\n\nRelated to the Government's goal of reducing child poverty, the initial districts for Sure Start development were selected \"according to the levels of deprivation within their areas\" the focus being particularly on disadvantaged areas but open to all families living in the catchment area. Such catchment areas were selected locally by the projects.\n\nSure Start was overseen by the Department for Children, Schools and Families and the Department for Work and Pensions. The programme has been described by Tony Blair as \"one of New Labour's greatest achievements\".\n\nEach project was allowed to develop in its own way depending on the expressed wishes of parents and the guidance of the various organisations heading up each one. Policy on such matters as choosing volunteers and even the services offered were a local level decision.\n\nSure Start local programmes were opened in waves, Round 1 indicates the first wave of programmes starting 1999. Round 6 represents the final wave of Sure Start local programmes mostly starting in 2003.\n\nEvery Child Matters proposed a switch from Sure Start Local Programmes (SSLPs) to Sure Start Children's Centres (SSCCs), which would be controlled by local authorities, and would be provided not just in the most disadvantaged areas. In the 2004 Comprehensive Spending Review, Chancellor Gordon Brown announced that the Government would provide funding for 2,500 Children's Centres by 2008. This target was later increased to 3,500 children's centres by 2010. Of the 524 original Sure Start local programmes, most are now Sure Start Children's Centres.\n\nSome Sure Start Local Programmes have become registered Charities and Companies Limited by guarantee. Sure Start Hounslow, a programme in West London, became a company limited by guarantee in 2004 and now delivers a range of services, many through Service Level Agreement with the local authority, not all of which focus entirely on children under five. This development has been one of many routes that Sure Start Local Programmes have taken to ensure sustainability during the \"tapering\" of the original Sure Start Grant.\n\nIn 2005, Norman Glass, one of the original architects of Sure Start wrote an article praising the increased government focus on the early years, but criticising cuts in funding per head; the change from child development to childcare and getting mothers into work; and the shift back to local authority control, rather than being run by boards including parents.\n\nChildren's Centres are expected to provide:\n\nCuts in general funding from central government to local authorities in England led to fears, in 2011, that up to 250 Sure Start centres would close. The former Secretary of State for Education, Michael Gove, has admitted that funding for Sure Start has not been protected, as most central funding of local authorities will no longer be ring-fenced. The decision would be left to local councils, though Children's Minister Sarah Teather said there was enough money available to maintain existing children's centres, should they wish.\n\nA number of local councils announced cuts to their Sure Start budgets, and parents and mothers' groups protested against these cuts, taking their campaign directly to Downing Street. Many councils retreated. In February 2017, all 44 Sure Start children's centres in Oxfordshire were closed after High Court appeals against the measure failed.\n\nMinisters said they want to refocus the scheme to help the most disadvantaged families. The government is now allowing parents to choose their own childcare provider, and to get part-funding provided via tax credits, rather than a centrally run service.\n\nIn 2017, a Briefing Paper for Pariliament summarised the changes to Sure Start that had occurred under the 2015–2017 Conservative Government. Although \"The Conservative Government did not make any significant operational or legislative changes to Sure Start during the 2015-2017 Parliament\", \"Arguably the most significant changes related to funding\". The Briefing Paper notes that in real terms, spending in 2015/16 was 47% less than in 2010/11 with budgets for 2016/17 showing a further planned reduction in spending. This Briefing Paper for MPs also reported a reduction of 208 Sure Start Children's Centre sites between 2015 and 2017.\n\nA 2007 study by researchers from the Universities of Oxford and Wales published in the British Medical Journal looking at parenting interventions within the Sure Start system in Wales examined 153 parents from socially deprived areas and showed that a course teaching improved parenting skills had great benefits in reducing problem behaviour in young children. Parents were taught to:\n\nThe study recommended that this evidence-based class be expanded from Wales to the rest of the UK, making it available for all parents who need it, stating that the Sure Start programme has not yet produced results as good as these in England.\n\nA lack of effectiveness in England has been suggested by a University of Durham study which suggested that Sure Start was ineffective at improving results in early schooling.\n\nA national longitudinal evaluation of Sure Start, known as NESS, was set up in 2001. Although early evaluations did not find Sure Start Local Programmes (SSLPs) to have been particularly effective, by 2008 NESS was able to conclude \"For the time being, it remains plausible, even if by no means certain, that the differences in findings across the first and second phases of the NESS Impact Study reflect actual changes in the impact of SSLPs resulting from the increasing quality of service provision, greater attention to the hard-to-reach and the move to Children's Centres, as well as the greater exposure to the programme of children and families in the latest phase of the impact evaluation.\"\n\nIn 2010, robust research conducted by NESS demonstrated significant effects of SSLPs on eight of 21 outcomes: two positive outcomes for children (lower BMIs and better physical health), four positive outcomes for mothers and families (more stimulating and less chaotic home environments, less harsh discipline, and greater life-satisfaction)\n\nA new publicly-funded national longitudinal evaluation of Sure Start, known as ECCE, was set up in 2009. This study, different in design to NESS, ran for 6 years and by 2015 had, \"identified a number of significant but relatively small positive effects in promoting better outcomes for each user group considered (child, mother, and families)\" although no impact was found on household employment status (whether or not one or adults in a household works) or on children's health. However, in 2016 the British Medical Journal noted that the benefits of Sure Start to children and families were being eroded by austerity cuts and that, \"disadvantaged families are at greatest risk from [these] austerity cuts\".\n\nIn 2017, the evidence concerning the effectiveness of Sure Start from both the NESS and the ECCE studies was summarised by a Briefing Paper that was written for the (then) Members of the UK Parliament.\n\n"}
{"id": "28852", "url": "https://en.wikipedia.org/wiki?curid=28852", "title": "Syphilis", "text": "Syphilis\n\nSyphilis is a sexually transmitted infection caused by the bacterium \"Treponema pallidum\" subspecies \"pallidum\". The signs and symptoms of syphilis vary depending in which of the four stages it presents (primary, secondary, latent, and tertiary). The primary stage classically presents with a single chancre (a firm, painless, non-itchy skin ulceration) but there may be multiple sores. In secondary syphilis, a diffuse rash occurs, which frequently involves the palms of the hands and soles of the feet. There may also be sores in the mouth or vagina. In latent syphilis, which can last for years, there are few or no symptoms. In tertiary syphilis, there are gummas (soft, non-cancerous growths), neurological, or heart symptoms. Syphilis has been known as \"the great imitator\" as it may cause symptoms similar to many other diseases.\nSyphilis is most commonly spread through sexual activity. It may also be transmitted from mother to baby during pregnancy or at birth, resulting in congenital syphilis. Other diseases caused by the \"Treponema\" bacteria include yaws (subspecies \"pertenue\"), pinta (subspecies \"carateum\"), and nonvenereal endemic syphilis (subspecies \"endemicum\"). These three diseases are not typically sexually transmitted. Diagnosis is usually made by using blood tests; the bacteria can also be detected using dark field microscopy. The Centers for Disease Control and Prevention (U.S.) recommend all pregnant women be tested.\nThe risk of sexual transmission of syphilis can be reduced by using a latex condom. Syphilis can be effectively treated with antibiotics. The preferred antibiotic for most cases is benzathine benzylpenicillin injected into a muscle. In those who have a severe penicillin allergy, doxycycline or tetracycline may be used. In those with neurosyphilis, intravenous benzylpenicillin or ceftriaxone is recommended. During treatment people may develop fever, headache, and muscle pains, a reaction known as Jarisch-Herxheimer.\nIn 2015, about 45.4 million people were infected with syphilis, with 6 million new cases. During 2015, it caused about 107,000 deaths, down from 202,000 in 1990. After decreasing dramatically with the availability of penicillin in the 1940s, rates of infection have increased since the turn of the millennium in many countries, often in combination with human immunodeficiency virus (HIV). This is believed to be partly due to increased promiscuity, prostitution, decreasing use of condoms, and unsafe sexual practices among men who have sex with men. In 2015, Cuba became the first country to eliminate mother-to-child transmission of syphilis.\nSyphilis can present in one of four different stages: primary, secondary, latent, and tertiary, and may also occur congenitally. It was referred to as \"the great imitator\" by Sir William Osler due to its varied presentations.\n\nPrimary syphilis is typically acquired by direct sexual contact with the infectious lesions of another person. Approximately 3 to 90 days after the initial exposure (average 21 days) a skin lesion, called a chancre, appears at the point of contact. This is classically (40% of the time) a single, firm, painless, non-itchy skin ulceration with a clean base and sharp borders approximately 0.3–3.0 cm in size. The lesion may take on almost any form. In the classic form, it evolves from a macule to a papule and finally to an erosion or ulcer. Occasionally, multiple lesions may be present (~40%), with multiple lesions being more common when coinfected with HIV. Lesions may be painful or tender (30%), and they may occur in places other than the genitals (2–7%). The most common location in women is the cervix (44%), the penis in heterosexual men (99%), and anally and rectally in men who have sex with men (34%). Lymph node enlargement frequently (80%) occurs around the area of infection, occurring seven to 10 days after chancre formation. The lesion may persist for three to six weeks if left untreated.\n\nSecondary syphilis occurs approximately four to ten weeks after the primary infection. While secondary disease is known for the many different ways it can manifest, symptoms most commonly involve the skin, mucous membranes, and lymph nodes. There may be a symmetrical, reddish-pink, non-itchy rash on the trunk and extremities, including the palms and soles. The rash may become maculopapular or pustular. It may form flat, broad, whitish, wart-like lesions on mucous membranes, known as condyloma latum. All of these lesions harbor bacteria and are infectious. Other symptoms may include fever, sore throat, malaise, weight loss, hair loss, and headache. Rare manifestations include liver inflammation, kidney disease, joint inflammation, periostitis, inflammation of the optic nerve, uveitis, and interstitial keratitis. The acute symptoms usually resolve after three to six weeks; about 25% of people may present with a recurrence of secondary symptoms. Many people who present with secondary syphilis (40–85% of women, 20–65% of men) do not report previously having had the classical chancre of primary syphilis.\n\nLatent syphilis is defined as having serologic proof of infection without symptoms of disease. It is further described as either early (less than 1 year after secondary syphilis) or late (more than 1 year after secondary syphilis) in the United States. The United Kingdom uses a cut-off of two years for early and late latent syphilis. Early latent syphilis may have a relapse of symptoms. Late latent syphilis is asymptomatic, and not as contagious as early latent syphilis.\n\nTertiary syphilis may occur approximately 3 to 15 years after the initial infection, and may be divided into three different forms: gummatous syphilis (15%), late neurosyphilis (6.5%), and cardiovascular syphilis (10%). Without treatment, a third of infected people develop tertiary disease. People with tertiary syphilis are not infectious.\n\nGummatous syphilis or late benign syphilis usually occurs 1 to 46 years after the initial infection, with an average of 15 years. This stage is characterized by the formation of chronic gummas, which are soft, tumor-like balls of inflammation which may vary considerably in size. They typically affect the skin, bone, and liver, but can occur anywhere.\n\nNeurosyphilis refers to an infection involving the central nervous system. It may occur early, being either asymptomatic or in the form of syphilitic meningitis, or late as meningovascular syphilis, general paresis, or tabes dorsalis, which is associated with poor balance and lightning pains in the lower extremities. Late neurosyphilis typically occurs 4 to 25 years after the initial infection. Meningovascular syphilis typically presents with apathy and seizures, and general paresis with dementia and tabes dorsalis. Also, there may be Argyll Robertson pupils, which are bilateral small pupils that constrict when the person focuses on near objects (accommodation reflex) but do not constrict when exposed to bright light (pupillary reflex).\n\nCardiovascular syphilis usually occurs 10–30 years after the initial infection. The most common complication is syphilitic aortitis, which may result in aortic aneurysm formation.\n\nCongenital syphilis is that which is transmitted during pregnancy or during birth. Two-thirds of syphilitic infants are born without symptoms. Common symptoms that develop over the first couple of years of life include enlargement of the liver and spleen (70%), rash (70%), fever (40%), neurosyphilis (20%), and lung inflammation (20%). If untreated, late congenital syphilis may occur in 40%, including saddle nose deformation, Higoumenakis sign, saber shin, or Clutton's joints among others. Infection during pregnancy is also associated with miscarriage.\n\n\"Treponema pallidum\" subspecies\" pallidum\" is a spiral-shaped, Gram-negative, highly mobile bacterium. Three other human diseases are caused by related \"Treponema pallidum\" subspecies, including yaws (subspecies \"pertenue\"), pinta (subspecies \"carateum\") and bejel (subspecies \"endemicum\"). Unlike subspecies \"pallidum\", they do not cause neurological disease. Humans are the only known natural reservoir for subspecies \"pallidum\". It is unable to survive more than a few days without a host. This is due to its small genome (1.14Mbp) failing to encode the metabolic pathways necessary to make most of its macronutrients. It has a slow doubling time of greater than 30 hours.\n\nSyphilis is transmitted primarily by sexual contact or during pregnancy from a mother to her fetus; the spirochete is able to pass through intact mucous membranes or compromised skin. It is thus transmissible by kissing near a lesion, as well as oral, vaginal, and anal sex. Approximately 30% to 60% of those exposed to primary or secondary syphilis will get the disease. Its infectivity is exemplified by the fact that an individual inoculated with only 57 organisms has a 50% chance of being infected. Most (60%) of new cases in the United States occur in men who have sex with men. Syphilis can be transmitted by blood products, but the risk is low due to screening of donated blood in many countries. The risk of transmission from sharing needles appears limited.\n\nIt is not generally possible to contract syphilis through toilet seats, daily activities, hot tubs, or sharing eating utensils or clothing. This is mainly because the bacteria die very quickly outside of the body, making transmission by objects extremely difficult.\n\nSyphilis is difficult to diagnose clinically during early infection. Confirmation is either via blood tests or direct visual inspection using dark field microscopy. Blood tests are more commonly used, as they are easier to perform. Diagnostic tests are unable to distinguish between the stages of the disease.\n\nBlood tests are divided into nontreponemal and treponemal tests.\n\nNontreponemal tests are used initially, and include venereal disease research laboratory (VDRL) and rapid plasma reagin (RPR) tests. False positives on the nontreponemal tests can occur with some viral infections, such as varicella (chickenpox) and measles. False positives can also occur with lymphoma, tuberculosis, malaria, endocarditis, connective tissue disease, and pregnancy.\n\nBecause of the possibility of false positives with nontreponemal tests, confirmation is required with a treponemal test, such as treponemal pallidum particle agglutination (TPHA) or fluorescent treponemal antibody absorption test (FTA-Abs). Treponemal antibody tests usually become positive two to five weeks after the initial infection. Neurosyphilis is diagnosed by finding high numbers of leukocytes (predominately lymphocytes) and high protein levels in the cerebrospinal fluid in the setting of a known syphilis infection.\n\nDark field microscopy of serous fluid from a chancre may be used to make an immediate diagnosis. Hospitals do not always have equipment or experienced staff members, and testing must be done within 10 minutes of acquiring the sample. Sensitivity has been reported to be nearly 80%; therefore the test can only be used to confirm a diagnosis, not to rule one out. Two other tests can be carried out on a sample from the chancre: direct fluorescent antibody (DFA) and polymerase chain reaction (PCR) tests. DFA uses antibodies tagged with fluorescein, which attach to specific syphilis proteins, while PCR uses techniques to detect the presence of specific syphilis genes. These tests are not as time-sensitive, as they do not require living bacteria to make the diagnosis.\n\n, there is no vaccine effective for prevention. Several vaccines based on treponemal proteins reduce lesion development in an animal model but research continues.\n\nCondom use reduces the likelihood of transmission during sex, but does not completely eliminate the risk. The Centers for Disease Control and Prevention (CDC) states, \"Correct and consistent use of latex condoms can reduce the risk of syphilis only when the infected area or site of potential exposure is protected. However, a syphilis sore outside of the area covered by a latex condom can still allow transmission, so caution should be exercised even when using a condom.\"\n\nAbstinence from intimate physical contact with an infected person is effective at reducing the transmission of syphilis. The CDC states, \"The surest way to avoid transmission of sexually transmitted diseases, including syphilis, is to abstain from sexual contact or to be in a long-term mutually monogamous relationship with a partner who has been tested and is known to be uninfected.\"\n\nCongenital syphilis in the newborn can be prevented by screening mothers during early pregnancy and treating those who are infected. The United States Preventive Services Task Force (USPSTF) strongly recommends universal screening of all pregnant women, while the World Health Organization (WHO) recommends all women be tested at their first antenatal visit and again in the third trimester. If they are positive, it is recommend their partners also be treated. Congenital syphilis is still common in the developing world, as many women do not receive antenatal care at all, and the antenatal care others receive does not include screening. It still occasionally occurs in the developed world, as those most likely to acquire syphilis are least likely to receive care during pregnancy. Several measures to increase access to testing appear effective at reducing rates of congenital syphilis in low- to middle-income countries. Point-of-care testing to detect syphilis appeared to be reliable although more research is needed to assess its effectiveness and into improving outcomes in mothers and babies.\n\nThe CDC recommends that sexually active men who have sex with men be tested at least yearly. The USPSTF also recommends screening among those at high risk.\n\nSyphilis is a notifiable disease in many countries, including Canada the European Union, and the United States. This means health care providers are required to notify public health authorities, which will then ideally provide partner notification to the person's partners. Physicians may also encourage patients to send their partners to seek care. Several strategies have been found to improve follow-up for STI testing, including email and text messaging of reminders for appointments.\n\nThe first-line treatment for uncomplicated syphilis remains a single dose of intramuscular benzathine benzylpenicillin. Doxycycline and tetracycline are alternative choices for those allergic to penicillin; due to the risk of birth defects, these are not recommended for pregnant women. Resistance to macrolides, rifampicin, and clindamycin is often present. Ceftriaxone, a third-generation cephalosporin antibiotic, may be as effective as penicillin-based treatment. It is recommended that a treated person avoid sex until the sores are healed.\n\nFor neurosyphilis, due to the poor penetration of benzathine penicillin into the central nervous system, those affected are given large doses of intravenous penicillin for a minimum of 10 days. If a person is allergic to penicillin, ceftriaxone may be used or penicillin desensitization attempted. Other late presentations may be treated with once-weekly intramuscular benzathine penicillin for three weeks. Treatment at this stage solely limits further progression of the disease and has a limited effect on damage which has already occurred.\n\nOne of the potential side effects of treatment is the Jarisch-Herxheimer reaction. It frequently starts within one hour and lasts for 24 hours, with symptoms of fever, muscle pains, headache, and a fast heart rate. It is caused by cytokines released by the immune system in response to lipoproteins released from rupturing syphilis bacteria.\n\nPenicillin is an effective treatment for syphilis in pregnancy but there is no agreement on which dose or route of delivery is most effective.\n\nIn 2012, about 0.5% of adults were infected with syphilis, with 6 million new cases. In 1999, it is believed to have infected 12 million additional people, with greater than 90% of cases in the developing world. It affects between 700,000 and 1.6 million pregnancies a year, resulting in spontaneous abortions, stillbirths, and congenital syphilis. During 2015, it caused about 107,000 deaths, down from 202,000 in 1990. In sub-Saharan Africa, syphilis contributes to approximately 20% of perinatal deaths. Rates are proportionally higher among intravenous drug users, those who are infected with HIV, and men who have sex with men. In the United States, rates of syphilis as of 2007 were six times greater in men than in women; they were nearly equal ten years earlier. African Americans accounted for almost half of all cases in 2010. As of 2014, syphilis infections continue to increase in the United States.\n\nSyphilis was very common in Europe during the 18th and 19th centuries. Flaubert found it universal among nineteenth-century Egyptian prostitutes. In the developed world during the early 20th century, infections declined rapidly with the widespread use of antibiotics, until the 1980s and 1990s. Since 2000, rates of syphilis have been increasing in the US, Canada, the UK, Australia and Europe, primarily among men who have sex with men. Rates of syphilis among US women have remained stable during this time, while rates among UK women have increased, but at a rate less than that of men. Increased rates among heterosexuals have occurred in China and Russia since the 1990s. This has been attributed to unsafe sexual practices, such as sexual promiscuity, prostitution, and decreasing use of barrier protection.\n\nLeft untreated, it has a mortality rate of 8% to 58%, with a greater death rate among males. The symptoms of syphilis have become less severe over the 19th and 20th centuries, in part due to widespread availability of effective treatment, and partly due to virulence of the bacteria. With early treatment, few complications result. Syphilis increases the risk of HIV transmission by two to five times, and coinfection is common (30–60% in some urban centers). In 2015, Cuba became the first country in the world to eradicate mother to child transmission of syphilis.\n\nThe exact origin of syphilis is disputed. Syphilis was definitely present in the Americas before European contact, and it may have been carried from the Americas to Europe by the returning crewmen from Christopher Columbus's voyage to the Americas; or it may have existed in Europe previously, but went unrecognized until shortly after Columbus returned. These are the \"Columbian\" and \"pre-Columbian\" hypotheses, respectively, with the \"Columbian\" hypothesis best supported by available evidence.\n\nThe first written records of an outbreak of syphilis in Europe occurred in 1494 or 1495 in Naples, Italy, during a French invasion (Italian War of 1494–98). Since it was claimed to have been spread by French troops, it was initially called the \"French disease\" by the people of Naples. In 1530, the pastoral name \"syphilis\" (the name of a character) was first used by the Italian physician and poet Girolamo Fracastoro as the title of his Latin poem in dactylic hexameter describing the ravages of the disease in Italy. It was also called the \"Great Pox\".\n\nIn the 16th through 19th centuries, syphilis was one of the largest public health burdens in prevalence, symptoms, and disability, although records of its true prevalence were generally not kept because of the fearsome and sordid status of sexually transmitted diseases in those centuries. At the time the causative agent was unknown but it was well known that it was spread sexually and also often from mother to child. Its association with sex, especially sexual promiscuity and prostitution, made it an object of fear and revulsion and a taboo. The magnitude of its morbidity and mortality in those centuries reflected that, unlike today, there was no adequate understanding of its pathogenesis and no truly effective treatments. Its damage was caused not so much by great sickness or death early in the course of the disease but rather by its gruesome effects decades after infection as it progressed to neurosyphilis with tabes dorsalis.\n\nThe causative organism, \"Treponema pallidum\", was first identified by Fritz Schaudinn and Erich Hoffmann, in 1905. The first effective treatment for syphilis was Salvarsan, developed in 1910 by Paul Ehrlich. The effectiveness of treatment with penicillin was confirmed in trials in 1943.\n\nBefore the discovery and use of antibiotics in the mid-twentieth century, mercury and isolation were commonly used, with treatments often worse than the disease. During the 20th century, as both microbiology and pharmacology advanced greatly, syphilis, like many other infectious diseases, became more of a manageable burden than a scary and disfiguring mystery, at least in developed countries among those people who could afford to pay for timely diagnosis and treatment.\n\nMany famous historical figures, including Franz Schubert, Arthur Schopenhauer, Édouard Manet, Charles Baudelaire, and Guy de Maupassant are believed to have had the disease. Friedrich Nietzsche was long believed to have gone mad as a result of tertiary syphilis, but that diagnosis has recently come into question.\nThe earliest known depiction of an individual with syphilis is Albrecht Dürer's \"Syphilitic Man\", a woodcut believed to represent a Landsknecht, a Northern European mercenary. The myth of the \"femme fatale\" or \"poison women\" of the 19th century is believed to be partly derived from the devastation of syphilis, with classic examples in literature including John Keats' \"La Belle Dame sans Merci\".\n\nThe artist Jan van der Straet painted \"Preparation and Use of Guayaco for Treating Syphilis\", a scene of a wealthy man receiving treatment for syphilis with the tropical wood guaiacum sometime around 1580.\n\nThe Tuskegee Study of Untreated Syphilis in the Negro Male\" was an infamous, unethical, and racist clinical study conducted between 1932 and 1972 by the U.S. Public Health Service. The purpose of this study was to observe the natural history of untreated syphilis; the African-American men in the study were told they were receiving free health care from the United States government.\n\nThe Public Health Service started working on this study in 1932 in collaboration with Tuskegee University, a historically black college in Alabama. Investigators enrolled in the study a total of 600 impoverished, African-American sharecroppers from Macon County, Alabama. Of these men, 399 had previously contracted syphilis before the study began, and 201 did not have the disease. The men were given free medical care, meals, and free burial insurance for participating in the study. The men were told that the study was only going to last six months, but it actually lasted 40 years. After funding for treatment was lost, the study was continued without informing the men that they would never be treated. None of the men infected were ever told that they had the disease, and none were treated with penicillin even after the antibiotic was proven to successfully treat syphilis. According to the Centers for Disease Control, the men were told that they were being treated for \"bad blood\", a colloquialism that described various conditions such as syphilis, anemia, and fatigue. \"Bad blood\"—specifically the collection of illnesses the term included—was a leading cause of death within the southern African-American community.\n\nThe 40-year study was controversial for reasons related to ethical standards. Researchers knowingly failed to treat patients appropriately after the 1940s validation of penicillin was found as an effective cure for the disease that they were studying. The revelation in 1972 of study failures by a whistleblower, Peter Buxtun, led to major changes in U.S. law and regulation on the protection of participants in clinical studies. Now studies require informed consent, communication of diagnosis, and accurate reporting of test results.\n\nSimilar experiments were carried out in Guatemala from 1946 to 1948. It was done during the administration of American President Harry S. Truman and Guatemalan President Juan José Arévalo with the cooperation of some Guatemalan health ministries and officials. Doctors infected soldiers, prostitutes, prisoners and mental patients with syphilis and other sexually transmitted diseases, without the informed consent of the subjects, and treated most subjects with antibiotics. The experiment resulted in at least 83 deaths. In October 2010, the U.S. formally apologized to Guatemala for the ethical violations that took place. The experiments were led by physician John Charles Cutler who also participated in the late stages of the Tuskegee syphilis experiment.\n\n"}
{"id": "54014641", "url": "https://en.wikipedia.org/wiki?curid=54014641", "title": "Terrell State Hospital", "text": "Terrell State Hospital\n\nTerrell State Hospital is a public psychiatric hospital located in Terrell, Texas, United States, established in 1885. The original hospital building was built according to the Kirkbride Plan.\n\n"}
{"id": "21962538", "url": "https://en.wikipedia.org/wiki?curid=21962538", "title": "The Food Defect Action Levels", "text": "The Food Defect Action Levels\n\nThe Food Defect Action Levels: Levels of natural or unavoidable defects in foods that present no health hazards for humans is a publication of the United States Food and Drug Administration's Center for Food Safety and Applied Nutrition detailing acceptable levels of food contamination from sources such as maggots, thrips, insect fragments, \"foreign matter\", mold, rodent hairs, and insect and mammalian feces.\n\nThe publication details the acceptable amounts of contaminants on a per food basis, listing both the defect source (pre-harvest infection, processing infestation, processing contamination, etc.) and significance (aesthetic, potential health hazard, mouth/tooth injury, etc.). For example, the limit of insect contaminants allowed in canned or frozen peaches is specified as: \"In 12 1-pound cans or equivalent, one or more larvae and/or larval fragments whose aggregate length exceeds 5 mm.\"\n\n\"The Food Defect Action Levels\" was first published in 1995. A printed version of the publication may be obtained by written request to the Food and Drug Administration (FDA) or by clicking here.\n\nThe insect fragments are classified as an aesthetic problem. \"The Food Defect Action Levels\" states that these contaminants \"pose no inherent hazard to health.\"\n\n\n"}
{"id": "40256246", "url": "https://en.wikipedia.org/wiki?curid=40256246", "title": "Webbed penis", "text": "Webbed penis\n\nWebbed penis, also called penis palmatus or penoscrotal fusion, is an acquired or congenital condition in which the scrotal skin extends onto the ventral penile shaft. The penile shaft is buried in scrotum or tethered to the scrotal midline by a fold or web of skin. The urethra and erectile bodies are usually normal. Webbed penis is usually asymptomatic, but the cosmetic appearance is often unacceptable. This condition may be corrected by surgical techniques.\n\nIn the congenital form, the deformity represents an abnormality of the attachment between the penis and the scrotum; the penis, the urethra, and the remainder of the scrotum typically are normal.\n\nWebbed penis may also be acquired (iatrogenic) after circumcision or other penile surgery, resulting from excessive removal of ventral penile skin; the penis can retract into the scrotum, resulting in secondary phimosis (trapped penis).\n"}
{"id": "45342152", "url": "https://en.wikipedia.org/wiki?curid=45342152", "title": "Women's Health Issues (journal)", "text": "Women's Health Issues (journal)\n\nWomen's Health Issues is a bimonthly peer-reviewed medical journal covering women's health care and policy. It is the official journal of the Jacobs Institute of Women's Health and published on their behalf by Elsevier. The editor-in-chief is Chloe E. Bird (RAND Corporation).\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 1.811, ranking it 4th out of 40 journals in the category \"Women's Studies\".\n\n"}
