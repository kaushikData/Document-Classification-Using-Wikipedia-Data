{"id": "10555897", "url": "https://en.wikipedia.org/wiki?curid=10555897", "title": "100,000,000 Guinea Pigs", "text": "100,000,000 Guinea Pigs\n\n100,000,000 Guinea Pigs: Dangers in Everyday Foods, Drugs, and Cosmetics is a book written by Arthur Kallet and F.J. Schlink first released in 1933 by the Vanguard Press and manufactured in the United States of America. Its central argument propounds that the American population is being used as guinea pigs in a giant experiment undertaken by the American producers of food stuffs and patent medicines and the like. Kallet and Schlink premise the book as being “written in the interest of the consumer, who does not yet realize that he is being used as a guinea pig…”\n\nThe book's key proposition is that a significant portion of the products sold to the public—particularly pharmaceuticals and food products—are released with little regard for or knowledge of how these products adversely affect the consumer. Corporations, often knowingly, release products which either do not do what they purport to do, or have dangerous side effects or defects. Furthermore, many officials and government departments, namely, the U.S. Food and Drug Administration, have fallen victim to regulatory capture.\n\nThe book goes on to state that the Pure Food and Drug Act of 1906 is not effective in arresting these trends, and real reform or consumer protection is obstructed by the powerful connections that offending corporations have with the government.\nIf the poison is such that it acts slowly and insidiously, perhaps over a long period of years (and several such will be considered in later chapters), then we poor consumers must be test animals all our lives; and when, in the end, the experiment kills us a year or ten years sooner than otherwise we would have died, no conclusions can be drawn and a hundred million others are available for further tests.\nThe authors develop ideas such as synergy effects, and the precautionary and substitution principles. They claim that many toxic substances, even in low concentrations, can act together to cause much more harmful effects than each substance would individually. Prolonged exposure to low amounts of toxic substances, even at very mild concentrations, can potentially have serious negative health impacts that consumers are not made aware of. These impacts are felt by all consumers because harmful substances are being ingested by consumers because of the use of dangerous pesticides, herbicides and other chemicals in food production. Preservatives are particularly criticized, and the increase in canned or packaged foods is cited as evidence of an increasing risk of such synergy effects because of the large amount of chemical byproducts these products include.\n\nThe book argues that many products would not be sold if properly labeled, and this failure to police product labeling has been a key failing of the Food and Drug Administration. Extensive reform and overhaul in government regulation and inspection of the food and drug industry is needed in order to adequately protect consumers from corporations and manufacturers who do not place the health of the consumer before profit. Examples cited include beauty products, which in the first quarter of the 20th century were found to contain arsenic, lead and even radium, the health effects of which were not understood or known to consumers at the time. The true label for a pineapple pie, they argue, would be closer to this:\nCorn starch-filled, glucose-sweetened pie with made with sub-standard canned pineapple, artificial (citric acid) lemon flavor and artificial coal tar color.\nThe book takes particular aim at the pharmaceutical market in the United States during the period, citing extensive lists of drugs which are often the subject of very strong and widespread campaigns of media promotion as \"wonder-drugs,\" yet which do not have any effect on the conditions they purport to cure, and often carry with them serious side effects that are not revealed to consumers. The authors claim that advertising for these drugs is deliberately misleading and uses a variety of dishonest techniques from false testimonials to fake experts. The authors also question the value of statements made by scientists who vouch for the safety of products, citing the example of a dean of the College of Pharmacy of Columbia University who had vouched for the safety of a drug that later proved fatal to many.\n\nIn the final analysis, the authors encourage consumers to be more active and questioning in their purchasing habits. Consumers should be vigilant in finding out more information about products and ingredients, and boycotting producers and their products that contain dangerous ingredients. They also call for stronger laws, tougher penalties for offending companies, and a much more concerted effort from authorities to implement consumer protection laws. The book concludes with the statement that \"Above all, let your voice be heard loudly and often, in protest against indifference, ignorance, and avarice responsible for the uncontrolled adulteration and misrepresentation of foods, drugs, and cosmetics.\"\n\nThe book proved to be extremely popular and a national bestseller in the years immediately following its release, and at least 13 printings of the book were published in the first six months of publication. Public reaction to the book was very strong. Many people were shocked at the extent of food contamination and drug side-effects, and \"100,000,000 Guinea Pigs\", along with several other books of a similar nature, were published during a period when a new consumer movement emerged. It is often cited, along with \"American Chamber of Horrors\" by Ruth De Forest Lamb, as being one of the key catalysts for increased government regulation over food and drugs in the United States which led to the passage in 1938 of the Federal Food, Drug, and Cosmetic Act.\n\nThe book was also the subject of strong opposition from several quarters—not just vested interests such as the drug companies, but also from the medical fraternity. The book was frequently criticized for being sensational propaganda, and many at the time questioned the credentials of the authors (both engineers) and the accuracy of the claims. Several professionals in the medical industry also pointed to the unscientific and spurious conclusions reached, with one commenting on \"data fantastically exploited and erroneously interpreted,\" \"extreme and unrealistic conclusion[s]\" and \"authors with technical qualifications more pronounced in the art of sensationalism than [in] the sciences of biology, chemistry, or public health.\"\n\nToday, many of the authors' scientific conclusions are indeed thought to have been mistaken. For example, they claimed that bran (roughage) has many negative effects on the intestine, which contradicts today's view that bran in moderation is beneficial to the intestines.\n\nHowever, it still remains an influential book on the topic of consumer affairs. Nearly forty years later, in 1972, John G. Fuller published his expose of the food, drug and cosmetic industries, honoring Kallet and Schlink by entitling his book \"200,000,000 Guinea Pigs: New Dangers in Everyday Foods, Drugs and Cosmetics\". In the Introduction, Fuller wrote, \"Today, nearly forty years later, the situation is worse, not better. … Time bombs are ticking away in several dark corners. … It is 1933 all over again—multiplied by logarithms. The difference is only a matter of form.\"\n\n"}
{"id": "723162", "url": "https://en.wikipedia.org/wiki?curid=723162", "title": "Abortion in Sweden", "text": "Abortion in Sweden\n\nAbortion in Sweden was first legislated by the Abortion Act of 1938. This stated that an abortion could be legally performed in Sweden upon medical, humanitarian, or eugenical grounds. That is, if the pregnancy constituted a serious threat to the woman's life, if she had been impregnated by rape, or if there was a considerable chance that any serious condition might be inherited by her child, she could request an abortion. The law was later augmented in 1946 to include socio-medical grounds and again in 1963 to include the risk of serious fetal damage. A committee investigated whether these conditions were met in each individual case and, as a result of this prolonged process, abortion was often not granted until the middle of the second trimester. As such, a new law was created in 1974, stating that the choice of an abortion is entirely up to the woman until the end of the eighteenth week. \n\nThe current legislation is the Abortion Act of 1974 (SFS 1974:595). This states that up until the end of the eighteenth week of the pregnancy, the choice of an abortion is entirely up to the woman, for any reason whatsoever. After the 18th, a woman needs a permission from the National Board of Health and Welfare (\"Socialstyrelsen\") to have an abortion. Permission for these late abortions is usually granted for cases in which the fetus or mother are unhealthy. Abortion is not allowed if the fetus is viable, which generally means that abortions after the 22nd week are not allowed. However, abortions after the 22nd week may be allowed in the rare cases where the fetus can not survive outside the womb even if it is carried to term.\n\nThe issue is largely settled in Sweden, and the question of the legality of abortion is not a highly controversial political issue.\n\nConsensus in Sweden is in favour of preventing unwanted pregnancies by the use of birth control and the primary goal is not to lower the amount of abortions, but rather the goal is that all children that are born should be wanted. The number of abortions statistically follows the number of pregnancies. In comparison with the other nordic countries, Sweden ranks high in number of abortions, and low in number of young parents, while the number of pregnancies in relation to total population is largely the same in all nordic countries.\n\nThe first law on legal abortions was passed in Sweden in 1938 when the law legalized abortion on a very limited scale, and only on serious medical consideration, after evaluation by the Royal Board of Health. From 1946 abortions could also be permitted on social medicinal grounds. During the 1960s, a successive change in Swedish society took place, and the general attitude towards sexuality, as well as abortion, became more liberal. This, among other things, led to an increase in the number of permitted abortions. \n\nThe current Abortion Act (SFS 1974:595 with later amendments in 1995 and 2007) entered into force on 1 January 1975. It permits abortion on the request of the pregnant woman until the 18th week, and thereafter only in cases of severe indications of medical risk. After the 18th week, abortions can only be performed after an evaluation by the National Board of Health and Welfare.\n\nIn 1989, the Board issued general advice on implementation of the law (SOSFS 1989:6). From 1 September 2004, these were superseded by new advice and policy (SOSFS 2004:4).\n\nSince 1 January 2008, foreign women - including asylum applicants, non-permanent residents, and those not registered in Sweden - are allowed to get an abortion in the country. 132 such abortions were performed in Sweden during 2009. The National Board of Health and Welfare called this a comparably small figure, in relation to the total number of abortions.\n\nThe National Board of Health and Welfare is the central national authority for social services, public health, and the health services in Sweden. Among the board's responsibilities are evaluation and monitoring of abortions performed in Sweden, as well as establishing norms by issuing provisions and general advice. The board is also responsible for the collection and publishing of official national statistics on abortions. Until 1995 reports were instead published by Statistics Sweden. \n\nStatistical reports are published yearly and are based on data from all clinics and hospitals where abortions are performed. Data is collected on the age of the women, earlier pregnancies and abortions, the length of the pregnancy at the time of abortion, method of abortion, and where the abortion was performed.\n\nOne of the National Health Board's main purposes with these reports is to measure changes and trends over time. The statistics on legal abortions stretches back to 1955 and, starting from 1975, data on frequencies for different age groups are available. From 1985 the women's home municipality was also recorded.\n\nThe number of induced abortions performed in Sweden rose markedly on a yearly basis from the early 1960s, but soon leveled off following the liberalization of the abortion law in 1975. It is not possible to tell whether the increase in the statistics after the Abortion Act of 1974 reflects actual circumstances, or just bias resulting from an increased will to report abortions after legalization. Since 1975, the total yearly number of cases has averaged between 30,000 and 38,000 abortions. \nThe number of abortions by age group were as follows: those performed on teenagers in 1975 were 30 in every 1,000, while those performed on women aged 20 to 24 years old was 27 in every 1,000. However, since 1977, the opposite has held true, with fewer abortions being performed on teenagers than women aged 20 to 24. The number of abortions among teenagers was around 12 per 1,000 women in 2017, a halving since 2006.\n\nMost abortions in Sweden are performed on women aged 25–29 years old, followed in order by the age groups 20–24 years old, 30–34 years old, 35–39 years old, 15–19 years old (teenage abortions), and 40–44 years old. Before the age of thirty most women have not established a family life and abortion is more common amongst this age group, with multiple sex partners in the younger age groups parenthood is less desired and abortion more likely. The fact that most women in the younger age groups are still studying, combined with them being new on the labour market, influences the choice to perform abortion.\n\nAlthough abortion rates vary widely in Sweden, according to geographical region, the highest rate of teenage abortions is registered in Gotland and in the metropolitan areas of Stockholm and Gothenburg. The lowest incidences are in the counties of Blekinge, Kronoberg, and Jönköping.\n\nIn 2017, 84 percent of the induced abortions were performed before the end of the 9th week of pregnancy and 55 percent before the end of the 7th week, compared to 45 and 4 percent respectively in 1991. The proportion of medical abortions constituted 93 percent of all abortions.\n\n\n"}
{"id": "21020731", "url": "https://en.wikipedia.org/wiki?curid=21020731", "title": "Albanian Red Cross", "text": "Albanian Red Cross\n\nThe Albanian Red Cross (Albanian: \"Kryqi i Kuq Shqiptar, KKSH\"), or ARC, is the national society member of the International Federation of Red Cross and Red Crescent Societies for Albania. The oldest humanitarian organization in Albania, it was founded on October 4, 1921, and was officially recognized by the Red Cross and Red Crescent Movement in 1923. Its headquarters are located in the Albanian capital of Tirana, and its 39 branches provide humanitarian assistance across the country in accordance with the Fundamental Principles.\n\nThe first activity of the Red Cross Movement in Albania was in 1920, when aid from the American Red Cross was distributed at the rear of the Albanian Army. On October 4, 1921, the Albanian Red Cross Society was founded and the organization's first statutes were accepted in 1922. In its first year, it began publishing a magazine, took over the operations of an orphanage, and assisted refugees from the surrounding region. In 1923, the International Committee of the Red Cross officially recognized the Albanian Red Cross, making it the 38th national society.\n\nThroughout the second half of the 1920s and the 1930s, the Albanian Red Cross expanded its coverage across Albania. It opened a nursing school, training nurses to serve the poorest and most vulnerable of the population. With the French Red Cross, it established counseling and milk distribution programs for undernourished children. During this period, the organization's activities were funded by charitable donations from the Albanian people, with significant support coming from the Albanian diaspora, particularly from the United States.\n\nDuring World War II, the main activities of the Albanian Red Cross was at the Greco-Italian War front. It established and operated a field hospital at the front, and also opened an office to assist displaced populations in searching for missing family members. In the immediate post-war years, the Albanian Red Cross distributed food, clothing, and cash to those struggling to rebuild. Shelters were set up in major cities for the elderly.\n\nAs the new Communist government began its socialist reconstruction of the country, the activities of the Albanian Red Cross adapted to fit this new direction. The late 1940s were a period of rapid industrialization, and the Albanian Red Cross began teaching first aid courses at workplaces and construction sites, opening a hospital to treat workers building the Durrës-Elbasan railway. It also opened two nurseries to aid working women, five orphanages, and a nursing home.\n\nIn the 1950s and 1960s, the Albanian Red Cross slowly began to be influenced by other organizations of the same name in Eastern Europe that had become \"de facto\" agents of government propaganda for national health programs rather than independent humanitarian organizations as the statutes and principles of the Red Cross Movement intended. By 1969, the majority of the Albanian Red Cross activities were carried out by the state, which resulted in the halt of all practical programs.\n\nFrom 1969-1990, the Albanian Red Cross did not operate.\n\nIn 1991, the Albanian Red Cross was reborn as a new organization and began again operating. The Albanian government ratified the Geneva Protocols in 1993, and in 1994, the Albanian Red Cross was granted official status. It has since grown into the largest humanitarian organization in Albania and has developed its capacity in multiple areas.\n\nThe Albanian Red Cross was recognized by the Albanian Parliament as an independent volunteer humanitarian organization under Law No. 7864 on September 29, 1994. As a member of the International Red Cross and Red Crescent Movement, it is guided by the Fundamental Principles and the Geneva Conventions. It is the sole Red Cross society for Albania. The Albanian government may grant budgetary funds to the Albanian Red Cross to carry out specific humanitarian activities. It does not pay tax.\n\n"}
{"id": "8228980", "url": "https://en.wikipedia.org/wiki?curid=8228980", "title": "American Student Dental Association", "text": "American Student Dental Association\n\nThe American Student Dental Association (ASDA) is a national student-run organization that attempts to protect and advance the rights, interests, and welfare of students pursuing careers in dentistry. It introduces students to lifelong involvement in organized dentistry and provides services, information, education, recreation, representation and limited amounts of advocacy.\n\nASDA was established to connect, support and advance the needs of dental students. ASDA represents 90 percent of all students from 66 U.S. dental schools. Since 2011, dental student membership has averaged more than 19,000. ASDA also welcomes hundreds of predental students each year.\n\nIn 1969, university students across the country staged demonstrations to protest the Vietnam War, restrictive school policies, dress codes and more. Dental school admission criteria became more selective and the competition for acceptance into dental schools increased. The resulting student profile was a brighter, more socially aware individual with diverse interests and talents.\n\nHowever, at this time dental school could be likened to boot camp. In fact, many instructors were indeed retired military officers. Students were told how long to wear their hair and sideburns. Some schools even had fingernail inspections. Only a handful of women and minority students could be found in dental school. To make matters worse, no system of due process existed, which meant students could be expelled with no available recourse for help. With a multitude of issues building, the solution presented itself-dental students needed to organize.\n\nThat year the federal government offered the Student American Medical Association (SAMA) a $1 million grant to coordinate student involvement in the Appalachia Project and the American Indian Health Program. Dentistry was the only health care discipline without its own national student organization. In order to receive the grant for the project, SAMA needed dental students to organize.\n\nThe presidents of SAMA (now known as the American Medical Student Association) and the Student American Pharmacy Association both attended the University of California at San Francisco. They approached Dennis Spain, a third year student at the university's dental school, to start a national association for dental students.\n\nIn January 1970, at a SAMA conference that included professional students of all disciplines, Spain met David Evaskus, a fourth year dental student at the University of Illinois. Impressed with the scope and depth of the projects in which the other student organizations were involved, Spain and Evaskus returned to their schools and began contacting dental school deans and students across the country.\n\nStudents held a meeting in Chicago Feb. 14-15, 1970, just prior to the midwinter meeting of the Chicago Dental Society. Although records indicate that 45 students from 26 dental schools participated, more dental schools may have been unofficially represented because several students came to the meeting without the approval of their schools' administrations. This assembly represented a new generation of dental students.\n\nThis new brand of leaders formed the Student American Dental Association (SADA), a national organization that would recruit students to serve in federal health care projects, function as an information clearinghouse on local student issues, coordinate student lobbying efforts and establish and promote student positions on professional issues. Members elected Dennis Spain, the event organizer, as board president. The newly established SADA planned to hold its first national convention and House of Delegates meeting in New York in October 1970.\n\nSADA wanted to maintain its independence but was unable to secure funding to support its planned national convention. At the same time, the American Dental Association began developing its own plans for student affairs to channel student requests to the appropriate ADA departments. Approved at the 1970 ADA Annual Session, one of the first activities was to help organize a dental student convention, which was later held Feb. 8-9, 1971.\n\nThe culmination of the conference was the formation of the American Student Dental Association (ASDA), a new national student dental organization. Its first president and other key officers were former SADA leaders. While this new organization had the ADA's approval and support, ASDA's guiding principles and leaders descended directly from the original student organization, SADA.\n\nIn its first year, ASDA participated in minority student recruitment, migratory worker health programs in several states and Indian Health Service programs. In addition, the association published a monthly newsletter and held regional conferences on issues concerning public health.\n\nOne of ASDA's earliest activities was the development of an advocacy program to respond to students' requests for support and assistance. ASDA's advocacy program helped students by either directing them to local sources of assistance, forwarding cases to the ADA Commission on Dental Accreditation, or simply shedding light on injustices through ASDA's newsletter.\n\nIn 1986, ASDA established its Political Education Network, composed of students who coordinate legislative activities and lobbying efforts at their dental schools. PEN monitored state and national legislative activities and organized political action at the grassroots level, such as letter-writing campaigns and voter registration drives. ASDA initiated its political advocacy network of dental students nearly 10 years before the\nADA created its grassroots network for dentists. In the fall of 2000, ASDA changed PEN's name to the Legislative Grassroots Network (LGN) to better reflect its purpose and efforts.\n\nASDA is the largest national organization solely dedicated to dental student concerns. Structured as a network of chapters based at each of the 60+ dental schools in the United States and Puerto Rico, ASDA is uniquely geared to respond to its members at the local, regional and national levels.\n\nTwo delegates at each dental school chapter serve as voting members of the ASDA House of Delegates. Their role is to voice the concerns of their constituents to the House of Delegates for action in the form of resolutions. As a result of passed resolutions, ASDA publishes policy statements on several issues, including dental education, licensure, the rise in tuition cost for dental students, dental research, midlevel providers and education financing. ASDA notifies groups or individuals, such as dental school deans, dental associations, state boards of dentistry and lawmakers, of its position on particular issues.\n\nIn 1999, ASDA and the American Dental Education Association hosted the first National Dental Student Lobby Day. Each year since then, students have gathered in Washington, D.C. to actively lobby members of Congress. This unique experience provides ASDA members an opportunity to meet with their legislators and advocate for their profession.\n\nRecent issues that ASDA has lobbied for:\n\nASDA supports the universal acceptance of all regional and state clinical licensing examinations. ASDA is leading the charge for the elimination of live patient exams to protect the public and ethical integrity of the profession. Our efforts to reach out to\nstate dental associations to encourage them to adopt similar policies have been successful.\n\nIn 2010, bill AB 1524 was passed in California recognizing licensure by portfolio. This innovative approach to licensure opens up opportunities in other states. The ADA has created a workgroup to develop a portfolio-style examination for the purpose of initial dental licensure. An ASDA representative is a member of the task force assigned to this project.\n\nOral health can only be of benefit to those who have the ability to access it. In America and beyond, barriers to care remain. Whether it's financial barriers, disabilities, geographical isolation or their dental health is relatively forgotten under a myriad of other health needs, patients are left with serious conditions that threaten their overall health and quality of life. These patients may need transportation, oral health education or financial incentives in order to access their dental needs. Often education, language, cultural or ethnic barriers only add to their unfortunate situation.\n\nOne potential solution is the expanded function of dental auxiliaries. ASDA endorses expanded functions for dental auxiliaries only when each has received the appropriate education and training to guarantee competence with proper supervision of a trained dentist, and when such functions fall within the laws established by their respective state of employment. Because of the comprehensive education and training requirements for dentists, ASDA believes that only dentists should perform irreversible procedures and prescribe medications.\n\nASDA is governed by 130 delegates (two students from each of its 65 dental school chapters). Chapters are grouped into 11 districts, each guided by an elected trustee. Each year, the delegates elect a president, two vice presidents and a Speaker of the House of Delegates. ASDA also has councils that guide the organization's work in a number of key areas.\n\nA resolution is a formal request or action that is presented to the House of Delegates for consideration. Delegates present discussion for or against a specific resolution, then the house votes to determine the outcome.\n\nASDA has grown into a strong, well-respected and influential organization. Today more than 88 percent of all dental students join the association, thanks in part to automatic enrollment at many schools. ASDA will continue attempting to provide its members the support and services they need to meet the future with confidence.\n\n"}
{"id": "25258698", "url": "https://en.wikipedia.org/wiki?curid=25258698", "title": "Apical foramen", "text": "Apical foramen\n\nIn anatomy the apical foramen is the opening at the apex of the root of a tooth, through which the nerve and blood vessels that supply the dental pulp pass. Thus it represents the junction of the pulp and the periodontal tissue.\n\nThe average size of the orifice is 0.3 to 0.4 mm in diameter. There can be two or more foramina separated by a portion of dentin and cementum or by cementum only. If more than one foramen is present on each root, the largest one is designated as the apical foramen and the rest are considered accessory foramina.\n\nIt is a point of interest in endodontics, as it is considered necessary to thoroughly chemomechanically debride the pulp space to remove all necrotic tissue and minimise bacterial load in the pulp space. Ideally this debridement would terminate exactly at the apical foramen. In reality determining the exact position of the apical foramen is problematic, requiring radiography and/or use of an electronic apex locator to produce a refined estimate. A tooth may have multiple small accessory canals in the root apex area forming an apical delta which can complicate the endodontic problem.\n\nAn apical constriction is often present. In immature teeth the root is not fully formed leading to an open apex. This is also seen in some pathological teeth.\n\n"}
{"id": "3283554", "url": "https://en.wikipedia.org/wiki?curid=3283554", "title": "Apicoectomy", "text": "Apicoectomy\n\nA root end surgery, also known as apicoectomy (\"apico-\" + \"-ectomy\"), root resection, retrograde root canal treatment (\"c.f.\" orthograde root canal treatment) or root-end filling, is an endodontic surgical procedure whereby a tooth's root tip is removed and a root end cavity is prepared and filled with a biocompatible material.\n\nMicrosurgical endodontics—dental surgery using a microscope—may be performed.\n\nThis is usually necessitated when a conventional root canal therapy had failed and a re-treatment was already unsuccessful or is not advised. State-of-the-art procedures make use of microsurgical techniques, such as a dental operating microscope, micro instruments, ultrasonic preparation tips and calcium-silicate based filling materials.\n\nRemoval of the root tip is indicated to remove the entire apical delta ensuring no uncleaned missed anatomy.\n\nExtraction may be the only alternative. Where necessary prosthetic replacement with a denture, dental bridge or dental implant may be considered.\n\nThe primary aim of any endodontic treatment is to disinfect the root canal system in order to reduce the bacterial load as much as possible, and to seal the system to prevent ingress or egress of bacteria or their byproducts. Failure is often due to leakage, and therefore any materials used to seal the end of the root must provide a good seal. It is also important that they are biocompatible; that is, that they are non-carcinogenic and non-toxic to the surrounding tissues or the body as a whole. They must also be stable in moisture and at body temperature. It is beneficial if they are easy to handle, as they are placed in small amounts under technically demanding conditions, and if they are easily identified on radiographs (i.e. radio-opaque).\n\nBelow is a list of some of the commonly used root-end filling materials. This list is by no means exhaustive.\n\nAmalgam is widely used as a root-end filling, and meets many of the desired criteria. It is easy to handle, easy to see on radiographs, not sensitive to moisture, and stable at body temperature. Amalgam provides a relatively good seal if placed correctly. There have been some concerns about toxicity, as amalgam contains mercury as an ingredient, but there is very little evidence to support these.\n\nComposite resin is commonly used as a filling material due to its aesthetic qualities and ability to effectively bond to tooth structure, especially enamel. It is less commonly used as a root-end filling material, as its placement is technique sensitive, particularly to moisture. Moisture contamination will result in a weakened bond that is very susceptible to leakage and subsequent failure. There is some evidence that, when placed correctly, composite resin can produce high success rates.\n\nMTA is a cement containing mineral oxides which absorb water to form a colloidal gel, which solidifies over a period of approximately 4 hours. It has proven very popular as a root-end filling material and has shown generally high success rates. MTA produces a high pH environment, which is bactericidal, and may stimulate osteoblasts to produce bone to fill in any defects caused by infection.\n\nModified versions of ZOE cement, such as IRM or Super EBA, have high compressive strength, high tensile strength, neutral pH, and low solubility.\n\nReported success rates for apicoectomy vary widely. Studies generally focus on one material or method of treatment compared to another, so it can be difficult to obtain any good evidence on the overall success rate. A meta-analysis published in 2010 indicated an overall success rate of 85-95% for surgical endodontic treatment using a modern technique, with the evidence level rated high. A similar systematic review published in 2009 suggested an overall success rate of 77.8% for surgical endodontic treatment at 2–4 years, falling to 71.8% at 4–6 years, and 62.9% at 6+ years. There are many factors which will affect the likelihood of success of apicoectomy. If performed correctly, it can be highly successful in preventing loss of teeth which would otherwise be extracted.\n"}
{"id": "14527587", "url": "https://en.wikipedia.org/wiki?curid=14527587", "title": "Average treatment effect", "text": "Average treatment effect\n\nThe average treatment effect (ATE) is a measure used to compare treatments (or interventions) in randomized experiments, evaluation of policy interventions, and medical trials. The ATE measures the difference in mean (average) outcomes between units assigned to the treatment and units assigned to the control. In a randomized trial (i.e., an experimental study), the average treatment effect can be estimated from a sample using a comparison in mean outcomes for treated and untreated units. However, the ATE is generally understood as a causal parameter (i.e., an estimate or property of a population) that a researcher desires to know, defined without reference to the study design or estimation procedure. Both observational studies and experimental study designs with random assignment may enable one to estimate an ATE in a variety of ways.\n\nOriginating from early statistical analysis in the fields of agriculture and medicine, the term \"treatment\" is now applied, more generally, to other fields of natural and social science, especially psychology, political science, and economics such as, for example, the evaluation of the impact of public policies. The nature of a treatment or outcome is relatively unimportant in the estimation of the ATE—that is to say, calculation of the ATE requires that a treatment be applied to some units and not others, but the nature of that treatment (e.g., a pharmaceutical, an incentive payment, a political advertisement) is irrelevant to the definition and estimation of the ATE.\n\nThe expression \"treatment effect\" refers to the causal effect of a given treatment or intervention (for example, the administering of a drug) on an outcome variable of interest (for example, the health of the patient). In the Neyman-Rubin \"Potential Outcomes Framework\" of causality a treatment effect is defined for each individual unit in terms of two \"potential outcomes.\" Each unit has one outcome that would manifest if the unit were exposed to the treatment and another outcome that would manifest if the unit were exposed to the control. The \"treatment effect\" is the difference between these two potential outcomes. However, this individual-level treatment effect is unobservable because individual units can only receive the treatment or the control, but not both. Random assignment to treatment ensures that units assigned to the treatment and units assigned to the control are identical (over a large number of iterations of the experiment). Indeed, units in both groups have identical distributions of covariates and potential outcomes. Thus the average outcome among the treatment units serves as a counterfactual for the average outcome among the control units. The differences between these two averages is the ATE, which is an estimate of the central tendency of the distribution of unobservable individual-level treatment effects. If a sample is randomly constituted from a population, the ATE from the sample (the SATE) is also an estimate of the population ATE (or PATE).\n\nWhile an experiment ensures, in expectation, that potential outcomes (and all covariates) are equivalently distributed in the treatment and control groups, this is not the case in an observational study. In an observational study, units are not assigned to treatment and control randomly, so their assignment to treatment may depend on unobserved or unobservable factors. Observed factors can be statistically controlled (e.g., through regression or matching), but any estimate of the ATE could be confounded by unobservable factors that influenced which units received the treatment versus the control.\n\nIn order to define formally the ATE, we define two potential outcomes : formula_1 is the value of the outcome variable for individual formula_2 if he is not treated, formula_3 is the value of the outcome variable for individual formula_2 if\nhe is treated. For example, formula_1 is the health status of the individual if he is not administered the drug under study and formula_3 is the health status if he is administered the drug.\n\nThe treatment effect for individual formula_2 is given by formula_8. In the general case, there is no reason to expect this effect to be constant across individuals.\n\nLet formula_9 denote the expectation operator for any given variable (that is, the average value of the variable across the whole population of interest). The Average treatment effects is given by: formula_10.\n\nIf we could observe, for each individual, formula_3 and formula_1 among a large representative sample of the population, we could estimate the ATE simply by taking the average value of formula_13 for the sample: formula_14 (where formula_15 is the size of the sample).\n\nThe problem is that we can not observe both formula_3 and formula_1 for each individual. For example, in the drug example, we can only observe formula_3 for individuals who have received the drug and formula_1 for those who did not receive it; we do not observe formula_1 for treated individuals and formula_3 for untreated ones. This fact is the main problem faced by scientists in the evaluation of treatment effects and has triggered a large body of estimation techniques.\n\nDepending on the data and its underlying circumstances, many methods can be used to estimate the ATE. The most common ones are\n\nOnce a policy change occurs on a population, a regression can be run controlling for the treatment. The resulting equation would be\nwhere y is the response variable and formula_23 measures the effects of the policy change on the population.\n\nThe difference in differences equation would be\nwhere T is the treatment group and C is the control group. In this case the formula_23 measures the effects of the treatment on the average outcome and is the average treatment effect.\n\nFrom the diffs-in-diffs example we can see the main problems of estimating treatment effects. As we can not observe the same individual as treated and non-treated at the same time, we have to come up with a measure of counterfactuals to estimate the average treatment effect.\n\nConsider an example where all units are unemployed individuals, and some experience a policy intervention (the treatment group), while others do not (the control group). The causal effect of interest is the impact a job search monitoring policy (the treatment) has on the length of an unemployment spell: On average, how much shorter would one's unemployment be if they experienced the intervention? The ATE, in this case, is the difference in expected values (means) of the treatment and control groups' length of unemployment.\n\nA positive ATE, in this example, would suggest that the job policy increased the length of unemployment. A negative ATE would suggest that the job policy decreased the length of unemployment. An ATE estimate equal to zero would suggest that there was no advantage or disadvantage to providing the treatment in terms of the length of unemployment. Determining whether an ATE estimate is distinguishable from zero (either positively or negatively) requires statistical inference.\n\nBecause the ATE is an estimate of the average effect of the treatment, a positive or negative ATE does not indicate that any particular individual would benefit or be harmed by the treatment. Thus the average treatment effect neglects the distribution of the treatment effect. Some parts of the population might be worse off with the treatment even if the mean effect is positive.\n"}
{"id": "6534029", "url": "https://en.wikipedia.org/wiki?curid=6534029", "title": "C. G. Jung Institute, Zürich", "text": "C. G. Jung Institute, Zürich\n\nThe C. G. Jung Institute, Zürich (German: C. G. Jung-Institut Zürich) was founded in Küsnacht, Switzerland, in 1948 by the psychiatrist Carl Gustav Jung, the founder of Analytical psychology (more commonly called Jungian psychology). Marie-Louise von Franz and Jolande Jacobi were also active in the foundation and early work of the institute.\n\nThe institute was founded in 1948 to provide training and conduct research in Analytical psychology and psychotherapy. Jung led the institute until 1961, the year of his death. The library of the institute holds around 15,000 books and periodicals related to Jungian psychology.\n\nSeveral other organizations named the C.G. Jung Institute exist around the world, e.g. in Los Angeles.\n\n"}
{"id": "584454", "url": "https://en.wikipedia.org/wiki?curid=584454", "title": "Cardiotocography", "text": "Cardiotocography\n\nCardiotocography (CTG) is a technical means of recording the fetal heartbeat and the uterine contractions during pregnancy. The machine used to perform the monitoring is called a cardiotocograph, more commonly known as an electronic fetal monitor (EFM).\n\nFetal monitoring was invented by Doctors Alan Bradfield, Orvan Hess and Edward Hon. A refined (antepartal, non-invasive, beat-to-beat) version (cardiotocograph) was later developed for Hewlett Packard by Konrad Hammacher.\n\nCTG monitoring is widely used to assess fetal wellbeing. A review found that in the antenatal period (before labour) there is no evidence to suggest that monitoring women with high-risk pregnancies benefits the mother or baby although research around this is old and should be interpreted with caution. The same review found that computerised CTG machines resulted in lower numbers of baby deaths than the traditional CTG machines (as shown in picture). More up-to-date research is needed to provide more information around this practice.\n\nCTG monitoring can sometimes lead to medical interventions which are not necessarily needed. Fetal vibroacoustic stimulation (sound played to the unborn baby through the mother’s abdomen) has been used to provoke the baby into being more active. This can improve their CTG monitoring so that the mother does not have to be monitored for as long. However the safety of this technique has not been fully assessed; hearing impairment, stress reactions and other effects should be investigated before this technique is used widely.\n\nExternal cardiotocography can be used for continuous or intermittent monitoring. The fetal heart rate and the activity of the uterine muscle are detected by two transducers placed on the mother’s abdomen (one above the fetal heart, to monitor heart rate and the other at the fundus of the uterus to measure frequency of contractions). Doppler ultrasound provides the information which is recorded on a paper strip known as a cardiotocograph (CTG). External tocometry is useful in showing the beginning and end of contractions, as well as frequency, but not the strength of contractions. The absolute values of pressure readings on an external tocometer are dependent on position, and are not sensitive in people who are obese. In cases where information on the strength, or precise timing, of contractions is needed, an internal tocometer is more appropriate.\n\nInternal cardiotocography uses an electronic transducer connected directly to the fetal scalp. A wire electrode is attached to the fetal scalp through the cervical opening and is connected to the monitor. This type of electrode is sometimes called a spiral or scalp electrode. Internal monitoring provides a more accurate and consistent transmission of the fetal heart rate than external monitoring because factors such as movement do not affect it. Internal monitoring may be used when external monitoring of the fetal heart rate is inadequate, or closer surveillance is needed. Internal tocometry can only be used if membranes (fore-waters) have ruptured either spontaneously or artificially, and the cervix is open. To gauge the strength of contractions, a small catheter (Intrauterine pressure catheter or IUPC) is passed into the uterus, past the fetus. Combined with an internal fetal monitor, an IUPC may give a more precise reading of the baby's heart rate and the strength of contractions.\n\nA typical CTG reading is printed on paper and/or stored on a computer for later reference. A variety of systems for centralized viewing of CTG have been installed in a large number of maternity hospitals in industrialised countries, allowing simultaneous monitoring of multiple tracings in one or more locations. Display of maternal vital signs, ST signals and an electronic partogram are available in the majority of these systems. A few of them have incorporated computer analysis of cardiotocographic signals or combined cardiotocographic and ST data analysis.\n\nIn the US, the Eunice Kennedy Shriver National Institute of Child Health and Human Development sponsored a workshop to develop a standardized nomenclature for use in interpreting intrapartum fetal heart rate and uterine contraction patterns. This nomenclature has been adopted by the Association of Women’s Health, Obstetric, and Neonatal Nurses (AWHONN), the American College of Obstetricians and Gynecologists (ACOG), and the Society for Maternal-Fetal Medicine.\n\nThe Royal College of Obstetricians and Gynaecologists and the Society of Obstetricians and Gynaecologists of Canada have also published consensus statements on standardized nomenclature for fetal heart rate patterns.\n\nInterpretation of a CTG tracing requires both qualitative and quantitative description of:\n\nThere are several factors used in assessing uterine activity. \n\nThe NICHD nomenclature defines uterine activity by quantifying the number of contractions present in a 10-minute window, averaged over 30 minutes. Uterine activity may be defined as:\n\nThe NICHD nomenclature defines baseline fetal heart rate as:\nThe baseline FHR is determined by approximating the mean FHR rounded to increments of 5 beats per minute (bpm) during a 10-minute window, excluding accelerations and decelerations and periods of marked FHR variability (greater than 25 bpm). There must be at least 2 minutes of identifiable baseline segments (not necessarily contiguous) in any 10-minute window, or the baseline for that period is indeterminate. In such cases, it may be necessary to refer to the previous 10-minute window for determination of the baseline. Abnormal baseline is termed \"bradycardia\" when the baseline FHR is less than 110 bpm; it is termed \"tachycardia\" when the baseline FHR is greater than 160 bpm.\n\nModerate baseline fetal heart rate variability reflects the delivery of oxygen to the fetal central nervous system. Its presence is reassuring in predicting an absence of metabolic acidemia and hypoxic injury to the fetus at the time it is observed. In contrast, the presence of mild baseline FHR variability, or an absence of FHR variability does not reliably predict fetal acidemia or hypoxia; lack of moderate baseline FHR variability may be a result of the fetal sleep cycle, or a result of medications, extreme prematurity, congenital anomalies, or pre-existing neurological injury.\n\nThe NICHD nomenclature defines baseline FHR variability as:\nBaseline FHR variability is determined in a 10-minute window, excluding accelerations and decelerations. Baseline FHR variability is defined as fluctuations in the baseline FHR that are irregular in amplitude and frequency. The fluctuations are visually quantitated as the amplitude of the peak-to-trough in beats per minute. Using this definition, the baseline FHR variability is categorized by the quantitated amplitude as:\n\nThe NICHD nomenclature defines an acceleration as a visually apparent abrupt increase in fetal heart rate. An abrupt increase is defined as an increase from the onset of acceleration to the peak in less than or equal to 30 seconds. To be called an acceleration, the peak must be greater than or equal to 15 bpm, and the acceleration must last greater than or equal to 15 seconds from the onset to return to baseline.\nA \"prolonged acceleration\" is greater than or equal to 2 minutes but less than 10 minutes in duration. \nAn acceleration lasting greater than or equal to 10 minutes is defined as a baseline change. \nBefore 32 weeks of gestation, accelerations are defined as having a peak greater than or equal to 10 bpm and a duration of greater than or equal to 10 seconds.\n\nPeriodic refers to decelerations that are associated with contractions; episodic refers to those not associated with contractions. There are four types of decelerations as defined by the NICHD nomenclature, all of which are visually assessed.\n\nAdditionally, decelerations can be \"recurrent\" or \"intermittent\" based on their frequency (more or less than 50% of the time) within a 20 min window.\n\nBefore 2008, fetal heart rate was classified as either \"reassuring\" or \"nonreassuring\". The NICHD workgroup proposed terminology of a three-tiered system to replace the older, undefined terms.\n\n\nFIGO has recently modified the guidelines on intrapartum fetal monitoring, proposing following interpretation: \n\nAccording to the Cochrane review from the February 2017, CTG was associated with fewer neonatal seizures but it is unclear if it had any impact on long-term neurodevelopmental outcomes. No clear differences in cerebral palsy, infant mortality or other standard measures of neonatal wellbeing, neither on any meaningful long-term outcomes could be shown. Continuous CTG was associated with the higher rates of caesarean sections and instrumental vaginal births. The authors see the challenge in how to discuss these results with women to enable them to make an informed decision without compromising the normality of labour. Future research should focus on events that happen in pregnancy and labour that could be the cause of long term problems for the baby.\n\n"}
{"id": "12780557", "url": "https://en.wikipedia.org/wiki?curid=12780557", "title": "CentraCare Health", "text": "CentraCare Health\n\nCentraCare Health is an integrated health care system in Central Minnesota. The nonprofit includes six hospitals, seven senior care facilities, 18 clinics, four pharmacies and numerous inpatient and outpatient specialty care services.\n\nFounded in 1886 by the Sisters of the Order of St. Benedict, St. Cloud Hospital is a Catholic, not-for-profit hospital located in St. Cloud, Minnesota. It offers inpatient and outpatient services, including care for heart disease and cancer, preventive health screenings and behavioral health services.\n\nThe teaching hospital employs more than 4,900 staff, 450 physicians and 1,000 volunteers. It serves 690,000 people in the surrounding 12-county area and ranks among the 100 TOP Hospitals nationwide according to Truven Health Analytics, a national healthcare rating agency.\n\n\nCentraCare Health — Long Prairie serves Todd County, Minnesota and employs more than 250 staff (2014). The Minnesota Department of Health certified CentraCare Clinic — Long Prairie as a designated health care home site through November 2015 and accredited Long Prairie Hospital as an Acute Stroke Ready Hospital.<ref name=\"Minnesota Stroke System: Designated Hospitals (Updated 3/30/2015)\"></ref> CentraCare Health — Long Prairie's long-term care facility was also among the 10 percent of Minnesota care centers awarded five stars on the Centers for Medicare and Medicaid Services (CMS) Nursing Home Compare rating system.\n\n\nThis network serves 10,000 residents in and around western Stearns County, Minnesota and employs more than 280 staff. The Diabetes Self-Management Education (DSME) program at CentraCare Health — Melrose is certified by the American Diabetes Association, and Melrose Clinic is a health care home-certified site through November 2015, per Minnesota Department of Health guidelines.\n\n\nCentraCare Health — Monticello has been recognized for its Diabetes Self-Management Education (DSME) program with certification from the American Diabetes Association (ADA), as well as by Minnesota Bridges to Excellence for its optimal vascular improvement efforts. In 2014, The American Heart Association recognized CentraCare Health — Monticello and eight other Minnesota hospitals with the Get With The Guidelines - Heart Failure Silver Quality Achievement Award. Monticello Care Center was also among the 10 percent of Minnesota care centers awarded five stars on the Centers for Medicare and Medicaid Services (CMS) Nursing Home Compare rating system.\n\n\nCentraCare Health — Sauk Centre serves 10,000 residents in and around western Stearns County, Minnesota and employs more than 230 staff. In 2014, Minnesota Bridges to Excellence recognized Sauk Centre Clinic for its optimal vascular and diabetes improvement efforts, and the Minnesota Department of Health re-certified the clinic as a designated health care home site through November 2015. Additionally, Sauk Centre Care Center was among the 10 percent of Minnesota care centers to receive five stars from the Centers for Medicare & Medicaid Services Nursing Home Compare rating system, which rates facilities based on health inspections, nursing staff evaluations and performance on quality measures.\n\n\nIn 2014, Koronis Manor Care Center received the Champion of Care Award from Heartland Hospice, a national provider of home health care, hospice care, skilled nursing, memory care and post-acute care.\n\nCentraCare Clinic operates 18 clinics in Central Minnesota and employs about 1,000 staff. CentraCare Clinic includes more than 260 providers who practice 25 medical specialties and offer outreach services in 40 communities. CentraCare \"eClinic\" provides an online diagnosis option. Clinic sites include:\n\nSt. Benedict's Senior Community provides nursing services, including short stay and hospice care, as well as services for those with Alzheimer’s disease or other memory-loss conditions. Senior housing offerings include retirement, assisted living and income-based apartments. St. Benedict's has locations in both St. Cloud and Monticello, Minnesota, and care center services include therapeutic recreation, nutrition, rehabilitation services, respite, hospice and spiritual care, social services and a beauty/barber salon.\n\nIn 2014, St. Benedict's Senior Community, St. Cloud received top-tier recognition from the American Health Care Association and National Center for Assisted Living (AHCA/NCAL) Quality Initiative, a national effort focused on reducing hospital readmissions, nursing staff turnover and antipsychotic medications, as well as increasing customer satisfaction. The St. Cloud facility was also among the 10 percent of Minnesota care centers awarded five stars on the Centers for Medicare and Medicaid Services (CMS) Nursing Home Compare rating system.\n\nCentraCare Health Foundation engages the philanthropic community in partnership to improve health and health care. The Foundation accepts charitable contributions for all CentraCare Health entities. In fiscal year 2014-2015, CentraCare Health Foundation gave more than $5 million in health care grants throughout Central Minnesota.\n"}
{"id": "47349607", "url": "https://en.wikipedia.org/wiki?curid=47349607", "title": "Chlorine gas poisoning", "text": "Chlorine gas poisoning\n\nChlorine gas poisoning is illness resulting from the effects of exposure to chlorine beyond the threshold limit value.\n\nThe signs of acute chlorine gas poisoning are primarily respiratory, and include difficulty breathing and cough; listening to the lungs will generally reveal crackles. There will generally be sneezing, nose irritation,burning sensation, and throat irritation. There may also be skin irritation or chemical burns and eye irritation or conjunctivitis. A person with chlorine gas poisoning may also have nausea, vomiting, or a headache.\n\nChronic exposure to relatively low levels of chlorine gas may cause pulmonary problems like acute wheezing attacks, chronic cough with phlegm, and asthma.\n\nOccupational exposures constitute the highest risk of toxicity and common domestic exposures result from the mixing of chlorine bleach with acidic washing agents such as acetic, nitric and phosphoric acid. They also occur as a result of the chlorination of table water. Other exposure risks occur during industrial or transportation accidents. Wartime exposure is rare.\n\nHumans can smell chlorine gas at ranges from 0.1–0.3 ppm. According to a review from 2010: \"At 1–3 ppm, there is mild mucus membrane irritation that can usually be tolerated for about an hour. At 5–15 ppm, there is moderate mucus membrane irritation. At 30 ppm and beyond, there is immediate chest pain, shortness of breath, and cough. At approximately 40–60 ppm, a toxic pneumonitis and/or acute pulmonary edema can develop... Concentrations of about 400 ppm and beyond are generally fatal over 30 minutes, and at 1,000 ppm and above, fatality ensues within only a few minutes.\"\n\nThe concentration of the inhaled gas and duration of exposure and water contents of the tissues exposed are the key determinants of toxicity; moist tissues like the eyes, throat, and lungs are the most susceptible to damage.\n\nOnce inhaled, chlorine gas diffuses into the epithelial lining fluid (ELF) of the respiratory epithelium and may directly interact with small molecules, proteins and lipids there and damage them, or may hydrolyze to hypochlorous acid and hydrochloric acid which in turn generate chloride ions and reactive oxygen species; the dominant theory is that most damage is via the acids.\n\nTest performed to confirm chlorine gas poisoning and monitor patients for supportive care include pulse oximetry, testing serum electrolyte, blood urea nitrogen (BUN), and creatinine levels, measuring arterial blood gases, chest radiography, electrocardiogram (ECG), pulmonary function testing, and laryngoscopy or bronchoscopy.\n\nThere is no antidote for chlorine poisoning; management is supportive after evacuating people from the site of exposure and flushing exposed tissues. For lung damage caused by inhalation, oxygen and bronchodilators may be administered.\n\nThere is no way to predict outcomes. Most people with mild to moderate exposure generally recover fully in three to five days, but some develop chronic problems such as reactive airway disease. Smoking or pre-existing lung conditions like asthma appear to increase the risk of long term complications.\n\nIn 2014, the American Association of Poison Control Centers reported that about 6000 exposures to chlorine gas in the US in 2013, compared with 13,600 exposures to carbon monoxide, which was the most common poison gas exposure; the year before they reported about 5,500 cases of chlorine gas poisoning compared with around 14,300 cases of carbon monoxide poisoning.\n\nChlorine gas was first used as a weapon in World War I. It was used several times by insurgents in the Iraqi insurgency (2003–11), and in Syria in the 2014 Kafr Zita chemical attack.\n\nThere have been many instances of mass chlorine gas poisonings in industrial accidents. In the US, a freight train derailed in South Carolina in 2005, releasing an estimated 11,500 gallons of chlorine gas. As a result, nine people died, and at least 529 persons sought medical care. In 2004 in Texas a freight train accident released 90,000 pounds of chlorine gas and other toxic chemicals. Forty-four persons were injured, including three who died. In August 2002 in Missouri, approximately 16,900 pounds of chlorine gas were released from a railroad tanker car when a flex hose ruptured during unloading at a chemical plant. Sixty-seven persons were injured. In Nigeria, eight people died in a July 2015 explosion of a chlorine gas storage tank at a water treatment plant in Jos. In July 2017, in Iran, at least 475 people, including nine firemen, suffered respiratory and other symptoms after a chlorine gas leak in the southwestern Iranian province of Khuzestan, the chancellor of Dezful University of Medical Sciences says.\n"}
{"id": "51491107", "url": "https://en.wikipedia.org/wiki?curid=51491107", "title": "Circle of Health International", "text": "Circle of Health International\n\nCircle of Health International, known as COHI for short, is a US based non-governmental organization founded in 2004 with the mission to work with women and their communities with a community based approach in times of crisis. As of 2016, COHI has responded to eighteen humanitarian emergencies and served over three million women globally. COHI has worked with midwives and public health professionals in Sri Lanka, Louisiana, Tibet, Tanzania, Israel, the Philippines, Palestine, Jordan, Syria, Oklahoma, Nicaragua, Sudan, Haiti, and Afghanistan.\n\nAs of 2016 COHI supports maternal and child health clinics in Haiti, midwives in an indigenous women's forum in Nicaragua, midwifery students and sexual health advocates in Nepal, a clinic for refugees in the Rio Grande Valley on the Mexico/US border, and works with survivors of human trafficking globally. COHI is also engaged in Austin's social enterprise community through a program, known as the COHI Cloth Network, to address women's poverty through income generation initiatives.\n\nIn 2004, COHI partnered with a local host organization, Tibetan Healing Fund, in Tongren, also known as Repkong, Eastern Tibet to aid with the training of midwives in order to create a more sustainable maternal health care system.\n\nIn 2004, COHI and their partners worked with Israeli and Palestinian woman to address midwifery and gender based violence (GBV). Conducted an assessment based on three main categories\n\n\nThe assessment included in depth interviews and recommendations for each sector of Israel and the West Bank's diverse populations: religious and secular Jews, immigrant populations (Ethiopian, Russian, Congolese), the Bedouin, and the Palestinian populations of Israel and the West Bank, both Muslim and Christian\n\nThe results from the assessment were used to advocate for the needs of Israeli and Palestinian women such as better postpartum care.\n\n"}
{"id": "59030436", "url": "https://en.wikipedia.org/wiki?curid=59030436", "title": "Death and state funeral of Ruhollah Khomeini", "text": "Death and state funeral of Ruhollah Khomeini\n\nOn 3 June 1989, just before midnight IRST, Grand Ayatollah Ruhollah Khomeini, leader of the Iranian Revolution and the first Supreme Leader and founder of the Islamic Republic of Iran, died in Jamaran, Greater Tehran aged 86 after spending eleven days at a local clinic, near his residency, after repeated heart failure (five heart attacks in ten days). Other sources put his age at 89, and list the cause of death as bleeding in digestive system. Khomeini was given a state funeral and then buried at the Behesht-e Zahra (\"The Paradise of Zahra\") cemetery in south Tehran.\n\nOn 5 June, the coffin with Khomeini's body was transferred to the Musalla, a vacant lot in north Tehran. The body was displayed there on a high podium made out of steel shipping containers, in an air-conditioned glass case, wrapped in a white shroud. It stayed there until the next day. Hundreds of thousands of mourners had seen the body. On 6 June, the body was brought down and the coffin opened for Grand Ayatollah Mohammad-Reza Golpaygani to lead the Salat al-Janazah (funeral prayer), which lasted for 20 minutes. Afterwards, since the crowds of mourners had swelled overnight to several millions, it was impossible to deliver the body to the cemetery through Tehran to the southern part of the city in an procession. Eventually, the body was transferred to an IRIA Bell Huey helicopter and brought by air to the cemetery. \n\nAt the cemetery, the crown surged past the makeshift barriers and the authorities lost control of the events. According to journalist John Kifner of \"The New York Times\":\n\nThe body was taken back to north Tehran to go through the ritual of preparation a second time. To thin the crowd, it was announced on television and radio that the funeral had been postponed. Five hours later, the body was returned to the cemetery and this time the guards were better prepared. The body was brought out of a helicopter, sealed in a metal box resembling an airline shipping container. Once again, the crowd broke through the cordon, but by weight of numbers the guards managed to push their way through to the grave. There, according to reporters for \"Time\" magazine:\n\nIn 1992, the construction of the Mausoleum of Ruhollah Khomeini on the burial site was completed.\n\n"}
{"id": "54644203", "url": "https://en.wikipedia.org/wiki?curid=54644203", "title": "Donald R. Atkinson", "text": "Donald R. Atkinson\n\nDonald Ray Atkinson (February 10, 1940, in Union City, Indiana–January 11, 2008, in Jackson County, Oregon) was an American counseling psychologist and professor at the University of California, Santa Barbara (UCSB). He was known for his extensive work in multicultural counseling psychology. He was the director of training for UCSB's Counseling Psychology Program for ten years (1979-1989), and previously as Assistant Dean of the Department of Education there for four years (1975-1979). Atkinson grew up in Baraboo, Wisconsin and graduated from Baraboo High School. He served in the United States Navy for two years. He wrote a book about Baraboo: \"Baraboo: A Selective History.\" He also wrote other books and articles about counseling. He died from pancreatic cancer in Jackson County, Oregon. He retired from the faculty of UCSB in 2002.\n\n"}
{"id": "58452468", "url": "https://en.wikipedia.org/wiki?curid=58452468", "title": "Dopesick Nation", "text": "Dopesick Nation\n\nDopesick Nation, also known as \"American Junkie\" is an American documentary television series that premiered on September 12, 2018. The series takes place in southern Florida and follows two recovering addicts, Allie and Frankie, and their journey to take on the opioid epidemic by assisting as many addicts into recovery as they can. The series also sheds light on the “recovery capital of America”, Delray Beach, and the corruption and exploitation that exist in the rehab industry.\n"}
{"id": "58624914", "url": "https://en.wikipedia.org/wiki?curid=58624914", "title": "East London Genes &amp; Health", "text": "East London Genes &amp; Health\n\nEast London Genes & Health is a genomic research study of 100,000 people of Bangladeshi and Pakistani origin. These ethnic groups have a rate of diabetes five times higher than the rest of the population. The project is managed by Queen Mary University of London.\n"}
{"id": "31694032", "url": "https://en.wikipedia.org/wiki?curid=31694032", "title": "Elephant hunting in Kenya", "text": "Elephant hunting in Kenya\n\nElephant hunting, which used to be an accepted activity in Kenya, was banned in 1973, as was the ivory trade. Illegal hunting continues, as there is still international demand for elephant tusks. Kenya pioneered the destruction of ivory as a way to combat this black market. Elephant poaching continues to pose a threat to the population.\n\nDuring colonial times, elephant hunting in Kenya was seen as a sport for noblemen and was exploited by the colonial governors. British East Africa was not unique in this: big-game hunting was popular in many parts of the Empire. Among the white hunters, the bull elephant was said to be the most exhilarating target. Small-bore rifles appeared to be the preferred option and aiming at the brain instead of the heart was another preference. The motive was not always monetary. However, many hunters were indiscriminate in their choice of elephants to kill – young, old, male or female, it did not matter, as the primary purpose was ivory to sell and elephant meat to feed their hunting party. \n\nThe East African Professional Hunter's Association was formed to regulate the industry and restrict its excesses. The Association, which came into being at the Norfolk Hotel, Nairobi, stemmed from a desire to regulate hunting in the wake of technological developments like the safari vehicle, which had made accessing remote hunting areas much easier. During its existence it was able to accomplish much to conserve East African wildlife and become perhaps one of the most respected societies in the world of its kind. \n\nOne of the most prolific of the white hunters was the Scottish adventurer W. D. M. Bell, who is reported to have killed over a thousand elephants, spread across several African countries. See the first of his memoirs, \"The Wanderings of an Elephant Hunter\" (1923), for more information. Some of the madness of the desire to shoot an elephant (albeit not in Kenya) is shown in \"White Hunter Black Heart\", a fictionalised version of what happened during the filming of the Hollywood classic \"African Queen\".\n\nIn 1963, the first year of independence, the Kenyan government issued 393 permits (hunting licenses) for elephants. \n\nIn the 1950s and 1960s, the Kenyan poacher received approximately Shs. 3-4/lb ($.79–1.05/kg); by the 1970s, it was Shs. 100/kg ($12.74/kg), increasing the black market value for the primary producer from about one-fifth to one-third of the real value.\n\nAccording to the American hunter Craig Boddington, elephant hunting was made illegal in Kenya in 1973 and all animal hunting without a permit in 1977. \n\nBy the late 1970s, the elephant population was estimated around 275,000, dropping to 20,000 in 1989. Between 1970 and 1977, Kenya lost more than half of its elephants. \n\nIn the 1970s, Ngina Kenyatta (Mama Ngina), wife of then-President Jomo Kenyatta, and other high-level government officials were allegedly involved in an ivory-smuggling ring that transported tusks out of the country in the state private aeroplane. \"New Scientist\" claimed that there was now documentary proof that at least one member of \"Kenya's royal family\" (the Kenyatta) had shipped over six tons of ivory to China. \n\nIn the 1970s, 1900 elephants were killed in Kenya for their ivory tusks, increasing to 8300 elephants in the 1980s.\n\nIn 1989, as a dramatic gesture to persuade the world to halt the ivory trade, President Daniel arap Moi ignited twelve tons of elephant tusks. \n\nIn the 1990s the widespread ban on commercial ivory trading reduced the industry to a fraction of what it had been and elephant populations have stabilised. But illegal poaching and sale on the black market still poses a serious threat, as does government bribery. The largest poaching incident in Kenya since the ivory trade ban occurred in March 2002, when a family of ten elephants was killed.\n\nIllegal elephant deaths decreased between 1990, when the CITES ban was issued, and 1997, when only 34 were illegally killed. Ivory seizures rose dramatically since 2006 with many illegal exports going to Asia. Poaching spiked seven-fold between 2007 and 2010. \n\nLarge scale tourism promotion picked up in Kenya following the imposed hunting ban in Kenya since 1977. It has been noted that \"photographic tourism\", or non-consumptive wildlife use, is contributing 12% of Kenya’s GDP. Hence, some groups have recommended that tourism be promoted rather than any kind of hunting or consumptive wildlife use, as it could divert the attention of the government of Kenya from the policy goal of wildlife preservation.\n\nThough elephant hunting has been banned for a 40-year period in Kenya, poaching has not reduced. Given the poverty of many of the people, and the high value of elephant tusks, they are shipped overseas and sold on the black market. Although Kenya has many national parks and reserves protecting wildlife, elephant populations are still at risk, a problem which is made worse by corruption and some officials supplementing their income with permitting poaching. The Kenyan government has attempted to crack down on elephant poaching with the aid of multi-nationals but has often been too late in preventing the poaching of many elephants whose tusks have been seized \"en masse\" in cases at Nairobi Airport and in Bangkok Airport where Kenyan tusks have often been imported.\n\nArrests continue at Nairobi's international airport, where 92 kilos of raw ivory were seized in 2010, and 96 kilos in 2011.\n\nAn individual case that received publicity in 2014 was the death of Satao, one of the world's largest elephants, in the Tsavo Trust. Despite Kenya Wildlife Service guards, poachers managed to shoot the bull with a poisoned arrows and cut off his tusks.\n\nTrophy hunting, purely as sport and as a conservation action, is now being considered for adoption in Kenya, as such a programme appears to have yielded positive results in Namibia and South Africa under a programme titled \"Community-Based Natural Resource Management\" (CBNRM). Under this programme, while cash was offered as an incentive for sport hunting, the basic aim was wildlife control on the communal land, providing benefits to the community as a whole. It is believed that trophy hunting might attract elephant poachers into moving into legal hunting and leaving elephant trading.\n\nThe Food and Agriculture Organization's (FAO) report states: \"Trophy hunting is generally self-regulating because low off-take is required to ensure high trophy quality and marketability in future seasons. Trophy hunting creates crucial financial incentives for the development and/or retention of wildlife as a land use over large areas in Africa, including in areas where ecotourism is not viable. Hunting plays an important role in the rehabilitation of degraded wildlife areas by enabling the income generation from wildlife without affecting population growth of trophy species.\"\n\nThe policy of trophy hunting has been adopted in 23 sub-Saharan African countries. The income generated in total in Africa is quoted to be USD 201 million/year, derived from about 18,500 international hunting clients covering an area of 1.4 million km². Since there is a lack of consensus among the clients about the efficacy of this method of biodiversity conservation in Africa, a study carried out by the Africa Wildlife Conservation Fund indicates that if Kenya makes trophy hunting legal again, nearly 90% of the clients would be interested to pursue this activity in that country. In this context, the importance of effective regulation of hunting operators and clients has also been highlighted.\n\nOne of the disadvantages of trophy hunting is the possible publicity backlash, such as Zimbabwe experienced with the killing of Cecil the lion.\n\n"}
{"id": "47659165", "url": "https://en.wikipedia.org/wiki?curid=47659165", "title": "Emma Louise Call", "text": "Emma Louise Call\n\nEmma Louise Call (1847–1937) was an American physician, and one of the first female physicians in the United States. Along with Sigmund Exner, she is one of the namesakes of Call-Exner bodies, a pathognomonic feature of granulosa cell tumors. These tumors are associated with ovarian cancers.\n\nCall received her MD from the University of Michigan Medical School in 1873, and moved to Vienna as Sigmund Exner's postgraduate student. In 1875, they published the manuscript announcing their findings of the pathology findings - eosinophilic follicles present in ovarian tumors - that would later be named after the two of them. This would be Call's only publication, as she later returned to Boston and practiced clinically as an obstetrician for 40 years. Call would become the first woman to receive membership into the Massachusetts Medical Society in 1884.\n\n\"I entered the Medical Department of the University the first year that women were admitted. The first class of women...were naturally the objects of much attention critical or otherwise (especially critical) so that in many ways it was quite an ordeal. I believe that only one of the medical faculty was even moderately in favor of the admission of women, so that it speaks well for their conscientiousness when I say (with possibly one exception) we felt that we had [a] square deal from them all.\"\n\nWith regard to Call's quote, Robert H. Young stated that \"Corydon Ford had taught Elizabeth Blackwell at Geneva Medical College and was tolerant of the women students, but not all the professors were so kind. The professor of chemistry, Silas Douglas, did not intervene when the men students stamped their feet and shouted as the women entered the lecture room for the one subject that was taught to mixed classes.\"\n"}
{"id": "21399033", "url": "https://en.wikipedia.org/wiki?curid=21399033", "title": "Esuvee", "text": "Esuvee\n\nESUVEE was a US year-long consumer education campaign on sport utility vehicle safety in 2005. There was also an associated website game. The goal of the campaign was to encourage safe driving of SUVs. The campaign emphasized the need to drive carefully to avoid rolling the vehicles over because SUVs handle like trucks, not cars. The focus was changing driver behavior, particularly among younger male drivers.\n\nThe campaign was a $27 million effort sponsored by the consumer protection agencies and the Attorneys General of all 50 states, the District of Columbia, Puerto Rico and the U.S. Virgin Islands. Part of the funding came from a settlement agreement with the Ford Motor Company to resolve lawsuits alleging that Ford’s marketing practices misled consumers on how to drive, load, and maintain the Ford Explorer. \n\nThe SUUVEE campaign was to inform the public about four safety elements to help save lives:\n"}
{"id": "985031", "url": "https://en.wikipedia.org/wiki?curid=985031", "title": "European Year of People with Disabilities", "text": "European Year of People with Disabilities\n\nThe year 2003 was designated the European Year of People with Disabilities (EYPD) by the European Commission. It is sometimes called the \"European Year of Disabled People\".\n\nThe European Commission set aside 12 million euros to recognise the European Year of People with Disabilities.\n\nThe Year was officially launched under the Greek Presidency in Athens on 26 January 2003. The opening ceremony in Athens was co-hosted by Julie Fernandez.\n\n"}
{"id": "56894934", "url": "https://en.wikipedia.org/wiki?curid=56894934", "title": "Euthanasia Sherman Meade", "text": "Euthanasia Sherman Meade\n\nDr. Euthanasia Sherman Meade (1836–1895) was a pioneer woman physician of the Pacific Coast. Dr. Meade was the first president of The Woman's Medical Club of California.\n\nEuthanasia Sherman was born in 1836 in Genessee, New Jersey. She was the niece of General W.T. Sherman. When she was 17 years old, she married a man her mother chose and shortly after moved to California. After the death of her only child in its birth, she turned to obstetrics, and later medicine. She returned to the East Coast and worked in hospitals during the war. In 1869, she graduated from the Woman's Medical College of Pennsylvania. \n\nWhen she returned to California, Dr. Meade opened an office on Mission Street in San Francisco. She was the first regular woman physician to establish herself in California. However, due to the times, she was met with little recognition in the medical profession. She developed asthma and eventually moved to San José where she practiced for 25 years. In 1876, Dr. Meade was admitted to the State Medical Society along with four other women physicians.\n\nDr. Meade died in 1895 due to a cerebral embolus from an endocarditis. She was cremated and her ashes placed at Cypress Lawn Cemetery.\n"}
{"id": "57089243", "url": "https://en.wikipedia.org/wiki?curid=57089243", "title": "Friedreich's Ataxia Research Alliance", "text": "Friedreich's Ataxia Research Alliance\n\nThe Friedreich's Ataxia Research Alliance (FARA) is a non-profit organization dedicated to curing Friedreich's ataxia and the related sporadic ataxias. Established in 1998, FARA supports basic and translational research, pharmaceutical/biotech drug development, clinical trials, and scientific conferences. It facilitates dialogue between the public and scientific community worldwide and promotes exchanges of information that drive medical advances.\n\nThe turnover of FARA in 2016 was circa $5 million with 86.4% spent on research grants, 3.7% on education, awareness, and outreach programs, and 1.9% on conferences. FARA hs received a Four Star rating from Charity Navigator for 8 consecutive years.\n"}
{"id": "1269698", "url": "https://en.wikipedia.org/wiki?curid=1269698", "title": "Goldbergturm", "text": "Goldbergturm\n\nThe Goldbergturm is a 51 metre high water tower on the Goldberg (\"gold mountain\") at in Sindelfingen, Germany. The Goldbergturm was built in 1963 and has in its basket, besides a water reservoir, a restaurant from which a stairway runs to its open-air observation deck.\n\n"}
{"id": "19023308", "url": "https://en.wikipedia.org/wiki?curid=19023308", "title": "HIV/AIDS in Kenya", "text": "HIV/AIDS in Kenya\n\nKenya has a severe, generalized HIV epidemic, but in recent years, the country has experienced a notable decline in HIV prevalence, attributed in part to significant behavioral change and increased access to ART(antiretroviral drugs). Adult HIV prevalence is estimated to have fallen from 10 percent in the late 1990s to about 4.8 percent in 2017. Women face considerably higher risk of HIV infection than men, and also experience a shorter life expectancy due to HIV/AIDS. The 7th edition of AIDS in Kenya reports an HIV prevalence rate of eight percent in adult women and four percent in adult men. Populations in Kenya that are especially at risk include injecting drug users and people in prostitution, whose prevalence rates are estimated at 53 percent and 27 percent, respectively. Men who have sex with men (MSM) are also at risk at a prevalence of 18.2%. Other groups also include discordant couples (where one partner is infected and the other is not) however successful ARV-treatment will prevent transmission. Other groups at risk are prison communities, uniformed forces, and truck drivers.\n\nThe following will be a timeline of the major events that have led up to today in the HIV/AIDS epidemic in Kenya from 1984-2018.\n\n1984: First case of HIV found in Kenya\n\n1985: Sex workers were involved in 26 new recorded cases of HIV\n\n1985: The condition is still a mystery in Kenya\n\n1986: There is a mass denial of the disease being a problem\n\n1987: A study was performed on the unique women from Majengo slum in Nairobi, who were \"immune\" to AIDS\n\n1988: HIV prevalence is now at 3,000 cases\n\n1990: Prevalence rises to 2.5% of the population as the government is still in denial\n\n1995: By 1995, the death toll rose to about 200,000 people\n\n1998: Prevalence rate is at 9.8%\n\n2003: Prevalence rate drops to 6.7%\n\n2003: Public sector releases low cost ARVs\n\n2005: Around 65,000 HIV positive individuals in Kenya are on ARVs\n\n2012: Prevalence has dropped to 5.6%, but Kenya still has the 4th largest HIV epidemic\n\nHere is a brief overview of the HIV epidemic in the country as reported by the Ministry of Education in June 2014, as well as the UNAIDS report on Kenya for 2017.\n\n\nAs the LGBTQ+ community has always been part of the fight against HIV/AIDS in all countries, the same has happened in Kenya. Although MSMs and other forms of same-sex contact are not the main risk group or cause of HIV/AIDS in Kenya, they have been a big proponent due to the laws that have been put in place by the country's government. Homosexuality is currently illegal in Kenya, allowing the government to 595 cases of homosexuality from 2010-2014. While MSMs and members of the LGBTQ+ community already face discrimination on a regular basis, they are being denied treatment due to their identity and the penal code in many places in Kenya.\n\nHowever, recently, activist groups have been able to challenge the constitutionality of the laws that are in place, resulting in the government contemplating if they should be repealed. This advance happened in 2016, and currently the verdict has not come in about the constitutionality of the penal code that is in place with respect to this community. Victory has been won in other places that had laws that discriminated against the LGBTQ+ community, so the activists have a positive outlook on this subject at this point in time.\n\nWith adolescents being a big risk group in Kenya, society has molded the outlook of this group and how they interact with HIV/AIDS. There are many social stigmas involved with people aged 15-19, as they go through harsh environments in schools and in the community. HIV and AIDS in school is viewed as a killer disease that is a sign of sexual immortality. Many adolescents feel afraid to disclose their status, due to the stigma that is behind it. Fear is also found in walking into health centers and asking about HIV/AIDS, due to the conception that health workers will look down on you and your condition.\n\nSocial media has also provided adolescents in Kenya with illegitimate information about the disease, prevention of the disease, and overall outlook. While the main fear is found in public shaming or judging, another problem is that adolescents are not represented in policy very well, compared to that of children and adults. Economic burdens that are placed on adolescents that do not have parents to provide the means for their education and wellbeing is another problem for the group. Sometimes, adolescents are forced into being sex workers to provide for themselves, resulting in an increased risk for HIV infection.\n\nSex workers have the highest prevalence among the risk groups of HIV/AIDS in Kenya. It has been reported that 29.3% of sex workers have HIV. The main problem within this community is the fear of coming forward about being raped or abused, because it could lead to prosecution for being a sex worker. Therefore, sex workers are less likely to go to anyone for help because of this fear. Being prosecuted and arrested can lead to an interruption in HIV treatment.\n\nThe Kenyan Ministry of Health published a report on June 2014 called \"Kenya HIV Prevention Revolution Road Map. \"The road map aims to dramatically strengthen HIV prevention, with the ultimate goal of reducing new HIV infections to zero by 2030. The following observations and conclusions were outlined:\n\n\nART was introduced to Kenya in the late 90s when the treatment was initially being rolled out. However, they did not start receiving low cost drugs until around 2003. Costs continued to decline, and with enough donor money, more than a million patients receive ART for free through the government. Recently, Kenyans with HIV got access to a high end drug for cheap due to an international deal. The cost per year for this drug treatment is US$75 and is a big improvement as it combines some drugs together to make the treatment plan cheaper and easier for patients living with HIV. This drug has been accessible in high income countries since 2014, but the new deal has placed the drug in middle and low income countries.\n\nDuring the initial outbreak of AIDS in the 80s, the Kenyan government stayed away from discussions about how big of a problem there was with HIV/AIDS in the country. By 1993, statements were finally made about the problem and how the situation should have been addressed sooner. Kenya could not afford to lose so many skilled workers to HIV/AIDS when there was currently no treatment for it. The response and plan to reduce the amount of HIV infections has been more extensive going into the new century and currently.\n\nHIV testing and counseling (HTC) has been one response to the HIV/AIDS crisis in Kenya. The government has encouraged getting tested and for people to be more open about the diagnosis so it can be addressed appropriately. Self testing kits for low cost have been introduced over recent years, along with community based testing and door-to-door campaigns. In 2008, only 860,000 people were being annually tested for HIV, compared to 9.9 million people that are being tested annually now. \n\nEven though condom use wasn't endorsed by the Kenyan government until 2001, the rate of condom usage has gradually increased ever since. Free condoms have been distributed throughout different communities, including the sex workers community. This has decreased unprotected sex, which is critical in lowering new HIV infections. \n\nEducation about HIV/AIDS has been in the school curriculum since 2003, and it has been effective in increasing knowledge within children about the disease. There has been some controversy about the ethics of teaching students about sexual health, due to the fear that it would encourage young people to have sex. However, rates of new HIV infections have said otherwise. Mass media campaigns have also been done to educate people about HIV/AIDS. \n\nPreventing mother to child transmission (PMTCT) has also been a big step in preventing the spread of HIV/AIDS. The country's dedication to eliminating this type of transmission has led to a drop of children born with HIV from 12,000 children in 2010 to 6,600 in 2015. Male partners have also been encouraged to take part in this type of treatment, by getting tested along with the soon-to-be mother.\n\nVoluntary medical male circumcision (VMMC) was implemented as an option in 2008 in Kenya as a prevention method. By 2016, 92% of men in Kenya are circumcised.\n\nHarm reduction is the distribution of clean needles and syringes along with counseling and medically assisted treatment with methadone, implemented by the government in 2012. The amount of Intravenous drug users that are using clean needles now is up to 90% compared to the 51% in 2012.\n\nPre-exposure prophylaxis has been an ongoing trend in Kenya, as HIV negative people have been receiving ART to prevent against any future infection of HIV. It is being offered for people who are in high risk groups that have an ongoing risk of HIV infection.\n\n\n"}
{"id": "506685", "url": "https://en.wikipedia.org/wiki?curid=506685", "title": "Hellboy", "text": "Hellboy\n\nHellboy is a fictional superhero created by writer-artist Mike Mignola. The character first appeared in \"San Diego Comic-Con Comics\" #2 (August 1993), and has since appeared in various eponymous miniseries, one-shots and intercompany crossovers. The character has been adapted into two live-action feature films in 2004 and that starred Ron Perlman in the title role, and two straight-to-DVD animated films, as well as three video games – \"\", \"\", and as a playable character in \"Injustice 2\". A film reboot starring David Harbour is set for release in 2019.\n\nA well-meaning half-demon whose true name is Anung Un Rama (\"and upon his brow is set a crown of flame\"), Hellboy was summoned from Hell to Earth as a baby on October 5th (given as his birth date by Mike Mignola) by Nazi occultists (spawning his hatred for the Third Reich). He was discovered by the Allied Forces; amongst them, Professor Trevor Bruttenholm, who formed the United States Bureau for Paranormal Research and Defense (B.P.R.D.). In time, Hellboy grew to be a large, red-skinned adult with a tail, horns (which he files off, leaving behind circular stumps on his forehead), cloven hooves for feet, and an oversized right hand made of stone (the \"Right Hand of Doom\"). He has been described as smelling of dry-roasted peanuts. Although a bit gruff, he shows none of the malevolence thought to be intrinsic to classical demons, and has an ironic sense of humor. This is said to be because of his upbringing under Professor Bruttenholm, who raised him as a normal boy.\n\nHellboy works for the B.P.R.D., an international non-governmental agency, and for himself against dark forces including Nazis and witches, in a series of tales that have their roots in folklore, pulp magazines, vintage adventure, Lovecraftian horror and horror fiction. In earlier stories, he is identified as the \"World's Greatest Paranormal Investigator\".\n\nHellboy, or \"Anung Un Rama\" as he was called, was conceived on October 5, 1617, the day his birth-mother Sarah Hughes was on her deathbed. In life, Sarah was a witch who gained her powers from being a consort of the demon Azzael, a duke of Hell who is Hellboy's \"biological\" father. Taking Sarah's body to hell when she attempted to repent on her deathbed within a church in East Bromwich, England, Azzael burned her away so their child would be born, and chopped off the newborn's right hand to replace it with the \"Right Hand of Doom\", a relic tied to the Ogdru Jahad. When the other princes of Hell learned of his actions, Azzael sent his half-demon child away while he was stripped of his powers and imprisoned in ice (like Lucifer in Dante's \"Divine Comedy\").\n\nThe child is eventually summoned to Earth in the final months of World War II by the \"Mad Monk\" Grigori Rasputin on Tarmagant Island, off the coast of Scotland, having been commissioned by the Nazis to change the tide of a losing war (\"Project Ragna Rok\"). As a direct result of this ritual, the child appears on Earth in a fireball at what remained of the ruined Bromwich Church on December 23, 1944. Proving not to be a devil, in the traditional sense, but a devil-like creature, the child was dubbed \"Hellboy\" by Professor Trevor \"Broom\" Bruttenholm.\n\nTaken by the United States Armed Forces to an Air Force base in New Mexico, Hellboy is raised by Professor Bruttenholm and the United States Army where the Bureau for Paranormal Research and Defense (BPRD), a private organization dedicated to combating occult threats, begins. Due to the success of his first mission in 1952, Hellboy is granted \"honorary human\" status by the United Nations and becomes a member of the BPRD as the \"world's greatest paranormal investigator\". As such, Hellboy interacts regularly with humans, primarily law enforcement officials, the military, and various \"scholars of the weird\", most of whom are not presented as overtly reacting to his strange appearance.\n\nAs an adult, having matured physically within years and aging slowly while having a teenaged mind, Hellboy becomes the primary agent for the BPRD, alongside other human and quasi-human agents that include Kate Corrigan, a professor of folklore at New York University; Abe Sapien, an amphibian humanoid (\"Ichthyo sapiens\"); and Liz Sherman, a young pyrokinetic. Things change for Hellboy during the events of \"\" when he finds Professor Bruttenholm after he disappears in an expedition in the Arctic, and witnesses his adopted father's death at the hands of a Lovecraftian frog monster. The search takes Hellboy, Abe and Liz to the Cavendish Hall mansion, which is a trap established by Rasputin to lure Hellboy into an embrace of his own \"destiny\", with the assistance of Sadu-Hem: one of the spawn of the Ogdru Jahad. Controlled by the spirit of one of the ancestral Cavendish men, Abe impales Rasputin. Liz's firestorm then incinerates Rasputin's body alongside Sadu-Hem's, and destroys Cavendish Hall. Soon after, during a visit to Bromwich Church, Hellboy gets a glimpse of his conception 300+ years ago, and learns he has two human half siblings: a nun and a priest whose spirits haunt the church after their deaths, attempting to stop Azzael from claiming Sarah.\n\nDuring the events of \"\", Hellboy's journey of self-discovery leads him to Romania to investigate the theft of an ancient box containing the corpse of Vladimir Giurescu, a Napoleonic officer who was in fact a vampire before he was \"killed\" on the order of a fearful Adolf Hitler. The culprit of the theft is revealed to be Ilsa Haupstein, one of the surviving members of Project Ragna Rok, who was revived from suspended animation and then aided in Giurescu's resurrection. Finding Castle Giurescu after splitting up with the other search groups, Hellboy learns that the source of Giurescu's rebirth is the ancient goddess Hecate. Though Hellboy destroys Hecate's original body, he faces her again after Rasputin unintentionally provides her with Ilsa's iron-maiden encased body. Hecate swallows Hellboy, but he returns to his own reality after he denounces the dark purpose he was born to perform.\n\nHellboy later learns that Liz is dying after losing her powers when she accidentally revived a homunculus while searching another location for Giurescu, finding Roger in the events of \"\" as he convinces the homunculus to save Liz's life. Following the events of \"\", gaining insight about his stone hand and being referenced as a harbinger of the Apocalypse, Hellboy is accompanied by Abe to hunt down the warlock Igor Bromhead in \"Box Full of Evil\". But it turned out to be a trap conducted by Bromhead and the demon Ualac to capture Hellboy so that the latter can claim Hellboy's normally invisible Crown of the Apocalypse. But this act allows Hellboy to no longer be controlled by his true name as he kills Ualac's mortal body before the demon and the crown are taken to Hell by the demon Astaroth, who is later revealed to be Hellboy's paternal uncle.\n\nIn the aftermath of \"\", assisted by the ghost of Lobster Johnson, Roger, and Abe, a disillusioned Hellboy resigns from the Bureau before it later gains new agents in Johann Kraus, the spirit of a German medium kept in a containment suit; and Captain Ben Daimio, a special operations soldier that became an Olmec were-jaguar. From there, Hellboy decides to find out the truth of his existence once and for all. But, as revealed in \"\", Hellboy ends up being stranded on an island where he inadvertently resurrected an ancient mystic who gained knowledge of the secret history of the creation of Ogdru Jahad and the Right Hand of Doom.\n\nSix years later, as \"\" opens, Hellboy's search takes him to England where he finds himself in the middle of a power vacuum caused by Bromhead incapacitating Hecate in Italy. Refusing to serve the witches as their king, Hellboy ends up in the dimension of Baba Yaga, a witch whom he encountered in the past and happens to be an ally of Rasputin's. Managing to defeat Baba Yaga's champion Koshchei, Hellboy returns to his reality and is led to Bromhead after he became monstrous and in agony from his attempt to take Hecate's powers for his own. Hellboy gives Bromhead a merciful death before returning to England during the events of \"\" where he encounters Alice Monaghan, a young woman he saved as a baby from a fairy named Gruagach who has revived the lunatic sorceress Nimue to fill the void left by Hecate.\n\nDuring that time, Hellboy encounters the spirit of Morgana le Fay who reveals to Hellboy both the names of his parents and that Sarah Hughes was her descendant, which names Hellboy as the last living heir to Arthur Pendragon and the rightful king of England. But as he also learns from Astaroth that he is destined to kill Satan and become the new king of Hell, Hellboy is reluctant to wield Excalibur and the army of undead British nobility amassed to face Nimue's army. Therefore, enlisting Baba Yaga's assistance with his eye as payment for the injury he did to her in their first meeting, Hellboy decides to face Nimue one-on-one in the events of \"\". But in the aftermath of his battle with Nimue, who was possessed by Ogdru Jahad at the time, Hellboy is killed by the witch in her final moments. As revealed in \"Hellboy in Hell\", Hellboy ends up trapped in Hell, where he encounters what's left of his demonic kin and the souls of the damned, and ultimately comes to terms with the destiny he has been shrugging off his whole life.\n\nAfforded by his demonic heritage as well as extensive physical training and bodybuilding, Hellboy possess superhuman strength that exceeds the 1 ton base limit, endurance, a degree of resistance to injury, and a healing factor that allows him to heal quickly from virtually all bodily injuries as well as renders him immune to all diseases. He also has the innate ability to comprehend ancient and magical languages. The extent of his strength is unclear, but he has torn down a large tree and hurled it at an opponent and has lifted massive stones. He has also picked up and thrown opponents weighing at least four to five hundred pounds. Hellboy has a high degree of resilience to injury. He can withstand powerful blows that would severely injure or kill a human. He survived being shot many times in the chest with an MG 42 machine gun before destroying it. He has survived being impaled through the chest with a sword, severe werewolf mauling, being beaten unconscious with heavy iron tongs, falling from extreme heights, being crushed by boulders, and more. In the film version it is stated that Hellboy is immune to all forms of fire and burns, including Liz Sherman's flames, and electrocution. Despite his ability to quickly recover from seemingly mortal wounds, he is far from invulnerable, and can be injured or bloodied by conventional weapons. It is revealed to Baba Yaga by the dead Russian nobility that Hellboy may not be slain even through supernatural means and that he appears to be as deathless as her warrior, Koschei the Deathless. In the films, Hellboy has shown skill in necromancy, animating a man's dead body so that it could give him directions.\n\nHellboy ages very differently from humans. In the story \"Pancakes\" he is two years old but appears to be somewhere between 6 and 10 human years old. In \"Nature of the Beast\", set in 1954, the ten-year-old Hellboy appears fully grown. His rapid physical maturation is in contrast to his actual rate of aging, however, which seems to be much slower than humans. Throughout the sixty-year span of time depicted in the comics, he does not age beyond the plateau of physical maturity. This mystical aging process is similar to the other demons and supernatural beings that populate Hellboy's world. The lifespan of a demon, or half-demon as Hellboy's mother was human, are left undefined within the comics and seem to range from decades to many thousands of years. In the movies, Hellboy's aging process is described by BRPD as \"reverse dog years\".\n\nIn addition to his natural physical abilities, Hellboy carries a variety of items in his utility belt and jacket that can be used against various supernatural forces. He has been known to carry holy relics, horseshoes, various herbs, and hand grenades. Though he commonly carries an oversized revolver, Hellboy freely admits to being a lousy shot and often fights hand-to-hand, preferring to use short-ranged physical weapons like swords, spears, and his massive stone fist over firearms. Hellboy's lack of formal combat training and education is compensated for by his decades of experience as a paranormal investigator, though encounters with unfamiliar threats have often forced him to resort to improvisation and using his wits.\n\nAs revealed in \"Strange Places\", Hellboy's right hand was originally the right hand of Anum, one of the \"greater spirits\" that watched over the burgeoning Earth and created the Ogdru Jahad. After sealing the Ogdru Jahad away, Anum was destroyed by his fellow spirits. Only his right hand remained intact as it was kept and preserved by many races throughout history, including the first race of man. The Right Hand of Doom eventually ended up in the possession of Azzael before he grafted it onto the newborn Hellboy.\n\nAs the hand which created and bound the Ogdru Jahad, it is also the key which will \"loose and command\" them; in other words, it is a catalyst that will bring about Ragnarok. The comic books themselves never mention how the Right Hand of Doom would actually perform these tasks; they only explain this is the case and someone or something intends to do it with or without Hellboy's consent. The film shows it working like a key: being turned twice in a special obelisk secured by Rasputin would release the Ogdru Jahad. It is made clear it is not necessary for the arm to be attached to Hellboy to perform its duties. It has been suggested if Hellboy dies while the Hand is attached to him, it would become useless. He has therefore concluded the only way to prevent its falling into the wrong hands is to keep and protect it.\n\nHellboy originated with a drawing Mike Mignola did at a comic book convention of a demon with the name \"Hellboy\" written on his belt. Mignola had no intention of doing anything serious with the concept, but eventually decided he liked the name.\n\nLater, Mignola became interested in doing a creator-owned comic, as he felt it made more sense to create his own characters for the stories he wanted to tell, rather than trying to shoehorn existing characters into these stories. Mignola elaborated, \"The kinds of stories I wanted to do I had in mind before I created Hellboy. It’s not like I created Hellboy and said, 'Hey, now what does this guy do?' I knew the kinds of stories I wanted to do, but just needed a main guy.\" He initially created Hellboy as part of a team of five, but scrapped this idea when he realized he could not think of any team names that he liked.\n\nMuch like other American comic book superheroes such as Batman, Spider-Man, Wolverine, Iron Man, Daredevil, and Spawn, Hellboy is constantly tormented by the knowledge of his past. One example being in \"Wake the Devil\" where he describes his mindset since the aftermath of \"Seed of Destruction\" by saying, \"I \"like\" not knowing. I've gotten by for fifty-two years without knowing. I sleep good \"not knowing\".\"\n\nBefore \"Hellboy\" was published independently at Dark Horse Comics, the concept was initially pitched to a board of directors for DC Comics, who loved it, but did not like the idea of it involving \"Hell\".\n\nThe early stories were conceived and drawn by Mignola with a script written by John Byrne and some later stories have been crafted by creators other than Mignola, including Christopher Golden, Guy Davis, Ryan Sook, and Duncan Fegredo. The increasing commitments from the \"Hellboy\" franchise meant that the 2008 one-shot \"In the Chapel of Moloch\" was the first \"Hellboy\" comic Mignola had provided the script and art for since \"The Island\" in 2005.\n\n\"Hellboy\" has an internal numbering on the inside cover of its issues. Below are the stories listed by their internal numbering for the comics.\n\"Hellboy in Hell\" is a finished series with its own numbering.\n\"Hellboy and the B.P.R.D.\" is an ongoing series of miniseries.\nSpecial stories were created for hardcover original graphic novels.\n\nAll in-continuity \"Hellboy\" comics are collected in trade paperbacks.\n\nAll \"Hellboy in Hell\" comics are collected in trade paperbacks.\n\nAll \"Hellboy and the B.P.R.D.\" comics are collected in trade paperbacks.\n\nThese editions collect the stories in the size they were originally drawn.\nThese editions collect the complete Hellboy series in chronological order.\n\n\nBeyond the \"Hellboy\" comic and its associated spin-offs, Hellboy has made appearances in other publications:\n\nThe character name \"Hell Boy\" was included in a drawing by Mike Mignola of a demon character in a black and white illustration, with the later recognized name appearing on the demon's belt buckle. This image, accompanied by a short biography of Mike Mignola and his latest creation, appeared in the pamphlet in 1991. It is the first published mention of the later recognized name. This image was reprinted in\" The Art of Hellboy\".\n\nA prototype incarnation of Hellboy appeared on the cover of \"Dime Press\" #4 (Glamour International Production, 1993), an obscure Italian fanzine, with \"Hellboy©Mignola 93\" written at the bottom of the cover. The cover, illustrated by Mignola and by the Italian artist Nicola Mari, show Hellboy in the act of attacking a \"diabolic\" version of the Italian SF comic book character Nathan Never (with bat wings and pointed tail). Mari at the time was one of the artists that worked on Nathan Never, and the first two years of life of this comic were the main topic of the fanzine. With the exception of the cover, there is no other mention of Hellboy within the fanzine. The character shown was still in a draft stage, and although close to the final design of Hellboy, it had gray skin and an outfit not common to the character.\n\n\"Mike Mignola's Hellboy\" by Mike Mignola and John Byrne featured the character's first full appearance, and was a four-page black-and-white story that had an approximately 1,500 book print run. It was published by Dark Horse Comics in \"San Diego Comic-Con Comics\" #2 (August 1993) for distribution at the San Diego Comic-Con convention held in San Diego, California.\n\nHellboy travels to an American ghost town, where he encounters a mangy mutt that transforms into Anubis, the Ancient Egyptian god of mummification.\n\nThe story was collected in the trade paperback \"\".\n\nHellboy makes a guest appearance in John Byrne's \"Next Men\" #21; this is the first American appearance in a full color cameo.\n\n\"Mike Mignola's Hellboy: World's Greatest Paranormal Investigator\" by Mike Mignola and John Byrne featured the character's next solo appearance. It was published by Dark Horse Comics in a special four-page mini-comic for distribution in \"Comics Buyer's Guide\" #1,070 (May 20, 1994).\n\nIn the story Hellboy battles with the disembodied head of Nazi scientist Herman von Klempt and his puppet henchman Brutus the Gorilla to rescue a captive girl from the doctor's transference of nutrient fluids process.\n\nThe story was collected in the trade paperback \"\".\n\n\"Hi, My Name is Hellboy\" by Mike Mignola was a one-page panel ad that related the character's fictional origins. It was published by Diamond Comic Distributors in catalog supplement \"Celebrate Diversity\" collector's edition (October 1994). The ad was collected in the trade paperback \"The Art of Hellboy\".\n\n\"Hellboy: The First 20 Years\" was published 1 April 2014.\n\nThe film was directed and co-written by Guillermo del Toro and starred Ron Perlman as Hellboy (the favorite of both del Toro and Mignola for the role), Selma Blair as Liz Sherman, Rupert Evans as FBI Special Agent John Myers (a character created for the film), John Hurt as Professor Trevor Bruttenholm, Doug Jones as Abe Sapien (voiced by an uncredited David Hyde Pierce), Karel Roden as Grigori Rasputin, and Jeffrey Tambor as FBI Senior Special Agent Tom Manning. The film depicts Hellboy as living at the BPRD with a dozen cats and limited access to the outside world, and considered an urban legend by the general populace.\n\nA sequel, \"\", was shot in Budapest by Guillermo del Toro and released in 2008, with Perlman and Blair returning. Jones also returned as Abe Sapien (undubbed this time), and also in two other roles: The Angel of Death and The Chamberlain. Revolution Studios had planned on making the film (which Columbia Pictures was to distribute), but the studio went out of business before filming. Universal Studios then picked it up. The plot is a shift to more folklore rather than action, with heavy European overtones. The character of Johann Krauss was added to the team, voiced by Seth MacFarlane. The character Roger the Homunculus was not, but he was written into the plot as a very prominent character in early drafts of the script. The character of Agent Myers from the first film does not return, his absence being explained by Liz remarking that Hellboy had him transferred to Antarctica out of jealousy. Hellboy also reveals himself to the outside world in this film, and Liz is revealed to be pregnant with his children, twins. On November 11, 2008, \"Hellboy II: The Golden Army\" was released on DVD.\n\nA sequel for \"Hellboy II: The Golden Army\" was in development in 2009, entitled \"Hellboy III: Dark Worlds\", with Guillermo del Toro slated to return as the film's director and writer as well as Ron Perlman, Doug Jones, Seth MacFarlane, Selma Blair and Jeffrey Tambor were set to reprise their roles. In the sequel, Hellboy would live his normal life as a father for his two newborn twins with Liz Sherman, but also has to face an extremely powerful enemy who wishes to rule and bring the darkness upon Earth. In 2017, it was announced that the sequel was canceled due to the difficulty of funding the film, and a reboot would happen instead.\n\nIn May 2017, a reboot, titled \"Hellboy: Rise of the Blood Queen\", was announced by \"Hellboy\" creator Mike Mignola on his personal Facebook page revealing that the project is set to be directed by Neil Marshall and star David Harbour as the titular character. Mignola also stated that the film would have an R rating unlike previous installments. It is expected to be released in 2019. In August 2017, Ian McShane was cast as Trevor Bruttenholm. Mila Jovovich was cast as the film's main antagonist, Nimue the Blood Queen. On August 10, 2017, the Hellboy reboot dropped the \"Rise of the Blood Queen\" title and is now simply referred to as \"Hellboy\". On August 16, 2017, Sasha Lane was cast as Alice Monaghan. On August 21, 2017, Ed Skrein was cast as Major Ben Daimio. However, upon discovering that Daimio was portrayed in the comic books as a Japanese-American character, Skrein announced a week later that he was pulling out to allow an actor of Asian heritage to be cast instead. With the casting of Jovovich as Nimue the Blood Queen and Lane as Alice Monaghan, the film will take inspiration from the ideas in \"\" and \"\", due to these issues being their only appearances in Hellboy comics. Further validating this is the casting of Brian Gleeson as Merlin because the character's only appearance in a Hellboy comic was in \"Hellboy: The Storm and the Fury\". \"Hellboy\" is scheduled to be released on April 12, 2019. The film was previously scheduled to be released on January 11, 2019.\n\nOn November 9, 2005, IDT Entertainment issued a press release announcing that the company had licensed the rights to develop \"animated content for television and home entertainment\" based on the \"Hellboy\" comic. Ron Perlman (Hellboy), Selma Blair (Liz Sherman), Doug Jones (Abe Sapien) and John Hurt (Professor Trevor \"Broom\" Bruttenholm) have all voiced their respective characters. Actress Peri Gilpin joined the cast as Professor Kate Corrigan.\n\nThe first two 75-minute animated movies, \"\" and \"\", were aired on Cartoon Network before being released on DVD. The first one aired October 28, 2006, and the second aired March 17, 2007.\n\nBoth stories have much more in common with the comic book \"Hellboy\" rather than the film — Abe Sapien is not psychic, for example, and the artwork and color palette is derived more closely to Mignola's original artwork. The DVD of \"Sword of Storms\" was released on February 6, 2007; it contains documentary material commentary and a \"Hellboy\" comic, \"Phantom Limbs\". \"Blood and Iron\" similarly contains a comic called \"The Yearning\".\n\nAfter the initial release, some stores included exclusive giveaways with copies of the \"Hellboy Animated: Blood and Iron\" DVD:\n\nA \"Hellboy 2 Pak\" limited edition DVD set was released July 1, 2008, that contained both films and a 7\" figure.\n\nA third animated Hellboy film, \"The Phantom Claw\", has been put on hold. Tad Stones, director and writer of the direct-to-video movies, says the film will star Lobster Johnson and will have some familiar characters, but Abe and Liz will not be in the film (at least not as main characters).\n\nChristopher Golden has written several novels about the character, the first two of which, \"The Lost Army\" and \"The Bones of Giants\", are part of the official \"Hellboy\" story canon. The events of both these novels are listed in the comic's official timeline featured in \"Hellboy: The Companion\". In particular, the Golden-penned character of Anastasia Bransfield was also described in the companion, despite having never actually appeared in a comic.\n\n\nA Hellboy video game called \"\", developed by Cryo Interactive, was released in 2000 for Microsoft Windows. It was ported to PlayStation as \"Hellboy: Asylum Seeker\".\n\nOn April 6, 2005, \"Hellboy\" movie director Guillermo del Toro announced on his official site that he had made a deal with developer Konami to create a new \"Hellboy\" video game based on the movie version of the character and his world, featuring new monsters, new villains, and a new storyline. Herman von Klempt and his war ape Kriegaffe #10 were slated to make appearances. On May 9, 2006, it was revealed that the \"Hellboy\" game would appear in the summer of 2007, on PlayStation 3, Xbox 360, and PlayStation Portable. The game was released in North America on June 24, 2008 with the name \"\". It is developed by Krome Studios, and published by Konami Digital Entertainment, Inc. As well as single player campaign where the player gets to play as Hellboy the game also features co-op play, featuring the characters Abe Sapien and Liz Sherman. Two additional levels and Lobster Johnson as a playable character (voiced by Bruce Campbell) as DLC were developed but were unreleased. \n\nA \"Hellboy\" video game called \"Hellboy II: The Golden Army - Tooth Fairy Terror\" was released for the iPhone by Tuesday Creative on January 14, 2009.\n\nHellboy is a playable DLC character in \"Injustice 2\", voiced by Michael-Leon Wooley as part of the \"Fighter Pack 2\". The character was released for download on Tuesday, November 14, 2017. He is brought to Injustice universe by Brainiac who decides to add him to his collection as he is fascinated by Hellboy's human-like mind and personality despite being a demon. In his ending, Hellboy escapes from Brainiac's collection and defeats him. As a result he is asked to assist in rounding up local supervillains before eventually returning to the B.P.R.D. but finds his work there unfulfilling and ends up retiring to Africa.\n\n\nThe miniseries \"Hellboy: Conqueror Worm\" won a 2002 Eisner Award for \"Best Limited Series\", while \"The Art of Hellboy\" won an Eisner in 2004 for \"Best Comics-Related Book\". Mignola won a 2000 Harvey Award for \"Best Artist\", based on \"Hellboy: Box Full of Evil\". \"Hellboy: Darkness Calls\" won a 2007 Eagle Award for \"Favourite Colour Comicbook – American\".\n\nThe character Hellboy was nominated for \"Favourite Comics Character\" at the 2004 and 2005 Eagle Awards. Other Eagle Award nominations include \"Favourite Comics Story published during 2007\" for \"Hellboy: Darkness Calls\", and \"Favourite Comics Hero\".\n\nThe comics writer Alan Moore listed \"Hellboy\" on his recommendations page, particularly \"Wake the Devil (Vol. 2)\", calling it \"the skillful cutting and the setting of the stone that we can see Mignola's sharp contemporary sensibilities at work\".\n\nIn March 2009, Hellboy won two categories in the fan voted Project Fanboy Awards for 2008: \"Best Indy Hero\" and \"Best Indy Character\".\n\n\n\n"}
{"id": "36168859", "url": "https://en.wikipedia.org/wiki?curid=36168859", "title": "Hendrick Chin A Sen", "text": "Hendrick Chin A Sen\n\nHendrick Rudolf \"Henk\" Chin A Sen (; 18 January 1934 – 11 August 1999) was a Surinamese politician who served as the President of Suriname from 15 August 1980 until 4 February 1982.\n\nChin A Sen was born in the town of Albina, on 18 January 1934. He studied medicine at the medical school of Paramaribo and graduated in 1959. From 1959-1961, he began a general practice, then went to the Netherlands to specialize as an internist. When he returned to Suriname, he worked in the Sint Vincentius hospital in Paramaribo. Then he joined the \"Nationalist Republican Party\" (PNR), a party which pursued the independence of Suriname, although he was not very active.\n\nOn 15 March 1980, after the Sergeants Coup, which brought Dési Bouterse and his military council to power, Chin A Sen was installed as Prime Minister of Suriname. The appointment of the non-politically active Chin A Sen came as a surprise. Chin A Sen formed a leftist cabinet which also included two members of the National Military Council (NMR). On 15 August 1980, after President Johan Ferrier resigned, Chin A Sen assumed the post of President as well. However, Bouterse and Chin A Sen soon fell out. Bouterse sought a society with a socialist and revolutionary base in which the NMR would pull the strings in the background while Chin A Sen sought the restoration of democracy.\n\nOn 4 February 1982, Chin A Sen was sacked by Bouterse and exiled himself first to Pittsburgh in the United States and then to the Netherlands. In the Netherlands, after the December murders of 1982, Chin A Sen was chosen as Chairman of the Council for the Liberation of Suriname. The Council opposed the reign of Bouterse and his supporters, but it was not very successful. Chin A Sen was later in connection with Ronnie Brunswijk and his Jungle Commando, who waged an armed struggle against Bouterse.\n\nAn attack in Rijswijk in 1985 in which 3 people died during a meeting of the Council for the Liberation, was probably directed against Chin A Sen. He was, however, not present at that moment.\n\nIn 1995, Chin A Sen returned to Paramaribo, where he resumed his work as an internist. He died at the age of 65 in Paramaribo.\n"}
{"id": "6393740", "url": "https://en.wikipedia.org/wiki?curid=6393740", "title": "Herman H. Spitz", "text": "Herman H. Spitz\n\nHerman H. Spitz is an American psychologist known for his work measuring intelligence among those with developmental disability. He was Director of Research at the E.R. Johnstone Training and Research Center, which was a State Institution for upper level mentally retarded adolescents and young adults, in Bordentown, New Jersey, until he retired in 1989. He worked under the direction of the Superintendent John M. Wall, who retired in 1990 having served from August 1969.\n\nSpitz studied concepts such as mental age, and the abilities of autistic savants. He co-authored a survey of attempts to raise intelligence among people with mental retardation. He reported on programs like the Carolina Abecedarian Early Intervention Project which advocated the early education of poor children. Through use of the Wechsler Adult Intelligence Scale, he reported that the Flynn effect of massive intelligence quotient gains in a single generation in many nations only applied to people in the average intelligence range. He also looked at the hereditarian hypothesis for general intelligence factor by examining Wechsler subtest patterns among mentally retarded test-takers. He published 2 books and over 100 papers in scholarly journals and books. His last book was Nonconscious Movements: From Mystical Messages to Facilitated Communication, Lawrence Erlbaum Associates (1997).\n\nIn 1994 he was one of 52 signatories on \"Mainstream Science on Intelligence,\" an editorial written by Linda Gottfredson and published in the \"Wall Street Journal\", which declared the consensus of the signing scholars on the measurement and significance of intelligence following the publication of the book \"The Bell Curve\".\n"}
{"id": "22791739", "url": "https://en.wikipedia.org/wiki?curid=22791739", "title": "History of attention deficit hyperactivity disorder", "text": "History of attention deficit hyperactivity disorder\n\nHyperactivity has long been part of the human condition, although hyperactive behaviour has not always been seen as problematic.\n\nThe terminology used to describe the symptoms of Attention-Deficit Hyperactivity Disorder, or ADHD, has gone through many changes over history, including \"minimal brain damage\", \"minimal brain dysfunction\", \"learning/behavioral disabilities\" and \"hyperactivity\". In the second edition of the Diagnostic and Statistical Manual of Mental Disorders, known as DSM-II (1968), the condition was called \"Hyperkinetic Reaction of Childhood\". It was in the 1980 DSM-III that \"ADD (Attention-Deficit Disorder) with or without hyperactivity\" was introduced. In 1987 this label was further refined to \"ADHD (Attention-Deficit Hyperactivity Disorder)\" in the DSM-III-R and subsequent editions, including the current DSM-5.\n\nA number of early writers described human behaviour patterns similar to today's definitions of ADHD.\n\nIn 1775, Melchior Adam Weikard, a prominent German physician, published the textbook \"Der Philosophische Arzt\". Weikard's text contained a description of ADHD-like behaviours, possibly the first ever such description in medical literature Weikard described many of the symptoms now associated with the inattentive dimension of ADHD in the Diagnostic and Statistical Manual of Mental Disorders. For instance, according to the English translation provided by Barkley and Peters, Weikard stated that:\n\nAn inattentive person won't remark anything but will be shallow everywhere. He studies his matters only superficially; his judgements are erroneous and he misconceives the worth of things because he does not spend enough time and patience to search a matter individually or by the piece with the adequate accuracy. Such people only hear half of everything; they memorize or inform only half of it or do it in a messy manner. According to a proverb they generally know a little bit of all and nothing of the whole…. They are mostly reckless, often copious considering imprudent projects, but they are also most inconstant in execution. They treat everything in a light manner since they are not attentive enough to feel denigration or disadvantages.\n\nAccording to Weikard, the treatment recommended was:\nThe inattentive person is to be separated from the noise or any other objects; he is to be kept solitary, in the dark, when he is too active. The easily agile fibres are to be fixated by rubbing, cold baths, steel powder, cinchona, mineral waters, horseback riding, and gymnastic exercises.\n\nScottish-born physician and author, Sir Alexander Crichton described, in 1798, a mental state much like the inattentive subtype of ADHD, in his book \"An Inquiry into the Nature and Origin of Mental Derangement.\" Crichton had received some of his medical training in Germany and may well have known Weikard given that his training occurred in several of the towns where Weikard was known to have practiced medicine. More detailed in his observation than Weikard, Crichton described attention problems as:\nThe incapacity of attending with a necessary degree of constancy to any one object, almost always arises from an unnatural or morbid sensibility of the nerves, by which means this faculty is incessantly withdrawn from one impression to another. It may be either born with a person, or it may be the effect of accidental diseases.\n\nWhen born with a person it becomes evident at a very early period of life, and has a very bad effect, inasmuch as it renders him incapable of attending with constancy to any one object of education. But it seldom is in so great a degree as totally to impede all instruction; and what is very fortunate, it is generally diminished with age.\nCrichton further observed:\nIn this disease of attention, if it can with propriety be called so, every impression seems to agitate the person, and gives him or her an unnatural degree of mental restlessness. People walking up and down the room, a slight noise in the same, the moving of a table, the shutting a door suddenly, a slight excess of heat or of cold, too much light, or too little light, all destroy constant attention in such patients, inasmuch as it is easily excited by every impression.\n\nCrichton noted that \"…they have a particular name for the state of their nerves, which is expressive enough of their feelings. They say they have the \"fidgets\".\" Dr. Crichton suggested that these children needed special educational intervention and noted that it was obvious that they had a problem attending even how hard they did try. \"Every public teacher must have observed that there are many to whom the dryness and difficulties of the Latin and Greek grammars are so disgusting that neither the terrors of the rod, nor the indulgence of kind intreaty can cause them to give their attention to them.\"\n\nBoth Melchior Adam Weikard and Alexander Crichton wrote about the occupationally disabling features of this disorder, including attentional problems, restlessness, early onset, and how it can affect schooling, without any of the moralism introduced by George Still and later authors.\n\nIn March 1902, Sir George Frederic Still (1868–1941), known as the father of British paediatrics, gave a series of lectures to the Royal College of Physicians in London under the name Goulstonian Lectures on ‘some abnormal psychical conditions in children’, which were published later the same year in \"The Lancet\".\n\nHe described 43 children who had serious problems with sustained attention and self-regulation, who were often aggressive, defiant, resistant to discipline, excessively emotional or passionate, which showed little inhibitory volition, and could not learn from the consequences of their actions; though their intellect was normal. He wrote: \"I would point out that a notable feature in many of these cases of moral defect without general impairment of intellect is a quite abnormal incapacity for sustained attention.\n\nDr. Still wrote: \"there is a defect of moral consciousness which cannot be accounted for by any fault of environment\". When Still was talking about moral control, he was referring to it as William James had done before him, but to Still, the moral control of behavior meant \"the control of action in conformity with the idea of the good of all.\"\n\n\"Another boy, aged 6 years, with marked moral defect was unable to keep his attention even to a game for more than a very short time, and as might be expected, the failure of attention was very noticeable at school, with the result that in some cases the child was backward in school attainments, although in manner and ordinary conversation he appeared as bright and intelligent as any child could be.\" He proposed a biological predisposition to this behavioral condition that was probably hereditary in some children and the result of pre- or postnatal injury in others.\n\nMany historians of ADHD have inferred that the children Still described in his series of three published lectures to the Royal College of Physicians would likely have qualified for the current disorder of ADHD combined type, among other disorders.\n\nThe clinical definition of \"ADHD\" dates to the mid-20th century, but was known by other names. Physicians developed a diagnosis for a set of conditions variously referred to as \"minimal brain damage\", \"minimal brain dysfunction\", \"minimal brain disorder\", \"learning/behavioral disabilities\" and \"hyperactivity\". Some of these labels became problematic as knowledge expanded. For example, as awareness grew that many children with no indication of brain damage also displayed the syndrome, the label which included the words \"brain damage\" did not seem appropriate.\n\nThe DSM-II (1968) began to call it \"Hyperkinetic Reaction of Childhood\" even though the professionals were aware that many of the children so diagnosed exhibited attention deficits without any signs of hyperactivity. In 1980, the DSM-III introduced the term \"ADD (Attention-Deficit Disorder) with or without hyperactivity.\" That terminology (ADD) technically expired with the revision in 1987 to ADHD in the DSM-III-R. In the DSM-IV, published in 1994, ADHD with sub-types was presented. The DSM-IV-TR was released in 2000, primarily to correct factual errors and make changes to reflect recent research; ADHD was largely unchanged.\n\nUnder the DSM-5, there are three ADHD presentations, including one which lacks the hyperactivity component. Approximately one-third of people with ADHD have the predominantly inattentive presentation (ADHD-I), meaning that they do not have the hyperactive or overactive behavior components of the other ADHD presentations.\n\nEven today, the ADHD terminology is objectionable to many. There is some preference for using the ADHD-I, ADD, and AADD terminology when describing individuals lacking the hyperactivity component, especially among older adolescents and adults who find the term \"hyperactive\" inaccurate, inappropriate and even derogatory.\n\nThe treatment of children with similar behavioral problems who had survived the epidemic of encephalitis lethargica from 1917 to 1918 and the pandemic of influenza from 1919 to 1920 led to terminology which referred to \"brain damage.\" This would also be called \"post-encephalitic behavior disorder.\" The association of symptoms similar to ADHD in the surviving children eventually led later authors to speculate that whenever the behavior pattern may be present, it may reflect an underlying disturbance of or damage to the brain. The syndrome came to be known as brain-injured child syndrome, to be amended later to minimal brain damage, and subsequently to minimal brain dysfunction.\n\nIn the 1970s, American research began to study the symptoms and development of children diagnosed with ADHD. By the 1980s, research was published confirming the continuation of ADHD symptoms beyond childhood. Some controversy exists over the findings of scholars such as Gabrielle Weiss in 1986, which showed a 66% continuation of symptoms into adulthood, contrasted with a lower 31% reported by Gittleman et al. Research continued from there, often based on the model that ADHD could only be continued and not recognized and diagnosed newly in adults and adolescents. Publications by numerous individuals, including Kelly and Ramundo as well as Hallowell and Ratey in the 1990s, complicated this model by not only inspiring self-diagnosis but also through promoting the social model of disability. Currently there exists significant social and medical debate surrounding medication and prevailing diagnosis of ADHD in adults as well as children. Partly, this is influenced by media and agenda setting. As analyzed by Conrad and Potter, \"ironically, controversy about ADHD raises the public's awareness and increases the diffusion of information about the disorder, which can indirectly contribute to diagnostic expansion.\"\n\n"}
{"id": "7191296", "url": "https://en.wikipedia.org/wiki?curid=7191296", "title": "House Gymnastics", "text": "House Gymnastics\n\nHouse Gymnastics is a sport created by James Robert Ford and Spencer Harrison.\n\nThis fitness regime is akin to an indoor version of Parkour or an internet based, Fluxus \"happening\", which encourages maximum audience participation. The participant uses their surroundings in their house as apparatus. When someone performs House Gymnastics, the aim is to create human sculptures that last around 3 seconds. Viewers can sign up as members, submit photos and enter the Move of Month competition.\n\nHouse Gymnastics originated during an attempt by Harrison and Ford to put up a bedroom blind. \"The Brace\" and \"The 25th Element\" were the first moves conceived. From there on in moves were being created on a daily basis. Language was developed to compliment the physicality of House Gymnastics, and new moves and areas of were given names. As such, words such as \"busted\" and \"amped\" were developed and entered the \"House Gymnastics\" vocabulary.\n\nInjuries due to the sport are seen as unfortunate, but inevitable, and one reviewer of the site and product said it would be more likely to lead to a trip to the hospital than a thrill, for people who aren't trained professionals.\n\n"}
{"id": "13292", "url": "https://en.wikipedia.org/wiki?curid=13292", "title": "Hypoxia (medical)", "text": "Hypoxia (medical)\n\nHypoxia is a condition in which the body or a region of the body is deprived of adequate oxygen supply at the tissue level. Hypoxia may be classified as either \"generalized\", affecting the whole body, or \"local\", affecting a region of the body. Although hypoxia is often a pathological condition, variations in arterial oxygen concentrations can be part of the normal physiology, for example, during hypoventilation training or strenuous physical exercise.\n\nHypoxia differs from hypoxemia and anoxemia in that hypoxia refers to a state in which oxygen supply is insufficient, whereas hypoxemia and anoxemia refer specifically to states that have low or zero arterial oxygen supply. Hypoxia in which there is complete deprivation of oxygen supply is referred to as anoxia.\n\nGeneralized hypoxia occurs in healthy people when they ascend to high altitude, where it causes altitude sickness leading to potentially fatal complications: high altitude pulmonary edema (HAPE) and high altitude cerebral edema (HACE). Hypoxia also occurs in healthy individuals when breathing mixtures of gases with a low oxygen content, e.g. while diving underwater especially when using closed-circuit rebreather systems that control the amount of oxygen in the supplied air. Mild, non-damaging intermittent hypoxia is used intentionally during altitude training to develop an athletic performance adaptation at both the systemic and cellular level.\n\nHypoxia is a common complication of preterm birth in newborn infants. Because the lungs develop late in pregnancy, premature infants frequently possess underdeveloped lungs. To improve lung function, doctors frequently place infants at risk of hypoxia inside incubators (also known as humidicribs) that provide continuous positive airway pressure.\n\nThe symptoms of generalized hypoxia depend on its severity and acceleration of onset.\n\nIn the case of altitude sickness, where hypoxia develops gradually, the symptoms include fatigue, numbness / tingling of extremities, nausea, and anoxia. These symptoms are often difficult to identify, but early detection of symptoms can be critical.\n\nIn severe hypoxia, or hypoxia of very rapid onset, ataxia, confusion / disorientation / hallucinations / behavioral change, severe headaches / reduced level of consciousness, papilloedema, breathlessness, pallor, tachycardia, and pulmonary hypertension eventually leading to the late signs cyanosis, slow heart rate / cor pulmonale, and low blood pressure followed by death.\n\nBecause hemoglobin is a darker red when it is not bound to oxygen (deoxyhemoglobin), as opposed to the rich red color that it has when bound to oxygen (oxyhemoglobin), when seen through the skin it has an increased tendency to reflect blue light back to the eye. In cases where the oxygen is displaced by another molecule, such as carbon monoxide, the skin may appear 'cherry red' instead of cyanotic. Hypoxia can cause premature birth, and injure the liver, among other deleterious effects.\n\nIf tissue is not being perfused properly, it may feel cold and appear pale; if severe, hypoxia can result in cyanosis, a blue discoloration of the skin. If hypoxia is very severe, a tissue may eventually become gangrenous.\nExtreme pain may also be felt at or around the site.\n\nTissue hypoxia from low oxygen delivery may be due to low haemoglobin concentration (anaemic hypoxia), low cardiac output (stagnant hypoxia) or low haemoglobin saturation (hypoxic hypoxia). The consequence of oxygen deprivation in tissues is a switch to anaerobic metabolism at the cellular level. As such, reduced systemic blood flow may result in increased serum lactate. Serum lactate levels have been correlated with illness severity and mortality in critically ill adults and in ventilated neonates with respiratory distress.\n\nOxygen passively diffuses in the lung alveoli according to a pressure gradient. Oxygen diffuses from the breathed air, mixed with water vapour, to arterial blood, where its partial pressure is around 100 mmHg (13.3 kPa). In the blood, oxygen is bound to hemoglobin, a protein in red blood cells. The binding capacity of hemoglobin is influenced by the partial pressure of oxygen in the environment, as described in the oxygen–hemoglobin dissociation curve. A smaller amount of oxygen is transported in solution in the blood.\n\nIn peripheral tissues, oxygen again diffuses down a pressure gradient into cells and their mitochondria, where it is used to produce energy in conjunction with the breakdown of glucose, fats, and some amino acids.\n\nHypoxia can result from a failure at any stage in the delivery of oxygen to cells. This can include decreased partial pressures of oxygen, problems with diffusion of oxygen in the lungs, insufficient available hemoglobin, problems with blood flow to the end tissue, and problems with breathing rhythm.\n\nExperimentally, oxygen diffusion becomes rate limiting (and lethal) when arterial oxygen partial pressure falls to 60 mmHg (5.3 kPa) or below. \n\nAlmost all the oxygen in the blood is bound to hemoglobin, so interfering with this carrier molecule limits oxygen delivery to the periphery. Hemoglobin increases the oxygen-carrying capacity of blood by about 40-fold, with the ability of hemoglobin to carry oxygen influenced by the partial pressure of oxygen in the environment, a relationship described in the oxygen–hemoglobin dissociation curve. When the ability of hemoglobin to carry oxygen is interfered with, a hypoxic state can result.\n\nIschemia, meaning insufficient blood flow to a tissue, can also result in hypoxia. This is called 'ischemic hypoxia'. This can include an embolic event, a heart attack that decreases overall blood flow, or trauma to a tissue that results in damage. An example of insufficient blood flow causing local hypoxia is gangrene that occurs in diabetes.\n\nDiseases such as peripheral vascular disease can also result in local hypoxia. For this reason, symptoms are worse when a limb is used. Pain may also be felt as a result of increased hydrogen ions leading to a decrease in blood pH (acidity) created as a result of anaerobic metabolism.\n\nThis refers specifically to hypoxic states where the arterial content of oxygen is insufficient. This can be caused by alterations in respiratory drive, such as in respiratory alkalosis, physiological or pathological shunting of blood, diseases interfering in lung function resulting in a ventilation-perfusion mismatch, such as a pulmonary embolus, or alterations in the partial pressure of oxygen in the environment or lung alveoli, such as may occur at altitude or when diving.\n\nCarbon monoxide competes with oxygen for binding sites on hemoglobin molecules. As carbon monoxide binds with hemoglobin hundreds of times tighter than oxygen, it can prevent the carriage of oxygen.\nCarbon monoxide poisoning can occur acutely, as with smoke intoxication, or over a period of time, as with cigarette smoking. Due to physiological processes, carbon monoxide is maintained at a resting level of 4–6 ppm. This is increased in urban areas (7–13 ppm) and in smokers (20–40 ppm). A carbon monoxide level of 40 ppm is equivalent to a reduction in hemoglobin levels of 10 g/L.\n\nCO has a second toxic effect, namely removing the allosteric shift of the oxygen dissociation curve and shifting the foot of the curve to the left. In so doing, the hemoglobin is less likely to release its oxygens at the peripheral tissues. Certain abnormal hemoglobin variants also have higher than normal affinity for oxygen, and so are also poor at delivering oxygen to the periphery.\n\nAtmospheric pressure reduces with altitude and with it, the amount of oxygen. The reduction in the partial pressure of inspired oxygen at higher altitudes lowers the oxygen saturation of the blood, ultimately leading to hypoxia. The clinical features of altitude sickness include: sleep problems, dizziness, headache and oedema.i\n\nThe breathing gas in scuba diving may contain an insufficient partial pressure of oxygen, particularly in malfunction of rebreathers. Such situations may lead to unconsciousness without symptoms since carbon dioxide levels are normal and the human body senses pure hypoxia poorly.\n\nA similar problem exists when inhaling certain odorless asphyxiant gases. Asphyxiant gases reduce/displace the normal oxygen concentration in breathing air, where prolonged exposure to this hypoxic breathing gas leads to unconsciousness, followed by death by inert gas asphyxiation (suffocation). When oxygen level dips below 19.5% v/v, the air is considered oxygen-deficient, where oxygen concentrations below 16% volume are considered highly dangerous for humans. As asphyxiant gases are relatively inert and odorless, their presence may not be noticed until the effects of elevated blood carbon dioxide (hypercapnia) are recognized by the body. Inert gas asphyxiation may be deliberate with use of a suicide bag. Accidental death has occurred in cases where concentrations of nitrogen in controlled atmospheres, or methane in mines, has not been detected or appreciated.\n\nHemoglobin's function can also be lost by chemically oxidizing its iron atom to its ferric form. This form of inactive hemoglobin is called methemoglobin and can be made by ingesting sodium nitrite as well as certain drugs and other chemicals.\n\nHemoglobin plays a substantial role in carrying oxygen throughout the body, and when it is deficient, anemia can result, causing 'anaemic hypoxia' if tissue perfusion is decreased. Iron deficiency is the most common cause of anemia. As iron is used in the synthesis of hemoglobin, less hemoglobin will be synthesised when there is less iron, due to insufficient intake, or poor absorption.\n\nAnemia is typically a chronic process that is compensated over time by increased levels of red blood cells via upregulated erythropoetin. A chronic hypoxic state can result from a poorly compensated anaemia.\n\nHistotoxic hypoxia results when the quantity of oxygen reaching the cells is normal, but the cells are unable to use the oxygen effectively as a result of disabled oxidative phosphorylation enzymes. This may occur in cyanide poisoning.\n\nIf oxygen delivery to cells is insufficient for the demand (hypoxia), electrons will be shifted to pyruvic acid in the process of lactic acid fermentation. This temporary measure (anaerobic metabolism) allows small amounts of energy to be released. Lactic acid build up (in tissues and blood) is a sign of inadequate mitochondrial oxygenation, which may be due to hypoxemia, poor blood flow (e.g., shock) or a combination of both. If severe or prolonged it could lead to cell death. \n\nIn humans, hypoxia is detected by the peripheral chemoreceptors in the carotid body and aortic body, with the carotid body chemoreceptors being the major mediators of reflex responses to hypoxia. This response does not control ventilation rate at normal p, but below normal the activity of neurons innervating these receptors increases dramatically, so much so to override the signals from central chemoreceptors in the hypothalamus, increasing p despite a falling p\n\nIt is seen in a few humans (encountered with hypoxia), there is word loss in their speech due to their state of confusion and cell damage in the brain.\n\nIn most tissues of the body, the response to hypoxia is vasodilation. By widening the blood vessels, the tissue allows greater perfusion.\n\nBy contrast, in the lungs, the response to hypoxia is vasoconstriction. This is known as hypoxic pulmonary vasoconstriction, or \"HPV\".\n\nWhen the pulmonary capillary pressure remains elevated chronically (for at least 2 weeks), the lungs become even more resistant to pulmonary edema because the lymph vessels expand greatly, increasing their capability of carrying fluid away from the interstitial spaces perhaps as much as 10-fold. Therefore, in patients with chronic mitral stenosis, pulmonary capillary pressures of 40 to 45 mm Hg have been measured without the development of lethal pulmonary edema.[Guytun and Hall physiology]\n\nHypoxia exists when there is a reduced amount of oxygen in the tissues of the body. Hypoxemia refers to a reduction in PO2 below the normal range, regardless of whether gas exchange is impaired in the lung, CaO2 is adequate, or tissue hypoxia exists. There are several potential physiologic mechanisms for hypoxemia, but in patients with COPD the predominant one is V/Q mismatching, with or without alveolar hypoventilation, as indicated by PaCO2. Hypoxemia caused by V/Q mismatching as seen in COPD is relatively easy to correct, so that only comparatively small amounts of supplemental oxygen (less than 3 L/min for the majority of patients) are required for LTOT. Although hypoxemia normally stimulates ventilation and produces dyspnea, these phenomena and the other symptoms and signs of hypoxia are sufficiently variable in patients with COPD as to be of limited value in patient assessment. Chronic alveolar hypoxia is the main factor leading to development of cor pulmonale—right ventricular hypertrophy with or without overt right ventricular failure—in patients with COPD. Pulmonary hypertension adversely affects survival in COPD, to an extent that parallels the degree to which resting mean pulmonary artery pressure is elevated. Although the severity of airflow obstruction as measured by FEV1 is the best correlate with overall prognosis in patients with COPD, chronic hypoxemia increases mortality and morbidity for any severity of disease. Large-scale studies of LTOT in patients with COPD have demonstrated a dose-response relationship between daily hours of oxygen use and survival. There is reason to believe that continuous, 24-hours-per-day oxygen use in appropriately selected patients would produce a survival benefit even greater than that shown in the NOTT and MRC studies.\n\nTo counter the effects of high-altitude diseases, the body must return arterial p toward normal. Acclimatization, the means by which the body adapts to higher altitudes, only partially restores p to standard levels. Hyperventilation, the body’s most common response to high-altitude conditions, increases alveolar p by raising the depth and rate of breathing. However, while p does improve with hyperventilation, it does not return to normal. Studies of miners and astronomers working at 3000 meters and above show improved alveolar p with full acclimatization, yet the p level remains equal to or even below the threshold for continuous oxygen therapy for patients with chronic obstructive pulmonary disease (COPD). In addition, there are complications involved with acclimatization. Polycythemia, in which the body increases the number of red blood cells in circulation, thickens the blood, raising the danger that the heart can’t pump it.\n\nIn high-altitude conditions, only oxygen enrichment can counteract the effects of hypoxia. By increasing the concentration of oxygen in the air, the effects of lower barometric pressure are countered and the level of arterial p is restored toward normal capacity. A small amount of supplemental oxygen reduces the equivalent altitude in climate-controlled rooms. At 4000 m, raising the oxygen concentration level by 5 percent via an oxygen concentrator and an existing ventilation system provides an altitude equivalent of 3000 m, which is much more tolerable for the increasing number of low-landers who work in high altitude. In a study of astronomers working in Chile at 5050 m, oxygen concentrators increased the level of oxygen concentration by almost 30 percent (that is, from 21 percent to 27 percent). This resulted in increased worker productivity, less fatigue, and improved sleep.\n\nOxygen concentrators are uniquely suited for this purpose. They require little maintenance and electricity, provide a constant source of oxygen, and eliminate the expensive, and often dangerous, task of transporting oxygen cylinders to remote areas. Offices and housing already have climate-controlled rooms, in which temperature and humidity are kept at a constant level. Oxygen can be added to this system easily and relatively cheaply.\n\nA prescription renewal for home oxygen following hospitalization requires an assessment of the patient for ongoing hypoxemia.\n\n"}
{"id": "33236555", "url": "https://en.wikipedia.org/wiki?curid=33236555", "title": "International Coffee Day", "text": "International Coffee Day\n\nInternational Coffee Day is an occasion that is used to promote and celebrate coffee as a beverage, with events now occurring in places across the world. The first official date was 1 October 2015, as agreed by the International Coffee Organization and was launched in Milan. This day is also used to promote fair trade coffee and to raise awareness for the plight of the coffee growers. On this day, many businesses offer free or discounted cups of coffee. Some businesses share coupons and special deals with their loyal followers via social networking. Some greeting card companies sell National Coffee Day greeting cards as well as free e-cards.\n\nAt a meeting on 3–7 March 2014, a decision was taken by the International Coffee Organization to launch the first official International Coffee Day in Milan as part of Expo 2015.\n\nVarious events have been held, called Coffee Day or National Coffee Day, with many of these on or around September 29.\n\nThe exact origin of International Coffee Day is unknown. An event was first promoted in Japan in 1983 by The All Japan Coffee Association (全日本コーヒー協会). In the United States \"National Coffee Day\" was mentioned publicly as early as 2005. The name \"International Coffee Day\" was first used by the Southern Food and Beverage Museum, which called a press conference on October 3, 2009 to celebrate it and to announce the first New Orleans Coffee Festival. It was promoted in China by the International Coffee Organization, first celebrated in 1997, and made into an annual celebration in early April 2001. Taiwan first celebrated International Coffee Day in 2009. Nepal first celebrated National Coffee Day on November 17, 2005. Indonesia, which first celebrated National Coffee Day on August 17, 2006, celebrates it on the same day as Indonesia's Independence Day.\n\n"}
{"id": "56243", "url": "https://en.wikipedia.org/wiki?curid=56243", "title": "Ixtab", "text": "Ixtab\n\nAt the time of the Spanish conquest of Yucatán (1527-1546), Ix Tab, or Ixtab ([iʃˈtaɓ], 'Woman whose work involves the use of a rope'), was the indigenous Mayan goddess of suicide by hanging. Playing the role of a psychopomp, she would accompany such suicides to heaven. No certain depictions of Ixtab are known.\n\nThe only description of the goddess occurs in the so-called ‘Relación’ of the 16th-century Spanish inquisitor, Diego de Landa:\n\nBeyond this description, there is only a very brief and somewhat obscure mention of Ix Tab in the Book of Chilam Balam of Tizimin and in the Pérez Codex, in a context of chaos, suffering, and hangings: “They suspended Ix Tab from their hands,” or, alternatively, “Ix Tab suspended them from her hands.”\n\n\"Ix Tab\" is the female form of \"ah tab\" 'hangman'. The function of Ix Tab as a benevolent 'hangwoman' could derive from a basic association with snares. Landa (Tozzer 1941: 155) mentions the hunting deity [\"Ah\"] \"Tabay\" ('Ensnarer' or 'Deceiver'), possibly a patron of hunting with snares, including such that hoist the prey into the air. Animals hoisted by such snares are found depicted in the Dresden and Madrid codices, the Madrid codex (MC45c) personifying one of these traps by a male hunting deity. Ix Tab could be understood as a specialized, female form of such a deity, luring the human quarry into the hanging rope personified by her. Suicides freely putting their heads into this 'snare' (prompted, perhaps, by a dream) could then be seen to consecrate themselves to her. On the other hand, the Xtabay of contemporary folklore is a seductive female demon ‘ensnaring’ or 'deceiving' her male human preys so as to madden and destroy them.\n\nThe Dresden Codex picture (DC53b) of a dead woman with a rope around the neck, suspended from a celestial bar, is often, and without further proof, taken to represent Ix Tab. However, since the picture occurs in a section devoted to eclipses of sun and moon, it may rather have been used to symbolize a lunar eclipse and its dire consequences for women, who were intimately associated with the moon goddess.\n\nIt has been claimed that the Pre-Spanish Maya did not have a suicide goddess, or a significant narrative of suicide by hanging. Originally, Ix Tab may only have been a hunting goddess (see above, 'Comparisons'). Today, the sensationalist idea of a ‘cult of Ix Tab’ appears to be invoked by popular Yucatecan media to portray suicide as an indigenous problem, given that Yucatán has a suicide rate more than double that of Mexico at large.\n\n"}
{"id": "13645928", "url": "https://en.wikipedia.org/wiki?curid=13645928", "title": "Mental Illness Awareness Week", "text": "Mental Illness Awareness Week\n\nMental Illness Awareness Week (MIAW) (also known as Mental Health Awareness Week) was established in 1990 by the U.S. Congress in recognition of efforts by the National Alliance on Mental Illness (NAMI) to educate and increase awareness about mental illness. It takes place every year during the first full week of October. During this week, mental health advocates and organizations across the U.S. join to sponsor events to promote community outreach and public education concerning mental illnesses such as major depressive disorder, bipolar disorder, and schizophrenia. Examples of activities held during the week include art/music events, educational sessions provided by healthcare professionals, advertising campaigns, health fairs, movie nights, candlelight vigils, and benefit runs.\n\nAn estimated 26.2 percent of Americans ages 18 and older—about one in four adults—are believed to be diagnosable with a mental illness in any given year. The numbers may be larger because stigma reduces reporting. Not only are these adults affected by one mental illness; 45% of these adults meet criteria for two or more disorders. These range from fairly common mood disorders to the much more serious anxiety and schizophrenia disorders. Among these, anxiety disorders were the most common, as some 40 million American adults ages 18 and older experience some form of anxiety disorder. Despite the large number of Americans affected by such disorders, stigma surrounding mental illness is a major barrier that prevents people from seeking the mental health treatment that they need. Programs during Mental Illness Awareness Week are designed to create community awareness and discussion in an effort to put an end to stigma and advocate for treatment and recovery.\n\nMental Illness Awareness Week coincides with similar organization campaigns in early October such as World Mental Health Day (World Federation for Mental Health), National Depression Screening Day (Screening for Mental Health), and National Day Without Stigma (Active Minds).\n\n\n"}
{"id": "30916247", "url": "https://en.wikipedia.org/wiki?curid=30916247", "title": "Neglected tropical disease research and development", "text": "Neglected tropical disease research and development\n\nNeglected tropical diseases (NTDs) are a set of infectious diseases affecting an estimated 1.4 billion people worldwide. The classification of this group of neglected diseases is linked to their frequent neglect in public and private sector expenditure and attention at local, national, and international levels, and their concentration among the poor. Research and development yielding safe, effective drugs and vaccines for their treatment and prevention has been recognized as a global health priority.\n\nNeglected tropical diseases are a set of infectious communicable diseases arising from a diverse group of parasitic worms, bacteria, and vector-borne protozoa. The NTDs result in an estimated 534,000 deaths annually and 57 million disability-adjusted life years (DALYs) lost. The social, economic, and health burden of these diseases falls primarily on low and middle income countries where the diseases are most prevalent. The NTDs represent the sixth greatest global health burden in terms of DALYs, equal to or potentially surpassing global malaria burden.\n\nNTD interventions include both programs to address environmental and social determinants of health (e.g., vector control, water quality, sanitation), and programs offering mass drug administration for disease prevention and treatment. Drug treatments exist to confront many of the NTDs and represent some of the world's essential medicines. Despite significant health and economic improvements using available medicines, the low number of new compounds being researched and developed for NTDs is an ongoing and significant challenge. The dearth of candidates in pharmaceutical company drug pipelines is primarily attributed to the high costs of drug development and the fact that NTDs are concentrated among the worlds' poor. Other disincentives to investment include weak existing infrastructure for distribution and sales, and concerns regarding intellectual property protection. However, the major stakeholders in NTD drug development—governments, foundations, pharmaceutical companies, academia, and NGOs—are involved in activities to help address the research and development shortfall and meet the many challenges presented by neglected tropical diseases. Initiatives include public private partnerships, global R&D capacity building, priority vouchers to speed drug approval processes, open source scientific collaborations, and harmonization of global governance structures concerning NTDs.\n\nThe diseases considered neglected tropical diseases vary. Malaria, HIV, and tuberculosis have received an amount of public attention and increased funding to no longer be considered neglected by some researchers. Outside \"The Big Three\", the seven most prevalent neglected tropical diseases in order of their global prevalence are ascariasis, trichuriasis, hookworm infection, schistosomiasis, lymphatic filariasis, and trachoma. These seven are among a larger list of thirteen major NTDs: onchocerciasis, leishmaniasis, Chagas' disease, leprosy, Human African trypanosomiasis (sleeping sickness), Dracunculiasis, and Buruli ulcer.\n\nThe World Health Organization's 2010 report dedicated to neglected tropical diseases offers an expanded list including dengue, rabies, yaws, cysticercosis, echinococcosis, and foodborne trematode infections.\nND=Not Determined\n\nIn their 2002 review of the U.S. Food and Drug Administration (FDA) databases and the European Agency for the Evaluation of Medicinal Products, Troullier \"et al\" found that 16 out of 1393 new chemical entities were approved for NTDs between 1975 and 1999 (~1%). Cohen \"et al\" revisited the data and using the same methodology found 32 new chemical entities during the time period. In a second analysis using an expanded list of NTDs based on the G-FINDER survey, the number was slightly higher, with 46 new drugs and vaccines approved (~3% of the total including HIV drugs). Between 2000 and 2009, there has been some increase with an additional 26 newly approved drugs and vaccines for NTDs.\n\nA number of factors are recognized as contributing to the low number. The barrier most reported is the high cost of drug development. Estimates are that pharmaceutical companies' development costs to approval fall between $500 million and $2 billion. DiMasi, Hansen, and Grabowski calculated an average of $802 million in year 2000 dollars. Furthermore, the time that drugs are approved for use averages seven years out of the twenty years on-patent, meaning a tendency for the market to focus on diseases of developed nations where high prices can be used to recoup research and development costs, and subsidize failed R&D efforts. In short, NTD research and development is considered a high investment risk given that NTDs predominantly affect the poor in low and middle income countries. Additional barriers include drug safety regulatory requirements, intellectual property protection problems, and poor infrastructure for distribution and sales.\n\nAlthough drug companies have not invested heavily in the NTDs, in several cases, rather than focus on profits, some have decided to donate key drugs to address NTDs. For example, Merk has had a program since the mid-1980s to donate ivermectin (Mectizan) indefinitely to support the global fight of onchoceriasis. GlaxoSmithKline and several other large pharmaceutical companies have donation programs as well. Drug donation however, does not ameliorate the deficiency of new chemical entities being researched and developed. This is especially of concern with reports of emerging resistance among existing drugs.\n\nGovernments, foundations, the non-profit sector, and private sector have found new connections to help address market deficiencies by providing funding support and spreading both the costs and risks of NTD research and development. The proliferation of public private partnerships (PPPs) has been recognized as a key innovation in the past decade, helping to unlock existing and new resources.\n\nMajor PPPs for NTDs include: the Sabin Vaccine Institute, Norvartis Vaccines Institute for Global Health, MSD Wellcome Trust Hilleman Laboratories, Infectious Diseases Research Institute, Institut Pasteur and INSERM, WIPO Re:Search, and the International Vaccine Institute. Likewise, a number of new academic drug development centers have been created in recent years drawing in industry partners. Support for these centers is frequently traced to the Bill and Melinda Gates Foundation, the Sandler Foundation, and the Wellcome Trust.\n\nGrowing NTD research and development capacity in middle income countries is an area of policy interest. A 2009 study of biotechnology companies in India, China, Brazil, and South Africa revealed sixty-two NTD products in development and on the market out of approximately five hundred products offered (~14%). When products to fight HIV, malaria, and TB were included in the analysis, the number increased to one hundred twenty-three products, approximately 25% of the total products offered.\n\nResearchers have argued that unlike most multinationals, small and mid-sized \"Global South\" companies see significant business opportunities in the development of NTD-related diagnostics, biologics, pharmaceuticals, and services. Potential actions to improve and expand this R&D capacity have been recommended including expansion of human capital, increased private investment, knowledge and patent sharing, infrastructure building for business incubation and innovation support.\n\nCompetitive innovation prizes have been used to spur development in a range of fields such as aerospace engineering, clean technology, and genomics. The X-Prize Foundation is launching a competition for high speed, point-of-care diagnostics for tuberculosis. A more widely defined annual \"Global Health EnterPrize\" for neglected tropical diseases has been proposed to reward health innovators, particularly those based in countries where NTDs represent a serious health burden.\n\nThe Bill & Melinda Gates Foundation offers the Grand Challenges Explorations Opportunities on a rolling basis. This grant program allows individuals from any organization or background to apply to address priority global health issues. Each project award is $100,000 dollars and is drawn from a Foundation funding pool of $100 million. Awardees have tended to offer research projects on topics that are highly speculative but offer potentially game-changing breakthroughs in global health.\n\nIn 2006, Ridley \"et al\" recommended the development of a priority review voucher (PRV) in the journal \"Health Affairs\". It gained interest by Senator Sam Brownback of Kansas who championed its introduction in the FDA Amendments Act of 2007. Under the enacted law, FDA approval of a non-NTD drug can be accelerated through the drug review process if paired to a drug that addresses a NTD. The potential economic benefit to a pharmaceutical company is estimated to be potentially as high as $300 million per drug. Three drugs have earned NTD PRVs to date (December 2014): Coartem (by Novartis, for malaria); bedaquiline (by Janssen, for TB) and miltefosine (by Knight, for leishmaniasis). However, the success of the PRV system is now under much scrutiny, given that Knight benefitted by $125 million from the sale of a PRV earned from a drug (miltefosine) that was largely researched and developed by the WHO. Medicins San Frontiers are now pressuring Knight to guarantee supplying miltefosine at cost price, thus far without success.\n\nThe PRV isn't limited to the pairing of drugs within a single company, rather can be transferred between companies. Companies with NTD drug candidates in their pipelines but without a blockbuster drug, are able to sell their voucher producing financial returns. In the EU, similar priority review incentives are now under consideration to increase the speed of regulatory pricing and reimbursement decisions.\n\nHowever, PRVs have been criticized as being open to manipulation and possibly encouraging errors through too rapid regulatory decision-making.\n\nSeveral companies and scientific organizations are participating in open source initiatives to share drug data and patent information over the web, and facilitate virtual collaboration on NTD research.\n\nOne rich area to explore is in the wealth of genomic data resulting from the sequencing of parasite genomes. These data offer opportunities for the exploration of new therapeutic products using computational, and open source collaboration methods for drug discovery. The Tropical Disease Initiative, for example, has used large amounts of computing power to generate the protein structures for ten parasite genomes. An open source drug bank was matched algorithmically to determine compounds with protein interaction activity, and two candidates were identified. In general, such methods may hold important opportunities for off-label use of existing approved drugs.\n\n\n"}
{"id": "17700823", "url": "https://en.wikipedia.org/wiki?curid=17700823", "title": "Olayinka Koso-Thomas", "text": "Olayinka Koso-Thomas\n\nOlayinka Koso-Thomas (born 1937) is a Nigerian-born doctor who lives in Sierra Leone. She is known internationally for her efforts to abolish female genital cutting. In 1998, she shared a Prince of Asturias Award for this work.\n"}
{"id": "29200343", "url": "https://en.wikipedia.org/wiki?curid=29200343", "title": "PHECC", "text": "PHECC\n\nThe Pre-Hospital Emergency Care Council (PHECC) is an independent statutory organisation responsible for implementing, monitoring and further developing the standards of care provided by all statutory, private and voluntary ambulance services in Ireland. It is also responsible for conducting examinations at six levels of pre-hospital care, the control of ambulance practitioner registration and the publication of clinical practice guidelines.\n\nThere are six levels of care set down by PHECC. They are divided into Responder levels and Practitioner levels. Currently, all practitioners working on emergency ambulances must be trained to a minimum of PHECC Level 5 (Paramedic) standard. A practitioner working on a non-emergency ambulance must be trained to a minimum of PHECC Level 4 (EMT) standard. Responder levels are suitable for fire service personnel (excluding Dublin Fire Brigade), voluntary ambulance organisations, sports clubs and workplace first aiders.\n\nResponder levels of care are designed to provide basic medical training to lay people, non-medical emergency services staff (Gardaí, fire services), sports club staff and those designated to provide first aid at work. In addition to this, voluntary ambulance services such as Civil Defence, Order of Malta Ambulance Corps, Irish Red Cross and the St John Ambulance Brigade of Ireland depend on responder training to allow their members provide on-site first aid at the various events they cover.\nCertain remote communities have set up their own individual Cardiac First Response programmes, where various people in the area are trained to PHECC Level 1 (CFR) standard and are provided with an Automated External Defibrillator. These responders can then be called or paged to the scene of a cardiac arrest to provide CPR and defibrillation, where the increased response time of an ambulance would greatly affect the patient's outcome.\n\nPractitioners must register annually with PHECC to practice their skills. They are bound by various laws, primarily tort law when attending an incident or treating a patient. Upon qualification and registration, a practitioner is issued with a licence and unique PIN which should be presented when treating a patient either on or off-duty. The practitioner's PIN must also be entered on Patient Care Report forms for any incident where the practitioner was involved; this is done for both reference purposes, record-keeping and proof of continuous professional development.\n\nAll organisations and companies who are providing an ambulance service in any capacity must register with PHECC as a CPG Approved Service Provider to work to the current edition of Clinical Practice Guidelines published by PHECC. Approved Service Providers are broken down into four categories. 2nd Edition CPG Providers are the Irish Defence Forces and Event Medical Services Ltd. 3rd Edition CPG Providers are listed below.\n\nIn 2010, PHECC launched a poster-advertising and television-ad campaign aimed at informing the general public of what to do if they should witness an adult suddenly collapse. Its focus was to emphasise the fact that CPR can still be effective without mouth-to-mouth contact. Pocket-sized cards were distributed with instructions on the steps to take:\n\n\n\n"}
{"id": "56055173", "url": "https://en.wikipedia.org/wiki?curid=56055173", "title": "PLAID syndrome", "text": "PLAID syndrome\n\nPLAID syndrome is a inherited condition characterised by antibody deficiency and immune dysregulation, first described in 2012. The name is an acronym of \"PLCG2-associated antibody deficiency and immune dysregulation\". It is characterised by cold-induced urticaria, autoimmunity, atopy and humoral immune deficiency.\n\nFamilial cold urticaria presents similar symptoms.\n"}
{"id": "24575786", "url": "https://en.wikipedia.org/wiki?curid=24575786", "title": "Personal fulfillment", "text": "Personal fulfillment\n\nPersonal fulfilment is achievement of life goals which are important to an individual, in contrast to the goals of society, family and other collective obligations.\n\nPersonal fulfilment is an ongoing journey for a human individual. It commences when an individual starts becoming conscious of oneself and one's surroundings. It is then that one's exploration begins to realize what one is capable of. Like the tentative steps of a child that invariably lead to a few falls followed by seeking for some help from people around. Then the joy of achievement when one can successfully take a few steps without falling. The appreciation of people around is a key component of achieving personal fulfilment. It is invariably followed by a sense of habituality (i.e., being able to perform any act, such as walking, habitually). Then boredom. Followed by a yearning for the next horizon, whatever it may be for an individual.\n\nKey components then of personal fulfilment are:\n\n\nConsciousness is the quality or state of being aware of an external object or something within oneself. It has been defined as sentience, awareness, subjectivity, the ability to experience or to feel wakefulness, having a sense of selfhood, and the executive control system of the mind.\n\nConsciousness of the self is a complex experience for an individual being. A prime aspect of consciousness and awareness of the self is the awakening to one's capabilities and potential.\n\nThe awakening to one's capabilities and potential is a continuous process throughout the period of existence of a being - from the birth of the physical being through the expiration of the same. Inherent to this process of being is a drive or a force that seems to be part and parcel of being in the physical being on earth. This drive manifests itself from the start of one's being - like a child crying as soon as it is born. It is the awakening of the child to its capabilities of exercising its vocal cords. The drive continues through the life of one's being (albeit in varying degrees of forcefulness and momentum at various points in time) through to death of the physical being as we know.\n\nThis drive is primal and is the single force that leads to exploration.\n"}
{"id": "32528773", "url": "https://en.wikipedia.org/wiki?curid=32528773", "title": "Pierre Dukan", "text": "Pierre Dukan\n\nPierre Dukan is a French medical doctor and nutritionist, and the creator of the Dukan Diet.\n\nIn 1975, Pierre Dukan was a general practitioner in Paris when he was first confronted with a case of obesity. At the time, being overweight or obese was thought to be best treated by low calorie and small sized meals. Dukan thought of an alternative way to prevent patients from regaining their lost weight after dieting. He designed a new approach in four phases, including stabilisation and consolidation. After 20 odd years of research Pierre Dukan published his findings in 2000 in his book \"Je ne sais pas maigrir\" (\"I don't know how to get slimmer\") which became a bestseller in France.\n\nThe book became a bestseller in the UK after Carole Middleton introduced her daughter Catherine, now The Duchess of Cambridge, to the diet before her wedding to Prince William. Catherine lost two dress sizes.\n\nIn July 2011 a French court ruled against Dukan in his attempt to sue rival nutritionist Jean-Michel Cohen for libel, after Cohen had criticised Dukan's method in the press.\n\nIn January 2012, Dukan suggested that the Baccalaureate exam, taken by 17-year-old children, should include one test which the children could pass simply by staying within appropriate weight limits. He faced a disciplinary hearing in connection with these remarks.\n\nIn March 2012 the French Ordre des médecins asserted that Dukan had violated the organization's medical code of practice by practising medicine as a business.\n\nIn May 2012, he was removed from the membership of the Ordre des Médecins at his request.\n\nIn January 2014, Dukan was struck off the medical register for promoting his diet commercially.\n"}
{"id": "48664343", "url": "https://en.wikipedia.org/wiki?curid=48664343", "title": "Pit additive", "text": "Pit additive\n\nPit additives is a commercially-produced material that aims to reduce fecal sludge build-up and control odor in pit latrines, septic tanks and wastewater treatment plants. Manufacturers claim to use effective microorganisms (EM) in their products. Current scientific evidence does not back up most claims made by manufacturers about the benefits. Removing sludge continues to be a problem in pit latrines and septic tanks.\n\nPit additives are advocated for use in sanitation systems like pit latrines and septic tanks. Additives consist of packages of micro-organisms or enzymes or both. More than 1,200 septic system additives were estimated to be available in the U.S. in 2011. However, very little peer-reviewed and replicated field research exists to confirm the efficacy of biological additives.\n\nPit additive claims include an increase in speed of the breakdown of sludge, which may also decrease odor. The claim is based on assertions that the additive contains nutrients or certain aerobic (oxygen-breathing) micro-organisms that will break down the sludge. Research, however, finds that these claims are unlikely to be true. The amount of bacteria introduced by pit additives is insignificant compared to the bacteria already present in the pit or septic tank.\n\nResearchers from the U.S. carried out field experiments in 2011 to assess the effect of additives on the performance of 20 septic tanks. These septic tanks served residences at a mobile home park located in Orange County, North Carolina. The researchers distinguished between tanks that were well maintained, poorly maintained and maintained to an intermediate level. \"Well maintained\" was defined as \"de-sludged in the last 2-3 years.: \"Poorly maintained\" had not been de-sludged for the last 15-20 years. Tanks put in the intermediate category fell somewhere in between. \n\nOnly well-maintained septic tanks showed some reduction in sludge build-up. To determine if the reduction could be attributed to pit additives, a follow up-study investigated the impact of three additives on just the well-maintained septic tanks. Overall, the research concluded there was limited evidence of additive impact on the performance of septic tanks. It should be stressed that these field experiments used additives other than EM (effective microorganisms), leaving the results open to the argument that the more varied composition of EM could make such additives more effective than the three additives tested.\n\nThe United States Environmental Protection Agency (USEPA) produced a fact sheet on the use of pit additives to improve the performance of septic tank treatment systems. The fact sheet concludes that bacteria and extracellular enzymes do not appear to significantly enhance normal biological decomposition processes in septic tanks. They go on to say that ‘some biological additives have been found to degrade or dissipate septic tank scum and sludge. However, whether this relatively minor benefit is derived without compromising long-term viability of the soil infiltration system has not been demonstrated conclusively’. They noted that some studies suggest that material degraded by additives in the tank actually adds to the suspended solids and other contaminants in the otherwise clarified septic tank effluent.\n\nProponents claim the additives in wastewater can facilitate reduction in organic load and pathogen removal, leading to significant improvements in effluent quality. They also claim benefits relating to the rate of sludge build-up and odor reduction. One source claims that septic tank additives can reduce hydrogen sulphide and ammonia production. Their reasoning is that additives contain natural’ organisms that prevail over the rather less ‘natural’ organisms that would otherwise dominate conditions in the treatment unit, whether this be a septic tank or some form of aerobic treatment. They even claim that by overcoming the effects of ‘unnatural’ substances such as bleach and other disinfectants, the use of septic tank additives allows septic tanks and other treatment systems to function in conditions that would otherwise have resulted in their becoming ‘dead’ and non-functional.\n\nOne short note claims that microorganisms in the additives contain various organic acids due to the presence of lactic acid bacteria. These secrete organic acids, enzymes antioxidants, and metallic chelates thus create an antioxidant environment, which assists in the enhancement of solid-liquid separation, which is the foundation for cleaning water. The authors of the note provide no explanation of how this works.\n\nHowever, the findings from various studies around the world indicate that:\nWhile pit additives can lead to some improvement in effluent quality, it is unlikely that the improvement would be enough to make a difference. Claims that pit additives can make otherwise ‘unsafe’ effluents ‘safe’ is unlikely to be justified.\n\nBased on the research conducted so far, spending money on pit additives is a waste of money. In 2011, one fifth of South African municipalities purchased various additives as part of their sanitation management programmes. Even at that time the Water Research Commission in South Africa advised against this practice. Instead they suggested spending that money on improved pit emptying methods and improved pit design, which might include use of low flush toilets with alternating leach pits.\n\nThe costs and health risks associated with manual pit emptying are significant. If an EM product could be developed to significantly reduce the filling rate of pits, this would be of enormous significance.\n\nAustralian scientists investigated the effect of additives in a wastewater treatment plant and a number of septic tanks. Their aim was to test the hypothesis that the additive reduces sludge volumes. They found significant reduction in pH levels at the wastewater treatment plant together with improved settlement of sludge but with a significant increase in organic matter (measured as biological oxygen demand). Their results for the septic tanks showed a homogenization of conditions in the tanks after application of septic tank additives, which they suggested was due to domination by a particular type of micro-organism. However, they found no reduction in suspended solids concentration in the effluent and concluded that there were not sufficient changes in sludge volume in the wastewater treatment plant or suspended solids in the septic tanks to indicate a clear benefit from the use of these kinds of additives in wastewater.\n\nA project in Karachi, Pakistan called the Orangi Pilot Project (OPP) has been making use of pit additives. The OPP promotes a treatment technology comprising a two-chamber tank. The first of these acts like the first compartment of a septic tank while the second is filled with gravel to provide filtration. It is not clear whether flow through the second compartment is upward or downward. This arrangement has some similarities to baffled reactor designs promoted by the German NGO BORDA, although standard BORDA designs provide more chambers, arranged in series and with all after the first chamber operating in an upward flow mode. The baffled reactor design is one of a number of ‘DEWATS’ (decentralised wastewater treatment systems) wastewater treatment technologies promoted by BORDA. All operate anaerobically and are examples of what might be termed enhanced primary treatment. If maintained well, enhanced primary treatment modules should perform better than a well maintained conventional septic tank but will still produce an effluent with high pathogen levels and relatively high biological oxygen demand and suspended solids concentrations.\n\nThe OPP is using the additives to improve the effluent produced at these small treatment plants, including the plant that treats effluent from a nursery in Karachi. It has also supported the installation of several small treatment plants using EM technology in rural Sindh and Punjab. Its partner organization Ali Hasan Mangi Memorial Trust (AHMMT) installed a small sewage treatment unit with additives to treat sewage from 300 houses in the village Khairodero in Larkana District. Another eleven are reported to be functioning and more are planned.\n\nDuring discussions at the Urban Resource Centre in Karachi in late 2011, the late Parveen Rehman of OPP stated that adding pit additives to the inlet chamber of these treatment facilities had resulted in improved effluent quality and a significant reduction in smell. However, it seems that OPP had not attempted to quantify the improvement and had not made any formal assessment of the effect of the pit additive on effluent quality.\n"}
{"id": "35938012", "url": "https://en.wikipedia.org/wiki?curid=35938012", "title": "Poliomyelitis in Pakistan", "text": "Poliomyelitis in Pakistan\n\nPakistan is one of the three remaining countries in the world where poliomyelitis (polio) is still categorized as an endemic viral infection, the others being Afghanistan and Nigeria. As of October 2015, there have been 38 documented cases of wild poliovirus in Pakistan in the past year. However, the disease is still present as of 2018.\n\nThough the polio immunization campaign in the country started in 1974, the efforts for eradication officially started in 1994. The infection remains endemic despite over 100 rounds of vaccination being carried out in the past decade. Pakistan had the world's highest number of polio cases in 2014, and as of October 2015, it has maintained this record. However, the number of cases in 2015 are a fraction of those recorded in 2014. The country has announced a goal of eradication by 2016.\n\nThe Expanded Programme of Immunization (EPI) was begun in Pakistan by the World Health Organization (WHO) in the 1970s to combat deaths from six vaccine-preventable diseases. In 1980, the EPI had vaccinated just 2% of the population against polio; by 1990, the coverage had increased to 54%. In a 1994 interview, CDC officials predicted total polio eradication in Pakistan within two to three years; by 1995, the WHO projected total eradication by the year 2000, a target that the organization reaffirmed in 1998.\n\nHowever, despite donor attention, the rate of vaccinations sagged. By 1991, only 83 percent of Pakistani children had been vaccinated. Research by the Center for Disease Control (CDC) in April 1998 cited a failure to vaccinate, vaccine failure, and inadequate immunization strategies as causes for the continued incidences of polio in this time. Reasons for under-vaccination included the population being uninformed, considering vaccination unimportant, and having to travel long distances to vaccination sites. In response to these challenges, organizations including the WHO, United Nations Children’s Fund (UNICEF), CDC, Rotary International, and the Bill and Melinda Gates Foundation pulled together to develop and fund intensive eradication campaigns, including door-to-door vaccinations.\n\nIn March 2001, about 27 million children were vaccinated across the country in the hope that Pakistan could be virus-free by the end of that year. As of 2004, when there were 30 million children in Pakistan under five, about 200,000 health workers were required for a vaccination campaign that was carried out eight times a year. A documentary, \"Polio True Stories\", was aired on several television channels to make people aware of the problems facing people affected by the disease.\n\nOpposition to the polio vaccination program by militant groups continues to thwart the success of eradication efforts. In 2014, the number of polio cases reached 306, the highest it has been since 2000.\n\nIn August 2015, the country launched an injectable polio vaccine intended to treat four million children and bring Pakistan closer to its goal of eradication by 2016. While the new vaccine is pricier than the traditional oral polio vaccine (OPV) and requires that a doctor or nurse administer it, the injection needs only one dose, not repeated doses, to generate immunity.\n\nLeading up to the early 2000s, Pakistani vaccination workers were making notable progress in eliminating the polio virus in the highly infected areas of Gadap, Quetta Block (Quetta, Pishin, Kila Abdullah Districts), FATA (Federally Administered Tribal Areas), Khyber Pakhtunkhwa, Peshawar and the neighboring areas of Mardan, Charsaddah, Nowshehra and Lakki Marwat.\n\nIn 2011, the U.S. Central Intelligence Agency (CIA) employed the services of Pakistani doctor, Shakil Afridi, and local health officials to stage a fake vaccination campaign in an attempt to confirm Osama Bin Laden's location in Abbottabad. This ruse was organized in an effort to gain more knowledge on Bin Laden's whereabouts prior to Operation Neptune Spear. While it is still unknown if Afridi was aware of the implications for his services in the campaign, the main goal of the CIA was to collect DNA samples of Osama Bin Laden's children from blood left on the needles used to deliver the Hepatitis B vaccination.\n\nThe CIA's fake vaccination campaign has had severe lasting effects on the North West corner of Pakistan. Since 2012, at least 70 polio workers have been killed in Pakistan. Many of the attacks have been claimed by the Taliban, who forward claims that the vaccination campaigns are a facade for intelligence gathering. The fake Hepatitis B campaign has caused people to question the motivations behind all vaccination campaigns, leading to a spike in poliomyelitis cases from 198 in 2011 to 306 cases in 2014. Many citizens of both the US and various other nations have criticized the CIA's vaccination campaign for the effects that it has had on Pakistan's public health.\n\nIn 2015, the Khyber Pakhtunkhwa government issued arrest warrants for 1,200 parents and guardians for refusing to administer vaccine to their children. 512 people were arrested on the charge but were later freed after signing an undertaking that they would not oppose vaccination.\n\nBy 2015, new cases of polio had dropped by 70% as compared to 2014, due to increased vaccination in parts of the northwest Pakistan that had previously been under the control of militants. These areas were secured by Pakistani forces in the Zarb-e-Azb Campgain against Taliban militants. Previously, vaccination teams were forbidden by militants from vaccinating children in the area.\n\nSome of the reasons which affect the eradication of polio are political unrest, poor health infrastructure, and government negligence. The most afflicted areas are those where militants are present and the government lacks absolute control, such as the Federally Administered Tribal Areas. Transmission of the virus from such areas then leads to spread through other parts of the country.\n\nDue to the fact that the vaccines are primarily produced in western countries, militant groups like Tehrik-i-Taliban propagandize that they are made out of pig fat or contain alcohol, the two things that are forbidden in Islam. Some clerics have also denounced the vaccines. There is also a myth prevalent in many of the areas with low literacy rates that the immunization sterilizes the local population.\n\nIn early 2012, it was reported that some parents refused to get their children vaccinated in Khyber Pakhtunkhwa and FATA on religious grounds but overall religious refusals in the rest of the country have \"decreased manifold\".\n\nBoth major sides of the Afghani civil war now support polio vaccination, and polio rates are declining rapidly in Afghanistan, with only seven cases in 2015 (as of August 18, 2015). In Pakistan there were 29 cases in the same period, with organizational difficulties slowing immunization, but more than ten million children have been vaccinated in 2015. This is the last remaining region with active polio cases as of 2015.\n\nSome researchers point to the unique role that Saudi Arabia is able to play in the affairs of the Muslim-majority Pakistan, both as the site of the annual Islamic pilgrimage (the Hajj) and as the seat of formal Islamic authority, in terms of the ability to pass fatwas and shape public opinion. Given that over 10% (200,000) of the Hajj pilgrims are Pakistani, Saudi Arabia has a vested interest in eradication campaigns in Pakistan to prevent spread during Hajj. One possible avenue for the Muslim world to eliminate the polio threat is to have local mosques and community centres promote vaccination, emphasizing that the sanctity of life is foremost in Islam. If supported by national and international Muslim organisations, with the aid of world health agencies and Muslim public health scholars, this policy could reduce the extent of misinformation the Taliban is able to spread.\n\nPakistan's healthcare system is burdened by poor public-sector funding, an ill-regulated private sector, and a lack of governmental transparency, all of which contribute to limiting the quality of public health services. Though the Polio Eradication Initiative is well funded, it is delivered through the underfunded public infrastructure. Members of the Polio Eradication Committee in Pakistan have also expressed concerns regarding the accountability of the organizations backing the campaign.\n\nSince the number of cases began to rise following the raid on the Osama Bin Laden compound, organizations including local health authorities, government workers, WHO, and UNICEF have been asserting blame rather than addressing the public health issues at hand. The public health system provides various avenues for institutionalized malpractice, wherein resources are leached. Staff misconduct is common, where staff members remain absent from duty, fail to run field operations, and divert vaccine for use in private facilities. This can result in the attachment of costs to services intended to be free.\n\nFecal-oral transmission is the most common source of transmission of the poliovirus in developing countries, including Pakistan. In addition to the poor health and water sanitation infrastructure, the transmission of the virus is also heightened because of the high population density and climate conditions. Studies have indicated that the polio vaccine has reduced per-dose efficacy in areas near the Tropics, including Pakistan and its neighboring country India. As a consequence of the climate in South Asia, sometimes ten or more doses of the vaccine need to be administered, each a month apart, in order to ensure immunity. The month-long spacing can itself be a reason for families not to complete the vaccination schedule, because transportation could prove unreliable from month to month, and parents might be unable to take time off of work for risk of losing the day's income or losing their job entirely.\n\nThe threat of natural disaster also plays a role in delaying total eradication of polio. For example, Pakistan is prone to earthquakes and heavy monsoon rains. In the 2010 Pakistan floods, of the 20 million affected, a majority were in the lowest socioeconomic percentile. In addition to cases of dengue fever, cholera, and measles, the WHO reported an upsurge in cases of polio. By November 2010, Pakistan accounted for about 62% of all polio cases from endemic countries, with most new cases being from areas affected by the floods.\n\nAnother reason for the resistance to the polio vaccine is one of mindset. There is a common perception that other issues are more pressing than vaccination. A manifestation of this is the policy perspective that, if the source of the virus is contaminated drinking water, efforts should be made to purify the water rather than focus on the treatment of disease caused by ingesting the water. Surveys of the Pakistani population show that a significant number feel that the allocation of funding to prevent the problem is more efficient than retroactive treatment once the problem (the spread of polio) has already happened.\n\nAdditionally, it has been shown that low parental - specifically, maternal - literacy and knowledge regarding vaccines and immunization schedules, poor socioeconomic status, and residence in rural areas all are attributable to decreased rates of immunization completion. Parental education is one of the most important determiners of whether children will complete their vaccinations. In a study of two-parent households in Pakistan, it was shown that the father's knowledge about health most impacted immunization decisions, with an effect so large that some researchers contend improving education will improve health more so than even the provision of health services.\n\nWidespread malnutrition in Pakistani children is a factor in lowered resistance to disabling diseases and reduced efficacy of the polio vaccine.\n\nPolio has had drastic effects on the health of the population of Pakistan and on the nation's healthcare infrastructure and economy. The WHO estimates that 65–75% of polio cases in developing countries occur in children under 3 years of age, with 95% of all cases occurring in children under 5 years of age. Researchers at the School of Public Health at Johns Hopkins University quantified the disease burden of various diseases in Pakistan; in the year 1990, a Pakistani person with polio averaged a loss of 1.13 healthy life years to the disease. The duration of disability of polio, averaged over 1000 people, was 81.84 years, the equivalent of diseases including diphtheria, childhood meningitis, and measles.\n\nThere has been limited research into the impacts of polio in Pakistan in recent years, but a 1988 health survey found that the most common handicaps among polio sufferers were associated with mobility, occupation, and social integration. The survey found differences in participants based on whether they lived in a village or a slum area: there was a higher rate of handicap in the village population, and higher frequencies of infectious, respiratory, and digestive diseases in the slum area. Both areas saw polio victims suffer from a higher incidence of musculoskeletal system diseases, as well as infections of the ear, and respiratory tract. Given 1–2 years with occupational therapists, 80% of patients with handicaps showed improvement in function.\n\n\n\n"}
{"id": "32799575", "url": "https://en.wikipedia.org/wiki?curid=32799575", "title": "Public Finance Balance of Smoking in the Czech Republic", "text": "Public Finance Balance of Smoking in the Czech Republic\n\nThe Public Finance Balance of Smoking in the Czech Republic was a 2001 report commissioned by Philip Morris's Czech division following concerns raised by the Czech health ministry that smoking's costs outweighed its fiscal benefits. The study was conducted by Arthur D. Little and found that smokers' early mortality and cigarette-tax revenue, outweighed the costs of health-care and lost tax revenue from early death. The study concluded through cost-benefit analysis \"based on up-to-date reliable data and consideration of all relevant contributing factors, the effect of smoking on the public finance balance in the Czech Republic in 1999 was positive, estimated at +5,815 mil. CZK.\"\n\nThe report which was leaked on July 16, 2001 was met with condemnation and subjected Philip Morris to vitriolic criticism from politicians, anti-smoking activists, and watchdog groups. Philip Morris subsequently disavowed the report and apologized for its conclusion. Nonetheless, the suggested \"death benefit\" raised by the report continues to be the subject of debate.\n\nThe report was unusual as historically, tobacco companies had disputed the link between smoking and early mortality, whereas the report used the early mortality as a selling point. Though similar studies in Europe had been done a decade earlier, Philip Morris stated that it had canceled any new similar reports in countries including Poland, Slovakia, Hungary, and Slovenia. CNN reported that an Arthur D. Little representative had told them that Philip Morris had commissioned similar studies in Canada and the Netherlands, though Philip Morris stated it had no such on-going reports.\n\nThe Czech Prime Minister, Miloš Zeman had previously noted the \"death benefit\" stating that \"By smoking, I contribute to the stability of the state budget. By buying cigarettes, I increase state revenues, and I will die of lung cancer, so the state won’t have to pay me a pension.\" In addition, Zeman had stated that \"As a smoker, I support the state budget, because in the Czech Republic we pay tax on tobacco. Also, smokers die sooner, and the state does not need to look after them in their old age.\"\n\nThe stated objective of the report \"was to determine whether costs imposed on public finance by smokers are offset by tobacco-related tax contributions and external positive effects of smoking.\"\n\nFollowing the leak, the company initially defended the report. Philip Morris spokesman Remi Calvert stating that \"It is very unfortunate that this is one aspect of the study that is being focused on\" adding that \"We understand that it appears quite cold, but tobacco is a controversial product.\" Robert Kaplan, director of communications at Philip Morris International stated that the report's purported death benefit was \"just one point\" and \"was not the point we were emphasizing.\"\n\nThe company subsequently apologized for the report. Kaplan later stated that \"We are not in any way suggesting that the social cost of smoking is of benefit to society.\" Steven Parrish, vice-president at Philip Morris, stating that \"We understand that this was not only a terrible mistake, but that it was wrong. To say it's inappropriate is an understatement.\" In an internal memo, CEO John R. Nelson agreed with critics that the report \"exhibited a callous and cynical disregard of basic human values.\" On July 26, 2001, Phillip Morris issued an apology in the Wall Street Journal:\n\nFor one of our tobacco companies to commission this study (AD Little Report concluding that smokers save the state money - by dying early) was not just a terrible mistake, it was wrong. All of us at Philip Morris, no matter where we work, are extremely sorry for this. No one benefits from the very real, serious and significant diseases caused by smoking. We understand the outrage that has been expressed and we sincerely regret this extraordinarily unfortunate incident. We will continue our best efforts to do the right thing in all our business, acknowledging mistakes when we make them and learning from them as we go forward.\nThe release of the report was viewed as a setback for Philip Morris which had been making charitable donations to improve its public image.\n\n\"Mladá Fronta Dnes\" described the report as \"first-class cynicism and hyena-ism\" comparing it to how Nazis determined the value of life in Nazi concentration camps adding \"What an offer: `come help us make money on the death of your citizens.\" The \"Sarasota Herald-Tribune\" described the report as \"The Philip Morris Health Plan\" comparing it to Jonathan Swift's \"A Modest Proposal\".\n\nFollowing the report, anti-smoking groups placed ads in prominent newspapers such as the \"New York Times\" depicting a corpse with a price tag stating \"$1,227, [£860] that's how much a study sponsored by Philip Morris said the Czech Republic saves on healthcare, pensions and housing every time a smoker dies\".\n\n\n\n\n\n\n\n"}
{"id": "3736545", "url": "https://en.wikipedia.org/wiki?curid=3736545", "title": "Pupillary distance", "text": "Pupillary distance\n\nPupillary distance (PD) or interpupillary distance (IPD) is the distance measured in millimeters between the centers of the pupils of the eyes. This measurement is different from person to person and also depends on whether they are looking at near objects or far away. Monocular PD refers to the distance between each eye and the bridge of the nose which may be slightly different for each eye due to anatomical variations. For people who need to wear prescription glasses consideration of monocular PD measurement by the optician helps to ensure that the lenses will be located in the optimum position. Purchasing glasses online can be a potential problem if the PD measurement isn't available.\nIn both the UK and most of Canada (excluding British Columbia), the PD measurement is classed as a dispensing tool rather than a part of the actual prescription of the person whose eyes were tested, thus there is no obligation for a PD to be provided on patient request. Whilst PD is an optometric term used to specify prescription eyewear, IPD is more critical for the design of binocular viewing systems, where both eye pupils need to be positioned within the exit pupils of the viewing system. These viewing systems include binocular microscopes, night vision devices or goggles (NVGs), and head-mounted displays (HMDs). IPD data are used in the design of such systems to specify the range of lateral adjustment of the exit optics or eyepieces. IPD is also used to describe the distance between the exit pupils or optical axes of a binocular optical system. The distinction with IPD is the importance of anthropometric databases and the design of binocular viewing devices with an IPD adjustment that will fit a targeted population of users. Because instruments such as binoculars and microscopes can be used by different people, the distance between the eye pieces is usually made adjustable to account for IPD. In some applications, when IPD is not correctly set, it can lead to an uncomfortable viewing experience and eye strain.\n\nDifferent methods for measuring exist but accurate measurement can usually be determined by an ECP during an eye examination. This is normally done with a small millimeter ruler referred to as a \"PD stick\" or with a corneal reflex pupillometer, which is a machine calibrated to help the optical professional more accurately measure the pupillary distance. There are also mobile phone and web apps that can measure one's pupillary distance.\n\nDevices such as stereo microscopes have small exit pupils, and adjustment for user IPD is necessary. These devices can be designed to fit a large range of IPDs as factors such as size and weight of the adjusting mechanism are not overly critical. In contrast to microscopes, the weight and bulk of NVGs and HMDs are large factors for wearing comfort and usability. The \"ANVIS\" NVG has an adjustment range of 52 to 72 mm. The \"Rockwell-Collins Optronics\" \"XL35\" and \"XL50\" binocular HMDs have a range of 55 to 75 mm. The \"1988 Army Survey\" can be used to evaluate the percentage of the Army population captured by these ranges.\n\nBinocular HMDs can be designed with a fixed IPD to minimize weight, bulk and cost. The fixed-IPD design strategy assumes that the exit pupil will be large enough to capture the IPD range of a targeted population. An adjustable IPD design assumes that the lateral adjustment range in conjunction with the exit pupil size is required to capture the targeted population.\n\nAnthropometric databases are available that include IPD. These include \"Military Handbook 743A\" and the \"2012 Anthropometric Survey of US Army Personnel\". These databases express the IPD for each gender and sample size as the mean and standard deviation, minimum and maximum, and percentiles (e.g., 5th and 95th; 1st and 99th, 50th or median). Representative data from the 1988 Anthropometric Survey are shown in the following table.\n\nIPD is also used in binocular vision science. For example, a bench-top haploscope may require setting the mirror separation for each experimental subject. Other experimental presentations may require the use of IPD to control for ocular convergence and binocular depth. \n\nSeveral binocular HMDs that support night vision position the sensors on the sides of the helmet, effectively extending the IPD by approximately 4x and creating hyperstereopsis. Hyperstereopsis increases ocular convergence and causes near objects to appear closer and with exaggerated depth and slant.\n\nIPD application is found in stereoscopy, virtual reality headsets gaming, education and training.\n\n\n"}
{"id": "2161615", "url": "https://en.wikipedia.org/wiki?curid=2161615", "title": "Rasch model", "text": "Rasch model\n\nThe Rasch model, named after Georg Rasch, is a family of psychometric models for creating measurements from categorical data, such as answers to questions on a reading assessment or questionnaire responses, as a function of the trade-off between (a) the respondent's abilities, attitudes, or personality traits and (b) the item difficulty. For example, they may be used to estimate a student's reading ability or the extremity of a person's attitude to capital punishment from responses on a questionnaire. In addition to psychometrics and educational research, the Rasch model and its extensions are used in other areas, including the health profession and market research because of their general applicability.\n\nThe mathematical theory underlying Rasch models is a special case of item response theory and, more generally, a special case of a generalized linear model. However, there are important differences in the interpretation of the model parameters and its philosophical implications that separate proponents of the Rasch model from the item response modeling tradition. A central aspect of this divide relates to the role of specific objectivity, a defining property of the Rasch model according to Georg Rasch, as a requirement for successful measurement.\n\nIn the Rasch model, the probability of a specified response (e.g. right/wrong answer) is modeled as a function of person and item parameters. Specifically, in the original Rasch model, the probability of a correct response is modeled as a logistic function of the difference between the person and item parameter. The mathematical form of the model is provided later in this article. In most contexts, the parameters of the model characterize the proficiency of the respondents and the difficulty of the items as locations on a continuous latent variable. For example, in educational tests, item parameters represent the difficulty of items while person parameters represent the ability or attainment level of people who are assessed. The higher a person's ability relative to the difficulty of an item, the higher the probability of a correct response on that item. When a person's location on the latent trait is equal to the difficulty of the item, there is by definition a 0.5 probability of a correct response in the Rasch model.\n\nA Rasch model is a \"model\" in one sense in that it represents the structure which data should exhibit in order to obtain measurements from the data; i.e. it provides a criterion for successful measurement. Beyond data, Rasch's equations model relationships we expect to obtain in the real world. For instance, education is intended to prepare children for the entire range of challenges they will face in life, and not just those that appear in textbooks or on tests. By requiring measures to remain the same (invariant) across different tests measuring the same thing, Rasch models make it possible to test the hypothesis that the particular challenges posed in a curriculum and on a test coherently represent the infinite population of all possible challenges in that domain. A Rasch model is therefore a model in the sense of an \"ideal\" or standard that provides a heuristic fiction serving as a useful organizing principle even when it is never actually observed in practice.\n\nThe perspective or paradigm underpinning the Rasch model is distinct from the perspective underpinning statistical modelling. Models are most often used with the intention of describing a set of data. Parameters are modified and accepted or rejected based on how well they fit the data. In contrast, when the Rasch model is employed, the objective is to obtain data which fit the model (Andrich, 2004; Wright, 1984, 1999). The rationale for this perspective is that the Rasch model embodies requirements which must be met in order to obtain measurement, in the sense that measurement is generally understood in the physical sciences.\n\nA useful analogy for understanding this rationale is to consider objects measured on a weighing scale. Suppose the weight of an object A is measured as being substantially greater than the weight of an object B on one occasion, then immediately afterward the weight of object B is measured as being substantially greater than the weight of object A. A property we require of measurements is that the resulting comparison between objects should be the same, or invariant, irrespective of other factors. This key requirement is embodied within the formal structure of the Rasch model. Consequently, the Rasch model is not altered to suit data. Instead, the method of assessment should be changed so that this requirement is met, in the same way that a weighing scale should be rectified if it gives different comparisons between objects upon separate measurements of the objects.\n\nData analysed using the model are usually responses to conventional items on tests, such as educational tests with right/wrong answers. However, the model is a general one, and can be applied wherever discrete data are obtained with the intention of measuring a quantitative attribute or trait.\n\nWhen all test-takers have an opportunity to attempt all items on a single test, each total score on the test maps to a unique estimate of ability and the greater the total, the greater the ability estimate. Total scores do not have a linear relationship with ability estimates. Rather, the relationship is non-linear as shown in Figure 1. The total score is shown on the vertical axis, while the corresponding person location estimate is shown on the horizontal axis. For the particular test on which the test characteristic curve (TCC) shown in Figure 1 is based, the relationship is approximately linear throughout the range of total scores from about 10 to 33. The shape of the TCC is generally somewhat sigmoid as in this example. However, the precise relationship between total scores and person location estimates depends on the distribution of items on the test. The TCC is steeper in ranges on the continuum in which there are a number of items, such as in the range on either side of 0 in Figures 1 and 2.\n\nIn applying the Rasch model, item locations are often scaled first, based on methods such as those described below. This part of the process of scaling is often referred to as item \"calibration\". In educational tests, the smaller the proportion of correct responses, the higher the difficulty of an item and hence the higher the item's scale location. Once item locations are scaled, the person locations are measured on the scale. As a result, person and item locations are estimated on a single scale as shown in Figure 2.\n\nFor dichotomous data such as right/wrong answers, by definition, the location of an item on a scale corresponds with the person location at which there is a 0.5 probability of a correct response to the question. In general, the probability of a person responding correctly to a question with difficulty lower than that person's location is greater than 0.5, while the probability of responding correctly to a question with difficulty greater than the person's location is less than 0.5. The Item Characteristic Curve (ICC) or Item Response Function (IRF) shows the probability of a correct response as a function of the ability of persons. A single ICC is shown and explained in more detail in relation to Figure 4 in this article (see also the item response function). The leftmost ICCs in Figure 3 are the easiest items, the rightmost items in the same figure are the most difficult items.\n\nWhen responses of a person are listed according to item difficulty, from lowest to highest, the most likely pattern is a Guttman pattern or vector; i.e. {1,1...,1,0,0,0...,0}. However, while this pattern is the most probable given the structure of the Rasch model, the model requires only probabilistic Guttman response patterns; that is, patterns which tend toward the Guttman pattern. It is unusual for responses to conform strictly to the pattern because there are many possible patterns. It is unnecessary for responses to conform strictly to the pattern in order for data to fit the Rasch model.\n\nEach ability estimate has an associated standard error of measurement, which quantifies the degree of uncertainty associated with the ability estimate. Item estimates also have standard errors. Generally, the standard errors of item estimates are considerably smaller than the standard errors of person estimates because there are usually more response data for an item than for a person. That is, the number of people attempting a given item is usually greater than the number of items attempted by a given person. Standard errors of person estimates are smaller where the slope of the ICC is steeper, which is generally through the middle range of scores on a test. Thus, there is greater precision in this range since the steeper the slope, the greater the distinction between any two points on the line.\n\nStatistical and graphical tests are used to evaluate the correspondence of data with the model. Certain tests are global, while others focus on specific items or people. Certain tests of fit provide information about which items can be used to increase the reliability of a test by omitting or correcting problems with poor items. In Rasch Measurement the person separation index is used instead of reliability indices. However, the person separation index is analogous to a reliability index. The separation index is a summary of the genuine separation as a ratio to separation including measurement error. As mentioned earlier, the level of measurement error is not uniform across the range of a test, but is generally larger for more extreme scores (low and high).\n\nThe class of models is named after Georg Rasch, a Danish mathematician and statistician who advanced the epistemological case for the models based on their congruence with a core requirement of measurement in physics; namely the requirement of \"invariant comparison\". This is the defining feature of the class of models, as is elaborated upon in the following section. The Rasch model for dichotomous data has a close conceptual relationship to the law of comparative judgment (LCJ), a model formulated and used extensively by L. L. Thurstone, and therefore also to the Thurstone scale.\n\nPrior to introducing the measurement model he is best known for, Rasch had applied the Poisson distribution to reading data as a measurement model, hypothesizing that in the relevant empirical context, the number of errors made by a given individual was governed by the ratio of the text difficulty to the person's reading ability. Rasch referred to this model as the \"multiplicative Poisson model\". Rasch's model for dichotomous data – i.e. where responses are classifiable into two categories – is his most widely known and used model, and is the main focus here. This model has the form of a simple logistic function.\n\nThe brief outline above highlights certain distinctive and interrelated features of Rasch's perspective on social measurement, which are as follows:\n\n\nThus, congruent with the perspective articulated by Thomas Kuhn in his 1961 paper \"The function of measurement in modern physical science\", measurement was regarded both as being founded in theory, and as being instrumental to detecting quantitative anomalies incongruent with hypotheses related to a broader theoretical framework. This perspective is in contrast to that generally prevailing in the social sciences, in which data such as test scores are directly treated as measurements without requiring a theoretical foundation for measurement. Although this contrast exists, Rasch's perspective is actually complementary to the use of statistical analysis or modelling that requires interval-level measurements, because the purpose of applying a Rasch model is to obtain such measurements. Applications of Rasch models are described in a wide variety of sources, including Alagumalai, Curtis & Hungi (2005), Bezruczko (2005), Bond & Fox (2007), Burro (2016), Fisher & Wright (1994), Masters & Keeves (1999), and the \"Journal of Applied Measurement\".\n\nThe Rasch model for dichotomous data is often regarded as an item response theory (IRT) model with one item parameter. However, rather than being a particular IRT model, proponents of the model regard it as a model that possesses a property which distinguishes it from other IRT models. Specifically, the defining property of Rasch models is their formal or mathematical \"embodiment\" of the principle of invariant comparison. Rasch summarised the principle of invariant comparison as follows:\n\nRasch models embody this principle because their formal structure permits algebraic separation of the person and item parameters, in the sense that the person parameter can be \"eliminated\" during the process of statistical estimation of item parameters. This result is achieved through the use of conditional maximum likelihood estimation, in which the response space is partitioned according to person total scores. The consequence is that the raw score for an item or person is the sufficient statistic for the item or person parameter. That is to say, the person total score contains all information available within the specified context about the individual, and the item total score contains all information with respect to item, with regard to the relevant latent trait. The Rasch model requires a specific structure in the response data, namely a probabilistic Guttman structure.\n\nIn somewhat more familiar terms, Rasch models provide a basis and justification for obtaining person locations on a continuum from total scores on assessments. Although it is not uncommon to treat total scores directly as measurements, they are actually counts of discrete observations rather than measurements. Each observation represents the observable outcome of a comparison between a person and item. Such outcomes are directly analogous to the observation of the rotation of a balance scale in one direction or another. This observation would indicate that one or other object has a greater mass, but counts of such observations cannot be treated directly as measurements.\n\nRasch pointed out that the principle of invariant comparison is characteristic of measurement in physics using, by way of example, a two-way experimental frame of reference in which each instrument exerts a mechanical force upon solid bodies to produce acceleration. Rasch stated of this context: \"Generally: If for any two objects we find a certain ratio of their accelerations produced by one instrument, then the same ratio will be found for any other of the instruments\". It is readily shown that Newton's second law entails that such ratios are inversely proportional to the ratios of the masses of the bodies.\n\nLet formula_1 be a dichotomous random variable where, for example, formula_2 denotes a correct response and formula_3 an incorrect response to a given assessment item. In the Rasch model for dichotomous data, the probability of the outcome formula_4 is given by:\n\nwhere formula_6 is the ability of person formula_7 and formula_8 is the difficulty of item formula_9. Thus, in the case of a dichotomous attainment item, formula_10 is the probability of success upon interaction between the relevant person and assessment item. It is readily shown that the log odds, or logit, of correct response by a person to an item, based on the model, is equal to formula_11. Given two examinees with different ability parameters formula_12 and formula_13 and an arbitrary item with difficulty formula_8, compute the difference in logits for these two examinees by formula_15. This difference becomes formula_16. Conversely, it can be shown that the log odds of a correct response by the same person to one item, \"conditional\" on a correct response to one of two items, is equal to the difference between the item locations. For example,\n\nwhere formula_18 is the total score of person \"n\" over the two items, which implies a correct response to one or other of the items. Hence, the conditional log odds does not involve the person parameter formula_19, which can therefore be \"eliminated\" by conditioning on the total score formula_20. That is, by partitioning the responses according to raw scores and calculating the log odds of a correct response, an estimate formula_21 is obtained without involvement of formula_19. More generally, a number of item parameters can be estimated iteratively through application of a process such as Conditional Maximum Likelihood estimation (see Rasch model estimation). While more involved, the same fundamental principle applies in such estimations.\n\nThe ICC of the Rasch model for dichotomous data is shown in Figure 4. The grey line maps the probability of the discrete outcome formula_23 (that is, correctly answering the question) for persons with different locations on the latent continuum (that is, their level of abilities). The location of an item is, by definition, that location at which the probability that formula_23 is equal to 0.5. In figure 4, the black circles represent the actual or observed proportions of persons within Class Intervals for which the outcome was observed. For example, in the case of an assessment item used in the context of educational psychology, these could represent the proportions of persons who answered the item correctly. Persons are ordered by the estimates of their locations on the latent continuum and classified into Class Intervals on this basis in order to graphically inspect the accordance of observations with the model. There is a close conformity of the data with the model. In addition to graphical inspection of data, a range of statistical tests of fit are used to evaluate whether departures of observations from the model can be attributed to random effects alone, as required, or whether there are systematic departures from the model.\n\nThe polytomous Rasch model, which is a generalisation of the dichotomous model, can be applied in contexts in which successive integer scores represent categories of increasing level or magnitude of a latent trait, such as increasing ability, motor function, endorsement of a statement, and so forth. The Polytomous response model is, for example, applicable to the use of Likert scales, grading in educational assessment, and scoring of performances by judges.\n\nA criticism of the Rasch model is that it is overly restrictive or prescriptive because an assumption of the model is that all items have equal discrimination, whereas in practice, items discriminations vary, and thus no dataset will ever show perfect data-model fit. A frequent misunderstanding is that the Rasch model does not permit each item to have a different discrimination, but equal discrimination is an assumption of invariant measurement, so differing item discriminations are not forbidden, but rather indicate that measurement quality does not equal a theoretical ideal. Just as in physical measurement, real world datasets will never perfectly match theoretical models, so the relevant question is whether a particular dataset provides sufficient quality of measurement for the purpose at hand, not whether it perfectly matches an unattainable standard of perfection.\n\nA criticism specific to the use of multiple choice items in educational assessment is that there is no provision in the model for guessing because the left asymptote always approaches a zero probability in the Rasch model. These variations are available in models such as the two and three parameter logistic models. However, the specification of uniform discrimination and zero left asymptote are necessary properties of the model in order to sustain sufficiency of the simple, unweighted raw score. In practice, the non-zero lower asymptote found in multiple-choice datasets is less of a threat to measurement than commonly assumed and typically does not result in substantive errors in measurement when well-developed test items are used sensibly \n\nVerhelst & Glas (1995) derive Conditional Maximum Likelihood (CML) equations for a model they refer to as the One Parameter Logistic Model (OPLM). In algebraic form it appears to be identical with the 2PL model, but OPLM contains preset discrimination indexes rather than 2PL's estimated discrimination parameters. As noted by these authors, though, the problem one faces in estimation with estimated discrimination parameters is that the discriminations are unknown, meaning that the weighted raw score \"is not a mere statistic, and hence it is impossible to use CML as an estimation method\" (Verhelst & Glas, 1995, p. 217). That is, sufficiency of the weighted \"score\" in the 2PL cannot be used according to the way in which a sufficient statistic is defined. If the weights are imputed instead of being estimated, as in OPLM, conditional estimation is possible and some of the properties of the Rasch model are retained (Verhelst, Glas & Verstralen, 1995; Verhelst & Glas, 1995). In OPLM, the values of the discrimination index are restricted to between 1 and 15. A limitation of this approach is that in practice, values of discrimination indexes must be preset as a starting point. This means some type of estimation of discrimination is involved when the purpose is to avoid doing so.\n\nThe Rasch model for dichotomous data inherently entails a single discrimination parameter which, as noted by Rasch, constitutes an arbitrary choice of the unit in terms of which magnitudes of the latent trait are expressed or estimated. However, the Rasch model requires that the discrimination is uniform across interactions between persons and items within a specified frame of reference (i.e. the assessment context given conditions for assessment).\n\nApplication of the model provides diagnostic information regarding how well the criterion is met. Application of the model can also provide information about how well items or questions on assessments work to measure the ability or trait. For instance, knowing the proportion of persons that engage in a given behavior, the Rasch model can be used to derive the relations between difficulty of behaviors, attitudes and behaviors. Prominent advocates of Rasch models include Benjamin Drake Wright, David Andrich and Erling Andersen.\n\n\n\n"}
{"id": "26980505", "url": "https://en.wikipedia.org/wiki?curid=26980505", "title": "Seawater desalination in Australia", "text": "Seawater desalination in Australia\n\nAustralia is the driest inhabitable continent on Earth and its installed desalination capacity comprises around 1% of the world’s total. Until a few decades ago, Australia met its demands for water by drawing freshwater from dams and water catchments. As a result of the water supply crisis during the severe 1997–2009 drought, state governments began building desalination plants that purify seawater using reverse osmosis technology. \n\nAustralia's first desalination plant dates from 1903 and several more operated during the 20th century. The first modern large-scale desalination plant was the Kwinana plant in Perth, completed in November 2006 and over 30 plants are currently operating across the country. Many plants are utilizing nearby wind or wave farms to use renewable energy and reduce operating costs, and solar powered desalination units are used for remote communities.\n\nUntil a few decades ago, Australia met its demands for water by drawing freshwater from dams and water catchments. However, during 2000-2010 a significant lack of rainfall drained water reservoirs. The most affected cities were the capitals, where there is high uncertainty in water supply and demand. In 2007, Sydney, the capital city of New South Wales, experienced a dramatic drop of its main dam Warragamba, where water levels dropped to 33% of normal.\n\nAustralia’s first desalination plant was constructed in 1903 to treat saline groundwater in the gold fields of Western Australia at Kalgoorlie. Several desalination plants were built in Australia between 1960 and 1980, especially following the revolution in membrane technology that made reverse osmosis economically viable, but vapor-compression desalination and multi-stage flash distillation plants were also built. By 2002, however, only two reverse osmosis desalination plants were still operating, one on Kangaroo Island and the other on Rottnest Island. Seawater reverse osmosis is the only type of desalination technology currently used for large-scale desalination plants in Australia, the most important of these plants being located in Perth and Sydney.\n\nCompared to existing sources, desalination is considered to be expensive, but research is underway to develop more effective desalination technology. Despite its drawbacks, it is considered a possible solution to the country's water shortages.\n\nAustralia is the driest inhabitable continent on earth and its installed desalination capacity is around 1% of the total world’s desalination capacity. The Department of Agriculture, Fisheries and Forestry has considered several desalination technologies processes in Australia:\n\nA solar powered desalination unit designed for remote communities has been tested in the Northern Territory. The reverse osmosis solar installation (ROSI) uses membrane filtration to provide a reliable and clean drinking water stream from sources such as brackish groundwater. Solar energy overcomes the usually high-energy operating costs as well as greenhouse emissions of conventional reverse osmosis systems. A photovoltaic solar array tracks the Sun and powers the pumps needed to process the water, using the plentiful sunlight available in remote regions of Australia not served by the power grid.\n\nIn Australia many desalination plants are utilizing wind farms to produce enough energy to operate nearby desalination plants. For example, the Kurnell Desalination Plant, with a capacity of producing 250 million liters (ML) of drinking water per day, supplies 15% of Sydney’s water needs via RO technology and is powered using “100 percent renewable energy” from the 140 MW Capital Wind Farm.\nThe Garden Island plant, currently planned for commissioning in 2014, will be powered by wave energy, using Carnegie Wave Energy's CETO system. This system uses submerged buoys to pressurise water offshore, which is piped onshore to either drive turbines for electricity generation or as in this case, to directly desalinate seawater. The Garden Island project is a commercial scale demonstration project, which follows a pilot project off the coast of Fremantle, Western Australia \nThe availability of renewable resources as well as their fluctuation in electricity production from region to region requires a customized design for each desalination facility. In order to maintain steady-state operations many facilities utilize renewably produced energy while connected to a smart grid, importing or exporting energy to the plant as required. The Perth Seawater Desalination Plant utilizes this strategy where 48 wind turbines produce 80MW on the Emu Downs Wind Farm to provide an overall 24MW to the desalination plant. Electrical energy from the renewable energy can also be stored in storage batteries and utilized when needed. As seen in the PV-powered RO system in Gillen Bore, Australia; producing 1,200 L/d. Or if the plant is not required full-time, it can operate using the power as it becomes available. In 2005 a PV-powered hybrid UF/RO filtration system providing 764 liters per day tolerated well power variation from changing weather conditions.\n\nHowever, there are limitations in the ability of renewable sources to provide for desalination facilities. Desalination is a continuous process while renewable energies provide inconsistent power. For any new desalination installation to claim it will be powered by renewable energy, additional renewable energy should be generated.\n\nAbout 2 million people occupy the Perth region in the south western corner of Western Australia. The Perth Seawater Desalination Plant (PSDP) was installed in late 2006 to produce up to 45 gigalitres of potable water per year. In addition, its brine discharge has been shown to have no adverse impact on the environment. The plant buys its power from electricity generated by the Emu Downs Wind Farm, located 200 kilometers north of Perth. The 83 megawatt wind farm consists of 48 wind turbines and contributes over 272 giga-watt-hours (GWhr) per year into the grid, fully offsetting the Perth SWRO Plant’s estimated electrical requirement of 180 GWhr per year. The plant has attracted interest from the world’s water industry and media, and has won numerous national and international awards including the International Desalination Association’s International Desalination Plant of the Year in 2007.\n\nAnother seawater desalination plant on the coast about 160 kilometres south of Perth is now operational. This plant is designed to have an initial annual output of 50 gigalitres, with the potential to double to 100 gigalitres.\n\n"}
{"id": "32535445", "url": "https://en.wikipedia.org/wiki?curid=32535445", "title": "St George's Church, Bergen", "text": "St George's Church, Bergen\n\nSt George's Church, Bergen () is a historic church in Bergen, Norway. This is also the site of the Leprosy Museum (\"Lepramuseet\"). \n\nThe complex is located on Kong Oscars gate, close to the central railway station, in the central part of the city of Bergen. The church is part of the Bergen arch-deanery in the Diocese of Bjørgvin.\n\nThe entire site, including the church, is now open to the public as a Leprosy museum which has been open to the public since 1970. The church remains consecrated and is kept in good order and condition. The church is no longer used as a regular parish church, but it is still used twice a month for English language worship services as well as occasional Swedish language services.\n\nThe church was originally the hospital chapel (\"Hospitalskirken\") in Bergen. A leprosy hospital in Bergen was documented in 1411, and was run at that time by the nuns of Nonneseter Abbey in Bergen. \nIt seems likely that this facility was the immediate forerunner of the hospital and church. The church initially just served the patients at the hospital, but eventually it became its own parish serving the surrounding parts of Årstad. \n\nThe parish was also the base for chaplains at the nearby city prison, poor house, and home for widows.\n\nThe church had burned down with the rest of the hospital in 1640. The church was rebuilt again after the Bergen citywide fire in 1702 when most of the city was burned to ashes. The church building was largely designed in late Baroque style. The present appearance of the church results from a major reconstruction of 1789-90.\n\n"}
{"id": "1595330", "url": "https://en.wikipedia.org/wiki?curid=1595330", "title": "Straight, Incorporated", "text": "Straight, Incorporated\n\nStraight, Incorporated was a controversial non-profit drug rehabilitation program in the United States that existed from 1976 to 1993 and served clients ranging in age from 13 to 20. The organization operated treatment centers in California, Virginia, Texas, Massachusetts, Michigan, Georgia, Maryland, Ohio and Florida. It is unknown how many people graduated from Straight's programs. \n\nFounded by Florida businessmen Mel Sembler and his wife Betty Schlesinger Sembler, along with Joseph Zappala, the program was praised by prominent figures such as former U.S. President George H. W. Bush and former First Lady Nancy Reagan, but was also subject to multiple accusations of abusive practices. In every state that Straight had a facility, abuse was either documented by state investigators or was alleged in civil suits.\n\nStraight was established in St. Petersburg, Florida, in 1976, following the closure the previous year of a program called Seed, Inc. When announcing its establishment, organizers said it would enroll youth ages as young as 10 to 18 who had a history of drug abuses or offenses, ranging from youth whose parents had noted \"minor drug trouble\" to those referred by courts, but would not treat \"addicts or those with a physical dependence on narcotics.\" James Hartz, a clinical psychologist, was announced as its first director. \n\nIn 1981, Dr. Robert DuPont, the founding director of the National Institute on Drug Abuse visited Straight to encourage the organization to expand by creating new facilities nationwide and by training counselors in Straight's methodology. DuPont noted that many American communities lacked drug treatment centers that served young people. By 1982, the program had expanded to enroll youth up to age 20. \n\nFirst Lady Nancy Reagan visited a Straight facility in Florida in 1982. Prior to the visit, she said she did not specifically endorse the program, but an aide told news media that Reagan was impressed with Straight because it was one of the few drug programs that enrolled adolescents, it did not receive government funding, and it was \"drug-free.\" In 1985, Reagan and Princess Diana visited Straight's facility in Springfield, Virginia. The two women attended a group \"rap session\", where Straight clients described their drug use and its sometimes violent consequences. \n\nOver the course of its existence, Straight was in conflict with state licensing officials in Virginia, Maryland and Florida on a number of occasions. As early as January 1978, Florida state officials reported concerns with the program that led it to consider withdrawing its operating license.\n\nVirginia's Department of Mental Health, Mental Retardation and Substance Abuse Services cited Straight's Springfield, Virginia center for violating state regulations repeatedly from the time the facility opened in 1982 to its closing in 1991. Virginia officials argued that state laws required that adolescents in Straight's programs be in school, while Straight believed that its clients should not be attending school until they had made progress in their treatment for substance abuse. In addition, Virginia regulators found that Straight's staff had held young clients against their will, allowed clients to restrain other clients and deprived clients of sleep, food and water as a punishment. Straight responded by denying certain allegations and changing some of its practices. In 1991, Straight decided to move its program from Springfield, Virginia to Columbia, Maryland as a result of what it considered harassment by regulators. \n\nFollowing the closure of Straight's Virginia facility, Maryland officials granted Straight a probationary license to operate a treatment center in Columbia, but only after Straight agreed to modify its practices, by providing educational programs to school-age students either on site or at Howard County, Maryland public schools and by letting parents determine where their children would spend the night while in the early stages of the program. Previously, Straight staff members assigned students to stay with families of Straight clients who were further along in the program. Maryland officials found \"no truth\" to \"allegations of child abuse, the use of physical restraints, [or] brainwashing\" leveled against Straight. Maryland regulators continued to express concerns with Straight's practices until February 1992 when Straight closed the facility amidst declining enrollment and financial problems. \n\nIn 1984, Florida officials found that 13 Straight clients were held in the program against their will and that another 15 had been coerced into enrolling. Straight took the matter to the Florida courts, which ruled that parents could force their minor children into drug rehab. \n\nIn 1993, Florida state investigators audited the state's licensing of Straight's St. Petersburg treatment center and found that officials at the Florida Department of Health and Rehabilitative Services had expressed concerns about Straight's practices, but that the agency granted Straight a license to operate despite those concerns. Regulators were concerned that Straight staff members denied medication to clients and used excessive force to restrain clients. According to the state audit, Straight co-founder Melvin Sembler, a prominent fundraiser for Republican politicians, and several Florida State Senators contacted the Department of Health and Rehabilitative Services in support of Straight. The audit concluded that \"it cannot be unequivocally corroborated that this outside influence actually altered the decision to issue the license [to Straight]\" but that \"it appears that some members of HRS experienced some degree of pressure to grant Straight a license.\"\n\nStraight opened a program in Yorba Linda, California in 1989, but, a year later, the state's Department of Social Services shut down the program after denying it a license to operate as a foster-family agency. State officials cited a record of \"unusual punishments\" at Straight, such as denying teenagers sleep and bathroom breaks. The state also complained about intimidation and ridicule of clients. About 40 Straight clients and parents protested the decision by picketing a local State licensing office, carrying placards with messages such as \"Straight Saves Kids' Lives\".\n\nIn May 1983, Straight, Inc was convicted of false imprisonment after being sued by 20-year-old Fred Collins Jr., who alleged he had been held captive by the program against his will. The program was ordered to pay $40,000 in compensatory and $180,000 in punitive damages. \n\nIn 1990, a jury awarded Karen Norton, a Florida resident, $721,000 in damages due to mistreatment by Straight. In 1982, while a patient in Straight's Florida facility, Norton alleged that staff members assaulted her, denied her health care and refused to give her permission to visit her dying grandfather. At the time, the \"St. Petersburg Times\" described the verdict as the largest award ever against Straight.\n\nStraight's philosophy emphasized the role of peer pressure in a young person's decision to use drugs and as a means for encouraging drug users to become \"straight\". The organization believed that effective treatment required isolating drug users from all of the factors that might explicitly or implicitly encourage drug use, including relationships with family and friends as well as elements of popular culture such as music and clothing. During this period of isolation, Straight clients would receive constant reinforcement from peers about the negative effects of drug use and the necessity of becoming clean. As young people progressed through the Straight program, they would be allowed to gradually assume new responsibilities, for instance by serving as counselors for other young people, and to return to school. \n\nIn 1986, the \"St. Petersburg Times\" followed a 15-year-old boy through his treatment at Straight's Tampa Bay facility. The \"Times\" described Straight's treatment program as follows: \nAt the core of the Straight experience were \"rap sessions\", or discussions led by a Straight staff member on topics such as the rules of the program, clients' experiences with drug use, their current feelings about their drug use and their personal and family problems. In order to be called on to speak at a rap session, a teenager would be required to practice \"Motivating\", a Straight tradition which the \"Times\" described as \"waving your hand in the air... so hard that your arm aches and you begin to perspire.\" The entire group would say \"love you\" when a person finished speaking and would regularly sing songs together. A typical day at a Straight facility consisted of a series of rap sessions from 9am to 7pm, interrupted by several breaks for meals and exercise. On Fridays, patients might finish their last rap session at midnight.\n\nStraight used a twelve-step program modeled after Alcoholics Anonymous. However, the Straight program was also divided into five stages: a client began the program in the first stage, known as \"humbling\", and would gradually advance to subsequent stages as staff members determined that his or her treatment was progressing. In the first phase of the program, patients were not allowed to talk to their parents and were led everywhere by their belt loops, a means of demonstrating to patients that they had lost control of their lives. Patients stayed overnight at the homes of other young people who were further along in the program. This first phase lasted a minimum of 14 days and often for months. Straight clients could progress to the second phase, where they would be allowed to spend the night at home, only once they had convinced staff members that they understood their dependence on drugs and wanted to change their behavior. \"St. Petersburg Times\" reporter David Finkel described the emotional intensity of the humbling phase as follows: \"Only when [a patient] is feeling worthless and miserable is he considered to be making progress.\"\n\nFamilies would become more involved in the second phase. Straight staff would schedule one or more meetings for a client and his or her immediate family, and rap sessions would be held for groups of parents to attend by themselves or with their children. Siblings of Straight clients over the age of 8 were required to attend their own separate meetings. As of 1987, Straight parents were required to comply with a list of rules that ran to six pages in length. Among these rules: both parents were required to be at home every night when their child was in the first or second phase of the program. In the third phase overnight business travel was permitted, and in the fifth phase, vacation was permitted, but in both cases parents were required to submit their plans to Straight for approval. Parents could be asked to host other children in the program overnight. \n\nIn the third phase of the Straight program, clients were allowed to return to school or to start a part-time job. However, clients were still expected to spend their evenings and weekends at the Straight facility, where they would take on new responsibilities, such as assisting with cleaning and greeting visitors. In the fourth and fifth phase of the treatment, clients were only required to come to Straight three or four days a week instead of seven. Fifth phase clients would help lead group sessions. While the first, second and third phases could be completed in a minimum of two or three weeks, clients were expected to spend at least three months in phase four and two months in phase five. At an absolute minimum, a young person could complete the entire Straight program in six months, but in typical cases, 10-14 months were required and sometimes longer periods of time, up to 28 months, were necessary.\n\nStraight graduates participated in follow-up classes once or twice a week, sometimes accompanied by their parents, for the six months following their completion of the program. Graduates were also eligible to return to Straight as paid, part-time staff members.\n\nThe \"St. Petersburg Times\" noted that Straight's treatment practices were designed as a \"gentler\" successor to an earlier program called The Seed, which was closed after an independent report noted that its methods were reminiscent of \"highly refined brainwashing techniques employed by the North Koreans during the 1950s.\"\n\nStraight stated that their methods, while radical, enabled 60% of patients to become drug-free.\n\nStraight officials took the position that drug use in all forms is harmful and requires treatment. In a 1983 speech in Bryan, Ohio, Straight administrator Dave Crock stated that the term \"drug abuse\" itself is problematic because it implies that occasional drug use might be acceptable while only more frequent use of drugs constitutes \"abuse\". In 1981, Straight's Executive Director, James Hartz, said that while his organization did not have a formal policy defining drug abuse, he personally felt that: \"...A 14-year-old who did alcohol and pot and never got arrested, never skipped school - that person in our opinion needs to work through his or her relationship to that drug just as much as the person who is 16 and who was out [breaking and entering], ripping off and so on and so forth.\"\n\nStraight's St. Petersburg branch charged a monthly fee of $385 in 1987, or about $778 in 2012 dollars. In addition, families paid $1089 ($2200 in 2012 dollars) at the outset of the program and then a $1600 evaluation fee ($3233 in 2012 dollars). Many insurance companies did not cover Straight's services. Parents were regularly asked to make small contributions to the organization. In 1990, Straight's program in Yorba Linda, California charged about $1,400 a month, or about $2,460 in 2012 dollars.\n\nA 1989 study in the \"Journal of Substance Abuse Treatment\" conducted interviews with 222 patients of Straight's Virginia facility at least six months after their treatment ended (two thirds of these patients had graduated from the program, while one-third had left before graduation). Before starting treatment at Straight, 97% of these individuals had used marijuana, 56% had used cocaine and 25% had used opiates such as heroin. After leaving the program, 26% reported using marijuana, 14% reported using cocaine and 4% reported using opiates. 35% of former clients reported feeling very satisfied with their experience at Straight, 35% reported feeling somewhat satisfied, 18% reported feeling somewhat dissatisfied and 12% reported feeling very dissatisfied. 53% of patients reported that Straight helped them \"a lot\", 21% reported that the program helped them \"a fair amount\", 18% reported that it helped them \"a little\" and 8% reported that it did not help them at all.\n\nAfter following a young man through Straight for over a year, journalist David Finkel reported feeling ambivalent about the program. Finkel observed \"phenomenal changes\" in the subject of his articles, and noted that the young man had stopped using drugs and that his attitude and his relationship with his parents had improved dramatically. But Finkel also felt that Straight was \"imperfect in many ways\", and criticized the high staff turnover at all levels of the organization, the lack of diversity among Straight clients and the organization's policy of holding minors against their will, if a parent consented. Finkel described this last policy as \"unnecessary\" and \"potentially abusive\". Finkel concluded that he would not consider Straight for his child if she was using drugs experimentally, but that he might try the program if his child had a more severe drug problem and other treatment options, such as counseling, had not succeeded.\n\n\n"}
{"id": "31504671", "url": "https://en.wikipedia.org/wiki?curid=31504671", "title": "Udaan Trust", "text": "Udaan Trust\n\nUdaan Trust is an Indian non-governmental organisation operating in the state of Maharashtra. It is the first HIV/AIDS organisation founded by homosexuals living with HIV/AIDS. Udaan focuses on issues of sexual health within the homosexual and transgender communities, particularly with regard to the prevention of HIV/AIDS. In order to accomplish this, Udaan provides services such as condom distribution, sex education, counseling, and medical services to at-risk populations.\n\nUdaan also attempts to increase awareness of issues relevant to the rights of homosexual and transgender individuals and people living with HIV/AIDS. The organisation does not merely provide services to these individuals - it is meant to reflect their interests, as Udaan employs MSM, TG, and PLHA people (men who have sex with men, transgender, and people living with HIV/AIDS, respectively). Each of the six members of the Board of Directors is a person living with HIV.\n\nUdaan is funded through personal donations, and attends to a network of an estimated 1500 people living with HIV. It has offices in Mumbai, Pune, Thane District, Raigad District, Jalgaon, Nashik, and Ahmednagar.\n\nUdaan operates within the framework provided by Avert Society, a joint enterprise of the U.S. and Indian governments. It was among a number of similar organisations criticised as ineffective by India's National Aids Control Organisation (NACO) in June 2011.\n\nUdaan Trust founded in 1992, in the state of Maharashtra.\n\nOn March 7, 2011, officials of the U.S. Agency for International Development visited Udaan to conduct a review of its work.\n\nOn March 13, 2011, Udaan donated two ambulances to the Gurdaspur police and staffed them with drivers and paramedics.\n"}
{"id": "5721453", "url": "https://en.wikipedia.org/wiki?curid=5721453", "title": "United States Global AIDS Coordinator", "text": "United States Global AIDS Coordinator\n\nThe Global AIDS Coordinator at the United States Department of State is the official responsible for overseeing U.S.-sponsored humanitarian aid programs to combat the AIDS epidemic around the world. The Global AIDS Coordinator has the rank of ambassador-at-large and Assistant Secretary.\nThe current Global AIDS Coordinator is Ambassador Deborah L. Birx.\n\nThe mission of the Office of the U.S. Global AIDS Coordinator (OGAC) is to lead the implementation of the U.S. President's Emergency Plan For AIDS Relief (PEPFAR), the largest commitment ever by any nation for an international health initiative dedicated to a single disease. Initiated in 2003, PEPFAR has been reauthorized three times. What began as an emergency response has since developed into one of the largest international health initiatives. PEPFAR has supported countries in the development of national health and information systems to support the fight of HIV/AIDS and other diseases, and is collaborating with countries around the world to work towards epidemic control.\n\nOGAC is also one of the few offices at the State Department that reports directly to the Secretary of State, instead of going through a Deputy Secretary of State.\n\n\n"}
{"id": "275206", "url": "https://en.wikipedia.org/wiki?curid=275206", "title": "Vaginismus", "text": "Vaginismus\n\nVaginismus is a condition in which involuntary muscle spams prevents vaginal penetration. This often results in pain with attempts at sex. Often it begins when sexual intercourse is first attempted.\nThe underlying cause is generally a fear that penetration will hurt. Risk factors include a history of sexual assault, endometriosis, vaginitis, or a prior episiotomy. Diagnosis is based on the symptoms and examination. It requires there to be no anatomical or physical problems and a desire for penetration on the part of the women.\nTreatment may include behavior therapy such as graduated exposure therapy and gradual vaginal dilatation. Surgery is not generally indicated. Botulinum toxin is being studied. About 0.5% of women are affected. Outcomes are generally good with treatment.\n\nSeverity as well as the pain during penetration varies between women.\n\nA woman is said to have primary vaginismus when she is unable to have penetrative sex or experience vaginal penetration without pain. It is commonly discovered in teenage girls and women in their early twenties, as this is when many girls and young women first attempt to use tampons, have penetrative sex, or undergo a Pap smear. Women with vaginismus may be unaware of the condition until they attempt vaginal penetration. A woman may be unaware of the reasons for their condition.\n\nA few of the main factors that may contribute to primary vaginismus include:\n\n\nPrimary vaginismus is often idiopathic.\n\nVaginismus has been classified by Lamont according to the severity of the condition. Lamont describes four degrees of vaginismus: In first degree vaginismus, the patient has spasm of the pelvic floor that can be relieved with reassurance. In second degree, the spasm is present but maintained throughout the pelvis even with reassurance. In third degree, the patient elevates the buttocks to avoid being examined. In fourth degree vaginismus (also known as grade 4 vaginismus), the most severe form of vaginismus, the patient elevates the buttocks, retreats and tightly closes the thighs to avoid examination. Pacik expanded the Lamont classification to include a fifth degree in which the patient experiences a visceral reaction such as sweating, hyperventilation, palpitations, trembling, shaking, nausea, vomiting, losing consciousness, wanting to jump off the table, or attacking the doctor. The Lamont classification continues to be used to the present and allows for a common language among researchers and therapists.\n\nAlthough the pubococcygeus muscle is commonly thought to be the primary muscle involved in vaginismus, Pacik identified two additionally-involved spastic muscles in treated patients under sedation. These include the entry muscle (bulbocavernosum) and the mid-vaginal muscle (puborectalis). Spasm of the entry muscle accounts for the common complaint that patients often report when trying to have intercourse: \"It's like hitting a brick wall\".\n\nSecondary vaginismus occurs when a person who has previously been able to achieve penetration develops vaginismus. This may be due to physical causes such as a yeast infection or trauma during childbirth, while in some cases it may be due to psychological causes, or to a combination of causes. The treatment for secondary vaginismus is the same as for primary vaginismus, although, in these cases, previous experience with successful penetration can assist in a more rapid resolution of the condition. Peri-menopausal and menopausal vaginismus, often due to a drying of the vulvar and vaginal tissues as a result of reduced estrogen, may occur as a result of \"micro-tears\" first causing sexual pain then leading to vaginismus.\n\nFurther factors that may contribute to either secondary or primary vaginismus include:\n\nWhich muscles are involved is unclear but may include the pubococcygeus muscle, levator ani, bulbocavernosus, circumvaginal, and perivaginal muscles.\n\nA Cochrane review found little high quality evidence regarding the treatment of vaginismus in 2012. Specifically it is unclear if systematic desensitisation is better than other measures including nothing.\n\nAccording to Ward and Ogden's qualitative study on the experience of vaginismus (1994), the three most common contributing factors to vaginismus are fear of painful sex; the belief that sex is wrong or shameful (often the case with patients who had a strict religious upbringing); and traumatic early childhood experiences (not necessarily sexual in nature).\n\nPeople with vaginismus are twice as likely to have a history of childhood sexual interference and held less positive attitudes about their sexuality, whereas no correlation was noted for lack of sexual knowledge or (non-sexual) physical abuse.\n\nOften, when faced with a person experiencing painful intercourse, a gynecologist will recommend Kegel exercises and provide some additional lubricants. Strengthening the muscles that unconsciously tighten during vaginismus may be extremely counter-intuitive for some people. Although vaginismus has not been shown to affect a person's ability to produce lubrication, providing additional lubricant can be helpful in achieving successful penetration. This is due to the fact that women may not produce natural lubrication if anxious or in pain. Treatment of vaginismus may involve the use Hegar dilators, (sometimes called vaginal trainers) progressively increasing the size of the dilator inserted into the vagina.\n\nBotulinum toxin A (Botox) has been considered as a treatment option, under the idea of temporarily reducing the hypertonicity of the pelvic floor muscles. Although no random controlled trials have been done with this treatment, experimental studies with small samples have shown it to be effective, with sustained positive results through 10 months. Similar in its mechanism of treatment, lidocaine has also been tried as an experimental option.\n\nAnxiolytics and antidepressants are other pharmacotherapies that have been offered to people in conjunction with other psychotherapy modalities, or if these patients experience high levels of anxiety from their condition. Results from these medications have not been consistent.\n\nTrue epidemiological studies of vaginismus have not been done, as diagnosis would require painful examinations that such women would most likely avoid. Data available is primarily reported statistics from clinical settings.\n\nA study of vaginismus in people in Morocco and Sweden found a prevalence of 6%. 18-20% of people in British and Australian studies were found to have manifest dyspareunia, while the rate among elderly British people was as low as 2%.\n\nA 1990 study of people presenting to sex therapy clinics found reported vaginismus rates of between 12% and 17%, while a random sampling and structured interview survey conducted in 1994 by National Health and Sexual Life Survey documented 10%-15% of people reported that in the past six months they had experienced pain during intercourse.\n\nThe most recent study-based estimates of vaginismus incidence range from 5% to 47% of people presenting for sex therapy or complaining of sexual problems, with significant differences across cultures. It seems likely that a society's expectations of person's sexuality may particularly impact on the people with the condition.\n\n"}
