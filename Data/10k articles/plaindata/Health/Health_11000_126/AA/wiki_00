{"id": "41495125", "url": "https://en.wikipedia.org/wiki?curid=41495125", "title": "Avian immune system", "text": "Avian immune system\n\nThe avian immune system refers to the system of biological structures and cellular processes that protects birds from disease.\n\nThe avian immune system resembles that of mammals since both evolved from a common reptilian ancestor and have inherited many commonalities. They have also developed a number of different strategies that are unique to birds. Most avian immunology research has been carried out on the domestic chicken, \"Gallus gallus domesticus\". Birds have lymphoid tissues, B cells, T cells, cytokines and chemokines like many other animals. In addition, they can also have tumours, immune deficiency and autoimmune diseases.\n\nThe physiology and immune system of birds resembles that of other animals. The lymphomyeloid tissues develop from epithelial or mesenchymal anlages that are full of haematopoetic cells. The bursa fabricus, thymus, spleen and lymph nodes all develop when haematopoetic stem cells enter the bursal or thymic anlages and become competent B and T cells.\nThe avian immune system is divided into two types of immunity, the innate and adaptive ones. The innate immune system includes physical and chemical barriers, blood proteins and phagocytic cells. In addition, complement serum proteins, which are a part of the innate immune system, work with antibodies to lyse target cell. Adaptive immunity, on the other hand, kicks in when the innate system fails to stop invading pathogens. The adaptive response includes targeted recognition of specific molecular features on the surface of the pathogen. Birds, like other animals, have B cells, T cells and humoral immunity as part of their adaptive response.\n\nVarious bird organs function to differentiate avian immune cells: the thymus, Bursa of Fabricius and bone marrow are primary avian lymphoid organs whereas the spleen, mucosal associated lymphoid tissues (MALT), germinal centers, and diffuse lymphoid tissues are secondary lymphoid organs. Birds do not have lymph nodes. The thymus, where T cells develop, is located in the neck of birds. The Bursa of Fabricius is an organ that is unique to birds and is the only site for B cell differentiation and maturation. Located in the rump of birds, this organ is full of stem cells and very active in young birds but atrophies after six months.\nBronchial associated lymphoid tissue (BALT) and gut associated lymphoid tissue (GALT) are found along the bronchus and intestines, respectively. In the avian respiratory system, there are heterophils, which are an important part of bird immunity. Within the head, there is head associated lymphoid tissues (HALT) that contain the Harderian gland, lacrimal gland and other structures in the larynx or nasopharynx. The Harderian gland is located behind the eyeballs and is the major component of HALT. It contains a large number of plasma cells and is the main secretory body of antibodies.\nAlongside these primary and secondary lymphoid organs, there is also the lymphatic circulatory system of vessels and capillaries that communicate with the blood supply and transport the lymph fluid throughout the bird’s body.\n\nThe antigen recognition by T cells is a remarkable process dependent on the T cell receptor (TCR). The TCR is randomly generated and thus has extensive diversity in the peptides-MHC complexes it can recognize. Using monoclonal antibodies that are specific for chicken T cell surface antigens, the development of T cells in birds is studied. The differentiation pathways, functional processes and molecules of T cells are highly conserved in birds. However, there are some novel features of T cells that are unique to birds. These include a new lineage of cytoplasmic CD3+ lymphoid cells (TCR0 cells) and a T cell sublineage that expresses a different receptor isotypes (TCR3) generated exclusively in the thymus.\nHomologues of the mammalian gamma, delta and alpha beta TCR (TCR1 and TCR2) are found in birds. However, a third TCR, called TCR3, has been found in avian T cell populations that lack both TCR1 and TCR2. These were found on all CD3+ T cells and were either CD4+ or CD8+. This subset of T cells, as others, develops in the thymus and gets seeded throughout the body with the exception of the intestines.\nThe pattern of accessory molecules expressed by avian T cells resembles mammalian α/β T cells. High CD8 expression precedes the dual expression of CD4 and CD8 but following clonal selection and expansion, avian T cells cease to express either CD4 or CD8.\n\nThe central organ for B cell development in birds is the Bursa of Fabricius. The function of the bursa was discovered when it was surgically removed from neonatal chicks and this led to an impaired antibody response to \"Salmonella typhimurium\". It is now clear that the bursa is the primary site of B cell lymphopoeisis and that avian B cell development has some unique properties compared to human or mouse models.\nAlmost all the B cell progenitors in the bursa of 4-day-old chickens express IgM on their cell surface. Studies have shown that B cells of 4 – 8 week old birds are derived from 2 – 4 allotypically committed precursor cells in each follicle. Bursal follicles are colonized by 2-5 pre-bursal stem cells and these undergo extensive proliferation after they are committed to an allotype. Expression of IgM is controlled by a biological clock as opposed to the bursal microenvironment. Moreover, the source of all B cells in adult birds was determined to be a population of self-renewing sIg+ B cells.\n\nIn studying the development of the avian immune system, the embryo offers several advantages such as the availability of many embryos at precise stages of development and distinct B and T cell systems. Each population differentiates from a primary lymphoid organ: T cells in the thymus and B cells in the Bursa of Fabricius. Research has found that early feeding of hydrated nutritional supplements in chickens heavily affects the immune system development. This is often measured by weight of the Bursa of Fabricius, improved resistance to disease and earlier appearance of IgA.\nUnlike other animals, newly hatched chicks are born with an incomplete immune system. Here, the amniotic fluid and yolk of the egg contain the maternal immunity to be passed on to the hatchling. Swallowing of the amniotic fluid during hatching confers immunity to these chicks until their immune system develops fully. In the first six weeks of the bird’s life, continuous gene conversion in the bursa completes the immune system. Upon hatch, birds do not have a library of genetic information for B cells to use for antibody production. Instead, B cells mature in the bursa during the first six weeks and then go on to seed other organs of the immune system. As a result, birds are highly susceptible to pathogens in the first few weeks after hatching. Research found that T cells from mature chickens proliferated extensively and produced high levels of IL-2 and other cytokines. On the other hand, T cells from 24 hour-old chickens failed to proliferate and could not secrete cytokines.\nGene conversion within the bursa leads to the development of antibodies that are diverse in their recognition ability. Mammalian V, D and J gene segments allow for many combinations and therefore, yield a vast repertoire of antibodies. However, birds have only a single functional copy of the V and J genes for the Ig light chain and a single functional copy of the V and J heavy chain genes. This results in a low diversity from gene rearrangements of Ig heavy and light chains. However, clusters of pseudogenes upstream of the heavy and light gene Ig loci take part in somatic gene conversion – a process where pseudogenes replace the V and V genes. This diversifies the repertoire of bird antibodies.\n\nLittle is known about the innate immune system of birds. Most research has been focused on chickens due to the increased threat of viral diseases within the poultry population. The innate immune response is known to be essential for viral infection and as a result, the publication of the full chicken genome sequence is a source for identifying possible adjuvants and immunity genes.\n\nAvian immunity begins to develop at the end of embryonic life but the majority of early immunity is obtained via passive acquisition of maternal antibodies. Such antibodies are found within the egg when it is laid and originated from the yolk of the egg. Kramer and Cho have shown immunoglobulins in both the egg white and in the embryo. Maternal IgA and IgM get transferred to the egg as it passes down the oviduct.\n\nAn important element of immune systems in various animals is the protein tristetraprolin (TTP). This plays a key anti-inflammatory role by regulating TNFα. Mouse models with TTP knockouts result in chronic and often deadly inflammation when exposed to small amounts of pathogen-associated molecular patterns (PAMPs). However, TTP and its homologs is altogether absent from birds. Avian genomes have been searched for similar sequences to TTP and bird cell lines have been exposed to foreign proteins and bacteria molecules known to stimulate TTP production but no evidence of TTP has been found. The missing protein poses a very different immune response regulation in birds as opposed to mammals, reptiles and amphibians.\n\nThe avian T cell population, like that of mammals develops in the thymus. However, the thymus in birds is a paired organ composed of many separated lobes of ovoid tissue in the neck. These are close to the vagus nerve and the jugular vein and are most active in young hatchlings. It is postulated that this organ is linked to erythropoeitic function and closely associated with the avian breeding cycle. The removal of the thymic lobes has been correlated to birds rejecting allogeneic skingrafts and delayed skin reactions.\n\nThe bursa of Fabricus is a globular or spherical lymphoepithelial organ. The inner surface is littered with folds, which resemble Peyer's patches in mammals and obscure the lumen. Its growth is correlated with the rapid body growth. It regresses and disappears about the time of sexual maturity. The bursa, as studied through bursectomy at different developmental stages, indicates sequential development of IgG, IgM and IgA. The secondary (peripheral) lymphoid tissue also includes unique lymphoid nodules in the digestive tract and solitary nodules scattered throughout the body, a characteristic of avian species. Meanwhile, lymph nodes only occur in some water, marsh and shore species.\n\nControl of infectious disease is essential for the production of healthy poultry flocks. Vaccination programs have been used extensively in North American factory farming methods to induce avian immune responses against bird pathogens. These include Marek’s Disease, Duck Hepatitis Virus, Chicken Anemia Virus, Turkeypox, Fowlpox and others. Bird immunity is reliant on a complex network of cell types and soluble factors that must properly function in order for large commercial poultry flocks to survive.\n\nInfectious bursal disease virus and chicken anemia are ubiquitous and have increased interest in combatting avian pathogens. Parasites of birds are another emerging concern since the crowded nature of poultry farms facilitates easy spreading.\n\nSeveral immunosuppressive agents are encountered by birds including viruses, bacteria, parasites, toxins, mycotoxins, chemicals and drugs. The most common immunosuppressive viruses are Infectious Bursal Disease Virus (IBDV), Avian Leukosis, Marek’s Disease (MD) and Hemorrhagic Enteritis Virus (HEV). Concurrent immunosuppressive infections are an emerging concern in the poultry industry whereby early infection with IBDV causes the MD virus to come out of dormancy and contribute to active disease. New studies show that stress is the number one cause of immunosuppression in birds. Stressors leave birds more susceptible to infectious agents and therefore, new poultry management guidelines need to be endorsed.\n\nThe migratory nature of birds poses a distinct danger for the spreading of diseases. Without being affected by the infectious agent, birds can act as vectors in spreading psittacosis, salmonellosis, campylobacteriosis, mycobacteriosis, avian influenza, giardiasis and cryptosporidiosis. These zoonotic diseases can be transmitted to humans.\nIn the case of avian influenza (H5N1 strain), water birds can be infected with the low pathogenic form or the high pathogenic form. The former induces mild symptoms such as a drop in egg production, ruffled feathers and mild effects on the avian respiratory tract. The highly pathogenic form spreads much more rapidly and can infect multiple tissues and organs. Massive internal bleeding and hemorrhaging follow and this has earned the H5N1 virus the moniker “chicken ebola.”\n\nMuch like other animals, birds are prone to cancers and tumours. This refers to the abnormal growth of cells in a tissue or organ that can be either malignant or benign. Internal cancers can occur in the kidneys, liver, stomach, ovary, muscles or bone. Squamous cell carcinoma is a form of skin cancer that birds obtain, manifesting on the wing tips, toes, and around the beak and eyes. The cause is believed to be high exposure to UV rays. Additionally, a cancer of the connective tissue, known as fibrosarcoma, is often seen in the leg or wing. This occurs in many parrot species, cockatiels, macaws and budgerigars. Treatment options include amputation and surgery.\n"}
{"id": "16616468", "url": "https://en.wikipedia.org/wiki?curid=16616468", "title": "Boston University College of Health and Rehabilitation Sciences (Sargent College)", "text": "Boston University College of Health and Rehabilitation Sciences (Sargent College)\n\nThe Boston University College of Health and Rehabilitation Sciences: Sargent College (SAR) is a unit of Boston University. The College offers both undergraduate and graduate degree programs to prepare students for both research and clinical careers in health care and the rehabilitation sciences.\n\nIn 1881, Dr. Dudley Allen Sargent founded the Sargent School of Physical Training in Cambridge, Massachusetts. During the decades that followed the establishment of the School, Dr. Sargent built a reputation as an innovator in physical conditioning and health promotion. At the Sargent School, students learned training techniques to strengthen and improve the physical capabilities of all people, including both disabled and healthy individuals. This emphasis on comprehensive health care remains a focus of the College today. Sargent College became part of Boston University in 1929, five years after Dr. Sargent's death.\n\nIn the fall of 1990, BU Sargent College moved to an extensively renovated facility at 635 Commonwealth Avenue. The six-story building contains classrooms, student lounges, research laboratories, an outpatient clinic, and faculty and staff offices.\n\nThe Sargent College has four academic departments that confer degrees upon undergraduate and graduate students:\n\nSAR offers the following degrees at the undergraduate level:\n\nSAR offers the following degrees in the graduate program:\n\nBU Sargent College houses the Center for Psychiatric Rehabilitation, the first nationally funded psychiatric rehabilitation center in the United States.\n\nOver the years, \"U.S. News & World Report\"'s Best Graduate School rankings have consistently found the graduate programs at BU Sargent College to be among the best in the country. Sargent College graduate programs ranked as follows: occupational therapy program was ranked #1, physical therapy: 14 (out of 217 programs), speech-language pathology: 12 (out of 249 programs).\n\n"}
{"id": "4983663", "url": "https://en.wikipedia.org/wiki?curid=4983663", "title": "Catastrophic antiphospholipid syndrome", "text": "Catastrophic antiphospholipid syndrome\n\nCatastrophic antiphospholipid syndrome (CAPS), also known as Asherson's syndrome, is an acute and complex biological process that leads to occlusion of small vessels of various organs. It was first described by Ronald Asherson in 1992. The syndrome exhibits thrombotic microangiopathy, multiple organ thrombosis, and in some cases tissue necrosis and is considered an extreme or catastrophic variant of the antiphospholipid syndrome.\n\nCAPS has a mortality rate of about 50%. With the establishment of a CAPS-Registry more has been learned about this syndrome, but its cause remains unknown. Infection, trauma, medication, and/or surgery can be identified in about half the cases as a \"trigger\". It is thought that cytokines are activated leading to a cytokine storm with the potentially fatal consequences of organ failure. A low platelet count is a common finding. Individuals with CAPS often exhibit a positive test to antilipid antibodies, typically IgG, and may or may not have a history of lupus or another connective tissue disease. Association with another disease such as lupus is called a secondary APS unless it includes the defining criteria for CAPS.\n\nClinically, the syndrome affects at least three organs and may affect many organs systems. Peripheral thrombosis may be encountered affecting veins and arteries. Intra-abdominal thrombosis may lead to pain. Cardiovascular, nervous, kidney, and lung system complications are common. The affected individual may exhibit skin purpura and necrosis. Cerebral manifestations may lead to encephalopathy and seizures. Myocardial infarctions may occur. Strokes may occur due to the arterial clotting involvement. Death may result from multiple organ failure.\n\nTreatments may involve the following steps:\n\n\n"}
{"id": "9568457", "url": "https://en.wikipedia.org/wiki?curid=9568457", "title": "Christian Medical and Dental Fellowship of Australia", "text": "Christian Medical and Dental Fellowship of Australia\n\nChristian Medical Fellowship of Australia has its historical roots in the Inter Varsity Fellowship (IVF) and the Christian Medical Fellowship (CMF) that started in the UK. At the same time as many other similar groups were being set up around the world after World War II, many separate Australian state fellowships of doctors and dentists were being founded.\n\nThese groups united as a national body in 1962 but it was not until 1998 that the Christian Medical and Dental Fellowship of Australia (CMDFA) was officially established. In 2000 the central office in Sydney opened to assist with growing administrative needs.\n\nCMDFA is linked with the International christian medical and dental association.\n\n\n"}
{"id": "2016232", "url": "https://en.wikipedia.org/wiki?curid=2016232", "title": "Clinical endpoint", "text": "Clinical endpoint\n\nIn a clinical research trial, a clinical endpoint generally refers to occurrence of a disease, symptom, sign or laboratory abnormality that constitutes one of the target outcomes of the trial, but may also refer to any such disease or sign that strongly motivates the withdrawal of that individual or entity from the trial, then often termed \"humane (clinical) endpoint\". \n\nThe primary endpoint of a clinical trial is the endpoint for which subjects are randomized and for which the trial is powered. Secondary endpoints are endpoints that are analyzed \"post hoc\", for which the trial may not be powered nor randomized.\n\nIn a general sense, a clinical endpoint is included in the entities of interest in a trial. The results of a clinical trial generally indicate the number of people enrolled who reached the pre-determined clinical endpoint during the study interval compared with the overall number of people who were enrolled. Once a patient reaches the endpoint, he or she is generally excluded from further experimental intervention (the origin of the term \"endpoint\").\n\nFor example, a clinical trial investigating the ability of a medication to prevent heart attack might use \"chest pain\" as a clinical endpoint. Any patient enrolled in the trial who develops chest pain over the course of the trial, then, would be counted as having reached that clinical endpoint. The results would ultimately reflect the fraction of patients who reached the endpoint of having developed chest pain, compared with the overall number of people enrolled.\n\nWhen an experiment involves a control group, the proportion of individuals who reach the clinical endpoint after an intervention is compared with the proportion of individuals in the control group who reached the same clinical endpoint, reflecting the ability of the intervention to prevent the endpoint in question.\n\nA clinical trial will usually define or specify a \"primary endpoint\" as a measure that will be considered success of the therapy being trialled (e.g. in justifying a marketing approval). The primary endpoint might be a statistically significant improvement in \"overall survival\" (OS). A trial might also define one or more \"secondary endpoints\" such as \"progression-free-survival\" (PFS) that will be measured and are expected to be met. A trial might also define \"exploratory endpoints\" that are less likely to be met.\n\nClinical endpoints can be obtained from different modalities, such as, behavioural or cognitive scores, or biomarkers from Electroencephalography (qEEG), MRI, PET, or biochemical biomarkers.\n\nIn clinical cancer research, common endpoints include discovery of local recurrence, discovery of regional metastasis, discovery of distant metastasis, onset of symptoms, hospitalization, increase or decrease in pain medication requirement, onset of toxicity, requirement of salvage chemotherapy, requirement of salvage surgery, requirement of salvage radiotherapy, death from any cause or death from disease. A cancer study may be powered for overall survival, usually indicating time until death from any cause, or disease specific survival, where the endpoint is death from disease or death from toxicity.\n\nThese are expressed as a period of time (survival duration) e.g., in months. Frequently the median is used so that the trial endpoint can be calculated once 50% of subjects have reached the endpoint, whereas calculation of an arithmetical mean can only be done after all subjects have reached the endpoint.\n\nThe disease free survival is usually used to analyze the results of the treatment for the localized disease which renders the patient apparently disease free, such as surgery or surgery plus adjuvant therapy. In the disease-free survival, the event is relapse rather than death. The people who relapse are still surviving but they are no longer disease-free. Just as in the survival curves not all patients die, in \"disease-free survival curves\" not all patients relapse and the curve may have a final plateau representing the patients who didn't relapse after the study's maximum follow-up. Because the patients survive for at least some time after the relapse, the curve for the actual survival would look better than disease free survival curve.\n\nThe Progression Free Survival is usually used in analysing the results of the treatment for the advanced disease. The event for the progression free survival is that the disease gets worse or progresses, or the patient dies from any cause. \"Time to Progression\" is a similar endpoint that ignores patients who die before the disease progresses.\n\nThe response duration is occasionally used to analyze the results of the treatment for the advanced disease. The event is progression of the disease (relapse). This endpoint involves selecting a subgroup of the patients. It measures the length of the response in those patients who responded. The patients who don't respond aren't included.\n\nOverall survival is based on death from any cause, not just the condition being treated, thus it picks up death from side effects of the treatment, and effects on survival after relapse.\n\nA humane endpoint can be defined as the point at which pain and/or distress is terminated, minimized or reduced for an entity in a trial (such as an experimental animal), by taking action such as killing the animal humanely, terminating a painful procedure, or giving treatment to relieve pain and/or distress. The occurrence of an individual in a trial having reached may necessitate withdrawal from the trial before the target outcome of interest has been fully reached.\n\nA surrogate endpoint (or \"marker\") is a measure of effect of a certain treatment that may correlate with a \"real\" clinical endpoint but doesn't necessarily have a guaranteed relationship. The National Institutes of Health (USA) define surrogate endpoint as \"a biomarker intended to substitute for a clinical endpoint\".\n\nSome studies will examine the incidence of a \"combined endpoint\", which can merge a variety of outcomes into one group. For example, the heart attack study above may report the incidence of the \"combined endpoint\" of chest pain, myocardial infarction, or death. An example of a cancer study powered for a combined endpoint is disease-free survival (DFS); trial participants experiencing either death or discovery of any recurrence would constitute the endpoint. Overall Treatment Utility is an example of a multidimensional composite endpoint in cancer clinical trials.\n\nRegarding humane endpoints, a combined endpoint may constitute a threshold where there is enough cumulative degree of disease, symptoms, signs or laboratory abnormalities to motivate an intervention.\n\nEach trial may define what is considered a complete response (CR) or partial response (PR) to the therapy or intervention. Hence the trials report the 'complete response rate' and the overall response rate which includes CR and PR. (e.g. see Response Evaluation Criteria in Solid Tumors, and Small-cell carcinoma#Treatment, and for immunotherapies : Immune-Related Response Criteria)\n\nVarious studies on a particular topic often do not address the same outcomes, making it difficult to draw clinically useful conclusions when a group of studies is looked at as a whole. The Core Outcomes in Women's Health (CROWN) Initiative is one effort to standardize outcomes.\n\n\n"}
{"id": "52219883", "url": "https://en.wikipedia.org/wiki?curid=52219883", "title": "Diabetic Association of Bangladesh", "text": "Diabetic Association of Bangladesh\n\nDiabetic Association of Bangladesh is a non-profit organization medical organization and is located in Dhaka, Bangladesh. Professor AK Azad Khan is the present president of the association.\n\nThe organization was established in 1956 in East Pakistan by Muhammad Ibrahim, National professor of Bangladesh. The organization started in Segun Bagicha as an outpatient clinic. In 1980 it established BIRDEM, a specialized research hospital on diabetics. The organization is a regional collaborating centre of the World Health Organization. It has presence with over 80 local centres. It raises awareness about diabetics and provides medical advice and screening.\n"}
{"id": "9782355", "url": "https://en.wikipedia.org/wiki?curid=9782355", "title": "E-Foundation for Cancer Research", "text": "E-Foundation for Cancer Research\n\nE-Foundation for Cancer Research is online and non-profit foundation for cancer research. This foundation provides free, professional cancer research courses that aim to enhance the knowledge and skills of health professionals and anyone concerned with the care of cancer patients.\n\n\nthey are freely available to a global body of learners; \nthey are flexible: learners can engage in the courses, and assess their understanding, at their own pace and from anywhere that they enjoy an Internet connection; \nthey are authoritative and reliable, authored and developed by specialists in their field.\n\n"}
{"id": "21894545", "url": "https://en.wikipedia.org/wiki?curid=21894545", "title": "Familial partial lipodystrophy", "text": "Familial partial lipodystrophy\n\nFamilial partial lipodystrophy (FPL), also known as Köbberling–Dunnigan syndrome, is a rare genetic metabolic condition characterized by the loss of subcutaneous fat.\n\nFPL also refers to a rare metabolic condition in which there is a loss of subcutaneous fat in the arms, legs and lower torso. The upper section of the body, face, neck, shoulders, back and trunk carry an excess amount of fat.\n\nAs the body is unable to store fat correctly this leads to fat around all the vital organs and in the blood (triglycerides). This results in heart problems, cirrhosis of the liver, lipoatrophic diabetes, and pancreatitis, along with various other complications.\n\nType 1 is believed to be underdiagnosed.\n\nA mutations in a number of genes have been associated with this condition. Mutations associated with FPL have been reported in \"LMNA\" (lamin A/C), \"PPARG\" (PPARγ), \"AKT2\" (AKT serine/threonine kinase 2), \"PLIN1\" (perilipin-1), and \"CIDEC\" (cell-death-inducing DFFA-like effector B).\n\nSix types (1-6) have been described. Types 1-5 are inherited in an autosomal dominant fashion. \n\nType 1 (Kobberling variety, FPL1) is very rare and has only been reported in women to date. Fat loss is confined to the limbs and mostly in the distal parts. Central obesity may be present. Complications include hypertension, insulin resistance and hypertriglyceridemia. The gene causing this condition is not yet known. This form was first described in 1975. \n\nType 2 (Dunnigan Variety, FPL2) is the most common form and is due to mutations in the LMNA gene. Over 500 cases have been reported to date. Development up to puberty is normal. Fat is then gradually lost in is the limbs and trunk. Fat may accumulate around the face and between the shoulder blades. Insulin resistance is common. Other conditions associated with this condition include acanthosis nigricans, fatty liver, hypertriglyceridemia and polycystic ovary syndrome in women. There is an increased risk of coronary heart disease. Cardiomyopathy and muscular dystrophy may occur rarely. Xanthoma and nail changes may occur. \n\nType 3 is due to mutations in the PPARG gene. It is rare with approximately 30 cases reported to date. It is similar to type 2 but tends to be milder. \n\nType 4 is due to mutations in the PLIN1 gene. It is rare with only a small number of cases reported. Fat loss tends to affect the lower limbs and buttocks. Insulin resistance and hypertriglyceridemia occur. Calf muscular hypertrophy may occur. \n\nType 5 is due to mutations in the AKT2 gene. It has been reported in four patients all members of the same family. Fat loss affects the upper and lower limbs. The patients also suffered from hypertension, insulin resistance and hypertriglyceridemia. \n\nType 6 due to mutations in the CIDEC gene. It is inherited in an autosomal recessive fashion and has been reported in only one patient to date. Features included fat loss, severe insulin resistance, fatty liver, acanthosis nigricans and diabetes.\n\nAnother gene that has been associated with this condition is AGPAT2.\n\nThis not known with certainty but is estimated to be about one per million. It appears to be more common in females than males.\n\n"}
{"id": "18311117", "url": "https://en.wikipedia.org/wiki?curid=18311117", "title": "Femarelle", "text": "Femarelle\n\nFemarelle is a dietary supplement that is a mixture of DT56a (a tofu extract) and flaxseed powder, that may act as a selective estrogen receptor modulator (SERM). In 2008 an application was submitted to the European Food Safety Authority to market Femarelle with a health claim, namely that it can reduce the risk for osteoporosis and other bone disorders; the EFSA found that \"the food/constituent for which the claim is made, i.e. Femarelle, has not been sufficiently characterised\" and that \" a cause and effect relationship has not been established between the consumption of Femarelle and increased BMD, increased bone formation, or decreased risk of osteoporosis or other bone disorders in post-menopausal women.\"\n\nFemarelle has been tested in small clinical trials. One studied its effect on the tissue lining the vagina, another on relief of hot flashes in menopause, and another on the risk of causing blood clots, which is a risk of hormone replacement therapy. While results were promising, the studies were too small and too short in duration from which to draw conclusions.\n\n"}
{"id": "1950212", "url": "https://en.wikipedia.org/wiki?curid=1950212", "title": "Food and Drugs Act", "text": "Food and Drugs Act\n\nThe Food and Drugs Act (the \"Act\") (formal title \"An Act respecting food, drugs, cosmetics and therapeutic devices\") is an act of the Parliament of Canada regarding the production, import, export, transport across provinces and sale of food, drugs, contraceptive devices and cosmetics (including personal cleaning products such as soap and toothpaste). It was first passed in 1920 and most recently revised in 1985. It attempts to ensure that these products are safe, that their ingredients are disclosed and that drugs are effective and are not sold as food or cosmetics. It also states that cures for disease listed in Schedule A (including cancer, obesity, anxiety, asthma, depression, appendicitis, and sexually transmitted diseases), cannot be advertised to the general public.\n\nAfter the launch of the Federal Department of Health in 1919, the \"Food and Drugs Act\" was presented in late 1920. Rules and regulations developed under the \"Act\" established the requirements for licensing and creating drugs in Canada. The law granted the Minister of Health the right to cancel or suspend licenses of companies failing to comply with the requirements.\n\nThe \"Food and Drugs Act\" was not significantly modified until 1947 when the foundations were laid for the current market today. In 1951, drug manufacturers were required to submit a file for each new drug prior to marketing their product. However, during the early 1960s, the drug thalidomide, which had been approved to enter the market, resulted in the deaths of thousands of infants and severe birth defects in others when the drug was taken by women in early stages of pregnancy.\n\nAs a result of the problems caused by the drug thalidomide, the \"Act\" was revisited and strengthened by Health Canada. The revised version placed new requirements on manufacturers to provide evidence for efficacy in seeking a Notice of Compliance, which must be obtained before any drug could be sold. The manufacturer must meet all the requirements before making any drug available to the public, but once the drug passes with no adverse reactions and without any changes needed to the drug's formula, it may never be subjected to review by Health Canada again. Some health advocates want post-approval surveillance to watch for unexpected problems.\n\nPart I provides general interpretations of the terms, and provides details of each of the topics discussed on what the \"Act\" entails:\n\nPart II of the \"Act\" focuses the administration and the Enforcement that allows the government to intervene with the manufacturer.\nIt entails:\n\nParts III (enacted in 1961) and IV (enacted in 1969) provided for implementation of controls required by the Convention on Psychotropic Substances. Part III dealt with \"controlled\" drugs such as amphetamine, methaqualone, and phenmetrazine, which have legitimate medical uses. Part IV focused on Schedule H \"restricted drugs\", those whose only legitimate use is for scientific research, such as the hallucinogens LSD, DMT, and MDMA. These parts established eight classes of regulated substances, ranging from Schedules A to H.\n\nThe 1996 \"Controlled Drugs and Substances Act\" repealed Parts III and IV.\n\nOn April 2008, an amendment to the \"Food and Drugs Act\", Canadian Bill C-51 was tabled in the House of Commons. The purpose of this bill was to modernize the regulatory system for foods and therapeutic products, to strengthen the oversight of the benefits and risks of therapeutic products throughout their life cycle, to support effective compliance and enforcement actions and to enable a greater transparency and openness of the regulatory system. Some of the proposed amendments are as follows:\n\n\nThe bill has been subject to criticism due to a perception that the bill would illegalize all food and Natural Health Products by categorizing them as drug products. Natural health products have not been regulated as drugs since the Natural Health Products Regulations were put into place on January 1, 2004. Health Canada has stated \"The Natural Health Product Regulations, introduced in 2004, will continue to operate the same way under Bill C-51. Canadians will continue to have access to natural health products that are safe, effective and of high quality.\"\n\n\n"}
{"id": "249930", "url": "https://en.wikipedia.org/wiki?curid=249930", "title": "Heart failure", "text": "Heart failure\n\nHeart failure (HF), also known as chronic heart failure (CHF), is when the heart is unable to pump sufficiently to maintain blood flow to meet the body's needs. Signs and symptoms of heart failure commonly include shortness of breath, excessive tiredness, and leg swelling. The shortness of breath is usually worse with exercise, while lying down, and may wake the person at night. A limited ability to exercise is also a common feature. Chest pain, including angina, does not typically occur due to heart failure.\nCommon causes of heart failure include coronary artery disease including a previous myocardial infarction (heart attack), high blood pressure, atrial fibrillation, valvular heart disease, excess alcohol use, infection, and cardiomyopathy of an unknown cause. These cause heart failure by changing either the structure or the functioning of the heart. The two types of heart failure - \"heart failure with reduced ejection fraction\" (HFrEF), and \"heart failure with preserved ejection fraction\" (HFpEF) - are based on whether the ability of the left ventricle to contract is affected, or the heart's ability to relax. The severity of disease is graded by the severity of symptoms with exercise. Heart failure is not the same as myocardial infarction (in which part of the heart muscle dies) or cardiac arrest (in which blood flow stops altogether). Other diseases that may have symptoms similar to heart failure include obesity, kidney failure, liver problems, anemia, and thyroid disease. Heart failure is diagnosed based on the history of the symptoms and a physical examination, with confirmation by echocardiography. Blood tests, electrocardiography, and chest radiography may be useful to determine the underlying cause.\n\nTreatment depends on the severity and cause of the disease. In people with chronic stable mild heart failure, treatment commonly consists of lifestyle modifications such as stopping smoking, physical exercise, and dietary changes, as well as medications. In those with heart failure due to left ventricular dysfunction, angiotensin converting enzyme inhibitors, angiotensin receptor blockers, or valsartan/sacubitril along with beta blockers are recommended. For those with severe disease, aldosterone antagonists, or hydralazine with a nitrate may be used. Diuretics are useful for preventing fluid retention and the resulting shortness of breath. Sometimes, depending on the cause, an implanted device such as a pacemaker or an implantable cardiac defibrillator (ICD) may be recommended. In some moderate or severe cases, cardiac resynchronization therapy (CRT) or cardiac contractility modulation may be of benefit. A ventricular assist device or occasionally a heart transplant may be recommended in those with severe disease that persists despite all other measures.\nHeart failure is a common, costly, and potentially fatal condition. In 2015 it affected about 40 million people globally. Overall around 2% of adults have heart failure and in those over the age of 65, this increases to 6–10%. Rates are predicted to increase. The risk of death is about 35% the first year after diagnosis; while by the second year the risk of death is less than 10% for those who remain alive. This degree of risk of death is similar to some cancers. In the United Kingdom, the disease is the reason for 5% of emergency hospital admissions. Heart failure has been known since ancient times with the Ebers papyrus commenting on it around 1550 BCE.\n\nHeart failure is a pathophysiological state in which cardiac output is insufficient to meet the needs of the body and lungs. The term \"congestive heart failure\" is often used, as one of the common symptoms is congestion, or build-up of fluid in a person's tissues and veins in the lungs or other parts of the body. Specifically, congestion takes the form of water retention and swelling (edema), both as peripheral edema (causing swollen limbs and feet) and as pulmonary edema (causing breathing difficulty), as well as ascites (swollen abdomen). This is a common problem in old age as a result of cardiovascular disease, but it can happen at any age, even in fetuses.\n\nThe term \"acute\" is used to mean rapid onset, and \"chronic\" refers to long duration. Chronic heart failure is a long-term condition, usually kept stable by the treatment of symptoms. Acute decompensated heart failure is a worsening of chronic heart failure symptoms which can result in acute respiratory distress. High-output heart failure can occur when there is an increased cardiac output. The circulatory overload caused, can result in an increased left ventricular diastolic pressure which can develop into pulmonary congestion (pulmonary edema).\n\nHeart failure is divided into two types based on ejection fraction, which is the proportion of blood pumped out of the heart during a single contraction. Ejection fraction is given as a percentage with the normal range being between 50 and 75%. The two types are:\n\n1) Heart failure due to reduced ejection fraction (HFrEF). Synonyms no longer recommended are \"heart failure due to left ventricular systolic dysfunction\" and \"systolic heart failure\". HFrEFe is associated with an ejection fraction of less than 40%.\n\n2) Heart failure with preserved ejection fraction (HFpEF). Synonyms no longer recommended include \"diastolic heart failure\" and \"heart failure with normal ejection fraction\". HFpEF occurs when the left ventricle contracts normally during systole, but the ventricle is stiff and does not relax normally during diastole, which impairs filling.\n\nHeart failure symptoms are traditionally and somewhat arbitrarily divided into \"left\" and \"right\" sided, recognizing that the left and right ventricles of the heart supply different portions of the circulation. However, heart failure is not exclusively \"backward failure\" (in the part of the circulation which drains to the ventricle).\n\nThere are several other exceptions to a simple left-right division of heart failure symptoms. Additionally, the most common cause of right-sided heart failure is left-sided heart failure. The result is that patients commonly present with both sets of signs and symptoms.\n\nThe left side of the heart is responsible for receiving oxygen-rich blood from the lungs and pumping it forward to the systemic circulation (the rest of the body except for the pulmonary circulation). Failure of the left side of the heart causes blood to back up (be congested) into the lungs, causing respiratory symptoms as well as fatigue due to insufficient supply of oxygenated blood. Common respiratory signs are increased rate of breathing and increased \"work\" of breathing (non-specific signs of respiratory distress). Rales or crackles, heard initially in the lung bases, and when severe, throughout the lung fields suggest the development of pulmonary edema (fluid in the alveoli). Cyanosis which suggests severe low blood oxygen, is a late sign of extremely severe pulmonary edema.\n\nAdditional signs indicating left ventricular failure include a laterally displaced apex beat (which occurs if the heart is enlarged) and a gallop rhythm (additional heart sounds) may be heard as a marker of increased blood flow or increased intra-cardiac pressure. Heart murmurs may indicate the presence of valvular heart disease, either as a cause (e.g. aortic stenosis) or as a result (e.g. mitral regurgitation) of the heart failure.\n\n\"Backward\" failure of the left ventricle causes congestion of the lungs' blood vessels, and so the symptoms are predominantly respiratory in nature. Backward failure can be subdivided into the failure of the left atrium, the left ventricle or both within the left circuit. The patient will have dyspnea (shortness of breath) on exertion and in severe cases, dyspnea at rest. Increasing breathlessness on lying flat, called orthopnea, occurs. It is often measured in the number of pillows required to lie comfortably, and in orthopnea, the patient may resort to sleeping while sitting up. Another symptom of heart failure is paroxysmal nocturnal dyspnea: a sudden nighttime attack of severe breathlessness, usually several hours after going to sleep. Easy fatigability and exercise intolerance are also common complaints related to respiratory compromise.\n\n\"Cardiac asthma\" or wheezing may occur.\n\nCompromise of left ventricular \"forward\" function may result in symptoms of poor systemic circulation such as dizziness, confusion and cool extremities at rest.\n\nRight-sided heart failure is often caused by pulmonary heart disease (cor pulmonale), which is typically caused by difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis.\n\nPhysical examination may reveal pitting peripheral edema, ascites, and liver enlargement. Jugular venous pressure is frequently assessed as a marker of fluid status, which can be accentuated by eliciting hepatojugular reflux. If the right ventricular pressure is increased, a parasternal heave may be present, signifying the compensatory increase in contraction strength.\n\n\"Backward\" failure of the right ventricle leads to congestion of systemic capillaries. This generates excess fluid accumulation in the body. This causes swelling under the skin (termed peripheral edema or anasarca) and usually affects the dependent parts of the body first (causing foot and ankle swelling in people who are standing up, and sacral edema in people who are predominantly lying down). Nocturia (frequent nighttime urination) may occur when fluid from the legs is returned to the bloodstream while lying down at night. In progressively severe cases, ascites (fluid accumulation in the abdominal cavity causing swelling) and liver enlargement may develop. Significant liver congestion may result in impaired liver function (congestive hepatopathy), and jaundice and even coagulopathy (problems of decreased or increased blood clotting) may occur.\n\nDullness of the lung fields to finger percussion and reduced breath sounds at the bases of the lung may suggest the development of a pleural effusion (fluid collection between the lung and the chest wall). Though it can occur in isolated left- or right-sided heart failure, it is more common in biventricular failure because pleural veins drain into both the systemic and pulmonary venous systems. When unilateral, effusions are often right sided.\n\nIf a person with a failure of one ventricle lives long enough, it will tend to progress to failure of both ventricles. For example, left ventricular failure allows pulmonary edema and pulmonary hypertension to occur, which increase stress on the right ventricle. Right ventricular failure is not as deleterious to the other side, but neither is it harmless.\n\nHeart failure may also occur in situations of \"high output\" (termed \"high-output heart failure\"), where the amount of blood pumped is more than typical and the heart is unable to keep up. This can occur in overload situations (blood or serum infusions), kidney diseases, chronic severe anemia, beriberi (vitamin B/thiamine deficiency), hyperthyroidism, cirrhosis, Paget's disease, multiple myeloma, arteriovenous fistulae, or arteriovenous malformations.\n\nViral infections of the heart can lead to inflammation of the muscular layer of the heart and subsequently contribute to the development of heart failure. Heart damage can predispose a person to develop heart failure later in life and has many causes including systemic viral infections (e.g., HIV), chemotherapeutic agents such as daunorubicin, cyclophosphamide, and trastuzumab, and abuse of drugs such as alcohol, cocaine, and methamphetamine. An uncommon cause is exposure to certain toxins such as lead and cobalt. Additionally, infiltrative disorders such as amyloidosis and connective tissue diseases such as systemic lupus erythematosus have similar consequences. Obstructive sleep apnea (a condition of sleep wherein disordered breathing overlaps with obesity, hypertension, and/or diabetes) is regarded as an independent cause of heart failure.\n\nChronic stable heart failure may easily decompensate. This most commonly results from an intercurrent illness (such as myocardial infarction (a heart attack), pneumonia), abnormal heart rhythms, uncontrolled hypertension, or a patient's failure to maintain a fluid restriction, diet, or medication. Other factors that may worsen CHF include: anemia, hyperthyroidism, excessive fluid or salt intake, and medication such as NSAIDs and thiazolidinediones. NSAIDs increase the risk twofold.\n\nA number of medications may cause or worsen the disease. This includes NSAIDS, a number of anesthetic agents such as ketamine, thiazolidinediones, a number of cancer medications, salbutamol, and tamsulosin.\n\nHeart failure is caused by any condition which reduces the efficiency of the heart muscle, through damage or overloading. As such, it can be caused by a wide number of conditions, including myocardial infarction (in which the heart muscle is starved of oxygen and dies), hypertension (which increases the force of contraction needed to pump blood) and amyloidosis (in which misfolded proteins are deposited in the heart muscle, causing it to stiffen). Over time these increases in workload will produce changes to the heart itself:\n\nThe heart of a person with heart failure may have a reduced force of contraction due to overloading of the ventricle. In a healthy heart, increased filling of the ventricle results in increased contraction force (by the Frank–Starling law of the heart) and thus a rise in cardiac output. In heart failure, this mechanism fails, as the ventricle is loaded with blood to the point where heart muscle contraction becomes less efficient. This is due to reduced ability to cross-link actin and myosin filaments in over-stretched heart muscle.\n\nNo system of diagnostic criteria has been agreed on as the gold standard for heart failure. The National Institute for Health and Care Excellence recommends measuring brain natriuretic peptide (BNP) followed by ultrasound of the heart if positive. This is recommended in those with shortness of breath. In those with heart failure who worsen both a BNP and a troponin are recommended to help determine likely outcomes.\n\nEchocardiography is commonly used to support a clinical diagnosis of heart failure. This modality uses ultrasound to determine the stroke volume (SV, the amount of blood in the heart that exits the ventricles with each beat), the end-diastolic volume (EDV, the total amount of blood at the end of diastole), and the SV in proportion to the EDV, a value known as the \"ejection fraction\" (EF). In pediatrics, the shortening fraction is the preferred measure of systolic function. Normally, the EF should be between 50% and 70%; in systolic heart failure, it drops below 40%. Echocardiography can also identify valvular heart disease and assess the state of the pericardium (the connective tissue sac surrounding the heart). Echocardiography may also aid in deciding what treatments will help the patient, such as medication, insertion of an implantable cardioverter-defibrillator or cardiac resynchronization therapy. Echocardiography can also help determine if acute myocardial ischemia is the precipitating cause, and may manifest as regional wall motion abnormalities on echo.\n\nChest X-rays are frequently used to aid in the diagnosis of CHF. In a person who is compensated, this may show cardiomegaly (visible enlargement of the heart), quantified as the cardiothoracic ratio (proportion of the heart size to the chest). In left ventricular failure, there may be evidence of vascular redistribution (\"upper lobe blood diversion\" or \"cephalization\"), Kerley lines, cuffing of the areas around the bronchi, and interstitial edema. Ultrasound of the lung may also be able to detect Kerley lines.\n\nAn electrocardiogram (ECG/EKG) may be used to identify arrhythmias, ischemic heart disease, right and left ventricular hypertrophy, and presence of conduction delay or abnormalities (e.g. left bundle branch block). Although these findings are not specific to the diagnosis of heart failure a normal ECG virtually excludes left ventricular systolic dysfunction.\n\nBlood tests routinely performed include electrolytes (sodium, potassium), measures of kidney function, liver function tests, thyroid function tests, a complete blood count, and often C-reactive protein if infection is suspected. An elevated B-type natriuretic peptide (BNP) is a specific test indicative of heart failure. Additionally, BNP can be used to differentiate between causes of dyspnea due to heart failure from other causes of dyspnea. If myocardial infarction is suspected, various cardiac markers may be used.\n\nBNP is a better indicator than N-terminal pro-BNP (NTproBNP) for the diagnosis of symptomatic heart failure and left ventricular systolic dysfunction. In symptomatic patients, BNP had a sensitivity of 85% and specificity of 84% in detecting heart failure; performance declined with increasing patient age.\n\nHyponatremia (low serum sodium concentration) is common in heart failure. Vasopressin levels are usually increased, along with renin, angiotensin II, and catecholamines in order to compensate for reduced circulating volume due to inadequate cardiac output. This leads to increased fluid and sodium retention in the body; the rate of fluid retention is higher than the rate of sodium retention in the body, this phenomenon causes \"hypervolemic hyponatremia\" (low sodium concentration due to high body fluid retention). This phenomenon is more common in older women with low body mass. Severe hyponatremia can result in accumulation of fluid in the brain, causing cerebral oedema and intracranial haemorrhage.\n\nAngiography is the X-ray imaging of blood vessels which is done by injecting contrast agents into the bloodstream through a thin plastic tube (catheter) which is placed directly in the blood vessel. X-ray images are called angiograms. Heart failure may be the result of coronary artery disease, and its prognosis depends in part on the ability of the coronary arteries to supply blood to the myocardium (heart muscle). As a result, coronary catheterization may be used to identify possibilities for revascularisation through percutaneous coronary intervention or bypass surgery.\n\nVarious measures are often used to assess the progress of patients being treated for heart failure. These include fluid balance (calculation of fluid intake and excretion), monitoring body weight (which in the shorter term reflects fluid shifts). Remote monitoring can be effective to reduce complications for people with heart failure.\n\nThere are many different ways to categorize heart failure, including:\n\n\"Functional\" classification generally relies on the New York Heart Association functional classification. The classes (I-IV) are:\nThis score documents the severity of symptoms and can be used to assess response to treatment. While its use is widespread, the NYHA score is not very reproducible and does not reliably predict the walking distance or exercise tolerance on formal testing.\n\nIn its 2001 guidelines the American College of Cardiology/American Heart Association working group introduced four stages of heart failure:\n\n\nThe ACC staging system is useful in that Stage A encompasses \"pre-heart failure\" – a stage where intervention with treatment can presumably prevent progression to overt symptoms. ACC Stage A does not have a corresponding NYHA class. ACC Stage B would correspond to NYHA Class I. ACC Stage C corresponds to NYHA Class II and III, while ACC Stage D overlaps with NYHA Class IV.\n\nThere are various algorithms for the diagnosis of heart failure. For example, the algorithm used by the Framingham Heart Study adds together criteria mainly from physical examination. In contrast, the more extensive algorithm by the European Society of Cardiology (ESC) weights the difference between supporting and opposing parameters from the medical history, physical examination, further medical tests as well as response to therapy.\n\nBy the Framingham criteria, diagnosis of congestive heart failure (heart failure with impaired pumping capability) requires the simultaneous presence of at least 2 of the following major criteria or 1 major criterion in conjunction with 2 of the following minor criteria. Major criteria include an enlarged heart on a chest x-ray, an S3 gallop (a third heart sound), acute pulmonary edema, episodes of waking up from sleep gasping for air, crackles on lung auscultation, central venous pressure of more than 16 cm at the right atrium, jugular vein distension, positive abdominojugular test, and weight loss of more than 4.5 kg in 5 days in response to treatment (sometimes classified as a minor criterion). Minor criteria include an abnormally fast heart rate of more than 120 beats per minute, nocturnal cough, difficulty breathing with physical activity, pleural effusion, a decrease in the vital capacity by one third from maximum recorded, liver enlargement, and bilateral ankle swelling.\n\nMinor criteria are acceptable only if they can not be attributed to another medical condition such as pulmonary hypertension, chronic lung disease, cirrhosis, ascites, or the nephrotic syndrome. The Framingham Heart Study criteria are 100% sensitive and 78% specific for identifying persons with definite congestive heart failure.\n\nThe ESC algorithm weights the following parameters in establishing the diagnosis of heart failure:\n\nThere are several terms which are closely related to heart failure and may be the cause of heart failure, but should not be confused with it. Cardiac arrest and asystole refer to situations in which there is \"no\" cardiac output at all. Without urgent treatment, these result in sudden death. Myocardial infarction (\"Heart attack\") refers to heart muscle damage due to insufficient blood supply, usually as a result of a blocked coronary artery. Cardiomyopathy refers specifically to problems within the heart muscle, and these problems can result in heart failure. Ischemic cardiomyopathy implies that the cause of muscle damage is coronary artery disease. Dilated cardiomyopathy implies that the muscle damage has resulted in enlargement of the heart. Hypertrophic cardiomyopathy involves enlargement and \"thickening\" of the heart muscle.\n\nA person's risk of developing heart failure is inversely related to their level of physical activity. Those who achieved at least 500 MET-minutes/week (the recommended minimum by U.S. guidelines) had lower heart failure risk than individuals who did not report exercising during their free time; the reduction in heart failure risk was even greater in those who engaged in higher levels of physical activity than the recommended minimum.\nHeart failure can also be prevented by lowering high blood pressure, high blood cholesterol, and controlling diabetes. Also, remaining at the right weight and reducing obesity can help. Lowering salt, alcohol, quitting smoking, and lowering sugar intake may help.\n\nTreatment focuses on improving the symptoms and preventing the progression of the disease. Reversible causes of the heart failure also need to be addressed (e.g. infection, alcohol ingestion, anemia, thyrotoxicosis, arrhythmia, hypertension). Treatments include lifestyle and pharmacological modalities, and occasionally various forms of device therapy and rarely cardiac transplantation.\n\nIn acute decompensated heart failure (ADHF), the immediate goal is to re-establish adequate perfusion and oxygen delivery to end organs. This entails ensuring that airway, breathing, and circulation are adequate. Immediate treatments usually involve some combination of vasodilators such as nitroglycerin, diuretics such as furosemide, and possibly noninvasive positive pressure ventilation (NIPPV). Supplemental oxygen is indicated in those with oxygen saturation levels below 90% but is not recommended in those with normal oxygen levels on room air.\n\nThe goals of treatment for people with chronic heart failure are the prolongation of life, the prevention of acute decompensation and the reduction of symptoms, allowing for greater activity.\n\nHeart failure can result from a variety of conditions. In considering therapeutic options, it is important to first exclude reversible causes, including thyroid disease, anemia, chronic tachycardia, alcohol abuse, hypertension and dysfunction of one or more heart valves. Treatment of the underlying cause is usually the first approach to treating heart failure. However, in the majority of cases, either no primary cause is found or treatment of the primary cause does not restore normal heart function. In these cases, behavioral, medical and device treatment strategies exist which can provide a significant improvement in outcomes, including the relief of symptoms, exercise tolerance, and a decrease in the likelihood of hospitalization or death. Breathlessness rehabilitation for chronic obstructive pulmonary disease (COPD) and heart failure has been proposed with exercise training as a core component. Rehabilitation should also include other interventions to address shortness of breath including psychological and education needs of patients and needs of carers.\n\nBehavioral modification is a primary consideration in chronic heart failure management program, with dietary guidelines regarding fluid and salt intake. Fluid restriction is important to reduce fluid retention in the body and to correct the hyponatremic status of the body. The evidence of benefit of reducing salt however is poor as of 2018.\n\nExercise should be encouraged and tailored to suit individual capabilities. The inclusion of regular physical conditioning as part of a cardiac rehabilitation program can significantly improve quality of life and reduce the risk of hospital admission for worsening symptoms; however, there is no evidence for a reduction in mortality rates as a result of exercise. Furthermore, it is not clear whether this evidence can be extended to people with heart failure with preserved ejection fraction (HFpEF) or to those whose exercise regimen takes place entirely at home.\n\nHome visits and regular monitoring at heart failure clinics reduce the need for hospitalization and improve life expectancy.\n\nFirst-line therapy for people with heart failure due to reduced systolic function should include angiotensin-converting enzyme (ACE) inhibitors (ACE-I) or angiotensin receptor blockers (ARBs) if the person develops a long term cough as a side effect of the ACE-I. Use of medicines from this class is associated with improved survival and quality of life in people with heart failure.\n\nBeta-adrenergic blocking agents (beta blockers) also form part of the first line of treatment, adding to the improvement in symptoms and mortality provided by ACE-I/ARB. The mortality benefits of beta blockers in people with systolic dysfunction who also have atrial fibrillation (AF) is more limited than in those who do not have AF. If the ejection fraction is not diminished (HFpEF), the benefits of beta blockers are more modest; a decrease in mortality has been observed but reduction in hospital admission for uncontrolled symptoms has not been observed.\n\nIn people who are intolerant of ACE-I and ARBs or who have significant kidney dysfunction, the use of combined hydralazine and a long-acting nitrate, such as isosorbide dinitrate, is an effective alternate strategy. This regimen has been shown to reduce mortality in people with moderate heart failure. It is especially beneficial in African-Americans (AA). In AAs who are symptomatic, hydralazine and isosorbide dinitrate (H+I) can be added to ACE-I or ARBs.\n\nIn people with markedly reduced ejection fraction, the use of an aldosterone antagonist, in addition to beta blockers and ACE-I, can improve symptoms and reduce mortality.\n\nSecond-line medications for CHF do not confer a mortality benefit. Digoxin is one such medication. Its narrow therapeutic window, a high degree of toxicity, and the failure of multiple trials to show a mortality benefit have reduced its role in clinical practice. It is now used in only a small number of people with refractory symptoms, who are in atrial fibrillation and/or who have chronic low blood pressure.\n\nDiuretics have been a mainstay of treatment for treatment of fluid accumulation, and include diuretics classes such as loop diuretics, thiazide-like diuretic, and potassium-sparing diuretic. Although widely used, evidence on their efficacy and safety is limited, with the exception of mineralocorticoid antagonists such as spironolactone. Mineralocorticoid antagonists in those under 75 years old appear to decrease the risk of death. A recent Cochrane review found that in small studies, the use of diuretics appeared to have improved mortality in individuals with heart failure. However, the extent to which these results can be extrapolated to a general population is unclear due to the small number of participants in the cited studies.\n\nAnemia is an independent factor in mortality in people with chronic heart failure. The treatment of anemia significantly improves quality of life for those with heart failure, often with a reduction in severity of the NYHA classification, and also improves mortality rates. The latest European guidelines (2012) recommend screening for iron-deficient anemia and treating with parenteral iron if anemia is found.\n\nThe decision to anticoagulate people with HF, typically with left ventricular ejection fractions <35% is debated, but generally, people with coexisting atrial fibrillation, a prior embolic event, or conditions which increase the risk of an embolic event such as amyloidosis, left ventricular noncompaction, familial dilated cardiomyopathy, or a thromboembolic event in a first-degree relative.\n\nVasopressin receptor antagonist can also be used to treat heart failure. Conivaptan is the first drug approved by US Food and Drug Administration for the treatment of euvolemic hyponatremia in those with heart failure. In rare cases hypertonic 3% saline together with diuretics may be used to correct hyponatremia.\n\nSacubitril/valsartan is a combination medication for the treatment of heart failure with reduced left ventricular ejection fraction. It has been in use as an alternative to ACE inhibitors and beta blockers.\n\nIn people with severe cardiomyopathy (left ventricular ejection fraction below 35%), or in those with recurrent VT or malignant arrhythmias, treatment with an automatic implantable cardioverter defibrillator (AICD) is indicated to reduce the risk of severe life-threatening arrhythmias. The AICD does not improve symptoms or reduce the incidence of malignant arrhythmias but does reduce mortality from those arrhythmias, often in conjunction with antiarrhythmic medications. In people with left ventricular ejection (LVEF) below 35%, the incidence of ventricular tachycardia (VT) or sudden cardiac death is high enough to warrant AICD placement. Its use is therefore recommended in AHA/ACC guidelines.\n\nCardiac contractility modulation (CCM) is a treatment for people with moderate to severe left ventricular systolic heart failure (NYHA class II–IV) which enhances both the strength of ventricular contraction and the heart's pumping capacity. The CCM mechanism is based on stimulation of the cardiac muscle by non-excitatory electrical signals (NES), which are delivered by a pacemaker-like device. CCM is particularly suitable for the treatment of heart failure with normal QRS complex duration (120 ms or less) and has been demonstrated to improve the symptoms, quality of life and exercise tolerance. CCM is approved for use in Europe, but not currently in North America.\n\nAbout one third of people with LVEF below 35% have markedly altered conduction to the ventricles, resulting in dyssynchronous depolarization of the right and left ventricles. This is especially problematic in people with left bundle branch block (blockage of one of the two primary conducting fiber bundles that originate at the base of the heart and carries depolarizing impulses to the left ventricle). Using a special pacing algorithm, biventricular cardiac resynchronization therapy (CRT) can initiate a normal sequence of ventricular depolarization. In people with LVEF below 35% and prolonged QRS duration on ECG (LBBB or QRS of 150 ms or more) there is an improvement in symptoms and mortality when CRT is added to standard medical therapy. However, in the two-thirds of people without prolonged QRS duration, CRT may actually be harmful.\n\nPeople with the most severe heart failure may be candidates for ventricular assist devices (VAD). VADs have commonly been used as a bridge to heart transplantation, but have been used more recently as a destination treatment for advanced heart failure.\n\nIn select cases, heart transplantation can be considered. While this may resolve the problems associated with heart failure, the person must generally remain on an immunosuppressive regimen to prevent rejection, which has its own significant downsides. A major limitation of this treatment option is the scarcity of hearts available for transplantation.\n\nPeople with heart failure often have significant symptoms, such as shortness of breath and chest pain. Palliative care should be initiated early in the HF trajectory, and should not be an option of last resort. Palliative care can not only provide symptom management, but also assist with advanced care planning, goals of care in the case of a significant decline, and making sure the patient has a medical power of attorney and discussed his or her wishes with this individual. A 2016 and 2017 review found that palliative care is associated with improved outcomes, such as quality of life, symptom burden, and satisfaction with care.\n\nWithout transplantation, heart failure may not be reversible and cardiac function typically deteriorates with time. The growing number of patients with Stage IV heart failure (intractable symptoms of fatigue, shortness of breath or chest pain at rest despite optimal medical therapy) should be considered for palliative care or hospice, according to American College of Cardiology/American Heart Association guidelines.\n\nPrognosis in heart failure can be assessed in multiple ways including clinical prediction rules and cardiopulmonary exercise testing. Clinical prediction rules use a composite of clinical factors such as lab tests and blood pressure to estimate prognosis. Among several clinical prediction rules for prognosticating acute heart failure, the 'EFFECT rule' slightly outperformed other rules in stratifying patients and identifying those at low risk of death during hospitalization or within 30 days. Easy methods for identifying low-risk patients are:\n\n\nA very important method for assessing prognosis in advanced heart failure patients is cardiopulmonary exercise testing (CPX testing). CPX testing is usually required prior to heart transplantation as an indicator of prognosis. Cardiopulmonary exercise testing involves measurement of exhaled oxygen and carbon dioxide during exercise. The peak oxygen consumption (VO2 max) is used as an indicator of prognosis. As a general rule, a VO2 max less than 12–14 cc/kg/min indicates a poor survival and suggests that the patient may be a candidate for a heart transplant. Patients with a VO2 max<10 cc/kg/min have a clearly poorer prognosis. The most recent International Society for Heart and Lung Transplantation (ISHLT) guidelines also suggest two other parameters that can be used for evaluation of prognosis in advanced heart failure, the heart failure survival score and the use of a criterion of VE/VCO2 slope > 35 from the CPX test. The heart failure survival score is a score calculated using a combination of clinical predictors and the VO2 max from the cardiopulmonary exercise test.\n\nHeart failure is associated with significantly reduced physical and mental health, resulting in a markedly decreased quality of life. With the exception of heart failure caused by reversible conditions, the condition usually worsens with time. Although some people survive many years, progressive disease is associated with an overall annual mortality rate of 10%.\n\nApproximately 18 of every 1000 persons will experience an ischemic stroke during the first year after diagnosis of HF. As the duration of follow-up increases, the stroke rate rises to nearly 50 strokes per 1000 cases of HF by 5 years.\n\nIn 2015 heart failure affected about 40 million people globally. Overall around 2% of adults have heart failure and in those over the age of 65, this increases to 6–10%. Above 75 years old rates are greater than 10%.\n\nRates are predicted to increase. Increasing rates are mostly because of increasing life span, but also because of increased risk factors (hypertension, diabetes, dyslipidemia, and obesity) and improved survival rates from other types of cardiovascular disease (myocardial infarction, valvular disease, and arrhythmias). Heart failure is the leading cause of hospitalization in people older than 65.\n\nIn the United States, heart failure affects 5.8 million people, and each year 550,000 new cases are diagnosed. In 2011, heart failure was the most common reason for hospitalization for adults aged 85 years and older, and the second most common for adults aged 65–84 years. It is estimated that one in five adults at age 40 will develop heart failure during their remaining lifetime and about half of people who develop heart failure die within 5 years of diagnosis. Heart failure is much higher in African Americans, Hispanics, Native Americans and recent immigrants from the eastern bloc countries like Russia. This high prevalence in these ethnic minority populations has been linked to high incidence of diabetes and hypertension. In many new immigrants to the U.S., the high prevalence of heart failure has largely been attributed to lack of preventive health care or substandard treatment. Nearly one out of every four patients (24.7%) hospitalized in the U.S. with congestive heart failure are readmitted within 30 days. Additionally, more than 50% of people seek re-admission within 6 months after treatment and the average duration of hospital stay is 6 days.\n\nHeart failure is a leading cause of hospital readmissions in the U.S. People aged 65 and older were readmitted at a rate of 24.5 per 100 admissions in 2011. In the same year, Medicaid patients were readmitted at a rate of 30.4 per 100 admissions, and uninsured patients were readmitted at a rate of 16.8 per 100 admissions. These are the highest readmission rates for both patient categories. Notably, heart failure was not among the top ten conditions with the most 30-day readmissions among the privately insured.\n\nIn the UK has despite moderate improvements in prevention, heart failure rates have increased due to population growth and ageing. Overall heart failure rates are similar to the four most common causes of cancer (breast, lung, prostate and colon) combined. People from deprived backgrounds are more likely to be diagnosed with heart failure and at a younger age.\n\nIn tropical countries, the most common cause of HF is valvular heart disease or some type of cardiomyopathy. As underdeveloped countries have become more affluent, there has also been an increase in the incidence of diabetes, hypertension and obesity, which have in turn raised the incidence of heart failure.\n\nMen have a higher incidence of heart failure, but the overall prevalence rate is similar in both sexes since women survive longer after the onset of heart failure. Women tend to be older when diagnosed with heart failure (after menopause), they are more likely than men to have diastolic dysfunction, and seem to experience a lower overall quality of life than men after diagnosis.\n\nSome sources state that people of Asian descent are at a higher risk of heart failure than other ethnic groups. Other sources however have found that rates of heart failure are similar to rates found in other ethnic groups.\n\nIn 2011, non-hypertensive heart failure was one of the ten most expensive conditions seen during inpatient hospitalizations in the U.S., with aggregate inpatient hospital costs of more than $10.5 billion.\n\nHeart failure is associated with a high health expenditure, mostly because of the cost of hospitalizations; costs have been estimated to amount to 2% of the total budget of the National Health Service in the United Kingdom, and more than $35 billion in the United States.\n\nThere is low-quality evidence that stem cell therapy may help. Although this evidence positively indicated benefit, the evidence was of lower quality than other evidence that does not indicate benefit. A 2016 Cochrane review found tentative evidence of longer life expectancy and improved left ventricular ejection fraction in persons treated with bone marrow-derived stem cells.\n\n"}
{"id": "36591857", "url": "https://en.wikipedia.org/wiki?curid=36591857", "title": "Herpes simplex keratitis", "text": "Herpes simplex keratitis\n\nHerpetic simplex keratitis, also known as herpetic keratoconjunctivitis and herpesviral keratitis, is a form of keratitis caused by recurrent herpes simplex virus (HSV) infection in the cornea.\n\nIt begins with infection of epithelial cells on the surface of the eye and retrograde infection of nerves serving the cornea. Primary infection typically presents as swelling of the conjunctiva and eyelids (blepharoconjunctivitis), accompanied by small white itchy lesions on the corneal surface. The effect of the lesions varies, from minor damage to the epithelium (superficial punctate keratitis), to more serious consequences such as the formation of dendritic ulcers. Infection is unilateral, affecting one eye at a time. Additional symptoms include dull pain deep inside the eye, mild to acute dryness, and sinusitis. Most primary infections resolve spontaneously in a few weeks. Healing can be aided by the use of oral and topical antivirals.\n\nSubsequent recurrences may be more severe, with infected epithelial cells showing larger dendritic ulceration, and lesions forming white plaques. The epithelial layer is sloughed off as the dendritic ulcer grows, and mild inflammation (iritis) may occur in the underlying stroma of iris. Sensation loss occurs in lesional areas, producing generalised corneal anaesthesia with repeated recurrences. Recurrence can be accompanied by chronic dry eye, low grade intermittent conjunctivitis, or chronic unexplained sinusitis. Following persistent infection the concentration of viral DNA reaches a critical limit. Antibody responses against the viral antigen expression in the stroma can trigger a massive immune response in the eye. The response may result in the destruction of the corneal stroma, resulting in loss of vision due to opacification of the cornea. This is known as immune-mediated stromal keratitis.\n\nHSV infection is very common in humans. It has been estimated that one third of the world population have recurrent infection. Keratitis caused by HSV is the most common cause of cornea-derived blindness in developed nations. Therefore, HSV infections are a large and worldwide public health problem. The global incidence (rate of new disease) of herpes keratitis is roughly 1.5 million, including 40,000 new cases of severe monocular visual impairment or blindness each year.\n\nPrimary infection most commonly manifests as blepharoconjunctivitis i.e. infection of lids and conjunctiva that heals without scarring. Lid vesicles and conjunctivitis are seen in primary infection. Corneal involvement is rarely seen in primary infection.\n\nRecurrent herpes of the eye is caused by reactivation of the virus in a latently infected sensory ganglion, transport of the virus down the nerve axon to sensory nerve endings, and subsequent infection of ocular surface.\nThe following classification of herpes simplex keratitis is important for understanding this disease:\n\nThis classic herpetic lesion consists of a linear branching corneal ulcer (dendritic ulcer). During eye exam the defect is examined after staining with fluorescein dye. The underlying cornea has minimal inflammation.\n\nPatients with epithelial keratitis complain of foreign-body sensation, light sensitivity, redness and blurred vision.\n\nFocal or diffuse reduction in corneal sensation develops following recurrent epithelial keratitis.\n\nIn immune deficient patients or with the use of corticosteroids the ulcer may become large and in these cases it is called geographic ulcer.\n\nEndothelial keratitis manifests a central endothelitis in a disc-shaped manner. Longstanding corneal edema leads to permanent scarring and is the major cause of decreased vision associated with HSV.\n\nLocalized endothelitis (localized inflammation of corneal endothelial layer) is the cause of disciform keratitis.\n\n\nHSV is a double-stranded DNA virus that has icosahedral capsid. HSV-1 infections are found more commonly in the oral area and HSV-2 in the genital area. Ocular herpes simplex is usually caused by HSV-1.\n\nA specific clinical diagnosis of HSV as the cause of dendritic keratitis can usually be made by ophthalmologists and optometrists based on the presence of characteristic clinical features. Diagnostic testing is seldom needed because of its classic clinical features and is not useful in stromal keratitis as there is usually no live virus. Laboratory tests are indicated in complicated cases when the clinical diagnosis is uncertain and in all cases of suspected neonatal herpes infection:\n\n\nTreatment of herpes of the eye is different based on its presentation: epithelial keratitis is caused by live virus while stromal disease is an immune response and metaherpetic ulcer results from inability of the corneal epithelium to heal:\n\nEpithelial keratitis is treated with topical antivirals, which are very effective with low incidence of resistance. Treatment of the disease with topical antivirals generally should be continued for 10–14 days. Aciclovir ophthalmic ointment and Trifluridine eye drops have similar effectiveness but are more effective than Idoxuridine and Vidarabine eye drops. Oral acyclovir is as effective as topical antivirals for treating epithelial keratitis, and it has the advantage of no eye surface toxicity. For this reason, oral therapy is preferred by some ophthalmologists.\n\nGanciclovir and brivudine treatments were found to be equally as effective as acyclovir in a systematic review.\n\nValacyclovir, a pro-drug of acyclovir likely to be just as effective for ocular disease, can cause thrombotic thrombocytopenic purpura/Hemolytic-uremic syndrome in severely immunocompromised patients such as those with AIDS; thus, it must be used with caution if the immune status is unknown.\n\nTopical corticosteroids are contraindicated in the presence of active herpetic epithelial keratitis; patients with this disease who are using systemic corticosteroids for other indications should be treated aggressively with systemic antiviral therapy.\n\nThe effect of interferon with an antiviral agent or an antiviral agent with debridement needs further assessment.\n\nHerpetic stromal keratitis is treated initially with prednisolone drops every 2 hours\naccompanied by a prophylactic antiviral drug: either topical antiviral or an oral agent such as acyclovir or valacyclovir. The prednisolone drops are tapered every 1–2 weeks depending on the degree of clinical improvement. Topical antiviral medications are not absorbed by the cornea through an intact epithelium, but orally administered acyclovir penetrates an intact cornea and anterior chamber. In this context, oral acyclovir might benefit the deep corneal inflammation of disciform keratitis.\n\nTreatment includes artificial tears and eye lubricants, stopping toxic medications, performing punctal occlusion, bandage contact lens and amniotic membrane transplant. These measures intend to improve corneal epithelial healing.\n"}
{"id": "33227331", "url": "https://en.wikipedia.org/wiki?curid=33227331", "title": "Ice pigging", "text": "Ice pigging\n\nIce pigging is the process in which an ice slurry is pumped into a pipe and forced along inside in order to remove sediment and other unwanted deposits to leave the pipe clean. It has many applications in the water, sewage and food industries. The University of Bristol has held many laboratory trials to investigate various key factors and refine the process. Ice pigging was invented and patented by Professor Joe Quarini of the University of Bristol.\n\nThe ice pigging method occupies a ‘middle ground’ between two of the traditional methods of cleaning pipes; that of ‘flushing’, which involves pumping high velocity water through pipes in order to carry away residue, and pigging, forcing a solid object through the pipe to push away any loose material. Ice pigging utilises the main principles behind these two methods. An ice-water slurry, typically between 50 and 90% ice fraction is introduced to and removed from pipes with little complication, provided the presence of suitable hydrants and valves. The high ice fraction lends several attributes which are particularly appealing for application in pigging; It is able to hold itself together under continuous shear. If the ice becomes stuck then given enough time it will just melt and be carried away. It is fairly inexpensive to make. It leaves the pipe walls themselves undamaged, and only removes waste residue. Because ice pigging is a relatively quick process (when compared to alternative methods), it can be use for a number of applications including: underground pipes for fresh water and sewage, food manufacturing pipe work and many others.\n\nIce pigging uses less water and requires less cleanup than traditional flushing or underground pipe pigging techniques. However, the process requires more energy since the ice slurries must be chilled and constantly agitated up until the point they are inserted into the pipes.\n\nThe University of Bristol have produced a paper entitled \"Investigation and development of an innovative pigging technique for the water supply industry.\" in which they have detailed the research that they have carried out. It looks particularly at how the properties of the ice pig behave with different ice fractions and varied levels of particulate loading as well as looking into the effects of shear strength, viscosity and heat transfer characteristics.\n\n\nwww.ice-pigging.com\n\n"}
{"id": "24753818", "url": "https://en.wikipedia.org/wiki?curid=24753818", "title": "Institut Lillois d'Ingénierie de la Santé", "text": "Institut Lillois d'Ingénierie de la Santé\n\nThe Institut Lillois d'Ingéniérie de la Santé (ILIS, the Lille Institute of Management and health sciences) is part of the Lille II University Law and Health. It is an institution allowed to deliver bachelor's and master's degrees in Health Sciences. The ILIS lays a special emphasis on students acquiring a broad range of professional skills and work experience during theirs years of study and before graduating. In essence quite close to the German model of University of Applied Sciences or Fachhochschule, the institute is keen to let its students accumulate work experience so as to allow them a chance to compete with engineer and business schools on the job market. The core curriculum focus on current healthcare issues such as elderly care, hospital quality management, nutrition,environment,clinical research and sales of drugs and medical devices. It has been renamed Faculty of Health Management and Health Engineering while keeping the acronym ILIS.\n\nILIS Founded was found in 1992 and graduated its first class in 1995. Change of Location and set up behind Medical School Henri Warembourg in 2002. A building extension was made in 2009. It was recognised as a fully fledged faculty in 2010.\n\nAt first, the ILIS was set up to provide for an unmet need from the healthcare sector. During the early 1990s, more stringent rules and regulations were enforced in the healthcare sector. Firms were also having difficulties recruiting employees having both a health and management background. Therefore, a new curriculum was set up focusing on quality assurance,management and audits. Progressively, the ILIS diversified and extender its programs to biomedical technology sales and marketing as well as safety and environmental concerns. ILIS graduates are mostly employed in positions that did not exist or were not well known before 1990 such as Health Quality Assurance Officer, Risk Manager, Application Specialist and Data Manager. The ILIS graduates have one of the highest employment rate of the Lille II University, with 80% of the students already hired at graduation.\n\nThe ILIS is located on the town of Loos in the suburb of Lille. It is near the Lille University Hospital, the medical school and the School of Pharmacy. It benefits from the Eurasanté Health & Biology competitive hub with firms like Bayer and Diagast.\n\nStudent intake occurs at every level provided the necessary pre-requisites have been met.Freshmen often enter the school after a scientific Baccalauréat or after the PACES competitive exams to enter medical school or other health faculties. At the end of the first year, students are interviewed to check their commitment to this particular program as well as their will to choose a clear career path. Sophomores are expected to have at least some months of work experience in any business. Every new student will at some time of his curriculum, usually at the beginning, pass this admission interview so as to receive clearance to continue their studies at the ILIS.\n\nThe first two years, a sound scientific background is required from students if they are to succeed. During their graduation year they will concentrate on quality management and audits. The fourth and fifth year is more marketing and management oriented with teamwork becoming even more important.The placements, the duration of which increases each year, allow students to integrate the reality of the working world progressively: 2 x 6 weeks full-time in the 2nd year, 2 x 2 months full-time in the 3rd year and 5 and 6 months full-time in the fourth and fifth year respectively .\n\nThe objectives of these placements have been clearly defined :\n\n\nBsc Public Health and Health Engineering\n<br>Bsc(Honours) Health Sciences\n\nMsc. Health Sciences but as a professional degree akin to the Master of Health Administration with some sections closely equivalent to Master of Public Health\n\nFaculty members form part of research teams.\n\nILIS is accredited by the French Ministry of Education and uses the ECTS European grading system.\n\n\n[French]\n\n<br>\n"}
{"id": "7602741", "url": "https://en.wikipedia.org/wiki?curid=7602741", "title": "Institution of Occupational Safety and Health", "text": "Institution of Occupational Safety and Health\n\nThe Institution of Occupational Safety and Health (IOSH) is a British organisation for health and safety professionals.\n\nIOSH is the chartered professional body for safety and health in the workplace. It acts as a champion, supporter, adviser, advocate and trainer for those who protect the safety, health and wellbeing of others.\n\nIOSH has over 46,000 members, from over 120 countries. This includes an extensive trainer network. Over 179,000 delegates attended IOSH training courses in 2016.\n\nIOSH was founded in 1945 when the Institution of Industrial Safety Officers (IISO) was formed as a division of the Royal Society for the Prevention of Accidents (RoSPA). The Institution gained its charitable status in 1962 and continues to operate as a not-for-profit organisation.\n\nIn 1981, the IISO was renamed as the Institution of Occupational Safety and Health (IOSH), and in 2002 was awarded a Royal Charter. From 2005, IOSH began awarding Chartered Safety and Health Practitioner status to recognise individual professionalism and commitment to continued learning and development.\n\nIn 2011 along with other Health and Safety bodies in the UK, IOSH developed the Occupational Safety & Health Consultants Register (OSHCR) to raise awareness and promote the use of certified health and safety consultants in the workplace.\n\nIOSH’s No Time to Lose campaign was launched in 2014 to highlight the causes of occupational cancer and help businesses take action. The No Time to Lose website provides a host of free resources and information on workplace cancer, and offers the opportunity to sign a pledge to make changes and support the campaign.\n\nThe Occupational Health Toolkit (OH Toolkit) is a free resource to help tackle common occupational health problems such as skin disorders, work related stress and non-work related conditions including diabetes and heart disease. The toolkit brings together information, guidance, case studies and training materials.\n\nAs part of their charitable work, IOSH produce a number of guides, such as Safe Start Up guides which are designed to help small business with health and safety. IOSH also fund and produce a number of research reports.\n\nIOSH regularly post consultations, where members can respond and have the opportunity to influence national and international policies.\n\nIOSH Blueprint is a framework designed to measure skills and competencies in occupational safety and health. The tool is currently in a beta testing stage. IOSH members, and a number of select organisations, are using the self-assessment tool to identify training and development needs.\n\nIOSH Training and Skills is a range of courses designed for different aspects of occupational health and safety. The courses are delivered by IOSH licensed trainers. Trainers must have suitable qualifications and experience before being approved to run IOSH courses.\n\"Managing Safely\" and \"Working Safely\" courses are also available in Arabic.\nIOSH tailored courses are developed by licensed training providers to meet the demands of specialist industry sectors, roles or skills. They are assessed and approved by IOSH to make sure they meet IOSH’s standards.\n\nIOSH recently launched their own level 3 certificate, NCFE IOSH Level 3 General Certificate in Safety and Health for Business, which offers a more direct route to IOSH membership. \n\n\nCategories of membership depend on a combination of academic qualifications, experience and achievement.\n\nChartered Fellows of the Institution are entitled to use the designation Chartered Safety and Health Practitioner and the designatory letters \"CFIOSH\". This is the highest grade. Chartered Fellows must have demonstrated an outstanding contribution to the discipline and profession of health and safety. All Chartered Fellows are required to maintain a Continuing Professional Development (CPD) record.\n\nChartered Members of the Institution are entitled to use the designation Chartered Safety and Health Practitioner and the designatory letters \"CMIOSH\". Chartered Member status requires approved educational qualifications and experience. All Chartered Members are required to maintain a Continuing Professional Development (CPD) record.\n\nGraduates of the Institution can use the designatory letters \"Grad IOSH\".These are academically qualified to become Chartered Members, and are undergoing professional development.\n\nTechnical Members of the Institution are entitled to use the designatory letters \"Tech IOSH\". They require approved educational qualifications at a lower level than graduates, plus professional experience. They are required to continue in professional development.\n\nAffiliate level is for those who have an interest in, or are employed in occupational safety and health, but are not yet eligible to join at other categories of membership.\n\nIOSH has recently launched a student membership category which lets OSH students join IOSH for free for the duration of their studies. \n\n\n"}
{"id": "27857931", "url": "https://en.wikipedia.org/wiki?curid=27857931", "title": "International Sports Sciences Association", "text": "International Sports Sciences Association\n\nThe International Sports Sciences Association is an organization that operates as a teaching institution and certification agency for fitness trainers, aerobic instructors, and medical professionals.\n\nThe ISSA offers a general fitness certification course for personal training and six specialized fitness certification courses, including fitness nutrition, sports nutrition, strength and conditioning, exercise therapy, senior fitness and youth fitness. The student obtains the status of \"Elite Trainer\" after completion of three coursesd \"Master Trainer\" after completion of six courses.\n\nInstruction is based on exercise assessment, nutritional planning, fitness instruction, sports medicine practice, and post-rehabilitation training. The school has enrolled over 130,000 students, both in fitness education and continuing education courses.\n\nNo official requirements or standards of physical training existed until the fitness boom of the 1980s. The International Sports Sciences Association was founded in 1988, when, \"recognizing the need for standardization and credibility, Dr. Sal Arria and Dr. Frederick Hatfield created a personal fitness training program to merge gym experience with practical and applied sciences.\" In keeping with this standard, the ISSA became a Provisional Affiliate of the National Board of Fitness Examiners in 2004.\n\nThe National Board of Fitness Examiners was founded to develop nationally based and uniform standards of practice for personal fitness trainers to be used to administer an NBFE provided written and practical examination followed by registration of those who successfully pass the test process. Under NBFE’s procedure, preparation for the NBFE examination can take several forms and includes training through a number of affiliated organizations who themselves include certifying organizations for fitness professionals, including the ISSA. In 2009, the ISSA became the first fitness organization to earn accreditation by a federally recognized accrediting agency—the Distance Education and Training Council (an accrediting agency recognized by the United States Department of Education and the Council for Higher Education Accreditation).\n\nAs a result, the ISSA was added to the list of GI Jobs' Military Friendly Schools in 2010, being approved by the Department of Defense for the Defense Activity for Non-Traditional Education Support (DANTES) to accept U.S. Armed Forces Tuition Assistance and the Military Spouse Career Advancement Account (MyCAA) Financial Assistance program.\n\n\n\nThe ISSA is accredited by the Accrediting Commission of the Distance Education and Training Council (DETC). The DETC is listed by the U.S. Department of Education as a nationally recognized agency, is recognized by the Council for Higher Education Accreditation (CHEA), and by the International Health, Racquet & Sportsclub Association (IHRSA).\n\nISSA is registered with the Better Business Bureau and has been awarded the highest company rating of A+. The ISSA is approved by the New York Chiropractic College for Postgraduate and Continuing Education. The ISSA is also a Provisional Affiliate of the National Board of Fitness Examiners (NFBE).\n\nThe ISSA is recognized by the Defense Activity for Non-traditional Education Support (DANTES) as a Nationally Accredited Distance Learning Program for Military Service Members. It is also approved for Armed Forces Tuition Assistance and Military Spouse Financial Assistance (MyCAA). \"The ISSA is also GI Bill approved by the Bureau for Private and Postsecondary Vocational Education (BPPVE) under contract with the Veterans Administration. Active military, veterans, and eligible spouses and dependents can receive reimbursement from the Veterans Administration for testing fees for any ISSA fitness certification courses.\"\n\n\n"}
{"id": "13566851", "url": "https://en.wikipedia.org/wiki?curid=13566851", "title": "John O. Butler", "text": "John O. Butler\n\nDr. John O. Butler was a dentist and periodontist in Chicago, Illinois, and founder of the John O. Butler Company, manufacturer of toothbrushes, dental floss in 1923 and sundry oral-care products under the Butler and GUM brand names. The company was purchased in 1988 by Sunstar Group, a major Japanese manufacturer of personal care products. In 2006 the John O. Butler Company adopted the name of its parent company, although the brand names Butler and GUM are still in use. John O. Butler Company provided Sunstar with both knowledge and access to the U.S. market. Sunstar provided John O. Butler with production methodology and technical assistance.\n"}
{"id": "25383846", "url": "https://en.wikipedia.org/wiki?curid=25383846", "title": "Kikuchi Keifuen Sanatorium", "text": "Kikuchi Keifuen Sanatorium\n\nKikuchi Keifuen Sanatorium or National Sanatorium Kikuchi　Keifuen is a sanatorium for leprosy patients or ex-leprosy patients at Kohshi-shi, Kumamoto-ken, Japan founded in 1909. The mean age of residents (ex-patients) is about eighty.\n\nThe Japanese Government promulgated the first leprosy prevention law on March 19, 1907 but it did not come into effect until April 1, 1909 because of financial constraints. Under this law, patients who did not have family to support them were forcibly treated in public leprosaria. Japan was divided into five areas, the fifth of which included Nagasaki-ken, Fukuoka Prefecture, Ooita Prefecture, Saga Prefecture, Kumamoto Prefecture, Miyazaki Prefecture and Kagoshima Prefecture. In this area, Kumamoto was selected as the site of the sanatorium.\n\nThe two main reasons for the leprosy prevention law were that foreigners visiting Japan after the Meiji Restoration (1868) were very much surprised to find leprosy sufferers wandering at large and claimed that something should be done about it and the Japanese Government was worried about the large number of people with the condition among those who were examined for the draft at age 20.\n\n\n\nThe number of patients in the sanatorium varied. It depended on the numbers admitted, the number of deaths among residents and the number of patients who escaped or were discharged, Recently they were encouraged to be discharged, but for a long period, the segregation policy which caused leprosy stigma influenced the number of those who left and were readmitted into society.\n\n\n\nOn July 9, 1940, 157 patients living around Honmyoji temple were forcibly hospitalized and sent to other sanatoriums. This incident was also called the Honmyoji incident. This was considered to be one of the \"no leprosy patients in our prefecture\" movements.\n\nMatsuo Fujimoto was considered to have received unfair treatments in two trials because he was a leprosy patient.\n\nChildren born from patients with leprosy were denied schooling at Kurokami primary school in 1954. There were strikes, riots and no schooling for some time. After one year, three children finally attended the school from the house of Mr. Takahashi, the President of Kumamoto College of Commerce.\nSee also Tatsudaryo Incident\n\nAlso called the Aisutaa incident, because of the name of the hotel. The hotel building was destroyed by the hotel administration.\n\n\n"}
{"id": "36186857", "url": "https://en.wikipedia.org/wiki?curid=36186857", "title": "Liechtenstein referendums, 2011", "text": "Liechtenstein referendums, 2011\n\nThree referendums were held in Liechtenstein during 2011. The first on approving the registered partnership law was held between 17 and 19 June, and was approved by 68.8% of voters. The law went into effect on 1 September. The second was held on 18 September on allowing abortion within the first twelve weeks of pregnancy. Prince Alois had threatened to veto the result if the result of the referendum should have turned out in favour, but ultimately it was rejected by voters. The third was on building a new national hospital in Vaduz was held on 30 October, and was also rejected.\n\nThe registered partnership law (Lebenspartnerschaft) was passed unanimously by the Landtag of Liechtenstein in the second reading on March 16 and published on March 21, 2011. However, the group Vox Populi, led by a cousin of archbishop Wolfgang Haas, announced its intention to force a referendum. According to the constitution, the organisation had until 21 April (30 days) to collect at least 1000 signatures. Because the necessary signatures were gathered (1208 valid signatures), a referendum was held on the evening of 17 June and the morning of 19 June 2011.\n\nThe registered partnership law was supported by the government and all parties in the Landtag, but opposed by the socially conservative advocacy group \"Vox Populi\" and the Roman Catholic Archdiocese of Vaduz.\n\nOn 28 June the Landtag passed a bill approving spending 83 million francs on the construction of a national hospital in Vaduz by 14 votes to 11. A request for an advisory referendum was rejected by 12 votes to 11, but a committee gathered 2,951 signatures between 8 July and 3 August, forcing a referendum.\n\nThe referendum achieved a good turnout of about 70 percent as of Friday evening because of a large majority of voters who had already used postal voting. Total turnout was 74.2 percent.\n\n"}
{"id": "728652", "url": "https://en.wikipedia.org/wiki?curid=728652", "title": "List of neurologists and neurosurgeons", "text": "List of neurologists and neurosurgeons\n\nThis is a list of neurologists and neurosurgeons, with their year of birth and death and nationality. This list compiles the names of neurologists and neurosurgeons with a corresponding Wikipedia biographical article, and is not necessarily a reflection of their relative importance in the field. Many neurologists and neurosurgeons are considered to be neuroscientists as well and some neurologists are also in the list of psychiatrists.\n\n\n"}
{"id": "23205672", "url": "https://en.wikipedia.org/wiki?curid=23205672", "title": "Maslach Burnout Inventory", "text": "Maslach Burnout Inventory\n\nThe Maslach Burnout Inventory (MBI) is an introspective psychological inventory consisting of 22 items pertaining to occupational burnout. The original form of the MBI was constructed by Christina Maslach and Susan E. Jackson with the goal to assess an individual's experience of burnout. The MBI measures three dimensions of burnout: emotional exhaustion, depersonalization, and personal accomplishment. The MBI takes between 10–15 minutes to complete and can be administered to individuals or groups.\n\nFollowing the publication of the MBI in 1981, new versions of the MBI were gradually developed to fit different groups and different settings. There are five versions of the MBI: Human Services Survey (MBI-HSS), Human Services Survey for Medical Personnel (MBI-HSS (MP)), Educators Survey (MBI-ES), General Survey (MBI-GS), and General Survey for Students (MBI-GS (S)). \n\nAn analysis of 84 published studies that report sample-specific reliability estimates for the three MBI scales found that the scales have strong reliability. Since the proposal of the Job Demands-Resources (JD-R) perspective on burnout, the MBI has shown strong relationships with job demands and resources. The MBI has been validated for human services populations, educator populations, and general work populations.\n\nThe MBI is often combined with the Areas of Worklife Survey (AWS) to assess levels of burnout and worklife context.\n\n\nThe 9-item Emotional Exhaustion (EE) scale measures feelings of being emotionally overextended and exhausted at one's work. Higher scores correspond to greater experienced burnout. This scale is used in the MBI-HSS, MBI-HSS (MP), and MBI-ES versions.\n\nThe MBI-GS and MBI-GS (S) use a shorter 5-item version of this scale called \"Exhaustion\".\n\nThe 5-item Depersonalization (DP) scale measures an unfeeling and impersonal response toward recipients of one's service, care, treatment, or instruction. Higher scores correspond to greater degrees of experienced burnout. This scale is used in the MBI-HSS, MBI-HSS (MP) and the MBI-ES versions.\n\nThe 8-item Personal Accomplishment (PA) scale measures feelings of competence and successful achievement in one's work with people. Lower scores correspond to greater experienced burnout. This scale is used in the MBI-HSS, MBI-HSS (MP), and MBI-ES versions.\n\nThe 5-item Cynicism scale measures an indifference or a distance attitude towards one's work. The cynicism measured by this scale is a coping mechanism for distancing oneself from exhausting job demands. Higher scores correspond to greater experienced burnout. This scale is used in the MBI-GS and MBI-GS (S) versions.\n\nThe 6-item Professional Efficacy scale measures feelings of competence and successful achievement in one's work. This sense of personal accomplishment emphasizes effectiveness and success in having a beneficial impact on people. Lower scores correspond to greater experienced burnout. This scale is used in the MBI-GS and MBI-GS (S) versions.\n\nThe MBI has five validated forms composed of 16-22 items to measure an individual's experience of burnout.\n\nThe MBI-HSS consists of 22 items and is the original and most widely used version of the MBI. It was designed for professionals in human services and is appropriate for respondents working in a diverse array of occupations, including nurses, physicians, health aides, social workers, health counselors, therapists, police, correctional officers, clergy, and other fields focused on helping people live better lives by offering guidance, preventing harm, and ameliorating physical, emotional, or cognitive problems. The MBI-HSS scales are Emotional Exhaustion, Depersonalization, and Personal Accomplishment.\n\nThe MBI-HSS (MP) is a variation of the MBI-HSS adapted for medical personnel. The most notable alteration is this form refers to \"patients\" instead of \"recipients\". The MBI-HSS (MP) scales are Emotional Exhaustion, Depersonalization, and Personal Accomplishment.\n\nThe MBI-ES consists of 22 items and is a version of the original MBI for use with educators. It was designed for teachers, administrators, other staff members, and volunteers working in any educational setting. This form was formerly known as MBI-Form Ed. The MBI-ES scales are Emotional Exhaustion, Depersonalization, and Personal Accomplishment.\n\nThe MBI-GS consists of 16 items and is designed for use with occupational groups other than human services and education, including those working in jobs such as customer service, maintenance, manufacturing, management, and most other professions. The MBI-GS scales are Exhaustion, Cynicism, and Professional Efficacy.\n\nThe MBI-GS (S) is an adaptation of the MBI-GS designed to assess burnout in college and university students. It is available for use but its psychometric properties are not yet documented. The MBI-GS (S) scales are Exhaustion, Cynicism, and Professional Efficacy.\n\nAll MBI items are scored using a 7 level frequency scale from \"never\" to \"daily.\" Initial development had 3 components: emotional exhaustion (9 items), depersonalization (5 items) and personal achievement (8 items). Each scale measures its own unique dimension of burnout. Scales should not be combined to form a single burnout scale. Scales include reverse-scored items. Maslach, Jackson and Leiter (1996) describe item scoring from 0 to 6. While a common convention is to avoid zeros for scales, one should be aware that altering the original 0-6 scores will not align with categories of each scale. There are score ranges that define Low, Moderate and High levels of each component/scale based on the 0-6 scoring. Using a 1-7 scale with the original category ranges will inflate the number of people in the upper 2 categories. Further, comparisons with existing literature may be misleading.\n\nThe 7-level frequency scale for all MBI scales is as follows:\n\nThe Maslach Burnout Inventory has been used in a variety of studies to study burnout, including with health professionals and teachers.\n"}
{"id": "26678780", "url": "https://en.wikipedia.org/wiki?curid=26678780", "title": "Miracle Treat Day (Dairy Queen)", "text": "Miracle Treat Day (Dairy Queen)\n\nMiracle Treat Day is an American and Canadian fundraising event for the Children's Miracle Network and is sponsored by Dairy Queen. The event is held annually in the United States and in Canada. On August 2,2018 At least $1 of all proceeds from each The Blizzard Treat sold at participating locations that day goes towards benefiting local community children's hospitals.\n\nThe purpose of this event is to raise funds for the 170+ Children's Miracle Network hospital's worldwide. Each year these hospitals treat about 17 million children and they impact the lives of more children than any other children's organization in the world.\n\nDairy Queen has been a supporter of Children’s Miracle Network since 1984. The first-ever Miracle Treat Day was on August 10, 2006 and has been held annually ever since. It is a tradition on the day of the event for the President and CEO of Dairy Queen, John Gainor, to personally deliver hundreds of Blizzard Treats to children and their families at local hospitals.\n\nIn 2006, Grammy-Winning Superstar LeAnn Rimes served as the honorary spokesperson for Dairy Queen's first-ever Miracle Treat Day. LeAnn has also performed many benefit concerts in support of the Children's Miracle Network.\n\nIn 2007, Miss America Lauren Nelson served as the honorary spokesperson for Dairy Queen’s Miracle Treat Day. \"With my love for children, I am proud to be spokesperson for Miracle Treat Day, as I know this event will do so much to help children all over North America,\" said Nelson.\n\nIn 2008, Miss America Kirsten Haglund served as the honorary spokesperson for Dairy Queen's Miracle Treat Day. Miss America titleholders have been involved with Children's Miracle Network, visiting hospitals and helping with fundraising activities since 1989.\n\nIn 2009, Miss America Katie Stam served as the honorary spokesperson for Dairy Queen's Miracle Treat Day. She served Blizzard Treats at a local Dairy Queen for the event and she also promoted the day by conducting several TV and radio interviews.\n\nDairy Queen marked 30 years in support of the Children's Miracle Network in 2014. In that time, Dairy Queen has helped raise more than $125 million. The biggest fundraising event that Dairy Queen holds for the cause is the Miracle Treat Day. Each year the event makes millions of dollars, and the goal is for the amount to continue to grow.\n\nA Dairy Queen franchise in Madison, South Dakota, owned by DeLon Mork, has sold the most Blizzards of any Dairy Queen in North America on Miracle Treat Day for the past 11 years. \n"}
{"id": "59171", "url": "https://en.wikipedia.org/wiki?curid=59171", "title": "Mumps", "text": "Mumps\n\nMumps is a viral disease caused by the mumps virus. Initial signs and symptoms often include fever, muscle pain, headache, poor appetite, and feeling tired. This is then usually followed by painful swelling of one or both parotid salivary glands. Symptoms typically occur 16 to 18 days after exposure and resolve after seven to ten days. Symptoms in adults are often more severe than in children. About a third of people have mild or no symptoms. Complications may include meningitis (15 percent), pancreatitis (four percent), inflammation of the heart, permanent deafness, and testicular inflammation which uncommonly results in infertility. Women may develop ovarian swelling but this does not increase the risk of infertility.\nMumps is highly contagious and spreads rapidly among people living in close quarters. The virus is transmitted by respiratory droplets or direct contact with an infected person. Only humans get and spread the disease. People are infectious to each other from about seven days before the start of symptoms to about eight days after. Once an infection has run its course, a person is typically immune for life. Reinfection is possible but the ensuing infection tends to be mild. Diagnosis is usually suspected due to parotid swelling and can be confirmed by isolating the virus on a swab of the parotid duct. Testing for IgM antibodies in the blood is simple and may be useful; however, it can be falsely negative in those who have been immunized.\nMumps is preventable by two doses of the mumps vaccine. Most of the developed world includes it in their immunization programs, often in combination with measles, rubella, and varicella vaccine. Countries that have low immunization rates may see an increase in cases among older age groups and thus worse outcomes. There is no specific treatment. Efforts involve controlling symptoms with pain medication such as paracetamol (acetaminophen). Intravenous immunoglobulin may be useful in certain complications. Hospitalization may be required if meningitis or pancreatitis develops. About one per ten thousand people who are infected die.\nWithout immunization about 0.1 percent to one percent of the population are affected per year. Widespread vaccination has resulted in a more than 90 percent decline in rates of disease. Mumps is more common in the developing world where vaccination is less common. Outbreaks, however, may still occur in a vaccinated population. Before the introduction of a vaccine, mumps was a common childhood disease worldwide. Larger outbreaks of disease would typically occur every two to five years. Children between the ages of five and nine were most commonly affected. Among immunized populations often those in their early 20s are affected. Around the equator it often occurs all year round while in the more northerly and southerly regions of the world it is more common in the winter and spring. Painful swelling of the parotid glands and testicles was described by Hippocrates in the 5th century BCE.\n\nMumps is usually preceded by a set of prodromal symptoms including low-grade fever, headache, and malaise. This is followed by progressive swelling of one or both parotid glands. Parotid gland swelling usually lasts about one week. Other symptoms of mumps can include dry mouth, sore face and/or ears and some patients find it difficult to talk. A vaccine has been available since the 1960s.\n\n\nThe mumps virus is an enveloped single-stranded, linear negative-sense RNA virus of the genus \"Rubulavirus\" and family \"Paramyxovirus\". The genome consists of 15,384 bases encoding nine proteins. Proteins involved in viral replication are the nucleoprotein, phosphoprotein, and polymerase protein while the genomic RNA forms the ribonucleocapsid. Humans are the only natural host for the virus.\n\nMumps is spread from person to person through contact with respiratory secretions, such as saliva from an infected person. When an infected person coughs or sneezes, the droplets aerosolize and can enter the eyes, nose, or mouth of another person. Mumps can also be spread by sharing eating utensils or cups. The virus can also survive on surfaces and then be spread after contact in a similar manner. A person infected with mumps is contagious from approximately seven days before the onset of symptoms until about eight days after symptoms start. The incubation period (time until symptoms begin) can be from 12–25 days, but is typically 16–18 days. 20-40 percent of persons infected with the mumps virus do not show symptoms, so it is possible to be infected and spread the virus without knowing it.\n\nDuring an outbreak, a diagnosis can be made by determining recent exposure and parotitis. However, when the disease incidence is low, other infectious causes of parotitis should be considered such as HIV, coxsackievirus, and influenza. Some viruses such as enteroviruses may cause aseptic meningitis that is very clinically similar to mumps.\n\nA physical examination confirms the presence of the swollen glands. Usually, the disease is diagnosed on clinical grounds, and no confirmatory laboratory testing is needed. If there is uncertainty about the diagnosis, a test of saliva or blood may be carried out; a newer diagnostic confirmation, using real-time nested polymerase chain reaction (PCR) technology, has also been developed. As with any inflammation of the salivary glands, the serum level of the enzyme amylase is often elevated.\n\nThe most common preventative measure against mumps is a vaccination with a mumps vaccine, invented by American microbiologist Maurice Hilleman at Merck. The vaccine may be given separately or as part of the MMR immunization vaccine that also protects against measles and rubella. In the US, MMR is now being supplanted by MMRV, which adds protection against chickenpox (varicella, HHV3). The WHO (World Health Organization) recommends the use of mumps vaccines in all countries with well-functioning childhood vaccination programmes. In the United Kingdom it is routinely given to children at age 13 months with a booster at 3–5 years (preschool) This confers lifelong immunity. The American Academy of Pediatrics recommends the routine administration of MMR vaccine at ages 12–15 months and at 4–6 years. In some locations, the vaccine is given again between four and six years of age, or between 11 and 12 years of age if not previously given. The efficacy of the vaccine depends on the strain of the vaccine, but is usually around 80 percent. The Jeryl Lynn strain is most commonly used in developed countries but has been shown to have reduced efficacy in epidemic situations. The Leningrad-Zagreb strain commonly used in developing countries appears to have superior efficacy in epidemic situations.\n\nBecause of the outbreaks within college and university settings, many governments have established vaccination programs to prevent large-scale outbreaks. In Canada, provincial governments and the Public Health Agency of Canada have all participated in awareness campaigns to encourage students ranging from grade one to college and university to get vaccinated.\n\nSome anti-vaccine activists protest against the administration of a vaccine against mumps, claiming that the attenuated vaccine strain is harmful, and/or that the wild disease is beneficial. There is no evidence whatsoever to support the claim that the wild disease is beneficial, or that the MMR vaccine is harmful. Claims have been made that the MMR vaccine is linked to autism and inflammatory bowel disease, including one study by Andrew Wakefield. The paper was discredited and retracted in 2010 and Wakefield was later stripped of his license after his work was found to be an \"elaborate fraud\". Also, subsequent studies indicate no link between vaccination with the MMR and autism. Since the dangers of the disease are well known, and the dangers of the vaccine are quite minimal, vaccination is nearly universally recommended.\n\nThe WHO, the American Academy of Pediatrics, the Advisory Committee on Immunization Practices of the Centers for Disease Control and Prevention, the American Academy of Family Physicians, the British Medical Association and the Royal Pharmaceutical Society of Great Britain currently recommend routine vaccination of children against mumps. The British Medical Association and Royal Pharmaceutical Society of Great Britain had previously recommended against general mumps vaccination, changing that recommendation in 1987.\n\nBefore the introduction of the mumps vaccine, the mumps virus was the leading cause of viral meningoencephalitis in the United States. However, encephalitis occurs rarely (less than two per 100,000). In one of the largest studies in the literature, the most common symptoms of mumps meningoencephalitis were found to be fever (97 percent), vomiting (94 percent) and headache (88.8 percent). The mumps vaccine was introduced into the United States in December 1967: since its introduction there has been a steady decrease in the incidence of mumps and mumps virus infection. There were 151,209 cases of mumps reported in 1968. From 2001 to 2008, the case average was only 265 per year, excluding an outbreak of less than 6000 cases in 2006 attributed largely to university contagion in young adults.\n\nThe treatment of mumps is supportive. Symptoms may be relieved by the application of intermittent ice or heat to the affected neck/testicular area and by acetaminophen for pain relief. Warm saltwater gargles, soft foods, and extra fluids may also help relieve symptoms. Acetylsalicylic acid (aspirin) is not used to treat children due to the risk of Reye syndrome.\n\nThere is no effective post-exposure recommendation to prevent secondary transmission, nor is the post-exposure use of vaccine or immunoglobulin effective.\n\nMumps is considered most contagious in the five days after the onset of symptoms, and isolation is recommended during this period. In someone who has been admitted to the hospital, standard and droplet precautions are needed. People who work in healthcare cannot work for five days.\n\nIn the United States there are typically between a couple of hundred and a couple of thousand cases a year.\n\n"}
{"id": "338082", "url": "https://en.wikipedia.org/wiki?curid=338082", "title": "Nasogastric intubation", "text": "Nasogastric intubation\n\nNasogastric intubation is a medical process involving the insertion of a plastic tube (nasogastric tube or NG tube) through the nose, past the throat, and down into the stomach. Orogastric intubation is a similar process involving the insertion of a plastic tube (orogastric tube) through the mouth.\n\nA nasogastric tube is used for feeding and administering drugs and other oral agents such as activated charcoal. For drugs and for minimal quantities of liquid, a syringe is used for injection into the tube. For continuous feeding, a gravity based system is employed, with the solution placed higher than the patient's stomach. If accrued supervision is required for the feeding, the tube is often connected to an electronic pump which can control and measure the patient's intake and signal any interruption in the feeding. Nasogastric tubes may also be used as an aid in the treatment of life threatening eating disorders, especially if the patient is not compliant with eating.\nNasogastric aspiration (suction) is the process of draining the stomach's contents via the tube. Nasogastric aspiration is mainly used to remove gastrointestinal secretions and swallowed air in patients with gastrointestinal obstructions. Nasogastric aspiration can also be used in poisoning situations when a potentially toxic liquid has been ingested, for preparation before surgery under anaesthesia, and to extract samples of gastric liquid for analysis.\n\nIf the tube is to be used for continuous drainage, it is usually appended to a collector bag placed below the level of the patient's stomach; gravity empties the stomach's contents. It can also be appended to a suction system, however this method is often restricted to emergency situations, as the constant suction can easily damage the stomach's lining. In non-emergency situations, intermittent suction is often applied giving the benefits of suction without the untoward effects of damage to the stomach lining.\n\nSuction drainage is also used for patients who have undergone a pneumonectomy in order to prevent anesthesia-related vomiting and possible aspiration of any stomach contents. Such aspiration would represent a serious risk of complications to patients recovering from this surgery.\n\nTypes of nasogastric tubes include:\n\nBefore an NG tube is inserted, it must be measured from the tip of the patient's nose, loop around their ear and then down to roughly 5 cm below the xiphoid process. The tube is then marked at this level to ensure that the tube has been inserted far enough into the patient's stomach. Many commercially available stomach and duodenal tubes have several standard depth markings, for example 18\" (46 cm), 22\" (56 cm), 26\" (66 cm) and 30\" (76 cm) from distal end; infant feeding tubes often come with 1 cm depth markings. The end of a plastic tube is lubricated (local anesthetic, such as 2% xylocaine gel, may be used; in addition, nasal vasoconstrictor and/or anesthetic spray may be applied before the insertion) and inserted into one of the patient's anterior nares. The tube should be directed straight towards the back of the patient as it moves through the nasal cavity and down into the throat. When the tube enters the oropharynx and glides down the posterior pharyngeal wall, the patient may gag; in this situation the patient, if awake and alert, is asked to mimic swallowing or is given some water to sip through a straw, and the tube continues to be inserted as the patient swallows. Once the tube is past the pharynx and enters the esophagus, it is easily inserted down into the stomach. The tube must then be secured in place to prevent it from moving.\n\nGreat care must be taken to ensure that the tube has not passed through the larynx into the trachea and down into the bronchi. The reliable method is to aspirate some fluid from the tube with a syringe. This fluid is then tested with pH paper (note not litmus paper) to determine the acidity of the fluid. If the pH is 4 or below then the tube is in the correct position. If this is not possible then correct verification of tube position is obtained with an X-ray of the chest/abdomen. This is the most reliable means of ensuring proper placement of an NG tube. The use of a chest x-ray to confirm position is the expected standard in the UK, with Dr/ physician review and confirmation. Future techniques may include measuring the concentration of enzymes such as trypsin, pepsin, and bilirubin to confirm the correct placement of the NG tube. As enzyme testing becomes more practical, allowing measurements to be taken quickly and cheaply at the bedside, this technique may be used in combination with pH testing as an effective, less harmful replacement of X-ray confirmation. If the tube is to remain in place then a tube position check is recommended before each feed and at least once per day.\n\nOnly smaller diameter (12 Fr or less in adults) nasogastric tubes are appropriate for long-term feeding, so as to avoid irritation and erosion of the nasal mucosa. These tubes often have guidewires to facilitate insertion. If feeding is required for a longer period of time, other options, such as placement of a PEG tube, should be considered.\n\nFunction of an NG tube properly placed and used for suction is maintained by flushing. This may be done by flushing small amounts of saline and air using a syringe or by flushing larger amounts of saline or water, and air, and then assessing for the air to circulate through one lumen of the tube, into the stomach, and out the other lumen. When these two techniques of flushing were compared, the latter was more effective.\n\nThe use of nasogastric intubation is contraindicated in patients with moderate-to-severe neck and facial fractures due to the increased risk of airway obstruction or improper tube placement. Special attention is necessary during insertion under these circumstances in order to avoid undue trauma to the esophagus. There is also a greater risk to patients suffering from bleeding disorders, particularly those resulting from the distended sub-mucosal veins in the lower third of the esophagus known as esophageal varices which may be easily ruptured due to their friability and also in GERD.\n\nAlternative measures, such as an orogastric intubation, should be considered under these circumstances, or if the patient will be incapable of meeting their nutritional and caloric needs for an extended time period (usually >24 hours).\n\nMinor complications include nose bleeds, sinusitis, and a sore throat.\n\nSometimes more significant complications occur including erosion of the nose where the tube is anchored, esophageal perforation, damage to a surgical anastomosis, pulmonary aspiration, a collapsed lung, or intracranial placement of the tube.\n\n"}
{"id": "52165644", "url": "https://en.wikipedia.org/wiki?curid=52165644", "title": "National Offshore Petroleum Safety and Environmental Management Authority", "text": "National Offshore Petroleum Safety and Environmental Management Authority\n\nThe National Offshore Petroleum Safety and Environmental Management Authority (NOPSEMA) is an Australian Commonwealth statutory agency established under the \"Offshore Petroleum and Greenhouse Gas Storage Act 2006\" (OPGGS Act). NOPSEMA serves as the national regulator for the health and safety, well integrity and environmental management aspects of offshore oil and gas operations in Australian Commonwealth waters; and in coastal waters where regulatory powers and functions have been conferred by state governments.\n\nNOPSEMA was established on 1 January 2012, superseding the National Offshore Petroleum Safety Authority (NOPSA). The agency is currently headed by Stuart Smith.\n\n"}
{"id": "2769944", "url": "https://en.wikipedia.org/wiki?curid=2769944", "title": "Paralytic shellfish poisoning", "text": "Paralytic shellfish poisoning\n\nParalytic shellfish poisoning (PSP) is one of the four recognized syndromes of shellfish poisoning, which share some common features and are primarily associated with bivalve mollusks (such as mussels, clams, oysters and scallops). These shellfish are filter feeders and accumulate neurotoxins, chiefly saxitoxin, produced by microscopic algae, such as dinoflagellates, diatoms, and cyanobacteria. Dinoflagellates of the genus \"Alexandrium\" are the most numerous and widespread saxitoxin producers and are responsible for PSP blooms in subarctic, temperate, and tropical locations. The majority of toxic blooms have been caused by the morphospecies \"Alexandrium catenella, Alexandrium tamarense\", and \"Alexandrium fundyense\", which together comprise the \"A. tamarense\" species complex. In Asia, PSP is mostly associated with the occurrence of the species \"Pyrodinium bahamense\".\n\nAlso some pufferfish, including chamaeleon puffer, contain saxitoxin, making their consumption hazardous.\n\nThe toxins responsible for most shellfish poisonings are water insoluble, heat and acid-stable, and ordinary cooking methods do not eliminate the toxins. The principal toxin responsible for PSP is saxitoxin. Some shellfish can store this toxin for several weeks after a harmful algal bloom passes, but others, such as butter clams, are known to store the toxin for up to two years. Additional toxins are found, such as neosaxiton and gonyautoxins I to IV. All of them act primarily on the nervous system.\n\nPSP can be fatal in extreme cases, particularly in immunocompromised individuals. Children are more susceptible. PSP affects those who come into contact with the affected shellfish by ingestion. Symptoms can appear ten to 30 minutes after ingestion, and include nausea, vomiting, diarrhea, abdominal pain, tingling or burning lips, gums, tongue, face, neck, arms, legs, and toes. Shortness of breath, dry mouth, a choking feeling, confused or slurred speech, and loss of coordination are also possible.\n\nPSP has been implicated as a possible cause of sea otter mortality and morbidity in Alaska, as one of its primary prey items, the butter clam \"(Saxidonus giganteus)\" bioaccumulates saxitoxin as a chemical defense mechanism. In addition, ingestion of saxitoxin-containing mackerel has been implicated in the death of humpback whales.\n\nAdditional cases where PSP was suspected as the cause of death in Mediterranean monk seals (\"Monachus monachus\") in the Mediterranean Sea have been questioned due to lack of additional testing to rule out other causes of mortality.\n\n\n"}
{"id": "855377", "url": "https://en.wikipedia.org/wiki?curid=855377", "title": "Pediatric urology", "text": "Pediatric urology\n\nPediatric urology is a surgical subspecialty of medicine dealing with the disorders of children's genitourinary systems. Pediatric urologists provide care for both boys and girls ranging from birth to early adult age. The most common problems are those involving disorders of urination, reproductive organs and testes.\n\nSome of the problems they deal with are:\n\nIn North America, most pediatric urologists are associated with children's hospitals. Training for board certification in pediatric urology typically consists of a surgery internship as part of a urology residency followed by subspecialty training in pediatric urology at a major children's hospital. In India, Pediatric Urology is practiced by Pediatric Surgeons with a special interest/ training in pediatric urology as well as by adult urologists who get trained in Pediatric Urology.\n\n\n"}
{"id": "46338565", "url": "https://en.wikipedia.org/wiki?curid=46338565", "title": "Poolburn Reservoir", "text": "Poolburn Reservoir\n\nPoolburn Reservoir, also known as Poolburn Dam, is a reservoir in Central Otago. Built during the Great Depression for irrigation but also as an employment initiative, the water is used by farmers in the Ida Valley.\n\nThe reservoir is located west of the Rough Ridge Range. Long Valley Creek feeds the reservoir, and the reservoir itself feeds the Pool Burn. The Pool Burn flows into the Ida Valley, combines with the Ida Burn, and breaks through the Raggedy Range as the Poolburn Gorge before flowing into the Manuherikia River.\n\nThere is access to Poolburn Reservoir from either Omakau in the Manuherikia Valley () or from Oturehua in the Ida Valley (). There is also access from Paerau over the Rock and Pillar Range (), but this is a dry-weather road only that can be negotiated by four-wheel drive only, and is not generally recommended. The high parts of the road are closed from early June to late September each year, and Poolburn Reservoir thus cannot be accessed by road during winter. The road from Paerau to the Ida Valley via Poolburn Reservoir is part of the Old Dunstan Road that gave gold diggers access to the gold fields during the early days of the Otago Gold Rush. The journey from Dunedin to the gold fields took five days. An alternative route to the gold fields starting in Palmerston was longer by but much easier to travel on, as it did cross any mountainous ranges or major rivers. This became the major wagon route and is today State Highway 85. With no major settlements along its way, much of the Old Dunstan Road was never fully formed, resulting in only seasonal access to Poolburn Reservoir. Since 2004, council staff fly in by helicopter to lock the mountainous roads each winter.\n\nA deputation went to see the Minister of Labour and Transport, Bill Veitch, in March 1929 with regards to a proposal for an irrigation dam for the benefit of Ida Valley farmers. The United Government approved the scheme by September of that year, partially as an employment initiative during the Great Depression, and charged the Department of Public Works with its construction and put £71,823 into the 1930 budget. Preliminary work, including the construction of the access road from the Ida Valley, started in late 1929. The Minister of Public Works, William Burgoyne Taverner, reported in August 1930 that the excavation for the concrete arch dam had been filled to approximately ground level. Poolburn Reservoir was completed by late 1931. The reservoir is located on the Old Dunstan Road and the flooded area had five hotels.\n\nThe concrete arch dam is about high and long at the dam's crest. Approximately of concrete were used to construct the dam. During construction, scaffolding collapsed resulting in seven men sustaining injuries, with two men initially in critical condition. In June 1931, one worker died from a fall at the site.\n\nToday, the dam is administered by the Ida Valley Irrigation Company. When full, the reservoir covers over . Poolburn Reservoir and Poolburn Dam are alternative names for the same body of water.\n\nBrown trout and rainbow trout were introduced into the reservoir, but only brown trout remain. The other aquatic species that can be caught is the New Zealand freshwater crayfish koura.\n\nPoolburn Reservoir was used to depict Rohan in \"The Lord of the Rings\" film trilogy.\n\nThere are many baches around the reservoir. An application to subdivide a land holding for housing was withdrawn just prior to the hearing, as the commissioner for the Central Otago District Council had recommended for consent to be refused.\n"}
{"id": "7454661", "url": "https://en.wikipedia.org/wiki?curid=7454661", "title": "Pulp polyp", "text": "Pulp polyp\n\nA pulp polyp, also known as chronic hyperplastic pulpitis, is a \"productive\" (i.e., growing) inflammation of dental pulp in which the development of granulation tissue is seen, and is characterised by the overgrowth of the tissue outside the boundary of a tooth's pulp chamber. A pulp polyp may be found in an open carious lesion (tooth cavity), in a fractured tooth, or when a dental restoration is missing. Due to lack of intrapulpal pressure in an open lesion, pulp necrosis does not take place as would have occurred in a closed caries case. \n\nThe condition is more commonly seen in children and young adults. A good vascular and immune supply is necessary for such a condition to develop.\n"}
{"id": "40850046", "url": "https://en.wikipedia.org/wiki?curid=40850046", "title": "Quality of Life in Depression Scale", "text": "Quality of Life in Depression Scale\n\nThe Quality of Life In Depression Scale (QLDS) is a disease specific patient-reported outcome which assesses the impact that depression has on a patient’s quality of life. \n\nIt is a 34 item self rated questionnaire which consists of dichotomous response questions, with the response being either True/Not True or Yes/No. It is scored binomially (0-1) and high scores on the QLDS indicated a lower quality of life.\n\nThe QLDS was developed by Galen Research in 1992 and was funded by Lilly Industries. It was developed in the United Kingdom.\n\nThe items in the QLDS were derived from statements made in qualitative interviews by 30 depressed or recently recovered patients. Further interviews were held with patients in order to assess whether the proposed scale had face and content validity.\n\nSince its development, the QLDS has been adapted and validated in 12 languages other than UK English, including Norwegian and Spanish. This has allowed the QLDS to be used in research and clinical studies worldwide. \n\nStudies utilizing the QLDS include investigations into venlafaxine, duloxetine and bupropion.\n"}
{"id": "16220926", "url": "https://en.wikipedia.org/wiki?curid=16220926", "title": "Regulation of ship pollution in the United States", "text": "Regulation of ship pollution in the United States\n\nIn the United States, several federal agencies and laws have some jurisdiction over pollution from ships in U.S. waters. States and local government agencies also have responsibilities for ship-related pollution in some situations.\n\nMARPOL 73/78 (the \"International Convention for the Prevention of Pollution From Ships\") is one of the most important treaties regulating pollution from ships. Six Annexes of the Convention cover the various sources of pollution from ships and provide an overarching framework for international objectives. In the U.S., the Convention is implemented through the Act to Prevent Pollution from Ships (APPS). Under the provisions of the Convention, the United States can take direct enforcement action under U.S. laws against foreign-flagged ships when pollution discharge incidents occur within U.S. jurisdiction. When incidents occur outside U.S. jurisdiction or jurisdiction cannot be determined, the United States refers cases to flag states, in accordance with MARPOL. These procedures require substantial coordination between the Coast Guard, the State Department, and other flag states, and the response rate from flag states has been poor. Different regulations apply to vessels, depending on the individual state.\n\nIn the United States, several federal agencies have some jurisdiction over ships in U.S. waters, but no one agency is responsible for or coordinates all of the relevant government functions. The U.S. Coast Guard and Environmental Protection Agency (EPA) have principal regulatory and standard-setting responsibilities, and the Department of Justice prosecutes violations of federal laws. EPA and the Department of Defense (DOD) are jointly issuing \"Uniform National Discharge Standards\" (\"UNDS\") for armed forces vessels. In addition, the Department of State represents the United States at meetings of the IMO and in international treaty negotiations and is responsible for pursuing foreign-flag violations. Other federal agencies have limited roles and responsibilities. For example, the National Oceanic and Atmospheric Administration (NOAA, Department of Commerce) works with the Coast Guard and EPA to report on the effects of marine debris. The Animal and Plant Health Inspection Service (APHIS) is responsible for ensuring quarantine inspection and disposal of food-contaminated garbage. These APHIS responsibilities are part of the Department of Agriculture. In some cases, states and localities have responsibilities as well.\n\nEPA issued its most recent \"Vessels General Permit,\" under the National Pollutant Discharge Elimination System (NPDES), a Clean Water Act (CWA) program, in 2013. The permit applies to large commercial vessels ( in length or greater) (except fishing vessels) and regulates 26 specific types of vessel discharges:\n\nApproximately 69,000 vessels, both domestic and foreign flagged, are covered by the VGP.\n\nEPA issued its \"Small Vessels General Permit\" (sVGP) for smaller commercial vessels in 2014, however this permit currently only applies to ballast water. A Congressionally-imposed moratorium applied to graywater and other wastestreams for these vessels, and commercial fishing vessels of any size, until January 19, 2018.\n\nCommercial vessels discharging sewage, except fishing vessels, are subject to the VGP or SVGP requirements. Recreational vessels are exempt from the permit requirements, but vessel operators must implement Best Management Practices to control their discharges.\n\nSection 312 of the CWA prohibits the dumping of untreated or inadequately treated sewage from vessels into the navigable waters of the United States (defined as within of shore). It is implemented jointly by EPA and the Coast Guard. Under commercial and recreational vessels with installed toilets are required to have marine sanitation devices (MSDs), which are designed to prevent the discharge of untreated sewage. EPA is responsible for developing performance standards for MSDs, and the Coast Guard is responsible for MSD design and operation regulations and for certifying MSD compliance with the EPA rules. MSDs are designed either to hold sewage for shore-based disposal or to treat sewage prior to discharge.\n\nThe Coast Guard regulations cover three types of MSDs. Large vessels use either Type II or Type III MSDs. In Type II MSDs, the waste is either chemically or biologically treated prior to discharge and must meet limits of no more than 200 fecal coliforms per 100 milliliters and no more than 150 milligrams per liter of suspended solids. Type III MSDs store wastes and do not treat them; the waste is pumped out later and treated in an onshore system or discharged outside U.S. waters. Type I MSDs use chemicals to disinfect the raw sewage prior to discharge and must meet a performance standard for fecal coliform bacteria of not greater than 1,000 per 100 milliliters and no visible floating solids. Type I MSDs are generally only found on recreational vessels or others under in length. The regulations, which have not been revised since 1976, do not require ship operators to sample, monitor, or report on their effluent discharges.\n\nCritics point out a number of deficiencies with this regulatory structure as it affects large vessels. First, the MSD regulations only cover discharges of bacterial contaminants and suspended solids, while the NPDES permit program for other point sources typically regulates many more pollutants such as chemicals, pesticides, heavy metals, oil, and grease that may be released by large vessels as well as land-based sources. Second, sources subject to NPDES permits must comply with sampling, monitoring, recordkeeping, and reporting requirements, which do not exist in the MSD rules.\n\nIn addition, the Coast Guard, responsible for inspecting vessels for compliance with the MSD rules, has been heavily criticized for poor enforcement of Section 312 requirements. In its 2000 report, the Government Accountability Office (GAO) said that Coast Guard inspectors \"rarely have time during scheduled ship examinations to inspect sewage treatment equipment or filter systems to see if they are working properly and filtering out potentially harmful contaminants.\" GAO reported that a number of factors limit the ability of Coast Guard inspectors to detect violations of environmental law and rules, including the inspectors' focus on safety, the large size some ships, limited time and staff for inspections, and the lack of an element of surprise concerning inspections. The Coast Guard carries out a wide range of responsibilities that encompass both homeland security (ports, waterways, and coastal security, defense readiness, drug and migrant interdiction) and non-homeland security (search and rescue, marine environmental protection, fisheries enforcement, aids to navigation). Since the September 11 terrorist attacks on the United States, the Coast Guard has focused more of its resources on homeland security activities. One likely result is that less of the Coast Guard's time and attention are available for vessel inspections for MSD or other environmental compliance.\n\nAnnex IV of MARPOL was drafted to regulate sewage discharges from vessels. It has entered into force internationally and would apply to ships that are flagged in ratifying countries, but because the United States has not ratified Annex IV, it is not mandatory that ships follow it when in U.S. waters. However, its requirements are minimal, even compared with U.S. rules for MSDs. Annex IV requires that vessels be equipped with a certified sewage treatment system or holding tank, but it prescribes no specific performance standards. Within three miles (5 km) of shore, Annex IV requires that sewage discharges be treated by a certified MSD prior to discharge. Between three and from shore, sewage discharges must be treated by no less than maceration or chlorination; sewage discharges beyond from shore are unrestricted. Vessels are permitted to meet alternative, less stringent requirements when they are in the jurisdiction of countries where less stringent requirements apply. In U.S. waters, vessels must comply with the regulations implementing Section 312 of the Clean Water Act.\n\nOn some ships, especially many of those that travel in Alaskan waters, sewage is treated using Advanced Wastewater Treatment (AWT) systems that generally provide improved screening, treatment, disinfection, and sludge processing as compared with traditional Type II MSDs. AWTs are believed to be very effective in removing pathogens, oxygen demanding substances, suspended solids, oil and grease, and particulate metals from sewage, but only moderately effective in removing dissolved metals and nutrients (ammonia, nitrogen and phosphorus).\n\nSection 312 has another means of addressing sewage discharges, through establishment of no-discharge zones (NDZs) for vessel sewage. A state may completely prohibit the discharge of both treated and untreated sewage from all vessels with installed toilets into some or all waters over which it has jurisdiction (up to from land). To create a no-discharge zone to protect waters from sewage discharges by vessels, the state must apply to EPA under one of three categories.\n\nPursuant to a state law in Alaska, graywater must be treated prior to discharge into that state's waters.\n\nShip discharges of solid waste are governed by two laws. Title I of the Marine Protection, Research, and Sanctuaries Act (MPRSA) applies to cruise ships and other vessels and makes it illegal to transport garbage from the United States for the purpose of dumping it into ocean waters without a permit or to dump any material transported from a location outside the United States into U.S. territorial seas or the contiguous zone (within from shore) or ocean waters. EPA is responsible for issuing permits that regulate the disposal of materials at sea (except for dredged material disposal, for which the U.S. Army Corps of Engineers is responsible). Beyond waters that are under U.S. jurisdiction, no MPRSA permit is required for a ship to discharge solid waste. The routine discharge of effluent incidental to the propulsion of vessels is explicitly exempted from the definition of dumping in the MPRSA.\n\nThe Act to Prevent Pollution from Ships (APPS) and its regulations, which implement U.S.-ratified provisions of MARPOL, also apply to ships. APPS prohibits the discharge of all garbage within of shore, certain types of garbage within offshore, and plastic anywhere. It applies to all vessels, whether seagoing or not, regardless of flag, operating in U.S. navigable waters and the Exclusive Economic Zone (EEZ). It is administered by the Coast Guard, which carries out inspection programs to insure the adequacy of port facilities to receive offloaded solid waste.\n\nThe Resource Conservation and Recovery Act (RCRA) is the primary federal law that governs hazardous waste management through a \"cradle-to-grave\" program that controls hazardous waste from the point of generation until ultimate disposal. The act imposes management requirements on generators, transporters, and persons who treat or dispose of hazardous waste. Under this act, a waste is hazardous if it is ignitable, corrosive, reactive, or toxic, or appears on a list of about 100 industrial process waste streams and more than 500 discarded commercial products and chemicals. Treatment, storage, and disposal facilities are required to have permits and comply with operating standards and other EPA regulations.\n\nThe owner or operator of a ship may be a generator and/or a transporter of hazardous waste, and thus subject to RCRA rules. Issues that the ship industry may face relating to RCRA include ensuring that hazardous waste is identified at the point at which it is considered generated; ensuring that parties are properly identified as generators, storers, treaters, or disposers; and determining the applicability of RCRA requirements to each. Hazardous waste generated onboard ships is stored onboard until the wastes can be offloaded for recycling or disposal in accordance with RCRA.\n\nA range of activities on board cruise generate hazardous wastes and toxic substances that would ordinarily be presumed to be subject to RCRA. Ships are potentially subject to RCRA requirements to the extent that chemicals used for operations such as ship maintenance and passenger services result in the generation of hazardous wastes. However, it is not entirely clear what regulations apply to the management and disposal of these wastes.30 RCRA rules that cover small-quantity generators (those that generate more than 100 kilograms but less than 1,000 kilograms of hazardous waste per month) are less stringent than those for large-quantity generators (generating more than 1,000 kilograms per month), and it is unclear whether ships are classified as large or small generators of hazardous waste. Moreover, some ship companies argue that they generate less than 100 kilograms per month and therefore should be classified in a third category, as \"conditionally exempt small-quantity generators,\" a categorization that allows for less rigorous requirements for notification, recordkeeping, and the like.\n\nA release of hazardous substances by a vessel could also theoretically trigger coverage under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA; also known as \"Superfund.\").\n\nIn addition to RCRA, hazardous waste discharges from ships are subject to Section 311 of the Clean Water Act, which prohibits the discharge of hazardous substances in harmful quantities into or upon the navigable waters of the United States, adjoining shorelines, or into or upon the waters of the contiguous zone.\n\nSection 311 of the Clean Water Act, as amended by the Oil Pollution Act of 1990, applies to ships and prohibits discharge of oil or hazardous substances in harmful quantities into or upon U.S. navigable waters, or into or upon the waters of the contiguous zone, or which may affect natural resources in the U.S. EEZ (extending offshore). Coast Guard regulations prohibit discharge of oil within from shore, unless passed through a 15-ppm oil water separator, and unless the discharge does not cause a visible sheen. Beyond , oil or oily mixtures can be discharged while a vessel is proceeding en route and if the oil content without dilution is less than 100 ppm. Vessels are required to maintain an Oil Record Book to record disposal of oily residues and discharges overboard or disposal of bilge water.\n\nIn addition to Section 311 requirements, APPS implements MARPOL Annex I concerning oil pollution. APPS applies to all U.S. flagged ships anywhere in the world and to all foreign flagged vessels operating in the navigable waters of the United States, or while at a port under U.S. jurisdiction. To implement APPS, the Coast Guard has promulgated regulations prohibiting the discharge of oil or oily mixtures into the sea within of the nearest land, except under limited conditions. However, because many ships are foreign registered and because APPS only applies to foreign ships within U.S. navigable waters, the APPS regulations have limited applicability to ship operations.\n\nThe VGP sets numeric ballast water discharge limits for large commercial vessels. The limits are expressed as the maximum acceptable concentration of living organisms per cubic meter of ballast water. The Coast Guard worked with EPA in developing the scientific basis and the regulatory requirements in the VGP.\n\nCongress amended the CWA in 1996 to require development of uniform national discharge standards (\"UNDS\") for military vessels. The standards are being developed jointly by EPA and DOD. Initial regulations were published in 1999, to identify and characterize a wide variety of discharge types from ships and boats. A final rule setting specific standards for 11 discharge types was published in 2017. A proposed regulation covering 11 additional discharge categories was published in 2016. The majority of vessels covered belong to the U.S. Navy, but the regulations also cover vessels of the Coast Guard, Marine Corps, Army, Military Sealift Command, and Air Force, totalling over 7,000 vessels.\n\n\n"}
{"id": "44531619", "url": "https://en.wikipedia.org/wiki?curid=44531619", "title": "Robert W. McNulty", "text": "Robert W. McNulty\n\nRobert W. McNulty (1897-1966) was a dental educator. He was president of the American Dental Education Association in 1960 (a title changed to Chairman of the Board of Directors of the American Dental Education Association in 2013). \n\nBorn in Braidwood, Illinois, McNulty graduated D.D.S. from Chicago's Loyola University School of Dentistry in 1926. He later in the 1940s served as dean of the Loyola University Dental School and as a president of the Illinois State Dental Society, becoming a nationally known author and lecturer on the problems of dental education. \n\nFrom 1950 till 1965 when he retired, Dr. McNulty was Dean of Los Angeles' USC Dental School. The USC Dental School's Robert W. McNulty Memorial Award for Scholastic Achievement is named in his honor. The Loyola University School of Dentistry inducted Dr. McNulty as the 11th member of its Hall of Fame. \n\nMcNulty was a Fellow of the American College of Dentists and a member of the American Academy of Dental History.\n\n\n \n"}
{"id": "10656343", "url": "https://en.wikipedia.org/wiki?curid=10656343", "title": "Salim Medical Centre, Dherai", "text": "Salim Medical Centre, Dherai\n\nSalim Medical Centre Derai is one of the main welfare medical centres run by qualified doctors and nursing staff for the welfare of the people of Derai . Salim Medical Centre Derai was established on 27 September 1995.\n\nSalim Medical Centre, Derai, near Saidu Sharif Airport postcode 19200, Swat Khyber Pakhtunkhwa, Pakistan.\n\n\n"}
{"id": "5799795", "url": "https://en.wikipedia.org/wiki?curid=5799795", "title": "Salvador Zubirán", "text": "Salvador Zubirán\n\nSalvador Zubirán Anchondo (23 December 1898, Cusihuiriachic, Chihuahua – 10 June 1998, Mexico City) was one of Mexico's most prominent physicians and nutritionists.\n\nHe received his MD from the National University of Mexico (UNAM) Faculty of Medicine and visited the Peter Bent Brigham Hospital in Boston, Massachusetts, United States, from 1924 to 1925, where he received a diploma.\n\nUpon his return to Mexico he started the nutrition department in Mexico City's General Hospital, and later received an assignment to start the National Institute of Nutrition (now the Instituto Nacional de Ciencias Médicas y de la Nutrición Salvador Zubirán, or INNSZ), one of the country's premier medical institutions. Zubirán also served as the rector of the National Autonomous University of Mexico (UNAM) during the 1940s; during this appointment, he was very influential in the creation of the University City.\n\nIn 1985, in Chihuahua, he married María Luisa López-Collada Márquez de Richardson, the widow of American banker William B. Richardson II. By this marriage, he became the stepfather of the American politician and one-time presidential hopeful Bill Richardson.\n\nHe was a recipient of the National Prize for Arts and Sciences in 1968 and of \nthe Belisario Domínguez Medal of Honor, the highest honor awarded by the Senate of Mexico, in 1986.\n"}
{"id": "48994652", "url": "https://en.wikipedia.org/wiki?curid=48994652", "title": "Sexualization, Media, and Society", "text": "Sexualization, Media, and Society\n\nSexualization, Media, and Society (SMS) is a peer-reviewed, interdisciplinary open-access academic journal, published by Sage, to provide a resource for diverse scholars and activists interested in critically examining the phenomenon of sexualized media as it affects individuals, relationships, communities, and societies.\n\nThe journal was founded in 2015 by co-editors Ana Bridges (University of Arkansas), Deirdre M. Condit (Virginia Commonwealth University), Gail Dines (Wheelock College), Jennifer A. Johnson (Virginia Commonwealth University), and Carolyn West (University of Washington Tacoma).\n"}
{"id": "47610745", "url": "https://en.wikipedia.org/wiki?curid=47610745", "title": "Skin cancer in Australia", "text": "Skin cancer in Australia\n\nSkin cancer in Australia kills over 2,000 each year, with more than 750,000 diagnosed and treated. Tanning became embedded in Australian culture and proved to be a controversial issue because of its popularity among teens and solarium users, despite correlations between tanning and an increased risk of developing melanoma. Australia experienced relative success through skin cancer prevention campaigns started in the 1980s and continued to invest and promote awareness through government-funded mass media strategies. Although Australia has one of the highest national rates of skin cancer, mortality trends in melanoma stabilized.\n\nAustralians culturally identify with the \"bronzed Aussie\" stereotype, viewing it as a positive body image associated with recreational sport and exercise-orientated lifestyles. With over 90% of melanomas derived through contact with the sun, skin cancer preventive initiatives in Australia strived to change this perception.\n\nSolariums and tanning salons are widely used and available throughout Australia. Tanning has been a phenomenon since the 1800s, with medical use of phototherapy, emerging popularity of sunbathing, and in the 1970s, with tanning salons and solariums becoming increasingly popular, had developed into a worldwide tanning industry. Although the tanning industry in Australia is relatively small by international standards, it has quadrupled in size since 1992. The tanning industry promotes tanning as a process to stimulate higher levels of vitamin D, associating it with reduced likelihood of sunburn and skin cancer, with increased well being and feelings of happiness.\n\nThe solarium industry is regulated on a state by state basis. The first states to regulate solarium use (2008) were Victoria, South Australia and Western Australia following the death of skin cancer victim Clare Oliver. The 2008 regulations required solaria to obtain a license, display health warnings. In Victoria, those under 16 and people with fair skin were banned from solaria and 16-17 year olds were required to have parental consent whereas in South Australia and Western Australia, under 18s were banned.\n\nIn February 2009, the Victorian Government introduced license changes, including banning under 18s, consistent with the revised Australian standard, released in January 2009. Victorian solarium legalisation was revised in late 2010, strengthening controls around citing evidence of age documents for under 18s.\n\nThe Australian standard requires that operators must:\nNew South Wales, Queensland, ACT and Tasmania introduced legislation applying these standards in 2009 and 2010. In 2011, the New South Wales government called for public submissions in relation to a proposal to extend the age ban to those under 30. In February 2012 the New South Wales Government announced its intention to ban tanning beds, starting in 2014. In October 2013, the Victorian parliament passed an official ban on solaria to take effect by 2015. As of 2018, there was illegal solarium use in Australia, often advertised on Gumtree.\n\nMelanoma, basal cell carcinomas and squamous cell carcinomas are predominantly caused by exposure to ultra violet radiation (UVR), and both UVA and UVB radiation has internationally been categorised as carcinogenic. Artificial UVR primarily used in tanning salons and sunbeds, has generated concern among health officials and it was observed to considerably heighten the risk of developing cutaneous malignant melanoma. A study in 2010 found strong evidence supporting association between indoor tanning salon use and increased risk of developing melanoma. The study demonstrated strong correlations between increased risk of melanomas and carcinogenic ultraviolet radiation related outcomes. These increased risks were specifically associated with younger participants, with tanning and solarium use relatively popular among teens and young adults. An increase of 59% in risk of developing melanoma was associated with people who used sun beds before they were 35.\n\nAustralia and New Zealand have the world's highest skin cancer rates. Factors include the large percentage of the population with fair skin prone to skin cancers and the high levels of ambient UV radiation. Similarly the Anglo-Celtic ancestry of many New Zealanders together with their outdoor lifestyle, is presumed to be a dominant factor in the risk, due to the effect of high UV levels on fair skin. The UV Index (UVI), defined as the sun-burning strength of ultraviolet rays (UVR), in Canada ranges between 1 and 10. The National Institute of Water and Atmospheric (NIWA) research recorded that New Zealand (similar latitude to Australia) the UVI exposure often exceeds 13 and is 40% more than that recorded at comparable latitudes in North America. In order to maintain effective prevention and national awareness Australia used a variety of campaigns and initiatives beginning in the early 1980s. The Slip, Slop, Slap campaign was initiated in 1981 introduced a seagull singing a catchy jingle “Slip on a shirt! Slop on some sunscreen! and Slap on a hat!”, promoted awareness and entered Australian culture. It was so successful that it remained part of the SunSmart slogan, which was updated to read, Slip, Slop, Slap, Slide (on sun glasses), Seek (shaded areas). SunSmart began in 1987, led by an Australian foundation focused on promoting skin cancer awareness. Social education, challenging societal and cultural ideals, is one of SunSmart's methods for promoting awareness and through its many successes now functions throughout all of Australia, under state Cancer Councils.\n\nThe first government-funded mass media skin cancer initiative began in Australia during 2006. Its skin cancer awareness message was delivered through radio, television and printed mediums.\n\nMedia promotion and education remains a vital and effective tool in Australian skin cancer awareness strategy. Despite its effectiveness members of the public remain indifferent or unaware of the risks caused by inefficient sun protection and skin care.\n\nSkin cancer has three main forms: basal cell carcinoma, squamous cell carcinoma and melanoma. The first two are the most prevalent forms. Although generally non-melanoma carcinomas have lower associations with mortality than melanomas, fatalities occur, with 534 reported deaths in 2011. 434,000 Australians in 2008 underwent treatment for non-melanoma carcinomas. Melanomas have the highest correlation with mortality, killing 1,544 in 2011. Australia shares with New Zealand, the highest diagnosis of melanoma throughout the world and also has the highest diagnosis of non-melanoma carcinomas. In 2011, New Zealand surpassed the Australian rate for invasive melanoma cases and now has the highest melanoma incidence in the world.\n\nAustralia has recorded increased mortality rates of melanoma from the 1950s, continuing to rise until the late 1980s and beginning to steady from 1990 onwards. Australia has some of the world's highest melanoma-related fatalities—double the mortality rates of south and central Europe. The introduction of preventive campaigns correlate with the transition to decreases in melanoma mortality. Sun protection, other forms of primary protection, early detection and increased public awareness have had the greatest impact.\n\nAwareness and early detection are the most efficient tools for avoiding skin cancer and are the basis of many effective prevention campaigns in Australia. Prevention initiatives such as SunSmart promote awareness by advocating effective sun protective methods, sun risk awareness and are an integral part of skin cancer prevention. SunSmart recommends wearing protective clothing, hats, sunscreen, seeking shade from the sun and wearing UV protective eyeware. SunSmart also provides recommendations on how to choose the right protective products. Regular skin checks are another important preventive step.\n\nThe standard for skin cancer screening in Australia is self-assessment and occasional clinic screening. Consistent with Australian guidelines for public skin cancer screening, Cancer Council Australia does not recommend annual or regular skin assessment because low national melanoma occurrences do not financially justify national public screening. A 2003 study analysing the effectiveness of skin cancer screening was implemented in northern Germany. Study doctors received 8 hours of training and over a year screened 19% of Schleswig-Holstein’s population. Initially melanoma detection increased 34%. After 5 years the population experienced over 50% decrease in melanoma mortality.\n\nAustralia spends more than $2 billion annually treating cancer, with skin cancer the most costly. From 2005-15 over $300 million was spent annually on diagnosis, treatment and pathology-related costs of skin cancer. With $512.3 million spent in 2010 on melanoma carcinomas, costs continued to rise. The number of life-years lost and loss of productivity both have a bearing on the financial cost of the disease. The direct and indirect expenses of skin cancer in the Australian state of New South Wales (NSW) have been investigated. Direct costs included those resources related to skin cancer management, and indirect costs were concerned with premature mortality and morbidity. Review of 150 000 skin cancer patients in NSW (2010) revealed a total lifetime cost estimation of AU$536 million, with 72% of this cost related to direct costs, and the remaining 28% to indirect costs; the direct costs were higher in females and indirect costs higher in males. Prevention initiatives make up an important part of financial expenditure for skin cancer funding and investments. Prevention programs are a productive tool, as they beneficially influence attitudes and behaviours towards skin cancer, but also deliver positive financial returns. A 2009 study analysed the economic impact of prevention programs such as SunSmart. SunSmart demonstrated positive health impacts, was cost effective. The Victoria-based program returned $3.60 for every $1 invested. It concluded that SunSmart prevented 103,000 skin cancers and more than 1,000 related fatalities from 1988-2003. The study found strong evidence that continued investment and support for SunSmart was economically sound and presented beneficial outcomes for Australia.\n\n"}
{"id": "438578", "url": "https://en.wikipedia.org/wiki?curid=438578", "title": "Smoking fetishism", "text": "Smoking fetishism\n\nSmoking fetishism (also known as capnolagnia) is a sexual fetish based on the pulmonary consumption (smoking) of tobacco, most often via cigarettes, cigars, and also, pipes and hookahs to some extent. As a fetish, its mechanisms regard sexual arousal from the observation or imagination of a person smoking, sometimes including oneself.\n\nCapnolagnia is not considered a disease but an unusual sexual practice, and many of the fetishists do not seek medical help unless this behavior strongly interferes with their daily lives. The majority of people simply learn to accept their fetish and manage to achieve gratification in an appropriate manner.\n\nA 2003 study found that the fetish was not previously the subject of academic study but had been mentioned in \"a few newspapers\".\n\nLike any fetish, the causes and mechanisms of a smoking fetish vary widely, with roots of sexual association in early childhood and adolescence. Typical causes and hypotheses include:\n\n\nThe diagnostic criteria for fetishism are:\n\nPeople who experience one or more of the symptoms below are considered to have a smoking fetish:\n\n"}
{"id": "57682344", "url": "https://en.wikipedia.org/wiki?curid=57682344", "title": "Spinal Cord Independence Measure", "text": "Spinal Cord Independence Measure\n\nOutcome measures in rehabilitation medicine, are tools used to evaluate the level of disability. They could be beneficial for physicians to judge the path of patient's recovery, for researchers to compare different management protocols and for politicians in order to find the cost-effectiveness of their decisions.\n\nAs an outcome measure specifically designed for spinal cord injury, the Spinal Cord Independence Measure is a tool that evaluates how much safe, cheap and independent, a patient can do basic activities of daily living.\n\nThe measure consists of 19 items categorized in three subscales:self care, respiration and sphincter management, and mobility.\n\nIts last version, SCIM III has been validated in many multicenter trials and translated into Italian, Spanish, Greek, Portuguese, Thai, Turkish and Persian languages. It has been concluded that SCIM III has the most appropriate psychometric properties for measuring functional level of spinal cord injured individuals.\n"}
{"id": "40461658", "url": "https://en.wikipedia.org/wiki?curid=40461658", "title": "Sporting Memories Network", "text": "Sporting Memories Network\n\nSporting Memories Network CIC is a UK registered social enterprise that runs community-wide sports reminiscence projects. These projects (provided by the \"Sporting Memories Network CIC\") introduce the use of archival sports images, reports, and memorabilia. This is done to engage older people in both stimulating conversation and reminiscence, thus promoting mental and physical well-being. It works in conjunction with a registered charity - The Sporting Memories Foundation that trains and supports volunteers to deliver sports reminiscence in England and Wales\n\nFounded in 2011, Sporting Memories Network Community Interest Company (CIC) ran an initial pilot project with fifteen care homes in Leeds, to test and refine their approach. The project, funded by Skills for Care, was evaluated by Dr. Michael Clark, Research Programme Manager, Personal Social Services Research Unit, London School of Economics. The project tested the effectiveness of training care staff and sought to engage relatives, volunteers, and more able residents in facilitating group activities. After receiving background training, each home was supplied with archival images and a training manual. They also received a weekly sports reminiscence newspaper called \"The Sporting Pink.\" In his evaluation report, Dr. Clark wrote: \"The Sporting Memories work is appealing to people (staff and residents) and draws out enthusiasm and personal information that would otherwise have been dormant\".\n\nEnthusiasm for an idea is important if it is to be widely used and effective. Training people in approaches which they do not believe in means that either the intervention will not be used, or will be used unenthusiastically. Consequently it is unlikely to benefit anyone. By tapping into widespread enthusiasm for, and connections with, sports, \"Sporting Memories\" can be inspirational both to care home staff and to residents. In addition, the training can be readily passed on, and used in a flexible, creative and sustainable manner. The materials can be used many times and adapted to the interests of residents over time. Those who had experienced it, reported that they would continue to use the work in their homes.\n\nThe network also works with professional sports clubs by raising awareness of dementia through scheduled league matches which are designated Memories Games. This aspect of the Sporting Memories Network work was acknowledged in the first annual report on the progress of the Prime Minister's challenge on dementia and their work with Everton Football Club was published as an example of best practice in the Alzheimer's Society 2013 report on Creating Dementia-Friendly Communities \n\nThe majority of the group activities utilize reminiscence therapy which is widely recognized as having a beneficial effect on people living with dementia, memory problems or depression. As identified in a study by Tolson et al. (2012), the use of football as a subject may be more attractive to older men living with dementia than traditional topics covered in reminiscence sessions or group activities. In the Journal of Dementia Care, North Berwick Day Centre manager Carol Wicker reported: \"A lot of men attend our day center and most of the staff and volunteers here are women. We try to find ways of engaging with men but it can be difficult to interact meaningfully and get to know people.\" The Sporting Memories Network has developed activities around many different sports, including Football, Cricket, Rugby, Golf, Tennis, and Motorsport.\n\nThere are community projects run by the network in England, Wales, and Scotland which are volunteer led. Premier League, Football League, Super league, and County Cricket Clubs are involved in hosting and running some of the groups. Other venues include libraries, museums, social clubs, and pubs.\n\nThe network has gained the support of current and former sports stars including: David Coulthard, Ross Brawn and Nico Rosberg from Formula One; and footballers Robbie Savage, Chris Kamara and Nigel Martyn. Sporting bodies, such as the Professional Footballers' Association, and the British Racing Drivers' Club, are also supporting the work.\n\nEach supporter has appeared on the network's website and shared their own favourite memories contributing to the 'memory bank' on the company's Replay Websites. In turn, these are used by group facilitators.\n\nOn 3 June 2014, the use of sporting memories was featured by BBC Radio 4 All in the Mind. Presented by Claudia Hammond, the programme interviewed gentlemen participating in a weekly sporting memories group in East Lothian. The gentlemen spoke of their experiences in the group, their own health issues, and the impact the group has had on them.Link It also featured on the lunchtime interview on BBC Test Match Special, alongside former South African cricketer Mike Procter, during the second day of the first test match England v India on 10 July 2014 - Link\n\nAwards\n\nOn 20 May 2014, Sporting Memories Network CIC was presented the award for \"Best National Dementia Friendly Initiative\" by Secretary of State for Health, Jeremy Hunt.\n\nOn 7 September 2014, Sporting Memories Network CIC appeared in the list of Britain's Top 50 New Radicals. The list was compiled by Nesta and \"The Observer\" newspaper and featured organisations or individuals using innovative approaches to tackle social challenges.\n\nOn 6 November 2014, Sporting Memories Network CIC was voted \"Best Football Community Scheme\" at the Football Business Awards. The shortlist consisted of Football Community Trusts from the following football clubs: Charlton Athletic, Chelsea Football Club, Crystal Palace, Everton, Fulham FC, Liverpool FC, Sunderland AFC, West Bromwich Albion, and Sporting Memories Network CIC\n\nSporting Memories Network has raised awareness of dementia at a number of professional football league, premier league, and super league clubs. Memories games to unite fans who share their own favorite memories of supporting their club. The games engage current and former players and club staff in the build-up to the games. These are usually scheduled league matches. Information about dementia and memory problems is made available to fans through various channels. The first memory game to raise awareness of dementia took place at Huddersfield Town.\n\nMemories games have taken place at:\n\n\n"}
{"id": "7480300", "url": "https://en.wikipedia.org/wiki?curid=7480300", "title": "Tecoatl", "text": "Tecoatl\n\nA tecoatl (plural tecoatles) is a stone canal making up part of an extensive ancient aqueduct network in the Tehuacán Valley in the state of Puebla in Mexico. The word \"tecoatl\" translates to \"stone snake\" in the Aztec language Nahuatl, but the canal system is far older than the Aztecs. The first segments of the system were laid down approximately 2500 years ago. \n\nArchaeological investigation of the tecoatl system began in the late 1960s, and further studies revealed that the tecoatles make up the longest prehistoric irrigation system in the New World. At one point 1200 kilometers of stone canals provided water to 330 km² of cultivated land in the Tehuacán Valley. \n\nThe tecoatles started out as simple channels dug in the earth and banked with small levees. They were carefully planned, sloping from high ground to low on a tortuous course that sloped less than two degrees at all times. The canal meandered amongst the fields, providing water from uphill mineral springs to crop fields during Southern Mexico's long dry season, which can last longer than six months. Without irrigation, agriculture is not possible even in the fertile hills and plains of the region.\n\nThe Tehuacán Valley is rich with mineral springs, and these water sources were eagerly tapped for farming by early residents of the area. The water in these springs is high in dissolved calcium carbonate in the form of calcite. As the water ran through the canals, calcite was slowly but steadily deposited on their walls, forming a hard, stony layer. Through concentration and evaporation calcite crystallized on the earthen surface of the canals, forming a leakproof shell of travertine. The travertine coating grew in thickness at a rate of about one centimeter per year.\n\nAt this rate, the canals began to grow in height. As each canal became shallower due to the travertine accumulation in its bed, water overflowing its levees deposited more travertine along the sides, building up the walls and effectively containing itself. When water did overflow and wash down the sides of the canal, it evaporated and deposited minerals along the base. Eventually, what was once a ditch in the soil became a tall stone aqueduct with a very wide base. The largest were five meters high and 30 meters wide, with water still flowing efficiently through the channel along the top. The farmers probably helped shape the canal, removing travertine buildup as needed, but for the most part each canal became a large winding calcareous aqueduct on its own, and earning it the appellation \"stone snake\".\n\nNearby aqueducts were built to channel river water as well, but these did not become tecoatles, because the water they bore was much lower in dissolved minerals than the water from the springs. The tecoatles are essentially fossilized structures. As the travertine crystallized on the walls of the tecoatles, it trapped biological material, such as small plants and algaes, as well as pollen from the crop plants grown alongside the water system. This material yields information about the composition of the water, the flow rate in the aqueducts, and the wild and cultivated flora of the area. The farmers grew maize, tomatoes, and peppers, and probably used wild cattails for a variety of purposes. Also, biological material trapped in the stone can be used for radiocarbon dating of the canals; the first ones were dug as early as 800 BC, and water ran through them as recently as the sixteenth century AD.\n\nCaran, S. Christopher and James A. Neely. (2006). Hydraulic engineering in prehistoric Mexico. \"Scientific American\" 295:4.\n\n"}
{"id": "4694327", "url": "https://en.wikipedia.org/wiki?curid=4694327", "title": "Third-party administrator", "text": "Third-party administrator\n\nA third-party administrator (TPA) is an organization that processes insurance claims or certain aspects of employee benefit plans for a separate entity. It is also a term used to define organizations within the insurance industry which administer other services such as underwriting, customer service. This can be viewed as outsourcing the administration of the claims processing, since the TPA is performing a task traditionally handled by the company providing the insurance or the company itself. Often, in the case of insurance claims, a TPA handles the claims processing for an employer that self-insures its employees. Thus, the employer is acting as an insurance company and underwrites the risk. The risk of loss remains with the employer, and not with the TPA. An insurance company may also use a TPA to manage its claims processing, provider networks, utilization review, or membership functions. While some third-party administrators may operate as units of insurance companies, they are often independent.\n\nThird-party administrators also handle many aspects of other employee benefit plans such as the processing of retirement plans and flexible spending accounts. Many employee benefit plans have highly technical aspects and difficult administration that can make using a specialized entity such as a TPA more cost effective than doing the same processing in house.\n\nThird-party administrators are prominent players in the health care industry and have the expertise and capability to administer all or a portion of the claims process. They are normally contracted by a health insurer or self-insuring companies to administer services, including claims administration, premium collection, enrollment and other administrative activities. A hospital or provider organization desiring to set up its own health plan will often outsource certain responsibilities to a third-party administrator.\n\nFor example, an employer may choose to help finance the health care costs of its employees by contracting with a TPA to administer many aspects of a self-funded health care plan.\n\nThis term is also now commonly used in commercial general liability (CGL) policies or so called \"casualty\" business. In these instances, the liability policies are written with a large (in excess of $50,000) self-insured retention (SIR) that operates somewhat like a deductible, but rather than being paid at the end of a claim (when a loss payment is made to a claimant), the money is paid up front by the insured for costs, expenses, attorney fees etc. as the claim moves forward. If there is a settlement or verdict within the SIR, then that is also paid by the insured up to the limit of the SIR, before the insurer steps in and pays its portion. The TPA acts like a claims adjuster for the insurance company and sometimes works in conjunction with the inside insurance company claims adjuster or an outside claims investigator as well as the defense counsel. The defense counsel in some situations is selected by the TPA. The point is that the larger the SIR, the more responsibility the TPA has over the control of the way the claim is handled and ultimately resolved. Some self-insured retentions are in the millions of dollars and the TPAs are large multinational non-insurance entities that handle all the claims. In contrast, some self-insureds choose not to outsource claims handling to a TPA, preferring instead to handle all claims in house. This is known as self-administration.\n\nRetirement plans such as a 401(k) are often partly managed by an investment company. Instead of handling all the plan contributions by employees, distributions to employees, and other aspects of plan processing, the investment company may contract with a third-party administrator to handle much of the administrative work and only handle the remaining investment work.\n\n"}
{"id": "56413297", "url": "https://en.wikipedia.org/wiki?curid=56413297", "title": "TranSMART", "text": "TranSMART\n\ntranSMART is an open-source data warehouse designed to store large amounts of clinical data from clinical trials, as well as data from basic research, so that it can be interrogated together for translational research. It is also designed to be used by many people, across organizations. It was developed by Johnson & Johnson, in partnership with Recombinant Data Corporation. The platform was released in Jan 2012 and has been governed by the tranSMART Foundation since its initiation in 2013.\n\nThe tranSMART platform has been adopted and evaluated by numerous pharmaceutical companies, not-for-profits and patient advocacy groups, academics, governmental organisations and service providers. At the Bio-IT World industry conference both the Innovative Medicines Initiative's U-BIOPRED project and The Michael J. Fox Foundation were awarded a Best Practices Award for their application of the platform.\n\ntranSMART is built on top of the i2b2 clinical data warehouse and leverages the i2b2 star schema for modelling clinical and low-dimensional data. High-dimensional omics data is stored in dedicated tables where each of the data types (e.g., gene expression, SNP or metabolomics) retains its specific data structure. Both the Oracle and PostgreSQL database management systems are supported for its data storage.\n\n\n"}
{"id": "24959380", "url": "https://en.wikipedia.org/wiki?curid=24959380", "title": "Trial of labour", "text": "Trial of labour\n\nIn obstetrics a trial of labour is the conduction of spontaneous labour in a moderate degree of cephalopelvic disproportion. It is performed under close observation by an obstetrician in order to assess a woman's chances of a successful vaginal birth. The physician may allow labor to continue against contraindications during birth or even stimulate labor with oxytocin when pelvic measurements are borderline to see if the fetal head will descend making vaginal delivery possible; if progressive changes in dilation and station do not occur, a cesarean delivery is performed.\n"}
{"id": "46283223", "url": "https://en.wikipedia.org/wiki?curid=46283223", "title": "Tubal factor infertility", "text": "Tubal factor infertility\n\nTubal factor infertility (TFI) is female infertility caused by diseases, obstructions, damage, scarring, congenital malformations or other factors which impede the descent of a fertilized or unfertilized ovum into the uterus through the Fallopian tubes and prevents a normal pregnancy and full term birth. Tubal factors cause 25-30% of infertility cases. Tubal factor is one complication of Chlamydia trachomatis infection in women.\n\nSexually transmitted Chlamydia and genital mycoplasma infections are preventable causes of infertility and negative pregnancy outcomes. When the infections progress and ascend, they can result in TFI. Infertility can have multiple possible causes and may not be recognized for years after a gonorrhea, Chlamydia or \"Mycoplasma\" infection has caused tubal damage, as the affected woman may not have attempted to become pregnant until years later.\n\nInfertility is the major symptom of TFI and is generally defined as a woman under 35 who has not become pregnant after 12 months without the use of contraception. Twelve months is the lower reference limit for \"Time to Pregnancy\" (TTP) by the World Health Organization. When the inability to conceive is accompanied by signs and symptoms of pelvic inflammatory disease such as lower abdominal pain, TFI may be present. A history of pelvic inflammatory disease, the laproscopic evidence of scarring and a diagnosis of salpingitis supports the diagnosis.\n\nTubal factor infertility can be due to Chlamydia infection and testing for Chlamydia antibodies is one diagnostic tool. A \"Mycoplasma genitalium\" infection has also been linked to TFI. Women have difficulty getting pregnant or carrying a baby to term due to the buildup of scar tissue in the Fallopian tubes causing damage to the cilia on the epithelial cells. TFI can also be due to endometriosis.\n\nThe physician will obtain a medical history and evaluate for tubal obstructions and infections. Obstruction can occur anywhere along the length of the tube. It can be partially or completely blocked. The extent of obstruction is typically assessed using hysterosalpingogram (HSG). Some use laparoscopy to establish the extent of the disease. Pelvic adhesions can be visualized, if present.\n\nDistal tubal obstruction is more often observed (70%) than proximal obstruction. It can be caused by hydrosalpinges, pelvic adhesions, or fusion of the fimbriae. Tubal obstruction is caused by infection, endometriosis, myomas, salpingitis isthmica nodosa (SIN), or dried mucus. Because the tube can spasm during the injection of the dye during a hysterosalpingogram, it can be misdiagnosed.\n\nIf tubal factor infertility is suspected to be the cause of the infertility treatment begins with or without confirmation of infection because of complications that may result from delayed treatment. Appropriate treatment depends on the infectious agent and utilizes antibiotic therapy. Treating the sexual partner for possible STIs helps in treatment and prevents reinfection.\nAntibiotic administration affects the short or long-term major outcome of women with mild or moderate disease.\n\nFor women with infections of mild to moderate severity, parenteral and oral therapies are prescribed . Typical antibiotics used are cefoxitin or cefotetan plus doxycycline, and clindamycin plus gentamicin. An alternative parenteral regimen is ampicillin/sulbactam plus doxycycline. Once infection has been eliminated, surgery may be successful in opening the lumen of the fallopian tubes to allow a successful pregnancy and birth.\n\n"}
{"id": "36177264", "url": "https://en.wikipedia.org/wiki?curid=36177264", "title": "Valley of Three Ponds", "text": "Valley of Three Ponds\n\nValley of Three Ponds ) is a large park and wooden area in the southern part of the city of Katowice, Poland. The name comes from the existence of three large ponds in the park area.\n"}
{"id": "1995890", "url": "https://en.wikipedia.org/wiki?curid=1995890", "title": "Wat (food)", "text": "Wat (food)\n\nWat, we̠t’, wot (, ) or tsebhi (, ) is an Ethiopian and Eritrean stew or curry that may be prepared with chicken, beef, lamb, a variety of vegetables, spice mixtures such as \"berbere\", and \"niter kibbeh\", a seasoned clarified butter.\n\nSeveral properties distinguish wats from stews of other cultures. Perhaps the most obvious is an unusual cooking technique: the preparation of a wat begins with chopped onions slow cooked, without any fat or oil, in a dry skillet or pot until much of their moisture has been driven away. Fat (usually \"niter kibbeh\") is then added, often in quantities that might seem excessive by modern Western standards, and the onions and other aromatics are sautéed before the addition of other ingredients. This method causes the onions to break down and thicken the stew.\n\nWat is traditionally eaten with injera, a spongy flat bread made from the millet-like grain known as teff. There are many types of wats. The popular ones are doro wat and siga wat, (Ge'ez: ሥጋ \"śigā\") made with beef.\n\nDoro wat is one such stew, made from chicken and sometimes hard-boiled eggs; the ethnologist Donald Levine records that doro wat ( \"dōrō we̠t’\", \"derhō tsebhi\") is the most popular traditional food in Ethiopia, often eaten as part of a group who share a communal bowl and basket of injera.\n\n"}
{"id": "34865122", "url": "https://en.wikipedia.org/wiki?curid=34865122", "title": "Werner Schuster (politician)", "text": "Werner Schuster (politician)\n\nRudolf Werner Schuster (20 January 1939 – 9 May 2001) was a Tanganyika-born German physician, specialist in health informatics, and SPD politician.\n\nRaised in a colonial East African household, Schuster studied medicine at the University of Tübingen in the 1960s and during his professional career worked mostly in health informatics and medical computer science. He was a city councillor at Idstein for seventeen years before serving as a member of the Bundestag from 1990 until his death. On the national stage he concerned himself with health and Africa and the fight against AIDS.\n\nSchuster was born in 1939 at Moshi in the Kilimanjaro Region of the Tanganyika Territory, now part of Tanzania, into a family of German settlers. He spent much of his childhood there before migrating to Germany with his parents, and all his life was able to speak Swahili.\n\nIn 1958 he gained his \"Abitur\" at the Gymnasium in Rosenheim, Upper Bavaria, and from 1959 to 1960 did his compulsory military service in the Mountain Infantry of the Bundeswehr. He then proceeded to Tübingen to study medicine. In 1966 he took the \"Staatsexamen\", and the same year received his doctorate from the Physiological Institute in Tübingen.\n\nIn 1970 Schuster gained his \"Approbation\", or license to practice, at St Joseph's Hospital in Bremerhaven on the coast of the North Sea. From 1970 to 1983 he was head of the health department in the Hesse Centre for Data Processing at Wiesbaden. In 1971 he joined the Medical Emergency Service agency in Wiesbaden. In 1983 he received the medical computer science certificate of the \"Gesellschaft für medizinische Informatik und Statistik\" (Society for Medical Informatics and Statistics). From 1984, he worked in the municipal data processing centre at Giessen.\n\nIn 1964, Schuster had joined the SPD. From 1972 to 1989 he was a city councillor at Idstein in the Taunus mountains and from 1975 to 1985 was leader of his political group. From 1985 to 1995 he was chairman of the SPD's Rheingau-Taunus-Kreis branch.\n\nIn 1985 he founded the \"Bürgerpartnerschaft Dritte Welt Idstein e.V.\" (Third World Civic Partnership Association Idstein) and\nlater established a civic partnership between Idstein and his native Moshi. This association, later renamed \"Bürgerpartnerschaft Eine Welt e.V., Idstein / People Help People - One World\", had the aim of achieving concrete development projects in Tanzania, focussing on the area of Moshi, at the foot of Mount Kilimanjaro. In Moshi itself, Schuster initiated the founding of a non-governmental organization called \"Friends in Development Association\". This sought to link its efforts into those of regional and national German NGOs.\n\nIn 1989 Schuster was elected to his local \"Kreistag\", and at the federal election of 1990 he was sent to the Bundestag, representing Rheingau-Taunus/Limburg-Weilburg. As a member of parliament Schuster was chiefly concerned with development and with health policy and took a particular interest in Africa and the fight against AIDS. He called for ten per cent of the gross national product of rich countries to be spent on combatting poverty in the Third World.\n\nIn a debate in the \"Bundestag\" on 21 June 1991 on the situation in the Sudan, Schuster said that the bombing of United Nations and Red Cross supply depots by government forces in the south of the country was \"completely perverse\", and his position was supported on all sides of the chamber. After a trip to Rwanda in 1993, Schuster unsuccessfully called upon the German federal government led by Helmut Kohl to offer financial support for an enlarged United Nations peace-keeping force there. The next year, some eight hundred thousand people died in the Rwandan genocide. In matters of development policy Schuster was respected across party boundaries. He was closely associated with Heidemarie Wieczorek-Zeul, who in 1998 became federal minister for economic cooperation and development under Gerhard Schröder, the new SPD Chancellor.\n\nIn December 1995, following the NATO bombing campaign in Bosnia, Schuster was one of only fifty-five SPD members of parliament who voted against sending four thousand German soldiers to join the IFOR international peacekeeping force in Bosnia, with the \"Bundestag\" approving the deployment by 543 votes to 107, with six abstentions and sixteen members absent.\n\nHe was married with three children. He died in Idstein on 9 May 2001, suffering from cancer of the liver.\n\nIn May 2003, on the initiative of the Bonn City Council, and in the presence of Heidemarie Wieczorek-Zeul and members of the Schuster family, a building at 201, Kaiserstrasse, was renamed the \"Dr. Werner Schuster Haus\". It contains the offices of several NGOs with development objectives. Reinhard Hermle, chairman of VENRO (the Association of German Development NGOs) stated \n\n\n"}
