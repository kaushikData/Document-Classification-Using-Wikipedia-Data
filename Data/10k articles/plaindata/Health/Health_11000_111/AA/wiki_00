{"id": "16948635", "url": "https://en.wikipedia.org/wiki?curid=16948635", "title": "1826–1837 cholera pandemic", "text": "1826–1837 cholera pandemic\n\nThe second cholera pandemic (1826–1837), also known as the Asiatic Cholera Pandemic, was a cholera pandemic that reached from India across western Asia to Europe, Great Britain and the Americas, as well as east to China and Japan. Cholera caused more deaths, more quickly, than any other epidemic disease in the 19th century. The medical community now believes cholera to be exclusively a human disease, spread through many means of travel during at the time, and spread through warm fecal-contaminated river waters and contaminated foods. During the second pandemic, the scientific community varied in its beliefs about the causes of cholera.\n\nThe first cholera pandemic (1817–24) began near Calcutta and spread throughout Southeast Asia to the Middle East, eastern Africa and the Mediterranean coast. While cholera had spread across India many times previously, this outbreak went further; it reached as far as China and the Mediterranean Sea before receding. Hundreds of thousands of people died as a result of this pandemic, including many British soldiers, which attracted European attention. This was the first of several cholera pandemics to sweep through Asia and Europe during the 19th and 20th centuries. This first pandemic spread over an unprecedented range of territory, affecting almost every country in Asia.\n\nHistorians believe that the first pandemic had lingered in Indonesia and the Philippines in 1830.\n\nAlthough not much is known about the journey of the cholera pandemic in east India, many believe that this pandemic began, like the first, with outbreaks along the Ganges Delta in India. From there the disease spread along trade routes to cover most of India. By 1828 the disease had traveled to China. Cholera was also reported in China in 1826 and 1835, and in Japan in 1831. In 1829, Iran was apparently infected with cholera from Afghanistan.\n\nCholera reached the southern tips of the Ural Mountains in 1829. On 26 August 1829 the first cholera case was recorded in Orenburg with reports of outbreaks in Bugulma (7 November), Buguruslan (5 December), Menselinsk (2 January 1830) and Belebeevsk (6 January). With 3500 cases including 865 fatal ones in Orenburg province, the epidemic stopped by February 1830. It swept across Europe for the first time during the second pandemic and reached as far west as the Caspian Sea.\n\nThe second cholera pandemic spread from India to Russia and then to the rest of Europe, claiming hundreds of thousands of lives.\n\nThe cholera epidemic struck Warsaw during the Polish–Russian War 1830–31 between 16 May and 20 August 1831; 4,734 people fell ill and 2,524 died. The epidemic of cholera brought to Poland and East Prussia by Russian soldiers forced Prussian authorities to close their borders to Russian transports. \"Cholera Riots\" occurred in Russia, caused by the anti-cholera measures undertaken by the tsarist government.\n\nBy early 1831, frequent reports of the spread of the pandemic in Russia prompted the British government to issue quarantine orders for ships sailing from Russia to British ports. By late summer, with the disease appearing more likely to spread to Britain, its Board of Health, in accordance with the prevailing miasma theory, issued orders recommending as a preventive the burning of \"decayed articles, such as rags, cordage, papers, old clothes, hangings...filth of every description removed, clothing and furniture should be submitted to copious effusions of water, and boiled in a strong ley; drains and privies thoroughly cleansed by streams of water and chloride of lime...free and continued admission of fresh air to all parts of the house and furniture should be enjoined for at least a week\".\n\nBased on the reports of two English doctors who had observed the epidemic in St. Petersburg, the Board of Health published a detailed description of the disease's symptoms and onset:\n\nThe epidemic reached Great Britain in December 1831: appearing in Sunderland, where it was carried by passengers on a ship from the Baltic. It also appeared in Gateshead and Newcastle. In London, the disease claimed 6,536 victims; in Paris, 20,000 died (out of a population of 650,000), with about 100,000 deaths in all of France. In 1832 the epidemic reached Quebec, Ontario, and Nova Scotia, Canada; and Detroit and New York City in the United States. It reached the Pacific coast of North America between 1832 and 1834. The pandemic prompted the passage of the landmark Public Health Act and the Nuisances Removal Act in 1848 in England.\n\nIn mid-1832, 57 Irish immigrants died who had been laying a stretch of railroad called Duffy's Cut, 30 miles west of Philadelphia. They had all contracted cholera.\n\nThe third cholera pandemic (1846–60) was the third major outbreak of cholera originating in India in the nineteenth century that reached far beyond its borders, which researchers at UCLA believe may have started as early as 1837 and lasted until 1863. In Russia, more than one million people died of cholera. In 1853–54, the epidemic in London claimed over 10,000 lives, and there were 23,000 deaths for all of Great Britain. This pandemic was considered to have the highest fatalities of the 19th-century epidemics.\n\nLike the earlier pandemics, cholera spread from the Ganges delta of India. It had high fatalities among populations in Asia, Europe, Africa and North America. In 1854, which was considered the worst year, 23,000 people died in Great Britain.\n\nDuring the second pandemic, the scientific community varied in its beliefs about the causes of cholera. In France doctors believed cholera was associated with the poverty of certain communities or poor environment. Russians believed the disease was contagious, although doctors did not understand how it spread. The United States believed that cholera was brought by recent immigrants, specifically the Irish, and epidemiologists understand they were carrying disease from British ports. Lastly, some British thought the disease might rise from divine intervention.\n\nNorwegian poet Henrik Wergeland wrote a stage-play inspired by the pandemic, which had reached Norway. In \"The Indian Cholera\", he criticized British colonialism for spreading the pandemic.\n\nAs a result of the epidemic, the medical community developed a major advance, the intravenous saline drip. It was developed from the work of Dr Thomas Latta of Leith, near Edinburgh. Latta established from blood studies that a saline drip greatly improved the condition of patients and saved many lives by preventing dehydration. But, he was one of the many medical personnel who died in the epidemic.\n\n"}
{"id": "37208433", "url": "https://en.wikipedia.org/wiki?curid=37208433", "title": "2012 outbreak of Salmonella", "text": "2012 outbreak of Salmonella\n\nIn general, the United States alone experiences 1 million cases of salmonellosis per year. In Europe, although there are around 100,000 incidents of salmonellosis reported annually, there has been a steady decrease in cases over the past four years. The exact number of those infected is impossible to know as not all cases are reported. Of these reported cases, some can be classified as foodborne disease outbreaks by the Center for Disease Control and Prevention (CDC) if \"two or more people get the same illness from the same contaminated food or drink\" or zoonotic outbreaks if \"two or more people get the same illness from the same pet or other animal\". In 2012, the various strains or serotypes of the \"Salmonella\" bacteria, related to the outbreaks in the United States, infected over 1800 people and killed seven. In Europe, the European Centre for Disease Prevention and Control (ECDC) reported 91,034 cases of \"Salmonella\" infection with 65,317 cases related to the 2012 outbreaks. Of those 65,317 cases, there were 61 deaths.\n\n\"Salmonella\" bacteria can be found in almost any product or animal that has been exposed to fecal matter. These exposures can occur from crops grown from waste-based fertilizers or from food items handled by infected humans. Salmonellosis is an intestinal disease, meaning that the bacteria must be ingested and processed through the intestines in order for infection to occur. Thus, salmonellosis is commonly spread to humans through ingestion of contaminated food items. It can also be spread through contact with reptiles and birds, usually after the person handles the animal or its environment (without hand-washing immediately) and then touches their mouth or food items. Those infected usually develop symptoms anywhere from 12–72 hours after first contact with \"Salmonella\" bacteria, and most do not require serious medical attention. This salmonellosis displays itself in humans with fever, abdominal pain, nausea, and, most commonly, diarrhea for a period of up to 7 days. Those requiring hospitalization usually are dehydrated or have extreme diarrhea, which can turn deadly, especially if the salmonella bacteria reaches the bloodstream. The elderly, young children, and those with weakened immune systems are most at risk for developing salmonellosis and suffering severe reactions. The most common serotypes of \"Salmonella\" in the United States and Europe are Enteritidis and Typhimurium.\n\nThe 2012 outbreak did not have one start and end date due to the multivariate origination sites and stages of investigation. Each outbreak followed its own pattern of contamination, spread, infection, and containment throughout the course of 2012. Worldwide, there were 15 different foodborne and zoonotic origins of the \"Salmonella\" outbreaks. Eighteen of the over 2,300 strains of \"Salmonella\" were found in infected humans and contaminated products in Europe and the U.S. As with all diseases, there were certain places and serotypes that contributed more to the magnitude of the 2012 outbreak. As such, the origins listed below had the greatest frequency of occurrence and overall impact on society.\n\n\"S\". Typhimurium has traditionally been an uncommon serotype of \"Salmonella\"; however, beginning in January 2012 through the end of 2012, the number of cases steadily rose, with 18 human cases in the United States alone (spread among eight states). The infection caused one death and four hospitalizations. These infections stem from contact with pet hedgehogs or with the animals' surroundings. No one pet provider was linked to the infected hedgehogs.\n\nOne epidemic of \"Salmonella enterica\" I 4,5,12:i:- in the United States in February 2012 affected 46 people across 22 states. The outbreak seemed to be linked to the handling of live or frozen feeder mice and rats for reptile and amphibian pets. A similar outbreak occurred in the United States and United Kingdom in 2009 and 2010 from the same two breeders implicated in this 2012 occurrence.\nAlso, more than a third of those infected were young children, highlighting their propensity for infection.\n\nThe outbreaks in 2012 that occurred due to contact with live poultry were of five different serotypes of \"Salmonella\" bacteria originating in three distinct locations. The first infections were reported in February 2012. Spanning 23 states, there were 93 humans infected with the Montevideo serotype of \"Salmonella\". All were infected with this strain from contact with baby ducklings or chicks from the Estes Hatchery located in Springfield, Missouri. This outbreak resulted in one casualty. \nOne month later (March 2012), 46 people in the United States were infected with \"Salmonella\" Hadar through contact with live poultry. There were no deaths, and the infected humans were located in 11 different states. The specific hatchery name is withheld, but it was concluded by the CDC that this strain of \"Salmonella\" originated in one unnamed hatchery in Idaho.\nIn the same month, one of three strains of \"Salmonella\" – Infantis, Newport, or Lille – were contracted by 195 people from contact with live poultry (whether for purposes of agriculture or pet keeping). Of those infected, two died from infection. This particular outbreak, though stemming from only the Mt. Healthy Hatchery in Ohio, expanded across 27 states.\n\nFirst investigated at the end of March 2012, there were a total of 84 cases of infected humans across 15 states of the United States by November 2012. These people's salmonellosis (either from \"Salmonella\" Sandiego or \"Salmonella\" Newport) stemmed from contact with small turtles or their habitats. Some of these turtles were purchased from street merchants. No one source of the \"Salmonella\" contaminations was identified, and no human deaths occurred. Due to past outbreaks, there is a law in place making it illegal to sell or own a turtle with a shell length less than four inches, as these seem to generally be the sources of most \"Salmonella\" bacteria in small turtles.\n\nFirst reported in April 2012, an outbreak of salmonellosis caused by rarer serotypes, \"Salmonella\" Bareilly and \"Salmonella\" Nchanga, was reported in 28 states, mostly in the Eastern U.S., having caused no deaths, but 425 cases of illness and 55 hospitalizations. This outbreak was linked to the consumption of raw scraped ground tuna product. The source was frozen raw yellowfin tuna product aka Nakaochi Scrape manufactured by Moon Marine USA Corporation. This product was voluntarily recalled after the CDC discovered strains of \"Salmonella\" in the packages. The company eventually recalled another type of canned tuna for fear it may have been contaminated, too. This outbreak was significant in that it was the first salmonellosis case in the United States connected to raw, scraped tuna products, as well as being the first time \"S\". Nchanga was discovered in food products in the United States.\n\nIn July 2012, an outbreak of salmonellosis occurred in the Netherlands and the United States. According to the National Institute for Public Health and the Environment, by the end of 2012, 1060 people in the Netherlands and 100 in the United States contracted salmonellosis from smoked salmon infected with \"Salmonella\" Thompson. On November 2, the RIVM confirmed that there were 4 deaths and over 1060 cases linked to \"S\". Thompson in the Netherlands. The infections were linked to smoked salmon from the manufacturer Foppen, where the contamination had occurred. A recall involving a quarter million customers was undertaken in the United States. The CDC did not classify this as an outbreak because there were not a verified number of people infected from only this source in the U.S.\n\nBecause animals are the main transporters of \"Salmonella\", crops can become infected. This generally occurs because of the use of manure-based fertilizers on farms. Some animals do not appear sick (especially infected poultry); however, they carry the \"Salmonella\" in their intestines and when they defecate, the bacteria spreads to the soil. Sometimes, the sick animals manifest the illness in ways similar to humans with similar consequences. They may have diarrhea, fever, and then die if they are not treated. After their death, the other animals raised with them can become infected through contact with their feces. Therefore, if this infected manure is used as a fertilizer for crops, then the crops will contain trace amounts of infectious \"Salmonella\" bacteria that can spread to humans after the crops are harvested.\n\nPoultry can become infected by living close together in hatcheries, where multiple animals are defecating. Some poultry are hatched to be sold domestically, while others are hatched for production to be consumed later. If live poultry are infected and sold domestically, they can infect other animals who may be around them in a domestic situation e.g. other pets. Because poultry do not show symptoms, infections are usually not obvious until humans or other animals in contact with the poultry become ill.\n\nEast Coast tomatoes tend to have higher rates of \"Salmonella\" infection than West Coast tomatoes. This seems to be due to the microbiome of the East Coast of the United States, where there are far less soil bacteria that destroy \"Salmonella\" versus the microbiome of the West Coast where these bacteria are abundant. Because of the anti-salmonella bacteria limiting its spread, fewer people on the West Coast report cases of salmonellosis. Additionally, street-sold natural tomatoes have been consistently tied to outbreaks of \"Salmonella\". Street-sold (unregulated) items are independently produced and sold to consumers. This does not include fresh produce markets. These strong links to salmonellosis is in due in part do the lack of Food and Drug Administration (FDA) oversight on unregistered vendors. Another cause of \"Salmonella\" bacteria growth may be due to contact with contaminated irrigation water. The tomatoes (or any plant) are much more likely to become infected if the soil in which the roots are located are contaminated or if the actual blossom is contaminated. These two entry points lead to the most likely scenarios in which a healthy tomato plant may become infected with some serotype of \"Salmonella\".\n\nThe various outbreaks of \"Salmonella\" serotypes in the U.S. and abroad began at different points either in late 2011 or early 2012; however, all cases were concluded at the end of 2012. Those who were infected either suffered through the symptoms or expired from \"Salmonella\"s destruction to their immune system. If there was a specific point of origin for an outbreak, a recall of the product helped to decrease more possible exposure. None of the people infected showed symptoms or maintained symptoms into 2013. This biotic disturbance had a major impact on society. It increased awareness about the ease in which bacteria spread among organisms. It helped to prompt researchers to look into ways to prevent crops from becoming contaminated in the field. Most importantly, this disturbance has been occurring with increasing frequency, which seems counter-intuitive because salmonellosis is an easily preventable disease; if simple food safety steps were taken, unnecessary hospitalizations and deaths could have been eliminated in 2012. \n\nIn order to prevent serious outbreaks similar to the one in 2012 in the future, some precautions must be taken. When handling food, one must be careful not to cross-contaminate raw poultry products or other meats with other foods. Anyone handling food should also be sure to wash their hands frequently. All persons should wash their hands after using the restroom as bacteria can survive for long periods of time outside of the body and the bacteria will be spread from the unwashed hands to possible food items or anything that may enter the mouth area. When cooking, one should take care to heat foods to at least 167 °F for a minimum of ten minutes to ensure that the entire product is evenly cooked. This is because heat and ultraviolet radiation are good neutralizers of \"Salmonella\" bacterium.\nRecent outbreaks have occurred, and although not all infections can be eliminated, most can be hindered either through coating crops with \"Salmonella\"-fighting bacteria or through human precautions.\n"}
{"id": "59005581", "url": "https://en.wikipedia.org/wiki?curid=59005581", "title": "Aidan de Brune", "text": "Aidan de Brune\n\nAidan de Brune (17 July 1874 – 15 February 1946), journalist, author, pedestrian was the first person to walk around the perimeter of Australia, unaccompanied and unassisted.\n\nDe Brune was born Herbert Charles Cull in London,England and started his professional life as a printer. He married Ethel Elizabeth Crofts in 1907 and a son, Lionel, was born in 1909.\n\nIn 1910 Cull went to Australia and arrived in Fremantle, Western Australia on 23 May 1910. His wife and child followed him and arrived in Albany, Western Australia on 26 November 1910.\n\nIn 1912 Cull's wife and son returned to England. Cull remained in Australia for the rest of his life. In early 1920 he was working for the \"Bunbury Herald\" newspaper and wrote two serial stories: \"The Pursuits of Mr Peter Pell\" and \"The Mystery of the Nine Stars.\" The latter story was unfinished when, in November 1920, Cull left the newspaper and began to walk from Fremantle to Sydney, following the Trans-Australian Railway. When he reached Sydney, in early 1921, Cull was calling himself Aidan de Brune.\n\nIn September 1921 he began a walk around the perimeter of Australia, from Sydney to Sydney, anticlockwise. De Brune described his goal to be \"to leave Sydney on foot, to walk ten thousand miles (more or less) around Australia, calling at all the ports en route on the four coasts, and to return to Sydney.\" He proposed taking twelve months to complete the walk. However, in the event, he took two and a half years, arriving back in Sydney on 4 March 1924.\n\nDe Brune kept a diary during his walk, in which he made daily entries detailing the distance walked each day and the total distance to-date. He also invited people he met along the way to certify his presence at the location he was at and to make comments. After he finished his walk he donated the diary and a typescript of the contents of the diary to the State Library of New South Wales.\n\nAfter his walk, Aidan de Brune settled in Sydney and began writing serialised mystery stories for newspapers.\n\nHerbert Charles Cull died on 15 February 1946. His death was registered as that of Aidan de Brune. He was buried in Botany Cemetery.\n\n"}
{"id": "37736784", "url": "https://en.wikipedia.org/wiki?curid=37736784", "title": "Boot (medical)", "text": "Boot (medical)\n\nA boot is a medical device worn during treatment and recovery of a variety of foot injuries. Along with orthopedic casts, leg braces, splints and orthotics, it is a form of immobilizing and weight bearing for injuries to the foot area.\n"}
{"id": "44717385", "url": "https://en.wikipedia.org/wiki?curid=44717385", "title": "Boyan Petrov", "text": "Boyan Petrov\n\nBoyan Petrov (, born 7 February 1973 - disappeared May 2018) was a Bulgarian zoologist and mountaineer, working at the National Museum of Natural History in Sofia.\n\nAt the time of his disappearance he had climbed 10 out of the 14 eight-thousanders, all without supplementary oxygen. Up to date, this achievement makes him the Bulgarian altitude climber with the highest number of successful ascents of peaks over 8000 meters. He was the first Bulgarian to summit four of those mountains - Gasherbrum I (2009), Kangchenjunga (2014), K2 (2014) and Manaslu (2015).\n\nOn 20 May 2014 he became the first Bulgarian to climb the third highest peak on Earth Kangchenjunga (8586m), as well as the first diabetic to ascend to such an altitude and without oxygen. On 23 July he climbed Broad Peak (8047m). On 31 July 2014 he became the first Bulgarian to climb the second highest peak on the planet K2 (8611m), which also made him the 35th person to climb three eight thousanders in less than 100 days. His double climb – on Broad Peak and K2 in 8 days, is also a world record. For these remarkable successes the Bulgarian climber was greeted with a video message by mountaineering legend Reinhold Messner. His documentary film about his climbs \"3x8000\" was broadcast on Bulgarian National Television in December 2014. He reached the summit of Mt. Dhaulagiri (8167m) on 29 September 2017 at 01:00 pm without the help of supplementary oxygen.\n\nBoyan Petrov was a cancer survivor and a diabetic. In 2008 he fell while climbing in the Alps and broke his leg. During the descent of Gasherbrum II in 2009 he fell in a glacial crevasse and was saved by a group of Spanish climbers. In 2013 Petrov suffered another leg fracture due to a car accident. Despite his injuries, in the following year he achieved a hat-trick by successfully climbing Kangchenjunga, Broad Peak and K2 with internal fixators in this leg. After the descent of K2, he suffered a serious hypoglycemic crisis while in base camp, lost consciousness and was able to recover with the help of Polish climbers.\n\nPetrov was declared missing on 5 May 2018 while climbing Shishapangma. The search for him was discontinued on 16 May. According to other climbers, he may have fallen into a crevasse on the way to the top.\n\n"}
{"id": "2115251", "url": "https://en.wikipedia.org/wiki?curid=2115251", "title": "Calcium carbimide", "text": "Calcium carbimide\n\nCalcium carbimide, sold as the citrate salt under the trade name Temposil, is a disulfiram-like drug. Its effects are similar to the drug disulfiram (Antabuse) in that it interferes with the normal metabolism of alcohol by preventing the breakdown of the metabolic byproduct acetaldehyde. The result is that when alcohol is consumed by users of calcium carbimide, they experience severe reactions which include symptoms such as sweating, difficulty breathing, rapid heartbeat, rash, nausea and vomiting, and headache.\n\nA recent 9-year study found that incorporation of supervised carbimide and the similar drug, disulfiram, into a comprehensive treatment program resulted in an abstinence rate of over 50%.\n\nTemposil was developed by Drs. Ken Ferguson and Gordon Bell, who tested the drug on themselves. It was patented in 1955 by the Alcoholism Research Foundation of Ontario.\n\n"}
{"id": "33610820", "url": "https://en.wikipedia.org/wiki?curid=33610820", "title": "Calix Society", "text": "Calix Society\n\nThe Calix Society is an organization in the United States founded in the 1940s which aims at addressing the particular spiritual needs of Catholics recovering from alcohol addiction. It affiliates closely with Alcoholics Anonymous, and believes in the effectiveness of the twelve-step program, but focuses on enabling Catholics who may have abandoned or neglected their faith during active alcoholism to return and have the fellowship of other Catholics in recovery. It promotes total abstinence for those in recovery, taking inspiration from Matt Talbot, and is concerned with the spiritual development and the sanctification of the whole personality of its members. The organization's motto is \"substituting the cup that stupifies for the cup that sanctifies\". The group has expanded since the 1940s to have active groups in 19 US states and in the UK.\n\n\n"}
{"id": "52342125", "url": "https://en.wikipedia.org/wiki?curid=52342125", "title": "Cannabis in Austria", "text": "Cannabis in Austria\n\nCannabis in Austria is legal for scientific and medical usage, but illegal for recreational usage. Possession of small amounts for personal use was decriminalized in 2016. The sale of cannabis seeds and plants is legal.\n\nOn 9 July 2008, the Austrian Parliament approved cannabis cultivation for scientific and medical uses. Cannabis cultivation is controlled by the Austrian Agency for Health and Food Safety (\"Österreichische Agentur für Gesundheit und Ernährungssicherheit, AGES\").\n\nOn January 1 2016, new regulations went into effect in Austria which removed criminal penalties for personal possession of cannabis.\n\nBoth Δ-THC and pharmaceutical preparations containing Δ-THC are listed in annex IV of the Austrian Narcotics Decree (\"Suchtgiftverordnung\"). Compendial formulations are manufactured upon prescription according to the German \"Neues Rezeptur-Formularium\".\n"}
{"id": "46721614", "url": "https://en.wikipedia.org/wiki?curid=46721614", "title": "Capital punishment in Kenya", "text": "Capital punishment in Kenya\n\nCapital punishment has been practiced in Kenya since before independence and is still provided for under Kenyan law. No executions have been carried out in Kenya since 1987, when Hezekiah Ochuka and Pancras Oteyo Okumu were hanged for treason.\n\nIn 2009, Kenya commuted all death sentences to life imprisonment, impacting over 4000 death row inmates. The move was made to compel these prisoners to work, something condemned men are exempted.\n\nDespite the lack of executions, death sentences are still passed in Kenya. In July 2013, Ali Babitu Kololo was sentenced to death for his role in the murder and kidnapping of two British tourists, and in 2014 a nurse received a death sentence after being convicted of carrying out an abortion on a woman who subsequently died.\n\nCapital punishment was introduced in Kenya in 1893 by the colonial government; the practice was uncommon in pre-colonial communities, which placed a high value on human life. In general, most African communities did not impose a death sentence on an individual unless as a last resort for an offender who had repeatedly \"made themselves dangerous beyond the limits of endurance of their fellows\". The Penal Code as created by the British required a mandatory death penalty for murder, treason and armed robbery. Numerous executions, documented as 1,090 in number, were carried out by the British colonial government during the Mau Mau Uprising.\n\nAfter the 1982 coup d'état attempt, Hezekiah Ochuka, Pancras Oteyo Okumu and two other masterminds of the coup were convicted of treason, sentenced to death and consequently hanged. They were the last people executed in Kenya to date.\n\nIn 2010, the Court of Appeal repealed the mandatory death sentence for murder in \"Mutiso v. Republic\", the third national court in common-law Africa to do so.\n\nIn 2016, President Uhuru Kenyatta commuted the death sentences of 2747 inmates on death row to life imprisonment, as was done by President Mwai Kibaki 7 years previously, where he commuted the sentences of 4000 inmates on death row to life imprisonment.\n"}
{"id": "1814465", "url": "https://en.wikipedia.org/wiki?curid=1814465", "title": "Chartered Institute of Environmental Health", "text": "Chartered Institute of Environmental Health\n\nThe Chartered Institute of Environmental Health (CIEH) is a professional membership body concerned with environmental health and promoting standards in the training and education of environmental health professionals.\n\nThe history of the Chartered Institute of Environmental Health can be traced back to 1883 when the original organisation was founded and called the Association of Public Sanitary Inspectors.\n\nIn 1984, the then Institute of Environmental Health Officers was granted a Royal Charter, a deed giving it special powers, rights and privileges. It became subject to scrutiny by the Privy Council and spent the next ten years taking additional steps to ensure the professional standards of its membership. This resulted in permission being given in 1994 for the organisation to reflect its chartered status through a change in its name to Chartered Institute of Environmental Health.\n\nThe Royal Charter states that the objects of CIEH are: 'to promote for the public benefit the theory and science of environmental health in all its aspects and to disseminate knowledge about environmental health.'\n\nCIEH is based in the UK with approximately 8,000 members worldwide; the majority being based in England, Wales and Northern Ireland.\n\nCIEH’s head office is Chadwick Court (named after Edwin Chadwick), in Southwark, London, with additional offices in Wales and Northern Ireland.\n\nIt also provides a range of professional qualifications and work-based learning including eLearning, corporate training and consultancy services. CIEH’s qualification portfolio includes food safety, health and safety, first aid, fire safety and environmental protection.\n\nCIEH also works with organisations in the private, public and charity sectors, helping them comply with legal requirements and best practice, as well as offering training for their employees.\n\nCIEH offers four grades of membership: Affiliate; Associate; Member and Fellow.\n\nMembers get access to news, regional networks, special interest groups and a range of other benefits.\n\nSince 2003 CIEH has awarded the status of Chartered Environmental Health Practitioner. Chartered status can be obtained only by Members with five years of work experience as a qualified EHP, 60 hours of CPD accumulated within the last three years as well as passing the Chartered Status Assessment.\n\nChartered Environmental Health Practitioners may use the post-nominal letters CEnvH and can gain entry to the Occupational Safety and Health Consultants Register (OSHCR).\n\n"}
{"id": "1983973", "url": "https://en.wikipedia.org/wiki?curid=1983973", "title": "Covered clinical study", "text": "Covered clinical study\n\nIn drug development, a covered clinical study refers to a clinical study, submitted to the Food and Drug Administration (FDA) as part of a marketing application (for example, as part of an NDA or 510(k)), about which the FDA may require disclosure of financial interest of the clinical investigator in the outcome of the study. For example, the applicant must disclose whether an investigator owns equity in the sponsor, or owns proprietary interest in the product under investigation.\n\nThe FDA defines a covered study as \"...any study of a drug, biological product or device in humans submitted in a marketing application or reclassification petition that the applicant or FDA relies on to establish that the product is effective (including studies that show equivalence to an effective product) or any study in which a single investigator makes a significant contribution to the demonstration of safety.\"\n"}
{"id": "1160447", "url": "https://en.wikipedia.org/wiki?curid=1160447", "title": "Disability in the United States", "text": "Disability in the United States\n\nAmericans with disabilities are one of the largest minority groups in the United States. Although the US does not have universal healthcare, Americans with disabilities can generally find adequate levels of subsidized support from a variety of sources, generally at the regional level. While most rural areas — especially in the Great Plains region — have little or no government-organized medical support infrastructure for the permanently disabled indigent population, most major urban centers have healthcare systems. The rights of Americans with disabilities are protected by the Americans with Disabilities Act of 1990.\n\nAccording to the \"Disability Status: 2000 - Census 2000 Brief\" approximately 20% of Americans have one or more diagnosed psychological or physical disability:\nCensus 2000 counted 49.7 million people with some type of long lasting condition or disability. They represented 19.3 percent of the 257.2 million people who were aged 5 and older in the civilian non-institutionalized population -- or nearly one person in five...\"\nThis percentage varies depending on how disabilities are defined. According to \"Census Brief 97-5\", \"About 1 in 5 Americans have some kind of disability, and 1 in 10 have a severe disability.\"\n\nThe United States Census Bureau is legally charged with developing information on the type and prevalence of disability in the population. The primary purpose of collecting ACS data on disability is to help the US Congress determine the allocation of federal funds and inform policies. It is also used to identify the characteristics of the disabled population of the United States. Determining the number and geographical location of people with disabilities is crucial for policies aimed at providing services like public transportation.\n\nACS does not directly measure disability There are other smaller survey studies that provide some insight on disability in the US. While studies like the National Health Interview Survey the Health and Retirement Study, the Behavioral Risk Factor Surveillance System, and the Health, Aging, and Body Composition (Health ABC) Study are used to infer valuable disability-related health characteristics in the US population. While responses to these items are commonly refer to as \"disability\", it could be argued the \n—it uses self- and proxy-reports to evaluate perceived ability to perform functional tasks. Existing publications have delineated details on the US population regarding disability by using information from the ACS. Publications have also outlined issues with disability data in the ACS. Research on disability continues to improve, and potential remedies are found for current methodological challenges. Because of the uniqueness, regarding federal funding and policy, researchers from various fields (e.g., sociology, epidemiology, and government) make wide use of ACS data to better understand disability in the US.\n\nAccording to the 2000 U.S. Census, the African American community has the highest rate of disability in the United States at 20.8 percent, slightly higher than the overall disability rate of 19.4%. Given these statistics, it can be suggested that African Americans with disabilities experience the most severe underemployment, unemployment, and under education compared to other disability groups.\n\nInvestigations on the \"poverty and disability nexus\" have consistently shown poverty and disability are correlated for all race-ethnic groups within the United States. Financial stability of people with disabilities would decrease the dependence on governmental support programs. Studies have been done with the U.S. Census Bureau data to examine the high prevalence of disabilities among welfare recipients. Thirteen percent of families with children under the age of 18, who are also receiving welfare benefits, had at least one child with a disability. Families with income below twice the poverty line were 50% more likely to have a child with a disability than those families with higher incomes. Children with disabilities from families with annual household incomes of higher than $50,000 were more likely to attend higher education.\n\nResearch suggests higher education does impact employment and income opportunities for people with disabilities. It is also noted near equivocal employment opportunities and salaries for people with disabilities to their peers without disabilities. While only one-fifth of people in the United States have at least a four-year college degree, some studies note possessing a four-year degree is the difference between absolute job security and joblessness.\n\nThe US Rehabilitation Act of 1973 requires all organizations that receive government funding to provide accessibility programs and services. A more recent law, the Americans with Disabilities Act of 1990 (ADA), which came into effect in 1992, prohibits private employers, state and local governments, employment agencies and labor unions from discriminating against qualified individuals with disabilities in job application procedures, hiring, firing, advancement, compensation, job training, or in the terms, conditions and privileges of employment. This includes organizations like retail businesses, movie theaters, and restaurants. They must make reasonable accommodation to people with different needs. Protection is extended to anyone with (A) a physical or mental impairment that substantially limits one or more of the major life activities of an individual, (B) a record of such an impairment, or (C) being regarded as having such an impairment. The second and third criteria are seen as ensuring protection from unjust discrimination based on a perception of risk, just because someone has a record of impairment or appears to have a disability or illness (e.g. features which may be erroneously taken as signs of an illness). Employment protection laws make discrimination against qualified individuals with a disability illegal and may also require provision of reasonable accommodation. Reasonable accommodations includes changes in the physical environment like making facilities more accessible but also include increasing job flexibility like job restructuring, part-time or modified work schedules or reassignment to vacant position. Though many hold attitudes that are more enlightened and informed than past years, the word “disability” carries few positive connotations for most employers. Negative attitudes by employers toward potential employees with disabilities can lead to misunderstanding and discrimination.\n\nThe US Social Security Administration (SSA), defines disability in terms of an individual's inability to perform substantial gainful activity (SGA), by which it means “work paying minimum wage or better”. The agency pairs SGA with a list of medical conditions that qualify individuals for disability benefits.\n\nThe SSA makes available to disabled Americans two forms of disability benefits: Social Security Disability Insurance, (SSDI) and Supplemental Security Income (SSI). Social Security pays disability benefits to citizens who have worked long enough and have a medical condition that has prevented them from working or is expected to prevent them from working for at least 12 months or end in death.\n\nBefore the Individuals with Disabilities Education Act was passed, children with disabilities were at-risk of not receiving a free, appropriate public education. For IDEA to apply, the child must first be determined to be able to benefit from public education. This benefit is not exclusively limited to school-aged children, but applies to children with disabilities from infancy.\n\nDue to societal stigma of disability, children are sometimes treated like disabled children, and not included in activities in which other children were able to participate. Educators can hold students with disabilities to lower expectations, which impacts their future educational attainment.\n\nUnder the Individuals with Disabilities Education Act, the school district must provide every disabled child with an Individualized Education Plan (IEP). The IEP is compiled by a team of school administrators and guardians, and may include a child advocate, counselors, occupational therapists, or other specialists. This team evaluates the goals for the child and determines what needs to be done in order for those goals to be met. Children with disabilities who do not have a parent or guardian advocating on their behalf are not as well served in the education system as their peers with parent or guardian advocates.\n\nTransition preparation from K-12 education to post-secondary education or career was initially written into IDEA to begin at age 12, but in the existing law, transition preparation does not begin until age 16. While this law provides a maximum age at which to begin transition preparation, students with disabilities have been known to receive transition preparation at a younger age, as the states might mandate a younger age, or the IEP team might determine a younger age is appropriate to begin the transition preparation of the student. Some students with disabilities have noted not receiving any transition preparation at all. The transition services are to be designed to be results-oriented rather than outcome-oriented. This is to ensure the transition services are designed for the student's success. Students are intended to attend their transition planning meetings with the IEP, yet not all students do. Some do attend, yet generally not take a leadership role - only fourteen percent do. This places the students with a disability in a passive role instead of an agentic role in their own life plans. In a 2007 study of a higher education institution located in the Midwestern United States, it was found that one-third of students with disabilities felt their transition preparation was lacking. Many in this group were unaware of laws that pertained to disability and higher education. This leaves them without an understanding of their learning needs and unable to advocate for themselves.\n\nSelf-advocacy plays an important role in the success of students with disabilities in higher education. While examination of self-advocacy skills has been largely limited to the impact in academic settings, self-advocacy skills, or the lack thereof, do also impact non-academic situations. A 2004 study noted only 3 percent of students with disabilities had self-advocacy training. Students with disabilities who are confident about their disability identity and self-advocacy skills are more likely to disclose their disabilities and advocate for their needs when interacting with faculty and staff. Advocacy service is also provided to students as staff from different programs help instructors in college, understand the needs of the disabled who are attending their classes. So much so that when disable students need extra time to complete their course then some arrangements are made to cater such students. Students with disabilities who were embarrassed of their disability identity and did not understand their needs as learners looked to faculty and staff for solutions to accommodation needs. Education helps students with disabilities learn self-advocacy skills that affect their ability to advocate for their health, insurance, and other needs.\n\nIn spite of IDEA and Section 504 providing support for education of people with disabilities, the educational outcomes of people with disabilities vary significantly from the outcomes of people without disabilities. After high school, a 2005 study found students with disabilities enroll in postsecondary education, whether college, technical school, or vocational school, at a rate of 46% compared to the rate of 63% for students without disabilities. This rate is up 23% since 1990, when the Americans with Disabilities Act of 1990 was passed. Specifically to four-year degree granting higher education institutions, 27% of students with disabilities attend compared to 54% of students without disabilities. High school completion and postsecondary education enrollment vary per disability type.\n\nStudents with disabilities are responsible for advocating for their accommodations and needs as learners in higher education environments. Many higher education institutions have staff to work with students with disabilities on their accommodation requests. Higher education institutions do vary in process to obtain accommodations and accommodations provided. The staff members at the higher education institutions can recommend accommodations. The faculty members, however, may choose to vary or not implement the accommodations at all based upon concerns of weakening academic integrity of the course or risking the possibility of endless accommodation requests. When working with faculty members about accommodations, nearly half of the students with disabilities recalled receiving a negative response, while the other half felt their faculty members were accommodating.\n\nFor people with disabilities, having a four-year college degree provides significant employment and salary advantages.\n\nIt is illegal for California insurers to refuse to provide car insurance to properly licensed drivers solely because they have a disability. It is also illegal for them to refuse to provide car insurance \"on the basis that the owner of the motor vehicle to be insured is blind,\" but they are allowed to exclude coverage for injuries and damages incurred while a blind unlicensed owner is actually operating the vehicle (the law is apparently structured to allow blind people to buy and insure cars which their friends, family, and caretakers can drive for them).\n\n"}
{"id": "40857121", "url": "https://en.wikipedia.org/wiki?curid=40857121", "title": "Disease in Imperial Rome", "text": "Disease in Imperial Rome\n\nThe Roman Empire is often thought of as one of the great civilizations and empires of all time, but the prevalence of disease in Rome's history is often overlooked. As said by Roman Physician Galen, \"This populous city, where daily ten thousand people can be discovered suffering from jaundice, and ten thousand from dropsy.\" While there are few documents remaining from the time period documenting demographics and other factors in disease, bone studies help to indicate various diseases, and speculations can be made on why some diseases were rampant through the empire.\n\nHygiene in ancient Rome was not ideal for combating diseases. Their sewer system, praised for its longevity, had many flaws. As \"Water History\"’s Roger Hanson explains, street drainage and sewage flowed through the same pipes, which led to sewage openings on the streets. Also, since most sewer systems were privately owned, they were also privately maintained and in turn neglected. Instead, citizens would turn to their latrines; if they lived on anything but the ground floor they would even throw their excrement onto the street. This led to exposure of sewage to flies, dogs, and bacteria, all of which helped spread disease among Romans.\n\nThe high poverty rate in Rome led to a need for public baths, or thermae since it was uncommon for the middle class citizens to own one of their own according to journalist Jay Stuller. When the heated bath water was not chemically cleansed or filtered with chemicals such as chlorine, bacteria thrived and spread. When Christianity came to Rome, it saw the public nudity of the bathing system and saw it as debauchery and therefore frowned upon. While the bathing system may not have been pristine, abstaining from cleanliness altogether brought upon many more potentially fatal diseases, especially in infants. This trend started and ended in Rome. As the necessity for bathing became recognized, the protests of the Christian church died down and a new elaborate bathing system was designed under the rule of Emperor Augustus. Even an imperial-version sauna was created for cleansing the body of toxins.\n\nIn contrast to today's diet, Romans ate little meat. According to scholar Linda Gigante, they consumed large amounts of grain, fruits, and some vegetables. Similar to the diet encouraged by modern United States' food stamps, the poor were given monthly supplies of grain and hardly had money to pay for anything else. Due to this, many Romans suffered from malnutrition and multiple vitamin deficiencies. Even those who had money for food didn’t always have the best choices. There was no food and drug regulatory agency in ancient times, so low food standards brought contamination and parasites. Also apparent is the water quality. The Roman Army’s drinking from the contaminated Tiber River contributed to their vulnerability to many diseases.\n\nRome had an extremely high population, and remnants of buildings suggest the average living space was very small. Many people crammed into small spaces led to very high rates of infection for transmittable diseases. The Antonine and Cyprian plagues were transmitted through touch, so a dense population rate would contribute highly to their spread.\n\nDeforestation of Rome's cities, particularly near the Tiber River, led to higher disease rates. The causality is as follows: deforestation lead to a rising water table, which increased marshes. This increased the larva in Rome, and in turn increased disease borne from blood-sucking bugs. As in many of today's third world countries, mosquitos and other vectors were carriers of various diseases, such as malaria and the Ross River virus.\n\nInfluenza, colds, and other ailments were just as apparent, if not more, in Imperial Rome as in today's life. However, they had many more noteworthy afflictions from catastrophic plagues to sexually transmitted diseases.\n\nThe Antonine Plague, possibly the most widespread and catastrophic outbreak of disease in Imperial Rome, was named after the emperor whose reign it originated in, Aurelius Antoninus according to Louise Cilliers and Francis Retief. Historical sources suggest that Roman soldiers returning from campaign in Mesopotamia spread the disease, which lasted from 165-180 AD. Based on the written observations of fever, diarrhea, and boils by the Greek physician Galen, historians infer that smallpox caused the plague. Including substantial army deaths, the outbreaks decimated an estimated two thirds of the Roman population, killing roughly 2000 people per day.\n\nCilliers and Reteif go on to describe that the second great plague affecting Rome. The Plague of Cyprian, mainly occurred from 251 to 266 AD with some traces lingering as late as 270 AD; although considered to be separate from the Antonine Plague, it is very similar and also believed to have originated from smallpox, or perhaps measles. Saint Cyprian makes the most vivid description of the effects of the disease as dysentery, loss of motor skills, and of course fever, and in turn has the disease named after him (also possibly due to the oppression of Christianity at the time). Notably, his list does not include skin rashes or swelling, which is the main separation from the bubonic plague and Antonine Plague. This plague was very widespread, possibly originating in Ethiopia and spreading to Scotland. With the skin contact-spreading nature of the disease and the crowded civilization style in Rome, the death toll was tremendous in the empire.\n\nMorbus Gallicus, better known in modern times as syphilis, or the \"French Disease\" was not prominent in ancient Europe but with recent bone studies, it has been found that a type of European treponematoses bacteria may have even affected children. However, according to an article published by Kristin Harper in 2008, ancient European civilizations may have suffered from a related form of the bacteria but not venereal syphilis itself, which may have had its origin in the pre-Columbian Americas. The term ‘syphilis\" was coined later on by a 15th-century Italian poet Girolamo Fracastoro, who wrote an epic poem of a boy named Syphilus who insulted Apollo, and was in turn punished with the disease. During the Medieval and Renaissance periods the likely mutated forms of the treponematoses resulted in epidemics.\n\nThe earliest known case of malaria is from Roman DNA dated to 450 AD. An excavation of a village shows signs of a serious malaria problem, with bone tests and traces of honeysuckle, a plant used to treat fevers. Also noted is that the area was a \"zone of pestilence\". Deforestation and sanitation issues were the main causes of malaria.\n\nMentagra, notably thought by the Imperial Romans to be spread by kissing, was a skin disease most commonly starting in the chin and moving on to the entire face and sometimes other body parts. The aesthetic factor was very unappealing, while the disease was hardly adverse to health at all. Even though it was not dangerous, Romans ironically went as far as scar-inducing cauterizations to rid them of the abhorrent disease.\n\nRespiratory disease, most prominently anthracosis, was common due to pollution in Roman homes according to Professor Luigi Capasso. Carbon was constantly produced with their lamps, cooking, and fireplaces. The carbon produced lesions on their lungs, apparent in bone studies (made possible by the well-preserved bodies stored under the remnants of a volcanic eruption of Vesuvius) and even a study on a Roman mummy.\n\nAn extensive study done by Mario Novak and Mario Slaus found many skeletal remains available for examination in one specific colony in ancient Rome, Colonia Iulia Iader also known as Zadar. With tests it was found that the mean age of death for men was 37.4 years (with a standard deviation of 9.43 years), and for women was 38.4 years (with a standard deviation of 9.29 years). While this is only a sample representation of our study population, it could give reasonable insights to the whole of Rome. In the remains, several indicators of nutritional stress were found widespread among certain age groups. With the rates of these nutritional problems, it was even found that Romans favored male children in things like breastfeeding, leaving the females with higher rates of malnutrition. Periostitis was also found in many samples, with a frequency indicating overcrowding and overall poor quality of life.\n\nRome had a few prominent physicians in its Imperial era who came up with treatment for various diseases, and were generally the only source of medicinal information. Their service was focused on the military, which was often the most vulnerable group to any given disease. Dioscorides served under Emperor Nero, experimenting with surgical techniques and medicinal herbs. Pliny the Elder also had a strong focus on botany, well known for his herbal knowledge. Galen, perhaps the most prominent Roman physician, studied anatomy as well as herbal remedies.\n\nNatural medicine was of great importance, seeing as they could not synthetically manufacture anything. Many traces of herbs at ancient Roman army bases have been found, as well as medicated wine. Army doctors had knowledge of the herbs, and perhaps even grew their own in their respective gardens. The Romans were not correct with all of the herbs uses, but a placebo effect possibly still made some of the herbs useful.\n\nMarigold was used mainly to prevent/reduce fevers.\n\nChamomile was identified by Pliny as treatment for headaches and inflammation of the kidneys and liver.\n\nHyssop was believed by Romans to prevent both the Antonine and Cyprian plagues. This inaccurate assumption was most likely based on the fact that Hyssop has oils that can treat coughing often found in plague victims.\n\nGarlic is suggested as a sort of panacea in Galen's studies, and as such was used to fend off virus, bacteria, parasites, and fungus in Imperial Rome.\n\nMarshmallow root was thought by Pliny to be effective as a cough remedy.\n\nPliny also found white horehound to be effective at treating coughs.\n\nTarragon was used by Romans to enhance their stamina in daily tasks, particularly soldiers who walked extensively.\n\nGladiators, and perhaps even soldiers, consumed parsley in the hopes that it would give them an edge above their opponent.\nthey were used to tame wild animals and for the gladiators to get the edge on their fights\n\nYarrow is an healing agent, and is shown to exist in Imperial times through stories such as Homer's Illiad, where it is used by Achilles to treat a wound.\n\nGalen's notes show uva ursi was commonly used to reduce bleeding and to nurse various types of wounds.\n\nBorage was used mainly to combat tissue inflammation/bruising.\n\nPlantain was believed to be effective at nursing wounds and also reversing the effects of various poisons.\n\nWith the low water quality and unideal diet of Roman armies, soldiers often suffered from dysentery. Pliny's remedy to this was for the soldiers to consume blackberry leaves and even bark.\n\nThe poor living conditions and many ailments of being in the Roman army often led to depression in the soldiers. For some reason, it was commonly believed that lying on thyme while sleeping would provide a more positive outlook. Although less common, some Roman civilians also practiced this. While this is a misconception, a placebo effect may have even stronger effects on mental ailments such as depression.\n\nFenugreek was readily available for the cavalry units in Imperial Rome, as they would feed it to their horses as a treatment for most sicknesses. It was also commonly used for cattle and other livestock. While that was the main use, it was also sometimes prescribed to citizens for things such as fevers and diseases resembling anthracosis. Fenugreek has been recorded in modern times as a somewhat effective anti-inflammatory, so this was one of the herbs Roman physicians correctly identified.\n\nRoman troops made use of stinging nettle when traveling to cold foreign climates. When stung with the needles, it provided a warm sensation that helped the soldiers gradually grow accustomed to harsh conditions.\n"}
{"id": "39897561", "url": "https://en.wikipedia.org/wiki?curid=39897561", "title": "District Programme Manager", "text": "District Programme Manager\n\nDistrict Programme Manager, DPM in short, is a title used for a government official designated to the monitoring and planning of district health strategies, as in the National Rural Health Mission and the District AIDS Prevention and Control Unit.\n\nSPECIFIC DUTIES AND RESPONSIBILITIES WILL INCLUDE:\n� Assist Civil Surgeon Convener of District Health Society in all the matters relating to overall management of human and financial resources under the package of NHM programme.\n� Coordinate and liaise with other consultant of the NRHM program at Central/State/District level, various department of the state government, Ministry of\nHealth & Family welfare, Government of India, State Institute of Health and Family Welfare and other Nodal/Collaborating agencies.\n� Provide managerial support to district and peripheral level programme support staff and grass root functionaries.\n� Manage human resource including contractual staff under NRHM programme which will include assisting Civil Surgeon in matters related to posting, transfer, performance monitoring, training etc.\n� Assist Civil Surgeon in overall control of financial matters and guide District Accountant in matters related to expenditure, releasing grant, preparation of budget etc.\n� Assist Civil Surgeon in overall logistic management.\n� Monitor managerial, administrative and financial aspect of NRHM programme in the district.\n� Provide logistic support to contractual and field staff for implementation of NRHM programme \n� He will provide necessary support to technical consultants appointed at state and field level during their field visits.\n� Analyze financial and physical progress report and take corrective measures for improving output.\n� Identify the cause of any unreasonable delay in the achievement of milestones, or in the release of funds and propose corrective action.\n� Provide regular report/feed back on programme to the Civil Surgeon of the district.\n� He will take appropriate actions in relation to feedback provided by medical officers/programme officers of the district in consultation with Civil Surgeon.\n� Undertake any other duties assigned to him by Civil Surgeon and his team.\n� Advise on the further development of the programme.\n\nTRAVEL:\nCandidate will require to extensively touring within entire district and occasionally outside district.\n\nKNOWLEDGE & SKILLS DESIRABLE:\n� Expert knowledge on public and private health system\n� Knowledge on public private partnership and NGO Sector\n� Working knowledge of computer (MS Office)\n� Good data analysis and report writing skills\n� Coordination and networking skills\n� Ability to work as a team \n"}
{"id": "3488668", "url": "https://en.wikipedia.org/wiki?curid=3488668", "title": "Eye care professional", "text": "Eye care professional\n\nAn eye care professional (ECP) is an individual who provides a service related to the eyes or vision. It is any healthcare worker involved in eye care, from one with a small amount of post-secondary training to practitioners with a doctoral level of education.\n\nOphthalmologists are “…medical and osteopathic doctors who provide comprehensive eye care, including medical, surgical and optical care.” In the US, this requires four years of college, four years of medical school, one year general internship, three years of residency, then optional fellowship for 1 to 2 years (typically 12–14 years of education after high school). An ophthalmologist can perform all the tests an optometrist can and in addition is a fully qualified medical doctor and surgeon. Ophthalmologists undergo extensive and intensive medical and surgical exams to qualify and entrance criteria to a training program is highly competitive. Some ophthalmologists receive additional advanced training (or fellowship) in specific areas of ophthalmology, such as retina, cornea, glaucoma, laser vision correction, pediatric ophthalmology, uveitis, pathology, or neuro-ophthalmology.\n\nAn ophthalmic medical practitioner is a medical doctor (MD) who specializes in ophthalmic conditions but who has not completed a specialization in ophthalmology.\n\nThe World Council of Optometry, a member of the World Health Organization, defines optometrists as \n“…the primary healthcare practitioners of the eye and visual system who provide comprehensive eye and vision care, which includes refraction and dispensing, detection/diagnosis and management of disease in the eye, and the rehabilitation of conditions of the visual system.”\n\nA Doctor of Optometry (OD) attends four years of college, four years of optometry school and then an optional one-year residency. Optometrists undergo extensive and intensive refractive and medical training mainly pertaining to the eye and the entrance criteria to attend optometry school is also highly competitive. An OD is fully qualified to treat eye diseases and disorders and specializes in optics and vision correction. Permissions granted by an optometric license vary by location.\n\nOrthoptists specialize in diagnosis and management of eye movement and coordination problems, misalignment of the visual axis, convergence and accommodation problems, and conditions such as amblyopia, strabismus, and binocular vision disorders, as outlined by the International Orthoptic Association. They may assist ophthalmologists in surgery, teach orthoptic students, students of other allied health professions, medical students, and ophthalmology residents and fellows, act as vision researchers, perform vision screening, perform low vision assessments and act as clinical administrators.\n\nOcularists specialize in the fabrication and fitting of ocular prostheses for people who have lost eyes due to trauma or illness.\n\nOpticians specialize in the fitting and fabrication of ophthalmic lenses, spectacles, contact lenses, low vision aids and ocular prosthetics. They may also be referred to as an \"optical dispenser\", \"dispensing optician\", \"ophthalmic dispenser\". The prescription for the corrective lenses must be supplied by an ophthalmologist, optometrist or in some countries an orthoptist. This is a regulated profession in most jurisdictions.\n\nA collective term for allied health personnel in ophthalmology. It is often used to refer to specialized personnel (unlike ocularists or opticians). In many countries these allied personnel may just be known as an \"ophthalmic assistant\". Their training is usually combined with a two or three year applied science degree and they assist an ophthalmologist or optometrist in the hospital or clinic with vision testing.\n\nIn the USA the Joint Commission on Allied Health Personnel in Ophthalmology administers OMP certifications:\n\nOculist is an older term that was primarily used to describe eye care professionals that are trained and specialized in the eye care field, specifically ophthalmologists and optometrists. The term is no longer used in the United States.\n\nA vision therapist, usually either an orthoptist or optometrist, works with patients that require vision therapy, such as low vision patients. Commonly, vision therapy is performed in children who develop problems with their vision mostly because they are using their eyes up close. This type of therapy is however generally used in patients who need visual correction but for whom the corrective lenses are not enough to reverse the condition. Visual therapy in children is performed by optometrists who specialize in children eye care. To specialize in vision therapy, doctors must complete extensive post-graduate training beyond their optometric degree, at which time they are eligible to sit for their national boards to become fully certified as specialists in children's vision. A doctor's title after passing the national board in vision therapy is Fellow in the College of Optometrists in Vision Development, or F.C.O.V.D. Optometrists who provide vision therapy but who have not yet sat for their certification exams are board-eligible Associates in the College of Optometrists in Vision Development.\nVision therapists typically use prisms, eye patches, filtered lenses, and computerized systems to conduct therapy sessions.\n\nMost eye care professionals do not practice iridology, citing a significant lack of scientific evidence for the practice.\n\nIn a gross oversimplification, it can be said that ophthalmologists are eye surgeons and primary eye care physicians while optometrists are primary eye care providers. There is considerable overlap in scope of practice between professions. Laws regarding licensure vary by location, but typically ophthalmologists are licensed to provide the same care as an optometrist, with the addition of surgical options. In most locations surgery is the biggest difference between the two professions. Optometrists frequently refer patients to ophthalmologists when the condition requires surgery or intra-ocular injection.\n\nHistorically, ophthalmology has developed as a specialization of medical doctors while optometry originated as a profession that fitted people with glasses. This difference has decreased as the majority of optometrists screen for and treat eye disease and many ophthalmologists fit people with corrective lenses. The difference in background previously caused some conflict between the two professions. Ophthalmologists have voiced concern that an optometrist's educational background is different from their own. Optometrists have criticized ophthalmologists of caring for the health structure of the eye while letting other vision disorders go untreated. For example, consider a patient with glaucoma and spasm of accommodation. Ophthalmologists would be concerned that an optometrist would fail to identify or otherwise mistreat the glaucoma. Optometrist would worry that the ophthalmologist would fail to identify or mistreat the spasm of accommodation. As of 2012, both these concerns are invalid because the education of both types of professionals prepares them to handle both conditions. (This may not be true globally as the definition and education of both professions varies country to country.) Because of cooperation between optometrists and ophthalmologists, the quality of care depends more on the abilities of the individual doctors than it does what type of professional they are.\n\nOrthoptists specialize in the diagnosis and management of problems with eye movement and coordination, such as misalignment of the visual axis, binocular vision problems, and pre/post surgical care of strabismus patients. They do not directly treat ocular disease with medications or surgery. Orthoptists are trained to treat patients using optical aids and eye exercises. Orthoptists are primarily found working alongside ophthalmologists and optometrists to co-manage binocular vision treatment, visual field loss management and accommodative therapy. They often do standard eye and vision testing along with computerised axillary testing.\n\nAll three types of professional perform screenings for common ocular problems affecting children (such as amblyopia and strabismus) and adults (such as cataracts, glaucoma, and diabetic retinopathy). All are required to participate in ongoing continuing education courses to maintain licensure and stay current on the latest standards of care.\n"}
{"id": "43731754", "url": "https://en.wikipedia.org/wiki?curid=43731754", "title": "Health in Norway", "text": "Health in Norway\n\nHealth in Norway, with its early history of poverty and infectious diseases along with famines and epidemics, was for most of the population not good at least into the 1800s. The country eventually changed from a peasant society to an industrial one and established a public health system in 1860.\n\nIn the early Norway faced major challenges. The differences between rich and poor were large, living conditions poor and infant mortality high. Economic conditions in the country improved, but still some social groups lived under constrained conditions. The nutritional status was poor as well as hygiene and living conditions. The conditions and class differences were worse in the cities than in the countryside.\n\nImmunization against smallpox was introduced in the first decade of the 19th century. In 1855, Gaustad Hospital opened as the first mental asylum in the country and was the start of an expansion in treating people with such disorders. After 1900 living standards and health conditions improved and the nutritional status improved as poverty decreased. Improvement in public health occurred during development in several areas such as social and living conditions, changes in disease and medical outbreaks, establishment of the health care system and emphasis on public health matters. Vaccination and increased treatment opportunities with antibiotics resulted in great improvements. Average income increased as did improvements in hygiene. Nutrition became better and more effective also improving general health.\n\nIn the 1900s the situation improved in Norway and, as a result of decreased poverty, nutritional status improved. Within 100 years Norway became a wealthy nation. Even though Norway experienced a setback during World War II, the country achieved steady development. Improved hygiene led to fewer infectious diseases and scientific discoveries lead to breakthroughs in many fields including health.\n\nHowever, an economic downturn in the 1920s exacerbated the nutritional situation within the country. Nutrition therefore became an important part of social policies. In periods there were high rates of unemployment, and poverty affected women and children most. Children often had to walk long distances to get work as shepherds during the summer in order to help their families with income. In mining towns as Røros, children also had to work in the mines. Living conditions improved during the 1900s. From being a poor country, Norway developed within 100 years to become a wealthy nation. Even though the country experienced a setback under the Second World War, the country achieved steady development. From 1975 Norway was self-sufficient in petroleum products and oil became an important part of the Norwegian economy. Improved hygiene led to fewer infectious diseases and scientific discoveries lead to breakthroughs in many fields including health.\n\nAfter 1945, smoking became a relevant factor. While infectious diseases decreased, chronic diseases as cardiovascular disease was blooming. From 2000, life expectancy was still on the increase. There are, however, still social differences when it comes to health. While globalization increases the demand for infectious control and knowledge, the Norwegian population demands more from the government in regard to health and treatment.\n\nEarly on, there were no statistics kept for the whole country on infant mortality, but in Asker and Bærum in 1809 infant mortality was 40 percent for all live births. In 1900, infant mortality was higher in Norway than in any other European country. Development of the welfare state has contributed to a great decrease in infant mortality rates. This can be attributed to better nutrition and living conditions, better education and economy, better treatment possibilities and preventive health care (especially immunization). The infant mortality rate increased again between the 1970 and 1980 due to sudden infant death syndrome (SIDS). SIDS was unknown from earlier, but the increase was dramatic. The trend was reversed when Norwegian parents were encouraged to lay their children on their backs and not their stomachs when sleeping.\n\nAt the beginning of the 19th century the total population was just under 1 million, however it doubled within the next hundred years even though many decided to emigrate. Industrialization resulted in many people emigrating from the countryside to the major cities for work. At the beginning of the 1900s the population was 2,2 million and increased to about 4,5 million through the 1900s. 15 percent of the country’s population lived in Oslo and Akershus. The proportion of people associated with agriculture, forestry and fishing declined while the percentage affiliated with industries increased.\n\nThe Norwegian government recognized that the population needed to improve its health if the country was to become a nation with strong economic development.\n\n\nIn the late 1800s microbes were discovered and prevention of diseases were now possible. Until now, spreading of infections had only been debated. With new discoveries within the field and greater understanding on how bacteria and viruses transfer and spread among humans it was possible to make significant changes in treatment and care of patients. One example was to isolate people with leprosy and tuberculosis in order to stop spreading.\n\nIn the 1900s many vaccines were developed and the first antibiotic, penicillin, came about in the 1940s. These introductions were very powerful tools in preventing and treatment of childhood diseases.\n\nMore vaccines became available and the child-vaccination-program was growing rapidly. Almost all feared childhood diseases were going extinct. Vaccines against measles (rubella) were introduced to the childhood immunization program in 1978. Rubella is dangerous to the fetus if the mother is affected during pregnancy. Today, all children are offered free vaccines and the offer is voluntary. The coverage for most vaccines is high.\n\nIn the early 1980s AIDS came as an unknown disease. Norway was early in preventing it in high-risk groups, through information campaigns. The HIV virus was later discovered and HIV tests became available from 1985.\n\nIncidences of tuberculosis became fewer and an increase in cases and mortality of chronic diseases appeared, especially cardiovascular disease. Tobacco is one of the most important causes of cardiovascular and cancer diseases. During World War 2, the tobacco use in Norway was limited because of strict rationing. After the war, sale of tobacco bloomed and so did the implications from consuming it.\nIn the late 1900s, chronic diseases are dominating and because of increased life expectancy, people live longer with these chronic diseases. \nAround millennium new treatment and prevention for cardiovascular diseases ensured a decrease in mortality, however, these diseases are still one of the greatest public challenges in the country. The incidence of coronary heart disease in Norway reduced significantly between 1995 and 2010 about 66% of the reduction due to changes in modifiable risk factors like activity levels, blood pressure, and cholesterol. Mortality reduced from 137 per 100,000 Person-years to 65.\n\nLifestyle diseases are a new concept from the second half of the 1900s. Tobacco use and increases in cholesterol levels show a strong correlation to higher risk of cardiovascular disease.\n\nMental health services are part of the Norwegian special health care services. In some cases this includes involuntary mental health treatment.\nThe four regional health service institutions, owned by the state, receive fixed economic support from the state budget. They are responsible for special health services including mental health care in hospitals, institutions, district mental health centers, child and adolescent mental health services and nursing homes.\n\nIn addition to providing treatment, the mental health care services provide research, education for health personnel, and follow-up of patients and their relatives.\n\nThere are different sectors within the mental health services. District mental health centers are responsible for general mental health service. They have outpatient facilities, inpatient facilities and emergency teams. Patients can be referred to the district mental health center by a general practitioner for diagnosing, treatment or admission.\n\nThere are specialized centers, ideally at central hospitals, for children and adolescents, the elderly, and severe cases such as drug addiction, personality disorders, obsessive compulsive disorders etc. Normally people who are discharged from treatment at central hospitals are referred to the district mental health centers for follow-up and treatment. Treatment can consist of psychotherapy with or without medications. Physical treatments, such as electroconvulsive therapy, are used for specific disorders. Treatment usually starts at the hospital, with the aim of continuing treatment at home or at the district mental health center.\n\nChild and adolescent mental health outpatient facilities offer mental health care for children and adolescents between 0–17 years of age. Central child and adolescent mental health service is aimed at challenges which cannot be handled in the regional state facilities, such as the general practitioner, school nurse, school, outreach services for youth and child services. The child and adolescent mental health services work closely with psychologists, child psychiatrists, family therapists, neurologists, social workers etc. Their aim is to diagnose and treat psychiatric disorders, behavioral disorders and learning disorders in close collaboration with care givers. For patients below the age of 16, parents must consent to admission.\n\nInvoluntary mental health care in Norway is divided into inpatient and outpatient facilities and observation. In involuntary inpatient facilities patients can be held against their will, and can be picked up by the police if needed. In involuntary outpatient services the patient lives at home or is voluntarily in an institution, but regularly has to report to the district mental health center. These patients cannot be held against their will, but can be picked up by the police in the case of missed appointments. For involuntary observation in hospital a person can be held for up to ten days, or in some cases for twenty days, in order for the hospital to decide whether the criteria for involuntary mental health care are met. The control committee has as their main task to ensure that every patient’s rights are secured and protected in a meeting with involuntary care.\n\nMental health services are financed through needs-based basic funding to the regional health services, outpatient clinic refunding, deductibles and ear-marked grants from the state budget. Rates for outpatient work are partly based on hours worked and partly based on procedures; there are rates for diagnosing, treatment and follow-up per telephone or in collaborative meetings. In addition patients pay a deductible for outpatient consultations.\n\nA survey done in 2011 showed that 10.2% of the population of Norway reported to have experienced symptoms of anxiety and depression within the last two weeks. The lifetime prevalence of severe depression is estimated to be 15.6%. Treatment and social services for the mentally ill cost society about 70 million Norwegian kroner (more than 10 million US dollars) yearly.\n\nNorway has a birth register, death register, cancer register, and population register, which enables to authorities to have an overview of the health situation in Norway. The total population in Norway as of 2012, was 4,994,000. The life expectancy at birth was 80 years for males and 84 years for females. The under five mortality per 1000 live births in 2012 were three cases. The probability of dying between 15 and 60 years for males is 73 and 44 for females per 1000 in population. The total expenditure on health per capita (international dollars) was 5,970. Total expenditure on health as percentage of GDP was 9%. Gross national income per capita (PPP international dollars) is 66,960.\n\nThe total fertility rate per women in 2012 was 1,9, the regional average was 1,7 and global average was 2,5. Prevalence of tuberculosis was 10 per 100 000 in population and the regional average was 56 while global average was 169. In Norway today, there are 5371 HIV positive people 3618 men and 1753 women. In 2008 the incidence of HIV positive people had a peak and the highest incidence of HIV positive. Since that, there have been a decrease in new cases.\nNorway was awarded first place according to the UNs Human Development Index (HDI) for 2013.\n\nNorway was awarded first place according to the UN's Human Development Index (HDI) for 2013.\n\nA new measure of expected human capital calculated for 195 countries from 1990 to 2016 and defined for each birth cohort as the expected years lived from age 20 to 64 years and adjusted for educational attainment, learning or education quality, and functional health status was published by the Lancet in September 2018. Norway had the seventh highest level of expected human capital with 25 health, education, and learning-adjusted expected years lived between age 20 and 64 years. \n\nStatistics for Norway:\n\nThe total fertility rate per woman in 2012 was 1.9, the regional average was 1.7 and the global average was 2.5. The prevalence of tuberculosis was 10 per 100 000 in the population and the regional average was 56 while the global average was 169. Tuberculosis incidence in Norway has increased from about 200 cases in 1997 to 400 cases in 2013. Vaccination against tuberculosis, BCG, was part of the National vaccination program but has not been included since the school year 2008/2009. \nIn Norway today, there are 5371 HIV positive people: 3618 men and 1753 women. In 2008 the incidence of HIV positive people peaked. Since then, there has been a decrease in new cases.\n\nA wealthy economy makes it possible to buy tobacco, fast food, sweet drinks, and sweets that few people had access to or could afford until after 1950. These days many people have desk jobs, cars, and less demanding housework. In large, physical activity is decreasing, electronics, computers, social media, and the internet demands more of daily life. Drugs have also become more available in society. ‘New living conditions’ such as these give rise to new challenges for public health. Only 30 percent of adults in Norway are fulfilling the advice to stay physical active for 150 minutes per week. In Norway today, municipalities have a higher health responsibility then earlier. Preventing diseases from birth should be prioritized in public health.\n\nIn Norway in 2008, approximately 17% of adult men used snus daily or occasionally, while 4% of adult women used snus daily or occasionally. In secondary schools in 2000-2004, 21% of boys and 4% of girls used snus daily or occasionally. Many people both smoke and use snus. \n\nThe proportion of smokers is higher among immigrants to Norway than among ethnic Norwegians. The highest proportion of smokers can be found among immigrants from Turkey, Iran, Vietnam and Pakistan.\n\nDiseases in the 21st century that are dominating in Norway are cardiovascular diseases, cancer, COPD, and diabetes. Technological progress and development within medical treatment have since the 1970s had huge impact on survival of diseases and especially cardiovascular diseases. Elders suffer from disabilities and chronic diseases such as cancer, dementia, and pain disorders. Elderly often have multiple diseases simultaneously, which together affect functional capacity, quality of life, and mental health.\n\nThe living standard of the Norwegian population has increased, though there are still differences between educational groups. Those with higher education and economy have generally the best health status. New public health legislation (Folkehelseloven) came into play in 2012, and the purpose of this act is to contribute to a society that promotes public health and evens out social inequalities in health.\n\n\n"}
{"id": "16540940", "url": "https://en.wikipedia.org/wiki?curid=16540940", "title": "Healthcare in Turkmenistan", "text": "Healthcare in Turkmenistan\n\nIn the post-Soviet era, reduced funding has put the health system in poor condition. In 2002 Turkmenistan had 50 hospital beds per 10,000 population, less than half the number in 1996. Overall policy has targeted specialized inpatient facilities to the detriment of basic, outpatient care. Since the late 1990s, many rural facilities have closed, making care available principally in urban areas. President Niyazov’s 2005 proposal to close all hospitals outside Ashgabat intensified this trend. Physicians are poorly trained, modern medical techniques are rarely used, and medications are in short supply. Doctors and pharmacists were required to study the works of Avicenna and tested on their knowledge of Saparmyrat Niyazov's spiritual writings, the Ruhnama. In 2004 Niyazov dismissed 15,000 medical professionals, exacerbating the shortage of personnel. In some cases, professionals have been replaced by military conscripts. Private health care is rare, as the state maintains a near monopoly. Free public health care was abolished in 2004.\n\nNiyazov's successor, Gurbanguly Berdimuhamedow was a dentist, and took a rather more positive approach to healthcare. Money was invested to modernize the health-care sector, building gleaming new medical facilities. He initiated an annual Month of Health and Sports, which involved people throughout the country taking long walks in parks and compulsory physical fitness classes at workplaces. $56 million was spent on an ophthalmology complex in Ashgabat and $47 million in a traumatology centre. The rural hospitals reopened, but they had severe shortages of the most basic medical equipment and hygiene standards were poor. Theoretically the state-funded health insurance covers part of the cost of hospital treatment and medication in public medical facilities, but there are widespread reports of bribery and corruption. There is an acute shortage of clinical staff and political pressure, for example discouraging from diagnoses of HIV . There is a considerable flow of medical tourism from patients looking for more reliable health systems.\n"}
{"id": "54646797", "url": "https://en.wikipedia.org/wiki?curid=54646797", "title": "ICD coding for rare diseases", "text": "ICD coding for rare diseases\n\nThe International Classification of Diseases (ICD) is the code used for the purpose of documenting a person's medical condition. It is usually important for health insurance reimbursement, administration, epidemiology, and research. Of the approximately 7,000 rare diseases, only about 500 have a specific code. An ICD code is needed for a person's medical records—it is important for health insurance reimbursement, administration, epidemiology, and research. Finding the best ICD code for a patient who has a rare disease can be a challenge.\n\nDifferent versions of the ICD code exist worldwide. The United States currently uses the ICD-10-CM, a Clinical Modification of the WHO standard for diagnoses adapted for insurance reimbursement and billing purposes. This version allows for further breakdown of a code, which increases diagnosis specificity. Currently, published material that reference ICD-9-CM codes, which were used before October 1, 2015; however, not every code in the ICD-9-CM has a corresponding code in ICD-10-CM. Europe and other parts of the world use the ICD-10. The root codes for ICD-10 and ICD-10-CM are the same, making it helpful for locating codes for general body systems and disease processes. \n\nA good place to start is to contact an advocacy organization for the rare disease. These organizations are often aware of how the condition has been coded for other patients with the same diagnosis and may be able to recommend one or more codes to use. Many disease advocacy organizations also have medical advisory boards or physician directories, which can help to find someone with experience coding for that particular condition. A search can be done on the GARD website for a list of disease advocacy organizations. A GARD Information Specialist can be contacted directly for assistance.\n\nOrphanet is a European reference portal for information on rare diseases and orphan drugs. Orphanet outlines the ICD-10 coding rules for rare diseases included in their database. The Orphanet database also often includes coding information for the Online Mendelian Inheritance in Man (OMIM), the Unified Medical Language System (UMLS), and more. When a diagnosis has not been established, or when a code does not exist for a specific rare disease, general coding guidelines indicate that it is acceptable to use codes that describe signs and symptoms. \n\nSeveral online resources can help locate ICD codes:\n\n"}
{"id": "32569579", "url": "https://en.wikipedia.org/wiki?curid=32569579", "title": "ISMETT", "text": "ISMETT\n\nISMETT, in Italian, Istituto Mediterraneo per i Trapianti e Terapie ad Alta Specializzazione translated as the Mediterranean Institute for Transplantation and Advanced Specialized Therapies, is a center for organ transplantation located in Palermo, Italy. ISMETT was founded in 1997 as a partnership between the Region of Sicily, the Civico and Cervello hospitals in Palermo, and the University of Pittsburgh Medical Center (UPMC).\n\nISMETT primarily specializes in performing all types of organ transplantations, using both deceased and living donor techniques. ISMETT has 70 beds (14 ICU, 21 in semi-intensive, and 35 inpatient), 7 outpatient beds, 4 operating rooms, a hospital pharmacy and laboratory analysis, infectious diseases and pathology in a facility that covers an area of .\n\nThe idea of creation of the Institute stemmed from a group of hepatologists at a hospital in Palermo that proposed to the University of Pittsburgh Medical Center (UPMC) the idea of a multi-organ transplant center to be realized in Sicily. Its creation was also prompted by a 1995 law passed in the United State that capped the total number of foreign patients that can be placed on a waiting list for transplants at 5%. For this reason, transplant leaders in Pittsburgh began looking toward opportunities in Europe and participated actively in the creation of a transplant center in Palermo. On May 23, 1996, the idea was presented to the Ministry of Health, who welcomed the project.\n\nThe first director was Ignazio R. Marino who performed the first successful liver transplantation in Sicily on July 31st, 1999. Ignazio R. Marino's team performed the first 100 solid organ transplantations of ISMETT.\n\nOn March 20, 1997, the officially approved the creation and clinical management of ISMETT in accordance with art. 9/bis of Legislative Decree no. 502/92 and proceeded with construction of the new institute. The construction began in 1999 with the laying of a foundation stone and was completed in 2004 when the new center opened its doors.\n\nAs of 2007, ISMETT had revenues of € 2,270,000. Its chairman was Camillo Ricordi, with Bruno Gridelli serving as its director-general and Ugo Palazzo serving as its director of health\n\nISMETT began its clinical activity in 1999. On July 31, 1999 it performed its first liver transplant from a cadaveric donor in Sicily. The same year ISMETT started a cadaveric and living donor kidney transplantation program. In 2004, ISMETT obtained permission to perform heart-lung transplants, thus becoming a multi-organ transplant center.\n\nIn 2003 ISMETT started a pediatric liver transplantation program, and that year performed the first children's transplant done in southern Italy.\n\nIn 2001, Ignazio Marino performed the first living donor transplant into and HIV-positive patient in Italy, a procedure that created controversy and criticism, including complaint from the Ministry of Health. The Ministry moved to censure Marino arguing that this particular type of transplant had the characteristics of clinical trials that required special authorization. That patient transplanted by Dr. Marino is still alive and enjoying an excellent quality of life free of dialysis, 16 years after the kidney transplant. Because of this success the rules in Italy have been changed and now HIV-positive patients are not any longer denied access to transplantation if they need it.\n\nIn 2007, for the first time in the world, an ISMETT team led by Dr. Bruno Gridelli performed a lung transplant on an HIV positive patient In this case, the intervention was authorized by the Ministry and was included within an experimental program launched by the National Transplant Center. This time, no controversy was recorded and the procedure received congratulatory press.\n\nIn 2007, the Regenerative Medicine and Cell Therapy Unit (Unità di Medicina Rigenerativa e Terapie Cellulari), termed the Cell Factory, was opened at ISMETT with the aim to initiate a program of regenerative medicine that researches the reparation of damaged organs. The mobile production laboratories (GMP Facility) of the unit have been made part of a project sponsored by the Region of Sicily and the Budget and Finance under the \"ICT for the excellence of the territories.\"\n\nAmong the research trials carried out at the Cell Factory is the injection of fetal hepatocytes as a therapy to bridge the patients waiting for a liver transplant, transplantation of pancreatic islets for patients with diabetes type 1, and transplantation of human fetal skin cells for the treatment of lesions of the skin.\n\nThe Center for Simulation became active at ISMETT in June, 2007 with the aim to prevent and reduce medical errors. The use of simulation for training staff is already quite widespread in certain sectors such as aviation, and has now spread into the medical field. The simulation center provides for the implementation of higher risk procedures without risk to personnel and without involving patients.\n\nThe center has five life-sized and technologically sophisticated simulator mannequins that can mimic the signs and symptoms of real patients. The center was created thanks to a donation from the Foundation Fiandaca. The training activities of the Centre, which is open to all health care providers, began in January, 2008 .\n\n\n"}
{"id": "39948152", "url": "https://en.wikipedia.org/wiki?curid=39948152", "title": "Impairment rating", "text": "Impairment rating\n\nAn impairment rating is a percentage intended to represent the degree of an individual's impairment, which is a deviation away from one's normal health status and functionality. Impairment is distinct from disability. An individual's impairment rating is based on the restrictive impact of an impairment, whereas disability is broadly the consequences one's impairment. Impairment ratings given to an individual by different medical examiners are sometimes problematically inconsistent with each other.\n"}
{"id": "8105682", "url": "https://en.wikipedia.org/wiki?curid=8105682", "title": "International Radon Project", "text": "International Radon Project\n\nThe International Radon Project (IRP) is a World Health Organization initiative to reduce the lung cancer risk around the world.\n\nThe IRP released their guidance to member countries in September 2009.\n\nExposure to radon in the home and workplace is one of the main risks of ionizing radiation causing tens of thousands of deaths from lung cancer each year globally. In order to reduce this burden it is important that national authorities have methods and tools based on solid scientific evidence and sound public health policy. The public needs to be aware of radon risks and the means to reduce and prevent these.\n\nIn 1996 WHO published a report containing several conclusions and recommendations covering the scientific understanding of radon risk and the need for countries to take action in the areas of risk management and risk communication.\n\nRecent findings from case-control studies on lung cancer and exposure to radon in homes completed in many countries allow for substantial improvement in risk estimates and for further consolidation of knowledge by pooling data from these studies. The consistency of the findings from the latest pooled analyses of case-control studies from Europe and North America as well as China provides a strong argument for an international initiative to reduce indoor radon risks.\n\nTo fulfill these goals, WHO has developed a program on public health aspects of radon exposure. This project enjoys high priority with WHO's Department of Public Health and Environment. The key elements of the International Radon Project include:\n\n\nTo achieve these aims WHO has formed a network of key partner agencies from some 40 Member States. This network is the basis for the WHO International Radon Project which was launched in 2005. Working groups will collect and analyse information on radon risk, radon policies, radon mitigation and prevention as well as risk communication. The project members meet regularly and work towards achieving the outlined objectives.\n\n"}
{"id": "13113949", "url": "https://en.wikipedia.org/wiki?curid=13113949", "title": "International Symposium on Endovascular Therapy", "text": "International Symposium on Endovascular Therapy\n\nThe International Symposium on Endovascular Therapy (ISET) is an educational meeting for physicians, scientists, engineers and allied health professionals working in the multidisciplinary field of peripheral and cardiac interventional medicine. The meeting provides information on the latest techniques and technology for the diagnosis, treatment and prevention of vascular diseases, among them stroke, heart attack, aneurysm and hardening of the arteries. The meeting highlights life-saving procedures that are considered less invasive than traditional surgery.\n\nThe five-day course was founded in 1989 by interventionist Barry Katzen, M.D., the founder and medical director of the Miami-based Baptist Cardiac & Vascular Institute (BCVI), which presents the meeting. \n\nThe ISET meeting is presented annually in South Florida and draws attendees and prominent faculty speakers from around the world. In addition to Dr. Katzen, the following serve as course directors and are responsible for the meeting’s programming: James F. Benenati, M.D.; Alex Powell, M.D.; Ramon Quesada, M.D.; Shaun Samuels, M.D.; and Constantino Peña, M.D.\n\n"}
{"id": "14403863", "url": "https://en.wikipedia.org/wiki?curid=14403863", "title": "International Ventilator Users Network", "text": "International Ventilator Users Network\n\nThe International Ventilator Users Network (IVUN) is a nonprofit network of mechanical ventilation users, respiratory health professionals, and ventilatory equipment manufacturers. Its focus is on the health and independent living of ventilator users, whether they are using assisted ventilation long-term – at home or in nursing facilities—or short-term in emergency rooms and critical care units.\n\nMany ventilator users have neuromuscular conditions, such as respiratory polio or post-polio syndrome, amyotrophic lateral sclerosis (ALS), muscular dystrophy, spinal muscular atrophy (SMA), spinal cord injury (SCI), or congenital central hypoventilation syndrome (CCHS). Historically, IVUN’s efforts have been primarily addressed to ventilator users with neuromuscular conditions. But people who have chronic obstructive pulmonary disease (COPD) or obesity hypoventilation syndrome may also need to use assisted ventilation.\n\nIVUN’s mission is “to enhance the lives and independence” of ventilator users “through education, advocacy, research, and networking” among ventilator uses, respiratory health professionals, and ventilatory equipment manufacturers. Support comes from individual members, donors, and sponsors.\n\nIVUN’s parent organization is Post-Polio Health International (PHI). It shares that organization’s headquarters and staff in St. Louis, Missouri, as well as its volunteer Board of Directors. IVUN’s publications, website, and volunteer advisory boards are its own, however.\n\nIVUN publishes (both online and in print) authoritative medical information based on interaction between ventilator users and healthcare professionals. Currently, users can find online free of charge a 16-page introductory document describing the history of ventilators, the various types of ventilators, and the types of user-interfaces; a packet of documents dealing with emergency medical care for home ventilator users (checklists for users, caregivers, physicians, and Emergency medical services personnel); and a comprehensive Home Ventilator Guide which provides technical information on home/portable ventilator equipment from manufacturers worldwide. Past issues of IVUN’s quarterly newsletter, \"Ventilator-Assisted Living\", are also online. IVUN’s staff maintain a telephone answer-line and answer e-mail inquiries during business hours.\n\nIVUN publishes, and makes available free online, the \"Resource Directory for Ventilator-Assisted Living\", which lists respiratory health professionals who are experts in long-term assisted ventilation, ventilatory equipment manufacturers and their contact information, and organizations whose members use assisted ventilation. IVUN’s website manages an equipment exchange, a peer-to-peer advice page, and ventilator news digests.\n\nIVUN staff regularly attend medical meetings and international conferences on home ventilator use, and coordinate presentations by ventilator users at medical meetings such as those for the American College of Chest Physicians (ACCP). In addition, IVUN/PHI’s periodic international conferences present many sessions relevant to ventilator users.\n\nWith respect to advocacy and research, IVUN’s activities are integrated with those of PHI. A recent research grant of direct relevance to ventilator users is a study, conducted in 2005 by a team at Johns Hopkins University, on the “Timing of Noninvasive Ventilation for Patients with Amyotrophic Lateral Sclerosis.” An earlier research award (2001) went to researchers at the University of Toronto, Canada for a study on ”Ventilator Users' Perspectives on the Important Elements of Health-Related Quality of Life.”\n\nIVUN was formally established under that name in 1987, but its roots go back to the polio epidemics of the 1940s and 1950s, when the possibility of long-term survival with mechanically assisted ventilation became a reality. Beginning in 1949, IVUN’s founder, Gini Laurie, was an American Red Cross volunteer in the polio wards of Cleveland’s City Hospital, and later in the part of that hospital (Toomey Pavilion) that became one of the 15 respiratory care and rehabilitation hospitals funded by the March of Dimes. Those centers all across the country began to close after the polio epidemics in the United States ended. Laurie, however, was determined to keep the respiratory polio survivors in touch with each other, and with their medical specialists.\n\nShe did this first by taking over the editorship of Toomey Pavilion’s alumni newsletter, the \"Toomeyville Jr. Gazette\", and circulating it widely throughout the United States. Copies also went to individuals and organizations in Canada, Great Britain, Europe, and Australia. In 1960, her informal organization of volunteers was incorporated under the name Iron Lung Polio Assistance, and her newsletter grew into a magazine called the \"Toomey j. Gazette\". In 1964, the organization changed its name to Iron Lung Polios and Multiplegics, to reflect more accurately Laurie’s cross-disability concerns. The magazine became the \"Rehabilitation Gazette\" in 1970, carrying a wide range of articles on independent living for people with physical disabilities. Laurie soon became one of the central figures in the development of the independent living movement and in the founding of the Centers for Independent Living in the United States.\n\nLaurie’s central concerns, however, always included ventilator users. Specifically, they concerned finding ways for ventilator users to leave hospitals and nursing homes with the support necessary for them to live active, effective lives as members of their communities. As some cross-disability organizations such as the American Coalition of Citizens with Disabilities (ACCD) dissolved into disability-specific organizations, IVUN was formed to make sure there were publications and networks specifically for long-term ventilator users.\n\n"}
{"id": "298931", "url": "https://en.wikipedia.org/wiki?curid=298931", "title": "Jujube", "text": "Jujube\n\nZiziphus jujuba (from Greek \"ζίζυφον\", \"zízyphon\"), commonly called jujube (; sometimes jujuba), red date, Chinese date, Korean date, or Indian date is a species of \"Ziziphus\" in the buckthorn family (Rhamnaceae).\n\nIt is a small deciduous tree or shrub reaching a height of , usually with thorny branches. The leaves are shiny-green, ovate-acute, long and wide, with three conspicuous veins at the base, and a finely toothed margin. The flowers are small, wide, with five inconspicuous yellowish-green petals. The fruit is an edible oval drupe deep; when immature it is smooth-green, with the consistency and taste of an apple, maturing brown to purplish-black, and eventually wrinkled, looking like a small date. There is a single hard kernel, similar to an olive pit, containing two seeds.\n\nIts precise natural distribution is uncertain due to extensive cultivation, but is thought to be in southern Asia, between Lebanon, northern India, and southern and central China, and possibly also southeastern Europe though more likely introduced there.\n\nThis plant has been introduced in Madagascar and grows as an invasive species in the western part of the island. This plant is known as the \"hinap\" or \"finab\" in the eastern part of Bulgaria where it grows wild but is also a garden shrub, kept for its fruit. The fruit is picked in the autumn. The trees grow wild in the eastern Caribbean, and are reported to exist in Jamaica, The Bahamas, and Trinidad as well. In Antigua and Barbuda, the fruit is called \"dumps\" or \"dums\"; and in The Bahamas, \"juju\". It is also known as \"pomme surette\" on the French islands of the Caribbean. This fruit, more precisely known as \"Indian jujube\" elsewhere, is different from the \"jujube\" fruit that is cultivated in various parts of southern California. Altun Ha an ancient Mayan city in Belize, located in the Belize District about 50 kilometres (31 mi) north of Belize City and the surrounding woods also boasts some jujube tree and shrub varieties where it is referred to as plums for lack of a better word among locals.\n\nThe species has a curious nomenclatural history, due to a combination of botanical naming regulations, and variations in spelling. It was first described scientifically by Carl Linnaeus as \"Rhamnus zizyphus\", in \"Species Plantarum\" in 1753. Later, in 1768, Philip Miller concluded it was sufficiently distinct from \"Rhamnus\" to merit separation into a new genus, which he named \"Ziziphus jujube\", using Linnaeus' species name for the genus but with a probably accidental single letter spelling difference, \"i\" for \"y\". For the species name he used a different name, as tautonyms (repetition of exactly the same name in the genus and species) are not permitted in botanical naming. However, because of Miller's slightly different spelling, the combination the earlier species name (from Linnaeus) with the new genus, \"Ziziphus zizyphus\", is \"not\" a tautonym, and was therefore permitted as a botanical name. This combination was made by Hermann Karsten in 1882. In 2006, a proposal was made to suppress the name \"Ziziphus zizyphus\" in favor of \"Ziziphus jujuba\", and this proposal was accepted in 2011. \"Ziziphus jujuba\" is thus the correct scientific name for this species.\n\nIn Arabic-speaking regions the jujube and alternatively the species \"Z. lotus\" are closely related to the lote-trees (sing. \"sidrah\", pl. \"sidr\") which are mentioned in the Quran, while in Palestine it is rather the species \"Z. spina-christi\" that is called \"sidr\".\n\nVarieties of jujube include Li, Lang, Sherwood, Silverhill, So, Shui Men and GA 866.\n\nJujube was domesticated in south Asia by 9000 BC. Over 400 cultivars have been selected.\n\nThe tree tolerates a wide range of temperatures and rainfall, though it requires hot summers and sufficient water for acceptable fruiting. Unlike most of the other species in the genus, it tolerates fairly cold winters, surviving temperatures down to about . This enables the jujube to grow in mountain or desert habitats, provided there is access to underground water throughout the summer. The jujube, \"Z. jujuba\" grows in cooler regions of Asia. Five or more other species of \"Ziziphus\" are widely distributed in milder climates to hot deserts of Asia and Africa.\n\nIn Madagascar, jujube trees grow extensively in the western half of the island, from the north all the way to the south. It is widely eaten by free-ranging zebus, and its seeds grow easily in zebu feces. It is an invasive species there, threatening mostly protected areas.\n\nThe freshly harvested, as well as the candied dried fruit, are often eaten as a snack, or with coffee. Smoked jujubes are consumed in Vietnam and are referred to as black jujubes. Both China and Korea produce a sweetened tea syrup containing jujube fruit in glass jars, and canned jujube tea or jujube tea in the form of teabags. To a lesser extent, jujube fruit is made into juice and jujube vinegar (called 枣醋 or 红枣醋 in Chinese). They are used for making pickles (কুলের আচার) in west Bengal and Bangladesh. In China, a wine made from jujube fruit is called \"hong zao jiu\" (红枣酒).\n\nSometimes pieces of jujube fruit are preserved by storing them in a jar filled with \"baijiu\" (Chinese liquor), which allows them to be kept fresh for a long time, especially through the winter. Such jujubes are called \"jiu zao\" (酒枣; literally \"alcohol jujube\"). The fruit is also a significant ingredient in a wide variety of Chinese delicacies.\n\nIn Vietnam and Taiwan, fully mature, nearly ripe fruit is harvested and sold on the local markets and also exported to Southeast Asian countries. The dried fruit is used in desserts in China and Vietnam, such as \"ching bo leung\", a cold beverage that includes the dried jujube, longan, fresh seaweed, barley, and lotus seeds.\n\nIn Korea, jujubes are called \"daechu\" (대추) and are used in \"daechucha\" teas and \"samgyetang\".\n\nIn Croatia, especially Dalmatia, jujubes are used in marmalades, juices, and \"rakija\" (fruit brandy).\n\nOn his visit to Medina, the 19th-century English explorer, Sir Richard Burton, observed that the local variety of jujube fruit was widely eaten. He describes its taste as \"like a bad plum, an unrepentant cherry and an insipid apple.\" He gives the local names for three varieties as \"Hindi (Indian), Baladi (native), Tamri (date-like).\" In Palestine a hundred years ago, a close variety was common in the Jordan valley and around Jerusalem. The bedouin valued the fruit, calling it \"nabk\". It could be dried and kept for winter or made into a paste which was used as bread.\n\nIn Persian cuisine, the dried drupes are known as \"annab\", while in neighboring Azerbaijan, it is commonly eaten as a snack, and is known as \"innab\". Confusion in the common name apparently is widespread. The \"innab\" is \"Z. jujuba\": the local name \"ber\" is not used for \"innab\". Rather, \"ber\" is used for three other cultivated or wild species, e.g., \"Z. spina-christi\", \"Z. mauritiana\", and \"Z. nummularia\" in Pakistan and parts of India and is eaten both fresh and dried. The Arabic name \"sidr\" is used for \"Ziziphus\" species other than \"Z. jujuba\".\n\nTraditionally in India, the fruit is dried in the sun and the hard nuts are removed. Then, it is pounded with tamarind, red chillies, salt, and jaggery. In some parts of the Indian state of Tamil Nadu, fresh whole ripe fruit is crushed with the above ingredients and dried under the sun to make cakes called \"ilanthai vadai\" or \"regi vadiyalu\" (Telugu). It is also commonly consumed as a snack.\n\nIn Northern and Northeastern India the fruit is eaten fresh with salt and chilli flakes and also preserved as candy, jam or pickle with oil and spices. \n\nIn Madagascar, jujube fruit is eaten fresh or dried. People also use it to make jam. A jujube honey is produced in the Atlas Mountains of Morocco.\n\nItaly has an alcoholic syrup called \"brodo di giuggiole\".\nIn Senegal Jujube is called Sii dem and the fruit is used as snack. The fruit is turned into dried paste used by school kids.\n\nThe commercial jujube candy popular in movie theaters originally contained jujube juice but now uses other flavorings.\n\nThe fruit and its seeds are used in Chinese and Korean traditional medicine, where they are believed to alleviate stress, and traditionally for anti-fungal, anti-bacterial, anti-ulcer, anti-inflammatory purposes and sedation, antispastic, antifertility/contraception, hypotensive and antinephritic, cardiotonic, antioxidant, immunostimulant, and wound healing properties. It is among the fruits used in Kampo.\n\nZiziphin, a compound in the leaves of the jujube, suppresses the ability to perceive sweet taste.\n\nIn Japan, the \"natsume\" has given its name to a style of tea caddy used in the Japanese tea ceremony, due to the similar shape. Its hard, oily wood was, along with pear, used for woodcuts to print the world's first books, starting in the 8th century and continuing through the 19th in China and neighboring countries. As many as 2000 copies could be produced from one jujube woodcut. \n\nWitch's brooms, prevalent in China and Korea, is the main disease affecting jujubes, though plantings in North America currently are not affected by any pests or diseases. In Europe, the last several years have seen some 80%–90% of the jujube crop eaten by insect larvae (see picture), including those of the false codling moth, \"Thaumatotibia (Cryptophlebia) leucotreta\". \n\n\n"}
{"id": "22315945", "url": "https://en.wikipedia.org/wiki?curid=22315945", "title": "LIFE Healthcare Group", "text": "LIFE Healthcare Group\n\nLIFE Healthcare Group, formerly Afrox Healthcare, is the second largest private hospital operator in South Africa, with 6,500 beds. It is owned by Brimstone Investment Corporation. It is also the largest black-owned hospital operator in South Africa.\n\nAfrox was traded on the JSE Securities Exchange until it was sold to a black empowerment group in 2005.\n\nIt bought Alliance Medical for about 10.4 billion rand in November 2016.\n"}
{"id": "48848862", "url": "https://en.wikipedia.org/wiki?curid=48848862", "title": "Lazzaretto of Manoel Island", "text": "Lazzaretto of Manoel Island\n\nThe Lazzaretto () is a former quarantine facility and hospital on Manoel Island in Gżira, Malta. It is a complex of various buildings dating back to between the 17th and 19th centuries. Most of the structures still exist, although they are in a bad state due to damage sustained during World War II and over 30 years of abandonment. It is planned that the Lazzaretto be restored.\n\nFrom 1526 onwards, Marsamxett Harbour began to be used for quarantine purposes. During the , a temporary lazzaretto was constructed on the island in the middle of the harbour, then known as the \"Isolotto\" and now called Manoel Island. Some warehouses a chapel dedicated to Saint Roch were also built at this point, but they were demolished in the late 18th century.\n\nIn 1643, the Grand Master of the Order of St. John, Giovanni Paolo Lascaris, decided to build a permanent lazzaretto due to fears of an epidemic. The Order acquired the island from the Church by exchanging it with some property at \"Tal-Fiddien\". The Lazzaretto as built by Lascaris consisted of a single building, but a second block was built and expanded by Grand Masters Nicolas Cotoner in 1670, Gregorio Carafa in 1683, Ramon Perellos y Roccaful in 1701 and António Manoel de Vilhena in around 1726. In 1797, Grand Master Emmanuel de Rohan-Polduc built a new block and some warehouses in the Lazzaretto.\nThe Lazzaretto was also used as a hospital, and it saw a lot of use during the plague epidemic of 1813–14, the cholera epidemic of 1865 and the plague epidemic of 1937. It also served as a military hospital for British, French and Italian soldiers during the Crimean War. Several notable figures stayed in the Lazzaretto throughout its history, including Lord Byron, Sir Walter Scott, Horace Vernet, Benjamin Disraeli and Alphonse de Lamartine.\n\nPart of the Lazzaretto which was known as the Profumo Office was used to fumigate incoming mail. Disinfected mail was marked with red wax seals from around 1816 to 1844, while a variety of cachets were used later on. Disinfection of mail on a large scale lasted until the 1880s, but was used in rare cases up to 1936.\n\nThe Lazzaretto remained in use by the health authorities until 1939, when it was requisitioned by the Admiralty to be used for military purposes during World War II. Between 1941 and 1942, it was used as a submarine depot, and the buildings were bombed a number of times by Italian or German aircraft. Many buildings were destroyed by this aerial bombardment, and some other structures had to be demolished due to the damage they had sustained.\n\nThe Lazzaretto reopened as a hospital in 1949, and remained so until the departure of the Royal Navy from Malta in the 1970s. It was subsequently abandoned, and it fell into a state of disrepair.\n\nSince its closure, part of the Lazzaretto was also used as a shelter for abandoned dogs.\n\nToday, the Lazzaretto is in a state of neglect, and parts of it have collapsed while others are in danger of collapsing.\n\nThe Lazzaretto is set to be restored by the development company MIDI plc, who also restored the nearby Fort Manoel and Fort Tigné. The planned restoration would include treatment of the existing stonework and reconstruction of destroyed structures. After restoration, the complex is to be converted into residences, offices, restaurants, a casino and a boutique hotel. An underground car park is also planned.\n\nThe complex is a Grade 2 national monument, and it is listed on the National Inventory of the Cultural Property of the Maltese Islands.\n\nThe Lazzaretto consists of a complex of buildings dating back to the 17th to 19th centuries. Nothing remains of the original 16th century temporary structure, and the oldest extant building within the complex is that built in 1643 during the magistracy of Grand Master Lascaris. This structure, which is known as the \"Palazzo Vecchio\" (Old Palace), the \"Palazzo Grande\" (Great Palace) or simply \"il-Palazz\" (the Palace), is two stories high and consists of eight halls built around a central courtyard. This building had a number of coats of arms on its façade, but these were defaced probably during the French occupation of Malta. The building also has two marble inscriptions, one undated and another dated 1787, while a third inscription dated 1797 was removed in the late 1960s. A copy of marble inscription dated 1814 is also affixed on the western façade of the building. The original inscription was affixed on a gallows in front of Fort Manoel, and it is now in storage in Valletta.\n\nThe complex also contains a building known as the New Palace, which was built in stages between the 1670s and the early 18th century. This block consists of warehouses built around two courtyards, and the façade facing the sea contains a series of high arches. The easternmost warehouse was demolished after it was bombed in World War II.\n\nIn the 18th century, a Pest House was built near the Old Palace for the isolation of patients suffering from the plague. A number of cattle sheds for the quarantining of animals were also built nearby. The Pest House was demolished to make way for a Disinfection Station, while the cattle sheds were destroyed in the war.\n\nBetween the Pest House and the Old Palace, there is the De Rohan Block which was completed in 1797. The Profumo Office was located near the De Rohan Block, but the building was destroyed in the war.\n\nThe Lazzaretto complex was surrounded by a high wall to prevent people from escaping. Six cemeteries were located in the area, but only one still exists today.\n\nA number of historical graffiti can be found within the Lazzaretto, with the earliest dated 1681 and the latest 1947.\n\n"}
{"id": "37918086", "url": "https://en.wikipedia.org/wiki?curid=37918086", "title": "Life story work", "text": "Life story work\n\nLife story work is a social work intervention with children and adults designed to recognise their past, present, and future. It is prominently used with children who will be adopted, and older adults as part of reminiscence therapies. Life story books are often incorporated into this work to give a visual aid and reminder of important events or feelings.\n\nLife story work as a concept has dated back to at least the 1960s, possibly further. The application of the concept to children in foster care and adoption was discussed in academia from the early 1980s onward. Life story work is well documented in the UK and Australia and has been incorporated into UK Adoption legislation. More recently it has been used in Eastern Europe.\n\nSocial workers should take the ultimate responsibility for ensuring that children who are expected to be adopted or to be in long term care have a life story book. Social workers often hold the most factual information about the child's background and reasons for becoming Looked After or Adopted and it is important that they provide this information for use in the life story work. Day-to-day carers such as; Foster Carers, Residential Support Workers or Adoptive Parents can offer the best informal life story work. They have the information about the day-to-day events in the child's life, their milestones and achievements.\n\nBirth Parents are a critical part of the life story work with adopted children as they can offer information to construct a family tree and provide pictures or descriptions of family members.\n\nSome suggest that life story work can be completed by any adult who is able and willing to spend time with the child and build a trusting relationship. However, some feel that some professionals such as student social workers, trainees and foster carers should not be encouraged to complete the life story work as they may not have the experience, training or ability to support the child with life story work over a long enough period of time. The person's needs must be taken into account regarding the gender, ethnicity, religion or culture of the adult assisting with the life story work.\n\nLife story books have been a part of adoption social workers' practice for over 30 years; though the quality of them has varied. It should take around 12 months to complete it though it may need to be updated as the child's understanding develops. Life story work is distinct from life story books, the process of life story work is to assist the person to understand and internalise the feelings associated with their past. However, a life story book does not need to involve the person and can be done by others, this is especially the case when done by social workers before placing a child for adoption. Rees & Goldberg state that a life story book should not include professional reports, later life letters, structured chronology (though this could be recorded in another, more child friendly, format), photo album and should not be an extension of the foster carer's memory book. However, life story books can often be seen as complementary or as an end product to life story work.\nA life story book is a system of recording information to answer the questions the participant may have in the future. It is an overview of a person's life to help them recall memories and understand their past.\nA child who does not fully understand their history is at risk of developing an imagined story of fictional family members leading to a misplaced sense of identity as they mature. Often, life story books are written from the perspective of the Past-Present though it has been suggested that writing it in this way causes the child anxiety as past issues may be too painful to come to terms with. Therefore, it has been suggested that a new approach: ‘Present – Past – Present – Future’ allows the child to feel that their life with their current family is secure and symbolically encourages the child to feel contained by their family.\n\nIt has been identified that ICT can assist with the presentation of life story work such as amending colour schemes to make it personal to the individual, and correcting spelling or grammatical errors. Hardware peripherals such as scanners, digital cameras and printers are also useful. Computer programmes have been developed to complete life story work with children in a way which they feel comfortable and do not find threatening.\n\nThe importance of digital technologies and digital media in the lives of children and young people has seen a growth in interest of the ability of these technologies to support self-reflection and build narrative coherence. In seeking to make the benefits of conventional life story work accessible to adolescents, Hammond and Cooper integrate the ideas of narrative psychology and build upon established approaches of undertaking life story work with younger children. Recognising that adolescents communicate differently than younger children, Hammond and Cooper incorporate a range of accessible digital technologies to provide interactive and practical activities which aim to support practitioners to empower adolescents to take the lead in the creation of, and reflection upon, their own autobiographical narratives.\n\nCreating stories for children that can create a link between their lives and behaviour and that of a fiction character can develop their understanding of why they think and feel the way they do.\n\nThe Children's Rights Director in England found that 71% of adopted children thought it was important to know about their lives before adoption. Specific questions that these children wanted answers to included why they were adopted and why they could not stay with birth families.\n\nThe Adoption and Children Act 2002 outlines the Local Authorities' and Adoption Agencies' expectations to provide information regarding individual's history upon request.\nGuidance relating to this Act specifically mentions Life Story Work to help them 'explore and understand their early history and life before their adoption'.\n\nSome Local Authorities in the UK have developed procedures which outline the need for all children who are Looked After to have life story work completed with them; usually in conjunction with the social worker and foster carer. Much of the foster carer's role is to collect items for the child's memory box or book and to encourage the child to participate.\n\nLife story work is used for people with dementia to support them to be able to tell the story of their life and to create visual tools to help their memories of past events. Present and future goals or plans should be identified.\n\nIt has also been found to be beneficial as a therapeutic tool for family members to assist in reviewing the person's life and enabling them to remember the person prior to the onset of dementia.\nStaff and carers of the person may also find it a useful tool to help them better care for a person. Furthermore, research shows that families feel reassured about staff care after life story work has been completed. It was also identified by staff to be a fun and interesting activity to do with dementia patients.\nLife story work should identify a person's achievements throughout life so this can allow the person with dementia to feel proud of themselves.\nLife story work has been linked to person centred/individualised care of people with dementia and national UK guidelines make particular reference to staff being able to learn about individuals' life stories.\n\nAn Australian study of ex-prisoners with 'intellectual' disabilities suggested that life story work may be beneficial to allow this group of people to better communicate their experiences. It has also been identified as a tool to promoting self-advocacy.\n\nResearch conducted within in-patient older adult mental health wards found that there was sometimes a shortage of qualified staff to complete the work and a reluctance to get involved due to other commitments and responsibilities. There were also limits to the amount of time they had available to complete the work and access to resources such as computers and scanners was identified as a problem.\n"}
{"id": "22812932", "url": "https://en.wikipedia.org/wiki?curid=22812932", "title": "Medicinal clay", "text": "Medicinal clay\n\nThe use of medicinal clay in folk medicine goes back to prehistoric times. Indigenous peoples around the world still use clay widely, which is related to geophagy. The first recorded use of medicinal clay goes back to ancient Mesopotamia.\n\nA wide variety of clays are used for medicinal purposes—primarily for external applications, such as the clay baths in health spas (mud therapy). Among the clays most commonly used are kaolin and the smectite clays such as bentonite, montmorillonite, and Fuller's earth.\n\nThe first recorded use of medicinal clay is on Mesopotamian clay tablets around 2500 B.C. Also, ancient Egyptians used clay. The Pharaohs’ physicians used the material as anti-inflammatory agents and antiseptics. It was used as a preservative for making mummies and is also reported that Cleopatra used clays to preserve her complexion.\n\nThe Ebers Papyrus of about 1550 BC (but containing the tradition going back many centuries earlier) is an important medical text from ancient Egypt. It describes the use of ochre for a wide variety of complaints, including for intestinal problems, as well as for various eye complaints.\n\nThis was a clay used in Classical Antiquity. It was mined on the island of Lemnos. Its use continued until the 19th century, as it was still listed in an important pharmacopoeia in 1848 (the deposits may have been exhausted by then).\n\nPliny reports about the Lemnian earth:\nif rubbed under the eyes, it moderates pain and watering from the same, and prevents the flow from the lachrymal ducts. In cases of haemorrhage it should be administered with vinegar. It is used against complaints of the spleen and kidneys, copious menstruation, also against poisons, and wounds caused by serpents.\n\nLemnian clay was shaped into tablets, or little cakes, and then distinctive seals were stamped into them, giving rise to its name \"terra sigillata\"—Latin for 'sealed earth'. Dioscorides also commented upon the use of \"terra sigillata\".\n\nAnother physician famous in antiquity, Galen, recorded numerous cases of the internal and external uses of this clay in his treatise on clay therapy.\nGalen... used as one of his means for curing injuries, festering wounds, and inflammations \"terra sigillata\", a medicinal red clay compressed into round cakes and stamped with the image of the goddess Diana. This clay, which came from the island of Lemnos, was known throughout the classical world.\n\nClay was prescribed by the Roman obstetrician, gynecologist, and pediatrician Soranus of Ephesus, who practiced medicine around 100-140 AD.\n\nThe other types of clay that were famous in antiquity were as follows.\n\n\nAll the above seem to have been bentonitic clays.\n\n\nIn medieval Persia, Avicenna (980-1037 CE), the 'Prince of Doctors', wrote about clay therapy in his numerous treatises.\n\nIbn al-Baitar (1197–1248), a Muslim scholar born at Malaga, Spain, and author of a famous work on pharmacology, discusses eight kinds of medicinal earth. The eight kinds are:\n\n\nA French naturalist Pierre Belon (1517–1564) was interested in investigating the mystery of the Lemnian clay. In 1543, he visited Constantinople where, after making enquiries, he encountered 18 types of different products marketed as Lemnian Earth (he was concerned about possible counterfeits).\n\nHe then made a special journey to Lemnos, where he continued his investigation, and tried to find the source of the clay. He discovered that it was extracted only once a year (on 6 August) under the supervision of Christian monks and Turkish officials.\n\nClay gathered from its original source deposit is refined and processed in various ways by manufacturers. This can include heating or baking the clay, since the raw clay tends to contain a variety of micro-organisms\n\nToo much processing, likewise, may reduce the clay's therapeutic potential. In particular, Mascolo et al. studied 'pharmaceutical grade clay' versus 'the natural and the commercial herbalist clay', and found an appreciable depletion of trace elements in the pharmaceutical grade clay. On the other hand, certain clays are typically heated or cooked before use.\n\nMedicinal clay is typically available in health food stores as a dry powder, or in jars in its liquid hydrated state – which is convenient for internal use. For external use, the clay may be added to the bath, or prepared in wet packs or poultices for application to specific parts of the body.\n\nOften, warm packs are prepared; the heat opens up the pores of the skin, and helps the interaction of the clay with the body.\n\nIn the European health spas, the clay is prepared for use in a multitude of ways – depending on the traditions of a particular spa; typically it is mixed with peat and matured in special pools for a few months or even up to two years.\n\n\"The majority of spas … use artificial ponds where the natural (\"virgin\") clay is mixed with mineral, thermo-mineral, or sea water that issues in the vicinity of the spas or inside the spa buildings.\"\n\nClays contain large amounts of trace minerals. It is common to see as many as 75 different trace minerals in Montmorillonite clays. Specific trace minerals that various clays possess vary very widely. Also, the amount of any particular trace mineral in any specific clay varies a lot among clays from different locations. For example, the amount of iron in various bentonite clays can vary from well below 1%, and up to 10%.\n\nMany types of skin conditions have been treated by the application of medicinal clay. Montmorillonite has shown its effectiveness in this area. It has also been used as a base ingredient for tissue engineering. Clay is used in many dermatological over-the-counter remedies, such as in acne treatments (this information may not be mentioned on the label specifically).\n\nThere are many over the counter remedies for internal use that contained clay before discontinuation. Examples include Kaopectate (Upjohn), Rheaban (Leeming Div., Pfizer), and Diar-Aid (Thompson Medical Co.). The labels on all of these showed the active ingredient to be Attapulgite, each tablet containing 600 (or 750 mg) of this component along with inert materials or adjuvants.\nHowever, since April 2003, attapulgite medication was discontinued due to lack of evidence according to the U.S. Foods and Drugs Administration.\n\nNumerous medicines also use Kaolinite clay, which has long been a traditional remedy to soothe an upset stomach. Also, Kaolin is or has been used as the active substance in liquid anti-diarrhea medicines such as Kaomagma. Such medicines were changed away from aluminium substances due to a scare over Alzheimer's disease, but have since changed back to compounds containing aluminum as they are most effective.\n\nIt has been used as a scientifically unsupported chelation treatment for heart disease and autism.\n\nOyanedel-Craver and Smith have studied sorption of four heavy metals (Pb, Cd, Zn and Hg) to 3 kinds of bentonite clay. The overall conclusion of the study was that the organoclays studied have considerable capacity for heavy metal sorption.\n\nIt has been found that prolonged exposure to bentonite in humans can actually have harmful effects.\n\nThe effects of weightlessness on human body were studied by NASA in the 1960s. Experiments demonstrated that weightlessness leads to a rapid bone depletion, so various remedies were sought to counter that. A number of pharmaceutical companies were asked to develop calcium supplements, but apparently none of them were as effective as clay. The special clay that was used in this case was Terramin, a reddish clay found in California. Benjamin Ershoff of the California Polytechnic Institute demonstrated that the consumption of clay counters the effects of weightlessness. He reported that \"the calcium in clay ...is absorbed more efficiently ... [clay] contains some factor or factors other than calcium which promotes improved calcium utilization and/or bone formation.\" He added, \"Little or no benefit was noted when calcium alone was added to the diet.\"\n\nSubstances discontinued such as kaolin and attapulgite were formerly considered gastric demulcents and diarrhea medication, until official studies by the USFDA disproved these views. Clays are classified as excipients and their main side-effects are that of neutral excipients, which is to impair and slow down absorption of antibiotics, hormones and heart medication amongst others by coating the digestive tract and this slowed down absorption can lead to increased toxicity of some medication (e.g. citrate salts) which can become toxic if not metabolized quickly enough, which is one contraindication of attapulgite.\nUsual mild side-effects are nausea, slowed down absorption of nutrients from food (in excess dosage of medicinal clay) and constipation.\n\n\n\n"}
{"id": "44897637", "url": "https://en.wikipedia.org/wiki?curid=44897637", "title": "Microperimetry", "text": "Microperimetry\n\nMicroperimetry, sometimes called Fundus related perimetry, is a type of visual field test which uses one of several technologies to create a \"retinal sensitivity map\" of the quantity of light perceived in specific parts of the retina in people who have lost the ability to fixate on an object or light source.\n\nVisual field testing is widely used to monitor pathologies affecting the periphery of vision such as glaucoma. During a conventional test, patients are asked to look steady (fixate) at a visual target, while light stimuli are projected at varying intensities in different retinal locations. This process is not, however, considered accurate in the evaluation of pathologies affecting the central part of the retina (macula and fovea centralis) patients with these pathologies are often unable to fixate reliably. By contrast, fundus perimetry, produces reliable results even in patients with unstable or eccentric fixation., or advance macular degeneration.\n\nWhen central vision is compromised, as in the case of macular scotoma, patients develop an eccentric or extra-foveal vision, normally with unstable fixation. \nThe retinal area used by eccentric viewers to substitute the foveal vision is known as the Preferred Retinal Locus (PRL) In Microperimetry systems, the fundus (eye) is imaged in real time, while an eye tracker compensates eye movements during stimuli projection, allowing correct matching between expected and projected stimulus position onto the retina. Simultaneously, the eye tracker plots the retinal movement during fixation attempt defining the PRL zone as well as fixation stability. Some microperimetry instruments calculate 2 different PRL zones during the examination. To create the fundus image an infrared telecamera is used, as in the case of the \"Nidek-MP1\", or a Scanning Laser Ophthalmoscope (SLO), as in the case of the \"Centervue-MAIA\".\n\nIn patients with central vision loss, microperimetry experts are able to analyse the eccentric retina in order to find zones with good retinal sensitivity. Once the best retinal area is selected, patients are asked to move their gaze towards that direction, while audio signals guide them to the desired target. This process is called biofeedback, and is based on the theory of brain plasticity. With several training sessions, some patients are able to gain better use of their peripheral vision.\n"}
{"id": "10088320", "url": "https://en.wikipedia.org/wiki?curid=10088320", "title": "Nicolae L. Lupu", "text": "Nicolae L. Lupu\n\nNicolae L. Lupu (November 4, 1876 – 1947) was a Romanian left-wing politician and social physician. Originally a leader of the Labor Party, which was joined with the Peasants' Party, Lupu served as Interior Minister in 1919–1920. He formed his own Peasants' Party–Lupu in 1927, and also steered the creation of a League Against Usury. His group became a dissident faction of the National Peasants' Party, and was reestablished, after World War II, as the Democratic Peasants' Party–Lupu. \n"}
{"id": "43315001", "url": "https://en.wikipedia.org/wiki?curid=43315001", "title": "Nik &amp; Eva Speakman", "text": "Nik &amp; Eva Speakman\n\nNik Speakman (born 1961) and Eva Speakman (born c.1969), known collectively as The Speakmans, are British therapists and life coaches.\n\nNik Speakman started a finance company, Personal Money Management, aged 26 with his former partner and was working as a success coach at the time of his and Eva's 2005 appearance in \"That's Rich\", a Granada Television series focusing on entrepreneurs in the north west of England. Eva Speakman ran the Heavenly Bodies gym in Oldham, at the time of the show.\n\nThe Speakmans were subsequently hosts of a Living TV show, \"A Life Coach Less Ordinary\", and in 2007 published a book, \"You Can Be Fantastic, Too!\". The couple hosted a television pilot for the American FOX network, \"Panic Attack\", in 2010.\n\nA second daytime television show, \"The Speakmans\", was broadcast by ITV. In \"The Speakmans\" they attempted to 'successfully treat ordinary people with extraordinary problems'. The show aired for 20 episodes, beginning on 14 July 2014.\n\nThe Speakmans are regular columnists in 'Love It Magazine'. The couple regularly appears on ITV's \"This Morning\" hosting a segment about anxiety issues and phobias.\n\nThe Speakmans use 'schema conditioning psychotherapy' to 'find and tackle the root cause of anxiety disorders'. Nik Speakman said of their approach on their television series that \"We're really hoping people will look at it and think: 'I can apply that to my life.' Even if it’s an entirely different problem, ultimately our therapy is the same...That’s what we want. There's no need for anyone to suffer. There is absolute hope for everyone.\" The Speakmans also believe in \"practising gratitude\", asserting that \"Everybody’s got problems and issues but equally everybody has got amazing things to be thankful for.\"\n\nTherapeutic techniques utilized by the couple include putting clients into a DeLorean DMC-12 sports car in order to 'travel 'back to the future' to confront past demons', a reference to the DeLorean time machine from the \"Back to the Future\" film series. The Speakmans presented a DeLorean car to the former Liberal Member of Parliament for Rochdale, Sir Cyril Smith, at his 80th birthday party in 2008.\n\nNoted celebrity clients of the Speakmans have included Peter Andre, Kerry Katona, Kym Marsh, Liz McClarnon, Katie Price, Holly Willoughby and Jeremy Kyle.\n\nThe Speakmans have diplomas in an unspecified subject from Newcastle University, but are not accredited psychotherapists.\n\nIn December 2015, The Speakmans appeared on BBC Radio 2's Breakfast Show with Sara Cox \n\nThe Speakmans have lived in Littleborough near Rochdale in Greater Manchester since 2000. The Speakmans converted their house from a former pub and restaurant into a nine-bedroomed house. The couple put their house up for sale in 2010 as they worked on the \"Panic Attack\" pilot in the United States, and planned to move to the south of England owing to their work in London and America.\n\nThe couple have two children and are supporters of a local children's home, the Duke Bar Burnley Wood NCH Children's Centre, and are Celebrity Ambassadors of Variety Children's Charity.\n\nIn October 2017, the Speakmans were awarded a \"Legends of Industry Award\" for their contribution to the Therapeutic/Motivational Industry.\n"}
{"id": "2984836", "url": "https://en.wikipedia.org/wiki?curid=2984836", "title": "Ophthalmology in medieval Islam", "text": "Ophthalmology in medieval Islam\n\nOphthalmology was one of the foremost branches in medieval Islamic medicine. The oculist or \"kahhal\" (کحال), a somewhat despised professional in Galen’s time, was an honored member of the medical profession by the Abbasid period, occupying a unique place in royal households. Medieval Islamic scientists (unlike their classical predecessors) considered it normal to combine theory and practice, including the crafting of precise instruments, and therefore found it natural to combine the study of the eye with the practical application of that knowledge. The specialized instruments used in their operations ran into scores. Innovations such as the “injection syringe”, a hollow needle, invented by Ammar ibn Ali of Mosul, which was used for the extraction by suction of soft cataracts, were quite common.\n\nMuslim physicians described such conditions as pannus, glaucoma (described as ‘headache of the pupil’), phlyctenulae, and operations on the conjunctiva. They were the first to use the words 'retina' and 'cataract'.\n\nTo become a practitioner, there was no one fixed method or path of training. There was even no formal specialization in the different branches of medicine, as might be expected. But some students did eventually approximate to a specialist by acquiring proficiency in the treatment of certain diseases or in the use of certain drugs.\n\nNevertheless, it was standard and necessary to learn and understand the works and legacy of predecessors. Among those one can mention, \"The alteration of the eye\" by Yuhanna ibn Masawayh, whose work can be considered the earliest work on Ophthalmology, followed by Hunain ibn Ishaq, known in the west as Johannitius, for his work \"The ten treatises of the eye\". One of Hunain ibn Ishaq's innovations was to describe the crystalline lens as being located in the exact center of the eye.\n\nThe next major landmark text on ophthalmology was the \"Choice of Eye Diseases\" written in Egypt by the Iraqi Ammar ibn Ali al-Mawsili who attempted the earliest extraction of cataracts using suction. He invented a hollow metallic syringe, which he applied through the sclerotic and successfully extracted the cataracts through suction. He wrote the following on his invention:\n\nAvicenna, in \"The Canon of Medicine\" (c. 1025), described sight as one of the five external senses. The Latin word \"retina\" is derived from Avicenna's Arabic term for the organ.\n\nIn his \"Colliget\", Averroes (1126–1198) was the first to attribute photoreceptor properties to the retina, and he was also the first to suggest that the principal organ of sight might be the arachnoid membrane (\"aranea\"). His work led to much discussion in 16th century Europe over whether the principal organ of sight is the traditional Galenic crystalline humour or the Averroist \"aranea\", which in turn led to the discovery that the retina is the principal organ of sight.\n\nIbn al-Nafis wrote a large textbook on ophthalmology called \"The Polished Book on Experimental Ophthalmology\". The book is divided into two sections: \"On the Theory of Ophthalmology\" and \"Simple and Compunded Ophthalmic Drugs\". Other significant works in medieval Islamic ophthalmology include Rhazes’ \"Continens\", Ali ibn Isa al-Kahhal’s \"Notebook of the Oculists\", and the ethnic Assyrian Christian Jibrail Bukhtishu’s \"Medicine of the Eye\", among numerous others.\n\nIn the Ottoman Empire, and well into the Republic of Turkey of the 20th century, a class of ambulatory eye surgeons, popularly known as the ‘kırlangıç oğlanları’ (‘sons of the swallow’) operated on cataract using special knives. From contemporary sources can be glimpsed that the reputation of these “blinding frauds” was far from spotless.\n\n\n"}
{"id": "24577201", "url": "https://en.wikipedia.org/wiki?curid=24577201", "title": "Pacific Islands Families Study", "text": "Pacific Islands Families Study\n\nThe Pacific Islands Families Study is a long-running, cohort study of 1398 children (and their parents) of Pacific Islands origin born in Auckland, New Zealand during the year 2000.\nThe cohort of participants was selected from babies born between 15 March 2000 and 17 December 2000 at Middlemore Hospital with at least one parent identifying as having Pacific Islands origin.\n\nThe three overall objectives of the PIF Study are:\n\nThe study has collected data from mothers, fathers, children and teachers. Data collection phases have occurred at 6 weeks after birth, 12 months, 24 months, 4 years, 6 years and 9 years.\n\nThe study is administered within the Faculty of Health and Environmental Sciences at Auckland University of Technology.\nMajor funding has been provided by the Foundation for Research, Science and Technology and the Health Research Council of New Zealand.\n\nAt approximately 6 weeks after birth, data were collected from interviews of the \"primary\" caregiver (usually the birth mother) and the \"collateral\" caregiver (the partner of the primary, usually the father).\nData were collected from 1376 families in relation to 1398 children (including 22 pairs of twins).\nThe ethnic mix of the original cohort was 47% Samoan, 21% Tongan, 17% Cook Island Maori, 4% Niuean and 11% other Pacific or non-Pacific.\n\nAt 12 months of age, 1224 primary caregivers and 825 collateral caregivers were interviewed in relation to 1241 children.\nAt 24 months, 1144 primary caregivers and 854 collateral caregivers were interviewed in relation to 1162 children.\n\nDetails of the first two years can be found in a technical report.\n\nAt the age of 4 years, data were collected from primary caregiver interviews and separate child assessments. At the age of 6 years, data were collected from primary caregiver interviews, collateral caregiver interviews, child assessments and teacher evaluations.\n\nAs each child turned 9, data were collected from primary caregiver interviews, child assessments and teacher child evaluations. , data on 11-year-olds are being collected from primary and collateral caregiver interviews, child assessments and teacher evaluations.\n\nAt 6 weeks of age, a total of 1,376 mothers (of 1398 children, including 22 pairs of twins) were interviewed. Of those, 1224 (89%) participated at 12-months and 1144 (83%) participated at 24-months. No important differential attrition was observed.\n\nKey findings for the first seven years are given in a summary of findings brochure.\n\nResearch topics resulting from the study include \npost natal depression, \nchronic middle ear disease, \nintimate partner violence, \nobesity, \ngambling, \nchild behaviour, \nmental health \nand smacking.\n\n"}
{"id": "22196821", "url": "https://en.wikipedia.org/wiki?curid=22196821", "title": "Parathyroiditis", "text": "Parathyroiditis\n\nParathyroiditis is a condition involving inflammation of the parathyroid gland.\n\nIt can be associated with hyperparathyroidism, though most cases are asymptomatic.\n"}
{"id": "53953741", "url": "https://en.wikipedia.org/wiki?curid=53953741", "title": "Patient Education and Counseling", "text": "Patient Education and Counseling\n"}
{"id": "4570698", "url": "https://en.wikipedia.org/wiki?curid=4570698", "title": "Post hoc analysis", "text": "Post hoc analysis\n\nIn a scientific study, post hoc analysis (from Latin \"post hoc\", \"after this\") consists of statistical analyses that were not specified before the data was seen. This typically creates a multiple testing problem because each potential analysis is effectively a statistical test. Multiple testing procedures are sometimes used to compensate, but that is often difficult or impossible to do precisely. \"Post hoc\" analysis that is conducted and interpreted without adequate consideration of this problem is sometimes called \"data dredging\" by critics, in part because the statistical associations that it finds may be spurious. \n\nSometimes the temptation to engage in \"post hoc\" analysis is motivated by a desire to produce positive results or see a project as successful. In the case of pharmaceutical research, there may be significant financial consequences to a failed trial, although the US Food and Drug Administration does not accept \"post hoc\" analysis. \n\nIn some cases, additional subgroup analysis may be requested by scientific peers or the editors of academic journals. In one such incident, journal editors demanded that the statistician Richard Peto provide a \"post hoc\" analysis of subgroups for the use of aspirin as secondary prevention for people who had experienced heart attacks. He refused the request as being statistically unsound and likely to lead to nonsensical results. When they persisted, he provided the editors with a subgroup analysis that evaluated the supposed response based upon the patients' astrological signs.\n\n"}
{"id": "20325140", "url": "https://en.wikipedia.org/wiki?curid=20325140", "title": "Preventable causes of death", "text": "Preventable causes of death\n\nThe World Health Organization has traditionally classified death according to the primary type of disease or injury. However, causes of death may also be classified in terms of preventable risk factors—such as smoking, unhealthy diet, sexual behavior, and reckless driving—which contribute to a number of different diseases. Such risk factors are usually not recorded directly on death certificates, although they are acknowledged in medical reports.\n\nIt is estimated that of the roughly 150,000 people who die each day across the globe, about two thirds—100,000 per day—die of age-related causes because they have aged. In industrialized nations the proportion is much higher, reaching 90 percent.<ref name=\"doi10.2202/1941-6008.1011\"></ref> Thus, albeit indirectly, biological aging (senescence) is by far the leading cause of death. Whether senescence as a biological process itself can be slowed down, halted, or even reversed is a subject of current scientific speculation and research.\n\nLeading causes of preventable death worldwide as of the year 2001, according to researchers working with the Disease Control Priorities Network (DCPN) and the World Health Organization (WHO). (The WHO's 2008 statistics show very similar trends.)\n\nIn 2001, on average 29,000 children died of preventable causes each day (that is, about 20 deaths per minute). The authors provide the context:\nThe three most common preventable causes of death in the population of the United States are smoking, high blood pressure, and being overweight.\n\nVarious injuries are the leading cause of death in children 9–17 years of age. In 2008, the top five worldwide unintentional injuries in children are as follows:\n\n"}
{"id": "47676832", "url": "https://en.wikipedia.org/wiki?curid=47676832", "title": "Prism fusion range", "text": "Prism fusion range\n\nThe prism fusion range (PFR) or fusional vergence amplitude is a clinical eye test performed by orthoptists, optometrists, and ophthalmologists to assess motor fusion, specifically the extent to which a patient can maintain binocular single vision (BSV) in the presence of increasing vergence demands. Motor fusion is largely accounted to amplitudes of fusional vergences and relative fusional vergences. Fusional vergence is the maximum vergence movement enabling BSV and the limit is at the point of diplopia (double vision). Relative fusional vergence is the maximum vergence movement enabling a patient to see a comfortable clear image and the limit is represented by the first point of blur. These motor fusion functions should fall within average values so that BSV can be comfortably achieved. Excessive stress on the vergence system or inability to converge or diverge adequately can lead to asthenopic symptoms, which generally result from decompensation of latent deviations (heterophoria) or loss of control of ocular misalignments. Motor anomalies can be managed in various ways, however, in order to commence treatment, motor fusion testing such as the PFR is required. \n\nThe PFR involves placing a prism bar in front of an eye. In a patient with BSV, a natural shift of the eye occurs. When measuring horizontal fusion ranges, base in prisms assess fusional divergence while base out prisms assess fusional convergence. The vertical fusional vergence amplitude can also be measured with base up and base down prisms although the horizontal PFR is typically the main focus when testing. When performing the PFR, prism strengths are increased, placing greater demand on the vergence system, eventually resulting in a break point accompanying diplopia. Break point, recovery and blur are key aspects of this assessment. The break point occurs at the loss of BSV, recovery point when BSV is regained from break and blur point is at the loss of comfortable BSV. These stops can be subjectively indicated when the patient notices a double or blurred image. Both subjective and objective measurements can be considered however the examiner’s objective observation is the gold standard.\n\nThis test is designed to assess the following:\n\nThe PFR is performed in bright lighting conditions at near (33 cm) or far (6m), using prism bars (horizontal and vertical) and an accommodative fixation target such as a letter on a fixation stick for near, or a Snellen Chart letter for distance. The patient should wear their refractive correction for the distance being tested.\n\nThe following method relates to assessment of the horizontal PFR.\n\nThe results from this method of assessment rely on the patient’s responses, and are therefore subjective. The assessment should also be performed objectively, in which the examiner observes the eye’s movement behind the prism, anticipating the break point at which the eye can no longer make a vergence movement to maintain BSV.\n\nWhen recording the results obtained from the PFR, it is important to include the following in order:\n\nThere are two ways to record the PFR results, the first being the fusion range (break without recovery) and the other including break and recovery. Follow the examples below for guidance:\n\nFusion range:\nBreak + recovery:\nPatient results should be compared to the normal values for prism fusional amplitudes to determine if the patient has any anomalies. Recovery should ideally be within 5Δ of break point.\nIf patient results do not reflect the normal values, they may have the following issues:\n\n\n\n\n"}
{"id": "47329282", "url": "https://en.wikipedia.org/wiki?curid=47329282", "title": "Regional hospital", "text": "Regional hospital\n\nA regional hospital is a hospital that serves a larger area than a local hospital. Regional hospitals sometimes serve specific needs that cannot be adequately met by a local or rural hospital, such as treating ebola or obstetric fistula, or providing elective orthopedic surgery.\n"}
{"id": "31423189", "url": "https://en.wikipedia.org/wiki?curid=31423189", "title": "Riad Michael", "text": "Riad Michael\n\nRiad Alexander Michael, also known as Geyser, is a German electronic musician as well as physician.\n\nAfter his noticed and multiple positively reviewed debut album \"Digger\" as \"Geyser\" has been published under the Colonian label \"Mehrwert Records\", he has founded his own label \"Geyser Recordings\" under which his further studio albums have been released. While Geyser has been described as downtempo, trip hop, lounge, big beat, drum and bass, dub, funk, fusion and abstract pop holding an originality that would do without style references, the productions as Riad Michael have been classified under techno, tech house, progressive house, trance, IDM and ambient. \n\nAs physician Riad Michael has reviewed in his dissertation the therapeutic drawing of music in psychogenic and psychosomatic illnesses as well as absolved a scientific further education in music therapy.\n\nAlbums\n\nSingles & EPs\n\nAlbums\n\nSingles & EPs\n\nCompilations\n\n\n"}
{"id": "49397388", "url": "https://en.wikipedia.org/wiki?curid=49397388", "title": "Saima Wazed", "text": "Saima Wazed\n\nSaima Wazed Hossain (born 9 December 1972) is a Bangladesh Autism activist. She is the daughter of Bangladesh's Prime Minister Sheikh Hasina. She is a member of World Health Organization's 25-member Expert Advisory Panel on mental health. To her family, she is known simply as \"Putul\".\n\nShe was born to Sheikh Hasina, the present Prime Minister of Bangladesh and M. A. Wazed Miah, a nuclear scientist. Her brother is Sajeeb Wazed Joy. She graduated from Barry University. She is a licensed school psychologist.\n\nShe organized the first South Asian conference on Autism in 2011 in Dhaka, Bangladesh. She is the chairperson of National Advisory Committee on Autism and Neurodevelopmental disorders. She campaigned for “Comprehensive and Coordinated Efforts for the Management of Autism Spectrum Disorders” resolution at the World Health Assembly which adopted the resolution, Autism Speaks praised her for spearheading \"a truly global push for support for this resolution\". She is a member of World Health Organization's 25-member Expert Advisory Panel on mental health.\n\nIn November, 2016, Wazed had been elected as chairperson of International Jury Board meeting of UNESCO for Digital Empowerment of Persons with Disabilities.\n\nIn April 2017, Wazed has been designated as WHO Champion for Autism” in South-East Asia. In July, 2017 she became the Goodwill Ambassador of the World Health Organization (WHO) for autism in South-East Asia Region.\n\nIn 2016, Wazed has conferred World Health Organization's South-East Asia Region Award for Excellence in Public Health. In 2017, she has been awarded the \"International Champion Award\" for her outstanding contribution to the field of autism.She received a distinguished alumni award from Barry University for her activism.\n\nSaima is married to Khandakar Masrur Hossain Mitu, son of Khandaker Mosharraf Hossain. The couple have 4 children.\n"}
{"id": "55524604", "url": "https://en.wikipedia.org/wiki?curid=55524604", "title": "Selibi Phikwe Government Hospital", "text": "Selibi Phikwe Government Hospital\n\nSelibi Phikwe Government Hospital is a government-run district hospital located in Selebi Phikwe,is a mining town located in the Central District of Botswana. It had a population of 49,849 in 2001 which is now estimated to have risen to c.52000.\n\nSelibi Phikwe Government Hospital is a Government health institution founded in 1970. The institution is located at the central district of Botswana in a town called Selebi Phikwe.\n\nBotswana Ministry of Health\n"}
{"id": "40548428", "url": "https://en.wikipedia.org/wiki?curid=40548428", "title": "Stephen Crohn", "text": "Stephen Crohn\n\nStephen Lyon Crohn (September 5, 1946 – August 23, 2013) also known as \"The man who can't catch AIDS\", was a man notable for a genetic mutation, which caused him to be immune to AIDS. He was a great-nephew of Burrill Bernard Crohn, for whom Crohn's disease is named.\n\nCrohn had the \"delta 32\" mutation on the CCR5 receptor, a protein on the surface of white blood cells that is involved in the immune system and serves as an access route for many forms of HIV virus to enter and infect host cells. This mutation rendered him effectively immune to many forms of HIV.\n\nCrohn committed suicide by a drug overdose on oxycodone and benzodiazepines at the age of 66.\n\n\n"}
{"id": "21446550", "url": "https://en.wikipedia.org/wiki?curid=21446550", "title": "Street dentistry", "text": "Street dentistry\n\nStreet dentistry is the unlicensed practice of dentistry in the street, usually for people who are unable to afford licensed dental care.\n\nBefore the 20th century, dentistry was largely unregulated. In Europe during the Middle Ages, it was often practiced by monks, who were the most educated of the period. Barbers and blacksmiths, too, performed dental services. One of the first attempts to regulate the practice of dentistry came in France in 1400, when royal decrees prohibited barbers not in the Guild of Barbers from performing surgical procedures except for bleeding, cupping, leeching, and extracting teeth.\n\nIn the United States in 1840, Horace H. Hayden and Chapin Harris established the world's first school of dentistry, the Baltimore College of Dental Surgery, and created the Doctor of Dental Surgery (DDS) degree. In the same year, the world’s first national dental organization, the American Society of Dental Surgeons, was founded. In 1841, Alabama instituted the first dental practice act, regulating the practice of dentistry in the United States.\n\nIn New York around the turn of the century, street dentists like Edgar R.R. \"Painless\" Parker flourished. Despite dentistry becoming regulated, unlicensed dentists still practiced, often offering inferior services, prompting some to call for their prosecution.\n\nAs many as 5000 unlicensed dentists may have practiced in New York in the early 1900s. In 1900, 283 complaints were received by the Law Committee of the New York State Dental Society. Some patients died from infections and abscesses resulting from lack of sanitation. Others died from improper administration of anaesthetic. According to a newspaper report in 1910, many of these dentists were immigrants whose home countries did not regulate dentistry as stringently as did the United States.\n\nIn India, street services are plentiful, with dentists operating alongside other street services, such as apothecaries, repairmen, and barbers. These practitioners charge far less than licensed dentists, often charging as little as 125 rupees for a procedure such as a bridge—which, at a licensed dentist, could be as expensive as 10,000 rupees. Many learn the trade from their parents.\n\nStreet dentistry is not confined to India. In Paris in 2003, 23 unlicensed dentists who practiced out of cafes and grocery stores were arrested. The dentists involved were Syrian immigrants who provided dental services mostly for other immigrants who did not have health insurance.\n\nIn 1998, a man was arrested in Van Nuys, California for operating an unlicensed dental practice out of the back of a toy store in a strip mall.\n\nIn India, Chapter V, Section 49 of the Dentist Act of 1948 requires dentists, dental mechanics, and dental hygienists to be licensed, making street dentistry illegal, though street dentists continue to practice. Most countries in the developed world have laws preventing the unlicensed practice of dentistry (see Dentistry throughout the world).\n\n"}
{"id": "1240348", "url": "https://en.wikipedia.org/wiki?curid=1240348", "title": "Strength training", "text": "Strength training\n\nStrength training is a type of physical exercise specializing in the use of resistance to induce muscular contraction which builds the strength, anaerobic endurance, and size of skeletal muscles.\n\nWhen properly performed, strength training can provide significant functional benefits and improvement in overall health and well-being, including increased bone, muscle, tendon, and ligament strength and toughness, improved joint function, reduced potential for injury, increased bone density, increased metabolism, increased fitness and improved cardiac function. Training commonly uses the technique of progressively increasing the force output of the muscle through incremental weight increases and uses a variety of exercises and types of to target specific muscle groups. Strength training is primarily an anaerobic activity, although some proponents have adapted it to provide the benefits of aerobic exercise through circuit training.\n\nStrength training is typically associated with the production of lactate, which is a limiting factor of exercise performance. Regular endurance exercise leads to adaptations in skeletal muscle which can prevent lactate levels from rising during strength training. This is mediated via activation of PGC-1alpha which alter the LDH (lactate dehydrogenase) isoenzyme complex composition and decreases the activity of the lactate generating enzyme LDHA, while increasing the activity of the lactate metabolizing enzyme LDHB.\n\nSports where strength training is central are bodybuilding, weightlifting, powerlifting, strongman, Highland games, shot put, discus throw, and javelin throw. Many other sports use strength training as part of their training regimen, notably tennis, American football, wrestling, track and field, rowing, lacrosse, basketball, pole dancing, hockey, professional wrestling, rugby union, rugby league, and soccer. Strength training for other sports and physical activities is becoming increasingly popular.\n\nThe benefits of weight training include greater muscular strength, improved muscle tone and appearance, increased endurance and enhanced bone density.\n\nMany people take up weight training to improve their physical attractiveness. There is evidence that a body type consisting of broad shoulders and a narrow waist, attainable through strength training, is the most physically attractive male attribute according to women participating in the research. Most men can develop substantial muscles; most women lack the testosterone to do it, but they can develop a firm, \"toned\" (see below) physique, and they can increase their strength by the same proportion as that achieved by men (but usually from a significantly lower starting point). An individual's genetic make-up dictates the response to weight training stimuli to a significant extent. Training can not exceed a muscle's intrinsic genetically determined qualities though polymorphic expression does occur e.g., Myosin heavy chains\n\nStudies also show that people are able to tell the strength of men based on photos of their bodies and faces, and that physical appearance indicates cues of strengths that are often linked to a man's physical formidability and, therefore, his attractiveness. This is aligned with studies that reveal those who undergo strength training attain more self-esteem and body cathexis when compared to individuals who do not undergo training or exercise. In addition, people who undergo strength training tend to have a more favorable body image even than those who also engage in regular physical activities such as walking and running. More women are also increasingly revealed to be dissatisfied with their body today than those surveyed in 1984 and they often turn to exercise such as strength training to improve their body shape.\n\nWorkouts elevate metabolism for up to 14 hours following 45-minutes of vigorous exercise.\n\nStrength training also provides functional benefits. Stronger muscles improve posture, provide better support for joints, and reduce the risk of injury from everyday activities. Older people who take up weight training can prevent some of the loss of muscle tissue that normally accompanies aging—and even regain some functional strength—and by doing so become less frail. They may be able to avoid some types of physical disability. Weight-bearing exercise also helps to prevent osteoporosis and to improve bone strength in those with osteoporosis. The benefits of weight training for older people have been confirmed by studies of people who began engaging in it even in their 80s and 90s.\n\nThough strength training can stimulate the cardiovascular system, many exercise physiologists, based on their observation of maximal oxygen uptake, argue that aerobics training is a better cardiovascular stimulus. Central catheter monitoring during resistance training reveals increased cardiac output, suggesting that strength training shows potential for cardiovascular exercise. However, a 2007 meta-analysis found that, though aerobic training is an effective therapy for heart failure patients, combined aerobic and strength training is ineffective.\n\nStrength training may be important to metabolic and cardiovascular health. Recent evidence suggests that resistance training may reduce metabolic and cardiovascular disease risk. Overweight individuals with high strength fitness exhibit metabolic/cardiovascular risk profiles similar to normal-weight, fit individuals rather than overweight unfit individuals.\n\nFor many people in rehabilitation or with an acquired disability, such as following stroke or orthopaedic surgery, strength training for weak muscles is a key factor to optimise recovery. For people with such a health condition, their strength training is likely to need to be designed by an appropriate health professional, such as a physiotherapist or an occupational therapist.\n\nStronger muscles improve performance in a variety of sports. Sport-specific training routines are used by many competitors. These often specify that the speed of muscle contraction during weight training should be the same as that of the particular sport.\n\nOne side effect of intense exercise is increased levels of dopamine, serotonin, and norepinephrine, which can help to improve mood and counter feelings of depression (It should be noted that dopamine and serotonin were not found to be increased by resistance training).\n\nDeveloping research has demonstrated that many of the benefits of exercise are mediated through the role of skeletal muscle as an endocrine organ. That is, contracting muscles release multiple substances known as myokines which promote the growth of new tissue, tissue repair, and various anti-inflammatory functions, which in turn reduce the risk of developing various inflammatory diseases.\n\nThe basic principles of strength training involve a manipulation of the number of repetitions (reps), sets, tempo, exercises and force to cause desired changes in strength, endurance or size by overloading of a group of muscles. The specific combinations of reps, sets, exercises, resistance and force depend on the purpose of the individual performing the exercise: to gain size and strength multiple (4+) sets with fewer reps must be performed using more force.\nA wide spectrum of regimens can be adopted to achieve different results, but the classic formula recommended by the American College of Sports Medicine reads as follows:\n\nTypically failure to use good form during a training set can result in injury or an inability to meet training goals – since the desired muscle group is not challenged sufficiently, the threshold of overload is never reached and the muscle does not gain in strength. There are cases when cheating is beneficial, as is the case where weaker groups become the weak link in the chain and the target muscles are never fully exercised as a result.\n\nStrength training has a variety of specialized terms used to describe parameters of strength training:\n\nFor developing endurance, gradual increases in volume and gradual decreases in intensity is the most effective program.\nSets of thirteen to twenty repetitions develop anaerobic endurance, with some increases to muscle size and limited impact on strength.\n\nIt has been shown that for beginners, multiple-set training offers minimal benefits over single-set training with respect to either strength gain or muscle mass increase, but for the experienced athlete multiple-set systems are required for optimal progress. However, one study shows that for leg muscles, three sets are more effective than one set.\n\nBeginning weight-trainers are in the process of training the neurological aspects of strength, the ability of the brain to generate a rate of neuronal action potentials that will produce a muscular contraction that is close to the maximum of the muscle's potential.\n\nWeights for each exercise should be chosen so that the desired number of repetitions can just be achieved.\n\nIn one common method, weight training uses the principle of progressive overload, in which the muscles are overloaded by attempting to lift at least as much weight as they are capable. They respond by growing larger and stronger. This procedure is repeated with progressively heavier weights as the practitioner gains strength and endurance.\n\nHowever, performing exercises at the absolute limit of one's strength (known as one rep max lifts) is considered too risky for all but the most experienced practitioners. Moreover, most individuals wish to develop a combination of strength, endurance and muscle size. One repetition sets are not well suited to these aims. Practitioners therefore lift lighter (sub-maximal) weights, with more repetitions, to fatigue the muscle and all fibres within that muscle as required by the progressive overload principle.\n\nCommonly, each exercise is continued to the point of momentary muscular failure. Contrary to widespread belief, this is not the point at which the individual thinks they cannot complete any more repetitions, but rather the first repetition that fails due to inadequate muscular strength. Training to failure is a controversial topic with some advocating training to failure on all sets while others believe that this will lead to overtraining, and suggest training to failure only on the last set of an exercise. Some practitioners recommend finishing a set of repetitions just before reaching a personal maximum at a given time. Adrenaline and other hormones may promote additional intensity by stimulating the body to lift additional weight (as well as the neuro-muscular stimulations that happen when in \"fight-or-flight\" mode, as the body activates more muscle fibres), so getting \"psyched up\" before a workout can increase the maximum weight lifted.\n\nWeight training can be a very effective form of strength training because exercises can be chosen, and weights precisely adjusted, to safely exhaust each individual muscle group after the specific numbers of sets and repetitions that have been found to be the most effective for the individual. Other strength training exercises lack the flexibility and precision that weights offer.\n\nSplit training involves working no more than three muscle groups or body parts per day, instead spreading the training of specific body parts throughout a training cycle of several days. It is commonly used by more advanced practitioners due to the logistics involved in training all muscle groups maximally. Training all the muscles in the body individually through their full range of motion in a single day is generally not considered possible due to caloric and time constraints. Split training involves fully exhausting individual muscle groups during a workout, then allowing several days for the muscle to fully recover. Muscles are worked roughly twice per week and allowed roughly 72 hours to recover. Recovery of certain muscle groups is usually achieved on days while training other groups, i.e. a 7-day week can consist of a practitioner training trapezius, side shoulders and upper shoulders to exhaustion on one day, the following day the arms to exhaustion, the day after that the rear, front shoulders and back, the day after that the chest. In this way all mentioned muscle groups are allowed the necessary recovery.\n\nThree important variables of strength training are intensity, volume, and frequency. Intensity refers to the amount of work required to achieve the activity and is proportional to the mass of the weights being lifted. Volume refers to the number of muscles worked, exercises, sets, and reps during a single session. Frequency refers to how many training sessions are performed per week.\n\nThese variables are important because they are all mutually conflicting, as the muscle only has so much strength and endurance, and takes time to recover due to microtrauma. Increasing one by any significant amount necessitates the decrease of the other two, e.g. increasing weight means a reduction of reps, and will require more recovery time and therefore fewer workouts per week. Trying to push too much intensity, volume and frequency will result in overtraining, and eventually lead to injury and other health issues such as chronic soreness and general lethargy, illness or even acute trauma such as avulsion fractures. A high-medium-low formula can be used to avoid overtraining, with either intensity, volume, or frequency being high, one of the others being medium, and the other being low. One example of this training strategy can be found in the following chart:\n\nA common training strategy is to set the volume and frequency the same each week (e.g. training 3 times per week, with 2 sets of 12 reps each workout), and steadily increase the intensity (weight) on a weekly basis. However, to maximize progress to specific goals, individual programs may require different manipulations, such as decreasing the weight, and increase volume or frequency.\n\nMaking program alterations on a daily basis (daily undulating periodization) seems to be more efficient in eliciting strength gains than doing so every 4 weeks (linear periodization), but for beginners there are no differences between different periodization models.\n\nThere are many complicated definitions for periodization, but the term simply means the division of the overall training program into periods which accomplish different goals.\n\nPeriodization is the modulating of volume, intensity, and frequency over time, to both stimulate gains and allow recovery.\n\nIn some programs for example; volume is decreased during a training cycle while intensity is increased. In this template, a lifter would begin a training cycle with a higher rep range than he will finish with.\n\nFor this example, the lifter has a 1 rep max of 225 lb:\n\nThis is an example of periodization where the number of repetitions decreases while the weight increases.\n\nThere are many methods of strength training. Examples include weight training, circuit training, isometric exercise, gymnastics, plyometrics, Parkour, yoga, Pilates, Super Slow.\n\nStrength training may be done with minimal or no equipment, for instance bodyweight exercises. Equipment used for strength training includes barbells and dumbbells, weight machines and other exercise machines, weighted clothing, resistance bands, , Swiss balls, wobble boards, indian clubs, pneumatic exercise equipment, hydraulic exercise equipment.\n\nStrength training exercise is primarily anaerobic. Even while training at a lower intensity (training loads of ~20-RM), anaerobic glycolysis is still the major source of power, although aerobic metabolism makes a small contribution. Weight training is commonly perceived as anaerobic exercise, because one of the more common goals is to increase strength by lifting heavy weights. Other goals such as rehabilitation, weight loss, body shaping, and bodybuilding often use lower weights, adding aerobic character to the exercise.\n\nExcept in the extremes, a muscle will fire fibres of both the aerobic or anaerobic types on any given exercise, in varying ratio depending on the load on the intensity of the contraction. This is known as the energy system continuum. At higher loads, the muscle will recruit all muscle fibres possible, both anaerobic (\"fast-twitch\") and aerobic (\"slow-twitch\"), in order to generate the most force. However, at maximum load, the anaerobic processes contract so forcefully that the aerobic fibers are completely shut out, and all work is done by the anaerobic processes. Because the anaerobic muscle fibre uses its fuel faster than the blood and intracellular restorative cycles can resupply it, the maximum number of repetitions is limited. In the aerobic regime, the blood and intracellular processes can maintain a supply of fuel and oxygen, and continual repetition of the motion will not cause the muscle to fail.\n\nCircuit weight training is a form of exercise that uses a number of weight training exercise sets separated by short intervals. The cardiovascular effort to recover from each set serves a function similar to an aerobic exercise, but this is not the same as saying that a weight training set is itself an aerobic process.\n\nWeight trainers commonly divide the body's individual muscles into ten major muscle groups. These do not include the hip, neck and forearm muscles, which are rarely trained in isolation. The most common exercises for these muscle groups are listed below.\n\nThe sequence shown below is one possible way to order the exercises. The large muscles of the lower body are normally trained before the smaller muscles of the upper body, because these first exercises require more mental and physical energy. The core muscles of the torso are trained before the shoulder and arm muscles that assist them. Exercises often alternate between \"pushing\" and \"pulling\" movements to allow their specific supporting muscles time to recover. The stabilizing muscles in the waist should be trained last.\n\nA number of techniques have been developed to make weight training exercises more intense, and thereby potentially increase the rate of progress. Many weight lifters use these techniques to bring themselves past a plateau, a duration where a weightlifter may be unable to do more lifting repetitions, sets, or use higher weight resistance.\n\nA drop set is an easy method of strength training where you perform a set of any exercise to failure or right before failure, and then reduce the weight and continue to lift for more repetitions with the decreased weight.\n\nPyramid sets are weight training sets in which the progression is from lighter weights with a greater number of repetitions in the first set, to heavier weights with fewer repetitions in subsequent sets.\n\nA reverse pyramid is the opposite in which the heavier weights are used at the beginning and progressively lightened.\n\nBurnouts combine pyramids and drop sets, working up to higher weights with low reps and then back down to lower weights and high reps.There are a few different ways one could perform burnout sets but the main idea is to perform an exercise until failure. You should start with a weight that is 75% of the amount of the maximum amount of weight you can lift for 1 rep. Once you’ve performed the exercise to exhaustion, reduce the weight and perform another set until failure, which will usually consist of much fewer repetitions. Burnout sets sound very similar to supersets but there are differences in the results they produce. Supersets help increase muscle mass, but are more efficient for producing muscle definition and shape. Burnout sets help increase muscle growth because of the buildup of lactic acid in the muscle when it’s forced to the point of failure.\n\nThe diminishing set method is where a weight is chosen that can be lifted for 20 reps in one set, and then 70 repetitions are performed in as few sets as possible.\n\nThe rest-pause training method takes one whole set and breaks it down into a few mini sets. There are two different goals that are associated with rest-pause training, you could use it to increase hypertrophy or increase strength. To increase hypertrophy you would perform a set with weight you are comfortable lifting for 6-10 reps and then set the weight down. Next, take 15 seconds worth of deep breaths and pick the weight back up and lift to failure. Lastly, repeat step two as many times as you want but it is commonly done twice. In order to increase strength using rest-pause method first you would choose a weight that is 85-95% of your one rep max. Then you would perform 1 rep with this weight and follow that up with a 30-45 second break. Then you could repeat this process as many times as you’d like.\n\nThe Giant set, is a form of training that targets one muscle group (e.g. the triceps) with four separate exercises performed in quick succession, often to failure and sometimes with the reduction of weight halfway through a set once muscle fatigue sets in. This form of intense training 'shocks' the muscles and as such, is usually performed by experienced trainers and should be used infrequently.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrength training may involve the combining of different training methods such as weight training, plyometrics, bodyweight exercises, and ballistic exercises. This is often done in order to improve a person's ability to apply their strength quickly. Or in other words, to improve their ability to apply explosive power.\n\nLoaded plyometrics involve the addition of weights to jumping exercises. The weights may be held or worn. For instance, vertical jumps whilst holding a trap bar or jumping split squats whilst holding dumbbells. This helps to enhance the explosive power of the athlete.\n\nComplex training, also known as contrast training, involves the alternation of weight training and plyometric exercises. Ideally, both sets of exercises should move through similar ranges of movement; such a pairing is called a complex, or contrast, pair. For instance, a set of heavy back squats at about 85-95% 1RM followed by a set of jumping exercises. The intention is to utilise the intense nervous system activation and increased muscle fibre recruitment from the heavy lift in the plyometric exercise; thereby increasing the power with which it can be performed. Over a period of training, this may result in the athlete being able to perform the plyometric exercise more powerfully, without the requirement of the preceding heavy lift. Working on the same principles, a sports specific action may be incorporated instead of the plyometric exercise; the intention, in this case, being to increase the athlete's ability to perform the sports specific action more powerfully.\n\nBallistic training, sometimes referred to as power training, is based upon the principle of maximising the acceleration phase of the exercise and minimising the deceleration phase; this helps to improve the athlete's explosive power. On this basis, ballistic training may include exercises which involve the throwing of a weight, such as a medicine ball, or jumping whilst holding or wearing a weight.\n\nContrast loading is the alternation of heavy and light loads i.e. a heavy bench press set at about 85-95% 1RM followed by a light bench press set at about 30-60% 1RM. The heavy set should be performed fast with the light set being performed as fast as possible. The joints should not be locked as this inhibits muscle fibre recruitment and reduces the speed at which the exercise can be performed. A loaded plyometric exercise, or ballistic exercise, may take the place of the light lift.\n\nSimilarly to complex training, contrast loading relies on the intense nervous system activation and enhanced muscle fibre recruitment from the heavy lift to help improve the power with which the subsequent exercise can be performed. This physiological effect is commonly referred to as post-activation potentiation, or the PAP effect. By way of explanation, if a light weight is lifted, and then a heavy weight is lifted, and then the same light weight is lifted again, then the light weight will feel lighter the second time it is lifted. This is due to the increased PAP effect from the heavy lift allowing for greater power to be applied and thus making the subsequent lighter lift feel even lighter than before. Explosive power training programmes are frequently designed to specifically utilise the PAP effect.\n\nStrength training is a safe form of exercise when the movements are controlled, and carefully defined. Or some safety measures can also be taken before the training. However, as with any form of exercise, improper execution and the failure to take appropriate precautions can result in injury. A helmet, boots, gloves, and back belt can aide in injury prevention. Principles of weight training safety apply to strength training.\n\nBodybuilding is a sport in which the goal is to increase muscle size and definition. Bodybuilding increases the endurance of muscles, as well as strength, though not as much as if it were the primary goal. Bodybuilders compete in bodybuilding competitions, and use specific principles and methods of strength training to maximize muscular size and develop extremely low levels of body fat. In contrast, most strength trainers train to improve their strength and endurance while not giving special attention to reducing body fat below normal. Strength trainers tend to focus on compound exercises to build basic strength, whereas bodybuilders often use isolation exercises to visually separate their muscles, and to improve muscular symmetry. Pre-contest training for bodybuilders is different again, in that they attempt to retain as much muscular tissue as possible while undergoing severe dieting. However, the bodybuilding community has been the source of many strength training principles, techniques, vocabulary, and customs.\n\nIt is widely accepted that strength training must be matched by changes in diet in order to be effective. Although aerobic exercise has been proven to have an effect on the dietary intake of macronutrients, strength training has not and an increase in dietary protein is generally believed to be required for building skeletal muscle.\n\nA review of 49 research studies found that supplementation of protein in the diet of healthy adults increased the size and strength of muscles during prolonged resistance exercise training; protein intakes of greater than 1.6 g/kg/day did not additionally increase fat-free mass or muscle size or strength. Protein that is neither needed for cell growth and repair nor consumed for energy is converted into urea mainly through the deamination process and is excreted by the kidneys. It was once thought that a high-protein diet entails risk of kidney damage, but studies have shown that kidney problems only occur in people with previous kidney disease. However failure to properly hydrate can put an increased strain on the kidney's ability to function. An adequate supply of carbohydrates (5–7 g per kg) is also needed as a source of energy and for the body to restore glycogen levels in muscles.\n\nA light, balanced meal prior to the workout (usually one to two hours beforehand) ensures that adequate energy and amino acids are available for the intense bout of exercise. The type of nutrients consumed affects the response of the body, and nutrient timing whereby protein and carbohydrates are consumed prior to and after workout has a beneficial impact on muscle growth. Water is consumed throughout the course of the workout to prevent poor performance due to dehydration. A protein shake is often consumed immediately following the workout, because both protein uptake and protein usage are increased at this time. Glucose (or another simple sugar) is often consumed as well since this quickly replenishes any glycogen lost during the exercise period.\nTo maximise muscle protein anabolism, recovery drink should contain glucose (dextrose), protein (usually whey) hydrolysate containing mainly dipeptides and tripeptides, and leucine.\nSome weight trainers also take ergogenic aids such as creatine or steroids to aid muscle growth. However, the effectiveness of some products is disputed and others are potentially harmful.\n\nDue to the androgenic hormonal differences between males and females, the latter are generally unable to develop large muscles regardless of the training program used. Normally the most that can be achieved is a look similar to that of a fitness model. Muscle is denser than fat, so someone who builds muscle while keeping the same body weight will occupy less volume; if two people weigh the same (and are the same height) but have different lean body mass percentages, the one with more muscle will appear thinner.\n\nIn addition, though bodybuilding uses the same principles as strength training, it is with a goal of gaining muscle bulk. Strength trainers with different goals and programs will not gain the same mass as a professional bodybuilder.\n\nSome weight trainers perform light, high-repetition exercises in an attempt to \"tone\" their muscles without increasing their size.\n\nThe word tone derives from the Latin \"tonus\" (meaning \"tension\"). In anatomy and physiology, as well as medicine, the term \"muscle tone\" refers to the continuous and passive partial contraction of the muscles, or the muscles' resistance to passive stretching during resting state as determined by a deep tendon reflex. Muscle tonus is dependent on neurological input into the muscle. In medicine, observations of changes in muscle tonus can be used to determine normal or abnormal states which can be indicative of pathology. The common strength training term \"tone\" is derived from this use.\n\nWhat muscle builders refer to as a \"toned physique\" or \"muscle firmness\" is one that combines reasonable muscular size with moderate levels of body fat, qualities that may result from a combination of diet and exercise.\n\nMuscle tone or firmness is derived from the increase in actin and myosin cross filaments in the sarcomere. When this occurs the same amount of neurological input creates a greater firmness or tone in the resting continuous and passive partial contraction in the muscle.\n\nExercises of 6–12 reps cause hypertrophy of the sarcoplasm in slow-twitch and high-twitch muscle fibers, contributing to overall increased muscle bulk. This is not to be confused with myofibril hypertrophy which leads to strength gains. Both however can occur to an extent during this rep range. \"Even though most are of the opinion that higher repetitions are best for producing the desired effect of muscle firmness or tone, it is not.\" Low volume strength training of 5 repetitions or fewer will increase strength by increasing actin and myosin cross filaments thereby increasing muscle firmness or tone. The low volume of this training will inhibit the hypertrophy effect.\n\nLowered-calorie diets have no positive effect on muscle hypertrophy for muscle of any fiber type. They may, however, decrease the thickness of subcutaneous fat (fat between muscle and skin), through an overall reduction in body fat, thus making muscle striations more visible.\n\nExercises like sit-ups, or abdominal crunches, performs less work than whole-body aerobic exercises thereby expending fewer calories during exercise than jogging, for example.\n\nHypertrophy serves to maintain muscle mass, for an elevated basal metabolic rate, which has the potential to burn more calories in a given period compared to aerobics. This helps to maintain a higher metabolic rate which would otherwise diminish after metabolic adaption to dieting, or upon completion of an aerobic routine.\n\nWeight loss also depends on the type of strength training used. Weight training is generally used for bulking, but the bulking method will more than likely not increase weight because of the diet involved. However, when resistance or circuit training is used, because they are not geared towards bulking, women tend to lose weight more quickly. Lean muscles require calories to maintain themselves at rest, which will help reduce fat through an increase in the basal metabolic rate.\n\nUntil the 20th century, the history of strength training was very similar to the history of weight training. With the advent of modern technology, materials and knowledge, the methods that can be used for strength training have multiplied significantly.\n\nHippocrates explained the principle behind strength training when he wrote \"that which is used develops, and that which is not used wastes away\", referring to muscular hypertrophy and atrophy. Progressive resistance training dates back at least to Ancient Greece, when legend has it that wrestler Milo of Croton trained by carrying a newborn calf on his back every day until it was fully grown. Another Greek, the physician Galen, described strength training exercises using the halteres (an early form of dumbbell) in the 2nd century. Ancient Persians used the \"meels\", which became popular during the 19th century as the Indian club, and has recently made a comeback in the form of the clubbell.\n\nThe dumbbell was joined by the barbell in the latter half of the 19th century. Early barbells had hollow globes that could be filled with sand or lead shot, but by the end of the century these were replaced by the plate-loading barbell commonly used today.\n\nStrength training with isometric exercise was popularised by Charles Atlas from the 1930s onwards. The 1960s saw the gradual introduction of exercise machines into the still-rare strength training gyms of the time. Strength training became increasingly popular in the 1980s following the release of the bodybuilding movie \"Pumping Iron\" and the subsequent popularity of Arnold Schwarzenegger.\n\nOrthopaedic specialists used to recommend that children avoid weight training because the growth plates on their bones might be at risk. The very rare reports of growth plate fractures in children who trained with weights occurred as a result of inadequate supervision, improper form or excess weight, and there have been no reports of injuries to growth plates in youth training programs that followed established guidelines. The position of the National Strength and Conditioning Association is that strength training is safe for children if properly designed and supervised.\n\nYounger children are at greater risk of injury than adults if they drop a weight on themselves or perform an exercise incorrectly; further, they may lack understanding of, or ignore the safety precautions around weight training equipment. As a result, supervision of minors is considered vital to ensuring the safety of any youth engaging in strength training.\n\nStrength training is the fourth most popular form of fitness in Australia. Due to its popularity amongst all ages, there is great scepticism on what the appropriate age to commence strength training in young athletes is. Some points of the opposing view of strength training in young adolescence are stunted growth, health and bone problems in later stages of life and unhealthy eating habits. Studies by Australian experts that have been recognised by the Australian Institute of Sport (AIS) have debunked these myths. There is no link between any prolonged health risks and strength training in pre-adolescence if the procedures of strength training are followed correctly and under suitable supervision. Strength training for pre-adolescents should focus on skills and techniques. Children should only work on strengthening all the big muscle groups, using free weight and body weight movements with relatively light loads. The benefits of these practices include increased strength performance, injury prevention and learning good training principles.\n\nOlder adults are prone to loss of muscle strength. With more strength older adults have better health, better quality of life, better physical function and fewer falls. In cases in which an older person begins strength training, their doctor or health care provider may neglect to emphasize a strength training program which results in muscle gains. Under-dosed strength training programs should be avoided in favor of a program which matches the abilities and goals of the person exercising.\n\nIn setting up an exercise program for an older adult, they should go through a baseline fitness assessment to determine their current limits. Any exercise program for older adults should match the intensity, frequency, and duration of exercise that the person can perform. The program should have a goal of increased strength as compared to the baseline measurement.\n\nRecommended training for older adults is three times a week of light strength training exercises. Exercise machines are a commonly used equipment in a gym setting, including treadmills with exercises such as walking or light jogging. Home-based exercises should usually consist of body weight or elastic band exercises that maintain a low level of impact on the muscles. Weights can also be used by older adults if they maintain a lighter weight load with an average amount of repetitions (10–12 reps) with suitable supervision. It is important for older adults to maintain a light level of strength training with low levels of impact to avoid injuries.\n\nOlder people who exercise against a resistance or force become stronger . Progressive resistance training (PRT) also improves physical functioning in older people, including the performance of simple (e.g.: walking, climbing stairs, rising from a chair more quickly) and complex daily activities (e.g.: bathing, cooking). Caution is recommended when transferring PRT exercises for clinical populations, as adverse effects are unclear.\n\n\n"}
{"id": "24210205", "url": "https://en.wikipedia.org/wiki?curid=24210205", "title": "The Swedish Parkinson Academy", "text": "The Swedish Parkinson Academy\n\nThe Swedish Parkinson Academy (; SPA) was founded in 2007 and is based at Lund University in Sweden.\n\nThe main aim of the academy is to stimulate preclinical and clinical research related to Parkinson's disease, especially to support translational projects (projects bridging the border between preclinical and clinical research, bringing promising preclinical results to clinical studies).\n\nMain research areas include:\n\nThe SPA employs (partly or fully) ten scientists and is led by a steering group consisting of representatives from preclinical and clinical neuroscience as well as patient organizations. SPA organizes education and meetings regarding Parkinson's disease, on Swedish and international level.\n\n"}
{"id": "153614", "url": "https://en.wikipedia.org/wiki?curid=153614", "title": "World AIDS Day", "text": "World AIDS Day\n\nWorld AIDS Day, designated on 1 December every year since 1988, is an international day dedicated to raising awareness of the AIDS pandemic caused by the spread of HIV infection and mourning those who have died of the disease. Government and health officials, non-governmental organizations, and individuals around the world observe the day, often with education on AIDS prevention and control.\n\nWorld AIDS Day is one of the eight official global public health campaigns marked by the World Health Organization (WHO), along with World Health Day, World Blood Donor Day, World Immunization Week, World Tuberculosis Day, World No Tobacco Day, World Malaria Day and World Hepatitis Day.\n\n, AIDS has killed between 28.9 million and 41.5 million people worldwide, and an estimated 36.7 million people are living with HIV, making it one of the most important global public health issues in recorded history. Thanks to recent improved access to antiretroviral treatment in many regions of the world, the death rate from AIDS epidemic has decreased since its peak in 2005 (1 million in 2016, compared to 1.9 million in 2005).\n\nWorld AIDS Day was first conceived in August 1987 by James W. Bunn and Thomas Netter, two public information officers for the Global Programme on AIDS at the World Health Organization in Geneva, Switzerland. Bunn and Netter took their idea to Dr. Jonathan Mann, Director of the Global Programme on AIDS (now known as UNAIDS). Dr. Mann liked the concept, approved it, and agreed with the recommendation that the first observance of World AIDS Day should be on 1 December 1988. Bunn, a former television broadcast journalist from San Francisco, had recommended the date of 1 December that believing it would maximize coverage of World AIDS Day by western news media, sufficiently long following the US elections but before the Christmas holidays.\n\nIn its first two years, the theme of World AIDS Day focused on children and young people. While the choice of this theme was criticized at the time by some for ignoring the fact that people of all ages may become infected with HIV, the theme helped alleviate some of the stigma surrounding the disease and boost recognition of the problem as a family disease.\n\nThe Joint United Nations Programme on HIV/AIDS (UNAIDS) became operational in 1996, and it took over the planning and promotion of World AIDS Day. Rather than focus on a single day, UNAIDS created the World AIDS Campaign in 1997 to focus on year-round communications, prevention and education. In 2004, the World AIDS Campaign became an independent organization.\n\nEach year, Popes John Paul II and Benedict XVI have released a greeting message for patients and doctors on World AIDS Day.\n\nIn 2016, a collection of HIV and AIDS related NGOs (including Panagea Global AIDS and The AIDS and Rights Alliance for Southern Africa) started a campaign to rename World AIDS Day to World HIV Day. They claim the change will put the emphasis on social justice issues, and the advancement of treatments like PrEP.\n\nIn the US, the White House began marking World AIDS Day with the iconic display of a AIDS Ribbon on the building's North Portico in 2007. White House aid Steven M. Levine, then serving in President George W. Bush's administration, proposed the display to symbolize the United States' commitment to combat the world AIDS epidemic through its landmark PEPFAR program. The White House display, now an annual tradition across four presidential administrations, quickly garnered attention, as it was the .\n\nSince 1993, the President of the United States has made an official proclamation for World AIDS Day (see section #US Presidential Proclamations for World AIDS Day for copies of those proclamations). On 30 November 2017, President Donald Trump proclaimed \"World AIDS Day\" for 1 December.\n\nAll the World AIDS Day campaigns focus on a specific theme, chosen following consultations with UNAIDS, WHO and a large number of grassroots, national and international agencies involved in the prevention and treatment of HIV/AIDS. As of 2008, each year's theme is chosen by the Global Steering Committee of the World AIDS Campaign (WAC).\n\nFor each World AIDS Day from 2005 through 2010, the theme was \"Stop AIDS. Keep the Promise\", designed to encourage political leaders to keep their commitment to achieve universal access to HIV/AIDS prevention, treatment, care and support by the year 2010.\n\nAs of 2012, the multi-year theme for World AIDS Day is \"Getting to Zero: Zero new HIV infections. Zero deaths from AIDS-related illness. Zero discrimination.\" The US Federal theme for the year 2014 is \"Focus, Partner, Achieve: An AIDS-Free Generation\".\n\nThe themes are not limited to a single day but are used year-round in international efforts to highlight HIV/AIDS awareness within the context of other major global events including the G8 Summit, as well as local campaigns like the Student Stop AIDS Campaign in the UK.\n\nSource:\n\n\n\n"}
{"id": "33153372", "url": "https://en.wikipedia.org/wiki?curid=33153372", "title": "Yamesvara Tank (Nala Kunda)", "text": "Yamesvara Tank (Nala Kunda)\n\nYamesvara Tank is located in Yamesvara Patna, Old Town Bhubaneswar. It is located in the left side of the Yamesvara Patna lane branching from Badheibanka Chowk to Kapilesvara. The tank is under Bharati Matha and it is now abandoned because the sewage water of the locality is discharging into the kunda. Hence it is also known as Nala kunda (Drain Tank). The tank is enclosed within a masonry embankment made of dressed laterite blocks.\n\ni) Present Name: Yamesvara Tank (Nala kunda)\n\nii) Past Name: —\n\nLat. 200, 14’ 41\" N.,\n\nLong. 850, 49’ 90\" E., \n\nElev. 86 ft\n\ni) Single/ Multiple: Multiple\n\nii) Public/ Private: Public.\n\niii) Any other (specify): Though the tank was originally under Bharati Matha, now no body claims ownership or looks after its maintenance.\n\ni) Precise date: —\n\nii) Approximate date: 11th Century A.D.\n\niii) Source of Information: In view of its close proximity to Bharati matha, the tank may go along with Bharati matha which is assigned to 11th century A.D.\n\ni) Precinct/ Building/ Structure/Landscape/Site/Tank: Tank\n\nii) Subtype: Spring\n\niii) Typology: Embankments.\n\ni) Abandoned/ in use: Abandoned.\n\nii) Present use: Nala kunda.\n\niii) Past use: Ritual and normal bath.\n\ni) Historic significance: Somavamsi epoch.\n\nii) Cultural significance: —\n\niii) Social significance: —\n\niv) Associational significance:—\n\ni) Surrounding: The tank has embankments on all the four sides. The temple of Maitresvara stands near its south eastern embankment within a distance of 10.00 m, Yamesvara temple in north-west, private buildings on its northern and southern embankments. Now it is almost a dead tank.\n\nii) Orientation: —\n\niii) Architectural features (Plan and Elevation): Square on plan measuring 38.60 m with a depth of 4.00 m from the present ground level.\n\niv) Raha niche & parsva devatas: —\n\nv) Decorative features: —\n\nDoorjambs: —\n\nLintel: —\n\nvi) Building material: Laterite\n\nvii) Construction techniques: Dry masonry.\n\nviii) Style: —\n\nix) Special features, if any: —\n\ni) Good/Fair/ Showing Signs of Deterioration/Advanced: Advanced state of deterioration because of the growth of wild vegetations on the both walls of the tank and inside the tank. Encroachment is another big problem the tank is facing.\n\nii) State of Decay/Danger of Disappearance: —\n\ni) Signs of distress: Private residential building on all sides of the tank discharge their waste water and through into the tank.\n\nii) Structural problems: Private residential buildings have been erected over the embankment walls of the tank.\n\niii) Repairs and Maintenance: Though the tank was originally under Bharati Matha, now no body claims ownership or looks after its maintenance.\n\ni) Architecture: A\n\nii) Historic: B\n\niii) Associational: C\n\niv) Social/Cultural:C\n\nv) Others: —\n\nConservation Problem and Remedies: Encroachment, growth of wild vegetations and discharging of sewage and drain water and trash into the tank. Southern embankment has partly collapsed. Encroachments from all sides prevent access into the tank.\n\n"}
{"id": "24490505", "url": "https://en.wikipedia.org/wiki?curid=24490505", "title": "Zam System", "text": "Zam System\n\nZam system () is a form of irrigation system in Pakistan. \"Zam\" means the flow of perennial water coming out of springs, whereas Rod Koh is the main torrent bed which remains usually dry, when there is no flood.\n\nThe flood and perennial water of the Zam is used for irrigation as well as for drinking purpose. Zam water is classified into two categories: \"Buga pani\" (flood water) and \"Kala Pani\" (perennial water). \n\nRod Koh (torrent-spate-irrigation) systems go back at least as early as 330 BC and provided economic basis for some of the early civilisations. Alexander the Great, according to Arrian, sailed down the river Jhelum to its junction with Indus River. His land forces marched in two bodies on either side of the river. They noticed some form of torrent agriculture although in a very poor state in a few locations of the Sulaiman piedmont. \n\nHeavy rains in the catchments, which extend up to Balochistan region, Afghanistan, Sulaiman Range, Shirani Hills and Bhattani Range result in water rushing into torrents in the foothill plains, named Daman area, where torrent agriculture (Rod Kohi) is practised.\n\n"}
