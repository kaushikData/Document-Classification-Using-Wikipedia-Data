{"id": "44594323", "url": "https://en.wikipedia.org/wiki?curid=44594323", "title": "Abortion in Burkina Faso", "text": "Abortion in Burkina Faso\n\nAbortion in Burkina Faso is only legal if the abortion will save the woman's life, the pregnancy gravely endangers the woman's physical or mental health, the child will potentially be born with an incurable disease, or in cases where the pregnancy is a result of rape or incest, so long as it is proven by a state prosecutor. Even these abortions are limited to the first ten weeks of pregnancy.\n\nIn Burkina Faso, any abortion performed under other conditions subjects the person who performs the procedure subject to one to five years’ imprisonment and imposition of a fine of 300,000 to 1,500,000 CFA francs.\n\nIn the early 1990s, at least 5% of women admitted into healthcare facilities for maternal health concerns had life-threatening complications from unsafe abortions, and 70% of these women were between 16 and 24 years of age. During the same time period, 35% of women who sought medical treatment for infertility had previously been recipients of an illegal abortion.\n"}
{"id": "2866297", "url": "https://en.wikipedia.org/wiki?curid=2866297", "title": "Abortion in the Czech Republic", "text": "Abortion in the Czech Republic\n\nAbortion in the Czech Republic is legally allowed up to 12 weeks of pregnancy, with medical indications up to 24 weeks of pregnancy, in case of grave problems with the fetus at any time. Those performed for medical indications are covered by public health insurance, but, otherwise abortion is relatively affordable in the Czech Republic. In Czech, induced abortion is referred to as \"interrupce\" or \"umělé přerušení těhotenství\", often colloquially \"potrat\" (\"miscarriage\").\n\nIn 1957 abortions were legalized in Czechoslovakia, although with restrictions that depended on the current policy of the government. In 1986 the restrictions were lifted resulting in growth of the number of abortions.\n\nSince 1993, abortions for non-medical reasons have not been paid for by the public health system. The absolute peak of the number of abortions was reached in 1990 at over 100,000 per year, but has declined steadily down since then, reaching less than 1/3 of the peak level in 2004. Reasons for this decrease have included the wider availability of contraception and better sex education.\n\nMedical abortion (with mifepristone) was registered in 2013.\n\nTotal number of abortions in 2009 was 40 528 of which 14 629 (i.e. 3.1%) were spontaneous abortions, 24 636 (60,79%) induced abortions (historically the lowest number ever) of which 77% were \"mini-interruptions\" (within 8 weeks of pregnancy). 1,300 ectopic pregnancies were aborted. Total abortions per woman is 0.53, induced abortions is 0.34.\n\n, the abortion rate was 10.7 abortions per 1,000 women aged 15–44 years.\n\nRegionally, the highest abortion ratio is in northern and north-western Bohemia due the structure of the population (in 2002 in Tachov District 31.3% of abortions were induced). The lowest ratios are in rural districts of southern Moravia and Bohemian-Moravian Highlands (in 2002 in Žďár nad Sázavou District 15.5% of abortions were induced). Abortion ratios in large industrial cities are generally higher compared to small towns and the countryside.\n\nMarried women form the largest segment but their ratio is decreasing in favour of unmarried young women. Women with tertiary level of education have about 6% of induced abortions. In 2009 7.5% of the women are foreigners living in the Czech Republic. Official statistics about abortion tourism (mainly from neighbouring Poland where legal induced abortion is strictly limited) do not exist but the numbers are estimated to be low.\n\nThe public in the Czech Republic generally supports the legality of abortion. This has been confirmed by a number of opinion polls.\n\n\n\n"}
{"id": "307039", "url": "https://en.wikipedia.org/wiki?curid=307039", "title": "Alcoholic liver disease", "text": "Alcoholic liver disease\n\nAlcoholic liver disease is a term that encompasses the liver manifestations of alcohol overconsumption, including fatty liver, alcoholic hepatitis, and chronic hepatitis with liver fibrosis or cirrhosis.\n\nIt is the major cause of liver disease in Western countries. Although steatosis (fatty liver) will develop in any individual who consumes a large quantity of alcoholic beverages over a long period of time, this process is transient and reversible. Of all chronic heavy drinkers, only 15–20% develop hepatitis or cirrhosis, which can occur concomitantly or in succession.\n\nThe mechanism behind this is not completely understood. 80% of alcohol passes through the liver to be detoxified. Chronic consumption of alcohol results in the secretion of pro-inflammatory cytokines (TNF-alpha, Interleukin 6 [IL6] and Interleukin 8 [IL8]), oxidative stress, lipid peroxidation, and acetaldehyde toxicity. These factors cause inflammation, apoptosis and eventually fibrosis of liver cells. Why this occurs in only a few individuals is still unclear. Additionally, the liver has tremendous capacity to regenerate and even when 75% of hepatocytes are dead, it continues to function as normal.\n\nThe risk factors presently known are:\n\nFatty change, or steatosis is the accumulation of fatty acids in liver cells. These can be seen as fatty globules under the microscope. Alcoholism causes development of large fatty globules (macro vesicular steatosis) throughout the liver and can begin to occur after a few days of heavy drinking. Alcohol is metabolized by alcohol dehydrogenase (ADH) into acetaldehyde, then further metabolized by aldehyde dehydrogenase (ALDH) into acetic acid, which is finally oxidized into carbon dioxide () and water (). This process generates NADH, and increases the NADH/NAD+ ratio. A higher NADH concentration induces fatty acid synthesis while a decreased NAD level results in decreased fatty acid oxidation. Subsequently, the higher levels of fatty acids signal the liver cells to compound it to glycerol to form triglycerides. These triglycerides accumulate, resulting in fatty liver.\n\nAlcoholic hepatitis is characterized by the inflammation of hepatocytes. Between 10% and 35% of heavy drinkers develop alcoholic hepatitis (NIAAA, 1993). While development of hepatitis is not directly related to the dose of alcohol, some people seem more prone to this reaction than others. This is called alcoholic steato necrosis and the inflammation appears to predispose to liver fibrosis. Inflammatory cytokines (TNF-alpha, IL6 and IL8) are thought to be essential in the initiation and perpetuation of liver injury by inducing apoptosis and necrosis. One possible mechanism for the increased activity of TNF-α is the increased intestinal permeability due to liver disease. This facilitates the absorption of the gut-produced endotoxin into the portal circulation. The Kupffer cells of the liver then phagocytose endotoxin, stimulating the release of TNF-α. TNF-α then triggers apoptotic pathways through the activation of caspases, resulting in cell death.\n\nCirrhosis is a late stage of serious liver disease marked by inflammation (swelling), fibrosis (cellular hardening) and damaged membranes preventing detoxification of chemicals in the body, ending in scarring and necrosis (cell death). Between 10% to 20% of heavy drinkers will develop cirrhosis of the liver (NIAAA, 1993). Acetaldehyde may be responsible for alcohol-induced fibrosis by stimulating collagen deposition by hepatic stellate cells. The production of oxidants derived from NADPH oxi- dase and/or cytochrome P-450 2E1 and the formation of acetaldehyde-protein adducts damage the cell membrane.\nSymptoms include jaundice (yellowing), liver enlargement, and pain and tenderness from the structural changes in damaged liver architecture. Without total abstinence from alcohol use, cirrhosis will eventually lead to liver failure. Late complications of cirrhosis or liver failure include portal hypertension (high blood pressure in the portal vein due to the increased flow resistance through the damaged liver), coagulation disorders (due to impaired production of coagulation factors), ascites (heavy abdominal swelling due to buildup of fluids in the tissues) and other complications, including hepatic encephalopathy and the hepatorenal syndrome.\nCirrhosis can also result from other causes than alcohol abuse, such as viral hepatitis and heavy exposure to toxins other than alcohol. The late stages of cirrhosis may look similar medically, regardless of cause. This phenomenon is termed the \"final common pathway\" for the disease.\nFatty change and alcoholic hepatitis with abstinence can be reversible. The later stages of fibrosis and cirrhosis tend to be irreversible, but can usually be contained with abstinence for long periods of time.\n\nIn the early stages, patients with ALD exhibits subtle and often no abnormal physical findings. It is usually not until development of advanced liver disease that stigmata of chronic liver disease become apparent. Early ALD is usually discovered during routine health examinations when liver enzyme levels are found to be elevated. These usually reflect alcoholic hepatic steatosis. Microvesicular and macrovesicular steatosis with inflammation are seen in liver biopsy specimens. These histologic features of ALD are indistinguishable from those of nonalcoholic fatty liver disease. Steatosis usually resolves after discontinuation of alcohol use. Continuation of alcohol use will result in a higher risk of progression of liver disease and cirrhosis. In patients with acute alcoholic hepatitis, clinical manifestations include fever, jaundice, hepatomegaly, and possible hepatic decompensation with hepatic encephalopathy, variceal bleeding, and ascites accumulation.Tender hepatomegaly may be present, but abdominal pain is unusual. Occasionally, the patient may be asymptomatic.\n\nIn people with alcoholic hepatitis, the serum aspartate aminotransferase (AST) to alanine aminotransferase (ALT) ratio is greater than 2:1. AST and ALT levels are almost always less than 500. The elevated AST to ALT ratio is due to deficiency of pyridoxal phosphate, which is required in the ALT enzyme synthetic pathway. Furthermore, alcohol metabolite–induced injury of hepatic mitochondria results in AST isoenzyme release. Other laboratory findings include red blood cell macrocytosis (mean corpuscular volume > 100) and elevations of serum gamma-glutamyl transferase (GGT), alkaline phosphatase (Alk Phos), and bilirubin levels. Folate level is reduced in alcoholic patients due to decreased intestinal absorption, increased bone marrow requirement for folate in the presence of alcohol, and increased urinary loss. The magnitude of leukocytosis (white blood cell depletion) reflects severity of liver injury. Histologic features include Mallory bodies, giant mitochondria, hepatocyte necrosis, and neutrophil infiltration in the area around the veins. Mallory bodies, which are also present in other liver diseases, are condensations of cytokeratin components in the hepatocyte cytoplasm and do not contribute to liver injury. Up to 70% of patients with moderate to severe alcoholic hepatitis already have cirrhosis identifiable on biopsy examination at the time of diagnosis.\n\nNot drinking further alcohol is the most important part of treatment. People with chronic HCV infection should abstain from any alcohol intake, due to the risk for rapid acceleration of liver disease.\n\nA 2006 Cochrane review did not find evidence sufficient for the use of androgenic anabolic steroids. Corticosteroids are sometimes used; however, this is recommended only when severe liver inflammation is present.\n\nSylimarin has been investigated as a possible treatment, with ambiguous results. One review claimed benefit for S-adenosyl methionine in disease models.\n\nThe effects of anti-tumor necrosis factor medications such as infliximab and etanercept are unclear and possibly harmful. Evidence is unclear for pentoxifylline. Propylthiouracil may result in harm.\n\nEvidence does not support supplemental nutrition in liver disease.\n\nAlthough in rare cases liver cirrhosis is reversible, the disease process remains mostly irreversible. Liver transplantation remains the only definitive therapy. Today, survival after liver transplantation is similar for people with ALD and non-ALD. The requirements for transplant listing are the same as those for other types of liver disease, except for a 6-month sobriety prerequisite along with psychiatric evaluation and rehabilitation assistance (i.e., Alcoholics Anonymous). Specific requirements vary among the transplant centers. Relapse to alcohol use after transplant listing results in delisting. Re-listing is possible in many institutions, but only after 3–6 months of sobriety. There are limited data on transplant survival in patients transplanted for acute alcoholic hepatitis, but it is believed to be similar to that in nonacute ALD, non-ALD, and alcoholic hepatitis with MDF less than 32.\n\nThe prognosis for people with ALD depends on the liver histology as well as cofactors, such as concomitant chronic viral hepatitis. Among patients with alcoholic hepatitis, progression to liver cirrhosis occurs at 10–20% per year, and 70% will eventually develop cirrhosis. Despite cessation of alcohol use, only 10% will have normalization of histology and serum liver enzyme levels. As previously noted, the MDF has been used to predict short-term mortality (i.e., MDF ≥ 32 associated with spontaneous survival of 50–65% without corticosteroid therapy, and MDF < 32 associated with spontaneous survival of 90%).The Model for End-Stage Liver Disease (MELD) score has also been found to have similar predictive accuracy in 30day (MELD > 11) and 90-day (MELD > 21) mortality. Liver cirrhosis develops in 6–14% of those who consume more than 60–80 g of alcohol daily for men and more than 20 g daily for women. Even in those who drink more than 120 g daily, only 13.5% will suffer serious alcohol-related liver injury. Nevertheless, alcohol-related mortality was the third leading cause of death in 2003 in the United States. Worldwide mortality is estimated to be 150,000 per year.\n"}
{"id": "542071", "url": "https://en.wikipedia.org/wiki?curid=542071", "title": "Anopheles", "text": "Anopheles\n\nAnopheles (Greek anofelís: \"useless\") is a genus of mosquito first described and named by J. W. Meigen in 1818. About 460 species are recognised; while over 100 can transmit human malaria, only 30–40 commonly transmit parasites of the genus \"Plasmodium\", which cause malaria in humans in endemic areas. \"Anopheles gambiae\" is one of the best known, because of its predominant role in the transmission of the most dangerous malaria parasite species (to humans) – \"Plasmodium falciparum\".\n\nThe name comes from the Greek , ', meaning \"not\", and , ', meaning \"profit\", and translates to \"useless\". \"Anopheles\" mosquitoes kill about 440,000 people each year because of malaria.\n\nSome species of \"Anopheles\" also can serve as the vectors for canine heartworm \"Dirofilaria immitis\", the filariasis-causing species \"Wuchereria bancrofti\" and \"Brugia malayi\", and viruses such as one that causes O'nyong'nyong fever. An association of brain tumor incidence and malaria suggests the \"Anopheles\" might transmit a virus or other agent that could cause a brain tumor.\n\nMosquitoes in other genera (\"Aedes\", \"Culex\", \"Culiseta\", \"Haemagogus\", and \"Ochlerotatus\") can also serve as vectors of disease agents, but not human malaria.\n\nThe ancestors of \"Drosophila\" and the mosquitoes diverged . The culicine and \"Anopheles\" clades of mosquitoes diverged between and . The Old and New World \"Anopheles\" species subsequently diverged between and . \"Anopheles darlingi\" diverged from the African and Asian malaria vectors ∼. The \"Anopheles gambiae\" and \"Anopheles funestus\" clades diverged between and . A molecular study of several genes in seven species has provided additional support for an expansion of this genus during the Cretaceous period. \n\nThe \"Anopheles\" genome, at 230–284 million base pairs (Mbp), is comparable in size to that of \"Drosophila\", but considerably smaller than those found in other culicine genomes (528 Mbp–1.9 Gbp). Like most culicine species, the genome is diploid with six chromosomes.\n\nThe only known fossils of this genus are those of \"Anopheles (Nyssorhynchus) dominicanus\" contained in Dominican amber from the Late Eocene ( to ) and \"Anopheles rottensis\" contained in German amber from the Late Oligocene ( to ).\n\nThe genus \"Anopheles\" Meigen (nearly worldwide distribution) belongs to the subfamily Anophelinae together with another two genera: \"Bironella\" Theobald (Australia only) and \"Chagasia\" Cruz (Neotropics). The taxonomy remains incompletely settled. Classification into species is based on morphological characteristics – wing spots, head anatomy, larval and pupal anatomy, chromosome structure, and more recently, on DNA sequences.\n\nThe genus has been subdivided into seven subgenera based primarily on the number and positions of specialized setae on the gonocoxites of the male genitalia. The system of subgenera originated with the work of Christophers, who in 1915 described three subgenera: \"Anopheles\" (widely distributed), \"Myzomyia\" (later renamed \"Cellia\") (Old World) and \"Nyssorhynchus\" (Neotropical). \"Nyssorhynchus\" was first described as \"Lavernia\" by Frederick Vincent Theobald. Frederick Wallace Edwards in 1932 added the subgenus \"Stethomyia\" (Neotropical distribution). \"Kerteszia\" was also described by Edwards in 1932, but then recognised as a subgrouping of \"Nyssorhynchus\". It was elevated to subgenus status by Komp in 1937, and it is also found in the Neotropics. Two additional subgenera have since been recognised: \"Baimaia\" (Southeast Asia only) by Harbach \"et al.\" in 2005 and \"Lophopodomyia\" (Neotropical) by Antunes in 1937.\n\nTwo main groupings within the genus \"Anopheles\" are used: one formed by the \"Cellia\" and \"Anopheles\" subgenera and a second by \"Kerteszia\", \"Lophopodomyia\" and \"Nyssorhynchus\". Subgenus \"Stethomyia\" is an outlier with respect to these two taxa. Within the second group, \"Kerteszia\" and \"Nyssorhynchus\" appear to be sister taxa.\n\nThe number of species currently recognised within the subgenera is given here in parentheses: \"Anopheles\" (206 species), \"Baimaia\" (1), \"Cellia\" (216), \"Kerteszia\" (12), \"Lophopodomyia\" (6), \"Nyssorhynchus\" (34) and \"Stethomyia\" (5).\n\nTaxonomic units between subgenus and species are not currently recognised as official zoological names. In practice, a number of taxonomic levels have been introduced. The larger subgenera (\"Anopheles\", \"Cellia\" and \"Nyssorhynchus\") have been subdivided into sections and series which in turn have been divided into groups and subgroups. Below subgroup but above species level is the species complex. Taxonomic levels above species complex can be distinguished on morphological grounds. Species within a species complex are either morphologically identical or extremely similar and can only be reliably separated by microscopic examination of the chromosomes or DNA sequencing. The classification continues to be revised.\n\nSubgenus \"Nyssorhynchus\" has been divided in three sections: \"Albimanus\" (19 species), \"Argyritarsis\" (11 species) and \"Myzorhynchella\" (4 species). The \"Argyritarsis\" section has been subdivided into \"Albitarsis\" and \"Argyritarsis\" groups.\n\nThe \"Anopheles\" group was divided by Edwards into four series: \"Anopheles\" (worldwide), \"Myzorhynchus\" (Palearctic, Oriental, Australasian and Afrotropical), \"Cycloleppteron\" (Neotropical) and \"Lophoscelomyia\" (Oriental); and two groups, \"Arribalzagia\" (Neotropical) and \"Christya\" (Afrotropical). Reid and Knight (1961) modified this classification and consequently subdivided the subgenus \"Anopheles\" into two sections, \"Angusticorn\" and \"Laticorn\" and six series. The Arribalzagia and Christya Groups were considered to be series. The Laticorn Section includes the \"Arribalzagia\" (24 species), \"Christya\" and \"Myzorhynchus\" series. The \"Angusticorn\" section includes members of the \"Anopheles\", \"Cycloleppteron\" and \"Lophoscelomyia\" series.\n\nAll species known to carry human malaria lie within either the \"Myzorhynchus\" or the \"Anopheles\" series.\n\nLike all mosquitoes, anophelines go through four stages in their life cycles: egg, larva, pupa, and imago. The first three stages are aquatic and together last 5–14 days, depending on the species and the ambient temperature. The adult stage is when the female \"Anopheles\" mosquito acts as malaria vector. The adult females can live up to a month (or more in captivity), but most probably do not live more than two weeks in nature.\n\nAdult females lay 50–200 eggs per oviposition. The eggs are quite small (about 0.5 × 0.2 mm). Eggs are laid singly and directly on water. They are unique in that they have floats on either side. Eggs are not resistant to drying and hatch within 2–3 days, although hatching may take up to 2–3 weeks in colder climates.\n\nThe mosquito larva has a well-developed head with mouth brushes used for feeding, a large thorax and a nine-segment abdomen. It has no legs. In contrast to other mosquitoes, the \"Anopheles\" larva lacks a respiratory siphon, so it positions itself so that its body is parallel to the surface of the water. In contrast, the feeding larva of a nonanopheline mosquito species attaches itself to the water surface with its posterior siphon, with its body pointing downwards.\n\nLarvae breathe through spiracles located on the eighth abdominal segment, so must come to the surface frequently. The larvae spend most of their time feeding on algae, bacteria, and other microorganisms in the surface microlayer. They dive below the surface only when disturbed. Larvae swim either by jerky movements of the entire body or through propulsion with the mouth brushes.\n\nLarvae develop through four stages, or instars, after which they metamorphose into pupae. At the end of each instar, the larvae molt, shedding their exoskeletons, or skin, to allow for further growth. First-stage larvae are about 1 mm in length; fourth-stage larvae are normally 5–8 mm in length.\n\nThe process from egg-laying to emergence of the adult is temperature dependent, with a minimum time of seven days.\n\nThe larvae occur in a wide range of habitats, but most species prefer clean, unpolluted water. Larvae of \"Anopheles\" mosquitoes have been found in freshwater or saltwater marshes, mangrove swamps, rice fields, grassy ditches, the edges of streams and rivers, and small, temporary rain pools. Many species prefer habitats with vegetation. Others prefer habitats with none. Some breed in open, sun-lit pools, while others are found only in shaded breeding sites in forests. A few species breed in tree holes or the leaf axils of some plants.\n\nPupa is also known as tumbler.The pupa is comma-shaped when viewed from the side. The head and thorax are merged into a cephalothorax with the abdomen curving around underneath. As with the larvae, pupae must come to the surface frequently to breathe, which they do through a pair of respiratory trumpets on their cephalothoraces. After a few days as a pupa, the dorsal surface of the cephalothorax splits and the adult mosquito emerges. The pupal stage lasts around 2–3 days in temperate areas.\n\nThe duration from egg to adult varies considerably among species, and is strongly influenced by ambient temperature. Mosquitoes can develop from egg to adult in as little as five days, but it can take 10–14 days in tropical conditions.\n\nLike all mosquitoes, adult \"Anopheles\" species have slender bodies with three sections: head, thorax and abdomen.\n\nThe head is specialized for acquiring sensory information and for feeding. It contains the eyes and a pair of long, many-segmented antennae. The antennae are important for detecting host odors, as well as odors of breeding sites where females lay eggs. The head also has an elongated, forward-projecting proboscis used for feeding, and two maxillary palps. These palps also carry the receptors for carbon dioxide, a major attractant for the location of the mosquito's host.\n\nThe thorax is specialized for locomotion. Three pairs of legs and a pair of wings are attached to the thorax.\n\nThe abdomen is specialized for food digestion and egg development. This segmented body part expands considerably when a female takes a blood meal. The blood is digested over time, serving as a source of protein for the production of eggs, which gradually fill the abdomen.\n\n\"Anopheles\" mosquitoes can be distinguished from other mosquitoes by the palps, which are as long as the proboscis, and by the presence of discrete blocks of black and white scales on the wings. Adults can also be identified by their typical resting position: males and females rest with their abdomens sticking up in the air rather than parallel to the surface on which they are resting.\n\nAdult mosquitoes usually mate within a few days after emerging from the pupal stage. In most species, the males form large swarms, usually around dusk, and the females fly into the swarms to mate.\n\nMales live for about a week, feeding on nectar and other sources of sugar. Females will also feed on sugar sources for energy, but usually require a blood meal for the development of eggs. After obtaining a full blood meal, the female will rest for a few days while the blood is digested and eggs are developed. This process depends on the temperature, but usually takes 2–3 days in tropical conditions. Once the eggs are fully developed, the female lays them and resumes host-seeking.\n\nThe cycle repeats itself until the female dies. While females can live longer than a month in captivity, most do not live longer than one to two weeks in nature. Their lifespans depend on temperature, humidity, and their ability to successfully obtain a blood meal while avoiding host defenses.\n\nIn a study by the London School of Hygiene & Tropical Medicine researchers found that female mosquitoes carrying malaria parasites are significantly more attracted to human breath and odours than uninfected mosquitoes. The research team infected laboratory-raised \"Anopheles gambiae\" mosquitoes with Plasmodium parasites, leaving a control group uninfected. Then tests were run on the two groups to record their attraction to human smells. Female mosquitoes are particularly drawn to foot odours, and one of the tests showed infected mosquitoes landing and biting a prospective host repeatedly. The team speculates that the parasite improves the mosquitoes' sense of smell. It may also reduce its risk aversion.\n\nAlthough malaria is nowadays limited to tropical areas, most notoriously the regions of sub-Saharan Africa, many \"Anopheles\" species live in colder latitudes (see this map from the CDC). Indeed, malaria outbreaks have, in the past, occurred in colder climates, for example during the construction of the Rideau Canal in Canada during the 1820s. Since then, the \"Plasmodium\" parasite (not the \"Anopheles\" mosquito) has been eliminated from first world countries.\n\nThe CDC warns, however, that \"\"Anopheles\" that can transmit malaria are found not only in malaria-endemic areas, but also in areas where malaria has been eliminated. The latter areas are thus constantly at risk of reintroduction of the disease.\n\nSome species are poor vectors of malaria, as the parasites do not develop well (or at all) within them. There is also variation within species. In the laboratory, it is possible to select strains of \"A. gambiae\" that are refractory to infection by malaria parasites. These refractory strains have an immune response that encapsulates and kills the parasites after they have invaded the mosquito's stomach wall. Scientists are studying the genetic mechanism for this response. Genetically modified mosquitoes refractory to malaria possibly could replace wild mosquitoes, thereby limiting or eliminating malaria transmission.\n\nUnderstanding the biology and behavior of \"Anopheles\" mosquitoes can help understand how malaria is transmitted, and can aid in designing appropriate control strategies. Factors affecting a mosquito's facility to transmit malaria include its innate susceptibility to \"Plasmodium\", its host choice and its longevity. Factors that should be taken into consideration when designing a control program include the susceptibility of malaria vectors to insecticides and the preferred feeding and resting location of adult mosquitoes.\n\nOn December 21, 2007, a study published in PLoS Pathogens found the hemolytic C-type lectin CEL-III from \"Cucumaria echinata\", a sea cucumber found in the Bay of Bengal, impaired the development of the malaria parasite when produced by transgenic \"A. stephensi\". This could potentially be used to control malaria by spreading genetically modified mosquitoes refractory to the parasites, although numerous scientific and ethical issues must be overcome before such a control strategy could be implemented.\n\nOne important behavioral factor is the degree to which an \"Anopheles\" species prefers to feed on humans (anthropophily) or animals such as cattle or birds (zoophily). Anthropophilic \"Anopheles\" are more likely to transmit the malaria parasites from one person to another. Most \"Anopheles\" mosquitoes are not exclusively anthropophilic or zoophilic. However, the primary malaria vectors in Africa, \"A. gambiae\" and \"A. funestus\", are strongly anthropophilic and, consequently, are two of the most efficient malaria vectors in the world.\n\nOnce ingested by a mosquito, malaria parasites must undergo development within the mosquito before they are infectious to humans. The time required for development in the mosquito (the extrinsic incubation period) ranges from 10–21 days, depending on the parasite species and the temperature. If a mosquito does not survive longer than the extrinsic incubation period, then she will not be able to transmit any malaria parasites.\n\nIt is not possible to measure directly the lifespans of mosquitoes in nature, but indirect estimates of daily survivorship have been made for several \"Anopheles\" species. Estimates of daily survivorship of \"A. gambiae\" in Tanzania ranged from 0.77 to 0.84, meaning at the end of one day, between 77% and 84% will have survived.\n\nAssuming this survivorship is constant through the adult life of a mosquito, less than 10% of female \"A. gambiae\" would survive longer than a 14-day extrinsic incubation period. If daily survivorship increased to 0.9, over 20% of mosquitoes would survive longer than the same period. Control measures that rely on insecticides (e.g. indoor residual spraying) may actually impact malaria transmission more through their effect on adult longevity than through their effect on the population of adult mosquitoes.\n\nMost \"Anopheles\" mosquitoes are crepuscular (active at dusk or dawn) or nocturnal (active at night). Some feed indoors (endophagic), while others feed outdoors (exophagic). After feeding, some blood mosquitoes prefer to rest indoors (endophilic), while others prefer to rest outdoors (exophilic), though this can differ regionally based on local vector ecotype, and vector chromosomal makeup, as well as housing type and local microclimatic conditions. Biting by nocturnal, endophagic \"Anopheles\" mosquitoes can be markedly reduced through the use of insecticide-treated bed nets or through improved housing construction to prevent mosquito entry (e.g. window screens). Endophilic mosquitoes are readily controlled by indoor spraying of residual insecticides. In contrast, exophagic/exophilic vectors are best controlled through source reduction (destruction of the breeding sites).\n\nBecause transmission of disease by the mosquito requires ingestion of blood, the gut flora may have a bearing on the success of infection of the mosquito host. This aspect of disease transmission has not been investigated until recently. The larval and pupal gut is largely colonised by photosynthetic cyanobacteria, while in the adult, Proteobacteria and Bacteroidetes predominate. Blood meals drastically reduce the diversity of organisms and favor enteric bacteria.\n\nInsecticide-based control measures (e.g. indoor spraying with insecticides, bed nets) are the principal ways to kill mosquitoes that bite indoors. However, after prolonged exposure to an insecticide over several generations, mosquito populations, like those of other insects, may evolve resistance, a capacity to survive contact with an insecticide. Since mosquitoes can have many generations per year, high levels of resistance can evolve very quickly. Resistance of mosquitoes to some insecticides has been documented with just within a few years after the insecticides were introduced. Over 125 mosquito species have documented resistance to one or more insecticides. The evolution of resistance to insecticides used for indoor residual spraying was a major impediment during the Global Malaria Eradication Campaign. Judicious use of insecticides for mosquito control can limit the evolution and spread of resistance. However, use of insecticides in agriculture has often been implicated as contributing to resistance in mosquito populations. Detection of evolving resistance in mosquito populations is possible, so control programs are well advised to conduct surveillance for this potential problem.. In Malawi and other places, a shrub known as mpungabwi (\"Ocimum americanum\") is used to repel mosquitoes.\n\nWith substantial numbers of malaria cases affecting people around the globe, in tropical and subtropical regions, especially in sub-Saharan Africa, where millions of children are killed by this infectious disease, eradication is back on the global health agenda.\n\nAlthough malaria has existed since old times, its eradication was possible in Europe, North America, the Caribbean and parts of Asia and southern Central America during the first regional elimination campaigns in the late 1940s. However, the same results were not achieved in sub-Saharan Africa.\n\nThough the World Health Organization adopted a formal policy on the control and eradication of the malaria parasite since 1955, only recently, after the Gates Malaria Forum in October 2007, did key organizations start the debate on the pros and cons of redefining eradication as a goal to control malaria. \nClearly, the cost of preventing malaria is much less than treating the disease, in the long run. However, eradication of mosquitoes is not an easy task. For effective prevention of malaria, some conditions should be met, such as conducive conditions in the country, data collection about the disease, targeted technical approaches to the problem, very active and committed leadership, total governmental support, sufficient monetary resources, community involvement, and skilled technicians from different fields, as well as an adequate implementation.\n\nA wide range of strategies is needed to achieve malaria eradication, starting from simple steps to complicated strategies which may not be possible to enforce with the current tools. \n\nAlthough mosquito control is an important component of malaria control strategy, elimination of malaria in an area does not require the elimination of all \"Anopheles\" mosquitoes. For instance, in North America and Europe, although the vector \"Anopheles\" mosquitoes are still present, the parasite has been eliminated. Some socioeconomic improvements (e.g., houses with screened windows, air conditioning), once combined with vector reduction efforts and effective treatment, lead to the elimination of malaria without the complete elimination of the vectors. Some important measures in mosquito control to be followed are: discourage egg-laying, prevent development of eggs into larvae and adults, kill the adult mosquitoes, do not allow adult mosquitoes into places of human dwelling, prevent mosquitoes from biting human beings and deny them blood meals.\n\nResearch in this sense continues, and a study has suggested sterile mosquitoes might be the answer to malaria elimination. This research suggests using the sterile insect technique, in which sexually sterile male insects are released to wipe out a pest population, could be a solution to the problem of malaria in Africa. This technique brings hope, as female mosquitoes only mate once during their lifetimes, and in doing so with sterile male mosquitoes, the insect population would decrease. This is another option to be considered by local and international authorities that may be combined with other methods and tools to achieve malaria eradication in sub-Saharan Africa.\n\nA number of parasites of this genus are known to exist, including microsporidia of the genera \"Amblyospora\", \"Crepidulospora\", \"Senoma\" and \"Parathelohania\".\n\nMicrosporidia infecting the aquatic stages of insects, a group that includes mosquitoes and black flies, and copepods appear to form a distinct clade from those infecting terrestrial insects and fish. Two distinct life cycles are found in this group. In the first type, the parasite is transmitted by the oral route and is relatively species nonspecific. In the second, while again the oral route is the usual route of infection, the parasite is ingested within an already infected intermediate host. Infection of the insect larval form is frequently tissue-specific, and commonly involves the fat body. Vertical (transovarial) transmission is also known to occur.\n\nFew phylogenetic studies of these parasites have been done, and their relationship to their mosquito hosts is still being determined. One study suggested \"Parathelohania\" is an early diverging genus within this group.\n\nThe parasite \"Wolbachia\" bacteria have also been studied for use as control agents.\n\n\n"}
{"id": "49442301", "url": "https://en.wikipedia.org/wiki?curid=49442301", "title": "Audiology and hearing health professionals in Nigeria", "text": "Audiology and hearing health professionals in Nigeria\n\nNigeria is a low income developing country located in Africa. Currently, there is a lack of information regarding audiological and hearing healthcare in Nigeria.\nGNI $Int PPP: Per Capita Gross National Income in international dollars; Pop. (000s): Population (000s); Auds: Total Audiologists; Auds/Mil. Pop: Audiologists per million people; ENTs: Total ENT surgeons; ENTs/Mil. Pop: ENT surgeons per million people; Aud Phys: Audiological Physicians; Aud Techs: Audiological Technicians; SLT: Speech-Language Therapists; TOD: Teachers of the Deaf\"\n"}
{"id": "24793046", "url": "https://en.wikipedia.org/wiki?curid=24793046", "title": "Australian Journal of Primary Health", "text": "Australian Journal of Primary Health\n\nThe Australian Journal of Primary Health is a quarterly peer-reviewed healthcare journal published by CSIRO Publishing on behalf of the Australian Institute for Primary Care and Ageing (La Trobe University). It was established in 1995 as the \"Australian Journal of Primary Health Interchange\" and obtained its current name in 2001. The journal covers all aspects of community health services and primary health care.\n\nThe current Editors-in-Chief are Amanda Kenny and Virginia Lewis.\n\nThe journal is abstracted and indexed in Applied Social Sciences Index and Abstracts, Australasian Medical Index, Australian Public Affairs Information Service, CINAHL, Embase, Google Scholar, Journal Citation Reports (Sciences Edition), Journal Citation Reports (Social Sciences Edition), MEDLINE, Science Citation Index Expanded, Scopus, Social Sciences Citation Index, Social SciSearch, Social Services Abstracts and Sociological Abstracts.\n\nAccording to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 1.152.\n"}
{"id": "21269148", "url": "https://en.wikipedia.org/wiki?curid=21269148", "title": "Auxiliary ego", "text": "Auxiliary ego\n\nAn auxiliary ego, also known as simply an auxiliary, is the position taken by other participants in a role-playing exercise, or psychodrama, in order to simulate particular situations for the protagonists. As role-playing can include more than one protagonist, each may be operating in the role of auxiliary ego to each other as the exploration of each role changes with the entrance of new situations. The director of the role-play, often a teacher or counselor, can also be an auxiliary and typically is when clients are in the position of protagonist.\n\n"}
{"id": "20935095", "url": "https://en.wikipedia.org/wiki?curid=20935095", "title": "Capital punishment in Cape Verde", "text": "Capital punishment in Cape Verde\n\nThere is no capital punishment in current Cape Verdean law. The highest sentence is 25 years. The last execution was performed in 1835, when the islands were part of the Portuguese Empire.\n"}
{"id": "12850316", "url": "https://en.wikipedia.org/wiki?curid=12850316", "title": "Cookie decorating", "text": "Cookie decorating\n\nCookie decorating dates back to at least the 14th century when in Switzerland, springerle cookie molds were carved from wood and used to impress Biblical designs into cookies.\n\nThe artistic element of cookie making also can be traced back to Medieval Germany where Lebkuchen was crafted into fancy shapes and decorated with sugar. The story of \"Hansel and Gretel \" published with Grimm's Fairy Tales in 1812 inspired German gingerbread cookie Christmas cards.\nAlso during the 17th century, Dutch and German settlers introduced cookie cutters, decorative molds, and festive holiday cookie decorations to the United States.\n\nToday cookie decorating traditions continue in many places in the world and include such activities as cookie decorating parties, competitions, creating cookie bouquets and cookie gift baskets, and simply decorating cookies with children as a fun family activity.\n\nGlaze, royal icing and fondant are all popular choices for decorating cookies.\n\nOne of the earliest recorded forms of cookie decorating is the springerle, and the oldest known springerle mold is housed at the Swiss national museum in Zurich, Switzerland. This round-shaped mold was carved from wood in the 14th century and pictures the Easter Lamb.\n\nA springerle mold or press (carved rolling pins) is used to imprint a picture or design on to a cookie. These cookies have been the traditional Christmas cookies in Bavaria and Austria for centuries. To add to the decorative effect, the designs may be colored with food coloring, or when used for decorative purposes only, with tempera or acrylic paints.\n\nSpringerle cookies originally displayed biblical scenes and were used to teach the illiterate about the Bible. Eventually, the cookies were decorated with secular scenes depicting images of life events, such as marriages and births.\n\nFood historians also trace the artistic element of cookie making back to Medieval Germany where Lebkuchen (gingerbread) was crafted into fancy shapes and decorated with sugar. However, the Lebkuchen guilds only permitted professional gingerbread bakers to make this, with the exceptions of Christmas and Easter when anyone was free to make their own.\n\nThe first gingerbread man may have been a Renaissance Man! This cookie is often credited by food historians to Queen Elizabeth I, who during her reign (1558 to 1603) gifted VIP visitors to the court with gingerbread cookies decorated in their likenesses.\n\nThese gingerbread \"portraits\" were decorated with cloves dipped in gold.\n\nDuring the 17th century, guild employed master bakers and artisans created intricate works of art with their gingerbread houses and cookies. It was also during this period in Germany when cookies, in the form of Lebkuchen, were introduced as Christmas decorations.\n\nIn 1812, Grimm's Fairy Tales was published, and the tale of \"Hansel and Gretel\" inspired 19th century bakeries to add to their fanciful gingerbread entourage, decorated gingerbread cookie Christmas cards and finely detailed molded cookies. Tinsmiths rose to the call and crafted cookie cutters into all imaginable forms for bakeries and homemakers who relished having unique cookie cutters.\n\nMany a Victorian Christmas tree was adorned with decorated cookies in the shapes of animals and gingerbread men.\n\nAlso during the 17th century, Dutch and German settlers introduced cookie cutters, decorative molds, and festive holiday decorations to the United States. Gingerbread was likely the first U.S.-made Christmas cookie. Sugar cookies, one of the most widely decorated of cookies today, evolved from the English.\n\nThe German cookie cutters produced more stylized cookies, many with secular Christmas subjects, and for less cost, than the local American tinsmiths. When import laws opened the floodgates to low-cost, German-imported cooking utensils, including cookie cutters, between 1871 and 1906, the American tradition of decorating cookies for Christmas tree ornamentation took hold. In response to this cookie cutter boom, U.S. published cookbooks began featuring cookies in decorative shapes such as bells and Santa Clauses.\n\nToday cookie decorating traditions continue in many places in the world and include: decorating cookies for Christmas and other holidays, cookie decorating parties, decorating cookies for cookie bouquets and gift baskets, trimming the Christmas tree with decorated cookies, and decorating cookies with the children, to name a few.\n\nCookie decorating events can even be found at history and art museums. And they are frequently found at holiday events, community centers and classrooms. Decorated cookies also win ribbons at county and state fairs.\n\nMany decorating techniques used today do not require elaborate cookie cutters. The simplest of shapes can be quite versatile in serving various themes. For example, a star-shaped cutter can be used for Christmas, 4 July, and messages of congratulations. A circle can be decorated as a sun, ball, flower, spider web, or smiley face.\n\nBut some occasions call for special cookie cutters, even custom-made ones. For example, in honor of a 50th wedding anniversary, a photograph of the couple's first car could be sent to a company, and the cutter would be custom made to depict this. Then, the person making the cookies would decorate them to complete the depiction.\n\nRoyal icing is often used to decorate cookies because it dries hard. At the White House 2005 Christmas, Thaddeus DuBois, the White House Executive Pastry Chef at that time, decorated snowflake cookies with brushed and piped royal icing. In this case, as with many of the decorated cookies Dubois made for the president, his family and their guests, the traditional royal icing was used, a mixture of raw egg whites, powdered sugar, and a drop of lemon juice.\n\nDue to health issues with raw egg consumption, such as salmonella, an egg-free variation of royal icing is sometimes alternatively used by decorators. Meringue powder is used instead of the egg whites to create stiffness. Pasteurized refrigerated egg whites are sold at grocery stores for a safer traditional recipe.\n\nA sugar glaze made without egg whites and consisting of powdered sugar, water, corn syrup and flavoring (such as almond) is another popular choice for decorating cookies.\nTo decorate a cookie with glaze, an outline is piped just inside the edge of the cookie. Then the design is filled by piping a line of glaze back and forth across the cookie, while staying within the boundaries of the outline.\n\nThe glaze must be applied quickly because royal icing crusts quickly, but it needs to be moist enough for the piped lines to melt into each, so that the design is smooth.\nWhen the icing may crust faster than a design can be filled, the design can be blocked off first into smaller sections. To block off the design, cookie decorators pipe the outline of the cookie as usual, but then section it off in smaller sections, filling them in one at a time. Empty nooks and crannies that the decorating tip didn't pipe into can be filled by carefully dragging a toothpick through the icing into any empty spaces.\n\nCookies can be decorated with more than one color of icing. This is accomplished by allowing the first color to dry completely (often for as long as 2 hours) before adding the second color of icing.\n\nWhile the goal is usually to keep the colors separated when filling in a design on a cookie with icing (such as the white of Santa's beard from the red of his suit), sometimes the colors or bled together on purpose to create a design such as a spider web design. First a white outline is piped and filled in with white. Then using the black icing, a spiral from the center to the outer edge is piped. The web is created by dragging a toothpick in a straight line from the center across the spiral to the outer edge. The more lines, the more intricate the web.\ndecorating bags,one filled with white icing and another with black, and both fit with small round tips.\n\nFondant is another type of icing that is used to decorate cookies. Fondant can be colored by kneading the coloring into the dough. It can be rolled out, and then cut in shapes to match the cookies or their designs. Fondant can be purchased ready-made; however it is not favored for its taste. A homemade fondant that is often praised for its taste and function is marshmallow fondant, which is also used by cake decorators for covering cakes.\n\nWhen rolling fondant, a silicone mat or parchment will eliminate the need for dusting the rolling surface with powdered sugar, and this will make it easier to lift and transfer the cut pieces to the cookies.\n\nThe rolled out fondant may be cut into shapes with the same cookie cutters used to cut the cookies. Once cut out, the fondant is placed on top of the cookie. Some types of fondant will adhere right away to the cookie. If the fondant doesn't stick well, the cookie surface may be brushed with a little vanilla extract, corn syrup or piping gel to provide more sticking power.\n\nFondant covered cookies, just like plain or icing covered cookies, lend themselves well to embellishments such as silver dragees and other shiny decorations. Tweezers can be a great help in positioning the tiny ornaments.\n\nAn impression mat can be used with fondant to add design and/or texture. First the fondant is rolled out and then the mat is placed face down on the fondant. Finally, by gently but firmly going over the mat with the rolling pin, the impression is made in the fondant. Then the shapes are cut out.\n\nFor example, to create a lace heart, a patch of white fondant is rolled out first on a silicone mat. Then an embossed fondant roller is slowly rolled across the surface of the fondant. A heart shaped cookie cutter is used to cut out the fondant hearts. The heart shaped fondant is then peeled off the silicone mat carefully so as not to mar the embossed design. Next, the fondant is trimmed and placed on top of the cookie. Finally the fondant-covered cookie may be brushed with a light dusting of pearl luster dust.\n\nMany of the same decorations used by cake decorators are also used on cookies. Sprinkles, as dragees, colored sugars, beads, non-pareils and finely chopped nuts, as well as more expensive decorations like edible gold leaf, are used to decorate cookies.\n\nThe silver and gold covered Dragées and other silver and gold cake and cookie decorations sometimes used have not been FDA approved in the United States. Some of these have been approved for human consumption in other countries, such as Easy Leaf's edible gold and silver in Italy.\n\nEdible gold and silver have been used for centuries to garnish foods and drinks. The precious metals come in sprinkles, small flakes and leaves and are available at specialty stores and online.\n\nHowever this usage is controversial. According to \"The Washington Post\", a U.S. Food and Drug Administration staff expert said that edible gold and silver had not gone through pre-market safety evaluations at the FDA \"because no one has sought pre-market approval.\"\n\nThe \"Washington Post\" article also reported the expert (who reportedly spoke only on the condition of anonymity) as saying he had not taken a position on edible metals, that they pass right through the body, and are \"an expensive way to throw away gold.\"\nTobias Freccia, founder of an edible gold retail website, was also quoted in the article saying a \"book of 500 gold leaves may cost $495, but a 100 mg shaker of the precious metals sells for $19.95.\"\n"}
{"id": "101174", "url": "https://en.wikipedia.org/wiki?curid=101174", "title": "Daniel McFadden", "text": "Daniel McFadden\n\nDaniel Little McFadden (born July 29, 1937) is an American econometrician who shared the 2000 Nobel Memorial Prize in Economic Sciences with James Heckman. McFadden's share of the prize was \"for his development of theory and methods for analyzing discrete choice\". He is the Presidential Professor of Health Economics at the University of Southern California and Professor of the Graduate School at University of California, Berkeley.\n\nMcFadden was born in Raleigh, North Carolina. He attended the University of Minnesota, where he received a B.S. in Physics, and a Ph.D. in Behavioral Science (Economics) five years later (1962). While at the University of Minnesota, his graduate advisor was Leonid Hurwicz, who was awarded the Economics Nobel Prize in 2007.\n\nIn 1964 McFadden joined the faculty of UC Berkeley, focusing his research on choice behavior and the problem of linking economic theory and measurement. In 1974 he introduced Conditional logit analysis.\nIn 1975 McFadden won the John Bates Clark Medal. In 1977 he moved to the Massachusetts Institute of Technology. In 1981 he was elected to the National Academy of Sciences. He returned to Berkeley in 1991, founding the Econometrics Laboratory, which is devoted to statistical computation for economics applications. He remains its director. He is a trustee of the Economists for Peace and Security. In 2000 he won the Erwin Plein Nemmers Prize in Economics.\n\nIn January 2011 McFadden was appointed the Presidential Professor of Health Economics at the University of Southern California (USC), and the announcement of this appointment was published on January 10, 2011. McFadden will have joint appointments at the USC Price School of Public Policy and the Department of Economics at the USC Dana and David Dornsife College of Letters, Arts and Sciences to examine fundamental problems facing the health care sector, looking specifically at how consumers make choices about health insurance and medical services.\n\n\n"}
{"id": "1494572", "url": "https://en.wikipedia.org/wiki?curid=1494572", "title": "DeCODE genetics", "text": "DeCODE genetics\n\ndeCODE genetics, Inc. (Icelandic: Íslensk erfðagreining) is a biopharmaceutical company based in Reykjavík, Iceland. The company was founded in 1996 by Kári Stefánsson to identify human genes associated with common diseases using population studies, and apply the knowledge gained to guide the development of candidate drugs. The company isolated genes believed to be involved in cardiovascular disease, cancer and schizophrenia, among other diseases (the company's research concerning the latter is said to represent the first time a gene has been identified by two independent studies to be associated with schizophrenia).\n\ndeCODE's approach to identifying genes, and in particular its proposal to set up an Icelandic Health Sector Database (HSD) containing the medical records of all Icelanders, was controversial, and prompted national and international criticism for its approach to the concepts of privacy and consent. \n\nThe company was removed from the NASDAQ Biotechnology Index in November 2008.\nIn November 2009 the company filed for chapter 11 bankruptcy in a US court, listing total assets of $69.9 million and debt of $313.9 million. According to American law, deCODE was allowed to continue its operations. In January 2010 most of deCODE genetics Inc.’s assets were purchased by Saga Investments LLC – an investment company whose owners include Polaris Venture Partners and ARCH Venture Partners - who said they intended to continue most services including deCODE diagnostics and deCODEme™ personal genome scans and management team.\n\nIn December 2012, deCODE genetics was purchased by Amgen for $415 million which in October 2013 spun off deCODE genetics' systems and database to a new company called NextCODE Health which in turn was acquired in January 2015 by the Chinese company WuXi PharmaTech for $65 million.\n\nDeCODE was founded in 1996 by Ernir Kristján Snorrason, Kári Stefánsson, and Kristleifur Kristjánsson.\n\nIn the late 1990s deCODE proposed to create the world's first population-wide genomic biobank by collecting data from the entire population of Iceland, which numbered 270,000 at the time. The plan had these three major components: creating a genealogical database, collecting biobank specimens by means of which genotyping could be done, and creating a national electronic health record system to connect genetic data to each individual's phenotype.\n\nIn December 1998 with lobbying from deCODE, the Icelandic Parliament passed the Act on Health Sector Database which permitted public bidding for the right of a company to create this health database and use it for various purposes. The parliament shortly thereafter granted deCODE the right to create this database after the company made a successful bid to do so.\n\nAs a step toward the personal genome, the company has announced that its deCODEme service is available for $985 to anyone who wishes to send a cheek swab to learn details about disease risk and ancestry. This service was launched in November 2007 and thereby became the first web-based service to offer a comprehensive genome scan and an online analysis of an individual's DNA. More than one million single nucleotide polymorphisms are included in the scan. deCODEme claims that the DNA profile it provides can supply its customers with a basis from which they are able calculate the relative risk of developing these diseases and thereby enable them to make better informed decisions about medical prevention and treatment. The deCODEme service currently includes information on the genetic susceptibility to close to 45 common diseases such as myocardial infarction, atrial fibrillation, several types of cancers and type-2 diabetes as well as providing insights into distant ancestry and geographical origins.\n\nThe deCODEme service was discontinued in January 2013 and deCODE genetics stopped selling personal genetic tests altogether.\n\nThe work of deCODE is criticised by Arnaldur Indriðason's novel \"Jar City\" from 2000, which was adapted as a film in 2006.\n\ndeCODE and Kári Stefánsson are satirised as VikingDNA and Professor Lárus Jóhannsson in \"Dauðans óvissi tími\" by Þráinn Bertelsson (Reykjavík: JPV Útgáfu, 2004).\n\ndeCODE and specifically Kári Stefánsson is presented as the creator of monstrous genetic hybrids in Óttar M. Norðfjörð's satirical 2007 work \"Jón Ásgeir & afmælisveislan \"([Reykjavík]: Sögur, 2007), and the history of DeCODE appears both directly and in allegorised form (under the fictional name OriGenes) in the same author's \"\" (Reykjavík: Sögur, 2011). deCODE is the model for the company CoDex, in \"CoDex 1962\" by Sjón.\n\n\n\n"}
{"id": "6333799", "url": "https://en.wikipedia.org/wiki?curid=6333799", "title": "Delayed ejaculation", "text": "Delayed ejaculation\n\nDelayed ejaculation, also called \"retarded ejaculation\" or \"inhibited ejaculation,\" is a man's inability for or persistent difficulty in achieving orgasm, despite typical sexual desire and sexual stimulation. Generally, a man can reach orgasm within a few minutes of active thrusting during sexual intercourse, whereas a man with delayed ejaculation either does not have orgasms at all or cannot have an orgasm until after prolonged intercourse which might last for 30–45 minutes or more. In most cases, delayed ejaculation presents the condition in which the man can climax and ejaculate only during masturbation, but not during sexual intercourse. It is the least common of the male sexual dysfunctions, and can result as a side effect of some medications. In one survey, 8% of men reported being unable to achieve orgasm over a 2-month period or longer in the previous year.\n\nDelayed ejaculation can be \"mild\" (men who still experience orgasm during intercourse, but only under certain conditions), \"moderate\" (cannot ejaculate during intercourse, but can during fellatio or manual stimulation), \"severe\" (can ejaculate only when alone), or \"most severe\" (cannot ejaculate at all). All forms may result in a sense of sexual frustration..\n\nMedical conditions that can cause delayed ejaculation include hypogonadism, thyroid disorders, pituitary disorders such as Cushing's disease, prostate surgery outcome, and drug and alcohol use. Difficulty in achieving orgasm can also result from pelvic surgery that involved trauma to pelvic nerves responsible for orgasm. Some men report a lack of sensation in the nerves of the glans penis, which may or may not be related to external factors, including a history of circumcision.\n\nDelayed ejaculation is a possible side effect of certain medications, including selective serotonin reuptake inhibitors (SSRIs), opiates such as morphine, methadone, or oxycodone, many benzodiazepines such as Valium, certain antipsychotics, and antihypertensives.\n\nPsychological and lifestyle factors have been discussed as potential contributors, including insufficient sleep, distraction due to worry, distraction from the environment, anxiety about pleasing their partner and anxiety about relationship problems.\n\nOne proposed cause of delayed ejaculation is adaptation to a certain masturbatory technique. Lawrence Sank (1998) wrote about the \"Traumatic masturbatory syndrome\", when the sensations a man feels when masturbating may bear little resemblance to the sensations he experiences during intercourse. Factors such as pressure, angle and grip during masturbation can make for an experience so different from sex with a partner that the ability to ejaculate is reduced or eliminated.\n\nOn the same note, it may be the visual factor present in masturbation that may delay vaginal ejaculation. As the sensation during masturbation is intrinsically linked with the visual input of a sexual model, be it male or female, the diminished view during sex may result in the loss of that link, and as such, delay ejaculation in the man. A possible cure for this may be a better view of the partner during intercourse. \n\nTherapy usually involves homework assignments and exercises intended to help a man get used to having orgasms through insertional intercourse, vaginal, anal, or oral, that is through the way to which he is not accustomed. Commonly, the couple is advised to go through three stages. At the first stage, a man masturbates in the presence of his partner. Sometimes, this is not an easy matter as a man may be used to having orgasms alone. After a man learns to ejaculate in the presence of his partner, the man's hand is replaced with the hand of his partner. In the final stage, the receptive partner inserts the insertive partner's penis into the partner's vagina, anus, or mouth as soon as the ejaculation is felt to be imminent. Thus, a man gradually learns to ejaculate inside the desired orifice by an incremental process.\n\nMeditation has demonstrated effectiveness in case studies.\n\nThere is yet no reliable medication for delayed ejaculation. PDE5 inhibitors such as Viagra have little effect. In fact, Viagra has a delaying effect on ejaculation, possibly through additional effect in the brain or decrease of sensitivity in the head of the penis.\n\n\n"}
{"id": "25390530", "url": "https://en.wikipedia.org/wiki?curid=25390530", "title": "Enamel infraction", "text": "Enamel infraction\n\nEnamel infractions are microcracks seen within the dental enamel of a tooth. They are commonly the result of dental trauma to the brittle enamel, which remains adherent to the underlying dentine. They can be seen more clearly when transillumination is used. \n\nEnamel infractions are found more often in older teeth, as the accumulated trauma is greatest. \nEnamel infractions can also be found as a result of iatrogenic damage inadvertently caused by instrumentation during dental treatments.\n\nMinor infraction may not require any treatment, however major infraction may require treatment including smoothing, fluoride treatment and crown restoration.\n"}
{"id": "15639300", "url": "https://en.wikipedia.org/wiki?curid=15639300", "title": "Françoise Meunier", "text": "Françoise Meunier\n\nFrançoise Meunier is a Belgian medical doctor and Director General of the European Organisation for Research and Treatment of Cancer (EORTC).\n\nShe graduated as a Medical Doctor at the Université Libre de Bruxelles (ULB) in 1974 and obtained a Master of Medical Oncology at the ULB in 1976. In 1977 she certified before the \"Educational Commission for Foreign Medical Graduates\" (ECFMG) in the United States. She obtained a Fulbright Prize in 1977 and in 1977 and 1978 worked as a Research Fellow at the Memorial Sloan-Kettering Cancer Center in New York City (United States). She obtained a Master in Internal Medicine at the ULB in 1979 and a PhD and Venia legendi in 1985. In 1990 she obtained a \"Certificate in Hospital Hygiene\" at the ULB.\n\nIn 1994 she became a Fellow of the Royal College of Physicians (FRCP) in the United Kingdom and in 1995 a Fellow (by distinction) of the Faculty of Pharmaceutical Medicine (Royal Colleges of Physicians). In 2001 she became a member of the Belgian College of Pharmaceutical Medicine (BCPM) and in 2003 graduated as \"Specialist in Health Database Management\" (Belgian Health Ministry). As of 2006, she is a member of the \"Académie Royale de Médecine de Belgique\".\n\n\nShe was raised to the rank of baroness by Albert II in 2007.\n\n\n"}
{"id": "3399344", "url": "https://en.wikipedia.org/wiki?curid=3399344", "title": "George Frederick Dick", "text": "George Frederick Dick\n\nGeorge Frederick Dick (July 21, 1881 – October 10, 1967) was an American physician and bacteriologist best known for his work with scarlet fever.\n\nDick studied scarlet fever whilst serving the Army Medical Corps during World War I. Dick continued with his research into scarlet fever following the war, and in 1923, in collaboration with his wife Gladys Rowena Dick, managed to locate the cause of the disease in a toxin produced by a strain of Streptococcus bacteria. Using this, they were able to create an antitoxin for treatment and a non-toxic vaccine for immunization.\n\nHe was a professor of clinical medicine at Rush Medical College, Chicago (1918–33), and then became the head of the department of medicine at the University of Chicago (1933–45).\n\n"}
{"id": "26082350", "url": "https://en.wikipedia.org/wiki?curid=26082350", "title": "Global Health Initiatives", "text": "Global Health Initiatives\n\nGlobal Health Initiatives (GHIs) are humanitarian initiatives that raise and disburse additional funds for infectious diseases– such as AIDS, tuberculosis, and malaria– for immunization and for strengthening health systems in developing countries. GHIs classify a type of global initiative, which is defined as an organized effort integrating the involvement of organizations, individuals, and stakeholders around the world to address a global issue (i.e.: climate change, human rights, etc.).\n\nExamples of GHIs are the President’s Emergency Plan for AIDS Relief (PEPFAR), the Global Fund to Fight AIDS, Tuberculosis and Malaria (Global Fund), and the World Bank's Multi-country AIDS Programme (MAP), all of which focus on HIV/AIDS. The Gavi (formerly the GAVI Alliance) focuses on immunization, particularly with respect to child survival.\n\nIn terms of their institutional structure, GHIs have little in common with each other. In terms of their function – specifically their ability to raise and disburse funds, provide resources and coordinate and/or implement disease control in multiple countries – GHIs share some common ground, even if the mechanisms through which each of these functions is performed are different.\n\nPEPFAR - an initiative established in 2003 by the Bush Administration - and PEPFAR II (PEPFAR’s successor in 2009 under the Obama Administration) are bilateral agreements between the United States and a recipient of its development aid for HIV/AIDS – typically an international non-government organisation INGO or a recipient country’s government. The Global Fund, established in 2002, and the GAVI Alliance, launched in 2000, are public-private partnerships that raise and disburse funds to treat AIDS, Tuberculosis and Malaria, and for immunization and vaccines. The World Bank is an International financial institution. It is the largest funder of HIV/AIDS within the United Nations system and has a portfolio of HIV/AIDS programmes dating back to 1989. In 2000, the Bank launched the first phase of its response to HIV/AIDS in Sub-Saharan Africa – the Multi-Country AIDS Program (MAP). This came to an end in 2006 when a second phase – Agenda for Action 2007-11 – came into effect.\n\nTracking funding from GHIs poses challenges. However, it is possible to determine the amounts of funding GHIs commit and disburse from sources such as the OECD CRS online database, as well as data provided by individual GHIs (Figure 1).\n\nSince 1989, the World Bank has committed approximately US$4.2bn in loans and credits for programs, and has disbursed US$3.1bn. Of this total, the Bank's MAP has committed US$1.9bn since 2000. Through bilateral contributions to HIV/AIDS and Tuberculosis programmes and donations to the Global Fund, PEPFAR has donated approximately US$25.6bn since 2003. In July 2008, the U.S Senate re-authorised a further US$48 bn over five years for PEPFAR II, of which US$6.7bn has been requested for FY 2010. During the period 2001-2010, donors have pledged US$21.1bn to the Global Fund, of which US$15.8bn has been paid by donors to the Fund. Gavi has approved US$3.7bn for the period 2000-2015\n\nThe amount of political priority given to Global Health Initiatives varies between national and international governing powers. Though evidence shows that there exists an inequity between resource allocation for initiatives concerning issues such as child immunization, HIV/AIDS, and family planning in comparison to initiatives for high-burden disorders such as malnutrition and pneumonia, the source of this variance is unknown due to lack of systematic research pertaining to this subject. Global political priority is defined as the extent to which national and international political leaders address an issue of international concern through support in the forms of human capital, technology, and/or finances in order to aid efforts to resolve the problem. Global political priority is demonstrated through national and international leaders expressing sustained concern both privately and publicly, political systems and organizations enacting policies to help alleviate the issue, and national and international agencies providing resource levels that reflect the severity of the given crisis.\n\nThe amount of attention a given global initiative receives is considerably dependent on the power and authority of actors connected to the issue, the power and impact of ideas defining and describing the issue, the power of political contexts framing the environments in which the actors operate to address the issue, as well as the weight and power of issue characteristics indicating the severity of the issue (i.e.: statistical indicators, severity metrics, efficacy of proposed interventions, etc.). Factors including objective measurability, scalability of the issue and proposed interventions, ability to track and monitor progress, risk of perceived harm, as well as simplicity and affordability of proposed solutions all contribute to the degree to which a given global initiative is likely to receive political attention.\n\nHowever, case studies have shown that the likelihood of global initiatives garnering public and political attention is not limited to the aforementioned factors. For example, initiatives concerning polio eradication continue to receive substantial resources in spite of the relatively small global burden of disease as compared to chronic diseases such as cancer, cardiovascular disorders, diabetes, and some communicable diseases such as pneumonia which comparatively attract fewer worldwide resources irrespective of the high morbidity and mortality rates associated with such diseases. These cases highlight the need for extensive research methods and evaluative measures to assess the relative causal weights of factors used to determine how global political priority is attributed to global health initiatives. Existing debates also attribute factors such as the increasing influences of economic globalization, international organizations, and economic actors with little to no previous health remit as each contributing to the evolution of global health governance.\n\nThere is much discussion about the extent to which the volume of these additional funds creates multiple effects that positively and/or negatively impact both health systems and health outcomes for specific diseases. Assessing the direct impact of GHIs on specific diseases and health systems poses challenges pertaining to the issue of attributing particular effects to individual GHIs. As such, a common response in evaluations of GHIs is to acknowledge the inherent limitations of establishing causal chains in what is a highly complex public health environment, and to base conclusions on adequacy statements resulting from trends that demonstrate substantial growth in process and impact indicators.\n\nHowever, existing literature argues that this approach towards evaluating GHIs can inadvertently result in overlooking the impact of social determinants on a disease, as implementers and evaluators are less likely to tackle the complexity of a disease within the larger social, political, cultural, and environmental system. Even if a GHI is effectively evaluated– perhaps showing a decrease in disease prevalence– the challenge remains of comprehensively analyzing the long-term impacts of the GHI by addressing the root social, political, or environmental causes of the disease. Accordingly, existing debates suggest that GHIs should be less concerned with the eradication of specific diseases, and should instead focus primarily on factors– such as basic living conditions, sanitation, and access to nutritious food– that are essential to delivering a sustainable heath program.\n\nA small number of institutions have shaped, and continue to shape, research on GHIs. In 2003, researchers at Abt Associates devised an influential framework for understanding the system-wide effects of the Global fund which has informed much subsequent research, including their own studies of system-wide effects of the Global Fund in Benin, Ethiopia, Georgia and Malawi - often referred to as the 'SWEF' studies.\n\nAbt continues to support ongoing research on the effects of GHIs in multiple countries. The Washington-based Center for Global Development has also been very active in its analysis of GHIs, particularly PEPFAR financing. The Center's HIV/AIDS Monitor is essential reading for researchers of GHIs. With hubs in London and Dublin, the Global Health Initiatives Network (GHIN) has been coordinating and supporting research in 22 countries on the effects of GHIs on existing health systems.\n\nKnowledge of the effects of GHIs on specific diseases and on health systems comes from multiple sources.Longitudinal studies enable researchers to establish baseline data and then track and compare GHI effects on disease control or country health systems over time. In addition to Abt Associates' SWEF studies, additional early examples of this type of analysis were three-year, multi-country studies of the Global Fund in Mozambique, Tanzania, Uganda and Zambia. In 2009, research findings were published from tracking studies in Kyrgyzstan, Peru and Ukraine that sought to identify the health effects of the Global Fund at national and sub-national levels.\n\nIn contrast to longitudinal studies, multi-country analyses of GHIs can provide a ‘snapshot’ of GHI effects but are often constrained by “aggressive timelines”. The Maximising Positive Synergies Academic Consortium, for example, reported in 2009 on the effects of the Global Fund and PEPFAR on disease control and health systems, drawing on data from 20 countries. Most GHI evaluations – both internally and externally commissioned – rely on this type of short-term analysis and, inevitably, there is often a trade-off between depth and breadth of reporting.\n\nSynthesis of data from multiple sources is an invaluable resource for making sense of the effects of GHIs. Early synthesis studies include a 2004 synthesis of findings on the effects of the Global Fund in four countries by researchers at the London School of Hygiene and Tropical Medicine (LSHTM), a 2005 study by McKinsey & Company and an assessment of the comparative advantages of the Global Fund and World Bank AIDS programs.\n\nTwo wide-ranging studies were published in 2009: a study of interactions between GHIs and country health systems commissioned by the World Health Organisation and a study by researchers from LSHTM and the Royal College of Surgeons in Ireland. The latter study - The effects of global health initiatives on country health systems: a review of the evidence from HIV/AIDS control – reviewed the literature on the effects of the Global fund, the World Bank MAP and PEPFAR on country health systems with respect to: 1) national policy; 2) coordination and planning; 3) stakeholder involvement; 4) disbursement, absorptive capacity and management; 5) monitoring & evaluation; and 6) human resources (Table 2).\n\nIn a comparison between the three largest donors in sponsoring efforts to win the fight against AIDS in Africa, a research study found that PEPFAR performs best in money transfer and data collection; the Global Fund outperforms in tailoring programmatic initiatives and sharing data; and MAP performs highest in collaborating with government systems, strengthening health systems, and helping to build the capacity of local recipients. Each of the four GHIs summarized has been evaluated at least once since 2005 and all four produce their own annual reports.\n\nThe primary purpose of the MAP initiative was to introduce a major upscaling of multi-sectoral approach to responding to the HIV/AIDS crisis in Sub-Saharan Africa by involving a multitude of stakeholders including community-based organizations (CBOs), non-governmental organizations (NGOs), line ministries, and state governments at the highest levels.\n\nA comprehensive study of MAP programs published in 2007 reviewed whether MAP was implemented as designed, but did not evaluate MAP or assess its impact. In addition, there have been two evaluations that provide important additional insight into the effectiveness of the Bank's HIV/AIDS programmes (though not specifically MAP focused). In 2005, the Bank conducted an internal evaluation - Committing to Results: Improving the Effectiveness of HIV/AIDS Assistance - which found that National AIDS strategies were not always prioritised or costed.\n\nSupervision, and monitoring and evaluation (M&E), were weak; civil society had not been engaged; political commitment and capacity had been overestimated, and mechanisms for political mobilisation were weak; and bank research and analysis, whilst perceived to be useful, was not reaching policy makers in Africa. In 2009, a hard-hitting evaluation of the Bank’s Health, Nutrition and Population support – Improving Effectiveness of Outcomes for the Poor in Health, Nutrition and Population – found that a third of the Bank’s HNP lending had not performed well, and that while the performance of the Bank’s International Finance Corporation investments had improved, accountability was weak.\n\nUnlike many implementing agencies, the Global Fund has no presence in the countries it supports; rather it is a financial mechanism which provides funding to countries in the form of grants through a Secretariat in Geneva on the competitive basis of country proposals. Special emphasis is placed on proposals demonstrating country ownership as well as those that meet other evidence-based, performance-based, and inclusivity-based criteria.\n\nA five-year, comprehensive evaluation of the Global Fund published a synthesis report in 2009 of findings from three Study areas. The Fund’s Technical Evaluation Research Group (TERG) Five Year Evaluation (5YE) of the Global Fund drew on data from 24 countries to evaluate the Fund’s organisational effectiveness and efficiency, partnership environment and impact on AIDS, TB and Malaria. The Evaluation highlighted the possible decline in HIV incidence rate in some countries, and rapid scale up of funding for HIV/AIDS, access and coverage, but also identified major gaps in support for national health information systems, and poor drug availability.\n\nThough GHIs have been instrumental in bringing national and international attention to crucial global health issues, existing debates suggest that they can also negatively impact country health systems. As such, disease-specific GHIs such as GAVI have worked to integrate health system strengthening (HSS) measures into programmatic implementation. However, the existing global debate questions the efficacy of HSS programs aimed at targeting technical solutions with clear measurable outcomes versus those more broadly focused on supporting holistic health systems.\n\nIn 2008, an evaluation of GAVI’s vaccine and immunization support - Evaluation of the GAVI Phase 1 performance - reported increased coverage of HepB3, Hib3 and DTP3 and increased coverage in rural areas but also a lack of cost data disaggregated by vaccine that prevented GAVI from accurately evaluating the cost effectiveness of its programs and vaccines, and an “unrealistic” reliance by GAVI on the market to reduce the cost of vaccines. The same year, a study of the financial sustainability of GAVI vaccine support - Introducing New Vaccines in the Poorest Countries: What did we learn from the GAVI Experience with - found that although GAVI funding equated to $5 per infant in developing countries per year for the period 2005-10, resource need was accelerating faster than growth in financing.\n\nFindings from two evaluations of GAVI’s support for health systems strengthening (HSS) were published in 2009. An external evaluation by HLSP found insufficient technical support provided to countries applying for GAVI grants, an under-performing Independent Review Committee (IRC), and weaknesses in GAVI’s monitoring of grant activities. The study also found that countries were using GAVI grants for ‘downstream’ short-term HSS fixes rather than ‘upstream’ and long-term structural reform.\nA study by John Snow, Inc praised the multi-year, flexible and country-driven characteristics of GAVI HSS grant funding and encouraged GAVI to continue this support. But also found weak M&E of grant activity, low Civil Society involvement in the HSS proposal development process, unclear proposal writing guidelines, and over-reliance by countries on established development partners for assistance in implementing health system reform.\n\nA quantitative study by Stanford University in 2009 – The President's Emergency Plan for AIDS Relief in Africa: An Evaluation of Outcomes – calculated a 10.5% reduction in the death rate in PEPFAR’s 12 focus countries, equating to 1.2 million lives saved at a cost of $2450 per death averted. In 2007, an evaluation of PEPFAR by the Institute of Medicine found that PEPFAR had made significant progress in reaching its targets for prevention, treatment and care but also reported that budget allocations \"limit the Country Teams ability to harmonize PEPFARs activities with those of the partner government and other donors\", and PEPFARs ABC (Abstinence, Be faithful, and correct and consistent Condom use) priorities \"fragment the natural continuum of needs and services, often in ways that do not correspond with global standards\".\n\nThe PEPFAR program has brought about substantial impact in its recipient countries. The level of urgency and scale of initiatives led through the PEPFAR program were commensurate with that of the HIV/AIDS epidemic at the time of implementation. Existing debates suggest that the next phase of the program consider placing emphasis on the development of knowledge surrounding HIV/AIDS programming.\n\n"}
{"id": "20603142", "url": "https://en.wikipedia.org/wiki?curid=20603142", "title": "Health care in France", "text": "Health care in France\n\nThe French health care system is one of universal health care largely financed by government national health insurance. In its 2000 assessment of world health care systems, the World Health Organization found that France provided the \"close to best overall health care\" in the world. In 2011, France spent 11.6% of GDP on health care, or US$4,086 per capita, a figure much higher than the average spent by countries in Europe but less than in the US. Approximately 77% of health expenditures are covered by government funded agencies.\n\nMost general physicians are in private practice but draw their income from the public insurance funds. These funds, unlike their German counterparts, have never gained self-management responsibility. Instead, the government has taken responsibility for the financial and operational management of health insurance (by setting premium levels related to income and determining the prices of goods and services refunded). The French government generally refunds patients 70% of most health care costs, and 100% in case of costly or long-term ailments. Supplemental coverage may be bought from private insurers, most of them nonprofit, mutual insurers. Until 2000, coverage was restricted to those who contributed to social security (generally, workers or retirees), excluding some poor segments of the population; the government of Lionel Jospin put into place universal health coverage and extended the coverage to all those legally resident in France. Only about 3.7% of hospital treatment costs are reimbursed through private insurance, but a much higher share of the cost of spectacles and prostheses (21.9%), drugs (18.6%) and dental care (35.9%) (figures from the year 2000). There are public hospitals, non-profit independent hospitals (which are linked to the public system), as well as private for-profit hospitals. \n\nFrance 1871–1914 followed well behind Bismarckian Germany, as well as Great Britain, in developing the welfare state including public health. Tuberculosis was the most dreaded disease of the day, especially striking young people in their 20s. Germany set up vigorous measures of public hygiene and public sanatoria, but France let private physicians handle the problem, which left it with a much higher death rate. The French medical profession jealously guarded its prerogatives, and public health activists were not as well organized or as influential as in Germany, Britain or the United States. For example, there was a long battle over a public health law which began in the 1880s as a campaign to reorganize the nation's health services, to require the registration of infectious diseases, to mandate quarantines, and to improve the deficient health and housing legislation of 1850. However the reformers met opposition from bureaucrats, politicians, and physicians. Because it was so threatening to so many interests, the proposal was debated and postponed for 20 years before becoming law in 1902. Success finally came when the government realized that contagious diseases had a national security impact in weakening military recruits, and keeping the population growth rate well below Germany's.\n\nThe current system has undergone several changes since its foundation in 1945, though the basis of the system remains state planned and operated.\n\nJean de Kervasdoué, a health economist, believes that French medicine is of great quality and is \"the only credible alternative to the Americanization of world medicine.\" According to Kervasdoué, France's surgeons, clinicians, psychiatrists, and its emergency care system (SAMU) are an example for the world. However, despite this, Kervasdoué criticizes the fact that hospitals must comply with 43 bodies of regulation and the nit-picking bureaucracy that can be found in the system. Kervasdoué believes that the state intervenes too much in regulating the daily functions of French hospitals.\n\nFurthermore, Japan, Sweden, and the Netherlands have health care systems with comparable performance to that of France's, yet spend no more than 8% of their GDP (against France's spending of more than 10% of its GDP).\n\nAccording to various experts, the battered state of the French social security system's finances is causing the growth of France's health care expenses. To control expenses, these experts recommend a reorganization of access to health care providers, revisions to pertinent laws, a repossession by CNAMTS of the continued development of medicines, and the democratization of budgetary arbitration to counter pressure from the pharmaceutical industry.\n\nThe entire population must pay compulsory health insurance. The insurers are non-profit agencies that annually participate in negotiations with the state regarding the overall funding of health care in France. There are three main funds, the largest of which covers 84% of the population and the other two a further 12%. A premium is deducted from all employees' pay automatically. The 2001 Social Security Funding Act, set the rates for health insurance covering the statutory health care plan at 5.25% on earned income, capital and winnings from gambling and at 3.95% on benefits (pensions and allowances).\n\nAfter paying the doctor's or dentist's fee, a proportion is reimbursed. This is around 75 to 80%, but can be as much as 100% (if you have a long duration medical problem such as a cancer). The balance is effectively a co-payment paid by the patient but it can also be recovered if the patient pays a regular premium to a voluntary health insurance scheme (more than 99% of the population as every worker is entitled, per law, to access to a company subsidized plan). Most of them are managed by non-for-profit groups.\n\nUnder recent rules (the coordinated consultation procedure, in French: \"\"parcours de soins coordonné\"), general practitioners (\"médecin généraliste\" or \"docteur\"\") are expected to act as \"gate keepers\" who refer patients to a specialist or a hospital when necessary. However the system offers free choice of the reference doctor, which is not restricted to only general practitioner and may still be a specialist or a doctor in a public or private hospital. The goal is to limit the number of consultations for the same illness. The incentive is financial in that expenses are reimbursed at much lower rates for patients who go directly to another doctor (except for dentists, ophthalmologists, gynaecologists and psychiatrists); vital emergencies are still exempt from requiring the advice from the reference doctor, which will be informed later. As costs are borne by the patient and then reimbursed (most of the time on the spot as all doctors and drugstores can read the \"Carte Vitale\", a smart card with all information on the patient and the co-insurance company), patients have freedom of choice of where to receive health care services.\n\nAround 62% of hospital beds in France are provided by public hospitals, around 14% by private non-profit organizations, and 24% by for-profit companies.\n\nMinister of Health and Solidarity is a cabinet position in the government of France. The healthcare portfolio oversees the public services and the health insurance part of Social Security. As ministerial departments are not fixed and depend on the Prime Minister's choice, the Minister sometimes has other portfolios among Work, Pensions, Family, the Elderly, Handicapped people and Women's Rights. In that case, they are assisted by junior Ministers who focus on specific parts of the portfolio.\n\nThe global system (social security system) will cover 70% of the global cost unless you have an ALD (long duration medical problem) such as cancer or diabetes where all expenses are covered (100%). In the Alsace-Moselle region, due to its special history as having belonged to France and Germany at one time or another, the social security system covers 90% of the global cost. People can subscribe to a \"mutuelle\" (non profit insurance) or a private for-profit insurance for additional cover. All workers have access to a specific plan where their company has to pay at least 50% of the cost.\n\nPrices range from €10/month (full basic coverage i.e. 100% of all expenses and medicines) to €100/month (luxury coverage including high level chamber while in hospital, professors for children if they have to remain at home, housemaid at home if needed...).\n\nIn large cities, such as Paris, the physicians (especially specialists) charge significantly more for consultations (i.e. 70-80 EUR as opposed to 25 EUR). Because they are not adhering to the fees imposed by the Assurance Maladie, patients are very poorly reimbursed (usually a fraction of that amount) with the mutuelle covering the rest of up to 100% of the official fees. For instance, for an ophthalmologist in Paris, if the patient pays 80 EUR, he will be reimbursed 5.9 EUR by the Assurance Maladie and a maximum of 25 EUR by the mutuelle. \n\nThe \"médecin généraliste\" is the responsible doctor for a patient long-term care. This implies prevention, education, care of diseases and traumas that do not require a specialist. They also follow severe diseases day-to-day (between acute crises that may require a specialist). Since 2006, every patient has to declare one generalist doctor as a \"médecin traitant\" (treating doctor) to the healthcare fund, who has to be consulted before being eventually referred to consult any specialist (gynecologists, psychiatrists, ophtamologists and dentists aside). This policy has been applied to unclog overconsultations of specialists for non severe reasons. \n\nThey survey epidemics, fulfil a legal role (consultation of traumas that can bring compensation, certificates for the practice of a sport, death certificates, certificates for hospitalization without consent in case of mental incapacity), and a role in emergency care (they can be called by the \"SAMU\", the emergency medical service). They often go to a patient's home if the patient cannot come to the consulting room (especially in case of children or old people) and they must also perform night and week-end duty.\n\nBecause the model of finance in the French health care system is based on a social insurance model, contributions to the program are based on income. Prior to reform of the system in 1998, contributions were 12.8% of gross earnings levied on the employer and 6.8% levied directly on the employee. The 1998 reforms extended the system so that the more wealthy with capital income (and not just those with income from employment) also had to contribute; since then the 6.8% figure has dropped to 0.75% of earned income. In its place a wider levy based on total income has been introduced, gambling taxes are now redirected towards health care and recipients of social benefits also must contribute. Because the insurance is compulsory, the system is effectively financed by general taxation rather than traditional insurance (as typified by auto or home insurance, where risk levels determine premiums).\n\nThe founders of the French social security system were largely inspired by the Beveridge Report in the United Kingdom and aimed to create a single system guaranteeing uniform rights for all. However, there was much opposition from certain socio-professional groups who already benefited from the previous insurance coverage that had more favourable terms. These people were allowed to keep their own systems. Today, 95% of the population are covered by 3 main schemes. One for commerce and industry workers and their families, another for agricultural workers and lastly the national insurance fund for self-employed non-agricultural workers.\n\nAll working people are required to pay a portion of their income into a health insurance fund, which mutualizes the risk of illness and which reimburses medical expenses at varying rates. Children and spouses of insured individuals are eligible for benefits, as well. Each fund is free to manage its own budget and reimburse medical expenses at the rate it saw fit.\n\nThe government has two responsibilities in this system:\n\nToday, this system is more or less intact. All citizens and legal foreign residents of France are covered by one of these mandatory programs, which continue to be funded by worker participation. However, since 1945, a number of major changes have been introduced. Firstly, the different health care funds (there are five: General, Independent, Agricultural, Student, Public Servants) now all reimburse at the same rate. Secondly, since 2000, the government now provides health care to those who are not covered by a mandatory regime (those who have never worked and who are not students, meaning the very rich or the very poor). This regime, unlike the worker-financed ones, is financed via general taxation and reimburses at a higher rate than the profession-based system for those who cannot afford to make up the difference.\n\nFinally, to counter the rise in health care costs, the government has installed two plans (in 2004 and 2006), which require most people to declare a referring doctor in order to be fully reimbursed for specialist visits, and which installed a mandatory co-payment of €1 (about US$1.35) for a doctor visit (limited to 50 € annually), 0.50 € (about US$0.77) for each prescribed medicine (also limited to 50 € annually) and a fee of €16–18 ($20–25) per day for hospital stays (considered to be the \"hotel\" part of the hospital stay; that is, an amount people would pay anyway for food, etc.) and for expensive procedures. Such declaration is not required for children below 16 years old (because they already benefit from another protection program), for foreigners without residence in France (who will get benefits depending on existing international agreements between their own national health care program and the French Social Security), or those benefiting from a health care system of French overseas territories, and for those people that benefit from the minimum medical assistance.\n\nAn important element of the French insurance system is solidarity: the more ill a person becomes, the less they pay. This means that for people with serious or chronic illnesses (with vital risks, such as cancers, AIDS, or severe mental illness, where the person becomes very dependent of his medical assistance and protection) the insurance system reimburses them 100% of expenses and waives their co-payment charges.\n\nFinally, for fees that the mandatory system does not cover, there is a large range of private complementary insurance plans available. The market for these programs is very competitive. Such insurance is often subsidised by the employer, which means that premiums are usually modest. 85% of French people benefit from complementary private health insurance.\n\nA government body, ANAES, Agence Nationale d'Accréditation et d'Evaluation en Santé (The National Agency for Accreditation and Health Care Evaluation) was responsible for issuing recommendations and practice guidelines. There are recommendations on clinical practice (RPC), relating to the diagnosis, treatment and supervision of certain conditions, and in some cases, to the evaluation of reimbursement arrangements. ANAES also published practice guidelines which are recommendations on good practice that doctors are required to follow according to the terms of agreements signed between their professional representatives and the health insurance funds. There are also recommendations regarding drug prescriptions, and to a lesser extent, the prescription or provision of medical examination. By law, doctors must maintain their professional knowledge with ongoing professional education. ANAES was combined with other commissions in the High Authority of Health on 13 August 2004.\n\nAmbulatory care includes care by general practitioners who are largely self-employed and mostly work alone, although about a third of all GPs work in a group practice. GPs do not exercise gatekeeper functions in the French medical system and people can see any registered medical practitioner of choice including specialists. Thus ambulatory care can take place in many settings.\n\nThe French healthcare system was named by the World Health Organization in 2008 as the best performing system in the world in terms of availability and organization of health care providers . It is a universal health care system. It features a mix of public and private services, relatively high expenditure, high patient success rates and low mortality rates, and high consumer satisfaction. Its aims are to combine low cost with flexibility of patient choice as well as doctors' autonomy. While 99.9% of the French population is covered, the rising cost of the system has been a source of concern, as has the lack of emergency service in some areas. In 2004, the system underwent a number of reforms, including introduction of the \"Carte Vitale\" smart card system, improved treatment of patients with rare diseases, and efforts aimed at reducing medical fraud. While private medical care exists in France, the 75% of doctors who are in the national program provide care free to the patient, with costs being reimbursed from government funds. Like most countries, France faces problems of rising costs of prescription medication, increasing unemployment, and a large aging population.\n\nExpenses related to the healthcare system in France represented 10.5% of the country's GDP and 15.4% of its public expenditures. In 2004, 78.4% of these expenses were paid for by the state. By 2015 the cost had risen to 11.5% of GDP - the third highest in Europe.\n\nIn a sample of 13 developed countries France was first in its population weighted usage of medication in 14 classes in both 2009 and 2013. The drugs studied were selected on the basis that the conditions treated had high incidence, prevalence and/or mortality, caused significant long-term morbidity and incurred high levels of expenditure and significant developments in prevention or treatment had been made in the last 10 years. The study noted considerable difficulties in cross border comparison of medication use.\n\nAbout 62 percent of French hospital capacity is met by publicly owned and managed hospitals. The remaining capacity is split evenly (18% each) between non-profit sector hospitals (which are linked to the public sector and which tend to be owned by foundations, religious organizations or mutual-insurance associations) and by for-profit institutions.\n\nWhile French doctors only earn about 60% of what American doctors make, their expenses are reduced because they pay no tuition for medical school (cost for a year range from €200 to 500 but students get paid during their internships in hospitals) and malpractice insurance is less costly compared with the United States (as all doctors subscribe to the same fund). Low medical malpractice insurance may also be the byproduct of past litigations often favoring the medical practitioners. This started to change due to the implementation of the Patients' Rights Law of 2002. The French National Insurance system also pays for a part of social security taxes owed by doctors that agree to charge the government-approved fees. The number of French doctors has recently declined. Reasons for this may be because they prefer to specialize and get jobs at hospitals rather than setting up General Practices. The workload for general practice doctors requires more hours and responsibility than workplace and supply doctors. \n\nHistorian Dannielle Horan claims that while many in the US deride the French system as \"socialized medicine\", the French do not consider their mixed public and private system \"socialized\" and the population tends to look down upon British- and Canadian-style socialized medicine.\n\nAccording to the Euro health consumer index the French healthcare system has a tendency to \"medicalize a lot of conditions, and to give patients a lot of drugs\".\n\nSiciliani and Hurst did a major comparison of countries reporting long waits for health care and countries that did not. In a comparison of health care funding, institutions and level of resources between countries, prevention of long waiting lists in France was attributed to a high number of doctors and hospital beds, combined with fee-for-service funding of doctors and private hospitals.\n\nIn France, many specialists treat patients outside hospitals; these ambulatory specialists are paid fee-for-service. Private hospitals were also paid by diem daily rates and fee-for-service in 2003, and provided much of total surgery. Fee-for-service rather than limited budgets, with access for patients with public health insurance helped prevent long waits for surgery (Siciliani and Hurst, 2003, pp. 69–70). (Now, public, private nonprofit hospitals and for-profit hospitals are all paid by a DRG system (source needed),\n\nHowever, assertions that France does not have waiting lists at all are not true. Long waits apparently remain unusual. However, some moderate waits have developed. French patients were relatively unlikely to report forgoing care because of waits (Eurostat, 2012). However, there are wait times for some procedures such as MRI scans, perhaps relating to low numbers of scanners, and in certain areas for certain specialties like ophthalmology, partly relating to unequal distributions of doctors (Chevreul et al., 2015, p. 182). \n\nThe Commonwealth Fund 2010 Health Policy Survey in 11 Countries reported found that a relatively high percentage of French patients reported waiting more than four weeks to see their most recent specialist appointment in France (higher than New Zealand, the U.K and Australia). This percentage held relatively constant over time, showing that waiting lists in France for appointments and elective surgery are not a new phenomenon. Fifty three percent of specialist appointments took less than 1 month (relatively low), and 28% more than two months. However, while moderate waits for elective surgery were common (only 46% said they had waited less than one month) the percentage reporting four-month-plus waits was only 7%, low and similar to the U.S., Switzerland, and the Netherlands. So, it appears that extremely long waits (like those in the U.K.'s NHS in the 1990s) are still rare. \n\nThis study has limitations. The number of people surveyed may not have been perfectly representative, although the figures held similar over time. The study also did not state the percentage of total appointments taking this long (whether a patient's appointments after the initial appointment were more timely or not), although the most recent appointment would presumably reflect both initial and subsequent appointments), or the total number of appointments available. The waits were self-reported, rather than collected from statistics; this may also lead the data to be not completely representative. \n\nIn terms of health care supply, France has far more doctors per capita than the U.K., Australia, New Zealand, and the U.S. . This suggests that while French patients in some cases have similar to current waiting times to the first 3 countries, the number of patients who receive appointments and treatment is significantly higher than in the U.K., Australia and New Zealand (whose global budgets for hospitals also likely capped the supply at lower levels). It is also relevant that while American, Swiss and German patients generally reported short waits, a significant minority of American patients reported waiting longer than 4 weeks for a specialist appointment (about 20%), and longer than 1 month for elective surgery (30%) . Thus, while waiting times in the U.S. are usually short, a higher percentage waits in the U.S. are longer than generally assumed. One study reported longer waiting times for uninsured American patients, who may face a disproportionate number of longer waiting times (citation needed)(founder Alejandro Castillo) .\n\n"}
{"id": "16540517", "url": "https://en.wikipedia.org/wiki?curid=16540517", "title": "Health in Paraguay", "text": "Health in Paraguay\n\nIn terms of major health indicators, health in Paraguay ranks near the median among South American countries. In 2003 Paraguay had a child mortality rate of 29.5 deaths per 1,000 children, ranking it behind Argentina, Colombia, and Uruguay but ahead of Brazil and Bolivia. The health of Paraguayans living outside urban areas is generally worse than those residing in cities. Many preventable diseases, such as Chagas' disease, run rampant in rural regions. Parasitic and respiratory diseases, which could be controlled with proper medical treatment, drag down Paraguay's overall health. In general, malnutrition, lack of proper health care, and poor sanitation are the root of many health problems in Paraguay.\n\nHealth care funding from the national government increased gradually throughout the 1980s and 1990s. Spending on health care rose to 1.7 percent of the gross domestic product (GDP) in 2000, nearly triple the 0.6 percent of GDP spent in 1989. But during the past decade, improvement in health care has slowed. Paraguay spends less per capita (US$13−20 per year) than most other Latin American countries. A 2001 survey indicated that 27 percent of the population still had no access to medical care, public or private. Private health insurance is very limited, with pre-paid plans making up only 11 percent of private expenditures on health care. Thus, most of the money spent on private health care (about 88 percent) is on a fee-for-service basis, effectively preventing the poor population from seeing private doctors. According to recent estimates, Paraguay has about 117 physicians and 20 nurses per 100,000 population.\n\nIn 2003 the prevalence rate of human immunodeficiency virus/acquired immune deficiency syndrome (HIV/AIDS) in Paraguay was estimated at 0.5 percent of the population, and officials reported 600 deaths from AIDS. The United Nations cautions that although the prevalence rate in Paraguay remains low, HIV/AIDS is increasing among stigmatized population groups. Transmission of the virus is primarily through sexual contact. According to 2004 estimates, nearly 15,000 Paraguayans were infected with HIV/AIDS.\n\n\n"}
{"id": "43531029", "url": "https://en.wikipedia.org/wiki?curid=43531029", "title": "Hemocue", "text": "Hemocue\n\nCo-founded in 1988 by Mr. Jan Lilja, HemoCue AB is a Swedish company, which develops, produces and markets medical diagnostic products like hemoglobin, glucose for point-of-care testing markets worldwide.\n\n"}
{"id": "2340491", "url": "https://en.wikipedia.org/wiki?curid=2340491", "title": "History of HIV/AIDS", "text": "History of HIV/AIDS\n\nAIDS is caused by a human immunodeficiency virus (HIV), which originated in non-human primates in Central and West Africa. While various sub-groups of the virus acquired human infectivity at different times, the global pandemic had its origins in the emergence of one specific strain – HIV-1 subgroup M – in Léopoldville in the Belgian Congo (now Kinshasa in the Democratic Republic of the Congo) in the 1920s.\n\nThere are two types of HIV: HIV-1 and HIV-2. \n\nHIV-1 is more virulent, is more easily transmitted and is the cause of the vast majority of HIV infections globally. The pandemic strain of HIV-1 is closely related to a virus found in chimpanzees of the subspecies \"Pan troglodytes troglodytes\", which live in the forests of the Central African nations of Cameroon, Equatorial Guinea, Gabon, Republic of Congo (or Congo-Brazzaville), and Central African Republic. \n\nHIV-2 is less transmittable and is largely confined to West Africa, along with its closest relative, a virus of the sooty mangabey (\"Cercocebus atys atys\"), an Old World monkey inhabiting southern Senegal, Guinea-Bissau, Guinea, Sierra Leone, Liberia, and western Ivory Coast.\nThe majority of HIV researchers agree that HIV evolved at some point from the closely related simian immunodeficiency virus (SIV), and that SIV or HIV (post mutation) was transferred from non-human primates to humans in the recent past (as a type of zoonosis). Research in this area is conducted using molecular phylogenetics, comparing viral genomic sequences to determine relatedness.\n\nScientists generally accept that the known strains (or groups) of HIV-1 are most closely related to the simian immunodeficiency viruses (SIVs) endemic in wild ape populations of West Central African forests. In particular, each of the known HIV-1 strains is either closely related to the SIV that infects the chimpanzee subspecies \"Pan troglodytes troglodytes\" (SIVcpz) or closely related to the SIV that infects western lowland gorillas (\"Gorilla gorilla gorilla\"), called SIVgor. The pandemic HIV-1 strain (group M or Main) and a rare strain found only in a few Cameroonian people (group N) are clearly derived from SIVcpz strains endemic in \"Pan troglodytes troglodytes\" chimpanzee populations living in Cameroon. Another very rare HIV-1 strain (group P) is clearly derived from SIVgor strains of Cameroon. Finally, the primate ancestor of HIV-1 group O, a strain infecting 100,000 people mostly from Cameroon but also from neighbouring countries, was confirmed in 2006, to be SIVgor. The pandemic HIV-1 group M is most closely related to the SIVcpz collected from the southeastern rain forests of Cameroon (modern East Province) near the Sangha River. Thus, this region is presumably where the virus was first transmitted from chimpanzees to humans. However, reviews of the epidemiological evidence of early HIV-1 infection in stored blood samples, and of old cases of AIDS in Central Africa, have led many scientists to believe that HIV-1 group M early human centre was probably not in Cameroon, but rather further south in the Democratic Republic of the Congo, more probably in its capital city, Kinshasa (formerly Léopoldville).\n\nUsing HIV-1 sequences preserved in human biological samples along with estimates of viral mutation rates, scientists calculate that the jump from chimpanzee to human probably happened during the late 19th or early 20th century, a time of rapid urbanisation and colonisation in equatorial Africa. Exactly when the zoonosis occurred is not known. Some molecular dating studies suggest that HIV-1 group M had its most recent common ancestor (MRCA) (that is, started to spread in the human population) in the early 20th century, probably between 1915 and 1941. A study published in 2008, analyzing viral sequences recovered from a biopsy made in Kinshasa, in 1960, along with previously known sequences, suggested a common ancestor between 1873 and 1933 (with central estimates varying between 1902 and 1921). Genetic recombination had earlier been thought to \"seriously confound\" such phylogenetic analysis, but later \"work has suggested that recombination is not likely to systematically bias [results]\", although recombination is \"expected to increase variance\". The results of a 2008 phylogenetics study support the later work and indicate that HIV evolves \"fairly reliably\". Further research was hindered due to the primates being critically endangered. Sample analyses resulted in little data due to the rarity of experimental material. The researchers, however, were able to hypothesize a phylogeny from the gathered data. They were also able to use the molecular clock of a specific strain of HIV to determine the initial date of transmission, which is estimated to be around 1915–1931.\n\nSimilar research has been undertaken with SIV strains collected from several wild sooty mangabey (\"Cercocebus atys atys\") (SIVsmm) populations of the West African nations of Sierra Leone, Liberia, and Ivory Coast. The resulting phylogenetic analyses show that the viruses most closely related to the two strains of HIV-2 that spread considerably in humans (HIV-2 groups A and B) are the SIVsmm found in the sooty mangabeys of the Tai forest, in western Ivory Coast.\n\nThere are six additional known HIV-2 groups, each having been found in just one person. They all seem to derive from independent transmissions from sooty mangabeys to humans. Groups C and D have been found in two people from Liberia, groups E and F have been discovered in two people from Sierra Leone, and groups G and H have been detected in two people from the Ivory Coast. These HIV-2 strains are probably dead-end infections, and each of them is most closely related to SIVsmm strains from sooty mangabeys living in the same country where the human infection was found.\n\nMolecular dating studies suggest that both the epidemic groups (A and B) started to spread among humans between 1905 and 1961 (with the central estimates varying between 1932 and 1945).\n\nAccording to the natural transfer theory (also called \"hunter theory\" or \"bushmeat theory\"), the \"simplest and most plausible explanation for the cross-species transmission\" of SIV or HIV (post mutation), the virus was transmitted from an ape or monkey to a human when a hunter or bushmeat vendor/handler was bitten or cut while hunting or butchering the animal. The resulting exposure to blood or other bodily fluids of the animal can result in SIV infection. Prior to WWII, some Sub-Saharan Africans were forced out of the rural areas because of the European demand for resources. Since rural Africans were not keen to pursue agricultural practices in the jungle, they turned to non-domesticated meat as their primary source of protein. This over-exposure to bushmeat and malpractice of butchery increased blood-to-blood contact, which then increased the probability of transmission. A recent serological survey showed that human infections by SIV are not rare in Central Africa: the percentage of people showing seroreactivity to antigens—evidence of current or past SIV infection—was 2.3% among the general population of Cameroon, 7.8% in villages where bushmeat is hunted or used, and 17.1% in the most exposed people of these villages. How the SIV virus would have transformed into HIV after infection of the hunter or bushmeat handler from the ape/monkey is still a matter of debate, although natural selection would favour any viruses capable of adjusting so that they could infect and reproduce in the T cells of a human host.\n\nHIV-1C, a subtype of HIV, was theorized to have its origins circulating in South America. The consumption of bushmeat is also the most probable cause for the emergence of HIV-1C in South America. However, the types of apes shown to carry the SIV virus are different in South America. The primary point of entry, according to researchers, is somewhere in the jungles of Brazil. An SIV strain, closely related to HIV, was interspersed within a certain clade of primates. This suggests that the zoonotic transmission of the virus may have happened in this area. Continual emigration between countries escalated the transmission of the virus. Other scientists believe that the HIV-1C strain circulated in South America at around the same time that the HIV-1C strain was introduced in Africa. Little research has been done on this theory because it is fairly young.\n\nThe discovery of the main HIV / SIV phylogenetic relationships permits explaining \"broad\" HIV biogeography: the early centres of the HIV-1 groups were in Central Africa, where the primate reservoirs of the related SIVcpz and SIVgor viruses (chimpanzees and gorillas) exist; similarly, the HIV-2 groups had their centres in West Africa, where sooty mangabeys, which harbour the related SIVsmm virus, exist. However, these relationships do not explain more detailed patterns of biogeography, such as why epidemic HIV-2 groups (A and B) only evolved in the Ivory Coast, which is one of only six countries harbouring the sooty mangabey. It is also unclear why the SIVcpz endemic in the chimpanzee subspecies \"Pan troglodytes schweinfurthii\" (inhabiting the Democratic Republic of Congo, Central African Republic, Rwanda, Burundi, Uganda, and Tanzania) did not spawn an epidemic HIV-1 strain to humans, while the Democratic Republic of Congo was the main centre of HIV-1 group M, a virus descended from SIVcpz strains of a subspecies (\"Pan troglodytes troglodytes\") that does not exist in this country. It is clear that the several HIV-1 and HIV-2 strains descend from SIVcpz, SIVgor, and SIVsmm viruses, and that bushmeat practice provides the most plausible venue for cross-species transfer to humans. However, some loose ends remain.\n\nIt is not yet explained why only four HIV groups (HIV-1 groups M and O, and HIV-2 groups A and B) spread considerably in human populations, despite bushmeat practices being widespread in Central and West Africa, and the resulting human SIV infections being common.\n\nIt also remains unexplained why all epidemic HIV groups emerged in humans nearly simultaneously, and only in the 20th century, despite very old human exposure to SIV (a recent phylogenetic study demonstrated that SIV is at least tens of thousands of years old).\n\nSeveral of the theories of HIV origin accept the established knowledge of the HIV/SIV phylogenetic relationships, and also accept that bushmeat practice was the most likely cause of the initial transfer to humans. All of them propose that the simultaneous epidemic emergences of four HIV groups in the late 19th-early 20th century, and the lack of previous known emergences, are explained by new factor(s) that appeared in the relevant African regions in that timeframe. These new factor(s) would have acted either to increase human exposures to SIV, to help it to adapt to the human organism by mutation (thus enhancing its between-humans transmissibility), or to cause an initial burst of transmissions crossing an epidemiological threshold, and therefore increasing the probability of continued spread.\n\nGenetic studies of the virus suggested in 2008 that the most recent common ancestor of the HIV-1 M group dates back to the Belgian Congo city of Léopoldville (modern Kinshasa), circa 1910. Proponents of this dating link the HIV epidemic with the emergence of colonialism and growth of large colonial African cities, leading to social changes, including a higher degree of non-monogamous sexual activity, the spread of prostitution, and the concomitant high frequency of genital ulcer diseases (such as syphilis) in nascent colonial cities.\n\nIn 2014, a study conducted by scientists from the University of Oxford and the University of Leuven, in Belgium, revealed that because approximately one million people every year would flow through the prominent city of Kinshasa, which served as the origin of the first known HIV cases in the 1920s, passengers riding on the region's Belgian railway trains were able to spread the virus to larger areas. The study also attributed a roaring sex trade, rapid population growth and unsterilised needles used in health clinics as other factors which contributed to the emergence of the Africa HIV epidemic.\n\nBeatrice Hahn, Paul M. Sharp, and their colleagues proposed that \"[the epidemic emergence of HIV] most likely reflects changes in population structure and behaviour in Africa during the 20th century and perhaps medical interventions that provided the opportunity for rapid human-to-human spread of the virus\". After the Scramble for Africa started in the 1880s, European colonial powers established cities, towns, and other colonial stations. A largely masculine labor force was hastily recruited to work in fluvial and sea ports, railways, other infrastructures, and in plantations. This disrupted traditional tribal values and favored casual sexual activity with an increased number of partners. In the nascent cities women felt relatively liberated from rural tribal rules and many remained unmarried or divorced during long periods, this being rare in African traditional societies. This was accompanied by unprecedented increase in people's movements.\n\nMichael Worobey and colleagues observed that the growth of cities probably played a role in the epidemic emergence of HIV, since the phylogenetic dating of the two older strains of HIV-1 (groups M and O), suggest that these viruses started to spread soon after the main Central African colonial cities were founded.\n\nAmit Chitnis, Diana Rawls, and Jim Moore proposed that HIV may have emerged epidemically as a result of the harsh conditions, forced labor, displacement, and unsafe injection and vaccination practices associated with colonialism, particularly in French Equatorial Africa. The workers in plantations, construction projects, and other colonial enterprises were supplied with bushmeat, which would have contributed to an increase in hunting and, it follows, a higher incidence of human exposure to SIV. Several historical sources support the view that bushmeat hunting indeed increased, both because of the necessity to supply workers and because firearms became more widely available.\n\nThe colonial authorities also gave many vaccinations against smallpox, and injections, of which many would be made without sterilising the equipment between uses (unsafe or unsterile injections). Chitnis \"et al.\" proposed that both these parenteral risks and the prostitution associated with forced labor camps could have caused serial transmission (or serial passage) of SIV between humans (see discussion of this in the next section). In addition, they proposed that the conditions of extreme stress associated with forced labor could depress the immune system of workers, therefore prolonging the primary acute infection period of someone newly infected by SIV, thus increasing the odds of both adaptation of the virus to humans, and of further transmissions.\n\nThe authors proposed that HIV-1 originated in the area of French Equatorial Africa in the early 20th century (when the colonial abuses and forced labor were at their peak). Later research established these theories were mostly correct: HIV-1 groups M and O started to spread in humans in late 19th–early 20th century. In addition, all groups of HIV-1 descend from either SIVcpz or SIVgor from apes living to the west of the Ubangi River, either in countries that belonged to the French Equatorial Africa federation of colonies, in Equatorial Guinea (then a Spanish colony), or in Cameroon (which was a German colony between 1884 and 1916, and then fell to Allied forces in World War I, and had most of its area administered by France, in close association with French Equatorial Africa).\n\nThis theory was later dubbed \"Heart of Darkness\" by Jim Moore, alluding to the book of the same title written by Joseph Conrad, the main focus of which is colonial abuses in equatorial Africa.\n\nIn several articles published since 2001, Preston Marx, Philip Alcabes, and Ernest Drucker proposed that HIV emerged because of rapid serial human-to-human transmission of SIV (after a bushmeat hunter or handler became SIV-infected) through unsafe or unsterile injections. Although both Chitnis \"et al.\" and Sharp \"et al.\" also suggested that this may have been one of the major risk factors at play in HIV emergence (see above), Marx \"et al.\" enunciated the underlying mechanisms in greater detail, and wrote the first review of the injection campaigns made in colonial Africa.\n\nCentral to Marx \"et al.\" argument is the concept of adaptation by serial passage (or serial transmission): an adventitious virus (or other pathogen) can increase its biological adaptation to a new host species if it is rapidly transmitted between hosts, while each host is still in the acute infection period. This process favors the accumulation of adaptive mutations more rapidly, therefore increasing the odds that a better adapted viral variant will appear in the host before the immune system suppresses the virus. Such better adapted variant could then survive in the human host for longer than the short acute infection period, in high numbers (high viral load), which would grant it more possibilities of epidemic spread.\n\nMarx \"et al.\" reported experiments of cross-species transfer of SIV in captive monkeys (some of which made by themselves), in which the use of serial passage helped to adapt SIV to the new monkey species after passage by three or four animals.\n\nIn agreement with this model is also the fact that, while both HIV-1 and HIV-2 attain substantial viral loads in the human organism, adventitious SIV infecting humans seldom does so: people with SIV antibodies often have very low or even undetectable SIV viral load. This suggests that both HIV-1 and HIV-2 are adapted to humans, and serial passage could have been the process responsible for it.\n\nMarx \"et al.\" proposed that unsterile injections (that is, injections where the needle or syringe is reused without sterilization or cleaning between uses), which were likely very prevalent in Africa, during both the colonial period and afterwards, provided the mechanism of serial passage that permitted HIV to adapt to humans, therefore explaining why it emerged epidemically only in the 20th century.\n\nMarx \"et al.\" emphasize the massive number of injections administered in Africa after antibiotics were introduced (around 1950) as being the most likely implicated in the origin of HIV because, by these times (roughly in the period 1950 to 1970), injection intensity in Africa was maximal. They argued that a serial passage chain of 3 or 4 transmissions between humans is an unlikely event (the probability of transmission after a needle reuse is something between 0.3% and 2%, and only a few people have an acute SIV infection at any time), and so HIV emergence may have required the very high frequency of injections of the antibiotic era.\n\nThe molecular dating studies place the initial spread of the epidemic HIV groups before that time (see above). According to Marx \"et al.\", these studies could have overestimated the age of the HIV groups, because they depend on a molecular clock assumption, may not have accounted for the effects of natural selection in the viruses, and the serial passage process alone would be associated with strong natural selection.\n\nDavid Gisselquist proposed that the mass injection campaigns to treat trypanosomiasis (sleeping sickness) in Central Africa were responsible for the emergence of HIV-1. Unlike Marx \"et al.\", Gisselquist argued that the millions of unsafe injections administered during these campaigns were sufficient to spread rare HIV infections into an epidemic, and that evolution of HIV through serial passage was not essential to the emergence of the HIV epidemic in the 20th century.\n\nThis theory focuses on injection campaigns that peaked in the period 1910–40, that is, around the time the HIV-1 groups started to spread. It also focuses on the fact that many of the injections in these campaigns were intravenous (which are more likely to transmit SIV/HIV than subcutaneous or intramuscular injections), and many of the patients received many (often more than 10) injections per year, therefore increasing the odds of SIV serial passage.\n\nJacques Pépin and Annie-Claude Labbé reviewed the colonial health reports of Cameroon and French Equatorial Africa for the period 1921–59, calculating the incidences of the diseases requiring intravenous injections. They concluded that trypanosomiasis, leprosy, yaws, and syphilis were responsible for most intravenous injections. Schistosomiasis, tuberculosis, and vaccinations against smallpox represented lower parenteral risks: schistosomiasis cases were relatively few; tuberculosis patients only became numerous after mid-century; and there were few smallpox vaccinations in the lifetime of each person.\n\nThe authors suggested that the very high prevalence of the Hepatitis C virus in southern Cameroon and forested areas of French Equatorial Africa (around 40–50%) can be better explained by the unsterile injections used to treat yaws, because this disease was much more prevalent than syphilis, trypanosomiasis, and leprosy in these areas. They suggested that all these parenteral risks caused not only the massive spread of Hepatitis C but also the spread of other pathogens, and the emergence of HIV-1: \"the same procedures could have exponentially amplified HIV-1, from a single hunter/cook occupationally infected with SIVcpz to several thousand patients treated with arsenicals or other drugs, a threshold beyond which sexual transmission could prosper.\" They do not suggest specifically serial passage as the mechanism of adaptation.\n\nAccording to Pépin's 2011 book, \"The Origins of AIDS\", the virus can be traced to a central African bush hunter in 1921, with colonial medical campaigns using improperly sterilized syringe and needles playing a key role in enabling a future epidemic. Pépin concludes that AIDS spread silently in Africa for decades, fueled by urbanization and prostitution since the initial cross-species infection. Pépin also claims that the virus was brought to the Americas by a Haitian teacher returning home from Zaire in the 1960s. \nSex tourism and contaminated blood transfusion centers ultimately propelled AIDS to public consciousness in the 1980s and a worldwide pandemic.\n\nJoão Dinis de Sousa, Viktor Müller, Philippe Lemey, and Anne-Mieke Vandamme proposed that HIV became epidemic through sexual serial transmission, in nascent colonial cities, helped by a high frequency of genital ulcers, caused by genital ulcer diseases (GUD). GUD are simply sexually transmitted diseases that cause genital ulcers; examples are syphilis, chancroid, lymphogranuloma venereum, and genital herpes. These diseases increase the probability of HIV transmission dramatically, from around 0.01–0.1% to 4–43% per heterosexual act, because the genital ulcers provide a portal of viral entry, and contain many activated T cells expressing the CCR5 co-receptor, the main cell targets of HIV.\n\nSousa \"et al.\" use molecular dating techniques to estimate the time when each HIV group split from its closest SIV lineage. Each HIV group necessarily crossed to humans between this time and the time when it started to spread (the time of the MRCA), because after the MRCA certainly all lineages were already in humans, and before the split with the closest simian strain, the lineage was in a simian. HIV-1 groups M and O split from their closest SIVs around 1931 and 1915, respectively. This information, together with the datations of the HIV groups' MRCAs, mean that all HIV groups likely crossed to humans in the early 20th century.\n\nThe authors reviewed colonial medical articles and archived medical reports of the countries at or near the ranges of chimpanzees, gorillas and sooty mangabeys, and found that genital ulcer diseases peaked in the colonial cities during their early growth period (up to 1935). The colonial authorities recruited men to work in railways, fluvial and sea ports, and other infrastructure projects, and most of these men did not bring their wives with them. Then, the highly male-biased sex ratio favoured prostitution, which in its turn caused an explosion of GUD (especially syphilis and chancroid). After the mid-1930s, people's movements were more tightly controlled, and mass surveys and treatments (of arsenicals and other drugs) were organized, and so the GUD incidences started to decline. They declined even further after World War II, because of the heavy use of antibiotics, so that, by the late 1950s, Léopoldville (which is the probable center of HIV-1 group M) had a very low GUD incidence. Similar processes happened in the cities of Cameroon and Ivory Coast, where HIV-1 group O and HIV-2 respectively evolved.\n\nTherefore, the peak GUD incidences in cities have a good temporal coincidence with the period when all main HIV groups crossed to humans and started to spread. In addition, the authors gathered evidence that syphilis and the other GUDs were, like injections, absent from the densely forested areas of Central and West Africa before organized colonialism socially disrupted these areas (starting in the 1880s). Thus, this theory also potentially explains why HIV emerged only after the late 19th century.\n\nUli Linke has argued that the practice of female genital mutilation (i.e., clitoridectomy and/or infibulation) is responsible for the high incidence of AIDS in Africa, since intercourse with a female who has undergone clitoridectomy is conducive to exchange of blood.\n\nMale circumcision may reduce the probability of HIV acquisition by men. Leaving aside blood transfusions, the highest HIV-1 transmissibility ever measured was from GUD-suffering female prostitutes to uncircumcised men—the measured risk was 43% in a single sexual act. Sousa \"et al.\" reasoned that the adaptation and epidemic emergence of each HIV group may have required such extreme conditions, and thus reviewed the existing ethnographic literature for patterns of male circumcision and hunting of apes and monkeys for bushmeat, focusing on the period 1880–1960, and on most of the 318 ethnic groups living in Central and West Africa. They also collected censuses and other literature showing the ethnic composition of colonial cities in this period. Then, they estimated the circumcision frequencies of the Central African cities over time.\n\nSousa \"et al.\" charts reveal that male circumcision frequencies were much lower in several cities of western and central Africa in the early 20th century than they are currently. The reason is that many ethnic groups not performing circumcision by that time gradually adopted it, to imitate other ethnic groups and enhance the social acceptance of their boys (colonialism produced massive intermixing between African ethnic groups). About 15–30% of men in Léopoldville and Douala in the early 20th century should be uncircumcised, and these cities were the probable centers of HIV-1 groups M and O, respectively.\n\nThe authors studied early circumcision frequencies in 12 cities of Central and West Africa, to test if this variable correlated with HIV emergence. This correlation was strong for HIV-2: among 6 West African cities that could have received immigrants infected with SIVsmm, the two cities from the Ivory Coast studied (Abidjan and Bouaké) had much higher frequency of uncircumcised men (60–85%) than the others, and epidemic HIV-2 groups emerged initially in this country only. This correlation was less clear for HIV-1 in Central Africa.\n\nSousa \"et al.\" then built computer simulations to test if an 'ill-adapted SIV' (meaning a simian immunodeficiency virus already infecting a human but incapable of transmission beyond the short acute infection period) could spread in colonial cities. The simulations used parameters of sexual transmission obtained from the current HIV literature. They modelled people's 'sexual links', with different levels of sexual partner change among different categories of people (prostitutes, single women with several partners a year, married women, and men), according to data obtained from modern studies of sexual activity in African cities. The simulations let the parameters (city size, proportion of people married, GUD frequency, male circumcision frequency, and transmission parameters) vary, and explored several scenarios. Each scenario was run 1,000 times, to test the probability of SIV generating long chains of sexual transmission. The authors postulated that such long chains of sexual transmission were necessary for the SIV strain to adapt better to humans, becoming an HIV capable of further epidemic emergence.\n\nThe main result was that genital ulcer frequency was by far the most decisive factor. For the GUD levels prevailing in Léopoldville in the early 20th century, long chains of SIV transmission had a high probability. For the lower GUD levels existing in the same city in the late 1950s (see above), they were much less likely. And without GUD (a situation typical of villages in forested equatorial Africa before colonialism) SIV could not spread at all. City size was not an important factor. The authors propose that these findings explain the temporal patterns of HIV emergence: no HIV emerging in tens of thousands of years of human slaughtering of apes and monkeys, several HIV groups emerging in the nascent, GUD-riddled, colonial cities, and no epidemically successful HIV group emerging in mid-20th century, when GUD was more controlled, and cities were much bigger.\n\nMale circumcision had little to moderate effect in their simulations, but, given the geographical correlation found, the authors propose that it could have had an indirect role, either by increasing genital ulcer disease itself (it is known that syphilis, chancroid, and several other GUDs have higher incidences in uncircumcised men), or by permitting further spread of the HIV strain, after the first chains of sexual transmission permitted adaptation to the human organism.\n\nOne of the main advantages of this theory is stressed by the authors: \"It [the theory] also offers a conceptual simplicity because it proposes as causal factors for SIV adaptation to humans and initial spread the very same factors that most promote the continued spread of HIV nowadays: promiscuous [sic] sex, particularly involving sex workers, GUD, and possibly lack of circumcision.\"\n\nIatrogenic theories propose that medical interventions were responsible for HIV origins. By proposing factors that only appeared in Central and West Africa after the late 19th century, they seek to explain why all HIV groups also started after that.\n\nThe theories centred on the role of parenteral risks, such as unsterile injections, transfusions, or smallpox vaccinations are accepted as plausible by most scientists of the field.\n\nDiscredited HIV/AIDS origins theories include several iatrogenic theories, such as the polio vaccine hypothesis which argues that the early oral polio vaccines were contaminated with a chimpanzee virus, leading to the Central African outbreak.\n\nIn most non-human primate species, natural SIV infection does not cause a fatal disease (but see below). Comparison of the gene sequence of SIV with HIV should, therefore, give us information about the factors necessary to cause disease in humans. The factors that determine the virulence of HIV as compared to most SIVs are only now being elucidated. Non-human SIVs contain a \"nef\" gene that down-regulates CD3, CD4, and MHC class I expression; most non-human SIVs, therefore, do not induce immunodeficiency; the HIV-1 \"nef\" gene, however, has lost its ability to down-regulate CD3, which results in the immune activation and apoptosis that is characteristic of chronic HIV infection.\n\nIn addition, a long-term survey of chimpanzees naturally infected with SIVcpz in Gombe, Tanzania found that, contrary to the previous paradigm, chimpanzees with SIVcpz infection do experience an increased mortality, and also suffer from a Human AIDS-like illness. SIV pathogenicity in wild animals could exist in other chimpanzee subspecies and other primate species as well, and stay unrecognized by lack of relevant long term studies.\n\nDavid Carr was an apprentice printer (usually mistakenly referred to as a sailor; Carr had served in the Navy between 1955 and 1957) from Manchester, England who died August 31, 1959, and was for some time mistakenly reported to have died from AIDS-defining opportunistic infections (ADOIs). Following the failure of his immune system, he succumbed to pneumonia. Doctors, baffled by what he had died from, preserved 50 of his tissue samples for inspection. In 1990, the tissues were found to be HIV-positive. However, in 1992, a second test by AIDS researcher David Ho found that the strain of HIV present in the tissues was similar to those found in 1990 rather than an earlier strain (which would have mutated considerably over the course of 30 years). He concluded that the DNA samples provided actually came from a 1990 AIDS patient. Upon retesting David Carr's tissues, he found no sign of the virus.\n\nOne of the earliest documented HIV-1 infections was discovered in a preserved blood sample taken in 1959 from a man from Léopoldville in the Belgian Congo. However, it is unknown whether this anonymous person ever developed AIDS and died of its complications.\nA second early documented HIV-1 infection was discovered in a preserved lymph node biopsy sample taken in 1960 from a woman from Léopoldville, Belgian Congo.\n\nIn May 1969 16-year-old African-American Robert Rayford died at the St. Louis City Hospital from Kaposi's sarcoma. In 1987 researchers at Tulane University School of Medicine detected \"a virus closely related or identical to\" HIV-1 in his preserved blood and tissues. The doctors who worked on his case at the time suspected he was a prostitute or the victim of sexual abuse, though the patient did not discuss his sexual history with them in detail.\n\nIn 1975 and 1976, a Norwegian sailor, with the alias name Arvid Noe, his wife, and his seven-year-old daughter died of AIDS. The sailor had first presented symptoms in 1969, eight years after he first spent time in ports along the West African coastline. A gonorrhea infection during his first African voyage shows he was sexually active at this time. Tissue samples from the sailor and his wife were tested in 1988 and found to contain HIV-1 (Group O).\n\nFrom 1972 to 1973, researchers drew blood from 75 children in Uganda to serve as controls for a study of Burkitt's lymphoma. In 1985, retroactive testing of the frozen blood serum indicated that antibodies to a virus related to HIV were present in 50 of the children.\n\nHIV-1 strains were once thought to have arrived in New York City from Haiti around 1971. It spread from New York City to San Francisco around 1976.\n\nHIV-1 is believed to have arrived in Haiti from central Africa, possibly from the Democratic Republic of the Congo around 1967. The current consensus is that HIV was introduced to Haiti by an unknown individual or individuals who contracted it while working in the Democratic Republic of the Congo circa 1966. A mini-epidemic followed, and circa 1969, yet another unknown individual took HIV from Haiti to the United States. The vast majority of cases of AIDS outside sub-Saharan Africa can be traced back to that single patient (although numerous unrelated incidents of AIDS among Haitian immigrants to the U.S. were recorded in the early 1980s, and as evidenced by the case of Robert Rayford, isolated incidents of this infection may have been occurring as early as 1966). The virus eventually entered male gay communities in large United States cities, where a combination of casual, multi-partner sexual activity (with individuals reportedly averaging over 11 unprotected sexual partners per year) and relatively high transmission rates associated with anal intercourse allowed it to spread explosively enough to finally be noticed.\n\nBecause of the long incubation period of HIV (up to a decade or longer) before symptoms of AIDS appear, and because of the initially low incidence, HIV was not noticed at first. By the time the first reported cases of AIDS were found in large United States cities, the prevalence of HIV infection in some communities had passed 5%. Worldwide, HIV infection has spread from urban to rural areas, and has appeared in regions such as China and India.\n\nA Canadian airline steward named Gaëtan Dugas was referred to as \"Case 057\" and later \"Patient O\" with the alphabet letter \"O\" standing for \"outside Southern California\", in an early AIDS study by Dr. William Darrow of the Centers for Disease Control. Because of this, many people had considered Dugas to be responsible for taking HIV to North America. However, HIV reached New York City around 1971 while Dugas did not start work at Air Canada until 1974. In Randy Shilts' 1987 book \"And the Band Played On\" (and the 1993 movie based on it), Dugas is referred to as AIDS' Patient Zero instead of \"Patient O\", but neither the book nor the movie states that he had been the first to bring the virus to North America. He was incorrectly called \"Patient Zero\" because at least 40 of the 248 people known to be infected by HIV in 1983 had had sex with him, or with someone who had sexual intercourse with him.\n\nA volunteer social worker called Betty Williams, a Quaker who worked with the homeless in New York from the seventies and early eighties onwards, has talked about people at that time whose death would be labelled as \"junkie flu\" or \"the dwindles\". In an interview for the Act Up Oral History Project in 2008, she said: \"Of course, the horror stories came, mainly concerning women who were injection-drug users ... who had who had PCP pneumonia (Pneumocystis pneumonia), and were told that they just had bronchitis.\" She continues: \"I actually believe that AIDS kind of existed among this group of people first, because if you look back, there was something called junkie pneumonia, there was something called the dwindles that addicts got, and I think this was another early AIDS population way too helpless to ever do anything for themselves on their own behalf.\"\n\nJulia Epstein writes in her book 'Altered Conditions: Disease, Medicine and Storytelling' that: \"As we uncover more of the early history of HIV infection, it becomes clear that by at least the 1970s the virus was already making major inroads into the immune systems of a number of diverse populations in the United States (the retrospectively diagnosed epidemic of 'junkie pneumonia' in New York City in the late 1970's for example) and had for some time been causing devastation in several countries in Africa.\"\n\nAnecdotal evidence suggests that so-called junkie pneumonia first began to afflict heroin addicts in New York in 1977. In her book 'EnGendering AIDS : deconstructing sex, text, and epidemic', Tamsin Wilton writes: \"People had been sickening and dying of mysterious conditions since the early 1970's, conditions that we can retrospectively diagnose as AIDS related. There was, for example, a phenomenon known as 'junkie pneumonia' which spread among some populations of injecting street drug users in the 1970's, and which is now believed to have been caused by HIV infection.\"\n\nMelinda Cooper writes in her book 'Family Values: Between Neoliberalism and the New Social Conservatism': \"It is plausible that these cases [of AIDS] did not come to light in the 1970s for the same reason that \"junkie pneumonia\" was not recognized as the sign of an emerging infectious disease: The people in question had such precarious access to health care that news of their death was never communicated to public health authorities.\"\n\nAn article by Pattrice Maurer in the newspaper Agenda from April 1992 explores some of the issues surrounding junkie pneumonia. It starts: \"In the late 1970s while the epidemic known as \"disco fever\" swept through the U.S., an epidemic known as \"junkie pneumonía\" raged among injection drug users in New York City.\" It continues: \"Few people were aware that large numbers of injections drug users were inexplicably dying of pneumonía. Those few who did notice these deaths did not feel compelled to investígate the public health puzzle they posed.\" The author's opinion is that if anyone had bothered to investigate these deaths, they would have found an immune system disorder that is now called AIDS.\n\nSteven Thrasher writes in The Guardian: \"Indeed, those of us who study AIDS have long known that long before common symptoms such as Kaposi sarcoma and pneumonia were showing up among hemophiliacs and gay men, they were likely affecting homeless people who lived off society’s radar, people who used IV (intravenous) drugs and those who avoided medical treatment out of fear.\"\n\nA chapter in The Proceedings of the World Conference of Therapeutic Communities (9th, San Francisco, California, September 1–6, 1985) gives details about serum samples that were tested for signs of HIV (then called HTLV-III/LAV) antibodies. Quoting: \"We have also conducted historical studies of the epidemic in New York City, using serum samples that were originally collected for other purposes. We have sera from IV drug users that go back to the middle 1960's. The first indication of HTLV-III/LAV antibody presence is in one of eleven samples from 1978 ... 29% of 40 samples in 1979 ... 44% of samples from 1980 and 52% of samples from 1982. The HTLV-III/LAV virus appears to have been introduced among IV drug users in the late 1970's in New York City.\"\n\nAnna Thompson writes on the website TheBody.com in an article dated Autumn 1993: \"Many women were dying in the late '70s of pneumonia, cervical cancer, and other illnesses complicated by \"mysteriously\" suppressed immune systems. Yet, it was not until 1981 that a case of AIDS in a woman was first reported by the Centers for Disease Control (CDC).\" She continues: \"The CDC's refusal to address women's issues led to the overall perception that women do not get AIDS.\"\n\nIn an article published in \"AIDS: Cultural Analysis/Cultural Activism\", author Douglas Crimp draws attention to anecdotal evidence about junkie pneumonia. Quoting: \"Even these statistics are based on CDC epidemiology that continues to see the beginning of the epidemic as 1981 ... in spite of widespread anecdotal reporting of a high rate of deaths throughout the 1970s from what was known as \"junkie pneumonia\" and was likely Pneumocystis pneumonia.\" The statistics Crimp writes about were taken from a New York Times article from October 1987 about a NYC Department of Health study that showed that 53% of AIDS sufferers were people who injected drugs - more than 150 percent higher than previously reported. Quoting: \"City health officials estimated that half of the city's 200,000 intravenous drug users were infected with the virus that causes AIDS\".\n\nThe study \"HIV-1 Infection Among Intravenous Drug Users in Manhattan, New York City, from 1977 through 1987\", published in February 1989, seeks to understand long term trends in the spread of HIV among intravenous drug users (IDUs). AIDS surveillance data and studies which detail the number of persons who tested HIV positive in Manhattan are used to compile information deemed critical to realising the extent of the AIDS epidemic. It starts by stating that up to September 1988, IDU was the risk behaviour in 19,139 (or 26.4%) of the first 72,223 cases of AIDS in the USA. Cases among IDUs in New York in the same period numbered 6,182 (approximately a third of national IDU cases). The study continues to outline the methodology used in the compilation of data. It says that while truly representative samples of IDUs within a community are probably impossible to obtain, samples of IDUs entering treatment provide a good source for monitoring trends. In the \"RESULTS\" section it states (quoting): \"The first evidence for HIV-1 infection among IV drug users in New York is from three cases of AIDS in children born in 1977. These cases were later reported to the New York City Department of Health AIDS Surveillance Unit. These children did not receive any known transfusions prior to developing AIDS and were born to mothers known to be IV drug users.\"\n\nIt continues to outline that the earliest known case of AIDS in an adult IDU occurred in 1979 (mixed risk) and that known cases among IDUs increased rapidly from the 8 cases in 1980 (3 mixed risk), to 31 cases in 1981, to 160 cases in 1982, and to 340 cases in 1983. Statistics on the amount of positive tests for HIV, mainly using archived samples, are: 1978 1 out of 11; 1979 13/50; 1980 8/21; 1981-83 14/28; 1984 75/137 and 38/63; 1986 36/55 and 1987 169 out of 294. In the COMMENTS section, it states: \"The three cases in 1977 of apparent perinatal transmission (mother-to-child) from IV drug-using women strongly suggest that the introduction of HIV-1 into the IV drug-use group occurred around 1975 or 1976, or perhaps even earlier.\" It says that without extensive samples from this period, it is not possible to be certain about the spread of HIV among IDUs, but the samples from IDUs with chronic liver disease suggest that the rates of infection were below 20% for the first 3 or 4 years after its introduction.\n\nHIV is thought to have entered the population of people using intravenous drugs in New York in approximately 1975. In Spring 1975, the government of New York underwent a fiscal crisis which led to the closing of many social services, with people who used intravenous drugs living in a hostile sociopolitical and legal environment. This fiscal crisis lead to many agencies with health responsibilities being particularly hard hit, which in turn might have led to an increase in HIV/AIDS and Tuberculosis (TB). Quoting from a 2006 American Journal of Public Health study: \"Between 1974 and 1977, the Department of Health (DOH) budget (in NY) was cut by 20%, and by 1977 the department had lost 1700 staff members—28% of its 1974 workforce. To achieve these reductions, the department closed 7 of 20 district health centers, cut $1 million from its methadone program, terminated the employment of 14 of 19 health educators, and closed 20 of 75 child health stations and 6 of 14 chest clinics (the units responsible for TB screening and diagnosis).\"\n\nA study published in the Journal of the American Medical Association links TB and HIV/AIDS. Quoting: \"\"Severe and unusual presentation of overwhelming tuberculosis in appropriate clinical circumstances may be considered an infection predictive of the presence of AIDS.\" Further, a study from 1987 states there was a link between the rise in TB, AIDS and drug users within the United States. Quoting: \"AIDS thus compounds the risk of acquiring tuberculosis, and in the United States most patients with AIDS and tuberculosis have been drug users.\" A newsletter from Spring 1987 by the National Coalition Of Gay STD Services has an article called \"Tuberculosis and AIDS - Connecticut\" that suggests an association between TB and AIDS within that state.\n\nThe AIDS epidemic officially began on June 5, 1981, when the U.S. Centers for Disease Control and Prevention in its \"Morbidity and Mortality Weekly Report\" newsletter reported unusual clusters of Pneumocystis pneumonia (PCP) caused by a form of \"Pneumocystis carinii\" (now recognized as a distinct species \"Pneumocystis jirovecii\") in five homosexual men in Los Angeles.\n\nOver the next 18 months, more PCP clusters were discovered among otherwise healthy men in cities throughout the country, along with other opportunistic diseases (such as Kaposi's sarcoma and persistent, generalized lymphadenopathy), common in immunosuppressed patients.\n\nIn June 1982, a report of a group of cases amongst gay men in Southern California suggested that a sexually transmitted infectious agent might be the etiological agent, and the syndrome was initially termed \"GRID\", or gay-related immune deficiency.\n\nHealth authorities soon realized that nearly half of the people identified with the syndrome were not homosexual men. The same opportunistic infections were also reported among hemophiliacs, users of intravenous drugs such as heroin, and Haitian immigrants – leading some researchers to call it the \"4H\" disease.\n\nBy August 1982, the disease was being referred to by its new CDC-coined name: Acquired Immune Deficiency Syndrome (AIDS).\n\nIn New York City, Nathan Fain, Larry Kramer, Larry Mass, Paul Popham, Paul Rapoport, and Edmund White officially established the Gay Men's Health Crisis (GMHC) in 1982.\n\nAlso in 1982, Michael Callen and Richard Berkowitz published \".\" In this short work, they described ways gay men could be sexual and affectionate while dramatically reducing the risk of contracting or spreading HIV. Both authors were themselves gay men living with AIDS. This booklet was one of the first times men were advised to use condoms when having sexual relations with other men.\n\nIn May 1983, a team of doctors at the Pasteur Institute in France including Françoise Barré-Sinoussi and Luc Montagnier reported that they had isolated a new retrovirus from lymphoid ganglions that they believed was the cause of AIDS. The virus was later named lymphadenopathy-associated virus (LAV) and a sample was sent to the U.S. Centers for Disease Control, which was later passed to the National Cancer Institute (NCI).\n\nIn May 1984 a team led by Robert Gallo of the United States confirmed the discovery of the virus, but they renamed it human T lymphotropic virus type III (HTLV-III).\n\nDr. Jay Levy's group at the University of California, San Francisco also played a role in the discovery of HIV. He independently isolated the AIDS virus in 1983 and named it the AIDS-associated Retrovirus (ARV), publishing his findings in the journal Science in 1984.\n\nIn January 1985, a number of more-detailed reports were published concerning LAV and HTLV-III, and by March it was clear that the viruses were the same --- indeed, it was later determined that the virus isolated by the Gallo lab was from the lymph nodes of the patient studied in the original 1983 report by Montagnier --- and was the etiological agent of AIDS.\n\nIn May 1986, the International Committee on Taxonomy of Viruses ruled that both names should be dropped and a new name, HIV (Human Immunodeficiency Virus), be used.\n\nWhether Gallo or Montagnier deserve more credit for the discovery of the virus that causes AIDS has been a matter of considerable controversy. Together with his colleague Françoise Barré-Sinoussi, Montagnier was awarded one half of the 2008 Nobel Prize in Physiology or Medicine for his \"discovery of human immunodeficiency virus\". Harald zur Hausen also shared the prize for his discovery that human papilloma virus leads to cervical cancer, but Gallo was left out. Gallo said that it was \"a disappointment\" that he was not named a co-recipient. Montagnier said he was \"surprised\" Gallo was not recognized by the Nobel Committee: \"It was important to prove that HIV was the cause of AIDS, and Gallo had a very important role in that. I'm very sorry for Robert Gallo.\" Dr Levy's contribution to the discovery of HIV was also cited in the Nobel Prize ceremony.\n\nSince June 5, 1981, many definitions have been developed for epidemiological surveillance such as the Bangui definition and the 1994 expanded World Health Organization AIDS case definition.\n\nAccording to a study published in the Proceedings of the National Academy of Sciences in 2008, a team led by Robert Shafer at Stanford University School of Medicine has discovered that the gray mouse lemur has an endogenous lentivirus (the genus to which HIV belongs) in its genetic makeup. This suggests that lentiviruses have existed for at least 14 million years, much longer than the currently known existence of HIV. In addition, the time frame falls in the period when Madagascar was still connected to what is now the African continent; the said lemurs later developed immunity to the virus strain and survived an era when the lentivirus was widespread among other mammals. The study is being hailed as crucial, because it fills the blanks in the origin of the virus, as well as in its evolution, and may be important in the development of new antiviral drugs.\n\nIn 2010, researchers reported that SIV had infected monkeys in Bioko for at least 32,000 years. Previous to this time, it was thought that SIV infection in monkeys had happened over the past few hundred years. Scientists estimated that it would take a similar amount of time before humans adapted naturally to HIV infection in the way monkeys in Africa have adapted to SIV and not suffer any harm from the infection.\n\nA 2016 Czech study of the genome of Malayan flying lemurs, an order of mammals parallel to primates and sharing an immediate common ancestor with them, found endogenous lentiviruses that emerged an estimated 40-60 million years ago based on rates of viral mutation versus modern lentiviruses.\n\nOther hypotheses for the origin of AIDS have been proposed. AIDS denialism argues that HIV or AIDS does not exist or that AIDS is not caused by HIV; some of its proponents believe that AIDS is caused by lifestyle, including sexuality or drug use, and not by HIV. Some conspiracy theories allege that HIV was created in a bioweapons laboratory, perhaps as an agent of genocide or an accident. These hypotheses have been rejected by scientific consensus; it is generally accepted among pathologists that \"…the evidence that HIV causes AIDS is scientifically conclusive\", and most \"scientific\" arguments for denialism are based on misrepresentations of outdated data.\n\n"}
{"id": "2104591", "url": "https://en.wikipedia.org/wiki?curid=2104591", "title": "Hugo Danner", "text": "Hugo Danner\n\nHugo Danner is a fictional character and the protagonist of Philip Wylie's 1930 novel \"Gladiator\". Born in the late 19th century with superhuman abilities via prenatal chemical experimentation, Danner tries to use his powers for good, making him a precursor of the superhero. However, Danner grows disillusioned with his inability to find a permanent outlet for his great strength, and dies frustrated.\n\nApart from Wylie's novel, the character has also appeared in a feature film and publications by Marvel Comics and DC Comics. Comedic actor Joe E. Brown portrayed him in a 1938 movie adaptation of the book. Decades later the character starred in an adaptation titled \"Man-God\" in Marvel's black-and-white comics-magazine \"Marvel Preview\" #9 (Winter 1976). He next appeared in DC's standard color comic book \"Young All-Stars\" in 1988 and 1989, as the estranged father of an illegitimate son named Iron Munro. In 2005, he returned in the comic-book miniseries \"Legend\", published by the DC imprint Wildstorm. Here Danner, in the late 1960s, fights as a U.S. Army supersoldier in the Vietnam War rather than as a super Legionnaire in World War I.\n\nIn early February 1930, Wylie described the inspiration for Danner's creation in the introduction to one of the earliest printings of \"Gladiator\":\nThe novel begins during the closing years of the 19th century, as Colorado science Professor Abednego Danner searches for a way to improve the innate weaknesses of human biology and create a new \"race that doesn't know fear — because it cannot know harm\". After 14 years of research he finally discovers \"alkaline radicals\" that vastly improve \"muscular strength and the nervous discharge of energy\". Following very successful animal testing, which yields super-strong tadpoles and a bulletproof kitten capable of taking down cattle as prey, he injects this super-serum into the womb of his pregnant wife, Matilda. Hugo Danner is born on Christmas several months later.\n\nHis parents imbue him with a strong moral compass during his formative years and warn him never to use his great strength in anger. However, during kindergarten, Danner nearly kills the school's bully in a one-sided fight after being assaulted by the child. This event brands him as an outsider in the eyes of the other children. Because they treat him with such abhorrence, his only solace comes in the form of unleashing his powers within the Colorado wilderness (uprooting trees, throwing huge boulders, leaping to the mountain tops, etc.). This stigma eventually wears off, though, and is forgotten by the time Danner enters high school.\n\nDanner leaves Colorado after graduating high school to attend Webster University. There, he becomes an unstoppable football star and the most popular boy in school. During the summer months, he works as a circus strong man and even enters a fight competition for money. However, after returning to college and accidentally killing another player during a game, he quits school and becomes a sailor. A year later, his ship is trapped in France by the outbreak of World War I in 1914.\n\nHe and a fellow American sign up with the French Foreign Legion and find themselves on the battlements after a short training period. He eventually reveals his powers to his Legionnaire superiors, who believe him to be a devil or supernatural Native American, and he is given free rein to roam the battlefield and kill as many German soldiers as he possibly can. Stories of \"Le Colorado\" (The Colorado) quickly spread from trench to trench and Danner becomes a sort of mythical hero. After a short stint in an American Legionnaire unit, Danner grows weary of war and devises a plan with an airplane:\n\nHowever, the Treaty of Versailles is signed the very day he prepares to leave, and so he is forced to abort his mission, much to his dismay. He returns to the United States and works in a number of professions — steel mill worker, bank teller, farm hand, and disarmament lobbyist — but his unique stature among mortal men forever brings him grief. He eventually offers his services to a noted history professor preparing a Mayan archaeological dig and travels with the group to the Yucatan Peninsula.\n\nDuring the trip, Danner wonders if the Mayans and Egyptians had discovered the same formula as his father, \"which could be poured into the veins of the slaves [who built the pyramids], making them stronger than engines\". There, he works as the head steward of the hired Mayan helpers and proves himself an asset to the team. But when a mishap reveals his great strength to the professor, the elderly man suggests Danner use his father's formula to create a new race of men known as the \"Sons of Dawn\", who will use their powers to right the world's wrongs. The idea appeals to him at first, but Danner fears that these sons would also be hated by humanity for their superiority and treated much the same as himself. In the end, he climbs a mountain amidst an oncoming thunderstorm, and is struck dead by lightning while praying to God. The formula is lost in the resulting fire. The professor discovers his charred remains three days later and buries him among the Mayan ruins.\n\nEven during his early years, Danner displays superhuman strength. He demolishes his wooden crib as a newborn, saves a man's life by lifting a two-ton supply wagon at 6 years old, and uproots entire trees at 10. He progressively grows stronger as he gets older. During his twenties, he can stop and kill a charging bull with a single punch, bend a railroad rail, lift a seventy-five millimeter howitzer cannon singlehandedly, lift a car and its driver singlehandedly, rip open a bank vault, and easily catch a falling 8,000-pound block of stone.\n\nDanner's physical strength extends to his legs, allowing him to leap great heights and distances. At age 10 he can leap 40 feet into the air and run faster than a train. During his freshman year in college, he easily breaks a world track record, though he actually doesn't run at his full speed. While in the service of the French Foreign Legion in World War I, he traverses thirty-seven miles round trip in just thirty minutes (a speed of about 148 mph); all while carrying 2,000 pounds of food, water, and ammunition for his unit. Despite his great strength, he is still susceptible to fatigue.\n\nHe first learns of his body's superior resistance to physical injury during the war. Bullets and bayonets glance off his tough skin. He believes he can even survive a plane crash. The only weaponry capable of penetrating his skin at the time are the largest artillery shells. Although, he still feels the effects of the elements, sweating under the heat of the sun and freezing during winter. Danner's only vulnerability is lightning, which ultimately kills him.\n\nDanner's bodily tissues have a somewhat greater density than those of an ordinary human being. Though he has an athletic build, his strength is far greater than what his frame would naturally allow. People guess him to be 155 pounds, but he actually weighs 211. During his stint as a sailor, he goes pearl diving with the natives of Cristobal and is able to dive deeper and stay under longer because of his density.\n\nHis body can heal damaged tissue much faster and more extensively than an ordinary human can. He collapses from exhaustion and wounds sustained from artillery shells after he goes berserk against the Germans to avenge a friend's death. When he wakes up in an army hospital, he finds \"his wounds had healed without the necessity of a single stitch\". Long periods of comatose sleep (up to 20 hours) and the consuming of huge meals also aid in his regenerative process.\n\nDanner was portrayed by comedic actor Joe E. Brown in the 1938 film \"The Gladiator\", which loosely adapted science-fiction drama as a comedy, and, among the many changes, renamed the protagonist \"Hugo Kipp\". The film begins when Kipp wins a large sum of money in a contest and decides to return to college. There, he is talked into joining the football team and fails to live up to his athletic father's legendary reputation. But after a Professor Danner (Lucien Littlefield) injects Kipp with a serum that gives him superhuman strength, he becomes an unstoppable player and wins the heart of the self-centered quarterback's girlfriend (June Travis). In the end, he faces Man Mountain Dean in a wrestling match, but the serum runs out at the last minute.\n\nIn comics, Danner first appeared in the 52-page story \"Man-God\", by writer Roy Thomas and artist Tony DeZuniga (credited as Antony DeZuniga) in Marvel Comics' \"Marvel Preview\" #9 (Winter 1976). Only the first half of the novel is adapted.\n\nDanner next appeared in DC Comics' \"Young All-Stars\" #9-11 (Feb.-April 1988) and #28-31 (Aug.-Nov. 1989), as the estranged father of Iron Munro, his illegitimate son. Munro was not a character or even a possibility in the original novel, where, as Danner's ailing scientist father explained, \"the effect of the process is not inherited by the future generations. It must be done over each time\".\n\nIn this storyline, Munro comes into possession of his father’s diary and learns of Danner's troubled life as the 20th century’s first metahuman. After having read the diary, Munro turns to the government's secret Project M, demanding to know the location of the \"Dinosaur Island\" mentioned by Danner. There he meets a person who leads him instead to Maple White Land in South America. Munro is surprised when he finds his father is alive. Danner apparently faked his death in the Yucatan and briefly returned home to have a one-night stand with his former high school sweetheart Anna Blake. The resulting union bore Munro, who took his surname from his mother's new husband (who believed the child to be his own). Danner has, by this time, succeeded in rearing the first generation of the Sons of Dawn. He later reveals to the Young-All Stars that the villain Übermensch has stolen the formula in order to create his own race of supermen.\n\nMunro is forced to oppose his father when Danner orders the Sons of Dawn to attack a Brazilian city. The combined might of the All-Star Squadron defeats the Sons of Dawn and Danner is killed during the final battle.\n\nDanner starred in the four-issue miniseries \"Legend\" (April–July 2005) by writer Howard Chaykin and artist Russ Heath, published by the DC imprint Wildstorm. Cover blurbs on the first two issues read, \"Inspired by Philip Wylie's Novel \"Gladiator\"\". The story now takes place in the second half of the 20th century, with the Vietnam War replacing World War I, but the story remaining for the most part intact.\n\nNo confirmation exists that Superman co-creator Jerry Siegel was influenced by \"Gladiator\". He and co-creator Joe Shuster began developing Superman in 1934.\n\nSuperman as originally conceived came from an unnamed planet whose inhabitants were millions of years more evolved than humans. When they reached maturity, \"the people of his race became gifted with titanic strength\". Their advanced evolution and great strength accounted for Superman's superhuman abilities on earth. As Siegel described them: \"When maturity was reached, he discovered he could easily: leap of a mile; hurdle a twenty-story building...raise tremendous weights...run faster than an express train...and that nothing less than a bursting shell could penetrate his skin!\"\n\nSiegel and Shuster compared Superman's strength and leaping abilities to those of an ant and a grasshopper, respectively, as did Abednego for his son. Danner also claims descent from a far-off land (Colorado) inhabited by a race of super-strong, indestructible men. In order to keep his true \"experimental\" origins a secret, he tells his Legionnaire superiors, \"[Colorado is a] place in America. A place that has scarcely been explored. I was born there. All the men of Colorado are born as I was born and are like me. We are very strong. We are great fighters. We cannot be wounded except by the largest shells.\" Other examples of similarities include both of their biological fathers are scientists (Jor-El and Abednego), both grow up in rural settings (Smallville and Indian Creek, Colorado), both are imbued with a strong moral compass from a young age both lift cars over their heads, and both hide their powers from the world.\n\nOne critic noted that Danner's \"creation and upbringing by a scientist father recall Doc Savage's origins\" and a \"prototype for the famous scene in which the fledgling Spider-Man defeats a hulking wrestler to make money is found in Wylie's novel; Hugo's bout in the ring is similar to that in the Spider-Man's origin story in 1962's \"Amazing Fantasy\" #15.\"\n\n"}
{"id": "20598355", "url": "https://en.wikipedia.org/wiki?curid=20598355", "title": "Human penis size", "text": "Human penis size\n\nThe most accurate measurement of the size of a human penis can be derived from several readings at different times since there is natural minor variability in size depending upon arousal level, time of day, room temperature, frequency of sexual activity, and reliability of measurement. When compared to other primates, including large examples such as the gorilla, the human penis is thickest, both in absolute terms and relative to the rest of the body.\n\nMeasurements vary, with studies that rely on self-measurement reporting a significantly higher average than those with a health professional measuring. , a systematic review of 15,521 men, and the best research to date on the topic, as the subjects were measured by health professionals, rather than self-measured, has concluded that the average length of an erect human penis is 13.12 cm (5.17 inches) long, while the average circumference of an erect human penis is 11.66 cm (4.59 inches). Flaccid penis length can sometimes be a poor predictor of erect length.\n\nMost human penis growth occurs between infancy and the age of five, and between about one year after the onset of puberty and, at latest, approximately 17 years of age.\n\nA statistically significant correlation between penis size and the size of other body parts has not been found in research. Some environmental factors in addition to genetics, such as the presence of endocrine disruptors, can affect penis growth. An adult penis with an erect length of less than , but otherwise formed normally, is referred to in medicine as a micropenis.\n\nWhile results vary slightly across reputable studies, the consensus is that the mean human penis, when erect, is in the range in length.\n\nA 2015 systematic review published by Veale et al. of medical research on the topic over the previous 30 years published in \"BJU International\" showed similar results, giving mean flaccid, stretched non-erect, and erect lengths of 9.16 cm, 13.24 cm, and 13.12 cm respectively, and mean flaccid and erect circumferences of 9.31 cm and 11.66 cm respectively. Erect lengths in the included studies were measured by pushing the pre-pubic fat pad to the bone, and flaccid or erect girth (circumference) was measured at the base or mid-shaft of the penis.\n\nOne study found the mean flaccid penis length to be (measured by staff). A review of several studies found average flaccid length to be . Length of the flaccid penis does not necessarily correspond to length of the erect penis; some smaller flaccid penises grow much longer, while some larger flaccid penises grow comparatively less.\n\nThe penis and scrotum can contract involuntarily in reaction to cold temperatures or nervousness, referred to by the slang term \"shrinkage\", due to action by the cremaster muscle. The same phenomenon affects cyclist and exercise bike users, with prolonged pressure on the perineum from the bicycle saddle and the straining of the exercise causing the penis and scrotum to contract involuntarily. An incorrect saddle may ultimately cause erectile dysfunction (see crotch pressure for more information).\n\nNeither age nor size of the flaccid penis accurately predicted erectile length. Stretched length has correlated with erect length in some cases. However, studies have also shown drastic differences between stretched and erect length.\n\nScientific studies have been performed on the erect length of the adult penis. Studies that have relied on self-measurement, including those from Internet surveys, consistently reported a higher average length than those that used medical or scientific methods to obtain measurements.\n\nThe following staff-measured studies are composed of different subgroups of the human population (in other words, specific age range or race; selection of those with sexual medical concerns or self-selection) that could cause a sample bias.\n\n\nSimilar results exist regarding studies of the circumference of the adult fully erect penis, with the measurement usually taken mid-shaft. As with length, studies that relied on self-measurement consistently reported a significantly higher average than those with staff measuring. In a study of penis size where measurements were taken in a laboratory setting, the average penis circumference when erect was 11.66 cm (4.59 inches).\n\nThe average stretched penile length at birth is about , and 90% of newborn boys will be between . Limited growth of the penis occurs between birth and 5 years of age, but very little occurs between 5 years and the onset of puberty. The average size at the beginning of puberty is with adult size reached about 5 years later. W.A. Schonfeld published a penis growth curve in 1943.\n\nAuthors of a paper reviewing research on area of penis sizes conclude that \"flaccid penile length is just under at birth and changes very little until puberty, when there is marked growth.\"\n\nAge is not believed to negatively correlate with penis size. \"Individual research studies have... suggested that penis size is smaller in studies focusing on older men, but Wylie and Eardley found no overall differences when they collated the results of various studies [over a 60 year period].\"\n\nA 2015 review of the literature found two studies finding height and stretched or flaccid length to be moderately correlated, seven studies finding weak correlation for flaccid, stretched, or erect length, and two studies that found no correlation between flaccid length and height.\n\nThe belief that penis size varies according to race is not supported by scientific evidence. A 2007 study reported that \"there is no scientific background to support the alleged 'oversized' penis in black people\". In fact, a study of 253 men from Tanzania found that the average stretched flaccid penis length of Tanzanian males is 11 cm (4.53 inches) long, smaller than the worldwide average, stretched flaccid penis length of 13.24 cm (5.21 inches), and average erect penis length of 13.12 cm (5.17 inches).\n\nA study of 115 men from Nigeria found that the average flaccid stretched penis length of Nigerian males is 13.37 cm (5.26 inches) long, which is near identical to the worldwide average, stretched flaccid penis length of 13.24 cm (5.21 inches) and average erect penis length of 13.12 cm (5.17 inches). A 2015 systematic review found that it was not possible to draw any conclusions about size and race from the available literature and that further research needed to be conducted.\n\nAndrogens like testosterone are responsible for penis enlargement and elongation during puberty. Penis size is positively correlated with increasing testosterone levels during puberty. However, after puberty, administration of testosterone is not capable of affecting penis size, and androgen deficiency in adult men only results in a small decrease in size. Growth hormone (GH) and insulin-like growth factor 1 (IGF-1) are also involved in penis size, with deficiency (such as that observed in growth hormone deficiency or Laron syndrome) at critical developmental stages having the potential to result in micropenis.\n\nThere are certain genes, like homeobox (Hox a and d) genes, which may have a role in regulating penis size. In humans, the AR gene located on the X chromosome at Xq11-12 which may determine the penis size. The SRY gene located on the Y chromosome may have a role to play. Variance in size can often be attributed to \"de novo\" mutations. Deficiency of pituitary growth hormone or gonadotropins or mild degrees of androgen insensitivity can cause small penis size in males and can be addressed with growth hormone or testosterone treatment in early childhood.\n\nAn adult penis with an erect length of less than 7 cm or 2.76 inches but otherwise formed normally is referred to in a medical context as having the micropenis condition. The condition affects 0.6% of men. Some of the identifiable causes are deficiency of pituitary growth hormone or gonadotropins, mild degrees of androgen insensitivity, a variety of genetic syndromes and variations in certain homeobox genes. Some types of micropenis can be addressed with growth hormone or testosterone treatment in early childhood. Operations are also available to increase penis size in cases of micropenis in adults.\n\nIt has been suggested that differences in penis size between individuals are caused not only by genetics, but also by environmental factors such as culture, diet and chemical or pollution exposure. Endocrine disruption resulting from chemical exposure has been linked to genital deformation in both sexes (among many other problems). Chemicals from both synthetic (e.g., pesticides, anti-bacterial triclosan, plasticizers for plastics) and natural (e.g., chemicals found in tea tree oil and lavender oil) sources have been linked to various degrees of endocrine disruption.\n\nBoth PCBs and the plasticizer DEHP have been associated with smaller penis size. DEHP metabolites measured from the urine of pregnant women have been significantly associated with the decreased penis width, shorter anogenital distance and the incomplete descent of testicles of their newborn sons, replicating effects identified in animals. Approximately 25% of US women have phthalate levels similar to those in the study.\n\nA 2007 study by the University of Ankara, Faculty of Medicine found that penile size may decrease as a result of some hormonal therapy combined with external beam radiation therapy. In addition, some estrogen-based fertility drugs like diethylstilbestrol (DES) have been linked to genital abnormalities or a smaller than normal penis (microphallus).\n\nPerceptions of penis size are culture-specific. Some prehistoric sculptures and petroglyphs depict male figures with exaggerated erect penises. Ancient Egyptian cultural and artistic conventions generally prevented large penises from being shown in art, as they were considered obscene, but the scruffy, balding male figures in the Turin Erotic Papyrus are shown with exaggeratedly large genitals. The Egyptian god Geb is sometimes shown with a massive erect penis and the god Min is almost always shown with an erection.\n\nThe ancient Greeks believed that small penises were ideal. Scholars believe that most ancient Greeks probably had roughly the same size penises as most other Europeans, but Greek artistic portrayals of handsome youths show them with inordinately small, uncircumcised penises with disproportionately large foreskins, indicating that these were seen as ideal. Large penises in Greek art are reserved exclusively for comically grotesque figures, such as satyrs, a class of hideous, horse-like woodland spirits, who are shown in Greek art with absurdly massive penises. Actors portraying male characters in ancient Greek comedy wore enormous, fake, red penises, which dangled underneath their costumes; these were intended as ridiculous and were meant to be laughed at.\n\nIn Aristophanes's comedy \"The Clouds\", \"Mr. Good Reason\" gives the character Pheidippides a description of the ideal youth: \"A glistening chest and glowing skin,/Broad shoulders, a small tongue,/A mighty bottom and a tiny prong.\" In Greek mythology, Priapus, the god of fertility, had an impossibly large penis that was always permanently erect. Priapus was widely seen as hideous and unattractive. A \"scholion\" on Apollonius of Rhodes's \"Argonautica\" states that, when Priapus's mother Aphrodite, the goddess of love and beauty, gave birth to him, she was so horrified by the size of his penis, his massive potbelly, and his huge tongue that she abandoned him to die in the wilderness. A herdsman found him and raised him as his son, later discovering that Priapus could use his massive penis to aid in the growth of plants.\n\nNonetheless, there are indications that the Greeks had an open mind about large penises. A statue of the god Hermes with an exaggerated penis stood outside the main gate of Athens and in Alexandria in 275 BC, a procession in honor of Dionysus hauled a 180-foot phallus through the city and people venerated it by singing hymns and reciting poems. The Romans, in contrast to the Greeks, seem to have admired large penises and large numbers of large phalli have been recovered from the ruins of Pompeii. Depictions of Priapus were very popular in Roman erotic art and literature. Over eighty obscene poems dedicated to him have survived.\n\nPenis size is alluded to in the Bible:\nWhen she carried on her whoring so openly and flaunted her nakedness, I turned in disgust from her, as I had turned in disgust from her sister.Yet she increased her whoring, remembering the days of her youth, when she played the whore in the land of Egyptand lusted after her lovers there, whose members were like those of donkeys, and whose issue was like that of horses. , English Standard Version.\nAncient Chinese legend holds that a man named Lao Ai had the largest penis in history and that he had an affair with Queen Dowager Zhao ( 280–228 BC), the mother of Qin Shi Huang, by pretending to be a eunuch. Ancient Koreans admired large penises and King Jijeung (437–514 AD) of the Silla Dynasty is said to have had a forty-five-centimeter penis that was so large his subordinates had to search for a woman that fit him. Traditional Japanese erotic paintings usually show genitals as exaggeratedly large. The oldest known painting of this type, found in the Hōryū-ji Temple in Ikaruga, dates to the eighth century AD and depicts a fairly large penis.\n\nThe ancient Indian sexual treatise \"Kama Sutra\", originally written in Sanskrit, probably between the second and fourth centuries AD, divides men into three classes based on penis size: \"hare\" size (5–7 cm when erect), \"bull\" size (10–15 cm), and \"horse\" size (18–20 cm). The treatise also divide women's vaginas into three sizes (\"deer\", \"mare\", and \"elephant\") and advises that a man match the size of the vagina of the woman he is having sex with to the size of his own penis. It also gives medically dubious advice on how to enlarge one's penis using wasp stings.\n\nIn medieval Arabic literature, a longer penis was preferred, as described in an \"Arabian Nights\" tale called \"Ali with the Large Member\". As a witty satire of this fantasy, the 9th-century Afro-Arab author Al-Jahiz wrote: \"If the length of the penis were a sign of honor, then the mule would belong to the Quraysh\" (the tribe to which Muhammad belonged and from which he descended).\n\nThe medieval Norsemen considered the size of a man's penis as the measure of his manliness, and a thirteenth-century Norse magic talisman from Bergen, a wooden stave inscribed with writing in runic script, promises its wearer: \"You will fuck Rannveig the Red. It will be bigger than a man's prick and smaller than a horse's prick.\" A late fourteenth century account of the life of Saint Óláfr from the \"Flateyjarbók\" describes a pagan ritual, which centered around a preserved horse's penis used as a cult artifact, which members of the cult would pass around in a circle, making up verses in praise of it, encouraging it and the other members of the group to behave in sexually suggestive ways.\n\nDuring the Renaissance, some men in Europe began to wear codpieces, which accentuated their genitals. There is no direct evidence that it was necessarily worn to enhance the apparent size of the wearer's penis, but larger codpieces were seen as more fashionable.\n\nMales may quite easily underestimate the size of their own penis relative to those of others. A survey by sexologists showed that many men who believed that their penis was of inadequate size had average-sized penises. Another study found sex education of standard penile measurements to be helpful and relieving for patients concerned about small penis size, most of whom had incorrect beliefs of what is considered medically normal. The perception of having a large penis is often linked to higher self-esteem. Fears of shrinking of the penis in folklore have led to a type of mass hysteria called penis panic, though the penis legitimately can shrink in size due to scar tissue formation in the penis from a medical condition called Peyronie's disease. Marketers of penis enlargement products exploit fears of inadequacy, but there is no consensus in the scientific community of any non-surgical technique that permanently increases either the thickness or length of the erect penis that already falls into the normal range.\n\nWidespread private concerns related to penis size have led to a number of folklore sayings and popular culture reflections related to penis size. Penis panic is a form of mass hysteria involving the believed removal or shrinking of the penis, known as genital retraction syndrome. The penis can significantly shrink due to scar tissue formation from a condition called Peyronie's disease which affects up to 10% of men. Products such as penis pumps, pills, and other dubious means of penis enlargement are some of the most marketed products in email spam. At present there is no consensus in the scientific community of any non-surgical technique that permanently increases either the thickness or length of the erect penis that already falls into the normal range (4.5\" to 7\").\n\nA study undertaken at Utrecht University found that the majority of homosexual men in the study regarded a large penis as ideal, and having one was linked to self-esteem. One study analysing the self-reported Kinsey data set found that the average penis of a homosexual man was larger than the average penis of their heterosexual counterparts (6.32 inches [16.05 cm] in length amongst gay men versus 5.99 in [15.21 cm] in heterosexuals, and 4.95 inches [12.57 cm] circumference amongst gay men versus 4.80 in [12.19 cm] in heterosexual men).\n\nA statistically significant correlation between penis size and the size of other body parts has not been found in research. One study, Siminoski and Bain (1988), found a weak correlation between the size of the stretched penis and foot size and height; however, it was too weak to be used as a practical estimator. Another investigation, Shah and Christopher (2002), which cited Siminoski and Bain (1988), failed to find any evidence for a link between shoe size and stretched penis size, stating \"the supposed association of penile length and shoe size has no scientific basis\".\n\nThere may be a link between the malformation of the genitalia and the human limbs. The development of the penis in an embryo is controlled by some of the same Hox genes (in particular HOXA13 and HOXD13) as those that control the development of the limbs. Mutations of some Hox genes that control the growth of limbs cause malformed genitalia (hand-foot-genital syndrome).\n\nIn a small study conducted by University of Texas–Pan American and published in BMC Women's Health, 50 undergraduate women were surveyed by two popular male athletes on campus about their perceptions of sexual satisfaction and it was concluded that the width of a penis feels better than the length of a penis, when subjects are asked to choose between the two (size was left unspecified). It was also concluded that this may show that penis size overall affects sexual satisfaction since women chose between the two options they were given.\n\nIn a cover story by \"Psychology Today\", 1,500 readers (about two-thirds women) were surveyed about male body image. Many of the women were not particularly concerned with penis size and over 71% thought men overemphasized the importance of penis size and shape. Generally, the women polled cared more about width than men thought, and less about length than men thought, although the strength of caring for either among women showed a similar pattern.\n\nAnother study, conducted at Groningen University Hospital, asked 375 sexually active women (who had recently given birth) the importance of penis size the results of which showed that 21% of women felt length was important and 32% felt that girth was important.\n\nA study conducted at the Australian National University, published in early 2013, showed that penis size influences a man's sex appeal, and the taller the man, the bigger the effect. The study showed 3D computer generated images at life size, altering the height and other physical attributes, with women typically registering preferences in under 3 seconds. A preference for taller men's larger penis size was notable.\n\nA U.S. study published in 2015 of the stated preferences of a panel of 75 women using 3D-printed models as scale references showed a preferred penis length of 16 cm and a preferred circumference of 12.2 cm for long-term sexual partners, with slightly larger preferred sizes of a length of 16.3 cm and circumference of 12.7 cm for one-time sexual encounters.\n\nThe term \"size queen\" is slang for anyone who prefers their sexual partner(s) to have a larger-than-average penis.\n\nThe human penis is thicker than that of any other primate, both in absolute terms and relative to the rest of the body. Early research, based on inaccurate measurements, concluded that the human penis was also longer. In fact, the penis of the common chimpanzee is no shorter than in humans, averaging 14.4 cm (5.7 inches), and some other primates have comparable penis sizes relative to their body weight.\n\nThe evolutionary reasons for the increased thickness have not been established. One explanation is that thicker penises are an adaptation to a corresponding increase in vaginal size. The vaginal canal is believed to have expanded in humans to accommodate the larger size of a newborn's cranium. Women may then have sexually selected men with penises large enough to fit their vagina, to provide sexual stimulation and ensure ejaculation.\n\nOne Australian study of 184 men looked at penis length and circumference in relationship to condom breakage or slippage. 3,658 condoms were used. The study found that when used correctly, condoms had a breakage rate of 1.34% and a slippage rate of 2.05%, for a total failure rate of 3.39%. Penile dimensions did not influence slippage, although penis circumference and broken condoms were strongly correlated, with larger sizes increasing the rate of breakage.\n\n\n"}
{"id": "3083190", "url": "https://en.wikipedia.org/wiki?curid=3083190", "title": "Hyperviscosity syndrome", "text": "Hyperviscosity syndrome\n\nHyperviscosity syndrome is a group of symptoms triggered by increase in the viscosity of the blood. Symptoms of high blood viscosity include spontaneous bleeding from mucous membranes, visual disturbances due to retinopathy, and neurologic symptoms ranging from headache and vertigo to seizures and coma.\n\nHyperviscosity occurs from pathologic changes of either cellular or protein fractions of the blood such as is found in polycythemias, multiple myeloma (particularly IgA and IgG3), leukemia, monoclonal gammopathies such as Waldenström macroglobulinemia, sickle cell anemia, and sepsis.\n\nTypes of hyperviscosity syndromes vary by pathology; including serum hyperviscosity, which may cause neurologic or ocular disorders; polycythemic hyperviscosity, which results in reduced blood flow or capillary perfusion and increased organ congestion; and syndromes of hyperviscosity, caused by reduced deformability of red blood cells, often evident in sickle cell anemia.\n\nHigh cell counts are seen in conditions such as polycythemia (raised red blood cells) or leukemia (more white blood cells, especially in acute leukemic blast crises).\n\nMay occur with a white blood cell count greater than 100,000/mm (100×10/L).\n\nAlthough elevated whole blood viscosity is a better measure of hyperviscosity and more common and clinically important, serum viscosity and plasma viscosity are more frequently measured. Normal plasma viscosity is between 1.4 and 1.8 centipoise while symptoms from hyperviscosity typically occur greater than 4 centipoise (about 4 times more viscous than water) and require emergency treatment.\n\nPatients will also have evidence of their underlying disorder. Those with myeloma will typically display a rouleaux formation on a peripheral smear and a large globulin gap, indicative of a significant paraprotein load. While viscosity can be directly measured, results can take a few days to return and thus a high index of suspicion is required to make the diagnosis in a timely manner. If hyperviscosity is suspected, treatment may need to be started prior to obtaining the official viscosity level.\n\nPlasmapheresis may be used to decrease viscosity in the case of myeloma, whereas leukapheresis or phlebotomy may be employed in a leukemic or polycythemic crisis, respectively. Blood transfusions should be used with caution as they can increase serum viscosity. Hydration is a temporizing measure to employ while preparing pheresis. Even after treatment, the condition will recur unless the underlying disorder is treated.\n\n\n"}
{"id": "52155283", "url": "https://en.wikipedia.org/wiki?curid=52155283", "title": "ICARE Institute of Medical Sciences and Research", "text": "ICARE Institute of Medical Sciences and Research\n\nICARE Institute of Medical Sciences & Research is a private medical college located in Haldia, West Bengal. It was established in 2011. The institute offers undergraduate seats for MBBS and postgraduate course MD (Pathology, Pharmacology, Microbiology, Biochemistry) which are recognised by the Medical Council of India. It is established by an society named Indian Centre for Advancent of Research and EDUCATION (ICARE). A 500 bedded multispeciality Bidhan Chanda Roy Hospital is attached with it.\n\nAddress :- .\n"}
{"id": "30833401", "url": "https://en.wikipedia.org/wiki?curid=30833401", "title": "IConji", "text": "IConji\n\niConji is a free pictographic communication system based on an open, visual vocabulary of characters with built-in translations for most major languages.\n\nIn May 2010 iConji Messenger was released with support for Apple iOS (iPhone, iPad, iPod) and most web browsers. Messenger enables point-to-point communication in a manner similar to SMS.\n\nIn December 2010, iConji Social was released as a web application only, with support for Facebook and Twitter as a broadcast medium. The application iConji Social supported delivery of iConji-enhanced messages via email.\n\niConji debuted with 1183 unique characters, known as the \"lexiConji\" (vocabulary), culled from base words used in common daily communications, word frequency lists, often-used mathematical and logical symbols, punctuation symbols, and the flags of all nations. The process of assembling a message from iConji characters is called \"iConjisation\" (see screenshot at right).\n\nSince most characters represent an entire word or concept, rather than a single letter or character, iConji has the potential to be a more efficient communication system than SMS. The usual jumble of text and confusing abbreviations can often be replaced by a short string of colorful icons that convey the identical meaning.\n\nWith the iConji Messenger and iConji Social apps, characters are displayed at a resolution of 32 x 32 pixels, using color PNGs with transparency to round the corners. As all iConji characters are developed first as vector graphics, this allows essentially infinite scalability, whether for producing new online or smartphone apps, or full-size posters for printed graphic applications such as signs or electronic displays.\n\nThus, future iConji applications, from in-house or outside developers, may incorporate larger or smaller versions of the characters using the freely available iConji API.\n\nIn December 2012, further development of iConji was brought to a close.\n\nKai Staats, founder and former CEO of Terra Soft Solutions, original developer of Yellow Dog Linux (YDL), was motivated to create a new communications system that combined the speed of SMS with the richness and linguistic depth of a global art project. His intent was to provide a means for communication that could bridge cultural divides. Thus, iConji is a pictographic communication system, not a spoken language.\n\nThe characters themselves are evocative of their meanings, and designed to be as cross-cultural as possible. It is a difficult task to even attempt to make pictographic symbols universal in their meaning. Further, not all cultures read symbols or text from left to right, which is the standard for iConji. In addition, some linguistic concepts are too abstract to represent graphically. The first row in the image above (The iConji user interface on an iPod.) shows characters for the pronouns (I, you, we, he, she, it, them), and the \"tilde\" which is defined as \"to be\" and its numerous conjugations (is, are, was, will be, and so on). These abstract concepts represent a significant barrier to universal pictographic representation, but the ability to read a translation in one's native language (if needed) can help bridge that gap. The character at far right is the \"null\" and can be used as a space, a placeholder, or a container for metadata.\n\nUnique to iConji is its inclusion of both an inferred meaning, suggested by the pictographs themselves, and the translations that accompany each character. At the close of 2010, these translations included English, French, German, Hindi, Italian, Japanese, Polish, Spanish, Swahili, Swedish, and Toki Pona. There is no practical limit to the number of languages that could be translated and included.\n\nLikewise, there is no limit to the number of individual characters that could be incorporated. The iConji vocabulary is open to revision - anyone in the world may design and contribute new characters for use in global communications. Through the Artist Community, users are able to add their own characters to the lexiConji (with approval), or revise existing character icons they feel could be better represented graphically.\n\nThe screenshot above shows most features and functionality of the iConji application. Starting at top right, the search icon (magnifying glass) opens a text field in the dark blue window that allows a text search for specific characters. Below that is the \"To:\" field, where the recipient can be inserted from a built-in address book. Below that is another field where selected iConji characters can be assembled into a string to compose the message (iConjisation).\n\nThe next section down is a 6x9 matrix of characters from which the user can select specific characters. In the iOS applications this is accomplished by a finger-tap; in the browser application by a point-and-click. In both cases, a hover pops up a small box displaying the definition of the character in the user's declared language.\n\nThe bottom-most line consists of what are called \"buckets\". Each user-customizable bucket contains another 54 characters that can be grouped by type or frequency of use. The seventh bucket contains 54 commonly used mathematical and logical symbols. The eighth bucket is \"bottomless\" and serves as a repository for all other characters, with no limit to the number contained. Selecting that bucket generates a scrollable list of those characters. \"It is the user's customization of these buckets that enables iConji to rival or exceed SMS in terms of efficiency and speed.\"\n\nA few other modern pictographic systems use inflection symbols to expand meanings, for example, Blissymbolics. iConji includes inflections for present, past, or future tense verbs, adjectives, adverbs, and possessives. The user can also add metadata, if desired, to clarify the meaning or include additional text content. All inflections are indicated as glyphs at standardized positions around the base and top of the iConji character. In most cases the inflection should be apparent from context, but for messages where ambiguity could arise, inflections provide a means to remove that ambiguity.\n\nFor example, the gallery below shows four inflected variations of the character defined as \"start, to start.\"\nMany iConji characters follow this \"noun\" + \"infinitive verb\" format to enable unambiguous translation from its base English into other languages. Given the widely varying conventions for verb conjugation found in other languages, this is arguably the most flexible way to present a base definition.\n\nThe sample iConjisation shown on the screenshot translates as follows:\n\nThis demonstrates clearly how meaning can be conveyed using a minimal number of characters. The fourth character is formally defined as \"clock, time\" but would be interpreted as \"o'clock\" when used in this context. If the sender and receiver usually meet for coffee at the same location, no other information is needed. If the intended location is different from the usual, the iConjisation could be changed to:\n\nHere, the triangular inflection mark on the second character (at) indicates the presence of \"metadata\" in that character. Metadata can be added or accessed via a text box pop-up by clicking on the character, and could include a business name, address, GPS coordinates, or other information. Alternatively, the sender can convey more specific location instructions using the characters themselves, for example:\n\nIn this example, the third character (I, me) has been inflected with the \"possessive\" modifier, changing the meaning to \"my.\" The two instances of the \"@\" character are included for grammatical clarity, but could likely be excluded without changing the interpreted meaning. In all of these examples, the recipient's response could be similarly concise, as the following three examples show:\n\nIn the first response, we see the flexibility of the iConji system, as well as some word-play. The first character is formally defined as \"sound, audio\" and can thus be used in many contexts. The second character is formally defined as \"angelic, saintly, good\" implying the overall meaning \"sounds good.\"\n\nIn the second response, additional information is returned by the recipient requesting the sender to \"arrive early\" using the adverb modifier on \"early.\" Whether the adverb modifier is really needed is a matter of question since, between frequent users, certain conventions will become established by previous usage.\n\nIn the final and most concise response, only a single character (yes) is returned by the recipient.\n\nIn February 2011 iConji launched its Artist Community. Anyone who saw the need for a new character, or a better version of an existing character, was encouraged to create and submit a unique design. There were several criteria for acceptance of a submitted character, but the process was made simple using freely available online graphic templates, instructions, and examples.\n\nCharacter icons were created as vector graphics in tools such as Adobe Illustrator, CorelDRAW, or the free online application SVG-edit. Alternatively, the proposed character icon could be hand-drawn, scanned as a 300 dpi bitmap, and converted to vector graphics format before being submitted as a potential addition to the iConji vocabulary.\n\nDefinitions for the characters also follow a strict format and, where ambiguities could exist, need to follow a format that utilizes \"extended definitions\" to remove those ambiguities, both for users and translators. For example:\n\n\nNote how the last example again defaults to the “noun + infinitive verb” format mentioned above.\n\nThe artist also had the opportunity, if desired, to associate metadata with their character explaining the story behind the design, who they are, and their country of residence. Once accepted, the character was made available for use globally, by all iConji users. The artist had the ability to track the use of their character using the iConji Explorer application on the iConji website (no longer maintained).\n\nThe first iConji Communications workshop was held at Colorado State University on April 7, 2011. 40 participants representing a dozen countries convened to discuss the cross-cultural potential of this pictographic system. Over 100 new pictographs were designed and entered into the iConji vocabulary.\n\nThere has been no shortage of attempts to create a new and “better” universal language. An exhaustive account of these efforts can be found in the book “In the Land of Invented Languages” by Arika Okrent. According to Okrent, in the last thousand years, more than 900 languages have been invented, often by individuals who believed they had a universal solution for global, cross-cultural communication. Most attempts at creating a new language failed due to overly-ambitious goals, or eccentricities of their inventor. Many attempts have been based on pictograms or logograms, including iConji and the following. \n\nOf all the recently invented pictographic systems, Blissymbolics is the most successful to date. It is used to assist communications-challenged individuals, providing them a structured means by which they may convey concepts, and more recently, providing a “point and click” software interface.\n\nThe Noun Project is another effort to establish cross-cultural communication. It purports to be a repository of universal icons, but suffers the same ambiguities as previous \"universal language\" attempts. The icons therein are primarily from public-domain icon databases, for example, restroom, traffic, mall, and airport icons, not all of which translate across cultures. The collection does include icons designed by individuals, but currently lacks guidelines for design beyond their stated desire for \"scale, proportion, and shape.\"\n\nAlso of note is Emoji, based on a vocabulary of 722 emoticons, and popular in electronic communications throughout Japan. Emoji icons are heavily slanted toward conveying emotional \"punctuation,\" and more useful in augmenting SMS than in communicating complete stand-alone messages.\n\nOther pictographic systems have been less successful, as described here: List of writing systems. There have also been many attempts to create universal \"spoken\" languages, the most notable of which is Esperanto. For more examples, see Universal language.\n\n"}
{"id": "23415923", "url": "https://en.wikipedia.org/wiki?curid=23415923", "title": "Inflight smoking", "text": "Inflight smoking\n\nInflight smoking is prohibited by almost all airlines. Smoking on domestic airliners based in the United States, for instance, was banned on all domestic flights with a duration of two hours or less beginning in 1988, with all domestic and international flights being smoke-free by 2000. According to FAA regulations, smoking lit cigarettes or anything else that produces smoke or flame is prohibited onboard most commercial aircraft. As of October 2015, the USDOT prohibits the use of electronic cigarettes on flights, as well as such devices from being transported in checked luggage.\n\nAdvocate Ralph Nader was among the first to call for a smoking ban on airlines. United Airlines was the first to implement a nonsmoking section, in 1971. Aurigny Air Services became the first airline to ban smoking entirely on its flights, in July 1977. However, both tobacco companies and airlines fought any regulation. Significantly, the Civil Aeronautics Board banned and then unbanned smoking in 1984, with chairman Dan McKinnon saying, \"Philosophically, I think nonsmokers have rights, but it comes into market conflict with practicalities and the realities of life.\" After years of debate over health concerns, Congressional action in 1987 led to a ban on inflight smoking.\n\nThe U.S. ban on inflight smoking began with domestic flights of two hours or less in April 1988, extended to domestic flights of six hours or less in February 1990, and to all domestic and international flights in 2000. In March 1995, the United States, Canada, and Australia banned smoking on international flights traveling between those countries.\nThe 1990 ban applied to the passengers and the cabin of the aircraft, but not the flight deck; pilots were allowed to continue smoking after the 1990 ban due to concerns over potential flight safety issues caused by nicotine withdrawal in chronic smokers.\n\nNormally, passengers found to be smoking on non-smoking flights will at least face a fine (up to $5,000) and at worst be arrested and detained upon landing. Due to stringent security measures, this often causes disruption; a flight may have to be diverted or a scheduled landing might have to be expedited upon arrival at the destination airport in order to escort the smoker from the plane.\n\nSuch regulations have on occasion met with defiance; in 2010 a Qatari diplomat was arrested upon arrival at Denver International Airport for smoking in the onboard lavatory on United Airlines Flight 663 and for making threats; when confronted by airline staff, he jokingly suggested that he was attempting to set his shoes on fire. On February 3, 2013, a family of four were accused of smoking during a Sunwing Airlines flight from Halifax to the Dominican Republic. They caused the flight to make an emergency landing at Bermuda L.F. Wade International Airport. The two eldest of the family were arrested by Bermuda Police Service and subsequently sentenced to a $500 fine or 10 days in prison.\n\nDue to the ubiquitous prohibition of in-flight smoking and the increasingly widespread use of electronic devices, the illuminated no-smoking signs have sometimes been re-purposed to inform passengers to switch devices off for take-off and landing. Where this is the case, the no-smoking sign is permanently printed on the overhead panels.\n\n"}
{"id": "36845923", "url": "https://en.wikipedia.org/wiki?curid=36845923", "title": "International Resources for the Improvement of Sight", "text": "International Resources for the Improvement of Sight\n\nInternational Resources for the Improvement of Sight (IRIS) is an International NGO that specializes in prevention of blindness and restoration of sight. It was founded in 1996 by Michele Claudel and John Stewart. It was initially founded to help people in Cambodia without access to eye care, it has since expanded to Nepal, Lao PDR and Sri Lanka. IRIS equips new eye clinics, strengthens existing ones, conducts eye screening, provides cataract surgery for poor people, primary eye care training for staff.\n\nIRIS has partnered with other NGOs to provide funding for services, such as with The Vision Charity in Sri Lanka. IRIS has also partnered with the Cambodian government program National Program for Eye Health in establishing eye clinics. Another partner is the International Centre for Eyecare Education.\n\nAs of December 2008, the combined total of IRIS clinic and eye camp programs since its inception in Cambodia had resulted in 125,319 people receiving eye examinations and 23,485 surgeries being performed free of charge, 15,291 of which were to remove cataracts.\n\nIn 2011, Roger Freeland Biggs was the CEO of IRIS and he was awarded an OBE \"for services to Health in Asia\".\n\n\n"}
{"id": "52510553", "url": "https://en.wikipedia.org/wiki?curid=52510553", "title": "James Cantine", "text": "James Cantine\n\nReverend James Cantine, D.D. (March 3, 1861 – July 1, 1940) was an American missionary, scholar, and traveler. While studying at New Brunswick Theological Seminary in New Jersey, he co-founded the Arabian Mission with John Lansing and Samuel Marinus Zwemer. The mission exists today as the American Mission Hospital of Bahrain. He was a missionary for forty years, which included establishing the first mission for the Reformed Church in Arabia, which was also the first mission in eastern Arabia. Between 1891 and 1929, he established mission posts, medical clinics, and churches in Arabia.\n\nHis wife, Elizabeth, was a nurse and the first single woman to become a missionary in Arabia. Together they founded a women's clinic in Muscat, Oman, worked at missionary posts, and when the United Missions was formed, they were both representatives of the Reformed Church in America for the organization. Cantine co-authored the book, \"The Golden Milestone: Reminiscences of Pioneer Days Fifty Years in Arabia\" with Zwemer.\n\nHe was born on the family homestead in Stone Ridge, New York on March 3, 1861 to James and Charlotte Hasbrouck Cantine. He had a sister, Catherine. His mother died on the family homestead in Stone Ridge on November 24, 1916 at 99 years of age, after having been a widow for many years.\n\nCantine graduated with a Civil Engineering Degree from Union College in 1883. After graduating, Cantine worked for three years as an engineer at Westinghouse Air Brake Company in Schenectady, but decided to become a missionary.\n\nHe studied at New Brunswick Theological Seminary in New Brunswick, New Jersey, where he met Dr. Lansing and, with Samuel Zwemer, was inspired to perform missionary work in Arabia. He was ordained on October 1, 1889 by his classis in the Fair Street Reformed Church, Kingston, New York. He received his Doctor of Divinity (D.D.) in 1903 from Union College.\n\nLansing, Cantine, and Zwemer established the Arabian Mission on August 1, 1889, at the Cantine homestead in Stone Ridge. About two and a half months later, on October 16, Cantine boarded a ship for Beirut; Zwemer would join him after he completed his last year at the seminary. He studied Islam and the Arabic language in Beirut and Syria, where he met and was influenced by Henry Harris Jessup, an American Presbyterian missionary and the main editor of the Protestant translation of the Arabic Bible. Both Cantine and Zwemer visited a number of cities in the Arabia region to determine the best place for them to establish a mission.\n\nCantine established a mission at Basrah, and Zwemer joined him in early 1892. Basrah was chosen because of its location along the eastern coast that enabled easy access to the whole of Northern Arabia. Located on the Persian Gulf, it was the first mission on Arabia's eastern coast, and it was the first mission for the Reform Church in Arabia.\n\nTheir approach was one of direct evangelism, by converting Arabs to Christianity. Prior to that, missionaries used an indirect approach, where they sought to revitalize the faith among people who were already Christians. Direct evangelism by foreigners had been forbidden by the Ottoman government. Cantine was primarily interested in spreading Christianity, and so he created relationships with other Christian missions in the area, but he also wanted their Arabian Mission to survive and at times bowed to political pressures to do so. Zwemer was a gifted linguist, writer, and orator and became the voice of the Arabian Mission. \"He had an irrepressible and aggressive personality, earnest where Cantine was tender, driving where his colleague was contemplative.\"\n\nTheir plans to establish a needed medical mission were delayed due to staffing issues and funding, but they carried on with their Bible work, taking over efforts of the British and Foreign Bible Society. In 1893, outposts were established in Bahrain and Muscat, by Zwemer and his brother, Peter, respectively. Since its beginning, missionaries and physicians came to support the work of the Arabian Mission and staff its posts. They sold about 200 booklets with Scripture within a couple of years. In 1894, the management of the independent mission was transferred to the Foreign Board of the church.\n\nThe missionaries hired Dr. Worrall in 1895, but he was severely ill much of his first summer. Cantine returned to the United States for part of the year and raised interest and funds for the mission. In the meantime, there were wars between local tribes, with looting at Muscat. The Bible shop was closed by the government. Another station was opened up at Amara, and Dr. Worrall was well enough to see patients in Basrah and Nasariah, creating a means to build relationships with the local people. They were also selling more Scriptures, about 80% of them to Moslems.\n\nZwemer settled in 1895 or 1896 in Bahrain, which had been judged to have very poor living conditions, like malaria, cholera, dysentery, and smallpox due to significant humidity and high temperatures. In his search for information about living conditions, Cantine found that it was considered \"the most unhealthy place in all the area.\"\n\nIn 1898, Cantine urgently left Basrah for Muscat because Samuel Zwemer and another colleague fell sick. Later, Muscat came to be known as \"Cantine's station\". In 1901, Cantine helped build a two-story building, the Mason Memorial Hospital after extensive negotiations with the ruler of Bahrain.\n\nElizabeth DePree, a nurse from Grand Rapids, Michigan, came to Arabia and was the first single woman to work as a missionary in Arabia. She was recruited by the mission and first studied and worked in Bahrain before coming to the mission in Oman. Cantine married DePree in 1904, and they continued their work together. They stayed in Muscat, Oman, where Elizabeth made house calls, worked at a daily clinic, and taught at a sewing school for girls. She became known as the \"mother of modern medicine in Oman\".\n\nHe returned to the United States, and with Zwemer, spoke about the Arabian Mission at the West End Collegiate Church on November 1, 1908, and two weeks later at the Lenox Avenue Collegiate Reformed Church, both in New York City.\n\nCantine and his wife opened a women's clinic in Muscat, which was staffed by Dr. Sarah Hosman. In 1921, the Cantine's were transferred to Baghdad to the Reform Church Missionary Society to help rebuild following World War II.\n\nIn 1924, the outposts and stations that they had established were merged with other missions in Mesopotamia and Arabia to form the United Missions in Mesopotania, and both of the Cantines were representatives of the Reform Church of America for the organization. Zwemer raised funds and Cantine implemented the plans to build the Union Church and Religious Center in Baghdad by mid-1926.\n\nElizabeth's health declined and they returned to the United States in April 1926. She died in Stone Ridge on August 30, 1927. He continued his missionary work in Arabia for a further of two years after her death, when he was no longer in good health. He returned to the homestead in Stone Ridge and continued promoting the work of the Arabian mission by speaking out to various sections of the Reformed Church.\n\nCantine was co-author of \"The Golden Milestone: Reminiscences of Pioneer Days Fifty Years in Arabia\" with Zwemer, which he spoke about in the United States. Cantine was honored for his 50 years of missionary work on the October 27, 1938 by the Reformed Church at Fair Street, where he was first ordained.\n\nHe had a heart attack in 1838 during a vacation in Florida, and a severe heart attack about May 1940, when he was taken to Benedictine Hospital in Kingston, New York. He remained there until his death on July 1, 1940. He was buried with his wife at the Fairview Cemetery in Stone Ridge, New York about July 5, 1940.\n\n"}
{"id": "46779645", "url": "https://en.wikipedia.org/wiki?curid=46779645", "title": "Jesús Aguilar Paz", "text": "Jesús Aguilar Paz\n\nJesús Aguilar Paz (15 October 1895 in Gualala, Santa Bárbara Department – 26 June 1974 in Tegucigalpa) was a Honduran chemist, pharmacist, cartographer, folklorist and teacher.\n\nIn 1915, he was appointed Secretary of the Escuela Normal de Occidente in the city of La Esperanza, Intibucá, where he was later appointed deputy director. Between 1915 and 1933 he made extensive trips around the country, taking sketches and making notes, which would become the subject of several books. In 1931 he published \"Tradiciones y leyendas de Honduras\" (\"Traditions and legends of Honduras\") and in 1933 he published the official general map of the municipalities of Honduras, \"Mapa General de la República de Honduras\". He was a member and editor of the \"Sociedad de Geografía e Historia de Honduras\" journal. From 1950 to 1953 he was Dean of the Faculty of Chemistry and Pharmacy of the National University of Honduras. In 1947, he published a book on chemistry, \"Interpretación química y Ley Periódica Universal\".\n"}
{"id": "2509547", "url": "https://en.wikipedia.org/wiki?curid=2509547", "title": "Living mulch", "text": "Living mulch\n\nIn agriculture, a living mulch is a cover crop interplanted or undersown with a main crop, and intended to serve the purposes of a mulch, such as weed suppression and regulation of soil temperature. Living mulches grow for a long time with the main crops, whereas cover crops are incorporated into the soil or killed with herbicides.\n\nOther benefits of mulches are slowing the growth of weeds, and protecting soil from water and wind erosion. Some living mulches were found to increase populations of the natural enemies of crop pests. Legumes used as living mulches also provide nitrogen fixation, reducing the need for fertilizer.\n\nWhen cover crops are turned over into the soil, they contribute nutrients to the main crop so that less chemical fertilizer is required. The amount of the contribution depends on the biomass, which varies over time and depends on rainfall and other factors. The greater the biomass, the greater the nutrient turnover of land. Legume cover crops turn over nitrogen fixed from the atmosphere. Reports indicate that legumes in general have higher foliar nitrogen contents, from 20 to 45 mg g-1.\n\nBare soil resulting from intensive tillage can lead to soil erosion, nutrient losses, and offsite movement of pesticides. In addition, weeds can germinate and grow without competition. Living mulches can reduce water runoff and erosion, and protect waterways from pollution. Living mulches have also been shown to increase the population of organisms which are natural enemies of some crop pests.\n\nLiving mulches control weeds in two ways. When they are seeded before weed establishment, they suppress weeds by competition. In some situations, the allelopathic properties of living mulches can be used to control weeds. For example, the allelopathic properties of winter rye (\"Secale cereale\"), ryegrasses (\"Lolium spp\"), and subterrain clover (\"Trifolium subterraneum\") can be used to control weeds in sweet corn (\"Zea mays\" var \"rugosa\") and snap beans (\"Phaseolus vulgaris\").\n\nPopulations of ground-dwelling predators were greater in a corn and soybean rotation with alfalfa and kura clover living mulches than without a living mulch. This situation was due in part to a change in the composition of vegetation in the agricultural system.\n\nUnfortunately, living mulches compete for nutrients and water with the main crop, and this can reduce yields. For example, Elkins et al. (1983) examined the use of tall fescue (\"Festuca arundinacea\"), smooth bromegrass (\"Bromus inermis\"), and orchargrass (\"Dactylis glomerata\") as living mulches. They found that herbicides killed 50% to 70% of the mulches but corn yield was reduced 5% to 10% at the end of the harvest.\n\nAlthough leguminous cover crops have large biomass production and turnover, they are not likely to increase soil organic matter. This is because legumes used as living mulches have greater N contents and a low C to N ratio. So when legume residue decomposes, soil microbes have sufficient N available to enhance their breakdown of organic materials in the soil.\n\nIn the tropics, it is common to seed tree crops with living mulches such as legume covers in oil palm plantations, coconut and rubber (Watson 1989).\n\nIn Mexico, legumes used traditionally as living mulches were tested as nematode and weed suppressors. The mulches included velvetbean (\"Mucuna pruriens\") jackbean (\"Canavalia ensiformis\"), jumbiebean (\"Leucaena leucocephala\") and wild tamarindo (\"Lysiloma latisiliquum\"). Aqueous extract of Velvetbean reduced the gall index of \"Meloidogyne incognita\" in the roots of tomato, but suppressed tomato rooting as well. In addition, Velvetbean suppressed the radical growth of the local weeds Alegria (\"Amaranthus hypochondriacus\") by 66% and Barnyardgrass (\"Echinochloa crus-galli\") by 26.5%.\n\nNicholson and Wien (1983) suggested the use of short turfgrasses and clovers as living mulches to improve the resistance soil compaction. These authors established Smooth Meadow-grass (\"Poa pratensis\") and white clover (\"Trifolium repens\") as living mulches since they did not cause reduction of yield corn (the accompanying main crop).\n\nIn one study, chewing fescue or red fescue (\"Festuca rubra\") and ladino clover (\"Trifolium repens\") were effective living mulches for controlling weed growth. Unfortunately, these cover crops also competed with corn for water which was particularly problematic during a dry period. The possibility of using ladino clover (\"Trifolium repens\") as a potential living mulch was also mentioned; however, this clover was difficult to kill with herbicides in winter.\n\nBecause they compete with the main crop, living mulches may eventually need to be mechanically or chemically killed.\n\nIt is important to judiciously select the appropriate herbicide rate for burning down a living mulch. In 1989, Echtenkamp and Moomaw found that herbicide rates were inadequate to suppress all the living mulches. Therefore, the mulches competed with the main crop for resources. In some cases, the clover could not be killed at the first herbicide application, so a second application was needed. For another treatment, rates that were so high that they caused the cover crop to be killed too rapidly, so that broadleaf weeds invaded the corn. This study suggested that the timing and dosage of herbicide should be carefully considered.\n\nLiving mulches were tested in a no-till corn-production systems with two methods for establishing grass and legume living mulches (grass and legume) between corn rows. In 1985, there was no difference between drilling and broadcasting seeds by hand in that study. However in 1986, drilling resulted in higher populations (97 plants m-2) than broadcasting (64 plants m-2), likely because of precipitation levels. Precipitation should be considered because farmers have no control over it.\n\nBeard (1973) recommended chewing fescue (red fescue) (\"Festuca rubra\" var \"commutata\" var \"shadow FESRU\") as a good living mulch because it adapts to the shady conditions under corn and soybean. This grass is also well adapted to dry and poor soils.\n\nLegume cover crops have important positive effects on the nutrient cycling of tree crops. Leguminous living mulches work in three ways:\nLehmann et al. (2000) measured the above ground biomass accumulation of \"Pueraria phaseoloides\", which is a living mulch used in tropical tree crops. They found that \"Pueraria\" accumulated 8.8 metric ton of dm (dry matter) ha-1 as compared with 4.4 t dm ha for \"Theobroma grandiflorum\", and 1.4 t dm ha-1 for \"Bactris gasipaes\". These latter two species are native cultivated species from the Amazon.\n\nVegetative cover as living mulches protect soil against wind and water erosion. Plants should form a mantle or thick mulch that protect soil from detachment. Living mulches intercept raindrops and reduce runoff. The protection that such vegetation provides against wind is influenced mainly by the amount of biomass that covers the ground (differs with each spp), plant geometry and row orientation.\n\nIn one experiment, water runoff and soil loss on a 14% slope was compared for rototilled (RT), no-till with corn stover mulch (NTCMS), no-till in CSM+ birdsfoot trefoil living mulch (NT-BFT) and no till in CSM and crownvetch living mulch (NT-CV). The results indicated that the water runoff was 6,350 L ha-1 for NT-BFT, 6,350 L ha-1 NO-CSM, 5,925 L ha-1 for NT-CV, and 145,000 L ha-1 for RT. The soil loss for the RT was 14.22 t ha-1 while with the other treatments it was less than 0.5 tons ha-1. The least soil loss was obtained with NT-CV 0.02 tons ha-1. The reduction of water runoff and erosion is one of the greatest advantages of having a cover crop. Soil can be easily eroded with no vegetative ground cover or plant residue. Ideally soil erosion should be less than 4 to 5 tons/ha/year.\n\n\n\n\n\n"}
{"id": "40800441", "url": "https://en.wikipedia.org/wiki?curid=40800441", "title": "Margaret McLarty", "text": "Margaret McLarty\n\nMargaret Chalmers McLarty, known as Margaret McLarty, (1908–1996) was a medical illustrator for the anaesthetic department in Oxford University. In 1960 she published \"Illustrating Medicine and Surgery\" a seminal volume on medical illustration and a core text for medical illustrators. She provided illustrations for the first two editions of \"Anatomy for Anaesthetists\" written with Harold Ellis in 1963. \n\nShe was trained by Audrey Arnott with whom she founded the Medical Artists Association of Great Britain on 2 April 1949.\n"}
{"id": "37987755", "url": "https://en.wikipedia.org/wiki?curid=37987755", "title": "Microlife", "text": "Microlife\n\nA microlife is a unit of risk representing half an hour change of life expectancy.\n\nIntroduced by David Spiegelhalter and Alejandro Leiva, microlives are intended as a simple way of communicating the impact of a lifestyle or environmental risk factor, based on the associated daily proportional effect on expected length of life. Similar to the micromort (one in a million probability of death) the microlife is intended for \"rough but fair comparisons between the sizes of chronic risks\". This is to avoid the biasing effects of describing risks in relative hazard ratios, converting them into somewhat tangible units. Similarly they bring long-term future risks into the here-and-now as a gain or loss of time. \n\nThe microlife exploits that for small hazard ratios the change in life expectancy is roughly linear. They are by necessity rough estimates, based on averages over population and lifetime. Effects of individual variability, short-term or changing habits, and causal factors are not taken into account. \n\n\n"}
{"id": "13841575", "url": "https://en.wikipedia.org/wiki?curid=13841575", "title": "Midwest Teen Sex Show", "text": "Midwest Teen Sex Show\n\nMidwest Teen Sex Show was a comedic, semi-educational video podcast featured monthly at their now defunct website with host Nikol Hasler, featuring comedian Britney Barber and produced and directed by Guy Clark.\n\nThe podcast series was created by Guy Clark and Nikol Hasler. While Clark and Hasler attended Woodstock High School together, they had not been in contact for years until they reconnected at her birthday party in 2006. Clark asked her to host the show shortly afterwards. Barber didn't meet them until after responding to an ad on Craigslist.\n\nSince the show's debut, it has been wildly popular. It has even had feature stories on \"The Morning Show with Mike and Juliet\", \"CBS Evening News with Katie Couric\", and \"Nightline\".\n\nThe show features tongue-in-cheek humor while providing basics on sex topics such as masturbation, homosexuality and dating older men. Working on a very low budget, episodes have been filmed at Hasler's former home in Waukesha, Wisconsin, Clark's mother's house in Woodstock, Illinois, as well as in Chicago.\n\nThe episodes are normally three to five minutes long. Hasler usually performs as the sarcastic host or interviewer, while Barber plays various comedic parts, often interacting with herself as the other character. Clark has also appeared in small parts in various episodes. In early 2008, two more regular performers were added, Neil Arsenty and Larissa Zageris, who also help write the show with the trio.\n\nThe theme song is by Gordon Tebo (who also went to high school with Clark and Hasler) and Britney Barber holds the Midwest Teen Sex Show sign in the farm field.\n\n\"Fetishes\" is the first episode to be sponsored by KoldCast.tv. Late last year, they began adding additional videos to the site besides the podcast, including short webisodes and live shows.\n\nThe show does not have a regular release schedule.\n\nListed with release dates.\n\n\"Fetishes\" is the first episode to have a sponsor (Koldcast)\n\nIn November 2007, more than 50,000 people were subscribing to the podcast through iTunes. By January 2008, that number was up to 60,000. By February, it was 70,000. Clark has stated the show averages 125,000 viewers an episode.\n\nThe show has had its share of controversy, particularly among sex-education teachers and therapists. While some praise it for tapping a hard-to-reach audience, others worry it's too racy for younger teens, and still others say the podcast focuses too much on humor and not enough on the facts kids need.\n\nIn \"The Older Boyfriend\" episode, when Hasler says, \"If you're in junior high and you're dating someone who's out of high school, he's a pedophile. And pedophilia's a disease. Would you date someone with cancer? No.\" The sarcastic remark (which was meant to be taken as a joke) drew a large amount of angry responses on the program's Web site as well as emails. When the 'Morning Show' episode re-aired in January, complaints about the remark flared up again. More controversy came when they started selling a satirical T-shirt that stated \"Homosexuality is a choice, like cancer.\" Reaction was so heated and split that the creators decided to discontinue the shirt after only a week.\n\nThe show's Web site has a disclaimer that \"all advice given is simply opinion and should not be taken as fact.\"\n\nOn May 14, 2009, Comedy Central released its 2009-10 programming slate, announcing \"a TV version of the popular web series \"Midwest Teen Sex Show\",\" in its pilot/presentation phase of development. Clark confirmed this news two days later on the website, stating, \"We’ll be taking a break from producing new episodes of the podcast as we focus on creating the sexiest most Midwesternest pilot the world has ever seen.\" Hasler also stated that the cast and crew will generally be the same \"though we will be looking for a few new faces.\" The show will be longer but the shooting and editing style will be \"true to the web show, but with the longer format we’ll be able to add some exciting new elements.\" The pilot was filmed in August in Los Angeles. Hasler announced on Facebook that Comedy Central passed on the deal on November 25. At this time, no future plans for the show have been announced.\n\n"}
{"id": "53393984", "url": "https://en.wikipedia.org/wiki?curid=53393984", "title": "Naji Abumrad", "text": "Naji Abumrad\n\nNaji Abumrad is an American surgeon, currently the John L. Sawyers Professor of Surgery, and formerly the Paul W. Sanger Professor from 1984 to 1992, at Vanderbilt University Medical Center. In 2014, he was elected to the American Association for the Advancement of Science.\n"}
{"id": "32671593", "url": "https://en.wikipedia.org/wiki?curid=32671593", "title": "National Center for Health Services Research", "text": "National Center for Health Services Research\n\nThe National Center for Health Services Research was a United States government program tasked with conducting research into health services.\n\nIt was established on July 23, 1974. It was transferred from the Heath Resources Administration to the Department of Health, Education, and Welfare on December 2, 1977. It was renamed the National Center for Health Services Research and Health Care Technology Assessment on October 30, 1984. The Center was terminated on December 19, 1989.\n\n"}
{"id": "55551786", "url": "https://en.wikipedia.org/wiki?curid=55551786", "title": "Nicorinse", "text": "Nicorinse\n\nNicorinse is a brand of mouthwash formulated to help users quit smoking cigarettes. It was invented by Dr. William Farone. Nicorinse is formulated to reduce the presence and residue of tobacco chemicals in the mouth, including nicotine. Nicorinse was originally launched in North America, then in the United Kingdom in February 2015. Compared to other smoking prevention products, it is notable for not containing nicotine.\n\n"}
{"id": "53319015", "url": "https://en.wikipedia.org/wiki?curid=53319015", "title": "Official Medicines Control Laboratory", "text": "Official Medicines Control Laboratory\n\nAn Official Medicines Control Laboratory (OMCL) is an official laboratory for the investigation and independent quality control of medicinal products and other similarly regulated substances.\n\nThe OMCLs examine medicinal products as to whether they meet the conditions of content, purity, etc., as specified in the approval. It is also checked whether the labeling and the usage information are in accordance with the legal requirements. Investigations are also carried out on products suspected of being counterfeits. Another focus of the OMCLs is the product delimitation between medicines on the one hand and other products such as foodstuffs, cosmetics, medical devices, care products, etc., which is often only analytically possible.\n\nIn the United Kingdom, the OMCL is the National Institute for Biological Standards and Control.\n\nIn Germany, the regional authorities coordinate their work in the Central Office of the Länder for Health Protection of Medicinal Products and Medical Devices (\"Zentralstelle der Länder für Gesundheitsschutz bei Arzneimitteln und Medizinprodukten\"; ZLG).\n\nThe European OMCLs are joined by the European Directorate for the Quality of Medicines (EDQM) in the so-called OMCL Network.\n\n\n"}
{"id": "43046032", "url": "https://en.wikipedia.org/wiki?curid=43046032", "title": "Overscreening", "text": "Overscreening\n\nOverscreening, also called unnecessary screening, is the performance of medical screening without a medical indication to do so. Screening is a medical test in a healthy person who is showing no symptoms of a disease and is intended to detect a disease so that a person may prepare to respond to it. Screening is indicated in people who have some threshold risk for getting a disease, but is not indicated in people who are unlikely to develop a disease. Overscreening is a type of unnecessary health care.\n\nOverscreening is problematic because it can lead to risky or harmful additional treatment when a healthy person gets a false positive result for screening which they should not have had. It also causes unnecessary stress for the person receiving the test, and it brings unnecessary financial costs which someone pays.\n\nThe general rule is that people should only be screened for a medical condition when there is a reason to believe that they ought to be screened, such a medical guideline recommendation for screening based on evidence from a person's medical history or physical examination.\n\nControversy and debate arises when new medical guidelines change screening recommendations.\n\nScreening is a type of medical test which is done on health people who do not show symptoms of a medical condition. Screenings are correctly performed when done on a person who has significant risk of developing a medical condition, and incorrectly performed when done on a person whose risk is not significant.\n\nThere can be debate about when risk becomes great enough to become significant and merit a recommendation for screening, but in discussions about overscreening, this is not the cause of the problem. Overscreening almost always happens when a person is screened routinely and without any consideration of their risk for a medical condition.\n\nOne early use of the term \"overscreening\" as \"unnecessary screening\" was in 1992 in the context of cervical cancer screening.\n\nA 1979 paper used the term \"overscreening\" to mean \"false positive result in a screening\".\n\nOverscreening is a type of unnecessary health care, so the causes of unnecessary health care are also causes of overscreening. Some causes include financial biases for physicians to recommend more treatment in health care systems using fee-for-service and physician self-referral practices; and physicians' practice of defensive medicine.\n\nOver time, recommendations to screen are made for populations with less risk in the past.\n\nClinical practice guidelines advise physicians to screen early to detect diseases. It has been considered that guideline committees might not appropriately do cost-effectiveness analysis, consider opportunity cost, or evaluate risks to patients when they broaden screening recommendations.\n\nOver time, the indicators for making a diagnosis are lower so that people with fewer symptoms are diagnosed with a disease sooner. Additionally, new diseases are named and treatment is recommended, including \"subclinical diseases\", \"preclinical diseases\", or \"pseudodiseases\", which are described as early versions of a disease which has not manifested.\n\nPatient demand is a sort of self-diagnosis in which patients request treatment regardless of whether the treatment they request is medically indicated. Causes for patients requesting treatment include increased access to health information on the Internet and direct-to-consumer advertising.\n\nEthical concerns of screening under these circumstances have been described.\n\nPhysicians sometimes use screening as a placebo for patients who wish to have some kind of care. The physician may recommend screening to placate the patient's demand for fast recovery in times when the recommended action would be to do nothing except wait. Research suggests that patients are more satisfied with their treatment when it is or seems expensive because patients believe that the more care they get, even if it is not necessary, then at least doing something is better than doing nothing.\n\nOverscreening is a type of unnecessary health care. One study about unnecessary screening before surgery reported that physicians order unnecessary tests because of tradition in the practice of medicine, anticipation that other physicians will expect the test results when they see the patient, defensive medicine, worries that a surgery may be canceled if the test is not done, and lack of understanding about when a test is actually indicated.\n\nA false positive medical test result is a false-positive test result of medical screening. It happens when a test indicates that a person has a medical condition when actually the person does not.\n\nOverscreening can be a problem because it can generate a false positive medical test result in a healthy person who does not have the medical condition which screening is supposed to detect. In such cases, the person who received the false positive test is more likely to get further unnecessary screening or even receive treatment for a condition which that person does not have. In either of these cases, the person becomes exposed to the risks and harms of treatment which they ought not be getting.\n\nIn general, people should not have medical screening unless the screening is indicated by the person's medical history, a physical examination, and a medical guideline. The rationale for this is that in cases in which a person is unlikely to have a medical condition, it can be more likely that a test will give a false positive result than it would be for the test to detect something which is unlikely considering the person's medical history. If a false positive result does occur in a patient unlikely to have that disease, then that patient will be likely to seek treatment.\n\nOverscreening tends to happen more in circumstances in which medical billing happens based on fee-for-service models rather than bundled payment. One reason for this is because health care providers have incentive to provide more services to increase their revenue. Furthermore, when patients are shielded from cost sharing, that also tends to increase rates of overscreening as when patients pay nothing for additional treatment, they tend to request more services even when they are not indicated.\n\nThe United States Preventive Services Task Force (USPSTF) recommended against PSA screening in healthy men finding that the potential risks outweigh the potential benefits. Guidelines from the American Urological Association, and the American Cancer Society recommend that men be informed of the risks and benefits of screening. The American Society of Clinical Oncology recommends screening be discouraged in those who are expected to live less than ten years, while in those with a life expectancy of greater than ten years a decision should be made by the person in question based on the potential risks and benefits. In general, they conclude that based on recent research, \"it is uncertain whether the benefits associated with PSA testing for prostate cancer screening are worth the harms associated with screening and subsequent unnecessary treatment.\" \n\nRecommendations to attend to mammography screening vary across countries and organizations, with the most common difference being the age at which screening should begin, and how frequently or if it should be performed, among women at typical risk for developing breast cancer. Some other organizations recommend mammograms begin as early as age 40 in normal-risk women, and take place more frequently, up to once each year. Women at higher risk may benefit from earlier or more frequent screening. Women with one or more first-degree relatives (mother, sister, daughter) with premenopausal breast cancer often begin screening at an earlier age, perhaps at an age 10 years younger than the age when the relative was diagnosed with breast cancer.\n\nElectrocardiograms are sometimes inappropriately used to screen low-risk patients with no symptoms for cardiac disease, perhaps as part of a routine annual exam. There is not much evidence that this test in low-risk individuals can improve health outcomes. False positive results, however, are likely to lead to follow-up invasive procedures, unnecessary further treatment, and a misdiagnosis. The harms of a non-indicated annual screening have been determined to outweigh the potential benefit, and for that reason, screening without an indication is discouraged.\n\nYoung athletes are sometimes screened with ECG as a requirement for them to play sports, and the necessity of this and harms from false positive results are debated.\n\nCardiac stress tests, including stress echocardiography and nuclear stress tests, are used to detect a block in blood flow to the heart. They do this by taking pictures of the heart while the heart is exercising. Persons who have symptoms of heart disease or who are high risk for a heart attack may need this test, while people without these symptoms and who are low risk generally do not.\n\nCoronary artery calcium scoring is a diagnosic test in the field of cardiovascular x-ray computed tomography. It is used to screen for coronary artery disease. Asymptomatic people who have low risk, including a lack of family history of premature coronary artery disease, should not be screened with this test. Coronary computed tomography angiography should not be used to screen people who are asymptomatic. Additionally, this test rarely provides insight which cannot be gained from coronary artery calcium scoring.\n\nOverscreening has been called \"unethical\".\n\n"}
{"id": "13876733", "url": "https://en.wikipedia.org/wiki?curid=13876733", "title": "Poems in the Waiting Room", "text": "Poems in the Waiting Room\n\nPoems in the Waiting Room (PitWR) is a U.K.-based and registered arts in health charity. The main aim of the charity is to supply short collections of poems for patients in National Health Service General Practice waiting rooms to read while waiting to see their doctor. The aim is to promote poetry, and to make the paient's wait more pleasant. The service is free to the waiting rooms and general practice managers, and is supported by grants and donations. The poems are presented as A4 sized three-fold cards typically reproducing between six and eight poems. Batches of cards are printed and distributed to waiting rooms four times a year. Patients are invited to take the cards away with them.\n\nAn additional service provided by the charity is 'PiTWR for Hospitals'. This provides larger print-runs of the poetry cards for distribution in hospitals. These are adapted to display the hospitals own message and sponsorship details.\n\nA key consideration for the charity is the selection of poems. Guidelines for the selection of poems have been devised with this in mind, and with help from a consultant psychiatrist as well as from poets. To quote the Editor \"In a patient centred health service, poetry arts in health too needs to be patient centred. The readers are patients – the worried well and the worried sick. The poems selected draw from the springs of well-being. In time of trouble, a measure of comfort is welcome\". The selection of poems is therefore different from, for example, the poetry that patients may themselves write as writing therapy. Poems selected for inclusion in PiTWR collections are a mix of contemporary work and poems from the canon of English poetry. Translations of poems from other traditions are also included. The essential is that they all contain positive images of hope, home, security, safe journey and arrival, beauty and transcendence, love and loving. The approach is indeed more akin to bibliotherapy rather than art therapy. Submissions from poets are encouraged, and a set of guidelines is provided to indicate the sort of poetry that meets the need of the charity.\n\nIn addition to the selection, production and distribution of the poetry cards the charity also undertakes research into the cost effectiveness of the scheme, and supports related arts in health initiatives. Recent work focusses on the extension of the scheme to support the production of special editions tailored for distribution in hospitals, rather than general practice waiting rooms. Collaborative work with other arts in health or literature based organisations, such as The Reader (magazine) is actively pursued.\n\nPoems in the Waiting Room, Registered Charity (No. 1099033), is incorporated as a company limited by guarantee (04836215) and is managed by the trustee body and executive committee, providing professional advice in literary and executive editing, production and distribution of the poetry cards.\n\nThe executive committee has been expanded since the retirement and death of the founder, Michael Lee, formerly Executive Chairman. The Chair is now held by Serge Lourie who took over on the retirement of Wendy French; Edmund Simon Lee is Honorary Chief Executive; Helen Lee is Honorary Secretary and became Editor on the retirement of Isobel Montgomery Campbell; Ciorsdan Glass is Executive Editor; Elizabeth Mary Anne Patience Consultant Analytical Psychologist; Michael Sheridan Stone is Director of PitWR Worldwide. \n\nThe UK-based charity has attracted wider attention, and encourages collaboration with projects set up in other countries, where these projects adopt the same editorial policy. Examples include Poems in the Waiting Room New Zealand established in 2008, based at Dunedin. \n\n"}
{"id": "12321153", "url": "https://en.wikipedia.org/wiki?curid=12321153", "title": "Qualifications for professional social work", "text": "Qualifications for professional social work\n\nProfessional social workers are generally considered those who hold a professional degree in social work. In a number of countries and jurisdictions, registration or licensure of people working as social workers is required and there are mandated qualifications. In other places, the professional association sets academic and experiential requirements for admission to membership.\n\nA social worker, practicing in the United States, usually requires a doctoral degree (Ph.D or DSW), master's degree (MSW) or a bachelor's degree (BSW or BASW) in social work from a Council on Social Work Education (CSWE) accredited program to receive a license in most states. In some areas, however, a social worker may be able to receive a license with a bachelor's or even associate degree in any discipline. The National Association of Social Workers (NASW) is the largest organization of professional social workers in the United States.\n\nDepending on the university, the four-year degree may be structured in different ways and draws upon many fields, including social work theory, psychology, human development, sociology, social policy, research methods, social planning and social administration.\n\nA person with a BSW is considered a \"generalist\" and the MSW is considered \"a specialist or advanced generalist\"; a Ph.D. or D.S.W. (Doctor of Social Work) generally conducts research, teaches, or analyzes policy, often in higher education settings.\n\nVarious states in the United States \"protect\" the use of the title social worker by statute. Use of the title requires licensure or certification in most states. The licensure or certification also requires a prelicensure examination through the ASWB (Association of Social Work Boards), with the exception of the State of California, who creates and administers their own licensing exam. Over half of all states offer licensure at various levels of social work practice, and clinical social work is regulated by licensure in all states. The pass rate for the Master's level licensing exam is around 74%.\n\nA four-year Bachelor of Social Work (BSW) is required for entry into the field in most parts of Canada. In Alberta, the entry-level requirement is the diploma in social work practice. A Master's degree in Social Work (MSW) is usually required to provide psychotherapy treatment. Authorized Social Workers with advanced clinical certification in Alberta, British Columbia and Saskatchewan are allowed to independently use the DSM (Diagnostic and Statistical Manual of Mental Disorders) in order to make a mental health diagnosis. These provinces hold a clinical registry for this purpose. In the province of Nova Scotia, MSW social workers can make provisional mental health diagnosis'.\n\nIn order to legally use the title \"social worker\", candidates must register with their provincial regulatory body. Some provinces also require an exam prerequisite for certification through the Association of Social Work Boards (ASWB).\n\nThe main qualification for social work is the undergraduate Bachelor's degree (BA, BSc or BSW) in social work, offered at British universities from September 2003 onwards. There is also available a master's degree (MA, MSc or MSW). These have replaced the previous qualifying award, the undergraduate Diploma in Social Work (DipSW), although the postgraduate counterpart, the postgraduate Diploma in Social Work (PGDipSW) is still awarded and allows the holder to register and practice as a social worker. The DipSW was first awarded in 1991 and phased out across the UK by 2009. Prior to this, the recognised qualification was the Certificate of Qualification in Social Work (CQSW), awarded between 1975 and 1991.\n\nPurporting to be either a social worker or a student social worker without registering with the relevant Social Work Register and holding or undergoing training for the recognised qualifications is now a criminal offence. Social workers must renew their registration every two years. These regulations offer protection to vulnerable people by guaranteeing the professional regulation of people working as social workers. They also promote workforce development, as all social workers must participate in at least fifteen days of professional training over a two-year period in order to be eligible for renewal of their registration.\n\nNon-registered or non-qualified social care practitioners in the United Kingdom, often referred to as Social Services Assistants, Child and Family Workers or Community Care Assistant or Community Care Workers (not to be confused with domiciliary or care home care workers), are unregistered social care practitioners that often do not hold any formal social work qualification and they must practice under the direct supervision of a registered social worker. This is not the case in Scotland where the scope of registration for social service workers is more advanced.\n\nWithin the mental health sector in the United Kingdom, social workers can train as an Approved Mental Health Professional and Approved Clinicians. With the implementation of the Mental Health Act 2007, this had replaced the previous Approved Social Worker role and is open to other professionals such as community psychiatric nurses, psychologists and occupational therapists, whilst maintaining a social work ethos. AMHPs are responsible for organising and contributing to assessments under the Mental Health Act 1983, as amended by the Mental Health Act 2007.\n\nAfter qualifying, social workers can undertake further training under the social work 'Post-Qualifying Framework'. Before 2007, there were four awards available under this framework:\n\nIn 2007, the General Social Care Council and UK partners implemented a new framework which unified these awards in a simpler structure allowing broader study to count towards three levels of social work award: specialist, higher specialist, and advanced.\n\nA four-year Bachelor of Social Work (BSW) is required for entry into the occupation of Social Worker in Australia, although some universities also offer a two-year, accelerated, graduate-entry MSW. Graduates of courses recognised by the Australian Association of Social Workers (AASW) are eligible for membership. A person with overseas qualifications can apply for consideration of recognition of their qualifications via a formal application for assessment by the AASW. Australia is alone among developed English-speaking OECD countries in having no registration requirements for social workers. Most employers stipulate that applicants must be \"eligible\" for membership of the AASW, and only graduates of courses recognised by the AASW are eligible for membership. However AASW membership is not compulsory and only a third of social workers are members. Continuing Professional Development (CPD) is an ongoing requirement of accredited membership of the AASW and must incorporate accountability, gaining new knowledge and information & skill development (CPD Policy 2011/12, AASW). No such requirement exists for non-members.\n\n\"Socionom\" is the Swedish and Danish term for a person with a degree in social work and related subjects. In Sweden the \"socionom\"/Bachelor of Social Work education is 3.5 years.\n"}
{"id": "25604", "url": "https://en.wikipedia.org/wiki?curid=25604", "title": "Radon", "text": "Radon\n\nRadon is a chemical element with symbol Rn and atomic number 86. It is a radioactive, colorless, odorless, tasteless noble gas. It occurs naturally in minute quantities as an intermediate step in the normal radioactive decay chains through which thorium and uranium slowly decay into lead and various other short-lived radioactive elements; radon itself is the immediate decay product of radium. Its most stable isotope, Rn, has a half-life of only 3.8 days, making radon one of the rarest elements since it decays away so quickly. However, since thorium and uranium are two of the most common radioactive elements on Earth, and they have three isotopes with very long half-lives, on the order of several billions of years, radon will be present on Earth long into the future in spite of its short half-life as it is continually being generated. The decay of radon produces many other short-lived nuclides known as radon daughters, ending at stable isotopes of lead.\n\nUnlike all the other intermediate elements in the aforementioned decay chains, radon is, under normal conditions, gaseous and easily inhaled. Radon gas is considered a health hazard. It is often the single largest contributor to an individual's background radiation dose, but due to local differences in geology, the level of the radon-gas hazard differs from location to location. Despite its short lifetime, radon gas from natural sources, such as uranium-containing minerals, can accumulate in buildings, especially, due to its high density, in low areas such as basements and crawl spaces. Radon can also occur in ground water – for example, in some spring waters and hot springs.\n\nEpidemiological studies have shown a clear link between breathing high concentrations of radon and incidence of lung cancer. Radon is a contaminant that affects indoor air quality worldwide. According to the United States Environmental Protection Agency, radon is the second most frequent cause of lung cancer, after cigarette smoking, causing 21,000 lung cancer deaths per year in the United States. About 2,900 of these deaths occur among people who have never smoked. While radon is the second most frequent cause of lung cancer, it is the number one cause among non-smokers, according to EPA estimates. As radon itself decays, it produces decay products, which are other radioactive elements called radon daughters (also known as radon progeny). Unlike the gaseous radon itself, radon daughters are solids and stick to surfaces, such as dust particles in the air. If such contaminated dust is inhaled, these particles can also cause lung cancer.\n\nRadon is a colorless, odorless, and tasteless gas and therefore is not detectable by human senses alone. At standard temperature and pressure, radon forms a monatomic gas with a density of 9.73 kg/m, about 8 times the density of the Earth's atmosphere at sea level, 1.217 kg/m. Radon is one of the densest gases at room temperature and is the densest of the noble gases. Although colorless at standard temperature and pressure, when cooled below its freezing point of , radon emits a brilliant radioluminescence that turns from yellow to orange-red as the temperature lowers. Upon condensation, radon glows because of the intense radiation it produces. Radon is sparingly soluble in water, but more soluble than lighter noble gases. Radon is appreciably more soluble in organic liquids than in water.\n\nBeing a noble gas, radon is chemically not very reactive. However, the 3.8-day half-life of radon-222 makes it useful in physical sciences as a natural tracer. Because radon is a gas at standard conditions, unlike its parents, it can readily be extracted from them for research.\n\nRadon is a member of the zero-valence elements that are called noble gases. It is inert to most common chemical reactions, such as combustion, because the outer valence shell contains eight electrons. This produces a stable, minimum energy configuration in which the outer electrons are tightly bound. 1037 kJ/mol is required to extract one electron from its shells (also known as the first ionization energy). In accordance with periodic trends, radon has a lower electronegativity than the element one period before it, xenon, and is therefore more reactive. Early studies concluded that the stability of radon hydrate should be of the same order as that of the hydrates of chlorine () or sulfur dioxide (), and significantly higher than the stability of the hydrate of hydrogen sulfide ().\n\nBecause of its cost and radioactivity, experimental chemical research is seldom performed with radon, and as a result there are very few reported compounds of radon, all either fluorides or oxides. Radon can be oxidized by powerful oxidizing agents such as fluorine, thus forming radon difluoride. It decomposes back to its elements at a temperature of above 250 °C, and is reduced by water to radon gas and hydrogen fluoride: it may also be reduced back to its elements by hydrogen gas. It has a low volatility and was thought to be . Because of the short half-life of radon and the radioactivity of its compounds, it has not been possible to study the compound in any detail. Theoretical studies on this molecule predict that it should have a Rn–F bond distance of 2.08 Å, and that the compound is thermodynamically more stable and less volatile than its lighter counterpart . The octahedral molecule was predicted to have an even lower enthalpy of formation than the difluoride. The higher fluorides RnF and RnF have been claimed, and are calculated to be stable, but it is doubtful whether they have yet been synthesized. The [RnF] ion is believed to form by the following reaction:\n\nFor this reason, antimony pentafluoride together with chlorine trifluoride and NFSbF have been considered for radon gas removal in uranium mines due to the formation of radon–fluorine compounds. The existence of RnF allows for safer handling of radon's parent radium as the fluoride, as the alpha radiation from Ra is not strong enough to cause radiolysis of the strong Ra–F bond; thus RaF decays to form involatile RnF. Additionally, salts of the [RnF] cation with the anions , , and are known. Radon is also oxidised by dioxygen difluoride to RnF at −100 °C.\n\nRadon oxides are among the few other reported ; only the trioxide (RnO) has been confirmed. Higher fluorides may have been observed in experiments where unknown radon-containing products distilled together with xenon hexafluoride, and perhaps in the production of radon trioxide: these may have been RnF, RnF, or both. Extrapolation down the noble gas group would suggest also the possible existence of RnO, RnO, and RnOF, as well as the first chemically stable noble gas chlorides RnCl and RnCl, but none of these have yet been found. Radon carbonyl RnCO has been predicted to be stable and to have a linear molecular geometry. The molecules and RnXe were found to be significantly stabilized by spin-orbit coupling. Radon caged inside a fullerene has been proposed as a drug for tumors. Despite the existence of Xe(VIII), no Rn(VIII) compounds have been claimed to exist; RnF should be highly unstable chemically (XeF is thermodynamically unstable). It is predicted that the most stable Rn(VIII) compound would be barium perradonate (BaRnO), analogous to barium perxenate. The instability of Rn(VIII) is due to the relativistic stabilization of the 6s shell, also known as the inert pair effect.\n\nRadon reacts with the liquid halogen fluorides ClF, ClF, ClF, BrF, BrF, and IF to form RnF. In halogen fluoride solution, radon is involatile and exists as the RnF and Rn cations; addition of fluoride anions results in the formation of the complexes and , paralleling the chemistry of beryllium(II) and aluminium(III). The standard electrode potential of the Rn/Rn couple has been estimated as +2.0 V, though there is no evidence for the formation of stable radon ions or compounds in aqueous solution.\n\nRadon has no stable isotopes. Thirty-seven radioactive isotopes have been characterized, with atomic masses ranging from 193 to 229. The most stable isotope is Rn, which is a decay product of Ra, a decay product of U. A trace amount of the (highly unstable) isotope Rn is also among the daughters of Rn.\n\nThree other radon isotopes have a half-life of over an hour: Rn, Rn and Rn. The Rn isotope is a natural decay product of the most stable thorium isotope (Th), and is commonly referred to as thoron. It has a half-life of 55.6 seconds and also emits alpha radiation. Similarly, Rn is derived from the most stable isotope of actinium (Ac)—named \"actinon\"—and is an alpha emitter with a half-life of 3.96 seconds. No radon isotopes occur significantly in the neptunium (Np) decay series, though a trace amount of the (extremely unstable) isotope Rn is produced.\n\nRn belongs to the radium and uranium-238 decay chain, and has a half-life of 3.8235 days. Its four first products (excluding marginal decay schemes) are very short-lived, meaning that the corresponding disintegrations are indicative of the initial radon distribution. Its decay goes through the following sequence:\n\nThe radon equilibrium factor is the ratio between the activity of all short-period radon progenies (which are responsible for most of radon's biological effects), and the activity that would be at equilibrium with the radon parent.\n\nIf a closed volume is constantly supplied with radon, the concentration of short-lived isotopes will increase until an equilibrium is reached where the rate of decay of each decay product will equal that of the radon itself. The equilibrium factor is 1 when both activities are equal, meaning that the decay products have stayed close to the radon parent long enough for the equilibrium to be reached, within a couple of hours. Under these conditions each additional pCi/L of radon will increase exposure, by 0.01 WL (Working Level -a measure of radioactivity commonly used in mining. A detailed explanation of WL is given in Concentration Units). These conditions are not always met; in many homes, the equilibrium fraction is typically 40%; that is, there will be 0.004 WL of daughters for each pCi/L of radon in air. Pb takes much longer (decades) to come in equilibrium with radon, but, if the environment permits accumulation of dust over extended periods of time, Pb and its decay products may contribute to overall radiation levels as well.\n\nBecause of their electrostatic charge, radon progenies adhere to surfaces or dust particles, whereas gaseous radon does not. Attachment removes them from the air, usually causing the equilibrium factor in the atmosphere to be less than one. The equilibrium factor is also lowered by air circulation or air filtration devices, and is increased by airborne dust particles, including cigarette smoke. In high concentrations, airborne radon isotopes contribute significantly to human health risk. The equilibrium factor found in epidemiological studies is 0.4.\n\nRadon was the fifth radioactive element to be discovered, in 1899 by Ernest Rutherford and Robert B. Owens, after uranium, thorium, radium and polonium. In 1900 Friedrich Ernst Dorn reported some experiments in which he noticed that radium compounds emanate a radioactive gas he named 'Radium Emanation' ('Ra Em'). Before that, in 1899, Pierre and Marie Curie observed that the gas emitted by radium remained radioactive for a month. Later that year, Robert B. Owens and Ernest Rutherford, at McGill University in Montreal, noticed variations when trying to measure radiation from thorium oxide. Rutherford noticed that the compounds of thorium continuously emit a radioactive gas that retains the radioactive powers for several minutes, and called this gas 'emanation' (from Latin \"emanare\"—to elapse and \"emanatio\"—expiration), and later \"Thorium Emanation\" (\"Th Em\"). In 1901, Rutherford and Harriet Brooks demonstrated that the emanations are radioactive, but credited the Curies for the discovery of the element. In 1903, similar emanations were observed from actinium by André-Louis Debierne and were called 'Actinium Emanation' ('Ac Em').\n\nSeveral shortened names were soon suggested for the three emanations: \"exradio\", \"exthorio\", and \"exactinio\" in 1904; \"radon\", \"thoron\", and \"akton\" in 1918; \"radeon\", \"thoreon\", and \"actineon\" in 1919, and eventually \"radon\", \"thoron\", and \"actinon\" in 1920. (The name radon is not related to that of the Austrian mathematician Johann Radon.) The likeness of the spectra of these three gases with those of argon, krypton, and xenon, and their observed chemical inertia led Sir William Ramsay to suggest in 1904 that the \"emanations\" might contain a new element of the noble gas family.\n\nIn the early part of the 20th century in the US, gold contaminated with the radon daughter Pb entered the jewelry industry. This was from gold seeds that had held Rn that had been melted down after the radon had decayed.\n\nIn 1909, Ramsay and Robert Whytlaw-Gray isolated radon, determined its melting temperature and approximate density. In 1910 they determined that it was the heaviest known gas. and wrote that \"L'expression de l'émanation du radium est fort incommode\", (the expression 'radium emanation' is very awkward) and suggested the new name niton (Nt) (from the Latin \"nitens\" meaning \"shining\") to emphasize the radioluminescence property, and in 1912 it was accepted by the International Commission for Atomic Weights. In 1923, the International Committee for Chemical Elements and International Union of Pure and Applied Chemistry (IUPAC) chose among the names radon (Rn), thoron (Tn), and actinon (An). Later, when isotopes were numbered instead of named, the element took the name of the most stable isotope, \"radon\", while Tn was renamed Rn and An was renamed Rn, which caused some confusion in the literature regarding the element's discovery as while Dorn had discovered radon the isotope, he had not been the first to discover radon the element. As late as the 1960s, the element was also referred to simply as \"emanation\". The first synthesized compound of radon, radon fluoride, was obtained in 1962. Even today, the word \"radon\" may refer to either the element or its isotope Rn, with \"thoron\" remaining in use as a short name for Rn to stem this ambiguity.\n\nThe danger of high exposure to radon in mines, where exposures can reach 1,000,000 Bq/m, has long been known. In 1530, Paracelsus described a wasting disease of miners, the \"mala metallorum\", and Georg Agricola recommended ventilation in mines to avoid this mountain sickness (\"Bergsucht\"). In 1879, this condition was identified as lung cancer by Harting and Hesse in their investigation of miners from Schneeberg, Germany. The first major studies with radon and health occurred in the context of uranium mining in the Joachimsthal region of Bohemia. In the US, studies and mitigation only followed decades of health effects on uranium miners of the Southwestern United States employed during the early Cold War; standards were not implemented until 1971.\n\nThe presence of radon in indoor air was documented as early as 1950. Beginning in the 1970s research was initiated to address sources of indoor radon, determinants of concentration, health effects, and mitigation approaches. In the United States, the problem of indoor radon received widespread publicity and intensified investigation after a widely publicized incident in 1984. During routine monitoring at a Pennsylvania nuclear power plant, a worker was found to be contaminated with radioactivity. A high concentration of radon in his home was subsequently identified as responsible.\n\nAll discussions of radon concentrations in the environment refer to Rn. While the average rate of production of Rn (from the thorium decay series) is about the same as that of Rn, the amount of Rn in the environment is much less than that of Rn because of the short half-life of Rn (55 seconds, versus 3.8 days respectively).\n\nRadon concentration in the atmosphere is usually measured in becquerel per cubic meter (Bq/m), the SI derived unit. Another unit of measurement common in the US is picocuries per liter (pCi/L); 1 pCi/L=37 Bq/m. Typical domestic exposures average about 48 Bq/m indoors, though this varies widely, and 15 Bq/m outdoors.\n\nIn the mining industry, the exposure is traditionally measured in \"working level\" (WL), and the cumulative exposure in \"working level month\" (WLM); 1 WL equals any combination of short-lived Rn daughters (Po, Pb, Bi, and Po) in 1 liter of air that releases 1.3 × 10 MeV of potential alpha energy; one WL is equivalent to 2.08 × 10 joules per cubic meter of air (J/m). The SI unit of cumulative exposure is expressed in joule-hours per cubic meter (J·h/m). One WLM is equivalent to 3.6 × 10 J·h/m. An exposure to 1 WL for 1 working month (170 hours) equals 1 WLM cumulative exposure. A cumulative exposure of 1 WLM is roughly equivalent to living one year in an atmosphere with a radon concentration of 230 Bq/m.\n\nRn decays to Pb and other radioisotopes. The levels of Pb can be measured. The rate of deposition of this radioisotope is weather-dependent.\n\nRadon concentrations found in natural environments are much too low to be detected by chemical means. A 1000 Bq/m (relatively high) concentration corresponds to 0.17 picogram per cubic meter. The average concentration of radon in the atmosphere is about 6 molar percent, or about 150 atoms in each ml of air. The radon activity of the entire Earth's atmosphere originates from only a few tens of grams of radon, consistently replaced by decay of larger amounts of radium and uranium.\n\nRadon is produced by the radioactive decay of radium-226, which is found in uranium ores, phosphate rock, shales, igneous and metamorphic rocks such as granite, gneiss, and schist, and to a lesser degree, in common rocks such as limestone. Every square mile of surface soil, to a depth of 6 inches (2.6 km to a depth of 15 cm), contains approximately 1 gram of radium, which releases radon in small amounts to the atmosphere. On a global scale, it is estimated that 2,400 million curies (90 EBq) of radon are released from soil annually.\n\nRadon concentration can differ widely from place to place. In the open air, it ranges from 1 to 100 Bq/m, even less (0.1 Bq/m) above the ocean. In caves or aerated mines, or ill-aerated houses, its concentration climbs to 20–2,000 Bq/m.\n\nRadon concentration can be much higher in mining contexts. Ventilation regulations instruct to maintain radon concentration in uranium mines under the \"working level\", with 95th percentile levels ranging up to nearly 3 WL (546 pCi Rn per liter of air; 20.2 kBq/m, measured from 1976 to 1985).\nThe concentration in the air at the (unventilated) Gastein Healing Gallery averages 43 kBq/m (1.2 nCi/L) with maximal value of 160 kBq/m (4.3 nCi/L).\n\nRadon mostly appears with the decay chain of the radium and uranium series (Rn), and marginally with the thorium series (Rn). The element emanates naturally from the ground, and some building materials, all over the world, wherever traces of uranium or thorium can be found, and particularly in regions with soils containing granite or shale, which have a higher concentration of uranium. Not all granitic regions are prone to high emissions of radon. Being a rare gas, it usually migrates freely through faults and fragmented soils, and may accumulate in caves or water. Owing to its very short half-life (four days for Rn), radon concentration decreases very quickly when the distance from the production area increases. Radon concentration varies greatly with season and atmospheric conditions. For instance, it has been shown to accumulate in the air if there is a meteorological inversion and little wind.\n\nHigh concentrations of radon can be found in some spring waters and hot springs. The towns of Boulder, Montana; Misasa; Bad Kreuznach, Germany; and the country of Japan have radium-rich springs that emit radon. To be classified as a radon mineral water, radon concentration must be above 2 nCi/L (74 kBq/m). The activity of radon mineral water reaches 2,000 kBq/m in Merano and 4,000 kBq/m in Lurisia (Italy).\n\nNatural radon concentrations in the Earth's atmosphere are so low that radon-rich water in contact with the atmosphere will continually lose radon by volatilization. Hence, ground water has a higher concentration of Rn than surface water, because radon is continuously produced by radioactive decay of Ra present in rocks. Likewise, the saturated zone of a soil frequently has a higher radon content than the unsaturated zone because of diffusional losses to the atmosphere.\n\nIn 1971, Apollo 15 passed 110 km (68 mi) above the Aristarchus plateau on the Moon, and detected a significant rise in alpha particles thought to be caused by the decay of Rn. The presence of Rn has been inferred later from data obtained from the Lunar Prospector alpha particle spectrometer.\n\nRadon is found in some petroleum. Because radon has a similar pressure and temperature curve to propane, and oil refineries separate petrochemicals based on their boiling points, the piping carrying freshly separated propane in oil refineries can become radioactive because of decaying radon and its products.\n\nResidues from the petroleum and natural gas industry often contain radium and its daughters. The sulfate scale from an oil well can be radium rich, while the water, oil, and gas from a well often contains radon. Radon decays to form solid radioisotopes that form coatings on the inside of pipework.\n\nHigh concentrations of radon in homes were discovered by chance in 1985 after the stringent radiation testing conducted at a nuclear power plant entrance revealed that Stanley Watras, an engineer at the plant, was contaminated by radioactive substances. Typical domestic exposures are of approximately 100 Bq/m (2.7 pCi/L) indoors. Some level of radon will be found in all buildings. Radon mostly enters a building directly from the soil through the lowest level in the building that is in contact with the ground. High levels of radon in the water supply can also increase indoor radon air levels. Typical entry points of radon into buildings are cracks in solid foundations, construction joints, cracks in walls, gaps in suspended floors, gaps around service pipes, cavities inside walls, and the water supply. Radon concentrations in the same location may differ by a factor of two over a period of 1 hour. Also, the concentration in one room of a building may be significantly different from the concentration in an adjoining room. The soil characteristics of the dwellings are the most important source of radon for the ground floor and higher concentration of indoor radon observed on lower floors. Most of the high radon concentrations have been reported from places near fault zones; hence the existence of a relation between the exhalation rate from faults and indoor radon concentrations is obvious.\n\nThe distribution of radon concentrations will generally differ from room to room, and the readings are averaged according to regulatory protocols. Indoor radon concentration is usually assumed to follow a lognormal distribution on a given territory. Thus, the geometric mean is generally used for estimating the \"average\" radon concentration in an area.\n\nThe mean concentration ranges from less than 10 Bq/m to over 100 Bq/m in some European countries. Typical geometric standard deviations found in studies range between 2 and 3, meaning (given the 68–95–99.7 rule) that the radon concentration is expected to be more than a hundred times the mean concentration for 2 to 3% of the cases.\n\nSome of the highest radon hazard in the United States is found in Iowa and in the Appalachian Mountain areas in southeastern Pennsylvania. The second highest readings in Ireland were found in office buildings in the Irish town of Mallow, County Cork, prompting local fears regarding lung cancer. Iowa has the highest average radon concentrations in the United States due to significant glaciation that ground the granitic rocks from the Canadian Shield and deposited it as soils making up the rich Iowa farmland. Many cities within the state, such as Iowa City, have passed requirements for radon-resistant construction in new homes.\n\nIn a few locations, uranium tailings have been used for landfills and were subsequently built on, resulting in possible increased exposure to radon.\n\nSince radon is a colorless, odorless gas the only way to know how much is present in the air or water is to perform tests. In the United States radon test kits are available to the public at retail stores, such as hardware stores, for home use and testing is available through licensed professionals, who are often home inspectors. Efforts to reduce indoor radon levels are called radon mitigation. In the U.S. the Environmental Protection Agency recommends all houses be tested for radon.\n\nRadon is obtained as a by-product of uraniferous ores processing after transferring into 1% solutions of hydrochloric or hydrobromic acids. The gas mixture extracted from the solutions contains , , He, Rn, , and hydrocarbons. The mixture is purified by passing it over copper at 720 °C to remove the and the , and then KOH and are used to remove the acids and moisture by sorption. Radon is condensed by liquid nitrogen and purified from residue gases by sublimation.\n\nRadon commercialization is regulated, but it is available in small quantities for the calibration of Rn measurement systems, at a price of almost $6,000 per milliliter of radium solution (which only contains about 15 picograms of actual radon at a given moment).\nRadon is produced by a solution of radium-226 (half-life of 1600 years). Radium-226 decays by alpha-particle emission, producing radon that collects over samples of radium-226 at a rate of about 1 mm/day per gram of radium; equilibrium is quickly achieved and radon is produced in a steady flow, with an activity equal to that of the radium (50 Bq). Gaseous Rn (half-life of about four days) escapes from the capsule through diffusion.\n\nAn early-20th-century form of quackery was the treatment of maladies in a radiotorium. It was a small, sealed room for patients to be exposed to radon for its \"medicinal effects\". The carcinogenic nature of radon due to its ionizing radiation became apparent later on. Radon's molecule-damaging radioactivity has been used to kill cancerous cells, but it does not increase the health of healthy cells. The ionizing radiation causes the formation of free radicals, which results in genetic and other cell damage, resulting in increased rates of illness, including cancer.\n\nExposure to radon, a process known as radiation hormesis, has been suggested to mitigate autoimmune diseases such as arthritis. As a result, in the late 20th century and early 21st century, \"health mines\" established in Basin, Montana attracted people seeking relief from health problems such as arthritis through limited exposure to radioactive mine water and radon. The practice is discouraged because of the well-documented ill effects of high-doses of radiation on the body.\n\nRadioactive water baths have been applied since 1906 in Jáchymov, Czech Republic, but even before radon discovery they were used in Bad Gastein, Austria. Radium-rich springs are also used in traditional Japanese onsen in Misasa, Tottori Prefecture. Drinking therapy is applied in Bad Brambach, Germany. Inhalation therapy is carried out in Gasteiner-Heilstollen, Austria, in Świeradów-Zdrój, Czerniawa-Zdrój, Kowary, Lądek Zdrój, Poland, in Harghita Băi, Romania, and in Boulder, United States. In the US and Europe there are several \"radon spas\", where people sit for minutes or hours in a high-radon atmosphere in the belief that low doses of radiation will invigorate or energize them.\n\nRadon has been produced commercially for use in radiation therapy, but for the most part has been replaced by radionuclides made in accelerators and nuclear reactors. Radon has been used in implantable seeds, made of gold or glass, primarily used to treat cancers.\nThe gold seeds were produced by filling a long tube with radon pumped from a radium source, the tube being then divided into short sections by crimping and cutting. The gold layer keeps the radon within, and filters out the alpha and beta radiations, while allowing the gamma rays to escape (which kill the diseased tissue). The activities might range from 0.05 to 5 millicuries per seed (2 to 200 MBq). The gamma rays are produced by radon and the first short-lived elements of its decay chain (Po, Pb, Bi, Po).\n\nRadon and its first decay products being very short-lived, the seed is left in place. After 12 half-lives (43 days), radon radioactivity is at 1/2000 of its original level. At this stage, the predominant residual activity originates from the radon decay product Pb, whose half-life (22.3 years) is 2000 times that of radon (and whose activity is thus 1/2000 of radon's), and its descendants Bi and Po.\n\nRadon emanation from the soil varies with soil type and with surface uranium content, so outdoor radon concentrations can be used to track air masses to a limited degree. This fact has been put to use by some atmospheric scientists. Because of radon's rapid loss to air and comparatively rapid decay, radon is used in hydrologic research that studies the interaction between groundwater and streams. Any significant concentration of radon in a stream is a good indicator that there are local inputs of groundwater.\n\nRadon soil-concentration has been used in an experimental way to map buried close-subsurface geological faults because concentrations are generally higher over the faults. Similarly, it has found some limited use in prospecting for geothermal gradients.\n\nSome researchers have investigated changes in groundwater radon concentrations for earthquake prediction. Radon has a half-life of approximately 3.8 days, which means that it can be found only shortly after it has been produced in the radioactive decay chain. For this reason, it has been hypothesized that increases in radon concentration is due to the generation of new cracks underground, which would allow increased ground water circulation, flushing out radon. The generation of new cracks might not unreasonably be assumed to precede major earthquakes. In the 1970s and 1980s, scientific measurements of radon emissions near faults found that earthquakes often occurred with no radon signal, and radon was often detected with no earthquake to follow. It was then dismissed by many as an unreliable indicator. As of 2009, it was under investigation as a possible precursor by NASA.\n\nRadon is a known pollutant emitted from geothermal power stations because it is present in the material pumped from deep underground. It disperses rapidly, and no radiological hazard has been demonstrated in various investigations. In addition, typical systems re-inject the material deep underground rather that releasing it at the surface, so its environmental impact is minimal.\n\nIn the 1940s and '50s, radon was used for industrial radiography, Other X-ray sources, which became available after World War II, quickly replaced radon for this application, as they were lower in cost and had less hazard of alpha radiation.\n\nRadon-222 decay products have been classified by the International Agency for Research on Cancer as being carcinogenic to humans, and as a gas that can be inhaled, lung cancer is a particular concern for people exposed to elevated levels of radon for sustained periods. During the 1940s and '50s, when safety standards requiring expensive ventilation in mines were not widely implemented, radon exposure was linked to lung cancer among non-smoking miners of uranium and other hard rock materials in what is now the Czech Republic, and later among miners from the Southwestern United States and South Australia. Despite these hazards being known in the early 1950s, this occupational hazard remained poorly managed in many mines until the 1970s. During this period, several entrepreneurs opened former uranium mines in the US to the general public and advertised alleged health benefits from breathing radon gas underground. Health benefits claimed included pain, sinus, asthma and arthritis relief but these were proven to be false and the government banned such ads in 1975.\n\nSince that time, ventilation and other measures have been used to reduce radon levels in most affected mines that continue to operate. In recent years, the average annual exposure of uranium miners has fallen to levels similar to the concentrations inhaled in some homes. This has reduced the risk of occupationally induced cancer from radon, although health issues may persist for those who are currently employed in affected mines and for those who have been employed in them in the past. As the relative risk for miners has decreased, so has the ability to detect excess risks among that population.\n\nResidues from processing of uranium ore can also be a source of radon. Radon resulting from the high radium content in uncovered dumps and tailing ponds can be easily released into the atmosphere and affect people living in the vicinity.\n\nIn addition to lung cancer, researchers have theorized a possible increased risk of leukemia due to radon exposure. Empirical support from studies of the general population is inconsistent, and a study of uranium miners found a correlation between radon exposure and chronic lymphocytic leukemia.\n\nMiners (as well as milling and ore transportation workers) who worked in the uranium industry in the United States between the 1940s and 1971 may be eligible for compensation under the Radiation Exposure Compensation Act (RECA). Surviving relatives may also apply in cases where the formerly employed person is deceased.\n\nRadon exposure (mostly radon daughters) has been linked to lung cancer in numerous case-control studies performed in the United States, Europe and China. There are approximately 21,000 deaths per year in the US due to radon-induced lung cancers. One of the most comprehensive radon studies performed in the United States by Dr. R. William Field and colleagues found a 50% increased lung cancer risk even at the protracted exposures at the EPA's action level of 4 pCi/L. North American and European Pooled analyses further support these findings. However, the discussion about the opposite results is still continuing, especially a recent retrospective case-control study of lung cancer risk which showed substantial cancer rate reduction for radon concentrations between 50 and 123 Bq per cubic meter.\n\nMost models of residential radon exposure are based on studies of miners, and direct estimates of the risks posed to homeowners would be more desirable. Because of the difficulties of measuring the risk of radon relative to smoking, models of their effect have often made use of them.\n\nRadon has been considered the second leading cause of lung cancer and leading environmental cause of cancer mortality by the United States Environmental Protection Agency. Others have reached similar conclusions for the United Kingdom and France. Radon exposure in homes and offices may arise from certain subsurface rock formations, and also from certain building materials (e.g., some granites). The greatest risk of radon exposure arises in buildings that are airtight, insufficiently ventilated, and have foundation leaks that allow air from the soil into basements and dwelling rooms.\n\nWHO presented in 2009 a recommended reference level (the national reference level), 100 Bq/m, for radon in dwellings. The recommendation also says that where this is not possible, 300 Bq/m should be selected as the highest level. A national reference level should not be a limit, but should represent the maximum acceptable annual average radon concentration in a dwelling.\n\nThe actionable concentration of radon in a home varies depending on the organization doing the recommendation, for example, the United States Environmental Protection Agency encourages that action be taken at concentrations as low as 74 Bq/m (2 pCi/L), and the European Union recommends action be taken when concentrations reach 400 Bq/m (11 pCi/L) for old houses and 200 Bq/m (5 pCi/L) for new ones. On 8 July 2010 the UK's Health Protection Agency issued new advice setting a \"Target Level\" of 100 Bq/m whilst retaining an \"Action Level\" of 200 Bq/m. The same levels (as UK) apply to Norway from 2010; in all new housings preventative measures should be taken against radon accumulation.\n\nResults from epidemiological studies indicate that the risk of lung cancer increases with exposure to residential radon. A well-known example of source of error is smoking, the main risk factor for lung cancer. In the West, tobacco smoke is estimated to cause about 90% of all lung cancers.\n\nAccording to the EPA, the risk of lung cancer for smokers is significant due to synergistic effects of radon and smoking. For this population about 62 people in a total of 1,000 will die of lung cancer compared to 7 people in a total of 1,000 for people who have never smoked. It cannot be excluded that the risk of non-smokers should be primarily explained by a combination effect of radon and passive smoking (see below).\n\nRadon, like other known or suspected external risk factors for lung cancer, is a threat for smokers and former smokers. This was demonstrated by the European pooling study. A commentary to the pooling study stated: \"it is not appropriate to talk simply of a risk from radon in homes. The risk is from smoking, compounded by a synergistic effect of radon for smokers. Without smoking, the effect seems to be so small as to be insignificant.\"\n\nAccording to the European pooling study, there is a difference in risk from radon between histological types. Small cell lung carcinoma, which practically only affects smokers have high risk from radon. For other histological types such as adenocarcinoma, the type that primarily affects never smokers, the risk from radon appears to be lower.\n\nA study of radiation from post mastectomy radiotherapy shows that the simple models previously used to assess the combined and separate risks from radiation and smoking need to be developed. This is also supported by new discussion about the calculation method, LNT, which routinely has been used.\n\nAn important, but unanswered question concerns the possibility that the cancer risk from passive smoking can increase with exposure to residential radon. The basic data for the European pooling study makes it impossible to exclude that such synergy effect is an explanation for the (very limited) increase in the risk from radon that was stated for non-smokers.\n\nA study from 2001, which included 436 cases (never smokers who had lung cancer), and a control group (1649 never smokers) showed that exposure to radon increased the risk of lung cancer in never smokers. But the group that had been exposed to passive smoking at home appeared to bear the entire risk increase, while those who were not exposed to passive smoking did not show any increased risk with increasing radon level.\n\nThe effects of radon if ingested are similarly unknown, although studies have found that its biological half-life ranges from 30–70 minutes, with 90 percent removal at 100 minutes. In 1999 National Research Council investigated the issue of radon in drinking water. The risk associated with ingestion was considered almost negligible. Water from underground sources may contain significant amounts of radon depending on the surrounding rock and soil conditions, whereas surface sources generally do not.\n\nAs well as being ingested through drinking water, radon is also released from water when temperature is increased, pressure is decreased and when water is aerated. Optimum conditions for radon release and exposure occur during showering. Water with a radon concentration of 10 pCi/L can increase the indoor airborne radon concentration by 1 pCi/L under normal conditions.\n\nThere are relatively simple tests for radon gas. In some countries these tests are methodically done in areas of known systematic hazards. Radon detection devices are commercially available. Digital radon detectors provide ongoing measurements giving both daily, weekly, short-term and long-term average readouts via a digital display. Short-term radon test devices used for initial screening purposes are inexpensive, in some cases free. There are important protocols for taking short-term radon tests and it is imperative that they be strictly followed. The kit includes a collector that the user hangs in the lowest habitable floor of the house for 2 to 7 days. The user then sends the collector to a laboratory for analysis. Long term kits, taking collections for up to one year or more, are also available. An open-land test kit can test radon emissions from the land before construction begins. Radon concentrations can vary daily, and accurate radon exposure estimates require long-term average radon measurements in the spaces where an individual spends a significant amount of time.\n\nRadon levels fluctuate naturally, due to factors like transient weather conditions, so an initial test might not be an accurate assessment of a home's average radon level. Radon levels are at a maximum during the coolest part of the day when pressure differentials are greatest. Therefore, a high result (over 4 pCi/L) justifies repeating the test before undertaking more expensive abatement projects. Measurements between 4 and 10 pCi/L warrant a long term radon test. Measurements over 10 pCi/L warrant only another short term test so that abatement measures are not unduly delayed. Purchasers of real estate are advised to delay or decline a purchase if the seller has not successfully abated radon to 4 pCi/L or less.\n\nBecause the half-life of radon is only 3.8 days, removing or isolating the source will greatly reduce the hazard within a few weeks. Another method of reducing radon levels is to modify the building's ventilation. Generally, the indoor radon concentrations increase as ventilation rates decrease. In a well ventilated place, the radon concentration tends to align with outdoor values (typically 10 Bq/m, ranging from 1 to 100 Bq/m).\n\nThe four principal ways of reducing the amount of radon accumulating in a house are:\n\nAccording to the EPA the method to reduce radon \"...primarily used is a vent pipe system and fan, which pulls radon from beneath the house and vents it to the outside\", which is also called sub-slab depressurization, active soil depressurization, or soil suction. Generally indoor radon can be mitigated by sub-slab depressurization and exhausting such radon-laden air to the outdoors, away from windows and other building openings. \"EPA generally recommends methods which prevent the entry of radon. Soil suction, for example, prevents radon from entering your home by drawing the radon from below the home and venting it through a pipe, or pipes, to the air above the home where it is quickly diluted\" and \"EPA does not recommend the use of sealing alone to reduce radon because, by itself, sealing has not been shown to lower radon levels significantly or consistently\".\n\nPositive-pressure ventilation systems can be combined with a heat exchanger to recover energy in the process of exchanging air with the outside, and simply exhausting basement air to the outside is not necessarily a viable solution as this can actually draw radon gas \"into\" a dwelling. Homes built on a crawl space may benefit from a radon collector installed under a \"radon barrier\" (a sheet of plastic that covers the crawl space).\nFor crawlspaces, the EPA states \"An effective method to reduce radon levels in crawlspace homes involves covering the earth floor with a high-density plastic sheet. A vent pipe and fan are used to draw the radon from under the sheet and vent it to the outdoors. This form of soil suction is called submembrane suction, and when properly applied is the most effective way to reduce radon levels in crawlspace homes.\"\n\n"}
{"id": "11628886", "url": "https://en.wikipedia.org/wiki?curid=11628886", "title": "Shawn Hornbeck Foundation", "text": "Shawn Hornbeck Foundation\n\nUntil 2013, the Shawn Hornbeck Foundation was a non-profit charitable organization based in Richwoods, Missouri, devoted to the search for and rescue of abducted children. It ran the Shawn Hornbeck Search and Rescue Team.\n\nThe rescue team was founded by Pam and Craig Akers following the disappearance of their son Shawn Hornbeck. Hornbeck was eleven years old on October 6, 2002, when he was kidnapped while riding his bicycle near his home in Richwoods, Missouri. Shawn Hornbeck was missing for over four years before being discovered on January 12, 2007. He had been kidnapped by Michael J. Devlin.\n\nThe Shawn Hornbeck Search and Rescue Team was a member of NASAR (National Association for Search and Rescue) and a member of SARCOM (Search and Rescue Council of Missouri). It was also involved with the National Search Dog Association.\n\nThe Akers founded the Shawn Hornbeck Search and Rescue Team following the disappearance of their son Shawn Hornbeck. Hornbeck was eleven years old when he was kidnapped while riding his bicycle near his Richwoods, Missouri, home on October 6, 2002.\n\nShortly after Hornbeck's disappearance, his parents appeared on \"The Montel Williams Show\", where self-described psychic Sylvia Browne told the Akers that Hornbeck was dead. Browne also described the abduction, telling them several things about the abductor that later proved to be incorrect.\n\nHornbeck was missing for over four years before being discovered on January 12, 2007. Police were searching for 13-year-old Ben Ownby of Beaufort, Missouri, who had recently gone missing. Aided by a descriptive tip from teenager Mitchell Hults of Beaufort, Missouri, police searched Michael J. Devlin's apartment in Kirkwood, Missouri. Hornbeck and Ownby were both found there.\n\nIn June 2007, Devlin was charged with 78 counts in the abductions and molestations of Hornbeck and Ownby. On October 8, 2007, Devlin pleaded guilty to all charges filed against him and was sentenced to life imprisonment.\n\nShortly after the rescue of Shawn Hornbeck, his parents announced their withdrawal from active involvement with the foundation. Its operation would be headed by family friend and co-founder Chris Diamond. Hornbeck is attending community college and has shown an interest in studying fields where he could work with children, such as psychology or social work. He gave advice to the family of kidnapping victim Jaycee Dugard, saying she needed time alone and that the impact of such events was life-changing and not easily comprehended by those who have not been directly affected by such occurrences.\n"}
{"id": "11188627", "url": "https://en.wikipedia.org/wiki?curid=11188627", "title": "Soroka Medical Center", "text": "Soroka Medical Center\n\nSoroka University Medical Center (Hebrew: המרכז הרפואי סורוקה, HaMerkaz HaRefu'i Soroka), part of the Clalit Health Services Group, is the only hospital in the Negev. Located in the city of Beersheba, Israel, it serves as the central hospital of the region and provides medical services to approximately one million residents of the South, from Kiryat Gat and Ashkelon to Eilat. Soroka is the third largest hospital in Israel, with 1,087 hospital beds, and it is spread over an area of in the center of Beer-Sheva. \n\nSoroka provides medical care to members of all populations in the region, including Negev Bedouins and Palestinians from the West Bank and Gaza Strip. It is a teaching hospital affiliated with the faculty of Health Sciences at Ben-Gurion University of the Negev whose campus is adjacent to the hospital.\n\nFollowing the War of Independence, the Medical Corps established a temporary military hospital in one of the Ottoman government buildings in Beersheva. A year later, the hospital was transferred to a British government compound, where it was run by the Hadassah Medical Association and named after Dr. Chaim Yassky. \n\nIn 1949, Clalit Health Fund of the Hebrew Workers in Eretz Israel opened a clinic in the city to serve citizens who were members of the Histadrut. This clinic required hospital services for continued treatment. The nearest hospital was the Kaplan Medical Center in Rehovot, but it was relatively far away, and patients had to travel and endure the poor road conditions of the time until they received treatment.\n\nIn 1950, Beersheva was declared a civic authority, with the result that thousands of immigrants went to settle there. With the increase in the number of residents, the existing small hospital was unable to answer the needs of the population.\n\nHadassah expressed willingness to expand its facilities, but due to budgetary constraints caused by the construction of Hadassah Hospital in Jerusalem they were not able to expand the hospital within a reasonable amount of time. \n\nDavid Ben-Gurion, with his national approach, thought that the government should establish a hospital in the Negev and that it should not be established by Hadassah or the Histadrut, but the Health Ministry had no funds to invest in this effort.\n\nDavid Tuviyahu, who served as mayor of the city of Beersheva, joined the effort to establish a larger, more spacious and modern hospital. For this purpose, he met with various individuals, among them Moshe Soroka, chairman of the Clalit Health Services. Soroka expressed his willingness in principle for the Histadrut Health Fund to establish a hospital, but Minister of Health Yosef Serlin, who aspired to reduce the activity of the fund and transfer it to the state, objected to this idea. \n\nIn August 1955, Dov Begun, representative of the Histadrut in the United States, convinced the president of the International Ladies' Garment Workers' Union, David Dubinsky (1892–1982) to donate 1 million USD (a quarter million every year for four years) toward establishing a hospital in the Negev that would commemorate the organization's name.\nAccording to press reports at the time, Dubinsky had indicated that the ILGWU might make a further $500,000 available to the hospital after the $1,000,000 contribution was completed, as the estimated construction cost was $1,500,000..\n\nAt the end of that year, a new government was formed and Yisrael Barzilai was appointed minister of health. He supported the establishment of the hospital and convinced Ben-Gurion to allow Clalit to set up a new hospital in Beersheva, while the Ministry of Health would establish one in Ashkelon (now Barzilai Medical Center). One of the things that convinced Ben-Gurion was his fear that the planned Sinai War would result in a shortage of hospital beds for wounded soldiers.\n\nOn July 23, 1956, ground was broken on the new hospital. The hospital building was designed by architects Arieh Sharon and Benjamin Idelson. \n\nIn October 1959, the opening ceremony of the Central Hospital of the Negev was held. At first, the hospital contained several vital departments: the General Surgery Department (in the framework of which were the Otolaryngology Department, the Ophthalmology Department, and the Urology Department), two internal medicine departments, the Orthopedic Department, the Cardiology Institute, and the Radiology Institute. Later, additional departments were opened. \n\nAfter the death of Moshe Soroka, the director of Clalit Health Organization in the 1950s who played a significant role in establishing the hospital, it was decided to name the hospital in his memory.\nIn 2018, Shlomi Codish was named director-general of the hospital replacing Ehud Davidson, who held the post for five years.\n\nThe hospital covers an area of 291 dunams, with a constructed area of more than 200,000 square meters, and includes 30 buildings.\n\nAmong the buildings on campus are the following: \n\n\nSoroka Medical Center has over 40 inpatient departments and 1,087 hospital beds. In addition to the hospital departments, there are dozens of other units that provide services to hospitalized and ambulatory patients, in the Emergency Medicine Department, institutes, and outpatient clinics.\n\nSoroka's Department of Emergency Medicine, with the largest volume of activity in Israel (over 240,000 visits annually), is the leading such department in the country according to a health care survey on service and quality conducted by the Ministry of Health.\n\nSoroka's delivery room has the most births of any in the country – over 17,000 babies are born every year.\n\nIn 2017, over 30,000 surgeries were performed at the hospital and 98,000 hospitalizations took place. There were over 590,000 visits to the outpatient clinics.\n\nSoroka has some 4,600 employees, including approximately 900 doctors, 1,600 nurses, 500 health workers and 500 administrative employees.\n\nSoroka Medical Center provides medical services to more than one million residents of the Negev who reside in a vast geographical area that comprises 60% of the country. The population of the Negev is relatively young, culturally diverse, largely of low to middle-level socioeconomic status, and has unique health needs.\n\nUnique populations cared for at Soroka include Bedouins, who make up nearly a third of the population and large groups of immigrants from Ethiopia and the former Soviet Union. \n\n\n\nSoroka Medical Center is a university medical center that maintains close ties with Ben-Gurion University of the Negev (BGU). In this framework, the hospital staff partners in training the future generation of clinicians and managers in the health system in various fields: medicine, nursing, physiotherapy, emergency medicine, pharmacy, medical laboratory, public health, and health systems management. Approximately 1,000 students study at the hospital annually. The campus of BGU's Faculty of Health Sciences is located in the hospital compound.\n\nMany clinical trials approved by the Helsinki Committee are conducted at Soroka. As of 2018, the committee is chaired by Prof. Reli Hershkovitz.\n\nA center for clinical research operates at Soroka, leading and promoting research with hospital staff and colleagues outside of the hospital in Israel and abroad, sometimes in cooperation with BGU.\n\nEvery year, approximately 300 new studies are approved at the hospital, and some 300 articles on research of clinical and managerial significance have been published in the scientific literature.\n\nSoroka Medical Center has accumulated extensive experience over the years in managing various emergency situations.\n\n\nThe emergency events are managed by a dedicated staff at the hospital in cooperation with many bodies in accordance with the nature of the event: the IDF, the Israel Police, Magen David Adom, the Home Front Command, other hospitals, and more.\n\nSoroka Medical Center is located in the community it serves and works to develop and promote programs aimed at improving quality of life and community health. The staff members of Soroka take part in a wide variety of social activities within and outside of the hospital. The activities are adapted to different target audiences.\nAmong the activities:\n• Soroka at the Bar – a series of lectures by hospital experts on various subjects for the general public \n• Ushpizin – lectures by medical staff from various fields of medicine for high school students and tours of various departments\n• Accompanying and Assisting Holocaust Survivors – Soroka staff members visit Holocaust survivors who are hospitalized and also conduct weekly visits to the homes of survivors. Soroka employees volunteer at Amcha clubs in various fields of activity.\n\n\n"}
{"id": "47938111", "url": "https://en.wikipedia.org/wiki?curid=47938111", "title": "Swedish Federation", "text": "Swedish Federation\n\nSvenska Federationen (Swedish Federation), was the Swedish branch of the British Ladies National Association for the Repeal of the Contagious Diseases Acts. It was established in 1878 with the purpose to repeal the so-called reglementation system, which required prostitute women to registration and regular medical examination to prevent sexually transmitted infections. It also opposed the sexual double standard, which regarded men as naturally unable to sexual self-control and viewed prostitutes as the sole problem. The \"Svenska Federationen\" was dissolved after the reglementation system was abolished in 1918. Between 1878 and 1905, it published its own paper, \"Sedlighetsvännen\" (Friend of Virtue).\n\nIn 1812, a new law allowed for forced medical examination of people suspected for suffering of a sexually transmitted infection to prevent the spreading of disease. The law did not specify gender, but it was in practice mostly forced upon women in the capital suspected of prostitution. The reason for this was the contemporary sexual view that men were incapable of sexual self-control, and the focus of the authorities were therefore placed on prostitute women, who were described as the problem in the battle against sexual disease. However, the law was met with rising opposition, as the police harassed women on mere suspicion of being prostitutes, such as for example bar waitresses, and a different method was therefore deemed necessary. E experiment with state licensed brothels, London and Stadt Hamburg, failed in 1838–41. In 1847, the first bureau for registration and regular weekly medical examinations of prostitutes were founded in Stockholm: in 1859, all prostitutes were forced to registration at such a bureau. This was not a national law: it was a question for the local city authorities, and after it was introduced in the capital, some Swedish cities, but not all, followed: the second was Gothenburg in 1865.\n\n\"Svenska Federationen\" was founded by A. Testuz, vicar of the French Reformed Church in Stockholm in February 1878. It was inspired by the British Ladies National Association for the Repeal of the Contagious Diseases Acts of Josephine Butler. It was organized in 1879 and formed its rules in Stockholm 24 November 1880. Known members was Ellen Bergman, a key front figure of the organization, and Karolina Widerström, Sweden's first woman physician and chairperson of the National Association for Women's Suffrage.\n\n\"Svenska Federationen\" deemed the regulation system as humiliating and socially stigmatizing: after being registered, the prostitutes were made to hand in their passport and exchange it for control books which were stamped after their weekly examination. This branded them as prostitutes which was at the time a social stigma, and prevented them from leaving town. They could only be removed from the registration bureau by a statement that they no longer engaged in prostitution, signed by an employer or a husband.\n\n\"Svenska Federationen\" also opposed the justification of this regulation, which was the contemporary sexual double standards that branded men as incapable of sexual self-control and that prostitutes were therefore necessary to protect other women from rape, which resulted in the view that it was the prostitute women who was solely blamed for the spreading of sexual disease: the organisation wished to shift the blame from prostitutes to their male customers. This attempt to change contemporary sexual standard and opposition to double standards were conducted in parallel with the \"Sedlighetsdebatten\".\n\nSince its foundation in 1878, \"Svenska Federationen\" published its own paper, and worked actively on public opinion through meetings, debate and publications. It also founded a Sunday club, a fund, hostels, working homes and employment agencies to help women who engaged in prostitution or were about to become so. It attempted to organize in all of the Swedish cities were the regulation system was in practice, as well as in Denmark and Norway, but most of its activity was to remain restricted to the capital of Stockholm.\n\nThe \"Svenska Federationen\" presented petitions to the governor of Stockholm in 1880 and 1902, and to the monarch in 1883. In 1903, a Motion (parliamentary procedure) was made in the Riksdag to investigate a method other than the reglementation of prostitutes to stop the spreading of Sexually transmitted infection. A government committee was formed, which presented their suggestion in 1910. The reglementation system was finally abolished in Sweden by the \"Lex Veneris\" Act of 1918. At the time of its abolition, the reglementation was already abolished in all cities where it had occurred except in the capital itself.\n\n"}
{"id": "10391870", "url": "https://en.wikipedia.org/wiki?curid=10391870", "title": "Swedish interactive thresholding algorithm", "text": "Swedish interactive thresholding algorithm\n\nUsually referred to as SITA, the Swedish interactive thresholding algorithm is a method to test for visual field loss, usually in glaucoma testing or monitoring. It is combined with a visual field test such as standard automated perimetry (SAP) or short wavelength automated perimetry (SWAP) to determine visual fields in a more efficient manner.\n\nStandard automated perimetry determines how dim of light (the threshold) can be seen at various points in an individual eye's visual field. Various algorithms have been developed to determine this threshold in the dozens to over a hundred individual points in a single visual field. The SITA algorithm optimizes the determination of perimetry thresholds by continuously estimating what the expected threshold is based on the patient's age and neighboring thresholds. In this manner, it can reduce the time necessary to acquire a visual field by up to 50%, and it decreases patient fatigue and increases reliability. SITA mode is now widely used in many computerized automated perimeters.\n\nThe testing mode interrupts testing when measurement error is reached. This results in a shorter test time with reportedly equal accuracy as other automated threshold visual fields.\n\nSee Visual Field Test\n"}
{"id": "52836578", "url": "https://en.wikipedia.org/wiki?curid=52836578", "title": "Ted Atkins", "text": "Ted Atkins\n\nTed Atkins (11 August 1958 – 20 August 2018) was an English explorer, engineer, mountaineer and inventor.\n\nExpelled from school, he later achieved various qualifications required by the RAF, and studied MBA to certificate.\n\nAfter thirty three years of Royal Air Force service he left as an Engineering Officer. He worked on Nimrod, Tornado jets and lastly as the Chief Engineer on Sea King Search and Rescue helicopters. Between engineering jobs he was the Staff Officer RAF Mountain Rescue Service in charge of teams in Scotland. In this position he took the first RAF team to climb Mt. Everest in 2001. This was a success and the first summit of Everest by an RAF team.\n\nTed has always been an outdoors person. He joined the RAF Mountain Rescue Service as a volunteer in 1979 as a ‘Troop’, and served on several teams before Everest. During Mountain Rescue service he led the first RAF team to climb the North Face of the Eiger. He spent a year exploring in Antarctica where he was a mountain leader, surveyor and cartographer making maps of places no one had ever seen. He made 28 first ascents of mountains there. That was then followed by a period of service with the Royal Navy on HMS Endurance, principally as a marine engineer but also serving with the Royal Marines as their Mountain Leader. In this role he was awarded his coveted ‘Green Beret’ for work with the Marines on an Antarctic rescue mission where he led one of the two detachments. For his Antarctic work, he was invested with the Polar Medal by HM The Queen. His last job with the Service was attached to the SAS as the Project Officer for a 100% successful ascent of Everest.\n\nHe has maintained a 44-year link of service to his old Air Training Corps squadron 2425 in Nottinghamshire throughout and served as a reservist.\n\nIn his early career Ted was a boxer and was an RAF champion. Mountaineering in all of its forms was always a part of his life. Ted was an avid rock climber, later in Scotland discovering what would become his passion till he died, winter climbing. He was a BASI and Joint Service ski instructor in Alpine and Nordic disciplines and a hang glider pilot.\n\nFrom rock and winter climbing Ted went to the Alps and onto the Himalayas, first in 1983 to Manaslu then onto the West Ridge of Everest in 1988. This is where he became involved with oxygen systems. This expedition was unsuccessful on a number of fronts but did sow the seed for the RAF Everest Expedition in 2001. In 1987 he was the Climbing Leader on the phenomenally successful Gimaghella expedition led by Marine Maj. Pat Parsons. This was a new route and first British ascent in good style.\n\nTed did not summit with the RAF Everest team because one of the team members got sick on the summit push. Ted and his partner Dr. Brian Kirkpatrick lost their summit making the rescue. He went back to Everest, this time to the south in Nepal in 2004 to climb the mountain on his own. Using his engineering head he deduced that the greatest aid to staying alive, keeping all fingers and toes, and success was to have a good oxygen delivery system. This was not available so he set out to invent one. This he did one day prior to setting off for the summit. The prototype had a condom inside a Coca-Cola bottle as the core component. People said he would die. One of his cylinders went missing on the mountain. He was left for dead on the summit but was saved by Mingma Sherpa and Andrew Lock. However the speed of his ascent had been noted by other climbers and he was asked by Jagged Globe to make commercial systems (without string or condoms was the condition). One year later and this system became the industry standard.\n\nTed set up business in Nepal and continued to develop and improve the oxygen systems used on Mt Everest and other high mountains. At this time it was considered that 1:10 people attempting Everest would die trying. Some years later he got a letter from the Nepalese Mountaineering Association thanking him for reducing the death rate from 1:10 to less than 1:700 (no typo). The Topout (Topout, to succeed, to summit a mountain or route) system was firmly established. The demand was such that Ted had to leave the Service early to concentrate on this new business. He went onto produce a new cylinder, cylinder valve, regulator and flow controller. Lastly he has built a plant to produce oxygen in Nepal in order to guarantee the quality of the gas that Topout supplies.\nA request to work with the Everest Skydive Team was taken up to give them a better oxygen system for jumps exiting the aircraft level with the summit of Everest. This was an outstanding success. The Topout Aero skydive system went on to become the industry standard some years ago and a number of world records have been set. This continues to evolve and now in the Military guise is MTOS, Military Tactical Oxygen System. As part of this new work Ted trained as a skydiver and earned one world record. He worked with Red Bull on the Mt Blanc project.\nTed was a Science and Engineering Ambassador with a remit to stimulate and encourage (mainly) young people into science and engineering. He was an acclaimed public and motivational speaker. He has delivered talks in: UK, USA, Nepal, France and Italy where he lived in the Dolomites. In Nepal he was involved with several charity organizations inc: Duke of Edinburgh and was a trustee of an orphanage. He wrote a column in Nepal called Outside In where he told of the visitors' views of Nepal with the aim of making things better. He spoke to, and worked with the industry in Nepal and tried to influence events.\n\nContinuing work on the evolution of various mountaineering oxygen systems has required big mountains to be climbed. Since Everest in 2004, Ted climbed: Makalu, Kangchenjunga, Lhotse, Lobuche East, and Ama Dablam. Ama Dablam was unique in that he parachuted into the Base Camp setting a world record then went onto climb the mountain. He was an avid skier on his home turf in Italy and a passionate mountaineer especially for winter climbing.\nHe died descending Monte Civetta, one of the highest mountains of the Dolomites in the province of Belluno (Italy), on 20 August 2018.\n\n"}
{"id": "26924538", "url": "https://en.wikipedia.org/wiki?curid=26924538", "title": "Tele-audiology", "text": "Tele-audiology\n\nTele-audiology is the utilization of telehealth to provide audiological services and may include the full scope of audiological practice.\n\nThis term was first used by Dr. Gregg Givens in 1999 in reference to a system being developed at East Carolina University in North Carolina, USA. The first Internet audiological test was accomplished in 2000 by Givens, Balch and Keller.\n\nThe first Transatlantic teleaudiology test was performed in April 2009 when Dr James Hall tested a patient in South Africa from Dallas at the AAA conference. Since that historic event the interest in tele-audiology increased significantly.\n\nThere are 2 types of teleaudiology tests:\n\nStore-and-forward (Asynchronous) tests\nTesting a patient and then transferring the results via emailing or the Internet to a professional that will look at the results\n\nReal-time (Synchronous) tests\nTesting a patient in real-time as if the patient is sitting in front of you. Audiologists are used to testing patients remotely because testing a patient in a sound booth while the audiologist sits outside the booth is virtually the same as testing a patient over the Internet. The window is not a real glass window but a teleconference window. The only real difference is that the physical distance changed.\n\n"}
{"id": "57241212", "url": "https://en.wikipedia.org/wiki?curid=57241212", "title": "The Travancore-Cochin Medical Practitioners' Act, 1953", "text": "The Travancore-Cochin Medical Practitioners' Act, 1953\n\nThe Travancore Cochin Medical Practitioners' Act (1953) regulates the qualifications and provides registration for medical doctors qualified in modern medicine, homeopathic medicine and indigenous medicine. At the time of enactment, it extended to the state of Travancore-Cochin, which later became Kerala state. The Act directs the establishment of the Council of Modern Medicine (with 9 members), Council of Homeopathic medicine (5 members) and the council of indigenous medicine (11 members). This Act allows giving registration for practicing medicine to those registered under this Act. The practitioners registered under this Act will be listed in a registry. The annual list of practitioners is published in gazettes. Practitioners registered under this Act are allowed to use the words \"legally qualified medical practitioner\" or \"duly qualified medical practitioner\".\n"}
{"id": "55505632", "url": "https://en.wikipedia.org/wiki?curid=55505632", "title": "Ukrainian Association of Psychoanalysis", "text": "Ukrainian Association of Psychoanalysis\n\nUkrainian Association of Psychoanalysis (UAP-ECPP-Ukraine) is an all-Ukrainian public organization which activity is aimed at the development and institualization of psychoanalysis in Ukraine, creation of the psychoanalysts’ professional community, development and dissemination of psychoanalytic and psychological culture and knowledge, providing population of Ukraine with psychological and psychoanalytic support as well as at satisfaction and protection of legitimate social, economic, professional, creative, spiritual, age-specific and other general interests of its members.\n\nThe Association unites individuals engaged in psychoanalytic activity, researches in the field of psychology and psychoanalysis as well as people who contribute to the development of philosophical, applied and clinical psychoanalysis. One of the main, but not the only task of Association is to create periodicals on psychoanalysis as well as to organize educational and cultural events in the field of depth psychology and aimed at the development of personality.\n\nUkrainian Association of Psychoanalysis was founded in 2000 with the purpose to revive psychoanalytic movement in our country. Its members participate in international research projects, conferences and symposiums. The Association organizes open local and international seminars, conferences, congresses, symposiums, presentations of psychoanalytic literature.\n\nSince 2004 Ukrainian Association of Psychoanalysis is a collective member of the European Confederation of Psychoanalytical Psychotherapies. On February 19, 2009 with ECPP Board decision, UAP became the branch-organization of the European Confederation of Psychoanalytical Psychotherapies (ЕCPP).\n\nIn 2003 a publishing house started its activity at the Association.\n\nPeriodicals and books published:\n\n- \"Psychoanalysis (Chronicle)\"\n\n- \"European Journal of Psychoanalysis\" (Russian-language version)\n\n- Victor Mazin «Man-Machine», “Oneirography: Ghosts and Dreams”, “Paranoia”, “Introduction into Lacan”, “S. Freud: Psychoanalytic Revolution” \n\n- Salomon Resnik «Mental space», “The Delusional Person”\n\n- Dictionary of Psychological Terms in Depth Psychology\n\n- Odessa’s realities of the Wolf-Man\n\n- Uvarova S., Grishkan S., Ulko N., Bojchenko N. “Psychological assistance in crisis situations”\n\n- Svetlana Uvarova “Love and Death in Psychoanalysis”\n"}
{"id": "17089620", "url": "https://en.wikipedia.org/wiki?curid=17089620", "title": "Water privatization in Colombia", "text": "Water privatization in Colombia\n\n\"This article was last updated on substance in June 2010. Please update it further if need be.\"\n\nPrivate sector participation (also called \"water privatization\" or \"public-private partnerships\", PPP) in water supply and sanitation in Colombia has been more stable and successful than in some other Latin American countries such as Argentina or Bolivia. According to the World Bank, between 1996 and 2007, more than 40 water and sanitation service provision contracts have been awarded to private or mixed companies in Colombia, serving a combined population of 7.3 million or more than 20% of the urban population. According to the Colombian water regulator, there were even more public-private partnerships for water and sanitation in Colombia in 2004: 125 private and 48 mixed public-private water companies, including large, medium and small companies. Most of the contracts were awarded in poor municipalities with highly deteriorated infrastructure. They relied mainly on public funding, complemented by limited private funding. The design was based on the central government providing grants in the start-up years to rehabilitate deteriorated systems and to expand access, while the contracting municipal governments also made budgetary transfers on an annual basis to complement revenues. Colombia thus departed from the standard concession approach, which requires private concessionaires to finance investments with their own resources. \n\nAccording to the World Bank, the key to success of private sector participation in the Colombian water sector has been the development of homegrown solutions, and, at times, skillfully adapting models used elsewhere to the particular circumstances and culture of Colombia.\n\nPrivate sector involvement in the Colombian water sector began in 1995 in Cartagena, with support from the World Bank. It was followed by a second contract in Barranquilla in 1996 and more concessions in the next years in Santa Marta, Tunja, Montería, Palmira, Girardot, and Riohacha. The first contracts followed mostly the mixed-ownership company model, with the municipality holding a majority of the shares but with management fully delegated to a private operator. In a \"mixed-capital\" company in which the service is jointly controlled by municipal governments and private \"operating partners\" operating partners must meet ambitious targets ensuring that coverage is extended to low-income areas quickly — sometimes before the current mayor concludes his term in office. In exchange they receive an annual management fee and a percentage of revenues. A second group of contracts started in 2001 with the implementation by the central government of the Enterprise Modernization Program (PME - Programa de Modernización de Empresas), which focused on turning around public water utilities in small cities and towns with high rates of poverty and poor network condition.\n\nStudies on the impact of private sector participation in Colombia show that it had a positive impact on service quality. There were also significant increases in access in a short time frame under private contracts. However, there are no conclusive results showing that access increased more rapidly under private contracts than in the case of public utilities. There has been a slight average reduction in water losses in privatized utilities. In at least one city (Cartagena), real tariffs declined substantially, indicating that the operator passed on efficiency gains to consumers. None of the studies showed negative impacts of water privatization on access, service quality, operational efficiency or tariff levels.\n\nAccording to a study by the World Bank relying on data of Colombia's regulatory agency, private water projects in Colombia have performed well in expanding access. The study notes that progress achieved in Colombia for access to water supply and sewerage services is not due to water privatization per se, but must be credited to good performers in both the public and private sectors, and a national policy that fosters accountability and efficiency. \n\nSuccessful public utilities include EAAB in Bogotá, which made strong progress in access over the last decade, reaching universal coverage. The performance of the public utility EPM in Medellín was comparable for expanding access to that of the largest PPP, that of Barranquilla. Colombia's nine largest and/or oldest PPPs, the two mixed-ownership companies in Barranquilla (serving 1.3 million people) and Cartagena (serving 1 million) also have good records in expanding access. That in Barranquilla made notable progress in both water supply and sewerage services: coverage rose from 86 percent to 96 percent for water and from 70 percent to 93 percent for sewerage (1997–2006). In Cartagena water supply coverage jumped from 74 percent to almost universal coverage, while sewer coverage went up from 62 percent to 79 percent (1996–2006). Cartagena achieved full water supply coverage despite a 50 percent jump in the size of its population during the same period, largely due to the arrival of poor rural migrants. Half a million people gained access and 60 percent of the new connections benefited families in the poorest income quintile. To achieve universal coverage, the operator in Cartagena made extensive use of community bulk-supply schemes that provide safe water to the many illegal settlements that were expanding on the city's periphery. \n\nIn Santa Marta, water coverage improved rapidly during the first three years, going from 74 percent to 87 percent, but has stagnated since 2001 (access to sewerage followed the same pattern). In Palmira (220,000) and Girardot(100,000), full coverage was achieved for both water and sewerage. In the city of Tunja (120,000 people) both water and sewer coverage went up from 89 percent in 1996 at the start of the concession to 100 percent four years later. The company Conhydra in the department of Antioquia achieved full water coverage within a few years in the towns of Marinilla, Santafe, and Puerto Berrio (combined population 170,000) starting from levels of 80 – 90 percent.\n\nThe first contract under the PME program was awarded in 2000 in Montería (350,000 people). Starting from a low 63 percent, water coverage had increased to 96 percent by 2007, catching up with the national urban average. The population with access to piped water more than doubled. The improvement in sewer coverage was more modest, up from 26 percent to around 40 percent. In Soledad (400,000 people), coverage went up from 65 percent to 84 percent for water, and from 36 percent to 73 percent for sewerage in just five years.\n\nThe evolution of the water losses - as measured by Non-revenue water - for the nine largest and/or oldest PPP projects shows a mixed record. Strong gains were made in Montería, Tunja, Marinilla, and Palmira, but the reduction was modest in Cartagena, Barranquilla, and Santa Marta, and no progress was achieved in Girardot or Soledad. Reductions were achieved while the average network pressure went up significantly as service continuity was reestablished, which would have increased losses if no improvements had been made in the hydraulics of the distribution networks.\n\nIn the case of Cartagena, the World Bank has estimated efficiency gains by comparing the evolution of the ratio of collected revenues to operational costs (operating ratio) on the one hand to that of the average tariff level on the other hand. The operating ratio is driven essentially by two factors: the evolution of operational costs and collection rates, which are controlled by the private operator, and the evolution of the average tariff, which is exogenous. Whenever the operating ratio increases faster than the tariff, efficiency gains are taking place. The Cartagena privatization achieved significant improvements in efficiency. This is reflected in the doubling over a decade (1995–2005) in the ratio of collected revenues to operational costs. At the same time, the average tariff was halved in real terms, suggesting that a significant portion of the savings achieved through efficiency gains was passed to customers.\n\nWater rationing was less frequent in ten cities five years after the private sector became involved in providing services. Studies based on household and public health surveys show that private operators tend to achieve better potability figures than public water utilities.\n\nThe largest private water company in Colombia is Triple A (AAA), a Colombian company serving more than 1.5 million people. It has a strategic partnership with the publicly owned water utility of Madrid (Canal Isabel II). It is in practice managed by nationals and has been branding itself as a Colombian private company. The company was willing to focus on a market segment that other companies tend to avoid: the poorest cities. For example, in Barranquilla, more than 76 percent of AAA's customers are in the three lowest of six income segments used by the Colombian government. In Soledad, 98 percent of the population is in the three lowest segments, and poverty levels are similarly high in all the other municipalities where AAA operates. Other Colombian water operators are EIS, Conhydra and Sala.\n\nColombian construction companies that were active in the water sector were awarded PPP contracts following tenders in which only local investors participated. In all these cases, governments chose to ease prequalification criteria to increase competition, resorting to various mechanisms to ensure the winning bidder would be able to operate the water utility. In Colombia, the winning bidders contracted experienced technical staff (often former managers and engineers from public utilities). Between 2001 and 2004, Colombian investors won almost all the PPP contracts awarded in that country under the PME program.\n\nIn 1996, more than 60 percent of Barranquilla's population had no water service at all or had it for only a few hours a day. The private operator was initially met with skepticism. When AAA began to install residential water meters in homes that had never had them, many consumers worried that the devices would result in big water bill increases. Opposition politicians like Guillermo Hoenisgberg initially opposed the private operator for ideological reasons, but later supported him. AAA earned the trust of its customers and political support by going to the poorest neighborhoods, interviewing thousands of people with the goal of understanding their expectations and assumptions regarding water and sanitation. The company then conducted a massive communications campaign with more than 40 staff, many of them social workers by profession, to work full-time in community outreach. Simultaneously, AAA adopted an aggressive external relations strategy that contrasts sharply with the cautious, low-profile approach preferred by water companies in many other Latin American cities. AAA runs elaborate public information campaigns in radio, television and the press. It also regularly hosts lunches and workshops for local journalists where company officials offer detailed explanations of the company's activities and answer questions.\n\nInstead of installing meters and immediately sending out bills, AAA decided to use a gradual approach. In some cases the company would start by delivering water for free, and then bill new customers for 10 cubic meters of water per month, even if their actual consumption was far higher. After two months the fee would be based on 20 cubic meters, and after six months, it would be based on actual consumption. According to company officials, this approach relieved most consumers’ anxiety about the water meter and encouraged them to start monitoring their own consumption.\n\nAAA also began to disconnect consumers who weren’t paying, a potentially explosive issue. Many wealthy individuals and important companies were in the habit of ignoring water bills. And the notion of cutting off water to people in poor communities was obviously problematic. The company therefore developed a comprehensive system to facilitate and encourage payment among low-income customers, acknowledging the reality that many low-income families in Barranquilla live day-to-day on small amounts of cash earned from informal occupations. As a result, AAA has established partnerships with virtually all of Barranquilla's pawnshops that enable people to pay their water bill while conducting other transactions. Similar partnerships allow customers to pay their water bills at banks, department stores, grocery chains, and sports clubs through the city. Despite these measures, thousands of AAA's low-income customers still miss their payments each month, and thousands more have their water cut off when they go for two months without paying. To ensure that cutoffs are short-lived, every month AAA billing agents set up portable, outdoor \"payment stations\" in low-income neighborhoods. Local residents approach the stations and work out customized payment plans (\"convenios de pago\") with the agents. The plans allow customers to catch up on missed payments over several months, so long as they pay the current month's fee—and they result in immediate restoration of water service. AAA also developed a program to reward customers who consistently pay their bill. The program, known as \"Supercliente\" (super customer) awards modest prizes and certificates to customers who stay on top of their bill. As a result, billing efficiency increased from 66% in 1996 to 87% in 2004.\n\nBarrera, Felipe, and Mauricio Olivera (2007). Does Society Win or Lose as a Result of Privatization? Provision of Public Services and Welfare of the Poor: The Case of Water Sector Privatization in Colombia, Research Network Working Paper #R-525, Latin American Research Network. Washington, D.C.: Inter-American Development Bank.\n"}
