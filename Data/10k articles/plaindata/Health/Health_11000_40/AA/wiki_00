{"id": "25953877", "url": "https://en.wikipedia.org/wiki?curid=25953877", "title": "Abortion in Hungary", "text": "Abortion in Hungary\n\nAbortion in Hungary was subject to legal liberalization as early as 1953. The abortion law was changed three times since then, in 1956, 1973, and 1992. As a result of that Hungary has a liberal abortion law. While women must get the approval of a committee before having an abortion, a number of various hypothetical situations have been placed into the law allowing an abortion, so the request has become a mere formality.\n\nHungary is influenced by Roman Catholicism, and, although abortion is legal, it is not easy to access: women must go through a specific procedure involving counseling, a waiting period, and a certificate from a midwife in order to obtain an abortion.\n\nAbortion is regulated under the \"Law on the Protection of the Foetus, 1992\". Under this law, an abortion is normally allowed until 12 weeks. However, in certain circumstances the limit can be extended to 18, 20 or 24 weeks. A pregnancy may be terminated at any time if the fetus is lethally afflicted (ie. anencephaly). There is mandatory counseling and a 3 days waiting period.\n\nIn 1998, the country's highest court demanded that a definition be supplied for the term \"grave crisis situation\", as there were concerns that women undergoing the procedure may not actually be in \"crisis\", and if they were, that they get psychiatric help after their abortion. On June 29, 2000, the Ministry of Health defined a \"grave crisis situation\" as \"when it causes bodily or mental impairment, or a socially intolerable situation\".\n\nThe Hungarian Government organized a pro-life campaign in 2011 where posters were shown with an image of a baby in the womb, with the caption saying, “I understand that you are not yet ready for me, but give me up to the adoption agency, LET ME LIVE!” The Government has been sharply criticised for using European Union funds on the campaign. An EU Commissioner Viviane Reding said that the campaign “goes against European values.”\n\nThe new constitution of Hungary, enacted in 2012, states that human life will be protected from the moment of conception, although, so far, the abortion law has not been changed.\n\nBetween 2010 and 2015 the number of reported abortions per year dropped by 22.9 per cent, which was attributed to the introduction of pro-family measures by the Hungarian Government.\nAdoption is promoted by the state where women do not want to keep their baby. There are also educational classes and aid to families.\n"}
{"id": "1205821", "url": "https://en.wikipedia.org/wiki?curid=1205821", "title": "Alcoholic polyneuropathy", "text": "Alcoholic polyneuropathy\n\nAlcoholic polyneuropathy (A.K.A alcohol leg) is a neurological disorder in which peripheral nerves throughout the body malfunction simultaneously. It is defined by axonal degeneration in neurons of both the sensory and motor systems and initially occurs at the distal ends of the longest axons in the body. This nerve damage causes an individual to experience pain and motor weakness, first in the feet and hands and then progressing centrally. Alcoholic polyneuropathy is caused primarily by chronic alcoholism; however, vitamin deficiencies are also known to contribute to its development. This disease typically occurs in chronic alcoholics who have some sort of nutritional deficiency. Treatment may involve nutritional supplementation, pain management, and abstaining from alcohol.\n\nAlcoholic polyneuropathy usually has a gradual onset over months or even years although axonal degeneration often begins before an individual experiences any symptoms. An early warning sign (prodrome) of the possibility of developing alcoholic polyneuropathy, especially in a chronic alcoholic, would be weight loss because this usually signifies a nutritional deficiency that can lead to the development of the disease.\n\nThe disease typically involves sensory and motor loss, as well as painful physical perceptions (paresthesias), though all sensory modalities may be involved. Symptoms that affect the sensory and motor systems seem to develop symmetrically. For example, if the right foot is affected, the left foot is affected simultaneously or soon becomes affected. In most cases, the legs are affected first, followed by the arms. The hands usually become involved when the symptoms reach above the ankle. This is called a stocking-and-glove pattern of sensory disturbances.\n\nPolyneuropathy spans a large range of severity. Some cases are seemingly asymptomatic and may only be recognized on careful examination. The most severe cases may cause profound physical disability.\n\nCommon manifestations of sensory issues include numbness or painful sensations in the arms and legs, abnormal sensations like “pins and needles,” and heat intolerance. Pain experienced by individuals depends on the severity of the polyneuropathy. It may be dull and constant in some individuals while being sharp and lancinating in others. In many subjects, tenderness is seen upon the palpitation of muscles in the feet and legs. Certain people may also feel cramping sensations in the muscles affected and others say there is a burning sensation in their feet and calves.\n\nSensory symptoms are gradually followed by motor symptoms. Motor symptoms may include muscle cramps and weakness, erectile dysfunction in men, problems urinating, constipation, and diarrhea. Individuals also may experience muscle wasting and decreased or absent deep tendon reflexes. Some people may experience frequent falls and gait unsteadiness due to ataxia. This ataxia may be caused by cerebellar degeneration, sensory ataxia, or distal muscle weakness. Over time, alcoholic polyneuropathy may also cause difficulty swallowing (dysphagia), speech impairment (disarthria), muscle spasms, and muscle atrophy.\n\nIn addition to alcoholic polyneuropathy, the individual may also show other related disorders such as Wernicke-Korsakoff syndrome and cerebellar degeneration that result from alcoholism-related nutritional disorders.\n\nThe general cause of this disease is prolonged and heavy consumption of alcohol accompanied by a nutritional deficiency. There is some debate over whether the main cause is the direct toxic effect of alcohol itself or whether the disease is a result of alcoholism-related malnutrition.\n\nFrequently alcoholics have disrupted social links in their lives and have an irregular lifestyle. This may cause an alcoholic to change their eating habits including more missed meals and a poor dietary balance. Alcoholism may also result in loss of appetite, alcoholic gastritis, and vomiting, which decrease food intake. Alcohol abuse damages the lining of the gastrointestinal system and reduces absorption of nutrients that are taken in. The combination of all of them may result in a nutritional deficiency that is linked to the development of alcoholic polyneuropathy.\n\nThere is evidence that providing individuals with adequate vitamins improves symptoms despite continued alcohol intake, indicating that vitamin deficiency may be a major factor in the development and progression of alcoholic polyneuropathy. In experimental models of alcoholic polyneuropathy utilizing rats and monkeys has not resulted in convincing evidence that proper nutritional intake along with alcohol results in polyneuropathy.\n\nIn addition, the consumption of alcohol may lead to the buildup of certain toxins in the body. For example, in the process of breaking down alcohol, the body produces acetaldehyde, which can accumulate to toxic levels in alcoholics. This suggests that there is a possibility ethanol (or its metabolites) may cause alcoholic polyneuropathy. There is evidence that polyneuropathy is also prevalent in well nourished alcoholics, supporting the idea that there is a direct toxic effect of alcohol.\n\nMany of the studies conducted that observe alcoholic polyneuropathy in patients are often criticized for their criteria used to assess nutritional deficiency in the subjects because they may not have completely ruled out the possibility of a nutritional deficiency in the genesis of the polyneuropathy. Many researchers favor the nutritional origin of this disease, but the possibility of alcohol having a toxic effect on the peripheral nerves has not been completely ruled out.\n\nThe pathophysiology of alcoholic polyneuropathy is an area of current research. Damage to the nervous system takes place before symptoms appear in individuals, beginning with segmental thinning and loss of myelin on the peripheral ends of the longest nerves. Segmental thinning is the demyelination of axons in small sections at a time. This occurrence increases leakage of an action potential current down the axon, so it is weakest at the peripheral end. Decrease in current causes further thinning of myelin.\n\nIn most cases, individuals with alcoholic polyneuropathy have some degree of nutritional deficiency. Alcohol, a carbohydrate, increases the metabolic demand for thiamine (vitamin B1) because of its role in the metabolism of glucose. Thiamine levels are usually low in alcoholics due to their decreased nutritional intake. In addition, alcohol interferes with intestinal absorption of thiamine, thereby further decreasing thiamine levels in the body. Thiamine is important in three reactions in the metabolism of glucose: the decarboxylation of pyruvic acid, d-ketoglutaric acid, and transketolase. A lack of thiamine in the cells may therefore prevent neurons from maintaining necessary adenosine triphosphate (ATP) levels as a result of impaired glycolosis. Thiamine deficiency alone could explain the impaired nerve conduction in those with alcoholic polyneuropathy, but other factors likely play a part.\n\nThe metabolic effects of liver damage associated with alcoholism may also contribute to the development of alcoholic polyneuropathy. Normal products of the liver, such as lipoic acid, may be deficient in alcoholics. This deficiency would also disrupt glycolosis and alter metabolism, transport, storage, and activation of essential nutrients.\n\nThe malnutrition many alcoholics suffer deprives them of important cofactors for the oxidative metabolism of glucose. Neural tissues depend on this process for energy, and disruption of the cycle would impair cell growth and function. Schwann cells produce myelin that wraps around the sensory and motor nerve axons to enhance action potential conduction in the periphery. An energy deficiency in Schwann cells would account for the disappearance of myelin on peripheral nerves, which may result in damage to axons or loss of nerve function altogether. In peripheral nerves, oxidative enzyme activity is most concentrated around the nodes of Ranvier, making these locations most vulnerable to cofactor deprivation. Lacking essential cofactors reduces myelin impedance, increases current leakage, and slows signal transmission. Disruptions in conductance first affect the peripheral ends of the longest and largest peripheral nerve fibers because they suffer most from decreased action potential propagation. Thus, neural deterioration occurs in an accelerating cycle: myelin damage reduces conductance, and reduced conductance contributes to myelin degradation. The slowed conduction of action potentials in axons causes segmental demyelination extending proximally; this is also known as retrograde degeneration.\n\nAcetaldehyde is toxic to peripheral nerves. There are increased levels of acetaldehyde produced during ethanol metabolism. If the acetaldehyde is not metabolized quickly the nerves may be affected by the accumulation of acetaldehyde to toxic levels.\n\nAlcoholic polyneuropathy is very similar to other axonal degenerative polyneuropathies and therefore can be difficult to diagnose. When alcoholics have sensorimotor polyneuropathy as well as a nutritional deficiency, a diagnosis of alcoholic polyneuropathy is often reached.\n\nTo confirm the diagnosis, a physician must rule out other causes of similar clinical syndromes. Other neuropathies can be differentiated on the basis of typical clinical or laboratory features. Differential diagnoses to alcoholic polyneuropathy include amyotrophic lateral sclerosis, beriberi, Charcot-Marie-Tooth disease, diabetic lumbosacral plexopathy, Guillain Barre Syndrome, diabetic neuropathy, mononeuritis multiplex and post-polio syndrome.\n\nTo clarify the diagnosis, medical workup most commonly involves laboratory tests, though, in some cases, imaging, nerve conduction studies, electromyography, and vibrometer testing may also be used.\n\nA number of tests may be used to rule out other causes of peripheral neuropathy. One of the first presenting symptoms of diabetes mellitus may be peripheral neuropathy, and hemoglobin A1C can be used to estimate average blood glucose levels. Elevated blood creatinine levels may indicate renal insufficiency and may also be a cause of peripheral neuropathy. A heavy metal toxicity screen should also be used to exclude lead toxicity as a cause of neuropathy.\n\nAlcoholism is normally associated with nutritional deficiencies, which may contribute to the development of alcoholic polyneuropathy. Thiamine, vitamin B-12, and folic acid are vitamins that play an essential role in the peripheral and central nervous system and should be among the first analyzed in laboratory tests. It has been difficult to assess thiamine status in individuals due to difficulties in developing a method to directly assay thiamine in the blood and urine. A liver function test may also be ordered, as alcoholic consumption may cause an increase in liver enzyme levels.\n\nAlthough there is no known cure for alcoholic polyneuropathy, there are a number of treatments that can control symptoms and promote independence. Physical therapy is beneficial for strength training of weakened muscles, as well as for gait and balance training.\n\nTo best manage symptoms, refraining from consuming alcohol is essential. Abstinence from alcohol encourages proper diet and helps prevent progression or recurrence of the neuropathy. Once an individual stops consuming alcohol it is important to make sure they understand that substantial recovery usually isn't seen for a few months. Some subjective improvement may appear right away, but this is usually due to the overall benefits of alcohol detoxification. If alcohol consumption continues, vitamin supplementation alone is not enough to improve the symptoms of most individuals.\n\nNutritional therapy with parenteral multivitamins is beneficial to implement until the person can maintain adequate nutritional intake. Treatments also include vitamin supplementation (especially thiamine). In more severe cases of nutritional deficiency 320 mg/day of benfotiamine for 4 weeks followed by 120 mg/day for 4 more weeks may be prescribed in an effort to return thiamine levels to normal.\n\nPainful dysesthesias caused by alcoholic polyneuropathy can be treated by using gabapentin or amitriptyline in combination with over-the-counter pain medications, such as aspirin, ibuprofen, or acetaminophen. Tricyclic antidepressants such as amitriptyline, or carbamazepine may help stabbing pains and have central and peripheral anticholinergic and sedative effects. These agents have central effects on pain transmission and block the active reuptake of norepinephrine and serotonin.\n\nAnticonvulsant drugs like gabapentin block the active reuptake of norepinephrine and serotonin and have properties that relieve neuropathic pain. However, these drugs take a few weeks to become effective and are rarely used in the treatment of acute pain.\n\nTopical analgesics like capsaicin may also relieve minor aches and pains of muscles and joints.\n\nIt is difficult to assess the prognosis of a patient because it is hard to convince chronic alcoholics to abstain from drinking alcohol completely. It has been shown that a good prognosis may be given for mild neuropathy if the alcoholic has abstained from drinking for 3–5 years. During the early stages of the disease the damage appears reversible when people take adequate amounts of vitamins, such as thiamine. If the polyneuropathy is mild, the individual normally experiences a significant improvement and symptoms may be completely eliminated within weeks to months after proper nutrition is established. When those people diagnosed with alcohol polyneuropathy experience a recovery, it is presumed to result from regeneration and collateral sprouting of the damaged axons.\n\nAs the disease progresses, the damage may become permanent. In severe cases of thiamine deficiency, a few of the positive symptoms (including neuropathic pain) may persist indefinitely. Even after the restoration of a balanced nutritional intake, those patients with severe or chronic polyneuropathy may experience lifelong residual symptoms. Alcoholic polyneuropathy is not life-threatening but may significantly affect one's quality of life. Effects of the disease range from mild discomfort to severe disability.\n\nThe rate of incidence of alcoholic polyneuropathy involving sensory and motor polyneuropathy varies from 10% to 50% of alcoholics depending on the subject selection and diagnostic criteria. If electrodiagnostic criteria is used, alcoholic polyneuropathy may be found in up to 90% of individuals being assessed. The distribution and severity of the disease depends on regional dietary habits, individual drinking habits, as well as an individual’s genetics. Large studies have been conducted and show that alcoholic polyneuropathy severity and incidence correlates best with the total lifetime consumption of alcohol. Factors such as nutritional intake, age, or other medical conditions are correlate in lesser degrees. For unknown reasons, alcoholic polyneuropathy has a high incidence in women.\n\nCertain alcoholic beverages can also contain congeners that may also be bioactive; therefore, the consumption of varying alcoholic beverages may result in different health consequences. An individual’s nutritional intake also plays a role in the development of this disease. Depending on the specific dietary habits, they may have a deficiency of one or more of the following: thiamine (vitamin B1), pyridoxine (vitamin B6), pantothenic acid and biotin, vitamin B12, folic acid, niacin (vitamin B3), and vitamin A.\n\nIt is also thought there is perhaps a genetic predisposition for some alcoholics that results in increased frequency of alcoholic polyneuropathy in certain ethnic groups. During the body’s processing of alcohol, ethanol is oxidized to acetaldehyde mainly by alcohol dehydrogenase; acetaldehyde is then oxidized to acetate mainly by aldehyde dehydrogenase (ALDH). ALDH2 is an isozyme of ALDH and ALDH2 has a polymorphism (ALDH2*2, Glu487Lys) that makes ADLH2 inactive; this allele is more prevalent among Southeast and East Asians and results in a failure to quickly metabolize acetaldehyde. The neurotoxicity resulting from the accumulation of acetaldehyde may play a role in the pathogenesis of alcoholic polyneuropathy.\n\nThe first description of symptoms associated with alcoholic polyneuropathy were recorded by John C. Lettsome in 1787 when he noted hyperesthesia and paralysis in legs more than arms of patients. Jackson has also been credited with describing polyneuropathy in chronic alcoholics in 1822. The clinical title of alcoholic polyneuropathy was widely recognized by the late nineteenth century. It was thought that the polyneuropathy was a direct result of the toxic effect alcohol had on peripheral nerves when used excessively. In 1928, George C. Shattuck argued that the polyneuropathy resulted from a vitamin B deficiency commonly found in alcoholics and he claimed that alcoholic polyneuropathy should be related to beriberi. This debate continues today over what exactly causes this disease, some argue it is just the alcohol toxicity, others claim the vitamin deficiencies are to blame and still others say it is some combination of the two.\n\nThe mechanism of axonal degeneration has not been clarified and is an area of continuing research on alcoholic polyneuropathy.\n\nFurther research is looking at the effect an alcoholics’ consumption and choice of alcoholic beverage on their development of alcoholic polyneuropathy. Some beverages may include more nutrients than others (such as thiamine), but the effects of this with regards to helping with a nutritional deficiency in alcoholics is yet unknown.\n\nThere is still controversy about the reasons for the development of alcoholic polyneuropathy. Some argue it is a direct result of alcohol's toxic effect on the nerves, but others say factors such as a nutritional deficiency or chronic liver disease may play a role in the development as well. This debate is ongoing and research is continuing in an effort to discover the real cause of alcoholic polyneuropathy.\n"}
{"id": "52383375", "url": "https://en.wikipedia.org/wiki?curid=52383375", "title": "Antibiotic use in dentistry", "text": "Antibiotic use in dentistry\n\nThere are many circumstances during dental treatment where antibiotics are prescribed by dentists to prevent further infection (e.g. post-operative infection). The most common antibiotic prescribed by dental practitioners is penicillin in the form of amoxicillin, however many patients are hypersensitive to this particular antibiotic. Therefore, in the cases of allergies, erythromycin is used instead.\n\nBacteraemia is a condition in which bacteria are present in the blood and may cause disease, including systemic disease such as infective endocarditis. Some dental treatments may cause bacteraemia, such as tooth extractions, subgingival scaling or even simple aggressive tooth brushing by patients.\n\nIf the bacteria involved in the bacteraemia reach the cardiac tissue, infective (or bacterial) endocarditis can develop, with fatal outcomes. Infective endocarditis is an infection of the endothelium lining of the heart. Infective endocarditis is known to dentists as a post-operative infection and is very serious and life-threatening, especially to patients at high risk of developing the disease, due to a weakened heart. This may be through having congenital heart defect, rheumatic or acquired valvular heart disease and prosthetic heart valves or vessels. The most common bacteria associated with infective endocarditis are streptococcus sanguinis.\n\nHistorically, the use of antibiotic prophylaxis to prevent post-operative infections, resulting from bacteraemia, and infective endocarditis was practiced by dentists, especially in patients at high risk (i.e. with heart problems). However, according to new recommendations from the National Institution for Health and Care Excellence (NICE), antibiotic prophylaxis should not be offered for all patients at risk of infective endocarditis. This is due to the ever-increasing antibiotic resistance and there is no or very little evidence to show whether antibiotic prophylaxis is effective or ineffective against post-operative infections. Moreover, it is yet to be established whether the benefits of administering antibiotics prophylactically outweighs the inherent risks, such as anaphylactic reaction related deaths. Ethically, there is still a need to discuss with patients, the benefits and disadvantages of antibiotic prophylaxis before they make a decision on whether they will go through with it or not.\n\nAn abscess is a painful collection of pus usually caused by bacterial infections. Abscesses are usually the secondary stage of infection. The initial stage of infection is the bacterial infection called cellulitis and is caused by facultative anaerobe bacteria such as Streptococci (e.g. streptococcus pyogenes). This occurs when bacteria gain access into the underlying tissues through odontogenic sources. Pus is usually not produced during this infection. Antibiotic treatment (usually penicillin) is used to prevent progression to a second stage of cellulitis – abscess.\n\nThese abscesses are formed from a blockage in a periodontal pocket and have a vital pulp associated with the tooth. Usually treatment involves the drainage and irrigation of the abscess with antiseptic mouthwash (0.2% Chlorhexidine) and antibiotic therapy is rarely required.\n\nThese abscesses are the most commonly occurring orofacial bacterial infection. They are often the result of an inflamed or necrotic dental pulp or an infection of pulpless root canals. This pulp death is often due to the invasion of bacteria from advanced caries. The first line of treatment is the removal of the source of inflammation or infection by local operative measures. Generally, the abscess can be eradicated through surgical drainage alone; however this is sometimes inadequate. Therefore, systemic antibiotic treatment may be required, but only if there is evidence of spreading infection. As the bacteria involved are known, antibiotic therapy selection can be specific, based on published susceptibilities. Penicillin in the form of amoxicillin is the most common antibiotic to use. However, 3% of the patient population is allergic to penicillin, so erythromycin is often used in cases of hypersensitivity.\n\nStudies conducted to investigate the effects of antibiotics on patients with acute periapical periodontitis and acute apical abscess showed that patients receiving antibiotics in addition to root canal treatment did not have a reduced level of inflammation as compared to the patients not receiving antibiotics. However the available research on this topic is not of optimal quality therefore the results are not entirely reliable.\n\nPhenoxymethyl Penicillin: Penicillin-based antibiotics are used commonly against a broad range of bacterial infections within the body, primarily due to non-toxic effects and minor side effects. In dentistry, phenoxymethyl penicillin is used as it is acid-resistant and can be administered orally. Its common uses include treatment against acute oral infections such as dental abscesses, pericoronitis, salivary gland infections and post-extraction infection. The main disadvantage however, is that patients can be allergic to penicillin based materials with a severe anaphylactic reaction occurring. Despite this, it is still commonly used due to it being highly cost effective and relatively safe. Alternative antibiotics include Erythromycin, cephalosporin and several others.\n\nTetracycline: A wide spectrum antibiotic used to treat multiple bacterial infections. If prescribed during permanent tooth eruption in the mouth, grey staining can occur on the erupting teeth, presenting as a grey band at the point of eruption. The severity of the stain can vary depending on the level of intake of tetracycline. In the UK, there are restrictions on when tetracycline can be prescribed as this staining can be quite severe. Due to the side effect of deposition of tetracycline within developing teeth, it should not be prescribed to children up to 8 years of age as well as pregnant or lactating women.\n\nTetracycline has been used with some success in the treatment of localised juvenile periodontitis and this has proven to be particularly effective with in vitro studies of organisms associated with chronic and juvenile periodontitis.\n\nAmpicillin and amoxicillin: These antibiotics are a part of the penicillin group of antibiotics but are effective against a broader range of organisms. Amoxicillin is a derivative of ampicillin. In Dentistry, Ampicillin is sometimes used when dealing with dentoalveolar infections, when the antibiotic sensitivity patterns of the causative organisms are unknown. Antibiotics are no longer used as prophylactic treatment of infective endocarditis in the UK, however, Amoxicillin was once used for prophylaxis of infective endocarditis in patients who have undergone oral surgery or deep scaling.\n\nWhile effective, ampicillin is associated with a higher incidence of drug rashes than penicillin and thus, should not be prescribed to patients suffering from Infectious mononucleosis or lymphocytic leukaemia as there is a higher risk of developing a drug rash.\n\nErythromycin: This is a wide spectrum antibiotic that has a similar range on the antibacterial spectrum to penicillin, making it the ideal first choice if patients are allergic to penicillin. It is also useful for treatment against B-lactamase-producing bacteria although it is not particularly as effective against oral and dental infections, due to such infections usually being caused by obligate anaerobes.\n\nCephalosporin: This is an example of a wide spectrum antibiotic that is relatively stable to staphylococcal penicillinase although this stability varies with different cephalosporins. Certain cephalosporins in dentistry can be administered orally while others can be given by injections. In the case of an allergy to penicillin, cephalosporins may be a suitable alternative.\n\nMetronidazole: This is an antimicrobial effective against some protozoa and strict anaerobes. In the UK, it has effective use in dentistry as it is the primary drug prescribed for acute ulcerative gingivitis. It is also sometimes used either alongside penicillin or alone against dentoalveolar infections with the advantage of having a low allergenicity. Mild side effects of metronidazole include transient rashes, furred tongue, an unpleasant taste in the mouth alongside several other side effects not restricted to the oral cavity.\n\nSulphonamides : This a group of drugs which is used in dentistry as they have a major advantage of being able to penetrate cerebrospinal fluid and this is particularly relevant when prescribing antibiotics, prophylactically against bacterial meningitis in patients who have had severe maxillofacial injuries, where the risk of infection is high. There are various other uses for sulphonamides as treatment with other parts of the body.\n\nCotrimoxazole: This is an antibiotic which incorporates sulphonamides and trimethoprim. It covers a broad spectrum of activity and in dentistry, is often used where there are clear signs and indications of bacterial infection that is sensitive to cotrimoxazole. This is determined by bacteriological sensitivity tests.\n"}
{"id": "25678949", "url": "https://en.wikipedia.org/wiki?curid=25678949", "title": "Automated ECG interpretation", "text": "Automated ECG interpretation\n\nAutomated ECG interpretation is the use of artificial intelligence and pattern recognition software and knowledge bases to carry out automatically the interpretation, test reporting, and computer-aided diagnosis of electrocardiogram tracings obtained usually from a patient.\n\nThe first automated ECG programs were developed in the 1970s, when digital ECG machines became possible by third generation digital signal processing boards. Commercial models, such as those developed by Hewlett-Packard, incorporated these programs into clinically used devices.\n\nDuring the 1980s and 1990s, extensive research was carried out by companies and by university labs in order to improve the accuracy rate, which was not very high in the first models. For this purpose, several signal databases with normal and abnormal ECGs were built by institutions such as MIT and used to test the algorithms and their accuracy.\n\n\nThe manufacturing industries of ECG machines is now entirely digital, and many models incorporate embedded software for analysis and interpretation of ECG recordings with 3 or more leads. Consumer products, such as home ECG recorders for simple, 1-channel heart arrhythmia detection, also use basic ECG analysis, essentially to detect abnormalities. Some application areas are:\n\nThe automated ECG interpretation is a useful tool when access to a specialist is not possible. Although considerable effort has been made to improve automated ECG algorithms, the sensitivity of the automated ECG interpretation is of limited value in the case of STEMI equivalent as for example with \"hyperacute T waves\", de Winter ST-T complex, Wellens phenomenon, Left ventricular hypertrophy, left bundle branch block or in presence of a pacemaker. Automated monitoring of ST-segment during patient transport is increasingly used and improves STEMI detection sensitivity, as ST elevation is a dynamical phenomenon.\n\n\n<br>Translated and reproduced by permission of the author.\n\n"}
{"id": "4827", "url": "https://en.wikipedia.org/wiki?curid=4827", "title": "Biomedical engineering", "text": "Biomedical engineering\n\nBiomedical Engineering (BME) or Medical Engineering is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g. diagnostic or therapeutic). This field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy.\nAlso included under the scope of a biomedical engineer is the management of current medical equipment within hospitals while adhering to relevant industry standards. This involves equipment recommendations, procurement, routine testing and preventative maintenance, through to decommissioning and disposal. This role is also known as a Biomedical Equipment Technician (BMET) or clinical engineering. \n\nBiomedical engineering has recently emerged as its own study, as compared to many other engineering fields. Such an evolution is common as a new field transition from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as MRIs and EKG/ECGs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.\n\nBioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data.\n\nBioinformatics is considered both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis \"pipelines\" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.\n\nBiomechanics is the study of the structure and function of the mechanical aspects of biological systems, at any level from whole organisms to organs, cells and cell organelles, using the methods of mechanics.\n\nA biomaterial is any matter, surface, or construct that interacts with living systems. As a science, biomaterials is about fifty years old. The study of biomaterials is called biomaterials science or biomaterials engineering. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.\n\nBiomedical optics refers to the interaction of biological tissue and light, and how this can be exploited for sensing, imaging, and treatment.\n\nTissue engineering, like genetic engineering (see below), is a major segment of biotechnology – which overlaps significantly with BME.\n\nOne of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. Biomedical engineers are currently researching methods of creating such organs. Researchers have grown solid jawbones and tracheas from human stem cells towards this end. Several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients. Bioartificial organs, which use both synthetic and biological component, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.\n\nGenetic engineering, recombinant DNA technology, genetic modification/manipulation (GM) and gene splicing are terms that apply to the direct manipulation of an organism's genes. Unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. Genetic engineering techniques have found success in numerous applications. Some examples include the improvement of crop technology (\"not a medical application\", but see biological systems engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.\n\nNeural engineering (also known as neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. Neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.\n\nPharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of Chemical Engineering, and Pharmaceutical Analysis. It may be deemed as a part of pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment. The ISPE is an international body that certifies this now rapidly emerging interdisciplinary science.\n\nThis is an \"extremely broad category\"—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism.\n\nA medical device is intended for use in:\n\nSome examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.\nStereolithography is a practical example of \"medical modeling\" being used to create physical objects. Beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies, treatments, patient monitoring, of complex diseases.\n\nMedical devices are regulated and classified (in the US) as follows (see also \"Regulation\"):\n\nMedical/biomedical imaging is a major segment of medical devices. This area deals with enabling clinicians to directly or indirectly \"view\" things not visible in plain sight (such as due to their size, and/or location). This can involve utilizing ultrasound, magnetism, UV, radiology, and other means.\n\nImaging technologies are often essential to medical diagnosis, and are typically the most complex equipment found in a hospital including: fluoroscopy, magnetic resonance imaging (MRI), nuclear medicine, positron emission tomography (PET), PET-CT scans, projection radiography such as X-rays and CT scans, tomography, ultrasound, optical microscopy, and electron microscopy.\n\nAn implant is a kind of medical device made to replace and act as a missing biological structure (as compared with a transplant, which indicates transplanted biomedical tissue). The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone or apatite depending on what is the most functional. In some cases, implants contain electronics, e.g. artificial pacemakers and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.\n\nArtificial body part replacements are one of the many applications of bionics. Concerned with the intricate and thorough study of the properties and function of human body systems, bionics may be applied to solve some engineering problems. Careful study of the different functions and processes of the eyes, ears, and other organs paved the way for improved cameras, television, radio transmitters and receivers, and many other useful tools. These developments have indeed made our lives better, but the best contribution that bionics has made is in the field of biomedical engineering (the building of useful replacements for various parts of the human body). Modern hospitals now have available spare parts to replace body parts badly damaged by injury or disease [Citation Needed]. Biomedical engineers work hand in hand with doctors to build these artificial body parts.\n\nClinical engineering is the branch of biomedical engineering dealing with the actual implementation of medical equipment and technologies in hospitals or other clinical settings. Major roles of clinical engineers include training and supervising biomedical equipment technicians (BMETs), selecting technological products/services and logistically managing their implementation, working with governmental regulators on inspections/audits, and serving as technological consultants for other hospital staff (e.g. physicians, administrators, I.T., etc.). Clinical engineers also advise and collaborate with medical device producers regarding prospective design improvements based on clinical experiences, as well as monitor the progression of the state of the art so as to redirect procurement patterns accordingly.\n\nTheir inherent focus on \"practical\" implementation of technology has tended to keep them oriented more towards \"incremental\"-level redesigns and reconfigurations, as opposed to revolutionary research & development or ideas that would be many years from clinical adoption; however, there is a growing effort to expand this time-horizon over which clinical engineers can influence the trajectory of biomedical innovation. In their various roles, they form a \"bridge\" between the primary designers and the end-users, by combining the perspectives of being both 1) close to the point-of-use, while 2) trained in product and process engineering. Clinical engineering departments will sometimes hire not just biomedical engineers, but also industrial/systems engineers to help address operations research/optimization, human factors, cost analysis, etc. Also see safety engineering for a discussion of the procedures used to design safe systems.\n\nRehabilitation engineering is the systematic application of engineering sciences to design, develop, adapt, test, evaluate, apply, and distribute technological solutions to problems confronted by individuals with disabilities. Functional areas addressed through rehabilitation engineering may include mobility, communications, hearing, vision, and cognition, and activities associated with employment, independent living, education, and integration into the community.\n\nWhile some rehabilitation engineers have master's degrees in rehabilitation engineering, usually a subspecialty of Biomedical engineering, most rehabilitation engineers have undergraduate or graduate degrees in biomedical engineering, mechanical engineering, or electrical engineering. A Portuguese university provides an undergraduate degree and a master's degree in Rehabilitation Engineering and Accessibility. Qualification to become a Rehab' Engineer in the UK is possible via a University BSc Honours Degree course such as Health Design & Technology Institute, Coventry University.\n\nThe rehabilitation process for people with disabilities often entails the design of assistive devices such as Walking aids intended to promote inclusion of their users into the mainstream of society, commerce, and recreation.\n\nRegulatory issues have been constantly increased in the last decades to respond to the many incidents caused by devices to patients. For example, from 2008 to 2011, in US, there were 119 FDA recalls of medical devices classified as class I. According to U.S. Food and Drug Administration (FDA), Class I recall is associated to \"a situation in which there is a reasonable probability that the use of, or exposure to, a product will cause serious adverse health consequences or death\"\n\nRegardless of the country-specific legislation, the main regulatory objectives coincide worldwide. For example, in the medical device regulations, a product must be: 1) safe \"and\" 2) effective and 3) for all the manufactured devices\n\nA product is safe if patients, users and third parties do not run unacceptable risks of physical hazards (death, injuries, …) in its intended use. Protective measures have to be introduced on the devices to reduce residual risks at acceptable level if compared with the benefit derived from the use of it.\n\nA product is effective if it performs as specified by the manufacturer in the intended use. Effectiveness is achieved through clinical evaluation, compliance to performance standards or demonstrations of substantial equivalence with an already marketed device.\n\nThe previous features have to be ensured for all the manufactured items of the medical device. This requires that a quality system shall be in place for all the relevant entities and processes that may impact safety and effectiveness over the whole medical device lifecycle.\n\nThe medical device engineering area is among the most heavily regulated fields of engineering, and practicing biomedical engineers must routinely consult and cooperate with regulatory law attorneys and other experts. The Food and Drug Administration (FDA) is the principal healthcare regulatory authority in the United States, having jurisdiction over medical \"devices, drugs, biologics, and combination\" products. The paramount objectives driving policy decisions by the FDA are safety and effectiveness of healthcare products that have to be assured through a quality system in place as specified under 21 CFR 829 regulation. In addition, because biomedical engineers often develop devices and technologies for \"consumer\" use, such as physical therapy devices (which are also \"medical\" devices), these may also be governed in some respects by the Consumer Product Safety Commission. The greatest hurdles tend to be 510K \"clearance\" (typically for Class 2 devices) or pre-market \"approval\" (typically for drugs and class 3 devices).\n\nIn the European context, safety effectiveness and quality is ensured through the \"Conformity Assessment\" that is defined as \"the method by which a manufacturer demonstrates that its device complies with the requirements of the European Medical Device Directive\". The directive specifies different procedures according to the class of the device ranging from the simple Declaration of Conformity (Annex VII) for Class I devices to EC verification (Annex IV), Production quality assurance (Annex V), Product quality assurance (Annex VI) and Full quality assurance (Annex II). The Medical Device Directive specifies detailed procedures for Certification. In general terms, these procedures include tests and verifications that are to be contained in specific deliveries such as the risk management file, the technical file and the quality system deliveries. The risk management file is the first deliverable that conditions the following design and manufacturing steps. Risk management stage shall drive the product so that product risks are reduced at an acceptable level with respect to the benefits expected for the patients for the use of the device. The technical file contains all the documentation data and records supporting medical device certification. FDA technical file has similar content although organized in different structure. The Quality System deliverables usually includes procedures that ensure quality throughout all product life cycle. The same standard (ISO EN 13485) is usually applied for quality management systems in US and worldwide.\n\nIn the European Union, there are certifying entities named \"Notified Bodies\", accredited by European Member States. The Notified Bodies must ensure the effectiveness of the certification process for all medical devices apart from the class I devices where a declaration of conformity produced by the manufacturer is sufficient for marketing. Once a product has passed all the steps required by the Medical Device Directive, the device is entitled to bear a CE marking, indicating that the device is believed to be safe and effective when used as intended, and, therefore, it can be marketed within the European Union area.\n\nThe different regulatory arrangements sometimes result in particular technologies being developed first for either the U.S. or in Europe depending on the more favorable form of regulation. While nations often strive for substantive harmony to facilitate cross-national distribution, philosophical differences about the \"optimal extent\" of regulation can be a hindrance; more restrictive regulations seem appealing on an intuitive level, but critics decry the tradeoff cost in terms of slowing access to life-saving developments.\n\nDirective 2011/65/EU, better known as RoHS 2 is a recast of legislation originally introduced in 2002. The original EU legislation \"Restrictions of Certain Hazardous Substances in Electrical and Electronics Devices\" (RoHS Directive 2002/95/EC) was replaced and superseded by 2011/65/EU published in July 2011 and commonly known as RoHS 2.\nRoHS seeks to limit the dangerous substances in circulation in electronics products, in particular toxins and heavy metals, which are subsequently released into the environment when such devices are recycled.\n\nThe scope of RoHS 2 is widened to include products previously excluded, such as medical devices and industrial equipment. In addition, manufacturers are now obliged to provide conformity risk assessments and test reports – or explain why they are lacking. For the first time, not only manufacturers, but also importers and distributors share a responsibility to ensure Electrical and Electronic Equipment within the scope of RoHS comply with the hazardous substances limits and have a CE mark on their products.\n\nThe new International Standard IEC 60601 for home healthcare electro-medical devices defining the requirements for devices used in the home healthcare environment. IEC 60601-1-11 (2010) must now be incorporated into the design and verification of a wide range of home use and point of care medical devices along with other applicable standards in the IEC 60601 3rd edition series.\n\nThe mandatory date for implementation of the EN European version of the standard is June 1, 2013. The US FDA requires the use of the standard on June 30, 2013, while Health Canada recently extended the required date from June 2012 to April 2013. The North American agencies will only require these standards for new device submissions, while the EU will take the more severe approach of requiring all applicable devices being placed on the market to consider the home healthcare standard.\n\nAS/ANS 3551:2012 is the Australian and New Zealand standards for the management of medical devices. The standard specifies the procedures required to maintain a wide range of medical assets in a clinical setting (e.g. Hospital). The standards are based on the IEC 606101 standards.\n\nThe standard covers a wide range of medical equipment management elements including, procurement, acceptance testing, maintenance (electrical safety and preventative maintenance testing) and decommissioning.\n\nBiomedical engineers require considerable knowledge of both engineering and biology, and typically have a Bachelor's (B.Tech., B.S.) or Master's (M.S., M.Tech., M.S.E., or M.Eng.) or a doctoral (Ph.D.) degree in BME (Biomedical Engineering) or another branch of engineering with considerable potential for BME overlap. As interest in BME increases, many engineering colleges now have a Biomedical Engineering Department or Program, with offerings ranging from the undergraduate (B.Tech., B.S., B.Eng. or B.S.E.) to doctoral levels. Biomedical engineering has only recently been emerging as \"its own discipline\" rather than a cross-disciplinary hybrid specialization of other disciplines; and BME programs at all levels are becoming more widespread, including the Bachelor of Science in Biomedical Engineering which actually includes so much biological science content that many students use it as a \"pre-med\" major in preparation for medical school. The number of biomedical engineers is expected to rise as both a cause and effect of improvements in medical technology.\n\nIn the U.S., an increasing number of undergraduate programs are also becoming recognized by ABET as accredited bioengineering/biomedical engineering programs. Over 65 programs are currently accredited by ABET.\n\nIn Canada and Australia, accredited graduate programs in biomedical engineering are common, for example in universities such as McMaster University, and the first Canadian undergraduate BME program at Ryerson University offering a four-year B.Eng. program. The Polytechnique in Montreal is also offering a bachelors's degree in biomedical engineering.\n\nAs with many degrees, the reputation and ranking of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees are also linked to the institution's graduate or research programs, which have some tangible factors for rating, such as research funding and volume, publications and citations. With BME specifically, the ranking of a university's hospital and medical school can also be a significant factor in the perceived prestige of its BME department/program.\n\nGraduate education is a particularly important aspect in BME. While many engineering fields (such as mechanical or electrical engineering) do not need graduate-level training to obtain an entry-level job in their field, the majority of BME positions do prefer or even require them. Since most BME-related professions involve scientific research, such as in pharmaceutical and medical device development, graduate education is almost a requirement (as undergraduate degrees typically do not involve sufficient research training and experience). This can be either a Masters or Doctoral level degree; while in certain specialties a Ph.D. is notably more common than in others, it is hardly ever the majority (except in academia). In fact, the perceived need for some kind of graduate credential is so strong that some undergraduate BME programs will actively discourage students from majoring in BME without an expressed intention to also obtain a master's degree or apply to medical school afterwards.\n\nGraduate programs in BME, like in other scientific fields, are highly varied, and particular programs may emphasize certain aspects within the field. They may also feature extensive collaborative efforts with programs in other fields (such as the University's Medical School or other engineering divisions), owing again to the interdisciplinary nature of BME. M.S. and Ph.D. programs will typically require applicants to have an undergraduate degree in BME, or \"another engineering\" discipline (plus certain life science coursework), or \"life science\" (plus certain engineering coursework).\n\nEducation in BME also varies greatly around the world. By virtue of its extensive biotechnology sector, its numerous major universities, and relatively few internal barriers, the U.S. has progressed a great deal in its development of BME education and training opportunities. Europe, which also has a large biotechnology sector and an impressive education system, has encountered trouble in creating uniform standards as the European community attempts to supplant some of the national jurisdictional barriers that still exist. Recently, initiatives such as BIOMEDEA have sprung up to develop BME-related education and professional standards. Other countries, such as Australia, are recognizing and moving to correct deficiencies in their BME education. Also, as high technology endeavors are usually marks of developed nations, some areas of the world are prone to slower development in education, including in BME.\n\nAs with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered Professional Engineer (PE), but, in US, in industry such a license is not required to be an employee as an engineer in the majority of situations (due to an exception known as the industrial exemption, which effectively applies to the vast majority of American engineers). The US model has generally been only to require the practicing engineers offering engineering services that impact the public welfare, safety, safeguarding of life, health, or property to be licensed, while engineers working in private industry without a direct offering of engineering services to the public or other businesses, education, and government need not be licensed. This is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine.\n\nBiomedical engineering is regulated in some countries, such as Australia, but registration is typically only recommended and not required.\n\nIn the UK, mechanical engineers working in the areas of Medical Engineering, Bioengineering or Biomedical engineering can gain Chartered Engineer status through the Institution of Mechanical Engineers. The Institution also runs the Engineering in Medicine and Health Division. The Institute of Physics and Engineering in Medicine (IPEM) has a panel for the accreditation of MSc courses in Biomedical Engineering and Chartered Engineering status can also be sought through IPEM.\n\nThe Fundamentals of Engineering exam – the first (and more general) of two licensure examinations for most U.S. jurisdictions—does now cover biology (although technically not BME). For the second exam, called the Principles and Practices, Part 2, or the Professional Engineering exam, candidates may select a particular engineering discipline's content to be tested on; there is currently not an option for BME with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). However, the Biomedical Engineering Society (BMES) is, as of 2009, exploring the possibility of seeking to implement a BME-specific version of this exam to facilitate biomedical engineers pursuing licensure.\n\nBeyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. One such example is the Certified Clinical Engineer (CCE) certification for Clinical engineers.\n\nIn 2012 there were about 19,400 biomedical engineers employed in the US, and the field was predicted to grow by 27% (much faster than average) from 2012 to 2022. Biomedical engineering has the highest percentage of women engineers compared to other common engineering professions.\n\n\n\n\n\n\n\n\n"}
{"id": "40872662", "url": "https://en.wikipedia.org/wiki?curid=40872662", "title": "Boston Association for Childbirth Education", "text": "Boston Association for Childbirth Education\n\nThe Boston Association for Childbirth Education (BACE) was established in 1953 by students of Jean Whiffen. BACE was one of the first American organizations focused on natural childbirth and maternal-based obstetrical care, and in 1962 sponsored the first breastfeeding support group in New England.\n\nBACE's founders were influenced by Grantly Dick-Read's ideas about natural childbirth. During the first five years of its existence, BACE organized small group meetings in individual women's homes. These were focused on teaching prospective parents about pregnancy, labor and delivery, breastfeeding, and other topics in order to encourage natural childbirth. In the 1960s, BACE began to incorporate trainings encouraging father-centered birth coaching.\n\nNurse Justine Kelliher (b. 1920) was one of the first instructors. Many women who BACE trained as lay breastfeeding instructors and supporters as part of the Nursing Mother's Council also later served as La Leche League International, an international nonprofit organization that distributes information on and promotes breastfeeding, leaders. \n\nIn 1960, BACE was one of the organizations represented at the founding convention of the International Childbirth Education Association (ICEA). \n\nBACE currently provides training and certification to become a childbirth educator.\n\n"}
{"id": "24271780", "url": "https://en.wikipedia.org/wiki?curid=24271780", "title": "Chinese National Human Genome Center, Beijing", "text": "Chinese National Human Genome Center, Beijing\n\nCHGB takes on Chinese important scientific and technological projects and impels genomics and proteomics in the world. In next several years, CHGB continues to serve for and support China important scientific and technological projects, set up and develop functional genomics research platform, train an excellent group, aiming at promoting competitive power and CHGB's value.\n\nCHGB promotes the commercialization of research products and initiate genome industry in China. As a national research institution, CHGB integrates all high-level activities in basic research, clinical investigation, population genetics and bioinformatics projects in Beijing and North China. \n\nProf. Boqin Qiang, academician of CAS, is Director and Chief Scientist of CHGB. Prof. Wu Min, academician of CAS, is the honorary Chairman of the academic committee. Prof. Yan Shen, academician of CAS, Prof. Fuchu He, academician of CAS, Prof. Dalong Ma, and Prof. Biao Chen are deputy directors of CHGB.\n\n\n"}
{"id": "832314", "url": "https://en.wikipedia.org/wiki?curid=832314", "title": "Clinic", "text": "Clinic\n\nA clinic (or outpatient clinic or ambulatory care clinic) is a healthcare facility that is primarily focused on the care of outpatients. Clinics can be privately operated or publicly managed and funded. They typically cover the primary healthcare needs of populations in local communities, in contrast to larger hospitals which offer specialised treatments and admit inpatients for overnight stays. \n\nMost commonly, the word clinic in English refers to a general medical practice, run by one or more general practitioners, but it can also mean a specialist clinic. Some clinics retain the name “clinic\" even while growing into institutions as large as major hospitals or becoming associated with a hospital or medical school.\n\nClinics are often associated with a general medical practice run by one or several general practitioners. Other types of clinics are run by the type of specialist associated with that type: physical therapy clinics by physiotherapists and psychology clinics by clinical psychologists, and so on for each health profession. (This can even hold true for certain services outside the medical field: for example, legal clinics are run by lawyers.)\n\nSome clinics are operated in-house by employers, government organizations, or hospitals, and some clinical services are outsourced to private corporations which specialize in providing health services. In China, for example, owners of such clinics do not have formal medical education. There were 659,596 village clinics in China in 2011. \n\nHealth care in India, China, Russia and Africa is provided to those countries' vast rural areas by mobile health clinics or roadside dispensaries, some of which integrate traditional medicine. In India these traditional clinics provide ayurvedic medicine and unani herbal medical practice. In each of these countries, traditional medicine tends to be a hereditary practice.\n\nThe word \"clinic\" derives from Ancient Greek \"klinein\" meaning to slope, lean or recline. Hence \"klinē\" is a couch or bed and \"klinikos\" is a physician who visits his patients in their beds. In Latin, this became \"clīnicus\". \n\nAn early use of the word clinic was \"one who receives baptism on a sick bed\".\n\nThe function of clinics differs from country to country. For instance, a local general practice run by a single general practitioner provides primary health care and is usually run as a for-profit business by the owner, whereas a government-run specialist clinic may provide subsidised or specialised health care.\n\nSome clinics function as a place for people with injuries or illnesses to come and be seen by a triage nurse or other health worker. In these clinics, the injury or illness may not be serious enough to require a visit to an emergency room (ER), but the person can be transferred to one if needed. \n\nTreatment at these clinics is often less expensive than it would be at a casualty department. Also, unlike an ER these clinics are often not open on a 24 x 7 x 365 basis. They sometimes have access to diagnostic equipment such as X-ray machines, especially if the clinic is part of a larger facility. Doctors at such clinics can often refer patients to specialists if the need arises.\n\nLarge outpatient clinics vary in size, but can be as large as hospitals.\n\nTypical large outpatient clinics house general medical practitioners (GPs) such as doctors and nurses to provide ambulatory care and some acute care services but lack the major surgical and pre- and post-operative care facilities commonly associated with hospitals. \n\nBesides GPs, if a clinic is a polyclinic, it can house outpatient departments of some medical specialties, such as gynecology, dermatology, ophthalmology, otolaryngology, neurology, pulmonology, cardiology, and endocrinology. In some university cities, polyclinics contain outpatient departments for the entire teaching hospital in one building.\n\nLarge outpatient clinics are a common type of healthcare facility in many countries, including France, Germany (long tradition), Switzerland, and most of the countries of Central and Eastern Europe (often using a mixed Soviet-German model), as well as in former Soviet republics such as Russia and Ukraine; and in many countries across Asia and Africa. \n\nRecent Russian governments have attempted to replace the polyclinic model introduced during Soviet times with a more western model. However, this has failed. \n\nIndia has also set up huge numbers of polyclinics for former defence personnel. The network envisages 426 polyclinics in 343 districts of the country which will benefit about 33 lakh (3.3 million) ex-servicemen residing in remote and far-flung areas. \n\nPolyclinics are also the backbone of Cuba's primary care system and have been credited with a role in improving that nation's health indicators.\n\nThere are many different types of clinics providing outpatient services. Such clinics may be public (government-funded) or private medical practices.\n\n\n"}
{"id": "18384766", "url": "https://en.wikipedia.org/wiki?curid=18384766", "title": "Complementary and Natural Healthcare Council", "text": "Complementary and Natural Healthcare Council\n\nThe Complementary and Natural Healthcare Council (CNHC) is a regulatory body in the United Kingdom which provides a voluntary register of complementary, rather than alternative medicine, therapists. The key purpose of CNHC is to act in the public interest and enable proper public accountability of the complementary therapists that it registers. \n\nThe CHNC was founded in 2008 with government funding and support and became fully operational in early 2009. In 2013 it was approved as the holder of an 'Accredited Voluntary Register' by the Professional Standards Authority for Health and Social Care (PSA). In December 2014 it became an 'Accredited Register', for the PSA.\n\nIn November 2000, the House of Lords Select Committee on Science and Technology reported on complementary and alternative medicine and considered the public health policy needs and NHS provision of these treatments. In one of its many areas of consideration, the report considered the needs to provide public protection by regulating practitioners. It noted that those practices that could injure patients were either already statutorily regulated (chiropractic and osteopathy) or were soon to be (herbalism and acupuncture). \n\nThe remaining largely placebo based therapies and those without a sound evidence base for their efficacy and robust regulatory systems (e.g. reiki, massage therapy, aromatherapy, yoga and homeopathy), suffered from having a large number of fragmented registration bodies with considerable diversity of standards. The House of Lords found this unacceptable and that \"in the best interests of their patients such therapies must each strive to unite under a single voluntary regulatory body\".\n\nThe House of Lords described the necessary features of an effective voluntary self-regulatory body. These included having a register of members, educational standards, a code of ethics and practice, a public complaints mechanism, and the capacity to represent the whole profession.\n\nFollowing publication of the report, the Department of Health (DH) asked The Prince's Foundation for Integrated Health (FIH), a not for profit organisation founded by HRH The Prince of Wales, to facilitate the development of a federal 'umbrella' regulator for these therapies. The Foundation promoted the inclusion of non-evidence based alternative therapies into public healthcare in the UK. but is now defunct. The process was funded by a DH grant of £900,000 over a three-year period from 2005 to 2008.\n\nOn behalf of the FIH, Professor Dame Joan Higgins was asked to be Chair of a Federal Working Group which was to look into setting up what was to become the Complementary and Natural Healthcare Council. Therapies who participated in the Working Group were Alexander technique, aromatherapy, Bowen technique, cranial therapy, homeopathy, massage therapy, naturopathy, nutritional therapy, reflexology, reiki, shiatsu and yoga therapy. Other eligible therapists were aromatherapists, reflexologists and reiki practitioners, although these practices were not represented.\n\nThe Complementary and Natural Healthcare Council is a not-for-profit private limited company made up of three elements:\n\nThe Council has five lay and four registrant members. Each Profession Specific Board has up to four registrants from the relevant profession. The Professional Committee is a pool of lay people from which investigating, conduct & competence and Health Panels are drawn.\n\nThe CNHC’s Code of Conduct, Performance and Ethics states:\n\nIn December 2008, CNHC stated on their website that they hoped to have 10,000 practitioners registered with them by the end of 2009. This was later amended without comment to 4,000 by Spring 2010. However, by August 2009 a total of only about 500 registrations had been made in four disciplines: Massage Therapy, Nutritional Therapy, Aromatherapy and (from 24 August) Reflexology.\n\nBy February 2011 practitioners in eleven disciplines were eligible, but according to the organisation's website the total number of registrants was still less than 4,000.\n\nLack of enthusiasm for the CNHC among practitioners may be partly ascribed to the fact that at present anyone may legally practise in the UK without qualifications as a reflexologist, aromatherapist, homeopath, naturopath, nutritional therapist, acupuncturist, etc., and that voluntary registration by the CHNC will make no difference to this.\n\nIn response to a Freedom of Information request, the Department of Health has confirmed that since the CNHC was set up the DH has provided funding as follows:\n£293,496 in 2007/8/9 (including start up costs), £409,300 in 2009/10 and £127,748 in 2010/11\n\nRequest was made for funding to be continued but this was dependent on satisfactory progress having been made, and as this was not the case official funding ceased in March 2011. \n\nThe CNHC do not publish details of the number of registrants it has attracted, but these are certainly only a fraction of the target of 10,000 set in 2007 and it is unlikely that income from fees is currently sufficient to cover running costs.\n\nFollowing complaints submitted by Simon Perry, a blogger and member of Leicester Sceptics in the Pub, regarding 14 reflexologists claiming to treat specific diseases without any credible evidence, Maggie Dunn, CEO of CNHC, has said that they will tighten up on therapists making unjustified claims for which they have no evidence. CNHC reviewed the claims made by the reflexologists against the Advertising Standards Authority (ASA) guidelines - and told the reflexologists to remove their claims. In a personal communication to Simon Perry from Maggie Dunn, the CNHC’s Chief Executive Officer, CNHC said that in the future:\n\nHowever CNHC has made no public statement in this regard - possibly because many therapists will not want their public and private claims for efficacy to be regulated.\nThe CNHC has attracted criticism, mainly for its role in appearing to legitimize what critics regard as quackery and for its efforts to promote alternate therapies which are often of dubious or unproven efficacy. It is satirized by skeptics as \"OfQuack\", mimicking other bodies such as OfCom.\n\"How does a regulator decide what is good practice and what is charlatanry when none of it has peer-reviewed, scientific evidence that it works?... Professor Michael Baum protested that 'this is like licensing a witches' brew as a medicine so long as the batwings are sterile'... It matters that Newsnight found homeopaths advising patients visiting malaria areas not to take anti-malarial drugs. And that patients are told not to give their children the MMR jab.\" Polly Toynbee, \"The Guardian\".\n\n\"Private Eye\" magazine argued that the council had a conflict of interests between promoting and regulating alternative medicine: \"Everyone would say that propagandists cannot be regulators because they cannot be trusted to act in the public interest.\" It quoted Edzard Ernst, professor of complementary medicine at the University of Exeter, as saying: \"This is ridiculous. It really is a farce. All they will need to prove is that [practitioners] are following an established technique they believe to be appropriate. It's a ludicrous system.\" \n\n\"Private Eye\" and \"The Economist\" reported that critics have nicknamed the council 'OfQuack'. \"Private Eye\" quoted a joke slogan, 'OfQuack - making quacks look professional since 2008'.\n\nIn January 2009, an online petition was started at the UK Government Petition website, asking for stricter requirements on efficacy and safety as a condition of certification. An official response to this was posted, but simply reiterated the terms of the current requirements. However, in response to the point made by the petitioners and others, the CNHC has now amended its website, deleting its original statement that regulation by them gives a guarantee of the efficacy of the procedures carried out by their registrants.\nIn addition to the medical criticism, CNHC have also been censured by the British Standards Institute for use of their trademarked term \"kitemark\", and have also been criticised for poor openness and an inconsistent approach to data protection.\nIt has, to date, failed to register more than a tiny proportion of the tens of thousands of CAM practitioners in the UK; it is argued that it is failing in its stated aim to protect the public against incompetence or malpractice in the disciplines it claims to regulate, let alone in those - homeopathy for example - whose practitioners have no interest in the CNHC and claim to regulate themselves. While there has been considerable criticism of CNHC with regard to \"quack\" therapies their Code of Practise may support an evidence-based approach.\n"}
{"id": "37781855", "url": "https://en.wikipedia.org/wiki?curid=37781855", "title": "Cosmetic Valley", "text": "Cosmetic Valley\n\nThe Cosmetic Valley or pôle cosmétique sciences de la beauté et du bien-être (in English, cosmetic science of beauty and well-being business cluster) is a technopole, the most important French business cluster specialized in the production of consumer goods in the industry of perfumes and cosmetics in France.\n\nCreated in 1994, it is mainly located in the departments of Eure-et-Loir and Loiret but also extends to the neighboring departments.\n\nThe Cosmetic Valley stretches across three regions and seven departments: Centre-Val de Loire (Eure-et-Loir, Indre-et-Loire, Loiret and Loir-et-Cher), Île-de-France (Yvelines and Val-d'Oise) and Normandy (Eure).\nThe headquarters of the cluster are located on the site of the Chartres Cathedral.\n\nEure-et-Loir has nearly 70% of companies in the sector. To department officials, the Cosmetic Valley is an essential element of the attractiveness of the territory, which would particularly illustrated by the introduction of foreign groups such as Reckitt Benckiser, but also small and medium enterprises (SMEs/SMBs) who joined the sector. Cosmetic Valley boasts the creation of over 1,500 jobs in the department in ten years.\n\n\n\nCosmetic Valley has been founded in 1994 on the initiative of Jean-Paul Guerlain in the region of Chartres. This is the first industrial sector emerged in Eure-et-Loir.\n\nIn 2005, the industries of Eure-et-Loir come together with those of the Loiret to be labeled \"national competitiveness cluster\" under the supervision of the government of Jean-Pierre Raffarin.\n\nThe 25th of August 2006, a decree defines the area of research and development of the cluster. Municipalities of the department of Eure, Eure-et-Loir, Indre-et-Loire, Loiret and Yvelines are concerned.\n\nIn 2007, stands at Orleans the first edition of \"Cosm'innov\", a convention describing the progress of scientific disciplines in the field of cosmetology.\n\nThe second edition of \"Cosm'innov\" takes place in March 2010 at Orléans].\n\nIn the early 2013, the cluster has 800 companies for approximately 70,000 jobs and €11 billion in revenue, seven universities, 136 colleges, 200 public research laboratories for 8,600 researcher, 100 research projects with a budget of approximately 200 million.\n\nAmong the major companies represented include L'Oréal (via its subsidiaries Maybelline and Yves Saint Laurent Opium), Shiseido (via Jean-Paul Gaultier, Issey Miyake, Serge Lutens), LVMH (via Christian Dior and Guerlain), Caudalie.\n\n"}
{"id": "48669573", "url": "https://en.wikipedia.org/wiki?curid=48669573", "title": "Disability in Pakistan", "text": "Disability in Pakistan\n\nThere were an estimated 3.28 million people with disabilities in Pakistan according to the 1998 census, the most recent available data as of 2015, including 1.37 million women. This would make them 2.49% of the population in that year.\n\nThere are 531 special schools in Pakistan and about 200 non-governmental organizations and disabled people's organizations offering education to people with disabilities.\n\nPakistan is a party to the United Nations Convention on the Rights of Persons with Disabilities, having signed the treaty on 15 September 2008 and ratified it on 5 July 2011.\n\nPakistan have been competing at the Paralympic Games from 1992.So far,Pakistan has received 2 medals in Paralympics history in 2008 Summer Paralympics and in the 2016 Summer Paralympics.\n\nPakistan national blind cricket team has participated in every editions of the Blind Cricket World Cup as well as in the Blind T20 World Cup tournaments.The Pakistan blind cricket team has won 2 Blind Cricket World Cup titles in 2002 and 2006.Hence became the first visually impaired cricket team to win 2 consecutive Blind Cricket World Cup titles. \n\nThe Pakistani blind cricket team also emerged as runners-up to South Africa in the inaugural edition of the Blind Cricket World Cup in 1998.\n\nThe Pakistan blind cricket team also emerged as runners-up to India in both Blind T20 World Cup tournaments.\n\n<nowiki>*</nowiki>Muhammad Akram,blind cricketer-Holds the record for registering the highest individual score in a Blind T20I innings\n\n<nowiki>*</nowiki>Masood Jan,blind cricketer-Holds the record for registering the highest individual innings in a Blind One-Day International\n\n<nowiki>*</nowiki>Haider Ali,Paralympic athlete-Only paralympic medallist for Pakistan in Paralympics history \n\n\n[{Ihsan ullah daudzai'}, activist for the physically and cross person with disabilities www.spdapsh.org\n"}
{"id": "37430845", "url": "https://en.wikipedia.org/wiki?curid=37430845", "title": "Elixia", "text": "Elixia\n\nElixia is a Nordic chain of fitness centers that offers everything from strength training, cardio and weight training to different types of classes, such as aerobics, dance, fitness, martial arts, yoga and cycling hours.\n\nELIXIA Holding AS is the parent company ELIXIA's operations, and the majority shareholder is Altor Fund III. In 2010, the turnover of the group was about NOK 930 million. Elixia Norway has 37 centers, Finland 14 and Sweden three centers. In total Elixia has about 185 000 members.\n\nELIXIA's vision is \"Keep Members for Life\".\n"}
{"id": "55743836", "url": "https://en.wikipedia.org/wiki?curid=55743836", "title": "Epidemiology of plague", "text": "Epidemiology of plague\n\nGlobally about 600 cases of plague are reported a year. In 2017 the countries with the most cases include the Democratic Republic of the Congo, Madagascar, and Peru.\n\nLocal outbreaks of the plague are grouped into three plague pandemics, whereby the respective start and end dates and the assignment of some outbreaks to either pandemic are still subject to discussion. The pandemics were:\nHowever, the late medieval Black Death is sometimes seen not as the start of the second, but as the end of the first pandemic – in that case, the second pandemic's start would be 1361; also vary the end dates of the second pandemic given in literature, e.g. ~1890 instead of ~1840.\n\n\nIn 1994, there was a pneumonic plague epidemic in Surat, India that resulted in 52 deaths and in a large internal migration of about 300,000 residents, who fled fearing quarantine.\n\nA combination of heavy monsoon rain and clogged sewers led to massive flooding which resulted in unhygienic conditions and a number of uncleared animal carcasses. It is believed that this situation precipitated the epidemic. There was widespread fear that the sudden rush of people from this area might spread the epidemic to other parts of India and the world, but that scenario was averted, probably as a result of effective public health response mounted by the Indian health authorities. Some countries, especially those in the nearby Gulf region, took the step of cancelling some flights and putting a pause on shipments from India.\n\nMuch like the Black Death that spread through medieval Europe, some questions still remain unanswered about the 1994 epidemic in Surat.\n\nInitial questions about whether it was an epidemic of plague arose because the Indian health authorities were unable to culture \"Yersinia pestis\", but this could have been due to poor laboratory procedures. Yet several lines of evidence strongly suggest that it was a plague epidemic: blood tests for Yersinia were positive, a number of individuals showed antibodies against Yersinia and the clinical symptoms displayed by the affected were all consistent with the disease being plague.\n\nThe Plague of Justinian in AD 541–542 is the first known attack on record, and marks the first firmly recorded pattern of bubonic plague. This disease is thought to have originated in China. It then spread to Africa from where the huge city of Constantinople imported massive amounts of grain, mostly from Egypt, to feed its citizens. The grain ships were the source of contagion for the city, with massive public granaries nurturing the rat and flea population. At its peak, Procopius said the plague was killing 10,000 people in Constantinople every day. The real number was more likely close to 5,000 a day. The plague ultimately killed perhaps 40% of the city's inhabitants, and then continued to kill up to a quarter of the human population of the eastern Mediterranean.\n\nIn AD 588 a second major wave of plague spread through the Mediterranean into what is now France. It is estimated that the Plague of Justinian killed as many as people across the world. It caused Europe's population to drop by around 50% between 541 and 700. It also may have contributed to the success of the Arab conquests. An outbreak of it in the AD 560s was described in AD 790 as causing \"swellings in the glands ... in the manner of a nut or date\" in the groin \"and in other rather delicate places followed by an unbearable fever\". While the swellings in this description have been identified by some as buboes, there is some contention as to whether the pandemic should be attributed to the bubonic plague, \"Yersinia pestis\", known in modern times.\n\nFrom 1331 to 1351, the Black Death, a massive and deadly pandemic originating in China, spread along the Silk Road and swept through Asia, Europe and Africa. It may have reduced the world's population from to between 350 and . China lost around half of its population, from around to around ; Europe around one third of its population, from about to about ; and Africa approximately of its population, from around to (mortality rates tended to be correlated with population density so Africa, being less dense overall, had the lowest death rate). This makes the Black Death the largest death toll from any known non-viral epidemic. Although accurate statistical data does not exist, it is thought that 1.4 million died in England ( of England's 4.2 million people), while an even higher percentage of Italy's population was likely wiped out. On the other hand, north-eastern Germany, Bohemia, Poland and Hungary are believed to have suffered less, and there are no estimates available for Russia or the Balkans. It is conceivable that Russia may not have been as affected due to its very cold climate and large size, hence often less close contact with the contagion.\n\nThe plague repeatedly returned to haunt Europe and the Mediterranean throughout the 14th to 17th centuries. According to Biraben, plague was present somewhere in Europe in every year between 1346 and 1671. The was particularly widespread in the following years: 1360–1363; 1374; 1400; 1438–1439; 1456–1457; 1464–1466; 1481–1485; 1500–1503; 1518–1531; 1544–1548; 1563–1566; 1573–1588; 1596–1599; 1602–1611; 1623–1640; 1644–1654; and 1664–1667; subsequent outbreaks, though severe, marked the retreat from most of Europe (18th century) and northern Africa (19th century). According to Geoffrey Parker, \"France alone lost almost a million people to plague in the epidemic of 1628–31.\"\n\nIn England, in the absence of census figures, historians propose a range of pre-incident population figures from as high as 7 million to as low as 4 million in 1300, and a postincident population figure as low as 2 million. By the end of 1350, the Black Death subsided, but it never really died out in England. Over the next few hundred years, further outbreaks occurred in 1361–62, 1369, 1379–83, 1389–93, and throughout the first half of the 15th century. An outbreak in 1471 took as much as 10–15% of the population, while the death rate of the plague of 1479–80 could have been as high as 20%. The most general outbreaks in Tudor and Stuart England seem to have begun in 1498, 1535, 1543, 1563, 1589, 1603, 1625, and 1636, and ended with the Great Plague of London in 1665.\nIn 1466, perhaps 40,000 people died of plague in Paris. During the 16th and 17th centuries, plague visited Paris for almost one year out of three. The Black Death ravaged Europe for three years before it continued on into Russia, where the disease hit somewhere once every five or six years from 1350 to 1490. Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, reducing its population by 10 to 30% during those years. Over 10% of Amsterdam's population died in 1623–1625, and again in 1635–1636, 1655, and 1664. There were 22 outbreaks of plague in Venice between 1361 and 1528. The plague of 1576–1577 killed 50,000 in Venice, almost a third of the population. Late outbreaks in central Europe included the Italian Plague of 1629–1631, which is associated with troop movements during the Thirty Years' War, and the Great Plague of Vienna in 1679. Over 60% of Norway's population died from 1348 to 1350. The last plague outbreak ravaged Oslo in 1654.\n\nIn the first half of the 17th century, the Great Plague of Milan claimed some 1.7 million victims in Italy, or about 14% of the population. In 1656, the plague killed about half of Naples' 300,000 inhabitants. More than 1.25 million deaths resulted from the extreme incidence of plague in 17th-century Spain. The plague of 1649 probably reduced the population of Seville by half. In 1709–1713, a plague epidemic that followed the Great Northern War (1700–1721, Sweden v. Russia and allies) killed about 100,000 in Sweden, and 300,000 in Prussia. The plague killed two-thirds of the inhabitants of Helsinki, and claimed a third of Stockholm's population. Western Europe's last major epidemic occurred in 1720 in Marseilles, in Central Europe the last major outbreaks happened during the plague during the Great Northern War, and in Eastern Europe during the Russian plague of 1770–72.\n\nThe Black Death ravaged much of the Islamic world. Plague was present in at least one location in the Islamic world virtually every year between 1500 and 1850. Plague repeatedly struck the cities of North Africa. Algiers lost 30,000–50,000 to it in 1620–1621, and again in 1654–1657, 1665, 1691, and 1740–1742. Plague remained a major event in Ottoman society until the second quarter of the 19th century. Between 1701 and 1750, 37 larger and smaller epidemics were recorded in Constantinople, and 31 between 1751 and 1800. Baghdad has suffered severely from visitations of the plague, and sometimes two-thirds of its population has been wiped out.\n\nThe Third Pandemic began in China's Yunnan province in 1855, spreading plague to all inhabited continents and ultimately killing more than people in India and China alone. Casualty patterns indicate that waves of this pandemic may have come from two different sources. The first was primarily bubonic and was carried around the world through ocean-going trade, transporting infected persons, rats, and cargoes harboring fleas. The second, more virulent strain was primarily pneumonic in character, with a strong person-to-person contagion. This strain was largely confined to Manchuria and Mongolia. Researchers during the \"Third Pandemic\" identified plague vectors and the plague bacterium (see above), leading in time to modern treatment methods.\n\nPlague occurred in Russia in 1877–1889 in rural areas near the Ural Mountains and the Caspian Sea. Efforts in hygiene and patient isolation reduced the spread of the disease, with approximately 420 deaths in the region. Significantly, the region of Vetlianka in this area is near a population of the bobak marmot, a small rodent considered a very dangerous plague reservoir. The last significant Russian outbreak of Plague was in Siberia in 1910 after sudden demand for marmot skins (a substitute for sable) increased the price by 400 percent. The traditional hunters would not hunt a sick Marmot and it was taboo to eat the fat from under the arm (the axillary lymphatic gland that often harboured the plague) so outbreaks tended to be confined to single individuals. The price increase, however, attracted thousands of Chinese hunters from Manchuria who not only caught the sick animals but also ate the fat, which was considered a delicacy. The plague spread from the hunting grounds to the terminus of the Chinese Eastern Railway and then followed the track for 2,700 km. The plague lasted 7 months and killed 60,000 people.\n\nThe bubonic plague continued to circulate through different ports globally for the next fifty years; however, it was primarily found in Southeast Asia. An epidemic in Hong Kong in 1894 had particularly high death rates, 90%. As late as 1897, medical authorities in the European powers organized a conference in Venice, seeking ways to keep the plague out of Europe. Mumbai plague epidemic struck the city of Bombay (Mumbai) in 1896. The disease reached the Territory of Hawaii in December 1899, and the Board of Health's decision to initiate controlled burns of select buildings in Honolulu's Chinatown turned into an uncontrolled fire which led to the inadvertent burning of most of Chinatown on January 20, 1900. Shortly thereafter, plague reached the continental US, initiating the San Francisco plague of 1900–1904. Plague persisted in Hawaii on the outer islands of Maui and Hawaii (The Big Island) until it was finally eradicated in 1960.\n\nResearch done by a team of biologists from the Institute of Pasteur in Paris and Johannes Gutenberg University Mainz in Germany by analyzing the DNA and proteins from plague pits was published in October 2010, reported beyond doubt that all 'the three major plagues' were due to at least two previously unknown strains of \"Yersinia pestis\" and originated from China. A team of medical geneticists led by Mark Achtman of University College Cork in Ireland reconstructed a family tree of the bacterium and concluded in an online issue of Nature Genetics published on 31 October 2010 that all three of the great waves of plague originated from China.\n\nThe word \"plague\" is believed to come from the Latin word \"plāga\" (\"blow, wound\") and \"plangere\" (“to strike, or to strike down”), via the German \"Plage\" (“infestation”).\n\nPlasmids of \"Y. pestis\" have been detected in archaeological samples of the teeth of seven Bronze Age individuals from 5000 years ago (3000 BC), in the Afanasievo culture in Siberia, the Corded Ware culture in Estonia, the Sintashta culture in Russia, the Unetice culture in Poland and the Andronovo culture in Siberia. \"Y. pestis\" existed over Eurasia during the Bronze Age. Estimates of the age of the Most recent common ancestor of all \"Y. pestis\" is estimated at 5,783 years Before Present.\n\nThe \"Yersinia\" murine toxin (\"ymt\") allows the bacteria to infect fleas, which can then transmit bubonic plague. Early ancestral versions of \"Y. pestis\" did not have the \"ymt\" gene, which was only detected in a 951 calibrated BC sample.\n\nThe Amarna letters and the Plague Prayers of Mursili II describe an outbreak of a disease among the Hittites, though some modern sources say it may be Tularemia. The First Book of Samuel describes a possible plague outbreak in Philistia, and the Septuagint version says it was caused by a \"ravaging of mice\".\n\nIn the second year of the Peloponnesian War (430 BC), Thucydides described an epidemic disease which was said to have begun in Ethiopia, passed through Egypt and Libya, then come to the Greek world. In the Plague of Athens, the city lost possibly one third of its population, including Pericles. Modern historians disagree on whether the plague was a critical factor in the loss of the war. Although this epidemic has long been considered an outbreak of plague, many modern scholars believe that typhus, smallpox, or measles may better fit the surviving descriptions. A recent study of DNA found in the dental pulp of plague victims suggests that typhoid was actually responsible.\n\nIn the first century AD, Rufus of Ephesus, a Greek anatomist, refers to an outbreak of plague in Libya, Egypt, and Syria. He records that Alexandrian doctors named Dioscorides and Posidonius described symptoms including acute fever, pain, agitation, and delirium. Buboes—large, hard, and non-suppurating—developed behind the knees, around the elbows, and \"in the usual places.\" The death toll of those infected was very high. Rufus also wrote that similar buboes were reported by a Dionysius Curtus, who may have practiced medicine in Alexandria in the third century BC. If this is correct, the eastern Mediterranean world may have been familiar with bubonic plague at that early date.\n\nIn the second century, the Antonine Plague, named after Marcus Aurelius’ family name of Antoninus and also known as the Plague of Galen, who had first hand knowledge of the disease, may in fact have been smallpox. Galen was in Rome when it struck in 166 AD, and was also present in the winter of 168–69 during an outbreak among troops stationed at Aquileia; he had experience with the epidemic, referring to it as very long lasting, and describes its symptoms and his treatment of it, though his references are scattered and brief. According to Barthold Georg Niebuhr \"this pestilence must have raged with incredible fury; it carried off innumerable victims. The ancient world never recovered from the blow inflected upon it by the plague which visited it in the reign of M. Aurelius.\" The mortality rate of the plague was 7–10 percent; the outbreak in 165/6–168 would have caused approximately 3.5 to 5 million deaths. Otto Seek believes that over half the population of the empire perished. J. F. Gilliam believes that the Antonine plague probably caused more deaths than any other epidemic during the empire before the mid-3rd century.\n"}
{"id": "24082935", "url": "https://en.wikipedia.org/wiki?curid=24082935", "title": "Gender differences in suicide", "text": "Gender differences in suicide\n\nGender differences in suicide rates have been shown to be significant. There are different rates of completed suicides and suicidal behavior between males and females. While women more often have suicidal thoughts, men die by suicide more frequently. This is also known as the gender paradox in suicide.\n\nGlobally, death by suicide occurred about 1.8 times more often among males than among females in 2008, and 1.7 times in 2015. In the western world, males die by suicide three to four times more often than do females. This greater male frequency is increased in those over the age of 65. Suicide attempts are between two and four times more frequent among females. Researchers have attributed the difference between attempted and completed suicides among the sexes to males using more lethal means to end their lives. The extent of suicidal thoughts is not clear, but research suggests that suicidal thoughts are more common among females than among males, particularly in those under the age of 25.\n\nThe role that gender plays as a risk factor for suicide has been studied extensively. While females show higher rates of non-fatal suicidal behavior and suicide ideation (thoughts), and reportedly attempt suicide more frequently than males do, males have a much higher rate of completed suicides.\n\nAs of recent World Health Organization (WHO) releases, challenges represented by social stigma, the taboo to openly discuss suicide, and low availability of data are still to date obstacles leading to poor data quality for both suicide and suicide attempts: \"given the sensitivity of suicide – and the illegality of suicidal behaviour in some countries – it is likely that under-reporting and misclassification are greater problems for suicide than for most other causes of death.\".\n\nMany researchers have attempted to find explanations for why gender is such a significant indicator for suicide.\nA common explanation relies on the social constructions of hegemonic masculinity and femininity. According to literature on gender and suicide, male suicide rates are explained in terms of traditional gender roles. Male gender roles tend to emphasize greater levels of strength, independence, risk-taking behavior, economic status, individualism. Reinforcement of this gender role often prevents males from seeking help for suicidal feelings and depression.\n\nNumerous other factors have been put forward as the cause of the gender paradox. Part of the gap may be explained by heightened levels of stress that result from traditional gender roles. For example, the death of a spouse and divorce are risk factors for suicide in both genders, but the effect is somewhat mitigated for females. In the Western world, females are more likely to maintain social and familial connections that they can turn to for support after losing their spouse. Another factor closely tied to gender roles is employment status. Males' vulnerability may be heightened during times of unemployment because of societal expectations that males should provide for themselves and their families.\n\nIt has been noted that the gender gap is less stark in developing nations. One theory put forward for the smaller gap is the increased burden of motherhood due to cultural norms. In regions where the identity of females is constructed around the family, having young children may correlate with lower risks for suicide. At the same time, stigma attached to infertility or having children outside of marriage can contribute to higher rates of suicide among women.\n\nIn 2003, a group of sociologists examined the gender and suicide gap by considering how cultural factors impacted suicide rates. The four cultural factors – power-distance, individualism, uncertainty avoidance, and masculinity – were measured for 66 countries using data from the World Health Organization. Cultural beliefs regarding individualism were most closely tied to the gender gap; countries that placed a higher value on individualism showed higher rates of male suicide. Power-distance, defined as the social separation of people based on finances or status, was negatively correlated with suicide. However, countries with high levels of power-distance had higher rates of female suicide. The study ultimately found that stabilizing cultural factors had a stronger effect on suicide rates for women than men.\n\nThe reported difference in suicide rates for males and females is partially a result of the methods used by each gender. Although females attempt suicide at a higher rate, they are more likely to use methods that are less immediately lethal. Males frequently complete suicide via high mortality actions such as hanging, carbon-monoxide poisoning, and firearms. This is in contrast to females, who tend to rely on drug overdosing. While overdosing can be deadly, it is less immediate and therefore more likely to be caught before death occurs. In Europe, where the gender discrepancy is the greatest, a study found that the most frequent method of suicide among both genders was hanging; however, the use of hanging was significantly higher in males (54.3%) than in females (35.6%). The same study found that the second most common methods were firearms (9.7%) for men and poisoning by drugs (24.7%) for women.\n\nIn the United States, both the Department of Health and Human Services and the American Foundation for Suicide Prevention address different methods of reducing suicide, but do not recognize the separate needs of males and females. In 2002, the English Department of Health launched a suicide prevention campaign that was aimed at high-risk groups including young men, prisoners, and those with mental health disorders. The Campaign Against Living Miserably is a charity in the UK that attempts to highlight this issue for public discussion. Some studies have found that because young females are at a higher risk of attempting suicide, policies tailored towards this demographic are most effective at reducing overall rates. Researchers have also recommended more aggressive and long-term treatments and follow up for males that show indications of suicidal thoughts. Shifting cultural attitudes about gender roles and norms, and especially ideas about masculinity, may also contribute to closing the gender gap.\n\nThe incidence of completed suicide is vastly higher among males than females among all age groups in most of the world. As of 2015, almost two-thirds of worldwide suicides (representing about 1.5% of all deaths) are committed by men.\n\nSince the 1950s, typically males have died from suicidal attempts three to five times more often than females. Use of mental health resources may be a significant contributor to the gender difference in suicide rates in the US. Studies have shown that females are 13–21% more likely than males to receive a psychiatric affective diagnosis. 72–89% of females who committed suicide had contact with a mental health professional at some point in their life and 41–58% of males who committed suicide had contact with a mental health professional.\n\nWithin the United States, there are variances in gendered rates of suicide by ethnic group. According to the CDC, as of 2013 the suicide rates of Whites and American Indians are more than twice the rates of African Americans and Hispanics. Explanations for why rates of attempted and completed suicide vary by ethnicity are often based on cultural differences. Among African American suicides, it has been suggested that females usually have better access to communal and familial relations that may mitigate other risk factors for suicide. Among Hispanic populations, the same study showed that cultural values of \"marianismo\", which emphasizes female docility and deference to males, may help explain the higher rate of suicide of Latinas relative to Latinos. The authors of this study did not extrapolate their conclusions on ethnicity to populations outside the United States.\n\nThe gender-suicide gap is generally highest in Western countries. Among the nations of Europe, the gender gap is particularly large in Eastern European countries such as Lithuania, Belarus, and Hungary. Some researchers attribute the higher rates in former Soviet countries to be a remnant of recent political instability. An increased focus on family led to females becoming more highly valued. Rapid economic fluctuations prevented males from providing fully for their families, which prevented them from fulfilling their traditional gender role. Combined, these factors could account for the gender gap. Other research indicates that higher instances of alcoholism among males in these nations may be to blame. In 2014, suicides rates amongst under-45 men in UK reached a 15-year high of 78% of the total 5,140.\n\nA higher male mortality from suicide is also evident from data of non-Western countries: the Caribbean, often considered part of the West is the most prominent example. In 1979–81, out of 74 countries with a non-zero suicide rate, 69 countries had male suicide rates greater than females, two reported equal rates for the sexes (Seychelles and Kenya), while three reported female rates exceeding male rates (Papua New Guinea, Macau, and French Guiana). The contrast is even greater today, with WHO statistics showing China as the only country where the suicide rate of females matches or exceeds that of males. Barraclough found that the female rates of those aged 5–14 equaled or exceeded the male rates only in 14 countries, mainly in South America and Asia.\n\nIn most countries, the majority of committed suicides are made by men but, in China, women are more likely to commit suicide. In 2015 China's ratio was around 8 males for every 10 females.\n\nTraditional gender roles in China hold women responsible for keeping the family happy and intact. Suicide for women in China is shown in literature to be an acceptable way to avoid disgrace that may be brought to themselves or their families. According to a 2002 review, the most common reasons for the difference in rate between genders are: \"the lower status of Chinese women, love, marriage, marital infidelity, and family problems, the methods used to commit suicide, and mental health of Chinese women.\" Another explanation for increased suicide in women in China is that pesticides are easily accessible and tend to be used in many suicide attempts made by women. The rate of nonlethal suicidal behavior is 40 to 60 percent higher in women than it is in men. This is due to the fact that more women are diagnosed as depressed than men, and also that depression is correlated with suicide attempts.\n\n"}
{"id": "4620141", "url": "https://en.wikipedia.org/wiki?curid=4620141", "title": "Gross motor skill", "text": "Gross motor skill\n\nGross motor skills are the abilities usually acquired during childhood as part of a child's motor learning. By the time they reach two years of age, almost all children are able to stand up, walk and run, walk up stairs, etc. These skills are built upon, improved and better controlled throughout early childhood, and continue in refinement throughout most of the individual's years of development into adulthood. These gross movements come from large muscle groups and whole body movement. These skills develop in a head-to-toe order. The children will typically learn head control, trunk stability, and then standing up and walking. (Humphrey) It is shown that children exposed to outdoor play time activities will develop better gross motor skills.\n\nMotor skills are movements and actions of the muscles. Typically, they are categorized into two groups: gross motor skills and fine motor skills. Gross motor skills are involved in movement and coordination of the arms, legs, and other large body parts and movements. Gross motor skills can be further divided into two subgroups of locomotor skills and object control skills. Gross locomotor skills would include running, jumping, sliding, and swimming. Object control skills would include throwing, catching and kicking. Fine motor skills are involved in smaller movements that occur in the wrists, hands, fingers, and the feet and toes. They participate in smaller actions such as picking up objects between the thumb and finger, writing carefully, and even blinking. These two motor skills work together to provide coordination. Less developed kids focus on their gross movements, while more developed kids have more control over their fine movements.\n\nGross motor skills, as well as many other activities, require postural control. Infants need to control the heads to stabilize their gaze and to track moving objects. They also must have strength and balance in their legs to walk.\nNewborn infants cannot voluntarily control their posture. Within a few weeks, though, they can hold their heads erect, and soon they can lift their heads while prone. By 2 months of age, babies can sit while supported on a lap or an infant seat, but sitting independently is not accomplished until 6 or 7 months of age. Standing also develops gradually across the first year of life. By about 8 months of age, infants usually learn to pull themselves up and hold on to a chair, and they often can stand alone by about 10 to 12 months of age. There is a new device called a “Standing Dani” developed to help special needs children with their posture.\n\nWalking upright requires being able to stand up and balance position from one foot to the other. Although infants usually learn to walk around the time of their first birthday, the neural pathways that control the leg alternation component of walking are in place from a very early age, possibly even at birth or before.[1] This is shown because 1- to 2-month-olds are given support with their feet in contact with a motorized treadmill, they show well-coordinated, alternating steps. If it were not for the problem of switching balance from one foot to the other, babies could walk earlier. Tests were performed on crawling and walking babies where slopes were placed in front of the path and the babies had to decide whether or not it was safe. The tests proved that babies who just learned how to walk did not know what they were capable of and often went down slopes that were not safe, whereas experienced walkers knew what they could do. Practice has a big part to do with teaching a child how to walk.\nVision does not have an effect on muscle growth but it could slow down the child’s process of learning to walk. According to the nonprofit Blind Children Center, “Without special training, fully capable infants who are visually impaired may not learn to crawl or walk at an appropriate age and gross and fine motor skills will not properly develop.” When the child is not able to see an object then there is no motivation for the child to try to reach for it. Therefore, they do not want to learn independently. Learning to walk is done by modeling others and watching them. Children when put in environments with older children will observe and try and copy the movements done. This helps the child learn through trial and error. The babies will imitate others, picking up the skills a lot faster than creating their own errors. Visually impaired children may need physical therapy to help them learn these gross motor skills faster. One hour of therapy each week is not enough so parents have to make sure they are involved in this process. The parent can help by telling the baby the direction of where the object is and encourage them to get it. You must have patience because every child has their own developmental schedule and it is even truer for the children with special needs. Focusing on the progress of your child is better than focusing on comparing your child to other children. (Humphrey)\n\nIt has been observed by scientists that motor skills generally develop from the center to the body outward and head to tail. Babies need to practice their skills; therefore they will grow and strengthen better. They need space and time to explore in their environment and use their muscles. “Tummy-time” is a good example of this. At first they are only able to lay their belly on the floor but by around two months they start to gain muscle to raise their head and chest off the ground. Some are also able to go on their elbows. They will also start to kick and bend their legs while lying there, this helps to prepare for crawling. By four months they are able to start to control their head and hold it steady while sitting up. Rolling from belly to back movements is started. At about five months the baby will start to wiggle their limbs to strengthen crawling muscles. Infants can start to sit up by themselves and put some weight on their legs as they hold onto something for support by six months. As they enter their first-year caregivers needs to be more active. The babies will want to get into everything so the house needs to become ‘baby proofed’. Babies are able to start to reach and play with their toys too. It is said that the use of baby walkers or devices that help to hold the baby upright are said to delay the process of walking. Research has been found that it delays developing the core torso strength, which can lead to different issues down in their future. Around ten months they should be able to stand on their own. Throughout their years of life different motor skills are formed. (Oswalt) With regards to the gait pattern, study shows that infant at 12 months old exhibit larger mediolateral motion, which may be caused by weak muscle strength and lack of stability. They also show a synchronized use of hip and shoulder while they are walking, which is different from a mature gait pattern performed by adults. The ankles didn't move as much among 12-month infants as compared to that of adults performing a mature walking.\n\nDevelopment in the second year of life, toddlers become more motorically skilled and mobile.\nThey are no longer content with being in a playpen and want to move all over the place.\nChild development experts believe that motor activity during the second year is vital to the child's competent development and that few restrictions,\nexcept for safety, should be placed on their motoric adventures. By 13 to 18 months, toddlers can move up and down steps and carry toys. Once they reach the top of the\nstairs though, they are not able to get back down. They also begin to move from one position to another more smoothly. (Oswalt) Significant changes in gait patterns are also observed in the second year. Infants in the second year have a discordant use of hip and shoulder while walking, which is closer to an adult walking pattern. They are also able to utilize the rage of motion of their ankles, toes, and heels more, which is similar to a mature walk. By 18 to 24 months, can move quicker or run for a short distance along with other motor skills. They also start to walk backwards and in circles and begin to run. They can also not only walk up the stairs with their hands and feet but are now able to hold onto the handrail and walk up. Near the end of their second year, complex gross motor skills begin to develop including \nthrowing and kicking. Their skills become more natural. Pedaling a tricycle and jumping in place is acquired. At the end they are very mobile and can go from place to place. It is normal for them to get themselves into small situations that could be dangerous such as walking into the street because their brain can’t send the information fast enough to their feet. Parents need to keep an eye on their children at all times. (Oswalt) In a majority of the select kinematic and kinetic variables, there are greater differences between two-year-old children and four-year-old-children than there are between four year old children and six year old children. The variables for which there were significant differences tended to be in displacement, velocity, and magnitude of force measurements. \n\nIn a majority of the select kinematic and kinetic variables, there are greater differences between two-year-old children and four-year-old-children than there are between four year old children and six year old children. The variables for which there were significant differences tended to be in displacement, velocity, and magnitude of force measurements. \n\nChildren with disabilities who are as young as seven months can learn to drive a power wheelchair. \n\nThis will give specific benefits to the leg, is paralyzed.\n\nEarly childhood is a critical period for the development of fundamental motor skills. As a preschooler, the child develops depending on his/her interactions with the surrounding environment. If the child is in an encouraging environment with constructive feedback, he or she will develop fundamental motor skills at a faster rate. Typically, females perform better fundamental movement skills at an earlier age than males. Although many studies prove this to be true, it is dominantly true in walking. Girls typically go through maturity faster than boys do, causing them to also be less active. This allows boys to be deemed more active, due to the fact that they mature much later than their opposing gender. However, this does not give a clear answer as to whether or not girls learn to walk before boys. One would think that learning to walk sooner would allow for a higher activity level, though since girls have a noticeably lower activity level than boys, one would assume that this would mean that girls would learn to walk after boys. But since they mature earlier, that would involve the walking stage. As they grow older, children become more proficient in their tasks allowing them to use their high developed skill for events such as sports where these motor skills are highly required. Children who do not master fundamental motor skills are less likely to participate in sport games during childhood and adolescence. This is one explanation of why boys tend to be more or less athletic than their opposite gender. Children at a younger age might not be able to perform at the level of older children, but their ability to learn and improve on motor skills is much higher. At 3 years of age, children enjoy simple movements, such as hopping, jumping, and running back and forth, just for the sheer delight of performing these activities. However, the findings in the article \"The relationship between fine and gross motor ability, self-perceptions and self-worth in children and adolescents\" it stated that there was not a statistical significance in athletic competence and social competence. This correlation coefficient was .368 and simply means that there is a low correlation between those two relationships. A child being able to perform certain gross and fine motor skills does not mean that they will have the ability to demonstrate social skills such as conversation, social awareness, sensitivity, and body language. This Their body stability is focused on the child's dynamic body base and is related to their visual perceptions such as height, depth, or width. A study was done to assess motor skill development and the overall rate and level of growth development. This study shows that at the preschool age children develop more goal-directed behaviors. This plays a big role, because their learning focuses around play and physical activity. While assessing the gross motor skills in children can be challenging, it is essential to do so in order to ensure that children are prepared to interact with the environment they live in. Different tests are given to these children to measure their skill level. At age 4, children continue to do the same actions as they did at age 3, but further their moving. They are beginning to be able to go down the stairs with one foot on each step. At 5 years of age, they are fully able to go down the stairs one foot at a time in addition to improvements in their balance and running. Their body stability becomes more mature and their trunk is fixed on their posture. Performances are more fluent and are less influenced by factors such a slope and width. During middle and late childhood, children's motor development becomes much smoother and more coordinated than it was in early childhood.\nAs they age, children become able to have control over their bodies and have an increased attention span. Having a child in a sport can help them with their coordination, as well as some social aspects. Teachers will suggest that their students may need occupational therapists in different situations. Students could get frustrated doing writing exercises if they are having difficulties with their writing skills. It also may affect the teacher because it is illegible. Some children also may have reports of their “hands getting tired”. There are many occupational therapists out there today to give students the help they need. These therapists were once used when something was seriously wrong with your child but now they are used to help children be the best they can be.\nIn the Article \"The Relationship Between Fundamental Motor Skills and Outside-School Physical Activity of Elementary School Children\" we can see that the developmental level of overhand throwing and jumping of elementary kids is related to skill specific physical activity outside of school. In the studies done boys were seen to have higher scores in developmental level of overhand throwing and higher scores for the Caltrac accelerometer, rapid-trunk movement, and motor skill related physical activity. Girls were seen to have higher scores in lower-intensity physical activities and physical inactivity. The study showed that the developmental level of the fundamental skills (Overhand-throwing and Jumping) are related to skill-specific physical activity outside of school in elementary children. We can conclude that boys at a younger age develop fundamental motor skills quicker than girls will. In other studies it has been seen that having a higher motor proficiency leads to kids being more active, and in most cases more athletic. This can lead to some issues in childhood development such as issues with weight, and increasing the public health epidemic of childhood obesity.\n\nBetween the ages of 7 and 12 there is an increase in running speed and are able to skip. Jumping is also acquired better and there is an increase in throwing and kicking. They’re able to bat and dribble a ball. (Age) Gross motor skills usually continue improving during adolescence. The peak of physical performance is before 30, between 18 and 26. Even though athletes keep getting better than their predecessors—running faster, jumping higher, and lifting more weight—the age at which they reach their peak performance has remained virtually the same. After age 30, most functions begin to decline. Older adults move slower than younger adults. This can be moving from one place to another or continually moving. Exercising regularly and maintaining a healthy lifestyle can slow this process. Aging individuals who are active and biologically healthy perform motor skills at a higher level than their less active, less healthy aging counterparts.\n\n\n\n"}
{"id": "32512846", "url": "https://en.wikipedia.org/wiki?curid=32512846", "title": "Healthcare in Greece", "text": "Healthcare in Greece\n\nHealthcare in Greece consists of a universal health care system provided through national health insurance, and private health care. According to the 2011 budget, the Greek healthcare system was allocated 6.1 billion euro, or 2.8% of GDP. In a 2000 report by the World Health Organization, the Greek healthcare system was ranked 14th worldwide in the overall assessment, above other countries such as Germany (25) and the United Kingdom (18), while ranking 11th at level of service. \n\nHealthcare in Greece is provided by the National Healthcare Service, or ESY ().\n\nHealthcare in Greece traces its roots to the ancient Greek civilization. Hospitals did not exist in the modern sense in the ancient Greek world, but temples dedicated to the healing god Aesculapius (called Asclepieia) functioned as healing places as well as places of worship. It is not known whether or not cities in ancient Greece provided free healthcare to their citizens, but recent study of the ruins of the Kos Asclepieion show that medical services were offered to everyone who could pay for them, including slaves and foreigners.\n\nThe Byzantine Empire is accredited by some for having invented the hospital as the institution we know it to be today. Professor Timothy S. Miller of the Johns Hopkins University argues that the Byzantine Empire was the first to employ a system of hospital-based healthcare, where the hospital became the chief institution of the medical profession in contrast to its function as a last resort in Western medieval Europe, carrying forward the medical knowledge of ancient Greece and Rome.\n\nIn July 2011, changes were made to the Greek healthcare system in accordance with austerity measures. Unemployed Greeks were entitled to healthcare from national health insurance for a maximum of a year, and after that period, healthcare was no longer universal and patients had to pay for their own treatment. Austerity measures also resulted in citizens being forced to contribute more towards the cost of their medications. As a result, many free clinics funded by private donations sprang up, and although officially illegal, were allowed to remain in operation.\n\nIn 2016, the Greek government voted to extend health coverage to uninsured people who are registered as unemployed and refugees from June 1 on, with those earning less than 2,400 euro a year entitled to free healthcare, with the threshold rising for families according to how many children they have.\n\nIn 2009 the hospital bed to 10,000 population ratio in the country was 48, above countries such as the United Kingdom (39), Spain (34) and Italy (39), but considerably below countries such as France (72) and Germany (83). On 1 July 2011, the Ministry for Health and Social Solidarity announced its intention to cut back the number of beds and hospitals in the country from 131 hospitals with 35,000 beds to 83 hospitals with 33,000 beds.\n\nCurrently the largest hospital in the country is Attica Psychiatric Hospital \"Dafni\" with 1,325 beds, while the largest general hospital is \"Evangelismos\" General Hospital of Athens with 1,100 beds. Public hospitals in Greece are constructed by a government-owned company by the name of DEPANOM. S.A. (, \"Public Corporation for the Construction of Hospital Units S.A.\"), which is also in charge of maintaining and upgrading the country's public medical facilities and equipment.\n\nEmergency, ambulance and air-ambulance services in Greece are provided by the \"National Center for Direct Aid\", known mostly by the acronym EKAB ().\n\nOn an OECD health report in 2011, Greece got the following results:\n\nHealthcare expenditure per capita went down by 28% between 2009 and 2011 - a more drastic cut than any other European country. However treatment results have not deteriorated, but according to the survey conducted by the Euro health consumer index in 2015 Albania was the European country in which unofficial payments to doctors were reported most commonly. The Greek rates of Caesarean sections is one of the highest in the world. \n\nWith respect to pharmaceutical drugs in use, ~20% were generic at the end of 2013 and the government has set a goal of reaching 60% generic use by the end of 2015. This planned major increase in generic use has been driven by conditions of economic support from the European Union and International Monetary Fund requiring that Greece reduce overall public spending on drugs.\n\nGreece has the highest number of doctors per head of population of any OECD country. 6.3 doctors per thousand people in 2013.\n"}
{"id": "16345498", "url": "https://en.wikipedia.org/wiki?curid=16345498", "title": "Home-stored product entomology", "text": "Home-stored product entomology\n\nHome stored product entomology is the study of insects which infest foodstuffs stored in the home. It deals with the prevention, detection and eradication of the pests. The five major categories of insects considered in this article are flour beetles, the drugstore beetle, the sawtoothed grain beetle, the Indian meal moth and fruit flies.\n\nThis is an important branch of forensic entomology because consumers who find contaminated products may choose to take legal action against the producers. Suitably qualified entomologists are likely to be able to determine the identity of contaminant species, even when no insects are found and the only evidence of infestation is the resulting damage. They should also be able to determine whether the foodstuff was contaminated before or after purchase.\n\nIn the United States, companies are required by the U.S. Food and Drug Administration (FDA) to have no more than a certain number of larvae, insects or insect fragments in their products; when this defect action level is exceeded, a consumer can pursue legal remedies.\n\nTwo different types of beetles are classified as flour beetles: the red flour beetle and the confused flour beetle. Both are similar in physical characteristics. They are flat and oval in shape and usually range around 1/8 inch long. Their exoskeletons are reddish brown with a shiny and smooth texture. The eggs, larvae, and pupae resemble each other closely in physical features, as well. The eggs usually tend to be a white color, or at times even colorless. They are very small in size and have a sticky outer covering that causes certain food particles to stick to them. The larvae have six legs, with two pointy projections toward the caudal end. Finally, the pupal stage (a cocoon-like form) is usually a white or brownish color. The beetle life cycle lasts about three years or more, with the larval stage ranging anywhere from 20 to over 100 days, and the pupal stage around eight days. Beetles usually breed in damaged grain, grain dust, high-moisture wheat kernels and flour. The female flour beetle can lay between 300 and 400 eggs during her lifetime [a period of 5 to 8 months]. The flour beetles mainly infest grains, including, but not limited to: cereal, corn meal oats, rice, flour, and crackers. This type of beetle is the most abundant insect pest of flour mills across the United States. Their small size allows them to maneuver through cracks and crevices and get into the home and other areas. Once they are present in areas with potential food sources, they can infest material such as flour, resulting in a sharp odor or moldy flavor. The red flour beetle is able to fly short distances and the confused flour beetle is unable to fly. While the confused flour beetle is more commonly found in the northern United States, the red flour beetles are more predominant in the southern United States in areas with warmer climates.\n\nThe red flour beetle and the confused flour beetle are commonly used as a model organisms, to study genetics and ecology. The genome of the red flour beetle has been sequenced.\n\nThis beetle is related to the commonly known cigarette beetle. Adult drugstore beetles are cylindrical with lengths ranging from 2.25 to 3.5 mm. They are a reddish-brown color and have elytra, sclerotized (hardened) wings that fold back over the abdomen and hinge upwards, allowing the hind wings to come out to fly. Females are capable of laying up to 75 eggs during a 13- to 65-day period. After the eggs are laid, they hatch into a larval period that can range anywhere from four to 20 weeks. After the larval period, drugstore beetle larvae move out of the substrate to build a cocoon and pupate. The pupation period takes a total of 12–18 days. The entire life cycle of the drugstore beetle lasts approximately two months, but can be as long as seven months. These stored product pests will infest almost anything readily available. Food products prone to infestation include: flours, dry mixes, breads, cookies and other spices. Nonfood material includes: wool, hair, leather and museum specimens. This specific type of beetle has symbiotic yeasts that produce B vitamins, which allow the beetle to survive even when consuming foods of low nutritional value. They are found in areas that have a warmer climate, yet are less plentiful in the tropics than the cigarette beetle.\n\nThe sawtoothed grain beetle is closely related to the merchant grain beetle, and is commonly found in kitchen cabinets feeding on items such as cereal, breakfast foods, dried fruits, macaroni, crackers, etc. They are the most common grain and stored product pest in the United States. They are very active and tend to crawl rapidly while searching for food. They are small insects, reaching a length of about 1/8 of an inch. Their name originates from their distinguishable, sawtooth-like projections found on each side of the thorax. The body of the beetle is flat and slender in shape, and brown in color. The size and shape of the mandibles allow the beetles to easily break through well-sealed packaged foods. An adult female can lay between 45 and 250 eggs that usually hatch within three to 17 days. The larvae have a caterpillar-like appearance, with a yellowish coloration to the body and a brown head. The larval period can last as long as 10 weeks, but can be as short as two weeks. Following the larval instars is the pupal period, which can last one to three weeks. The pupal stage is characterized by the unique process by which these beetles stick together pieces of food material to form protective coverings around their bodies. A fully mature adult beetle, under optimal conditions, can live a maximum of four years, a long lifespan for an arthropod.\n\nIndianmeal moths can infest a variety of foods found in the home. Coarsely ground grains, cereals, dried fruits, and herbs are common items the moths have been known to infest. They have also been found in animal feeds, such as dry dog food, fish food and even bird seed. Adult moths are small; generally, their length averages about 3/8 inch, with a 5/8 inch wing span. As adults, the moths are easily identified by an overall grayish, dirty complexion. However, the wing tips have a bronze color that helps differentiate this particular moth from other household moths. The adults have a distinct forewing pattern, as well, which consists of a light-colored base with about two-thirds of the distal area a red to copper color. The larval stage, or caterpillar, is characterized by a pinkish or yellowish-green body color with a dark brown head. The larval stage of the moth’s life cycle is centered on food sources; during the last instar, these larvae are characterized by a movement towards a protected area to pupate. These caterpillars have the capacity to chew through plastic packaging and will often produce silk that loosely binds to food fragments. The pupal stage is generally observed as tiny cocoons that hang from the ceiling; these cocoons can also be found on walls, as well as near the food source. A female can lay over 200 eggs and will usually die after this process because adults Indian meal moths do not eat.\n\nFruit flies are found near ripened or fermenting fruit. Tomatoes, melons, squash, grapes and other perishable items brought in from the garden are a common cause of an indoor infestation. Fruit flies can also be attracted to rotting bananas, potatoes, onions and other unrefrigerated produce purchased at the grocery store and taken home. The body of the fruit fly is tan towards the front part of the body and black towards the rear. They usually have red eyes and are about 1/8 inch long. Females have the ability to lay over 500 eggs, usually in fermenting fruit as a food source. The only environment necessary for successful reproduction is a moist film and fermenting material. Generally, fruit flies are a problem during late summer and fall due to their attraction to ripening and fermenting fruits and vegetables. The entire life cycle can be completed in about a week. Rarely, because of their ability to fly in and out of the home through windows and screens, they have the capability of contaminating food with bacteria and disease-producing organisms.\n\nCareful observation is necessary when detecting the cause of an insect infestation. Each of the five different insects discussed has a unique pattern of destruction. These observations are imperative, as there are not always larvae, pupae, or adults readily available for examination and identification. In the absence of physical specimens, conclusions can be drawn about the probable insect infesting the product just by noting the damage done to the particular food. By noting the type of food and the damage done, a nearly accurate conclusion can be drawn about the type of insect causing the damage, allowing a conjecture about the type of control needed. Having an insect specimen and accurately identifying it can lead to eradication, and ultimately, prevention.\n\nFoods commonly infested include:\n\nOther items include, but are not limited to:\nRodent baits (that contain grain as a feeding attractant), dry pet food, bird seed, grass seed, some powdered soap detergents, dried flowers, potpourri, items stuffed with dried beans or other plant material, and tobacco products.\n\nTo identify an insect, and consequently make a decision about the type of control to be implemented, the type of food must first be noted, especially in the absence of a specimen. Although identifying the food is a general start to begin to identify the insect, it must be remembered that it is not always the most accurate method, but is mostly used as a guideline, as some insects are more likely than others to be found in certain types of grain, flour, etc. The type of food is not always conclusive to the type of insect found in it, as insects are not extremely picky, and many families and species are found on a wide range of different foodstuffs. Using the infested item as a guideline, noting the type of damage done to the product is the next step. Some insects, like the drugstore beetle, leave telltale tiny holes in the damaged product, while Indianmeal moths are notorious for the spider web-like threads left behind in the food they infest. These observations can generally lead to a mostly accurate conclusion about the type of insects causing the damage, but obviously the most accurate conclusion relies on any specimen found either directly in the stored product or in the vicinity. The larvae, pupae, and adults can be found directly in the product while usually only the pupae and adults are found in the vicinity of the product. It is not practical to assume any person has knowledge of general entomology, so the following analysis focuses on the five major pests that most commonly infest stored products, beginning with the type of foods infested, signs indicative of a particular insect infestation, and a description of the larvae, pupae, and adults, including behavior, as well as appearance.\n\nThis beetle is similar to the saw-toothed grain beetles in both habits and types of products infested. It is a serious pest in flour mills and wherever cereal products and other dried products are stored and/or processed. Generally, the beetle is attracted to grain with a high moisture content, and usually causes the grain to acquire a grayish tint. The beetle may also impart a bad odor, which then affects the taste of the infested products, as well as encouraging the growth of mold in the grain. This foul odor and taste in the various food products are caused by pheromones and toxic quinone compounds.\n\nThe sawtoothed grain beetle feeds on a plethora of feeds, but is not capable of attacking whole or undamaged grains; therefore, the larvae are commonly found in processed grains (flour and meal), dry dog food, dried fruits, candy bars, tobacco, drugs, dried meats, and a variety of other stored food products.\n\nThese beetles will infest almost anything- they are found most often, however, in flour, bread, spices, breakfast foods, and meal. In the case of an infestation, contaminated products have telltale tunnels which have the appearance of tiny holes. These beetles do not sting, bite, or harm pets or damage a house, yet have the potential, in large infestations, to become a nuisance by flying on doors and windows in heavy populations.\n\nIndian meal moths infest both cereal and stored grain products, packaged goods, and surface layers of shelled corn. The most telltale sign of the Indian meal moth is the silk webbing the larvae (caterpillars) produce when feeding on the surfaces of foods. This silk webbing may appear to be or resemble cobwebs inside the products' containers. Often, a few larvae may be found in the packaging of the product, along with the ‘cobwebs’, cast skins and frass.\n\nLarvae are white worms with black heads, which, when ready to pupate, crawl up the walls of the home in most cases, and are suspended from the ceiling attached by a single silken thread. Most complaints about these moths come during the warmer parts of the year- usually July through August- but the moths can appear during any month. As with all insects important to stored product entomology, it cannot be automatically assumed that products were previously infested, yet, it is more common for these moths to contaminate products before purchase than for the moth to fly into a home through open windows or doors. An important aspect of the Indian meal moth is that the larva is the only stage of the insect's life cycle to feed on stored products, the adults do not.\n\nFruit flies are attracted to ripened fruits and vegetables, usually in the kitchen area, but will breed in garbage disposals, empty bottles and cans, wet or damp mops or cleaning rags, and trash containers. The only requirement for these flies to breed is a moist film of fermenting material. Infestations can originate from over-ripened fruits or vegetables that were previously infested, and then brought into the home, or from fruit over-ripening in the home. Since adults can also fly from the outside through screened doors or windows, it can not always be assumed that the product in question was infested before it was brought into the home. The larvae are found on the inside layer of the fruit, directly beneath the skin. If the outer layer of the fruit is removed, the rest of the fruit can be salvaged. Fruit flies are primarily a nuisance pest.\n\nDefect action levels have been a part of the food industry for nearly a century. The first established defect action level was created in 1911 for mold in tomato pulp. However, limits for insect fragments and larvae were not added until the 1920s on various fruits and vegetables. In 1938, the Federal Food, Drug and Cosmetic Act was established to provide a more defined reference based on strict limitations and methods.\n\nMajor companies spend a large amount of money every year to aid in the prevention of food contamination. Most of these dollars are well-spent and do, in fact, prevent food from becoming contaminated on a large scale; however, many \"defects\" are found in consumers' meals on a daily basis. The Food and Drug Administration states, “it is economically impractical to grow, harvest, or process raw products that are totally free of nonhazardous, naturally occurring, unavoidable defects”.\n\nThe general public proposes that companies should use more chemicals or pesticides to control this “problem”, though the amount of pesticide and chemicals necessary to eradicate all insects from foodstuff would pose a threat to any human’s health, much more harmful than a controlled quantity of insect and rodent fragments. The food defect action levels, as proposed by the FDA, is a list of ordinances and guidelines by which manufacturers and industrial food agencies must abide to ensure the safe service of foodstuff. However, these detection levels are labeled with maximum limitations only. Due to the impossibility of preventing all unavoidable defects in foods, the FDA attempts to prevent these health hazards from reaching a harmful level. Therefore, it is understood and regarded that all manufactures are allowed to have low numbers of insect and rodent hairs present in food, as long as the product is still considered “safe” for human consumption.\n\nTo prevent the infestation of foodstuffs by pests of stored products, or “pantry pests”, a thorough inspection must be conducted of the food item intended for purchase at the supermarket or the place of purchase. The expiration date of grains and flour must also be noted, as products that sit undisturbed on the shelf for an extended period of time are more likely to become infested. This does not, however, exclude even the freshest of products from being contaminated. Packaging should be inspected for tiny holes that indicate there might be an infestation. If there is evidence of an insect infestation, the product should not be purchased. The store should be notified immediately, as further infestation must be prevented. Most stores have a plan of action for insect infestations. Bringing an infested product into a pantry or a home leads to a greater degree of infestation.\nIn the home, putting cereal or grain-type items in protective containers will also help to prevent an infestation or the spread of insects from one product to another. Insects can chew through thin plastic, foil, cardboard and other packaging used for product for resale; transferring purchased products into heavy glass containers that can be tightly sealed or heavy plastic containers can improve sanitation and prevent infestation. Using the oldest products first and buying grains and cereals in smaller quantities which can be used quickly, depending on the size or intake of the family, decreases the chances of infestation. Fruit flies, however, present an entirely different approach to prevention. The primary method to controlling and eliminating fruit flies is to eradicate sources of attraction. Ripened produce should be either eaten, discarded, or refrigerated. Any damaged or cracked fruit or vegetable needs to be trimmed, and the damaged piece discarded in case larvae or eggs are present in the area in question. Careful attention must be paid to potential breeding sites that, when forgotten, could cause a massive infestation- all recycling and compost bins must be cleaned, and areas must be checked for forgotten, rotting fruit. Because of their small size, fruit flies are capable of breeding on the inside of the lid of a container. Therefore, when personally canning fruits or vegetables, beer, cider, or wine, the container must be well-sealed. Adults moths can lay eggs under the lid of a jar, allowing the larvae to crawl into the food source when hatched. Homeowners should also outfit their doors and windows with tight mesh screens to prevent the adult fruit flies from flying in from outdoors. Preventive methods and sanitation are the keys to avoiding an infestation or contamination of foodstuffs.\n\nAlthough not seen when groceries are purchased, some products have the possibility of being infested prior to being placed in the pantry. A periodic check of susceptible foodstuffs is necessary, especially in summer months when most insects are more active. In the event an infestation is discovered, steps must be taken to eradicate the insects. Controlling an infestation is a lengthy process and insects may still be seen, albeit in dwindling numbers, for several weeks. All infested items, as well as uninfested items, must be removed from shelves, thoroughly cleaned and vacuumed. After vacuuming, the waste containing the infested material must be removed and discarded. Items should be checked for beetles, larvae, and pupae; all food items must be inspected, as well, and special attention must be paid to items rarely used. The infested items may either be discarded, heated, or frozen to kill the insects. If the food is chosen to be discarded, the item must be completely removed from surrounding premises to prevent reinfestation. Freezing products for three to four days or heating them to about 130 to 140 °F for 30 to 40 minutes will rid the product of the pests. Decorative ornaments and objects made with plant material and seed in the vicinity of stored products will increase the risk of reinfestation; insects can feed on those items until they locate stored products. These items should also be thrown out or disinfected by freezing or heating.\nCleaning the area where the infested products were found is advisable, as well. Cleaning with bleach or ammonia, however, will not help with the eradication of the pests. Using a vacuum cleaner to clean the area thoroughly, especially in cracks and corners where insects may hide, will decrease the chances of reinfestation. Because food will be stored in that area again, pesticides are not a good method of eradication. Pesticides can leave a residue that can contaminate food products stored near it. Also, once a pest is inside the container, the pesticides have no effect. If the infestation is so severe that pesticides are the only way to contain the problem, a professional should be contacted immediately. Do not try to apply pesticides to any area where food is stored for human or animal consumption. Contamination can occur and cause illness or more severe conditions. Proper storage and cleanliness are the only ways to prevent an infestation from occurring. Sanitation is the key to prevention and eradication of any pests.\nIf these insects are found infesting stored products, it is not practical to automatically assume the store or producer is at fault. Although some infestations are not the consumer's fault, producers are held by the FDA action defect levels to ensure their product does not contain more than the allotted amount of insects, insect fragments, or larvae. If a stored product is found to be infested by insects, and it is suspected that someone other than the consumer is at fault, a forensic entomologist can be contacted to make a determination. Again, it should not be assumed that these are the only five insects found in a common household pantry, but due to the large number of infestations by these five major groups, it can be safely deduced that it is perhaps one of these five. Stored product entomology is an important forensic field that is important to not only the government and the FDA but the general public, as it is involved in the consumption of food in everyday life.\n\nOther stored product pests:\n\n\n\n"}
{"id": "30778342", "url": "https://en.wikipedia.org/wiki?curid=30778342", "title": "Infectious causes of cancer", "text": "Infectious causes of cancer\n\nEstimates place the worldwide risk of cancers from infectious causes at 16.1%. Viral infections are risk factors for cervical cancer, 80% of liver cancers, and 15–20% of the other cancers. This proportion varies in different regions of the world from a high of 32.7% in Sub-Saharan Africa to 3.3% in Australia and New Zealand. Viruses are the usual infectious agents that cause cancer but Mycobacterium, some other bacteria and parasites also have an effect.\n\nA virus that can cause cancer is called an \"oncovirus\". These include human papillomavirus (cervical carcinoma), Epstein-Barr virus (B-cell lymphoproliferative disease and nasopharyngeal carcinoma), Kaposi's sarcoma herpesvirus (Kaposi's Sarcoma and primary effusion lymphomas), hepatitis B and hepatitis C viruses (hepatocellular carcinoma), and Human T-cell leukemia virus-1 (T-cell leukemias). Bacterial infection may also increase the risk of cancer, as seen in \"Helicobacter pylori\"-induced gastric carcinoma. Parasitic infections strongly associated with cancer include \"Schistosoma haematobium\" (squamous cell carcinoma of the bladder) and the liver flukes, \"Opisthorchis viverrini\" and \"Clonorchis sinensis\" (cholangiocarcinoma).\n\nInfection is the fourth most important risk factor for cancer mortality in the developed world, causing about 10% of cancer mortality (see cancer prevention), coming after tobacco (~30% of cancers), diet (~30%) and obesity (~15%). Cancer causes 22.5% of deaths in the United States, so that about 2% of mortality in the United States appears to be due to cancers caused by infections. This is comparable to mortality caused by influenza and pneumonia, which cause 2.1% of deaths in the United States.\n\nWorldwide in 2015, the most common causes of cancer death were lung cancer (1.6 million deaths), liver cancer (745,000 deaths), and stomach cancer (723,000 deaths). Lung cancer is largely due to non-infectious causes, such as tobacco smoke. However, liver and stomach cancer are primarily due to infectious causes. Liver cancer is largely caused by infectious hepatitis B virus (HBV) plus hepatitis C virus (HBC) and stomach cancer is largely caused by \"Helicobacter pylori\" bacteria. World-wide, the estimated number of people chronically infected with HBV and/or HCV is ~325 million. Over half of the world's population is colonized with \"H. pylori\" and it is estimated that \"H. pylori\"-positive patients have a 1-2% risk of developing distal gastric cancer.\n\nDNA damage and genomic instability appear to be the basic causes of sporadic (non-familial) cancer. While infections have many effects, infectious organisms that increase the risk of cancer are frequently a source of DNA damage or genomic instability, as discussed below for oncogenic viruses and an oncogenic bacterium.\n\nIn Western developed countries, human papillomavirus (HPV), hepatitis B virus (HBV) and hepatitis C virus (HCV) are the most frequently encountered oncogenic DNA viruses.\n\nWorldwide, HPV causes the second largest fraction of infection-associated cancers or 5.2% of the global cancer burden.\n\nIn the United States, HPV causes most cervical cancers, as well as some cancers of the vagina, vulva, penis, anus, rectum, and oropharynx (cancers of the back of the throat, including the base of the tongue and tonsils). Each year in the United States, about 39,800 new cases of cancer are found in parts of the body where HPV is often found. HPV causes about 31,500 of these cancers.\n\nAs reviewed by Münger et al. there are about 200 HPVs. They can be classified into mucosal and cutaneous HPVs. Within each of these HPV groups, individual viruses are designated high risk or low risk according to the propensity for malignant progression of the lesions that they cause. Among the HPV high-risk viruses, the HPV E6 and E7 oncoproteins functionally inactivate the p53 and retinoblastoma tumor suppressors respectively. In addition, the high-risk HPV E6 and E7 oncoproteins can each independently induce genomic instability in normal human cells. They generate mitotic defects and aneuploidy through the induction of centrosome abnormalities.\n\nHepatitis virus-associated hepatocarcinogenesis is a serious health concern. Liver cancer in the United States is primarily due to three main factors: hepatitis C virus (HCV) (22%), hepatitis B virus (HBV) (12%) and alcohol use (47%). In 2017 there will be about 40,710 new cases of liver cancer in the United States. World-wide, liver cancer mortality is more often due to hepatitis B virus (HBV) (33%), less often due to hepatitis C virus (HCV) (21%), and still frequently due to alcohol use (30%). World-wide, liver cancer is the 4th most frequent cause of cancer mortality, causing 9% of all cancer mortality (total liver cancer deaths in 2015 being 810,500), and coming, in frequency, after lung, colorectal and stomach cancers.\n\nAs reviewed by Takeda et al., HCV and HBV cause carcinogenic DNA damage and genomic instability by a number of mechanisms. HBV, and especially HCV, cause chronic inflammation in the liver, increasing reactive oxygen species (ROS) formation. ROS interact directly with DNA, causing multiple types of DNA damages (26 ROS-induced DNA damages are described by Yu et al.) It also appears that chronic inflammation caused by HCV infection triggers the aberrant up-regulation of activation-induced cytidine deaminase (AID) in hepatocytes. AID creates mutations in DNA by deamination (a DNA damage) of the cytosine base, which converts cytosine into uracil. Thus, it changes a C:G base pair into a mutagenic U:G mismatch. In a still further cause of DNA damage, HCV core protein binds to the NBS1 protein and inhibits the formation of the Mre11/NBS1/Rad50 complex, thereby inhibiting DNA binding of repair enzymes. As a result of reduced DNA repair mutagenic DNA damages can accumulate.\n\n\"H. pylori\" causes over 63% of all stomach cancers, which corresponds to more than 5.5% of all cancers in the world. As reviewed by Chang and Parsonnet, chronic \"H. pylori\" infection in the human stomach is characterized by chronic inflammation. This is accompanied by epithelial cell release of reactive oxygen species (ROS) and reactive nitrogen species (RNOS), followed by the assembly of activated macrophages at the stomach site of infection. The macrophages also release ROS and RNOS. Levels of 8-oxo-2'-deoxyguanosine (8-OHdG), one of the predominant forms of free radical-induced oxidative DNA damages, are increased more than 8-fold in DNA after infection by \"H. pylori\", especially if the \"H. pylori\" are cagA positive. The increase in 8-OHdG likely increases mutation. In addition, oxidative stress, with high levels of 8-OHdG in DNA, also affects genome stability by altering chromatin status. Such alterations can lead to abnormal methylation of promoters of tumor suppressor genes.\n\nTuberculosis is a risk factor for lung cancer.\n\nViruses are one of the most important risks factor for cancer development in humans.\nInfection by some hepatitis viruses, especially hepatitis B and hepatitis C, can induce a chronic viral infection that leads to liver cancer in about 1 in 200 of people infected with hepatitis B each year (more in Asia, fewer in North America), and in about 1 in 45 of people infected with hepatitis C each year. People with chronic hepatitis B infection are more than 200 times more likely to develop liver cancer than uninfected people. Liver cirrhosis, whether from chronic viral hepatitis infection or alcohol abuse or some other cause, is independently associated with the development of liver cancer, and the combination of cirrhosis and viral hepatitis presents the highest risk of liver cancer development. Because chronic viral hepatitis is so common, and liver cancer so deadly, liver cancer is one of the most common causes of cancer-related deaths in the world, and is especially common in East Asia and parts of sub-Sarahan Africa.\n\nHuman papillomaviruses (HPV) are another particularly common cancer-causing virus. HPV is well known for causing genital warts and essentially all cases of cervical cancer, but it can also infect and cause cancer in several other parts of the body, including the larynx, lining of the mouth, nose, and throat, anus, and esophagus. The Papanicolaou smear (\"Pap\" smear) is a widely used cancer screening test for cervical cancer. DNA-based tests to identify the virus are also available.\n\nHerpesviruses are a third group of common cancer-causing viruses. Two types of herpesviruses have been associated with cancer: the Epstein–Barr virus (EBV) and human herpesvirus 8 (HHV-8). EBV appears to cause all nonkeratinizing nasopharyngeal carcinomas and some cases of lymphoma, including Burkitt's lymphoma—the association is especially strong in Africa—and Hodgkin's disease. EBV has also been found in a variety of other types of cancer cells, although its role in causing these other cancers is not well established. KSHV/HHV-8 causes all cases of Kaposi's sarcoma, and has been found in some cases of a cancer-related condition called Castleman's disease. Studies involving other kinds of cancer, particularly prostate cancer, have been inconsistent. Both of these herpesviruses are commonly found in cancerous cells of primary effusion lymphoma. Herpesviruses also cause cancer in animals, especially leukemias and lymphomas.\n\nHuman T cell lymphotropic virus (HTLV-1) was the first human retrovirus discovered by Robert Gallo and colleagues at NIH. The virus causes Adult T-cell leukemia, a disease first described by Takatsuki and colleagues in Japan and other neurological diseases.\n\nMerkel cell polyomavirus is the most recently discovered human cancer virus, isolated from Merkel cell carcinoma tissues in 2008, by the same group that discovered KSHV/HHV-8 in 1994, using a new technology called digital transcriptome subtraction. About 80% of Merkel cell carcinomas are caused by Merkel cell polyomavirus; the remaining tumors have an unknown etiology and possibly a separate histogenesis. This is the only member of this group of viruses known to cause human cancer but other polyomaviruses are suspects for being additional cancer viruses.\n\nHIV does not directly cause cancer, but it is associated with a number of malignancies, especially Kaposi's sarcoma, non-Hodgkin's lymphoma, anal cancer and cervical cancer. Kaposi's sarcoma is caused by human herpesvirus 8. AIDS-related cases of anal cancer and cervical cancer are commonly caused by human papillomavirus. After HIV destroys the immune system, the body is no longer able to control these viruses, and the infections manifest as cancer. Certain other immune deficiency states (e.g. common variable immunodeficiency and IgA deficiency) are also associated with increased risk of malignancy.\n\nIn addition to viruses, certain kinds of bacteria can cause some cancers. The most prominent example is the link between chronic infection of the wall of the stomach with \"Helicobacter pylori\" and gastric cancer. Although only a minority of those infected with \"Helicobacter\" go on to develop cancer, since this bacterial infection is quite common, it may be responsible for most of these cancers. The mechanism by which \"H. pylori\" causes cancer may involve chronic inflammation, or the direct action of some of its virulence factors, for example, CagA has been implicated in carcinogenesis.\n\nOne meta-analysis of serological data comparing prior \"C. pneumoniae\" infection in patients with and without lung cancer found results suggesting prior infection was associated with a slightly increased risk of developing lung cancer.\n\nThe parasites that cause schistosomiasis (bilharzia), especially \"S. haematobium\", can cause bladder cancer and cancer at other sites. Inflammation triggered by the worm's eggs appears to be the mechanism by which squamous cell carcinoma of the bladder is caused. In Asia, infection by \"S. japonicum\" is associated with colorectal cancer.\n\nDistomiasis, caused by parasitic liver flukes, is associated with cholangiocarcinoma (cancer of the bile duct) in East Asia.\n\nMalaria is associated with Burkitt's lymphoma in Africa, especially when present in combination with Epstein-Barr virus, although it is unclear whether it is causative.\n\nParasites are also a significant cause of cancer in animals. \"Cysticercus fasciolaris\", the larval form of the common tapeworm of the cat, \"Taenia taeniaformis\", causes cancer in rats. \"Spirocerca lupi\" is associated with esophageal cancer in dogs, at least within the southern United States.\n\nA novel type of case, reported in 2015, involved an immunocompromised man whose tapeworm underwent malignant transformation, causing metastasis of tapeworm cell neoplasia throughout his body. This was not a cancer of his own cells but of the parasite's. This isolated case has no substantive bearing on public health but is interesting for being \"a novel disease mechanism that links infection and cancer.\"\n\n\n\nPelini P. (1999-2000). \" Cancer and metastasis to the lung caused by the bacterium Pseudomonas\" Figshare, Rome, Italy.. doi: 10.6084/M9.FIGSHARE.3382954\n"}
{"id": "4737981", "url": "https://en.wikipedia.org/wiki?curid=4737981", "title": "Irish Blood Transfusion Service", "text": "Irish Blood Transfusion Service\n\nThe Irish Blood Transfusion Service (IBTS), or Seirbhís Fuilaistriúcháin na hÉireann in Irish, was established in Ireland as the \"Blood Transfusion Service Board\" (\"BTSB\") by the \"Blood Transfusion Service Board (Establishment) Order, 1965\". It took its current name in April 2000 by Statutory Instrument issued by the Minister for Health and Children to whom it is responsible. The Service provides blood and blood products for humans.\n\nThe service is the successor to the \"National Blood Transfusion Association\" which was established in 1948 and was, itself, born from the work carried out by the St. John Ambulance Brigade of Ireland in setting up an 'on call' blood donor panel to serve hospitals in the Dublin area. In 1975 the \"Cork Blood Transfusion Service\" was amalgamated with the board, and in 1991 the \"Limerick Blood Transfusion Service\" was amalgamated with the board.\n\nThe symbol of the service is a stylised pelican, and for most of its existence the headquarters of the service was located at Pelican House (first in Lower Leeson Street and then Mespil Road) in Dublin. In 2000 the service moved to the National Blood Centre on the grounds of St. James's Hospital near Dublin Heuston railway station, on which it remains. The service maintains regional facilities at Ardee, Carlow, Cork, Limerick and Tuam.\n\nBetween 1977 and 1994 a number of people unknowingly received Hepatitis C-infected blood, and clear evidence of this did not become available until the mid-1990s. Most of those infected by the blood were women. The \"Hepatitis C and HIV Compensation Tribunal\" was established by the \"Hepatitis C Compensation Tribunal Act, 1997\", and amended by the \"Hepatitis C Compensation Tribunal (Amendment) Act, 2002\", to compensate people who contracted Hepatitis C or HIV as a result of receiving blood or blood products from the Service.\n\nThe frequency of blood groups in Ireland is as follows:\n\nIt is important that the IBTS collects enough O Rh D positive blood as almost half the population are that blood type. Donors with O Rh D negative are known as universal donors. Their blood can be transfused to patients of any other blood group in an emergency or if the patient's own blood group is unavailable. Because any patient can receive O Rh D negative blood, the IBTS need to have extra O Rh D negative blood available at all times.\n\nThe service depends entirely on voluntary donations from the public. New donors must be aged between 18 and 64, weigh over 50 kilograms (7 stone 12 lbs), and be in good health. At every donation haemoglobin levels are checked and donors complete a detailed health and lifestyle questionnaire. Donors can give blood every 90 days.\n\nThe IBTS imposes a number of restrictions on those who can give blood. These comply with those of the European Union, World Health Organization, and the Irish Medicines Board, and are similar to other countries. These restrictions ensure that blood products are safe for recipients. A four-month restriction is placed on donors who have had piercings or tattoos or had acupuncture, and a similar restriction on anyone who has visited a tropical area (three months). There is a year-long deferral for those who have visited a malarial area. Donors who have travelled to some parts of the United States or Canada have to wait for four weeks before donating due to spread of the West Nile Virus there. Certain medications or conditions can also exclude people from donation.\n\nAdditionally, there are groups of people who are barred from donating blood based on their membership of high-risk groups. This includes people who have lived for a year or more in the United Kingdom (UK) between the years 1980–1996 and those who received medical procedures in the UK since 1 January 1980, due to the risk of variant Creutzfeldt–Jakob disease (vCJD) in that country. vCJD has a long incubation period and no laboratory test can detect it. People who have ever been injected with any kind of non-prescription drug, and anyone who have ever been paid for sex with money or drugs are also permanently barred from donating blood.\n\nMen who have sex with men (MSM) may donate blood if they have not engaged in oral or anal sex with another man at least 12 months prior to a donation. This policy came into effect from 16 January 2017.\n\nThe IBTS \"accepts that they are being discriminatory; we discriminate against several groups in the community insofar as we refuse to allow them to donate blood on the basis of perceived increased risk of spreading infections through blood transfusion\". Several campaigns have been launched in an effort to reverse the ban. Gay Doctors Ireland denounced the ban as \"unscientific\" and outdated.\n\nOn 27 July 2015, Tomás Heneghan, a 23-year-old University of Limerick student and journalist from Galway began a legal challenge in the High Court against the permanent deferral imposed on MSM donors. He argued that the questionnaire and interview process used by the IBTS does not adequately assess the risk of disease transmission posed by his donation. He claimed this is in breach of EU law. He said that both failed to consider the length of time between a donor's last sexual experience and the end of a “window period” in which infections are sometimes not detected. Heneghan's previous sexual activity posed no risk of infection, according to HSE-approved advice and he said the service had no evidence upon which it could legitimately impose a lifelong ban on him donating blood.\n\nFollowing several adjournments of the case to allow the blood service and Department of Health to examine and develop the donation policies, in late June 2016 the Irish Blood Transfusion Service recommended that the lifetime ban on MSM be reduced to a 12-month ban. Later that week the Minister for Health Simon Harris agreed to the recommendations and announced the reduction would take place. However no timeline was initially reported for the implementation of the new policies.\n\nOn 26 July 2016 Heneghan dropped his High Court challenge against the service as an end to the lifetime deferral on MSM blood donors had been announced in the interim. Heneghan then wrote about his experiences of challenging the ban in a number of national media outlets. He also appeared on TV3's Ireland AM show to speak about his case.\n\nOn 2 October 2016, it was reported that Minister Harris would implement the new policy from 16 January 2017, almost seven months after he announced the policy change.\n\nOn 16 January 2017, Heneghan (now 25) attended a blood donation clinic in D'Olier Street, Dublin and became the first man who has had sex with another man to donate blood openly in the Republic of Ireland since the lifetime deferral policy was first introduced in the 1980s. However he also criticised the new 12 month deferral policy on MSM and called on Ireland's Health Minister to initiate a review of the IBTS and replace the 12 month deferral period for MSM with no deferral or a 3 month deferral on all donors following sexual intercourse.\n\nPreviously in August 2013 Heneghan had alleged the Irish Blood Transfusion Service had discriminated against him despite his assertion that he had never had oral or anal sex with another man.\n\nDonors are recognised for their commitment by being awarded as follows: A silver award is given for 10 donations; a gold award for 20 donations; a gold drop-shaped lapel pin (representing blood) for 50 donations; and presentation at an awards dinner ceremony, and a porcelain pelican, for 100 donations.\n\nThe Irish Blood Transfusion Service is also responsible for the collection of blood platelets and for managing the Unrelated Bone Marrow registry in Ireland. Donors can give platelets at the National Blood Centre in St James Hospital in Dublin or at St Finbarr's Hospital in Cork. Donors can join the unrelated bone marrow registry through their local blood clinic by offering an extra blood sample and satisfying suitability criteria.\n\n\n"}
{"id": "51696564", "url": "https://en.wikipedia.org/wiki?curid=51696564", "title": "Italy towel", "text": "Italy towel\n\nThe Italy towel, also known as the Korean exfoliating mitt, is a mass-produced bath product used to scrub and peel the outermost layer of skin; it was invented in Busan by Kim Pil-gon in 1962. Since then, the Italy towel has become a household item in Korean homes and a staple item in Korean saunas. The Korean exfoliating mitt was named the Italy towel because the viscose fabric used to make it was imported from Italy at the time.\n"}
{"id": "10444815", "url": "https://en.wikipedia.org/wiki?curid=10444815", "title": "John Martin Munro Kerr", "text": "John Martin Munro Kerr\n\nJohn Martin Munro Kerr (5 December 1868 – 7 October 1960) was Regius Professor of Midwifery at the University of Glasgow from 1927 to 1934. A scholar and surgeon of international acclaim he won both the Katherine Bishop Harman Prize in 1934 for his book \"Maternal Mortality and Morbidity\" (1933) and was the first recipient of the Blair Bell Medal for obstetrics and gynaecology.\n\nJ M Munro Kerr was born at Kelvingrove Street in Glasgow in 1868 the son of George Munro Kerr (15 November 1836 – 23 June 1907), a Scottish ship-owner from Greenock, and Jessie Elizabeth Martin. His grandfather, John Kerr, had been a West Indian Merchant and ship-owner who married an American-born wife, Mary Clark. J.M. Munro Kerr graduated from the University of Glasgow MB CM in 1890. As a senior undergraduate he was present in 1889 when Murdoch Cameron performed the first series of follow-up Caesarean Section operations at the Glasgow Royal Maternity Hospital, carried out after Cameron’s famous initial success in 1888.\n\nFluent in German and French, Munro Kerr spent a number of years after his graduation in Germany, Austria and Ireland studying obstetrics and gynaecology at Berlin, Vienna and Dublin. From 1894 he acted as Professorial Assistant to Murdoch Cameron, a position that entailed both academic work at the University of Glasgow and practical experience on the obstetrical and gynaecological wards of Glasgow Royal Maternity Hospital and Glasgow's Western Infirmary.\n\nAppointed Visiting Surgeon at the Maternity Hospital in 1900, he published to great success \"Operative Midwifery\" in 1908. The text was originally written as the thesis for his MD at Glasgow. Munro Kerr was elected to the chair of Obstetrics and Gynaecology at Glasgow Anderson College in 1910, and in a rapidly successful career he took the Muirhead chair of Obstetrics and Gynaecology at the University of Glasgow in 1911. His First Assistant at this time was Louise McIlroy. \n\nA Foundation Fellow Royal College of Obstetricians and Gynaecologists in 1929 Munro Kerr was also its first Vice President until 1932. Succeeding Murdoch Cameron on his retirement as Professor of Midwifery at Glasgow, Munro Kerr took the chair in the New Year of 1927 holding the position until his retirement in 1934. Munro Kerr won the Katherine Bishop Harman Prize in 1934 for his widely successful book \"Maternal Mortality and Morbidity\" (1933).\n\nDuring World War II he acted as Medical Superintendent of the Kent and Canterbury Hospital.\n\nMunro Kerr was made an honorary LLD by Glasgow in 1935. He was the first recipient of the Blair Bell Medal, awarded to him by the Royal Society of Medicine in 1950.\n\nFollowing his retirement he lived in Canterbury that it was there that he died in 1960.\n\n\n"}
{"id": "41337286", "url": "https://en.wikipedia.org/wiki?curid=41337286", "title": "John Maxwell Anderson", "text": "John Maxwell Anderson\n\nJohn Maxwell Anderson FRCS, FRCSED (1928 – 31 January 1982) was a Scottish consultant surgeon and cancer specialist whose research focused on tissue transplantation, cancer immunology and chemotherapy.\n\nJohn Maxwell Anderson was educated at Madras College, St Andrews and Strathallan School, Perthshire. He graduated from the University of St Andrews in 1952 (MB ChB). Following the completion of his national service with the Royal Air Force, Anderson had a varied postgraduate training. In 1959 he became a Fellow of the Royal College of Surgeons (FRCS) and a Fellow of the Royal College of Surgeons of Edinburgh (FRCSED).\n\nIn 1960 Anderson was appointed senior registrar to Professor Ian Aird at the Hammersmith Hospital in London. Thereafter, he went to Harvard Medical School and Peter Bent Brigham Hospital in Boston where he conducted valuable original research. In 1966 Anderson was appointed consultant surgeon at Glasgow Royal Infirmary, eventually becoming consultant general surgeon. He published several notable books and papers during his career.\n\nAnderson's grandfather, Jamie Anderson, won The Open Championship in three consecutive years between 1877 and 1879.\n\n"}
{"id": "892969", "url": "https://en.wikipedia.org/wiki?curid=892969", "title": "Laboratoires Pierre Fabre", "text": "Laboratoires Pierre Fabre\n\nLaboratoires Pierre Fabre is a French multinational pharmaceutical and cosmetics company. The company had a consolidated turnover of 1.978 billion euros in 2012 (including 54% international). It is headquartered in the city of Castres, Midi-Pyrénées, France.\n\nFounded in 1962 by Pierre Fabre (1926-2013), the company is present in over 130 countries. Laboratories Pierre Fabre had approximately 10,000 employees in 2012, 33% of whom are internationally based, while the remaining 6,700 employees were based in France. The company's business activity is focused on research, development, manufacturing and marketing of cosmetics, prescription medicines and family health products. Pharmaceuticals and phytotherapy represent 47% of turnover, whereas cosmetics represent 53% (percentages of 2012 turnover).\n\nPierre Fabre is best known for its vinorelbine (Navelbine), an anticancer drug of the \"vinca\" alkaloid class. They also developed vinflunine, a fluorinated \"vinca\" alkaloid derivative available in Australia for \"advanced or metastatic transitional cell carcinoma of the urothelial tract after failure of a prior platinum containing regimen.\"\n\nOlivier Bohuon, now Chief Executive of Smith & Nephew, was Chief Executive from September 2010 to April 2011.\n\nThe Pierre Fabre Foundation was recognized as a public utility in 1999 and its mission is to help third-world countries to obtain quality drugs, ensure better quality control of drugs, use local therapeutic resources and train scientists for the inspection of drugs.\n\n"}
{"id": "33165749", "url": "https://en.wikipedia.org/wiki?curid=33165749", "title": "List of United Nations Security Council Resolutions related to the conflicts in former Yugoslavia", "text": "List of United Nations Security Council Resolutions related to the conflicts in former Yugoslavia\n\nThis list contains the resolutions of the UN Security Council connected to the conflicts in former Yugoslavia in period from 1991–2000\n"}
{"id": "25345991", "url": "https://en.wikipedia.org/wiki?curid=25345991", "title": "List of incidents involving ricin", "text": "List of incidents involving ricin\n\nThis is a list of incidents involving the poison ricin.\n\nOn September 7, 1978, the Bulgarian dissident Georgi Markov was jabbed in the leg in public on Waterloo Bridge in the middle of London by a man using a weapon built into an umbrella. The weapon embedded a small pellet containing ricin into Markov's leg. Markov died four days later.\n\nOn August 14, 1981, exposed CIA double agent Boris Korczak was shot with an air gun which fired a minuscule pellet containing ricin into his kidney. This attempt on his life happened while he was shopping at Giant Food Store in Vienna, Virginia. Korczak and the CIA are convinced that this was the work of the KGB as he had penetrated deep into the secret organization and damaged them for millions of dollars. Korczak survived, which he attributes to the fact that he was shot in the kidney and that his body treated the projectile as though it were a kidney stone, thus limiting exposure of his body to the toxin.\n\nIn April 1993, Thomas Lavy was caught while trying to smuggle 130 grams of ricin from Alaska into Canada. Lavy stated that he purchased the ricin to poison coyotes on his farm in Arkansas and keep them away from his chickens. Lavy was stopped at the Beaver Creek border crossing by Canadian custom agents who found, along with the 130 grams of ricin, $89,000, a knife, four guns, and 20,000 rounds of ammunition.\n\nAuthorities discovered various toxic substances in the house of Thomas Leahy in Janesville, Wisconsin. They discovered the substances after they had been called to Leahy's home after he had shot his son in the face, following a night of drinking. Among the chemicals discovered were 0.67 grams of ricin and nicotine mixed with a solvent that allowed it to penetrate the skin and have lethal effects.\nAuthorities also found books relating to the production of chemical and biological agents.\nChemicals were also found in a storage shed that Leahy kept in Harvard, Illinois.\nHe reportedly told his sister that he was going to use the poison to coat razor blades and mail them to his enemies in hopes that they would cut themselves and be exposed.\nLeahy pleaded guilty to possession of the ricin and was sentenced to eight years for the shooting and six-and-one-half years for possessing dangerous materials.\n\nInternal Revenue Service (IRS) investigators searched the home of James Dalton Bell, a 39-year-old electronics engineer, and discovered a cache of chemicals, which included sodium cyanide (500 grams), diisopropyl fluorophosphate, and a range of corrosive acids.\nSubsequent analysis of computer files confiscated from the residence revealed that Bell engaged in e-mail communications with a friend, Robert East, a 46-year-old merchant marine radio operator, that expressed a desire to obtain castor beans to see if they could extract ricin.\nBell had already acquired the home addresses of nearly 100 federal employees from the Federal Bureau of Investigation (FBI), IRS, and Bureau of Alcohol, Tobacco and Firearms, and computer files from voter registration.\nBell was in the process of producing and acquiring chemical and biological agents.\n\nThree members of a splinter group of the North American Militia in Michigan were arrested on weapons and conspiracy charges.\nThe April 1998 indictment was the result of an investigation involving an Alcohol, Tobacco, and Firearms (ATF) agent who infiltrated the group in March 1997.\nWhen federal law enforcement raided the homes of these men, they discovered an arsenal of weapons and a videotape.\nProduced in a cooking-show format, the tape gave instructions on how to manufacture bombs and other assorted militia-type weaponry, including a feature segment on how to extract ricin from castor beans.\nDuring the court proceedings, prosecutors drew attention to the ricin segment, stating that the men were \"collecting information on the manufacture and use of ricin.\" However, other than the videotape, no materials associated with ricin production were found in any of the raids.\n\nPress reports indicated that FBI agents had apprehended a man in Tampa, Florida, for threatening to kill court officials and \"wage biological warfare\" in Jefferson County, Colorado.\nJames Kenneth Gluck, 53, a former Colorado resident, sent a 10-page letter to Jefferson County judges threatening to kill them with a biological agent.\nHe specifically identified one judge by name.\nFBI agents arrested Gluck on 5 November 1999 as he left a public library near his home in Tampa.\nPolice, fire, and hazardous materials (HazMat) crews responded to the scene along with the FBI and blocked off Gluck's street.\nUpon searching his residence the next day, agents discovered that Gluck had the necessary ingredients to make ricin, though no refined ricin was actually found.\nThey also found test tubes and beakers, as well as a copy of The Anarchist Cookbook and books on biological toxicology, in a makeshift laboratory in his home.\n\nThe Russian Federal Security Service told the Itar-Tass news service it had intercepted a recorded conversation between two Chechen field commanders in which they discussed using homemade poisons against Russian troops.\nAccording to Itar-Tass, Chechen Brigadier General Rizvan Chitigov asked Chechen field commander Hizir Alhazurov, who is now living in the United Arab Emirates, for instructions on the \"homemade production of poison\" for use against Russian soldiers.\nRussian authorities reportedly raided Chitigov's home and seized materials, including instructions on how to use toxic agents to contaminate consumer goods, a small chemical laboratory, three homemade explosives, two land mines, and 30 grenades.\nThe confiscated papers reportedly also contained instructions on how to produce ricin from castor beans.\n\nKenneth R. Olsen, 48, was arrested for possession of the biological agent ricin in his Spokane Valley, WA, office cubicle. Co-workers at Agilent, a high-tech company, tipped FBI officials about the software engineer after discovering documents on \"how to kill\", undetectable poisons, and bomb-making Olsen had printed out from his computer. Olsen insisted that his research was for a Boy Scout project, but did not say more. Further investigation of his office produced test tubes, castor beans, glass jars, and approximately 1 gram of ricin. In July 2003 Olsen was convicted of possessing a chemical weapon and possessing a biological weapon. He was sentenced to 165 months, almost 14 years in prison.\nReports have emerged that Ansar al-Islam, a Sunni militant group, has been involved in testing poisons and chemicals including ricin.\nAccording to one report the group tested ricin powder as an aerosol on animals such as donkeys and chickens and perhaps even an unwitting human subject.\nNo more specific details have been released.\n\nOn 5 January 2003 the Metropolitan Police raided a flat in north London and arrested six Algerian men whom they claimed were manufacturing ricin as part of a plot for a poison attack on the London Underground. No ricin was recovered as a result of this raid. Only one person was convicted (of conspiracy to cause a public nuisance by the use of poisons and/or explosives to cause disruption, fear or injury) and jailed for 17 years. He had previously received a life sentence for stabbing and killing a policeman during the raid.\n\nThe U.S. Secretary of State Colin Powell used this incident in his 5th February 2003 speech to the UN as part of the case for the 2003 Invasion of Iraq, as the \"UK poison cell\" part of the alleged Abu Musab al-Zarqawi global terrorist network.\n\nIn 2003, a package and letter sealed in a \"ricin-contaminated\" envelope was intercepted in Greenville, South Carolina, at a United States Postal Service processing center.\n\nRicin was detected in the mail at the White House in Washington, D.C. in November 2003. The letter containing it was intercepted at a mail handling facility off the grounds of the White House, and it never reached its intended destination. The letter contained a fine powdery substance that later tested positive for ricin. Investigators said it was low potency and was not considered a health risk. This information was not made public for nearly 3 months, when preliminary tests showed the presence of ricin in an office mailroom of U.S. Senate Majority Leader Bill Frist's office. There were no signs that anyone who was near the contaminated area developed any medical problems. Several Senate office buildings were closed as a precaution.\n\nIn January 2006, ricin was found in a home in suburban Richmond, Virginia in the form of mashed castor beans. The suspect, Chetanand Sewraz, was allegedly isolating the toxin to kill his estranged wife.\n\nIn February 2008, a man who stayed in a Las Vegas motel room where ricin was found was taken to the hospital in critical condition. The man, Roger Von Bergendorff, was hospitalized on February 14; however, the ricin was not found until February 27 when a relative retrieved his luggage because the motel had not been paid for two weeks. Firearms and an \"anarchist type textbook\" were found in the same motel room where several vials of ricin were found, police reported. According to Las Vegas 8 Television news, police noted the ricin section of the textbook was highlighted. On March 3, FBI agents searched at Riverton, Utah house and several storage lockers in West Jordan, Utah linked to Bergendorff, but did not find any traces of ricin.\nBergendorff awoke from a coma on March 14. He was questioned by police as to why he had such a large quantity of ricin. Subsequently, he was arrested on April 16 and charged with possession of a biological toxin and two weapons offenses.\n\nThe managers of eleven gay bars in the Capitol Hill region of Seattle received letters from an anonymous sender claiming to be in possession of 67 grams of ricin that would be used to dose exactly 5 patrons from each establishment with the intent of killing them.\n\nSpeculations that the terrorist was possibly a homosexual himself abound, particularly as the letter directly quotes a poem by gay author Mark Doty in a recently published anthology.\n\nDuring the raid on the homes of a man and son in June 2009, a very small amount of ricin was allegedly found in a sealed jam jar kept in a kitchen cupboard. A father and son, Ian and Nicky Davison were arrested under the 2000 Terrorism Act. The arrests followed a long-running intelligence-led operation against extreme right-wing activity. Ian Davison was sentenced to ten years in May 2010, for preparing acts of terrorism, three counts of possessing material useful to commit acts of terrorism and possessing a prohibited weapon; his son was given two years youth detention for possessing material useful to commit acts of terrorism.\n\nOn June 4, 2009 local ABC affiliate KOMO 4 News reported that authorities had isolated a suburban home in Everett, WA and part of the surrounding neighborhood after the suspected discovery of ricin in the home. The suspected discovery of ricin occurred after the residents, a husband and wife, returned from the hospital following a domestic disturbance report.\n\nIn January 2011, FBI agents discovered what was thought to be ricin in a Coventry Township, Ohio home, and later reported that tests confirmed its presence.\n\nIn 2011, the FBI arrested four men in the U.S. state of Georgia, who were allegedly plotting to deploy explosives and biological weapons to kill a number of American politicians, media figures, Internal Revenue Service employees, and innocent civilians. The four men were Frederick Thomas, 73, Dan Roberts, 67; Ray H Adams, 65; and Samuel J. Crump, 68. Thomas is from Cleveland, Georgia; the other three men are from Toccoa. They were members of a domestic militia group and believed they had to commit murder in order to \"save this country\". According to \"The Guardian\", Crump had planned to make 10 pounds of ricin and spread it in major cities and along Atlanta, Jacksonville, Newark, Washington D.C., and New Orleans highways and bomb federal buildings in Atlanta. They also discussed dispersing ricin from an airplane in the sky over Washington D.C. and possibly attack other targets with explosives. Adams is a former Agriculture Research Service employee, while Crump used to work at the Centers for Disease Control and Prevention.\n\nAccording to court documents, Thomas was inspired by the online pro-militia novel \"Absolved\" by Mike Vanderboegh, which features small bands of U. S. citizens rising up against the federal government. Vanderboegh denied responsibility for inspiring the attack, saying in a blog post \"I am as much to blame for the Georgia Geriatric Terrorist Gang as Tom Clancy is for Nine Eleven.\" Earlier, Vanderboegh had attracted controversy after urging health care reform opponents to throw bricks through the windows of Democratic Party offices; several such incidents occurred after Vanderboegh made his statement.\n\nOn August 22, 2012, Thomas and Roberts have been sentenced to 5 years in federal prison.\n\nOn April 16, 2013, an envelope that tested positive for ricin was intercepted at the US Capitol's off-site mail facility in Washington, DC. According to reports, the envelope was addressed to the office of Senator Roger Wicker, R-Mississippi.\n\nThe next day, an envelope addressed to President Obama was tested positive for ricin. Another letter was sent to a Mississippi judge and is also being tested for ricin.\n\nBoth letters included the phrases \"to see a wrong and not expose it, is to become a silent partner to its continuance.\" and \"I am KC and I approve this message.\"\n\nIn May 2013, while going through a divorce, US actress Shannon Richardson called the police and accused her husband of mailing ricin to several politicians.\n\nNathan Richardson has not been charged with any crime. He told investigators that his wife set him up. Investigators found that Shannon Richardson indeed mailed the ricin herself, in an effort to set up her estranged husband.\n\nShannon Richardson was arrested on June 7, 2013 for alleged connections with ricin laced letters sent to politicians including President Barack Obama and New York City mayor Michael Bloomberg. She was charged with ″mailing a threatening letter to President Barack Obama″. On June 6, she confessed that she had mailed the three letters, knowing they contained ricin, but claimed her husband made her do it. On December 10, she pleaded guilty to sending the letters. The plea limits her potential sentence to 18 years.\n\nA 37-year-old female ingested the pulp of 30 castor beans in an attempt to commit suicide from ricin poisoning. Ricin was created during processing of the bean and trace amounts of the toxin were later found in her residence. She was in intensive care for a week but survived the incident without lasting physical effects.\n\nOn March 21, 2014, 19-year-old Nicholas Todd Helman was arrested for sending a scratch-and-sniff birthday card laced with ricin to a man dating his ex-girlfriend. \nHelman was charged with attempted murder and risking catastrophe after lab tests showed that the card he placed in the man's family mailbox on March 6 contained traces of the toxic substance. Helman bragged of the toxic card to a coworker at Target in Warrington later that day. The coworker then notified authorities, and the police called the man's home and spoke to his mother, asking whether she had retrieved the mail that day. When Helman was first questioned about the incident on March 7, he told police that he had only coated the card with sodium hydroxide, which he chose because it resembled the toxin anthrax.\nHelman also admitted to sending threatening messages to the man via Facebook, and police seized from him what appeared to be sodium hydroxide and a notebook with a ricin recipe. Helman was charged on March 7 with terroristic threats and harassment.\nIn the meantime, authorities sent the card away for subsequent lab tests. The results confirmed that the card indeed had traces of ricin, prompting the Warminster Police Department to arrest Helman at his Hatboro apartment, assisted by numerous agencies including the Hatboro police, a hazmat team, a SWAT team, police officers and officials with the FBI. After a standoff that lasted several hours, Helman was led out of his apartment and into a police vehicle by officers clad in armor and hazmat gear.\n\nOn July 29, 2015, 31-year-old Mohammed Ali from Liverpool, England was convicted at the Old Bailey of attempting to possess a chemical weapon. In January 2015, Ali had attempted to buy 500 mg of ricin on the dark web, but he had been in contact with an FBI agent and was sent a harmless powder. Ali said that he had been influenced by the television series \"Breaking Bad\". On September 18, 2015, Ali was sentenced to eight years in prison.\n\nOn February 2, 2017, 27 year old William Christopher Gibbs drove himself to the hospital Emergency Room, stating that he had been exposed to Ricin. His car was subsequently tested for the substance and found to be positive. This resulted in the Army National Guard and the local fire department being called out to his home town of Morganton, and the area was swept by personnel in hazmat suits. The FBI subsequently declared that \"no evidence that any poisonous or toxic substances have been dispersed or that the public is at risk\", but that they would continue to investigate. He remains in the local county jail, but may yet be charged with Federal offenses. Multiple sources have indicated that Gibbs was connected to the white supremacist group, Creativity Alliance.\n\nOn October 2, 2018 ricin particles were detected in mail sent to the Pentagon. This mail was addressed to Secretary of Defense Jim Mattis,Chief of Naval Operations John M. Richardson,President Donald J. Trump, and Senator of Texas Ted Cruz.\n"}
{"id": "36222220", "url": "https://en.wikipedia.org/wiki?curid=36222220", "title": "Lorna Smith Benjamin", "text": "Lorna Smith Benjamin\n\nLorna Smith Benjamin (born 1934) is an American psychologist best known for her innovative treatment of patients with personality disorders who have not responded to traditional therapies or medications.\n\nShe received a B.A. in psychology from Oberlin College in Ohio, and a masters degree and Ph.D. from the University of Wisconsin, where she studied under Harry Harlow, working with the baby monkeys in his famous \"wire mother\" experiments.\n\nBenjamin has practiced as a licensed psychologist in Wisconsin and Utah. She taught at the University of Wisconsin Medical School from 1971 to 1986 and was a professor of psychology at the University of Utah from 1988 until her retirement in 2012.\n\nIn 1968 she began work on the Structural Analysis of Social Behavior (SASB), a method she invented to categorize personality disorders. Originally conceived as a tool for studying primate behavior, SASB was used to understanding personality disorders when they were first described in the Diagnostic and Statistical Manual of Mental Disorders in 1980. SASB describes human relationships as fitting along two axes: \"love-hate,\" and \"enmeshment-differentiation,\" with the additional dimension of \"interpersonal focus.\" Benjamin received an honorary degree from the Umeå University, Sweden, for her work with SASB.\n\nAfter many years of work on the SASB model, Benjamin developed Interpersonal Reconstructive Therapy (IRT), based on SASB. Rather than concentrating on the amelioration of symptoms (except in crisis situations), IRT focuses on identifying the patterns underlying a patient's maladaptive behavior and guiding them toward the formation of new, healthier patterns. SASB is used both to aid patients in understanding problems in their relationships, and to help them conceive of the form that improved relationships would take.\n\nBenjamin is founder of the Interpersonal Reconstructive Therapy Clinic at the University of Utah Neuropsychiatric Institute. She and Kenneth Critchfield are co-directors of the clinic.\n\n\n"}
{"id": "2677666", "url": "https://en.wikipedia.org/wiki?curid=2677666", "title": "Medical restraint", "text": "Medical restraint\n\nMedical restraints are physical restraints used during certain medical procedures to restrain patients with the minimum of discomfort and pain and to prevent them from injuring themselves or others.\n\nThere are many kinds of mild, safety-oriented medical restraints which are widely used. For example, the use of bed rails is routine in many hospitals and other care facilities, as the restraint prevents patients from rolling out of bed accidentally. Newborns frequently wear mittens to prevent accidental scratching. Some wheelchair users use a belt or a tray to keep them from falling out of their wheelchairs. In fact, not using these kinds of restraints when needed can lead to legal liability for preventable injuries.\n\nMedical restraints are generally used to prevent people with severe physical or mental disorders from harming themselves or others. A major goal of most medical restraints is to prevent injuries due to falls. Other medical restraints are intended to prevent a harmful behavior, such as hitting people.\n\nEthically and legally, once a person is restrained, the safety and well being of the restrained person falls upon the restrainer, appropriate to the type and severity of the restraining method. For example, a person who is placed in a secured room should be checked at regular intervals for indications of distress. At the other extreme, a person who is rendered semi-conscious by pharmacological (or chemical) sedation should be constantly monitored by a well-trained individual who is dedicated to protecting the restrained person's physical and medical safety. Failure to properly monitor a restrained individual may result in criminal and civil prosecution, depending on jurisdiction.\n\nAlthough medical restraints, used properly, can help prevent injury, they can also be dangerous. The United States Food and Drug Administration (FDA) estimated in 1992 that improper use of restraints results in at least 100 deaths each year, most by strangulation. FDA also noted reports of injuries — including broken bones and burns — caused by the improper use of restraints. Medical restraints in psychiatric hospitals in Japan are sometimes kept on patients for weeks and months \n, and they are thought to have caused several deaths due to deep vein thrombosis and pulmonary embolism. More information about Japanese use of restraints is described in the page on physical restraints.\n\nBecause of the potential for abuse, the use of medical restraints is regulated in many jurisdictions. At one time in California, psychiatric restraint was viewed as a treatment. However, with the passing of SB-130, which became law in 2004, the use of psychiatric restraint(s) is no longer viewed as a treatment, but can be used as a behavioral intervention when an individual is in imminent danger of serious harm to self or others.\n\nThere are many types of medical restraint:\n\n\nA number of private national and regional companies teach physical (non-mechanical) restraint techniques for companies and agencies that care for or have custody of people who might become aggressive. The strategies vary widely, with many based on police or martial art pain compliance techniques, with others using only pain-free techniques. Most also emphasize verbal de-escalation and defusing skills before using any physical skills. A non-inclusive list:\n\n\nThroughout the last decade or so, there has been an increasing amount of evidence and literature supporting the idea of a restraint free environment due to their contradictory and dangerous effects. This is due to the adverse outcomes associated with restraint use, which include: falls and injuries, incontinence, circulation impairment, agitation, social isolation, and even death.\n\nCurrent United States law requires that most involuntary medical restraints may only be used when ordered by a physician. Such a physician's order, which is subject to renewal upon expiration if necessary, is valid only for a maximum of 24 hours.\nThe article on physical restraint discusses medical restraints in specific countries. It states:\" \n\nJapanese law states that psychiatric hospitals may use restraints on patients only if there is a danger that the patients will harm themselves or others. The law also states that a designated psychiatrist must approve the use of restraints and examine the patient at least every 12 hours to determine whether the situation has changed and the patient should be removed from restraints.\nHowever in practice, Japanese psychiatric hospitals use restraints fairly often and for long periods. Despite being required to certify every 12 hours whether a patient still needs restraints, Japanese psychiatric hospitals keep patients in restraints for a much longer time than hospitals in other countries. According to a survey conducted on 689 patients in 11 psychiatric hospitals in Japan, the average time spent in physical restraints is 96 days.\nMeanwhile, the average time in most other developed countries is at most several hours to tens of hours. \n\nThe number of people who are physically restrained in Japanese psychiatric hospitals continues to increase. In 2014 more than 10,000 people were restrained-the highest ever recorded, and more than double the number a decade earlier. It is thought that some of that increase includes older patients with dementia. As a result, the Japanese Ministry of health has revised its guidelines for elderly people in nursing homes to have more restrictions against body restraints. The changes took effect on 1 April 2018.\n\n1. Long restraint times led to the death of a Japanese patient in 2016 by pulmonary embolism after three weeks in continuous restraints. \n\n2. In May of 2017, 27-year old Kelly Robert Savage, a dual New Zealand/US Citizen who had been teaching English in the Japan Exchange and Teaching (JET) program, died after being held in restraints. He was held in restraints nearly continuously for ten days at Yamato Hospital, a psychiatric hospital in Kanagawa Prefecture (大和病院、神奈川県), until he was discovered to be in cardiac arrest. He was transferred to a general hospital, where his heart was re-started. But he never regained consciousness and died a week later. Although the autopsy was inconclusive, several of Savage’s doctors, and his family consider that a likely cause of his death was deep vein thrombosis and pulmonary embolism due to his lack of movement caused by the restraints. The family have been working with Toshio Hasegawa, Professor of Health Faculty at Kyorin University, to set up a group to appeal to the country to reduce the use of physical restraints in psychiatric treatment. Savage’s case was reported widely and a national debate began on the use of restraints in psychiatric care. The Japanese Ministry of Health, Labour and Welfare has convened a committee to review their guidelines on the use of restraints to see if they need to be changed.The review was meant to be completed before the end of the Japanese fiscal year on 31 March 2018. However, the deadline was extended to 31 March 2019.\n\n3. On 15 November 2017 a 28-year old Japanese man was found dead while he was restrained in a psychiatric care facility in the Tokyo area. The police are looking into the death.\n\n4. In 2018, three new cases of wrongful use of restraints were publicised in Japan when family members or victims themselves sued hospitals. First, in May of 2018 a woman sued for ongoing psychological trauma stemming from her hospitalisation for anorexia when she was 14 years old. She had been kept in restraints for 77 days. In July the family members of a woman whose autopsy confirmed that she had died of pulmonary embolism after being kept for 8 days in restraints, sued the psychiatric hospital restraining her. In August the family of a man who died after being kept in restraints for six days sued the psychiatric hospital that restrained him.\n\nThe Millfields Charter is an electronic charter which promotes an end to the teaching to frontline healthcare staff of all prone (face down) restraint holds. In June 2013 the UK government announced that it was considering a ban on the use of face-down restraint in English mental health hospitals.\n\nFace down restraints are used more often on women and girls than on men. 51 out of 58 mental health trusts use restraints unnecessarily when other techniques would work. Organisations opposed to restraints include Mind and Rethink Mental Illness. YoungMinds and Agenda claim restraints are “frightening and humiliating” and “re-traumatises” patients especially women and girls who have previously been victims of physical and/or sexual abuse. The charities sent an open letter to health secretary, Jeremy Hunt showing evidence from 'Agenda, the alliance for women and girls at risk', revealing that patients are routinely restrained in some mental health units while others use non-physical ways to calm patients or stop self-harm. According to the letter over half of women with psychiatric problems have suffered abuse, restraint can cause physical harm, can frighten and humiliate the victim. Restraint, specially face down restraint can re-traumatise patients who previously suffered violence and abuse. “Mental health units are meant to be caring, therapeutic environments, for people feeling at their most vulnerable, not places where physical force is routine.”\n\nGovernment guidelines state that face down restraint should not be used at all and other types of physical restraint are only for last resort. Research by Agenda found one fifth of women and girl patients in mental health units had suffered physical restraint. Some trusts averaged over twelve face down restraints per female patient. Over 6% of women, close to 2,000 were restrained face-down in total more than 4,000 times. The figures vary widely between regions.\n\nSome trusts hardly use restraints, others use them routinely. A woman patient was in several hospitals and units at times for a decade with mental health issues, she said in some units she suffered restraints two or three times daily. Katharine Sacks-Jones director of Agenda, maintains trusts use restraint when alternatives would work. Sacks-Jones maintains women her group speak to repeatedly describe face down restraint as a traumatic experience. On occasions male nurses have used it when a woman did not want her medication. “If you are a woman who has been sexually or physically abused, and mental health problems in women often have close links to violence and abuse, then a safer environment has to be just that: safe and not a re-traumatising experience. (...) Face-down restraint hurts, it is dangerous, and there are some big questions around why it is used more on women than men.” The use of restraints in UK psychiatric facilities is increasing.\"\n\n"}
{"id": "3166445", "url": "https://en.wikipedia.org/wiki?curid=3166445", "title": "Mind (charity)", "text": "Mind (charity)\n\nMind is a mental health charity in England and Wales. Founded in 1946 as the National Association for Mental Health (NAMH), it celebrated its 70th anniversary in 2016.\n\nMind offers information and advice to people with mental health problems and lobbies government and local authorities on their behalf. It also works to raise public awareness and understanding of issues relating to mental health. Since 1982, it has awarded an annual prize for \"Book of the Year\" having to do with mental health, in addition to three other prizes. Since 2008 Mind has hosted the annual Mind Media Awards, celebrating the best portrayals and reporting of mental health across the media. \n\n132 local Mind associations (independent, affiliated charities) provide services such as supported housing, floating support schemes, care homes, drop-in centres and self-help support groups. Local Mind associations are often very different in size, make up and character—it is a common misconception that they all work to the same policy and procedural framework. Mind is a national brand but all local associations are unique, although they do all sign up to certain shared aims and ethical guidelines.\n\nMind was originally known as the National Association for Mental Health (NAMH), founded in 1946 from three voluntary organisations that provided services for the \"maladjusted, emotionally disturbed or mentally handicapped to any degree.\" The name MIND was introduced in 1972, and the lowercase version \"Mind\" was introduced in the 1990s.\n\nThe National Association for Mental Health was formed (initially as national Council) by the merging of the following three organisations toward the end of the second world war:\n\n\nThe NCMH had been an organisation of psychiatrists and psychologists, while the CAMH comprised representatives of various voluntary bodies. Among other things, they helped run and monitor institutions for the mentally handicapped, and developing training for mental health professionals. They were both part of the social hygiene movement, and had advocated eugenics and sterilisation as a means of dealing with those considered too mentally deficient to be assisted into healthy productive work and contented family life.\n\nThe beginnings of the National Association for Mental Health also coincided with the development of the National Health Service and the welfare state.\n\nIn 1969, numerous Scientologists joined the NAMH and attempted to ratify as official policy a number of points concerning the treatment of psychiatric patients. When their identity was realised they were expelled from the organisation \"en masse\". The Church of Scientology in 1971 unsuccessfully sued the NAMH over the matter in the High Court, and the case became notable in British charity law.\n\nMind has celebrated World Mental Health Day annually since it was first observed in 1992. This occurs on 10 October.\n\nPaul Farmer became chief executive of Mind in 2006, moving from his position as director of public affairs at the charity Rethink.\n\nIn 2008 the charity Mental Health Media (formerly the Mental Health Film Council founded in 1963 following a Mind initiative) was merged into Mind, shutting down its Open Up service which had sought to empower mental health service users to speak up in their communities, and bringing with it control over its Mental Health Media Awards.\n\nStephen Fry succeeded Melvyn Bragg in 2011 as President of Mind.\n\nIn addition to its other activities, Mind campaigns for the rights of people who have experience of mental distress. Mind's current campaigns include:\n\nIn addition, Mind is part of the Time to Change coalition, along with Rethink. Time to Change is an England-wide campaign to end mental health discrimination.\n\nMind campaigns for the inclusion and involvement of (ex)users of mental health services. In its own organisation, at least two service users must be on the executive committee of each local Mind group. The charity operates Mind Link, a national network of service users, which is represented on Mind's Council of Management, its ultimate decision making body.\n\nFor 30 years Mind has celebrated published fiction or non-fiction writing by or about people with emotional or mental distress with the annual Mind Book of the Year Award.\n\nSince 2008 Mind took over control of the annual Mental Health Media Awards, which it renamed the Mind Media Awards. This is intended to \"recognise and celebrate the best portrayals of mental distress, and reporting of mental health, in the media\". However, the operational running of the Awards ceremony and the selection of judges is carried out by private company Keystone Conference & Events Management Ltd.\n\nWithin the complex debate on mental illness causality, Mind has developed a list of factors which in its view may trigger mental illness episodes.\n\nMind is involved in a campaign Rethink Mental Illness to reduce the stigma associated with psychiatric illness.\n\nNational Mind takes donations, sponsorship, grants and operates charity shops across England and Wales. Each local Mind association is an independent charity responsible for its own funding, although they are provided some project funds from national Mind. The total gross income of the local associations in 2009 was £87 million which, combined with the national Mind income of £25 million, gave a total of £112 million. At least some local associations report that the majority of their income is from the British government through local governmental and NHS grants (e.g. 74%).\n\nMind states that, while it accepts corporate support in general, it does not accept any money from pharmaceutical companies. This policy is binding on all local Minds who are not permitted to accept sponsorship or donations from pharmaceutical companies for their own events, or for fees or expenses for attending conferences.\n\nIn July 2015, Mind worked closely with Ofcom on the regulator’s UK Calling campaign to make sure communications were clear and easy to understand.\n\n\nGeneral: \n\n\n"}
{"id": "20152691", "url": "https://en.wikipedia.org/wiki?curid=20152691", "title": "Ministry of Health (Portugal)", "text": "Ministry of Health (Portugal)\n\nThe Ministry of Health (), is a Portuguese government ministry.\n"}
{"id": "1862494", "url": "https://en.wikipedia.org/wiki?curid=1862494", "title": "Mirror box", "text": "Mirror box\n\nA mirror box is a box with two mirrors in the center (one facing each way), invented by Vilayanur S. Ramachandran to help alleviate phantom limb pain, in which patients feel they still have a limb after having it amputated. The wider use of mirrors in this way is known as mirror therapy or mirror visual feedback (MVF).\n\nIn a mirror box the patient places the good limb into one side, and the residual limb into the other. The patient then looks into the mirror on the side with the good limb and makes \"mirror symmetric\" movements, as a symphony conductor might, or as we do when we clap our hands. Because the subject is seeing the reflected image of the good hand moving, it appears as if the phantom limb is also moving. Through the use of this artificial visual feedback it becomes possible for the patient to \"move\" the phantom limb, and to unclench it from potentially painful positions.\n\nBased on the observation that phantom limb patients were much more likely to report paralyzed and painful phantoms if the actual limb had been paralyzed prior to amputation (for example, due to a brachial plexus avulsion), Ramachandran and Rogers-Ramachandran proposed the \"learned paralysis\" hypothesis of painful phantom limbs Their hypothesis was that every time the patient attempted to move the paralyzed limb, they received sensory feedback (through vision and proprioception) that the limb did not move. This feedback stamped itself into the brain circuitry through a process of Hebbian learning, so that, even when the limb was no longer present, the brain had learned that the limb (and subsequent phantom) was paralyzed.\n\nDespite considerable research, as of 2016 the underlying neural mechanisms of mirror therapy (MT) are still unclear.\n\nAlthough the effectiveness of mirror therapy in reducing pain was previously questioned, recent research has produced a variety of beneficial outcomes. A study published in 2016 concluded that \"Mirror therapy (MT) is a valuable method for enhancing motor recovery in poststroke hemiparesis.\" \n\nSystematic reviews of the research literature have arrived at conflicting conclusions about the effectiveness of MT. A 2014 review found that MVF can exert a strong influence on the motor network, mainly through increased cognitive penetration in action control. However, a 2016 review concluded that the level of evidence is insufficient to recommend MT as a first intention treatment for phantom limb pain.\n\nThe effectiveness of mirror therapy continues to be evaluated.\n\nSince the 2000s, mirror therapy has also been available through virtual reality or robotics. However, these expensive technologies have not proven to be more effective than conventional mirror boxes. \n\n\n"}
{"id": "4290270", "url": "https://en.wikipedia.org/wiki?curid=4290270", "title": "National Corndog Day", "text": "National Corndog Day\n\nNational Corndog Day is a celebration concerning basketball, the corn dog (A corn dog is usually a hot dog sausage coated in a thick layer of cornmeal batter), Tater Tots, and American beer that occurs in March of every year on the first Saturday of the NCAA Men's Division I Basketball Championship.\n\nNational Corndog Day was inaugurated in 1992 in Corvallis, Oregon by Brady Sahnow and Henry Otley. The first celebration was informal and involved only corndogs and basketball. In subsequent years, National Corndog day was expanded to include tater tots and beer and gradually spread to other cities. The celebration currently is sponsored by Foster Farms, a Livingston, California-based poultry producer, PBR, a US Midwest-based beer company, and Jones Soda. Operations for National Corndog Day currently are governed by a board of directors consisting of event hosts (or \"city captains\") based in various cities in the United States.\n\nBy 2007, parties celebrating National Corndog Day occurred at 113 locations in more than 30 U.S. states, the District of Columbia and Australia. In 2008 participation increased to nearly 5, 000 parties on five continents, including one at McMurdo Station in Antarctica. In 2009 participation fell back to the trend line from the 2008 peak, with nearly 400 on March 21, 2009. National Corndog Day 2009 took place on Saturday, March 21st, 2009. \n\nOn March 16, 2012, Oregon Governor John A. Kitzhaber issued a Proclamation declaring March 17, 2012 to be National Corndog Day.\n\n\n\n"}
{"id": "43857266", "url": "https://en.wikipedia.org/wiki?curid=43857266", "title": "National Institute of Immunology, India", "text": "National Institute of Immunology, India\n\nNational Institute of Immunology (NII) is an autonomous research institute located in New Delhi, under the Department of Biotechnology (DBT) for research in immunology.\n\nNII was established on 24 June 1981, with Prof. M. G. K. Menon as Chairman of its governing body. It has its origins in the ICMR–WHO Research & Training Centre in Immunology at the All India Institute of Medical Sciences (AIIMS), Delhi, which was merged with NII in 1982.However, NII continued to function from AIIMS laboratory of its honorary Director Prof G.P. Talwar, till its new building was constructed in 1983, carved out of the Jawaharlal Nehru University (JNU) campus. G.P. Talwar is the founder director of this institute. A first of its kind vaccine for leprosyin India have been developed by NII and it was named as mycobacterium indicus prani.\n\n"}
{"id": "15766466", "url": "https://en.wikipedia.org/wiki?curid=15766466", "title": "National Network to End Domestic Violence", "text": "National Network to End Domestic Violence\n\nThe National Network to End Domestic Violence (NNEDV) is a 501(c)(3) not-for-profit organization founded in 1990, based in the District of Columbia. It is a network of state domestic violence coalitions, representing over 2,000 member organizations nationwide. National Network to End Domestic Violence works to address the many aspects of domestic violence.\n\nNational Network to End Domestic Violence performs legislative policy work with all three branches. National Network to End Domestic Violence has been called to testify before the U.S. Congress on domestic violence issues to assist state coalitions in better serving the needs of the victim by presenting research on domestic violence issues for pending legislation.\n\nNational Network to End Domestic Violence works proactively with Congress to make ending domestic violence a national priority.\n\nNational Network to End Domestic Violence's members are state and territorial coalitions representing domestic violence shelters and programs in every state and territory in the nation. They work closely with the state and territorial coalitions to understand the ongoing and emerging needs at the local and state level, and then ensure those needs are heard and understood by policymakers at the national level.\n\nEnsuring the funding of domestic violence programs remains a continued concern. National Network to End Domestic Violence's policy efforts also focus on gun violence, the technology sector, and economic and financial security.\n\nNational Network to End Domestic Violence worked for the 2013 reauthorization of the Violence Against Women Act. Violence Against Women Act closed critical gaps in funding, ensuring all domestic violence survivors receive services.\n\nNational Network to End Domestic Violence participated in the Office on Violence Against Women conferral process, sharing information about Violence Against Women's Act's impact on the field, gaps in the federal response, and challenges and successes of implementation.\n\nNational Network to End Domestic Violence also participated in stakeholder meetings about the implementation of Violence Against Women Act's Campus SaVE provisions and worked to ensure that the negotiated rule-making committee considered, in particular, the needs of student victims of dating abuse and stalking, as well as the confidentiality and safety needs of student victims. National Network to End Domestic Violence also communicated the same messages to the White House Task Force on Campus Sexual Assault.\n\nHistorically, Violence Against Women Act was the first piece of federal legislation to specifically provide protections for members of the LGBT community.\n\nNational Network to End Domestic Violence continues to work to ensure that programs such as the Family Violence Prevention and Services Act and Victims of Crime Act receive adequate funding.\n\nNational Network to End Domestic Violence leads the national Campaign for Funding to End Domestic and Sexual Violence. Through this work, National Network to End Domestic Violence is at the forefront of advocating for increased funding and resources for local programs and state coalitions.\n\nAs part of this advocacy, National Network to End Domestic Violence coordinates and implements a strategic action plan that combines national level work with grasstops and grassroots mobilization around the federal budget. The primary focus of appropriations advocacy is on increasing funding under the Violence Against Women Act, the Family Violence Prevention Services Act, and the Victims of Crime Act.\n\nNational Network to End Domestic Violence has emerged as one of the leading organizations for commonsense firearms legislation, specifically, an improved and expanded background checks system. The clear connection to this work—preventing domestic violence homicides—has helped to build momentum around such legislation.\n\nNational Network to End Domestic Violence advocates for closing loopholes in the background checks system and for needed improvements to data collection through NICS. National Network to End Domestic Violence continues to work with Everytown for Gun Safety (formerly Mayors Against Illegal Guns), as well as with other national organizations, to provide critical information and targeted action alerts to the field around proposed legislation to address gun violence.\n\nNational Network to End Domestic Violence hosted a webinar with coalitions on gun violence and conducted a thorough review of state-level domestic violence homicide data, as well as information about lethality assessment programs and fatality review teams in the states. National Network to End Domestic Violence won a significant victory on this issue under the United States Supreme Court decision in U.S. v. Castleman, where the Court upheld a definition under the law that will continue to prohibit convicted domestic violence abusers from possessing firearms. National Network to End Domestic Violence's amicus curae brief in the case, which outlined the importance of upholding this protection through a common-sense interpretation of the law, was referenced by the Supreme Court.\n\nIn February 2013, National Network to End Domestic Violence signed onto an amicus curae brief filed in the case Commonwealth v. Claybrook. This case involved three men who sexually assaulted a college freshman in her dorm room. A Pennsylvania jury convicted the men, and the trial court denied their motions for judgment of acquittal and/or a new trial on the charges. The Superior Court overturned the convictions.\n\nThe advocates' amicus brief argued that the Superior Court's decision was based on misconceptions and myths about sexual assault, including the victim's supposed insufficient resistance, which was long ago removed as a requirement in Pennsylvania. The Pennsylvania Supreme Court reversed the Superior Court and remanded the case.\n\nThe case of Souratgar v. Fair involved the question of whether to remove a child from the custody of his mother and return the child to the custody of his father in Singapore, in a circumstance where the father had been physically and verbally abusive toward the mother, including in the presence of the child. The district court held that the child was not in \"grave risk of harm\" in living with his father.\n\nThe advocates amicus brief argued that this conclusion runs counter to the weight of evidence that children exposed to domestic violence are themselves at serious risk of harm and that both mother and child were continuously abused post-separation from the abuser and that this abuse must be taken seriously.\n\nThe case of Cromartie v. RCM involved a woman who was fired for allegedly allowing her abusive partner to enter the work site in violation of workplace rules. The case asked whether or not this firing constitutes a separation from employment \"due to domestic violence\" subject to unemployment compensation.\n\nThis is the first case requiring interpretation of D.C. Code Section 51-131 in the District of Columbia; it is also the first case in the country on this subject to reach a court of appeals.\n\nThe advocates amicus brief argues that her allowing her partner onto the worksite was not truly voluntary but was rather a product of the pattern of coercion and power exerted by her abuser and characteristic of domestic violence.\n\nConducted since 2006, the National Network to End Domestic Violence Annual Census is an annual noninvasive, unduplicated count of adults and children who seek services from United States domestic violence shelter programs during a single 24-hour survey period. This Census takes into account the dangerous nature of domestic violence by using a survey designed to protect the confidentiality and safety of victims. It also allows for a true representation of the gaps in services, and the strain it causes upon US domestic violence shelters.\n\nThe tenth annual census report conducted on September 16, 2015, had a 93% participation rate among identified local domestic violence in the United States and territories. T\n\nhe survey reported that during a 24-hour period on the census day, 71,828 victims were served. Among those victims, 40,302 victims found refuge in emergency shelters or transitional housing, and 31,526 adults and children received non-residential assistance and services, including counseling, legal advocacy, and children’s support groups.\n\nThe survey also reported that there were 12,197 unmet requests for services in one day, with 63% of the unmet need under the housing umbrella.\n\nThe Coalition Capacity Project offers technical assistance for coalitions of groups working with survivors, such as organizational and leadership development. An annual roundtable brings coalition leaders together.\n\nFounded by Cindy Southworth in 2000 and brought to National Network to End Domestic Violence in 2002, the Safety Net Project provides training to help community agencies and programs respond to the needs of survivors. The project has trained more than 78,000 advocates, police officers, prosecutors, and other community agency members. The Safety Net Project sits on the advisory boards of Pinterest, Twitter, and Facebook. It has launched a sister SafetyNet in Australia.\n\nDeveloped in 2010, with a grant from the MAC AIDS Foundation, Positively Safe addresses the intersection between HIV/AIDS and domestic violence. Together with the National Domestic Violence & HIV/AIDS Advisory Committee, National Network to End Domestic Violence developed a curriculum to train service providers in both fields. The curriculum has a large focus on building collaboration to address the intersection and prevent HIV and domestic violence.\n\nIn 2015 the Positively Safe project launched a toolkit for domestic violence and HIV/AIDS service providers, with resources on safety planning, linkage to retention in care, conversation guides, and more. The project will continue to expand the resources in the toolkit.\n\nIn 2013, National Network to End Domestic Violence was able to present its curriculum to the President’s Working Group on the Intersection of HIV/AIDS, Violence against Women and Girls, and Gender-Related Health Disparities. Because of National Network to End Domestic Violence's ongoing commitment to these issues, the project was funded to provide training to select groups on the intersection.\n\nWomensLaw was founded in February 2000 by a group of lawyers, teachers, advocates and web designers interested in using the Internet to educate survivors of domestic violence about their legal rights and ways to get help. WomensLaw has two components, namely a website and an email hotline. It joined NNEDV in 2010. The website provides legal information related to domestic violence. The email hotline is a service through which victims, friends, family, and advocates can ask questions anonymously.\n\nOn February 8, 2006, National Network to End Domestic Violence's staff member Cindy Southworth testified before the Senate Consumer Affairs, Product Safety, and Insurance subcommittee. The hearing focused upon pretexting and phone records. \n\nSouthworth's testimony focused upon the necessity of keeping domestic violence survivor's information confidential. Southworth said, \"All companies that collect and retain personal information about their customers should enhance the security and privacy options available to consumers, and create levels of security that are not easily breached from within or from outside of the company. Given the creative and persistent tactics of perpetrators, companies must work with consumers to identify the methods of security that will work best for general consumers, as well as methods for consumers in higher-risk situations, including victims of domestic violence and law enforcement officers.\" \n\nOn June 4, 2014, Southworth, representing both National Network to End Domestic Violence and the Minnesota Council for Battered Women, testified before the Senate Subcommittee on Privacy, Technology, and the Law. Southworth testified in support of Minnesota Senator Al Franken's proposed legislation, The Location Privacy Protection Act of 2014. The bill sought to ban these stalking apps and would require companies to inform consumers when their information is being used and what for what purpose. \n\nSouthworth testified that these apps are not covert in their marketing. Calling attention to apps such as HelloSpy, Southworth showed screenshots from the website showing women in various domestic violence situations. In one picture, under the section advertising the importance of catching cheating spouses, a man stands holding a woman's arm tightly while her face shows clear abrasions. This is a pattern among the majority of these alleged spy applications. Southworth testified that \"consent is critical...and a reminder that the user's location is being tracked is critical.\" Southworth also provided additional recommendations on behalf of the organization and stood in firm support of the legislation.\n\nKim Gandy has served as the president and chief executive officer of the organization since 2012. Gandy had previously served as vice president of the Feminist Majority Foundation in Virginia.\n\nPast presidents of National Network to End Domestic Violence include Congresswoman Donna Edwards, Sue Else (current CEO of Girl Scouts of Historic Georgia), and former White House Advisor on Violence Against Women Lynn Rosenthal.\n\nCindy Southworth serves as Executive Vice President.\n\nIn 1990, the organization was founded as the Domestic Violence Coalition on Public Policy by a group of domestic violence coalitions in order to promote federal legislation related to domestic violence. Along with local, state, and national domestic violence coalitions, National Network to End Domestic Violence led the efforts to pass the landmark Violence Against Women Act, authored by then-Senator Joe Biden.\n\nOn February 25, 1995, after the passage of the Violence Against Women Act, the organization changed its name to National Network to End Domestic Violence. The Violence Against Women Act's implementation led to National Network to End Domestic Violence's work on transitional housing.\n\n"}
{"id": "48551445", "url": "https://en.wikipedia.org/wiki?curid=48551445", "title": "Our Health Partnership", "text": "Our Health Partnership\n\nOur Health Partnership is a large provider of primary care services in Birmingham and Shropshire established in November 2015.\nOur Health Partnership is one of the UK’s biggest GP partnerships. It brings together 47 surgeries in the Midlands and Shropshire. By using our shared expertise to tackle the challenges of GP practice today, we can keep local surgeries thriving and provide the excellent care that our patients need now and into the future.It is described as a super-partnership which will generate efficiencies of scale when compared with traditional General Practice in the UK. It claims that the constituent practices will retain their own operational autonomy.\n\nThe board is made up of nine elected GP partners 7 from Birminghan and 2 from Shropshire, an appointed Practice Manager and an Operations Director and Finance Director. It is also supported by an external strategic advisor. The Board is responsible for the central functions, including accounting, HR, finance and Quality (including CQC). \n\nThe Chair is Dr Vish Ratnasuriya MBE, a GP partner at Lordswood House Medical Practice in Harborne. He has a special interest in Sports and Exercise Medicine, and was team doctor for Solihull Moors football club for 10 seasons. He was clinical contracting lead for the Royal Orthopaedic Hospital and musculoskeletal medicine across Birmingham for CrossCity CCG for over 5 years. \n\n"}
{"id": "12570028", "url": "https://en.wikipedia.org/wiki?curid=12570028", "title": "Patton State Hospital", "text": "Patton State Hospital\n\nPatton State Hospital is a forensic psychiatric hospital in San Bernardino, California, United States. Though the hospital has a Patton, California address, it lies entirely within the San Bernardino city limits. Operated by the California Department of State Hospitals , Patton State Hospital is a forensic hospital with a licensed bed capacity of 1287 for people who have been committed by the judicial system for treatment.\n\nEstablished in 1890 and opened in 1893 as the Southern California State Asylum for the Insane and Inebriates, it was renamed Patton State Hospital after Harry Patton, a member of the first Board of Managers, in 1927. The hospital's original structure was built in accordance with the Kirkbride Plan. The original buildings were demolished after they were badly damaged in the earthquake of 1923.\n\nThe hospital is accredited by the Joint Commission on Accreditation of HealthCare Corganizations (JCAHO) \n\nFrom its opening until 1934, some 2,024 patients died and were buried on the hospital grounds. A memorial for them was erected and in 2011-2012 efforts were under way to identify all the deceased.\n\n\n"}
{"id": "31564169", "url": "https://en.wikipedia.org/wiki?curid=31564169", "title": "Philosophy of happiness", "text": "Philosophy of happiness\n\nThe philosophy of happiness is the philosophical concern with the existence, nature, and attainment of happiness. Philosophers believe, happiness can be understood as the moral goal of life or as an aspect of chance; indeed, in most European languages the term happiness is synonymous with luck. Thus, philosophers usually explicate on happiness as either a state of mind, or a life that goes well for the person leading it.\n\nPlato (c. 428 c. 347 BCE), using Socrates (c. 470 399 BCE) as the main character in his philosophical dialogues, outlined the requirements for happiness in \"The Republic\".\n\nIn \"The Republic\", Plato asserts that those who are moral are the only ones who may be truly happy. Thus, one must understand the cardinal virtues, particularly justice. Through the thought experiment of the Ring of Gyges, Plato comes to the conclusion that one who abuses power enslaves himself to his appetites, while the man who chooses not to remains rationally in control of himself, and therefore is happy.\n\nHe also sees a type of happiness stemming from social justice through fulfilling one's social function; since this duty forms happiness, other typically seen sources of happiness such as leisure, wealth, and pleasure are deemed lesser, if not completely false, forms of happiness.\n\nAristotle (384 – 322 BCE) held that \"eudaimonia\" (Greek: ) is the goal of human thought and action. Eudaimonia is usually translated as happiness, but \"human flourishing\" may be a more accurate translation. Eudaimonia involves activity, exhibiting virtue (\"arete\", Greek: ἀρετή) in accordance with virtue.\n\nWithin the \"Nicomachean Ethics\", Aristotle points to the fact that many aims are really only intermediate aims, and are desired only because they make the achievement of higher aims possible. Therefore, things such as wealth, intelligence, and courage are valued only in relation to other things, while eudaimonia is the only thing valuable in isolation.\n\nAristotle regarded virtue as necessary for a person to be happy and held that without virtue the most that may be attained is contentment. Aristotle has been criticized for failing to show that virtue is necessary in the way he claims it to be, and he does not address this moral skepticism.\n\nAntisthenes (c. 445 – c. 365 BCE), often regarded as the founder of Cynicism, advocated an ascetic life lived in accordance with virtue. Xenophon testifies that Antisthenes had praised the joy that sprang \"from out of one's soul,\" and Diogenes Laertius relates that Antisthenes was fond of saying: \"I would rather go mad than feel pleasure.\" He maintained that virtue was sufficient in itself to ensure happiness, only needing the strength of a Socrates.\n\nHe, along with all following Cynics, rejected any conventional notions of happiness involving money, power, and fame, to lead entirely virtuous, and thus happy, lives. Thus, happiness can be gained through rigorous training (askesis, Greek: ) and by living in a way which was natural for humans, rejecting all conventional desires, preferring a simple life free from all possessions.\n\nDiogenes of Sinope (c. 412 – c. 323 BCE) is most frequently seen as the perfect embodiment of the philosophy. The Stoics themselves saw him as one of the few, if not only, who have had achieved the state of sage.\n\nStoicism was a school of philosophy established by Zeno of Citium (c. 334 – c. 262 BCE). While Zeno was syncretic in thought, his primary influence were the Cynics, with Crates of Thebes (c. 365 – c. 285 BCE) as his mentor.\n\nStoics believe that \"virtue is sufficient for happiness\". One who has attained this sense of virtue would become a sage. In the words of Epictetus, this sage would be \"sick and yet happy, in peril and yet happy, dying and yet happy, in exile and happy, in disgrace and happy,\"\n\nThe Stoics therefore spent their time trying to attain virtue. This would only be achieved if one was to dedicate their life studying Stoic logic, Stoic physics, and Stoic ethics.\n\nThe Cyrenaics were a school of philosophy established by Aristippus of Cyrene (c. 435 – c. 356 BCE). The school asserted that the only good is positive pleasure, and pain is the only evil. They posit that all feeling is momentary so all past and future pleasure have no real existence for an individual, and that among present pleasures there is no distinction of kind. Claudius Aelianus, in his \"Historical Miscellany\", writes about Aristippus:\n\nSome immediate pleasures can create more than their equivalent of pain. The wise person should be in control of pleasures rather than be enslaved to them, otherwise pain will result, and this requires judgement to evaluate the different pleasures of life.\n\nEpicureanism was founded by Epicurus (c. 341 – c. 270 BCE). The goal of his philosophy was to attain a state of tranquility (\"ataraxia\", Greek: ) and freedom from fear, as well as absence of bodily pain (\"aponia\", Greek: ). Toward these ends, Epicurus recommended an ascetic lifestyle, noble friendship, and the avoidance of politics.\n\nOne aid to achieving happiness is the \"tetrapharmakos\" or the four-fold cure:\n\n\"Do not fear god,<br>\nDo not worry about death;<br>\nWhat is good is easy to get, and<br>\nWhat is terrible is easy to endure.\"<br>\n(\"Philodemus, Herculaneum Papyrus, 1005, 4.9–14\").\n\nThe School of the Sextii was founded by Quintus Sextius the Elder (fl. 50 BCE). It characterized itself mainly as a philosophical-medical school, blending Pythagorean, Platonic, Cynic, and Stoic elements together. They argued that to achieve happiness, one ought to be vegetarian, have nightly examinations of conscience, and avoid both consumerism and politics, and believe that an elusive incorporeal power pervades the body.\n\nSt. Augustine of Hippo (354 – 430 AD) was an early Christian theologian and philosopher whose writings influenced the development of Western Christianity and Western philosophy.\n\nFor St. Augustine, all human actions revolve around love, and the primary problem humans face is the misplacing of love. Only in God can one find happiness, as He is source of happiness. Since humanity was brought forth from God, but has since fallen, one's soul dimly remembers the happiness from when one was with God. Thus, if one orients themselves toward the love of God, all other loves will become properly ordered. In this manner, St. Augustine follows the Neoplatonic tradition in asserting that happiness lays in the contemplation of the purely intelligible realm.\n\nSt. Augustine deals with the concept of happiness directly in his treatises \"De beata vita\" and \"Contra Academicos\".\n\nBoethius (c. 480–524 AD) was a philosopher, most famous for writing \"The Consolation of Philosophy\". The work has been described as having had the single most important influence on the Christianity of the Middle Ages and early Renaissance and as the last great work of the Classical Period. The book describes many themes, but among them he discusses how happiness can be attainable despite changing fortune, while considering the nature of happiness and God.\n\nHe posits that happiness is acquired by attaining the perfect good, and that perfect good is God. He then concludes that as God ruled the universe through Love, prayer to God and the application of Love would lead to true happiness.\n\nAvicenna (c. 980–1037), also known as 'Ibn-Sina', was polymath and jurist; he is regarded as one of the most significant thinkers in the Islamic Golden Age. According to him, happiness is the aim of humans, and that real happiness is pure and free from worldly interest. Ultimately, happiness is reached through the conjunction of the human intellect with the separate active intellect.\n\nAl-Ghazali (c. 1058–1111) was a Muslim theologian, jurist, philosopher, and mystic of Persian descent. Produced near the end of his life, al-Ghazali wrote \"The Alchemy of Happiness\" (\"Kimiya-yi Sa'ādat\", (). In the work, he emphasizes the importance of observing the ritual requirements of Islam, the actions that would lead to salvation, and the avoidance of sin. Only by exercising the human faculty of reason - a God-given ability - can one transform the soul from worldliness to complete devotion to God, the ultimate happiness.\n\nAccording to Al-Ghazali, there are four main constituents of happiness: self-knowledge, knowledge of God, knowledge of this world as it really is, and the knowledge of the next world as it really is.\n\nMaimonides (c. 1135-1204) was a Jewish philosopher and astronomer, who became one of the most prolific and influential Torah scholars and physicians. He writes that happiness is ultimately and essentially intellectual.\n\nSt. Thomas Aquinas (1225 – 1274 AD) was a philosopher and theologian, who became a Doctor of the Church in 1323. His system syncretized Aristotelianism and Catholic theology within his \"Summa Theologica\". The first part of the second part is divided into 114 articles, the first five deal explicitly with the happiness of humans. He states that happiness is achieved by cultivating several intellectual and moral virtues, which enable us to understand the nature of happiness and motivate us to seek it in a reliable and consistent way. Yet, one will be unable to find the greatest happiness in this life, because final happiness consists in a supernatural union with God. As such, man’s happiness does not consist of wealth, status, pleasure, or in any created good at all. Most goods do not have a necessary connection to happiness, since the ultimate object of man’s will, can only be found in God, who is the source of all good.\n\nMichel de Montaigne (1533-1592) was a French philosopher. Influenced by Aristotelianism and Christianity, alongside the conviction of the separation of public and private spheres of life, Montaigne writes that happiness is a subjective state of mind and that satisfaction differs from person to person. He continues by acknowledging that one must be allowed a private sphere of life to realize those particular attempts of happiness without the interference of society.\n\nJeremy Bentham (1748-1832) was a British philosopher, jurist, and social reformer. He is regarded as the founder of modern utilitarianism.\n\nHis particular brand of utilitarianism indicated that the most moral action is that which causes the highest amount of utility, where defined utility as the aggregate pleasure after deducting suffering of all involved in any action. Happiness, therefore, is the experience of pleasure and the lack of pain. Actions which do not promote the greatest happiness is morally wrong - such as ascetic sacrifice. This manner of thinking permits the possibility of a calculator to measure happiness and moral value.\n\nArthur Schopenhauer (1788-1860) was a German philosopher. His philosophy express that egotistical acts are those that are guided by self-interest, desire for pleasure or happiness, whereas only compassion can be a moral act.\n\nSchopenhauer explains happiness in terms of a wish that is satisfied, which in turn gives rise to a new wish. And the absence of satisfaction is suffering, which results in an empty longing. He also links happiness with the movement of time, as we feel happy when time moves faster and feel sad when time slows down.\n\nWładysław Tatarkiewicz (1886-1980) was a Polish philosopher, historian of philosophy, historian of art, esthetician, and ethicist.\n\nFor Tatarkiewicz, happiness is a fundamental ethical category.\n\nHerbert Marcuse (1898–1979) was a German-American philosopher, sociologist, and political theorist, associated with the Frankfurt School of critical theory.\n\nIn his 1937 essay 'The Affirmative Character of Culture,' he suggests culture develops tension within the structure of society, and in that tension can challenge the current social order. If it separates itself from the everyday world, the demand for happiness will cease to be external, and begin to become an object of spiritual contemplation.\n\nIn the \"One-Dimensional Man\", his criticism of consumerism suggests that the current system is one that claims to be democratic, but is authoritarian in character, as only a few individuals dictate the perceptions of freedom by only allowing certain choices of happiness to be available for purchase. He further suggests that the conception that 'happiness can be bought' is one that is psychologically damaging.\n\nViktor Frankl (1905-1997) was an Austrian neurologist, psychiatrist, Holocaust survivor and founder of logotherapy. His philosophy revolved around the emphasis on meaning, the value of suffering, and responsibility to something greater than the self; only if one encounters those questions can one be happy.\n\nRobert Nozick (1938-2002) was an American philosopher.\n\nIn his 1974 book, \"Anarchy, State, Utopia\", he proposed a thought experiment where one is given the option to enter a machine that would give the maximum amount of unending hedonistic pleasure for the entirety of one's life.\n\nScientism is the approach that the empirical sciences are the most valuable branches of learning and culture.\n\nHappiness economics is the quantitative and theoretical study of happiness, positive and negative affect, well-being, quality of life, life satisfaction and related concepts, typically combining economics with other fields such as psychology and sociology. The tracking of Gross National Happiness or the satisfaction of life grow increasingly popular as the economics of happiness challenges traditional economic aims.\n\nRichard Layard has been very influential in this area. He has shown that mental illness is the main cause of unhappiness.\n\nSonja Lyubomirsky asserted in her 2007 book, \"The How of Happiness\", that happiness is 50 percent genetically determined (based on twin studies), 10 percent circumstancial, and 40 percent subject to self-control. Lyubomirsky suggests a twelve-point program to maximize the final 40 percent.\n\nNot all cultures seek to maximise happiness, and some cultures are averse to happiness.\n\n\n\n"}
{"id": "51388795", "url": "https://en.wikipedia.org/wiki?curid=51388795", "title": "Phycotechnology", "text": "Phycotechnology\n\nPhycotechnology refers to the technological applications of algae, both micro- and macroalgae. Algae is extremely useful in various fields. An example for natural phycotechnology is the converting of atmospheric nitrogen into bioaccessible nitrogenous compounds by diazotrophic cyanobacteria (blue-green algae). Species of cyanobacteria like \"Nostoc\", \"Arthrospira\" (Spirulina) and \"Aphanizomenon\" are used as food and feed due to their easy digestibility and nutrient content. Species of \"Dunaliella\" provide products like glycerol, carotenoids, and proteins. Algal-produced proteins can be biofactories for the production of therapeutic substances. Algae is also being used to assist in the remediation of pollution, to create bio-fuels, and as a bio-insecticide.\n\nCurrently micro-algae are being exploited for environmental protection as the species of Chlorella, \"Chlamydomonas\", and \"Scenedesmus\" carry out selective uptake, accumulation and biodegradation of pollutants and thus help in remediation. They are used in biological reclamation of sewage since they can immobilize heavy metals from aquatic systems. Microalgae can be used as biocontrol agents like 'Insect' a commercial bio-insecticide sold in USA, prepared from the dead biomass of diatom frustules. Microalgae are of significant use in healthcare. Chlorellin from the green microalga \"Chlorella\" is an effective antibiotic against Gram positive and Gram-negative bacteria. Algae are an excellent feed stock for green fuel as they are used for the production of biodiesel, bioethanol, biogasoline, biomethanol, biobutanol, and recently biohydrogen. The full genome sequences of many species of cyanobacteria and eukaryotic algae have been used for evolutionary studies and the identification and comparison of the genes coding for specific proteins. Collections of cDNAs and ESTs also aid in genomic research by providing fast and inexpensive ways to discover new genes and their functions and map their positions on chromosomes. Many species of diatoms are used for synthesizing nanoparticles and are being explored for their use as drug delivery systems.\n"}
{"id": "3444688", "url": "https://en.wikipedia.org/wiki?curid=3444688", "title": "Pollution prevention", "text": "Pollution prevention\n\nPollution prevention (P2) is a strategy for reducing the amount of waste created and released into the environment, particularly by industrial facilities, agriculture, or consumers. Many large corporations view P2 as a method of improving the efficiency and profitability of production processes by waste reduction and technology advancements. Legislative bodies have enacted P2 measures, such as the Pollution Prevention Act of 1990 and the Clean Air Act Amendments of 1990 by the United States Congress.\n\nPollution prevention is any action (large or small) that reduces the amount of contaminants released into the environment. By implementing P2 processes, fewer hazards will be posed to both public health and natural well being. Thus, P2 is a key method to preserving natural resources. It can also have significant financial benefits in large scale processes. In the long run, by implementing suggested P2 processes, large corporations may realize great benefits. If companies produce less waste, they do not have to worry about proper disposal. Thus, P2 is also a proactive measure to reduce costs of waste disposal and elimination.\n\nShipping ports are a significant source of pollution due to the heavy cargo traffic that these areas receive. The impact of these ships is quite widespread, affecting coastal communities and ecosystems across the globe. Most major shipping ports are located near environmentally sensitive estuaries. These areas are particularly impacted by high levels of diesel exhaust, particulate matter, nitrogen oxides, ozone, and sulfur oxides. The solution for reducing port-related pollution is multi-fold, encompassing attainable alternatives and long-term reduction goals. Examples of simple steps include a restriction on engine idling for ships in the port and the use of cleaner grade diesel fuels. Some more expensive measures can be taken to mitigate the pollution of ships. Replacing older model ships with ships containing new engines allows the ships to meet modern emission standards. Exhaust systems can also be retrofitted in order to reduce exhaust emissions. Looking ahead into the future, there are a few technologies being developed. For example, plugging ships into “shore-side” power sources may eliminate the need for idling engines. Additionally, various sources of alternative fuel are being developed, the most significant of which is a fuel cell unit.\n\nDue to increased trade, the emissions from ships are expected to become the second largest source of diesel particulate matter by 2020. One approach to reduction as set forth by the International Forum on Globalization (IFG) is to increase the amount of local trading, thereby reducing the number of miles that ships have to travel. Another approach regards the strategic placement of ports close to land transportation infrastructure (i.e. roads and railroads). Again, this reduces the number of miles that vehicles have to travel between the initial and final destinations. Railroads that reach all the way to ports are a significant way to produce less toxic pollutants, as this eliminates the need for less-efficient trucks to transport the goods from the coastal port to the inland railroad infrastructure.\n\nThere are many health hazards associated with pollution that P2 strategies aim to mitigate. Long-term exposure to certain pollutants can cause cancer, heart disease, asthma, birth defects, and premature death. Additionally, excessive pollution to water sources can be detrimental to the biodiversity present in different areas around the globe.\n\nIn order to enforce some of the notions of P2, the United States Congress passed the Pollution Prevention Act of 1990. Congress declared that pollution should be prevented and reduced wherever possible; in addition, any waste that must be released into the environment must be done in a responsible, environmentally-conscious manner. The law requires the United States Environmental Protection Agency (EPA) to:\n\nIn order to enforce the points outlined in the Act, EPA is directed to present a report to Congress biennially. Another aspect of the report required that companies fill out a toxic chemical release form allowing EPA to collect information on the levels of pollution released into the environment.\n\nThe Clean Air Act Amendments of 1990 provided many P2 strategies, including governmental intervention, research and development programs, guidelines for efficient technologies, reduction of vehicle emissions, and a suggested Congressional status report.\n\nThe EPA \"2010-2014 Pollution Prevention Program Strategic Plan\" introduced a number of ways to reduce harmful industrial outputs (i.e. greenhouse gases, hazardous materials) while conserving natural resources.\n\nAs an environmental management strategy, P2 shares many attributes with cleaner production, a term used more commonly outside the United States. Pollution prevention encompasses more specialized sub-disciplines including green chemistry and green design (also known as environmentally conscious design).\n\nThe possibilities of P2 strategies are still being implemented at the corporate level, but benefits are already being realized by many companies. Recently, the view of P2 in industrial businesses has shifted from one of necessity to one of strategic advantage. If companies invest in P2 methods early in their development, they realize greater gains not too far down the road. Additionally, if companies do not produce waste, they do not have to worry about properly disposing of it. Thus, P2 is a proactive measure taken to reduce costs in the long run that would have been dedicated to disposal and elimination of waste.\n\nThere are two main ways to reduce waste through P2: waste reduction and technology improvements. Waste reduction at the source implies the same amount of input raw materials with less waste and more output of the product. Technology improvements imply changes to the production process that reduce the amount of output waste, such as an improved recycling process. Companies are moving past simply complying with the minimum environmental requirements, and they are taking a more strategic, forward-thinking stance on tackling the issue. However, the advancements in technology are still limited, implying that P2 techniques are still in the early stages of formation. The most profitable strategy is “in-process recycling.” Though it is not the most efficient form of “reduction at the source,” recycling is very profitable due to its ease of process. By engaging in recycling practices, industries not only cut down on the amount of material discarded as environmentally-hazardous waste, but they also increase profitability by reducing the amount of raw material purchased. The most widespread strategy is “reduction at the source,” which is the idea that byproducts of production can be reduced through efficient and careful use of natural resources. This method reduces the amount of dangerous pollutants present in waste before the waste is released. In turn, this creates a safer environment free of hazardous waste. This idea ties strongly into the benefits to corporations of investing in newer, more efficient technology.\n\nIn order to reduce costs of P2 techniques, many officials are turning to pollution elimination strategies, thereby eliminating any need for end-of-pipe solutions. A task force was created by the EPA in order to directly target reduction strategies. The P2 program task force has 5 main goals:\n\n\nVoluntary approaches to P2 are on the rise. Governmental organization often collaborate with businesses and regulatory agencies to create a structure of guidelines. There are four types of voluntary approach programs: public voluntary programs, negotiated agreements, unilateral commitments, and private agreements. Public voluntary agreements are the least restrictive. Environmental authorities collaborate and create specific guidelines. Companies are then invited to follow these procedures on a strictly voluntary basis. Negotiated agreements are created through collaboration between public authorities and industry authorities. The agreement establishes bargains that are beneficial to the industry. Unilateral commitments are established by industry authorities alone, and the guidelines they set are self-regulated. Private agreements are established between “polluters” and other affected parties. The regulations set forth create a compromise regarding a variety of pollution regulation strategies. The United States mainly follows the end-of-pipe prevention strategy. However, US President Richard Nixon created the Environmental Protection Agency (EPA) in 1970, and one of its principal missions was to regulate pollution. EPA’s implementation of policies is almost entirely voluntary.\n\nThere are a few keys to a successful voluntary approach. First, the program needs a dependable source of funding (from the government, usually). The program also needs a dynamic relationship with the targeted industries. This creates a base of trust between all involved in the agreement. In terms of regulation, the program should be monitored by a reliable source. In order to assure that the program will establish itself long term, there should be visible benefits to the participants and obvious results to the greater community. The long-term establishment of the program also comes from setting attainable goals to measure progress.\n\nEPA has published waste minimization guidelines that comprise 5 major steps:\n\nThis framework mainly benefits smaller facilities.\n\nThe Waste Reduction Algorithm (WRA) is used to quantitatively describe the benefits of different P2 strategies based on each unique facility. The WRA tracks pollutants through the entire production process in order to obtain accurate measurements.\n\nBy maximizing P2 opportunities, some companies choose to redesign their entire industrial process. Managers focus more on what enters and moves through the entire process, instead of only focusing on the output. Overall, the P2 strategies that financially benefit companies are the most likely to be implemented. However, since P2 has only recently been realized as a cost benefit, many corporations have not adopted significant measures to realize the potential gain.\n\nPollution prevention can also be viewed as a form of environmental entrepreneurship, as companies are increasingly concerned with the cost benefits of reducing the amount of waste created. For example, 3M has accrued a savings of over $750 million since 1973 due to their implementation of P2 incentives. If implemented correctly, P2 strategies can result in an increase in yield, a commonly sought after result of industrial processes. Pollution prevention can also reduce the cost of waste removal because of the reduces amount of waste to remove. By preventing the amount of pollution released, companies can avoid some of the liability costs imposed when large amounts of pollution are released and contaminate the land on which the facility is located.\n\nAccording to EPA, there are some everyday steps that can be taken to prevent pollution:\n\n\nAdditional examples of P2 include using energy efficient machinery, developing clean-burning fuel, reducing the amount of chemicals released into water sources, creating a production process that results in a reduced amount of waste, and utilizing water conservation techniques.\n\n\n"}
{"id": "17887915", "url": "https://en.wikipedia.org/wiki?curid=17887915", "title": "Ralph Hazlett Upson", "text": "Ralph Hazlett Upson\n\nRalph Hazlett Upson (June 21, 1888 – August 13, 1968) was a pioneer in the aviation field, holding Airship Pilot's Certificate #7, Balloon Pilot's Certificate #48 and Pilot's License #10290. Upson designed the world's only all-metal stressed-skin airship and contributed to aerospace technology research.\nHe won the 1913 International Balloon Race (with Ralph Preston). He was the second individual to win the Wright Brothers Medal in 1929.\n\nHe was born in New York City on June 21, 1888, to Grace Hazlett, a physician, and William Ford Upson, a Wall Street attorney. His brother William Hazlett Upson was a noted author. Upson graduated from the Stevens Institute of Technology in 1910 and began work in the aeronautical office at the Goodyear Tire and Rubber Company in Akron, Ohio. In the 1920s, he worked in the area of manned balloon flight at Goodyear. During World War I and immediately thereafter (1917-1921), Upson worked on the development of the US Navy's B, C, D, E, and F type airships and the US Army's AC and TC types. In 1920, he left Goodyear to work on development of a metalclad airship, becoming president (and chief engineer) of the Aircraft Development Corporation. The US Navy awarded a contract for one metalclad airship in 1928, and the next year Upson's concept of a duraluminum-skinned airship, the ZMC-2, launched from Detroit's Gross Ile Airport on 20 August 1929. The ZMC-2 remained in service until 1941, making 752 flights.\n\nIn the 1930s, Upson did consulting work for the US Navy, the National Advisory Committee for Aeronautics, and taught aerodynamics at the University of Michigan in Ann Arbor. He also designed the Union Pacific Railroad's \"City of Salina\", a streamlined train designed with aerodynamic considerations. During World War 2, Upson was head of the aeronautics department at the Heinz Company, a builder of plywood gliders. He joined the staff of New York University in 1944 and moved in 1946 to the University of Minnesota, teaching aeronautical engineering until 1956. From 1956 to 1964, he was a research specialist at Boeing. Among his projects there was the aborted X-20 Dyna-Soar orbital space glider.\n\nUpson had a heart attack while climbing Glacier Peak and died on August 13, 1968, at the age of 80 at Burien General Hospital in Burien, Washington.\n"}
{"id": "58830921", "url": "https://en.wikipedia.org/wiki?curid=58830921", "title": "Salt crust", "text": "Salt crust\n\nSalt crust is the name given to the method of covering of an ingredient, commonly fish such as sea bass and grouper, in a sand-like mixture of usually egg whites and salt. The salt acts as insulation and helps cook the food in an even and gentle manner. After baking, the golden-brown shell is cracked, resulting in moist and evenly cooked food. The technique is used often with chefs as it simultaneously roasts, steams and seasons. \n\nTraditionally fish is seen most commonly using the salt crust method to be baked but in recent times with salt becoming much cheaper and available than the past, has seen a rise in use in other areas. Usually temperatures of approximately 200º C are used in baking of the salt encrusted fish with the fish being flavoured and seasoned before being covered in the mixture of egg whites and salt. The head and tail are left uncovered and is baked until the crust is golden brown. To serve, the crust is broken and the moistness should be preserved.  Vegetables such as celeriac and beetroot are now being seen being baked in a salt crust as well. For beetroot, the stem and roots are cut off and then wrapped tightly in baking paper then covered in a salt crust completely sealed. Another non-traditional method is seen in the version of a pie with ramekins being used and a salt crust to cover the top. \n\nBeef Tenderloin baked with a salt-crust is a Colombian method for preparing beef tenderloin. The meat is crusted with salt then wrapped in a kitchen towel before being baked on a top of hot coals. Another example is chicken being baked in a salt crust. There are variations on this dish such as instead of using salt the chicken is wrapped in lotus leaves or even clay is used. The underlying principle is the same, the chicken is baked in its own juices, preventing evaporation through the sealing of the crust. For thousands of years, Sicily was supported by its salt lagoons as it was one of its important commodities. Baking fish in a salt crust is a popular method of cooking seafood over southern Italy. Fish native to the region such as bass, bream trout or snapper is traditionally used. A combination of coarse salt for the base and fine salt for the top is used as a crust with no egg being used as a binder. Steak can also be cooked using a similar method rather than broiling it and risk the loss of its juices. Coating a pan with salt and cooking the steak, ideally approximately an inch thick or less, on top yields a more moist and flavourful cut. The pan is heated until the salt crackles and steaks cooked with this method usually involves a crispy crust made of the salt. This is a key difference to other salt crusts as usually it is discarded but in this case it is used to add texture and flavour to the steak.\n\nThe technique of covering food in salt to create a sealed place is used by many cultures: Indian clay-pot cooking, French \"en papillote\" style and Oriental techniques which also emulate a sealed space. Oriental style uses instead of salt, wrapping food in plant matter such as leaves or bark. The techniques all aim to lock in moisture and maximise the effect of the seasoning. Covering the ingredient in salt crust is the first part and the second part is baking it. Baking typically occurs in an oven and the fuel is supplied by wood, coal, gas, or electricity. The crust also acts as a mini-oven, locking the heat inside as salt is a poor conductor of heat creating an oven within an oven. This slows heat transfer to the food creating a slow and low dry oven, beneficial to most proteins. \n\nIt is said that Egyptians were the first civilisation to utilise salt as a preservative in meat and fish. The earliest Chinese records show that the technique of preserving fish in salt goes back to 2000 B.C. whilst salted fish and bird meat was discovered in aboriginal Egyptian graves. The salt-crusted fish has appeared in many different countries such as France, Italy, Spain and China. A huge amount of salt is needed to prepare the dish, and as such even a few hundred years ago it was very expensive, due to salt being a rare commodity in that time period. Earlier than that, the dish would have been reserved only for the wealthy and the prosperous. Nowadays, it can easily be produced at home or in restaurants. By drizzling the crust in alcohol, restaurants burn the crust before being presented to the customer in flames. \n\nThe first recorded reference in China resembling the technique of baking in a salt crust is Salt-Baked Chicken from Dong Jing in the province of Guangdong during the Qing Dynasty (1644-1911). The chicken was cooked and preserved in the salt fields of the area giving them added flavour. A more recent traditionally recipe is building a cocoon of salt around the chicken protecting it from direct heat ensuring even cooking and the possibility of drying out the meat. The chicken is wrapped in a tight parcel concentrating all the natural flavours of the chicken resulting in a succulent and tender product. This can be seen as the combination of the salt crust method with the French \"en papillote\" style. In a Muslim cookbook originating from the thirteenth century, a layer of salt is placed on a new terracotta tile as a base and the fish is placed on top and another layer of salt is added on, then finally being placed in an oven. The Portuguese also have a history of drying and salting fish with their famous dish, Bacalhau, as before refrigeration, food needed to preserve to extend their expiry date. The fish used, cod, even caused battles between countries resulting in an Icelandic victory. Callaloo was created by African slaves under Spanish rule sometime around 1530. Crab was originally used in the dish, but alternatives were found as the supply dwindled by the mid 1500's. Fresh fish was used by plantation owners as fertilizer but were then doused in salt to prevent slaves from eating the produce. Techniques such as boiling until the salt came off, soaking the fish until the salt was removed and even frying the fish in its salt crust were pioneered to tackle the problem.\n\nHowever, the earliest recipe found for salt-baked first comes from the fourth century BCE in Archestratus' \"Life of Luxury\". The recipe details for a whole, round white fish such as sea bass, snapper or sea bream that was cleaned then gutted. The fish is seasoned with thyme being inserted into the cavity of the fish prior to the salt crust encapsulating it in two pounds of salt glued together with water and egg whites. Without earlier documentation, the Ancient Greeks seem to be the first ones to pioneer this technique.\n\nSalt is vital for life and has been valued by many different cultures and even more so for local Thai communities as it has been a source of income over generations. They showcase their salt producing heritage and their pride of it through a festival to commemorate and celebrate their history. The Art of Salt Festival lasts for three days offering attractions of sculptures, educational displays where birds, people and other animals are crafted out of salt. A dish which is popular among the locals is the salt crusted fish, stuffed with lemongrass and the salt crust protects the fish from drying out while its being grilled. \n\nIn ancient times salt was used as part of religious offerings in Egypt and even for preparation of the afterlife. Funeral offerings of salt were given to Egyptian tombs out of consideration for the deceased as well as food such as salted birds and fish to be eaten in the afterlife. Salt was used in combination with natron in the desiccation process. Salt was also used as a currency of trade between the Phoenicians and their surrounding empire of the Mediterranean. Researchers have suggested that even the Mayans salted their fish and meat to meet their dietary needs and to use it as a good which could be traded easily as it would be able to stored for longer times due to the preservative nature of salt. \n\nIn Japan salt is even considered sacred as they believe salt is cleansing and is often a defining flavour of Japanese dishes. It plays a major role in the making of dried fish and in the pickling of vegetables. Turkey is also another important meat which old villages utilised the salt crust to not only to hold in all the juices but also so that the crust could be sealed and marked preventing tampering. Chicken baked in a salt crust is a known traditional dish to the province of Hatay with records showing it was done as early in the Ottoman period. It is regarded that the Chinese applying the technique on the fowl has been transferred and adopted to Anatolia through migrations. \n\nA study conducted by the Sinop University in Turkey was done to determine the difference a salt crust makes on the rainbow trout (\"Oncorhynchus mykiss\"). The rainbow trout was prepared by first being gutted without being scaled with the head and tail being intact. Parsley and bay leaf were placed in succession with cuts of onion and lemon into an incision of the fish’s body, sealed with aluminium foil. This was done to prevent the absorption of salt into the actual flesh of the rainbow trout. The salt crust was made by a combination of rock salt, egg whites and water. The covered fish was baked at 180C for 45 minutes or until golden brown. The fish was allowed to cool down to room temperature and then the outer layer was broken using a knife.\nThe salt content was calculated according to the Mohr Method defined by Dr Deniz Korkmaz, \"where alkaline or alkaline earth chlorides such as sodium chloride (table salt) react with silver nitrate in the presence of a few drops of potassium chromate solution as indicator is a simple, direct and accurate method for chloride determination.\" Other components such as crude protein, lipids and ash were done by \"AOAC: Official Methods of Analysis, 1980\".\n\nThe results show that moisture (70.33% down to 62.45%) and carbohydrate levels (5.56% down to 1.42%) decreased while all the other values increased. Analysis from a statistical viewpoint shows that the salt and carbohydrate values were not found to be significantly different (P>0.05) however the other values were found to be significantly different (P>0.05). Sensory testing was however not done but the flavour was tested by trained professionals who concluded that the baking of the fish in a salt crust was satisfactory in terms of sensory characteristics. Some of the trained panelists added more salt to the dish. Hence it can be deduced that the dish can be readily eaten by people who suffer from various illnesses like hypertension.\n\n"}
{"id": "423063", "url": "https://en.wikipedia.org/wiki?curid=423063", "title": "Season extension", "text": "Season extension\n\nIn agriculture, season extension refers to anything that allows a crop to be grown and harvested beyond its normal outdoor growing season and harvesting window.\n\nFor colder climates, the fully heated and artificially lit greenhouse is the ultimate season extension device, allowing some crops to be grown year-round, through sub-zero winters. An energy-expensive approach. There are many other ways to beat the cold, for earlier spring planting and growing into the fall and winter:\n\n\nSeason extension techniques are most effective when combined with crop varieties selected for the extended growing conditions. Many approaches are used in large-scale agriculture, as well as in small-scale organic farming, and home gardening.\n\nUsing unheated, unlit methods, depending on the crop, up to several weeks of productivity can be added, where shortened period of sunlight and cold weather end the growing season.\n\nSeason extension can apply to other climates, where conditions other than cold and shortened period of sunlight end the growing year (\"e.g.\" a rainy season).\n\nIn its more passive forms with minimal warming, what season extension accomplishes is to allow plants to remain metabolically idle (with very slow growth) but avoid dying. This allows growers to make a conceptual distinction between extending the \"growing\" season and extending the \"harvest\" season. The reality is a spectrum, and the same farm can do both (on a crop-by-crop or field-by-field basis), but nonetheless these two conceptual poles can be distinguished. The former type of extension is what a hothouse does, such as producing tropical fruit in winter by totally conquering the outdoor climate at high expense. In contrast, the latter type of extension can be viewed not as a way to escape the context of seasonality entirely but rather, working within the context of seasonality, to achieve food preservation in a way that uses life itself for preservation instead of using technology applied to dead plant or animal tissue (as mechanical refrigeration, freezing, or canning would). From this viewpoint it can even be compared to ancient norms of meat preservation, which relied on the principle that a good way to avoid meat spoilage is to keep the animal alive until just before consumption. In other words, the best postharvest strategy sometimes can be to confine the postharvest period to hours or days instead of weeks or months (by keeping the plant or animal alive as long as possible—the ultimate form of preservation). In the meat example, in preindustrial contexts livestock were brought to market on the hoof, and sailing ships on months-long voyages would keep turtles in forced semihibernation until each one was slaughtered to make a meal.\n"}
{"id": "42752062", "url": "https://en.wikipedia.org/wiki?curid=42752062", "title": "Sempor Dam", "text": "Sempor Dam\n\nSempor Dam is an embankment dam on the Sempor River in District Gombong, Kebumen, Central Java Province, Republic of Indonesia. In addition to being a tourist attraction Sempor Dam is a source of irrigation water for farmers. It helps irrigate thousands of paddy fields in the delta. It also provides for flood control and has a 1 MW hydroelectric power station at its base. Construction on the dam started in 1967 but in the same year water from flash floods over-topped it, causing the dam to fail on November 29, 1967. The wave of water killed 160 people in three towns, including Magelang. and delivered widespread damage to the area. Construction later restarted on the dam and it was completed in 1978. Its power station was later commissioned in 1980.\n"}
{"id": "8006446", "url": "https://en.wikipedia.org/wiki?curid=8006446", "title": "Shoe-fitting fluoroscope", "text": "Shoe-fitting fluoroscope\n\nShoe-fitting fluoroscopes, also sold under the names X-ray Shoe Fitter, Pedoscope and Foot-o-scope, were X-ray fluoroscope machines installed in shoe stores from the 1920s until about the 1970s in the United States, Canada, United Kingdom, South Africa, Germany and Switzerland. In the UK, they were known as Pedoscopes, after the company based in St. Albans that manufactured them. At the beginning of the 1930s, Bally was the first company to import pedoscopes into Switzerland from the UK. In the second half of the 20th century, growing awareness of radiation hazards and increasingly stringent regulations forced their gradual phasing out.\n\nA shoe-fitting fluoroscope was a metal construction covered in finished wood, approximately high in the shape of short column, with a ledge with an opening where the child (or the adult customer) would then place his or her feet in the opening provided and while remaining in a standing position, look through a viewing porthole at the top of the fluoroscope down at the x-ray view of the feet and shoes. Two other viewing portholes on either side enabled the parent and a sales assistant to observe the child's toes being wiggled to show how much room for the toes there was inside the shoe. The bones of the feet were clearly visible, as was the outline of the shoe, including the stitching around the edges.\n\nThere are multiple claims for the invention of the shoe-fitting fluoroscope. The most likely is Dr. Jacob Lowe who demonstrated a modified medical device at shoe retailer conventions in 1920 in Boston and in 1921 in Milwaukee. Dr. Lowe filed a US patent application in 1919, granted in 1927, and assigned it to the Adrian Company of Milwaukee for $15,000. Syl Adrian claims his brother, Matthew Adrian, invented and built the first machine in Milwaukee; his name is featured in a 1922 ad for an X-ray shoe fitter. Then there is Clarence Karrer, the son of an X-ray equipment distributor claims to have built the first unit in 1924 in Milwaukee, but had his idea stolen and patented by one of his father's employees. In the meantime, the British company Pedoscope filed a British patent application in 1924, granted in 1926, and claimed to have been building these machines since 1920.\n\nThe X-ray Shoe Fitter Corporation of Milwaukee and Pedoscope Company became the largest manufacturers of shoe-fitting fluoroscopes in the world.\n\nThe risk of radiation burns to extremities were known since Wilhelm Röntgen's 1895 experiment, but this was a short-term effect with early warning from erythema. The long-term risks from chronic exposure to radiation began to emerge with Hermann Joseph Muller's 1927 paper showing genetic effects, and the incidence of bone cancer in radium dial painters of the same time period. However, there was not enough data to quantify the level of risk until atomic bomb survivors began to experience the long-term effects of radiation in the late 1940s. The first scientific evaluations of these machines in 1948 immediately sparked concern for radiation protection and electrical safety reasons, and found them ineffective at shoe fitting.\n\nLarge variations in dose were possible depending on the machine design, displacement of the shielding materials, and the time and frequency of use. Radiation surveys showed that American machines delivered an average of 13 roentgen (r) (roughly 0.13 sievert (Sv) of equivalent dose in modern units) to the customer's feet during a typical 20 second viewing, with one capable of delivering 116 r (~1 Sv) in 20 seconds. British Pedoscopes were about ten times less powerful. A customer might try several shoes in a day, or return several times in a year, and radiation dose effects may be cumulative. A dose of 300 r can cause growth disturbance in a child, and 600 r can cause erythema in an adult. Hands and feet are relatively resistant to other forms of radiation damage, such as carcinogenesis.\n\nAlthough most of the dose was directed at the feet, a substantial amount would scatter or leak in all directions. Shielding materials were sometimes displaced to improve image quality, to make the machine lighter, or out of carelessness, and this aggravated the leakage. The resulting whole-body dose may have been hazardous to the salesmen, who were chronically exposed, and to children, who are about twice as radiosensitive as adults. Monitoring of American salespersons found dose rates at pelvis height of up to 95 mr/week, with an average of 7.1 mr/week (up to ~50 mSv/a, avg ~3.7 mSv/an effective dose). A 2007 paper suggested that even higher doses of 0.5 Sv/a were plausible. The most widely accepted model of radiation-induced cancer posits that the incidence of cancers due to ionizing radiation increases linearly with effective (i.e., whole-body) dose at a rate of 5.5% per Sv.\n\nYears or decades may elapse between radiation exposure and a related occurrence of cancer, and no follow-up studies of customers can be performed for lack of records. A 1950 medical article on the machines pointed out though: \"Present evidence indicates that at least some radiation injuries are statistical processes that do not have a threshold. If this evidence is valid, there is no exposure which is absolutely safe and which produces no effect.\" Three shoe salespersons have been identified with rare conditions that might be associated with their chronic occupational exposure: a severe radiation burn requiring amputation in 1950, a case of dermatitis with ulceration in 1957, and a case of basal-cell carcinoma of the sole in 2004.\n\nThere were no applicable regulations when the shoe-fitting fluoroscopes were first invented. An estimated 10,000 machines were sold in the US, 3,000 in the UK, 1,500 in Switzerland, and 1,000 in Canada before authorities began discouraging their use. As understanding grew of the long-term health effects of radiation, a variety of bodies began speaking out and regulating the machines.\n\nA shoe-fitting fluoroscope can be seen near the beginning of the film Billion Dollar Brain starring Michael Caine. \n\n\nPatents\n"}
{"id": "18034287", "url": "https://en.wikipedia.org/wiki?curid=18034287", "title": "Sir Seewoosagur Ramgoolam Medical College", "text": "Sir Seewoosagur Ramgoolam Medical College\n\nSir Seewoosagur Ramgoolam Medical College (SSRMC), created in 1999, is the first medical college of Mauritius, established by the Indian Ocean Medical Institute Trust (IOMIT) in Mauritius in the memory of the 'father of the nation', late Sir Seewoosagur Ramgoolam. R.P.N. Singh, an educationist and a social planner from India, is the founder chairman and managing trustee of the trust.\n\nThe college is in Belle Rive, near the city of Curepipe which is 15km from the capital city of Port Louis.\n\nThe courses offered by the medical college are:\n\nIt also offers post graduate courses ( MD/MS) in five disciplines.\nthe pattern of mbbs has syllabus recommended by mci and it has also computer based learning facility as recommended by GMC,UK.\nThe Indian Ocean Medical Institute Trust (IOMIT) was established in 1996 with the A.V.I.D. objective of promotion of education and learning and setting up a medical college in Mauritius in the name of the 'father of the nation' late Sir Seewoosagur Ramgoolam, who was a medical doctor.\n\nThe policy planners of the government of Mauritius including the representatives of the Ministry of Health, Ministry of Education Science & Technology, Ministry of Economic Planning, International Trade and Development, Tertiary Education Commission, University of Mauritius and Chairman of the IOMIT planned the setting up of the SSR Medical College in a task force under the Central Policy Analysis and Evaluation Unit, Prime Minister's Office, government of Mauritius. It was agreed to set up the SSR Medical College affiliated to the University of Mauritius, which would award the graduate and postgraduate degrees.\n\nThe SSR Medical College is spread over about 30 acres of land provided by the government with buildings covering an area of about housing the pre-clinical and the para-clinical departments. The Ministry of Health and Quality of life has promised 'unflinching support' and provides Jawahar Lal Nehru Hospital, Victoria, SSR and other specialty hospitals having more than 2000 beds for the clinical training of the students of the SSR Medical College.\nex students of this college can be found all over the world.\n"}
{"id": "12809618", "url": "https://en.wikipedia.org/wiki?curid=12809618", "title": "Social work with groups", "text": "Social work with groups\n\nSocial work with groups represents a broad domain of direct social work practice (Garvin, Gutierrez & Galinskey, 2004). Social workers work with a variety of groups in all settings in which social work is practiced. While some have proposed that social work practice with groups reflects any and all groups within which social workers participate, other definitional parameters have been established (Garvin et al., 2004). Middleman and Wood (1990) have proposed that for practice to qualify as social work with groups four conditions must be met: the worker should focus attention on helping the group members become a system of mutual aid; the group worker must understand the role of the group process itself as the primary force responsible for individual and collective change; the group worker seeks to enhance group autonomy; the group worker helps the group members experience their groupness upon termination (Middleman & Wood, 1990). Middleman and Wood (1990) observe that social group work meets their criteria of social work with groups. They also point out that\n\"given our definition of work with groups, therapy can be the content and can be included also, contingent upon the way in which the group as a whole and groupness are used\" in accord with the identified criteria (p. 11). As long as the criteria is met, structured group work \"where the worker is the expert until her knowledge has been imparted to the group\" could be regarded as social work with groups as well (Middleman & Wood, 1990, p. 11–12).\n\nA common conceptualization of the small group drawn from the social work literature is as \n\"a social system consisting of two or more persons who stand in status and role relationships with one another and possessing a set of norms or values which regulate the attitudes and behaviors of the individual members in matters of consequence to the group. A group is a statement of relationship among person. Therefore, social systems have structure and some degree of stability, interaction, reciprocity, interdependence and group bond. Open social systems do not exist in a vacuum; they are part of and transact with… their surroundings….\" (Klein, 1972, pp.125-126).\n\nSocial group work and group psychotherapy have primarily developed along parallel paths. Where the roots of contemporary group psychotherapy are often traced to the group education classes of tuberculosis patients conducted by Joseph Pratt in 1906, the exact birth of social group work can not be easily identified (Kaiser, 1958; Schleidlinger, 2000; Wilson, 1976). Social group work approaches are rooted in the group activities of various social agencies that arose in the latter part of the 19th century and the early years of the 20th century. Social upheaval and new found demands as a result of post Civil War industrialization, migration and immigration created many individual and societal needs (Brown, 1991; Kaiser, 1958; Middleman, 1968; Reid, 1991; Schwartz, 1977; Wilson, 1976). Some of these needs were met through group work endeavors found in settlement houses as well as religious and charity organizations (Middleman, 1968; Wilson, 1976). Additionally group work could be found in the progressive education movement (Dewey, 1910), the play and recreation movement (Boyd, 1935), informal education, camping and youth service organizations invested in 'character building' (Alissi, 1980; Schwartz, 1977; Williamson, 1929; Wilson, 1976).\n\nAs Clara Kaiser (1958) has indicated there have been numerous philosophical and theoretical influences on the development of social group work. Chief amongst these influences are the ethics of Judeo-Christian religions; the settlement house movement's charitable and humanitarian efforts; theories eminent in progressive education, especially those of John Dewey (1910); sociological theories about the nature of the relationship between man and society, i.e. Mead (1934); the democratic ethic articulated by early social philosophers; the psychoanalytic theories of Rank and Freud; the practice wisdom, theory building, educational and research efforts of early social group workers (Alissi, 1980; Kaiser, 1958; Wilson, 1976). Early theoretical, research and practice efforts of Grace Coyle (1930, 1935, 1937, 1947, 1948), Wilber Newstetter (1935), and Neva Boyd (1935) paved the way for the advancement and development of social group work.\n\nIn the summer of 1934 Grace Coyle organized a two-week group work institute for forty YWCA and settlement house workers at Fletcher Farm, Vermont.\nGrace Coyle presented an early theoretical framework for social group work articulating the need for a democratic value base (Coyle, 1935), identifying the role of the worker as a group builder (Coyle, 1937) and noting the benefits of 'esprit de corps' or group morale (Coyle, 1930). As the editor of several small group research compendiums Hare (1976) would later point out, \"many of her insights about group process were ahead of her time\" (p. 388).\n\nSocial group work was introduced to the social work profession when it made its debut at the National Conference for Social Work in 1935. At this conference, Newsletter (1935) introduced the concept of social group work to the social work profession and identified group work as a field, process and set of techniques. He described group work as an \"educational process\" concerned with \"the development and social adjustment of an individual through voluntary group association\" and \"the use of this association as a means of furthering other socially desirable ends\" (p. 291).\n\nThe period of time between the 1930s and the 1950s was one of growth and expansion for social group work (Alissi, 1980; Wilson, 1976). The economic despair of and varied psychosocial needs resultant of the Great Depression paved the way for greater affiliation between the social work profession and the field of group work (Alissi, 1980; Konopka, 1983; Wilson, 1976). The psychological needs of returning war veterans who served in World War II resulted in the more frequent application of social group work in psychiatric treatment (Konopka, 1983). During this period of time not only would the field of social group work debut at the National Conference for Social Work but additional advances would be made. Academic courses and research institutions were established; a professional organization was formed, The American Association of Social Work with Groups (AAGW); and a journal, The Group, was established. The first textbooks would appear as well, written by Harleigh Trecker (1948) and Gertrude Wilson and Gladys Ryland (1949).\n\nThe 1950s would usher in even greater affiliation of group work with the profession of social work (Alissi, 1980; Andrews, 2001). The merger of the AAGW with six other organizations to form the National Association of Social Work (NASW) in 1955 solidified the identification and integration of social group work with the social work profession (Alissi, 1980; Andrews, 2001). The impact of the merger was reflected in efforts at definitional shifts regarding group work.\n\nIn 1956 the NASW formed a group work section which issued a new definition that contrasted in focus with that proposed by the AAGW. The new definition dismissed the idea of group work with normal growth and development and instead saw group work as a \n\"service to a group where the primary purpose is to help members improve social adjustment, and the secondary purpose is to help the group achieve objectives approved by society…the definition assumes that the members have adjustment problems\" (Alissi, 1980, p. 24).\n\nThe 1960s and the 1970s saw the expansion of the social welfare state; the Vietnam War; the emergence of the war on poverty; the Woman's Rights Movement; the Black Power Movement; and the Lesbian and Gay Rights Movement (Balgopal & Vassil, 1983; Somers, 1976). The above social, intellectual and cultural factors influenced the social work profession including social group work (Balgopal & Vassil, 1983; Somers, 1976). With such a wide range of social and therapeutic needs there seemed to be an even greater appreciation of group work (Balgopal & Vassil, 1983; Hartford, 1964; Somers, 1976). Having expanded into differing practice settings, the purposes and goals of group work had been more broadly described at this juncture than in previous decades.\n\nGroup work scholars made great strides in developing practice theories. The work of Vinter and Schwartz and their respective associates would dominate the group work scene for much of this decade and the next (Galinsky & Schopler, 1974). In Vinter's approach (1967) the treatment group is thought of as a small social system \"whose influences can be planfully guided to modify client behavior\" (p. 4). In this approach the worker takes a central position in providing treatment, interventions are planned, group process is highly structured, and great emphasis is given to outcome evaluation and research (Vinter, 1967; Garvin, 1987; Galinsky & Schopler, 1974). Schwartz (1961) proposed his vision of the small group as an enterprise in mutual aid.\n\nIn 1965 Bernstein and colleagues introduced another social group work practice theory (Bernstein, 1978; Lowy, 1978; Garland, Kolodney & Jones, 1978). The centerpiece of the edited collection was a developmental stage model, known as the Boston Model, which presented a framework for understanding how groups navigate degrees of emotional closeness over time (Bernstein, 1978; Garland, Kolodney & Jones, 1978). In 1966 Papell and Rothman (1966) presented a typology of social group work that included the social goals model (in the tradition of Coyle), the remedial model (as developed by Vinter) and the reciprocal model (as articulated by Schwartz). In 1968 Middleman (1968) made a seminal contribution in articulating an approach to group work practice that utilized non-verbal activities. In 1976 Roberts and presented a collection of ten group work practice theories (Roberts & , 1976) further illustrating the diversity of approaches to group practice.\n\nAs theory building proliferated there was a simultaneous effort to distill the essential elements of social group work. In 1980 Papell and Rothman wrote, \n\"The process of distilling and identifying the central identity of group work in the contemporary period has already begun\" (p.7.\n\nThe late seventies saw the reemergence of a professional journal, Social Work with Groups in 1978. Additionally, in 1978 social group workers formed a committee to host a symposium in honor of Grace Coyle which paved the way for an annual conference in subsequent years ( & Kurland, 2001). The conference planning committee was transformed into the membership driven organization, The Association for the Advancement of Social Work with Groups now an international organization (AASWG, 2006).\n\nContemporary group work practice continues to be informed by the work of early pioneers and the vanguards of the 1960s and 1970s. In addition to the Mutual Aid Model of social work with groups, the Cognitive-Behavioral Group Work Model is recognized as influential on contemporary group work practice (Rose, 2004). The approach suggested by Rose (1989, 2004) integrates cognitive and behavioral interventions with small group strategies. While primacy is not placed on establishing the group as a mutual aid system in quite the same way as with the Mutual Aid Model, Rose (2004) suggests the worker promote group discussion and member interaction. Furthermore, drawing upon Yalom's Therapeutic Factor construct Rose (2004) points out the benefits of universality, altruism, and group cohesion as well as mutual reinforcement, factors which are conceptually resonant with mutual aid.\n\nIn 1964 the Committee on Practice of the Group Work Section of the National Association of Social Workers proposed that group work was applicable for the following purposes: corrective/treatment; prevention; normal social growth and development; personal enhancement; and citizenship indoctrination (Hartford, 1964). Common needs addressed by social work groups include coping with major life transitions; the need to acquire information or skills; the need to improve social relationships; and the need to cope with illness; and the need to cope with feelings of loss or loneliness; amongst other reasons (Gitterman & Shulman, 2005; & Kurland, 2001).\n\n and Kurland (2001) identify the value system informing group work practice with \"the ultimate value of social work\" which they suggest is \"that human beings have opportunities to realize their potential for living in ways that are both personally satisfying and socially desirable\" (p. 15). Humanistic values guide social work practice with groups, inform worker role and use of self, and the understanding of membership in a social work group.\nHumanistic values \"cast people in society as responsible for and to one another\" (Glassman & Kates, 1990, p. 13). The perspective espoused by several social work group work experts is that not only are people responsible for one another but that mutual interdependence is preferable to individualism (Falck, 1989; Getzel, 1978; Glassman & Kates, 1990; & Kurland, 2001; Schwartz, 1961; Shulman, 2006; Steinberg, 2004).\n\nThe following humanistic values have been highlighted by social work educators, such as Gisela Konopka, as integral to social work practice with groups: 1) \"individuals are of inherent worth\"; 2) \"people are mutually responsible for each other; and 3) \"people have the fundamental right to experience mental health brought about by social and political conditions that support their fulfillment\" (Glassman & Kates, 1990, p. 14).\n\nDemocratic norms of group participation which flow from humanistic values are actively shaped by group workers as they promote cooperation and \"fluid distribution of position, power and resources\" (Glassman & Kates, 1990, p. 14).\n\nOpportunities for mutual aid to be found in the group encounter offer the major rationale for the provision of group services by social workers. Gitterman (2006), a social work educator and group work scholar has elaborated on the role of mutual aid in the small group noting that \"as members become involved with one another, they develop helping relationships and become invested in each other and in participating in the group\" (p. 93). The mutual aid processes that unfold help group members \"to experience their concerns and life issues as universal,\" to \"reduce isolation and stigma,\" to \"offer and receive help from each other,\" and to \"learn from each other's views, suggestions and challenges\" (Gitterman, 2006, p. 93).\n\nNot only do group services offer opportunities for social support as Toseland and Siporin (1986) explain \"there is also an important helper therapy principle that operates in groups\" (p. 172). Toseland and Siporin (1986) elaborate: \"clients are able to help others and in so doing receive help for themselves\" (p. 172).\n\nMutual aid as group work technology can be understood as an exchange of help wherein the group member is both the provider as well as the recipient of help in service of achieving common group and individual goals (Borkman, 1999; Gitterman, 2006; Lieberman, 1983; & Kurland, 2001; Schwartz, 1961; Shulman, 2006, Steinberg, 2004; Toseland & Siporin, 1986). The rationale for cultivating mutual aid in the group encounter is premised on mutual aid's resonance with humanistic values and the following propositions: 1) members have strengths, opinions, perspectives, information, and experiences that can be drawn upon to help others in the group; 2) helping others helps the helper, a concept known as the helper-therapy principle (Reissman, 1965) which has been empirically validated (Roberts et al., 1999); and 3) some types of help, such as confrontation, are better received when emanating from a peer rather than the worker (Shulman, 2006).\n\nMutual aid is often erroneously understood as simply the exchange of support. Mutual aid is better conceptualized as multidimensional with at least 10 types of processes or activities that occur amongst and between members, including: sharing data, the dialectic process, discussion of taboo topics, the all in the same boat phenomenon, developing a universal perspective, mutual support, mutual demand (including confrontation), rehearsal of new skills, individual problem solving, and the strengths in numbers phenomenon (Gitterman, 2004; Shulman, 2006; Steinberg, 2004).\n\nThe Mutual Aid Model of group work practice (Gitterman, 2004) has its roots in the practice theory proposed by William Schwartz (1961) which was introduced in the article, \"The Social Worker in the Group\". Schwartz (1961) envisioned the group as an\n\"enterprise in mutual aid, an alliance of individuals who need each other in varying degrees, to work on certain common problems\" (p.266).\n\"the fact is that this is a helping system in which clients need each other as well as the worker. This need to use each other, to create not one but many helping relationships, is a vital ingredient of the group process and constitutes a need over and above the specific tasks for which the group was formed\" (1961, p. 266).\nWhile referred to as social group work (Papell & Rothman,1966), Schwartz preferred to think of this model as social work with groups (Schwartz, 1976). Schwartz (1976) regarded this approach as resonant with the demands of a variety of group types including, natural and formed; therapeutic and task; open and closed; and voluntary and mandatory. Schwartz (1961, 1964) initially thought of this approach as an organic systems model (as he viewed the group as an organic whole) later to refer to it as the mediating model and then the interactionist model (Schwartz, 1977). The model initially proposed by Schwartz has been further developed most notably by Lawrence Shulman and Alex Gitterman, who have since referred to this model as the Mutual Aid Model (Gitterman, 2004, 2005; Shulman, 1979, 1992, 1999, 2005b).\n\nThe Cognitive-Behavioral Group Work Model is recognized as influential contemporary group work practice approach (Rose, 2004). The approach suggested by Rose (1989, 2004) integrates cognitive and behavioral interventions with small group strategies. While primacy is not placed on establishing the group as a mutual aid system in quite the same way as with the Mutual Aid Model, Rose (2004) suggests the worker promote group discussion and member interaction. Furthermore, drawing upon Yalom's Therapeutic Factor construct Rose (2004) points out the benefits of universality, altruism, and group cohesion as well as mutual reinforcement, factors which are conceptually resonant with mutual aid.\n\nThe involuntary client can be understood as someone who is pressured by some external source to seek social services (Rooney and Chovanec, 2004). Mandated involuntary clients are pressured to seek services as a result of the legal system (Rooney & Chovanec, 2004). Rooney and Chovanec (2004) identify reactance theory as an explanatory framework for the attitude and behaviors of the involuntary client and the mandated involuntary client. Reactance theory suggests that as a person is pressured to relinquish certain behaviors as a result of treatment efforts they experience reactance, \"a motivational drive to restore those free behaviors\" (Rooney & Chovanec, 2004, p. 213). Rooney and Chovanec (2004) suggest an approach that draws upon the Transtheoretical (Stages of Change) Model and Motivational Interviewing in identifying strategies for engaging involuntary clients in the group process. Tom Caplan (2008) suggests the Needs ABC Model.\n\nBehroozi (1992) has noted tensions between the concept of working with mandated clients and professional ethics, such as the belief in fostering self-determination. The chief concern is whether or not \"involuntary applicants\" are in fact \"clients\", as to become a client of a professional social worker requires \"mutual agreement\" (Behroozi, 1992, p. 224). In social work practice, the primary task given this issue is to help the applicant \"transform to clienthood\" (Behroozi, 1992, p. 224). In the absence of this transformation, the mandated \"client\" is likely to be superficially compliant and deny they have any problems warranting social work attention (Behroozi, 1992; Breton, 1993; Milgram & Rubin, 1992).\n\nMost conceptualizations of group development are predicated on the belief that the group is closed, with unchanging membership (Schopler & Galinsky, 1990). The findings of an exploratory study conducted by Schopler and Galinsky (1990) concluded that movement beyond beginnings is possible. However, the impact of open membership is likely to result in a more cyclical pattern of group development with regression occurring when members enter and/or leave the group (Schopler & Galinsky, 1990).\n\nAs a concept, open-endedness exists along a continuum dependent upon the duration of the group (Gitterman, 1989; Schopler and Galinsky, 1995a; Shulman, 2006). When membership is open but the group is of a long duration a core group of members is likely to emerge (Schopler and Galinsky, 1995a; Shulman, 1999, 2006). When this occurs the core group assumes responsibilities for indoctrinating new members (Gitterman, 1989; Schopler & Galinsky, 1995a; Shulman, 1999).\n\n\n"}
{"id": "29273278", "url": "https://en.wikipedia.org/wiki?curid=29273278", "title": "Society of Interventional Radiology", "text": "Society of Interventional Radiology\n\nThe Society of Interventional Radiology (SIR) is an American national organization of physicians, scientists and allied health professionals dedicated to improving public health through the use of minimally invasive, image-guided therapeutic interventions for disease management.\n\nIt was founded in 1973 as the \"Society of Cardiovascular Radiology\" by an active group in the field who wanted to further develop interventional aspects of radiology. It changed its name to the \"Society of Cardiovascular and Interventional Radiology\" in 1983. In April 2002, the name was changed to \"Society of Interventional Radiology\" in order to emphasise the expanding role of interventional radiology that is no longer limited to the cardiovascular system. The society comprises about 7,000 members (March 2017): including practicing physicians, trainees, scientists and clinical associates, such as physician assistants, nurse practitioners, radiologic technologists and paramedical professionals.Katharine L. Krol served as first woman president. \n\n"}
{"id": "8616483", "url": "https://en.wikipedia.org/wiki?curid=8616483", "title": "Sports law in the United States", "text": "Sports law in the United States\n\nSports law in the United States overlaps substantially with labor law, contract law, competition or antitrust law, and tort law. Issues like defamation and privacy rights are also integral aspects of sports law. This area of law was established as a separate and important entity only a few decades ago, coinciding with the rise of player-agents and increased media scrutiny of sports law topics.\n\nMembership is voluntary. The NCAA operates along a series of bylaws that govern the areas of ethical conduct, amateur eligibility, financial aid, recruiting, gender equity, championship events, and academic standards. The NCAA has enforcement power and can introduce a series of punishments up to the death penalty, the company term for the full shut-down of a sporting activity at an offending college. Coaches are offered contracts and if any contractual agreement is violated NCAA has the right to hold any person(s) under the contract liable.\n\nTitle IX is an increasingly important issue in college sports law. The act, passed in 1972, makes it illegal for a federally funded institution to discriminate on the basis of sex or gender. In sports law, the piece of legislation often refers to the effort to achieve equality for women's sports in colleges. The Office of Civil Rights (OCR) is charged with enforcing this legislation. This agency implemented a three-prong test for schools to adhere to:\n\n\nIn 1995 the Gender in Equity Disclosure Act was passed to require schools to make an annual, public report on male-female athletic participation rates, recruiting by gender, and financial support. The U.S. Supreme Court's decision in \"Brown University v. Cohen\", is an important aspect of litigation for women sports.\n\nA critical piece of federal legislation, the Amateur Sports Act of 1978 guarantees certain due process rights including hearings and appeals for U.S. athletes under the governance of the USOC and its NGBs.\n\nIn 1967, the National Labor Relations Board accepted that players have the right to form unions or players' associations. It is now common for professional athletes to organize into associations or unions in order to negotiate collective bargaining agreements' (CBAs) with their sport's owners. Under federal labor law, players and owners must negotiate mandatory issues, those relating to hours, wages, and working conditions, in good faith. All other issues are deemed \"permissive,\" and do not have to be negotiated. Once a CBA is in place, players agree not to strike and owners promise not to lock out players. By way of example, the 2005 National Hockey League season was cancelled because of an owners' lockout after the parties' CBA had expired. In 1994, Major League Baseball lost half its season and the playoffs because ballplayers went on strike over the issue of a salary cap. Historically, the most controversial issues subject to CBA negotiation are free agency, minimum salary, squad size, draft, salary cap, grounds for termination, and suspension.\n\nIn nearly all professional sports, the issue of limits on the use of performance-enhancing drugs has become an integral aspect of CBA negotiations. Drug policies are not uniform for all professional sports. Typically, each CBA explains the policy regarding drug testing, list of banned drugs, violations, penalties, privacy issues, and rights of appeal. Drug violations may lead to suspensions and loss of salary. The BALCO controversy involving high-profile professional athletes and coaches highlights the allegedly widespread use of performance-enhancing drugs in different sports.\n\nPlayer agents, made famous by the famous line from a player to his agent (\"Show me the money!\") in the movie \"Jerry Maguire\", are generally certified by each sport's players' association. Once certified, player agents or contract advisors may negotiate individual player contracts. Agents who are entrusted to conduct business on a player's behalf owe a fiduciary duty, i.e., a duty to remain loyal, act honestly, behave ethically, and act in the player's best interest when negotiating. More than half the states in the United States currently regulate the activities of player agents in addition to union regulation for bad acts. Super agents like baseball's Scott Boras and football's Drew Rosenhaus are frequently the subject of media profiles.\n\nThe first body to assist player agents in learning the ins and outs of contract negotiations, endorsements and media relations was the Association for Representatives of Athletes. The co-founders and leaders of ARPA, since absorbed into the NFL Players Association, were Professor William Weston (University of Baltimore Law School) and Professor Michael E. Jones (University of Massachusetts Lowell). The late Bob Woolf is acknowledged as being one of the first player agents for assisting Boston Red Sox pitcher Earl Wilson to negotiate his player contract.\n\nUntil a few decades ago, most United States professional sports leagues' contracts retained clauses contracts that essentially prevented players from leaving their original teams by their own choice. These \"reserve clauses\" were upheld because courts found that these sports leagues did not operate in interstate trade or commerce, meaning they did not fall under antitrust laws. See \"Federal Baseball Club v. National League\" (1922). This interpretation has largely been eroded today. However, Major League Baseball may still retain limited anti-trust exemptions (it is unclear whether the entire exemption has been overruled by Flood Act because the true extent of the exemption was vague). It is important to note that the formation of players unions for the purpose of negotiating contracts with management is exempt from anti-trust scrutiny under labor law. The by-product of good faith negotiations between management and players unions in the form of a CBA is also exempt from anti-trust scrutiny.\n\nUntil recently, torts were never part of the landscape of sports law. A tort can be defined as an actionable wrong However, in 1975 an Illinois appeals court established that players can be found guilty of negligence if their actions are \"deliberate, willful or with a reckless disregard for the safety of another player so as cause injury to that player.\" See \"Nabozny v. Barnhill\". Negligence torts are typically harder to prove in contact sports, where violent actions and injuries are more common and thus more expected (\"assumption of risk\" or \"self-defense\"). Spectators can also sue for negligence if their injuries could not have been expected (not \"foreseeable\") given the nature of the sporting event they were attending. A baseball fan sitting in the bleachers could reasonably expect a baseball might come toward the seat, but a wrestling fan sitting courtside would not reasonably expect a wrestler to come flying his or her way.\n\nSports' tort law extends into other less obvious areas. Team doctors could be liable for medical malpractice, a form of negligence, for giving a player a false clean bill of health so the player may continue to perform. A player who purposefully causes bodily harm to another athlete, coach, or spectator may be guilty of committing an intentional tort along with a criminal act of assault and battery. The law of defamation protects a person's good character or reputation. The publication of false information about a well-known athlete (\"public figure\") may be actionable if it was published with a reckless disregard for the truth or actual malice. The growth of non-traditional media outlets, e.g. web pages, instant messaging, cable, etc. has added a new dynamic to this area of the law.\n\nClosely related to the subject of torts in some ways, is the area of publicity rights. While the tort of defamation protects a person's reputation, the right of publicity permits a person to commercially exploit his or her likeness, name, and image. This area of sports law includes trademarks, tradenames, domain names, and copyrights.\n\n\nInternational amateur sports are run by a variety of organizations. The International Olympic Committee (IOC) is made up of each country's Olympic Committee, which in turn recognizes a national governing body (NGB) for each Olympic related sport. The United States Olympic Committee (USOC) is the national governing body for all U.S. athletes in the Olympic and Pan-American Games. The IOC is the international governing body for the summer and winter Olympic Games.\n\nLabor issues are not unique to United States law. The European Union has dealt with countless sports-related legal issues. The most important development in this area was the Bosman ruling, in which the European Court of Justice invalidated restrictions imposed by EU member countries and UEFA (the governing body for football within Europe) on foreign EU nationals. Bosman was extended to countries with associate trading relationships with the EU by the Kolpak ruling. The 6+5 rule was a proposed rule by FIFA that sought to limit the effects of Bosman and its offshoots on football clubs; it sparked considerable legal controversy in Europe and was abandoned in 2010.\n\nThe subject of drug testing, especially in international sports like cycling and track and field, is under the jurisdiction of each sport's NGB and international federation, the USOC, the IOC, and the World Anti-Doping Agency. The final arbitrator in resolving drug related disputes is the Court of Arbitration for Sports.\n\nAustralia\n\nThe capacity for the law of assault to intervene in contact sports is limited by the athlete's willing participation. By engaging in a sport, participants are held to accepted the inherent risks of such an activity as applied in \"Rootes v Shelton\"\".\"\n\nHowever, questions of legality arise where the conduct was deliberate as was the case in \"McCracken v Melbourne Storm & Orcs,\" where Melbourne Storm players sought to intentionally injure McCracken during play. Similarly, issues also arise where conduct can be characterised to fall \"outside the scope of the Plaintiff's consent to degree of physical contact during the game\", thus invoking compensation.\n\n\n\n"}
{"id": "960460", "url": "https://en.wikipedia.org/wiki?curid=960460", "title": "Tennis elbow", "text": "Tennis elbow\n\nTennis elbow, also known as lateral epicondylitis, is a condition in which the outer part of the elbow becomes painful and tender. The pain may also extend into the back of the forearm and grip strength may be weak. Onset of symptoms is generally gradual. Golfer's elbow is a similar condition that affects the inside of the elbow.\nIt is due to excessive use of the muscles of the back of the forearm. Typically this occurs as a result of work or sports, classically racquet sports. The diagnosis is typically based on the symptoms with medical imaging used to rule out other potential causes. It is more likely if pain increases when a person tries to bend back their wrist when their wrist is held in a neutral position. It is classified as a chronic tendinosis, not a tendinitis.\nTreatment involves decreasing activities that bring on the symptoms together with physical therapy. Pain medications such as NSAIDS or acetaminophen may be used. A brace over the upper forearm may also be helpful. If the condition does not improve corticosteroid injections or surgery may be recommended. Many people get better within one month to two years.\nAbout 2% of people are affected. Those 30 to 50 years old are most commonly affected. The condition was initially described in 1873. The name \"lawn tennis elbow\" first came into use for the condition in 1882.\n\n\nSymptoms associated with tennis elbow include, but are not limited to, radiating pain from the outside of the elbow to the forearm and wrist, pain during extension of wrist, weakness of the forearm, a painful grip while shaking hands or torquing a doorknob, and not being able to hold relatively heavy items in the hand. The pain is similar to the condition known as \"golfer's elbow\", but the latter occurs at the medial side of the elbow.\n\nThe term \"tennis elbow\" is widely used (although informal), but the condition should be understood as not limited to tennis players. The medical term \"lateral epicondylitis\" is most commonly used for the condition.\n\nSince histological findings reveal noninflammatory tissue, the term “lateral elbow tendinopathy,\" \"tendinosis,” or “angio-fibroblastic tendinosis” have been suggested instead of “lateral epicondylitis”. \n\nTennis elbow is a type of repetitive strain injury resulting from tendon overuse and failed healing of the tendon. In addition, the extensor carpi radialis brevis muscle plays a key role.\n\nEarly experiments suggested that tennis elbow was primarily caused by overexertion. However, studies show that trauma such as direct blows to the epicondyle, a sudden forceful pull, or forceful extension cause more than half of these injuries. Repeatedly mis-hitting a tennis ball in the early stages of learning the sport causes shock to the elbow joint and may contribute to contracting the condition.\n\nHistological findings include granulation tissue, microrupture, degenerative changes, and there is no traditional inflammation.\n\nLongitudinal sonogram of the lateral elbow displays thickening and heterogeneity of the common extensor tendon that is consistent with tendinosis, as the ultrasound reveals calcifications, intrasubstance tears, and marked irregularity of the lateral epicondyle. Although the term “epicondylitis” is frequently used to describe this disorder, most histopathologic findings of studies have displayed no evidence of an acute, or a chronic inflammatory process. Histologic studies have demonstrated that this condition is the result of tendon degeneration, which replaces normal tissue with a disorganized arrangement of collagen. Therefore, the disorder is more appropriately referred to as \"tendinosis\" or \"tendinopathy\" rather than \"tendinitis.\"\n\nColour Doppler ultrasound reveals structural tendon changes, with vascularity and hypo-echoic areas that correspond to the areas of pain in the extensor origin.\n\nThe pathophysiology of lateral epicondylitis is degenerative. Non-inflammatory, chronic degenerative changes of the origin of the extensor carpi radialis brevis (ECRB) muscle are identified in surgical pathology specimens. It is unclear if the pathology is affected by prior injection of corticosteroid.\n\nTennis players generally believe tennis elbow is caused by the repetitive nature of hitting thousands of tennis balls, which leads to tiny tears in the forearm tendon attachment at the elbow.\n\nThe extensor digiti minimi also has a small origin site medial to the elbow that this condition can affect. The muscle involves the extension of the little finger and some extension of the wrist allowing for adaption to \"snap\" or flick the wrist—usually associated with a racquet swing. Most often, the extensor muscles become painful due to tendon breakdown from over-extension. Improper form or movement allows for power in a swing to rotate through and around the wrist—creating a moment on that joint instead of the elbow joint or rotator cuff. This moment causes pressure to build impact forces to act on the tendon causing irritation and inflammation.\n\nAt least one author questions that lateral epicondylitis is caused by repetitive microtrauma/overuse, maintaining the theory is likely overstated and lacks scientific support.\n\nOther speculative risk factors for lateral epicondylitis include taking up tennis later in life, unaccustomed strenuous activity, decreased mental chronometry and speed and repetitive eccentric contraction of muscle (controlled lengthening of a muscle group).\n\nTo diagnose tennis elbow, physicians perform a battery of tests in which they place pressure on the affected area while asking the patient to move the elbow, wrist, and fingers. Diagnosis is made by clinical signs and symptoms that are discrete and characteristic. For example, when the elbow fully extended, the patient feels points of tenderness over the affected point on the elbow. The most common location of tenderness is at the origin of the extensor carpi radialis brevis muscle from the lateral epicondyle (extensor carpi radialis brevis origin), 1 cm distal and slightly anterior to the lateral epicondyle. There is also pain with passive wrist flexion and resistive wrist extension (Cozen's test).\n\nX-rays can confirm and distinguish possibilities of existing causes of pain that are unrelated to tennis elbow, such as fracture or arthritis. Rarely, calcification can be found where the extensor muscles attach to the lateral epicondyle. Medical ultrasonography and magnetic resonance imaging (MRI) are other valuable tools for diagnosis but are frequently avoided due to the high cost. MRI screening can confirm excess fluid and swelling in the affected region in the elbow, such as the connecting point between the forearm bone and the extensor carpi radialis brevis muscle.\n\nWhere lateral epicondylitis is caused by playing tennis, another factor of tennis elbow injury is experience and ability. The proportion of players who reported a history of tennis elbow had an increased number of playing years. As for ability, poor technique increases the chance for injury much like any sport. Therefore, an individual must learn proper technique for all aspects of their sport. The competitive level of the athlete also affects the incidence of tennis elbow. Class A and B players had a significantly higher rate of tennis elbow occurrence compared to class C and novice players. However, an opposite, but not statistically significant, trend is observed for the recurrence of previous cases, with an increasingly higher rate as ability level decreases.\n\nOther ways to prevent tennis elbow:\n\n\nVibration dampers (otherwise known as \"gummies\") are not believed to be a reliable preventative measure. Rather, proper weight distribution in the racket is thought to be a more viable option in negating shock.\n\nEvidence for the treatment of lateral epicondylitis before 2010 was poor. There were clinical trials addressing many proposed treatments, but the trials were of poor quality.\n\nIn some cases, severity of tennis elbow symptoms mend without any treatment, within six to 24 months. Tennis elbow left untreated can lead to chronic pain that degrades quality of daily living.\n\nThere are several recommendations regarding prevention, treatment, and avoidance of recurrence that are largely speculative including stretches and progressive strengthening exercises to prevent re-irritation of the tendon and other exercise measures.\n\nOne way to help treat minor cases of tennis elbow is to simply relax the affected arm. The rest lets stress and tightness within the forearm slowly relax and eventually have the arm in working condition—in a day or two, depending on the case.\n\nEccentric exercise using a rubber bar is highly effective at eliminating pain and increasing strength. The exercise involves grasping a rubber bar, twisting it, then slowly untwisting it. Although it can be considered an evidence-based practice, long-term results have not yet been determined.\n\nModerate evidence exists demonstrating that joint manipulation directed at the elbow and wrist and spinal manipulation directed at the cervical and thoracic spinal regions results in clinical changes to pain and function. There is also moderate evidence for short-term and mid-term effectiveness of cervical and thoracic spine manipulation as an add-on therapy to concentric and eccentric stretching plus mobilisation of wrist and forearm. Although not yet conclusive, the short-term analgesic effect of manipulation techniques may allow more vigorous stretching and strengthening exercises, resulting in a better and faster recovery process of the affected tendon in lateral epicondylitis.\n\nLow level laser therapy, administered at specific doses and wavelengths directly to the lateral elbow tendon insertions, may result in short-term pain relief and less disability.\n\nOrthosis is a device externally used on the limb to improve the function or reduce the pain. Orthotics are useful therapeutic interventions for initial therapy of tennis elbow. There are two main types of orthoses prescribed for this problem: counterforce elbow orthoses and wrist extension orthoses.\n\nCounterforce orthosis has a circumferential structure surrounding the arm. This orthosis usually has a strap which applies a binding force over the origin of the wrist extensors. The applied force by orthosis reduces the elongation within the musculotendinous fibers. Wrist extensor orthosis maintains the wrist in the slight extension. This position reduces the overloading strain at the lesion area.\n\nStudies indicated both type of orthoses improve the hand function and reduce the pain in people with tennis elbow.\nExtracorporeal shockwave therapy well safe is of unclear benefit.\n\nAlthough anti-inflammatories are a commonly prescribed treatment for tennis elbow, the evidence for their effect is usually anecdotal with only limited studies showing a benefit. A systematic review found that topical non-steroidal anti-inflammatory drugs (NSAIDs) may improve pain in the short term (up to 4 weeks) but was unable to draw firm conclusions due to methodological issues. Evidence for oral NSAIDs is mixed.\n\nEvidence is poor for long term improvement from injections of any type, whether corticosteroids, botulinum toxin, prolotherapy or other substances. Corticosteroid injection may be effective in the short term however are of little benefit after a year, compared to a wait-and-see approach. A recent randomized control trial comparing the effect of corticosteroid injection, physiotherapy, or a combination of corticosteroid injection and physiotherapy found that patients treated with corticosteroid injection versus placebo had lower complete recovery or improvement at 1 year (Relative risk 0.86). Patients that received corticosteroid injection also had a higher recurrence rate at 1 year versus placebo (54% versus 12%, relative risk 0.23).\nComplications from repeated steroid injections include skin problems such as hypopigmentation and fat atrophy leading to indentation of the skin around the injection site. Botulinum toxin type A to paralyze the forearm extensor muscles in those with chronic tennis elbow that has not improved with conservative measures may be viable.\n\nIn recalcitrant cases surgery may be an option. Surgical methods include:\n\nSurgical techniques for lateral epicondylitis can be done by open surgery, percutaneous surgery or arthroscopic surgery, with no evidence that any particular type is better or worse than another.\n\nResponse to initial therapy is common, but so is relapse (25% to 50%) and/or prolonged, moderate discomfort (40%).\n\nDepending upon severity and quantity of multiple tendon injuries that have built up, the extensor carpi radialis brevis may not be fully healed by conservative treatment. Nirschl defines four stages of lateral epicondylitis, showing the introduction of permanent damage beginning at Stage 2.\n\nIn tennis players, about 39.7% have reported current or previous problems with their elbow. Less than one quarter (24%) of these athletes under the age of 50 reported that the tennis elbow symptoms were \"severe\" and \"disabling,\" while 42% were over the age of 50. More women (36%) than men (24%) considered their symptoms severe and disabling. Tennis elbow is more prevalent in individuals over 40, where there is about a four-fold increase among men and two-fold increase among women. Tennis elbow equally affects both sexes and, although men have a marginally higher overall prevalence rate as compared to women, this is not consistent within each age group, nor is it a statistically significant difference.\n\nPlaying time is a significant factor in tennis elbow occurrence, with increased incidence with increased playing time being greater for respondents under 40. Individuals over 40 who played over two hours doubled their chance of injury. Those under 40 increased it 3.5 fold compared to those who played less than two hours per day.\n\nGerman physician F. Runge is usually credited for the first description of the condition, calling it \"writer's cramp\" (\"Schreibekrampf\") in 1873. Later, it was called \"washer women's elbow\". British surgeon Henry Morris published an article in \"The Lancet\" describing \"lawn tennis arm\" in 1883. The popular term \"tennis elbow\" first appeared the same year in a paper by H. P. Major, described as \"lawn-tennis elbow\".\n\n"}
{"id": "52153751", "url": "https://en.wikipedia.org/wiki?curid=52153751", "title": "Timeline of healthcare in Russia", "text": "Timeline of healthcare in Russia\n\nThis is a timeline of healthcare in Russia. Major events such as crises, policies and organizations are described.\n\n"}
{"id": "48785820", "url": "https://en.wikipedia.org/wiki?curid=48785820", "title": "U.S. News &amp; World Report Best Hospitals Rankings", "text": "U.S. News &amp; World Report Best Hospitals Rankings\n\nThe U.S. News & World Report Best Hospitals Rankings is a hospital rating publication of \"U.S. News & World Report\".\n\nIn 1997 one research team said that the value of the ratings were limited because at that time public medical data was difficult to access, and a lack of quality data limited the usefulness of any ratings.\n\nIn 2001 a research paper said that \"US News\" ratings were based on medical school assessments, counts of research publications, student opinion surveys, and counts of faculty. The paper noted that the rankings were broadly accepted, cited, and used to make decisions by all sorts of stakeholders. The public image of the rankings was that they were unbiased.\n\nA 2005 study considered \"US News\" ratings with \"Hospital Compare\", which is a rating published by Centers for Medicare and Medicaid Services. That study found that the two ratings systems frequently made different recommendations for the best hospitals.\n\nA 2009 study said that when a hospital's place in \"US News\" rankings changed, then consequently there could be as much as a 5% change in certain kinds of patient admissions to that hospital which might be attributed to that change in rankings.\n\nA 2010 study criticized the \"US News\" rankings for not disclosing the relationship between their ranking methodology and the reputation of any given hospital. The study stated that \"US News\" rankings matched other rankings about the market reputation of hospital brands, and alleged that \"US News\" rankings are overly influenced by brand image while doing less to assign a rank by health outcome metrics of patients who use medical services in those hospitals.\n\nA 2012 study compared the \"US News\" rankings with hospital rankings at Consumer Reports. That study said that the two systems ranked hospitals differently, with 8% of graded hospitals having a similar position in each other's rankings. In larger urban areas with multiple hospitals, the two ranking systems made different recommendations in 81% of cases.\n"}
