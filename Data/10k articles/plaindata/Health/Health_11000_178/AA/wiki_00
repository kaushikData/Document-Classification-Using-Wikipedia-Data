{"id": "3114073", "url": "https://en.wikipedia.org/wiki?curid=3114073", "title": "1994 expanded World Health Organization AIDS case definition", "text": "1994 expanded World Health Organization AIDS case definition\n\nThe 1994 expanded World Health Organization AIDS case definition came around through the developments in the understanding of the spectrum of severe HIV-related illness both in developed and developing countries, and the increased availability of laboratory diagnostic methods, a meeting was convened in Geneva, Switzerland by the World Health Organization Global Programme on AIDS to review the 1985 World Health Organization AIDS surveillance case definition (\"Bangui definition\") and to modify and expand them for use in adults and adolescents. Both the 1985 World Health Organization AIDS surveillance case definition and the 1994 expanded World Health Organization AIDS case definition are case definitions for AIDS surveillance and not for clinically staging HIV infection.\n\nThe main change from the \"Bangui definition\" is the addition of an HIV test for HIV antibody. If this test gives a positive result and one or more of the following conditions, the individual is considered to have AIDS.\n\n"}
{"id": "30911813", "url": "https://en.wikipedia.org/wiki?curid=30911813", "title": "4,4'-Thiodianiline", "text": "4,4'-Thiodianiline\n\n4,4'-Thiodianiline (TDA) is an aromatic amine which is presumed to be carcinogenic to humans.\n\nTDA is not combustible, but when heated it may decompose to form irritating and toxic fumes. An analogue of TDA is dapsone.\n\nSulfur is boiled in excess aniline over several hours to produce three isomers (1,1'; 1,4; 4,4') of TDA. The same journal documents syntheses of similar and overlapping compounds by Merz and Weith in 1871, and K. A. Hoffman in 1894. A study by Nietzki and Bothof shows indications that including an oxide of lead may maximize the yield of the 4,4' variant that this page refers to.\n\nTDA was used as a chemical intermediate in the production of three dyes: CI mordant yellow 16, milling red G and milling red FR, as well as the medicine Dapsone.\n\nTDA is no longer produced in the USA.\n\nTDA has caused mutations in some strains of \"Salmonella typhimurium\" and has caused tumors in laboratory mice and rats.\n"}
{"id": "44329400", "url": "https://en.wikipedia.org/wiki?curid=44329400", "title": "Anthony Costello", "text": "Anthony Costello\n\nAnthony Costello (born 20 February 1953) is a British paediatrician. Until 2015. Costello was Professor of International Child Health and Director of the Institute for Global Health at the University College London. Costello was most notable for his work on improving survival among mothers and their newborn infants in poor populations of developing countries.\n\nCostello was born in Beckenham, and graduated from school at St Joseph's Academy. Costello attended St Catharine's College, Cambridge where he took a degree in Experimental Psychology and qualified as a doctor in Medical Sciences after clinical training at the Middlesex Hospital in London. He then trained in Paediatrics and Neonatology at University College London.\n\nHe and his wife, Helen, have two sons, Harry and Ned, and one daughter, Freya.<ref name=\"https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(09)60929-6/fulltext\"></ref>\n\nAfter living in Baglung district in western Nepal from 1984–1986, two days walk from a road, he became fascinated by challenges to mother and child health in poor, remote populations. His areas of scientific expertise include the evaluation of cost-effective interventions to reduce maternal and newborn deaths, women’s groups, strategies to tackle malnutrition, international aid and the health effects of climate change. In 1999 he published a pioneering book on how to improve newborn infant health in developing countries.\n\nWith a Nepali organisation (MIRA), that he helped to establish, a large community trial of participatory learning and action using women’s groups in the remote mountains of Makwanpur district, Nepal was published in The Lancet in 2004. He went on to establish partnerships and further studies with local organisations in eastern India, Mumbai, Bangladesh and Malawi. Seven cluster randomised controlled trials of women’s groups in Nepal, India, Bangladesh and Malawi, led to a meta-analysis published in the Lancet in May 2013.\n\nResults showed that in populations where more than 30% of pregnant women joined the women's group programme, maternal death and newborn deaths were cut by one third. The intervention has now been recommended by the World Health Organisation for scale-up in poor, rural populations.\nCostello chaired the 2009 Lancet Commission on Managing the Health Effects of Climate Change, and was co-chair of a new Lancet Commission which links the UK, China, Norway and Sweden on emergency actions to tackle the climate health crisis, published in June 2015.\n\nAt WHO he has helped to lead the Global Strategy for Women’s, Children’s and Adolescents’ Health (2016‒2030) with its three objectives of survive, thrive and transform – to end preventable mortality, to promote health and well-being, and to expand enabling environments. Its guiding principles include equity, universality, human rights, development effectiveness and sustainability.\n\nWith the WHO team he has also launched the global accelerated action for the health of adolescents (AA-HA!) and established an expert review group called Maternal and Newborn Information for tracking Outcomes and Results (MONITOR) to harmonize maternal and newborn health indicators.\n\nIn February 2017, together with UNICEF and UNFPA he helped to launched the Network for Improving Quality of Care for Maternal, Newborn and Child Health to introduce evidence-based interventions to improve quality of care for maternal and newborn health supported by a learning system. The Network involves Ethiopia, Nigeria, India, Bangladesh, Malawi, Côte d'Ivoire, Uganda, Tanzania and Ghana. He also leads work on community empowerment for family health - what it means, how to measure it, and how to plan interventions at district level.\n\nWith the Lancet he is a co-chair of their new Countdown Commission on Climate Change which reports progress annually on climate change adaptation, mitigation, economics, energy policy and public engagement. With UNICEF he is helping WHO to coordinate a new Lancet Commission on redesigning child health for the Sustainable Development Goals era.\n\nCostello holds fellowships of the Academy of Medical Sciences and of the Royal College of Physicians. In April 2011, Costello received the James Spence Medal, the highest honour of the Royal College of Paediatrics and Child Health where he is a fellow. He serves on the Board of the global Partnership for Maternal, Newborn & Child Health, chaired by Dr Graca Machel. In May 2016 he received the BMJ Lifetime Achievement Award.\n\n"}
{"id": "4463960", "url": "https://en.wikipedia.org/wiki?curid=4463960", "title": "Botswana Private Medical &amp; Health Services Workers' Union", "text": "Botswana Private Medical &amp; Health Services Workers' Union\n\nThe Botswana Private Medical & Health Services Workers' Union (BPM&HSWU) is a trade union affiliate of the Botswana Federation of Trade Unions in Botswana.\n\n"}
{"id": "15659548", "url": "https://en.wikipedia.org/wiki?curid=15659548", "title": "Buffaloe v. Hart", "text": "Buffaloe v. Hart\n\nBuffaloe v. Hart, 114 N.C. App. 52 (1994) was a North Carolina Court of Appeals case dealing with a breach of contract. \n\nHomer Buffaloe verbally agreed to buy five tobacco barns in Franklin County from Lowell Thomas Hart and Patricia Hart. Buffaloe had previously rented the barns the barns based on verbal agreements. Both parties agreed to sell the barns in four annual installments based on a handshake deal without any written documentation. Buffaloe sent them a personal check with the purpose written in the subject line but they sent it back to him as they had found a buyer willing to pay more. Buffaloe sued for breach of contract.\n\nAccording to \"Buffaloe\", part performance on one party's behalf can trump the statute of frauds requirements outlined in the Uniform Commercial Code which requires the contract be in writing. In contract law, the sale of goods exceeding $500 is governed by the UCC. Additionally, the UCC calls for a written agreement to accompany the sale of goods in certain instances. The jury found that\n"}
{"id": "33680836", "url": "https://en.wikipedia.org/wiki?curid=33680836", "title": "Calvin C.J. Sia", "text": "Calvin C.J. Sia\n\nCalvin C.J. Sia (born Calvin Chia Jung Sia on June 3, 1927) is a primary care pediatrician from Hawaii who developed innovative programs to improve the quality of medical care for children in the United States and Asia. Two particular programs have been implemented throughout America: the Medical Home concept for primary care that has been promoted by the American Academy of Pediatrics and the federal Emergency Medical Services for Children program administered by the U.S. Department of Health and Human Services’ Health Resources and Services Administration, Maternal and Child Health Bureau. His Medical Home model for pediatric care and early childhood development began to take root in several Asian countries in 2003.\n\nSia is also creator of Hawaii Healthy Start Home Visiting Program to prevent child abuse and neglect and co-founder of Hawaii's Zero to Three program and Healthy and Ready to Learn Center. The Hawaii Healthy Start program, which targets expecting and new parents who may be at risk of abusing or neglecting their children, became the model for the Healthy Families America home visiting program that the United States Department of Justice's Office of Justice Programs identified in 2010 as a \"promising\" approach to child abuse prevention. The Healthy and Ready to Learn Center was a three-year pilot project to initiate training and health delivery services in an integrated system of care, with pediatric residents and graduate students in social work and early childhood education working as a team.\n\nIn addition, Sia spearheaded the creation of the Variety School for learning disabled children, a Honolulu-based educational institution for children ages 5 through 13. Sia retired from his Honolulu-based medical practice in 1996, after almost 40 years of treating patients, but continues to promote Medical Home and community pediatrics as professor of Pediatrics at the University of Hawaii John A. Burns School of Medicine. Although he retired as chairman of the American Medical Association Section Council on Pediatrics in 2007, a post he assumed in 1983, Sia continues to play a national role as an emeritus member of the executive committee of the National Center for Medical Home Implementation Project Advisory Committee, an organization he formerly served as chairman.\n\nSia is a 1945 graduate of Punahou School in Honolulu and a graduate of Dartmouth College in 1950. He received his medical degree at Western Reserve University School of Medicine in 1955 and did a general rotating internship as a lieutenant in the U.S. Army Medical Corps at William Beaumont Army Hospital in El Paso, Texas from 1955-1956. Sia then served his pediatric residency under Dr. Irvine McQuarrie at Kauikeolani Children's Hospital in Honolulu, and obtained his license to practice medicine in Hawaii in 1958. He was certified by the American Board of Pediatrics in 1960 and recertified in 1987. The University of Hawaii awarded Sia an honorary Doctor of Humane Letters degree in 1992.\n\nAs a young practicing pediatrician, Sia joined the early cadre of American Academy of Pediatrics consultants for Head Start and Parent Child Centers in Hawaii in the 1960s and developed a strong interest in prenatal, neonatal, and postnatal causes of physical and mental disabilities in children. In a paper he presented in 1964 to the Hawaii Academy of Sciences on advances in neonatology, Sia cited progress in the care of premature babies but also noted that \"completeness\" of the first physical exam and the education of nurses to be on the alert for early signs of disabilities were possible ways to save newborns with previously lethal birth defects. He concluded by observing, \"One of the basic problems will be in solving the causes and prevention of prematurity.\"\n\nInspired by one of his mentors, Dr. Robert E. Cooke, the Johns Hopkins pediatrician behind the creation of the Hopkins hospital's Kennedy Institute for Handicapped Children, Sia helped establish Hawaii's Variety School for Learning Disabilities in 1967 and served as chairman of its board of directors for many years. Sia broadened the scope of his community work to address all children with special health care needs. In the early 1970s, he invited Dr. C. Henry Kempe, founder of the Denver-based National Center for the Prevention and Treatment of Child Abuse and Neglect, and Dr. Ray E. Helfer of Michigan—two pioneers in the identification and treatment of child abuse—to help him and a small group of child advocates develop a plan to prevent and treat child abuse and neglect in the islands. That effort netted one of the first 12 demonstration grant awards by the newly created National Center on Child Abuse and Neglect in 1975, with $1 million going to establish the first Hawaii Family Stress Center. The center, later renamed the Hawaii Family Support Center, established several child abuse and neglect programs on Oahu, including a home-visiting program based on Kempe's effective use of \"lay therapists.\" These were home visitors from the community, properly trained and supervised by public health nurses and social workers who could earn the trust of at-risk families and focus on family strengths to reduce environmental risk and prevent child abuse and neglect. The center's goal was to identify vulnerable families before their day-to-day stresses, isolation, and lack of parenting knowledge and good role models gave rise to abusive and neglectful behavior.\n\nThe center's operations coincided with an effort launched by Dr. Vince L. Hutchins and Dr. Merle McPherson of the Maternal and Child Health Bureau in 1977 to revise and update the\nmission of the federal agency's Title V and companion \"crippled children's\" programs to address child development and the prevention of developmental, behavioral and psychosocial problems. McPherson took note of Sia's call for a continuous system of care originating with the primary care pediatrician. The AAP collaborated in this effort by asking each state’s AAP chapter to develop a Child Health Plan that set priorities for using MCHB block grants. Sia spearheaded the Hawaii planning effort, bringing together representatives from the Hawaii AAP Chapter, the UH medical school, the Hawaii Medical Association, and Kapiolani Medical Center for Women and Children. Armed with anecdotal evidence showing home visitors were able to promote\neffective parenting and ultimately improve outcomes, the group wrote a plan that incorporated a coordinated system of care that emphasized wellness and prevention for\nchildren, especially those with special needs.\n\nThis was the birth of the Medical Home concept for primary care, to which Sia attached the slogan, “Every Child Deserves a Medical Home.” Under this idea, which the American Academy of Pediatrics adopted as a policy statement in 1992, the medical care of all infants, children and adolescents should be accessible, continuous, comprehensive, family-centered, coordinated, compassionate, and culturally effective. It should be delivered or directed by well-trained physicians who provide primary care and help to manage and facilitate essentially all aspects of pediatric care. The physician should be known to the child and family and should be able to develop a partnership of mutual responsibility and trust with them. As Sia and his co-authors of a 2006 monograph on the Medical Home noted, this new model broadens the traditional focus on acute care to include prevention and well care at one end of the continuum and chronic care management of children with special health care needs at the other. One expert observed, for example, that for a child born with spina bifida, Sia's Medical Home model would have the family and its health care provider compose a list of specialists and therapists who would be caring for the child and a timeline of anticipated surgeries and interventions. The aim would be to have as few emergencies and unanticipated events as possible.\n\nAs the lead author of an often-cited article published by the journal Pediatrics in May 2004, Sia traced the development of the Medical Home concept.\n\nBy 1984, Sia had begun to implement the Medical Home concept in Hawaii. As chairman of an ad hoc state legislative task force on child abuse, he persuaded Hawaii lawmakers to authorize the Hawaii Healthy Start Home Visiting Program for the prevention of child abuse and neglect. This state-funded pilot program, carried out by Hawaii Family Support Center in collaboration with the Hawaii Department of Health, focused on a neighborhood in the Ewa community on Oahu, a community with relatively high rates of child abuse and neglect. A year later, he spearheaded the Hawaii Medical Association's effort to obtain a grant from the U.S. Maternal and Child Health Bureau, under the Special Projects of Regional and National Significance (SPRANS) initiative, to train primary care physicians to provide a \"Medical Home\" for all children with special health care needs. The demonstration project—which sought to help first-time families give their newborn children the best start in life—was so successful it was expanded from a small part of Oahu to other areas of Hawaii, and as word of the demonstrated positive outcomes spread, Hawaii’s Healthy Start became a model for parenting education programs nationwide. In the early 1990s, Healthy Families America and the National Healthy Start Association began to standardize and credential programs to ensure effectiveness and research-based practices. Across the United States, according to the MCHB, the home visiting program has shown that it can reduce child maltreatment and increase children’s readiness for school.\n\nMeanwhile, Sia launched the Hawaii Early Intervention Program for infants and toddlers in 1986 and also became actively involved with Hawaii’s Early Intervention Coordinating Council for Zero to Three, placing this under Hawaii’s Department of Health instead of the Department of Education. The focus of this effort was to support the Medical Home system of care with prevention and early intervention programs.\n\nAt a June 1987 conference called by Surgeon General C. Everett Koop and sponsored by the AAP and MCHB to address children with special needs, Sia and his delegation from Hawaii made a presentation of the Medical Home concept. Koop appeared to embrace it by issuing a report that endorsed a system of family-centered, community-based, coordinated care for children with special needs. This was followed in 1989 by the first National Medical Home Conference, which drew 26 AAP state chapters to Hawaii for presentations organized by Sia and MCHB officials on how to train pediatricians in the Medical Home system of care. This led to consultations to introduce the Medical Home training program to interdisciplinary teams of pediatricians, families, and other health care–related professionals in Florida, Minnesota, Nebraska, Pennsylvania, Washington and other states.\n\nThe pace of activity prompted Sia to close his private medical practice in 1996 so he could devote his time as principal investigator on various early childhood grant projects promoting the Medical Home and its integrated system of care. He launched several initiatives with a MCHB Health Education Collaboration grant in support of interprofessional training in early childhood, a Carnegie Corporation of New York Starting Points planning grant in early childhood, and Consuelo Foundation of Hawaii's Healthy and Ready to Learn grant–all with the emphasis on integrating the continuum of care of the Medical Home with other health, family, and community services from a holistic approach. The MCHB funding enabled him to travel across the country to promote the Medical Home concept to various\ncommunities, state AAP chapters, family advocacy groups and state Title V maternal and child health officers.\n\nA three-year pilot project creating a Healthy and Ready to Learn Center in Hawaii began in 1992 and helped gauge the effectiveness of Sia's family-centered interprofessional collaboration approach. Lessons learned from this project were subsequently adopted by the Office of Children and Youth of the Governor's Office of Hawaii with Sia as Co-Principal Investigator. The Carnegie Corp. Starting Points grant then was assumed by the Good Beginnings Alliance in Hawaii.\n\nSia, serving as chairman of the American Medical Association's Section Council on Pediatrics and other AMA- and AAP-related posts, used those platforms and his network of contacts with other groups to help introduce the Medical Home concept into the care of adults as well as children, although his primary focus has remained on pediatric care. In 2007, the AAP, American Academy of Family Physicians, American Academy of Pediatrics, American College of Physicians and the American Osteopathic Association adopted the Joint Principles of the Patient-Centered Medical Home that set a standard definition of a Medical Home. A year later, the AMA adopted the principles, which have since received support from over 700 member organizations of the Patient Centered Primary Care Collaborative, including primary care and specialty care societies, all major health plans and consumer organizations. In addition, the term Medical Home now regularly shows up in the literature of parent groups such as Family Voices, in family practice journals and on the websites of state public health and medical agencies.\n\nBeginning in 2000, Sia expanded his efforts related to early child development and the Medical Home to Asia. In 2003, he created the Asia-US Partnership, a think tank based at the University of Hawaii medical school whose mission is to improve child health in Asia and the United States through cross-cultural\nexchanges with leaders in pediatrics. That same year, Sia initiated and chaired the first of several AUSP Early Child Development and Primary Care conferences, bringing together pediatric and early childhood development experts from Asia and the United States to translate the science of early child development into policy and action. Participants have come from China (Beijing, Shanghai and Hong Kong), the Philippines, Singapore and Thailand and the United States. According to conference reports, these international exchanges have stimulated translation of the science on early child development and primary care into action programs in the broad areas of advocacy, service delivery, research, and training among the Asian early childhood professionals leadership. Sia has continued to serve as co-chairman of these events, including the sixth international conference, held in the Philippines capital of Manila, in May 2011. After hosting the earliest AUSP conferences in Hawaii, Sia decided to move the 2009 event to Shanghai and tapped a team of Chinese doctors to serve as conference host, signaling what he called a new phase of activity aimed at developing greater shared leadership and stronger \"country teams.\"\n\nWhile planting the seeds of the Medical Home concept in Hawaii, Sia embarked on a related advocacy campaign focused on emergency care for children. In 1979, as president of the Hawaii Medical Association, Sia urged members of the American Academy of Pediatrics to develop multifaceted Emergency Medical Services programs designed to decrease disability and death in children. By January 1981, AAP's Executive Board had approved formation of a Section on Emergency Medicine, with Sia as one of its seven charter members. He along with José B. Lee then-executive officer of the Hawaii Medical Association Emergency Medical Services Program began working closely with Senator Daniel Inouye, whom he happened to meet on a flight to Washington, D.C., to create a National Emergency Medical Services for Children System (EMSC) demonstration grant program to address acute injuries, illnesses and other childhood crises. The program was launched after the October 1984 enactment of EMSC legislation (Public Law 98-555), a bipartisan measure sponsored by Inouye and Republican Senators Orrin Hatch of Utah and Lowell Weicker of Connecticut and endorsed by Surgeon General C. Everett Koop. States receiving these demonstration grants established an emergency medical care service system for children that upgraded training and equipment for first responders and emergency departments to treat children. Hawaii ultimately received a grant to initiate its own emergency care system for children, which improved care coordination with the primary care physician. EMSC is now an established statewide system of care for children in all 50 states and territories.\n\nSeveral national and state organizations have recognized Sia for developing innovative and responsive family-centered grassroots services. Among the awards he has received are these:\n\n\nSia was born in Beijing, China to Dr. Richard Ho Ping Sia, a physician and former Rockefeller Institute researcher in infectious diseases whose work laid the groundwork for the Avery–MacLeod–McCarty experiment on DNA and bacterial transformation, and Mary Li Sia, a Honolulu-born author of several Chinese cookbooks. His mother's parents were Kong Tai Heong and Li Khai Fai, doctors who worked on the 1899 plague outbreak. Sia and his older sister Sylvia and younger sister Julia, all United States citizens by birth, grew up in Hawaii, where the family settled in 1939 after living under Japanese occupation in Beijing for nearly two years.\n\nSia married Katherine Li in 1951. Sia has three sons, Richard H.P. Sia, a journalist; Jeffrey H.K. Sia, a Honolulu-based attorney and former president of the Hawaii State Bar Association; and Dr. Michael H.T. Sia, a pediatrician and chairman of Pediatrics at Kapiolani Medical Center for Women and Children; and six grandchildren.\n\n"}
{"id": "4632309", "url": "https://en.wikipedia.org/wiki?curid=4632309", "title": "Catch the Sperm", "text": "Catch the Sperm\n\nCatch the Sperm is a Swiss computer game recognized as an entertaining way for health professionals to promote prevention of HIV.\n\nThe original Catch the Sperm (now known as CTS Style I) is a 3.4-megabite action game created in 2001 and updated in 2002 and 2003 by Phenomedia for the Swiss Federal Office of Public Health's Stop AIDS Campaign. The game can be played on a personal computer, a mobile telephone, or a cellular telephone.\n\nThe game was designed for free international distribution. Copyright of the Catch The Sperm series is owned by Scandinavian Games .\n\nCatch the Sperm is available in seven versions:\n\nEach version has its own theme, sperm characters, virus villains, and screen graphics, some of which seem to have been designed to parody the original images .\n\nThe original Catch the Sperm (CTS Style 1) was created in 2001. It is set within the interior of a vagina or a rectum and the game commences amid orgasmic moans and groans. The characters are a Neutral Sperm, a Dual Techno Sperm, and a Precious Sperm. Various viruses are included as the villains.\n\nCTS Style 2 is a 2002 update of the original Catch the Sperm game. Set in a blue environment that is ambiguous enough so that it could represent either the ocean depths or outer space, it features a Frozen Sperm, a Supersperm, a Fool Sperm, a Jungle Sperm, an I-Sperm, and a Jurassic Precious Sperm.\n\nThe Carnival edition of Catch the Sperm is set against a background of what appears to be surreal paintings and includes drifting balloons and floating strands of confetti, some of which are spiral shaped like uncoiling DNA double helixes. The characters are a Waggis, a Joker, a Sambina, a Lipstick, a Clown, and Gold, Silver, and Bronze Sperm. There are six viruses.\n\nThe Summer edition of Catch the Sperm is set in a sunny Caribbean ocean of light blue water, flowering underwater plants, and bright sunshine. The characters are a Submarine, an Orca, a Swimmer, a Water, and a Diver Sperm. As with the Carnival edition, the Summer version also includes six viruses.\n\nThe Christmas edition of Catch the Sperm is a winter landscape over which the sperm characters fly rather than swim. As they travel through the frosty night, they pass illuminated and decorated Christmas trees, lit candles, bows, and icicles. The characters are a Chocolate Sperm that, as its name suggests, resembles milk chocolate; a bearded Santa Claus Sperm, wearing a red stocking hat with a white pompom and a red flannel body suit; a Cold Sperm, wearing a blue-and-green stocking cap and matching earmuffs; a white Snowy Sperm, wearing a black top hat; and a Wrapped Sperm that is decorated with a bright red bow.\n\nIn addition, Catch the Sperm is available in many foreign language additions, including English, German, and Japanese.\n\nTo protect the couple who are having sex, the player shoots condoms from an imaginary gun that can be moved up or down along the right edge of the screen to align with oncoming threats. If the condoms find their targets, they snare sperm cells and the AIDS virus.\n\nA player loses 50 points each time that his or her sperm gun is fired, but players accumulate 100 points for snaring sperm cells, additional points for trapping viruses, and bonus points for snaring special sperm. At some point in the play, the AIDS virus mutates, and play speeds up rapidly. The player may allow any number of sperm to slip past, although this will result in a lower score, but to allow even a single virus to speed past is to lose the game \n\nUpon successfully completing a game, player are rewarded with a special \"Congratulations\" screen before embarking on the next level of play. He or she can also visit an Internet web site whereupon the player can post his or her score if it is among the highest of all other players. The world's top three players of the week receive a Catch the Sperm mouse pad inside of which mock sperm swim in a liquid.\n\n"}
{"id": "51277173", "url": "https://en.wikipedia.org/wiki?curid=51277173", "title": "Cellular agriculture", "text": "Cellular agriculture\n\nCellular agriculture focuses on the production of agriculture products from cell cultures using a combination of biotechnology, tissue engineering, molecular biology, and synthetic biology to create and design new methods of producing proteins, fats, and tissues that would otherwise come from traditional agriculture. It also refers to the creation of animal products such as meat, milk, and eggs, produced in cell culture rather than raising and slaughtering farmed livestock. The most well known cellular agriculture concept is cultured meat.\n\nAlthough cellular agriculture is a nascent scientific discipline, cellular agriculture products were first commercialized in the early 20th century with insulin and rennet.\n\nOn March 24, 1990, the FDA approved a bacteria that had been genetically engineered to produce rennet, making it the first genetically engineered product for food. Rennet is a mixture of enzymes that turns milk into curds and whey in cheese making. Traditionally, rennet is extracted from the inner lining of the fourth stomach of calves. Today, cheese making processes use rennet enzymes from genetically engineered bacteria, fungi, or yeasts because they are unadulterated, more consistent, and less expensive than animal-derived rennet.\n\nIn 2004, Jason Matheny founded New Harvest which was initially a cultured-meat advocacy organization whose mission today is to \"accelerate breakthroughs in cellular agriculture.\" New Harvest is the only organization focused exclusively on advancing the field of cellular agriculture and is funding the first cellular agriculture PhD at Tufts University.\n\nBy 2014, \"IndieBio\", a synthetic biology accelerator in San Francisco, has incubated several cellular agriculture startups, hosting Muufri (making milk from cell culture), Clara Foods (making egg whites from cell culture), Gelzen (making gelatin from bacteria and yeast), Afineur (making cultured coffee beans) and Pembient (biofabricating rhino horn). Muufri and Clara Foods were both launched by New Harvest.\n\nIn 2015, \"Mercy for Animals\" created its sister organization called \"Good Food Institute\", which promotes cellular agriculture meat and dairy alternatives in addition to plant-based options.\n\nIn July 13, 2016, New Harvest hosted the world's first international conference on cellular agriculture in San Francisco, California. The day after the conference, New Harvest hosted the first closed-door workshop for industry, academic and government stakeholders in cellular agriculture.\n\nSeveral key research tools are at the foundation of research in cellular agriculture. These include:\n\nA fundamental missing piece in the advancement of cultured meat is the availability of the appropriate cellular materials. While some methods and protocols from human and mouse cell culture may apply to agricultural cellular materials, it has become clear that most do not. This is evidenced by the fact that established protocols for creating human and mouse embryonic stem cells have not succeeded in establishing ungulate embryonic stem cell lines.\n\nThe ideal criteria for cell lines for the purpose of cultured meat production include: immortality, high proliferative ability, surface independence, serum independence, and tissue-forming ability. The specific cell types most suitable for cellular agriculture are likely to differ from species to species.\n\nToday the status quo for growing animal tissue in culture involves the use of fetal bovine serum (FBS). FBS is a blood product extracted from fetal calves. This product supplies cells with nutrients and stimulating growth factors, but is unsustainable and resource-heavy to produce, with large batch-to-batch variation.\n\nAfter the creation of the cell lines, efforts to remove serum from the growth media are key to the advancement of cellular agriculture as fetal bovine serum has been the target of most criticisms of cellular agriculture and cultured meat production. It is likely that two different media formulations will be required for each cell type: a proliferation media, for growth, and a differentiation media, for maturation.\n\nAs biotechnological processes are scaled, experiments start to become increasingly expensive, as bioreactors of increasing volume will have to be created. Each increase in size will require a re-optimization of various parameters such as unit operations, fluid dynamics, mass transfer and reaction kinetics.\n\nFor cells to form tissue, it is helpful for a material scaffold to be added to provide structure. Scaffolds are crucial for cells to form tissues larger than 100 µm across. An ideal scaffold must be non-toxic for the cells, edible, and allow for the flow of nutrients and oxygen. It must also be cheap and easy to produce on a large scale without the need for animals.\n\nThe final phase for creating cultured meat involves bringing together all the previous pieces of research to create large (>100 µm in diameter) pieces of tissue that can be made of mass-produced cells without the need of serum, where the scaffold is suitable for cells and humans.\n\nCellular agriculture is a scientific field that designs new mechanisms to produce existing agriculture products. While the majority of discussion has been around food applications, particular cultured meat, cellular agriculture can be used to create any kind of agricultural product, including those that never involved animals to begin with, like Gingko Biowork's fragrances.\n\nOver the last several years, several cellular agriculture start-ups have been created applying cellular agriculture to make a number of agricultural products and consumables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA joint program between New Harvest and the Tissue Engineering Research Center (TERC), an NIH-supported initiative established in 2004 to advance tissue engineering. The fellowship program offers funding for Masters and PhD students at Tufts university who are interested in bioengineering tunable structures, mechanics, and biology into 3D tissue systems related to their utility as foods.\n\nNew Harvest brings together pioneers in the cellular agriculture and new, interested parties from industry and academia to share relevant learnings for cellular agriculture's path moving forward.\n\nThe inaugural GFI conference is an event focused on accelerating the commercialization of plant-based and clean meat.\n\nThe Cultured Meat Symposium is a conference held in Silicon Valley highlighting top industry insights of the clean meat revolution.\n\n\nClean meat, consumer attitudes and the transition to a cellular agriculture food economy\n\nA Closer Look at Cellular Agriculture and the Processes Defining It\n\nAs lab-grown meat advances, U.S. lawmakers call for regulation\n\nCELLULAR AGRICULTURE: A WAY TO FEED TOMORROW’S SMART CITY?\n\nCellular Agriculture, Intentional Imperfection And 'Post Truth': The Transformative Food Trends Of 2017\n\nThe 4 Key Biotechnologies Needed to Get Cellular Agriculture to Commercialization\n\nCellular agriculture: Growing meat in a lab setting\n"}
{"id": "27554048", "url": "https://en.wikipedia.org/wiki?curid=27554048", "title": "Certificate of pharmaceutical product", "text": "Certificate of pharmaceutical product\n\nThe certificate of pharmaceutical product (abbreviated: CPP) is a certificate issued in the format recommended by the World Health Organization (WHO), which establishes the status of the pharmaceutical product and of the applicant for this certificate in the exporting country. It is issued for a single product, because manufacturing arrangements and approved information for different pharmaceutical forms and strengths can vary.\n\nThe Certificate of a Pharmaceutical Product is needed by the importing country when the product in question is intended for registration (licensing, authorisation) or renewal (prolongation) of registration, with the scope of commercialisation or distribution in that country.\nCertification has been recommended by WHO to help undersized drug regulatory authorities or drug regulatory authorities without proper quality assurance facilities in importing countries to assess the quality of pharmaceutical products as prerequisite of registration or importation.\n\nIn the presence of such CPP, WHO recommends to national authorities to ensure that analytical methods can be confirmed by the national laboratory, to review and if necessary to adapt product information as per local labelling requirements, and to assess bioequivalence and stability data if necessary.\n\nHowever, regulatory practices often vary in importing countries. Thus, in addition to CPP, assessment of application dossiers to support drug registrations, with different levels and complexity of requirements are considered necessary to satisfy full assurance on the appropriate quality of drugs.\n\nThe content of CPP consists of the following main data:\n\nWhen applicable, information if the manufacturing site is periodically inspected by certifying authority and if the manufacturing site complies with Good Manufacturing Practice (GMP) as recommended by WHO.\n\nAlthough issuing authorities claim that their CPP conform to WHO format (a statement to confirm whether or not the document is issued in the format recommended by WHO should be included in the certificate), their format and content may vary from an issuing country to another. Also, some authorities do not issue CPP if the respective drug is not licensed in the exporting country (e.g. Italy). In this last case, a Certificate of Exportation is issued instead, with a format and content similar to those of CPP.\n\nMost competent authorities in importing countries require CPP to be issued by the country of origin.\n\nAlso, even though this certificate is released in its original form, addressed to a specific importing country and stamped with the seal of issuing authority on each page, many authorities in importing countries may unnecessarily request authentication of such a document in the form of legalisation by their embassy in the exporting country or by apostillation (\"Abuse of scheme\").\n\n"}
{"id": "329277", "url": "https://en.wikipedia.org/wiki?curid=329277", "title": "Cherry picking", "text": "Cherry picking\n\nCherry picking, suppressing evidence, or the fallacy of incomplete evidence is the act of pointing to individual cases or data that seem to confirm a particular position while ignoring a significant portion of related cases or data that may contradict that position. It is a kind of fallacy of selective attention, the most common example of which is the confirmation bias. Cherry picking may be committed intentionally or unintentionally. This fallacy is a major problem in public debate.\n\nThe term is based on the perceived process of harvesting fruit, such as cherries. The picker would be expected to only select the ripest and healthiest fruits. An observer who only sees the selected fruit may thus wrongly conclude that most, or even all, of the tree's fruit is in a likewise good condition. This can also give a false impression of the quality of the fruit (since it is only a sample and is not a representative sample).\n\nCherry picking has a negative connotation as the practice neglects, overlooks or directly suppresses evidence that could lead to a complete picture.\n\nA concept sometimes confused with cherry picking is the idea of gathering only the fruit that is easy to harvest, while ignoring other fruit that is higher up on the tree and thus more difficult to obtain (see low-hanging fruit).\n\nCherry picking can be found in many logical fallacies. For example, the \"fallacy of anecdotal evidence\" tends to overlook large amounts of data in favor of that known personally, \"selective use of evidence\" rejects material unfavorable to an argument, while a false dichotomy picks only two options when more are available. Cherry picking can refer to the selection of data or data sets so a study or survey will give desired, predictable results which may be misleading or even completely contrary to reality.\n\nCherry picking is one of the epistemological characteristics of denialism and widely used by different science denialists to seemingly contradict scientific findings. For example, it is used in climate change denial, denial of the negative health effects of the consumption tobacco products and passive smoking and evolution denial by creationists. \n\nIn a 2002 study, a review of previous medical data found cherry picking in tests of anti-depression medication:\n\nIn argumentation, the practice of \"quote mining\" is a form of cherry picking, in which the debater selectively picks some quotes supporting a position (or exaggerating an opposing position) while ignoring those that moderate the original quote or put it into a different context. Cherry picking in debates is a large problem as the facts themselves are true but need to be put in context. Because research cannot be done live and is often untimely, cherry-picked facts or quotes usually stick in the public mainstream and, even when corrected, lead to widespread misrepresentation of groups targeted.\n\nA one-sided argument (also known as card stacking, stacking the deck, ignoring the counterevidence, slanting, and suppressed evidence) is an informal fallacy that occurs when only the reasons supporting a proposition are supplied, while all reasons opposing it are omitted.\n\nPhilosophy professor Peter Suber has written: \"The one-sidedness fallacy does not make an argument invalid. It may not even make the argument unsound. The fallacy consists in persuading readers, and perhaps ourselves, that we have said enough to tilt the scale of evidence and therefore enough to justify a judgment. If we have been one-sided, though, then we haven't yet said enough to justify a judgment. The arguments on the other side may be stronger than our own. We won't know until we examine them. So the one-sidedness fallacy doesn't mean that your premises are false or irrelevant, only that they are incomplete.\"\n\n\"With rational messages, you need to decide if you want to use a one-sided argument or a two-sided argument. A one-sided argument only presents the pro side of the argument, while a two-sided argument presents both sides. Which one you use will depend on which one meets your needs and the type of audience. Generally, one-sided arguments are better with audiences already favorable to your message. Two-sided arguments are best with audiences who are opposed to your argument, are better educated or have already been exposed to counter arguments.\"\n\nCard stacking is a propaganda technique that seeks to manipulate audience perception of an issue by emphasizing one side and repressing another. Such emphasis may be achieved through media bias or the use of one-sided testimonials, or by simply censoring the voices of critics. The technique is commonly used in persuasive speeches by political candidates to discredit their opponents and to make themselves seem more worthy.\n\nThe term originates from the magician's gimmick of \"stacking the deck\", which involves presenting a deck of cards that appears to have been randomly shuffled but which is, in fact, 'stacked' in a specific order. The magician knows the order and is able to control the outcome of the trick. In poker, cards can be stacked so that certain hands are dealt to certain players.\n\nThe phenomenon can be applied to any subject and has wide applications. Whenever a broad spectrum of information exists, appearances can be rigged by highlighting some facts and ignoring others. Card stacking can be a tool of advocacy groups or of those groups with specific agendas. For example, an enlistment poster might focus upon an impressive picture, with words such as \"travel\" and \"adventure\", while placing the words, \"enlist for two to four years\" at the bottom in a smaller and less noticeable point size.\n\n"}
{"id": "811714", "url": "https://en.wikipedia.org/wiki?curid=811714", "title": "Comparison of the healthcare systems in Canada and the United States", "text": "Comparison of the healthcare systems in Canada and the United States\n\nComparison of the healthcare systems in Canada and the United States is often made by government, public health and public policy analysts. The two countries had similar healthcare systems before Canada changed its system in the 1960s and 1970s. The United States spends much more money on healthcare than Canada, on both a per-capita basis and as a percentage of GDP. In 2006, per-capita spending for health care in Canada was US$3,678; in the U.S., US$6,714. The U.S. spent 15.3% of GDP on healthcare in that year; Canada spent 10.0%. In 2006, 70% of healthcare spending in Canada was financed by government, versus 46% in the United States. Total government spending per capita in the U.S. on healthcare was 23% higher than Canadian government spending, and U.S. government expenditure on healthcare was just under 83% of total Canadian spending (public and private) though these statistics don't take into account population differences.\n\nStudies have come to different conclusions about the result of this disparity in spending. A 2007 review of all studies comparing health outcomes in Canada and the US in a Canadian peer-reviewed medical journal found that \"health outcomes may be superior in patients cared for in Canada versus the United States, but differences are not consistent.\" Some of the noted differences were a higher life expectancy in Canada, as well as a lower infant mortality rate than the United States.\n\nOne commonly cited comparison, the 2000 World Health Organization's ratings of \"overall health service performance\", which used a \"composite measure of achievement in the level of health, the distribution of health, the level of responsiveness and fairness of financial contribution\", ranked Canada 30th and the US 37th among 191 member nations. This study rated the US \"responsiveness\", or quality of service for individuals receiving treatment, as 1st, compared with 7th for Canada. However, the average life expectancy for Canadians was 80.34 years compared with 78.6 years for residents of the US.\n\nThe WHO's study methods were criticized by some analyses.\nWhile life-expectancy and infant mortality are commonly used in comparing nationwide health care, they are in fact affected by many factors other than the quality of a nation's health care system, including individual behavior and population makeup. A 2007 report by the Congressional Research Service carefully summarizes some recent data and noted the \"difficult research issues\" facing international comparisons.\n\nIn 2004, government funding of healthcare in Canada was equivalent to $1,893 per person. In the US, government spending per person was $2,728.\n\nThe Canadian healthcare system is composed of at least 10 mostly autonomous provincial healthcare systems that report to their provincial governments, and a federal system which covers the military and First Nations. This causes a significant degree of variation in funding and coverage within the country.\n\nCanada and the US had similar healthcare systems in the early 1960s, but now have a different mix of funding mechanisms. Canada's universal single-payer healthcare system covers about 70% of expenditures, and the Canada Health Act requires that all insured persons be fully insured, without co-payments or user fees, for all medically necessary hospital and physician care. About 91% of hospital expenditures and 99% of total physician services are financed by the public sector. In the United States, with its mixed public-private system, 16% or 45 million American residents are uninsured at any one time. The U.S. is one of two OECD countries not to have some form of universal health coverage, the other being Turkey. Mexico established a universal healthcare program by November 2008.\n\nThe governments of both nations are closely involved in healthcare. The central structural difference between the two is in health insurance. In Canada, the federal government is committed to providing funding support to its provincial governments for healthcare expenditures as long as the province in question abides by accessibility guarantees as set out in the Canada Health Act, which explicitly prohibits billing end users for procedures that are covered by Medicare. While some label Canada's system as \"socialized medicine\", health economists do not use that term. Unlike systems with public delivery, such as the UK, the Canadian system provides public coverage for a combination of public and private delivery. Princeton University health economist Uwe E. Reinhardt says that single-payer systems are not \"socialized medicine\" but \"social insurance\" systems, since providers (such as doctors) are largely in the private sector. Similarly, Canadian hospitals are controlled by private boards or regional health authorities, rather than being part of government.\n\nIn the US, direct government funding of health care is limited to Medicare, Medicaid, and the State Children's Health Insurance Program (SCHIP), which cover eligible senior citizens, the very poor, disabled persons, and children. The federal government also runs the Veterans Administration, which provides care directly to retired or disabled veterans, their families, and survivors through medical centers and clinics.\n\nThe U.S. government also runs the Military Health System. In fiscal year 2007, the MHS had a total budget of $39.4 billion and served approximately 9.1 million beneficiaries, including active-duty personnel and their families, and retirees and their families. The MHS includes 133,000 personnel, 86,000 military and 47,000 civilian, working at more than 1,000 locations worldwide, including 70 inpatient facilities and 1,085 medical, dental, and veterans' clinics.\n\nOne study estimates that about 25 percent of the uninsured in the U.S. are eligible for these programs but remain unenrolled; however, extending coverage to all who are eligible remains a fiscal and political challenge.\n\nFor everyone else, health insurance must be paid for privately. Some 59% of U.S. residents have access to health care insurance through employers, although this figure is decreasing, and coverages as well as workers' expected contributions vary widely. Those whose employers do not offer health insurance, as well as those who are self-employed or unemployed, must purchase it on their own. Nearly 27 million of the 45 million uninsured U.S. residents worked at least part-time in 2007, and more than a third were in households that earned $50,000 or more per year.\n\nDespite the greater role of private business in the US, federal and state agencies are increasingly involved, paying about 45% of the $2.2 trillion the nation spent on medical care in 2004. The U.S. government spends more on healthcare than on Social Security and national defense combined, according to the Brookings Institution.\n\nBeyond its direct spending, the US government is also highly involved in healthcare through regulation and legislation. For example, the Health Maintenance Organization Act of 1973 provided grants and loans to subsidize Health Maintenance Organizations and contained provisions to stimulate their popularity. HMOs had been declining before the law; by 2002 there were 500 such plans enrolling 76 million people.\n\nThe Canadian system has been 69–75% publicly funded, though most services are delivered by private providers, including physicians (although they may derive their revenue primarily from government billings). Although some doctors work on a purely fee-for-service basis (usually family physicians), some family physicians and most specialists are paid through a combination of fee-for-service and fixed contracts with hospitals or health service management organizations.\n\nCanada's universal health plans do not cover certain services. Non-cosmetic dental care is covered for children up to age 14 in some provinces. Outpatient prescription drugs are not required to be covered, but some provinces have drug cost programs that cover most drug costs for certain populations. In every province, seniors receiving the Guaranteed Income Supplement have significant additional coverage; some provinces expand forms of drug coverage to all seniors, low-income families, those on social assistance, or those with certain medical conditions. Some provinces cover all drug prescriptions over a certain portion of a family's income. Drug prices are also regulated, so brand-name prescription drugs are often significantly cheaper than in the U.S. Optometry is only covered in some provinces and is sometimes only covered for children under a certain age. Visits to non-physician specialists may require an additional fee. Also, some procedures are only covered under certain circumstances. For example, circumcision is not covered, and a fee is usually charged when a parent requests the procedure; however, if an infection or medical necessity arises, the procedure would be covered.\n\nAccording to Dr. Albert Schumacher, former president of the Canadian Medical Association, an estimated 75 percent of Canadian healthcare services are delivered privately, but funded publicly.\n\nFrontline practitioners whether they're GPs or specialists by and large are not salaried. They're small hardware stores. Same thing with labs and radiology clinics ... The situation we are seeing now are more services around not being funded publicly but people having to pay for them, or their insurance companies. We have sort of a passive privatization.\n\nIn both Canada and the United States, access can be a problem. Studies suggest that 40% of U.S. citizens do not have adequate health insurance, if any at all. In Canada, 5% of Canadian citizens have not been able to find a regular doctor, with a further 9% having never looked for one. Yet, even if some cannot find a family doctor, every Canadian citizen is covered by the national health care system. The U.S. data is evidenced in a 2007 Consumer Reports study on the U.S. health care system which showed that the underinsured account for 24% of the U.S. population and live with skeletal health insurance that barely covers their medical needs and leaves them unprepared to pay for major medical expenses. When added to the population of uninsured (approximately 16% of the U.S. population), a total of 40% of Americans ages 18–64 have inadequate access to healthcare, according to the Consumer Reports study. The Canadian data comes from the 2003 Canadian Community Health Survey,\n\nIn the U.S., the federal government does not guarantee universal healthcare to all its citizens, but publicly funded healthcare programs help to provide for the elderly, disabled, the poor, and children. The Emergency Medical Treatment and Active Labor Act or EMTALA also ensures public access to emergency services. The EMTALA law forces emergency healthcare providers to stabilize an emergency health crisis and cannot withhold treatment for lack of evidence of insurance coverage or other evidence of the ability to pay. EMTALA does not absolve the person receiving emergency care of the obligation to meet the cost of emergency healthcare not paid for at the time and it is still within the right of the hospital to pursue any debtor for the cost of emergency care provided. In Canada, emergency room treatment for legal Canadian residents is not charged to the patient at time of service but is met by the government.\n\nAccording to the United States Census Bureau, 59.3% of U.S. citizens have health insurance related to employment, 27.8% have government-provided health-insurance; nearly 9% purchase health insurance directly (there is some overlap in these figures), and 15.3% (45.7 million) were uninsured in 2007. An estimated 25 percent of the uninsured are eligible for government programs but unenrolled. About a third of the uninsured are in households earning more than $50,000 annually. A 2003 report by the Congressional Budget Office found that many people lack health insurance only temporarily, such as after leaving one employer and before a new job. The number of chronically uninsured (uninsured all year) was estimated at between 21 and 31 million in 1998. Another study, by the Kaiser Commission on Medicaid and the Uninsured, estimated that 59 percent of uninsured adults have been uninsured for at least two years. One indicator of the consequences of Americans' inconsistent health care coverage is a study in \"Health Affairs\" that concluded that half of personal bankruptcies involved medical bills. Although other sources dispute this, it is possible that medical debt is the principal cause of bankruptcy in the United States.\n\nA number of clinics provide free or low-cost non-emergency care to poor, uninsured patients. The National Association of Free Clinics claims that its member clinics provide $3 billion in services to some 3.5 million patients annually.\n\nA peer-reviewed comparison study of healthcare access in the two countries published in 2006 concluded that U.S. residents are one third less likely to have a regular medical doctor, one fourth more likely to have unmet healthcare needs, and are more than twice as likely to forgo needed medicines. The study noted that access problems \"were particularly dire for the US uninsured.\" Those who lack insurance in the U.S. were much less satisfied, less likely to have seen a doctor, and more likely to have been unable to receive desired care than both Canadians and insured Americans.\n\nAnother cross-country study compared access to care based on immigrant status in Canada and the U.S. Findings showed that in both countries, immigrants had worse access to care than non-immigrants. Specifically, immigrants living in Canada were less likely to have timely Pap tests compared with native-born Canadians; in addition, immigrants in the U.S. were less likely to have a regular medical doctor and an annual consultation with a health care provider compared with native-born Americans. In general, immigrants in Canada had better access to care than those in the U.S., but most of the differences were explained by differences in socioeconomic status (income, education) and insurance coverage across the two countries. However, immigrants in the U.S. were more likely to have timely Pap tests than immigrants in Canada.\n\nCato Institute has expressed concerns that the U.S. government has restricted the freedom of Medicare patients to spend their own money on healthcare, and has contrasted these developments with the situation in Canada, where in 2005 the Supreme Court of Canada ruled that the province of Quebec could not prohibit its citizens from purchasing covered services through private health insurance. The institute has urged the Congress to restore the right of American seniors to spend their own money on medical care.\n\nThe Canada Health Act covers the services of psychiatrists, who are medical doctors with additional training in psychiatry but does not cover treatment by a psychologist or psychotherapist unless the practitioner is also a medical doctor. Goods and Services Tax or Harmonized Sales Tax (depending on the province) applies to the services of psychotherapists. Some provincial or territorial programs and some private insurance plans may cover the services of psychologists and psychotherapists, but there is no federal mandate for such services in Canada. In the U.S., the Affordable Care Act includes prevention, early intervention, and treatment of mental and/or substance use disorders as an \"essential health benefit\" (EHB) that must be covered by health plans that are offered through the Health Insurance Marketplace. Under the Affordable Care Act, most health plans must also cover certain preventive services without a copayment, co-insurance, or deductible. In addition, the U.S. Mental Health Parity and Addiction Equity Act (MHPAEA) of 2008 mandates \"parity\" between mental health and/or substance use disorder (MH/SUD) benefits and medical/surgical benefits covered by a health plan. Under that law, if a health care plan offers mental health and/or substance use disorder benefits, it must offer the benefits on par with the other medical/surgical benefits it covers.\n\nOne complaint about both the U.S. and Canadian systems is waiting times, whether for a specialist, major elective surgery, such as hip replacement, or specialized treatments, such as radiation for breast cancer; wait times in each country are affected by various factors. In the United States, access is primarily determined by whether a person has access to funding to pay for treatment and by the availability of services in the area and by the willingness of the provider to deliver service at the price set by the insurer. In Canada, the wait time is set according to the availability of services in the area and by the relative need of the person needing treatment.\n\nAs reported by the Health Council of Canada, a 2010 Commonwealth survey found that 39% of Canadians waited 2 hours or more in the emergency room, versus 31% in the U.S.; 43% waited 4 weeks or more to see a specialist, versus 10% in the U.S. The same survey states that 37% of Canadians say it is difficult to access care after hours (evenings, weekends or holidays) without going to the emergency department over 34% of Americans. Furthermore, 47% of Canadians and 50% of Americans who visited emergency departments over the past two years feel that they could have been treated at their normal place of care if they were able to get an appointment.\n\nA report published by Health Canada in 2008 included statistics on self-reported wait times for diagnostic services. The median wait time for diagnostic services such as MRI and CAT scans is two weeks with 89.5% waiting less than 3 months. The median wait time to see a special physician is a little over four weeks with 86.4% waiting less than 3 months. The median wait time for surgery is a little over four weeks with 82.2% waiting less than 3 months. In the U.S., patients on Medicaid, the low-income government programs, can wait three months or more to see specialists. Because Medicaid payments are low, some have claimed that some doctors do not want to see Medicaid patients. For example, in Benton Harbor, Michigan, specialists agreed to spend one afternoon every week or two at a Medicaid clinic, which meant that Medicaid patients had to make appointments not at the doctor's office, but at the clinic, where appointments had to be booked months in advance. A 2009 study found that on average the wait in the United States to see a medical specialist is 20.5 days.\n\nIn a 2009 survey of physician appointment wait times in the United States, the average wait time for an appointment with an orthopedic surgeon in the country as a whole was 17 days. In Dallas, Texas the wait was 45 days (the longest wait being 365 days). Nationwide across the U.S. the average wait time to see a family doctor was 20 days. The average wait time to see a family practitioner in Los Angeles, California was 59 days and in Boston, Massachusetts it was 63 days.\n\nStudies by the Commonwealth Fund found that 42% of Canadians waited 2 hours or more in the emergency room, vs. 29% in the U.S.; 57% waited 4 weeks or more to see a specialist, vs. 23% in the U.S., but Canadians had more chances of getting medical attention at nights, or on weekends and holidays than their American neighbors without the need to visit an ER (54% compared to 61%). Statistics from the Canadian free market think tank Fraser Institute in 2008 indicate that the average wait time between the time when a general practitioner refers a patient for care and the receipt of treatment was almost four and a half months in 2008, roughly double what it had been 15 years before.\n\nA 2003 survey of hospital administrators conducted in Canada, the U.S., and three other countries found dissatisfaction with both the U.S. and Canadian systems. For example, 21% of Canadian hospital administrators, but less than 1% of American administrators, said that it would take over three weeks to do a biopsy for possible breast cancer on a 50-year-old woman; 50% of Canadian administrators versus none of their American counterparts said that it would take over six months for a 65-year-old to undergo a routine hip replacement surgery. However, U.S. administrators were the most negative about their country's system. Hospital executives in all five countries expressed concerns about staffing shortages and emergency department waiting times and quality.\n\nIn a letter to the \"Wall Street Journal\", Robert Bell, the President and CEO of University Health Network, Toronto, said that Michael Moore's film \"Sicko\" \"exaggerated the performance of the Canadian health system — there is no doubt that too many patients still stay in our emergency departments waiting for admission to scarce hospital beds.\" However, \"Canadians spend about 55% of what Americans spend on health care and have longer life expectancy and lower infant mortality rates. Many Americans have access to quality healthcare. All Canadians have access to similar care at a considerably lower cost.\" There is \"no question\" that the lower cost has come at the cost of \"restriction of supply with sub-optimal access to services,\" said Bell. A new approach is targeting waiting times, which are reported on public websites.\n\nIn 2007 Shona Holmes, a Waterdown, Ontario woman who had a Rathke's cleft cyst removed at the Mayo Clinic in Arizona, sued the Ontario government for failing to reimburse her $95,000 in medical expenses.\nHolmes had characterized her condition as an emergency, said she was losing her sight and portrayed her condition as a life-threatening brain cancer.\nIn July 2009 Holmes agreed to appear in television ads broadcast in the United States warning Americans of the dangers of adopting a Canadian-style health care system.\nThe ads she appeared in triggered debates on both sides of the border.\nAfter her ad appeared critics pointed out discrepancies in her story, including that Rathke's cleft cyst, the condition she was treated for, was not a form of cancer, and was not life-threatening.\n\nHealthcare is one of the most expensive items of both nations' budgets. In the United States, the various levels of government spend more per capita than levels of government do in Canada. In 2004, Canada government-spending was $2,120 (in US dollars) per person, while the United States government-spending $2,724.\n\nA 1999 report found that after exclusions, administration accounted for 31.0% of healthcare expenditures in the United States, as compared with 16.7% in Canada. In looking at the insurance element, in Canada, the provincial single-payer insurance system operated with overheads of 1.3%, comparing favourably with private insurance overheads (13.2%), U.S. private insurance overheads (11.7%) and U.S. Medicare and Medicaid program overheads (3.6% and 6.8% respectively). The report concluded by observing that gap between U.S. and Canadian spending on administration had grown to $752 per capita and that a large sum might be saved in the United States if the U.S. implemented a Canadian-style system.\n\nHowever, U.S. government spending covers less than half of all healthcare costs. Private spending is also far greater in the U.S. than in Canada. In Canada, an average of $917 was spent annually by individuals or private insurance companies for health care, including dental, eye care, and drugs. In the U.S., this sum is $3,372. In 2006, healthcare consumed 15.3% of U.S. annual GDP. In Canada, only 10% of GDP was spent on healthcare. This difference is a relatively recent development. In 1971 the nations were much closer, with Canada spending 7.1% of GDP while the U.S. spent 7.6%.\n\nSome who advocate against greater government involvement in healthcare have asserted that the difference in costs between the two nations is partially explained by the differences in their demographics. Illegal immigrants, more prevalent in the U.S. than in Canada, also add a burden to the system, as many of them do not carry health insurance and rely on emergency rooms — which are legally required to treat them under EMTALA — as a principal source of care. In Colorado, for example, an estimated 80% of undocumented immigrants do not have health insurance.\n\nThe mixed system in the United States has become more similar to the Canadian system. In recent decades, managed care has become prevalent in the United States, with some 90% of privately insured Americans belonging to plans with some form of managed care. In \"managed care\", insurance companies control patients' health care to reduce costs, for instance by demanding a second opinion prior to some expensive treatments or by denying coverage for treatments not considered worth their cost.\n\nAdministrative costs are also higher in the United States than in Canada.\n\nThrough all entities in its public–private system, the US spends more per capita than any other nation in the world, but is the only wealthy industrialized country in the world that lacks some form of universal healthcare. In March 2010, the US Congress passed regulatory reform of the American \"health insurance\" system. However, since this legislation is not fundamental \"healthcare\" reform, it is unclear what its effect will be and as the new legislation is implemented in stages, with the last provision in effect in 2018, it will be some years before any empirical evaluation of the full effects on the comparison could be determined.\n\nHealthcare costs in both countries are rising faster than inflation. As both countries consider changes to their systems, there is debate over whether resources should be added to the public or private sector. Although Canadians and Americans have each looked to the other for ways to improve their respective health care systems, there exists a substantial amount of conflicting information regarding the relative merits of the two systems. In the U.S., Canada's mostly monopsonistic health system is seen by different sides of the ideological spectrum as either a model to be followed or avoided.\n\nSome of the extra money spent in the United States goes to physicians, nurses, and other medical professionals. According to health data collected by the OECD, average income for physicians in the United States in 1996 was nearly twice that for physicians in Canada. In 2012, the gross average salary for doctors in Canada was CDN$328,000. Out of the gross amount, doctors pay for taxes, rent, staff salaries and equipment. When comparing average incomes of doctors in Canada and U.S., it should be kept in mind that malpractice insurance premiums may differ significantly between Canada and the U.S., and the proportion of doctors who are specialists differs. In Canada, less than half of doctors are specialists whereas more than 70% of doctors are specialists in the U.S.\n\nCanada has fewer doctors per capita than the United States. In the U.S, there were 2.4 doctors per 1,000 people in 2005; in Canada, there were 2.2. Some doctors leave Canada to pursue career goals or higher pay in the U.S., though significant numbers of physicians from countries such as China, India, Pakistan and South Africa immigrate to practice in Canada. Many Canadian physicians and new medical graduates also go to the U.S. for post-graduate training in medical residencies. As it is a much larger market, new and cutting-edge sub-specialties are more widely available in the U.S. as opposed to Canada. However, statistics published in 2005 by the Canadian Institute for Health Information (CIHI), show that, for the first time since 1969 (the period for which data are available), more physicians returned to Canada than moved abroad.\n\nBoth Canada and the United States have limited programs to provide prescription drugs to the needy. In the U.S., the introduction of Medicare Part D has extended partial coverage for pharmaceuticals to Medicare beneficiaries. In Canada all drugs given in hospitals fall under Medicare, but other prescriptions do not. The provinces all have some programs to help the poor and seniors have access to drugs, but while there have been calls to create one, no national program exists. About two thirds of Canadians have private prescription drug coverage, mostly through their employers. In both countries, there is a significant population not fully covered by these programs. A 2005 study found that 20% of Canada's and 40% of America's sicker adults did not fill a prescription because of cost.\n\nFurthermore, the 2010 Commonwealth Fund International Health Policy Survey indicates that 4% of Canadians indicated that they did not visit a doctor because of cost compared with 22% of Americans. Additionally, 21% of Americans have said that they did not fill a prescription for medicine or have skipped doses due to cost. That is compared with 10% of Canadians.\n\nOne of the most important differences between the two countries is the much higher cost of drugs in the United States. In the U.S., $728 per capita is spent each year on drugs, while in Canada it is $509. At the same time, consumption is higher in Canada, with about 12 prescriptions being filled per person each year in Canada and 10.6 in the United States. The main difference is that patented drug prices in Canada average between 35% and 45% lower than in the United States, though generic prices are higher. The price differential for brand-name drugs between the two countries has led Americans to purchase upward of $1 billion US in drugs per year from Canadian pharmacies.\n\nThere are several reasons for the disparity. The Canadian system takes advantage of centralized buying by the provincial governments that have more market heft and buy in bulk, lowering prices. By contrast, the U.S. has explicit laws that prohibit Medicare or Medicaid from negotiating drug prices. In addition, price negotiations by Canadian health insurers are based on evaluations of the clinical effectiveness of prescription drugs, allowing the relative prices of therapeutically similar drugs to be considered in context. The Canadian Patented Medicine Prices Review Board also has the authority to set a fair and reasonable price on patented products, either comparing it to similar drugs already on the market, or by taking the average price in seven developed nations. Prices are also lowered through more limited patent protection in Canada. In the U.S., a drug patent may be extended five years to make up for time lost in development. Some generic drugs are thus available on Canadian shelves sooner.\n\nThe pharmaceutical industry is important in both countries, though both are net importers of drugs. Both countries spend about the same amount of their GDP on pharmaceutical research, about 0.1% annually\n\nThe United States spends more on technology than Canada. In a 2004 study on medical imaging in Canada, it was found that Canada had 4.6 MRI scanners per million population while the U.S. had 19.5 per million. Canada's 10.3 CT scanners per million also ranked behind the U.S., which had 29.5 per million. The study did not attempt to assess whether the difference in the number of MRI and CT scanners had any effect on the medical outcomes or were a result of overcapacity but did observe that MRI scanners are used more intensively in Canada than either the U.S. or Great Britain. This disparity in the availability of technology, some believe, results in longer wait times. In 1984 wait times of up to 22 months for an MRI were alleged in Saskatchewan. However, according to more recent official statistics (2007), all emergency patients receive MRIs within 24 hours, those classified as urgent receive them in under 3 weeks and the maximum elective wait time is 19 weeks in Regina and 26 weeks in Saskatoon, the province's two largest metropolitan areas.\n\nAccording to the Health Council of Canada's 2010 report \"Decisions, Decisions: Family doctors as gatekeepers to prescription drugs and diagnostic imaging in Canada\", the Canadian federal government invested $3 billion over 5 years (2000–2005) in relation to diagnostic imaging and agreed to invest a further $2 billion to reduce wait times. These investments led to an increase in the number of scanners across Canada as well as the number of exams being performed. The number of CT scanners increased from 198 to 465 and MRI scanners increased from 19 to 266 (more than tenfold) between 1990 and 2009. Similarly, the number of CT exams increased by 58% and MRI exams increased by 100% between 2003 and 2009. In comparison to other OECD countries, including the US, Canada's rates of MRI and CT exams falls somewhere in the middle. Nevertheless, the Canadian Association of Radiologists claims that as many as 30% of diagnostic imaging scans are inappropriate and contribute no useful information.\n\nThe extra cost of malpractice lawsuits is a proportion of health spending in both the U.S. (1.7% in 2002) and Canada (0.27% in 2001 or $237 million). In Canada the total cost of settlements, legal fees, and insurance comes to $4 per person each year, but in the United States it is over $16. Average payouts to American plaintiffs were $265,103, while payouts to Canadian plaintiffs were somewhat higher, averaging $309,417. However, malpractice suits are far more common in the U.S., with 350% more suits filed each year per person. While malpractice costs are significantly higher in the U.S., they make up only a small proportion of total medical spending. The total cost of defending and settling malpractice lawsuits in the U.S. in 2004 was over $28 billion. Critics say that defensive medicine consumes up to 9% of American healthcare expenses., but CBO studies suggest that it is much smaller.\n\nThere are a number of ancillary costs that are higher in the U.S. Administrative costs are significantly higher in the U.S.; government mandates on record keeping and the diversity of insurers, plans and administrative layers involved in every transaction result in greater administrative effort. One recent study comparing administrative costs in the two countries found that these costs in the U.S. are roughly double what they are in Canada. Another ancillary cost is marketing, both by insurance companies and health care providers. These costs are higher in the U.S., contributing to higher overall costs in that nation.\n\nIn the World Health Organization's rankings of healthcare system performance among 191 member nations published in 2000, Canada ranked 30th and the U.S. 37th, while the overall health of Canadians was ranked 35th and Americans 72nd. However, the WHO's methodologies, which attempted to measure how efficiently health systems translate expenditure into health, generated broad debate and criticism.\n\nResearchers caution against inferring healthcare quality from some health statistics. June O'Neill and Dave O'Neill point out that \"... life expectancy and infant mortality are both poor measures of the efficacy of a health care system because they are influenced by many factors that are unrelated to the quality and accessibility of medical care\".\n\nIn 2007, Gordon H. Guyatt et al. conducted a meta-analysis, or systematic review, of all studies that compared health outcomes for similar conditions in Canada and the U.S., in \"Open Medicine\", an open-access peer-reviewed Canadian medical journal. They concluded, \"Available studies suggest that health outcomes may be superior in patients cared for in Canada versus the United States, but differences are not consistent.\" Guyatt identified 38 studies addressing conditions including cancer, coronary artery disease, chronic medical illnesses and surgical procedures. Of 10 studies with the strongest statistical validity, 5 favoured Canada, 2 favoured the United States, and 3 were equivalent or mixed. Of 28 weaker studies, 9 favoured Canada, 3 favoured the United States, and 16 were equivalent or mixed. Overall, results for mortality favoured Canada with a 5% advantage, but the results were weak and varied. The only consistent pattern was that Canadian patients fared better in kidney failure.\n\nIn terms of population health, life expectancy in 2006 was about two and a half years longer in Canada, with Canadians living to an average of 79.9 years and Americans 77.5 years. Infant and child mortality rates are also higher in the U.S. Some comparisons suggest that the American system underperforms Canada's system as well as those of other industrialized nations with universal coverage. For example, a ranking by the World Health Organization of health care system performance among 191 member nations, published in 2000, ranked Canada 30th and the U.S. 37th, and the overall health of Canada 35th to the American 72nd. The WHO did not merely consider health care outcomes, but also placed heavy emphasis on the health disparities between rich and poor, funding for the health care needs of the poor, and the extent to which a country was reaching the potential health care outcomes they believed were possible for that nation. In an international comparison of 21 more specific quality indicators conducted by the Commonwealth Fund International Working Group on Quality Indicators, the results were more divided. One of the indicators was a tie, and in 3 others, data was unavailable from one country or the other. Canada performed better on 11 indicators; such as survival rates for colorectal cancer, childhood leukemia, and kidney and liver transplants. The U.S. performed better on 6 indicators, including survival rates for breast and cervical cancer, and avoidance of childhood diseases such as pertussis and measles. It should be noted that the 21 indicators were distilled from a starting list of 1000. The authors state that, \"It is an opportunistic list, rather than a comprehensive list.\"\n\nSome of the difference in outcomes may also be related to lifestyle choices. The OECD found that Americans have slightly higher rates of smoking and alcohol consumption than do Canadians as well as significantly higher rates of obesity. A joint US-Canadian study found slightly higher smoking rates among Canadians. Another study found that Americans have higher rates not only of obesity, but also of other health risk factors and chronic conditions, including physical inactivity, diabetes, hypertension, arthritis, and chronic obstructive pulmonary disease.\n\nWhile a Canadian systematic review stated that the differences in the systems of Canada and the United States could not alone explain differences in healthcare outcomes, the study didn't consider that over 44,000 Americans die every year due to not having a single payer system for healthcare in the United States and it didn't consider the millions more that live without proper medical care due to a lack of insurance.\n\nThe United States and Canada have different racial makeups, different obesity rates and different alcoholism rates, which would likely cause the US to have a shorter average life expectancy and higher infant mortality even with equal healthcare provided. The US population is 12.2% African Americans and 16.3% Hispanic Americans (2010 Census), whereas Canada has only 2.5% African Canadians and 0.97% Hispanic Canadians (2006 Census). African Americans have higher mortality rates than any other racial or ethnic group for eight of the top ten causes of death. The cancer incidence rate among African Americans is 10% higher than among European Americans. U.S. Latinos have higher rates of death from diabetes, liver disease, and infectious diseases than do non-Latinos. Adult African Americans and Latinos have approximately twice the risk as European Americans of developing diabetes. The infant mortality rates for African Americans is twice that of whites. Unfortunately, directly comparing infant mortality rates between countries is difficult, as countries have different definitions of what qualifies as an infant death.\n\nAnother issue with comparing the two systems is the baseline health of the patient's for which the systems must treat. Canada has only half the obesity rate that the US system must deal with (14.3% vs 30.6%). On average, obesity reduces life expectancy by 6–7 years.\n\nA 2004 study found that Canada had a slightly higher mortality rate for acute myocardial infarction (heart attack) because of the more conservative Canadian approach to revascularizing (opening) coronary arteries.\n\nNumerous studies have attempted to compare the rates of cancer incidence and mortality in Canada and the U.S., with varying results. Doctors who study cancer epidemiology warn that the diagnosis of cancer is subjective, and the \"reported\" incidence of a cancer will rise if screening is more aggressive, even if the \"real\" cancer incidence is the same. Statistics from different sources may not be compatible if they were collected in different ways. The proper interpretation of cancer statistics has been an important issue for many years. Dr. Barry Kramer of the National Institutes of Health points to the fact that cancer incidence rose sharply over the past few decades as screening became more common. He attributes the rise to increased detection of benign early stage cancers that pose little risk of metastasizing. Furthermore, though patients who were treated for these benign cancers were at little risk, they often have trouble finding health insurance after the fact.\n\nCancer survival time increases with later years of diagnosis, because cancer treatment improves, so cancer survival statistics can only be compared for cohorts in the same diagnosis year. For example, as doctors in British Columbia adopted new treatments, survival time for patients with metastatic breast cancer increased from 438 days for those diagnosed in 1991–1992, to 667 days for those diagnosed in 1999–2001.\n\nAn assessment by Health Canada found that cancer mortality rates are almost identical in the two countries. Another international comparison by the National Cancer Institute of Canada indicated that incidence rates for most, but not all, cancers were higher in the U.S. than in Canada during the period studied (1993–1997). Incidence rates for certain types, such as colorectal and stomach cancer, were actually higher in Canada than in the U.S. In 2004, researchers published a study comparing health outcomes in the Anglo countries. Their analysis indicates that Canada has greater survival rates for both colorectal cancer and childhood leukemia, while the United States has greater survival rates for Non-Hodgkin's lymphoma as well as breast and cervical cancer.\n\nA study based on data from 1978 through 1986 found very similar survival rates in both the United States and in Canada. However, a study based on data from 1993 through 1997 found lower cancer survival rates among Canadians than among Americans.\n\nA few comparative studies have found that cancer survival rates vary more widely among different populations in the U.S. than they do in Canada. Mackillop and colleagues compared cancer survival rates in Ontario and the U.S. They found that cancer survival was more strongly correlated with socio-economic class in the U.S. than in Ontario. Furthermore, they found that the American survival advantage in the four highest quintiles was statistically significant. They strongly suspected that the difference due to prostate cancer was a result of greater detection of asymptomatic cases in the U.S. Their data indicates that neglecting the prostate cancer data reduces the American advantage in the four highest quintiles and gives Canada a statistically significant advantage in the lowest quintile. Similarly, they believe differences in screening mammography may explain part of the American advantage in breast cancer. Exclusion of breast and prostate cancer data results in very similar survival rates for both countries.\n\nHsing et al. found that prostate cancer mortality incidence rate ratios were lower among U.S. whites than among any of the nationalities included in their study, including Canadians. U.S. African Americans in the study had lower rates than any group except for Canadians and U.S. whites. Echoing the concerns of Dr. Kramer and Professor Mackillop, Hsing later wrote that reported prostate cancer incidence depends on screening. Among whites in the U.S., the death rate for prostate cancer remained constant, even though the incidence increased, so the additional reported prostate cancers did not represent an increase in real prostate cancers, said Hsing. Similarly, the death rates from prostate cancer in the U.S. increased during the 1980s and peaked in early 1990. This is at least partially due to \"attribution bias\" on death certificates, where doctors are more likely to ascribe a death to prostate cancer than to other diseases that affected the patient, because of greater awareness of prostate cancer or other reasons.\n\nBecause health status is \"considerably affected\" by socioeconomic and demographic characteristics, such as level of education and income, \"the value of comparisons in isolating the impact of the healthcare system on outcomes is limited,\" according to health care analysts. Experts say that the incidence and mortality rates of cancer cannot be combined to calculate survival from cancer. Nevertheless, researchers have used the ratio of mortality to incidence rates as one measure of the effectiveness of healthcare. Data for both studies was collected from registries that are members of the North American Association of Central Cancer Registries, an organization dedicated to developing and promoting uniform data standards for cancer registration in North America.\n\nThe U.S. and Canada differ substantially in their demographics, and these differences may contribute to differences in health outcomes between the two nations. Although both countries have white majorities, Canada has a proportionately larger immigrant minority population. Furthermore, the relative size of different ethnic and racial groups vary widely in each country. Hispanics and peoples of African descent constitute a much larger proportion of the U.S. population. Non-Hispanic North American aboriginal peoples constitute a much larger proportion of the Canadian population. Canada also has a proportionally larger South Asian and East Asian population. Also, the proportion of each population that is immigrant is higher in Canada.\n\nA study comparing aboriginal mortality rates in Canada, the U.S. and New Zealand found that aboriginals in all three countries had greater mortality rates and shorter life expectancies than the white majorities. That study also found that aboriginals in Canada had both shorter life expectancies and greater infant mortality rates than aboriginals in the United States and New Zealand. The health outcome differences between aboriginals and whites in Canada was also larger than in the United States.\n\nThough few studies have been published concerning the health of Black Canadians, health disparities between whites and African Americans in the U.S. have received intense scrutiny. African Americans in the U.S. have significantly greater rates of cancer incidence and mortality. Drs. Singh and Yu found that neonatal and postnatal mortality rates for American African Americans are more than double the non-Hispanic white rate. This difference persisted even after controlling for household income and was greatest in the highest income quintile. A Canadian study also found differences in neonatal mortality between different racial and ethnic groups. Although Canadians of African descent had a greater mortality rate than whites in that study, the rate was somewhat less than double the white rate.\n\nThe racially heterogeneous Hispanic population in the U.S. has also been the subject of several studies. Although members of this group are significantly more likely to live in poverty than are non-Hispanic whites, they often have disease rates that are comparable to or better than the non-Hispanic white majority. Hispanics have lower cancer incidence and mortality, lower infant mortality, and lower rates of neural tube defects. Singh and Yu found that infant mortality among Hispanic sub-groups varied with the racial composition of that group. The mostly white Cuban population had a neonatal mortality rate (NMR) nearly identical to that found in non-Hispanic whites and a postnatal mortality rate (PMR) that was somewhat lower. The largely Mestizo, Mexican, Central, and South American Hispanic populations had somewhat lower NMR and PMR. The Puerto Ricans who have a mix of white and African ancestry had higher NMR and PMR rates.\n\nIn 2002, automotive companies claimed that the universal system in Canada saved labour costs. In 2004, healthcare cost General Motors $5.8 billion, and increased to $7 billion. The UAW also claimed that the resulting escalating healthcare premiums reduced workers' bargaining powers.\n\nIn Canada, increasing demands for healthcare, due to the aging population, must be met by either increasing taxes or reducing other government programs. In the United States, under the current system, more of the burden will be taken up by the private sector and individuals.\n\nSince 1998, Canada's successive multibillion-dollar budget surpluses have allowed a significant injection of new funding to the healthcare system, with the stated goal of reducing waiting times for treatment. However, this may be hampered by the return to deficit spending as of the 2009 Canadian federal budget.\n\nOne historical problem with the U.S. system was known as job lock, in which people become tied to their jobs for fear of losing their health insurance. This reduces the flexibility of the labor market. Federal legislation passed since the mid-1980s, particularly COBRA and HIPAA, has been aimed at reducing job lock. However, providers of group health insurance in many states are permitted to use experience rating and it remains legal in the United States for prospective employers to investigate a job candidate's health and past health claims as part of a hiring decision. Someone who has recently been diagnosed with cancer, for example, may face job lock not out of fear of losing their health insurance, but based on prospective employers not wanting to add the cost of treating that illness to their own health insurance pool, for fear of future insurance rate increases. Thus, being diagnosed with an illness can cause someone to be forced to stay in their current job.\n\nMore imaginative solutions in both countries have come from the sub-national level.\n\nIn Canada, the right-wing and now defunct Reform Party and its successor, the Conservative Party of Canada considered increasing the role of the private sector in the Canadian system. Public backlash caused these plans to be abandoned, and the Conservative government that followed re-affirmed its commitment to universal public medicine.\n\nIn Canada, it was Alberta under the Conservative government that had experimented most with increasing the role of the private sector in healthcare. Measures included the introduction of private clinics allowed to bill patients for some of the cost of a procedure, as well as 'boutique' clinics offering tailored personal care for a fixed preliminary annual fee.\n\nIn the U.S., President Bill Clinton attempted a significant restructuring of health care, but the effort collapsed under political pressure against it despite tremendous public support. The 2000 U.S. election saw prescription drugs become a central issue, although the system did not fundamentally change. In the 2004 U.S. election healthcare proved to be an important issue to some voters, though not a primary one.\n\nIn 2006, Massachusetts adopted a plan that vastly reduced the number of uninsured making it the state with the lowest percentage of non-insured residents in the union. It requires everyone to buy insurance and subsidizes insurance costs for lower income people on a sliding scale. Some have claimed that the state's program is unaffordable, which the state itself says is \"a commonly repeated myth\". In 2009, in a minor amendment, the plan did eliminate dental, hospice and skilled nursing care for certain categories of noncitizens covering 30,000 people (victims of human trafficking and domestic violence, applicants for asylum and refugees) who do pay taxes.\n\nIn July 2009, Connecticut passed into law a plan called SustiNet, with the goal of achieving health care coverage of 98% of its residents by 2014.\n\nUS President Donald Trump has declared his intent to repeal the Affordable Care Act, but has failed to do so, thus far.\n\nThe Canada Health Act of 1984 \"does not directly bar private delivery or private insurance for publicly insured services,\" but provides financial disincentives for doing so. \"Although there are laws prohibiting or curtailing private health care in some provinces, they can be changed,\" according to a report in the New England Journal of Medicine. Governments attempt to control health care costs by being the sole purchasers and thus they do not allow private patients to bid up prices. Those with non-emergency illnesses such as cancer cannot pay out of pocket for time-sensitive surgeries and must wait their turn on waiting lists. According to the Canadian Supreme Court in its 2005 ruling in \"Chaoulli v. Quebec\", waiting list delays \"increase the patient's risk of mortality or the risk that his or her injuries will become irreparable.\" The ruling found that a Quebec provincial ban on private health insurance was unlawful, because it was contrary to Quebec's own legislative act, the 1975 Charter of Human Rights and Freedoms.\n\nIn the United States, Congress has enacted laws to promote consumer-driven healthcare with health savings accounts (HSAs), which were created by the Medicare bill signed by President George W. Bush on December 8, 2003. HSAs are designed to provide tax incentives for individuals to save for future qualified medical and retiree health expenses. Money placed in such accounts is tax-free. To qualify for HSAs, individuals must carry a high-deductible health plan (HDHP). The higher deductible shifts some of the financial responsibility for health care from insurance providers to the consumer. This shift towards a market-based system with greater individual responsibility increased the differences between the US and Canadian systems.\n\nSome economists who have studied proposals for universal healthcare worry that the consumer driven healthcare movement will reduce the social redistributive effects of insurance that pools high-risk and low-risk people together. This concern was one of the driving factors behind a provision of the Patient Protection and Affordable Care Act, informally known as \"Obamacare\", which limited the types of purchases which could be made with HSA funds. For example, as of January 1, 2011, these funds can no longer be used to buy over-the-counter drugs without a medical prescription.\n\n\n"}
{"id": "12269347", "url": "https://en.wikipedia.org/wiki?curid=12269347", "title": "Dead checking", "text": "Dead checking\n\nDead checking is U.S. military jargon for the practice of verifying the death of Iraqi insurgents and the subsequent killing of those who remain alive when U.S. Armed Forces enter an insurgent house in hot battle as part of Operation Iraqi Freedom.\n\nThe term was in use as early as November 2004 when reporter Evan Wright of The Village Voice quoted an unnamed enlisted U.S. Marine and Iraq war veteran as saying, \"They teach us to do dead-checking when we're clearing rooms. You put two bullets into the guy's chest and one in the brain. But when you enter a room where guys are wounded you might not know if they're alive or dead. So they teach us to dead-check them by pressing them in the eye with your boot, because generally a person, even if he's faking being dead, will flinch if you poke him there. If he moves, you put a bullet in the brain. You do this to keep the momentum going when you're flowing through a building. You don't want a guy popping up behind you and shooting you.\" \n\nThe term was used again by the Associated Press in July 2007, when Corporal Saul H. Lopezromo, a defense witness in the murder trial of Corporal Trent D. Thomas testified that the procedure of dead checking was routine and stated, \"I don't see it as an execution, sir, I see it as killing the enemy.\" Lopezromo later added, \"If somebody is worth shooting once, they're worth shooting twice.\"\n\nThe Los Angeles Times in July 2007 reported that Corporal Lopezromo testified, \"Marines are taught dead-checking in boot camp, the School of Infantry at Camp Pendleton, and the pre-deployment training at Twentynine Palms called Mojave Viper.\"\n"}
{"id": "9851256", "url": "https://en.wikipedia.org/wiki?curid=9851256", "title": "Diego Peretti", "text": "Diego Peretti\n\nDiego Peretti (born 25 February 1963) is an Argentine actor, screenwriter and former psychiatrist.\n\nPeretti was born in Buenos Aires, and practiced as a psychiatrist for fourteen years. He took part in several movies and TV series, including the 2004 romantic comedy \"No sos vos, soy yo\", and \"Los Simuladores\" a popular TV series aired on Telefé. He starred opposite Carolina Peleritti in \"¿Quién dice que es fácil?\", and, drawing on his background in psychiatry for a number of roles, has starred in a number of crime dramas, including \"El fondo del mar\" (\"The Bottom of the Ocean\", 2004), \"Tiempo de valientes\" (\"A Time for Valor\", 2005), and \"La Señal\" (\"The Signal\", 2007).\n\nIn 2012, he is starring \"En terapia\", the local adaptation of In Treatment aired on TV Pública, the public channel managed by the National Government.\n\nCinema\n\nTV series\n\n\n"}
{"id": "51533648", "url": "https://en.wikipedia.org/wiki?curid=51533648", "title": "Disability in the Middle Ages", "text": "Disability in the Middle Ages\n\nDisability is poorly documented in the Middle Ages, though disabled people constituted a large part of Medieval society as part of the peasantry, clergy, and nobility. Very little was written or recorded about a general disabled community at the time, but their existence has been preserved through religious texts and some medical journals.\n\nThe disabled community were a definite part of Medieval society. Disability was not considered an extraordinary quality among the medieval people and therefore were not heavily documented. Disability as a category of impairment was not seen in Medieval language, but rather terms such as \"blynde\", \"dumbe\", and \"lame\" were seen to attribute those with physical impairments. The idea of disability being undesirable or unholy stemmed from the later Eugenics movement that began in the early 20th Century. Many scholars, such as Henri-Jacques Stiker, author of A History of Disability, would argue that people living with disabilities \"were no less undistinguished at the dawn of the Middle Ages from the economically weak.\"\n\nDue to the intensive labor that constituted agriculture during this time period, many peasants and serfs have been found with extensive spinal and limb injuries, as well as stunted growth, malnutrition and general deformity.\n\nDisabled people were found among all parts of society. Monarchs across Europe were noted as having those with short stature, hunchbacks, or others with disabilities in their courts where they filled roles such as that of the King's Fool or court jester. This rank gave the disabled person a level of prestige. They were allowed to mock or tell the truth to the ruler, even if it displeased them to hear it.\n\nChristianity, the dominant religion in western Europe, held mixed views on disability. Within the Bible, disability was aligned with sin and punishment, but also with healing and martyrdom.\n\nSome Medieval priests and scholars believed that a body would be corrupted by sin and therefore divine punishment took the form of physical illnesses. However, opposing schools of thought revolved around the concept that those with disabilities showed a higher form of piety than those who did not have physical impairments.\n\nIn the Old Testament, God gave people disabilities as a form of divine punishment, with the root of it being a payment for the sins they have committed.\n\nIn the New Testament, there is much more content concerning healing and the miracles performed by Jesus Christ.\n\nCanon Law, the legal codes that the Catholic Church followed, contained few restrictions against those with disabilities, such as the barring of physically disabled from becoming holy men. However, there were contradictory laws within the codes, allowing for those who were disabled after reaching priesthood to be able to move up the hierarchy and become bishops.\n\nThe Fourth Lateran Council of 1215 enacted a law that linked bodily infirmity to \"sometimes\" be caused by sin. Therefore, Medieval physicians were asked to hear their patients confessions so that their soul would be \"cured\" before the body was assessed.\n\nAmong the monasteries and churches of Medieval Europe, charity was often given to those seen as disadvantaged or impaired. The disabled community were among the largest of groups to have benefited from the charity of local monks and priests.\n\nFrance's Louis IX (Saint Louis) granted blind people a rare legal right to beg on the streets of Paris.\n\nWithin the medical world of the Middle Ages, illness and disability were causally linked to sin. Since religion played a large role within Medieval society, many of the changes and deformities to the human body were attributed to one's sins, dating back to Original Sin. There were mixed reactions and perspectives of people with disabilities, because different groups of Christians viewed disabilities in different ways.\n\nGenerally, Medieval physicians attributed much of their work to a combination of the ability to treat illness, such as fever and blisters, and religious faith. If an illness or physical impairment did not subside over time, it was considered an \"incurable illness\" and therefore was deemed as an act of God.\n\nMental disorders were often classified under demonic possession, as they were not within the physician's ability to diagnose or treat at the time and therefore patients with mental disorders were left to pray for divine healing. [citation needed]\n\nWithin the High Middle Ages, an almshouse in the City of London began caring for people with mental disabilities, and Bethlem hospital became the first mental institution in Europe.\n\nLepers offered one of the most familiar images of disability in the medieval period. A disease that affected many throughout medieval Europe, leprosy was caused by a combination of poor hygiene and lack of resources such as proper treatments for the disease. At the time, there were mixed feelings about this group. Some, such as Francis of Assisi, argued that lepers were those who transformed themselves into the images of Jesus Christ and were to be treated as living symbol for his martyrdom. Others, especially after the Bubonic Plague began to ravage Europe towards the Late Medieval period, condemned the lepers as sinful and having been the very people to spread the plague. To stop the spread of the horrifying disease, officials put individuals displaying symptoms and sometimes family members into leper houses. They were often in secluded locations and fashioned after monasteries.\n\nA few notable cases of glass delusion occurred during the Late Middle Ages. This mental disorder involved a person believing that they were made of glass and that they were fragile, able to break or shatter upon impact.\n\nMelancholia was believed to be caused by an imbalance of the Four humours within the human body, where an excess of black bile caused depression-like symptoms of sadness and sluggish, lethargic behavior.\n\nThough disability was present throughout the Middle Ages, very few cases were documented during the Early and High Medieval periods, as few physicians could properly diagnose many conditions.\n\n"}
{"id": "46646396", "url": "https://en.wikipedia.org/wiki?curid=46646396", "title": "Effects of global warming on humans", "text": "Effects of global warming on humans\n\nClimate change has brought about possibly permanent alterations to Earth's geological, biological and ecological systems. These changes have led to the emergence of a not so large-scale environmental hazards to human health, such as extreme weather, ozone depletion, increased danger of wildland fires, loss of biodiversity, stresses to food-producing systems and the global spread of infectious diseases. In addition, climatic changes are estimated to cause over 150,000 deaths annually.\n\nTo date, a neglected aspect of the climate change debate, much less research has been conducted on the impacts of climate change on health, food supply, economic growth, migration, security, societal change, and public goods, such as drinking water, than on the geophysical changes related to global warming. Human impacts can be both negative and positive. Climatic changes in Siberia, for instance, are expected to improve food production and local economic activity, at least in the short to medium term. Whereas, Bangladesh has experienced an increase in climate-sensitive diseases such as malaria, dengue, childhood diarrhoea, and pneumonia, among vulnerable communities. Numerous studies suggest, however, that the current and future impacts of climate change on human society are and will continue to be overwhelmingly negative.\n\nThe majority of the adverse effects of climate change are experienced by poor and low-income communities around the world, who have much higher levels of vulnerability to environmental determinants of health, wealth and other factors, and much lower levels of capacity available for coping with environmental change. A report on the global human impact of climate change published by the Global Humanitarian Forum in 2009, estimated more than 300,000 deaths and about $125 billion in economic losses each year, and indicating that most climate change induced mortality is due to worsening floods and droughts in developing countries.\n\nMost of the key vulnerabilities to climate change are related to climate phenomena that exceed thresholds for adaptation; such as extreme weather events or abrupt climate change, as well as limited access to resources (financial, technical, human, institutional) to cope. In 2007, the IPCC published a report of key vulnerabilities of industry, settlements, and society to climate change. This assessment included a level of confidence for each key vulnerability:\n\nClimate change poses a wide range of risks to population health – risks that will increase in future decades, often to critical levels, if global climate change continues on its current trajectory. The three main categories of health risks include: (i) direct-acting effects (e.g. due to heat waves, amplified air pollution, and physical weather disasters), (ii) impacts mediated via climate-related changes in ecological systems and relationships (e.g. crop yields, mosquito ecology, marine productivity), and (iii) the more diffuse (indirect) consequences relating to impoverishment, displacement, resource conflicts (e.g. water), and post-disaster mental health problems.\n\nClimate change thus threatens to slow, halt or reverse international progress towards reducing child under-nutrition, deaths from diarrheal diseases and the spread of other infectious diseases. Climate change acts predominantly by exacerbating the existing, often enormous, health problems, especially in the poorer parts of the world. Current variations in weather conditions already have many adverse impacts on the health of poor people in developing nations, and these too are likely to be 'multiplied' by the added stresses of climate change.\n\nA changing climate thus affects the prerequisites of population health: clean air and water, sufficient food, natural constraints on infectious disease agents, and the adequacy and security of shelter. A warmer and more variable climate leads to higher levels of some air pollutants. It increases the rates and ranges of transmission of infectious diseases through unclean water and contaminated food, and by affecting vector organisms (such as mosquitoes) and intermediate or reservoir host species that harbour the infectious agent (such as cattle, bats and rodents). Changes in temperature, rainfall and seasonality compromise agricultural production in many regions, including some of the least developed countries, thus jeopardising child health and growth and the overall health and functional capacity of adults. As warming proceeds, the severity (and perhaps frequency) of weather-related disasters will increase – and appears to have done so in a number of regions of the world over the past several decades. Therefore, in summary, global warming, together with resultant changes in food and water supplies, can indirectly cause increases in a range of adverse health outcomes, including malnutrition, diarrhea, injuries, cardiovascular and respiratory diseases, and water-borne and insect-transmitted diseases.\n\nHealth equity and climate change have a major impact on human health and quality of life, and are interlinked in a number of ways. The report of the WHO Commission on Social Determinants of Health points out that disadvantaged communities are likely to shoulder a disproportionate share of the burden of climate change because of their increased exposure and vulnerability to health threats. Over 90 percent of malaria and diarrhea deaths are borne by children aged 5 years or younger, mostly in developing countries. Other severely affected population groups include women, the elderly and people living in small island developing states and other coastal regions, mega-cities or mountainous areas.\n\nA 2011 article in the \"American Psychologist\" identified three classes of psychological impacts from global climate change:\n\nThis trend towards more variability and fluctuation is perhaps more important, in terms of its impact on human health, than that of a gradual and long-term trend towards higher average temperature. Infectious disease often accompanies extreme weather events, such as floods, earthquakes and drought. These local epidemics occur due to loss of infrastructure, such as hospitals and sanitation services, but also because of changes in local ecology and environment.\n\nClimate change may lead to dramatic increases in prevalence of a variety of infectious diseases. Beginning in the mid-'70s, there has been an “emergence, resurgence and redistribution of infectious diseases”. Reasons for this are likely multi-causal, dependent on a variety of social, environmental and climatic factors, however, many argue that the “volatility of infectious disease may be one of the earliest biological expressions of climate instability”. Though many infectious diseases are affected by changes in climate, vector-borne diseases, such as malaria, dengue fever and leishmaniasis, present the strongest causal relationship. One major reason that change in climate increases the prevalence of vector borne disease is that temperature and rainfall play a key role in the distribution, magnitude, and viral capacity of mosquitoes, who are primary vectors for many vector borne diseases. Observation and research detect a shift of pests and pathogens in the distribution away from the equator and towards Earth's poles. A tool that has been used to predict this distribution trend is the Dynamic Mosquito Simulation Process (DyMSiM). DyMSiM uses epidemiological and entomological data and practices to model future mosquito distributions based upon climate conditions and mosquitos living in the area. This modeling technique helps identify the distribution of specific species of mosquito, some of which are more susceptible to viral infection than others.\n\nBeyond distribution, rising temperatures can decrease viral incubation time \"in vivo\" in vectors increasing the viral transmissibility leading to increases in infection rates.\n\nIncreased precipitation like rain could increase the number of mosquitos indirectly by expanding larval habitat and food supply. Malaria kills approximately 300,000 children (under age 5) annually, poses an imminent threat through temperature increase . Models suggest, conservatively, that risk of malaria will increase 5-15% by 2100 due to climate change. In Africa alone, according to the MARA Project (Mapping Malaria Risk in Africa), there is a projected increase of 16–28% in person-month exposures to malaria by 2100.\n\nDengue \n\nThere are 4 distinct viruses responsible for Dengue: DENV-1, DENV-2, DENV-3, and DENV-4. Dengue fever is spread by the bite of the female mosquito known as \"Aedes aegypti.\" This species of mosquito can travel up to 400 meters in search of water to lay their eggs, but often remain closer to human habitation. A mosquito becomes infected with dengue when it bites and takes the blood of an infected human. After approximately one week, the mosquito can then transmit the dengue infection to other humans through her bite. While dengue cannot be spread from person to person, an infected person can infect more mosquitos, thus, furthering the spread of the disease. Overall, the female mosquito is a highly effective vector of this disease. \n\nWhen bitten by an infected mosquito, dengue has an incubation period of 4-10 days. Once infected with the dengue virus, humans experience severe flu-like symptoms. Also known as \"break-bone fever\", dengue can affect infants, children, and adults and can be fatal. Those infected exhibit a high fever (40°C/ 104°F) along with at least two of the following symptoms: severe headache, pain behind the eye, nausea, vomiting, swollen glands, muscle and joint pains, and rash. These symptoms usually last 2-7 days. Dengue can become fatal due to plasma leaking, fluid accumulation, respiratory distress, severe bleeding, or organ impairment. Warning signs of this include a decrease in temperature decrease (below 38°C/ 100°F) in conjunction with: severe abdominal pain, persistent vomiting, rapid breathing, bleeding gums, blood in vomit, and/or fatigue and restlessness.\n\nWhere the mosquito, \"Aedes aegypti\", lives and the amount of mosquitos present is strongly influenced by the amount of water-bearing containers or pockets of standstill water in an area, daily temperature and variation in temperature, moisture, and solar radiation. While dengue fever is primarily considered a tropical and subtropical disease, the geographic ranges of the aedes aegypti are expanding. Globalization, trade, travel, demographic trends, and warming temperatures are all attributed to the recent spread to this primary vector of dengue. \n\nDengue is now ranked as the most important vector-borne viral disease in the world. Today, an estimated 50–100 million dengue fever infections occur annually.  In just the past 50 years, transmission has increased drastically with new cases of the disease (incidence) increasing 30-fold. Once localized to a few areas in the tropics, dengue fever is now endemic in over 100 countries in Southeast Asia, the Americas, Africa, the Eastern Mediterranean, and the Western Pacific with Southeast Asia and the Western Pacific regions being the most seriously affected. Recently the number of reported cases has continually increased along with dengue spreading to new areas. Explosive outbreaks are also occurring. Moreover, there is the possible threat of outbreak in Europe with local transmission of dengue being reported for the first time in France and Croatia in 2010.\n\nOne country that has seen significant impacts from dengue is Bangladesh. Dengue has been endemic in Bangladesh since its first major outbreak in 2000. With its high population, shifting weather patterns, and low and flat geography that is only one meter above sea level, Bangladesh is also one of the world’s most vulnerable countries when it comes to climate change Climate change is predicted to increase temperatures and precipitation, both of which affect dengue transmission, as dengue is weather dependent, and most often occurs in wetter and warmer climates. Standing water allows habitats and breeding grounds for the mosquito vector, while warmer temperatures assist in larval development, replication of the virus, and period of infectivity. Studies have found lag effects of, on average, two months between high temperatures and dengue transmission, indicating the time that has lapsed between observed weather changes and new observed dengue cases.\n\nDhaka is Bangladesh's biggest city, and also the highest risk area in Bangladesh for transmission of dengue, with its topical climate and population of approximately 11.8 million people. The annual average temperature in Dhaka is 25°C and almost all of the average rainfall occurs during May through September. If higher temperature and increased precipitation continue, we could see temperatures increase and the rainy season extended, leading to an increased transmission period for dengue. \n\nThere were 25,059 cases of dengue in Dhaka from 2000 to 2010, with an average of 168 cases a month. While dengue testing is frequently performed in the private health care setting; it is frequently underperformed in the public health care setting, due to lack of testing accessibility. This indicates that there are potentially more cases of dengue than are getting diagnosed or reported.\n\nDengue incidence has only increased in the last few decades, and is projected to continue to do so with changing climate conditions. There have been prediction models of temperature created to project the effects of global warming on the planet. Based on these, the Intergovernmental Panel on Climate Change estimates that the mean annual temperature of Southeast Asia will have increased by 3.3 degrees Celsius by 2100, assuming no other changes.. Taking this estimate, researchers predict an increase of 16, 030 cases in Dhaka, Bangladesh by the year 2100. This represents a 40-times increase in dengue incidence.\n\nIncreased public health surveillance and preparation is needed in areas like Bangladesh that are seeing an upward trend in climatic changes and vector-borne disease like dengue virus. \n\nSociodemographic factors include, but are not limited to: patterns of human migration and travel, effectiveness of public health and medical infrastructure in controlling and treating disease, the extent of anti-malarial drug resistance and the underlying health status of the population at hand. Environmental factors include: changes in land-use (e.g. deforestation), expansion of agricultural and water development projects (which tend to increase mosquito breeding habitat), and the overall trend towards urbanization (i.e. increased concentration of human hosts). Patz and Olson argue that these changes in landscape can alter local weather more than long term climate change. For example, the deforestation and cultivation of natural swamps in the African highlands has created conditions favourable for the survival of mosquito larvae, and has, in part, led to the increasing incidence of malaria. The effects of these non-climatic factors complicate things and make a direct causal relationship between climate change and malaria difficult to confirm. It is highly unlikely that climate exerts an isolated effect.\n\nPreparing for the Future\n\nEffective policies which take into consideration predictive climate change models and measures are key to preparing for and managing changes in incidence and reestablishment of diseases. As climate change continues to alter where diseases are prevalent, harmonizing surveillance systems on a multi-national scale will be vital to improve evidence-based disease control and decision making. Implementation of vaccination and other prevention measures as well as increasing community education and awareness and education of the impacts of the disease and other adverse health events among public decision makers will help prepare and combat changes in disease rates and location.\n\nClimate change may dramatically impact habitat loss, for example, arid conditions may cause the deforestation of rainforests, as has occurred in the past.\n\nA sustained wet-bulb temperature exceeding 35°C is a threshold at which the resilience of human systems is no longer able to adequately cool the skin. A study by NOAA from 2013 concluded that heat stress will reduce labor capacity considerably under current emissions scenarios. There is evidence to show that high temperatures can increase mortality rates among fetuses and children Although the main focus is often on the health impacts and risks of higher temperatures, it should be remembered that they also reduce learning and worker productivity, which can impact a country's economy and development.\n\nLow Temperature\n\nClimate change contributes to cold snaps due to disruptions in the polar vortex, which in turn is caused by a decline in Arctic sea ice, and will cause frigid, cold air to spill from the Arctic and into areas of the northern hemisphere that usually don't experience such cold temperatures, such as the North American southeast, mid-west, northeast, and parts of Europe. This is a predicted short-term effect of climate change in the winter. This brings along extreme cold temperatures for a short period of time, and results in large scale disruption to human life. A statistic from data on the winter season of 2013-14 found that of the most notable of the winter storms - most of which were caused by the disruption of the polar vortex - caused $263 Million Dollars in damage, 32 fatalities, and 9 injuries. Furthermore, infrastructure damage in the form of closed roads, schools, airports, and other civil functions occurred throughout the northeast, and in some parts of the Midwestern and Southeastern United States. at the JFK International Airport in Chicago in the 2014 cold snap where a commercial airliner skidded off the runway and into a nearby snowbank. The winter season of 2013-2014 also caused some crop damage as shown in Ohio losing 97% of their grape harvest. Further harvests in the following years were also affected as freeze damage reached deep into the trunks of some plants killing off the plant. The total damages extended to roughly $4 Million Dollars, impacting Ohio's economy and wine production. Cold Events are expected to increase in the short term while in the long term the increasing global temperature is going to give way to more heat related events.\n\nThe freshwater resources that humans rely on are highly sensitive to variations in weather and climate. In 2007, the IPCC reported with high confidence that climate change has a net negative impact on water resources and freshwater ecosystems in all regions. The IPCC also found with very high confidence that arid and semi-arid areas are particularly exposed to freshwater impacts.\n\nAs the climate warms, it changes the nature of global rainfall, evaporation, snow, stream flow and other factors that affect water supply and quality. Specific impacts include:\n\nClimate change causes displacement of people in several ways, the most obvious—and dramatic—being through the increased number and severity of weather-related disasters which destroy homes and habitats causing people to seek shelter or livelihoods elsewhere. Effects of climate change such as desertification and rising sea levels gradually erode livelihood and force communities to abandon traditional homelands for more accommodating environments. This is currently happening in areas of Africa’s Sahel, the semi-arid belt that spans the continent just below its northern deserts. Deteriorating environments triggered by climate change can also lead to increased conflict over resources which in turn can displace people.\n\nThe IPCC has estimated that 150 million environmental migrants will exist by the year 2050, due mainly to the effects of coastal flooding, shoreline erosion and agricultural disruption. However, the IPCC also cautions that it is extremely difficult to measure the extent of environmental migration due to the complexity of the issue and a lack of data.\n\nAccording to the Internal Displacement Monitoring Centre, more than 42 million people were displaced in Asia and the Pacific during 2010 and 2011, more than twice the population of Sri Lanka. This figure includes those displaced by storms, floods, and heat and cold waves. Still others were displaced drought and sea-level rise. Most of those compelled to leave their homes eventually returned when conditions improved, but an undetermined number became migrants, usually within their country, but also across national borders.\n\nAsia and the Pacific is the global area most prone to natural disasters, both in terms of the absolute number of disasters and of populations affected. It is highly exposed to climate impacts, and is home to highly vulnerable population groups, who are disproportionately poor and marginalized. A recent Asian Development Bank report highlights “environmental hot spots” that are particular risk of flooding, cyclones, typhoons, and water stress.\n\nSome Pacific Ocean island nations, such as Tuvalu, Kiribati, and the Maldives, are considering the eventual possibility of evacuation, as flood defense may become economically unrealistic. Tuvalu already has an ad hoc agreement with New Zealand to allow phased relocation. However, for some islanders relocation is not an option. They are not willing to leave their homes, land and families. Some simply don’t know the threat that climate change has on their island and this is mainly down to the lack of awareness that climate change even exists. In Vutia on Viti Levu, Fiji’s main island, half the respondents to a survey had not heard of climate change (Lata and Nuun 2012). Even where there is awareness many believe that it is a problem caused by developed countries and should therefore be solved by developed countries.\n\nGovernments have considered various approaches to reduce migration compelled by environmental conditions in at-risk communities, including programs of social protection, livelihoods development, basic urban infrastructure development, and disaster risk management. Some experts even support migration as an appropriate way for people to cope with environmental changes. However, this is controversial because migrants – particularly low-skilled ones – are among the most vulnerable people in society and are often denied basic protections and access to services.\n\nClimate change is only one factor that may contribute to a household's decision to migrate; other factors may include poverty, population growth or employment options. For this reason, it is difficult to classify environmental migrants as actual \"refugees\" as legally defined by the UNHCR. Neither the UN Framework Convention on Climate Change nor its Kyoto Protocol, an international agreement on climate change, includes any provisions concerning specific assistance or protection for those who will be directly affected by climate change.\n\nIn small islands and megadeltas, inundation as a result of sea level rise is expected to threaten vital infrastructure and human settlements. This could lead to issues of statelessness for populations in countries such as the Maldives and Tuvalu and homelessness in countries with low-lying areas such as Bangladesh.\n\nThe World Bank predicts that a “severe hit” will spur conflict and migration across the Middle East, Central Asia, and Africa.\n\nClimate change has the potential to exacerbate existing tensions or create new ones — serving as a threat multiplier. It can be a catalyst for violent conflict and a threat to international security. A meta-analysis of over 50 quantitative studies that examine the link between climate and conflict found that \"for each 1 standard deviation (1σ) change in climate toward warmer temperatures or more extreme rainfall, median estimates indicate that the frequency of interpersonal violence rises 4% and the frequency of intergroup conflict rises 14%\". The IPCC has suggested that the disruption of environmental migration may serve to exacerbate conflicts, though they are less confident of the role of increased resource scarcity. Of course, climate change does not always lead to violence, and conflicts are often caused by multiple interconnected factors.\n\nA variety of experts have warned that climate change may lead to increased conflict. The Military Advisory Board, a panel of retired U.S. generals and admirals, predicted that global warming will serve as a \"threat multiplier\" in already volatile regions. The Center for Strategic and International Studies and the Center for a New American Security, two Washington think tanks, have reported that flooding \"has the potential to challenge regional and even national identities,\" leading to \"armed conflict over resources.\" They indicate that the greatest threat would come from \"large-scale migrations of people — both inside nations and across existing national borders.\" However, other researchers have been more skeptical: One study found no statistically meaningful relationship between climate and conflict using data from Europe between the years 1000 and 2000.\n\nThe link between climate change and security is a concern for authorities across the world, including United Nations Security Council and the G77 group of developing nations. Climate change's impact as a security threat is expected to hit developing nations particularly hard. In Britain, Foreign Secretary Margaret Beckett has argued that \"An unstable climate will exacerbate some of the core drivers of conflict, such as migratory pressures and competition for resources.\"\nThe links between the human impact of climate change and the threat of violence and armed conflict are particularly important because multiple destabilizing conditions are affected simultaneously.\n\nExperts have suggested links to climate change in several major conflicts:\n\n\nAdditionally, researchers studying ancient climate patterns (paleoclimatology) have shown that long-term fluctuations of war frequency and population changes have followed cycles of temperature change since the preindustrial era. A 2016 study finds that \"drought can contribute to sustaining conflict, especially for agriculturally dependent groups and politically excluded groups in very poor countries. These results suggest a reciprocal nature–society interaction in which violent conflict and environmental shock constitute a vicious circle, each phenomenon increasing the group’s vulnerability to the other.\"\n\nThe consequences of climate change and poverty are not distributed uniformly within communities. Individual and social factors such as gender, age, education, ethnicity, geography and language lead to differential vulnerability and capacity to adapt to the effects of climate change. Climate change effects such as hunger, poverty and diseases like diarrhea and malaria, disproportionately impact children; about 90 percent of malaria and diarrhea deaths are among young children. Children are also 14–44 percent more likely to die from environmental factors, again leaving them the most vulnerable. Those in urban areas will be affected by lower air quality and overcrowding, and will struggle the most to better their situation.\n\nAs the World Meteorological Organization explains, \"recent increase in societal impact from tropical cyclones has largely been caused by rising concentrations of population and infrastructure in coastal regions.\" Pielke \"et al.\" (2008) normalized mainland U.S. hurricane damage from 1900 to 2005 to 2005 values and found no remaining trend of increasing absolute damage. The 1970s and 1980s were notable because of the extremely low amounts of damage compared to other decades. The decade 1996–2005 has the second most damage among the past 11 decades, with only the decade 1926–1935 surpassing its costs. The most damaging single storm is the 1926 Miami hurricane, with $157 billion of normalized damage.\n\nThe American \"Insurance Journal\" predicted that \"catastrophe losses should be expected to double roughly every 10 years because of increases in construction costs, increases in the number of structures and changes in their characteristics.\" The Association of British Insurers has stated that limiting carbon emissions would avoid 80% of the projected additional annual cost of tropical cyclones by the 2080s. The cost is also increasing partly because of building in exposed areas such as coasts and floodplains. The ABI claims that reduction of the vulnerability to some inevitable effects of climate change, for example through more resilient buildings and improved flood defences, could also result in considerable cost-savings in the longterm.\n\nA major challenge for human settlements is sea level rise, indicated by ongoing observation and research of rapid declines in ice-mass balance from both Greenland and Antarctica. Estimates for 2100 are at least twice as large as previously estimated by IPCC AR4, with an upper limit of about two meters. Depending on regional changes, increased precipitation patterns can cause more flooding or extended drought stresses water resources.\n\nFor historical reasons to do with trade, many of the world's largest and most prosperous cities are on the coast. In developing countries, the poorest often live on floodplains, because it is the only available space, or fertile agricultural land. These settlements often lack infrastructure such as dykes and early warning systems. Poorer communities also tend to lack the insurance, savings, or access to credit needed to recover from disasters.\n\nIn a journal paper, Nicholls and Tol (2006) considered the effects of sea level rise:\n\nThe most vulnerable future worlds to sea-level rise appear to be the A2 and B2 [IPCC] scenarios, which primarily reflects differences in the socio-economic situation (coastal population, Gross Domestic Product (GDP) and GDP/capita), rather than the magnitude of sea-level rise. Small islands and deltaic settings stand out as being more vulnerable as shown in many earlier analyses. Collectively, these results suggest that human societies will have more choice in how they respond to sea-level rise than is often assumed. However, this conclusion needs to be tempered by recognition that we still do not understand these choices and significant impacts remain possible.\n\nThe IPCC reported that socioeconomic impacts of climate change in coastal and low-lying areas would be overwhelmingly adverse. The following impacts were projected with very high confidence:\n\nA study in the April 2007 issue of \"Environment and Urbanization\" reports that 634 million people live in coastal areas within 30 feet (9.1 m) of sea level. The study also reported that about two thirds of the world's cities with over five million people are located in these low-lying coastal areas.\n\nOil and natural gas infrastructure is vulnerable to the effects of climate change and the increased risk of disasters such as storm, cyclones, flooding and long-term increases in sea level. Minimising these risks by building in less disaster prone areas, can be expensive and impossible in countries with coastal locations or island states. All thermal power stations depend on water to cool them. Not only is there increased demand for fresh water, but climate change can increase the likelihood of drought and fresh water shortages. Another impact for thermal power plants, is that increasing the temperatures in which they operate reduces their efficiency and hence their output. The source of oil often comes from areas prone to high natural disaster risks; such as tropical storms, hurricanes, cyclones, and floods. An example is Hurricane Katrina's impact on oil extraction in the Gulf of Mexico, as it destroyed 126 oil and gas platforms and damaged 183 more.\n\nHowever, previously pristine arctic areas will now be available for resource extraction\n\nClimate change, along with extreme weather and natural disasters can affect nuclear power plants in a similar way to those using oil, coal, and natural gas. However, the impact of water shortages on nuclear power plants cooled by rivers will be greater than on other thermal power plants. This is because old reactor designs with water-cooled cores must run at lower internal temperatures and thus, paradoxically, must dump more heat to the environment to produce a given amount of electricity. This situation has forced some nuclear reactors to be shut down and will do so again unless the cooling systems of these plants are enhanced to provide more capacity. Nuclear power supply was diminished by low river ﬂow rates and droughts, which meant rivers had reached the maximum temperatures for cooling. Such shutdowns happened in France during the 2003 and 2006 heat waves. During the heat waves, 17 reactors had to limit output or shut down. 77% of French electricity is produced by nuclear power; and in 2009 a similar situation created a 8GW shortage, and forced the French government to import electricity. Other Cases have been reported from Germany, where extreme temperatures have reduced nuclear power production 9 times due to high temperatures between 1979 and 2007. In particular: \nSimilar events have happened elsewhere in Europe during those same hot summers. Many scientists agree that if global warming continues, this disruption is likely to increase.\n\nChanges in the amount of river flow will correlate with the amount of energy produced by a dam. Lower river flows because of drought, climate change, or upstream dams and diversions, will reduce the amount of live storage in a reservoir; therefore reducing the amount of water that can be used for hydroelectricity. The result of diminished river flow can be a power shortage in areas that depend heavily on hydroelectric power. The risk of flow shortage may increase as a result of climate change. Studies from the Colorado River in the United States suggests that modest climate changes (such as a 2 degree change in Celsius that could result in a 10% decline in precipitation), might reduce river run-off by up to 40%. Brazil in particular, is vulnerable due to its having reliance on hydroelectricity as increasing temperatures, lower water ﬂow, and alterations in the rainfall regime, could reduce total energy production by 7% annually by the end of the century.\n\nThe scientific evidence for links between global warming and the increasing cost of natural disasters due to weather events is weak, but, nevertheless, prominent mainstream environmental spokesmen such as Barack Obama and Al Gore have emphasized the possible connection. For the most part increased costs due to events such as Hurricane Sandy are due to increased exposure to loss resulting from building insured facilities in vulnerable locations. This information has been denounced by Paul Krugman and ThinkProgress as climate change denial.\n\nAn industry directly affected by the risks of climate change is the insurance industry. According to a 2005 report from the Association of British Insurers, limiting carbon emissions could avoid 80% of the projected additional annual cost of tropical cyclones by the 2080s. A June 2004 report by the Association of British Insurers declared \"Climate change is not a remote issue for future generations to deal with; it is, in various forms here already, impacting on insurers' businesses now.\" The report noted that weather-related risks for households and property were already increasing by 2–4% per year due to the changing weather conditions, and claims for storm and flood damages in the UK had doubled to over £6 billion over the period from 1998–2003 compared to the previous five years. The results are rising insurance premiums, and the risk that in some areas flood insurance will become unaffordable for those in the lower income brackets.\n\nFinancial institutions, including the world's two largest insurance companies: Munich Re and Swiss Re, warned in a 2002 study that \"the increasing frequency of severe climatic events, coupled with social trends could cost almost 150 billion US$ each year in the next decade\". These costs would burden customers, taxpayers, and the insurance industry, with increased costs related to insurance and disaster relief.\n\nIn the United States, insurance losses have also greatly increased. It has been shown that a 1% climb in annual precipitation can increase catastrophe loss by as much as 2.8%. Gross increases are mostly attributed to increased population and property values in vulnerable coastal areas; though there was also an increase in frequency of weather-related events like heavy rainfalls since the 1950s.\n\nRoads, airport runways, railway lines and pipelines, (including oil pipelines, sewers, water mains etc.) may require increased maintenance and renewal as they become subject to greater temperature variation. Regions already adversely affected include areas of permafrost, which are subject to high levels of subsidence, resulting in buckling roads, sunken foundations, and severely cracked runways.\n\n\n\n"}
{"id": "1644698", "url": "https://en.wikipedia.org/wiki?curid=1644698", "title": "Exercise intolerance", "text": "Exercise intolerance\n\nExercise intolerance is a condition of inability or decreased ability to perform physical exercise at what would be considered to be the normally expected level or duration. It also includes experiences of unusually severe post-exercise pain, fatigue, nausea, vomiting or other negative effects. Exercise intolerance is not a disease or syndrome in and of itself, but can result from various disorders.\n\nIn most cases, the specific reason that exercise is not tolerated is of considerable significance when trying to isolate the cause down to a specific disease. Dysfunctions involving the pulmonary, cardiovascular or neuromuscular systems have been frequently found to be associated with exercise intolerance, with behavioural causes also playing a part.\n\n\n\n\n\n\n\nCytochrome b mutations can frequently cause isolated exercise intolerance and myopathy and in some cases multisystem disorders. The mitochondrial respiratory chain complex III catalyses electron transfer to cytochrome c. Complex III is embedded in the inner membrane of the mitochondria and consists of 11 subunits. Cytochrome b is encoded by the mitochondrial DNA which differs from all other subunits which are encoded in the nucleus. Cytochrome b plays a major part in the correct fabricating and function of complex III.\n\nThis mutation occurred in an 18-year-old man who had experienced exercise intolerance for most of his adolescence. Symptoms included extreme fatigue, nausea, a decline in physical activity ability and myalgia.\n\nIndividuals with elevated levels of cerebrospinal fluid can experience increased head pain, throbbing, pulsatile tinnitus, nausea and vomiting, faintness and weakness and even loss of consciousness after exercise or exertion.\n\nExercise is key for many heart and back patients, and a variety of specific exercise techniques are available for both groups. Some exercise specialists are trained in modifications specific to these patients.\n\nIn individuals with heart failure and normal EF (ejection fraction), including aortic distensibility, blood pressure, LV diastolic compliance and skeletal muscle function, aerobic exercise has the potential to improve exercise tolerance. A variety of pharmacological interventions such as verapamil, enalapril, angiotensin receptor antagonism, and aldosterone antagonism could potentially improve exercise tolerance in these individuals as well.\n\nResearch on individuals suffering from Chronic obstructive pulmonary disease (COPD), has found a number of effective therapies in relation to exercise intolerance. These include:\nA combination of these therapies (Combined therapies), have shown the potential to improve exercise tolerance as well.\n\nCertain conditions exist where exercise may be contraindicated or should be performed under the direction of an experienced and licensed medical professional acting within his or her scope of practice. These conditions include:\n\n\nThe above list does not include all potential contraindications or precautions to exercise.\nAlthough it has not been shown to promote improved muscle strength, passive range-of-motion exercise is sometimes used to prevent skin breakdown and prevent contractures in patients unable to safely self-power.\n"}
{"id": "56682199", "url": "https://en.wikipedia.org/wiki?curid=56682199", "title": "Genetics of GnRH deficiency conditions", "text": "Genetics of GnRH deficiency conditions\n\nTo date at least twenty five different genes have been implicated in causing GnRH deficiency conditions such Kallmann syndrome (KS) or other forms of hypogonadotropic hypogonadism through a disruption in the production or activity of GnRH. These genes involved cover all forms of inheritance and no one gene defect has been shown to be common to all cases which makes genetic testing and inheritance prediction difficult.\n\nThe number of genes known to cause cases of KS / CHH is still increasing. In addition it is thought that some cases of KS / CHH are caused by two separate gene defects occurring at the same time.\n\nA table of known genes responsible for cases of GnRH deficiency conditions is shown below. Listed are the estimated prevalence of cases caused by the specific gene, additional associated symptoms and the form of inheritance. Between 35-45% of cases of KS / CHH have an unknown genetic cause.\n\n"}
{"id": "57864207", "url": "https://en.wikipedia.org/wiki?curid=57864207", "title": "Holyoke Water Works", "text": "Holyoke Water Works\n\nThe Holyoke Water Works (HWW), sometimes referred to as the Holyoke Reservoir System, is a public drinking water utility and municipal service agency of the City of , which provides clean drinking water to that city. Founded in 1872 by an act of , the system was developed as a series of reservoirs to serve the growing city's residents and industry at the end of the 19th century. Today its primary drinking water source is the Tighe-Carmody Reservoir in Southampton, Massachusetts, while it maintains reserve drinking water supplies at the Ashley and the Whiting Street Reservoirs.\n\nThe Water Works is entirely responsible for drinking water supplies, infrastructure, and watershed land conservancy, however it does not maintain sewage or stormwater treatment infrastructure or services, which fall under the responsibilities Department of Public Works. \n\n"}
{"id": "407814", "url": "https://en.wikipedia.org/wiki?curid=407814", "title": "Hygiene hypothesis", "text": "Hygiene hypothesis\n\nIn medicine, the hygiene hypothesis states a lack of early childhood exposure to infectious agents, symbiotic microorganisms (such as the gut flora or probiotics), and parasites increases susceptibility to allergic diseases by suppressing the natural development of the immune system. In particular, the lack of exposure is thought to lead to defects in the establishment of immune tolerance.\n\nThe hygiene hypothesis has also been called the \"biome depletion theory\" and the \"lost friends theory\".\n\nThe original formulation of the hygiene hypothesis dates from 1989 when David Strachan proposed that lower incidence of infection in early childhood could be an explanation for the rapid 20th century rise in allergic diseases such as asthma and hay fever.\n\nIt is now also recognised that the \"reduced microbial exposure\" concept applies to a much broader range of chronic inflammatory diseases than asthma and hay fever, which includes diseases such as type 1 diabetes and multiple sclerosis, and also some types of depression and cancer.\n\nIn 2003 Graham Rook proposed the \"old friends hypothesis\" which some claim offers a more rational explanation for the link between microbial exposure and inflammatory disorders. He argues that the vital microbial exposures are not colds, influenza, measles and other common childhood infections which have evolved relatively recently over the last 10,000 years, but rather the microbes already present during mammalian and human evolution, that could persist in small hunter gatherer groups as microbiota, tolerated latent infections or carrier states. He proposes that humans have become so dependent on these \"old friends\" that their immune systems neither develop properly nor function properly without them.\n\nStrachan's original formulation of the hygiene hypothesis also centred around the idea that smaller families provided insufficient microbial exposure partly because of less person-to-person spread of infections, but also because of \"improved household amenities and higher standards of personal cleanliness\". It seems likely that this was the reason he named it the \"hygiene hypothesis\". Although the \"hygiene revolution\" of the nineteenth and twentieth centuries may have been a major factor, it now seems more likely that, although public health measures such as sanitation, potable water and garbage collection were instrumental in reducing our exposure to cholera, typhoid and so on, they also deprived people of their exposure to the \"old friends\" that occupy the same environmental habitats.\n\nThe rise of autoimmune diseases and acute lymphoblastic leukemia in young people in the developed world was linked to the hygiene hypothesis.\n\nSome evidence indicates that autism is correlated to factors (such as certain cytokines) that are indicative of an immune disease. One publication speculated that the lack of early childhood exposure could be a cause of autism.\n\nThe risk of chronic inflammatory diseases also depends on factors such as diet, pollution, physical activity, obesity, socio-economic factors and stress. Genetic predisposition is also a factor.\n\nAlthough the idea that exposure to certain infections may decrease the risk of allergy is not new, Strachan was one of the first to formally propose it, in an article published in the \"British Medical Journal\" in 1989. This article proposed to explain the observation that hay fever and eczema, both allergic diseases, were less common in children from larger families, which were presumably exposed to more infectious agents through their siblings, than in children from families with only one child.\n\nThe hypothesis was extensively investigated by immunologists and epidemiologists and has become an important theoretical framework for the study of chronic inflammatory disorders. It explains the increase in allergic diseases that has been seen since industrialization and the higher incidence of allergic diseases in more developed countries. Epidemiological studies continue to confirm the protective effect of large family size and of growing up on a farm. However, exposure to common childhood infections such as chickenpox or measles is not thought to be protective.\n\nThe \"old friends hypothesis\" proposed in 2003 may offer a better explanation for the link between microbial exposure and inflammatory diseases. This hypothesis argues that the vital exposures are not common childhood and other recently evolved infections, which are no older than 10,000 years, but rather microbes already present in hunter-gatherer times when the human immune system was evolving. Conventional childhood infections are mostly \"crowd infections\" that kill or immunise and thus cannot persist in isolated hunter-gatherer groups. Crowd infections started to appear after the neolithic agricultural revolution, when human populations increased in size and proximity. The microbes that co-evolved with mammalian immune systems are much more ancient. According to this hypothesis, humans became so dependent on them that their immune systems can neither develop nor function properly without them.\n\nRook proposed that these microbes most likely include:\n\n\nThe modified hypothesis later expanded to include exposure to symbiotic bacteria and parasites.\n\n\"Evolution turns the inevitable into a necessity.\" This means that the majority of mammalian evolution took place in mud and rotting vegetation and more than 90 percent of human evolution took place in isolated hunter-gatherer communities and farming communities. Therefore, the human immune systems have evolved to anticipate certain types of microbial input, making the inevitable exposure into a necessity. The organisms that are implicated in the hygiene hypothesis are not proven to cause the disease prevalence, however there are sufficient data on lactobacilli, saprophytic environment mycobacteria, and helminths and their association. These bacteria and parasites have commonly been found in vegetation, mud, and water throughout evolution.\n\nMultiple possible mechanisms have been proposed for how the 'Old Friends' microorganisms prevent autoimmune diseases and asthma. They include: \n\nThe \"microbial diversity\" hypothesis, proposed by Paolo Matricardi and developed by von Hertzen, holds that diversity and turnover of bacterial species in the gut mucosa and other sites is a key factor for priming and regulating the immune system, rather than stable colonization with a particular species. It is not clear whether diversity per se, or that a diverse population will include certain organisms without which the immune system fails to develop. Rook likened the embryonic immune system to a computer that contains programmes but little data. During gestation and infancy exposure to diverse organisms builds a \"database\" that allows the immune system to identify and respond to harmful agents and normalize once the danger is eliminated.\n\nFor allergic disease, the most important times for exposure are: early in development; later during pregnancy; and the first few days or months of infancy. Exposure needs to be maintained over a significant period. This fits with evidence that delivery by Caesarean section may be associated with increased allergies, whilst breastfeeding can be protective. The extent to which exposures need to be maintained after infancy and whether these conditions could be managed by on-going exposure is as yet unknown.\n\nHumans and the microbes they harbor have co-evolved for thousands of centuries; however, it is thought that the human species has gone through numerous phases in history characterized by different pathogen exposures. For instance, in very early human societies, small interaction between its members has given particular selection to a relatively limited group of pathogens that had high transmission rates. When societies became larger, the introduction of agriculture some 10,000 years ago made the spreading of new pathogens more likely, and thus exposures to pathogens that favored high population densities to thrive. Furthermore, pastoralism has made zoonotic pathogen transmissions even more favorable. It is considered that the human immune system is likely subjected to a selective pressure from pathogens that are responsible for down regulating certain alleles and therefore phenotypes in humans, the thalassemia genes that are shaped by the \"Plasmodium\" species expressing the selection pressure being a model for this theory.\n\nRecent comparative genomic studies have shown that immune response genes (protein coding and non-coding regulatory genes) have less evolutionary constraint, and are rather more frequently targeted by positive selection from pathogens that coevolve with the human subject. Of all the various types of pathogens known to cause disease in humans, helminths warrant special attention, because of their ability to modify the prevalence or severity of certain immune-related responses in human and mouse models. In fact recent research has shown that parasitic worms have served as a stronger selective pressure on select human genes encoding interleukins and interleukin receptors when compared to viral and bacterial pathogens. Helminths are thought to have been as old as the adaptive immune system, suggesting that they may have co-evolved, also implying that our immune system has been strongly focused on fighting off helminthic infections, insofar as to potentially interact with them early in infancy. The host-pathogen interaction is a very important relationship that serves to shape the immune system development early on in life.\n\nAllergic conditions are caused by inappropriate immunological responses to harmless antigens driven by a T2-mediated immune response, T2 cells produce interleukin 4, interleukin 5, interleukin 6, interleukin 13 and predominantly immunoglobulin E. Many bacteria and viruses elicit a T1-mediated immune response, which down-regulates T2 responses. T1 immune responses are characterized by the secretion of pro-inflammatory cytokines such as interleukin 2, IFNγ, and TNFα. Factors that favor a predominantly T1 phenotype include: older siblings, large family size, early day care attendance, infection (TB, measles, or hepatitis), rural living, or contact with animals. A T2-dominated phenotype is associated with high antibiotic use, western lifestyle, urban environment, diet, and sensitivity to dust mites and cockroaches. T1 and T2 responses are reciprocally inhibitory, so when one is active, the other is suppressed.\n\nThe mechanism of action of the hygiene hypothesis was insufficient stimulation of the T1 arm, stimulating the cell defence of the immune system and leading to an overactive mother T2 arm, stimulating the antibody-mediated immunity of the immune systems, which in turn led to allergic disease.\nThis explanation however, cannot explain the rise in incidence (similar to the rise of allergic diseases) of several T1-mediated autoimmune diseases, including inflammatory bowel disease, multiple sclerosis and type I diabetes. [Figure 1Bach] However, the North South Gradient seen in the prevalence of multiple sclerosis has been found to be inversely related to the global distribution of parasitic infection.[Figure 2Bach] Additionally, research has shown that MS patients infected with parasites displayed T2 type immune responses as opposed to the proinflammatory T1 immune phenotype seen in non-infected multiple sclerosis patients.[Fleming] Parasite infection has also been shown to improve inflammatory bowel disease and may act in a similar fashion as it does in multiple sclerosis.[Lee]\n\nAn alternative explanation is that the developing immune system must receive stimuli (from infectious agents, symbiotic bacteria, or parasites) to adequately develop regulatory T cells. Without that stimuli it becomes more susceptible to autoimmune diseases and allergic diseases, because of insufficiently repressed T1 and T2 responses, respectively. For example, all chronic inflammatory disorders show evidence of failed immunoregulation. Secondly, helminths, non-pathogenic ambient pseudocommensal bacteria or certain gut commensals and probiotics, drive immunoregulation. They block or treat models of all chronic inflammatory conditions. Thirdly, some such organisms (or molecules that they secrete), specifically expand populations of regulatory T cells (Treg), or cause dendritic cells to switch to regulatory forms that preferentially drive immunoregulation. Finally, when multiple sclerosis patients become infected with helminths, the disease stops progressing and circulating myelin-recognising regulatory T cells appear in the peripheral blood. This indicates that helminths act as adjuvants for regulatory T cells. This observation led to clinical trials.\n\nThe hygiene hypothesis is supported by epidemiological data. Studies have shown that various immunological and autoimmune diseases are much less common in the developing world than the industrialized world and that immigrants to the industrialized world from the developing world increasingly develop immunological disorders in relation to the length of time since arrival in the industrialized world. This is true for asthma and other chronic inflammatory disorders.\n\nRecently, \"Opisthorchis felineus\" chronic helminthic infection in the endemic region of Russia was found to be associated with lower total serum cholesterol levels and a significant attenuation of atherosclerosis in humans.\n\nIn developed countries where childhood diseases were eliminated, the asthma rate for youth is approximately 10%. In the 19th century, hay-fever, an easily recognisable allergy, was a very rare condition.\n\nLongitudinal studies in Ghana demonstrate an increase in immunological disorders as it grew more affluent and presumably cleaner. These results have been replicated by Weinberg et al. who amassed data from a variety of African countries comparing urban and rural environments as well as high and low socioeconomic status (SES). In all four countries urban and high SES groups had a higher prevalence of exercise induced bronchospasm. The use of antibiotics in the first year of life has been linked to asthma and other allergic diseases. The use of antibacterial cleaning products has also been associated with higher incidence of asthma. Increased asthma rates are associated with birth by Caesarean section. The data supporting links to antibiotic use and caesarean section (but not to antibacterial use) are rapidly strengthening.\n\nAntibiotic usage, which reduces the diversity of gut microbiota, is another cited factor. Although several studies have shown associations between antibiotic use and later development of asthma or allergy, other studies suggest that the effect is due to more frequent antibiotic use in asthmatic children. Trends in vaccine use may also be relevant, but epidemiological studies provide no consistent support for a detrimental effect of vaccination/immunization on atopy rates. In support of the old friends hypothesis, the intestinal microbiome was found to differ between allergic and non-allergic Estonian and Swedish children (although this finding was not replicated in a larger cohort), and the biodiversity of the intestinal flora in patients with Crohn’s disease was diminished.\n\nIn 2015, a study found that washing dishes by hand as opposed to using a dishwasher, along with eating food directly from a farm or fermented food, might lead to reduced risk of certain conditions, including asthma, eczema, and possibly hay fever, though the data found on hay fever was not regarded as statistically significant. It was stated, however, that more research was needed to determine if there was an actual causal effect between these practices and a reduced risk of allergies and asthma.\n\nOne study showed that Staphylococci helped reduce inflammation. Early life exposure to specific microbe-enriched environments decreases susceptibility to diseases, such as inflammatory bowel disease and asthma, whereas its absence, as in antibiotic treatment during childhood, may have the opposite effect.\n\nSince allergies and other chronic inflammatory diseases are largely diseases of the last 100 years or so, the \"hygiene\" revolution of the last 200 years came under scrutiny as a possible cause. During the 1800s radical improvements to sanitation and water quality occurred in Europe and North America. The introduction of toilets and sewer systems and the cleanup of city streets, and cleaner food were part of this program. This in turn led to a rapid decline in infectious diseases, particularly during the period 1900-1950, through reduced exposure to infectious agents.\n\nPublic health activities have also played a part in affecting diet and lifestyle, such as physical activity levels and locations.\n\nIt has been suggested that public awareness of the initial form of the \"hygiene hypothesis\" has led to an \"increased disregard\" for hygiene in the home.\n\nWhile no hygiene-related treatments are part of the standard of care, various approaches are under investigation. Helminth therapy is one alternative. Probiotics (drinks or foods) have never been shown to reintroduce microbes to the gut. As yet, therapeutically relevant microbes have not been specifically identified.\n\nLifestyle changes could increase microbial exposure, but whether this on balance improves the balance of risks remains the subject of research. Proposals include natural childbirth, sustained breast feeding and physical interaction between siblings, and encouraging children to spend more time in \"uncleaned\" outdoor environments.\n\nShould these therapies become accepted, public policy implications include providing green spaces in urban areas or even providing access to agricultural environments for children.\n\nHelminthic therapy is the treatment of autoimmune diseases and immune disorders by means of deliberate infestation with a helminth larva or ova. Helminthic therapy is currently being studied as a promising treatment for several (non-viral) autoimmune diseases including Crohn's disease, multiple sclerosis, asthma, and ulcerative colitis. Autoimmune liver disease can be modulated by active helminth infections.\n\nThe anti-inflammatory effects of helminth infection are prompting interest and research into diseases that involve inflammation but that are not currently considered to include autoimmunity or immune dysregulation as a causative factor. Heart disease and arteriosclerosis both have similar epidemiological profiles as autoimmune diseases and both involve inflammation. Their increased incidence cannot be solely attributed to environmental factors. Recent research explored the eradication of helminths as contributing to this discrepancy.\n\nHelminthic therapy emerged from the search for reasons why the incidence of immunological disorders and autoimmune diseases correlates with the level of industrial development.\n\nRelated therapies include use other types of infectious organisms, such as protozoa.\n\nNo evidence supports the idea that reducing modern practices of cleanliness and hygiene would have any impact on rates of chronic inflammatory and allergic disorders, but a significant amount of evidence that it would increase the risks of infectious diseases.\n\nIf home and personal cleanliness contributes to reduced exposure to vital microbes, its role is likely to be small. The idea that homes can be made “sterile” through excessive cleanliness is implausible. The evidence shows that, as fast as they are removed by cleaning, microbes are replaced, via dust and air from outdoors, by shedding from the body and other living things as well as from food. The key point may be that the microbial content of urban housing has altered, not because of home and personal hygiene habits, but because they are part of urban environments. Diet and lifestyle changes also affects the gut, skin and respiratory microbiota.\n\nAt the same time that concerns about allergies and other chronic inflammatory diseases have been increasing, so also have concerns about infectious disease. Infectious diseases continue to exert a heavy health toll. Preventing pandemics and reducing antibiotic resistance are global priorities. Hygiene is a cornerstone of containing these threats.\n\nThe International Scientific Forum on Home Hygiene has developed a risk management approach to reducing home infection risks. This approach uses microbiological and epidemiological evidence to identify the key routes of infection transmission in the home. These data indicate that the critical routes involve the hands, hand and food contact surfaces and cleaning utensils. Clothing and household linens involve somewhat lower risks. Surfaces that contact the body, such as baths and hand basins, can act as infection vehicles, as can surfaces associated with toilets. Airborne transmission can be important for some pathogens. A key aspect of this approach is that it maximises protection against pathogens and infection, but is more relaxed about visible cleanliness in order to sustain normal exposure to other human, animal and environmental microbes.\n\nThere are other hypotheses that try to explain the increase in allergies in developed nations. Major areas of focus include infant feeding, over-exposure and exposure to certain pollutants.\n\nInfant feeding topics includes breastfeeding, when babies begin to eat solid foods and the type of these foods, cow's milk vs other milks and variations in milk processing.\n\nOver-exposure to allergens in occupational situations can cause allergic responses, such as Laboratory animal allergy, bird lung, farmer's lung and bakers lung (See Wheat allergy).\n\nThe pool chlorine hypothesis was proposed by Albert Bernard and his colleagues as an alternative hypothesis based on epidemiological evidence in 2003.\n\n\n\n"}
{"id": "42955029", "url": "https://en.wikipedia.org/wiki?curid=42955029", "title": "Jersey cabbage", "text": "Jersey cabbage\n\nThe Jersey cabbage (\"Brassica oleracea longata\", also known as Jersey kale or cow cabbage,) and by a variety of local names including giant cabbage, long jacks, tree cabbage and the French \"chour\" and \"chou à vacque\". It is a variety of cabbage native to the Channel Islands that grows to a great height and was formerly commonly used there as livestock fodder and for making walking sticks.\n\nThe 'Jersey cabbage' develops a long stalk, commonly reaching in height, and can grow as tall as . Historically the stalks were made into walking sticks, of which 30,000 a year were being sold by the early 20th century, many for export. They were also used for fencing and as rafters. Much of the stalk is bare; the islanders stripped leaves to accentuate this effect and induce it to grow without twisting, varnished the stalk, and created a handle either by heat-treating and bending the root end or by planting at an angle to produce a naturally bent root.\n\nThe lower leaves were fed to livestock, (one variety in Portugal was grown specifically for the purpose,), and were reportedly of great value: \"The Farmer's Magazine\" stated in 1836 that five plants would support 100 sheep or 10 cows, and sheep fed them were rumoured to produce silky wool up to in length. The open cabbage at the top is comparatively small: \"the size of the cabbages at the top was so infinitesimal that one seemed forced to the conviction that nature meant them to be stalks, not cabbages\".\n\nThe plant is now rarely grown in the Channel Islands, except for feeding rabbits. Although, it is still cultivated for walking sticks by Philip and Jacquelyn Johnson, who were shown on the BBC One series \"Countryfile\" in January 2010.\n\n"}
{"id": "3757181", "url": "https://en.wikipedia.org/wiki?curid=3757181", "title": "Jimmy Five", "text": "Jimmy Five\n\nJimmy Five, known as Cebolinha in Brazil, is one of Monica's Gang main characters. He was created in 1960, and currently has his own printed comic book, called \"Cebolinha\", which were first released in 1973. His English name is Jimmy Five due to his hair composed of only five strands.\n\nJimmy's family name is \"Cebola\" (\"Onion\" in Portuguese), and he has a little baby sister called Mary Angela (in Portuguese, Maria Cebolinha, after her brother's name), also based on a real person.\n\nIn the first adaptations of comics into English, the character's name was Stanley, and later Frizz.\n\nJimmy Five first appeared in the comic books \"Zaz Trás\" and \"Bidu\" in 1960. Mauricio de Sousa, creator of Monica's Gang, says he based the character on a child he knew while growing up in Mogi das Cruzes. He was a friend of his brother Márcio, and the boy would also switch the letter \"r\" for \"l\" and, because of his pointy hair, he earned the nickname \"cebolinha\" from Márcio's and Mauricio's father. At first, he was a 4-year-old boy who was a friend of Franklin and his gang, he was the youngest boy. In the first stories he was hairy, after Sousa returned to the newspaper strips, his design became more and more simple and began to be drawn with fewer hair strands.\n\nHe also has become the protagonist of the comics in 1961, in the place of Franklin and Blu. He had several supporting friends in the very first comic strips, with more prominence in Specs and Smudge as his best friends and sidekicks. After the introduction of Monica (based on the Sousa's daughter) in the comic strips in 1963 he began to share the leading role with her over the years. In 1964 he came to have his current look with 5 hair strands. In 1970 with the release of the comic book, Monica became the title protagonist in place of Jimmy Five, but a comic book for Jimmy Five began to be published 3 years later.\n\nJimmy Five is incapable of pronouncing the letter \"r\" , replacing it with the letter \"l\", in the Portuguese version, or with the letter \"w\", in the English version. When the letter is used in the end of a word, however, he pronounces it normally (as in \"car\" or \"locker\").\n\nOut of the main cast of Monica's Gang he is the only one to regularly wear shoes (when barefoot, he is also one of the rare characters to be shown with toes). He often complains and despairs over his main physical feature - his lack of any hair other than five single strands. His madcap attempts to rectify this often causes him (and those around him) a great deal of grief. He was originally drawn with a full head of hair, which often becomes a topic in his laments to the comic artist to \"help him out\" and restore his full head of hair.\n\nHe is always plotting to steal either Samson or the title of \"owner of the street\" from Monica with his \"infallible plans\" (which were initially created by Specs), which always end in failure, mostly because Smudge (his best friend) accidentally reveals to Monica that she is in a trap. On some stories, he gathers the other boys of the gang just to pick on Monica.\n\nIn some earlier stories, he devised background plans to find out the secret of her strength, but he always ended up beaten solely by her. Even with these frictions, Jimmy and Monica are still friends to each other. In the futuristic special edition stories, they are often portrayed as married or dating each other. Indeed, in Monica Teen, they are seen kissing.\n\nIt was once revealed that Jimmy is not the first one in his family to have his famous speech impediment and that it caused all his relatives (minus his father) to believe he will never be able to pronounce 'r'. However, this is disproven in the Monica Teen stories, where he is said to take up speech-language pathology sessions to correct his speech impediment; however, he reverts to mispronunciation when under stress or close to girls (especially Monica).\n\nWhereas his family was always composed of his mother, father, and little sister, he once had a little brother, introduced in a 1972 story. Jimmy himself even ended the story asking his readers to send name suggestions to Editora Abril (which published Monica's Gangs' comics at that time), but the toddler ended up disappearing from the stories. Mauricio stated that he simply didn't have time to plan the continuation of his arrival, and the character was never featured again.\n\n\n\n"}
{"id": "1353592", "url": "https://en.wikipedia.org/wiki?curid=1353592", "title": "Kretek", "text": "Kretek\n\nKretek are cigarettes made with a blend of tobacco, cloves and other flavors. The word \"kretek\" itself is an onomatopoetic term for the crackling sound of burning cloves.\n\nPartly due to favorable taxation compared to \"white\" cigarettes, kreteks are by far the most widely smoked form of cigarettes in Indonesia, where they are preferred by about 90% of smokers.\nIn Indonesia, there are hundreds of kretek manufacturers, including small local makers and major brands. Most of the widely known international brands, including , Bentoel, Minak Djinggo, Djarum, Gudang Garam, and Wismilak originate from Indonesia. Nat Sherman of the United States produces cigarettes branded as \"A Touch of Clove\" but they are not true kreteks since there is clove flavoring infused into small crystals located inside the filter, rather than actual clove spice mixed with the tobacco.\n\nThe origin of kretek cigarettes can be traced to the late 19th century. The creator of kretek was Haji Jamhari, a native of Kudus in Indonesia's Central Java region. Suffering from chest pains, Jamhari attempted to reduce the pain by rubbing clove oil on his chest. Jamhari sought a means of achieving a deeper relief and smoked his hand-rolled cigarettes after adding dried clove buds and rubber tree sap. According to the story, his asthma and chest pains vanished immediately. Word of Jamhari's product spread rapidly among his neighbors, and the product soon became available in pharmacies as \"rokok cengkeh\"; clove cigarettes. First marketed as a medicinal product, kreteks became widely popular.\n\nIn those years, the locals used to hand-roll kreteks to sell on order without any specific brand, packing, or limits on ingredients used in production. A resident of Kudus named Nitisemito had the idea of starting serial production and selling kreteks under a proprietary brand name. Unlike other manufacturers, Nitisemito, who first created the Bal Tiga brand in 1906, enjoyed great success by implementing unprecedented marketing techniques, such as using embossed packs or offering free-of-charge promotional materials. Commercial manufacture did not start in earnest until the 1930s.\nFurthermore, he also developed a means of production called the abon system which offered opportunities for other entrepreneurs with insufficient capital. In this system, a person called an \"abon\" assumes the job of delivering finished products to the company which pays the price of piecework done whereas the company is liable to supply the necessary production materials to the \"abons\". Most manufacturers have since opted to have their workers working under the roof of their own factories, to maintain quality standards. Nowadays, only a few kretek manufacturers make use of the abon system.\n\nDuring the period from 1960 until 1970, kreteks became a national symbol against \"white cigarettes\". In the mid-1980s, the number of machine-produced cigarettes exceeded that of hand-rolled ones. One of the largest income sources of Indonesia, the kretek industry comprises 500 large and small manufacturers employing a total of around 10 million people.\n\nSince 2009, kreteks are not legal for sale in the United States. A variation of the kretek is sold: \"cigars\" that are similar in size and shape to the original kreteks, also with a filter and the original tobacco/clove blend, but in a tobacco-based paper.\n\nThe quality and variety of tobacco play an important role in kretek production. One kretek brand can contain more than 30 types of tobacco. Minced dried clove buds weighing about 1/3 of the tobacco blend are added. Sometimes, the last process which machine-made or hand-rolled kreteks go through is the spraying of sweetener at the butt end of the cigarette.\n\nDjarum Black cigarettes sold in Europe, South Africa and South American countries have 10–12 mg tar and 1 mg nicotine, as indicated on the pack. This level of tar and nicotine is comparable to the majority of other regular or \"full-flavor\" cigarettes available. Djarum Black cigarettes produced for consumption in Indonesia contain a significantly higher quantity of tar and nicotine, 25 mg and 1.6 mg respectively. In Canada, Djarum Black cigarettes are listed as containing 44.2–86 mg of tar and 1.73–3.24 mg of nicotine, a significant amount more than most other cigarettes. \n\nThe venous plasma nicotine and carbon monoxide levels from 10 smokers were tested after smoking kreteks and were found to be similar to non-clove brands of cigarettes, such as Marlboro.\n\nRats were given equal inhalation doses of conventional tobacco cigarettes and kreteks over a short period. Those that had inhaled kreteks did not appear to show worse health effects compared to those that had inhaled conventional cigarettes. The study was repeated with a 14-day exposure and kreteks again did not produce worse health effects than conventional cigarettes.\n\nThe eugenol in clove smoke causes a numbing of the throat which can diminish the gag reflex in users, leading researchers to recommend caution for individuals with respiratory infections. There have also been a few cases of aspiration pneumonia in individuals with normal respiratory tracts possibly because of the diminished gag reflex.\n\nIn the United States, cigarettes were the subject of legal restrictions and political debate, including a proposed 2009 US Senate bill that would have prohibited cigarettes from having a \"characterizing flavor\" of certain ingredients other than tobacco and menthol.\n\nA study by the U.S. Centers for Disease Control found kreteks account for a relatively small percentage of underage smoking, and their use was declining among high school students. Critics of the bill argued that support of the bill by the large U.S. tobacco maker Philip Morris, which makes only conventional and menthol cigarettes, indicated that the bill was an attempt to protect the company from competition.\n\nSome U.S. states, including Utah, New Mexico, and Maryland, passed laws that prohibit the sale of kreteks.\nOn 14 March 2005, Philip Morris International announced the purchase of Indonesian tobacco company PT HM Sampoerna after acquiring a 40% stake in Sampoerna from a number of Sampoerna's principal shareholders.\n\nIn 2009, the Family Smoking Prevention and Tobacco Control Act was introduced in the US Congress and signed into law by President Barack Obama, giving the FDA significantly more regulatory power over tobacco; one of the provisions in the law includes a ban on the use of flavors in tobacco, other than menthol. The ban includes kreteks. As of September 22, 2009, the clove cigarette was no longer legal to sell or distribute in the US, and cigarettes purchased overseas are subject to seizure by U.S. Customs. There is an exception to this rule when receiving cigarettes as gifts through the USPS and is only allowed if certain guidelines are followed. This rule does not allow for purchase of tobacco products overseas but allows the receipt of gifts from domestic individuals and international individuals. However, Kretek International Inc., importer of the Djarum brand, continued to offer the clove and tobacco products as little cigars, which have lower taxes (in some U.S. states) and looser restrictions than cigarettes.\n\nOn April 12, 2010 Indonesia filed a formal complaint with the World Trade Organization stating the ban on kreteks in America amounts to discrimination because menthol cigarettes are exempt from the new regulation. Trade Ministry Director General of International Trade Gusmardi Bustami has stated that the Indonesian government has asked the WTO panel to review US violations on trade regulations, including the General Agreement on Tariffs and Trade (GATT) 1994, Technical Barriers to Trade (TBT) and Sanitary and Phytosanitary (SPS) Agreement. The TBT Agreement is of special importance as it defines clove cigarettes and menthol cigarettes as \"like products\". Claims of discrimination are enhanced when noting that 99% of kreteks were imported from countries other than the United States (chiefly Indonesia), while menthol cigarettes are produced almost entirely by American tobacco manufacturers. Indonesia's case is further strengthened by comparing the number of young kretek smokers in America with the number of young menthol cigarette smokers. According to US health reports, 43% of young smokers smoke menthol cigarettes, which accounts for nearly 25% of the total cigarette consumption in the United States. Young smokers habituated to kreteks, however, account for less than 1% of cigarette consumption in the US, and <1% of the total cigarettes sold in the US. On April 4, 2012, the WTO ruled in favor of Indonesia's claim, though it is unclear how this will affect U.S. law.\n\nKretek cigarettes are among others sold in the Netherlands, Germany, France, Australia, Brazil etc. In Europe only smaller packs and thinner cigarettes are sold to adhere to the EU established maximum amount of nicotine and tar levels. In South Africa they are also sold in smaller packs of 10 with between 10–12 mg tar, and 1-1.2 mg nicotine.\n\nIndonesia is the world's largest producer of clove cigarettes, and exports up to US$500 million of the product a year.\n\n\n"}
{"id": "51301325", "url": "https://en.wikipedia.org/wiki?curid=51301325", "title": "Lip Bumper", "text": "Lip Bumper\n\nLip Bumper is an appliance used in Dentistry, especially Orthodontics, for various purposes to correct a dentition by preventing the pressure from the soft tissue. Lip bumper is usually used in an orthodontic treatment where one has a crowded maxillary or mandibular teeth in an arch.\n\nLip Bumper can be used for expansion of the teeth in the mandibular arch. In orthodontics, tooth-size discrepancy phenomenon occurs when there is crowding presented. As a treatment, either extractions of teeth or expansion of the arch can be done to correct the tooth-size discrepancy. Lip bumper is placed in front of the anterior teeth to keep the pressure of the lips and cheeks away from the front teeth and back teeth respectively. As cited by Werner et al., the lip bumper can be used for reduce lower anterior crowding, increase arch circumference, and move the permanent lower molars distally for the purpose of keeping anchorage.\n\nSpecific changes that occur because of this appliance including anterior teeth tipping forward, molar teeth tipping backwards and the increase in width of the arch formed by the lower teeth. The wire is kept 1.5 - 2.0mm from the front surface of the anterior teeth. Recently, advances have been made to use Lip Bumper with Mini-Screws for distalization of the upper back teeth.\n\n\"Korn Lip Bumper\" is a maxillary lip bumper which was developed by Korn and Shapiro. This lip bumper is made up of .040in Stainless Steel wire and involves multiple loops. This lip bumper sits high in the maxillary vestibule and 2-3mm away from the attached gingiva. This type of bumper is often used with a bite plate.\n\nSide-effects caused by Lip Bumper includes the possible disruption in the eruption pattern of the 2nd molars in the mandibular arch. The pressure of the lips on the lip bumper, causes the 1st molar to tip distally which effects and causes the impaction of the 2nd molar.\n\nComponents of lip bumper include bands on the molar teeth, a thick Stainless Steel wire with loops made bilaterally and a thick acrylic or rubber shield that goes in the anterior part of the wire to not cause irritation to the mucosa of the lower lip.\n"}
{"id": "44464253", "url": "https://en.wikipedia.org/wiki?curid=44464253", "title": "Lytton Quarantine Station", "text": "Lytton Quarantine Station\n\nLytton Quarantine Station is a heritage-listed former quarantine station at 160 South Street, Lytton, City of Brisbane, Queensland, Australia. It was built from 1913 to . It is also known as Customs Reserve and Lytton Quarantine Complex and Animal Detention Centre. It is included within the Fort Lytton National Park. It was added to the Queensland Heritage Register on 22 September 2000.\n\nThe Lytton Quarantine Station was established in 1913-1914, to accommodate newly arrived immigrants and persons considered to be at risk of causing infection to the general population.. Situated at an isolated location at the mouth of the Brisbane River, the place illustrates early 20th century attitudes to quarantine practices and the provision of quarantine facilities, and is important as part of a continuum of sites in and adjacent to Moreton Bay, used for quarantine purposes from 1844.\n\nThere were no human quarantine facilities at Moreton Bay during the penal era 1824-42, as all immigration came via Sydney. Following the opening of the district to free settlement in February 1842, a quarantine station was formed in 1844 at Dunwich, on Stradbroke Island, at the site of a former goods transfer depot established by convicts in the late 1820s. From 1864, Dunwich served as both quarantine station and benevolent asylum. The quarantine station was relocated briefly to St Helena Island in Moreton Bay in 1866-67, along with the newly erected gaol there, but was soon returned to Dunwich. From 1874 to 1915 Peel Island in Moreton Bay served as Brisbane's human quarantine station.\n\nThe Lytton site, just south of Fort Lytton at the mouth of the Brisbane River, had a variety of uses prior to the establishment of the Quarantine Station in the early 20th century, occupying a former customs reserve (established 1858-59), sections of the early township of Lytton (surveyed in 1859), and an animal quarantine facility (established 1889).\n\nThe history of the Lytton district is closely aligned to the establishment during the 1840s and 1850s of the Port of Moreton Bay at Brisbane Town, on the Brisbane River, rather than at Cleveland on the Bay. Moreton Bay was proclaimed a Port of Entry, with the facilities to collect customs, in June 1846. Brisbane was declared a warehousing port in 1849 and a Customs House was erected there in 1850. In 1857 the New South Wales colonial government began to investigate the suitability of establishing a customs station at the south head of the Brisbane River (present-day Lytton). In August 1857, surveyor James Warner completed a preliminary survey of a site for a village, which was approved in November 1858, and in December 1858 tenders were called for the construction of a Customs Station nearby on the river. In February 1859 Warner officially surveyed sections 1 to 13 of the village of Lytton, as well as sites for a customs landing place and a signal station. From 1 July 1859 a Tide Surveyor was stationed at Lytton. His job was to meet and board all ships entering the harbour, to prevent the evasion of customs duties. He was assisted by a coxswain and four boatmen, also stationed at the Lytton customs reserve. Following separation from New South Wales in December 1859, the Queensland government maintained Lytton's role as the customs entry to the Port of Moreton Bay. From Water Police-cum-Customs Officers were stationed on the gaol hulk Proserpine at the mouth of the Brisbane River. The Sub-inspector of the Water Police, based at the Lytton Customs Reserve, was also appointed as a health officer, and inspected all vessels entering the port.\n\nEarly buildings and structures associated with the Lytton Customs Station included: customs station (1858-9); electric telegraph station (1861); pilots' houses and sheds (1862); boatman's cottage (1863); boat slip (1863); health officer's house (1865); Lytton Wharf (1866); and additional boatmen's quarters (1872).\n\nSome sections of Lytton township were alienated between 1860 and 1863, mostly by Brisbane speculators who anticipated the development of wharf facilities at Lytton. Few private buildings were erected there. The Crown and Anchor Hotel operated at Lytton briefly in 1865-66, when a government wharf adjacent to the Customs Reserve was built in 1866 to tranship railway stores and plant for construction of the Southern and Western Railway. From 1878 until the Lytton Hotel served local farmers and a military presence. Lytton State School was established . In general, the district remained agricultural.\n\nSir George Bowen, on completion of his term as Governor and departure from Moreton Bay on 4 January 1868, officially named and designated Lytton as Brisbane's port.\n\nIn 1877, Lytton was identified as the site for a defence fortification to guard the mouth of the Brisbane River. In 1880-82 Fort Lytton was constructed adjacent to and north of the Customs Reserve.\n\nIn the 1880s, colonial concern with the quarantine of imported animals led to the June 1889 proclamation of a small Reserve for Quarantine Purposes at Lytton (Reserve 230 - 1a 3r 29p), on the river adjacent to the Lytton Town Reserve and south of the Customs Reserve. Its location suggests it was for maritime quarantine. By May 1890 builders John Petrie & Son had completed a cottage, stables and forage room on the Lytton Quarantine Reserve, and in 1893, dog kennels were constructed. In April 1894, in anticipation of an expected influx of imported animals following the lifting of restrictions on stock imports, an extension of to the Reserve for Quarantine Purposes at Lytton was proclaimed as Reserve 231. This extension occupied the southern half of the Customs Reserve. In 1891 and 1900, much of Lytton township was resumed for military purposes, with part of the 1900 resumption allocated to the Animal Quarantine Station. Several existing residences were used for customs personnel and others were removed from the site in the early 20th century.\n\nFollowing the formation of the Commonwealth of Australia in 1901, quarantine became a federal responsibility under the Commonwealth Quarantine Act 1908. From 1908-1911, the Queensland Commissioner of Public Health (a position created under Queensland's Health Act of 1900) performed the duties of quarantine officer for the Commonwealth. In 1910 Dr John Simeon Colebrook Elkington was appointed to this position, and in the following three years established Queensland's public health infrastructure, and oversaw the administration and implementation of State and Commonwealth quarantine practices. In 1922 Elkington wrote a text on quarantine procedures for the Commonwealth, which not only governed quarantine practice in Australia but was adopted as the model in at least three other countries.\n\nIn January 1912, the administration of quarantine facilities and practices in Queensland was transferred wholly to the Federal Quarantine Bureau. In December 1913, Elkington resigned his State position and joined the Commonwealth Health Service in Brisbane, overseeing the establishment of the quarantine facility at Lytton.\n\nThe establishment of a human quarantine station at Lytton was made practicable following the widening and deepening of the channel through the bar at the mouth of the Brisbane River, completed in 1911. This permitted deep-draught, ocean-going vessels to enter the Brisbane River. Construction of the Lytton Quarantine Station was undertaken by the Queensland Department of Works, financed by a Commonwealth loan, and approved by the Commonwealth Department of Home Affairs. Tenders were called in June 1913 for the construction of an administration building, isolation hospital, observation block, attendant's quarters, domestic quarters and stores, with the contract let to Lyons and Paton with a price of . The quarantine buildings were erected on the north eastern part of the reserve (now a vacant site, part of which is incorporated within Fort Lytton National Park, and part on freehold land owned by Caltex Refineries (Qld) Ltd). It is possible that some of the buildings associated with the isolation hospital established at Colmslie in the early 1900s were relocated to the Lytton Quarantine Reserve at this time. In 1914, tenders were called for a laundry, meat store, incinerating shed, footpath and tramway/trolley lines.\nThe quarantine station buildings were timber-framed and timber-clad with corrugated iron roofs and elevated on low concrete stumps. The wards were segregated with first, second and third class facilities (similar to arrangements on board ship). Some wards were mosquito-proofed, but most were not. At various times the quarantine station buildings also provided accommodation for persons stationed at adjacent Fort Lytton. The fort also played a role in the function of the quarantine station, controlling ships attempting to enter the Brisbane River without appropriate health clearances. In 1919 the world-wide outbreak of Spanish Influenza which followed the end of the First World War was delayed in Brisbane by some three months, following a strict quarantine at Lytton of personnel returning on troop ships.\n\nDuring Elkington's era, a venereal diseases hospital was established on the southern portion of the Lytton Quarantine Reserve, adjacent to South Street. This area is now owned by Caltex Refineries (Qld) Ltd.\n\nBy 1928, the Lytton Quarantine Station facilities had been extended and enhanced. Vessels were met at the river mouth by the Health Authorities who boarded each ship (two launches were kept at Lytton) and inspected all goods and personnel on board. Suspect people and goods were landed at Lytton for fumigation and isolation. Jetty facilities permitted passengers to disembark on foot and a crane was provided for conveying sick passengers, luggage and stores to shore. A tram line ran from the jetty to the reception house. A permanent staff of a foreman assistant, two engine-drivers, a coxswain and four assistants was employed to maintain the facilities and attend to non-human quarantine, which included disinfection of luggage and materials using a steam disinfector (autoclave) with attachments for cyanide or formaldehyde gas. A bathing block permitted 10 cabin passengers and 20 crew, third class or deck passengers to bathe simultaneously. Separate accommodation in tents or troop huts had been established for Asiatics, with separate kitchen, dining-room, shelter shed, lavatory and bath blocks. Lighting was supplied from kerosene lamps. A water carriage sewerage system discharged directly into the river, with a chlorination chamber available for disinfection of sewerage as necessary. The medical officer's and foreman assistant's quarters were connected to the city telephone service and internal telephones served most of the principal buildings. A cemetery was located within the grounds. The Lytton facility functioned as a human quarantine station until the early 1980s, by which time the decision had been made to phase out human quarantine services. Arrangements were made for Kenmore Repatriation Hospital to provide observation facilities in the event that they were required, and in 1982 the Lytton facility reverted to its earlier role as an animal quarantine station, re-designated the Lytton Quarantine Complex and Animal Detention Centre. Facilities comprised a launch jetty, workshop, inflammable store, guinea pig area, imports treatment laboratory, utility buildings area, offices, stores, and animal quarantine detention facilities. In addition to its role as an animal quarantine centre, Lytton served as a communications centre, a base for south side shipping clearance and inspection, a facility for dealing with quarantine treatment and detention of materials, a training area and a depot for stores. Many of the buildings associated with the human quarantine station at Lytton were removed after the 1982 closure, some relocated to sites in the Lytton and Wynnum districts.\n\nBy the late 1980s the Lytton facility had closed completely. In 1988 management of part of the site and buildings, including the quarantine station jetty, was taken over by the Department of Environment, Conservation and Tourism, through the Queensland Parks and Wildlife Service, associated with the administration of adjacent Fort Lytton, which was declared an Historic National Park in 1990. The building currently used by the Queensland Parks and Wildlife Service as the administration centre for Fort Lytton, was relocated from elsewhere on the former quarantine station reserve, where it once functioned as the doctor's quarters. In 1999, that part of the former Lytton Quarantine Station site managed by the Queensland Parks and Wildlife Service was incorporated into Fort Lytton National Park.\n\nThe remainder of the former Lytton Quarantine Station site was transferred to the owners of the adjacent oil refinery (now Caltex Refineries (Qld) Ltd) in the late 1980s. No buildings associated with the quarantine station remain on this freehold land, but archaeological evidence of past human activity on the site was identified in a 1994 historical and archaeological survey of the former Lytton Quarantine Station, prepared for the Office of the Co-ordinator-General, Queensland. Although there has been little development of the freehold part of the site to date, for the purposes of this entry in the Queensland Heritage Register, the listing boundary is restricted to that part of the former Lytton Quarantine Station now incorporated within Fort Lytton National Park, at the northern end of the site, which contains a number of extant buildings and structures associated with the functioning of the site as a quarantine station. This does not in any way detract from the significance of the remainder of the site, which remains important for its association with early land use in the Lytton district from at least the 1850s, for its association with the human quarantine facility established at Lytton in the early 20th century, for its potential to reveal archaeological evidence, and for contributing to the sense of isolation which was characteristic of the quarantine facility established here in the 1910s.\n\nThe former Lytton Quarantine Station is located on low, flat land adjacent to and south of Fort Lytton at the southern head of the mouth of the Brisbane River.\n\nThe principal structures and elements on that part of the former Lytton Quarantine Station now incorporated within Fort Lytton National Park, include: Jetty - constructed of timber planks bolted together and supported by wooden piles reinforced with concrete. It is associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nThe reception house is a small, timber-framed, weatherboard-clad building with fibrous-cement sheeting and battening in the gabled ends. It has a corrugated iron roof and a verandah at the north end of the building which is constructed over tramway tracks that once linked the quarantine complex to the jetty to the west. The interior has been altered to accommodate offices, but the building remains substantially intact. It is associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nThe tram/trolley bridge foundations comprise pairs of steel reinforced concrete footings set apart, leading west from the Reception House toward the jetty. In some places wooden sleepers are visible, but the majority of the decking and all the rails are missing. This element is associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nThe bath house is a large, rectangular, timber-framed, building with weatherboard-cladding to above sill height, above which is fibrous-cement sheeting and battening. It has a corrugated iron roof with ventilators along the ridge, and rests on a concrete slab. No evidence of services such as shower fittings, etc. remains, but the original interior layout is still discernible and the upper level observation post, at the north end of the building, is intact. Two square brick-lined drainage traps () are located on the eastern side of the shower block. The building is associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nThe boiler house is a tall, rectangular brick building with engaged piers, corrugated iron roof, substantial external chimney stack and external drains, with boilers and associated fittings internally. The whole is remarkable intact, and is associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nThe fuel shed is a small, later, timber-framed, weatherboard-clad structure with a low-pitched corrugated iron roof, adjacent to the eastern end of the boiler room.\n\nThe disinfecting block is a large, rectangular, timber-framed, building with weatherboard-cladding to above sill height, above which is fibrous-cement sheeting and battening. It has a corrugated iron roof and rests on a concrete slab. The internal autoclave and associated fittings are in-situ, along with trolley track and trolleys. Trolley or tram lines run west out of the centre of the western end of the building, toward the jetty. An overhead pipeline from the boiler house to the south side of the disinfecting block attaches to the autoclave. The building is remarkably intact, and is associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nThe laundry block is a rectangular, timber-framed, building with weatherboard-cladding to above sill height, above which is fibrous-cement sheeting and battening. It has a corrugated iron gabled roof with iron ventilators along the ridge and on the west side of the roof. The building rests on a concrete slab, and has a verandah over a concrete pathway along part of the western side of the building. There is a later, tall metal roller door at the north end of the building, but the remaining doors and fenestration appear to be original. Concrete footings associated with former tank stands are evident on the east side of the building. The building is remarkably intact, and is associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s. Enclosed laundry yard - the yard area to the east and south of the laundry block has a concrete retaining wall approximately high and wide. The west and east sides of the yard are bounded by round metal posts capped with concrete while the south side is defined by tram/trolley rails set vertically as posts ( high). There is an open concrete drain along the eastern side of the yard, inside the fence posts. The yard contains 12 square wooden posts approximately in height, which are likely to have supported clothes lines. A service inspection hatch is located centrally in the southern section of the yard. This has a concrete housing with a steel lid with embossed cross-hatching and the lettering \"Harvey & Son\". A block of late 20th century relocatable toilets is also sited in this yard. Despite this, the yard remains substantially intact, and is associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nThe two timber-framed, weatherboard clad launchmen's cottages are located along the eastern boundary of that part of the former Lytton Quarantine Station which has been incorporated into Fort Lytton National Park, south of the laundry and laundry yard. Originally there were three cottages, but the middle house has been removed. The two remaining houses have corrugated iron roofs and rest on low concrete piers. Although no interior inspection has been made, these houses appear to be reasonably intact. They are associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nThe doctor's quarters is a two-storeyed building currently functioning as the administration centre for Fort Lytton National Park. It was initially a single-storeyed, timber-framed, weatherboard-clad house which has been relocated from elsewhere on the site to its present position south of the Bath House, raised, to create a ground floor which is enclosed with fibrous-cement planking. Although no longer in situ, the place was associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nThe worshop is a small, rectangular, brick structure, resting on a concrete slab. It has a corrugated iron roof which is lined with timber boards. Internally, exposed roof trusses have been cut and braced to provide higher clearance for machinery. Outside, along the eastern side of the building is a concrete slab and seven pairs of concrete piers in ramp-like formation. On the western side of the building is This structure appears to correspond with a building marked \"workshop\" identified on a 1918 plan of the site. It now functions as a machinery store.\n\nAt the far southwest of the site is a small, early sewerage treatment facility long and wide. It is of concrete construction, and has a central rectangular wire mesh cage filled with coarse rubble which possibly functioned as primary filtration of effluent. The plant remains substantially intact, and is associated with the earliest phase of the development of the quarantine station at Lytton, in the 1910s.\n\nApart from the relocation of the former Doctor's Quarters to the northwestern end of the site, the surviving buildings and other structural and archaeological elements maintain their original spatial relationship. The surrounding grounds are mostly grassed, with a new bitumen road, which leads to the present entrance to Fort Lytton, established west of the original metalled roadway into the quarantine station. There are a number of mature, though not large, trees, at the southern end of the site, associated with the former quarantine station.\n\nAt the north east end of the site is an unoccupied block, fenced off, and formerly the site of the administrative building, attendant's quarters, stores, isolation hospital and observation block, associated with the establishment of the Lytton Quarantine Station in 1913-14. All of these buildings have been removed from the site, but a number of archaeological elements associated with the quarantine station have been identified. These include the concrete foundation slab and some wall remnants of the former Laboratory Building/Morgue at the far eastern end of the site; the concrete foundation slab of a building marked \"laundry\" on a 1918 plan, at the western side of the site; the concrete foundation slab of a small building identified as \"Receiving Shed\" on a 1918 plan, at the northern end of the site; and elements of the early drainage system associated with the former Lytton Quarantine Station.\n\nThe Lytton Quarantine Station extended a considerable distance to the south of what is now Fort Lytton National Park, and included a venereal diseases hospital and animal quarantine facilities. The 1994 archaeological survey of all of the site identified 237 features associated with the historic use of the site from at least the 1910s. That part of the former Lytton Quarantine Station not included within Fort Lytton National Park is not included within the listing boundary for the entry in the Queensland Heritage Register.\n\nThe Lytton Quarantine Stationwas listed on the Queensland Heritage Register on 22 September 2000 having satisfied the following criteria.\n\nThe place is important in demonstrating the evolution or pattern of Queensland's history.\n\nThe Lytton Quarantine Station and site is important in illustrating the evolution of Queensland's history, being associated with:\nIn particular, the place is important in illustrating the evolution of attitudes and approaches to the management of infectious diseases in Australia in the immediate post-Federation period. The place is also significant historically as part of a continuum of sites in and adjacent to Moreton Bay used for human quarantine purposes from 1844 to 1982.\n\nThe place demonstrates rare, uncommon or endangered aspects of Queensland's cultural heritage.\n\nWhile only some of the Lytton buildings remain, the site still provides important, rare surviving evidence of early 20th century Australian quarantine stations.\n\nThe place has potential to yield information that will contribute to an understanding of Queensland's history.\n\nThe place has potential to yield historical archaeological information, especially with regard to the location of the burial ground associated with the human quarantine facility, the likely location of which was adjacent to the morgue at the northeast end of the site.\n\nThe place is important in demonstrating the principal characteristics of a particular class of cultural places.\n\nThe surviving quarantine buildings, being substantially intact and mostly in situ, are important in illustrating the principal characteristics of an early 20th century Australian marine quarantine station, including the isolated location, building types (materials, forms, design, aesthetics, coherency and functions), spatial arrangements and grounds layout, and provision of services. Of the three early 20th century quarantine stations established by the Federal Government in Queensland (at Thursday Island, Cape Pallarenda (Townsville) and Lytton (Brisbane)), the Lytton facility was the first and the largest.\n\nThe place is important because of its aesthetic significance.\n\nAppropriate interpretation of the surviving buildings, structures and other elements at the northern end of the former Lytton Quarantine Station has the potential to graphically and physically illustrate the experience of the quarantine process which many immigrants to Australia encountered.\n\nThe place has a strong or special association with a particular community or cultural group for social, cultural or spiritual reasons.\n\nThe place has considerable social significance, being associated with the reception of immigrants in Queensland for nearly 70 years. Being quarantined at Lytton was often an immigrant's first experience of life in Queensland and in Australia.\n\nThe place has a special association with the life or work of a particular person, group or organisation of importance in Queensland's history.\n\nThe place has a special association with the work of Dr JSC Elkington, who was responsible for the design and development of Queensland's early 20th century public health infrastructure, and who became an authority on quarantine practice in post-Federation Australia. The Lytton Quarantine Station remains illustrative of his work.\n"}
{"id": "18151214", "url": "https://en.wikipedia.org/wiki?curid=18151214", "title": "Macrodystrophia lipomatosa", "text": "Macrodystrophia lipomatosa\n\nMacrodystrophia lipomatosa (ML) is a rare congenital disorder characterized by localised overgrowth of a part of an extremity or less commonly a whole extremity. The involvement of more than one extremity is even more uncommon. There is a slight predilection for the lower limb affection namely the foot. The overgrown region consists predominantly of adipose tissue. Yet, other tissue components that represent the mesenchyme may be involved.\n\nMacrodystrophia lipomatosa can manifest in functional incapacitation and esthetic problems. It is usually noticed by parents at birth or shortly after. It may be subject to an increase in size especially around puberty. The diagnosis is largely built upon establishing a comprehensive correlation between history and clinical examination on the one hand and characteristic imaging features on the other hand namely plain radiographs, ultrasound and magnetic resonance imaging examination.\n\nPlain radiographs may show bone overgrowth and deformity together with osteoarthritis changes in advanced cases. Magnetic resonance imaging (MRI) has been shown to delineate the extent of abnormal fatty tissue in various sequences. It may also be used as a tool for the differential diagnosis of ML.\n\nThe differential diagnosis of macrodystrophia lipomatosa includes a wide array of disorders which result in various degrees and patterns of limb overgrowth especially those occurring in childhood and adolescence. \n\nNo medical therapy exists for such a disorder. Treatment depends upon the patient's symptomatology. Gait and functional problems may be addressed by foot wear adjustments. Esthetic complaints may be addressed through plastic surgery procedures such as debulking surgery. If a significant deformity is present a corrective osteotomy can be performed in conjugation. Digital amputation is reserved for severe deformities with or without pain.\n\n"}
{"id": "1024739", "url": "https://en.wikipedia.org/wiki?curid=1024739", "title": "Medical examiner", "text": "Medical examiner\n\nA medical examiner is a person trained in medicine or a medical organization that investigates deaths and injuries that occur under unusual or suspicious circumstances, to perform post-mortem examinations, and in some jurisdictions to initiate inquests. Medical examiners, coroners and forensic pathologists are often thought to be the same, but they are different professions. Coroners have a completely different role as well as having differing responsibilities. In some parts of the United States, unlike a medical examiner or forensic pathologist, coroners are elected officials and may not require the extensive medical training that the former must go through.\n\nA medical examiner's duties may vary depending on location. The medical examiners’ job is usually extensive and has a lot that goes into it. Typically, a medical examiner's duties may include:\n\n\nIn some jurisdictions, a coroner performs these and other duties. It’s not uncommon for a medical examiner to visit crime scenes or to testify in court. This takes a certain amount of confidence in which the medical examiner has to rely on their expertise to make a true testimony and accurately testify the facts of their findings. Medical examiners specialize in forensic knowledge and rely on this during their work. In addition to studying cadavers, they are also trained in toxicology, DNA technology and forensic serology (blood analysis). Pulling from each area of knowledge, a medical examiner can accurately determine a cause of death. This information can help law enforcement crack a case and is crucial to their ability to track criminals in the event of a homicide or other related events.\n\nWithin the United States, there is a mixture of coroner and medical examiner systems, and in some states, dual systems. The requirements to hold office vary widely between jurisdictions.\n\nIn the UK, formal medical training is required for medical examiners. Many employers also request training in pathology while others do not. In the UK, a medical examiner is always a medically trained professional, whereas a coroner is a judicial officer.\n\nPilot studies in Sheffield and seven other areas, which involved medical examiners looking at more than 27,000 deaths since 2008, found 25% of hospital death certificates were inaccurate and 20% of causes of death were wrong. Suzy Lishman, president of the Royal College of Pathologists, said it was crucial there was \"independent scrutiny of causes of death\".\n\nQualifications for medical examiners in the US vary from jurisdiction to jurisdiction. In Wisconsin, for example, some counties do not require individuals to have any special educational or medical training to hold this office. In most jurisdictions, a medical examiner is required to have a medical degree, although in many this need not be in pathology. Other jurisdictions have stricter requirements, including additional education in pathology, law, and forensic pathology. Medical examiners are typically appointed officers.\n\nIn the United States, the road to becoming a medical examiner is a long and hard one. They require a lot of extensive training in order to become experts in their field. After high school, the additional schooling may take 11–18 years. They must attend a college or university to receive a bachelor’s degree in the sciences. Biology is usually the most common. On average, it takes four years to complete a bachelor’s degree. A medical degree (Doctor of Medicine, MD) is required to become a medical examiner. To prepare for medical school, the MCAT (Medical College Admissions Test) must be taken and passed. Medical school is another four years with the first two dedicated to academics and the rest of the two used to gain clinical experience.\n\nAdditional training is required after medical school. The first step is to complete pathological forensic training. This usually consists of anatomic and clinical pathology training which takes anywhere from four to five years to complete. After this, an anatomic pathology residency and/or a fellowship in forensic pathology should be completed. Before practicing, they must also become certified through the American Board of Pathology; this certificate is good for life.\n\nThe general job outlook for medical examiners in the United States is considered to be excellent. Salary varies greatly by state and location, but it is estimated to average between $105,000 and $500,000 a year.\n\n\nMedical examiners are common characters in many crime shows, especially American shows. The following characters are well known medical examiners:\n\n\n\n\n\n"}
{"id": "3530962", "url": "https://en.wikipedia.org/wiki?curid=3530962", "title": "Medical practice consultants", "text": "Medical practice consultants\n\nMedical Practice Consultants or Healthcare Management Consultants typically advise licensed healthcare providers and health-systems on business and administrative issues, but not clinical issues. \n\nThese topics commonly include governance, operations, human resources, finance, billing, coding, transactions and marketing, but there are dozens of subcategories and specialties within the field. \n\nMore and more hospitals engage them as physician practices are acquired and more physicians are employed. There are several trade association in the US for healthcare business consultants, the largest by-far being the National Society of Certified Healthcare Business Consultants (NSCHBC) that also serves dental, podiatric, chiropractic, physical therapy, and other subspecialty practice management consultants. NSCHBC offers a credential in the field; Certified Healthcare Business Consultant or \"CHBC\"; a directory searchable by specialty and state; and continuing education programs. Consulting rates vary from under $100 per hour to over $400 per hour. \n"}
{"id": "47840821", "url": "https://en.wikipedia.org/wiki?curid=47840821", "title": "National Children's Center for Rural and Agricultural Health and Safety", "text": "National Children's Center for Rural and Agricultural Health and Safety\n\nThe National Children's Center for Rural and Agricultural Health and Safety (NCCRAHS) is a part of the National Farm Medicine Center, one of the research centers of the Marshfield Clinic Research Foundation. The center is located headquartered in Marshfield, WI. and is primarily funded by the National Institute for Occupational Safety and Health (NIOSH), which is part of the Centers for Disease Control and Prevention (CDC). The director is Barbara C. Lee, PhD.\n\nThe National Children's Center for Rural and Agricultural Health and Safety (NCCRAHS) was established in 1997, one of ten centers funded by the National Institute for Occupational Safety and Health. The mission of the NCCRAHS is to \"enhance the health and safety of all children exposed to hazards associated with agricultural work and rural environments\".\n\n"}
{"id": "3431970", "url": "https://en.wikipedia.org/wiki?curid=3431970", "title": "Netcare", "text": "Netcare\n\n\"Not to be confused with the electronic health care system Alberta Netcare.\" \nNetcare (\"Network Healthcare Holdings Limited\") is a South African health care company. It is the largest provider of private healthcare (ahead of LIFE Healthcare, formally known as Afrox Healthcare and Mediclinic International) in both South Africa and the United Kingdom. It acquired a controlling stake in the UK's General Healthcare Group in 2006, and provides services to the National Health Service via its subsidiary General Healthcare Group. The current chairman is Meyer Kahn.\n\nNetcare was established in 1996, and was listed on the JSE Limited (Johannesburg Stock Exchange) the same year. It initially expanded into the UK in 2001. In 2002 it won The Ophthalmic Chain contract\nin Kent, Merseyside, Cumbria, Lancashire, Hampshire, and Thames Valley, to carry out 44,500 cataract removals over a 5-year period and the £2.5bn contract for the Greater Manchester Surgical Centre, a 48-bed facility at Trafford General Hospital to provide 44,863 elective procedures over 5 years with a diagnostics programme valued at £1bn. In 2004 it signed a contract to carry out 41,600 cataract operations for the NHS at sites throughout the UK including Cumberland Infirmary.\n\nThe company acquired a controlling stake in General Healthcare Group, the UK's largest private hospital group with 50 hospitals, in 2006 for £2.2 billion. This brought Netcare’s total number of hospitals to 120 with over 11,000 beds, 510 operating theatres, and 37 pharmacies. GHG had a subsidiary company, Amicus Health which tendered for NHS contracts. It had contracts with Stracathro Hospital for 8000 episodes of elective surgery in orthopaedics, urology, general surgery and gastroenterology from 2006-9.The company used Vanguard Healthcare mobile units to treat NHS cataract patients across the UK. The mobile cataract units in Cumbria had failure rates 6 times that of local NHS facilities.\n\n6 patients who were treated at the Royal Hospital Haslar in Portsmouth in 2006 sued after faulty hip operations. Netcare blamed a single rogue surgeon who had since been excluded from operating and said this was ‘an isolated incident’.\n\nIn 2010 five doctors from Netcare, two transplant unit staff, and the chief executive officer were charged for allegedly participating in an international kidney trading syndicate in which poor Brazilians and Romanians were paid to donate kidneys to wealthy Israelis. The company pleaded guilty to conducting 109 illegal operations between 2001 and 2003. It admitted receiving 3.8 million rand (£342,000) from the syndicate. Netcare KwaZulu, in South Africa's eastern KwaZulu-Natal province, paid 7,820,000 rand (£704,000) in fines.\n\n"}
{"id": "23619121", "url": "https://en.wikipedia.org/wiki?curid=23619121", "title": "No Net Cost Tobacco Act of 1982", "text": "No Net Cost Tobacco Act of 1982\n\nThe No Net Cost Tobacco Act of 1982 (P.L. 97-218) required that the Tobacco Price Support Program operate at no net cost to taxpayers, other than for the administrative expenses common to all price support programs. To satisfy this mandate, sellers and buyers (including importers) of tobacco were assessed equally to build a capital account that was drawn upon to reimburse the Commodity Credit Corporation (CCC) for any losses of principal and interest resulting from nonrecourse loan operations. Other provisions of this law provided for reducing the level of support for tobacco and made various modifications to the marketing quota and acreage allotment programs. No net cost assessments ended when price support was terminated after the 2004 crop.\n\n"}
{"id": "14846065", "url": "https://en.wikipedia.org/wiki?curid=14846065", "title": "Nothing But Nets", "text": "Nothing But Nets\n\nNothing But Nets is a global, grassroots campaign of the United Nations Foundation to raise awareness and funding to fight malaria, a leading cause of death among children in Africa.\n\nThe campaign aims to prevent malaria deaths by purchasing, distributing, and teaching the proper use of mosquito bed nets to end malaria deaths in Sub-Saharan Africa. As of 2012, the campaign has distributed nets in twenty countries throughout Sub-Saharan Africa with current plans to expand into Kenya and Ethiopia.\n\nThe UN Foundation and its partners use the money raised by donations to Nothing But Nets to fight malaria. In conjunction with the Measles Initiative, a global vaccination effort to fight measles, Nothing But Nets purchases and distributes bed nets in countries and communities in greatest need.\n\nRick Reilly's 2006 column about malaria in \"Sports Illustrated\", wherein he challenged his readers to donate at least $10 for the purchase of anti-malaria bed nets, brought the issue to national attention. Thousands of Americans across the country donated leading to the creation of the Nothing But Nets campaign.\n\nThe UN Foundation has partnered with groups as diverse as National Basketball Association’s NBA Cares, Major League Soccer WORKS, the people of the United Methodist Church, and \"Sports Illustrated\" to bring Nothing But Nets to the American public. These Founding Partners have been joined by corporate, multimedia, and financial partners to make a significant impact by raising awareness and funds to purchase and distribute bed nets to save lives. Recent significant partners have included the Bill and Melinda Gates Foundation and the Boy Scouts of America. Stephen Curry of the Golden State Warriors is also a big advocate of Nothing But Nets.\n\n\n"}
{"id": "28924009", "url": "https://en.wikipedia.org/wiki?curid=28924009", "title": "Osborn v. Irwin Memorial Blood Bank", "text": "Osborn v. Irwin Memorial Blood Bank\n\nIn Osborn v. Irwin Memorial Blood Bank, 5 Cal.App.4th 234 (1992), the Court of Appeals of California considered certain questions pertaining to whether a blood bank could be held liable for negligence or negligent misrepresentation after a patient contracted HIV/AIDS as a result of a blood transfusion.\n\nIn 1983, Michael Osborn, an infant, underwent surgery to repair a congenital heart defect. During that surgery, Osborn received a blood transfusion. At the time of the transfusion, donated blood was not routinely tested for human immunodeficiency virus, because the cause of AIDS had not definitively been identified yet.\n\nFearing that AIDS could be transmitted by blood, the Osborns had requested to make directed blood donations specifically earmarked for use by Michael. They first contacted the surgeon, who informed them that they needed to contact the blood bank. The receptionist at the blood bank, however, said that directed donations were not allowed, and so the surgery proceeded using blood from the public supply. He subsequently was diagnosed with acquired immune deficiency syndrome (AIDS).\n\nThe patient and his family sued the University of California (where the surgery occurred) and the Irwin Memorial Blood Bank (which supplied the blood) for multiple causes of action. A jury trial took place. The blood bank introduced evidence that its safety procedures were at least as good as those generally prevailing at the time. The blood bank also sought to introduce evidence that, because the patient was type A negative, he could not receive any donations from members of his family, as a result of which only a small amount of blood (or no blood at all) would have been able to be provided by means of directed donation, but the judge did not allow defendants to present this evidence to the jury.\n\nAt the conclusion of the evidence the trial judge granted the defendants' motions for nonsuit on several of the causes of action, and directed a verdict in favor of the university on the remaining causes of action. Thus, only the blood bank remained as a defendant, and the following questions were submitted to the jury:\n\n\nThe jury returned a general verdict in favor of the plaintiffs and awarded them a total of $750,000. The blood bank moved for judgment notwithstanding the verdict and also moved for an amended judgment on the grounds that the damages awarded were in excess of those allowed under the Medical Injury Compensation Reform Act (MICRA). The trial court acknowledged that it had erred when it had held that MICRA did not apply, and the blood bank moved for a new trial.\n\nThe court granted the blood bank's motion for judgment notwithstanding the verdict as to the counts of negligence and intentional misrepresentation and, conditioned on the plaintiffs' acceptance, reduced the award to $416,307. Both sides appealed.\n\nThere were three issues on appeal:\n\n\nThe California Court of Appeals ruled in favor of defendants on the count of negligence, held that the defendants were entitled to a new trial on the issue of negligent misrepresentation, and ruled in favor of the university.\n\n"}
{"id": "26626752", "url": "https://en.wikipedia.org/wiki?curid=26626752", "title": "Pavlovian session", "text": "Pavlovian session\n\nThe Pavlovian session () was the joint session of the USSR Academy of Sciences and the USSR Academy of Medical Sciences held on June 28 to July 4, 1950. The session was organized by the Soviet Government headed by Joseph Stalin in order to fight Western influences in Russian physiological sciences. During the session, a number of Ivan Pavlov's former students attacked another group of his students (Leon Orbeli, Pyotr Anokhin, Aleksey Speransky, Ivan Beritashvili) whom they accused of deviating from Pavlov's teaching. As the result of this session, Soviet physiology self-excluded itself from the international scientific community for many years.\n\nThe Pavlovian session followed a sequence of Stalin's interferences in academic affairs during the post-war time:\n\n\nThe interference in physiology, psychology and psychiatry was initiated in the summer of 1949 when Stalin instructed the Minister of\nHealth Yefim Smirnov to hold a session on Pavlov's teachings. On 28 September 1949, on the eve of the 100th anniversary of Pavlov's birth, Yuri Zhdanov reported to Stalin about the \"serious trouble\" with the development of Pavlov's teaching and put the blame on Orbeli, Beritashvili, and arrested Stern. In replying to this report, Stalin wrote: \"In my opinion, the greatest harm to Academician Pavlov's teaching was done by Academician Orbeli... The sooner Orbeli will be exposed and the more thoroughly his monopoly will be eliminated, the better. Beritov and Stern are not so dangerous because they oppose to Pavlov openly and thus facilitate the reprisal of science against these amateurs of science... Now something about the tactics of the struggle against the opponents of Academician Pavlov's theory. At first, it is necessary to stealthily collect Academician Pavlov's supporters, organize them, assign roles, and only after this to gather the session of physiologists... where it will be necessary to give decisive battle to the opponents. Without this, it can fail. Remember: the enemy should be firmly beaten, with reliance on complete success.\" Georgy Malenkov supervised the organization of the meeting.\n\nFour keynote speakers outlined the main topics of the session: Sergey Vavilov, the President of the USSR Academy of the Sciences; Ivan Petrovich Razenkov, the Vice-President of the USSR Academy of Medical Sciences; Konstantin Bykov, the Director of\nthe General Physiological Department at the Institute of Experimental Medicine; and Anatoly Grigorievitch Ivanov-Smolenskiy, a psychiatrist.\n\nIn his inaugural address, Sergey Vavilov, praised Stalin and Pavlov for their materialistic approach to the problem of relationship between the material and mental. He stated that Pavlov was a great scientist whom Stalin and the Soviet Government esteemed very highly. Vavilov noted that Soviet physiologists had made great achievements since Pavlov's death, but some did not follow Pavlov's teaching and even attempted a revision of Pavlov's views. Open or concealed opposition to Pavlov's materialistic theory was expected and quite understandable for bourgeois scientists who suggested that Pavlov's theory of conditioned reflexes should be shelved and only his experimental methods might be useful. However, even Soviet scientists did very little to develop important trends suggested by Pavlov. For example, experts who participated in a broad discussion of materialistic linguistics in Pravda did not even mention the role of Pavlov's theory in the study of language. Vavilov explained that the goal of the joint session of physiologists and psychiatrists was to conduct \"a critical and self-critical examination of how matters stand with regard to the development of Pavlov's legacy in the Soviet Union\". He concluded: \"There can be no doubt that it is only a return to Pavlov's road that physiology can be most effective, most beneficial to our people and most worthy of the Stalin epoch of the building of Communism. Glory to Pavlov's genius! Long live the leader of peoples, our great scientist and preceptor in all our major undertakings, Comrade Stalin!\"\n\nIvan Razenkov spoke after Vavilov. He emphasized the importance of opposing the \"reactionary idealist trend\" in physiology following the example of Trofim Lysenko who contributed to a \"decisive victory\" of Ivan Vladimirovich Michurin's teachings over Weismannism-Morganism. Razenkov praised Pavlov's contribution to practical medicine and criticized Pavlov's students for not applying the progressive ideas of Pavlov and Ivan Sechenov to theoretical and practical medicine. He blamed Pavlov's immediate disciples and successors: L.N. Fydorov, the former director of the Institute of Experimental Medicine, Leon Orbeli, the director of the Pavlov Institute of Evolutionary Physiology, Pyotr Anokhin, the head of the Moscow Institute of Physiology, and Aleksey Speransky, the head of the Institute of General and Experimental Pathology. According to Razenkov, these scientists did not fight hard enough to defend Pavlov's materialist theory against the assaults of Western idealist physiologists, such as Sherrington, Lashley and Fulton, and Pavlov's opponents in Russia, such as Beritov. Razenkov also expressed some self-criticism for not conforming with the Party and the Government's expectation of the Academy of Medical Sciences. He informed that the Government had created a new scientific institution, the Institute of Physiology of the Higher Nervous System, to advance Pavlov's teachings, and Pavlov's faithful disciple Konstantin Bykov had been named the director of that institute. Razenkov emphasized the importance of the application of the work conducted by Bykov and his colleagues to clinical practice. He concluded his speech with a praise to the \"peerless scientist, Comrade Stalin\".\n\nThe next keynote speaker, Konstantin Bykov, asserted that medical science must be built on the foundation of correct humanitarian sciences in addition to biology and psychology. He praised the triumph of Michurian biology based on the philosophy of materialism. He also praised the \"decisive blow struck at reactionary idealist theories\" by Pavlov. Bykov divided the history of physiology and psychology into two periods: idealistic pre-Pavlovian stage and Pavlovian materialistic stage. Bykov condemned the West-European theories of the pre-Pavlovian stage which explained complex nervous phenomena based on idealistic analytical physiology. The authors of these theories failed to recognize class roots of scientific views. According to Bykov, Pavlov made a transition from analytical to synthetic thinking. He discovered a new class of reflexes, conditioned reflexes. He then developed the theory of higher nervous activity. Under the Soviet System, Pavlovian physiology could develop and flourish. However, some of Pavlov's students failed to follow his theory of higher nervous function and instead diverted to irrelevant issues. Even worse, they accepted Western theories. Bykov named Pavlov's disciples who correctly followed the theories of their teacher: Anatoly Ivanov-Smolenskiy and Ezras Asratovich Asratian. Then, he named the ones who deviated from the right path: Orbeli, Anokhin, Speransky and their coworkers. In particular, Orbeli followed idealist sensory theories of Ewald Hering and Wilhelm Wundt and even claimed that they had similarities with Pavlov's materialist theory. Orbeli's associates A.G. Ginetsinskiy and A.V. Lebedinskiy wrote a textbook for physicians \"Principles of the Physiology in Man and Animals\". in which they treated Pavlov's results as inferior to Western studies. In Anokhin's case, Bykov noted that, although Anoknin had deviated from Pavlov's ideas when Pavlov was still alive, there was still some hope for him and he might correct his mistakes and contribute to Soviet physiology. Bykov praised the contributions of Pavlov's ideas to medicine, emphasized the importance of following the right direction of Pavlov's teachings and resisting false Western theories. Finally, he spoke about Stalin's work that suggested improvement of science through criticism and self-criticism.\n\nIn his long speech, Anatoly Ivanov-Smolenskiy reviewed of Pavlov's achievements in the development of the theory of higher nervous activity. According to Ivanov-Smolenskiy, Pavlov's contribution to psychiatry was \"of immense value\" as opposed to the failure of foreign scientists who did not achieve anything important. Ivanov-Smolenskiy then praised some Russian physiologists and condemned the others. He praised L. A. Andreev and M. K. Petrova as the followers of Pavlov's legacy. He accused Anokhin, Kupalov, and Orbeli. Anokhin was blamed for suggesting that Pavlov's theory was isolated from foreign science and needed improvement, for leaning toward Sherrington's concept of integration, and for criticizing Pavlov's conception of cortical inhibition. Kupalov was accused of distorting Pavlov's conceptualization of reflexes. Ivanov-Smolenskiy characterized Orbeli's views of the relation between subjective experience and objective reality as anti-Pavlovian because — unlike Pavlov who believed that subjective, psychological experience was superimposed on the objective experience of the environment — Orbeli separated the subjective and objective and adhered to psychophysiological parallelism. Orbeli was also blamed for diverging from Pavlov's deterministic position on the mechanisms of higher nervous activity.\n\nIn the sessions that followed the keynote speeches, a number of speakers continued to attack the accused Pavlovians, and the accused confessed to their errors and expressed apologies.\n\nEzras Asratian spoke on June 29. According to him, several Pavlovians failed the expectations of the Communist Party and the Soviet government. In particular, they failed to pursue research in several important fields, for example cortical localization of functions and fixation of inherited conditioned reflexes in the next generation. They also failed to challenge the anti-Pavlovian theories of Western physiologists.\n\nIn 1982, M.G. Yaroshevsky, criticizing the Pavlovian session, wrote that, in fact, Ivanov-Smolenskiy and his disciples did nothing but pervert the kernel of Pavlovian teaching, substituting for it a mechanistic view of the brain activity. These so-called scholars of Pavlov emasculated the ground of his theory and extremely damaged the prospects of Soviet science.\n\nA precursor of later abuses in psychiatry in the Soviet Union and the most somber event in the history of Russian-Soviet psychiatry was the so-called 'Joint Session' of the USSR Academy of Medical Sciences and the Board of the All-Union Neurologic and Psychiatric Association, held in the name of Ivan Pavlov in October 1951, considered the matter of several leading neuroscientists and psychiatrists of the time (for example, Grunya Sukhareva, Vasily Gilyarovsky, Raisa Golant, Aleksandr Shmaryan, Mikhail Gurevich) who were charged with practicing 'anti-Pavlovian, anti-Marxist, idealistic, reactionary' science damaging to Soviet psychiatry. These talented psychiatrists had to admit publicly to their wrong beliefs and mistakes and promise to profess only Pavlov's teaching. During the Joint Session, scientists falsely acknowledged their 'wrongdoings' and gave up their beliefs, out of fear. But in the closing speech, the lead author of the policy report Andrei Snezhnevsky stated that they \"have not disarmed themselves and continue to remain in the old anti-Pavlovian positions\", thereby causing \"grave damage to the Soviet scientific and practical psychiatry\", and the vice president of the USSR Academy of Medical Sciences accused them that they \"diligently fall down to the dirty source of American pseudo-science\". The fear and less than noble ambitions of the accusers including Irina Strelchuk, Vasily Banshchikov, Oleg Kerbikov, and Andrei Snezhnevsky were also likely to make them serve in the role of inquisitors. Not surprisingly, many of them were advanced and appointed to leadership positions shortly after the session.\n\nThe Joint Session also affected neuroscience in such a way that the best neuroscientists of the time, such as academicians Pyotr Anokhin, Aleksey Speransky, Lina Stern, Ivan Beritashvili, and Leon Orbeli, who headed various scientific directions at that time, were labeled as anti-Pavlov, anti-materialist and reactionaries, and discharged from their positions. These scientists lost their laboratories, and some were subjected to tortures in prisons. The Moscow, Leningrad, Ukrainian, Georgian, and Armenian schools of neuroscience and neurophysiology were damaged, at least for a while. The Joint Session ravaged productive research in neurosciences and psychiatry for years to come. It was pseudoscience that took over.\n\nAfter the joint meeting of the USSR Academy of Medical Sciences and the USSR Academy of Sciences (Pavlovian Session of 1950), pathophysiology of higher nervous activity was established as a new discipline mandatory for all of the USSR psychiatrists who underwent retraining in accordance with this concept. According to the postulates of pathophysiology of higher nervous activity, the development of all mental disorders was explained in terms of the changed relations between the excitation and inhibition, their interference and different phases of the inhibition. Psychological approaches during diagnosing, treating and explaining the mechanisms of mental disorders have been banned and virtually excluded from the practice of psychiatrists. This ban was based on the ideological concept of labeling all psychological theories of personality, especially psychoanalytic ones, as reactionary and idealistic.\n\nAfter the joint session of the Academy of Sciences and the Academy of Medical Sciences on June 28 — July 4, 1950 and during the session of the Presidium of the Academy of Medical Sciences and the Board of the All-Union Society of Neuropathologists and Psychiatrists on October 11–15, 1951, the leading role was given to Snezhnevky's school. The 1950 decision to give monopoly over psychiatry to the Pavlovian school of Professor Andrei Snezhnevsky was one of crucial factors of the onset of political psychiatry. The Soviet doctors, under the incentive of Snezhnevsky, devised 'Pavlovian theory of schizophrenia' on the strength of which they diagnosticated this illness in political oppositionists.\n\n"}
{"id": "788074", "url": "https://en.wikipedia.org/wiki?curid=788074", "title": "Physical strength", "text": "Physical strength\n\nPhysical strength is the measure of an animal's exertion of force on physical objects. Increasing physical strength is the goal of strength training.\n\nAn individual's physical strength is determined by two factors; the cross-sectional area of muscle fibers recruited to generate force and the intensity of the recruitment. Individuals with a high proportion of type I slow twitch muscle fibers will be relatively weaker than a similar individual with a high proportion of type II fast twitch fibers, but would have a greater inherent capacity for physical endurance. The genetic inheritance of muscle fiber type sets the outermost boundaries of physical strength possible (barring the use of enhancing agents such as testosterone), though the unique position within this envelope is determined by training. Individual muscle fiber ratios can be determined through a muscle biopsy. Other considerations are the ability to recruit muscle fibers for a particular activity, joint angles, and the length of each limb. For a given cross-section, shorter limbs are able to lift more weight. The ability to gain muscle also varies person to person, based mainly upon genes dictating the amounts of hormones secreted, but also on sex, age, health of the person, and adequate nutrients in the diet. A one-repetition maximum test is the most accurate way to determine maximum muscular strength.\n\nThere are various ways to measure physical strength of a person or population. Strength capability analysis is usually done in the field of ergonomics where a particular task (e.g. lifting a load, pushing a cart, etc.) and/or a posture is evaluated and compared to the capabilities of the section of the population that the task is intended towards. The external reactive moments and forces on the joints are usually used in such cases. The strength capability of the joint is denoted by the amount of moment that the muscle force can create at the joint to counter the external moment.\n\nSkeletal muscles produce reactive forces and moments at the joints. To avoid injury or fatigue, when person is performing a task, such as pushing or lifting a load, the external moments created at the joints due to the load at the hand and the weight of the body segments must be ideally less than the muscular moment strengths at the joint.\n\nOne of the first sagittal-plane models to predict strength was developed by Chaffin in 1969. Based on this model, the external moments at each joint must not exceed the muscle strength moments at that joint.\n\nM < S\n\nWhere, S is the muscle strength moment at joint, j, and M is the external moment at the joint, j, due to load, L and the body segments preceding the joint in the top-down analysis.\n\nTop-down analysis is the method of calculating the reactive moments and forces at each joint starting at the hand, all the way till the ankle and foot. In a 6-segment model, the joints considered are elbow, shoulder, L5/S1 disc of the spine, hip, knee and ankle. It is common to ignore the wrist joint in manual calculations. Software intended for such calculation use the wrist joint also, dividing the lower arm into hand and forearm segments.\n\nStatic strength prediction is the method of predicting the strength capabilities of a person or a population (based on anthropometry) for a particular task and/or posture (an isometric contraction). Manual calculations are usually performed using the top-down analysis on a six or seven-link model, based on available information about the case and then compared to standard guidelines, such as the one provided by the National Institute for Occupational Safety and Health, to predict capability.\n\n"}
{"id": "7851157", "url": "https://en.wikipedia.org/wiki?curid=7851157", "title": "Piscina Mirabilis", "text": "Piscina Mirabilis\n\nThe Piscina Mirabilis is an ancient Roman cistern on the Bacoli hill at the western end of the Gulf of Naples, southern Italy. It was one of the largest ancient cisterns and was situated there in order to provide the Roman western imperial fleet at Portus Julius with drinking water.\n\nThe cistern was dug entirely out of the tuff hill and was high, long, and wide. The capacity was . It was supported by vaulted ceilings and a total of 48 pillars. It was supplied with water from the main Roman aqueduct, the Aqua Augusta, which brought water from sources in Serino near Avellino, 100 kilometres distant, to most of the sites around Naples.\n\nThe ancient cistern is currently in private hands, but it may be visited by the public.\n\n\n"}
{"id": "31615916", "url": "https://en.wikipedia.org/wiki?curid=31615916", "title": "Positive psychology in the workplace", "text": "Positive psychology in the workplace\n\nImplementing positive psychology in the workplace means creating an environment that is relatively enjoyable and productive. This also means creating a work schedule that does not lead to emotional and physical distress.\n\nPositive psychology in the workplace is about shifting attention away from negative aspects such as work violence, stress, burnout, and job insecurity. Through the employment of positive psychology, a working environment with a goal of promoting positive affect in its employees can be created.\n\nFun should not be looked at as something that cannot be achieved during work but rather as a motivation factor for the staff. Along this line, it is important to examine the role of: helping behaviors, team building exercises, job resources, job security and work support.\n\nThe emerging field of positive psychology also helps to creatively manage organizational behaviors and to increase productivity in the workplace through applying positive organizational forces. Recent researches on job satisfaction and employee retention have created a great need to focus on implementing positive psychology in the workplace.\n\nAccording to the United States Department of Labor, “In 2009 employed persons worked an average of 7.5 hours on the days they worked, which were mostly weekdays. [In addition to that], 84 percent of employed persons did some or all of their work at their workplace.[1]” This indicates that majority of the population spend their waking hours at work, outside their homes. Therefore, employers must do their best to create a low stress and inspiring work environment to yield greater productivity.\n\nMichelle T. Iaffaldano and Paul M. Muchinsky were among the first people to ignite interest in the connection between job satisfaction and job performance. The meta-analytic research of these individuals impacted the way in which later research on the topic was conducted, especially regarding sample sizes.\n\nMartin E.P. Seligman and Mihaly Csikszentmihalyi are noted frontrunners in the area of positive psychology as a field of study. They state that “psychology has become a science largely about healing. Therefore its concentration on healing largely neglects the fulfilled individual and thriving community”. Seligman and Csikszentmihalyi further stress that, “the aim of positive psychology is to begin to catalyze a change in the focus of psychology from preoccupation only with repairing the worst things in life to also building positive qualities.”\n\nAbraham Maslow and Carl Rogers developed Humanistic Psychology that focuses on the positive potential of people and on helping people reach their full potential.\n\nPeter Warr is noted for his early work on work well being. “Proponents of the well-being perspective argue that the presence of positive emotional states and positive appraisals of the worker and his or her relationships within the workplace accentuate worker performance and quality of life”. A common idea in work environment theories is that demands match or slightly exceed the resources. With regards to research concerning positive outcomes within the employment setting, several models have been established like \"Demand Control\", \"Job Demands-Resources\", and \"Job Characteristics\".\n\nRobert A. Karasek is credited with this particular work design model. In Karasek’s model, workplace stress is in indicator of how taxing a worker's job is and how much control, authority, discretion, and decision latitude the worker has over his/her tasks and duties. This creates four kinds of jobs—passive, active, low strain and high strain The Demand Control Model (DCM) has been used by researchers to design jobs that enhance the psychological and physical well-being. This model promotes a work design that proposes high demand and high control, fostering an environment that encourages learning and simultaneously offers autonomy.\n\nThis model is based on the assumption that “workers with active jobs are more likely to seek challenging situations that promote mastery, thereby encouraging skill and knowledge acquisition”. It also points out the role of social support, referring to the quality interactions between colleagues and managers. However, there is some controversy over this model because some researchers believe it lacks evidence for the interaction between demand and control.\n\nThe DCM is commonly criticized for its inability to consistently replicate findings to support its basic assumption. The DCM has been criticized for “its simplicity, inability to capture the complexity of work environments.However, there is evidence supporting the idea that “high amounts of job control is associated with increases in job satisfaction and decreased depression, however, high demands with out adequate control may lead to increase anxiety”.\n\nThe job demands-resources model (JD-R) is an expansion of the DCM and is founded on the same principle that high job demands and high job resources produce employees with more positive work attitudes. The difference between the JD-R and DCM is that the JD-R expounds upon the differentiation between demand and resources, as well as encompasses a broader view of resources. This model refers to demands as “ those physical, psychological, social, or organizational aspects of the job that require sustained physical and/or psychological effort. This may refer to jobs that require contact with customers. Resources are regarded as “those physical, psychological, social, or organizational aspects of the job that are either/or: (1) functional in achieving work goals; (2) reduce job demands and the associated physiological and psychological costs; and (3) stimulate personal growth, learning, and development”. Another difference between these two theories is that the JD-R postulates that resources can be predictors of motivation and learning related outcomes. The findings by Bakker and colleagues supports their hypothesis that many resources may be linked to job well-being. They also found that “task enjoyment and organizational commitment are the result of combinations of many different job demands and job resources. Enjoyment and commitment were high when employees were confronted with challenging and stimulating tasks, and simultaneously had sufficient resources at their disposal”.\n\nThe job characteristics model (JCM) is “an influential theory of work design developed by Hackman and Oldham. It is based upon five characteristics - skill variety, task identity, task significance, task autonomy, and task feedback - which are used to identify the general content and structure of jobs”. This model argues that employees with a personal need for growth and development, as well as knowledge and skill, will display more positive work outcomes. These include things such as: job satisfaction, lower absenteeism, and better work turnover. This model is based upon an idea that high task control and feedback are two essential elements for maximizing work potential. Stronger experiences of these five traits is said to lead to greater job satisfaction and better performance.\n\nIn order to protect the physical and mental health of workers, the demands of the job must be balanced by easily accessible job resources in order to prevent burnout in employees yet encourage employee engagement. The interaction between the demand and resources within a job determines employee engagement or burnout. Engagement signifies a positive employee who is committed to the safety within the workplace for self and others. In contrast, burnout represents a negative employee possessing elements of anxiety, depression, and work-related stress. Engagement increases as job resources like knowledge of safety are present. On the other hand, burnout increases when more job demands are present without the buffering effects of job resources.\n\nHazards in the workplace can be seen as a combination of the physical demands of the work and the complexity of the work. Job resources provide a buffering effect that protects the employees from job demands like high work pressure, an unfavorable physical environment, and emotionally demanding interactions. Employees are better equipped to handle changes in their work environment when resources are readily available. The resources a job can provide include autonomy, support, and knowledge of safety. Autonomy allows employees the freedom to decide how to execute their work. Support can originate directly from a supervisor or from other workers in the environment. And lastly, employees must have knowledge about safety procedures and policies. When the employee is able to work in a safe environment, workers are more satisfied with their jobs. A safe environment provides support and resources that promote healthy employees.\n\nEmotional intelligence is the ability to recognize, and interpret emotions that can be used to regulate emotions and assist cognitive processes which promote emotional and intellectual growth. Emotional intelligence has been researched by Carmelli (2003) in order to see its effect on employees work performance. Due to the social nature of the interactions of the employees, emotional intelligence is essential in order to work well with co-workers. When employees work well together their task performance improves and as a result the business benefits. With emotional intelligence, employees are better able to perceive others needing help and are more willing to help for intrinsic benefits.\n\nIsen & Reeve (2005) proposed that positive affect led to positive intrinsic motivation for completing a task. As a result of the intrinsic motivation, the employees enjoyed the task more and were more optimistic when having to complete more uninteresting task. The combination of having the freedom to choose tasks and maintaining positive affect results in better task performance. Positive affect promotes self-control to remain focused on any task and forward-looking thinking that motivates workers to look-forward to more enjoyable tasks.\n\nConcepts of positive psychology like hope and altruism provide a positive work environment that influences the moods and attitudes of workers. Youssef & Luthans (2007) examined the effects hope, optimism, and resilience had in the workplace on employees’ job performance, job satisfaction, work happiness, and organizational commitment. Hope and resilience had a more direct effect on organizational commitment whereas hope had a greater impact on performance. Hope allows employees to be better at creating more realistic plans for completing task so as not to focus on the failure that accompanies an incomplete task. Optimism strengthens the employee’s resilience to break through barriers and causes the employee to build social support and other strengths to overcome obstacle he or she may encounter.\n\nPositive psychology also encourages maintaining positive mood in the work environment to encourage productivity on an individual level and organizational level. Organizational citizenship behaviors (OCB) refer to behaviors like altruism and compliance that are not formal tasks in that the behaviors are not a mandatory of the workers job description. They are considered extra-role behaviors that help in gauging the workers commitment to the job and to the rules of the job in the absence of monitoring these behaviors. OCB have proven to improve the moods of employees and the moods in the workplace. A helping behavior improves mood because the individual is no longer focused of negative moods; helping others acts as a distracter for the employee. Altruism is effective because it has more impact in a social setting like the workplace and is more extrinsically rewarding. OCB encourage positive interactions among workers and lead to better psychological health for employees.\n\nAccording to Froman (2010), having a more hopeful perspective about life leads one to being more optimistic about responding to opportunities. Workers are more resilient to adversity and are able to bounce back more quickly. When organizations encourage positive attitudes in their employees, they grow and flourish. As a result, the organization profits and grows from the human capital of productive employees and the monetary capital resulting from productive workers.\n\nChan (2010) studied fun activities in the workplace that created a positive work environment that could retain and attract employees and encourage employee well-being. Activities must be enjoyable and pleasurable. The activities also encourage employees to be more responsible and a team player. These qualities empower employees to more engaged with their work, take on more leadership roles, and experience less stress. Making work fun promotes positive, happy moods in employees that in turn increase job satisfaction and organizational commitment. According to Chan’s framework, workplace fun must be staff-oriented, supervisor-oriented, social-oriented, or strategy oriented. While staff-oriented activities focus on creating fun work for employees, supervisor-oriented activities create a better relationship between the employees and supervisors. Social-oriented activities create social events that are organizational-based (i.e. company barbecue or Christmas office party). Strategy-oriented activities allow more autonomy with employees in different aspects of their work in hopes of cultivating strengths within the organization’s employees. The framework proposes that a fun work environment promotes employee well-being in addition to fostering creativity, enthusiasm, satisfaction and communication among the organization’s employees. The research found in this study hopes to encourage implementing other work fun activities in other various industries in order to engage and retain positive employees.\n\nThere are several examples of popular, real-world uses of infusing Positive Psychology in the workplace. In such contexts such as a workplace, researchers often hope to examine and measure variable levels of such factors such as productivity and organization. One such popular model is the aforementioned Job Characteristics Model (JCM), which applies influential theories of work as it correlates to the five central characteristics of skill variety, task identity, task significance, task autonomy, and task feedback. However, such practices such as business teams within a workplace often present the varying dynamics of positivity and negativity in business behaviors. There are often a plethora of special research teams that go into looking at certain workplaces in order to help report to employers the status of their employees. Furthermore, the three psychological states often measured and examined are: meaningfulness of performed work, responsibility of outcomes, and results knowledge. In mixing together these aspects, a score is generated in order to observe a range reflecting a job quality. In addition, each score details the differing degrees of autonomy and necessary feedback as it relates to ensuring high quality work. Most research points to the fact that typical teams of high performance are those that function high on positivity in their workplace behaviors.\n\nAdequate research regarding whether the practice of measuring factors, such as positive behaviors is lacking. More specifically, in attempting to measure some form of a variable in order to later ensure a positive environment context in the workplace, there is debate to an extent regarding which proper components to value and measure. Additionally, the act and process of specifically looking into certain factors of productivity in the workplace can also go on to influence workers negatively due to pressure.\n\nThe multitudes of research and new, developing information detailing the possibility of positive psychology at work often deals with reporting workplace safety, the engagement of the employees, productivity, and overall happiness. Moreover, understanding the significance of a healthy work environment can directly provide and contribute to work mastery and work ethic. Motivation, researchers have learned, helps to keep a reinforced sense of both discipline and a higher perception which then yields to higher levels of efficiency for both employees and employers.\n\n\n"}
{"id": "36687158", "url": "https://en.wikipedia.org/wiki?curid=36687158", "title": "Qazi–Markouizos syndrome", "text": "Qazi–Markouizos syndrome\n\nQazi–Markouizos syndrome is a rare hereditary condition characterized by non-progressive, congenital hypotonia, severe intellectual disability, an increased proportion of type 2 muscle fibers, which additionally exhibited increased size, as well as dysharmonic skeletal maturation. To date, the molecular mechanism of Qazi–Markouizos syndrome, which is also known as Puerto Rican infant hypotonia syndrome, remains unknown.\n\n"}
{"id": "1362038", "url": "https://en.wikipedia.org/wiki?curid=1362038", "title": "Quaternary ammonium cation", "text": "Quaternary ammonium cation\n\nQuaternary ammonium cations, also known as quats, are positively charged polyatomic ions of the structure , R being an alkyl group or an aryl group. Unlike the ammonium ion () and the primary, secondary, or tertiary ammonium cations, the quaternary ammonium cations are permanently charged, independent of the pH of their solution. Quaternary ammonium salts or quaternary ammonium compounds (called quaternary amines in oilfield parlance) are salts of quaternary ammonium cations.\nQuaternary ammonium compounds are prepared by the alkylation of tertiary amines with a halocarbon. In older literature this is often called a Menshutkin reaction, however modern chemists usually refer to it simply as quaternization. The reaction can be used to produce a compound with unequal alkyl chain lengths; for example when making cationic surfactants one of the alkyl groups on the amine is typically longer than the others. A typical synthesis is for benzalkonium chloride from a long-chain alkyldimethylamine and benzyl chloride:\n\nQuaternary ammonium cations are unreactive toward even strong electrophiles, oxidants, and acids. They also are stable toward most nucleophiles. The latter is indicated by the stability of the hydroxide salts such as tetramethylammonium hydroxide and tetrabutylammonium hydroxide. Because of their resilience, many unusual anions have been isolated as the quaternary ammonium salts. Examples include tetramethylammonium pentafluoroxenate, containing the highly reactive pentafluoroxenate () ion. Permanganate can be solubilized in organic solvents, when deployed as its salt.\n\nWith exceptionally strong bases, quat cations degrade. They undergo Sommelet–Hauser rearrangement and Stevens rearrangement, as well as dealkylation under harsh conditions. Quaternary ammonium cations containing N–C–C–H units can also undergo the Hofmann elimination and Emde degradation.\nQuaternary ammonium salts are used as disinfectants, surfactants, fabric softeners, and as antistatic agents (e.g. in shampoos). In liquid fabric softeners, the chloride salts are often used. In dryer anticling strips, the sulfate salts are often used. Spermicidal jellies also contain quaternary ammonium salts.\n\nQuaternary ammonium compounds have also been shown to have antimicrobial activity. Certain quaternary ammonium compounds, especially those containing long alkyl chains, are used as antimicrobials and disinfectants. Examples are benzalkonium chloride, benzethonium chloride, methylbenzethonium chloride, cetalkonium chloride, cetylpyridinium chloride, cetrimonium, cetrimide, dofanium chloride, tetraethylammonium bromide, didecyldimethylammonium chloride and domiphen bromide. Also good against fungi, amoebas, and enveloped viruses, quaternary ammonium compounds are believed to act by disrupting the cell membrane. Quaternary ammonium compounds are lethal to a wide variety of organisms except endospores, \"Mycobacterium tuberculosis\" and non-enveloped viruses.\n\nQuaternary ammonium compounds are cationic detergents, as well as disinfectants, and as such can be used to remove organic material. They are very effective in combination with phenols. Quaternary ammonium compounds are deactivated by anionic detergents (including common soaps). Also, they work best in soft waters. Effective levels are at 200 ppm. They are effective at temperatures up to .\n\nQuaternary ammonium salts are commonly used in the foodservice industry as sanitizing agents.\n\nIn organic chemistry, quaternary ammonium salts are employed as phase transfer catalysts (PTCs). Such catalysts accelerate reactions between reagents dissolved in immiscible solvents. The highly reactive reagent dichlorocarbene is generated via PTC by reaction of chloroform and aqueous sodium hydroxide.\nIn the 1950s, distearyldimethylammonium chloride (DHTDMAC), was introduced as a fabric softener. This compound was discontinued because the cation biodegrades too slowly. Contemporary fabric softeners are based on salts of quaternary ammonium cations where the fatty acid is linked to the quaternary center via ester linkages; these are commonly referred to as betaine-esters or ester-quats and are susceptible to degradation, e.g., by hydrolysis. Characteristically, the cations contain one or two long alkyl chains derived from fatty acids linked to an ethoxylated ammonium salt. Other cationic compounds can be derived from imidazolium, guanidinium, substituted amine salts, or quaternary alkoxy ammonium salts.\n\nCycocel (chlormequat chloride) reduces plant height by inhibiting the production of gibberellins, the primary plant hormones responsible for cell elongation. Therefore, their effects are primarily on stem, petiole and flower stalk tissues. Lesser effects are seen in reductions of leaf expansion, resulting in thicker leaves with darker green color.\n\nQuaternary ammonium compounds are present in osmolytes, specifically glycine betaine, which stabilize osmotic pressure in cells.\n\nCholine is also a precursor for the neurotransmitter acetylcholine.\n\nCholine is also a constituent of lecithin, which is present in many plants and animal organs. It is found in phospholipids. For example, phosphatidylcholines, a major component of biological membranes, are a member of the lecithin group of fatty substances in animal and plant tissues.\n\nQuaternary ammonium compounds can display a range of health effects, amongst which are mild skin and respiratory irritation up to severe caustic burns on skin and gastrointestinal lining (depending on concentration), gastrointestinal symptoms (e.g., nausea and vomiting), coma, convulsions, hypotension and death.\n\nThey are thought to be the chemical group responsible for anaphylactic reactions that occur with use of neuromuscular blocking drugs during general anaesthesia in surgery. Quaternium-15 is the single most often found cause of allergic contact dermatitis of the hands (16.5% in 959 cases)\n\nQuaternary ammonium-based disinfectants (Virex and Quatricide) were tentatively identified as the most probable cause of jumps in birth defects and fertility problems in caged lab mice.\n\nThe quantification of quaternary ammonium compounds in environmental and biological samples is problematic using conventional chromatography techniques because the compounds are highly soluble in water. While analyzing them by liquid chromatography coupled tandem mass spectrometry it has been found that they follow an exception rule. Under standard electrospray ionization (ESI) conditions, mono- and di-quaternary ammonium compounds form molecular ions with the formula of \"\" rather than . Formation of is observed for di-quaternary ammonium compounds (like diquat) as precursor ion and as product ion due to the loss of one of the quaternary charge during CID. In di-quaternary ammonium compounds, this process can also result in the formation of fragment ions with higher mass as compared to their precursor ion. Hydrophilic interaction liquid chromatographic separation has been reported to demonstrate a successful separation of quaternary ammonium compounds for their quantification in ESI-MS/MS with higher precision.\n\n"}
{"id": "32275854", "url": "https://en.wikipedia.org/wiki?curid=32275854", "title": "Scrotal ultrasound", "text": "Scrotal ultrasound\n\nScrotal (or transscrotal) ultrasound is a medical ultrasound examination of the scrotum. It is used in the evaluation of testicular pain, and can help identify solid masses.\n\nAlthough the development of new imaging modality such as computerized tomography and magnetic resonance imaging have open a new era for medical imaging, high resolution sonography remains as the initial imaging modality of choice for evaluation of scrotal\ndisease. Many of the disease processes, such as testicular torsion, epididymo-orchitis, and intratesticular tumor, produce the common symptom of pain at presentation, and differentiation of these conditions and disorders is important for determining the\nappropriate treatment. High resolution ultrasound helps in better characterize some of the intrascrotal lesions, and suggest a more specific diagnosis, resulting in more appropriate treatments and avoiding unnecessary operation for some of the diseases.\n\nFor any scrotal examination, thorough palpation of the scrotal contents and history taking\nshould precede the sonographic examination. Patients are usually examined in the supine\nposition with a towel draped over his thighs to support the scrotum. Warm gel should\nalways be used because cold gel can elicit a cremasteric response resulting in thickening of\nthe scrotal wall; hence a thorough examination is difficult to be performed. A high\nresolution, near-focused, linear array transducer with a frequency of 7.5 MHz or greater is\noften used because it provides increased resolutions of the scrotal contents. Images of both\nscrotum and bilateral inguinal regions are obtained in both transverse and longitudinal\nplanes. Color Doppler and pulsed Doppler examination is subsequently performed,\noptimized to display low-flow velocities, to demonstrate blood flow in the testes and\nsurrounding scrotal structures. In evaluation of acute scrotum, the asymptomatic side\nshould be scanned first to ensure that the flow parameters are set appropriately. A\ntransverse image including all or a portion of both testicles in the field of view is obtained to\nallow side-to-side comparison of their sizes, echogenicity, and vascularity. Additional views\nmay also be obtained with the patient performing Valsalva maneuver.\n\nThe normal adult testis is an ovoid structure measuring 3 cm in anterior-posterior\ndimension, 2–4 cm in width, and 3–5 cm in length. The weight of each testis normally ranges\nfrom 12.5 to 19 g. Both the sizes and weights of the testes normally decrease with age. At\nultrasound, the normal testis has a homogeneous, medium-level, granular echotexture. The\ntesticle is surrounded by a dense white fibrous capsule, the tunica albuginea, which is often\nnot visualized in the absence of intrascrotal fluid. However, the tunica is often seen as an\nechogenic structure where it invaginates into the testis to form the mediastinum testis. In the testis, the seminiferous tubules converge to form the rete testes, which is\nlocated in the mediastinum testis. The rete testis connects to the epididymal head via the\nefferent ductules. The epididymis is located posterolateral to the testis and measures 6–7 cm\nin length. At sonography, the epididymis is normally iso- or slightly hyperechoic to the\nnormal testis and its echo texture may be coarser. The head is the largest and most easily\nidentified portion of the epididymis. It is located superior-lateral to the upper pole of the\ntesticle and is often seen on paramedian views of the testis. The normal epididymal\nbody and tail are smaller and more variable in position.\n\nThe testis obtains its blood supply from the deferential, cremasteric and testicular arteries.\nThe right and left testicular arteries, branches of the abdominal aorta, arise just distal to the\nrenal arteries, provide the primary vascular supply to the testes. They course through the\ninguinal canal with the spermatic cord to the posterior superior aspect of the testis. Upon\nreaching the testis, the testicular artery divides into branches, which penetrate the tunica\nalbuginea and arborize over the surface of the testis in a layer known as tunica vasculosa.\nCentripetal branches arising from the capsular arteries carry blood toward the mediastinum,\nwhere they divide to form the recurrent rami that carry blood away from the mediastinum\ninto the testis. The deferential artery, a branch of the superior vesicle artery and the\ncremasteric artery, a branch of the inferior epigastric artery, supply the epididymis, vas\ndeferens, and peritesticular tissue.\n\nFour testicular appendages have been described: the appendix testis, the appendix\nepididymis, the vas aberrans, and the paradidymis. They are all remnants of embryonic\nducts. Among them, the appendix testis and the appendix epididymis are usually seen at scrotal US. The appendix testis is a\nmullerian duct remnant and consists of fibrous tissue and blood vessels within an envelope\nof columnar epithelium. The appendix testis is attached to the upper pole of the testis and\nfound in the groove between the testis and the epididymis. The appendix epididymis is\nattached to the head of the epididymis. The spermatic cord, which begins at the deep\ninguinal ring and descends vertically into the scrotum consists of vas deferens, testicular\nartery, cremasteric artery, deferential artery, pampiniform plexuses, genitofemoral nerve,\nand lymphatic vessel.\n\nOne of the primary indications for scrotal sonography is to evaluate for the presence of\nintratesticular tumor in the setting of scrotal enlargement or a palpable abnormality at\nphysical examination. It is well known that the presence of a solitary intratesticular solid\nmass is highly suspicious for malignancy. Conversely, the vast majority of extratesticular\nlesions are benign.\n\nPrimary intratesticular malignancy can be divided into germ cell tumors and non–germ cell\ntumors. Germ cell tumors are further categorized as either seminomas or nonseminomatous\ntumors. Other malignant testicular tumors include those of gonadal stromal origin,\nlymphoma, leukemia, and metastases.\n\nApproximately 95% of malignant testicular tumors are germ cell tumors, of which\nseminoma is the most common. It accounts for 35%–50% of all germ cell tumors. Seminomas occur in a slightly older age group when compared with other nonseminomatous tumor, with a peak incidence in the forth and fifth decades. They are less\naggressive than other testicular tumors and usually confined within the tunica albuginea at\npresentation. Seminomas are associated with the best prognosis of the germ cell tumors\nbecause of their high sensitivity to radiation and chemotherapy.\n\nSeminoma is the most common tumor type in cryptorchid testes. The risk of developing a\nseminoma is increased in patients with cryptorchidism, even after orchiopexy. There is an\nincreased incidence of malignancy developing in the contralateral testis too, hence\nsonography is sometimes used to screen for an occult tumor in the remaining testis.\nOn US images, seminomas are generally uniformly hypoechoic, larger tumors may be more\nheterogeneous [Fig. 3]. Seminomas are usually confined by the tunica albuginea and rarely\nextend to peritesticular structures. Lymphatic spread to retroperitoneal lymph nodes and\nhematogenous metastases to lung, brain, or both are evident in about 25% of patients at the\ntime of presentation.\n\nNonseminomatous germ cell tumors most often affect men in their third decades of life.\nHistologically, the presence of any nonseminomatous cell types in a testicular germ cell\ntumor classifies it as a nonseminomatous tumor, even if most of the tumor cells belong to\nseminona. These subtypes include yolk sac tumor, embryonal cell carcinoma, teratocarcinoma,\nteratoma, and choriocarcinoma. Clinically nonsemionatous tumors usually present as mixed\ngerm cell tumors with variety cell types and in different proportions.\n\nEmbryonal cell carcinoma\n\nEmbryonal cell carcinomas, a more aggressive tumor than\nseminoma usually occurs in men in their 30s. Although it is the second most common\ntesticular tumor after seminoma, pure embryonal cell carcinoma is rare and constitutes only\nabout 3 percent of the nonseminomatous germ cell tumors. Most of the cases occur in\ncombination with other cell types.\nAt ultrasound, embryonal cell carcinomas are predominantly hypoechoic lesions with ill\ndefined margins and an inhomogeneous echotexture. Echogenic foci due to hemorrhage,\ncalcification, or fibrosis are commonly seen. Twenty percent of embryonal cell carcinomas\nhave cystic components. The tumor may invade into the tunica albuginea\nresulting in contour distortion of the testis [Fig. 4].\n\nYolk sac tumor\n<br>Yolk sac tumors also known as endodermal sinus tumors account for 80%\nof childhood testicular tumors, with most cases occurring before the age of 2 years. Alpha-fetoprotein is normally\nelevated in greater than 90% of patients with yolk sac tumor (Woodward et al, 2002, as cited\nin Ulbright et al, 1999). In its pure form, yolk sac tumor is rare in adults; however yolk sac\nelements are frequently seen in tumors with mixed histologic features in adults and thus\nindicate poor prognosis. The US appearance of yolk sac tumor is usually nonspecific and\nconsists of inhomogeneous mass that may contain echogenic foci secondary to hemorrhage.\nChoriocarcinoma --- Choriocarcinoma is a highly malignant testicular tumor that usually\ndevelops in the 2nd and 3rd decades of life. Pure choriocarcinomas are rare and represent\nonly less than 1 percent of all testicular tumors. Choriocarcinomas\nare composed of both cytotrophoblasts and syncytiotrophoblasts, with the latter responsible\nfor the clinical elevation of human chorionic gonadotrophic hormone level. As microscopic\nvascular invasion is common in choriocarcinoma, hematogeneous metastasis, especially to\nthe lungs is common. Many\nchoriocarcinomas show extensive hemorrhagic necrosis in the central portion of the tumor;\nthis appears as mixed cystic and solid components at ultrasound.\n\nTeratoma\nAlthough teratoma is the second most common testicular tumor in children, it\naffects all age groups. Mature teratoma in children is often benign, but teratoma in adults,\nregardless of age, should be considered as malignant. Teratomas are composed of all three\ngerm cell layers, i.e. endoderm, mesoderm and ectoderm. At ultrasound, teratomas\ngenerally form well-circumscribed complex masses. Echogenic foci representing\ncalcification, cartilage, immature bone and fibrosis are commonly seen [Fig. 5]. Cysts are\nalso a common feature and depending on the contents of the cysts i.e. serous, mucoid or\nkeratinous fluid, it may present as anechoic or complex structure [Fig. 6].\nSex cord-stromal (gonadal stromal) tumors of the testis, account for 4 per cent of all\ntesticular tumors. The most common are Leydig and Sertoli cell tumors.\nAlthough the majority of these tumors are benign, these tumors can produce hormonal\nchanges, for example, Leydig cell tumor in a child may produce isosexual virilization. In\nadult, it may have no endocrine manifestation or gynecomastia, and decrease in libido may\nresult from production of estrogens. These tumors are typically small and are usually\ndiscovered incidentally. They do not have any specific ultrasound appearance but appear as\nwell-defined hypoechoic lesions. These tumors are usually removed because they cannot be\ndistinguished from malignant germ cell tumors.\n\nLeydig cell tumors are the most common type of sex cord–stromal tumor of the testis,\naccounting for 1%–3% of all testicular tumors. They can be seen in any age group, they are\ngenerally small solid masses, but they may show cystic areas, hemorrhage, or necrosis. Their sonographic appearance is\nvariable and is indistinguishable from that of germ cell tumors.\n\nSertoli cell tumors are less common, constituting less than 1% of testicular tumors. They are\nless likely than Leydig cell tumors to be hormonally active, but gynecomastia can occur. Sertoli cell tumors are typically well circumscribed, unilateral, round to lobulated masses.\n\nClinically lymphoma can manifest in one of three ways: as the primary site of involvement,\nor as a secondary tumor such as the initial manifestation of clinically occult disease or\nrecurrent disease. Although lymphomas constitute 5% of testicular tumors and are almost\nexclusively diffuse non-Hodgkin B-cell tumors, only less than 1 % of non-Hodgkin\nlymphomas involve the testis.\n\nPatients with testicular lymphoma are usually old aged around 60 years of age, present with\npainless testicular enlargement and less commonly with other systemic symptoms such as\nweight loss, anorexia, fever and weakness. Bilateral testicle involvements are common and\noccur in 8.5% to 18% of cases. At sonography, most lymphomas are homogeneous and diffusely replace the testis [Fig. 7].\nHowever focal hypoechoic lesions can occur, hemorrhage and necrosis are rare. At times,\nthe sonographic appearance of lymphoma is indistinguishable from that of the germ cell\ntumors [Fig. 8], then the patient’s age at presentation, symptoms, and medical history, as\nwell as multiplicity and bilaterality of the lesions, are all important factors in making the\nappropriate diagnosis.\n\nPrimary leukemia of the testis is rare. However due to the presence of blood-testis barrier,\nchemotherapeutic agents are unable to reach the testis, hence in boys with acute\nlymphoblastic leukemia, testicular involvement is reported in 5% to 10% of patients, with\nthe majority found during clinical remission. The sonographic\nappearance of leukemia of the testis can be quite varied, as the tumors may be unilateral or\nbilateral, diffuse or focal, hypoechoic or hyperechoic. These findings\nare usually indistinguishable from that of the lymphoma [Fig. 9].\n\nEpidermoid cysts, also known as keratocysts, are benign epithelial tumors which usually occur in the second to fourth decades and accounts for only 1–2% of all intratesticular tumors. As these tumors have a benign biological behavior and with no malignant potential,\npreoperative recognition of this tumor is important as this will lead to testicle preserving\nsurgery (enucleation) rather than unnecessary orchiectomy.\nClinically, epidermoid cyst cannot be differentiated from other testicular tumors, typically\npresenting as a non-tender, palpable, solitary intratesticular mass. Tumor markers such as\nserum beta-human chorionic gonadotropin and alpha-feto protein are negative.\nThe ultrasound patterns of epidermoid cysts are variable and include:\necholucent rim;\nhypoechogenicities.\n\nHowever, these patterns, except the latter one, may be considered as non-specific as\nheterogeneous echotexture and shadowing calcification can also be detected in malignant\ntesticular tumors. The onion peel pattern of epidermoid cyst [Fig. 10]\ncorrelates well with the pathologic finding of multiple layers of keratin debris produced by\nthe lining of the epidermoid cyst. This sonographic appearance should be considered\ncharacteristic of an epidermoid cyst and corresponds to the natural evolution of the cyst.\nAbsence of vascular flow is another important feature that is helpful in differentiation of\nepidermoid cyst from other solid intratesticular lesions.\n\nAlthough most of the extratesticular lesions are benign, malignancy does occur; the most\ncommon malignant tumors in infants and children are rhabdomyosarcomas. Other malignant tumors include liposarcoma, leiomyosarcoma, malignant fibrous histiocytoma and mesothelioma.\n\nRhabdomyosarcoma is the most common tumor of the lower genitourinary tract in children in\nthe first two decades, it may develop anywhere in the body, and 4% occur in the paratesticular\nregion which carries a better outcome than lesions elsewhere in the genitourinary tract. Clinically, the patient usually presents with non-specific complaints of a unilateral, painless intrascrotal swelling not associated with fever.\n\nTransillumination test is positive when a hydrocele is present, often resulting in a\nmisdiagnosis of epididymitis, which is more commonly associated with hydrocele.\nThe ultrasound findings of paratesticular rhabdomyosarcoma are variable. It usually\npresents as an echo-poor mass [Fig. 11a] with or without hydrocele. With color Doppler sonography these tumors are generally hypervascular.\n\nMalignant mesothelioma is an uncommon tumor arising in body cavities lined by\nmesothelium. The majority of these tumors are found in the pleura, peritoneum and less\nfrequently pericardium. As the tunica vaginalis is a layer of reflected peritoneum,\nmesothelioma can occur in the scrotal sac. Although trauma, herniorrhaphy and long term\nhydrocele have been considered as the predisposing factors for development of malignant mesothelioma, the only well\nestablished risk factor is asbestos exposure. Patients with\nmalignant mesothelioma of the tunica vaginalis frequently have a progressively enlarging\nhydrocele and less frequently a scrotal mass, rapid re-accumulation of fluid after aspiration\nraises the suggestion of malignancy.\n\nThe reported ultrasound features of mesothelioma of the tunica vaginalis testis are variable.\nHydrocele, either simple or complex is present and may be associated with:\nhydrocele as the only finding and\n\nLeiomyomas are benign neoplasms that may arise from any structure or organ containing\nsmooth muscle. The majority of genitourinary leiomyomas are found in the renal capsule,\nbut this tumor has also been reported in the epididymis, spermatic cord, and tunica\nalbuginea. Scrotal leiomyomas have been reported in patients from the fourth to ninth\ndecades of life with most presenting during the fifth decade. These tumors are generally\nslow growth and asymptomatic. The sonographic features of leiomyomas have been\nreported as solid hypoechoic or heterogeneous masses that may or may not contain\nshadowing calcification. Other findings include whorl shaped configuration [Fig. 13a] of the nodule and multiple, narrow areas of shadowing not cast by calcifications [Fig. 13b], but corresponding to transition zones between the various tissue components of the mass are characteristic of leiomyoma and may help\ndifferentiate it from other scrotal tumors.\n\nLipoma is the most common nontesticular intrascrotal tumor. It can be divided into 3 types\ndepending upon the site of origination and spread:\n\nAt ultrasound, lipoma is a well–defined, homogeneous, hyperechoic paratesticular lesion of\nvarying size [Fig. 14]. The simple finding of an echogenic fatty mass within the inguinal\ncanal, while suggestive of a lipoma, should also raise a question of fat from the omentum\nsecondary to an inguinal hernia. However lipomas are well-defined masses, whereas\nherniated omentum appears to be more elongated and can be traced to the inguinal area,\nhence scanning along the inguinal canal as well as the scrotum is necessary to make the\ndifferential diagnosis. Magnetic resonance imaging and computerized tomography are\nhelpful in doubtful cases.\n\nMalignant extratesticular tumors are rare. Most of the malignant tumors are solid and have\nnonspecific features on ultrasonography. The majority of the malignant extratesticular\ntumors arise from spermatic cord with liposarcoma being the most common in adults. On gross specimen, liposarcoma is a solid, bulky lipomatous tumor\nwith heterogeneous architecture, often containing areas of calcification. Although the sonographic appearances of liposarcoma are variable and nonspecific, it\nstill provides a clue about the presence of lipomatous matrix.Echogenic areas corresponding\nto fat often associated with poor sound transmission and areas of heterogeneous\nechogenicity corresponding to nonlipomatous component are present. Some\nliposarcomas may also mimic the sonographic appearance of lipomas [Fig. 16] and hernias\nthat contain omentum, but lipomas are generally smaller and more homogeneous and\nhernias are elongated masses that can often be traced back to the inguinal canal. CT and MR\nimaging are more specific, as they can easily recognize fatty component along with other\nsoft tissue component more clearly than ultrasound.\n\nAdenomatoid tumors are the most common tumors of the epididymis and account for\napproximately 30% of all paratesticular neoplasms, second only to lipoma. They are usually unilateral, more common on the left side, and usually involve the\nepididymal tail. Adenomatoid tumor typically occurs in men during the third and fourth\ndecades of life. Patients usually present with a painless scrotal mass that is smooth, round\nand well circumscribed on palpation. They are believed to be of mesothelial origin and are\nuniversally benign. Their sonographic appearance is that of a round shaped, well-defined,\nhomogeneous mass with echogenicity ranging from hypo- to iso- to hyperechoic.\n\nFibrous pseudotumors, also known as fibromas are thought to be reactive, nonneoplastic\nlesions. They can occur at any age, about 50% of fibromas are associated with hydrocele, and\n30% are associated with a history of trauma or inflammation (Akbar et al, 2003). Although\nthe exact cause of this tumor is not completely understood, it is generally believed that these\nlesions represent a benign reactive proliferation of inflammatory and fibrous tissue, in\nresponse to chronic irritation.\nSonographic evaluation generally shows one or more solid nodules arising from the tunica\nvaginalis, epididymis, spermatic cord and tunica albuginea [Fig. 18]. A hydrocele is frequently\npresent too. The nodules may appear hypoechoic or hyperechoic, depending on the amount of collagen or fibroblast present.\nAcoustic shadowing may occur in the absence of calcification due to the dense collagen\ncomponent of this tumor. With color Doppler sonography, a small to moderate amount of\nvascularity may be seen [Fig. 19].\n\nEpididymitis and epididymo-orchitis are common causes of acute scrotal pain in adolescent\nboys and adults. At physical examination, they usually are palpable as tender and enlarged\nstructures. Clinically, this disease can be differentiated from torsion of the spermatic cord by\nelevation of the testes above the pubic symphysis. If scrotal pain decreases, it is more likely\nto be due to epidiymitis rather than torsion (Prehn’s sign). Most cases of epididymitis are\nsecondary to sexually transmitted disease or retrograde bacteria infection from the urinary\nbladder. The infection usually begins in the epididymal tail and spreads to the epididymal\nbody and head. Approximately 20% to 40 % of cases are associated with orchitis due to\ndirect spread of infection into the testis.\n\nAt ultrasound, the findings of acute epididymitis include an enlarged hypoechoic or\nhyperechoic (presumably secondary to hemorrhage) epididymis [Fig. 20a]. Other signs of\ninflammation such as increased vascularity, reactive hydrocele, pyocele and scrotal wall\nthickening may also be present. Testicular involvement is confirmed by the presence of\ntesticular enlargement and an inhomogeneous echotexture. Hypervascularity on color\nDoppler images [Fig. 20b] is a well-established diagnostic criterion and may be the only\nimaging finding of epididymo-orchitis in some men.\n\nAlthough the genitourinary tract is the most common site of extra-pulmonary involvement\nby tuberculosis, tuberculous infection of the scrotum is rare and occurs in approximately 7%\nof patients with tuberculosis. At the\ninitial stage of infection, the epididymis alone is involved. However if appropriate\nantituberculous treatment is not administered promptly, the infection will spreads to the\nipsilateral testis. The occurrence of isolated testicular tuberculosis is rare. Clinically patients with tuberculous epididymo-orchitis may present with painful or painless enlargement of the scrotum, hence they cannot be distinguished from lesions such as testicular tumor, testicular infarction and may mimic testicular torsion..\n\nAt ultrasound, tuberculous epididymitis is characterized by an enlarged epididymis with\nvariable echogenicity. The presence of calcification, caseation necrosis, granulomas and\nfibrosis can result in heterogeneous echogenicity [Fig. 21a]. The ultrasound findings of\ntuberculous orchitis are as follow: (a) diffusely enlarged heterogeneously hypoechoic testis\n(b) diffusely enlarged homogeneously hypoechoic testis (c) nodular enlarged\nheterogeneously hypoechoic testis and (d) presence of multiple small hypoechoic nodules in\nan enlarged testis [Fig. 21b].\n\nAlthough both bacterial and tuberculous infections may involve both the epididymis and\nthe testes, an enlarged epididymis with heterogeneously hypoechoic pattern favors a\ndiagnosis of tuberculosis (Muttarak and Peh, 2006, as cited in Kim et al, 1993 and Chung et\nal, 1997). With color Doppler ultrasound, a diffuse increased blood flow pattern is seen in\nbacterial epididymitis, whereas focal linear or spotty blood flow signals are seen in the\nperipheral zone of the affected epididymis in patients with tuberculosis.\n\nFournier gangrene is a polymicrobial necrotizing fascitis involving the perineal, perianal, or\ngenital regions and constitutes a true surgical emergency with a potentially high mortality\nrate. It usually develops from a perineal or genitourinary infection, but can arise following\nlocal trauma with secondary infection of the wound. 40–60% of patients are being diabetic.\nAlthough the diagnosis of Fournier gangrene is often made clinically, diagnostic imaging is\nuseful in ambiguous cases.\n\nThe sonographic hallmark of Fournier gangrene is presence of subcutaneous gas within the\nthickened scrotal wall. At ultrasound, the gas appears as numerous, discrete, hyperechoic\nfoci with reverberation artifacts [Fig. 22]. Evidence of gas within the scrotal wall may be\nseen prior to clinical crepitus. The only other condition manifesting with gas at sonographic\nexamination is an inguinoscrotal hernia. This can be differentiated from Fournier gangrene\nby the presence of gas within the protruding bowel lumen and away from the scrotal wall.\n(Levenson et al, 2008).\n\nThe normal testis consists of several hundred lobules, with each lobule containing several\nseminiferous tubules. The seminiferous tubules of each lobule merge to form the straight\ntubes, which in turn converge to form the rete testis. The rete testis tubules, which lie within\nthe mediastinum testis, are an anastomosing network of irregular channels with a broad\nlumen, which then empties into the efferent ductules to give rise to the head of the\nepididymis. Obstruction in the epididymis or efferent ductules may lead to cystic dilatation\nof the efferent ductules, which usually presents as an epididymal cyst on ultrasound.\nHowever, in the more proximal portion this could lead to the formation of an intratesticular\ncyst or dilatation of the tubules, so called tubular ectasia. Factors contributing to the\ndevelopment of tubular ectasia include epididymitis, testicular biopsy, vasectomy or an\naging process. Clinically this lesion is usually asymptomatic. The\nultrasound appearance of a microcystic or multiple tubular-like lesions located at the\nmediastinal testis [Fig. 23] and associated with an epididymal cyst in a middle-aged or\nelderly patient should alert the sonographer to the possibility of tubular ectasia.\nThe differential diagnosis of a multicystic lesion in testis should include a cystic tumor,\nespecially a cystic teratoma. A cystic teratoma is usually a palpable lesion containing both\nsolid and cystic components; and the cysts are normally larger than that of tubular ectasia,\nwhich appear microcystic [Fig. 24]. Furthermore, the location of tubular ectasia in the\nmediastinum testis is also helpful in making the differential diagnosis.\n\nHistologically, testicular microlithiasis refers to the scattered laminated calcium deposits in\nthe lumina of the seminiferous tubules. These calcifications arise from degeneration of the\ncells lining the seminiferous tubules. At ultrasonography, microliths appear as tiny punctate\nechogenic foci, which typically do not shadow. Although minor microcalcification within a\ntestis is considered normal, the typical US appearance of testicular microlithiasis is of\nmultiple nonshadowing echogenic foci measuring 2–3 mm and randomly scattered\nthroughout the testicular parenchyma [Fig. 25] (Dogra et al, 2003, as cited in Janzen et al,\n1992). The clinical significance of testicular microlithiasis is that it is associated with\nincreased risk of testicular malignancy, thus follow up of affected individuals with scrotal\nsonography is necessary to ensure that a testicular tumor does not develop.\n\nThe normal testis and epididymis are anchored to the scrotal wall. If there is a lack of\ndevelopment of these attachments, the testis is free to twist on its vascular pedicle. This will\nresult in torsion of the spermatic cord and interruption of testicular blood flow. Testicular\ntorsion occurs most commonly at 12 to 18 years but can occur at any age. Torsion results in\nswelling and edema of the testis, and as the edema increases, testicular perfusion is further\naltered. The extent of testicular ischemia depends on the degree of torsion, which ranges\nfrom 180° to 720° or greater. The testicular salvage rate depends on the degree of torsion and\nthe duration of ischemia. A nearly 100% salvage rate exists within the first 6 hours after the\nonset of symptoms; a 70% rate, within 6–12 hours; and a 20% rate, within 12–24 hours. Therefore testicular torsion is a surgical emergency and the role of ultrasound is to differentiate it from epididymitis as both disease\npresents with acute testicular pain clinically.\n\nThere are two types of testicular torsion: extravaginal and intravaginal. Extravaginal torsion\noccurs exclusively in newborns. Ultrasound findings include an enlarged heterogeneous\ntestis, ipsilateral hydrocele, thickened scrotal wall and absence of vascular flow in the testis\nand spermatic cord. The ultrasound findings of intravaginal torsion vary with the duration and the degree of rotation of the spermatic cord. Gray scale ultrasound may appear normal if the torsion is just occurred. At 4-6 hours\nafter onset of torsion, enlarged testis with decreased echogenicity is seen. At 24 hours after\nonset, the testis appears heterogeneous due to vascular congestion, hemorrhage and\ninfarction. As gray scale ultrasound is often normal during early onset of\ntorsion, Doppler sonography is considered as essential in early diagnosis of testicular\ntorsion. The absence of testicular flow at color and power Doppler ultrasound is considered\ndiagnostic of ischemia, provided that the scanner is set for detection of slow flow,\nthe sampling box is small and the scanner is adjusted for the lowest repetition frequency\nand the lowest possible threshold setting.\n\nVaricocele refers to an abnormal dilatation of the veins of the spermatic cord due to\nincompetence of valve in the spermatic vein. This results in impaired blood drainage into\nthe spermatic vein when the patient assumes a standing position or during Valsalva’s\nmaneuver. Varicoceles are more common on the left side due to the following reasons (a)\nThe left testicular vein is longer; (b) the left testicular vein enters the left renal vein at a right\nangle; (c) the left testicular artery in some men arches over the left renal vein, thereby\ncompressing it; and (d) the descending colon distended with feces may compress the left\ntesticular vein.\n\nThe US appearance of varicocele consists of multiple, hypoechoic, serpiginous, tubular like\nstructures of varying sizes larger than 2 mm in diameter that are usually best visualized\nsuperior or lateral to the testis [Fig. 27a]. Color flow and duplex Doppler US optimized for\nlow-flow velocities help confirm the venous flow pattern, with phasic variation and\nretrograde filling during a Valsalva’s maneuver [Fig. 27b]. Intratesticular varicocele may\nappear as a vague hypoechoic area in the testis or mimics tubular ectasia. With color\nDoppler, this intratesticular hypoechoic area also showed reflux of vascular flow during\nValsalva’s maneuver [Fig. 28].\n\nNormally the testes begin its descent through the inguinal canal to the scrotum at 36 weeks’\nof gestation and completed at birth. Failure in the course of testes descent will result in\nundescended testes (Cryptorchidism). \n\nUndescended testis is found in 4% of full-term infants but only 0.8% of\nmales at the age of 1 year have true cryptorchidism. Although an undescended testis can be\nfound anywhere along the pathway of descent from the retroperitoneum to the scrotum, the\ninguinal canal is the most common site for an undescended testis. Deviation of testis from\nthe normal pathway of descent will result in ectopic testis that is commonly seen in\npubopenile, femoral triangle and perineal regions.\n\nBesides infertility, undescended testes carry an increased risk of malignancy even for the\nnormally located contralateral testis. The risk of malignancy is estimated to be as high as 10 times the normal individual with seminoma being the most common malignancy.\nThe incidence of infertility is decreased if surgical orchiopexy is carried out before the 1-3 years\nbut the risk of malignancy does not change.\nBecause of the superficial location of the inguinal canal in children, sonography of\nundescended testes should be performed with a high frequency transducer. At ultrasound,\nthe undescended testis usually appears small, less echogenic than the contralateral normal\ntestis and usually located in the inguinal region [Fig. 29]. With color Doppler, the vascularity\nof the undescended testis is poor.\n\nAt sonography, the appendix testis usually appears as a 5 mm ovoid structure located in the\ngroove between the testis and the epididymis. Normally it is isoechoic to the testis but at\ntimes it may be cystic. The appendix epididymis is of the same size as the appendix testis\nbut is more often pedunculated. Clinically pain may occur with torsion of either appendage.\nPhysical examination showed a small, firm nodule is palpable on the superior aspect of the\ntestis and a bluish discoloration known as ‘‘blue dot’’ sign may be seen on the overlying\nskin. Torsion of the appendiceal\ntestis most frequently involved in boys aged 7-14 years (Dogra and Bhatt 2004).\nThe sonographic features of testicular appendiceal torsion includes a circular mass with\nvariable echogenicity located adjacent to the testis or epididymis [Fig. 30], reactive\nhydrocele and skin thickening of the scrotum is common, increased peripheral vascular flow\nmay be found around the testicular appendage on color Doppler ultrasound. Surgical\nintervention is unnecessary and pain usually resolves in 2 to 3 days with an atrophied or\ncalcified appendages remaining.\n\nUltrasound remains as the mainstay in scrotal imaging not only because of its high accuracy,\nexcellent depiction of scrotal anatomy, low cost and wide availability, it is also useful in\ndetermining whether a mass is intra- or extra-testicular, thus providing us useful and\nvaluable information to decide whether a mass is benign or malignant even though\nmalignancy do occur in extratesticular tumors and vice versa. Furthermore, ultrasound also\nprovides information essential to reach a specific diagnosis in patients with testicular\ntorsion, testicular appendiceal torsion and inflammation such as epididymo-orchitis,\nFournier gangrene etc, thus enabling us to avoid unnecessary operation.\n\n"}
{"id": "25779828", "url": "https://en.wikipedia.org/wiki?curid=25779828", "title": "Self-esteem functions", "text": "Self-esteem functions\n\nSelf-esteem can be defined as how favorably individuals evaluate themselves. According to Leary, self-esteem is the relationship between one’s real self and one’s ideal self, feeding off of favorable behaviors. It refers to an individual's sense of his or her value or worth, or the extent to which a person values, approves of, appreciates, prizes, or likes him or herself. Self-esteem is generally considered the evaluative component of the self-concept, a broader representation of the self that includes cognitive and behavioral aspects as well as evaluative or affective ones. There are several different proposals as to the functions of self-esteem. One proposal includes to satisfy the inherent need to feel good about one self. Another one would be to minimize social rejection . Self-esteem is also known as A way for a person to remain dominant in relationships (Barkow, 1980). Self-esteem is known to protect people from fear that has potential of arising from the prospect of death- terror management theory. Self-esteem helps motivate people to achieve their goals- high self-esteem leading to coping in situations and low self-esteem leading to avoidance.\n\nThe sociometer theory was developed by Mark Leary (1999) to explain the functions of self-esteem. A sociometer is a measure of how desirable one would be to other people - this is influenced by one’s self-esteem. They suggested that self-esteem has evolved to monitor one’s social acceptance and is used as a gauge for avoiding social devaluation and rejection.\nThe sociometer theory is strongly grounded in evolutionary theories which suggest that survival depends on social acceptance for reasons such as protection, reciprocal behaviours and most importantly reproduction. The monitoring of one's acceptance via self-esteem is therefore crucial in order to achieve these kinds of social interactions and be better able to compete for the social benefits of them.\n\nKirkpatrick and Ellis (2003) expanded on Leary’s work and suggested that the sociometer’s function was not only to ensure that an individual was not excluded from their social group but also to rate the strength of the social group compared to other groups. Leary and his colleagues stated that a sociometer is a measure of how a person is desirable by the people and this is oftentimes influenced through a person's self-esteem.\n\nSelf-determination theory (SDT) states that man is born with an intrinsic motivation to explore, absorb and master his surroundings and that true high self-esteem is reported when the basic psychological nutrients, or needs, of life (relatedness, competency and autonomy) are in balance Nayler, C. (2005) Theories of Self Esteem. Positive Psychology.\n\nThe ethological perspective (Barkow, 1980) suggests that self-esteem is an adaptation that has evolved for the purpose of maintaining dominance in relationships. It is said that human beings have evolved certain mechanisms for monitoring dominance in order to facilitate reproductive behaviours such attaining a mate. Because attention and favorable reactions from others were associated with being dominant, feelings of self-esteem have also become associated with social approval and deference. From this perspective, the motive to evaluate oneself positively in evolutionary terms is to enhance one’s relative dominance (Leary, 1999).\n\nLeary et al. (2001) tested the idea of dominance and social acceptance on self-esteem. Trait self-esteem appeared to be related to the degree to which participants felt accepted by specific people in their lives, but not to the degree to which participants thought those individuals perceived them as dominant. Acceptance and dominance appeared to have independent effects on self-esteem.\n\nThe terror management theory, developed by Sheldon Solomon at al. (1991), which in relation to self-esteem states that having self-esteem helps protect individuals from the fear they experience at the prospect of their own death. It is suggested that people are constantly searching for ways to enhance their self-esteem in order to quell unconscious death anxiety. This internalisation of cultural values is also a key factor in Terror Management Theory (TMT) in which self-esteem is seen as a culturally based construction derived from integrating specific contingencies valued by society into one's own ‘worldview’. High self-esteem promotes positive affect and personal growth, psychological well-being and coping as a buffer against anxiety in the knowledge of our eventual certain death, and reduces defensive anxiety related behaviour. Nayler, C. (2005) Theories of Self Esteem. Positive Psychology. Terror management theory, based primarily on the writings of Ernest Becker (1962, 1971, 1973, 1975) and Otto\nRank (1936, 1941), posits that self-esteem is sought because it provides protection against the fear of death (Greenberg, Pyszczynski,\n& Solomon, 1986; Solomon, Greenberg, & Pyszczynski,1991a). From this perspective, the fear of death is rooted\nin an instinct for self-preservation that humans share with other species. Jones, E. McGreggor, H. Pyszczynski, T. Simon, L. & Solomon, S. (1997). Terror Management\nTheory and Self –Esteem Reduces Mortality Salience Effects. Personality and Social Psychology 72, 24-36\n\nSome researchers believe that having a high self-esteem facilitates goal achievement.\nBednar, Wells, and Peterson (1989) proposed that self-esteem is a form of subjective feedback about the adequacy of the self. This feedback (self-esteem) is positive when the individual copes well with circumstances and is negative when he or she avoids threats. In turn, self-esteem affects subsequent goal achievement; high self-esteem increases coping, and low self-esteem leads to further avoidance (Leary, 1999).\n\nIllusion of control is the tendency for human beings to believe they can control, or at least influence, outcomes that they demonstrably have no influence over, a mindset often seen in those who gamble (Langer, 1975). However, for individuals who are not gamblers Taylor and Brown (1988) suggest it may serve to be a function of self-esteem. Belief that there is a level of control over the situation a person is in, may lead to an increased level of motivation and performance in a self-regulating manner. In other words, one will work harder to become successful if they believe they have control over their success. A high self-esteem would be needed for this belief of control and so the need for a sense of control may be a function of self-esteem.\nWhen applying sociometer theory, it suggests that the illusion of control is an adaptive response in order to self-regulate behaviour to cultural norms and thereby provide an individual with an increased level of self-esteem. In social psychology, the illusion of control is grouped with two other concepts and termed as the ‘positive illusions’. Refer to link for a blog entry on illusion of control. http://interaction-dynamics.com/blog/tag/illusion-of-control/.\n\n"}
{"id": "146075", "url": "https://en.wikipedia.org/wiki?curid=146075", "title": "Self-help", "text": "Self-help\n\nSelf-help or self-improvement is a self-guided improvement—economically, intellectually, or emotionally—often with a substantial psychological basis. Many different self-help group programs exist, each with its own focus, techniques, associated beliefs, proponents and in some cases, leaders. Concepts and terms originating in self-help culture and Twelve-Step culture, such as recovery, dysfunctional families, and codependency have become firmly integrated in mainstream language.\n\nSelf-help often utilizes publicly available information or support groups, on the Internet as well as in person, where people in similar situations join together. From early examples in self-driven legal practice and home-spun advice, the connotations of the word have spread and often apply particularly to education, business, psychology and psychotherapy, commonly distributed through the popular genre of self-help books. According to the \"APA Dictionary of Psychology\", potential benefits of self-help groups that professionals may not be able to provide include friendship, emotional support, experiential knowledge, identity, meaningful roles, and a sense of belonging.\n\nGroups associated with health conditions may consist of patients and caregivers. As well as featuring long-time members sharing experiences, these health groups can become support groups and clearing-houses for educational material. Those who help themselves by learning and identifying about health problems can be said to exemplify self-help, while self-help groups can be seen more as peer-to-peer support.\n\nWithin classical antiquity, Hesiod's \"Works and Days\" \"opens with moral remonstrances, hammered home in every way that Hesiod can think of.\" The Stoics offered ethical advice \"on the notion of \"eudaimonia\"—of well-being, welfare, flourishing.\" The genre of mirror-of-princes writings, which has a long history in Greco-Roman and Western Renaissance literature, represents a secular cognate of Biblical wisdom-literature. Proverbs from many periods, collected and uncollected, embody traditional moral and practical advice of diverse cultures.\n\nThe hyphenated compound word \"self-help\" often appeared in the 1800s in a legal context, referring to the doctrine that a party in a dispute has the right to use lawful means on their own initiative to remedy a wrong.\n\nFor some, George Combe's \"Constitution\" [1828], in the way that it advocated personal responsibility and the possibility of naturally sanctioned self-improvement through education or proper self-control, largely inaugurated the self-help movement;\" In 1841, an essay by Ralph Waldo Emerson, entitled Compensation, was published suggesting \"every man in his lifetime needs to thank his faults\" and \"acquire habits of \"self-help\"\" as \"our strength grows out of our weakness.\" Samuel Smiles (1812–1904) published the first self-consciously personal-development \"self-help\" book—entitled \"Self-Help\"—in 1859. Its opening sentence: \"Heaven helps those who help themselves\", provides a variation of \"God helps them that help themselves\", the oft-quoted maxim that had also appeared previously in Benjamin Franklin's \"Poor Richard's Almanac\" (1733–1758). In the 20th century, \"Carnegie's remarkable success as a self-help author\" further developed the genre with \"How to Win Friends and Influence People\" in 1936. Having failed in several careers, Carnegie became fascinated with success and its link to self-confidence, and his books have since sold over 50 million copies. Earlier, in 1902, James Allen published \"As a Man Thinketh\", which proceeds from the conviction that \"a man is literally what he thinks, his character being the complete sum of all his thoughts.\" Noble thoughts, the book maintains, make for a noble person, whilst lowly thoughts make for a miserable person; and Napoleon Hill's \"Think and Grow Rich\" (1937) described the use of repeated positive thoughts to attract happiness and wealth by tapping into an \"Infinite Intelligence\".\n\nIn the final third of the 20th century, \"the tremendous growth in self-help publishing...in self-improvement culture\" really took off—something which must be linked to postmodernism itself—to the way \"postmodern subjectivity constructs self-reflexive subjects-in-process.\" Arguably at least, \"in the literatures of self-improvement...that crisis of subjecthood is not articulated but enacted—demonstrated in ever-expanding self-help book sales.\"\n\nThe conservative turn of the neoliberal decades also meant a decline in traditional political activism, and increasing \"social isolation; Twelve-Step recovery groups were one context in which individuals sought a sense of community...yet another symptom of the psychologizing of the personal\" to more radical critics. Indeed, \"some social theorist have argued that the late-20th century preoccupation with the self serves as a tool of social control: soothing political unrest...[for] one's own pursuit of self-invention.\"'\n\nWithin the context of the market, group and corporate attempts to aid the \"seeker\" have moved into the \"self-help\" marketplace, with Large Group Awareness Trainings, LGATs\nand psychotherapy systems represented. These offer more-or-less prepackaged solutions to instruct people seeking their own individual betterment, just as \"the literature of self-improvement directs the reader to familiar frameworks...what the French \"fin de siècle\" social theorist Gabriel Tarde called 'the grooves of borrowed thought'.\"\n\nA subgenre of self-help book series also exists: such as the \"for Dummies\" guidesand \"The Complete Idiot's Guide to...\"—compare how-to books.\n\nAt the start of the 21st century, \"the self-improvement industry, inclusive of books, seminars, audio and video products, and personal coaching, [was] said to constitute a 2.48-billion dollars-a-year industry\" in the United States alone. By 2006, research firm Marketdata estimated the \"self-improvement\" market in the U.S. as worth more than $9 billion—including infomercials, mail-order catalogs, holistic institutes, books, audio cassettes, motivation-speaker seminars, the personal coaching market, weight-loss and stress-management programs. Marketdata projected that the total market size would grow to over $11 billion by 2008. \nIn 2012 Laura Vanderkam wrote of a turnover of 12 billion dollars.\nIn 2013 Kathryn Schulz examined \"an $11 billion industry\".\n\nSelf-help and mutual-help are very different from—though they may complement—service delivery by professionals: note for example the interface between local self-help and International Aid's service delivery model.\n\nConflicts can and do arise on that interface, however, with some professionals considering that \"the twelve-step approach encourages a kind of contemporary version of 19th-century amateurism or enthusiasm in which self-examination and very general social observations are enough to draw rather large conclusions.\"\n\nThe rise of self-help culture has inevitably led to boundary disputes with other approaches and disciplines. Some would object to their classification as \"self-help\" literature, as with \"Deborah Tannen's denial of the self-help role of her books\" so as to maintain her academic credibility, aware of the danger that \"writing a book that becomes a popular success...all but ensures that one's work will lose its long-term legitimacy.\"\n\nPlacebo effects can never be wholly discounted. Thus careful studies of \"the power of subliminal self-help tapes...showed that their content had no real effect...But that's not what the participants thought.\" \"If they thought they'd listened to a self-esteem tape (even though half the labels were wrong), they felt that their self-esteem had gone up. No wonder people keep buying subliminal tape: even though the tapes don't work, people think they do.\" One might then see much of the self-help industry as part of the \"skin trades. People need haircuts, massage, dentistry, wigs and glasses, sociology and surgery, as well as love and advice.\"—a skin trade, \"not a profession and a science\" Its practitioners would thus be functioning as \"part of the personal service industry rather than as mental health professionals.\" While \"there is no proof that twelve-step programs 'are superior to any other intervention in reducing alcohol dependence or alcohol-related problems',\" at the same time it is clear that \"there is something about 'groupishness' itself which is curative.\" Thus for example \"smoking increases mortality risk by a factor of just 1.6, while social isolation does so by a factor of 2.0...suggest[ing] an added value to self-help groups such as Alcoholics Anonymous as surrogate communities.\"\n\nSome psychologists advocate a positive psychology, and explicitly embrace an empirical self-help philosophy; \"the role of positive psychology is to become a bridge between the ivory tower and the main street—between the rigor of academe and the fun of the self-help movement.\" They aim to refine the self-improvement field by way of an intentional increase in scientifically sound research and well-engineered models. The division of focus and methodologies has produced several subfields, in particular: general positive psychology, focusing primarily on the study of psychological phenomenon and effects; and personal effectiveness, focusing primarily on analysis, design and implementation of qualitative personal growth. This includes the intentional training of new patterns of thought and feeling. As business strategy communicator Don Tapscott puts it, \"The design industry is something done to us. I'm proposing we each become designers. But I suppose 'I love the way she thinks' could take on new meaning.\"\n\nBoth self-talk, the propensity to engage in verbal or mental self-directed conversation and thought, and social support can be used as instruments of self-improvement, often by empowering, action-promoting messages. Psychologists have designed series of experiments that are intended to shed light into how self-talk can result in self-improvement. In general, research has shown that people prefer to use second person pronouns over first person pronouns when engaging in self-talk to achieve goals, regulate one’s own behavior, thoughts, or emotions, and facilitate performance. If self-talk has the expected effect, then writing about personal problems using language from their friends’ perspective should result in greater amount of motivational and emotional benefits comparing to using language from their own perspective. When you need to finish a difficult task and you are not willing to do something to finish this task, trying to write a few sentence or goals imaging what your friends have told you gives you more motivational resources comparing to you write to yourself. Research done by Ireland and others have revealed that, as expected, when people are writing using many physical and mental words or even typing a standard prompt with these kinds of words, adopting a friend’s perspective while freely writing about a personal challenge can help increase people’s intention to improve self-control by promoting the positivity of emotions such as pride and satisfaction, which can motivate people to reach their goal.\n\nThe use of self-talk goes beyond the scope of self-improvement for performing certain activities, self-talk as a linguistic form of self-help also plays a very important role in regulating people’s emotions under social stress. First of all, people using non-first-person language tend to exhibit higher level of visual self-distancing during the process of introspection, indicating that using non-first-person pronouns and one’s own name may result in enhanced self-distancing. More importantly, this specific form of self-help also has been found can enhance people’s ability to regulate their thoughts, feelings, and behavior under social stress, which would lead them to appraise social-anxiety-provoking events in more challenging and less threatening terms. Additionally, these self-help behaviors also demonstrate noticeable self-regulatory effects through the process of social interactions, regardless of their dispositional vulnerability to social anxiety.\n\nScholars have targeted self-help claims as misleading and incorrect. In 2005 Steve Salerno portrayed the American self-help movement—he uses the acronym \"SHAM: the Self-Help and Actualization Movement\"—not only as ineffective in achieving its goals, but also as socially harmful. \"Salerno says that 80 percent of self-help and motivational customers are repeat customers and they keep coming back 'whether the program worked for them or not'.\" Others similarly point out that with self-help books \"supply increases the demand... The more people read them, the more they think they need them... more like an addiction than an alliance.\"\n\nSelf-help writers have been described as working \"in the area of the ideological, the imagined, the narrativized... although a veneer of scientism permeates the[ir] work, there is also an underlying armature of moralizing.\"\n\nChristopher Buckley in his book \"God is My Broker\" asserts: \"The only way to get rich from a self-help book is to write one\".\n\nIn 1987 Gerald M. Rosen reported that people do not gain as much from reading self-help material as people would from the same material received in therapy. In general, he was critical of proliferation of self-help books.\n\nKathryn Schulz suggests that \"the underlying theory of the self-help industry is contradicted by the self-help industry’s existence\".\n\nThe self-help world has become the target of parodies. Walker Percy's odd genre-busting \"Lost in the Cosmos\" has been described as \"a parody of self-help books, a philosophy textbook, and a collection of short stories, quizzes, diagrams, thought experiments, mathematical formulas, made-up dialogue\". In their 2006 book \"Secrets of The Superoptimist\", authors W.R. Morton and Nathanel Whitten revealed the concept of \"superoptimism\" as a humorous antidote to the overblown self-help book category. In his comedy special \"Complaints and Grievances\" (2001), George Carlin observes that there is \"no such thing\" as self-help: anyone looking for help from someone else does not technically get \"self\" help; and one who accomplishes something without help, did not need help to begin with. In Margaret Atwood's semi-satiric dystopia \"Oryx and Crake\", university literary studies have declined to the point that the protagonist, Snowman, is instructed to write his thesis on self-help books as literature; more revealing of the authors and of the society that produced them than genuinely helpful.\n\n"}
{"id": "1350961", "url": "https://en.wikipedia.org/wiki?curid=1350961", "title": "Sexercise", "text": "Sexercise\n\nSexercise (it is referred to by some as eroticise) is physical exercise performed in preparation for sexual activity and designed to tone, build, and strengthen muscles. Sexercises are often performed as part of a sex diet lifestyle, which seeks to maximize the health benefits of regular sexual activity. Exercise is known to improve and quicken the flow of oxygenated blood, in higher and consistent amounts, along with other beneficial chemical compounds, to the genitalia, which is important for fertility and important during intercourse.\n\nSexercises range from Kegel exercise to aerobic and cardiovascular routines. Flexibility for performing contortion specifically for erotic or sexual positions may also be practised.\n\n"}
{"id": "56104000", "url": "https://en.wikipedia.org/wiki?curid=56104000", "title": "Sheikh Zayed Foundation", "text": "Sheikh Zayed Foundation\n\nSheikh Zayed Foundation, is a non-profit organization that was created in 1993 in Rabat. It focuses on the health sector. The foundation aims at contributing to the Moroccan university-hospital sphere as well as ensuring quality social services oriented towards future health practitioners. It is named after Sheikh Zayed bin Sultan Al Nahyan, Emir of Abu Dhabi, founder and first president of the United Arab Emirates.\n\nThe activities of the foundation are represented by four essential components: Medical care, teaching, research and medical support for populations in difficulty. These components strengthen the health sector in Morocco and actively contribute to the improvement of treatments, hospital infrastructure and medical equipment.\n\nThe Sheikh Zayed Foundation was established in Rabat on September 10, 1993, by King Hassan II and Sheikh Zayed Ibn Sultan Al-Nahyan under a private non-profit model. The foundation was created by (Dahir) Royal Decree-Law No. 1-93-228 with an initial donation of $50 million by the Emirati Sovereign. The vision of the foundation is based on the principles of excellence, solidarity and respect for differences.\n\nSince 2003, the foundation put in place a development program focused on launching new medical, education and training institutions. This new strategy aims to gradually strengthen the presence of its components throughout Morocco.\n\nIn 2016, the foundation announces the launching of a genetic research center in partnership with UIASS and the institute of Genetic Diseases \"Imagine\" of Paris.\n\nThe foundation is made up of five healthcare facilities, one university campus with a training capacity in the form of three faculties and two institutes of higher education, three research centers, one center of bioequivalence studies, and several mobile care units. The Sheikh Zayed foundation organized the first African Health Forum that took place in May 2015. The forum focused on the theme of HIV, with the presence of the medicine Nobel prize winner Françoise Barré-Sinoussi.\n\nIn 2003, the King Mohammed VI appointed Mounir Majidi as the president of the Sheikh Zayed foundation. Mounir Majidi implemented a new economic model for the foundation to regain balance and financial autonomy. From 2003 to 2016, the foundation’s turnover went from 2.9 million euros to 58.2 million euros, with an average annual growth of 26%. The foundation yields 5.5 million euros per year, a net result fully reinvested in infrastructural development.\n"}
{"id": "29911578", "url": "https://en.wikipedia.org/wiki?curid=29911578", "title": "St Tola", "text": "St Tola\n\nSt Tola goats cheese is a range of handmade goat's milk cheese made in Inagh, County Clare. A range of cheese are produced varying from fresh soft cheese to a Gouda style hard cheese.\n\nOriginally, the business was set up by Meg and Derrick Gordan on their 25-acre farm in the early 1980s, but since 1999 has been taken over by their neighbour, Siobhán Ni Ghairbhith, on her farm. \n\nSt Tola produce a number of goat's milk cheeses:\n\n\n"}
{"id": "2347581", "url": "https://en.wikipedia.org/wiki?curid=2347581", "title": "Steve de Shazer", "text": "Steve de Shazer\n\nSteve de Shazer (June 25, 1940, Milwaukee – September 11, 2005, Vienna) was a psychotherapist, author, and developer and pioneer of solution focused brief therapy. In 1978, he founded the Brief Family Therapy Center (BFTC) in Milwaukee, Wisconsin with his wife Insoo Kim Berg.\n\nHe wrote six significant books, translated into 14 languages; wrote many papers; and lectured internationally.\n\nDe Shazer was originally trained as a classical musician and worked as a jazz saxophonist. He received a Bachelor in Fine Arts and an MSSW in Social Work from the University of Wisconsin–Milwaukee. He never studied at the Mental Research Institute in Palo Alto, California, even though some rumours have it he did. He was a lifelong friend of John Weakland though, and saw him as his mentor.\n\nDe Shazer died in Vienna while traveling on a training and consulting tour in Europe. De Shazer has a nephew, Tony de Shazer and two great nieces Elodie and Amelie de Shazer. \n\n"}
{"id": "4092680", "url": "https://en.wikipedia.org/wiki?curid=4092680", "title": "Stress ball", "text": "Stress ball\n\nA stress ball or hand exercise ball is a malleable toy, usually not more than 7 cm in diameter, which is squeezed in the hand and manipulated by the fingers, ostensibly to relieve stress and muscle tension or to exercise the muscles of the hand.\n\nDespite the name, many stress balls are not spherical. Some are molded in amusing shapes, and pad- or transfer-printed with corporate logos. They are presented to employees and clients of companies as promotional gifts. Stress balls are the third most popular promotional gift in the United Kingdom. Because of the many non-spherical shapes now available, stress balls are generically known as stress relievers.\n\nThere are several different types of stress balls that originate from many different countries. The most common type of stress ball in America is the “bean bag” type, commonly known as a “Hacky Sack”. The stress ball that is most common in Australia is the foam type, this type prevents stress through resistance from squeezing the ball. The third type of stress ball is the Chinese form known as the Baoding ball. These are not like the others as these are not squeezable, they are solid, they usually come in pairs so you can roll them together to make a soothing sound and a smooth sensation feeling in your hands.\n\nSome stress relievers are made from closed-cell polyurethane foam rubber. These are made by injecting the liquid components of the foam into a mold. The resulting chemical reaction creates carbon dioxide bubbles as a byproduct, which in turn creates the foam.\n\nStress balls, especially those used in physical therapy, can also contain gel of different densities inside a rubber or cloth skin. Another type uses a thin rubber membrane surrounding a fine powder. The latter type can be made at home by filling a balloon with baking soda. Some balls similar to a footbag are marketed and used as stress balls.\n\n"}
{"id": "51099763", "url": "https://en.wikipedia.org/wiki?curid=51099763", "title": "Timeline of healthcare in Kenya", "text": "Timeline of healthcare in Kenya\n\nThis is a timeline of healthcare in Kenya, focusing especially on modern science-based medicine healthcare. Major events such as crises, policies and organizations are included.\n\n"}
{"id": "903516", "url": "https://en.wikipedia.org/wiki?curid=903516", "title": "Tonsillitis", "text": "Tonsillitis\n\nTonsillitis is inflammation of the tonsils, typically of rapid onset. It is a type of pharyngitis. Symptoms may include sore throat, fever, enlargement of the tonsils, trouble swallowing, and large lymph nodes around the neck. Complications include peritonsillar abscess.\nTonsillitis is most commonly caused by a viral infection, with about 5% to 40% of cases caused by a bacterial infection. When caused by the bacterium group A streptococcus, it is referred to as strep throat. Rarely bacteria such as \"Neisseria gonorrhoeae\", \"Corynebacterium diphtheriae\", or \"Haemophilus influenzae\" may be the cause. Typically the infection is spread between people through the air. A scoring system, such as the Centor score, may help separate possible causes. Confirmation may be by a throat swab or rapid strep test.\nTreatment efforts involve improving symptoms and decreasing complications. Paracetamol (acetaminophen) and ibuprofen may be used to help with pain. If strep throat is present the antibiotic penicillin by mouth is generally recommended. In those who are allergic to penicillin, cephalosporins or macrolides may be used. In children with frequent episodes of tonsillitis, tonsillectomy modestly decreases the risk of future episodes.\nAbout 7.5% of people have a sore throat in any three-month period and 2% of people visit a doctor for tonsillitis each year. It is most common in school aged children and typically occurs in the fall and winter months. The majority of people recover with or without medication. In 40% of people, symptoms resolve within three days, and in 80% symptoms resolve within one week, regardless of if streptococcus is present. Antibiotics decrease symptom duration by approximately 16 hours.\n\nCommon signs and symptoms include:\n\nLess common symptoms include:\n\nIn cases of acute tonsillitis, the surface of the tonsil may be bright red and with visible white areas or streaks of pus.\n\nTonsilloliths occur in up to 10% of the population frequently due to episodes of tonsillitis.\n\nThe most common cause is viral infection and includes adenovirus, rhinovirus, influenza, coronavirus, and respiratory syncytial virus. It can also be caused by Epstein-Barr virus, herpes simplex virus, cytomegalovirus, or HIV. The second most common cause is bacterial infection of which the predominant is Group A β-hemolytic streptococcus (GABHS), which causes strep throat. Less common bacterial causes include: \"Staphylococcus aureus\" (including methicillin resistant Staphylococcus aureus or MRSA ), \"Streptococcus pneumoniae\", \"Mycoplasma pneumoniae\", \"Chlamydia pneumoniae\", \"Bordetella pertussis\", \"Fusobacterium\" sp., \"Corynebacterium diphtheriae\", \"Treponema pallidum\", and \"Neisseria gonorrhoeae\".\n\nAnaerobic bacteria have been implicated in tonsillitis and a possible role in the acute inflammatory process is supported by several clinical and scientific observations.\n\nUnder normal circumstances, as viruses and bacteria enter the body through the nose and mouth, they are filtered in the tonsils. Within the tonsils, white blood cells of the immune system destroy the viruses or bacteria by producing inflammatory cytokines like phospholipase A2, which also lead to fever. The infection may also be present in the throat and surrounding areas, causing inflammation of the pharynx.\n\nSometimes, tonsillitis is caused by an infection of spirochaeta and treponema, in this case called Vincent's angina or Plaut-Vincent angina.\n\nThe diagnosis of group A beta-hemolytic streptococcus (GABHS) tonsillitis can be confirmed by culture of samples obtained by swabbing both tonsillar surfaces and the posterior pharyngeal wall and plating them on sheep blood agar medium. The isolation rate can be increased by incubating the cultures under anaerobic conditions and using selective growth media. A single throat culture has a sensitivity of 90–95% for the detection of GABHS (which means that GABHS is actually present 5–10% of the time culture suggests that it is absent). This small percentage of false-negative results are part of the characteristics of the tests used but are also possible if the patient has received antibiotics prior to testing. Identification requires 24 to 48 hours by culture but rapid screening tests (10–60 minutes), which have a sensitivity of 85–90%, are available. Older antigen tests detect the surface Lancefield group A carbohydrate. Newer tests identify GABHS serotypes using nucleic acid (DNA) probes or polymerase chain reaction. Bacterial culture may need to be performed in cases of a negative rapid streptococcal test.\n\nTrue infection with GABHS, rather than colonization, is defined arbitrarily as the presence of >10 colonies of GABHS per blood agar plate. However, this method is difficult to implement because of the overlap between carriers and infected patients. An increase in antistreptolysin O (ASO) streptococcal antibody titer 3–6 weeks following the acute infection can provide retrospective evidence of GABHS infection and is considered definitive proof of GABHS infection.\n\nIncreased values of secreted phospholipase A2 and altered fatty acid metabolism in patients with tonsillitis may have diagnostic utility.\n\nTreatments to reduce the discomfort from tonsillitis include:\n\nWhen tonsillitis is caused by a virus, the length of illness depends on which virus is involved. Usually, a complete recovery is made within one week; however, symptoms may last for up to two weeks.\n\nIf the tonsillitis is caused by group A streptococcus, then antibiotics are useful, with penicillin or amoxicillin being primary choices. Cephalosporins and macrolides are considered good alternatives to penicillin in the acute setting. A macrolide such as erythromycin is used for people allergic to penicillin. Individuals who fail penicillin therapy may respond to treatment effective against beta-lactamase producing bacteria such as clindamycin or amoxicillin-clavulanate. Aerobic and anaerobic beta lactamase producing bacteria that reside in the tonsillar tissues can \"shield\" group A streptococcus from penicillins.\n\nChronic cases can be treated with tonsillectomy (the surgical removal of tonsils) as a choice for treatment. Children have had only a modest benefit from tonsillectomy for chronic cases of tonsillitis.\n\nSince the advent of penicillin in the 1940s, a major preoccupation in the treatment of streptococcal tonsillitis has been the prevention of rheumatic fever, and its major effects on the nervous system (Sydenham's chorea) and heart. Recent evidence would suggest that the rheumatogenic strains of group A beta hemolytic strep have become markedly less prevalent and are now only present in small pockets such as in Salt Lake City, USA. This brings into question the rationale for treating tonsillitis as a means of preventing rheumatic fever.\n\nComplications may rarely include dehydration and kidney failure due to difficulty swallowing, blocked airways due to inflammation, and pharyngitis due to the spread of infection.\n\nAn abscess may develop lateral to the tonsil during an infection, typically several days after the onset of tonsillitis. This is termed a peritonsillar abscess (or quinsy).\n\nRarely, the infection may spread beyond the tonsil resulting in inflammation and infection of the internal jugular vein giving rise to a spreading septicaemia infection (Lemierre's syndrome).\n\nIn chronic/recurrent cases (generally defined as seven episodes of tonsillitis in the preceding year, five episodes in each of the preceding two years or three episodes in each of the preceding three years), or in acute cases where the palatine tonsils become so swollen that swallowing is impaired, a tonsillectomy can be performed to remove the tonsils. Patients whose tonsils have been removed are still protected from infection by the rest of their immune system.\n\nIn strep throat, very rarely diseases like rheumatic fever or glomerulonephritis can occur. These complications are extremely rare in developed nations but remain a significant problem in poorer nations. Tonsillitis associated with strep throat, if untreated, is hypothesized to lead to pediatric autoimmune neuropsychiatric disorders associated with streptococcal infections (PANDAS).\n"}
{"id": "20509316", "url": "https://en.wikipedia.org/wiki?curid=20509316", "title": "Volgograd Reservoir", "text": "Volgograd Reservoir\n\nThe Volgograd Reservoir () is a reservoir in Russia formed at the Volga River by the dam of the Volga Hydroelectric Station. It lies within the Volgograd Oblast and Saratov Oblast and named after the city of Volgograd. It was constructed during 1958-1961.\n\nIts area is 3,117 km², volume is 31,5 km³, length is 540 km, maximal width is 17 km, average depth is 10.1 m. It is the third largest reservoir on Volga in Russia (after Kuybyshev Reservoir and Rybinsk Reservoir).\n"}
{"id": "47463290", "url": "https://en.wikipedia.org/wiki?curid=47463290", "title": "Warfarin resistance", "text": "Warfarin resistance\n\nWarfarin resistance is a rare condition in which people have varying degrees of tolerance to the anticoagulant drug warfarin. In incomplete warfarin resistance, people only respond to high doses of warfarin; in complete warfarin resistance, the drug has no effect. This can be because the drug is metabolized quickly or because the clotting cascade does not interact with warfarin as it should. One gene that has been identified in warfarin resistance is VKORC1, a gene responsible for warfarin metabolism. It is inherited in an autosomal dominant pattern.\n"}
