{"id": "55207906", "url": "https://en.wikipedia.org/wiki?curid=55207906", "title": "A Secret Institution", "text": "A Secret Institution\n\nA Secret Institution, a 19th-century woman's lunatic asylum narrative, is the autobiography of Clarissa Caldwell Lathrop. Published in 1890 after she had regained her freedom, it details Lathrop's institutionalization at Utica Lunatic Asylum for voicing suspicions that someone was trying to poison her. Written novelistically, book reviews of the time suggested that it was poorly written and fell short of its object, while 21st-century reviewers praise the exposing of 19th-century mental institutions, which confined outspoken women.\n\n\"A Secret Institution\" (New York: Bryant Publishing Co.) has the external appearance of a novel, but was intended as a statement of facts concerning the way things were managed in the Utica Lunatic Asylum. The author, Clarissa Caldwell Lathrop, said that in 1880, at the instance of her mother and sister, she was confined in that establishment and remained there for two years, although she was confident of her sanity during that entire period. She was finally brought out on a writ of \"habeas corpus\" applied for by James Bailey Silkman. She stated that, like herself, he had been incarcerated at Utica asylum under a false charge of lunacy, trumped up by relatives, and that on regaining his freedom, he published an announcement that he would \"help any sane person out of an asylum who would communicate with him, giving his office address at New York City.\" After Lathrop found a way to notify Silkman, he applied for a writ, on which she was taken to Hudson River State Hospital at Poughkeepsie, where her sanity was established and she was freed by Judge George G. Barnard.\n\nThe \"Catholic World\" book review of 1891 commented that: \nThe \"Belford's Magazine\" book review of 1891 commented that,\n\n\n"}
{"id": "36177269", "url": "https://en.wikipedia.org/wiki?curid=36177269", "title": "Access Project", "text": "Access Project\n\nThe Access Project is a non-profit organization dedicated to increasing the quality and accessibility of health care in Rwanda. Founded in 2002 by public health expert Josh Ruxin and economist Jeffrey Sachs, the organization provides technical and operational assistance to improve the management capacity of rural health centers, with a focus on maternal and child health.\n\nColumbia University's Earth Institute initiative also builds new health centers in partnership with the Rwanda Ministry of Health (MOH) and led the country's first neglected tropical diseases (NTD) control program.\n\nThe Access Project's approach is to apply private sector principles to the management of health centers across Rwanda. Through hands-on trainings and support, the organization seeks to make health centers efficient, accountable, and self-sustaining. Interventions focus on eight \"management domains,\" from planning and coordination to drug procurement and financial management. In places where infrastructure does not exist or is beyond repair, Access works with the MOH to build new health centers. Together with the MOH, Access has built six health centers in Gashora, Gataraga, Kintobo, Juru, Ngeruka, and Nyarugenge.\n\nSeed funded by Rob Glaser, the CEO of Real Networks and a former Microsoft executive, and his Glaser Progress Foundation, the Access Project has also received funding from the GE Foundation, the MAC AIDS Fund, and the Schmidt Family Foundation. In addition, Access has been funded by Pfizer, the MAIA Foundation, the Legatum Foundation, and the Garth Brooks Teammates for Kids Foundation.\n\n"}
{"id": "855717", "url": "https://en.wikipedia.org/wiki?curid=855717", "title": "Alcoholic ketoacidosis", "text": "Alcoholic ketoacidosis\n\nAlcoholic ketoacidosis is a common reason for admission of alcohol dependent persons in hospitals emergency rooms. The term refers to a metabolic acidosis syndrome caused by increased ketone levels in serum. Glucose concentration is usually normal or a little lower.\n\nIn 1940, Drs Edward S. Dillon, W. Wallace, and Leon S. Smelo, first described alcoholic ketoacidosis as a distinct syndrome. They stated that \"because of the many and complex factors, both physiologic and pathologic, which influence the acid-base balance of the body, a multitude of processes may bring about the state of acidosis as an end result.\"\n\nIn 1971, David W. Jenkins and colleagues described cases of three non‐diabetic patients with a history of chronic heavy alcohol misuse and recurrent episodes of ketoacidosis. This group also proposed a possible underlying mechanism for this metabolic disturbance, naming it alcoholic ketoacidosis.\n\nPatients regularly report nausea, vomiting, and pain in abdomen which are the most commonly observed complaints. This syndrome is rapidly reversible and, if taken care of has a low mortality. Other patients present tachypnoea, tachycardia, and hypotension.\n\nThe main differences between patients with diabetic ketoacidosis is that patients with alcoholic ketoacidosis are usually alert and lucid despite the severity of the acidosis and marked ketonaemia.\n\nHowever, there are cases where alcoholic ketoacidosis can cause death of the patient if not treated with administration of dextrose and saline solutions.\n"}
{"id": "38540045", "url": "https://en.wikipedia.org/wiki?curid=38540045", "title": "Aqueduct (water supply)", "text": "Aqueduct (water supply)\n\nAn aqueduct is a watercourse constructed to carry water from a source to a distribution point far away. In modern engineering, the term aqueduct is used for any system of pipes, ditches, canals, tunnels, and other structures used for this purpose. The term aqueduct also often refers specifically to a bridge on an artificial watercourse. The word is derived from the Latin ' (\"water\") and ' (\"to lead\"). Aqueducts were used in ancient Greece, ancient Egypt, and ancient Rome. In modern times, the largest aqueducts of all have been built in the United States to supply the country's biggest cities. The simplest aqueducts are small ditches cut into the earth. Much larger channels may be used in modern aqueducts. Aqueducts sometimes run for some or all of their path through tunnels constructed underground. Modern aqueducts may also use pipelines. Historically, agricultural societies have constructed aqueducts to irrigate crops and supply large cities with drinking water.\n\nAlthough particularly associated with the Romans, aqueducts were devised much earlier in Greece and the Near East and Indian subcontinent, where peoples such as the Egyptians and Harappans built sophisticated irrigation systems. Roman-style aqueducts were used as early as the 7th century BC, when the Assyrians built an 80 km long limestone aqueduct, which included a 10 m high section to cross a 300 m wide valley, to carry water to their capital city, Nineveh.\n\nThe Indian subcontinent is believed to have some of the earliest aqueducts. Evidence can be found at the sites of present-day Hampi, Karnataka. The massive aqueducts near river Tungabhadra supplying irrigation water were once long. The waterways supplied water to royal bath tubs.\n\nIn Oman from the Iron Age, in Salut, Bat, and other sites, a system of underground aqueducts called falaj or qanāts were constructed, a series of well-like vertical shafts, connected by gently sloping horizontal tunnels.\n\nThere are three types of falaj:\n\nThese enabled large scale agriculture to flourish in a dry land environment.\n\nIn Persia from early times a system of underground aqueducts called qanāts were constructed, a series of well-like vertical shafts, connected by gently sloping tunnels. This technique:\n\nThroughout Petra, Jordan, the Nabataean engineers took advantage of every natural spring and every winter downpour to channel water where it was needed. They constructed aqueducts and piping systems that allowed water to flow across mountains, through gorges and into the temples, homes, and gardens of Petra's citizens. Walking through the Siq, one can easily spot the remains of channels that directed water to the city center, as well as durable retention dams that kept powerful flood waters at bay.\n\nOn the island of Samos, the Tunnel of Eupalinos was built during the reign of Polycrates (538-522 BC). It is considered an underground aqueduct and brought fresh water to Pythagoreion for roughly a thousand years.\n\nRoman aqueducts were built in all parts of the Roman Empire, from Germany to Africa, and especially in the city of Rome, where they totaled over . The aqueducts supplied fresh water to public baths and for drinking water, in large cities across the empire, and set a standard of engineering that was not surpassed for more than a thousand years. Bridges, built in stone with multiple arches, were a distinctive feature of Roman aqueducts and hence the term \"aqueduct\" is often applied specifically to a bridge for carrying water.\n\nNear the Peruvian town of Nazca, an ancient pre-Columbian system of aqueducts called Puquios were built and are still in use today. They were made of intricately placed stones, a construction material widely used by the Nazca culture. The time period in which they were constructed is still debated, but some evidence supports circa A.D. 540–552, in response to drought periods in the region.\n\nThe Guayabo National Monument of Costa Rica, a park covering the largest archaeological site in the country, contains a system of aqueducts. The complex network of uncovered and covered aqueducts still functions well. The aqueducts are constructed from rounded river stones, which are mostly made of volcanic rock. The civilization that constructed the aqueduct system remains a mystery to archaeologists; it is suspected that Guayabo's aqueducts sat at a point of ancient cultural confluence between Aztecs, Mayans, and Incas.\n\nWhen Europeans saw the Aztec capital Tenochtitlán, early in the 16th century, the city was watered by two aqueducts. One of these, Chapultepec Aqueduct, built circa 1420, was rebuilt by the Spanish almost three hundred years later. Originally tracing part of its path over now-gone Lake Texcoco, only a fragment remains in Mexico City today.\n\nExtensive usage of elaborate aqueducts have been found to have been used in ancient Sri Lanka.The best example is the Yoda Ela or Jaya Ganga, an long water canal carrying excess water between two artificial reservoirs with a gradient of 10 to 20 cm per kilometer during the fifth century AD. However, the ancient engineering methods in calculating the exact elevation between the two reservoirs and the exact gradient of the canal to such fine precision had been lost with the fall of the civilization in 13th Century.\n\nIn modern times, the largest aqueducts of all have been built in the United States to supply the country's biggest cities. The Catskill Aqueduct carries water to New York City over a distance of 120 miles (190 km), but is dwarfed by aqueducts in the far west of the country, most notably the Colorado River Aqueduct, which supplies the Los Angeles area with water from the Colorado River nearly 400 km to the east and the 701.5 mi (1,129 km) California Aqueduct, which runs from the Sacramento-San Joaquin River Delta to Lake Perris. The Central Arizona Project is the largest and most expensive aqueduct constructed in the United States. It stretches 336 miles from its source near Parker, Arizona to the metropolitan areas of Phoenix and Tucson. An aqueduct in New Zealand, \"the Oamaru Borough Race\" was constructed in the late 19th century to deliver water (and water-power) about 50 km from the Waitaki River at Kurow to the coastal town of Oamaru.\n\nIn Spain, the Tagus-Segura Water Transfer system of aqueducts opened in 1979 and transports water from north to south.\n\nIn China, the South–North Water Transfer Project aims to connect the Yangtze River basin to Beijing through three separate systems. The project will reuse part of the Grand Canal of China.\n\nThe simplest aqueducts are small ditches cut into the earth. Much larger channels may be used in modern aqueducts, for instance the Central Arizona Project uses wide channels. A major factor in the design of all open channels is its gradient. A higher gradient allows a smaller channel to carry the same amount of water as a larger channel with a lower gradient, but increases the potential of the water to damage the aqueduct's structure. A typical Roman aqueduct had a gradient of about 1:4800.\n\nA constructed functional rill is a small canal or aqueduct of stone, brick, concrete, or other lining material; usually rectilinear in cross section; for water transportation from a source such as a river-creek, spring, reservoir, qanat, or aqueduct; for domestic consumption or agricultural irrigation of crop land uses.\n\nRills were traditionally used in Middle Eastern and Mediterranean climate cultures of ancient and historical eras; and other climates and continents worldwide. They are distinguished from a 'water ditch' by being lined to reduce absorption losses and to increase durability. The \"Falaj\" irrigation system at the Al Ain Oasis, in present-day Abu Dhabi Emirate, uses rills as part of its qanat water system. Sometimes in the Spanish language they are called \"Acequias\".\n\nRills are also used for aesthetic purposes in landscape design. Rills are used as narrow channels of water inset into the pavement of a garden, as linear water features, and often tiled and part of a fountain design.\n\nThe historical origins are from paradise garden religious images that first translated into ancient Persian Gardens. Rills were later exceptionally developed in the Moorish (Spanish) Gardens of Al-andalus, such as at the Alhambra in Granada; and also in other Islamic gardens, cultures, and countries. Early 20th century examples are in the Maria Louisa Park gardens in Seville, Spain; and at the Casa del Herrero gardens in Montecito, California.\n\nAqueducts sometimes run for some or all of their path through tunnels constructed underground. A version of this common in North Africa and Central Asia that has vertical wells at regular intervals is called a qanat. One historic example found in Syria, the Qanat Firaun, extends over 100 kilometers.\n\nModern aqueducts may also make extensive use of pipelines. Pipelines are useful for transporting water over long distances when it needs to move over hills, or where open channels are poor choices due to considerations of evaporation, freezing, pollution, or environmental impact. They can also be used to carry treated water.\n\nHistorically, agricultural societies have constructed aqueducts to irrigate crops. Archimedes invented the water screw to raise water for use in irrigation of croplands.\n\nAnother use for aqueducts is to supply large cities with drinking water. It also help drought prone areas with water supplies. Some of the Roman aqueducts still supply water to Rome today. In California, United States, three large aqueducts supply water over hundreds of miles to the Los Angeles area. Two are from the Owens River area, and a third is from the Colorado River.\n\nIn modern civil engineering projects, detailed study and analysis of open-channel flow is commonly required to support flood control, irrigation systems, and large water supply systems when an aqueduct rather than a pipeline is the preferred solution.\n\nIn the past, aqueducts often had channels made of earth or other porous materials but significant amounts of water are lost through such unlined aqueducts. As water gets increasingly scarce, these canals are being lined with concrete, polymers, or impermeable soil. In some cases, a new aqueduct is built alongside the old one because it cannot be shut down during construction.\n\n\n\n\n\n\n"}
{"id": "49162947", "url": "https://en.wikipedia.org/wiki?curid=49162947", "title": "Asteroid laser ablation", "text": "Asteroid laser ablation\n\nAsteroid laser ablation is a proposed method for deflecting asteroids, involving the use of a laser array to alter the orbit of an asteroid. Laser Ablation works by heating up a substance enough to allow gaseous material to eject, either through sublimation (solid to gas) or vaporization (liquid to gas). For most asteroids this process occurs between temperatures in the range of 2700-3000 Kelvin. The ejecting material creates a thrust, which over an extended period of time can change the trajectory of the asteroid. As a proof of concept on a small scale, Travis Brashears, a researcher at UC Santa Barbara's Experimental Cosmology Lab, led by Dr. Philip Lubin, has already experimentally verified that laser ablation can de-spin and spin-up an asteroid. Further testing and development of this method are being done by groups at UC Santa Barbara, NASA and the University of Strathclyde.\n\nModern humans, or Homo sapiens, have existed for approximately 200,000 years. By comparison, the dinosaurs survived on Earth for over 100 million years before the Chixculub asteroid wiped them out. Despite the advancements in technology made by humans over the past hundred years there are still no viable defense mechanisms against an asteroid impact. Asteroids could still potentially pose a serious threat to every major city on earth and even to our whole species.\n\nFebruary, 2013, the Chelyabinsk Meteor exploded at a height of 30 kilometers over western Russia. The meteor, which weighed around 15 million pounds, was estimated to be traveling 40,000 miles per hour and entered Earth's atmosphere at an angle of 20 degrees. The explosion was between 20-30 times stronger than the bomb dropped on Hiroshima; the resulting shock wave broke windows on the ground and injured around 1,500 people. Due to the meteor's relatively shallow angle it exploded high in Earth atmosphere. However, had the meteor reached Earth's surface or exploded lower in the atmosphere the results could have been catastrophic.\n\nDespite NASA's efforts to detect Near Earth Objects (NEO's), the Chelyabinsk Meteor went undetected. In recent years, NASA, in partnership with the European Space Agency, have been increasing their efforts to track all NEO's with the potential to cross Earth's orbit. On their website, NASA has a public list of all known NEO's which present a potential impact risk. However, the list remains incomplete and the question of what to do in the event of an imminent impact remains unanswered.\n\nLaser Ablation is a promising method because it allows an asteroid to be redirected without breaking the asteroid into smaller pieces, each of which may pose its own threat to Earth. The nuclear impactor is another proposed method for deflecting asteroids, but is less promising than laser ablation for both political and technical reasons: \nLaser ablation is already being experimentally tested in labs as a method for asteroid deflection and there are plans to begin testing on the International Space Station (ISS), and in low earth orbit.\n\nShort acting laser ablation is used, to verify and explore, the effectiveness of the powerful thermal X-ray pulse that would be emitted upon the detonation of an asteroid stand-off nuclear explosive device. Investigations to this end were conducted in 2015 by exposing common meteorite fragments to tuned laser pulses, provided by Sandia National Laboratory.\n\n\nThere are two types of proposed asteroid laser ablation systems, a stand-on system and a stand-off system. The main difference is the size and position of the laser array used.\n\nA stand-on system consists of a small laser array which is sent to the target asteroid. The system would be limited to small or medium size asteroids and would require detection relatively far in advance. For a short notice threat a stand-on system would be ineffective, due to the time required to build and send the system to the target. Additionally, a new system would be required for each target. In the short term a stand-on system could be affordable and feasible; however, in the long term a larger system able to deflect asteroids of all sizes is more practical.\n\nA stand-off system is a large laser array which would orbit the Earth or possibly the moon. It would range from approximately the size of the ISS to around 10 times larger. The system would be able to deflect even the largest asteroids, which can be hundreds of kilometers across, and also ideally be able to target multiple asteroids at once if necessary. Although this system would be the most effective against a wide variety of threats, its size, and consequentially its cost, make it an unrealistic option for the near future. The implementation of this type of system would likely require the cooperation and collaboration of multiple governments and agencies.\n\nThere are many factors which contribute to the effectiveness of an asteroid laser ablation system. Researchers have been looking at the strength of the laser, as well as, the shape and composition of the target asteroid, as two of the most important factors.\n\nA stronger laser can create a greater thrust on an asteroid. Researchers at UC Santa Barbara have experimentally simulated the time it would take to redirect a moderately sized asteroid using different strength lasers. The strongest lasers tested could hypothetically require under a year to redirect an asteroid a safe distance from the Earth, while the weakest lasers could take up to 10 years. \nChoosing the optimal laser strength is a question of balancing the cost, energy use, and desired level of protection.\n\nTypically such systems require substantial amounts of power. For space-based systems, this might require either some form of nuclear power, or power from a Space-Based Solar Power satellite. Many proponents of Space-Based Solar Power imagine one of the benefits of such an infrastructure includes the ability to divert asteroids and comets are alter their trajectory for exploitation via asteroid mining, as well as for laser-sail based interstellar propulsion.\n\nAsteroids vary greatly in their composition and shape. An asteroid's composition can range from entirely metallic, to entirely rocky, to a mix of rock and metals. The composition must be taken into account since each material behaves differently when ablated. Initial trials at the University of Strathclyde have shown that laser ablation could be more effective on dense metallic asteroids, because of the shape made by the ejecting material.\n\nIn simulations asteroids have been assumed to be spherical; however, in reality most asteroids are irregularly shaped. One of the next steps for research on laser ablation is simulating how a thrust may affect the trajectory of irregularly shaped asteroids.\n"}
{"id": "10947777", "url": "https://en.wikipedia.org/wiki?curid=10947777", "title": "Bandim Health Project", "text": "Bandim Health Project\n\nThe Bandim Health Project works with population based health research in one of the world's poorest countries, Guinea-Bissau in West Africa.\n\nThe core of the project is a health and demographic surveillance system which registers more than 100,000 people in six suburbs of the capital Bissau. Furthermore, 182 representative clusters of 100 women and their children are followed in the rural areas. Information on health, diseases, immunisations, breast-feeding, etc. is collected, primarily focusing on women and children. Admissions to the country's sole pediatric ward in the capital are recorded.\n\nThe Bandim Health Project is member of the INDEPTH Network of health and demographic surveillance sites in Africa, Asia and Oceania.\n\nThe Bandim Health Project was initiated in 1978 by Peter Aaby. The project is currently based on collaboration between the Ministry of Health in Guinea-Bissau, Statens Serum Institut in Denmark, and researchers affiliated to The University of Southern Denmark, as well as the University of Aarhus, Denmark.\n\nIn 2012, the Danish National Research Foundation funded the establishment of the Center of Excellence, The Research Center for Vitamins and Vaccines (CVIVA) based on the Bandim Health Project and its research into non-specific effects of vaccines.\n\nThe Bandim Health Project works with population based health research, focusing on women and children. The project's fields of research include:\n\nOne of the most important findings was that a new measles vaccine used in low-income countries was associated with a two-fold increase in mortality among girls. This discovery led to the withdrawal of the vaccine. Had it not been withdrawn, it could have cost at least ½ million additional female deaths per year in Africa alone.\n\nThe Bandim Health Project is led by Peter Aaby. The National Research Coordinator is Amabelia Rodrigues. Since the project's foundation in 1978, more than 700 scientific articles have been published, and more than 40 PhD or doctoral degrees and 13 Masters of International Health degrees have been obtained by researchers employed by the project.\n\nBandim Health Project is placed in Guinea-Bissau and also has a small department at Statens Serum Institut in Denmark. Bandim Health Project is also affiliated with University of Southern Denmark, where Peter Aaby is an adjunct professor and Christine Benn holds a professorship in Global Health.\n\n\n"}
{"id": "277248", "url": "https://en.wikipedia.org/wiki?curid=277248", "title": "Blinded experiment", "text": "Blinded experiment\n\nA blind or blinded-experiment is an experiment in which information about the test is masked (kept) from the participant, to reduce or eliminate bias, until after a trial outcome is known. It is understood that bias may be intentional or subconscious, thus no dishonesty is implied by blinding. If both tester and subject are blinded, the trial is called a double-blind experiment.\n\nBlind testing is used wherever items are to be compared without influences from testers' preferences or expectations, for example in clinical trials to evaluate the effectiveness of medicinal drugs and procedures without placebo effect, observer bias, or conscious deception; and comparative testing of commercial products to objectively assess user preferences without being influenced by branding and other properties not being tested.\n\nBlinding can be imposed on researchers, technicians, or subjects. The opposite of a blind trial is an open trial. Blind experiments are an important tool of the scientific method, in many fields of research—medicine, psychology and the social sciences, natural sciences such as physics and biology, applied sciences such as market research, and many others. In some disciplines, such as medicinal drug testing, blind experiments are considered essential.\n\nIn some cases, while blind experiments would be useful, they are impractical or unethical; an example is in the field of developmental psychology: although it would be informative to raise children under arbitrary experimental conditions, such as on a remote island with a fabricated enculturation, it is a violation of ethics and human rights.\n\nThe terms blind (adjective) or to blind (transitive verb) when used in this sense are figurative extensions of the literal idea of blindfolding someone. The terms masked or to mask may be used for the same concept; this is commonly the case in ophthalmology, where the word 'blind' is often used in the literal sense.\n\nSome argue that the use of the term \"blind\" for academic review or experiments is offensive and prefer the alternative term \"masked\" or \"anonymous\".\n\nThe French Academy of Sciences originated the first recorded blind experiments in 1784: the Academy set up a commission to investigate the claims of animal magnetism proposed by Franz Mesmer. Headed by Benjamin Franklin and Antoine Lavoisier, the commission carried out experiments asking mesmerists to identify objects that had previously been filled with \"vital fluid\", including trees and flasks of water. The subjects were unable to do so. The commission went on to examine claims involving the curing of \"mesmerized\" patients. These patients showed signs of improved health, but the commission attributed this to the fact that these patients believed they would get better—the first scientific suggestion of the now well-known placebo effect.\n\nIn 1799, the British chemist Humphry Davy performed another early blind experiment. In studying the effects of nitrous oxide (laughing gas) on human physiology, Davy deliberately did not tell his subjects what concentration of the gas they were breathing, or whether they were breathing ordinary air.\n\nBlind experiments went on to be used outside of purely scientific settings. In 1817, a committee of scientists and musicians compared a Stradivarius violin to one with a guitar-like design made by the naval engineer François Chanot. A well-known violinist played each instrument while the committee listened in the next room to avoid prejudice.\n\nOne of the first essays advocating a blinded approach to experiments in general came from Claude Bernard in the latter half of the 19th century, who recommended splitting any scientific experiment between the theorist who conceives the experiment and a naive (and preferably uneducated) observer who registers the results without foreknowledge of the theory or hypothesis being tested. This suggestion contrasted starkly with the prevalent Enlightenment-era attitude that scientific observation can only be objectively valid when undertaken by a well-educated, informed scientist.\n\nDouble-blind methods came into especial prominence in the mid-20th century.\n\nSingle-blind describes experiments where information that could introduce bias or otherwise skew the result is withheld from the participants, but the experimenter will be in full possession of the facts.\n\nIn a single-blind experiment, the individual subjects do not know whether they are so-called \"test\" subjects or members of an \"experimental control\" group. Single-blind experimental design is used where the experimenters either must know the full facts (for example, when comparing sham to real surgery) and so the experimenters cannot themselves be blind, or where the experimenters will not introduce further bias and so the experimenters need not be blind. However, there is a risk that subjects are influenced by interaction with the researchers – known as the experimenter's bias. Single-blind trials are especially risky in psychology and social science research, where the experimenter has an expectation of what the outcome should be, and may consciously or subconsciously influence the behavior of the subject.\n\nA classic example of a single-blind test is the Pepsi Challenge. A tester, often a marketing person, prepares two sets of cups of cola labeled \"A\" and \"B\". One set of cups is filled with Pepsi, while the other is filled with Coca-Cola. The tester knows which soda is in which cup but is not supposed to reveal that information to the subjects. Volunteer subjects are encouraged to try the two cups of soda and polled for which ones they prefer. One of the problems with a single-blind test like this is that the tester can unintentionally give subconscious cues which influence the subjects. In addition, it is possible the tester could intentionally introduce bias by preparing the separate sodas differently (e.g., by putting more ice in one cup or by pushing one cup closer to the subject). If the tester is a marketing person employed by the company which is producing the challenge, there's always the possibility of a conflict of interest where the marketing person is aware that future income will be based on the results of the test.\n\nDouble-blind describes an especially stringent way of conducting an experiment which attempts to eliminate subjective, unrecognized biases carried by an experiment's subjects (usually human) \"and\" conductors. Double-blind studies were first used in 1907 by W. H. R. Rivers and H. N. Webber in the investigation of the effects of caffeine.\n\nIn most cases, double-blind experiments are regarded to achieve a higher standard of scientific rigor than single-blind or non-blind experiments.\n\nIn these double-blind experiments, neither the participants nor the researchers know which participants belong to the control group, nor the test group. Only after all data have been recorded (and, in some cases, analyzed) do the researchers learn which participants were which. Performing an experiment in double-blind fashion can greatly lessen the power of preconceived notions or physical cues (e.g., placebo effect, observer bias, experimenter's bias) to distort the results (by making researchers or participants behave differently from in everyday life). Random assignment of test subjects to the experimental and control groups is a critical part of any double-blind research design. The key that identifies the subjects and which group they belonged to is kept by a third party, and is not revealed to the researchers until the study is over.\n\nDouble-blind methods can be applied to any experimental situation in which there is a possibility that the results will be affected by conscious or unconscious bias on the part of researchers, participants, or both. For example, in animal studies, both the carer of the animals and the assessor of the results have to be blinded; otherwise the carer might treat control subjects differently and alter the results.\n\nComputer-controlled experiments are sometimes also erroneously referred to as double-blind experiments, since software may not cause the type of direct bias between researcher and subject. Development of surveys presented to subjects through computers shows that bias can easily be built into the process. Voting systems are also examples where bias can easily be constructed into an apparently simple machine based system. In analogy to the human researcher described above, the part of the software that provides interaction with the human is presented to the subject as the blinded researcher, while the part of the software that defines the key is the third party. An example is the ABX test, where the human subject has to identify an unknown stimulus X as being either A or B.\n\nA triple-blind study is an extension of the double-blind design; the committee monitoring response variables is not told the identity of the groups. The committee is simply given data for groups A and B. A triple-blind study has the theoretical advantage of allowing the monitoring committee to evaluate the response variable results more objectively. This assumes that appraisal of efficacy and harm, as well as requests for special analyses, may be biased if group identity is known. However, in a trial where the monitoring committee has an ethical responsibility to ensure participant safety, such a design may be counterproductive since in this case monitoring is often guided by the constellation of trends and their directions. In addition, by the time many monitoring committees receive data, often any emergency situation has long passed.\n\nDouble-blinding is relatively easy to achieve in drug studies, by formulating the investigational drug and the control (either a placebo or an established drug) to have identical appearance (color, taste, etc.). Patients are randomly assigned to the control or experimental group and given random numbers by a study coordinator, who also encodes the drugs with matching random numbers. Neither the patients nor the researchers monitoring the outcome know which patient is receiving which treatment, until the study is over and the random code is revealed.\n\nEffective blinding can be difficult to achieve where the treatment is notably effective (indeed, studies have been suspended in cases where the tested drug combinations were so effective that it was deemed unethical to continue withholding the findings from the control group, and the general population), or where the treatment is very distinctive in taste or has unusual side-effects that allow the researcher and/or the subject to guess which group they were assigned to. It is also difficult to use the double blind method to compare surgical and non-surgical interventions (although sham surgery, involving a simple incision, might be ethically permitted). A good clinical protocol will foresee these potential problems to ensure blinding is as effective as possible. It has also been argued that even in a double-blind experiment, general attitudes of the experimenter such as skepticism or enthusiasm towards the tested procedure can be subconsciously transferred to the test subjects.\n\nEvidence-based medicine practitioners prefer blinded randomised controlled trials (RCTs), where that is a possible experimental design. These are high on the hierarchy of evidence; only a meta analysis of several well designed RCTs is considered more reliable.\n\nModern nuclear physics and particle physics experiments often involve large numbers of data analysts working together to extract quantitative data from complex datasets. In particular, the analysts want to report accurate systematic error estimates for all of their measurements; this is difficult or impossible if one of the errors is observer bias. To remove this bias, the experimenters devise blind analysis techniques, where the experimental result is hidden from the analysts until they've agreed—based on properties of the data set \"other than\" the final value—that the analysis techniques are fixed.\n\nOne example of a blind analysis occurs in neutrino experiments, like the Sudbury Neutrino Observatory, where the experimenters wish to report the total number \"N\" of neutrinos seen. The experimenters have preexisting expectations about what this number should be, and these expectations must not be allowed to bias the analysis. Therefore, the experimenters are allowed to see an unknown fraction \"f\" of the dataset. They use these data to understand the backgrounds, signal-detection efficiencies, detector resolutions, etc.. However, since no one knows the \"blinding fraction\" \"f\", no one has preexisting expectations about the meaningless neutrino count \"N\"' = \"N\" × \"f\" in the visible data; therefore, the analysis does not introduce any bias into the final number \"N\" which is reported. Another blinding scheme is used in B meson analyses in experiments like BaBar and CDF; here, the crucial experimental parameter is a correlation between certain particle energies and decay times—which require an extremely complex and painstaking analysis—and particle charge signs, which are fairly trivial to measure. Analysts are allowed to work with all the energy and decay data, but are forbidden from seeing the sign of the charge, and thus are unable to see the correlation (if any). At the end of the experiment, the correct charge signs are revealed; the analysis software is run once (with no subjective human intervention), and the resulting numbers are published. Searches for rare events, like electron neutrinos in MiniBooNE or proton decay in Super-Kamiokande, require a different class of blinding schemes.\n\nThe \"hidden\" part of the experiment—the fraction \"f\" for SNO, the charge-sign database for CDF—is usually called the \"blindness box\". At the end of the analysis period, one is allowed to \"unblind the data\" and \"open the box\".\n\nIn a police photo lineup, an officer shows a group of photos to a witness or crime victim and asks him or her to pick out the suspect. This is basically a single-blind test of the witness's memory, and may be subject to subtle or overt influence by the officer. There is a growing movement in law enforcement to move to a double-blind procedure in which the officer who shows the photos to the witness does not know which photo is of the suspect.\n\nIn recruiting musicians to perform in orchestras and so on, blind auditions are now routinely done: the musicians perform behind a screen so that their physical appearance and gender cannot prejudice the listener judging the performance.\n\n \n"}
{"id": "43122505", "url": "https://en.wikipedia.org/wiki?curid=43122505", "title": "Bureau of Health Information", "text": "Bureau of Health Information\n\nThe Bureau of Health Information is an independent, board-governed statutory authority responsible for reporting on the performance of the health system in New South Wales, Australia.\n\nIt produces regular reports on health system accessibility, appropriateness, effectiveness, efficiency, equity and sustainability.\nThe reports are used to inform efforts to improve patient care and strengthen healthcare policy in NSW.\n\nThe Bureau was established in September 2009 by the NSW Government under the \"Health Services Act 1997\" following the “Final Report of the Special Commission of Inquiry into Acute Care Services in NSW Public Hospitals” by Peter Garling, SC, which recommended that:\n\n\"a Bureau of Health Information be established to access, interpret and report on all data relating to safety and quality of patient care and facilitate its interpretation and re-issue to the unit level on a regular basis.\"\n\n\n"}
{"id": "5092805", "url": "https://en.wikipedia.org/wiki?curid=5092805", "title": "Care Not Killing", "text": "Care Not Killing\n\nCare, Not Killing is an alliance of several organisations who are opposed to the legalisation of euthanasia or physician-assisted suicide in the United Kingdom. Their goals include promoting more and better palliative care, ensuring that existing laws against euthanasia and assisted suicide are not weakened or repealed during the lifetime of the current Parliament, influencing the balance of public opinion against any weakening of the law. They are opposed in their efforts by pro-assisted dying groups such as Dignity in Dying and Humanists UK.\n\nThe members of the alliance include:\n\nCare, Not Killing launched a petition in opposition to a Bill put forward by Lord Joffe. This was signed by 102,363 people in four weeks, and presented to 10 Downing Street on May 12, 2006.\n\n\n\n"}
{"id": "18301326", "url": "https://en.wikipedia.org/wiki?curid=18301326", "title": "Contraceptive implant", "text": "Contraceptive implant\n\nA contraceptive implant is an implantable medical device used for the purpose of birth control. The implant may depend on the timed release of hormones to hinder ovulation or sperm development, the ability of copper to act as a natural spermicide within the uterus, or it may work using a non-hormonal, physical blocking mechanism. As with other contraceptives, a contraceptive implant is designed to prevent pregnancy, but it does not protect against sexually transmitted infections.\n\nSeveral options exist for women, depending on the approval status of devices in their region.\n\nThe contraceptive implant is hormone-based and highly effective, approved in more than 60 countries and used by millions of women around the world. The typical implant is a small flexible tube measuring about 40mm in length and is inserted under the skin (typically in the upper arm) by a health care professional. After it is inserted it prevents pregnancy by releasing hormones that prevent ovaries from releasing eggs and thicken cervical mucous. The two most common versions are the single-rod etonogestrel implant and the two-rod levonorgestrel implant.\n\nBrands include:\n\n\nBenefits of the implant include fewer, lighter periods; improved symptoms of premenstrual syndrome; long-lasting, up to three years; smoker- and breastfeeding-safe; and the convenience of not needing to remember to use it every day. In some cases, negative side effects do occur, the most common being irregular bleeding for the first six to 12 months. Less common symptoms include change in appetite, depression, moodiness, hormonal imbalance, sore breasts, weight gain, dizziness, pregnancy symptoms, and lethargy.\n\nAn intrauterine device (IUD) is a small contraceptive device, often 'T'-shaped and containing either copper or the hormone levonorgestrel, which is implanted into the uterus. They are long-acting, reversible, and the most effective types of reversible birth control. Failure rates with the copper IUD is about 0.8% while the levonorgestrel IUD has a failure rate of 0.2% in the first year of use. Among types of birth control they, along with birth control implants, result in the greatest satisfaction among users. As of 2011, IUDs are the most widely used form of reversible contraception worldwide. IUDs also tend to be one of the most cost-effective methods of contraception for women.\n\nSeveral barriers exist to expanding research into implantable and other contraceptive methods for men, including vague regulatory guidelines, long device development timelines, men's attitudes towards convenience, and a significant lack of funding. Several implantable devices have been attempted, both hormonal and non-hormonal.\n\nIn 2001, Dutch pharmaceutical company Organon announced clinical trials of its implantable etonogestrel-based male contraceptive would begin in Europe and the U.S., anticipating a marketable product as early as 2005. Despite promising results, research development stopped, with outside speculation that lack of marketability was a factor. Organon representative Monique Mols stated in 2007 that \"[d]espite 20 years of research, the development of a [hormonal] method acceptable to a wide population of men is unlikely.\" Schering/Bayer had been working on a similar annual implant with quarterly injections but cancelled the research in 2006/2007, declaring that men would most likely view it as \"not as convenient as a woman taking a pill once a day.\"\n\nIn 2005, a collaboratory project led by the Population Council, the University of California, Los Angeles, and the Medical Research Council began researching a matchstick-sized implant that contains MENT (7α-methyl-19-nortestosterone or Trestolone), a \"synthetic steroid that resembles testosterone.\" Clinical trials were set to begin in 2011 or 2012, and the project was ongoing as of 2016, with hopes of gaining approval as the first reversible male contraceptive.\n\nIn 2006, Shepherd Medical Company received FDA approval for a clinical trial of its non-hormonal implant called an intra vas device (IVD), which consists of two plugs that block sperm flow in the vas deferens. Working on the success of its pilot study and solid results from its clinical trials, the company announced it would expand its trials to three U.S. cities later that year. Questions remained about how reversible the procedure would be in the long-term; however, it was expected to be more reversible than a vesectomy. In 2008, the company disbanded due to the economic crisis but has stated it would restart its research with proper funding.\n\nIn January 2016 news broke of a non-hormonal, implantable valve — the Bimek SLV — with a switch that attaches to the vas deferens, allowing the owner to stop and resume the flow of sperm on demand. A clinical trial of 25 participants was announced to further test the efficacy of the device.\n\nImplantable contraception is also an option for animals, particularly for animal managers at zoos and other captive animal facilities who require reversible contraception methods for managing population growth in limited captive habitat. The Association of Zoos and Aquariums' (AZA) Reproductive Management Center (formerly known as the AZA Wildlife Contraception Center) at the Saint Louis Zoo in St. Louis, Missouri has played a major role in researching and disseminating contraception information to hundreds of institutions around the world since 1999 via its Contraception Database, which houses over 30,000 records for hundreds of species. One of the most popular contraceptive methods used by zoos (as well as in domestic animals) is the melengestrol acetate (MGA) implant, a progestin-based hormonal contraceptive developed in the mid-1970s. Other progestin-based implants that have been placed in animals include Norplant, Jadelle, and Implanon. Androgen-based implants that use agonist (stimulating) gonadotropin-releasing hormone (GnRH) and, to a lesser degree, IUDs have also seen use in several domestic and exotic species. Whatever the implant, some care must be taken to minimize the risk of implant migration or loss\n"}
{"id": "40341841", "url": "https://en.wikipedia.org/wiki?curid=40341841", "title": "Course (food)", "text": "Course (food)\n\nIn dining, a course is a specific set of food items that are served together during a meal, all at the same time. A course may include multiple dishes or only one, and often includes items with some variety of flavors. For instance, a hamburger served with fries would be considered a single course, and most likely the entire meal. Likewise, an extended banquet might include many courses, such as a course where a soup is served by itself, a course where cordon bleu is served at the same time as its garnish and perhaps a side dish, and later a dessert such as a pumpkin pie. Courses may vary in size as well as number depending on the culture where the meal takes place.\n\nMeals are composed of one or more courses, which in turn are composed of one or more dishes.\n\nWhen dishes are served mostly in a single course, this is called service à la française; when dishes are served mostly in separate courses, this is called service à la russe.\n\nThe word is derived from the French word \"cours\" (run), and came into English in the 14th century. It came to be used perhaps because the food in a banquet serving had to be brought at speed from a remote kitchen – in the 1420 cookbook \"Du fait de cuisine\" the word \"course\" is used interchangeably with the word for serving.\n"}
{"id": "50364392", "url": "https://en.wikipedia.org/wiki?curid=50364392", "title": "Disability in North Korea", "text": "Disability in North Korea\n\nReliable information about disability in North Korea, like other information about social conditions in the country, is difficult to find. As of 2016, North Korea is a signatory to the United Nations Convention on the Rights of Persons with Disabilities.\n\nUnder Kim Il-sung, disabled veterans enjoyed a high social status. A factory to employ disabled soldiers was established in 1970. Today, life for the disabled is \"sad, if not horrible\", according to North Korea scholar Fyodor Tertitskiy.\n\nAs a state party to the International Covenant on Economic, Social and Cultural Rights (ICESCR), the Convention on the Rights of the Child (CRC), and the Convention on the Rights of Persons with Disabilities (CRPD), North Korea has international obligations to refrain from discriminating against its people based on disability (among others). Under Article 2 of the CRC, \"States Parties shall respect and ensure the rights set forth in the present Convention to each child within their jurisdiction \"without discrimination\" of any kind, irrespective of the child’s or their parent’s or legal guardian’s race, colour, sex, language, religion, political or other opinion, national, ethnic or social origin, property, \"disability\", birth or other status\" (emphasis added).\n\nNorth Korea ratified CRPD in December 2016.\n\nIn May 2017 the United Nations special rapporteur on the rights of people with disabilities made a first official visit of eight days to North Korea. At a news conference at the end of her visit the raporteur, Catalina Devandas Aguilar, called for more attention to be given to disabled people in the country.\n\nIn 2006, the Associated Press reported from South Korea that a North Korean doctor who defected, Ri Kwang-chol, has claimed that babies born with physical defects are rapidly put to death and buried.\n\nThe charity Handicap International reports that it has been operating in North Korea since 1999, assisting the Korean Federation for the Protection of Disabled People, including supporting orthopedic centers serving thousands of disabled people. The International Committee of the Red Cross reported in 2006 that it had assisted in setting up a rehabilitation center for disabled people in Pyongyang. The International Campaign to Ban Landmines reports that North Korea \"has a comprehensive system for assisting persons with disabilities; however, this system is limited by the general economic situation of the country.\"\n\nStill, the United Nations Special Rapporteur on the situation of human rights in the Democratic People's Republic of Korea, Marzuki Darusman, stated the following in his report before the UN Human Rights Council's twenty-second session:\nAs early as 2003 the Commission on Human Rights expressed deep concern at the \"mistreatment of and discrimination of disabled children\". Since 2006 the General Assembly has consistently decried \"continuing reports of violations of the human rights and fundamental freedoms of persons with disabilities, especially on the use of collective camps and of coercive measures that target the rights of a person with disabilities to decide freely and responsibly on the number and spacing of their children.” Whereas in 2006 the Special Rapporteur noted, \"to date, the situation facing those with disabilities are sent away from the capital city, and particularly those with the mental disabilities are detained in areas or camps known as 'Ward 49' with harsh and subhuman conditions.\"\n\nNorth Korea adopted a law in 2003 to promote equal access for disabled people to public services and claimed in its second report on compliance with the International Covenant on Economic, Social and Cultural Rights that its handicapped citizens are protected. North Korea acceded to this covenant on September 14, 1981. However, its law has not been implemented, and North Korean refugees in the South testify that the handicapped are severely discriminated against unless they are wounded soldiers who say their wounds were the result of American aggression in the Korean War.\n\nBy 2008, the United Nations reported that the government was \"beginning to consider welfare for the disabled\".\n\nThe disabled, with the exception of veterans, have been relocated to places far away from cities since the rule of Kim Il-sung.\n\nIn the early 2000s, it was reported that persons with disabilities in North Korea were locked away in camps, and \"subjected to harsh and sub-human conditions\". Vitit Muntarbhorn, the United Nations' special rapporteur on human rights, reported in 2006 that North Koreans with disabilities were excluded from the country's showcase capital, Pyongyang, and kept in camps where they were categorised by disability. Defectors reported the existence of \"collective camps for midgets\", whose inmates were forbidden from having children. However the charity Handicap International reports that it has been operating in North Korea since 1999 assisting the Korean Federation for the Protection of Disabled People, and the International Committee of the Red Cross reported in 2006 that it had assisted in setting up a rehabilitation centre for disabled people in Pyongyang.\n\nNorth Korea made its Paralympic Games début at the 2012 Summer Paralympics in London, sending a single wildcard representative (Rim Ju-song, a left arm and left leg amputee) to compete in swimming. Yahoo! News reported in 2012 that a Paralympic cultural centre exists in Pyongyang.\nThe country sent two track and field athletes to the 2016 Summer Paralympics in Rio de Janeiro.\n\n\n"}
{"id": "1211335", "url": "https://en.wikipedia.org/wiki?curid=1211335", "title": "Drug diversion", "text": "Drug diversion\n\nDrug diversion is a medical and legal concept involving the transfer of any legally prescribed controlled substance from the individual for whom it was prescribed to another person for any illicit use. The definition varies slightly among different jurisdictions, but the transfer of a controlled substance alone usually does not constitute a diversion, since certain controlled substances that are prescribed to a child are intended to be administered by an adult, as directed by a medical professional. The term comes from the \"diverting\" of the drugs from their original licit medical purpose. In some jurisdictions, drug diversion programs are available to first time offenders of diversion drug laws, which \"divert\" offenders from the criminal justice system to a program of education and rehabilitation.\n\nControlled prescription drug classes which are commonly diverted include:\n\nAccording to the United States Department of Justice, \"Most pharmaceuticals abused in the United States are diverted by doctor shopping, forged prescriptions, theft and, increasingly, via the Internet.\" To reduce the occurrence of pharmaceutical diversion by doctor shopping and prescription fraud, almost all states have established prescription monitoring programs (PMPs) that facilitate the collection, analysis, and reporting of information regarding pharmaceutical drug prescriptions.\n\n21 U.S.C. § 823 of the Controlled Substances Act provides for registration of manufacturers and distributors of controlled substances. The criteria for registering manufacturers of Schedule I and II drugs are particularly strict and call for \"limiting the importation and bulk manufacture of such controlled substances to a number of establishments which can produce an adequate and uninterrupted supply of these substances under adequately competitive conditions for legitimate medical, scientific, research, and industrial purposes.\" The Attorney General must make a positive determination that the registration would be \"consistent with the public interest.\"\n\nFor manufacturers of other drugs, and for drug distributors, the regulations are substantially less strict: \"The Attorney General shall register an applicant… unless he determines that the issuance of such registration is inconsistent with the public interest.\" The criteria for both manufacture and distribution is somewhat biased in favor of established industries, favoring \"past experience\" and a record of compliance with drug laws The Controlled Substances Act also provides for the registration of medical practitioners (i.e., physicians, dentists, veterinarians, etc.), pharmacies and hospitals that prescribe, administer, or dispense controlled substances directly to patients, as well as individuals conducting approved research involving controlled substances. This category also includes narcotic treatment programs that administer and dispense primarily Methadone for narcotic addiction treatment.\n\nThis activity can occur in many venues; \n\"The Cincinnati Post\" has reported on its frequency. John Burke, a leading expert on the issue, was quoted as saying, \"Pharmaceutical diversion is kind of funny because it's going on in every community, but it appears not to exist unless you go after it purposely,\" \n\nAccording to the US Justice Department, in 2011 CVS pharmacies in Sanford, Florida, ordered enough painkillers to supply a population eight times its size. Sanford has a population of 53,000 but the supply would support 400,000. According to the Drug Enforcement Administration (DEA), in 2010 a single CVS pharmacy in Sanford ordered 1.8 million oxycodone pills, an average of 137,994 pills a month. Other pharmacy customers in Florida averaged 5,364 oxycodone pills a month. DEA investigators serving a warrant to a CVS pharmacy in Sanford on October 18, 2011 noted that \"approximately every third car that came through the drive-thru lane had prescriptions for oxycodone or hydrocodone.\"\n\nAccording to the DEA, a pharmacist at that location stated to investigators that \"her customers often requested certain brands of oxycodone using street slang,\" an indicator that the drugs were being diverted and not used for legitimate pain management. In response, CVS in a statement issued February 17 in response to opioid trafficking questions from \"USA Today\" said the company is committed to working with the DEA and had taken \"significant actions to ensure appropriate dispensing of painkillers in Florida.\" \n\nIn February 2012, Joseph Rannazzisi, chief of the Drug Enforcement Administration’s Office of Diversion Control, issued immediate suspension orders against Cardinal Health's supply of oxycodone to suspected pill mills. Deputy Attorney General James M. Cole then called Rannazzisi to a meeting at Justice Department headquarters where Cole warned him “it made good sense to listen to what Cardinal had to say”. Rannazzisi was fired from the drug diversion office in August 2015. Cardinal was never fined.\n\nCardinal, alongside McKesson Corporation and AmerisourceBergen, spent $13 million lobbying Congress to pass Congressman Tom Marino's \"Ensuring Patient Access and Effective Drug Enforcement Act\". The bill, which increases the burden of proof enforcers need to show against drug distributors, was signed into law by President Barack Obama in April 2016.\n\n\n"}
{"id": "25032938", "url": "https://en.wikipedia.org/wiki?curid=25032938", "title": "Dyslalia", "text": "Dyslalia\n\nDyslalia means difficulties in talking due to structural defects in speech organs. for example; sigmatism: which is defective pronunciation of sibilant sounds for example \"S\" pronounced as \"TH\" and Rhotacism: in which the letter \"R\" pronounced as \"I or Y\". It does not include speech impairment due to neurological or other factors.\n\n\n"}
{"id": "25017612", "url": "https://en.wikipedia.org/wiki?curid=25017612", "title": "Early history of food regulation in the United States", "text": "Early history of food regulation in the United States\n\nThe history of early food regulation in the United States started with the 1906 Pure Food and Drug Act, when the United States federal government began to intervene in the food and drug businesses. When that bill proved ineffective, the administration of President Franklin D. Roosevelt revised it into the Federal Food, Drug and Cosmetic Act of 1937. This has set the stage for further government intervention in the food, drug and agricultural markets.\n\nBefore the 1906 Pure Food and Drug Act, most food oversight was mandated to state laws, which were enacted during the colonial days and served mainly trade interests. They set standards of weight, and \"provided for inspections of exports like salt meats, fish and flour\". In 1848, the first national law concerned with regulating food come out of the Mexican–American War, and \"banned the importation of adulterated drugs\". Food inspection was largely thought to be the duty of the consumer, not the government.\n\nWith the advent of modern machinery, food production (especially grain) moved forward at an alarming pace. For example, the “canning line” increased the efficiency of canning foods in an industrial setting. An 1886 report by the Illinois Bureau of Labor Statistics claimed that “New machinery has displaced fully 50 percent of the muscular labor formerly required to do a given amount of work”. Because of these improvements to agriculture, packaged cereals and canned foods became popular. Synthetic medicines (made in labs instead of natural medicines) and chemicals that altered the growing and processing of food began to appear.\n\nProcessed food was more easily transported, thanks to improvements in transportation. Chemical additives were used to \"heighten color, modify flavor, soften texture, deter spoilage, and even transform ... apple scraps, glucose, coal-tar dye, and timothy seed\" into a \"strawberry jam\". However, for these new products, there was no regulation and manufacturers were able to put whatever ingredients they wanted in products like “tonics” without having to list them. Products were often labeled and packaged to appear larger than they were, or packaged to appear to have a higher concentration of food.\n\nThis began to worry high-quality producers who worried that their products might be undermined by deceitful goods. Farmers felt threatened by unfair competition as shady producers adulterated \"fertilizers, deodorized rotten eggs, revived rancid butter, and substituted glucose for honey\". Real strawberry jam producers felt threatened by the bad strawberry “spread” substitutes, since consumers could not tell the difference while buying.\n\nThe first court case involving \"adulterated\" products was in 1886, in which farmers pitted quote “the reigning champion, butter, against a challenger, oleomargarine. Butter won and oleomargarine was taxed\". \"Adulterated\" products often used chemicals or additives to mask poor quality wheat, sour milk, or meat gone bad. In response, these \"unethical\" companies asserted that it was a consumer’s duty to protect themselves from shoddy products. The Division of Chemistry started looking into the adulteration of agricultural commodities around 1867, and in 1883 Harvey Washington Wiley was appointed chief chemist.\n\nThe law \"forbade interstate and foreign commerce in adulterated and misbranded food and drugs\". If a product was found to be in violation, it could be seized and condemned; if a seller was found violating they could be fined and jailed. The law did not define food standards by chemists, but it did prohibit the \"adulteration of food by the removal of valuable constituents, the substitution of ingredients so as to reduce quality, the addition of deleterious ingredients and the use of spoiled animal and vegetable products\". Misleading or false labeling was now considered misbranding and thus illegal.\n\nThe 1906 US Pure Food and Drug Act “defined food adulterations as a danger to health and as consumer fraud”. The “Meat Inspection Act,” which accompanied the law, made tax payers pay for the new regulation. The Department of Chemistry was transformed into a regulatory body charged with regulating packaging, labeling and protecting the consumer. The jam industry was one of the first to be subject to regulation. If a jam did not fit a certain standard of fruit-to-sugar-to-pectin ratio, it bore a “Compound Jam” label. After World War One, these companies, like BRED-SPRED, started aggressive marketing campaigns and attractive packaging to promote themselves. Branded with ‘distinctive names’ like “Peanut Spread” and “Salad Bouquet,” they sold a weak product (like a low ratio of peanuts, or weak vinegar) as high-quality substitutes. Similar deceptive labeling in canned foods prompted the McNary-Mapes Amendment in 1930 which “authorized standards of quality, condition, and/or fill-of-containers.” If a product was sub-standard, it had to display a ‘crepe label’ which read ‘below US standard, low quality, but not illegal’.\n\nAided by Eleanor Roosevelt, the \"American Chamber of Horrors\" helped illuminate the deficiencies in the old 1906 Act. Launched in 1933 with the book \"100,000,000 Guinea Pigs\" by Arthur Kallet and Frederick J. Schlink, the U.S. Food and Drug Administration (FDA) put on an exhibit to illustrate the need for a new law. Eleanor Roosevelt toured it to help elevate its status as a public relations tool. It showed jars with deceptive labeling and packaging which, in the case of jarred chicken, hid dark meat and was jarred in deceptive containers which seemed larger than they were.\n\nIncluded were examples of harmful drugs, including Banbar, a “cure” for diabetes, protected under the 1906 law, and Lash Lure, an eyelash dye that caused many of its women users to go blind. Also legal under the old law was Raditor, a “radium-containing tonic that sentenced users to a slow and painful death.” This, along with the above court cases, caused the FDA to focus on replacing the now outdated “Wiley Act” of 1906.\n\nThe Pure Foods Movement of the 1870s was a grass-roots movement creating the \"principal source of political support for the Pure Food and Drugs Act of 1906\". It was a coalition of many different groups, which is why it became so influential. The following explains the influential groups and individuals involved, as it was not an official coalition, rather a movement created by different individual interests.\n\nThe Ladies Health Association was the first women's group to join the pure foods movement. Starting in 1884, they began a campaign to rid New York City of unsanitary slaughter houses. These women were \"energized to take legal action almost as much by the attitude of the city bureaucrats [who were apathetic] was by the need to protect their families and the neighborhood\". If the city agency in charge of regulating slaughterhouses had been willing to listen to the Association and clean up the slaughterhouses, the women would have never continued their crusade. However, after a hearing, a slaughterhouse owner refused to clean up his property and this caused the women to pursue the execution of the penalty and continue a \"constant vigilance\" to keep it from happening again.\n\nInspired by the Association, 11 other city health protective associations grew out of the need to clean up stockyards and slaughterhouses. In Louisiana, Mrs. Richard Bloor took individual action and visited a packinghouse and afterwards \"sent a description of the conditions to Upton Sinclair to use in his exposés of the meat industry\". The Women's Christian Temperance Union (WCTU) was borne out of a need to protect communities from alcohol abuse and worked mostly on the local level.\n\nWorking mostly only on a local level, they set the tone for the Pure Food movement that would soon follow. Many club women were heavily involved in the temperance movement and began to associate adulterated foods as having the same consequences as alcohol abuse. This is because both inflicted harm on communities. Both were common abuses prevalent in poor communities, and led to malnourishment, violence and other social problems. Women’s organizations began addressing these issues and broadened their activities beyond normal WCTU activities and more women who wanted to protect their communities joined their cause. Members of the WCTU, the Ladies Health Association, and women's clubs laid the foundation for further \"pure food, drink, and drug campaigns in the early 1880s, while their activities centered around study, self-improvement, and philanthropy\".\n\nHarvey W. Wiley became the leader of the pure foods crusade. When Wiley was appointed, he decidedly set the Division of Chemistry in a different direction. He expanded the Division's research and conducted the Foods and Food Adulterants study, which demonstrated his concern about chemicals used in food. He also created the \"Poison Squad\" experiments, in which young, healthy men volunteered to ingest food additive chemicals to determine their impact on human health. Wiley unified many different concerned groups, including state inspectors, the General Federation of Women's Clubs, journalists, reform wing of business, congress members and associations of physicians and pharmacists. As Wiley worked to bring awareness to the pure foods crusades, it gained momentum and legitimacy. His \"poison squad\" brought national awareness to the problem, whereas women's groups brought local attention to the cause.\n\nIn 1906, Upton Sinclair published \"The Jungle\", a book which exposed the filthy conditions of Chicago slaughterhouses. Sinclair wrote the book while living in Chicago; he talked to workers and their families and his focus was the plight of the workers. However, the book turned people away from \"tubercular beef\" instead of turning them socialist like Sinclair wanted. The book was a best seller and the public outcry prompted President Theodore Roosevelt to send officials to investigate. Their “report was so shocking that its publication would ‘be well-nigh ruinous to our export trade in meat’”. This report, Neill-Reynolds, underscored the terrible conditions illustrated by Sinclair. It indicated a need for \"'a drastic and thorogooing [sic]' federal inspection of all stockyards, packinghouses and their products\". \"The Jungle\", combined with the shocking reports of the Neill-Reynolds Report (published June 1906) proved to be the final push to help the Pure Food and Drug Act move quickly through congress.\n\nThe Federal Food, Drug and Cosmetic Act was signed by President Franklin D. Roosevelt on June 25, 1938. The first attempt at reform, The “Tugwell Bill” was a “legislative disaster”. Spurred by public outcry from the Elixir Sulfanilamide disaster (in which 100 people were killed because under the 1906 law, “premarketing toxicity testing was not required”), congress rushed to enact a new bill. Even with the Elixar disaster, the bill itself was not subject to much public awareness.\n\nThis resulted in the 1938 Food, Drug and Cosmetic Act which “pioneered policies designed to protect the pocketbooks of consumers and food standards were enacted to ensure the ‘value expected’ of consumers”. “It changed the drug focus of the Food and Drug Administration from that of a policing agency primarily concerned with the confiscation of adulterated drugs to a regulatory agency increasingly involved with overseeing the evaluation of new drugs”.\n\nThe following is a list of substantial changes from the previous 1906 law\n\n"}
{"id": "51538056", "url": "https://en.wikipedia.org/wiki?curid=51538056", "title": "Factories Act,1948, India", "text": "Factories Act,1948, India\n\nThe Factories Act,1948 (Act No. 63 of 1948), as amended by the Factories (Amendment) Act, 1987 (Act 20 of 1987)), serves to assist in formulating national policies in India with respect to occupational safety and health in factories and docks in India. It deals with various problems concerning safety, health, efficiency and well-being of the persons at work places.\n\nThe Act is administered by the Ministry of Labour and Employment in India through its Directorate General Factory Advice Service & Labour Institutes (DGFASLI) and by the State Governments through their factory inspectorates. DGFASLI advises the Central and State Governments on administration of the Factories Act and coordinating the factory inspection services in the States.\n\nThe Act is applicable to any factory using power & employing 10 or more workers and if not using power, employing 20 or more workers on any day of the preceding twelve months, and in any part of which a manufacturing process is being carried on with the aid of power, or is ordinarily so carried on, or whereon twenty or more workers are working, or were working on any day of the preceding twelve months, and in any part of which a manufacturing process is being carried on without the aid of power, or is ordinarily so carried on; but this does not include a mine, or a mobile unit belonging to the armed forces of the union, a railway running shed or a hotel, restaurant or eating place.\n\nVarious provisions are described in following chapters:\n\n"}
{"id": "3255966", "url": "https://en.wikipedia.org/wiki?curid=3255966", "title": "Health technology in the United States", "text": "Health technology in the United States\n\nHealth technology is defined by the World Health Organization as the \"application of organized knowledge and skills in the form of devices, medicines, vaccines, procedures and systems developed to solve a health problem and improve quality of lives\". This includes pharmaceuticals, devices, procedures, and organizational systems used in the healthcare industry, as well as computer-supported information systems. In the United States, these technologies involve standardized physical objects, as well as traditional and designed social means and methods to treat or care for patients.\n\nMedical technology, or \"medtech\", encompasses a wide range of healthcare products and is used to treat diseases and medical conditions affecting humans. Such technologies are intended to improve the quality of healthcare delivered through earlier diagnosis, less invasive treatment options and reduction in hospital stays and rehabilitation times. Recent advances in medical technology have also focused on cost reduction. Medical technology may broadly include medical devices, information technology, biotech, and healthcare services.\n\nThe impacts of medical technology involve social and ethical issues. For example, physicians can seek objective information from technology rather than read subjective patient reports.\n\nA major driver of the sector's growth is the consumerization of medtech. Supported by the widespread availability of smartphones and tablets, providers are able to reach a large audience at low cost, a trend that stands to be consolidated as wearable technologies spread throughout the market.\n\nIn the years 2010-2015, venture funding has grown 200%, allowing US$11.7 billion to flow into health tech businesses from over 30,000 investors in the space.\n\nCompanies such as Surgical Theater, provide new technology capable of capturing 3D virtual images of patients' brains to use as practice for operations. 3D printing allows medical companies to produce prototypes to practice on before an operation created with artificial tissue.\n\nMedical virtual reality provides doctors multiple surgical scenarios that could happen and allows them to practice and prepare themselves for these situations. It also permits medical students a hands on experience of different procedures without the consequences of making potential mistakes. ORamaVR is one of the leading companies that employs such medical virtual reality technologies to transform medical education (knowledge) and training (skills) in order to improve patient outcomes, reduce surgical errors and training time and democratise medical education and training.\n\nPhones that can track one's whereabouts, steps and more can serve as medical devices, and medical devices have much the same effect as these phones. In the research article, \"Privacy Attitudes among Early Adopters of Emerging Health Technologies\" by Cynthia Cheung, Matthew Bietz, Kevin Patrick and Cinnamon Bloss discovered people were willing to share personal data for scientific advancements, although they still expressed uncertainty about who would have access to their data. People are naturally cautious about giving out sensitive personal information.\n\nIn 2015 the Medical Access and CHIP Reauthorization Act (MACRA) was passed which will be put into play in 2018 pushing towards electronic health records. \"Health Information Technology: Integration, Patient Empowerment, and Security\" by K. Marvin provided multiple different polls based on people's views on different types of technology entering the medical field most answers where responded with somewhat likely and very few completely disagreed on technology being used in medicine. Marvin discusses the maintenance required to protect medical data and technology against cyber attacks as well as providing a proper data backup system for the information.\n\nPatient Protection and Affordable Care Act (ACA) also known as Obamacare and health information technology health care is entering the digital era. Although with this development it needs to be protected. Both health information and financial information now made digital within the health industry might become a larger target for cybercrime. Even with multiple different types of safeguards hackers some how still find their way in so the security that is in place needs to constantly be updated to prevent these breaches.\n\nThe term medical technology may also refer to the duties performed by clinical laboratory professionals in various settings within the public and private sectors. The work of these professionals encompass clinical applications of chemistry, genetics, hematology, immunohematology (blood banking), immunology, microbiology, serology, urinalysis and miscellaneous body fluid analysis. Depending on location, educational level and certifying body, these professionals may be referred to as biomedical scientists, medical laboratory scientists (MLS), medical technologists (MT), medical laboratory technologists and medical laboratory technicians.\n\nAll medical equipment introduced commercially must meet both United States and international regulations. The devices are tested on their material, effects on the human body, all components including devices that have other devices included with them, and the mechanical aspects.\n\nMedical device user fee and modernization act of 2002 was created to make the FDA hurry up on their approval process of medical technology. By introducing sponsor user fees for a faster review time with predetermined performance target for review time.\n\n36 devices and apps were approved by the FDA in 2016.\n\nMedical technology has evolved into smaller portable devices, for instance smartphones, touchscreens, tablets, laptops, digital ink, voice and face recognition and more. With this technology, innovations like electronic health records (EHR), health information exchange (HIE), Nationwide Health Information Network (NwHIN), personal health records (PHRs), patient portals, nanomedicine, genome-based personalized medicine, Geographical Positioning System (GPS), radio frequency identification (RFID), telemedicine, clinical decision support (CDS), mobile home health care and cloud computing came to exist.\n\n3D printing can be used to produce specialized splints, prostheses, parts for medical devices and inert implants. The end goal of 3D printing is being able to print out customized replaceable body parts.\n\nThe concept of health technology assessment (HTA) was first coined in 1967 by the U.S. Congress in response to the increasing need to address the unintended and potential consequences of health technology, along with its prominent role in society. It was further institutionalized with the establishment of the congressional Office of Technology Assessment (OTA) in 1972-1973. HTA is defined as a comprehensive form of policy research that examines short- and long-term consequences of the application of technology, including benefits, costs, and risks. Due to the broad scope of technology assessment, it requires the participation of individuals besides scientists and health care practitioners such as managers and even the consumers.\n\nThere are several American organizations that provide health technology assessments and these include the Centers for Medicare and Medicaid Services (CMS) and the Veterans Administration through its VA Technology Assessment Program (VATAP). The models adopted by these institutions vary, although they focus on whether a medical technology being offered is therapeutically relevant. A study conducted in 2007 noted that the assessments still did not use formal economic analyses.\n\nAside from its development, however, assessment in the health technology industry has been viewed as sporadic and fragmented Issues such as the determination of products that needed to be developed, cost, and access, among others, also emerged. These - some argue - need to be included in the assessment since health technology is never purely a matter of science but also of beliefs, values, and ideologies. One of the mechanisms being suggested – either as an element of- or an alternative to the current TAs is bioethics, which is also referred to as the \"fourth-generation\" evaluation framework. There are at least two dimensions to an ethical HTA. The first involves the incorporation of ethics in the methodological standards employed to assess technologies while the second is concerned with the use of ethical framework in research and judgment on the part of the researchers who produce information used in the industry.\n\nSmartphones, tablets, and wearable computers have allowed people to monitor their own health. These devices run numerous applications that are designed to provide simple health services and the monitoring of one's health. An example of this is Fitbit, a fitness tracker that is worn on the user's wrist. This wearable technology allows people to track their steps, heart rate, floors climbed, miles walked, active minutes, and even sleep patterns. The data collected and analyzed allow users not just to keep track of their own health but also help manage it, particularly through its capability to identify health risk factors.\n\nThere is also the case of the Internet, which serves as a repository of information and expert content that can be used to \"self-diagnose\" instead of going to their doctor. For instance, one need only enumerate symptoms as search parameters at Google and the search engine could identify the illness from the list of contents uploaded to the world wide web, particularly those provided by expert/medical sources. These advance may eventually have some effect on doctor visits from patients and change the role of the health professionals from \"gatekeeper to secondary care to facilitator of information interpretation and decision-making.\" Apart from basic services provided by Google in Search, there are also companies such as WebMD or Infermedica with its product Symptomate that already offer dedicated symptom-checking apps.\n\nThere are numerous careers to choose from in health technology in the USA. Listed below are some job titles and average salaries.\n"}
{"id": "48714865", "url": "https://en.wikipedia.org/wiki?curid=48714865", "title": "Hedonophobia", "text": "Hedonophobia\n\nHedonophobia is an excessive fear or aversion to obtaining pleasure. The purported background of some such associated feelings may be due to an egalitarian-related sentiment, whereby one feels a sense of solidarity with individuals in the lowest Human Development Index countries. For others, a recurring thought that some things are too good to be true has resulted in an ingrainedness that they are not entitled to feel too good. The condition is relatively rare. Sometimes, it can be triggered by a religious upbringing wherein asceticism is propounded.\n\nHedonophobia is formally defined as the fear of experiencing pleasure. 'Hedon' or 'hedone' comes from ancient Greek, meaning 'pleasure' + fear: 'phobia'. Hedonophobia is the inability to enjoy pleasurable experiences, and is often a persistent malady. Diagnosis of the condition is usually related to the age of 'maturity' in each country where the syndrome exists. For instance, in the US a person must be 18 years old to be considered an adult, whereas in Canada he or she must be 18 or 19 years old, depending on the province of residence. Globally, the ages range from (+/-) 12 to 24 years and are mainly determined by traditional ethical practices from previous societies. High anxiety, panic attacks, and extreme fear are symptoms that can result from anticipating pleasure of any kind. Expecting or anticipating pleasure at some point in the future can also trigger an attack.\n\nHedonophobics have a type of guilt about feeling pleasure or experiencing pleasurable sensations, due to a cultural background or training (either religious or cultural) that eschews pleasurable pursuits as frivolous or inappropriate. Oftentimes, social guilt is connected to having fun while others are suffering, and is common for those who feel undeserving or have self-worth issues to work through. Also, there is a sense that they shouldn't be given pleasures due to their lack of performance in life, and because they have done things that are deemed \"wrong\" or \"undeserving.\"\n\nTo determine the depth of the diagnosis for those who suffer from hedonophobia, background is crucial. For example, when a child is taught that a strong work ethic is all that makes them worthy of the good things in life, guilt becomes a motivator to move away from pleasure when they begin to experience it. The individual learns that pleasures are bad, and feeling good is not as sanctified as being empathetic towards those who suffer.\n\nC.B.T. (Cognitive Behavioral Therapy) is an effective approach to the resolution of past beliefs that infiltrate and affect the sufferer's current responses to various situations. Medication is only necessary when there is an interference in the person's normal daily functioning. Various techniques are used by those afflicted with the condition to hide, camouflage or mask their aversion to pleasure.\n\nAny relationship that includes things that are pleasurable is re-established when the sufferer learns that he is not worthy of anything pleasurable, or that he only deserves the opposite of those things which are pleasurable. A disconnect is necessary to determine the sufferer's lack of ability to intervene in the overall process.\n"}
{"id": "28094450", "url": "https://en.wikipedia.org/wiki?curid=28094450", "title": "James J. Cimino", "text": "James J. Cimino\n\nJames J. Cimino, is a physician-scientist and biomedical informatician elected in 2014 to the Institute of Medicine of the National Academy of Science. He pioneered the theory and formalisms of medical concept representation underpinning the use of controlled medical vocabularies in electronic medical records in support of clinical decision-making. Training under Octo Barnett at Harvard University, he also contributed to the initiation of the Unified Medical Language System. In addition, he actively practices medicine as an internist and has devoted many years to develop and innovate clinical information systems that have been integrated in the New York–Presbyterian Hospital, and the Columbia University Medical Center.\n\nCimino is the inaugural director of the Informatics Institute in the School of Medicine and co-director of the UAB Center for Clinical and Translational Science. Previously, Cimino was Chief of the Laboratory for Informatics Development at the NIH Clinical Center, and Professor of Biomedical Informatics at Columbia University (2002-2007).\n\nCimino has published over 200 articles (H-index>62). \n\n\n"}
{"id": "36466832", "url": "https://en.wikipedia.org/wiki?curid=36466832", "title": "Ligneous conjunctivitis", "text": "Ligneous conjunctivitis\n\nLigneous conjunctivitis is a rare form of chronic conjunctivitis characterized by recurrent, fibrin-rich pseudomembranous lesions of wood-like consistency that develop mainly on the underside of the eyelid (tarsal conjunctiva). It is generally a systemic disease which may involve the periodontal tissue, the upper and lower respiratory tract, kidneys, middle ear, and female genitalia. It can be sight-threatening, and death can occasionally occur from pulmonary involvement.\n\nIt has been speculated that ligneous conjunctivitis may be a manifestation of IgG4-related disease (IgG4-RD) involving the conjunctiva.\n\nHistopathological findings from affected humans indicate that wound healing is impaired due to a deficiency in plasmin-mediated extracellular fibrinolysis. Episodes may be triggered by minor trauma, eye surgery, or by systemic events such as infections or antifibrinolytic therapy. Histology shows amorphous subepithelial deposits of eosinophilic material consisting predominantly of fibrin.\nLigneous conjunctivitis may be managed by topical treatments of plasminogen, topical and subconjunctival fresh frozen plasma, and fibrinolytic therapy.\n"}
{"id": "14243461", "url": "https://en.wikipedia.org/wiki?curid=14243461", "title": "Miyuki Ishikawa", "text": "Miyuki Ishikawa\n\nIshikawa was born in Kunitomi, Miyazaki Prefecture and graduated from the University of Tokyo. She later married Takeshi Ishikawa. The relationship did not produce any children.\n\nShe worked as a hospital director in the and was an experienced midwife.\n\nIn the 1940s, there were many babies in her maternity hospital, and Miyuki Ishikawa found herself facing what she perceived to be something of a quandary. The parents of many of these infants were poor and unable to raise their children properly without financial struggle, and she herself was unable to help the infants because of a lack of social and charitable services.\n\nIn order to solve this dilemma, Ishikawa chose to neglect numerous infants, many of whom died as a direct result of this abuse. The exact number of victims is unknown, but it is estimated that she killed at least 103 babies. Almost all of the other midwives employed by the \"Kotobuki\" maternity hospital were disgusted by this practice and resigned from their positions.\n\nLater she also attempted to garner payment for these murders. She and her husband Takeshi solicited large sums of money from the parents, claiming that it would be less than the actual expense of raising these unwanted children. A doctor, Shiro Nakayama, was also complicit in this scheme and aided the couple by falsifying death certificates. The Shinjuku ward office ignored their actions.\n\nSimilar cases had occurred in Japan before this incident. The people of Itabashi were accused in 1930 of murdering 41 foster children. Hatsutarō Kawamata was arrested in 1933 for murdering at least 25 foster children. The Japanese government was aware of this crisis, but did nothing.\n\nJapanese tradition also disputed the rights of infants. Cases of infanticide by a parent had typically been regarded as bodily injury resulting in death under the Criminal Code of Japan until 1907.\n\nTwo police officers from the Waseda police station accidentally found the remains of five of Ishikawa's victims on January 12, 1948. Autopsies performed on the bodies of the five babies proved that they had not died of natural causes. She and Takeshi were arrested on January 15, 1948.\n\nThe victims were deserted children, and so she insisted that parents were responsible for their deaths. The public supported the assertion, but Yuriko Miyamoto criticized them, saying it was an example of discrimination.\n\nUpon further investigation the police found over 40 dead bodies in the house of a mortician. Thirty corpses were later discovered in a temple. The sheer number of dead bodies recovered and the length of time over which the murders took place made it difficult for the authorities to determine the exact number of victims. Consequently, the exact death toll remains unknown.\n\nThe authorities viewed her homicides as a crime of omission. In the Tokyo District Court, Ishikawa was sentenced to eight years in prison, Takeshi and Dr. Shiro Nakayama were each sentenced to four years imprisonment. The couple appealed their sentences and in 1952 the Tokyo High Court revoked the original sentence and sentenced Ishikawa to four years in prison and Takeshi to two years.\n\nThis incident is regarded as the principal reason the Japanese Government began to consider the legalisation of abortion in Japan. One of the reasons this incident was thought to have occurred was as the result of an increase in the number of unwanted infants born in Japan. On July 13, 1948, the Eugenic Protection Law (now the Mother's Body Protection Law) and a national examination system for midwives was established. On June 24, 1949, abortion for economic reasons was legalised under the Eugenic Protection Law in Japan.\n\n\n"}
{"id": "6931292", "url": "https://en.wikipedia.org/wiki?curid=6931292", "title": "Modulated ultrasound", "text": "Modulated ultrasound\n\nUltrasound can be modulated to carry an audio signal (like radio signals are modulated). This is often used to carry messages underwater, in underwater diving communicators, and short-range (under five miles) communication with submarines; the received ultrasound signal is decoded into audible sound by a modulated-ultrasound receiver. A modulated ultrasound receiver is a device that receives a modulated ultrasound signal and decodes it for use as sound, navigational-position information, etc. Its function is somewhat like that of a radio receiver.\n\nDue to the absorption characteristics of seawater, ultrasound (sound at frequencies greater than human hearing, or approximately greater than 20,000 hertz) is not used for long-range underwater communications. The higher the frequency, the faster the sound is absorbed by the seawater, and the more quickly the signal fades. For this reason, most underwater \"telephones\" either operate in \"baseband\" mode (at the same frequency as the voice and is basically a loudspeaker), in a \"UQC-1\" mode (as defined in MIL-C-15240D) with a modulated carrier of 7,500 Hz, or in \"UQC-2\" mode (as defined in MIL-C-22509) from around 8,500 hertz to approximately 12,000 hertz, or in the later \"WQC-2\" mode from 8,500 hertz to approximately 100,000 hertz - with most use around 32,500 hertz) (Also see NATO STANAG-1074 ED.4 for descriptions of internationally used frequencies.)\n\nSee Sound from ultrasound for modulated ultrasound that can make its carried signal audible without needing a receiver set.\n"}
{"id": "33030155", "url": "https://en.wikipedia.org/wiki?curid=33030155", "title": "Mumm-Ra", "text": "Mumm-Ra\n\nMumm-Ra is a fictional character and the primary antagonist of the \"ThunderCats\" franchise. He is an undead evil sorcerer bound to the servitude of four malevolent, godlike entities known as the Ancient Spirits of Evil. Native to the planet of Third Earth, Mumm-Ra's goal is to destroy the ThunderCats and ensure his world remains under his control.\n\nDuring development of the show, the creators were looking for various names for the main villain character, nothing sounded \"evil\" enough, not until Haim Saban suggested to use Mummia Raa (A Bad Mummy in Hebrew), it was shortened to Mumm-Ra and the rest is history .\n\nThe demon sorcerer Mumm-Ra is the self-proclaimed \"ever-living source of evil\" on Third Earth, having powers of sorcery and an apparently unlimited lifespan. He is, in fact, a bound servant to the Ancient Spirits of Evil (represented by four anthropomorphic statues of a boar, crocodile, vulture, and ox thus resembling oversized twisted mockeries of Egyptian canopic jars, within the burial chamber of his pyramid), who provide him with increased power and virtual immortality to further his pursuit of spreading their dark influence throughout Third Earth.\n\nResiding within the Black Pyramid amid the ruins of what appears to be an ancient Egyptian civilization, Mumm-Ra exists in a decayed, weakened form that must return to a stone sarcophagus to replenish his energy. He can summon the power to transform himself into a more vigorous and muscular form - Mumm-Ra, the Ever-Living - by reciting the incantation: \"Ancient Spirits of Evil, transform this decayed form to Mumm-Ra, the Ever-Living!\". While in this form, Mumm-Ra possesses fortification of his mystical might — casting spells, throwing energy bolts, etc. — to battle his foes. He can also alter his physical form into a variety of alter-egos to deceive his enemies.\n\nSeemingly invincible in whatever form he chooses, Mumm-Ra appears to have a singular weakness: seeing his own hideous reflection neutralizes his ability to remain outside the Black Pyramid and forces him to withdraw there in his emaciated mummy form. However, at the beginning of the second season, the Ancient Spirits of Evil overcame this shortcoming. Mumm-Ra is a master of deception, and will use whatever means necessary to fight against the forces of good. In later episodes, while endowing Mumm-Ra with his powers, the statues of the Ancient Spirits of Evil came down from their perches, and extend their arms over him.\n\nHe uses his magic to create disguises and deceive the ThunderCats on various occasions. Among these are: Diamondfly (in the episode \"Queen of Eight Legs\"), Gregory Gregion (\"All That Glitters\"), Silky (\"The Garden of Delights\"), The Netherwitch (\"The Astral Prison\"), and Pumm-Ra (in the episode \"Pumm-Ra\"). He once took the form of King Arthur to acquire the legendary magic sword Excalibur, using it against the Sword of Omens.\n\nIn a few episodes, Mumm-Ra has an even more powerful form beyond \"Mumm-Ra the Ever-Living\" called \"Mumm-Ra the All-Powerful\": in this manifestation, Mumm-Ra absorbs the entire power of the Ancient Spirits of Evil to become grander in size and strength, and the design pattern on his loin cloth changes, as does his voice. This form is only presented in the series three times. In another incarnation, calling himself \"Mumm-Ra the Dream Master,\" he is able to enter dreams to subliminally influence the ThunderCats in their sleep as a form of mind control.\n\nMumm-Ra is regarded as immortal, and when defeated, he simply returns to his sarcophagus. Mumm-Ra cannot be truly killed; even in cases where his body is destroyed, he will eventually be restored, as he often states: \"Wherever evil exists, Mumm-Ra lives!\"\n\nLater, Mumm-Ra is shown to have a blue undead, bulldog-like companion named Ma-Mutt, capable of flight and supernatural feats of strength and speed. He is generally evil, though some episodes depict him as having sympathetic qualities. Ma-Mutt is the only living creature Mumm-Ra shows any compassion and love for - in one episode, after the machine he was using in his evil plan was destroyed, he frantically searched for Ma-Mutt, and upon finding him, apologized for being horrible to him earlier in the episode.\n\nFed up with Mumm-Ra's repeated failures, the Ancient Sprirts of Evil demand that he destroys the ThunderCats or else he will be exiled from Third Earth. When Mumm-Ra fails the deadline, the Ancient Spirits of Evil trap Mumm-Ra in a crystal with a miniature pyramid and cast to the farthest reaches of the universe. When the ThunderCats decide to colonize New Thundera to rebuild it, the Ancient Spirits of Evil free Mumm-Ra and rebuild his cauldron on Third Earth. In one episode, it was said that Mumm-Ra once owned the Sphere of Seti which increased his power. When he managed to recover it after it was found by Char, he planned to use it to increase his power and free himself from servitude to the Ancient Spirits of Evil. Angered at his plot as well as at his deceptive attempts to hide it, the Ancient Spirits seal off the Black Pyramid, forcing Mumm-Ra to choose between sustenance (the pyramid) or power (the sphere). Ultimately, Mumm-Ra admits that he cannot survive without the pyramid—thus he begrudgingly surrenders the sphere and once again resumes his role as a servant of the Ancient Spirits.\n\nIn the 2011 version, Mumm-Ra is an Ancient Spirit of Evil employed by the Ancient Spirits of Evil (confirmed by series Art Director Dan Norton) with changes including demon wings on his Ever-Living form, a larger body, and a weakness to intensely bright light. His ultimate goal is universal domination and enforcing his ideal order on it. When utilizing the power of the Gauntlet of Plun-Darr, Mumm-Ra's power increases and he gains a robed armored form (this continuity's equivalent to the All-Powerful form), with a skeletal mask and Egyptian headdress. In his Ever-Living form, he continues the show's trend of all characters and creatures being animals by showing traits of a leaf-nosed vampire bat.\n\nCenturies prior to the new series, Mumm-Ra used advanced technology and magic at his disposal to enslave the ancestors of the ThunderCats and the Animals into serving him so he would gather the Four Powerstones (the War Stone, the Tech Stone, the Spirit Stone, and the Soul Stone) from various planets to place on the Sword of Plun-Darr, a weapon forged from a star he had collapse at the cost of the entire Plun-Darr galaxy. But managing to take the War Stone (which would become the Eye of Thundera) while staging a rebellion with the Animals' help, the ThunderCat Leo defeats Mumm-Ra and strips him of the other Powerstones. But when Mumm-Ra's pyramid spacecraft was pulled into Third Earth's atmosphere, Mumm-Ra entered his tomb to bide his time for everyone else to die in the resulting crash. However, the controls were smashed and Mumm-Ra was trapped within his pyramid as the stones and survivors spread across Third Earth.\n\nMany centuries later, Mumm-Ra used Grune to release him from his prison and masterminded Thundera's downfall. With the aid of Grune and Slythe, Mumm-Ra not only plans to obtain the Eye of Thundera, but to regain the other three Powerstones in order to get to the Sword of Plundarr and the remaining stones. Mumm-Ra also resurrected Pumyra to serve him and planted her amongst the Thunderian slaves. After Grune ends up trapped in the Astral Plane, Mumm-Ra has Slithe recruit the homicidal Addicus and sociopathic Kaynar to not only restore the Lizard army morale and punish any who quit, but also increase their numbers with members of the new generals' respective races. Later, through a calculated scheme involving the reveal of his agent Pumyra, Mumm-Ra regains the Sword of Plun-Darr (along with his All-Powerful form) and the Tech Stone while gaining a new ally in Vultaire. The series, had it not been cancelled, would presumably have followed Mumm-Ra's search for the final Stone, the Soul Stone.\n\nAn absolute master of magic, Mumm-Ra possesses nearly unlimited knowledge of mystical arts from all corners of the universe. As such, he can summon their use to assist him in almost any manner he chooses. His abilities include electricity manipulation, levitation, necromancy, alchemy, shapeshifting, temporal manipulation, teleportation, psychokinesis, transfiguration, mind control, astral projection, scrying, and so on.\n\nWhile in the form of Mumm-Ra the Ever-Living, he becomes a conduit for the Ancient Spirits of Evil; such that he can amplify his aforementioned abilities to a nearly cosmic scale, gain superhuman strength, as well as retain his undead status despite the passage of time or any injuries sustained. The price of this power however, is extremely limited stamina: once Mumm-Ra the Ever-Living has surpassed a given threshold of energy-expenditure, he must revert to his mummified form, and enter his sarcophagus to rejuvenate himself. Also, Mumm-Ra's power in his Ever-Living form is directly linked to the condition of both his sarcophagus and the Ancient Spirits' four statues within his pyramid; should any of them be harmed or destroyed, his power weakens immediately and considerably.\n\nIn the 2011 show, Mumm-Ra's Ever-Living form similarly amplifies his already formidable mystic powers, while not seeming to have any definite limit as to how long he can maintain this form. Once he recovers the Gauntlet of Plun-Darr (the precursor to the Gauntlet of Omens, Lion-O's gauntlet), he is able to use a double-edged sword decorated with bat wings and a ring where the Eye of Thundera could fit. He can use the gauntlet in his Ever-Living form so as to achieve an All-Powerful form (or a form analogous to the 1980s version) which increases his power further, allowing his double-edged sword to become a dual-bladed scimitar. However, this form can only be achieved if he has two or more of the Powerstones in his possession. If his Powerstones are stripped from his gauntlet while in his All-Powerful form, Mumm-Ra reverts to his skinny, mummified self.\n\nMumm-Ra has been generally well received by the public and fans alike. Viewers have noted the character for his notoriously frightening appearance and deep, raspy voice. In addition, Mumm-Ra is also praised for being much more competent than other famous cartoon villains from the 1980s; such as Skeletor of \"Masters of the Universe\" fame.\n\nWatchMojo.com placed Mumm-Ra 5th place in their \"Top 10 Television Cartoon Villains\" list, stating that he was the most memorable character in the \"ThunderCats\" series, labeling him as \"a bad guy that audiences could actually fear\".\n\n"}
{"id": "43750573", "url": "https://en.wikipedia.org/wiki?curid=43750573", "title": "NAGE EMS", "text": "NAGE EMS\n\nNAGE EMS is a labor union, a division of NAGE / SEIU Local 5000 (National Association of Government Employees) that represents EMS professionals working for private ambulance (Emergency Medical Services, EMT, Paramedic, Dispatcher, Registered Nurses, Support Staff) in self-autonomous union locals. NAGE EMS locals elect their own local governing executive boards and officers and are provided representational, legal, contract, and political support by NAGE but govern and manage their own affairs.\n\nNAGE EMS was created by NAGE in 2012 by NAGE leaders to address the unique, complicated conditions that EMS professionals must navigate and to address the perceived negative effects of changes in EMS on the EMS professionals providing services to the public. NAGE believed the privatization push that began 20 years ago in EMS led to private companies supplying a significant part of the public safety network with 911 emergency and non-emergency services to communities. The NAGE EMS division was codified into the NAGE National Bylaws at the 2014 NAGE National Convention\n\n“Corporate and privately owned EMS has grown over the past few years, and that increases the downward pressure on compensation for those hard working employees. Our mission at NAGE EMS is to help those employees through superior union representation, strong contracts, and safe working conditions for NAGE EMS members and the people they serve.” - David J. Holway, NAGE President on NAGE EMS.\n\nNAGE EMS locals set their own political positions and advocate for their own political positions. Because NAGE EMS is a division of NAGE / SEIU Local 5000, NAGE EMS locals gain access to the political resources of NAGE. Because NAGE is affiliated with SEIU, NAGE gains access to the political resources of SEIU. NAGE EMS locals rely on these resources to help advance their own political agendas and issues. NAGE EMS locals also participate in COPE (Committee On Political Education). COPE is a voluntary political action fund that supports NAGE EMS political and legislative activities. COPE contributions are used to support grassroots lobbying efforts and help elect local, state and federal candidates who support the collective bargaining agreements, issues and concerns of NAGE EMS members.\n\nNAGE, on behalf of NAGE EMS locals is on the congressional record in California as being opposed to California Senate Bill 556, a bill that would require EMS professionals working for a private ambulance company performing public safety services (911 emergency services) under a public contract to wear specific labels. NAGE requested California Governor Brown veto the bill\n\nSome critics of NAGE EMS claim that NAGE EMS is actually the IAEP (International Association of EMTs and Paramedics, another division of NAGE)in disguise or that NAGE EMS is some form of re-branding of IAEP. NAGE claims that \"NAGE has two EMS divisions. The IAEP represents public, third-service, and private EMS across the U.S. NAGE EMS is focused specifically on private EMS.\"\n\nSome critics of NAGE EMS claim that NAGE EMS was created after a failed affiliation between NAGE and NEMSA National Emergency Medical Services Association (NEMSA). NAGE claims \"NAGE tried to address issues specific to private EMS in several ways, including affiliating with other unions. Finally, the NAGE Board of Directors chose to create a NAGE division specifically to address issues related to private EMS.\"\n\n"}
{"id": "37920677", "url": "https://en.wikipedia.org/wiki?curid=37920677", "title": "NHS Property Services", "text": "NHS Property Services\n\nNHS Property Services is a limited company owned by the Department of Health (DoH) in the United Kingdom that took over the ownership of around 3,600 National Health Service (NHS) facilities in April 2013.\n\nFollowing the Health and Social Care Act 2012, strategic health authorities (SHAs) and primary care trusts (PCTs) in England were abolished and replaced with GP led commissioning consortia in April 2013. All properties owned by the SHAs and PCTs not passed to the commissioning groups were transferred to NHS Property Services. The company now manages, maintains and develops the properties on behalf of the DoH.\n"}
{"id": "17593979", "url": "https://en.wikipedia.org/wiki?curid=17593979", "title": "NHS targets", "text": "NHS targets\n\nConservative governments set targets for the NHS in the 1990s – for example, guaranteeing a maximum two-year wait for non-emergency surgery and reducing rates of death from specific diseases. The Blair government introduced far more targets and managed performance far more aggressively - a management regime sometimes referred to as 'targets and terror'. Targets have been blamed for distorting clinical priorities, and in particular for one organisation achieving a target at the expense of another. For example, ambulances have been forced to queue up outside a busy emergency departments so that the ambulances might not be able to meet their target in responding to emergency calls, but the hospital can meet its A&E target. Excess emphasis on the targets can mean that other important aspects of care, especially those not easily measured, may be neglected. NHS England under the Conservative governments reduced the number of targets, in particular removing most of those relating to health inequality, and encouraged a system wide approach. However shortage of staff and funding meant that performance against targets declined. Guidance published in February 2018 conceded that most of the targets would not be met before April 2019.\n\nThe NHS Constitution for England specifies waiting times in the accompanying Handbook, but does not provide a remedy should they be breached.\n\nA four-hour target in emergency departments was introduced by the Department of Health for National Health Service acute hospitals in England. The original target was set at 100%, but lowered to reflect clinical concerns that there will always be patients who need to spend slightly longer in A&E, under observation. Setting a target that, by 2004, at least 98% of patients attending an A&E department must be seen, treated, and admitted or discharged in under four hours. The target was further moved to 95% of patients within four hours in 2010 as a result of the coalition's claims that 98% was not clinically justified. Trusts which failed to meet the target could be fined. In July 2016 NHS trusts were set new \"performance improvement trajectories\". For 47 of the 140 trusts with \"type one\" major A&E facilities this meant a target of less than 95% waiting under 4 hours. In January 2017 Jeremy Hunt announced that the target would in future only apply to \"urgent health problems\". In January 2018 only 77.1% of patients were admitted or discharged within four hours, the worst ever performance for type one A&E departments. In December 2018 it was reported that patients with only minor ailments could be excluded from the target and a new target introduced so the most urgent cases should be seen within an hour.\n\nIn Scotland the target is for 95% of A&E patients to be either admitted, transferred or discharged in four hours. It was last met in July 2017.\n\nThe UK Labour government had identified a requirement to promote improvements in A&E departments, which had suffered underfunding for a number of years. The target, accompanied by extra financial support, was a key plan to achieve the improvements. Tony Blair felt the targets had been successful in achieving their aim. \"We feel, and maybe we are wrong, that one way we've managed to do that promote improvements in A&E is by setting a clear target\".\n\n48% of departments said they did not meet the target for the period ending 31 December 2004. Government figures show that in 2005-06, 98.2% of patients were seen, diagnosed and treated within four hours of their arrival at A&E, the first full financial year in which this has happened.\n\nThe 4-hour target triggered the introduction of the acute assessment unit (also known as the medical assessment unit), which works alongside the emergency department but is outside it for statistical purposes in the bed management cycle. It is claimed that though A&E targets have resulted in significant improvements in completion times, the current target would not have been possible without some form of patient re-designation or re-labeling taking place, so true improvements are somewhat less than headline figures might suggest and it is doubtful that a single target (fitting all A&E and related services) is sustainable.\n\nAlthough the four-hour target helped to bring down waiting times when it was first introduced, since September 2012 (after the introduction of the Health and Social Care Act 2012 and top-down reorganisation of the NHS) hospitals in England struggled to stick to it, prompting suggestions that A&E departments may be reaching a limit in terms of what can be achieved within the available resources. The announcement of the reduction of the target from 98% to 95% was immediately followed by a reduction in attainment to the lower level.\n\nBy December 2014, the number of patients being treated within four hours had fallen to 91.8%. From December 2015 the 95% target over England as a whole was missed every month. From October 2016 to December 2016 only 4 out of 139 hospitals with major type 1 A&E departments met the target. In November 2018 the British Medical Association reported that performance on emergency admissions, trolley waits for more than four hours and A&E patients seen within four hours in the summer of 2017 was worse than in the winters of 2011 to 2015. Performance against the 4 hour wait target in the summer of 2018 was the worst second quarter performance recorded. Only 88.9% of patients were seen within four hours in September. The number of people admitted, transferred or discharged within four hours in emergency departments in September was up more than 3% compared to September 2017 with a 7% rise in emergency admissions said by John Appleby to be astonishing. \n\nAccording to the BMA the main reasons for not reaching this target are:\n\n\nIn 2014, research conducted by QualityWatch, a joint programme from the Nuffield Trust and the Health Foundation, tracked 41 million visits to A&E departments in England in order to better understand the pressures leading to increased waiting times and breaches of the four-hour target. Researchers identified a rise in older patients and related increase in long-term conditions as key factors, alongside extremes of temperature (in both summer and winter) and crowding at peak times. They noted that the majority of pressure was falling on major A&E units, and proposed that rising demand as a result of ageing and population growth may be pushing already stretched emergency departments beyond maximum capacity.\n\nIn July 2017 the Royal College of Emergency Medicine produced a report saying that the NHS needed at least 5,000 more beds to achieve safe bed occupancy levels and hit the four-hour target.\n\nEven though exceptions are allowed to the targets, concerns have been raised that the target has put pressure on A&E staff to compromise patient care. A significant proportion (90%) of A&E consultants welcomed the four hour target in a study but felt that 98% was too high a target.\n\nAt the same time as the four target was introduced a target that no patient should wait longer than 12 hours before they are admitted to a ward, if that is required, was introduced. Between January and March 2012 only 15 patients in England waited more than 12 hours, but in the same months in 2017 1,597 patients breached the target. In January 2018 1,043 patients waited over 12 hours for a bed, the worst figure ever recorded. 272 were at University Hospitals of North Midlands NHS Trust.\n\nOver four million patients were waiting for non urgent hospital care as of July 2017. The Royal College of Surgeons together with other medical groups fear patients are waiting longer in anxiety and pain for hospital procedures. The target was that 90% of patients admitted to hospital for treatment and 95% of those not admitted should receive consultant-led care within 18 weeks unless it is clinically appropriate not to do so, or they choose to wait. The proportion of people waiting more than the six week target for diagnostic tests was at its highest since records began in September 2018.\n\nIn December 2017 there were 1,750 patients waiting a year or more, the highest total since August 2012. 242 were at Imperial College Healthcare NHS Trust, 156 at Mid Essex Hospital Services NHS Trust and 114 at Royal Cornwall Hospitals NHS Trust. 11.8% of those waiting for a procedure had waited 18 weeks or more. By March 2018 there were 2,647. The largest numbers were at Northern Lincolnshire and Goole Hospitals NHS Foundation Trust, Imperial College Healthcare NHS Trust, King's College Hospital NHS Foundation Trust, Royal Cornwall Hospitals NHS Trust and East Kent Hospitals University NHS Foundation Trust.\n\n445,360 had been waiting six months or more by the end of December 2017 - three times more than in 2013. The President of the Royal College of Surgeons said it was “completely shameful” that patients were being forced to resort to paying for operations the NHS should provide as these waiting times led to an increase of 53% between 2012 and 2016 in the numbers paying personally for private operations.\n\nBetween January and March 2018 25,475 operations were cancelled at the last minute for non-clinical reasons by NHS providers - 20% more than the first quarter of 2017, and the highest number since records began in 1994-95. This was 1.3% of all elective activity - the highest proportion recorded since 2004-05.\n\nNHS ambulance services#Targets in England\n"}
{"id": "21167095", "url": "https://en.wikipedia.org/wiki?curid=21167095", "title": "Nap", "text": "Nap\n\nA nap is a short period of sleep, typically taken during daytime hours as an adjunct to the usual nocturnal sleep period. Naps are most often taken as a response to drowsiness during waking hours. A nap is a form of biphasic or polyphasic sleep, where the latter terms also include longer periods of sleep in addition to one single period. Cultural attitudes toward napping during the work day vary. In many Western cultures, children and the elderly are expected to nap during the day and are provided with designated periods and locations in which to do so. In these same cultures, most working adults are not expected to sleep during the day and napping on the job is widely considered unacceptable. Other cultures (especially those in hot climates) serve their largest meals at midday, with allowance for a nap period (siesta) afterwards before returning to work.\n\nNapping is physiologically and psychologically beneficial. Napping for 20 minutes can help refresh the mind, improve overall alertness, boost mood and increase productivity. Napping may benefit the heart. In a six-year study of Greek adults, researchers found that men who took naps at least three times a week had a 37 percent lower risk of heart-related death. \n\nFor years, scientists have been investigating the benefits of napping, including the 30-minute nap as well as sleep durations of 1–2 hours. Performance across a wide range of cognitive processes has been tested. Studies demonstrate that naps are as good as a night of sleep for some types of memory tasks. A NASA study led by David F. Dinges, professor at the University of Pennsylvania School of Medicine, found that naps can improve certain memory functions and that long naps are more effective than short ones. In that NASA study, volunteers spent several days living on one of 18 different sleep schedules, all in a laboratory setting. To measure the effectiveness of the naps, tests probing memory, alertness, response time, and other cognitive skills were used.\n\nThe National Institute of Mental Health funded a team of doctors, led by Alan Hobson, Robert Stickgold, and colleagues at Harvard University for a study which showed that a midday nap reverses information overload. Reporting in \"Nature Neuroscience\", Sara Mednick, Stickgold and colleagues also demonstrated that, in some cases, a 1-hour nap could even boost performance to an individual's top levels. The NIMH team wrote: \"The bottom line is: we should stop feeling guilty about taking that 'power nap' at work.\"\n\nThe siesta habit has recently been associated with a 37% reduction in coronary mortality, possibly due to reduced cardiovascular stress mediated by daytime sleep (Naska et al., 2007). Nevertheless, epidemiological studies on the relations between cardiovascular health and siesta have led to conflicting conclusions, possibly because of poor control of moderator variables, such as physical activity. It is possible that people who take a siesta have different physical activity habits, e.g. waking earlier and scheduling more activity during the morning. Such differences in physical activity may mediate different 24-hour profiles in cardiovascular function. Even if such effects of physical activity can be discounted for explaining the relationship between siesta and cardiovascular health, it is still unknown whether it is the daytime nap itself, a supine posture or the expectancy of a nap that is the most important factor. It was recently suggested that a short nap can reduce stress and blood pressure (BP), with the main changes in BP occurring between the time of lights off and the onset of stage 1 (Zaregarizi, M. 2007 & 2012).\n\nZaregarizi and his team have concluded that the acute time of falling asleep was where beneficial cardiovascular changes take place. This study has indicated that a large decline in blood pressure occurs during the daytime sleep-onset period only when sleep is expected; however, when subjects rest in a supine position, the same reduction in blood pressure is not observed. This blood pressure reduction may be associated with the lower coronary mortality rates seen in Mediterranean and Latin American populations where siestas are common. Zaregarizi assessed cardiovascular function (blood pressure, heart rate, and measurements of blood vessel dilation) while nine healthy volunteers, 34 years of age on average, spent an hour standing quietly; reclining at rest but not sleeping; or reclining to nap. All participants were restricted to 4 hours of sleep on the night prior to each of the sleep laboratory tests. During three daytime naps, he noted significant reductions in blood pressure and heart rate. By contrast, the team did not observe changes in cardiovascular function while the participants were standing or reclining at rest.\n\nThese findings also show that the greatest decline in blood pressure occurs between lights-off and onset of daytime sleep itself. During this sleep period, which lasted 9.7 minutes on average, blood pressure decreased, while blood vessel dilation increased by more than 9 percent.\n\n\"There is little change in blood pressure once a subject is actually asleep,\" Zaregarizi noted, and he found minor changes in blood vessel dilation during sleep (Zaregarizi, M. 2007 & 2012).\n\nFor those suffering from insomnia or depression, naps may aggravate already disrupted sleep-wake patterns.\n\nA power nap, also known as a Stage 2 nap, is a short slumber of 20 minutes or less which terminates before the occurrence of deep slow-wave sleep (SWS), intended to quickly revitalize the napper. The expression \"power nap\" was coined by Cornell University social psychologist James Maas.\n\nThe 20-minute nap increases alertness and motor skills. Various durations may be recommended for power naps, which are very short compared to regular sleep. The short duration prevents nappers from sleeping so long that they enter the slow wave portion of the normal sleep cycle without being able to complete the cycle. Entering deep, slow-wave sleep and failing to complete the normal sleep cycle, can result in a phenomenon known as sleep inertia, where one feels groggy, disoriented, and even sleepier than before beginning the nap. In order to attain optimal post-nap performance, a Stage 2 nap must be limited to the beginning of a sleep cycle, specifically sleep stages N1 and N2, typically 18–25 minutes. \n\nExperimental confirmation of the benefits of this brief nap comes from a Flinders University study in Australia in which 5, 10, 20, or 30 minute sleeps were given. The greatest immediate improvement in measures of alertness and cognitive performance came after the 10 minutes of sleep. The 20 and 30 minute sleeps showed evidence of sleep inertia immediately after the naps and improvements in alertness more than 30 minutes later but not to a greater level than after the 10 minutes of sleep.\n\nPeople who regularly take these short naps, or catnaps as they used to be called, may develop a good idea of the duration which works best for them, as well as which tools, environment, position, and associated factors help produce the best results. Power naps are effective even when schedules allow a full night's sleep. Mitsuo Hayashi and Tadao Hori have demonstrated that a nap improves mental performance, even after a full night's sleep.\n\nA short nap preceded by the intake of caffeine was investigated by British researchers. In a driving simulator and a series of studies, Horne and Reyner looked at the effects of cold air, radio, a break with no nap, a nap, caffeine pill vs. placebo and a short nap preceded by caffeine on mildly sleep-deprived subjects. The caffeine nap was by far the most effective in reducing driving \"incidents\" and subjective sleepiness. Caffeine in coffee takes up to a half-hour to have an alerting effect, hence \"a short (<15min) nap will not be compromised if it is taken immediately after the coffee.\"\n\nA contemporary idea called polyphasic sleeping entails avoiding long sleeps, instead taking regularly spaced short naps. Sara Mednick, whose sleep research investigates the effects of napping, included a chapter, \"Extreme Napping\", in her book \"Take a Nap!\". In response to questions from readers about the \"uberman\" schedule of \"polyphasic sleeping\", she commented as follows:\nPolyphasic sleep is claimed to be of particular benefit in cases where very long periods of wakefulness (+24 hours) are necessary and a normal circadian rhythm is impossible (such as certain cases of military duty or emergency service). Critics of polyphasic sleep dispute the notion that the human brain can simply be classically conditioned to tolerate a state of near constant wakefulness without deleterious effects. Studies by the US military showed the success rate of introducing polyphasic sleep varied widely depending on individual physiology/psychology, the amount of time provided to acclimate to the new schedule, and the type and difficulty of the tasks being performed. Attempting to acclimate to a polyphasic schedule often induced significant stresses in the participants, with a corresponding deficits in alertness and skill that became progressive and did not improve until the restoration of normal sleep. Many of the more radical cycles (such as the \"Uberman Schedule\", allowing only 15 minute naps every 4 hours) are so demanding that to date their only successes have been anecdotal reports from serious devotees and are widely considered to be harmful and physically unsustainable for the average person.\n\n\n"}
{"id": "57583705", "url": "https://en.wikipedia.org/wiki?curid=57583705", "title": "Narcotics Control Act 1990", "text": "Narcotics Control Act 1990\n\nThe Narcotics Control Act 1990 is a Bangladeshi law that regulates narcotics including production, distribution, transaction, possession, transportation, cultivation, etc. The act was passed in 1990.\n\nThe Narcotics act was passed in 1990 by the Parliament of Bangladesh. It repealed the previous laws, the Opium Act of 1878, the Excise Act of 1909, the Dangerous Drugs Act of 1930, the Opium Smoking Act of 1932 and the Prohibition Rules of 1950. The government added amendments to the act in 2000, 2002, and 2004. The act came into force on 2 January 1990. The act made the Department of Narcotics Control the central agency of the government drug police. The act made the National Narcotics Control Board the highest government body responsible for Narcotics control policy in Bangladesh. Department of Narcotics Control was established on 2 January 1990. In 2018 the government created a draft Narcotics Control Act 2018 to modernize the Narcotics Control Act 1990.\n"}
{"id": "17582359", "url": "https://en.wikipedia.org/wiki?curid=17582359", "title": "Nelson Kiang", "text": "Nelson Kiang\n\nNelson Yuan-Sheng Kiang is founder and former director of the Eaton-Peabody Laboratory of Auditory Physiology at the Massachusetts Eye and Ear Infirmary and professor emeritus of Otology and Laryngology at the Harvard Medical School and also professor emeritus at the Massachusetts Institute of Technology. He is also emeritus in Neurology at the Massachusetts General Hospital and a trustee of the Massachusetts Eye and Ear Infirmary. \n\nKiang received his Ph.D. from the University of Chicago in 1955, with William D. Neff as his advisor, and he has many notables in the auditory field among his ex-students. He received an honorary M.D. from the University of Geneva. In 1992 he founded the Speech and Hearing Sciences Program (now Speech and Hearing Bioscience and Technology Ph.D. program) which he directed until he retired. His former student M. Charles Liberman replaced him as director of the Eaton-Peabody Laboratory. \n\nKiang is an honorary or advisory professor at four Chinese universities and advises people in five ministries in China. His present interests are in world health and world education.\n\nKiang recounts the early history of the EPL in a memoir that is part of the EPL history project.\n\nKiang has been credited as the first to demonstrate the auditory brainstem response, and to propose using such electrical signals from the brain to diagnose hearing disorders.\n\nIn the early 1960s, as a pioneering researcher on sound coding in the auditory nerve, Kiang proposed the possibility of translating sound into multi-channel electrical signals as a way to give a sense of hearing to people with cochlear deafness – that is, cochlear implants.\nIn the early 1970s, after cochlear implantation experiments on humans had started with single-channel devices, Kiang strongly supported those urging caution with such human experimentation, arguing that our knowledge of coding in the auditory system was not yet sufficient and that single-channel devices were not up to the task. Both single- and multi-channel experimentation continued, and human experimentation along with continued auditory research led to advances (and continue to do so into the 21st century) that support success with modern multi-channel cochlear implants.\n\nIn the 1990s, Kiang suggested investigating the response of the vestibular part of the inner ear to very low frequencies in relation to wind turbine syndrome.\n\n"}
{"id": "18796003", "url": "https://en.wikipedia.org/wiki?curid=18796003", "title": "Offices, Shops and Railway Premises Act 1963", "text": "Offices, Shops and Railway Premises Act 1963\n\nThe Offices, Shops and Railway Premises Act 1963 is an Act of the Parliament of the United Kingdom. At the time of its passage, the Act was intended to extend the protection of workplace health, safety and welfare under the Factories Act 1961 to other employees in Great Britain. Though some of it remains in force, it has largely been superseded by the Health and Safety at Work etc. Act 1974 and regulations made under it.\n\nBreach of the residual provisions is still a crime punishable on summary conviction in the Magistrates' Court by a fine of up to £400 or, on indictment in the Crown Court, imprisonment for up to two years and an unlimited fine.\n\nIn the event of damage arising from a breach of the Act, there may be civil liability for breach of statutory duty. Though no such liability is stipulated by the Act itself, none is excluded and the facts could be such as to give rise to a cause of action in that tort. A breach not actionable in itself may be evidential towards a claim for common law negligence. In particular, a criminal conviction may be given in evidence.\n\nThe Act stemmed from the 1949 Gowers Report which had already led to the Mines and Quarries Act 1954, Agriculture (Safety, Health and Welfare Provisions) Act 1956 and Factories Act 1961. The 1963 Act extended protection to a further 8 million employees.\n\nSections 4 to 16 defined general broad requirements for safe and healthy workplace working conditions:\nThese provisions were repealed and superseded, as far as they applied to \"workplaces\", by the Workplace (Health, Safety and Welfare) Regulations 1992 with effect from 1 January 1993 for new workplaces and 1 January 1996 for established workplaces. There is still a potential residual scope of application to \"offices, shops and railway premises\" that are not \"workplaces\" as the definition of \"workplace\" is in some ways limited.\n\nSection 17 made requirements for the safeguarding of machinery but was repealed and superseded by the Provision and Use of Work Equipment Regulations 1992 between 1 January 1993 and 1 January 1997.\n\nSection 18 prohibited persons under 18 from cleaning certain hazardous machinery but was repealed and superseded by the Health and Safety (Young Persons) Regulations 1997 on 3 March 1997.\n\nSections 20 to 22 gave the Secretary of State the power to make regulations under the Act but these powers were repealed with the coming into force of the 1974 Act.\n\nSection 23 restricted manual lifting of weights that might cause injury but these requirements were repealed and superseded by the Manual Handling Operations Regulations 1992 on 1 January 1993.\n\n----\n"}
{"id": "53649451", "url": "https://en.wikipedia.org/wiki?curid=53649451", "title": "PAS 43", "text": "PAS 43\n\nPAS 43 is a British Standard for safe working in recovery of broken-down vehicles.\n\n, the latest standard is PAS 43:2015.\n"}
{"id": "55912670", "url": "https://en.wikipedia.org/wiki?curid=55912670", "title": "Patricia Bauer", "text": "Patricia Bauer\n\nPatricia J. Bauer (born 1957) is Asa Griggs Candler Professor of Psychology at Emory University. She is known for her research in the field of cognitive development, with a specific focus on how children develop their earliest memories and how their memory is influenced by parents, peers, and the environment around them. Her research has explored the phenomenon of childhood amnesia and how social, cognitive, and neural changes relate to the development of autobiographical memory.\n\nBauer was awarded the American Psychological Association Distinguished Scientific Award for an Early Career Contribution to Psychology in the Developmental Area (1993) and the American Psychological Foundation Robert L. Fantz Award (1993). Her book \"Remembering the Times of Our Lives: Memory in Infancy and Beyond\" was named Book of the Year (2007) by the Cognitive Development Society\".\" With Simona Ghetti, Bauer edited the volume \"Origins and Development of Recollection: Perspectives from Psychology and Neuroscience\" (2012). With Robyn Fivush, she edited \"The\" \"Wiley Handbook on the Development of Children's Memory\" (2014). Bauer has served as editor of the \"Journal of\" \"Cognition and Development\" (2005–2009) and the \"Society for Research in Child Development Monographs\" (2013–2018).\n\nBauer received her Bachelor of Sciences degree in Psychology at Indiana Central University in 1981. She went to graduate school at Miami University where she obtained her M.A. and Ph. D. in Experimental Developmental Psychology. In graduate school, Bauer was mentored by Cecilia Shore, who introduced her to Piaget's genetic epistemology. Cecilia's guidance educated her in applying constructivist theory to a variety of issues regarding language and cognition.\n\nUpon graduation, Bauer completed a post-doctoral fellowship at the University of California, San Diego, where she conducted studies of categorization in infants and young children in collaboration with Jean Mandler.\n\nPrior to joining the faculty of Emory University in 2007, Bauer held faculty positions at the Institute of Child Development at the University of Minnesota (1989-2005) and at Duke University (2005-2007). While at the University of Minnesota, Bauer was awarded the Horace T. Morse University of Minnesota Alumni Association Award for Outstanding Contributions to Undergraduate Education (2002). Here, she put together a small team of intelligent and ambitious graduate and undergraduate students, motivated to chase further research on the origins of long-term memory and its theoretical shift from infancy to early childhood.\n\nBauer is widely recognized for her research on the early development of memory, with a specific focus on autobiographical memory of people, objects, and events of personal significance. She utilizes various methods to carry out her research, including eye-tracking and electrophysiological measures. Bauer's work highlights a number of factors that contribute to the formation of memories, including the environment in which an event occurred and its emotional intensity, and the extent to which the information to relates to other pieces of information already stored in memory.\n\nIn one of her papers co-authored with Mandler, Bauer tested the ability of toddlers to remember events and whether their abilities were comparable to those of preschool-age children as well as adults. This study demonstrated the importance of causal connections and temporal structure in supporting children's earliest event memories. Toddlers, like older children and adults, are more likely to remember a series of events if causal relationships were present that linked the elements in the series. Other notable work on the development of memory is Bauer and colleagues' SRCD monograph on \"Parameters of Remembering and Forgetting in the Transition from Infancy to Early Childhood.\"\n\n\n"}
{"id": "48340", "url": "https://en.wikipedia.org/wiki?curid=48340", "title": "Pesticide", "text": "Pesticide\n\nPesticides are substances that are meant to control pests, including weeds. The term pesticide includes all of the following: herbicide, insecticides (which may include insect growth regulators, termiticides, etc.) nematicide, molluscicide, piscicide, avicide, rodenticide, bactericide, insect repellent, animal repellent, antimicrobial, fungicide, disinfectant (antimicrobial), and sanitizer. The most common of these are herbicides which account for approximately 80% of all pesticide use. Most pesticides are intended to serve as plant protection products (also known as crop protection products), which in general, protect plants from weeds, fungi, or insects.\n\nIn general, a pesticide is a chemical or biological agent (such as a virus, bacterium, or fungus) that deters, incapacitates, kills, or otherwise discourages pests. Target pests can include insects, plant pathogens, weeds, molluscs, birds, mammals, fish, nematodes (roundworms), and microbes that destroy property, cause nuisance, or spread disease, or are disease vectors. Although pesticides have benefits, some also have drawbacks, such as potential toxicity to humans and other species.\n\nThe Food and Agriculture Organization (FAO) has defined \"pesticide\" as:\n\nPesticides can be classified by target organism (e.g., herbicides, insecticides, fungicides, rodenticides, and pediculicides – see table), chemical structure (e.g., organic, inorganic, synthetic, or biological (biopesticide), although the distinction can sometimes blur), and physical state (e.g. gaseous (fumigant)). Biopesticides include microbial pesticides and biochemical pesticides. Plant-derived pesticides, or \"botanicals\", have been developing quickly. These include the pyrethroids, rotenoids, nicotinoids, and a fourth group that includes strychnine and scilliroside.\n\nMany pesticides can be grouped into chemical families. Prominent insecticide families include organochlorines, organophosphates, and carbamates. Organochlorine hydrocarbons (e.g., DDT) could be separated into dichlorodiphenylethanes, cyclodiene compounds, and other related compounds. They operate by disrupting the sodium/potassium balance of the nerve fiber, forcing the nerve to transmit continuously. Their toxicities vary greatly, but they have been phased out because of their persistence and potential to bioaccumulate. Organophosphate and carbamates largely replaced organochlorines. Both operate through inhibiting the enzyme acetylcholinesterase, allowing acetylcholine to transfer nerve impulses indefinitely and causing a variety of symptoms such as weakness or paralysis. Organophosphates are quite toxic to vertebrates and have in some cases been replaced by less toxic carbamates. Thiocarbamate and dithiocarbamates are subclasses of carbamates. Prominent families of herbicides include phenoxy and benzoic acid herbicides (e.g. 2,4-D), triazines (e.g., atrazine), ureas (e.g., diuron), and Chloroacetanilides (e.g., alachlor). Phenoxy compounds tend to selectively kill broad-leaf weeds rather than grasses. The phenoxy and benzoic acid herbicides function similar to plant growth hormones, and grow cells without normal cell division, crushing the plant's nutrient transport system. Triazines interfere with photosynthesis. Many commonly used pesticides are not included in these families, including glyphosate.\n\nThe application of pest control agents is usually carried out by dispersing the chemical in a (often hydrocarbon-based) solvent-surfactant system to give a homogeneous preparation. A virus lethality study performed in 1977 demonstrated that a particular pesticide did not increase the lethality of the virus, however combinations which included some surfactants and the solvent clearly showed that pretreatment with them markedly increased the viral lethality in the test mice.\n\nPesticides can be classified based upon their biological mechanism function or application method. Most pesticides work by poisoning pests. A systemic pesticide moves inside a plant following absorption by the plant. With insecticides and most fungicides, this movement is usually upward (through the xylem) and outward. Increased efficiency may be a result. Systemic insecticides, which poison pollen and nectar in the flowers, may kill bees and other needed pollinators.\n\nIn 2010, the development of a new class of fungicides called paldoxins was announced. These work by taking advantage of natural defense chemicals released by plants called phytoalexins, which fungi then detoxify using enzymes. The paldoxins inhibit the fungi's detoxification enzymes. They are believed to be safer and greener.\n\nSince before 2000 BC, humans have utilized pesticides to protect their crops. The first known pesticide was elemental sulfur dusting used in ancient Sumer about 4,500 years ago in ancient Mesopotamia. The Rig Veda, which is about 4,000 years old, mentions the use of poisonous plants for pest control. By the 15th century, toxic chemicals such as arsenic, mercury, and lead were being applied to crops to kill pests. In the 17th century, nicotine sulfate was extracted from tobacco leaves for use as an insecticide. The 19th century saw the introduction of two more natural pesticides, pyrethrum, which is derived from chrysanthemums, and rotenone, which is derived from the roots of tropical vegetables. Until the 1950s, arsenic-based pesticides were dominant. Paul Müller discovered that DDT was a very effective insecticide. Organochlorines such as DDT were dominant, but they were replaced in the U.S. by organophosphates and carbamates by 1975. Since then, pyrethrin compounds have become the dominant insecticide. Herbicides became common in the 1960s, led by \"triazine and other nitrogen-based compounds, carboxylic acids such as 2,4-dichlorophenoxyacetic acid, and glyphosate\".\n\nThe first legislation providing federal authority for regulating pesticides was enacted in 1910; however, decades later during the 1940s manufacturers began to produce large amounts of synthetic pesticides and their use became widespread. Some sources consider the 1940s and 1950s to have been the start of the \"pesticide era.\" Although the U.S. Environmental Protection Agency was established in 1970 and amendments to the pesticide law in 1972, pesticide use has increased 50-fold since 1950 and 2.3 million tonnes (2.5 million short tons) of industrial pesticides are now used each year. Seventy-five percent of all pesticides in the world are used in developed countries, but use in developing countries is increasing. A study of USA pesticide use trends through 1997 was published in 2003 by the National Science Foundation's Center for Integrated Pest Management.\n\nIn the 1960s, it was discovered that DDT was preventing many fish-eating birds from reproducing, which was a serious threat to biodiversity. Rachel Carson wrote the best-selling book \"Silent Spring\" about biological magnification. The agricultural use of DDT is now banned under the Stockholm Convention on Persistent Organic Pollutants, but it is still used in some developing nations to prevent malaria and other tropical diseases by spraying on interior walls to kill or repel mosquitoes.\n\nPesticides are used to control organisms that are considered to be harmful. For example, they are used to kill mosquitoes that can transmit potentially deadly diseases like West Nile virus, yellow fever, and malaria. They can also kill bees, wasps or ants that can cause allergic reactions. Insecticides can protect animals from illnesses that can be caused by parasites such as fleas. Pesticides can prevent sickness in humans that could be caused by moldy food or diseased produce. Herbicides can be used to clear roadside weeds, trees, and brush. They can also kill invasive weeds that may cause environmental damage. Herbicides are commonly applied in ponds and lakes to control algae and plants such as water grasses that can interfere with activities like swimming and fishing and cause the water to look or smell unpleasant. Uncontrolled pests such as termites and mold can damage structures such as houses. Pesticides are used in grocery stores and food storage facilities to manage rodents and insects that infest food such as grain. Each use of a pesticide carries some associated risk. Proper pesticide use decreases these associated risks to a level deemed acceptable by pesticide regulatory agencies such as the United States Environmental Protection Agency (EPA) and the Pest Management Regulatory Agency (PMRA) of Canada.\n\nDDT, sprayed on the walls of houses, is an organochlorine that has been used to fight malaria since the 1950s. Recent policy statements by the World Health Organization have given stronger support to this approach. However, DDT and other organochlorine pesticides have been banned in most countries worldwide because of their persistence in the environment and human toxicity. DDT use is not always effective, as resistance to DDT was identified in Africa as early as 1955, and by 1972 nineteen species of mosquito worldwide were resistant to DDT.\n\nIn 2006 and 2007, the world used approximately of pesticides, with herbicides constituting the biggest part of the world pesticide use at 40%, followed by insecticides (17%) and fungicides (10%). In 2006 and 2007 the U.S. used approximately of pesticides, accounting for 22% of the world total, including of conventional pesticides, which are used in the agricultural sector (80% of conventional pesticide use) as well as the industrial, commercial, governmental and home & garden sectors. The state of California alone used 117 million pounds. Pesticides are also found in majority of U.S. households with 88 million out of the 121.1 million households indicating that they use some form of pesticide in 2012. As of 2007, there were more than 1,055 active ingredients registered as pesticides, which yield over 20,000 pesticide products that are marketed in the United States.\n\nThe US used some 1 kg (2.2 pounds) per hectare of arable land compared with: 4.7 kg in China, 1.3 kg in the UK, 0.1 kg in Cameroon, 5.9 kg in Japan and 2.5 kg in Italy. Insecticide use in the US has declined by more than half since 1980 (.6%/yr), mostly due to the near phase-out of organophosphates. In corn fields, the decline was even steeper, due to the switchover to transgenic Bt corn.\n\nFor the global market of crop protection products, market analysts forecast revenues of over 52 billion US$ in 2019.\n\nPesticides can save farmers' money by preventing crop losses to insects and other pests; in the U.S., farmers get an estimated fourfold return on money they spend on pesticides. One study found that not using pesticides reduced crop yields by about 10%. Another study, conducted in 1999, found that a ban on pesticides in the United States may result in a rise of food prices, loss of jobs, and an increase in world hunger.\n\nThere are two levels of benefits for pesticide use, primary and secondary. Primary benefits are direct gains from the use of pesticides and secondary benefits are effects that are more long-term.\n\nControlling pests and plant disease vectors\n\nControlling human/livestock disease vectors and nuisance organisms\n\nControlling organisms that harm other human activities and structures\n\nIn one study, it was estimated that for every dollar ($1) that is spent on pesticides for crops can yield up to four dollars ($4) in crops saved. This means based that, on the amount of money spent per year on pesticides, $10 billion, there is an additional $40 billion savings in crop that would be lost due to damage by insects and weeds. In general, farmers benefit from having an increase in crop yield and from being able to grow a variety of crops throughout the year. Consumers of agricultural products also benefit from being able to afford the vast quantities of produce available year-round.\n\nOn the cost side of pesticide use there can be costs to the environment, costs to human health, as well as costs of the development and research of new pesticides.\n\nPesticides may cause acute and delayed health effects in people who are exposed. Pesticide exposure can cause a variety of adverse health effects, ranging from simple irritation of the skin and eyes to more severe effects such as affecting the nervous system, mimicking hormones causing reproductive problems, and also causing cancer. A 2007 systematic review found that \"most studies on non-Hodgkin lymphoma and leukemia showed positive associations with pesticide exposure\" and thus concluded that cosmetic use of pesticides should be decreased. There is substantial evidence of associations between organophosphate insecticide exposures and neurobehavioral alterations. Limited evidence also exists for other negative outcomes from pesticide exposure including neurological, birth defects, and fetal death.\n\nThe American Academy of Pediatrics recommends limiting exposure of children to pesticides and using safer alternatives:\n\nOwing to inadequate regulation and safety precautions, 99% of pesticide related deaths occur in developing countries that account for only 25% of pesticide usage.\n\nOne study found pesticide self-poisoning the method of choice in one third of suicides worldwide, and recommended, among other things, more restrictions on the types of pesticides that are most harmful to humans.\n\nA 2014 epidemiological review found associations between autism and exposure to certain pesticides, but noted that the available evidence was insufficient to conclude that the relationship was causal.\n\nLarge quantities of presumably nontoxic petroleum oil by-products are introduced into the environment as pesticide dispersal agents and emulsifiers. A 1976 study found that an increase in viral lethality with a concomitant influence on the liver and central nervous system occurs in young mice previously primed with such chemicals.\n\nThe World Health Organization and the UN Environment Programme estimate that each year, 3 million workers in agriculture in the developing world experience severe poisoning from pesticides, about 18,000 of whom die. According to one study, as many as 25 million workers in developing countries may suffer mild pesticide poisoning yearly. There are several careers aside from agriculture that may also put individuals at risk of health effects from pesticide exposure including pet groomers, groundskeepers, and fumigators.\n\nPesticide use is widespread in Latin America, as around US $3 billion are spend each year in the region. It has been recorded that pesticide poisonings have been increasing each year for the past two decades. It was estimated that 50–80% of the cases are unreported. It is indicated by studies that organophosphate and carbamate insecticides are the most frequent source of pesticide poisoning.\n\nPesticide use raises a number of environmental concerns. Over 98% of sprayed insecticides and 95% of herbicides reach a destination other than their target species, including non-target species, air, water and soil. Pesticide drift occurs when pesticides suspended in the air as particles are carried by wind to other areas, potentially contaminating them. Pesticides are one of the causes of water pollution, and some pesticides are persistent organic pollutants and contribute to soil contamination.\n\nIn addition, pesticide use reduces biodiversity, contributes to pollinator decline, destroys habitat (especially for birds), and threatens endangered species.\nPests can develop a resistance to the pesticide (pesticide resistance), necessitating a new pesticide. Alternatively a greater dose of the pesticide can be used to counteract the resistance, although this will cause a worsening of the ambient pollution problem.\n\nThe Stockholm Convention on Persistent Organic Pollutants, listed 9 of the 12 most dangerous and persistent organic chemicals that were (now mostly obsolete) organochlorine pesticides. Since chlorinated hydrocarbon pesticides dissolve in fats and are not excreted, organisms tend to retain them almost indefinitely. Biological magnification is the process whereby these chlorinated hydrocarbons (pesticides) are more concentrated at each level of the food chain. Among marine animals, pesticide concentrations are higher in carnivorous fishes, and even more so in the fish-eating birds and mammals at the top of the ecological pyramid. Global distillation is the process whereby pesticides are transported from warmer to colder regions of the Earth, in particular the Poles and mountain tops. Pesticides that evaporate into the atmosphere at relatively high temperature can be carried considerable distances (thousands of kilometers) by the wind to an area of lower temperature, where they condense and are carried back to the ground in rain or snow.\n\nIn order to reduce negative impacts, it is desirable that pesticides be degradable or at least quickly deactivated in the environment. Such loss of activity or toxicity of pesticides is due to both innate chemical properties of the compounds and environmental processes or conditions. For example, the presence of halogens within a chemical structure often slows down degradation in an aerobic environment. Adsorption to soil may retard pesticide movement, but also may reduce bioavailability to microbial degraders.\n\nIn one study, the human health and environmental costs due to pesticides in the United States was estimated to be $9.6 billion: offset by about $40 billion in increased agricultural production.\n\nAdditional costs include the registration process and the cost of purchasing pesticides: which are typically borne by agrichemical companies and farmers respectively. The registration process can take several years to complete (there are 70 different types of field test) and can cost $50–70 million for a single pesticide. At the beginning of the 21st century, the United States spent approximately $10 billion on pesticides annually.\n\nAlternatives to pesticides are available and include methods of cultivation, use of biological pest controls (such as pheromones and microbial pesticides), genetic engineering, and methods of interfering with insect breeding. Application of composted yard waste has also been used as a way of controlling pests. These methods are becoming increasingly popular and often are safer than traditional chemical pesticides. In addition, EPA is registering reduced-risk conventional pesticides in increasing numbers.\n\nCultivation practices include polyculture (growing multiple types of plants), crop rotation, planting crops in areas where the pests that damage them do not live, timing planting according to when pests will be least problematic, and use of trap crops that attract pests away from the real crop. Trap crops have successfully controlled pests in some commercial agricultural systems while reducing pesticide usage; however, in many other systems, trap crops can fail to reduce pest densities at a commercial scale, even when the trap crop works in controlled experiments. In the U.S., farmers have had success controlling insects by spraying with hot water at a cost that is about the same as pesticide spraying.\n\nRelease of other organisms that fight the pest is another example of an alternative to pesticide use. These organisms can include natural predators or parasites of the pests. Biological pesticides based on entomopathogenic fungi, bacteria and viruses cause disease in the pest species can also be used.\n\nInterfering with insects' reproduction can be accomplished by sterilizing males of the target species and releasing them, so that they mate with females but do not produce offspring. This technique was first used on the screwworm fly in 1958 and has since been used with the medfly, the tsetse fly, and the gypsy moth. However, this can be a costly, time consuming approach that only works on some types of insects.\n\nThe term \"push-pull\" was established in 1987 as an approach for integrated pest management (IPM). This strategy uses a mixture of behavior-modifying stimuli to manipulate the distribution and abundance of insects. \"Push\" means the insects are repelled or deterred away from whatever resource that is being protected. \"Pull\" means that certain stimuli (semiochemical stimuli, pheromones, food additives, visual stimuli, genetically altered plants, etc.) are used to attract pests to trap crops where they will be killed. There are numerous different components involved in order to implement a Push-Pull Strategy in IPM.\n\nMany case studies testing the effectiveness of the push-pull approach have been done across the world. The most successful push-pull strategy was developed in Africa for subsistence farming. Another successful case study was performed on the control of \"Helicoverpa\" in cotton crops in Australia. In Europe, the Middle East, and the United States, push-pull strategies were successfully used in the controlling of \"Sitona lineatus\" in bean fields.\n\nSome advantages of using the push-pull method are less use of chemical or biological materials and better protection against insect habituation to this control method. Some disadvantages of the push-pull strategy is that if there is a lack of appropriate knowledge of behavioral and chemical ecology of the host-pest interactions then this method becomes unreliable. Furthermore, because the push-pull method is not a very popular method of IPM operational and registration costs are higher.\n\nSome evidence shows that alternatives to pesticides can be equally effective as the use of chemicals. For example, Sweden has halved its use of pesticides with hardly any reduction in crops. In Indonesia, farmers have reduced pesticide use on rice fields by 65% and experienced a 15% crop increase. A study of Maize fields in northern Florida found that the application of composted yard waste with high carbon to nitrogen ratio to agricultural fields was highly effective at reducing the population of plant-parasitic nematodes and increasing crop yield, with yield increases ranging from 10% to 212%; the observed effects were long-term, often not appearing until the third season of the study.\n\nHowever, pesticide resistance is increasing. In the 1940s, U.S. farmers lost only 7% of their crops to pests. Since the 1980s, loss has increased to 13%, even though more pesticides are being used. Between 500 and 1,000 insect and weed species have developed pesticide resistance since 1945.\n\nPesticides are often referred to according to the type of pest they control. Pesticides can also be considered as either biodegradable pesticides, which will be broken down by microbes and other living beings into harmless compounds, or persistent pesticides, which may take months or years before they are broken down: it was the persistence of DDT, for example, which led to its accumulation in the food chain and its killing of birds of prey at the top of the food chain. Another way to think about pesticides is to consider those that are chemical pesticides are derived from a common source or production method.\n\nNeonicotinoids are a class of neuro-active insecticides chemically similar to nicotine. Imidacloprid, of the neonicotanoid family, is the most widely used insecticide in the world. In the late 1990s neonicotinoids came under increasing scrutiny over their environmental impact and were linked in a range of studies to adverse ecological effects, including honey-bee colony collapse disorder (CCD) and loss of birds due to a reduction in insect populations. In 2013, the European Union and a few non EU countries restricted the use of certain neonicotinoids.\n\nOrganophosphate and carbamate insecticides have a similar mode of action. They affect the nervous system of target pests (and non-target organisms) by disrupting acetylcholinesterase activity, the enzyme that regulates acetylcholine, at nerve synapses. This inhibition causes an increase in synaptic acetylcholine and over-stimulation of the parasympathetic nervous system. Many of these insecticides, first developed in the mid 20th century, are very poisonous. Although commonly used in the past, many older chemicals have been removed from the market due to their health and environmental effects (\"e.g.\" DDT, chlordane, and toxaphene). However, many organophosphates are not persistent in the environment.\n\nPyrethroid insecticides were developed as a synthetic version of the naturally occurring pesticide pyrethrin, which is found in chrysanthemums. They have been modified to increase their stability in the environment. Some synthetic pyrethroids are toxic to the nervous system.\n\nA number of sulfonylureas have been commercialized for weed control, including: amidosulfuron, flazasulfuron, metsulfuron-methyl, rimsulfuron, sulfometuron-methyl, terbacil, nicosulfuron, and triflusulfuron-methyl. These are broad-spectrum herbicides that kill plants weeds or pests by inhibiting the enzyme acetolactate synthase. In the 1960s, more than crop protection chemical was typically applied, while sulfonylureates allow as little as 1% as much material to achieve the same effect.\n\nBiopesticides are certain types of pesticides derived from such natural materials as animals, plants, bacteria, and certain minerals. For example, canola oil and baking soda have pesticidal applications and are considered biopesticides. Biopesticides fall into three major classes:\n\n\nPesticides that are related to the type of pests are:\nThe term pesticide also include these substances:\n\nDefoliants: Cause leaves or other foliage to drop from a plant, usually to facilitate harvest.\nDesiccants: Promote drying of living tissues, such as unwanted plant tops.\nInsect growth regulators: Disrupt the molting, maturity from pupal stage to adult, or other life processes of insects. \nPlant growth regulators: Substances (excluding fertilizers or other plant nutrients) that alter the expected growth, flowering, or reproduction rate of plants.\nWood preservatives: They are used to make wood resistant to insects, fungus, and other pests.\n\nIn most countries, pesticides must be approved for sale and use by a government agency.\n\nIn Europe, recent EU legislation has been approved banning the use of highly toxic pesticides including those that are carcinogenic, mutagenic or toxic to reproduction, those that are endocrine-disrupting, and those that are persistent, bioaccumulative and toxic (PBT) or very persistent and very bioaccumulative (vPvB). Measures were approved to improve the general safety of pesticides across all EU member states.\n\nThough pesticide regulations differ from country to country, pesticides, and products on which they were used are traded across international borders. To deal with inconsistencies in regulations among countries, delegates to a conference of the United Nations Food and Agriculture Organization adopted an International Code of Conduct on the Distribution and Use of Pesticides in 1985 to create voluntary standards of pesticide regulation for different countries. The Code was updated in 1998 and 2002. The FAO claims that the code has raised awareness about pesticide hazards and decreased the number of countries without restrictions on pesticide use.\n\nThree other efforts to improve regulation of international pesticide trade are the United Nations London Guidelines for the Exchange of Information on Chemicals in International Trade and the United Nations Codex Alimentarius Commission. The former seeks to implement procedures for ensuring that prior informed consent exists between countries buying and selling pesticides, while the latter seeks to create uniform standards for maximum levels of pesticide residues among participating countries.\n\nPesticides safety education and pesticide applicator regulation are designed to protect the public from pesticide misuse, but do not eliminate all misuse. Reducing the use of pesticides and choosing less toxic pesticides may reduce risks placed on society and the environment from pesticide use. Integrated pest management, the use of multiple approaches to control pests, is becoming widespread and has been used with success in countries such as Indonesia, China, Bangladesh, the U.S., Australia, and Mexico. IPM attempts to recognize the more widespread impacts of an action on an ecosystem, so that natural balances are not upset. New pesticides are being developed, including biological and botanical derivatives and alternatives that are thought to reduce health and environmental risks. In addition, applicators are being encouraged to consider alternative controls and adopt methods that reduce the use of chemical pesticides.\n\nPesticides can be created that are targeted to a specific pest's lifecycle, which can be environmentally more friendly. For example, potato cyst nematodes emerge from their protective cysts in response to a chemical excreted by potatoes; they feed on the potatoes and damage the crop. A similar chemical can be applied to fields early, before the potatoes are planted, causing the nematodes to emerge early and starve in the absence of potatoes.\n\nIn the United States, the Environmental Protection Agency (EPA) is responsible for regulating pesticides under the Federal Insecticide, Fungicide, and Rodenticide Act (FIFRA) and the Food Quality Protection Act (FQPA).\n\nStudies must be conducted to establish the conditions in which the material is safe to use and the effectiveness against the intended pest(s). The EPA regulates pesticides to ensure that these products do not pose adverse effects to humans or the environment. Pesticides produced before November 1984 continue to be reassessed in order to meet the current scientific and regulatory standards. All registered pesticides are reviewed every 15 years to ensure they meet the proper standards. During the registration process, a label is created. The label contains directions for proper use of the material in addition to safety restrictions. Based on acute toxicity, pesticides are assigned to a Toxicity Class.\n\nSome pesticides are considered too hazardous for sale to the general public and are designated restricted use pesticides. Only certified applicators, who have passed an exam, may purchase or supervise the application of restricted use pesticides. Records of sales and use are required to be maintained and may be audited by government agencies charged with the enforcement of pesticide regulations. These records must be made available to employees and state or territorial environmental regulatory agencies.\n\nThe EPA regulates pesticides under two main acts, both of which amended by the Food Quality Protection Act of 1996. In addition to the EPA, the United States Department of Agriculture (USDA) and the United States Food and Drug Administration (FDA) set standards for the level of pesticide residue that is allowed on or in crops. The EPA looks at what the potential human health and environmental effects might be associated with the use of the pesticide.\n\nIn addition, the U.S. EPA uses the National Research Council's four-step process for human health risk assessment: (1) Hazard Identification, (2) Dose-Response Assessment, (3) Exposure Assessment, and (4) Risk Characterization.\n\nRecently Kaua'i County (Hawai'i) passed Bill No. 2491 to add an article to Chapter 22 of the county's code relating to pesticides and GMOs. The bill strengthens protections of local communities in Kaua'i where many large pesticide companies test their products.\n\nPesticide residue refers to the pesticides that may remain on or in food after they are applied to food crops. The maximum allowable levels of these residues in foods are often stipulated by regulatory bodies in many countries. Regulations such as pre-harvest intervals also often prevent harvest of crop or livestock products if recently treated in order to allow residue concentrations to decrease over time to safe levels before harvest. Exposure of the general population to these residues most commonly occurs through consumption of treated food sources, or being in close contact to areas treated with pesticides such as farms or lawns.\n\nMany of these chemical residues, especially derivatives of chlorinated pesticides, exhibit bioaccumulation which could build up to harmful levels in the body as well as in the environment. Persistent chemicals can be magnified through the food chain and have been detected in products ranging from meat, poultry, and fish, to vegetable oils, nuts, and various fruits and vegetables.\n\n\n"}
{"id": "54996", "url": "https://en.wikipedia.org/wiki?curid=54996", "title": "Prader–Willi syndrome", "text": "Prader–Willi syndrome\n\nPrader–Willi syndrome (PWS) is a genetic disorder due to loss of function of specific genes. In newborns symptoms include weak muscles, poor feeding, and slow development. Beginning in childhood the person becomes constantly hungry which often leads to obesity and type 2 diabetes. There is also typically mild to moderate intellectual impairment and behavioral problems. Often the forehead is narrow, hands and feet small, height short, skin light in color, and those affected are unable to have children.\nAbout 74% of cases occur when part of the father's chromosome 15 is deleted. In another 25% of cases the person has two copies of chromosome 15 from their mother and none from their father. As parts of the chromosome from the mother are turned off they end up with no working copies of certain genes. PWS is not generally inherited but instead the genetic changes happen during the formation of the egg, sperm, or in early development. There are no known risk factors. Those who have one child with PWS have less than a 1% chance of the next child being affected. A similar mechanism occurs in Angelman syndrome except there is a defective chromosome 15 from the mother or two copies from the father.\nPrader–Willi syndrome has no cure. Treatment, however, may improve outcomes, especially if carried out early. In newborns feeding difficulties may be supported with feeding tubes. Strict food supervision is typically required starting around the age of three in combination with an exercise program. Growth hormone therapy also improves outcomes. Counseling and medications may help with some behavioral problems. Group homes are often necessary in adulthood.\nPWS affects between 1 in 10,000 and 30,000 people. Males and females are affected equally. The condition is named after Andrea Prader, Heinrich Willi, and Alexis Labhart who described it in detail in 1956. An earlier description occurred in 1887 by John Langdon Down.\n\nThere are many signs and symptoms of Prader–Willi syndrome. The symptoms can range from poor muscle tone during infancy to behavioral problems in early childhood. Some symptoms that are usually found in infants, besides poor muscle tone, would be a lack of eye coordination; some are born with almond-shaped eyes; and due to poor muscle tone the infant may not have a strong sucking reflex. Their cry is weak, and they have difficulty waking up. Another sign of this condition is a thin upper lip.\n\nMore aspects seen in a clinical overview include hypotonia and abnormal neurologic function, hypogonadism, developmental and cognitive delays, hyperphagia and obesity, short stature, and behavioral and psychiatric disturbances.\n\nHolm \"et al.\" (1993) describe the following features and signs as pretest indicators of PWS, although not all will be present.\n\nIndividuals with PWS are at risk of learning and attention difficulties. Curfs and Fryns (1992) conducted research into the varying degrees of learning disability found in PWS. Their results, using a measure of IQ, were as follows:\n\nCassidy found that 40% of individuals with PWS have borderline/low average intelligence, a figure higher than the 32% found in Curfs and Fryns' study. However, both studies suggest that most individuals (50–65%) fall within the mild/borderline/low average intelligence range.\n\nChildren with PWS show an unusual cognitive profile. They are often strong in visual organization and perception, including reading and vocabulary, but their spoken language (sometimes affected by hypernasality) is generally poorer than their comprehension. A marked skill in completing jigsaw puzzles has been noted, but this may be an effect of increased practice.\n\nAuditory information processing and sequential processing are relatively poor, as are arithmetic and writing skills, visual and auditory short-term memory and auditory attention span. These sometimes improve with age, but deficits in these areas remain throughout adulthood.\n\nThere may be an association with psychosis.\n\nPrader–Willi syndrome is frequently associated with a constant, extreme, ravenous insatiable appetite which persists no matter how much the patient eats, often resulting in morbid obesity. Caregivers need to strictly limit the patients' access to food, usually by installing locks on refrigerators and on all closets and cabinets where food is stored. It is the most common genetic cause of morbid obesity in children. There is currently no consensus as to the cause for this symptom, although genetic abnormalities in chromosome 15 disrupt the normal functioning of the hypothalamus. Given that the hypothalamus arcuate nucleus regulates many basic processes, including appetite, there may well be a link. In the hypothalamus of people with PWS, nerve cells that produce oxytocin, a hormone thought to contribute to satiety, have been found to be abnormal.\n\nPeople with Prader–Willi syndrome have high ghrelin levels, which are thought to directly contribute to the increased appetite, hyperphagia, and obesity seen in this syndrome. Cassidy states the need for a clear delineation of behavioral expectations, the reinforcement of behavioural limits and the establishment of regular routines.\n\nThe main mental health difficulties experienced by people with PWS include compulsive behaviour (usually manifested in skin picking) and anxiety. Psychiatric symptoms, for example, hallucinations, paranoia and depression, have been described in some cases and affect approximately 5–10% of young adults. Patients are also often extremely stubborn and prone to anger. Psychiatric and behavioural problems are the most common cause of hospitalization.\n\nIt is typical for to 70–90% of affected individuals to develop behavioral patterns in early childhood. Aspects of these patterns can include stubbornness, temper tantrums, controlling and manipulative behavior, difficulty with change in routine, and compulsive-like behaviors.\n\nThere are several aspects of PWS that support the concept of growth hormone deficiency in individuals with PWS. Specifically, individuals with PWS have short stature, are obese with abnormal body composition, have reduced fat free mass (FFM), have reduced lean body mass (LBM) and total energy expenditure, and have decreased bone density.\n\nPWS is characterized by hypogonadism. This is manifested as undescended testes in males and benign premature adrenarche in females. Testes may descend with time or can be managed with surgery or testosterone replacement. Adrenarche may be treated with hormone replacement therapy.\n\nPWS is commonly associated with development of strabismus. In one study, over 50% of patients had strabismus, mainly esotropia.\n\nPWS is a disorder caused by an epigenetic phenomenon known as imprinting. PWS is caused by the deletion of the paternal copies of the SNRPN and necdin genes along with clusters of snoRNAs: SNORD64, SNORD107, SNORD108 and two copies of SNORD109, 29 copies of SNORD116 (HBII-85) and 48 copies of SNORD115 (HBII-52). These are on chromosome 15 located in the region 15q11-13. This so-called PWS/AS region may be lost by one of several genetic mechanisms which, in the majority of instances occurs through chance mutation. Other less common mechanisms include; uniparental disomy, sporadic mutations, chromosome translocations, and gene deletions. Due to imprinting, the maternally inherited copies of these genes are virtually silent, only the paternal copies of the genes are expressed. PWS results from the loss of paternal copies of this region. Deletion of the same region on the maternal chromosome causes Angelman syndrome (AS). PWS and AS represent the first reported instances of imprinting disorders in humans.\n\nThe risk to the sibling of an affected child of having PWS depends upon the genetic mechanism which caused the disorder. The risk to siblings is <1% if the affected child has a gene deletion or uniparental disomy, up to 50% if the affected child has a mutation of the imprinting control region, and up to 25% if a parental chromosomal translocation is present. Prenatal testing is possible for any of the known genetic mechanisms.\n\nA microdeletion in one family of the snoRNA HBII-52 has excluded it from playing a major role in the disease.\n\nStudies of human and mouse model systems have shown deletion of the 29 copies of the C/D box snoRNA SNORD116 (HBII-85) to be the primary cause of Prader–Willi syndrome.\n\nIt is traditionally characterized by hypotonia, short stature, hyperphagia, obesity, behavioral issues (specifically OCD-like behaviors), small hands and feet, hypogonadism, and mild intellectual disability. However, with early diagnosis and early treatment (such as with growth hormone therapy), the prognosis for persons with PWS is beginning to change. Like autism, PWS is a spectrum disorder and symptoms can range from mild to severe and may change throughout the person's lifetime. Various organ systems are affected.\n\nTraditionally, Prader–Willi syndrome was diagnosed by clinical presentation. Currently, the syndrome is diagnosed through genetic testing; testing is recommended for newborns with pronounced hypotonia. Early diagnosis of PWS allows for early intervention as well as the early prescription of growth hormone. Daily recombinant growth hormone (GH) injections are indicated for children with PWS. GH supports linear growth and increased muscle mass, and may lessen food preoccupation and weight gain.\n\nThe mainstay of diagnosis is genetic testing, specifically DNA-based methylation testing to detect the absence of the paternally contributed Prader–Willi syndrome/Angelman syndrome (PWS/AS) region on chromosome 15q11-q13. Such testing detects over 97% of cases. Methylation-specific testing is important to confirm the diagnosis of PWS in all individuals, but especially those who are too young to manifest sufficient features to make the diagnosis on clinical grounds or in those individuals who have atypical findings.\n\nPrader–Willi syndrome is often misdiagnosed as other syndromes due to many in the medical community's unfamiliarity with PWS. Sometimes it is misdiagnosed as Down syndrome, simply because of the relative frequency of Down syndrome compared to PWS.\n\nPrader–Willi syndrome has no cure; however, several treatments are in place to lessen the condition's symptoms. During infancy, subjects should undergo therapies to improve muscle strength. Speech and occupational therapy are also indicated. During the school years, children benefit from a highly structured learning environment as well as extra help. The largest problem associated with the syndrome is severe obesity. Access to food must be strictly supervised and limited, usually by installing locks on all food-storage places including refrigerators.\n\nBecause hypotonia can be a symptom of PWS, it is vital to provide proper nutrition during infancy. It is also very important to stress physical activity in individuals with PWS for all ages in order to optimize strength and promote a healthy lifestyle.\n\nPrescription of daily recombinant growth hormone injections are indicated for children with PWS. GH supports linear growth and increased muscle mass, and may lessen food preoccupation and weight gain.\n\nBecause of severe obesity, obstructive sleep apnea is a common sequela, and a positive airway pressure machine is often needed. There may come a time when a person who has been diagnosed with PWS may have to undergo surgical procedures. One surgery that has proven to be unsuccessful for treating the obesity is gastric bypass. Patients with Prader–Willi syndrome have a very high tolerance to pain; therefore they may be experiencing significant abdominal symptoms such as acute gastritis, appendicitis, or cholecystitis and not be aware of it until later.\n\nBehavior and psychiatric problems should be detected early for the best results. These issues are best when treated with parental education and training. Sometimes medication is introduced as well. Serotonin agonists have been most effective in lessening temper tantrums and improving compulsivity.\n\nPWS affects approximately 1 in 10,000 to 1 in 25,000 newborns. There are more than 400,000 people who live with PWS around the world.\n\nDespite its rarity, Prader–Willi syndrome has been often referenced in popular culture, partly due to curiosity surrounding the insatiable appetite and obesity that are symptoms.\n\nPrader–Willi syndrome has been depicted and documented several times in television. A fictional individual with Prader–Willi syndrome featured in the episode \"Dog Eat Dog\" of the television series \"\", which aired on November 24, 2005. In the UK media in July 2007, Channel 4 aired a 2006 documentary called \"Can't Stop Eating\", surrounding the everyday lives of two people with Prader–Willi syndrome, Joe and Tamara. In the May 9, 2010 episode of \",\" Sheryl Crow helped Ty Pennington rebuild a home for a family whose youngest son, Ethan Starkweather, was living with Prader–Willi syndrome. In the March 22, 2012 episode of \"Mystery Diagnosis\" on the Discovery Health channel, Conor Heybach, who has Prader–Willi syndrome, shared his story of how he was diagnosed with it.\n\n"}
{"id": "57335546", "url": "https://en.wikipedia.org/wiki?curid=57335546", "title": "Real world evidence", "text": "Real world evidence\n\nReal world evidence (RWE) in medicine means evidence obtained from real world data (RWD), which are observational data obtained outside the context of randomized controlled trials (RCTs) and generated during routine clinical practice. In order to assess patient outcomes and to ensure that patients get treatment that is right for them, real world data needs to be utilized. RWE is generated by analyzing data which is stored in electronic health records (EHR), medical claims or billing activities databases, registries, patient-generated data, mobile devices, etc. It may be derived from retrospective or prospective observational studies and observational registries. In the USA the 21st Century Cures Act required the FDA to expand the role of real world evidence.\nReal World Evidence comes into play when clinical trials cannot really account for the entire patient population of a particular disease. Patients suffering from comorbidities or belonging to a distant geographic region or age limit who did not participate in any clinical trial may not respond to the treatment in question as expected. RWE provides answers to these problems and also to analyze effects of drugs over a longer period of time. Pharmaceutical companies and Health Insurance Payers study RWE to understand patient pathways to deliver appropriate care for appropriate individuals and to minimize their own financial risk by investing on drugs that work for patients.\n\n\n\n"}
{"id": "19372045", "url": "https://en.wikipedia.org/wiki?curid=19372045", "title": "Regenerative thermal oxidizer", "text": "Regenerative thermal oxidizer\n\nA regenerative thermal oxidizer (RTO) is a piece of industrial equipment used for the treatment of exhaust air. The system is a type of thermal oxidizer that uses a bed of ceramic material to absorb heat from the exhaust gas. It then uses this captured heat to preheat the incoming process gas stream and destroy air pollutants emitted from process exhaust streams at temperatures ranging from 815 °C (1,500 F) to 980 °C (1,800 F).\n\nThese gas streams are usually produced by processes requiring ventilation, including paint booths, printing, and paper mills. Municipal waste treatment facilities such as mechanical biological treatment plants are required by law in Germany to incorporate these systems. Biological alternatives to this system include biofilters and bioscrubbers.\n\nThey are suited to applications with low VOC concentrations but high waste stream flow rates. This is due to their high thermal energy recovery. The basic operation of an RTO consists of passing a hot gas stream over a heat sink material in one direction and recovering that heat by passing a cold gas stream through that same heat sink material in an alternate cycle. They are used to destroy air toxins, odors, VOCs, and that are discharged in industrial process exhausts.\n\nThere are a number of RTO suppliers throughout North America, Europe and Asia. From a technical stand point the most advanced form of RTO technology is the rotary valve RTO. \n\n\n\nDifferent solutions to the waste gas problems of one of the mentioned industries are available: \n"}
{"id": "21134458", "url": "https://en.wikipedia.org/wiki?curid=21134458", "title": "Registered Health Information Technician", "text": "Registered Health Information Technician\n\nRegistered Health Information Technician (RHIT) is a professional certification administered by the American Health Information Management Association (AHIMA) in the United States. Passing the exam results in licensure as a health information technician. To be eligible, an individual must achieve an associate degree approved by the Commission on Accreditation for Health Informatics and Information Management Education (CAHIIM) or graduate from a foreign association that has a reciprocity agreement with AHIMA. The health information technician is responsible for organizing comprehensively and accurately the medical record of each patient. They ensure that all the necessary forms are present, properly identified, signed and filed. Health information technicians are able to translate complex data into understandable, interesting and simplified information for the general public. They take the medical records produced by the health care provider and translate them into a code that identifies each diagnoses and procedure utilized in treating the patient. RHIT's can further their career advancement by completing a CAHIIM-accredited Baccalaureate degree in health information management, and take a national credential exam in health information administration as a certified Registered Health Information Administrator (RHIA). \n"}
{"id": "5307067", "url": "https://en.wikipedia.org/wiki?curid=5307067", "title": "Resource Allocation Working Party", "text": "Resource Allocation Working Party\n\nThe Resource Allocation Working Party was a group set up within the National Health Service in 1975 to suggest a mechanism whereby resources for secondary care could be matched to need (Gatrell, 2002).\n\nBetween 1948 and 1968 NHS financial allocations were essentially based on sequential inflation uplifts. A Regional Health Authority or Teaching Hospital could argue for an increase. The richer parts of the country had better funding in 1948 than the more deprived areas and so the differences between the various regions widened over time. In 1976-1977 there was an almost 30% difference in the revenue allocation between the 14 regions, with the North West having the least and North-East Thames region the most per head of population.\n\nRichard Crossman developed a formula based on population, beds and cases but its fundamental problem was that the formula was partly based on utilisation and current resources. Since utilisation depends on availability of resources which were unequally distributed it could not rectify the problem. When Barbara Castle was Secretary of State for Health in 1972, the problem of regional resource inequality was addressed again. Her Special Adviser Professor Brian Abel-Smith had a particular interest in this problem (on which he had already advised Crossman, whose Special Adviser he had been earlier). He chaired the Advisory Committee to the Social Medicine and Health Services Research Unit at St Thomas' Hospital. He drew the attention of the Committee to the problems of resource allocation and encouraged them to consider possible research to rectify this unacceptable situation. They produced a proposal for a complicated randomised controlled trial of different funding formulae, but the Minister, David Owen, rejected it as interesting but politically impossible.\n\nOwen established the Resource Allocation Working Party (RAWP), to examine the possibilities of a better funding formula. It came to the conclusion that Standardised Mortality Ratios were a reasonable indicator of regional variations in health care needs in the acute sector. The Report of the Working Party also emphasised the need to develop and apply positive preventive measures such as promoting changes in smoking habits and improving the environments in which people live and work.\n\nThe Royal Commission on the National Health Service drew attention to the inequalities of funding. Expenditure on NHS services in Scotland was £127.10 per head of population, in the NW Thames region £122.38, in the West Midlands £91.52.\n\nThe four Metropolitan Thames Regional Health Authorities and most of the London Teaching Hospitals were disadvantaged by, and unhappy about, the new formula. The simplicity and transparency of the formula made it difficult for politicians to manipulate. The idea that mortality should be used to influence the distribution of health resources was questioned on the grounds that most health care is provided for people who do not die. The formula devised by the Resource Allocation Working Party survived until 1989 and did reduce the funding gap between the Northern regions and London. It was replaced by a more complex formula announced in the publication of Working for Patients in 1989,and there have since been further changes and debate, particularly about the relative weighting to be given to old age, which favours more prosperous Southern areas, and deprivation which favours poorer Northern areas.\n\nBarnett Formula\n\nGatrell, A.C. (2002) Geographies of Health: an Introduction, Oxford: Blackwell.\n"}
{"id": "26666508", "url": "https://en.wikipedia.org/wiki?curid=26666508", "title": "Rick Hodes", "text": "Rick Hodes\n\nRick Hodes is an American medical doctor specializing in cancer, heart disease, and spinal conditions. Since the 1980s he has worked in Ethiopia and has adopted a number of children from the country. Currently, he is the senior consultant at a Catholic mission working with sick destitutes suffering from heart disease (rheumatic and congenital), spine disease (TB and scoliosis), and cancer. He is medical director of the American Jewish Joint Distribution Committee.\n\nHodes has been responsible for the health of Ethiopians immigrating to Israel and has worked with refugees in Rwanda, Zaire, Tanzania, Somalia, and Albania.\n\nHodes graduated from Middlebury College, University of Rochester Medical School, and trained in internal medicine at Johns Hopkins University. He first went to Ethiopia as a relief worker during the 1984 famine. He returned there on a Fulbright Fellowship to teach internal medicine, and in 1990 was hired by the American Jewish Joint Distribution Committee, a humanitarian group, as the medical advisor for the country. His original position was to care for 25,000 potential immigrants to Israel. In 1991, he was an active contributor during Operation Solomon, helping the Ethiopian Jews airlifted to Israel. \n\nIn 2001 Hodes adopted two Ethiopian children, putting them on his insurance plan so they could receive treatment in the US for spinal tuberculosis (Pott's disease). Since then he has adopted a total of five children from the country.\n\nIn 2007, Hodes was selected as a \"CNN Hero,\" a program that highlights ordinary people for their extraordinary achievements. The American College of Physicians has awarded him “Mastership,” and the Rosenthal Award for creative practice of medicine\n\nHodes work in Ethiopia was the subject of a HBO documentary, \"Making the Crooked Straight\" and a Marilyn Berger book, \"This Is a Soul: The Mission of Rick Hodes\".\n\nRick Hodes is a baal teshuva, a secular Jew who has embraced Orthodox Judaism.\n\n"}
{"id": "158400", "url": "https://en.wikipedia.org/wiki?curid=158400", "title": "Sepsis", "text": "Sepsis\n\nSepsis is a life-threatening condition that arises when the body's response to infection causes injury to its own tissues and organs. Common signs and symptoms include fever, increased heart rate, increased breathing rate, and confusion. There may also be symptoms related to a specific infection, such as a cough with pneumonia, or painful urination with a kidney infection. In the very young, old, and people with a weakened immune system, there may be no symptoms of a specific infection and the body temperature may be low or normal, rather than high. Severe sepsis is sepsis causing poor organ function or insufficient blood flow. Insufficient blood flow may be evident by low blood pressure, high blood lactate, or low urine output. Septic shock is low blood pressure due to sepsis that does not improve after fluid replacement.\nSepsis is caused by an inflammatory immune response triggered by an infection. Most commonly, the infection is bacterial, but it may also be fungal, viral, or protozoan. Common locations for the primary infection include the lungs, brain, urinary tract, skin, and abdominal organs. Risk factors include very young age, older age, a weakened immune system from conditions such as cancer or diabetes, major trauma, or burns. An older method of diagnosis was based on meeting at least two systemic inflammatory response syndrome (SIRS) criteria due to a presumed infection. In 2016, SIRS was replaced with a shortened sequential organ failure assessment score (SOFA score) known as the quick SOFA score (qSOFA) which is two of the following three: increased breathing rate, change in level of consciousness, and low blood pressure. Blood cultures are recommended preferably before antibiotics are started, however, infection of the blood is not required for the diagnosis. Medical imaging should be used to look for the possible location of infection. Other potential causes of similar signs and symptoms include anaphylaxis, adrenal insufficiency, low blood volume, heart failure, and pulmonary embolism.\nSepsis is usually treated with intravenous fluids and antibiotics. Typically, antibiotics are given as soon as possible. Often, ongoing care is performed in an intensive care unit. If fluid replacement is not enough to maintain blood pressure, medications that raise blood pressure may be used. Mechanical ventilation and dialysis may be needed to support the function of the lungs and kidneys, respectively. To guide treatment, a central venous catheter and an arterial catheter may be placed for access to the bloodstream. Other measurements such as cardiac output and superior vena cava oxygen saturation may be used. People with sepsis need preventive measures for deep vein thrombosis, stress ulcers and pressure ulcers, unless other conditions prevent such interventions. Some might benefit from tight control of blood sugar levels with insulin. The use of corticosteroids is controversial. Drotrecogin alfa, originally marketed for severe sepsis, has not been found to be helpful, and was withdrawn from sale in 2011.\nDisease severity partly determines the outcome. The risk of death from sepsis is as high as 30%, from severe sepsis as high as 50%, and from septic shock as high as 80%. The number of cases worldwide is unknown as there is little data from the developing world. Estimates suggest sepsis affects millions of people a year. In the developed world approximately 0.2 to 3 people per 1000 are affected by sepsis yearly, resulting in about a million cases per year in the United States. Rates of disease have been increasing. Sepsis is more common among males than females. The medical condition has been described since the time of Hippocrates. The terms \"septicemia\" and \"blood poisoning\" have been used in various ways and are no longer recommended.\n\nIn addition to symptoms related to the actual cause, sepsis is frequently associated with the following – fever, low body temperature, rapid breathing, a fast heart rate, confusion, and edema. Early signs include a rapid heart rate, decreased urination, and high blood sugar. Signs of established sepsis include confusion, metabolic acidosis (which may be accompanied by a faster breathing rate that leads to respiratory alkalosis), low blood pressure due to decreased systemic vascular resistance, higher cardiac output, and disorders in blood-clottting that may lead to organ failure.\n\nThe drop in blood pressure seen in sepsis can cause lightheadedness and is part of the criteria for septic shock.\n\nInfections leading to sepsis are usually bacterial, but may be fungal or viral. Gram-positive bacteria was the predominant cause of sepsis before the introduction of antibiotics in the 1950s. After the introduction of antibiotics, gram-negative bacteria became the predominant cause of sepsis from the 1960s to the 1980s. After the 1980s, gram-positive bacteria, most commonly staphylococci, are thought to cause more than 50% of cases of sepsis. Other commonly implicated bacteria include \"Streptococcus pyogenes\", \"Escherichia coli\", \"Pseudomonas aeruginosa\", and \"Klebsiella\" species. Fungal sepsis accounts for approximately 5% of severe sepsis and septic shock cases; the most common cause of fungal sepsis is infection by \"Candida\" species of yeast, a frequent hospital-acquired infection.\n\nThe most common sites of infection resulting in severe sepsis are the lungs, the abdomen, and the urinary tract. Typically, 50% of all sepsis cases start as an infection in the lungs. No definitive source is found in one third to one half of cases.\n\nEarly diagnosis is necessary to properly manage sepsis, as initiation of rapid therapy is key to reducing deaths from severe sepsis. Some hospitals use alerts generated from electronic health records to bring attention to potential cases as early as possible.\n\nWithin the first three hours of suspected sepsis, diagnostic studies should include white blood cell counts, measuring serum lactate, and obtaining appropriate cultures before starting antibiotics, so long as this does not delay their use by more than 45 minutes. To identify the causative organism(s), at least two sets of blood cultures using bottles with media for aerobic and anaerobic organisms should be obtained, with at least one drawn through the skin and one drawn through each vascular access device (such as an IV catheter) in place more than 48 hours. Bacteria are present in the blood in only about 30% of cases. Another possible method of detection is by polymerase chain reaction. If other sources of infection are suspected, cultures of these sources, such as urine, cerebrospinal fluid, wounds, or respiratory secretions, also should be obtained, as long as this does not delay the use of antibiotics.\n\nWithin six hours, if blood pressure remains low despite initial fluid resuscitation of 30 ml/kg, or if initial lactate is ≥ 4 mmol/l (36 mg/dl), central venous pressure and central venous oxygen saturation should be measured. Lactate should be re-measured if the initial lactate was elevated. Evidence for point of care lactate measurement over usual methods of measurement, however, is poor.\n\nWithin twelve hours, it is essential to diagnose or exclude any source of infection that would require emergent source control, such as necrotizing soft tissue infection, infection causing inflammation of the abdominal cavity lining, infection of the bile duct, or intestinal infarction. A pierced internal organ (free air on abdominal x-ray or CT scan), an abnormal chest x-ray consistent with pneumonia (with focal opacification), or petechiae, purpura, or purpura fulminans may be evident of infection.\n\nPreviously, SIRS criteria had been used to define sepsis. If the SIRS criteria are negative, it is very unlikely the person has sepsis; if it is positive, there is just a moderate probability that the person has sepsis. According to SIRS, there were different levels of sepsis: sepsis, severe sepsis, and septic shock. The definition of SIRS is shown below:\n\n\nIn 2016 a new consensus was reached to replace screening by systemic inflammatory response syndrome (SIRS) with qSOFA. However, the American College of Chest Physicians (CHEST) raised concerns that qSOFA and SOFA criteria may lead to delayed diagnosis of serious infection, leading to delayed treatment. Although SIRS criteria can be too sensitive and not specific enough in identifying sepsis, SOFA also has its own limitation and is not intended to replace the SIRS definition. qSOFA has also been found to be poorly sensitive though decently specific for the risk of death with SIRS possibly better for screening.\n\nExamples of end-organ dysfunction include the following:\n\nMore specific definitions of end-organ dysfunction exist for SIRS in pediatrics.\n\nConsensus definitions, however, continue to evolve, with the latest expanding the list of signs and symptoms of sepsis to reflect clinical bedside experience.\n\nA 2013 review concluded moderate-quality evidence exists to support use of the procalcitonin level as a method to distinguish sepsis from non-infectious causes of SIRS. The same review found the sensitivity of the test to be 77% and the specificity to be 79%. The authors suggested that procalcitonin may serve as a helpful diagnostic marker for sepsis, but cautioned that its level alone cannot definitively make the diagnosis. A 2012 systematic review found that soluble urokinase-type plasminogen activator receptor (SuPAR) is a nonspecific marker of inflammation and does not accurately diagnose sepsis. This same review concluded, however, that SuPAR has prognostic value, as higher SuPAR levels are associated with an increased rate of death in those with sepsis.\n\nThe differential diagnosis for sepsis is broad and has to examine (in order to exclude) the non-infectious conditions that may cause the systemic signs of SIRS: alcohol withdrawal, acute pancreatitis, burns, pulmonary embolism, thyrotoxicosis, anaphylaxis, adrenal insufficiency, and neurogenic shock. Hyperinflammatory syndromes such as hemophagocytic lymphohistiocytosis (HLH) may have similar symptoms and should also be included in differential diagnosis.\n\nIn common clinical usage, neonatal sepsis refers to a bacterial blood stream infection in the first month of life, such as meningitis, pneumonia, pyelonephritis, or gastroenteritis, but neonatal sepsis also may be due to infection with fungi, viruses, or parasites. Criteria with regard to hemodynamic compromise or respiratory failure are not useful because they present too late for intervention.\n\nSepsis is caused by a combination of factors related to the particular invading pathogen(s) and to the status of the immune system of the host. The early phase of sepsis characterized by excessive inflammation (sometimes resulting in a cytokine storm) may be followed by a prolonged period of decreased functioning of the immune system. Either of these phases may prove fatal. On the other hand, systemic inflammatory response syndrome (SIRS) occurs in people without the presence of infection, for example, in those with burns, polytrauma, or the initial state in pancreatitis and chemical pneumonitis. However, sepsis also causes similar response to SIRS.\n\nBacterial virulence factors, such as glycocalyx and various adhesins, allow colonization, immune evasion, and establishment of disease in the host. Sepsis caused by gram-negative bacteria is thought to be largely due to a response by the host to the lipid A component of lipopolysaccharide, also called endotoxin. Sepsis caused by gram-positive bacteria may result from an immunological response to cell wall lipoteichoic acid. Bacterial exotoxins that act as superantigens also may cause sepsis. Superantigens simultaneously bind major histocompatibility complex and T-cell receptors in the absence of antigen presentation. This forced receptor interaction induces the production of pro-inflammatory chemical signals (cytokines) by T-cells.\n\nThere are a number of microbial factors that may cause the typical septic inflammatory cascade. An invading pathogen is recognized by its pathogen-associated molecular patterns (PAMPs). Examples of PAMPs include lipopolysaccharides and flagellin in gram-negative bacteria, muramyl dipeptide in the peptidoglycan of the gram-positive bacterial cell wall, and CpG bacterial DNA. These PAMPs are recognized by the pattern recognition receptors (PRRs) of the innate immune system, which may be membrane-bound or cytosolic. There are four families of PRRs: the toll-like receptors, the C-type lectin receptors, the NOD-like receptors, and the RIG-I-like receptors. Invariably, the association of a PAMP and a PRR will cause a series of intracellular signalling cascades. Consequentially, transcription factors such as nuclear factor-kappa B and activator protein-1, will up-regulate the expression of pro-inflammatory and anti-inflammatory cytokines.\n\nUpon detection of microbial antigens, the host systemic immune system is activated. Immune cells not only recognise pathogen-associated molecular patterns, but also damage-associated molecular patterns from damaged tissues. An uncontrolled immune response is then activated because leukocytes are not recruited to the specific site of infection, but instead they are recruited all over the body. Then, an immunosuppression state ensues when the proinflammatory T helper cell 1 (TH1) is shifted to TH2, mediated by interleukin 10, which is known as \"compensatory anti-inflammatory response syndrome\". The apoptosis (cell death) of lymphocytes further worsens the immunosuppression. Subsequently, multiple organ failure ensues because tissues are unable to use oxygen efficiently due to inhibition of cytochrome c oxidase.\n\nInflammatory responses cause multiple organ dysfunction syndrome through various mechanisms as described below. Increased permeability of the lung vessels causes leaking of fluids into alveoli, which results in pulmonary edema and acute respiratory distress syndrome (ARDS). Impaired utilization of oxygen in the liver impairs bile salt transport, causing jaundice (yellowish discoloration of skin). In kidneys, inadequate oxygenation results in tubular epithelial cell injury (of the cells lining the kidney tubules), and thus causes acute kidney injury (AKI). Meanwhile, in a human heart, impaired calcium transport, and low production of adenosine triphosphate (ATP), can cause myocardial depression, reducing cardiac contractility and causing heart failure. In the gastrointestinal tract, increased permeability of the mucosa alters the microflora, causing mucosal bleeding and paralytic ileus. In the central nervous system, direct damage of the brain cells and disturbances of neurotransmissions causes altered mental status. Cytokines such as tumor necrosis factor, interleukin 1, and interleukin 6 may activate procoagulation factors in the cells lining blood vessels, leading to endothelial damage. The damaged endothelial surface inhibits anticoagulant properties as well as increases antifibrinolysis, which may lead to intravascular clotting, the formation of blood clots in small blood vessels, and multiple organ failure]].\n\nThe low blood pressure seen in those with sepsis is the result of various processes, including excessive production of chemicals that dilate blood vessels such as nitric oxide, a deficiency of chemicals that constrict blood vessels such as vasopressin, and activation of ATP-sensitive potassium channels. In those with severe sepsis and septic shock, this sequence of events leads to a type of circulatory shock known as distributive shock.\n\nEarly recognition and focused management may improve the outcomes in sepsis. Current professional recommendations include a number of actions (\"bundles\") to be followed as soon as possible after diagnosis. Within the first three hours someone with sepsis should have received antibiotics and, intravenous fluids if there is evidence of either low blood pressure or other evidence for inadequate blood supply to organs (as evidenced by a raised level of lactate); blood cultures also should be obtained within this time period. After six hours the blood pressure should be adequate, close monitoring of blood pressure and blood supply to organs should be in place, and the lactate should be measured again if initially, it was raised. A related bundle, the \"Sepsis Six\", is in widespread use in the United Kingdom; this requires the administration of antibiotics within an hour of recognition, blood cultures, lactate and hemoglobin determination, urine output monitoring, high-flow oxygen, and intravenous fluids.\n\nApart from the timely administration of fluids and antibiotics, the management of sepsis also involves surgical drainage of infected fluid collections and appropriate support for organ dysfunction. This may include hemodialysis in kidney failure, mechanical ventilation in lung dysfunction, transfusion of blood products, and drug and fluid therapy for circulatory failure. Ensuring adequate nutrition—preferably by enteral feeding, but if necessary, by parenteral nutrition—is important during prolonged illness. Medication to prevent deep vein thrombosis and gastric ulcers also may be used.\n\nTwo sets of blood cultures (aerobic and anaerobic) should be taken without delaying the initiation of antibiotics. Cultures from other sites such as respiratory secretions, urine, wounds, cerebrospinal fluid, and catheter insertion sites (in-situ more than 48 hours) can be taken if infections from these sites are suspected. In severe sepsis and septic shock, broad-spectrum antibiotics (usually two, a β-lactam antibiotic with broad coverage, or broad-spectrum carbapenem combined with fluoroquinolones, macrolides, or aminoglycosides) are recommended. However, combination of antibiotics is not recommended for the treatment of sepsis but without shock and immunocompromised persons unless the combination is used to broaden the anti-bacterial activity. The choice of antibiotics is important in determining the survival of the person. Some recommend they be given within one hour of making the diagnosis, stating that for every hour of delay in the administration of antibiotics, there is an associated 6% rise in mortality. Others did not find a benefit with early administration.\n\nSeveral factors determine the most appropriate choice for the initial antibiotic regimen. These factors include local patterns of bacterial sensitivity to antibiotics, whether the infection is thought to be a hospital or community-acquired infection, and which organ systems are thought to be infected. Antibiotic regimens should be reassessed daily and narrowed if appropriate. Treatment duration is typically 7–10 days with the type of antibiotic used directed by the results of cultures. If the culture result is negative, antibiotics should be de-escalated according to person's clinical response or stopped altogether if infection is not present to decrease the chances that the person is infected with multiple drug resistance organisms. In case of people having high risk of being infected with multiple drug resistance organisms such as \"Pseudomonas aeruginosa\", \"Acinetobacter baumannii\", addition of antibiotic specific to gram-negative organism is recommended. For Methicillin-resistant Staphylococcus aureus (MRSA), vancomycin or teicoplanin is recommended. For Legionella infection, addition of macrolide or fluoroquinolone is chosen. If fungal infection is suspected, an echinocandin, such as caspofungin or micafungin, is chosen for people with severe sepsis, followed by triazole (fluconazole and itraconazole) for less ill people. Prolonged antibiotic prophylaxis is not recommended in people who has SIRS without any infectious origin such as acute pancreatitis and burns unless sepsis is suspected.\n\nOnce daily dosing of aminoglycoside is sufficient to achieve peak plasma concentration for clinical response without kidney toxicity. Meanwhile, for antibiotics with low volume distribution (vancomycin, teicoplanin, colistin), loading dose is required to achieve adequate therapeutic level to fight infections. Frequent infusions of beta-lactam antibiotics without exceeding total daily dose would help to keep the antibiotics level above minimum inhibitory concentration (MIC), thus providing better clinical response. Giving beta-lactam antibiotics continuously may be better than giving them intermittently. Access to therapeutic drug monitoring is important to ensure adequate drug therapeutic level while at the same time preventing the drug from reaching toxic level.\n\nThe Surviving Sepsis Campaign has recommended 30 ml/kg of fluid to be given in adults in the first 3 hours followed by fluid titration according to blood pressure, urine output, respiratory rate, and oxygen saturation with a target mean arterial pressure (MAP) of 65 mmHg. In children an initial amount of 20ml/kg is reasonable in shock. In cases of severe sepsis and septic shock where a central venous catheter is used to measure blood pressures dynamically, fluids should be administered until the central venous pressure (CVP) reaches 8–12mmHg. Once these goals are met, the central venous oxygen saturation (ScvO2), i.e., the oxygen saturation of venous blood as it returns to the heart as measured at the vena cava, is optimized. If the ScvO2 is less than 70%, blood may be given to reach a hemoglobin of 10 g/dL and then inotropes are added until the ScvO2 is optimized. In those with acute respiratory distress syndrome (ARDS) and sufficient tissue blood fluid, more fluids should be given carefully.\n\nCrystalloid is recommended as the fluid of choice for resuscitation. Albumin can be used if large amount of crystalloid is required for resuscitaition. Crystalloid solutions and albumin are better than other fluids (such as hydroxyethyl starch) in terms of risk of death. Starches also carry an increased risk of acute kidney injury, and need for blood transfusion. Various colloid solutions (such as modified gelatin) carry no advantage over crystalloid. Albumin also appears to be of no benefit over crystalloids.\n\nThe Surviving Sepsis Campaign recommended packed red blood cells transfusion for hemoglobin levels below 70 g/L if there is no myocardial ischemia, hypoxemia, or acute bleeding. In a 2014 trial, blood transfusions to keep target hemoglobin above 70 or 90 g/L did not make any difference to survival rates; meanwhile, those with a lower threshold of transfusion received fewer transfusions in total. Erythropoietin is not recommended in the treatment of anemia with septic shock because it may precipitate blood clotting events. Fresh frozen plasma transfusion usually does not correct the underlying clotting abnormalities before a planned surgical procedure. However, platelet transfusion is suggested for platelet counts below (10 × 10/L) without any risk of bleeding, or (20 × 10/L) with high risk of bleeding, or (50 × 10/L) with active bleeding, before a planned surgery or an invasive procedure. IV immunoglobulin is not recommended because its beneficial effects are uncertain. Monoclonal and polyclonal preparations of intravenous immunoglobulin (IVIG) do not lower the rate of death in newborns and adults with sepsis. Evidence for the use of IgM-enriched polyclonal preparations of IVIG is inconsistent. On the other hand, the use of antithrombin to treat disseminated intravascular coagulation is also not useful. Meanwhile, the blood purification technique (such as hemoperfusion, plasma filtration, and coupled plasma filtration adsorption) to remove inflammatory mediators and bacterial toxins from the blood also does not demonstrate any survival benefit for septic shock.\n\nIf the person has been sufficiently fluid resuscitated but the mean arterial pressure is not greater than 65 mmHg, vasopressors are recommended. Norepinephrine (noradrenaline) is recommended as the initial choice.\n\nNorepinephrine raises blood pressure through a vasoconstriction effect, with little effect on stroke volume and heart rate. If a single vasopressor is not enough to raise the blood pressure, epinephrine (adrenaline) or vasopressin may be added. However, one of the adrenaline side effects is that it reduces blood flow to abdominal organs and may cause increased lactate levels. Vasopressin can be used in septic shock because studies have shown that there is a relative deficiency of vasopressin when shock continues for 24 to 48 hours. However, vasopressin reduces blood flow to the heart, finger/toes, and abdominal organs, resulting in a lack of oxygen supply to these tissues. Dopamine is typically not recommended. Although dopamine is useful to increase the stroke volume of the heart, it causes more abnormal heart rhythms than norepinephrine and also has an immunosuppressive effect. Dopamine is not proven to have protective properties on the kidneys. Dobutamine may be used if heart function is poor or blood flow is insufficient despite sufficient fluid volumes and blood pressure.\n\nThe use of steroids in sepsis is controversial. Studies do not give a clear picture as to whether and when glucocorticoids should be used. The 2016 Surviving Sepsis Campaign recommends against their use in those with septic shock if intravenous fluids and vasopressors are able to stabilize the person's cardiovascular function. Low dose hydrocortisone is only used if both intravenous fluids and vasopressors are not able to adequately treat septic shock. A 2015 Cochrane review found low-quality evidence of benefit.\n\nDuring critical illness, a state of adrenal insufficiency and tissue resistance to corticosteroids may occur. This has been termed critical illness–related corticosteroid insufficiency. Treatment with corticosteroids might be most beneficial in those with septic shock and early severe ARDS, whereas its role in others such as those with pancreatitis or severe pneumonia is unclear. However, the exact way of determining corticosteroid insufficiency remains problematic. It should be suspected in those poorly responding to resuscitation with fluids and vasopressors. Neither ACTH stimulation testing nor random cortisol levels are recommended to confirm the diagnosis. The method of stopping glucocorticoid drugs is variable, and it is unclear whether they should be slowly decreased or simply abruptly stopped. However, the 2016 Surviving Sepsis Campaign recommended to taper steroids when vasopressors are no longer needed.\n\nA target tidal volume of 6 mL/kg of predicted body weight (PBW) and a plateau pressure less than 30 cm HO is recommended for those who require ventilation due to sepsis-induced severe ARDS. High positive end expiratory pressure (PEEP) is recommended for moderate to severe ARDS in sepsis as it opens more lung units for oxygen exchange. Predicted body weight is based on sex and height. Recruitment maneuvers may be necessary for severe ARDS by briefly raising the transpulmonary pressure. It is recommended that the head of the bed be raised if possible to improve ventilation. However, β2 adrenergic receptor agonists are not recommended to treat ARDS because it may reduce survival rates and precipitate abnormal heart rhythms. A spontaneous breathing trial using continuous positive airway pressure (CPAP), T piece, or inspiratory pressure augmentation can be helpful in reducing the duration of ventilation. Minimizing intermittent or continuous sedation is helpful in reducing the duration of mechanical ventilation.\n\nGeneral anesthesia is recommended for people with sepsis who require surgical procedures to remove the infective source. Usually inhalational and intravenous anesthetics are used. Requirements for anesthetics may be reduced in sepsis. Inhalational anesthetics can reduce the level of proinflammatory cytokines, altering leukocyte adhesion and proliferation, inducing apoptosis (cell death) of the lymphocytes, possibly with a toxic effect on mitochondrial function. Although etomidate has a minimal effect on the cardiovascular system, it is often not recommended as a medication to help with intubation in this situation due to concerns it may lead to poor adrenal function and an increased risk of death. The small amount of evidence there is, however, has not found a change in the risk of death with etomidate.\n\nParalytic agents should be avoided unless ARDS is suspected.\n\nEarly goal directed therapy (EGDT) is an approach to the management of severe sepsis during the initial 6 hours after diagnosis. It is a step-wise approach, with the physiologic goal of optimizing cardiac preload, afterload, and contractility. It includes giving early antibiotics. EGDT also involves monitoring of hemodynamic parameters and specific interventions to achieve key resuscitation targets which include maintaining a central venous pressure between 8–12 mmHg, a mean arterial pressure of between 65–90 mmHg, a central venous oxygen saturation (ScvO) greater than 70% and a urine output of greater than 0.5 ml/kg/hour. The goal is to optimize oxygen delivery to tissues and achieve a balance between systemic oxygen delivery and demand. An appropriate decrease in serum lactate may be equivalent to ScvO and easier to obtain.\n\nIn the original trial, early goal directed therapy was found to reduce mortality from 46.5% to 30.5% in those with sepsis, and the Surviving Sepsis Campaign has been recommending its use. However, three more recent large randomized control trials (ProCESS, ARISE, and ProMISe), did not demonstrate a 90-day mortality benefit of early goal directed therapy when compared to standard therapy in severe sepsis. It is likely that some parts of EGDT are more important than others. Following these trials the use of EGDT is still considered reasonable.\n\nNeonatal sepsis can be difficult to diagnose as newborns may be asymptomatic. If a newborn shows signs and symptoms suggestive of sepsis, antibiotics are immediately started and are either changed to target a specific organism identified by diagnostic testing or discontinued after an infectious cause for the symptoms has been ruled out. Despite early intervention, death occurs in 13% of children who develop septic shock, with the risk partly based on other health problems. Those without multiple organ system failure or who require only one inotropic agent mortality is low.\n\nTreating fever in sepsis, including people in septic shock, has not been associated with any improvement in mortality over a period of 28 days. Treatment of fever still occurs for other reasons.\n\nA 2012 Cochrane review concluded that N-acetylcysteine does not reduce mortality in those with SIRS or sepsis and may even be harmful.\n\nRecombinant activated protein C (drotrecogin alpha) was originally introduced for severe sepsis (as identified by a high APACHE II score), where it was thought to confer a survival benefit. However, subsequent studies showed that it increased adverse events—bleeding risk in particular—and did not decrease mortality. It was removed from sale in 2011. Another medication known as eritoran also has not shown benefit.\n\nIn those with high blood sugar levels, insulin to bring it down to 7.8–10 mmol/L (140–180 mg/dL) is recommended with lower levels potentially worsening outcomes. Glucose levels taken from capillary blood should be interpreted with care because such measurements may not be accurate. If a person has an arterial catheter, arterial blood is recommended for blood glucose testing.\n\nIntermittent or continuous renal replacement therapy may be used if indicated. However, sodium bicarbonate is not recommended for a person with lactic acidosis secondary to hypoperfusion. Low molecular weight heparin (LMWH), unfractionated heparin (UFH), and mechanical prophylaxis with intermittent pneumatic compression devices are recommended for any person with sepsis at moderate to high risk of venous thromboembolism. Stress ulcer prevention with proton-pump inhibitor (PPI) and H2 antagonist are useful in a person with risk factors of developing upper gastrointestinal bleeding (UGIB) such as on mechanical ventilation for more than 48 hours, coagulation disorders, liver disease, and renal replacement therapy. Achieving partial or full enteral feeding (delivery of nutrients through a feeding tube) is chosen as the best approach to provide nutrition for a person who is contraindicated for oral intake or unable to tolerate orally in the first seven days of sepsis when compared to intravenous nutrition. However, omega-3 fatty acids are not recommended as immune supplements for a person with sepsis or septic shock. The usage of prokinetic agents such as metoclopramide, domperidone, and erythromycin are recommended for those who are septic and unable to tolerate enteral feeding. However, these agents may precipitate prolongation of the QT interval and consequently provoke a ventricular arrhythmia such as torsades de pointes. The usage of prokinetic agents should be reassessed daily and stopped if no longer indicated.\n\nSevere sepsis will prove fatal in approximately 20–35% of people, and septic shock will prove fatal in 30–70% of people. Lactate is a useful method of determining prognosis, with those who have a level greater than 4 mmol/L having a mortality of 40% and those with a level of less than 2 mmol/L having a mortality of less than 15%.\n\nThere are a number of prognostic stratification systems, such as APACHE II and Mortality in Emergency Department Sepsis. APACHE II factors in the person's age, underlying condition, and various physiologic variables to yield estimates of the risk of dying of severe sepsis. Of the individual covariates, the severity of underlying disease most strongly influences the risk of death. Septic shock is also a strong predictor of short- and long-term mortality. Case-fatality rates are similar for culture-positive and culture-negative severe sepsis. The Mortality in Emergency Department Sepsis (MEDS) score is simpler, and useful in the emergency department environment.\n\nSome people may experience severe long-term cognitive decline following an episode of severe sepsis, but the absence of baseline neuropsychological data in most people with sepsis makes the incidence of this difficult to quantify or to study.\n\nSepsis causes millions of deaths globally each year and is the most common cause of death in people who have been hospitalized. The worldwide incidence of sepsis is estimated to be 18 million cases per year. In the United States sepsis affects approximately 3 in 1,000 people, and severe sepsis contributes to more than 200,000 deaths per year.\n\nSepsis occurs in 1–2% of all hospitalizations and accounts for as much as 25% of ICU bed utilization. Due to it rarely being reported as a primary diagnosis (often being a complication of cancer or other illness), the incidence, mortality, and morbidity rates of sepsis are likely underestimated. A study by the Agency for Healthcare Research and Quality (AHRQ) of selected States found that there were approximately 651 hospital stays per 100,000 population with a sepsis diagnosis in 2010. It is the second-leading cause of death in non-coronary intensive care unit (ICU) and the tenth-most-common cause of death overall (the first being heart disease). Children under 12 months of age and elderly people have the highest incidence of severe sepsis. Among U.S. patients who had multiple sepsis hospital admissions in 2010, those who were discharged to a skilled nursing facility or long term care following the initial hospitalization were more likely to be readmitted than those discharged to another form of care. A study of 18 U.S. States found that, amongst Medicare patients in 2011, sepsis was the second most common principal reason for readmission within 30 days.\n\nSeveral medical conditions increase a person's susceptibility to infection and developing sepsis. Common sepsis risk factors include age (especially the very young and old); conditions that weaken the immune system such as cancer, diabetes, or the absence of a spleen; and major trauma and burns.\n\nFrom 1979 to 2000, data from the United States National Hospital Discharge Survey showed that the incidence of sepsis increased fourfold, to 240 cases per 100,000 population, with higher incidence in men when compared to women. During the same time frame, the in-hospital case fatality rate was reduced from 28% to 18%. However, according to the nationwide inpatient sample from the United States, the incidence of severe sepsis increased from 200 per 10,000 population in 2003 to 300 cases in 2007 for population aged more than 18 years. The incidence rate is particularly high among infants, with the incidence of 500 cases per 100,000 population. Mortality related to sepsis increases with age, from less than 10% in the age group of 3 to 5 years to 60% by sixth decade of life. The increase in average age of the population, alongside the presence of more people with chronic diseases or on immunosuppressive medications, and also the increase in the number of invasive procedures being performed, has led to an increased rate of sepsis.\n\nThe term \"σήψις\" (sepsis) was introduced by Hippocrates in the fourth century BC, and it meant the process of decay or decomposition of organic matter. In the eleventh century, Avicenna used the term \"blood rot\" for diseases linked to severe purulent process. Though severe systemic toxicity had already been observed, it was only in the 19th century that the specific term – sepsis – was used for this condition.\n\nThe terms \"septicemia\", also spelled \"septicaemia\", and \"blood poisoning\" referred to the microorganisms or their toxins in the blood and are no longer commonly used. The modern term for this is bacteremia.\n\nBy the end of the 19th century, it was widely believed that microbes produced substances that could injure the mammalian host and that soluble toxins released during infection caused the fever and shock that were commonplace during severe infections. Pfeiffer coined the term endotoxin at the beginning of the 20th century to denote the pyrogenic principle associated with \"Vibrio cholerae\". It was soon realised that endotoxins were expressed by most and perhaps all gram-negative bacteria. The lipopolysaccharide character of enteric endotoxins was elucidated in 1944 by Shear. The molecular character of this material was determined by Luderitz et al. in 1973.\n\nIt was discovered in 1965 that a strain of C3H/HeJ mice were immune to the endotoxin-induced shock. The genetic locus for this effect was dubbed \"Lps\". These mice were also found to be hypersusceptible to infection by gram-negative bacteria. These observations were finally linked in 1998 by the discovery of the toll-like receptor gene 4 (TLR 4). Genetic mapping work, performed over a period of five years, showed that TLR4 was the sole candidate locus within the Lps critical region; this strongly implied that a mutation within TLR4 must account for the lipopolysaccharide resistance phenotype. The defect in the TLR4 gene that led to the endotoxin resistant phenotype was discovered to be due to a mutation in the cytoplasm.\n\nControversy occurred in the scientific community over the use of mouse models in research into sepsis in 2013, when scientists published a review of the mouse immune system compared to the human immune system, and showed that on a systems level, the two worked very differently; the authors noted that as of the date of their article over 150 clinical trials of sepsis had been conducted in humans, almost all of them supported by promising data in mice, and that all of them had failed. The authors called for abandoning the use of mouse models in sepsis research; others rejected that but called for more caution in interpreting the results of mouse studies, and more careful design of preclinical studies. One approach is to rely more on studying biopsies and clinical data from people who have had sepsis, to try to identify biomarkers and drug targets for intervention.\n\nSepsis was the most expensive condition treated in United States' hospital stays in 2013, at an aggregate cost of $23.6 billion for nearly 1.3 million hospitalizations. Costs for sepsis hospital stays more than quadrupled since 1997 with an 11.5 percent annual increase. By payer, it was the most costly condition billed to Medicare and the uninsured, the second-most costly billed to Medicaid, and the fourth-most costly billed to private insurance.\n\nA large international collaboration entitled the \"Surviving Sepsis Campaign\" was established in 2002 to educate people about sepsis and to improve patient outcomes with sepsis. The Campaign has published an evidence-based review of management strategies for severe sepsis, with the aim to publish a complete set of guidelines in subsequent years.\n\nSepsis Alliance is a charitable organization that was created to raise sepsis awareness among both the general public and healthcare professionals.\n\n"}
{"id": "16079692", "url": "https://en.wikipedia.org/wiki?curid=16079692", "title": "Sewage treatment", "text": "Sewage treatment\n\nSewage treatment is the process of removing contaminants from municipal wastewater, containing mainly household sewage plus some industrial wastewater. Physical, chemical, and biological processes are used to remove contaminants and produce treated wastewater (or treated effluent) that is safe enough for release into the environment. A by-product of sewage treatment is a semi-solid waste or slurry, called sewage sludge. The sludge has to undergo further treatment before being suitable for disposal or application to land.\n\nSewage treatment may also be referred to as wastewater treatment. However, the latter is a broader term which can also refer to industrial wastewater. For most cities, the sewer system will also carry a proportion of industrial effluent to the sewage treatment plant which has usually received pre-treatment at the factories themselves to reduce the pollutant load. If the sewer system is a combined sewer then it will also carry urban runoff (stormwater) to the sewage treatment plant. Sewage water can travel towards treatment plants via piping and in a flow aided by gravity and pumps. The first part of filtration of sewage typically includes a bar screen to filter solids and large objects which are then collected in dumpsters and disposed of in landfills. Fat and grease is also removed before the primary treatment of sewage.\n\nThe term \"sewage treatment plant\" (or \"sewage treatment works\" in some countries) is nowadays often replaced with the term wastewater treatment plant or wastewater treatment station.\n\nSewage can be treated close to where the sewage is created, which may be called a \"decentralized\" system or even an \"on-site\" system (in septic tanks, biofilters or aerobic treatment systems). Alternatively, sewage can be collected and transported by a network of pipes and pump stations to a municipal treatment plant. This is called a \"centralized\" system (see also sewerage and pipes and infrastructure).\n\nSewage is generated by residential, institutional, commercial and industrial establishments. It includes household waste liquid from toilets, baths, showers, kitchens, and sinks draining into sewers. In many areas, sewage also includes liquid waste from industry and commerce. The separation and draining of household waste into greywater and blackwater is becoming more common in the developed world, with treated greywater being permitted to be used for watering plants or recycled for flushing toilets.\n\nSewage may include stormwater runoff or urban runoff. Sewerage systems capable of handling storm water are known as combined sewer systems. This design was common when urban sewerage systems were first developed, in the late 19th and early 20th centuries. Combined sewers require much larger and more expensive treatment facilities than sanitary sewers. Heavy volumes of storm runoff may overwhelm the sewage treatment system, causing a spill or overflow. Sanitary sewers are typically much smaller than combined sewers, and they are not designed to transport stormwater. Backups of raw sewage can occur if excessive infiltration/inflow (dilution by stormwater and/or groundwater) is allowed into a sanitary sewer system. Communities that have urbanized in the mid-20th century or later generally have built separate systems for sewage (sanitary sewers) and stormwater, because precipitation causes widely varying flows, reducing sewage treatment plant efficiency.\n\nAs rainfall travels over roofs and the ground, it may pick up various contaminants including soil particles and other sediment, heavy metals, organic compounds, animal waste, and oil and grease. Some jurisdictions require stormwater to receive some level of treatment before being discharged directly into waterways. Examples of treatment processes used for stormwater include retention basins, wetlands, buried vaults with various kinds of media filters, and vortex separators (to remove coarse solids).\n\nIn highly regulated developed countries, industrial effluent usually receives at least pretreatment if not full treatment at the factories themselves to reduce the pollutant load, before discharge to the sewer. This process is called industrial wastewater treatment or pretreatment. The same does not apply to many developing countries where industrial effluent is more likely to enter the sewer if it exists, or even the receiving water body, without pretreatment.\n\nIndustrial wastewater may contain pollutants which cannot be removed by conventional sewage treatment. Also, variable flow of industrial waste associated with production cycles may upset the population dynamics of biological treatment units, such as the activated sludge process.\n\nSewage collection and treatment is typically subject to local, state and federal regulations and standards.\n\nTreating wastewater has the aim to produce an effluent that will do as little harm as possible when discharged to the surrounding environment, thereby preventing pollution compared to releasing untreated wastewater into the environment.\n\nSewage treatment generally involves three stages, called primary, secondary and tertiary treatment.\n\nPretreatment removes all materials that can be easily collected from the raw sewage before they damage or clog the pumps and sewage lines of primary treatment clarifiers. Objects commonly removed during pretreatment include trash, tree limbs, leaves, branches, and other large objects.\n\nThe influent in sewage water passes through a bar screen to remove all large objects like cans, rags, sticks, plastic packets etc. carried in the sewage stream. This is most commonly done with an automated mechanically raked bar screen in modern plants serving large populations, while in smaller or less modern plants, a manually cleaned screen may be used. The raking action of a mechanical bar screen is typically paced according to the accumulation on the bar screens and/or flow rate. The solids are collected and later disposed in a landfill, or incinerated. Bar screens or mesh screens of varying sizes may be used to optimize solids removal. If gross solids are not removed, they become entrained in pipes and moving parts of the treatment plant, and can cause substantial damage and inefficiency in the process.\n\nGrit consists of sand, gravel, cinders, and other heavy materials. It also includes organic matter such as eggshells, bone chips, seeds, and coffee grounds. Pretreatment may include a sand or grit channel or chamber, where the velocity of the incoming sewage is adjusted to allow the settlement of sand and grit. Grit removal is necessary to (1) reduce formation of heavy deposits in aeration tanks, aerobic digesters, pipelines, channels, and conduits; (2) reduce the frequency of digester cleaning caused by excessive accumulations of grit; and (3) protect moving mechanical equipment from abrasion and accompanying abnormal wear. The removal of grit is essential for equipment with closely machined metal surfaces such as comminutors, fine screens, centrifuges, heat exchangers, and high pressure diaphram pumps. Grit chambers come in 3 types: horizontal grit chambers, aerated grit chambers and vortex grit chambers. Vortex type grit chambers include mechanically induced vortex, hydraulically induced vortex, and multi-tray vortex separators. Given that traditionally, grit removal systems have been designed to remove clean inorganic particles that are greater than , most grit passes through the grit removal flows under normal conditions. During periods of high flow deposited grit is resuspended and the quantity of grit reaching the treatment plant increases substantially. It is, therefore important that the grit removal system not only operate efficiently during normal flow conditions but also under sustained peak flows when the greatest volume of grit reaches the plant.\n\nClarifiers and mechanized secondary treatment are more efficient under uniform flow conditions. Equalization basins may be used for temporary storage of diurnal or wet-weather flow peaks. Basins provide a place to temporarily hold incoming sewage during plant maintenance and a means of diluting and distributing batch discharges of toxic or high-strength waste which might otherwise inhibit biological secondary treatment (including portable toilet waste, vehicle holding tanks, and septic tank pumpers). Flow equalization basins require variable discharge control, typically include provisions for bypass and cleaning, and may also include aerators. Cleaning may be easier if the basin is downstream of screening and grit removal.\n\nIn some larger plants, fat and grease are removed by passing the sewage through a small tank where skimmers collect the fat floating on the surface. Air blowers in the base of the tank may also be used to help recover the fat as a froth. Many plants, however, use primary clarifiers with mechanical surface skimmers for fat and grease removal.\n\nIn the primary sedimentation stage, sewage flows through large tanks, commonly called \"pre-settling basins\", \"primary sedimentation tanks\" or \"primary clarifiers\". The tanks are used to settle sludge while grease and oils rise to the surface and are skimmed off. Primary settling tanks are usually equipped with mechanically driven scrapers that continually drive the collected sludge towards a hopper in the base of the tank where it is pumped to sludge treatment facilities. Grease and oil from the floating material can sometimes be recovered for saponification (soap making).\n\nSecondary treatment is designed to substantially degrade the biological content of the sewage which are derived from human waste, food waste, soaps and detergent. The majority of municipal plants treat the settled sewage liquor using aerobic biological processes. To be effective, the biota require both oxygen and food to live. The bacteria and protozoa consume biodegradable soluble organic contaminants (e.g. sugars, fats, organic short-chain carbon molecules, etc.) and bind much of the less soluble fractions into floc.\n\nSecondary treatment systems are classified as fixed-film or suspended-growth systems.\n\nSome secondary treatment methods include a secondary clarifier to settle out and separate biological floc or filter material grown in the secondary treatment bioreactor.\n\nThe purpose of tertiary treatment is to provide a final treatment stage to further improve the effluent quality before it is discharged to the receiving environment (sea, river, lake, wet lands, ground, etc.). More than one tertiary treatment process may be used at any treatment plant. If disinfection is practised, it is always the final process. It is also called \"effluent polishing.\"\n\nSand filtration removes much of the residual suspended matter. Filtration over activated carbon, also called \"carbon adsorption,\" removes residual toxins.\n\nLagoons or ponds provide settlement and further biological improvement through storage in large man-made ponds or lagoons. These lagoons are highly aerobic and colonization by native macrophytes, especially reeds, is often encouraged. Small filter-feeding invertebrates such as \"Daphnia\" and species of \"Rotifera\" greatly assist in treatment by removing fine particulates.\n\nBiological nutrient removal (BNR) is regarded by some as a type of secondary treatment process, and by others as a tertiary (or \"advanced\") treatment process.\n\nWastewater may contain high levels of the nutrients nitrogen and phosphorus. Excessive release to the environment can lead to a buildup of nutrients, called eutrophication, which can in turn encourage the overgrowth of weeds, algae, and cyanobacteria (blue-green algae). This may cause an algal bloom, a rapid growth in the population of algae. The algae numbers are unsustainable and eventually most of them die. The decomposition of the algae by bacteria uses up so much of the oxygen in the water that most or all of the animals die, which creates more organic matter for the bacteria to decompose. In addition to causing deoxygenation, some algal species produce toxins that contaminate drinking water supplies. Different treatment processes are required to remove nitrogen and phosphorus.\n\nNitrogen is removed through the biological oxidation of nitrogen from ammonia to nitrate (nitrification), followed by denitrification, the reduction of nitrate to nitrogen gas. Nitrogen gas is released to the atmosphere and thus removed from the water.\n\nNitrification itself is a two-step aerobic process, each step facilitated by a different type of bacteria. The oxidation of ammonia (NH) to nitrite (NO) is most often facilitated by \"Nitrosomonas\" spp. (\"nitroso\" referring to the formation of a nitroso functional group). Nitrite oxidation to nitrate (NO), though traditionally believed to be facilitated by \"Nitrobacter\" spp. (nitro referring the formation of a nitro functional group), is now known to be facilitated in the environment almost exclusively by \"Nitrospira\" spp.\n\nDenitrification requires anoxic conditions to encourage the appropriate biological communities to form. It is facilitated by a wide diversity of bacteria. Sand filters, lagooning and reed beds can all be used to reduce nitrogen, but the activated sludge process (if designed well) can do the job the most easily. Since denitrification is the reduction of nitrate to dinitrogen (molecular nitrogen) gas, an electron donor is needed. This can be, depending on the waste water, organic matter (from feces), sulfide, or an added donor like methanol. The sludge in the anoxic tanks (denitrification tanks) must be mixed well (mixture of recirculated mixed liquor, return activated sludge [RAS], and raw influent) e.g. by using submersible mixers in order to achieve the desired denitrification.\n\nSometimes the conversion of toxic ammonia to nitrate alone is referred to as tertiary treatment.\n\nOver time, different treatment configurations have evolved as denitrification has become more sophisticated. An initial scheme, the Ludzack–Ettinger Process, placed an anoxic treatment zone before the aeration tank and clarifier, using the return activated sludge (RAS) from the clarifier as a nitrate source. Influent wastewater (either raw or as effluent from primary clarification) serves as the electron source for the facultative bacteria to metabolize carbon, using the inorganic nitrate as a source of oxygen instead of dissolved molecular oxygen. This denitrification scheme was naturally limited to the amount of soluble nitrate present in the RAS. Nitrate reduction was limited because RAS rate is limited by the performance of the clarifier.\n\nThe \"Modified Ludzak–Ettinger Process\" (MLE) is an improvement on the original concept, for it recycles mixed liquor from the discharge end of the aeration tank to the head of the anoxic tank to provide a consistent source of soluble nitrate for the facultative bacteria. In this instance, raw wastewater continues to provide the electron source, and sub-surface mixing maintains the bacteria in contact with both electron source and soluble nitrate in the absence of dissolved oxygen.\n\nMany sewage treatment plants use centrifugal pumps to transfer the nitrified mixed liquor from the aeration zone to the anoxic zone for denitrification. These pumps are often referred to as \"Internal Mixed Liquor Recycle\" (IMLR) pumps. IMLR may be 200% to 400% the flow rate of influent wastewater (Q). This is in addition to Return Activated Sludge (RAS) from secondary clarifiers, which may be 100% of Q. (Therefore, the hydraulic capacity of the tanks in such a system should handle at least 400% of annual average design flow (AADF). At times, the raw or primary effluent wastewater must be carbon-supplemented by the addition of methanol, acetate, or simple food waste (molasses, whey, plant starch) to improve the treatment efficiency. These carbon additions should be accounted for in the design of a treatment facility's organic loading.\nFurther modifications to the MLE were to come: Bardenpho and Biodenipho processes include additional anoxic and oxidative processes to further polish the conversion of nitrate ion to molecular nitrogen gas. Use of an anaerobic tank following the initial anoxic process allows for luxury uptake of phosphorus by bacteria, thereby biologically reducing orthophosphate ion in the treated wastewater. Even newer improvements, such as Anammox Process, interrupt the formation of nitrate at the nitrite stage of nitrification, shunting nitrite-rich mixed liquor activated sludge to treatment where nitrite is then converted to molecular nitrogen gas, saving energy, alkalinity, and secondary carbon sourcing. Anammox™ (ANaerobic AMMonia OXidation) works by artificially extending detention time and preserving denitrifiying bacteria through the use of substrate added to the mixed liquor and continuously recycled from it prior to secondary clarification. Many other proprietary schemes are being deployed, including DEMON™, Sharon-ANAMMOX™, ANITA-Mox™, and DeAmmon™. The bacteria Brocadia anammoxidans can remove ammonium from waste water through anaerobic oxidation of ammonium to hydrazine, a form of rocket fuel.\n\nEvery adult human excretes between of phosphorus annually. Studies of United States sewage in the late 1960s estimated mean per capita contributions of in urine and feces, in synthetic detergents, and lesser variable amounts used as corrosion and scale control chemicals in water supplies. Source control via alternative detergent formulations has subsequently reduced the largest contribution, but the content of urine and feces will remain unchanged. Phosphorus removal is important as it is a limiting nutrient for algae growth in many fresh water systems. (For a description of the negative effects of algae, \"see\" Nutrient removal). It is also particularly important for water reuse systems where high phosphorus concentrations may lead to fouling of downstream equipment such as reverse osmosis.\n\nPhosphorus can be removed biologically in a process called enhanced biological phosphorus removal. In this process, specific bacteria, called polyphosphate-accumulating organisms (PAOs), are selectively enriched and accumulate large quantities of phosphorus within their cells (up to 20 percent of their mass). When the biomass enriched in these bacteria is separated from the treated water, these biosolids have a high fertilizer value.\n\nPhosphorus removal can also be achieved by chemical precipitation, usually with salts of iron (e.g. ferric chloride), aluminum (e.g. alum), or lime. This may lead to excessive sludge production as hydroxides precipitate and the added chemicals can be expensive. Chemical phosphorus removal requires significantly smaller equipment footprint than biological removal, is easier to operate and is often more reliable than biological phosphorus removal. Another method for phosphorus removal is to use granular laterite.\n\nSome systems use both biological phosphorus removal and chemical phosphorus removal. The chemical phosphorus removal in those systems may be used as a backup system, for use when the biological phosphorus removal is nor removing enough phosphorus, or may be used continuously. In either case, using both biological and chemical phosphorus removal has the advantage of not increasing sludge production as much as chemical phosphorus removal on its own, with the disadvantage of the increased initial cost associated with installing two different systems.\n\nOnce removed, phosphorus, in the form of a phosphate-rich sewage sludge, may be dumped in a landfill or used as fertilizer. In the latter case, the treated sewage sludge is also sometimes referred to as biosolids.\n\nThe purpose of disinfection in the treatment of waste water is to substantially reduce the number of microorganisms in the water to be discharged back into the environment for the later use of drinking, bathing, irrigation, etc. The effectiveness of disinfection depends on the quality of the water being treated (e.g., cloudiness, pH, etc.), the type of disinfection being used, the disinfectant dosage (concentration and time), and other environmental variables. Cloudy water will be treated less successfully, since solid matter can shield organisms, especially from ultraviolet light or if contact times are low. Generally, short contact times, low doses and high flows all militate against effective disinfection. Common methods of disinfection include ozone, chlorine, ultraviolet light, or sodium hypochlorite. Chloramine, which is used for drinking water, is not used in the treatment of waste water because of its persistence. After multiple steps of disinfection, the treated water is ready to be released back into the water cycle by means of the nearest body of water or agriculture. Afterwards, the water can be transferred to reserves for everyday human uses.\n\nChlorination remains the most common form of waste water disinfection in North America due to its low cost and long-term history of effectiveness. One disadvantage is that chlorination of residual organic material can generate chlorinated-organic compounds that may be carcinogenic or harmful to the environment. Residual chlorine or chloramines may also be capable of chlorinating organic material in the natural aquatic environment. Further, because residual chlorine is toxic to aquatic species, the treated effluent must also be chemically dechlorinated, adding to the complexity and cost of treatment.\n\nUltraviolet (UV) light can be used instead of chlorine, iodine, or other chemicals. Because no chemicals are used, the treated water has no adverse effect on organisms that later consume it, as may be the case with other methods. UV radiation causes damage to the genetic structure of bacteria, viruses, and other pathogens, making them incapable of reproduction. The key disadvantages of UV disinfection are the need for frequent lamp maintenance and replacement and the need for a highly treated effluent to ensure that the target microorganisms are not shielded from the UV radiation (i.e., any solids present in the treated effluent may protect microorganisms from the UV light). In the United Kingdom, UV light is becoming the most common means of disinfection because of the concerns about the impacts of chlorine in chlorinating residual organics in the wastewater and in chlorinating organics in the receiving water. Some sewage treatment systems in Canada and the US also use UV light for their effluent water disinfection.\n\nOzone () is generated by passing oxygen () through a high voltage potential resulting in a third oxygen atom becoming attached and forming . Ozone is very unstable and reactive and oxidizes most organic material it comes in contact with, thereby destroying many pathogenic microorganisms. Ozone is considered to be safer than chlorine because, unlike chlorine which has to be stored on site (highly poisonous in the event of an accidental release), ozone is generated on-site as needed. Ozonation also produces fewer disinfection by-products than chlorination. A disadvantage of ozone disinfection is the high cost of the ozone generation equipment and the requirements for special operators.\n\nMicropollutants such as pharmaceuticals, ingredients of household chemicals, chemicals used in small businesses or industries, environmental persistent pharmaceutical pollutant (EPPP) or pesticides may not be eliminated in the conventional treatment process (primary, secondary and tertiary treatment) and therefore lead to water pollution. Although concentrations of those substances and their decomposition products are quite low, there is still a chance to harm aquatic organisms. For pharmaceuticals, the following substances have been identified as \"toxicologically relevant\": substances with endocrine disrupting effects, genotoxic substances and substances that enhance the development of bacterial resistances. They mainly belong to the group of environmental persistent pharmaceutical pollutants. Techniques for elimination of micropollutants via a fourth treatment stage during sewage treatment are being tested in Germany, Switzerland and the Netherlands. However, since those techniques are still costly, they are not yet applied on a regular basis. Such process steps mainly consist of activated carbon filters that adsorb the micropollutants. Ozone can also be applied as an oxidative method. Also the use of enzymes such as the enzyme laccase is under investigation. A new concept which could provide an energy-efficient treatment of micropollutants could be the use of laccase secreting fungi cultivated at a wastewater treatment plant to degrade micropollutants and at the same time to provide enzymes at a cathode of a microbial biofuel cells. Microbial biofuel cells are investigated for their property to treat organic matter in wastewater.\n\nTo reduce pharmaceuticals in water bodies, also \"source control\" measures are under investigation, such as innovations in drug development or more responsible handling of drugs.\n\nOdors emitted by sewage treatment are typically an indication of an anaerobic or \"septic\" condition. Early stages of processing will tend to produce foul-smelling gases, with hydrogen sulfide being most common in generating complaints. Large process plants in urban areas will often treat the odors with carbon reactors, a contact media with bio-slimes, small doses of chlorine, or circulating fluids to biologically capture and metabolize the noxious gases. Other methods of odor control exist, including addition of iron salts, hydrogen peroxide, calcium nitrate, etc. to manage hydrogen sulfide levels.\n\nHigh-density solids pumps are suitable for reducing odors by conveying sludge through hermetic closed pipework.\n\nFor conventional sewage treatment plants, around 30 percent of the annual operating costs is usually required for energy. The energy requirements vary with type of treatment process as well as wastewater load. For example, constructed wetlands have a lower energy requirement than activated sludge plants, as less energy is required for the aeration step. Sewage treatment plants that produce biogas in their sewage sludge treatment process with anaerobic digestion can produce enough energy to meet most of the energy needs of the sewage treatment plant itself.\n\nIn conventional secondary treatment processes, most of the electricity is used for aeration, pumping systems and equipment for the dewatering and drying of sewage sludge. Advanced wastewater treatment plants, e.g. for nutrient removal, require more energy than plants that only achieve primary or secondary treatment.\n\nThe sludges accumulated in a wastewater treatment process must be treated and disposed of in a safe and effective manner. The purpose of digestion is to reduce the amount of organic matter and the number of disease-causing microorganisms present in the solids. The most common treatment options include anaerobic digestion, aerobic digestion, and composting. Incineration is also used, albeit to a much lesser degree.\n\nSludge treatment depends on the amount of solids generated and other site-specific conditions. Composting is most often applied to small-scale plants with aerobic digestion for mid-sized operations, and anaerobic digestion for the larger-scale operations.\n\nThe sludge is sometimes passed through a so-called pre-thickener which de-waters the sludge. Types of pre-thickeners include centrifugal sludge thickeners rotary drum sludge thickeners and belt filter presses. Dewatered sludge may be incinerated or transported offsite for disposal in a landfill or use as an agricultural soil amendment.\n\nMany processes in a wastewater treatment plant are designed to mimic the natural treatment processes that occur in the environment, whether that environment is a natural water body or the ground. If not overloaded, bacteria in the environment will consume organic contaminants, although this will reduce the levels of oxygen in the water and may significantly change the overall ecology of the receiving water. Native bacterial populations feed on the organic contaminants, and the numbers of disease-causing microorganisms are reduced by natural environmental conditions such as predation or exposure to ultraviolet radiation. Consequently, in cases where the receiving environment provides a high level of dilution, a high degree of wastewater treatment may not be required. However, recent evidence has demonstrated that very low levels of specific contaminants in wastewater, including hormones (from animal husbandry and residue from human hormonal contraception methods) and synthetic materials such as phthalates that mimic hormones in their action, can have an unpredictable adverse impact on the natural biota and potentially on humans if the water is re-used for drinking water. In the US and EU, uncontrolled discharges of wastewater to the environment are not permitted under law, and strict water quality requirements are to be met, as clean drinking water is essential. (For requirements in the US, \"see Clean Water Act.\") A significant threat in the coming decades will be the increasing uncontrolled discharges of wastewater within rapidly developing countries.\n\nSewage treatment plants can have multiple effects on nutrient levels in the water that the treated sewage flows into. These nutrients can have large effects on the biological life in the water in contact with the effluent.\nStabilization ponds (or sewage treatment ponds) can include any of the following:\n\nPhosphorus limitation is a possible result from sewage treatment and results in flagellate-dominated plankton, particularly in summer and fall.\n\nA phytoplankton study found high nutrient concentrations linked to sewage effluents. High nutrient concentration leads to high chlorophyll a concentrations, which is a proxy for primary production in marine environments. High primary production means high phytoplankton populations and most likely high zooplankton populations, because zooplankton feed on phytoplankton. However, effluent released into marine systems also leads to greater population instability.\n\nThe planktonic trends of high populations close to input of treated sewage is contrasted by the bacterial trend. In a study of \"Aeromonas\" spp. in increasing distance from a wastewater source, greater change in seasonal cycles was found the furthest from the effluent. This trend is so strong that the furthest location studied actually had an inversion of the \"Aeromonas\" spp. cycle in comparison to that of fecal coliforms. Since there is a main pattern in the cycles that occurred simultaneously at all stations it indicates seasonal factors (temperature, solar radiation, phytoplankton) control of the bacterial population. The effluent dominant species changes from \"Aeromonas caviae\" in winter to \"Aeromonas sobria\" in the spring and fall while the inflow dominant species is \"Aeromonas caviae\", which is constant throughout the seasons.\n\nWith suitable technology, it is possible to reuse sewage effluent for drinking water, although this is usually only done in places with limited water supplies, such as Windhoek and Singapore.\n\nIn arid countries, treated wastewater is often used in agriculture. For example, in Israel, about 50 percent of agricultural water use (total use was in 2008) is provided through reclaimed sewer water. Future plans call for increased use of treated sewer water as well as more desalination plants as part of water supply and sanitation in Israel.\n\nConstructed wetlands fed by wastewater provide both treatment and habitats for flora and fauna. Another example for reuse combined with treatment of sewage are the East Kolkata Wetlands in India. These wetlands are used to treat Kolkata's sewage, and the nutrients contained in the wastewater sustain fish farms and agriculture.\n\nFew reliable figures exist on the share of the wastewater collected in sewers that is being treated in the world. A global estimate by UNDP and UN-Habitat is that 90% of all wastewater generated is released into the environment untreated. In many developing countries the bulk of domestic and industrial wastewater is discharged without any treatment or after primary treatment only.\n\nIn Latin America about 15 percent of collected wastewater passes through treatment plants (with varying levels of actual treatment). In Venezuela, a below average country in South America with respect to wastewater treatment, 97 percent of the country’s sewage is discharged raw into the environment. In Iran, a relatively developed Middle Eastern country, the majority of Tehran's population has totally untreated sewage injected to the city’s groundwater. However, the construction of major parts of the sewage system, collection and treatment, in Tehran is almost complete, and under development, due to be fully completed by the end of 2012. In Isfahan, Iran's third largest city, sewage treatment was started more than 100 years ago.\n\nOnly few cities in sub-Saharan Africa have sewer-based sanitation systems, let alone wastewater treatment plants, an exception being South Africa and – until the late 1990s – Zimbabwe. Instead, most urban residents in sub-Saharan Africa rely on on-site sanitation systems without sewers, such as septic tanks and pit latrines, and fecal sludge management in these cities is an enormous challenge.\n\nBasic sewer systems were used for waste removal in ancient Mesopotamia, where vertical shafts carried the waste away into cesspools. Similar systems existed in the Indus Valley civilization in modern-day India and in Ancient Crete and Greece. In the Middle Ages the sewer systems built by the Romans fell into disuse and waste was collected into cesspools that were periodically emptied by workers known as 'rakers' who would often sell it as fertilizer to farmers outside the city.\n\nModern sewage systems were first built in the mid-nineteenth century as a reaction to the exacerbation of sanitary conditions brought on by heavy industrialization and urbanization. Due to the contaminated water supply, cholera outbreaks occurred in 1832, 1849 and 1855 in London, killing tens of thousands of people. This, combined with the Great Stink of 1858, when the smell of untreated human waste in the River Thames became overpowering, and the report into sanitation reform of the Royal Commissioner Edwin Chadwick, led to the Metropolitan Commission of Sewers appointing Joseph Bazalgette to construct a vast underground sewage system for the safe removal of waste. Contrary to Chadwick's recommendations, Bazalgette's system, and others later built in Continental Europe, did not pump the sewage onto farm land for use as fertilizer; it was simply piped to a natural waterway away from population centres, and pumped back into the environment.\n\nOne of the first attempts at diverting sewage for use as a fertilizer in the farm was made by the cotton mill owner James Smith in the 1840s. He experimented with a piped distribution system initially proposed by James Vetch that collected sewage from his factory and pumped it into the outlying farms, and his success was enthusiastically followed by Edwin Chadwick and supported by organic chemist Justus von Liebig.\n\nThe idea was officially adopted by the Health of Towns Commission, and various schemes (known as sewage farms) were trialled by different municipalities over the next 50 years. At first, the heavier solids were channeled into ditches on the side of the farm and were covered over when full, but soon flat-bottomed tanks were employed as reservoirs for the sewage; the earliest patent was taken out by William Higgs in 1846 for \"tanks or reservoirs in which the contents of sewers and drains from cities, towns and villages are to be collected and the solid animal or vegetable matters therein contained, solidified and dried...\" Improvements to the design of the tanks included the introduction of the horizontal-flow tank in the 1850s and the radial-flow tank in 1905. These tanks had to be manually de-sludged periodically, until the introduction of automatic mechanical de-sludgers in the early 1900s.\n\nThe precursor to the modern septic tank was the cesspool in which the water was sealed off to prevent contamination and the solid waste was slowly liquified due to anaerobic action; it was invented by L.H Mouras in France in the 1860s. Donald Cameron, as City Surveyor for Exeter patented an improved version in 1895, which he called a 'septic tank'; septic having the meaning of 'bacterial'. These are still in worldwide use, especially in rural areas unconnected to large-scale sewage systems.\n\nIt was not until the late 19th century that it became possible to treat the sewage by biologically decomposing the organic components through the use of microorganisms and removing the pollutants. Land treatment was also steadily becoming less feasible, as cities grew and the volume of sewage produced could no longer be absorbed by the farmland on the outskirts.\n\nEdward Frankland conducted experiments at the sewage farm in Croydon, England, during the 1870s and was able to demonstrate that filtration of sewage through porous gravel produced a nitrified effluent (the ammonia was converted into nitrate) and that the filter remained unclogged over long periods of time. This established the then revolutionary possibility of biological treatment of sewage using a contact bed to oxidize the waste. This concept was taken up by the chief chemist for the London Metropolitan Board of Works, William Libdin, in 1887:\n\nFrom 1885 to 1891 filters working on this principle were constructed throughout the UK and the idea was also taken up in the US at the Lawrence Experiment Station in Massachusetts, where Frankland's work was confirmed. In 1890 the LES developed a 'trickling filter' that gave a much more reliable performance.\n\nContact beds were developed in Salford, Lancashire and by scientists working for the London City Council in the early 1890s. According to Christopher Hamlin, this was part of a conceptual revolution that replaced the philosophy that saw \"sewage purification as the prevention of decomposition with one that tried to facilitate the biological process that destroy sewage naturally.\"\n\nContact beds were tanks containing the inert substance, such as stones or slate, that maximized the surface area available for the microbial growth to break down the sewage. The sewage was held in the tank until it was fully decomposed and it was then filtered out into the ground. This method quickly became widespread, especially in the UK, where it was used in Leicester, Sheffield, Manchester and Leeds. The bacterial bed was simultaneously developed by Joseph Corbett as Borough Engineer in Salford and experiments in 1905 showed that his method was superior in that greater volumes of sewage could be purified better for longer periods of time than could be achieved by the contact bed.\n\nThe Royal Commission on Sewage Disposal published its eighth report in 1912 that set what became the international standard for sewage discharge into rivers; the '20:30 standard', which allowed Biochemical oxygen demand and suspended solid per litre ().\n\n\n"}
{"id": "43580508", "url": "https://en.wikipedia.org/wiki?curid=43580508", "title": "Shields Warren", "text": "Shields Warren\n\nShields Warren (26 February 1898 – 1 July 1980) was an American pathologist. He was among the first to study the pathology of radioactive fallout. Warren influenced and mentored Eleanor Josephine Macdonald, epidemiologist and cancer researcher.\n\n\n\n\n\n\n"}
{"id": "18653559", "url": "https://en.wikipedia.org/wiki?curid=18653559", "title": "Surrogate alcohol", "text": "Surrogate alcohol\n\nSurrogate alcohol is a term for any substance containing ethanol that is intentionally consumed by humans but is not meant for human consumption. Some definitions of the term also extend to illegally produced alcoholic beverages.\n\nConsumption of such substances carries extreme health risks, both from the ethanol content and other more toxic substances that may be present. Users risk both acute poisoning from substances such as methanol, and chronic poisoning from substances such as lead.\n\nMost people turn to these as a last resort either out of desperation, being underaged or being unable to afford consumable alcoholic beverages (with homeless alcoholics) or due to lack of access to drinking ethanol (for example with prison inmates and individuals in psychiatric wards).\n\nMany alcoholic liquids contain alcohol but are not meant to be ingested in the same way as alcoholic beverages. Typical surrogate alcohols include:\n\nMost surrogate alcohols have very high alcoholic levels, some as high as 95%, and thus can lead to alcohol poisoning, along with other symptoms of alcohol abuse such as vertigo, impaired coordination, balance and judgment, nausea, vomiting, blurred vision, and even long-term effects such as heart failure, stroke, and death.\n\nBesides alcohol, there are many other toxic substances in surrogate alcohol such as hydrogen peroxide, antiseptics, ketones, as well as alcohols other than ethanol (drinking alcohol) such as isopropanol and methanol. Methanol, and, to a far lesser extent isopropanol, is a poison. The effect of other chemicals on health has not been adequately studied, and so the health risks, while assumed, are unclear. However, observations in countries with high consumption of surrogate alcohols, such as Russia, indicate that the impurities in the consumed drink lead to high mortality rates.\n\nIn December 2016, 62 people died from drinking surrogate alcohol in the Russian city of Irkutsk.\n\n\n"}
{"id": "35012968", "url": "https://en.wikipedia.org/wiki?curid=35012968", "title": "Tapologo", "text": "Tapologo\n\nTapologo is a 2007 documentary film.\n\nIn Freedom Park, a squatter settlement in South Africa, a group of HIV-infected former sex-workers, created a network called Tapologo. They learned to nurse their community, transforming degradation into solidarity and squalor into hope. Catholic bishop Kevin Dowling participates in Tapologo, and raises doubts on the official doctrine of the Catholic Church regarding AIDS and sexuality in the African context.\n"}
{"id": "32436435", "url": "https://en.wikipedia.org/wiki?curid=32436435", "title": "Tornwaldt cyst", "text": "Tornwaldt cyst\n\nA Tornwaldt cyst is a benign cyst located in the upper posterior nasopharynx. It can be seen on computed tomography (CT) or magnetic resonance imaging (MRI) of the head as a well-circumscribed round mass lying in the midline. In most cases, treatment is not necessary. It was first described by Gustav Ludwig Tornwaldt.\n\n\n"}
{"id": "27881764", "url": "https://en.wikipedia.org/wiki?curid=27881764", "title": "Wheelchair trainer", "text": "Wheelchair trainer\n\nA wheelchair trainer or wheelchair treadmill is an apparatus that allows a manual wheelchair user to simulate linear (translational) travel while remaining stationary in a manner similar to an ambulatory person walking or running on a treadmill or a cyclist pedaling a bicycle on a bicycle trainer. The rear wheelchair wheels are placed in contact with vertical or horizontal rollers which may also be attached to flywheels, mechanical resistance or braking mechanisms, motors and various speed and force sensors. Flywheels may be sized to provide a user of a certain mass with a rotational inertia equivalent to their translational (linear) inertia in order to more realistically approximate actual wheelchair propulsion.\n\nWheelchair trainers having independent contact rollers permit simulated directional travel (omnidirectional treadmill). Trainers may also incorporate rotary encoders, accelerometers and torque sensors to enable interface with computer data acquisition systems (DAQ) for analysis of propulsion kinematics. A quadrature rotary encoder or hall effect sensor can be implemented to provide sufficient speed and direction information to enable virtual navigation interface with video games in a manner similar to using a joystick or gaming console. Calculation of rolling resistance between the tire & contact roller interface, axle friction, and inertial characteristics of wheelchair wheels and flywheels may be used in determination of stationary propulsion dynamics.\n\nThe United States Department of Veterans Affairs (VA) has substantially invested in decades long research and development for two wheelchair trainer devices: the Wheelchair Aerobic Fitness Trainer (WAFT), and GameWheels. The history of wheelchair trainer development may be summarized from the 10 patents issued by the United States Patent and Trademark Office for stationary wheelchair trainers/treadmills/ergometers/dynamometer/simulators issued from 1980 to 2009.\n\nThe last patent is being commercialized under the trademark Trekease, designed to serve as an acronym for Translational & Rotational Equivalent Kinetic Energy Aerobic Stationary Exertainment and as a homonym for Trekkies – fans of Star Trek. None of the other cited patents, including the experimental prototypes developed by the VA, are currently being commercialized; however simple unidirectional ramp and roller systems similar in design to patent #4,966,362 are being marketed by others. (See also External links).\n\nArcade game software and clinical data acquisition use were first introduced by the Veterans Administration's WAFT as a means of promoting stationary wheelchair propulsion as a beneficial aerobic exercise. Clinical professionals are not currently in agreement regarding the cardiovascular health benefits associated with manual wheelchair propulsion and the possible long term repetitive use injuries attributed to manual wheelchair operation. These debates have encouraged developments to enhance wheelchair seating, back support, frame, wheel, and hand-rim designs. Innovative lever styled mechanisms add a new level to improve the overall efficiency, posture and ergonomics of manual wheelchair propulsion. Utilizing lever propulsion technologies on a wheelchair trainer equipped with flywheel and resistance enables one to engage in an activity similar to rowing with all its associated health benefits and risks.\n\nDynamics:\nAerobics:\n\nErgonomics:\n\nCitations\n\nConferences\nWeb pages\n\n"}
{"id": "47759880", "url": "https://en.wikipedia.org/wiki?curid=47759880", "title": "Women's health in Ethiopia", "text": "Women's health in Ethiopia\n\nWomen's health in Ethiopia can be broken down into several sections: general health status, women’s status, maternal health, women and HIV, harmful traditional practices, and violence against women.\n\nEthiopia is the oldest independent and second most-populous country in Africa. A projection from the 2007 census, it has a total of 90 million inhabitants in 2015 (CSA 2015).\n\nThe sex ratio between male and female is almost equal; women in the reproductive age group constitute 23.4% of the population. The total fertility rate declined from 5.5 in 2000 to 4.1 in 2014.\n\nThe average life expectancy for an Ethiopian has increased from 45 years in 1990 to 64 years in 2014, which is higher than the African average (58 years) but lower than the global average of 70 years. This makes Ethiopia one of the six countries that made top individual gains since 1990. The achievement is attributed to the dramatic drop in the under five mortality (U5MR) and an improvement in other socio-economic determinants of health. The U5MR has dropped from 204/1000 live births in 1990 to 64/1000 live births in 2013, the target for achieving MDG4 being 67/1000 live births. Thus Ethiopian has achieved the MDG 4 by the year 2013 well ahead of the proposed time for 2015.\n\nIn Ethiopia, women of reproductive age constitute about a quarter of the entire population of 90 million. Around 80% of the labor force is engaged in agriculture, and 84% live in rural parts of the country. Poverty is multidimensional, and its impact is different on men and women. 43% of rural women aged 10 years and above are economically active, mostly in agriculture.\n\nGood progress has been registered in maternal mortality reduction, a 69% reduction according to UN estimates from the 1400/100000 live births in 1990 to 420/100,000 live births in 2013.\nMaternal wellbeing is crucial for the nations development. Majority of maternal deaths occur in the peripartum period. Deliveries attended by a skilled health care provider were shown to improve both maternal and neonatal survival. In Ethiopian most of the deliveries occur at home and unattended by skilled provider\n\nHIV Sero – prevalence in adults aged between 15–49 years in Ethiopia is estimated at 1.5% [5.5% urban, 0.7% rural]. However, the prevalence in women is about double than the prevalence in men, 1.9% versus 1%, respectively.\n\nSeventy four percent of women had a FGM out of which, 6% have their vaginas sewn closed. Around 8% of all marriages occur after through abduction.\n\nThe most common forms of violence against women in Ethiopia include physical violence (intimate partner violence) and sexual violence. A study done in Kofele, (Oromia region, Arsi, Ethiopia) in 2004 showed the lifetime and the 12 months prevalence of physical violence among married women was 64% and 55%, respectively. Another study performed in North-west Ethiopia (Dabat High school) in 2005 revealed that sexual harassment was seen in 44% of the students and rape was experienced by 33.3% of sexually active students. A tertiary level of education the sexual violence was also alarmingly high as this was also confirmed by a study conducted at Addis Ababa University in 2004 on more than 600 students, where sexual harassment was experienced by 58% in life time and 41.8% in the last one year, rape was experienced by 12.7% of the students, and 27.5% of the student had escaped an attempted rape.\n"}
{"id": "41096270", "url": "https://en.wikipedia.org/wiki?curid=41096270", "title": "Your Health Idaho", "text": "Your Health Idaho\n\nYour Health Idaho, sometimes known as the Idaho Health Insurance Exchange, is the health insurance marketplace for the U.S. state of Idaho. The exchange enables people and small businesses to purchase health insurance at federally subsidized rates.\n\nHealth insurance exchanges were established as a part of the 2010 Patient Protection and Affordable Care Act to enable individuals to purchase health insurance in state-run marketplaces. In this legislation, states could choose to establish their own health insurance exchanges; if they choose not to do so, the federal government would run one for the state.\n\nIn 2014, Healthcare.gov will be responsible for enrolling consumers in on-exchange health plans in the state of Idaho.\n\n4 insurance companies are participating in the Idaho Health Insurance Exchange in 2014:\n\n\n"}
{"id": "55931200", "url": "https://en.wikipedia.org/wiki?curid=55931200", "title": "Římov Reservoir", "text": "Římov Reservoir\n\nŘímov Reservoir is a lake in České Budějovice District.\n\n"}
