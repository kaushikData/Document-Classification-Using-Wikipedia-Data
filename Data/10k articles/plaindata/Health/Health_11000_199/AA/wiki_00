{"id": "1937", "url": "https://en.wikipedia.org/wiki?curid=1937", "title": "Alexander Fleming", "text": "Alexander Fleming\n\nSir Alexander Fleming (6 August 1881 – 11 March 1955) was a Scottish physician, microbiologist, and pharmacologist. His best-known discoveries are the enzyme lysozyme in 1923 and the world's first antibiotic substance benzylpenicillin (Penicillin G) from the mould \"Penicillium notatum\" in 1928, for which he shared the Nobel Prize in Physiology or Medicine in 1945 with Howard Florey and Ernst Boris Chain. He wrote many articles on bacteriology, immunology, and chemotherapy.\n\nFleming was knighted for his scientific achievements in 1944. In 1999, he was named in \"Time\" magazine's list of the . In 2002, he was chosen in the BBC's television poll for determining the 100 Greatest Britons, and in 2009, he was also voted third \"greatest Scot\" in an opinion poll conducted by STV, behind only Robert Burns and William Wallace.\n\nBorn on 6 August 1881 at Lochfield farm near Darvel, in Ayrshire, Scotland, Alexander was the third of four children of farmer Hugh Fleming (1816–1888) from his second marriage to Grace Stirling Morton (1848–1928), the daughter of a neighbouring farmer. Hugh Fleming had four surviving children from his first marriage. He was 59 at the time of his second marriage, and died when Alexander was seven.\n\nFleming went to Loudoun Moor School and Darvel School, and earned a two-year scholarship to Kilmarnock Academy before moving to London, where he attended the Royal Polytechnic Institution. After working in a shipping office for four years, the twenty-year-old Alexander Fleming inherited some money from an uncle, John Fleming. His elder brother, Tom, was already a physician and suggested to him that he should follow the same career, and so in 1903, the younger Alexander enrolled at St Mary's Hospital Medical School in Paddington; he qualified with an MBBS degree from the school with distinction in 1906.\n\nFleming had been a private in the London Scottish Regiment of the Volunteer Force since 1900, and had been a member of the rifle club at the medical school. The captain of the club, wishing to retain Fleming in the team, suggested that he join the research department at St Mary's, where he became assistant bacteriologist to Sir Almroth Wright, a pioneer in vaccine therapy and immunology. In 1908, he gained a BSc degree with Gold Medal in Bacteriology, and became a lecturer at St Mary's until 1914.\nFleming served throughout World War I as a captain in the Royal Army Medical Corps, and was Mentioned in Dispatches. He and many of his colleagues worked in battlefield hospitals at the Western Front in France. In 1918 he returned to St Mary's Hospital, where he was elected Professor of Bacteriology of the University of London in 1928. In 1951 he was elected the Rector of the University of Edinburgh for a term of three years.\n\nDuring World War I, Fleming witnessed the death of many soldiers from sepsis resulting from infected wounds. Antiseptics, which were used at the time to treat infected wounds, often worsened the injuries. In an article he submitted for the medical journal \"The Lancet\" during World War I, Fleming described an ingenious experiment, which he was able to conduct as a result of his own glass blowing skills, in which he explained why antiseptics were killing more soldiers than infection itself during World War I. Antiseptics worked well on the surface, but deep wounds tended to shelter anaerobic bacteria from the antiseptic agent, and antiseptics seemed to remove beneficial agents produced that protected the patients in these cases at least as well as they removed bacteria, and did nothing to remove the bacteria that were out of reach. Sir Almroth Wright strongly supported Fleming's findings, but despite this, most army physicians over the course of the war continued to use antiseptics even in cases where this worsened the condition of the patients.\n\nAt St Mary's Hospital Fleming continued his investigations into antibacterial substances. Testing the nasal secretions from a patient with a heavy cold, he found that nasal mucus had an inhibitory effect on bacterial growth. This was the first recorded discovery of lysozyme, an enzyme present in many secretions including tears, saliva, skin, hair and nails as well as mucus. Although he was able to obtain larger amounts of lysozyme from egg whites, the enzyme was only effective against small counts of harmless bacteria, and therefore had little therapeutic potential.\n\nBy 1927, Fleming had been investigating the properties of staphylococci. He was already well-known from his earlier work, and had developed a reputation as a brilliant researcher, but his laboratory was often untidy. On 3 September 1928, Fleming returned to his laboratory having spent August on holiday with his family. Before leaving, he had stacked all his cultures of staphylococci on a bench in a corner of his laboratory. On returning, Fleming noticed that one culture was contaminated with a fungus, and that the colonies of staphylococci immediately surrounding the fungus had been destroyed, whereas other staphylococci colonies farther away were normal, famously remarking \"That's funny\". Fleming showed the contaminated culture to his former assistant Merlin Price, who reminded him, \"That's how you discovered lysozyme.\" Fleming grew the mould in a pure culture and found that it produced a substance that killed a number of disease-causing bacteria. He identified the mould as being from the genus \"Penicillium\", and, after some months of calling it \"mould juice\", named the substance it released \"penicillin\" on 7 March 1929. The laboratory in which Fleming discovered and tested penicillin is preserved as the Alexander Fleming Laboratory Museum in St. Mary's Hospital, Paddington.\n\nHe investigated its positive anti-bacterial effect on many organisms, and noticed that it affected bacteria such as staphylococci and many other Gram-positive pathogens that cause scarlet fever, pneumonia, meningitis and diphtheria, but not typhoid fever or paratyphoid fever, which are caused by Gram-negative bacteria, for which he was seeking a cure at the time. It also affected \"Neisseria gonorrhoeae,\" which causes gonorrhoea, although this bacterium is Gram-negative.\n\nFleming published his discovery in 1929, in the British \"Journal of Experimental Pathology,\" but little attention was paid to his article. Fleming continued his investigations, but found that cultivating \"Penicillium\" was quite difficult, and that after having grown the mould, it was even more difficult to isolate the antibiotic agent. Fleming's impression was that because of the problem of producing it in quantity, and because its action appeared to be rather slow, penicillin would not be important in treating infection. Fleming also became convinced that penicillin would not last long enough in the human body (\"in vivo\") to kill bacteria effectively. Many clinical tests were inconclusive, probably because it had been used as a surface antiseptic. In the 1930s, Fleming's trials occasionally showed more promise, but Fleming largely abandoned penicillin work, leaving Howard Florey and Ernst Boris Chain at the Radcliffe Infirmary in Oxford to take up research to mass-produce it, with funds from the U.S. and British governments. They started mass production after the bombing of Pearl Harbor. By D-Day in 1944, enough penicillin had been produced to treat all the wounded in the Allied forces.\n\nIn Oxford, Ernst Boris Chain and Edward Abraham were studying the molecular structure of the antibiotic. Abraham was the first to propose the correct structure of penicillin. Shortly after the team published its first results in 1940, Fleming telephoned Howard Florey, Chain's head of department, to say that he would be visiting within the next few days. When Chain heard that Fleming was coming, he remarked \"Good God! I thought he was dead.\"\n\nNorman Heatley suggested transferring the active ingredient of penicillin back into water by changing its acidity. This produced enough of the drug to begin testing on animals. There were many more people involved in the Oxford team, and at one point the entire Dunn School was involved in its production.\n\nAfter the team had developed a method of purifying penicillin to an effective first stable form in 1940, several clinical trials ensued, and their amazing success inspired the team to develop methods for mass production and mass distribution in 1945.\n\nFleming was modest about his part in the development of penicillin, describing his fame as the \"Fleming Myth\" and he praised Florey and Chain for transforming the laboratory curiosity into a practical drug. Fleming was the first to discover the properties of the active substance, giving him the privilege of naming it: penicillin. He also kept, grew, and distributed the original mould for twelve years, and continued until 1940 to try to get help from any chemist who had enough skill to make penicillin. But Sir Henry Harris said in 1998: \"Without Fleming, no Chain; without Chain, no Florey; without Florey, no Heatley; without Heatley, no penicillin.\"\n\nFleming's accidental discovery and isolation of penicillin in September 1928 marks the start of modern antibiotics. Before that, several scientists had published or pointed out that mould or \"Penicillium sp.\" were able to inhibit bacterial growth, and even to cure bacterial infections in animals. Ernest Duchesne in 1897 in his thesis \"Contribution to the study of vital competition in micro-organisms: antagonism between moulds and microbes\", or also Clodomiro Picado Twight whose work at the Institut Pasteur in 1923 on the inhibiting action of fungi of the \"Penicillin sp.\" genre in the growth of staphylococci drew little interest from the directors of the Institut at the time. Fleming was the first to push these studies further by isolating the penicillin, and by being motivated enough to promote his discovery at a larger scale.\n\nFleming also discovered very early that bacteria developed antibiotic resistance whenever too little penicillin was used or when it was used for too short a period. Almroth Wright had predicted antibiotic resistance even before it was noticed during experiments. Fleming cautioned about the use of penicillin in his many speeches around the world. On 26 June 1945, he made the following cautionary statements: \"the microbes are educated to resist penicillin and a host of penicillin-fast organisms is bred out ... In such cases the thoughtless person playing with penicillin is morally responsible for the death of the man who finally succumbs to infection with the penicillin-resistant organism. I hope this evil can be averted.\" He cautioned not to use penicillin unless there was a properly diagnosed reason for it to be used, and that if it were used, never to use too little, or for too short a period, since these are the circumstances under which bacterial resistance to antibiotics develops.\n\nThe popular story of Winston Churchill's father paying for Fleming's education after Fleming's father saved young Winston from death is false. According to the biography, \"Penicillin Man: Alexander Fleming and the Antibiotic Revolution\" by Kevin Brown, Alexander Fleming, in a letter to his friend and colleague Andre Gratia, described this as \"A wondrous fable.\" Nor did he save Winston Churchill himself during World War II. Churchill was saved by Lord Moran, using sulphonamides, since he had no experience with penicillin, when Churchill fell ill in Carthage in Tunisia in 1943. \"The Daily Telegraph\" and \"The Morning Post\" on 21 December 1943 wrote that he had been saved by penicillin. He was saved by the new sulphonamide drug Sulphapyridine, known at the time under the research code M&B 693, discovered and produced by May & Baker Ltd, Dagenham, Essex – a subsidiary of the French group Rhône-Poulenc. In a subsequent radio broadcast, Churchill referred to the new drug as \"This admirable M&B\". It is highly probable that the correct information about the sulphonamide did not reach the newspapers because, since the original sulphonamide antibacterial, Prontosil, had been a discovery by the German laboratory Bayer, and as Britain was at war with Germany at the time, it was thought better to raise British morale by associating Churchill's cure with a British discovery, penicillin.\n\nFleming's discovery of penicillin changed the world of modern medicine by introducing the age of useful antibiotics; penicillin has saved, and is still saving, millions of people around the world.\n\nThe laboratory at St Mary's Hospital where Fleming discovered penicillin is home to the Fleming Museum, a popular London attraction. His alma mater, St Mary's Hospital Medical School, merged with Imperial College London in 1988. The \"Sir Alexander Fleming Building\" on the South Kensington campus was opened in 1998, where his son Robert and his great granddaughter Claire were presented to the Queen; it is now one of the main preclinical teaching sites of the Imperial College School of Medicine.\n\nHis other alma mater, the Royal Polytechnic Institution (now the University of Westminster) has named one of its student halls of residence \"Alexander Fleming House\", which is near to Old Street.\n\nOn 24 December 1915, Fleming married a trained nurse, Sarah Marion McElroy of Killala, County Mayo, Ireland. Their only child, Robert Fleming (1924–2015), became a general medical practitioner. After his first wife's death in 1949, Fleming married Dr. Amalia Koutsouri-Vourekas, a Greek colleague at St. Mary's, on 9 April 1953; she died in 1986.\n\nFrom 1921 until his death in 1955, Fleming owned a country home in Barton Mills, Suffolk.\n\nOn 11 March 1955, Fleming died at his home in London of a heart attack. He is buried in St Paul's Cathedral.\n\n\n\n"}
{"id": "5245841", "url": "https://en.wikipedia.org/wiki?curid=5245841", "title": "Allostatic load", "text": "Allostatic load\n\nAllostatic load is \"the wear and tear on the body\" that accumulates as an individual is exposed to repeated or chronic stress. The term was coined by McEwen and Stellar in 1993. It represents the physiological consequences of chronic exposure to fluctuating or heightened neural or neuroendocrine response that results from repeated or prolonged chronic stress.\n\nThe regulatory model of allostasis claims that the brain's primary role as an organ is the predictive regulation or the stabilisation of internal sensations. Allostasis involves the regulation of homeostasis in the body to decrease physiological consequences on the body. Predictive regulation refers to the brain's ability to anticipate needs and prepare to fulfill them before they arise. Therefore, in this model, the brain is responsible for efficient stimuli regulation.\n\nPart of efficient regulation is the reduction of uncertainty. Humans naturally do not like feeling as if surprise is inevitable. Because of this, we constantly strive to reduce the uncertainty of future outcomes, and allostasis helps us do this by anticipating needs and planning how to satisfy them ahead of time. But it takes a significant amount of the brain's energy to do this, and if it fails to resolve the uncertainty, the situation may become chronic and result in the experience of \"allostatic load\".\n\nThe concept of allostatic load provides that \"the neuroendocrine, cardiovascular, neuroenergetic, and emotional responses become persistently activated so that blood flow turbulences in the coronary and cerebral arteries, high blood pressure, atherogenesis, cognitive dysfunction and depressed mood accelerate disease progression.\" In other words, all of the long-standing effects of continuously activated stress responses are referred to as allostatic load. And allostatic load can even result in permanently altered brain architecture and systemic pathophysiology.\n\nFurther, as a result of these physical effects, allostatic load also minimizes an organism's ability to cope with and reduce uncertainty in the future, which cements the entire cycle. There are direct and Indirect effects on health resulting in a higher allostatic load.\n\nAllostatic load is generally measured through a composite index of indicators of cumulative strain on several organs and tissues, primarily biomarkers associated with the neuroendocrine, cardiovascular, immune and metabolic systems.\n\nIndices of allostatic load are diverse across studies and are frequently assessed differently, using different biomarkers and different methods of assembling an allostatic load index. Allostatic load is not unique to humans and may be used to evaluate the physiological effects of chronic or frequent stress in non-human primates as well.\n\nIn the endocrine system, the increase or repeated levels of stress results in the increased levels of the hormone Corticotropin-Releasing Factor (CRH), which is associated with activation of HPA axis. HPA axis is the central stress response system which is responsible for modulating inflammatory responses that occur throughout the body. The prolonged stress levels can also lead to decreased levels of cortisol in the morning and increased levels in the afternoon, leading to greater daily output of cortisol which in the long term increases blood sugar levels.\n\nIn the nervous system, structural and functional abnormalities are a result of chronic prolonged stress. The increase of stress levels causes a shortening of dendrites in a neuron. Therefore, the shortening of dendrites causes the decrease in attention. Chronic stress also causes greater response to fear of the unlearned in the nervous system, and fear conditioning.\n\nIn the immune system, the increase in levels of chronic stress results in the elevation of inflammation. The increase in inflammation levels is caused by the ongoing activation of the sympathetic nervous system. The impairment of cell-mediated acquired immunity is also a factor resulting in the immune system due to chronic stress.\n\nThe greatest contribution to the allostatic load is the effects of stress on the brain. Allostasis. therefore, is the systems in the body that help achieve homeostasis. Homeostasis is the regulation of physiological processes. The systems in the body respond to the state of the body and also to the external environment. The relationship between allostasis and allostatic load is the concept of anticipation. Anticipation can drive the output of mediators. Examples of mediators include hormones and cortisol. Excess amounts of such mediators will result in an increase in allostatic load, contributing to anxiety and anticipation. \n\nAnother relationship between allostasis and allostatic load are the health-damaging and health-promoting behaviours which contribute to allostatic load. Theses behaviours include cigarette smoking, consumption of alcohol, poor diet and physical inactivity. \n\nThere are three physiological processes which cause an increase in allostatic load, these include: \n\n\nThe importance of homeostasis, therefore, is to regulate the stress levels encountered on the body to reduce allostatic load. \n\nThe effects of these forms of dysfunctional allostasis cause increased allostatic load and may, over time, lead to the development of disease, sometimes with decompensation of the allostatically controlled problem. Allostatic load effects can be measured in the body. When tabulated in the form of allostatic load indices using sophisticated analytical methods, it gives an indication of cumulative lifetime effects of all types of stress on the body.\n\nTo reduce and manage high Allostatic Load, an individual should pay attention to structural and behavioural factors. Structural factors include the social environment, and access to health services. Behavioural factors include diet, physical health and tobacco smoking, which can lead to chronic disease. Actions such as tobacco smoking are brought about from the stress levels that an individual experiences. Therefore, controlling stress levels from the beginning, for example by not leading to tobacco smoking, will reduce the chance of chronic disease development and high allostatic load. \n\nLow SES (socio-economic status) effects allostatic load significantly and therefore, focusing on the causes of low SES will reduce allostatic load levels. Societal polarisation, material deprivation, and psychological demands on health should be reduced to manage allostatic load. The increased support from the community and the social environment will manage high allostatic load. A way to reduce and manage high allostatic load is to empower financial help from the government. Empowerment ensures the management of allostatic load and improve health by allowing people to gain control and improve their psychological health. Therefore, the improvement of inequalities in health will increase the stress levels and improve health, while reducing the chances of high allostatic load on the body. \n\nInterventions can include encouraging sleep quality and quantity, social support, self-esteem and wellbeing, improving diet, avoiding alcohol or drug consumption and participating in physical activity. Providing cleaner and safer environments and the incentive towards a higher education will reduce the chance of stress and improve mental health significantly, therefore, reducing the onset of high allostatic load.\n\nAllostatic load differs by sex and age, and the social status of an individual. Protective factors could, at various times of an individual's life span, be implemented to reduce stress and, in the long run, eliminate the onset of allostatic load. Protective factors include parental bonding, education, social support, health workplaces, and a sense of meaning towards life and choices being made.\n\n"}
{"id": "1448106", "url": "https://en.wikipedia.org/wiki?curid=1448106", "title": "Auditory neuropathy", "text": "Auditory neuropathy\n\nAuditory neuropathy (AN) is a variety of hearing loss in which the outer hair cells within the cochlea are present and functional, but sound information is not faithfully transmitted to the auditory nerve and brain properly. Also known as auditory neuropathy/auditory dys-synchrony (AN/AD) or auditory neuropathy spectrum disorder (ANSD).\n\nA neuropathy usually refers to a disease of the peripheral nerve or nerves, but the auditory nerve itself is not always affected in auditory neuropathy spectrum disorders.\n\nBased on clinical testing of subjects with auditory neuropathy, the disruption in the stream of sound information has been localized to one or more of three probable locations: the inner hair cells of the cochlea, the synapse between the inner hair cells and the auditory nerve, or a lesion of the ascending auditory nerve itself.\n\nDiagnosis is possible after a test battery, that must necessarily include the following: the auditory brainstem response and otoacoustic emissions. Auditory brainstem response should be tested with both polarities (helps in identifying cochlear microphonics).\n\nAuditory neuropathy is diagnosed when a person has present otoacoustic emissions and/or cochlear microphonics in combination with absent or abnormal auditory brainstem response. Changing the polarity of the stimulus will result in an inversion of the cochlear microphonic. Patients with auditory neuropathy spectrum disorders have to date never been shown to have normal middle ear muscle reflexes at 95 dB HL or less despite having normal otoacoustic emissions.\nAuditory neuropathy can occur spontaneously, or in combination with diseases like Charcot-Marie-Tooth disease and Friedreich's ataxia.\n\nIt appears that regardless of the audiometric pattern (hearing thresholds) or of their function on traditional speech testing in quiet the vast majority of sufferers have very poor hearing in background noise situations.\n\nWhen testing the auditory system, there really is no characteristic presentation on the audiogram.\n\nWhen diagnosing someone with auditory neuropathy, there is no characteristic level of functioning either. People can present relatively little dysfunction other than problems of hearing speech in noise, or can present as completely deaf and gaining no useful information from auditory signals.\n\nHearing aids are sometimes prescribed, with mixed success.\n\nSome people with auditory neuropathy obtain cochlear implants, also with mixed success.\n\nUniversal Newborn Hearing Screenings (UNHS) is mandated in a majority of the United States. Auditory neuropathy is sometimes difficult to catch right away, even with these precautions in place. Parental suspicion of a hearing loss is a trustworthy screening tool for hearing loss, too; if it is suspected, that is sufficient reason to seek a hearing evaluation from an audiologist.\n\nIn most parts of Australia, hearing screening via AABR testing is mandated, meaning that essentially all congenital (i.e., not those related to later onset degenerative disorders) auditory neuropathy cases should be diagnosed at birth.\n\n\n\n"}
{"id": "7010827", "url": "https://en.wikipedia.org/wiki?curid=7010827", "title": "Auxiliary label", "text": "Auxiliary label\n\nAn auxiliary label or cautionary and advisory label (CAL) is a label added on to a dispensed medication package by a pharmacist in addition to the usual prescription label. These labels are intended to provide supplementary information regarding safe administration, use, and storage of the medication.\n\nAuxiliary labels are generally small stickers consisting of a pictogram and one or more lines of text intended to enhance patient knowledge. Effectiveness of such labels depends on the number of labels, design of the label, and their position on the medication package or vial. Simplifying auxiliary labels can improve patient comprehension. Auxiliary label information can enhance but does not replace verbal counselling of the patient by the pharmacist. Auxiliary labels became popular during the second half of the nineteenth century. Deciding what auxiliary labels are suitable for a particular prescription requires knowledge of the drug's classification, interactions, and side effects.\n\n"}
{"id": "2260904", "url": "https://en.wikipedia.org/wiki?curid=2260904", "title": "Bloodshot (comics)", "text": "Bloodshot (comics)\n\nBloodshot is a fictional comic book superhero appearing in books published by the American publisher Valiant Comics. The character was created by Kevin VanHook, Don Perlin and Bob Layton.\n\n\"Bloodshot\" was created in 1992 by Kevin VanHook, Don Perlin, and Bob Layton, with the name being suggested by David Chlystek, during a wave of popularity for the Valiant Universe and became an immediate hit with readers, with the first issue ranking number 4 on the Diamond Comic Distributors Top 100 for November 1992. As one of the most popular Valiant characters, Bloodshot had a continuing run of success, selling millions of copies until Valiant was purchased out for $65 million by Acclaim Entertainment. The comics have been translated into a number of languages including French, German, Italian, Spanish, Norwegian, Filipino and Chinese, Turkish, among others. In 1996, Bloodshot and a majority of other Valiant Universe characters were re-booted under the banner of Acclaim Comics. The re-launch saw the title moving in a new direction which could be more easily adaptable to video games. Valiant Entertainment is the current owner of the Valiant Character Catalog.\n\nA new volume of \"Bloodshot\" was released on July 2012, written by Duane Swierczynski, following Valiant Comics's relaunch. A new volume of \"Bloodshot\" was released on September 9, 2015, titled \"Bloodshot: Reborn\".\n\nBloodshot is a former soldier with powers of regeneration and meta-morphing made possible through nanites injected into his blood. After having his memory wiped numerous times, Bloodshot is out to discover who he really is and get vengeance on those who did this to him. Bloodshot's bloodstream contains a billion nanocomputers, enabling him to heal from injuries quickly, interface with technology, and shape shift his mass.\n\nProject Rising Spirit's program to create the ultimate soldier stretches back decades, and reached critical mass in the depths of World War II. Though the initial products of the program were crudely enhanced and highly expendable foot soldiers suitable only for the simplest scenarios, these early subjects were able to complete high-intensity missions regardless of any obstacles. Over the decades, these results ensured that the \"Bloodshot\" program was consistently earmarked for future development and continued to break new ground.\n\nThroughout the 1970s, '80s, and '90s, P.R.S. redefined the nanotechnology that would power Bloodshot, and develop a new artificial intelligence system that allowed him to be deployed in a wider array of off-the-battlefield counterintelligence and espionage missions. While Bloodshot was self-aware to a degree, the human-machine hybrid was still deemed to inflict unacceptably high levels of collateral damage. In 2007, another P.R.S. scientist, Dr. Emmanuel Kuretich, discovered his employers' intention to redirect the Bloodshot program in to a new area: the targeting and capture of the extremely rare, psionically powered children called \"psiots.\" Alarmed, Kuretich fled P.R.S. for Toyo Harada's Harbinger Foundation, where he secretly began forging a plan to undo P.R.S.\n\nSeveral years later, while on a routine mission in Afghanistan, Bloodshot is captured by Kuretich, who forcibly extracts the records of Bloodshot's missions with the intent of exposing Project Rising Spirit to the world. The process, however, unlocks all of Bloodshot's false memories, which had been used to motivate him in war zones throughout the world, all at once.\n\nP.R.S., fearing that Bloodshot has finally gone rogue, decides to rein him in. Bloodshot escapes capture with the aid of an ambulance driver named Kara Murphy before finally returning to P.R.S. to uncover the truth behind his identity. Tipped off by Kuretich, Bloodshot storms a P.R.S. facility in Nevada, but finds only the sub-basement facility known as the Nursery. Built to house the empowered psiot children captured by Bloodshot, freeing the Nursery's captives has been Kuretich and Harada's true objective all along. Bloodshot is forced to combat the Nursery's sadistic psiot jailer Gamma and P.R.S.'s outdated cyborg hit squad Chainsaw. Bloodshot frees the captive children, including generation Zero, an action that inadvertently sets off a chain reaction of events that would culminate in the Harbinger Wars in the Las Vegas Strip.\n\nConfronted by Toyo Harada in the aftermath of the P.R.S. breakout, Bloodshot's capacity for free will is overridden by the \"Harada Protocol\", a piece of hidden P.R.S. programming that compels him to terminate Harada. A vicious battle ensues, which leaves Harada seriously wounded, and Bloodshot with a depleted nanite count.\n\nIn the closing moments of the Harbinger wars, Bloodshot again falls into Harada's clutches, and spends several weeks as a prisoner of the Harbinger Foundation. During this time, Harada subjects Bloodshot to cruel and painful experiments as he attempts to solve the secret behind the incredibly rare nanites in Bloodshot's bloodstream.\n\nHowever, Project Rising Spirit quickly expedites a rescue led by the H.A.R.D. Corps. After storming Harada's El Segundo facility, they successfully extract Bloodshot, who shortly thereafter accepts a spot on the H.A.R.D. Corps roster. Bloodshot is rewarded with a hard copy of the P.R.S. file on his true identity.\n\nBloodshot is deployed across the globe on various missions alongside the H.A.R.D. Corps, including as assignment to retrieve a former P.R.S. test subject codenamed Prodigal and his supposedly \"immortal\" bodyguard.\n\nVolume One and Volume Two of \"Bloodshot\" keeps the same core story elements with different settings and plots.\n\nAngelo Mortalli has become the ultimate killing machine. His memories have been erased and his blood has been infused with microscopic computers called nanites. These nanites allow him to heal wounds quickly, dominate electronic devices, and fully control every aspect of his body to maximize his physical capabilities. A modern-day Frankenstein, he wages a one-man war, taking out the mob, the police and his covert government creators, in his struggle to find out who he was and what he has become.\n\nAngelo Mortalli, a.k.a. Bloodshot\n\nMortalli is a ruthless killer climbing the mob ranks when his crime family betrays him and has him framed for murder. He goes into federal witness protection but is betrayed by one of the FBI members guarding him. He is then kidnapped and taken to be part of an experimental procedure known as \"Project Rising Spirit\". His body is injected with microscopic computers called nanites. The nanites quickly go to work, rebuilding his brain and then the rest of his body, unexpectedly reviving Angelo but erasing his memory in the process. The nanites now fully control Angelo’s body (manipulating blood flow, adrenaline levels etc.) giving him augmented reflexes and strength, enhanced hearing; the ability to heal rapidly, and control of electronic devices. Angry, violent and unsure of what he has become, he escapes. Taking the name Bloodshot, he begins the process of piecing together who he was.\n\nDon Gino Canelli\n\nAngelo Mortalli’s mafia boss and his girlfriend’s father is Gino, a ruthless man who will do anything to protect his family. He treats Angelo like a son. When he learns that Angelo has been cheating on his daughter, he orders Angelo set up for the murder of a rival without a second thought.\n\nHideyoshi Iwatsu\n\nIwatsu is an old and dangerous Japanese scientist who created the Bloodshot procedure. He is extremely immoral, using unwilling “volunteers” for his experiments, most of whom die as a result.\n\nTanaka\n\nTanaka is Hideyoshi Iwatsu’s aide and right-hand man. More trusted by Hideyoshi than he does his own son, Tanaka is a brilliant manipulator. His arrogance blinds him, however, and leads to stupid mistakes on his part.\n\nAngelo Mortalli has been subjected against his will to a scientific experiment that has remade him into the ultimate killing machine, an engine of destruction codenamed Bloodshot. But, in the process, his memories were lost. His creators mean to use him as a weapon, an unstoppable assassin. But he breaks free and escapes. In this rendition, the writer Len Kaminski elaborates on BloodShot's nano technology. They turn out to be like sentient beings that see him as their God and provoke him to self-induce a trance-like state using a mixture of substances in order to communicate with him directly. Deadly operatives of the super-secret government agency that sponsored his creation hunt him down. Following a broken trail of fragmentary recollections, Bloodshot fights a running battle against them, the mob and the police.\n\nBloodshot\n\nThe first and only success of Project Lazarus, Bloodshot is a miracle of technology, far exceeding his creators’ expectations. Microscopic machines called nanites injected into his blood maximize his physicality, heal his wounds, extend his senses and imbue him with the ability to interface with any electronic device. In his former life as Angelo Mortalli, he learns, he was a rising star in a New York crime family…but the trail doesn’t end there. He was someone else before he was Mortalli. Who was Raymond Garrison?\n\nSimon Oreck\n\nThe Director in Chief of the Domestic Operations Authority (DOA), a covert operations arm of the United States government. An information trader, his business is knowledge and he has his eyes and ears in every branch of government – all of which is fed to him constantly by the wall of computers and monitors that fill his office. Despite usually preferring his department to remain behind the scenes, Oreck offers to commit DOA resources to helping the FBI extract information on the Cianelli crime family from the brain of recently deceased mobster Angelo Mortalli. But as always, Oreck is hiding his true motives.\n\nDoctor Frederick J. Stroheim\n\nSon of Nazi scientist Klaus Stroheim, the “Mengele of Dachau,” Frederick inherited his father’s genius and his utter inhumanity. Project Lazarus is his brainchild.\n\nGina DeCarlo\n\nIn the process of eliminating all evidence that Angelo Mortalli ever existed, D.O.A. operatives expunged all documentation, razed his home, and even had phone books reprinted without his name. He also discovers that they coldly murdered Gina, the love of his life, to silence her. As if he didn’t hate them enough already….\n\nThe Chainsaw\n\nKnown as the Special Circumstances Division, codename: The Chainsaw are a highly trained team of mercenaries that work for the Domestic Operations Authority (DOA). Utilizing a state-of-the-art group combat technology called Chainlink, the members of The Chainsaw are provided with instant data and communication tools that allow for perfect combat tactics and strategy. Each member is specialized in one area of covert warfare and utilizes the latest in DOA technology.\n\nMichael Pileggi\n\nA made man in the Cianelli crime family, Michael Pileggi is one of the few who know the truth behind Angelo Mortalli’s death. Don Cianelli had ordered Pileggi to fake a funeral for Mortalli in order to prevent the Don’s daughter and Mortalli’s girlfriend Gina from becoming suspicious. When he hears that someone has been asking questions about Mortalli and killing his men for answers, he becomes paranoid and holes up in his heavily guarded mansion.\n\nThere are 25 issues in this volume, including #0. A 2012 relaunch by Valiant Publishing: Bloodshot is a man searching for his identity. He is a weapon, one built for military destruction. Using false memories the government has motivated him to do things while believing to himself that he was protecting an imagined family. But no more. When the weapon awakens, it malfunctions thus beginning a war over the control of Bloodshot’s mind. Leaving a trail of destruction he breaks away from the military base he is being held at. A man marked with gray skin and a red mark on his chest, Bloodshot chooses to trust no one as he fights to figure out who he used to be, and what it is he has become. Series written by Duane Swierczynski. The series tied into the Harbinger Wars event with issues 10 through 13. Starting with issue 14, the series was retitled Bloodshot and Hard Corps, with the character as a member of the Hard Corps team. The series crossed over with Archer And Armstrong in a story arc titled \"Mission: Improbable\".\n\nGenerally, the characters published by Valiant inhabit a shared universe/continuity. Characters appear or are referenced in each other's books. For example, there is a very definite connection between Rai and Bloodshot. Bloodshot's silicon based, nanite powered blood flows through Takao Konishi's veins (the last Rai). It grants him some of Bloodshot's memories and all of Bloodshot's powers. The entire line of warriors who were known as Rai were created by Grandmother in the image of Bloodshot to honor his heroism.\n\nBloodshot is one of the best selling Valiant characters with total sales in all languages approaching seven million comics. Shortly before the debut of the Bloodshot series the title character made two introductory appearances in popular titles \"Rai\" and \"Eternal Warrior\". Based on these appearances there was a groundswell of demand for the character to return and high anticipation for the premier issue. \"Bloodshot\" #1 (February 1993) was a much anticipated comic that became a best selling issue and has gone on to sell approximately one million copies. The original series was written by Kevin VanHook and drawn by Don Perlin. The premiere issue featured the first \"Chromium\" comic book cover.\n\n\"Bloodshot\" #1 (July 2012) was awarded \"Best Comic\" by Diamond Comic Distributors (the American comics industry's leading distributor) and \"Best Innovation\" for its chromium cover. \"Bloodshot\" was named one of the Top Ten Comic-Book Series of 2012 by Nerdage.\n\nOn March 1, 2012, it was announced that Columbia Pictures had acquired the film rights to the Valiant Comics' \"Bloodshot\", which would be produced by Neal H. Moritz's Original Film along with Valiant Entertainment's Jason Kothari and Dinesh Shamdasani, with Jeff Wadlow hired to write the screenplay. On April 21, 2015, Valiant, Sony Pictures and Original Film announced a five-picture deal to bring the Valiant's heroes to the big screen, first two films would be of \"Bloodshot\", the next two would be of \"Harbinger\", and the last one would the crossover of both comics, titled \"Harbinger Wars\". Moritz, Tony Jaffe of Original Film would produce the film along with Dinesh Shamdasani of Valiant, from a co-script by Wadlow and Eric Heisserer. David Leitch and Chad Stahelski were hired to direct the film. In September 2016, Sony reported that the studio was first developing the film on \"Harbinger\" while the \"Bloodshot\" would be develop later, while in March 2017, Dave Wilson was confirmed as a director for the film. On July 25, 2017, \"Deadline\" reported that Jared Leto was in talks to play the villain character, Angelo Mortalli, in the film. On January 9, 2018, Vin Diesel is in talks to portray the character in a solo film.\n\n\n\nConsidered the most rare and sought after comic book published by Valiant comics is the platinum error edition of \"Bloodshot\" #0. It is rumored that fewer than 25 copies of the Platinum error covers exist, though it is assumed there were up to 300 based on interviews with staff members involved in the production process.\n\nThe 1993 series has begun being collected in hardcovers under the Valiant Masters banner:\nThe 2012 relaunch series has been collected into several volumes:\nAdditionally, the 2012 relaunch series has been collected into Deluxe Edition hardcovers:\n\nA new Bloodshot series was launched in 2015 titled \"Bloodshot Reborn\".\n\nA mini serie was launched in 2016 titled \"Bloodshot USA\". It's Reborn's Sequel.\n\n"}
{"id": "2061705", "url": "https://en.wikipedia.org/wiki?curid=2061705", "title": "Clinical audit", "text": "Clinical audit\n\nClinical audit is a process that has been defined as \"a quality improvement process that seeks to improve patient care and outcomes through systematic review of care against explicit criteria and the implementation of change\".\n\nThe key component of clinical audit is that performance is reviewed (or audited) to ensure that what you \"should\" be doing is \"being\" done, and if not it provides a framework to enable improvements to be made. It had been formally incorporated in the healthcare systems of a number of countries, for instance in 1993 into the United Kingdom's National Health Service (NHS), and within the NHS there is a clinical audit guidance group in the UK. .\n\nClinical audit comes under the Clinical Governance umbrella and forms part of the system for improving the standard of clinical practice.\n\nOne of first clinical audits was undertaken by Florence Nightingale during the Crimean War of 1853–55.\nOn arrival at the medical barracks hospital in Scutari in 1854, Nightingale was appalled by the unsanitary conditions and high mortality rates among injured or ill soldiers. She and her team of 38 nurses applied strict sanitary routines and standards of hygiene to the hospital and equipment; in addition, Nightingale had a talent for mathematics and statistics, and she and her staff kept meticulous records of the mortality rates among the hospital patients. Following these changes the mortality rates fell from 40% to 2%, and the results were instrumental in overcoming the resistance of the British doctors and officers to Nightingale's procedures. Her methodical approach, as well as the emphasis on uniformity and comparability of the results of health care, is recognised as one of the earliest programs of outcomes management.\n\nAnother notable figure who advocated clinical audit was Ernest Codman (1869–1940). Codman became known as the first true medical auditor following his work in 1912 on monitoring surgical outcomes. Codman's \"end result idea\" was to follow every patient's case history after surgery to identify errors made by individual surgeons on specific patients. Although his work is often neglected in the history of health care assessment, Codman's work anticipated contemporary approaches to quality monitoring and assurance, establishing accountability, and allocating and managing resources efficiently.\n\nWhilst Codman's 'clinical' approach is in contrast with Nightingale's more 'epidemiological' audits, these two methods serve to highlight the different methodologies that can be used in the process of improvement to patient outcome.\n\nDespite the successes of Nightingale in the Crimea and Codman in Massachusetts, clinical audit was slow to catch on. This situation was to remain for the next 130 or so years, with only a minority of healthcare staff embracing the process as a means of evaluating the quality of care delivered to patients.\n\nAs concepts of clinical audit have developed, so too have the definitions which sought to encapsulate and explain the idea. These changes generally reflect the movement away from the medico-centric views of the mid-Twentieth Century to the more multidisciplinary approach used in modern healthcare. It also reflects the change in focus from a professionally centred view of health provision to the view of the patient-centred approach. These changes can be seen from comparison of the following definitions.\n\nIn 1989, the White Paper, \"Working for patients\", saw the first move in the UK to standardise clinical audit as part of professional healthcare. The paper defined medical audit (as it was called then) as\n\"the systematic critical analysis of the quality of medical care including the procedures used for diagnosis and treatment, the use of resources and the resulting outcome and quality of life for the patient.\"\n\nMedical audit later evolved into clinical audit and a revised definition was announced by the NHS Executive:\n\"Clinical audit is the systematic analysis of the quality of healthcare, including the procedures used for diagnosis, treatment and care, the use of resources and the resulting outcome and quality of life for the patient.\"\n\nThe National Institute for Health and Clinical Excellence (NICE) published the paper \"Principles for Best Practice in Clinical Audit\", which defines clinical audit as\n\"a quality improvement process that seeks to improve patient care and outcomes through systematic review of care against explicit criteria and the implementation of change. Aspects of the structure, processes, and outcomes of care are selected and systematically evaluated against explicit criteria. Where indicated, changes are implemented at an individual, team, or service level and further monitoring is used to confirm improvement in healthcare delivery.\"\n\n\nClinical audit comes under the Clinical Governance umbrella and forms part of the system for improving the standard of clinical practice.\n\nClinical Governance is a system through which NHS organisations are accountable for continuously improving the quality of services; it ensures that there are clean lines of accountability within NHS trusts and that there is a comprehensive programme of quality improvement systems. The six pillars of clinical governance are:\n\n\nClinical audit was incorporated within Clinical Governance in the 1997 White Paper, \"The New NHS : Modern, Dependable\", which brought together disparate service improvement processes and formally established them into a coherent Clinical Governance framework.\n\nWithin Strategic Health Authorities the clinical governance lead is responsible for ensuring that there is a clinical audit programme within local trusts, and that this reflects national audit priorities. The clinical governance lead ultimately retains accountability for clinical audit, but may choose to delegate this role to another, the clinical audit lead. At a local level this individual will then be responsible for creating a clinical audit strategy, setting audit priorities, agreeing the audit programme, implementing the strategy and implementing the audit programme. The clinical governance lead however retains responsibility for ensuring that these tasks are completed and that clinical audit remains integrated with the other aspects of clinical governance.\n\nThe clinical audit lead has a clear role in creating the strategy for embedding clinical audit within the organisation, but the individual chosen must have more than just a nominal strategic role. The clinical audit lead should have a high profile within the organisation, and must champion clinical audit both to colleagues and management alike. The clinical audit lead should be actively involved in linkages to the other aspects of clinical governance to allow for the dissemination of clinical audit information and the setting of local clinical audit priorities.\n\n Clinical audit can be described as a cycle or a spiral, \"see figure\". Within the cycle there are stages that follow the \"systematic\" process of: establishing best practice; measuring against criteria; taking action to improve care; and monitoring to sustain improvement. As the process continues, each cycle aspires to a higher level of quality.\nThese processes are related to change management methodology and use the techniques of PDSA cycles, LEAN, Six Sigma, root cause analysis and process mapping.\n\nStage 1: Identify the problem or issue\n\nThis stage involves the selection of a topic or issue to be audited, and is likely to involve measuring adherence to healthcare processes that have been shown to produce best outcomes for patients. Selection of an audit topic is influenced by factors including:\n\nAdditionally, audit topics may be recommended by national bodies, such as NICE or the Healthcare Commission, in which NHS trusts may agree to participate. The Trent Accreditation Scheme recommends a culture of audit to participating hospitals inside and outside of the UK, and can provide advice on audit topics.\n\nStage 2: Define criteria and standards\n\nDecisions regarding the overall purpose of the audit, either as what should happen as a result of the audit, or what question you want the audit to answer, should be written as a series of statements or tasks that the audit will focus on. Collectively, these form the audit \"criteria\". These criteria are explicit statements that define what is being measured and represent elements of care that can be measured objectively. The \"standards\" define the aspect of care to be measured, and should always be based on the best available evidence.\n\n\nStage 3: Data collection\n\nTo ensure that the data collected are precise, and that only essential information is collected, certain details of what is to be audited must be established from the outset. These include:\n\n\nSample sizes for data collection are often a compromise between the statistical validity of the results and pragmatical issues around data collection. Data to be collected may be available in a computerised information system, or in other cases it may be appropriate to collect data manually or electronically using data capture solutions such as Formic, depending on the outcome being measured. In either case, considerations need to be given to what data will be collected, where the data will be found, and who will do the data collection.\n\nEthical issues must also be considered; the data collected must relate only to the objectives of the audit, and staff and patient confidentiality must be respected - identifiable information must not be used. Any potentially sensitive topics should be discussed with the local Research Ethics Committee.\n\nStage 4: Compare performance with criteria and standards\n\nThis is the analysis stage, whereby the results of the data collection are compared with criteria and standards. The end stage of analysis is concluding how well the standards were met and, if applicable, identifying reasons why the standards weren't met in all cases. These reasons might be agreed to be acceptable, i.e. could be added to the exception criteria for the standard in future, or will suggest a focus for improvement measures.\n\nIn theory, any case where the standard (criteria or exceptions) was not met in 100% of cases suggests a potential for improvement in care. In practice, where standard results were close to 100%, it might be agreed that any further improvement will be difficult to obtain and that other standards, with results further away from 100%, are the priority targets for action. This decision will depend on the topic area – in some ‘life or death’ type cases, it will be important to achieve 100%, in other areas a much lower result might still be considered acceptable.\n\nStage 5: Implementing change\n\nOnce the results of the audit have been published and discussed, an agreement must be reached about the recommendations for change. Using an action plan to record these recommendations is good practice; this should include who has agreed to do what and by when. Each point needs to be well defined, with an individual named as responsible for it, and an agreed timescale for its completion.\n\nAction plan development may involve refinement of the audit tool particularly if measures used are found to be inappropriate or incorrectly assessed. In other instances new process or outcome measures may be needed or involve linkages to other departments or individuals. Too often audit results in criticism of other organisations, departments or individuals without their knowledge or involvement. Joint audit is far more profitable in this situation and should be encouraged by the Clinical Audit lead and manager.\n\nRe-audit: Sustaining Improvements\n\nAfter an agreed period, the audit should be repeated. The same strategies for identifying the sample, methods and data analysis should be used to ensure comparability with the original audit. The re-audit should demonstrate that the changes have been implemented and that improvements have been made. Further changes may then be required, leading to additional re-audits.\n\nThis stage is critical to the successful outcome of an audit process - as it verifies whether the changes implemented have had an effect and to see if further improvements are required to achieve the standards of healthcare delivery identified in stage 2.\n\nResults of good audit should be disseminated both locally via the Strategic Health Authorities and nationally where possible.\nProfessional journals, such as the BMJ and the Nursing Standard publish the findings of good quality audits, especially if the work or the methodology is generalisable.\n\nWhile clinical audit makes great sense, there can be problems in persuading hospitals and clinicians to undertake and apply clinical audit in their work. Nonetheless, in the UK clinical audit is one of the corpus of clinical governance measures that are required to be enacted throughout the NHS.\n\nOutside the UK, hospital accreditation schemes, such as the Trent Accreditation Scheme, have promoted the development and execution of clinical audit as a part of clinical governance in places such as Hong Kong and Malta.\n\n\n"}
{"id": "27360421", "url": "https://en.wikipedia.org/wiki?curid=27360421", "title": "Doctor's office", "text": "Doctor's office\n\nA doctor's office in American English, a doctor's surgery in British English, a doctor’s rooms in Australian English or a doctor's practice, is a medical facility in which one or more medical doctors, usually general practitioners (GP), receive and treat patients.\n\nDoctors' offices are the primary place where ambulatory care is given, and are often the first place that a sick person would go for care, except in an emergency, in which case one would go to an emergency department at a hospital.\n\nIn most developed countries, where health services are guaranteed by the state in some form, most medical visits to doctors take place in their offices. In the United States, where this is not the case, many people who cannot afford health insurance or doctor's visits must either go to free or reduced-cost clinics or an emergency department at a hospital for care, instead of a doctor's office.\n\nFor healthy people, most visits to doctors' offices revolve around a once-yearly recommended physical examination. This exam usually consists of gathering information such as a patient's blood pressure, heart rate, weight, and height, along with checking for any irregularities or signs of illness around the body. GPs will also ask the patients about any mental health problems that they may be experiencing, and may refer them to a psychiatrist for further examination in the event that they do indeed have such problems. If there are any other health problems that must be addressed by a medical specialist, such as a cardiologist, a referral will be given.\n\nThe staff of a doctor's office usually consists of nurses, receptionists, and doctors. Sometimes, many doctors of different medical specialties may be housed in one building, allowing easy referrals.\n\nDoctors' offices can range from spartan to luxurious. A basic office usually consists of a waiting room and examination room(s). Examination rooms usually consist of an examination table, upon which the patient sits or lies down, and various other equipment, depending on the office. Examples of the equipment found in an examination room include:\n"}
{"id": "46959461", "url": "https://en.wikipedia.org/wiki?curid=46959461", "title": "EN 12566", "text": "EN 12566\n\nEN 12566 - Small wastewater treatment systems for up to 50 PT refers to a set of European standards which specify the general requirements for packaged and/or site assembled wastewater treatment plants used for domestic wastewater treatment for up to 50 PT (population total). The standards consist of the following parts: \n\n\n\n"}
{"id": "45470591", "url": "https://en.wikipedia.org/wiki?curid=45470591", "title": "EYDAP", "text": "EYDAP\n\nThe Athens Water Supply and Sewerage Company (, \"Eteria Ydrefsis ke Apohetefsis Protevusas\", abbr. , EYDAP) is the largest Greek enterprise in its sector. Based in Galatsi in Athens, it is serving 4.3 million customers in the Greater Metropolitan area of Athens with fresh water and 3.5 million customers with sewers.\n\nEYDAP was founded in 1980 after the merge of the two water suppliers Hellenic Water Company (EEY) and Greater Athens Sewerage Organization (OAP).\n\nThe Greek state holds a majority stake in EYDAP, with further 27 percent being listed at the Athens Exchange where it belongs to the 25 companies forming the FTSE/Athex Large Cap index.\n\nFollowing the Greek government-debt crisis, EYDAP was planned to be fully privatized under the terms of the Eurozone Memorandum. In May 2014, the Greek Council of State however blocked the transfer of the government's stake to its privatization fund, the Hellenic Republic Asset Development Fund. The constitutional court ruled that the sale would be unconstitutional. Following this decision, a merger of EYDAP with the Thessaloniki Water Supply & Sewage Co. (EYATH) was taken into consideration instead.\n"}
{"id": "38813814", "url": "https://en.wikipedia.org/wiki?curid=38813814", "title": "Emilio Comici", "text": "Emilio Comici\n\nLeonardo Emilio Comici (Trieste, 21 February 1901 – Sëlva in Val Gardena, 19 October 1940) was an Italian mountain climber and caver. He made numerous ascents in the Eastern Alps, particularly in the Dolomites (where he made over 200 first ascents during his career) and in the Julian Alps. Comici was nicknamed the \"Angel of the Dolomites\".\n\nIn the 1930s and 1940s Comici and other climbers (including Riccardo Cassin, Raffaele Carlesso and Alvise Andrich) represented the Italian answer to the achievements of German climbers. Comici perfected the Bavarian technique of mountain climbing, and began the era of \"sixth grade\" climbing (at that time the highest climbing grade considered humanly surmountable). He was the inventor and proponent of using multi-step aid ladders, solid belays, the use of a trail/tag line, and hanging bivouacs, contributing greatly to the techniques of big wall climbing.\n\nEmilio Comici was the son of Antonio Comici and Regina Cartago. A longshoreman in his youth, he began mountain climbing after caving for ten years (1918-1927), following the Trieste tradition of mountaineering represented by Napoleone Cozzi and Julius Kugy. As a caver, Comici set a world depth record of near Trieste. He began climbing at the suggestion of friends from the Trieste chapter of the Italian Alpine Club, gaining his first experience in the nearby Val Rosandra.\n\nIn 1932 Comici moved to Lake Misurina in the municipality of Auronzo di Cadore, where he opened a climbing school. Comici's students included Riccardo Cassin, later a prominent climber. From 1938 to 1940 he served as \"podestà\" (mayor) of Sëlva in Val Gardena, where he also directed the ski school. He was a supporter of the Fascist regime of Benito Mussolini, which promoted him by sending him on lecture tours.\n\nHe died in an accidental fall caused by a frayed rope on the training cliffs of the Sëlva climbing area in Val Gardena. For a long time the precise circumstances were not reported by the Fascist authorities, who did not want to cast a shadow on the famous figure of Comici.\n\nIn addition to his talents as a climber, Comici is remembered for his aesthetic concept of climbing, perceiving it as a means of self-expression through harmonious movement. It was Comici who originated the concept of climbing \"direttissima\" routes, following the path a drop of water would take down the mountain. Comici's book \"Alpinismo Eroico\" employs rhetoric characteristic of the era in which it was written.\n\nThe Rifugio Zsigmondy-Comici, or Zsigmondyhütte, in the Sexten Dolomites is named for Comici and Emil Zsigmondy. The Rifugio Emilio Comici and the Campanile Comici, both in the Langkofel Group, are also named for Comici. A wooden monument memorializes Comici at the foot of the wall in Vallunga where he died.\n\nFrom 13 to 14 August 1933 Comici and the brothers Angelo and Giuseppe Dimai made the first ascent of the north face of the Cima Grande di Lavaredo, thus opening the eponymous \"Via Comici-Dimai\" (Comici-Dimai Route) or Comici route. In this climb of 400 meters, they used rope, 150 meters of cord, 90 hooks, climbing slings, 40 carabiners and 80 pitons. There had been several previous failed attempts on the face by other climbers. However, the technique used by Comici's party, and specifically their use of pitons, became the subject of debate among mountaineers. In September 1937, Comici repeated his route as a solo climber in just 3.5 hours. The route was then rated at a difficulty of VI. Today, the rating is VI/A0 (UIAA) or in free climbing VII (UIAA).\n\n\n"}
{"id": "40135180", "url": "https://en.wikipedia.org/wiki?curid=40135180", "title": "Epidemiology of attention deficit hyperactive disorder", "text": "Epidemiology of attention deficit hyperactive disorder\n\nADHD is estimated to affect about 6 to 7 percent of people aged 18 and under when diagnosed via the DSM-IV criteria. Hyperkinetic disorder when diagnosed via the ICD-10 criteria give rates of between 1 and 2 percent in this age group.\n\nChildren in North America appear to have a higher rate of ADHD than children in Africa and the Middle East - however, this may be due to differing methods of diagnosis used in different areas of the world. If the same diagnostic methods are used rates are more or less the same between countries.\n\nIt is estimated that ADHD affects between 5.4-8.7% of children in Africa. Data quality however is not high.\n\nA 2008 evaluation of the “KiGGS” survey, monitoring 14,836 girls and boys (age between 3 and 17 years), showed that 4.8% of the participants had an ADHD diagnosis. While 7.9% of all boys had ADHD, only 1.8% girls had it, too. Another 4.9% of the participants (6.4% boys : 3.6% girls) were suspected ADHD cases, because they showed a rate ≥7 on the Strengths and Difficulties Questionnaire (SDQ) scale. The number of ADHD diagnoses was 1.5% (2.4% : 0.6%) among preschool children (3–6 years old), 5,3 % (8.7% : 1.9%) at age 7–10 years, and had its peak at 7.1% (11.3% : 3.0%) in the age group of 11–13 years. Among 14 to 17 years old adolescents the rate was 5.6% (9.4% : 1.8%).\n\nRates in Spain are estimated at 6.8% among people under 18.\n\nIn the United States it is diagnosed in 2-16 percent of school children. The rates of diagnosis and treatment of ADHD are much higher on the east coast of the United States than on its west coast. The frequency of the diagnosis differs between male children (10%) and female children (4%) in the United States. This difference between genders may reflect either a difference in susceptibility or that females with ADHD are less likely to be diagnosed than males. Boys outnumber girls across all three subtyping categories, but the exact magnitude of these differences seems to depend on both the informant (parent, teacher, etc.) and the subtype. In two community-based investigations, conducted by DuPaul and associates, boys outnumbered girls by only 2.2:1 in parent-generated samples and 2.3:1 in teacher-based input.\n\nRates of ADHD diagnosis and treatment have increased in both the United Kingdom and the United States since the 1970s. This is believed to be primarily due to changes in how the condition is diagnosed and how readily people are willing to treat it with medications rather than a true change in the frequency. In the UK an estimated 0.5 per 1,000 children had ADHD in the 1970s, while 3 per 1,000 received ADHD medications in the late 1990s. In the UK in 2003, 3.6 percent of male children and less than 1 percent in female children had the diagnosis. In the United States the number of children with the diagnosis increase from 12 per 1000 in the 1970s to 34 per 1000 in the late 1990s, to 95 per 1,000 in 2007, and 110 per 1,000 in 2011. It is believed that the changes to the diagnostic criteria in 2013 from the DSM 4TR to the DSM 5 will increase the number of people with ADHD especially among adults.\n"}
{"id": "18430559", "url": "https://en.wikipedia.org/wiki?curid=18430559", "title": "Essentiale", "text": "Essentiale\n\nEssentiale (polyenylphosphatidylcholine or PPC, with or without synergistic vitamins) is a preparation of essential phospholipids. Essentiale normalizes the metabolism of lipids and proteins, improves the detoxification function of the liver, restores the cellular structure of the liver and retards the producing of conjunctive tissue. Essentiale medications are indicated for the treatment of fatty degeneration of the liver, hepatitis (including toxic hepatitis, liver damage caused by medicines or alcohol abuse), cirrhosis of the liver, and disturbances in liver function associated with different illnesses.\n\nPhospholipids are essential structural components of all cellular membranes. \nEssential phospholipids (EPL substance -- an active ingredient in Essentiale medications) is a complex of substances of natural origin (ethers of cholinephosphoric acid (phosphatidylcholine) and unsaturated fatty acids (linoleic, linolenic, olein). \nEssentiale medications possess membranotropic properties, exert metabolic and hepatoprotective action, and regulate lipid and carbohydrate metabolism. \nEssential phospholipids increase the functional status of the liver.\n\nIntervening in the disturbance of the liver's metabolism, Essential phospholipids enter the cell membrane and combine with the endogenous phospholipids. Essential phospholipids improve the patient's clinical status and the liver's lab indices.\n\nCombination of Essentiale with cordiamin (nikethamide) and vitamin E (50 mg/kg for 35 days) considerably activates the mono-oxygenase, glucoro- and glutathione transferase systems of the liver, resulting in the free-radical processes becoming less intense.\n\nThe spectrum of Essential phospholipids' activity in chronic degenerative liver diseases is illustrated by the following properties: \n\nEssentiale is used to treat the following diseases:\n\nEssentiale medications are manufactured under 4 trade names:\n\nESSENTIALE - solution for intravenous injections in vials 5 ml. Essentiale contains essential phospholipids (EPL substance) 250 mg, Pyridoxine chidrochloride 2,5 mg (Vitamin B), Cyanocobolamine 0,1 mg (Vitamin B), Sodium Pantothenate 1.5 mg, Nicotinamide 25 mg.\n\nESSENTIALE FORTE N - Capsules N30 or N100. One capsule contains essential phospholipids (EPL substance) 300 mg.\n\nESSENTIALE N - solution for intravenous injection in vials 5 ml. Essentiale N contains essential phospholipids (EPL substance) 250 mg.\n\nESSENTIALE FORTE - capsules N50. Essentiale forte contains essential phospholipids (EPL substance) 300 mg, Thiamine mono nitrate (Vitamin B) 6 mg, Riboflavine (Vitamiin B) 6 mg, Pyridoxine chidrochloride 6 mg (Vitamin B), Cyanocobolamine 0.06 mg (Vitamin B), Nicotinamide 30 mg, Tocopherole acetate 6 mg (Vitamin E).\n\nDo not use Essentiale in hypersensitivity or allergy to any ingredients of the preparation. \nThe application of Essentiale in newborn children is not safe. During pregnancy women are recommended to consult their health care provider prior to taking Essentiale.\n\nIn very rare cases it can cause: abdominal pain, nausea, diarrhea and allergic reaction (skin rash).\n\nCapsules should be stored at temperature not more than 20 °C.\nVials should be stored at 2° - 8 °C\n\nAccording to research conducted by Bronx Veterans Affairs Medical Center and the Mount Sinai School of Medicine in 2003, phospholipids treatment did not affect progression of liver diseases in those who continued to drink heavily.\n"}
{"id": "18352770", "url": "https://en.wikipedia.org/wiki?curid=18352770", "title": "Evidence-Based Complementary and Alternative Medicine", "text": "Evidence-Based Complementary and Alternative Medicine\n\nEvidence-Based Complementary and Alternative Medicine is a peer-reviewed open-access medical journal covering alternative medicine published by Hindawi Publishing Corporation. The journal was established in 2004 by Edwin L. Cooper, who also served as its editor-in-chief until 2010, when the journal moved from Oxford University Press to Hindawi.\n\nInitially, the journal was entirely open access, without publication charge to the authors except for color figures, but Oxford University Press changed its policy in 2008 and made reviews, editorials, and commentaries subscription-based, while maintaining open access for original research papers. Hindawi returned the journal to a full open access model, but authors have to pay an article processing charge.\n\nAccording to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 1.931.\n\nOne of the founding editors, Professor Edzard Ernst, has described the journal as \"useless rubbish\", primarily due to ineffective peer review.\n"}
{"id": "29726014", "url": "https://en.wikipedia.org/wiki?curid=29726014", "title": "Genetic gain", "text": "Genetic gain\n\nGenetic gain is the amount of increase in performance that is achieved through artificial genetic improvement programs. This is usually used to refer to the increase after one generation has passed.\n"}
{"id": "43639934", "url": "https://en.wikipedia.org/wiki?curid=43639934", "title": "Gilbert Wheeler Beebe", "text": "Gilbert Wheeler Beebe\n\nGilbert Wheeler Beebe (3 April 1912 – 3 March 2003), also known as Gil Beebe, was an American epidemiologist and statistician known for monumental studies of radiation-related mortality and morbidity among populations exposed to ionizing radiation from the atomic bombings of Hiroshima and Nagasaki in 1945 and the Chernobyl reactor accident in 1986.\n\nBeebe was born in 1903 in Mahwah, New Jersey. Beebe attended Dartmouth College and graduated in 1933. He attended Columbia University and completed graduate studies in sociology and statistics and earned the A.M. in 1938 and Ph.D. in 1942.\nBeebe carried out a landmark cohort study of contraceptive services in economically depressed areas.\n\nBeebe was a captain in the United States Army and served in the Office of the Surgeon General of the United States Army during World War II. He worked with Michael DeBakey to help create the Medical Follow-up Agency (MFUA) at the National Academy of Sciences (NAS).\nBeebe directed the MFUA until his retirement at age 65.\nBeebe worked with Seymour Jablon at the MFUA to reorganize the Atomic Bomb Casualty Commission (ABCC) in Japan.\nHe spent several tours of duty in Japan, a total of seven years in Hiroshima and Nagasaki as the ABCC chief of Statistics in 1958–1960, 1966–1968, and 1973–1975.\nThe Medical Follow-up Agency (MFUA) was succeeded by the Radiation Effects Research Foundation (RERF).\nThe RERF produces epidemiological and statistical information that helps to form our current knowledge of radiation related risk of cancer and other long-term health effects in human populations.\n\nIn 1977 Beebe joined the National Cancer Institute in what would become the Division of Cancer Epidemiology and Genetics. During this time Beebe worked with John D. Boice, Jr.\nIn 1986 after the Chernobyl accident, he organized and led an international study of thyroid cancer and leukemia risk among radiation-exposed populations in Belarus and Ukraine.\n\nIn 2002 Beebe retired from the NCI and remained active as NCI Scientist Emeritus until the day he died. Beebe died in 2003 in Washington, D.C. from acute pulmonary failure.\nBeebe was survived by his wife of 69 years, Ruth, four children, Alfred, Beatrice, Brian, and Christopher, and five grandchildren.\n\nIn 1973 he was elected as a Fellow of the American Statistical Association.\n\nEstablished in 2002 by the National Academy of Sciences Board on Radiation Effects Research (a predecessor of the Nuclear and Radiation Studies Board) to honor the scientific achievements of Dr. Gilbert W. Beebe National Cancer Institute, who was one of the designers and key implementers of the epidemiology studies of Japanese atomic bomb survivors (called , a Japanese word that literally translates as \"explosion-affected people\" and is used to refer to people who were exposed to radiation from the bombings) and co-founder of the Medical Follow-up Agency. The symposium is used to promote discussions among scientists, federal staff, and other interested parties concerned with radiation health effects.\n\nSymposia that have been held addressed a wide range of topics related to radiation and health:\n\n\n\n\n\n\n\n"}
{"id": "2607178", "url": "https://en.wikipedia.org/wiki?curid=2607178", "title": "Glucosinolate", "text": "Glucosinolate\n\nThe glucosinolates are natural components of many pungent plants such as mustard, cabbage, and horseradish. The pungency of those plants is due to mustard oils produced from glucosinolates when the plant material is chewed, cut, or otherwise damaged. These natural chemicals most likely contribute to plant defence against pests and diseases, and impart a characteristic bitter flavor property of cruciferous vegetables.\n\nGlucosinolates occur as secondary metabolites of almost all plants of the order Brassicales. Ordered in the Brassicales are for example the economically important family Brassicaceae as well as Capparaceae and Caricaceae.\nOutside of the Brassicales, the genera \"Drypetes\" and \"Putranjiva\" in the family Putranjivaceae are the only other known occurrence of glucosinolates.\nGlucosinolates occur in various edible plants such as cabbage (white cabbage, Chinese cabbage, broccoli) watercress, horseradish, capers and radishes where the breakdown products often contribute a significant part of the distinctive taste. The glucosinolates are also found in seeds of these plants.\n\nGlucosinolates constitute a natural class of organic compounds that contain sulfur and nitrogen and are derived from glucose and an amino acid. They are water-soluble anions and belong to the glucosides. Every glucosinolate contains a central carbon atom, which is bound via a sulfur atom to the thioglucose group and via a nitrogen atom to a sulfate group (making a sulfated aldoxime). In addition, the central carbon is bound to a side group; different glucosinolates have different side groups, and it is variation in the side group that is responsible for the variation in the biological activities of these plant compounds. \nThe semisystematic naming of glucosinolates consists of the chemical name of that side chain followed by \"glucosinolate\". Spelling glucosinolate names in one or two words (e.g. allylglucosinolate versus allyl glucosinolate) are both in use and has equivalent meaning. Isothiocyanates must be spelled in two words.\n\nThe essence of glucosinolate chemistry is the ability of a glucosinolate to convert into an isothiocyanate (a \"mustard oil\") upon hydrolysis of the thioglucoside bond by the enzyme myrosinase.\n\nSome glucosinolates:\n\nAbout 132 different glucosinolates are known to occur naturally in plants. They are synthesized from certain amino acids: So-called aliphatic glucosinolates derived from mainly methionine, but also alanine, leucine, isoleucine, or valine. (Most glucosinolates are actually derived from chain-elongated homologues of these amino acids, e.g. glucoraphanin is derived from dihomomethionine, which is methionine chain-elongated twice). Aromatic glucosinolates include indolic glucosinolates, such as glucobrassicin, derived from tryptophan and others from phenylalanine, its chain-elongated homologue homophenylalanine, and sinalbin derived from tyrosine.\n\nThe plants contain the enzyme myrosinase, which, in the presence of water, cleaves off the glucose group from a glucosinolate. The remaining molecule then quickly converts to an isothiocyanate, a nitrile, or a thiocyanate; these are the active substances that serve as defense for the plant. Glucosinolates are also called mustard oil glycosides. The standard product of the reaction is the isothiocyanate (mustard oil); the other two products mainly occur in the presence of specialised plant proteins that alter the outcome of the reaction.\n\nIn the chemical reaction illustrated above, the red curved arrows in the left side of figure are simplified compared to reality, as the role of the enzyme myrosinase is not shown. However, the mechanism shown is fundamentally in accordance with the enzyme-catalyzed reaction.\n\nIn contrast, the reaction illustrated by red curved arrows at the right side of the figure, depicting the rearrangement of atoms resulting in the isothiocyanate, is expected to be non-enzymatic. This type of rearrangement can be named a Lossen-reaarrangement, or a Lossen-\"like\" rearrangement, since this name was first used for the analogous reaction leading to an organic isocyanate (R-N=C=O).\n\nTo prevent damage to the plant itself, the myrosinase and glucosinolates are stored in separate compartments of the cell or in different cells in the tissue, and come together only or mainly under conditions of physical injury.\n\nThe use of glucosinolate-containing crops as primary food source for animals can have negative effects if the concentration of glucosinolate is higher than what is acceptable for the animal in question, because some glucosinolates have been shown to have toxic effects (mainly as goitrogens) in both humans and animals at high doses. However, tolerance level to glucosinolates varies even within the same genus (e.g. \"Acomys cahirinus\" and \"Acomys russatus\").\n\nThe glucosinolate sinigrin, among others, was shown to be responsible for the bitterness of cooked cauliflower and Brussels sprouts. Glucosinolates may alter animal eating behavior.\n\nGlucosinolates are studied for their potential to affect human health, as well as plant breeding, physiology, genetics, and food applications. , studies on possible anticancer mechanisms have been conducted, but there is no clinical evidence to indicate the safety and efficacy of using glucosinolates for treating cancer or any human disease. The compositions of glucosinolates and their hydrolysis products vary by vegetable.\n\nGlucosinolates and their products have a negative effect on many insects, resulting from a combination of deterrence and toxicity. In an attempt to apply this principle in an agronomic context, some glucosinolate-derived products can serve as antifeedants, i.e., natural pesticides. \n\nIn contrast, the diamondback moth, a pest of cruciferous plants, may recognize the presence of glucosinolates, allowing it to identify the proper host plant. Indeed, a characteristic, specialised insect fauna is found on glucosinolate-containing plants, including butterflies, such as large white, small white, and orange tip, but also certain aphids, moths, such as the southern armyworm, sawflies, and flea beetles. For instance, the large white butterfly deposits its eggs on these glucosinolate-containing plants, and the larvae survive even with high levels of glucosinolates and eat plant material containing glucosinolates. The whites and orange tips all possess the so-called nitrile specifier protein, which diverts glucosinolate hydrolysis toward nitriles rather than reactive isothiocyanates. In contrast, the diamondback moth possesses a completely different protein, glucosinolate sulfatase, which desulfates glucosinolates, thereby making them unfit for degradation to toxic products by myrosinase.\n\nOther kinds of insects (specialised sawflies and aphids) sequester glucosinolates. In specialised aphids, but not in sawflies, a distinct animal myrosinase is found in muscle tissue, leading to degradation of sequestered glucosinolates upon aphid tissue destruction. This diverse panel of biochemical solutions to the same plant chemical plays a key role in the evolution of plant-insect relationships.\n\n\n"}
{"id": "27238728", "url": "https://en.wikipedia.org/wiki?curid=27238728", "title": "HealthCentral", "text": "HealthCentral\n\nHealthCentral is an American privately owned online health company, that was founded by an emergency department doctor in 1999. HealthCentral was later acquired by a group of venture investors in 2005. Initial investments were made from Polaris Venture Partners, Sequoia Capital, The Carlyle Group, and Allen & Company. In 2005, the group of new investors then brought on Christopher M. Schroeder as CEO. In 2008, InterActiveCorp made a significant minority investment in the company, and the initial group of investors joined in the round of funding to reinvest in HealthCentral.\n\nHealthCentral's mission statement is \"empower people to improve and take control of their health and well-being.\" The company owns 35 health sites focused on specific conditions.\n\nIn 2006, HealthCentral acquired FoodFit.com, an online wellness resource focused on healthy eating and active living. In 2008, HealthCentral acquired consumer drug information company MedTrackAlert and HIV/AIDS website The Body.com. The company acquired health technology company Wellsphere.com in 2009.\n\n"}
{"id": "15985768", "url": "https://en.wikipedia.org/wiki?curid=15985768", "title": "Health in the Republic of Ireland", "text": "Health in the Republic of Ireland\n\nThere have been dramatic reductions in mortality from the three principal causes of death in Ireland - heart disease, cancer and stroke - in recent years age-standardised mortality rate for heart disease has fallen by 59% between 1990-2011 and now stands just above the OECD rate at 136 deaths per 100,000 population per annum. Similarly, the age-standardised mortality rate for stroke has fallen by 51% in the same period to below the OECD average (61 deaths from stroke per 100,000 population per annum). Deaths from cancer have fallen by 21% between 1990-2011 to 217 per 100,000.\nIn 2005:\n\n\nA new measure of expected human capital calculated for 195 countries from 1990 to 2016 and defined for each birth cohort as the expected years lived from age 20 to 64 years and adjusted for educational attainment, learning or education quality, and functional health status was published by The Lancet in September 2018. Ireland had the sixteenth highest level of expected human capital with 24 health, education, and learning-adjusted expected years lived between age 20 and 64 years. \n\nIn the Republic of Ireland, childhood vaccination (up to age 16) requires the consent of the parents. The Department of Health strongly recommend vaccinations.\n\nSee also Healthcare in the Republic of Ireland\n"}
{"id": "43339210", "url": "https://en.wikipedia.org/wiki?curid=43339210", "title": "Hydroxybupropion", "text": "Hydroxybupropion\n\nHydroxybupropion (code name BW 306U), or 6-hydroxybupropion, is the major active metabolite of the antidepressant and smoking cessation drug bupropion. It is formed from bupropion by the liver enzyme CYP2B6 during first-pass metabolism. With oral bupropion treatment, hydroxybupropion is present in plasma at area under the curve concentrations that are as many as 16–20 times greater than those of bupropion itself, demonstrating extensive conversion of bupropion into hydroxybupropion in humans. As such, hydroxybupropion is likely to play a very important role in the effects of oral bupropion, which could accurately be thought of as functioning largely as a prodrug to hydroxybupropion.\n\nCompared to bupropion, hydroxybupropion is similar in its potency as a norepinephrine reuptake inhibitor (IC = 1.7 µM) (and likely also acts as a norepinephrine releasing agent, similarly to bupropion), but is substantially weaker as a dopamine reuptake inhibitor (IC = >10 µM). Like bupropion, hydroxybupropion is also a non-competitive antagonist of nACh receptors, such as αβ and αβ, but is even more potent in comparison.\n\nBupropion is extensively and rapidly absorbed in the gastrointestinal tract but experiences extensive first pass metabolism rendering its systemic bioavailability limited. Exact bioavailability has yet to be determined given an intravenous form does not exist. Absorption is suggested to be between 80-90%. Its distribution half-life is between 3–4 hours and exhibits moderate human plasma protein binding (between 82-88%) with the parent compound and hydroxybupropion displaying the highest affinity. Bupropion is a racemic mixture and is metabolized hepatically primarily via oxidative cleavage of its side chains by CYP2B6. Hydroxybupropion is the most potent of the metabolites. It is formed via the “\"hydroxylation of the tert-butyl group\"” by CYP2B6 and is excreted renally. Cmax vales of hydroxybupropion are 4-7 times that of bupropion, while the exposure to hydroxybupropion is \"10 fold\" that of bupropion. Hydroxybupropion's elimination half-life is roughly 20 hours, give or take 5 hours and will reach steady state concentrations within 8 days.\n\nAlthough there are patents proposing uses and formulations of this compound, hydroxybuproprion is not currently marketed as a drug in and of itself and is only available for use in non-clinical research. Hydroxybupropion is not a scheduled drug or a controlled substance. One can access GLP (Good Lab Practice) documents detailing assays/techniques to further research and isolate this drug. Otherwise, there is little regulatory data available for hydroxybupropion at this time. Moreover, there is little information to suggest hydroxybupropion has an abuse potential. However, it has been studied as a possible therapeutic for alcohol and nicotine abuse as a codrug.\n\nThere are few clinical trials or toxicology studies assessing hydroxybupropion alone at this time. There are clinical studies which assess hydroxybupropion in conjunction with bupropion suggesting hydroxybupropion to be the primary form of the compound responsible for its clinical efficacy. Also, transdermal delivery of bupropion and hydroxybupropion has been assessed finding bupropion to be the superior candidate given its elevated diffusion rate through skin samples. There are few toxicology studies assessing hydroxybupropion alone at this time. However, there are some studies which assess this compound in conjunction with others or its parent compound.\n\n"}
{"id": "31094752", "url": "https://en.wikipedia.org/wiki?curid=31094752", "title": "Hypersexual disorder", "text": "Hypersexual disorder\n\nHypersexual disorder is a pattern of behavior involving intense preoccupation with sexual fantasies, urges and activities, leading to adverse consequences and clinically significant distress or impairment in social, occupational or other important functions. It was proposed in 2010 for inclusion in the Diagnostic and Statistical Manual of Mental Disorders Fifth Edition (DSM-5) of the American Psychiatric Association (APA). \n\nPeople with hypersexual disorder experience multiple, unsuccessful attempts to control or diminish the amount of time spent engaging in sexual fantasies, urges, and behaviors in response to dysphoric mood states or stressful life events. \n\nFor a valid diagnosis of hypersexual disorder to be established, symptoms must persist for a period of at least 6 months and occur independently of a use mania or a medical condition.\n\nHypersexual disorder was recommended for inclusion in the DSM-5 (Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition) by the Sexual and Gender Identity Disorders Workgroup (Emerging Measures and Models, Conditions for Further Study). It was ultimately not approved. The term \"hypersexual disorder\" was reportedly chosen because it did not imply any specific theory for the causes of hypersexuality, which remain unknown. A proposal to add sexual addiction to the DSM system had been previously rejected by the APA, as not enough evidence suggested to them that the condition is analogous to substance addictions, as that name would imply.\n\nRory Reid, a research psychologist in the Department of Psychiatry at the University of California Los Angeles (UCLA), led a team of researchers to investigate the proposed criteria for Hypersexual Disorder. Their findings were published in the \"Journal of Sexual Medicine\" where they concluded that the given criteria is valid and the disorder could be reliably diagnosed. \n\nThe DSM-IV-TR, published in 2000, includes an entry called \"Sexual Disorder—Not Otherwise Specified\" (Sexual Disorder NOS), for disorders that are clinically significant but do not have code. The DSM-IV-TR notes that Sexual Disorder NOS would apply to, among other conditions, \"distress about a pattern of repeated sexual relationships involving a succession of lovers who are experienced by the individual only as things to be used\".\n\n"}
{"id": "191003", "url": "https://en.wikipedia.org/wiki?curid=191003", "title": "Hypothalamic–pituitary–adrenal axis", "text": "Hypothalamic–pituitary–adrenal axis\n\nThe hypothalamic–pituitary–adrenal axis (HPA axis or HTPA axis) is a complex set of direct influences and feedback interactions among three components: the hypothalamus, the pituitary gland (a pea-shaped structure located below the thalamus), and the adrenal (also called \"suprarenal\") glands (small, conical organs on top of the kidneys).\n\nThese organs and their interactions constitute the HPA axis, a major neuroendocrine system that controls reactions to stress and regulates many body processes, including digestion, the immune system, mood and emotions, sexuality, and energy storage and expenditure. It is the common mechanism for interactions among glands, hormones, and parts of the midbrain that mediate the general adaptation syndrome (GAS). While steroid hormones are produced mainly in vertebrates, the physiological role of the HPA axis and corticosteroids in stress response is so fundamental that analogous systems can be found in invertebrates and monocellular organisms as well.\n\nThe HPA axis, HPG axis, HPT axis, and the hypothalamic–neurohypophyseal system are the four major neuroendocrine systems through which the hypothalamus and pituitary direct neuroendocrine function.\n\nThe key elements of the HPA axis are:\n\nCRH and vasopressin are released from neurosecretory nerve terminals at the median eminence. CRH is transported to the anterior pituitary through the portal blood vessel system of the hypophyseal stalk and vasopressin is transported by axonal transport to the posterior pituitary gland. There, CRH and vasopressin act synergistically to stimulate the secretion of stored ACTH from corticotrope cells. ACTH is transported by the blood to the adrenal cortex of the adrenal gland, where it rapidly stimulates biosynthesis of corticosteroids such as cortisol from cholesterol. Cortisol is a major stress hormone and has effects on many tissues in the body, including the brain. In the brain, cortisol acts on two types of receptor – mineralocorticoid receptors and glucocorticoid receptors, and these are expressed by many different types of neurons. One important target of glucocorticoids is the hypothalamus, which is a major controlling centre of the HPA axis.\n\nVasopressin can be thought of as \"water conservation hormone\" and is also known as \"antidiuretic hormone.\" It is released when the body is dehydrated and has potent water-conserving effects on the kidney. It is also a potent vasoconstrictor.\n\nImportant to the function of the HPA axis are some of the feedback loops:\n\nRelease of CRH from the hypothalamus is influenced by stress, physical activity, illness, by blood levels of cortisol and by the sleep/wake cycle (circadian rhythm). In healthy individuals, cortisol rises rapidly after wakening, reaching a peak within 30–45 minutes. It then gradually falls over the day, rising again in late afternoon. Cortisol levels then fall in late evening, reaching a trough during the middle of the night. This corresponds to the rest-activity cycle of the organism. An abnormally flattened circadian cortisol cycle has been linked with chronic fatigue syndrome, insomnia and burnout.\n\nThe HPA axis has a central role in regulating many homeostatic systems in the body, including the metabolic system, cardiovascular system, immune system, reproductive system and central nervous system. The HPA axis integrates physical and psychosocial influences in order to allow an organism to adapt effectively to its environment, use resources, and optimize survival.\n\nAnatomical connections between brain areas such as the amygdala, hippocampus, prefrontal cortex and hypothalamus facilitate activation of the HPA axis. Sensory information arriving at the lateral aspect of the amygdala is processed and conveyed to the amygdala's central nucleus, which then projects out to several parts of the brain involved in responses to fear. At the hypothalamus, fear-signaling impulses activate both the sympathetic nervous system and the modulating systems of the HPA axis.\n\nIncreased production of cortisol during stress results in an increased availability of glucose in order to facilitate fighting or fleeing. As well as directly increasing glucose availability, cortisol also suppresses the highly demanding metabolic processes of the immune system, resulting in further availability of glucose.\n\nGlucocorticoids have many important functions, including modulation of stress reactions, but in excess they can be damaging. Atrophy of the hippocampus in humans and animals exposed to severe stress is believed to be caused by prolonged exposure to high concentrations of glucocorticoids. Deficiencies of the hippocampus may reduce the memory resources available to help a body formulate appropriate reactions to stress.\n\nThere is bi-directional communication and feedback between the HPA axis and immune system. A number of cytokines, such as IL-1, IL-6, IL-10 and TNF-alpha can activate the HPA axis, although IL-1 is the most potent. The HPA axis in turn modulates the immune response, with high levels of cortisol resulting in a suppression of immune and inflammatory reactions. This helps to protect the organism from a lethal overactivation of the immune system, and minimizes tissue damage from inflammation.\n\nThe CNS is in many ways \"immune privileged,\" but it plays an important role in the immune system and is affected by it in turn. The CNS regulates the immune system through neuroendocrine pathways, such as the HPA axis. The HPA axis is responsible for modulating inflammatory responses that occur throughout the body.\n\nDuring an immune response, proinflammatory cytokines (e.g. IL-1) are released into the peripheral circulation system and can pass through the blood brain barrier where they can interact with the brain and activate the HPA axis. Interactions between the proinflammatory cytokines and the brain can alter the metabolic activity of neurotransmitters and cause symptoms such as fatigue, depression, and mood changes. Deficiencies in the HPA axis may play a role in allergies and inflammatory/ autoimmune diseases, such as rheumatoid arthritis and multiple sclerosis.\n\nWhen the HPA axis is activated by stressors, such as an immune response, high levels of glucocorticoids are released into the body and suppress immune response by inhibiting the expression of proinflammatory cytokines (e.g. IL-1, TNF alpha, and IFN gamma) and increasing the levels of anti-inflammatory cytokines (e.g. IL-4, IL-10, and IL-13) in immune cells, such as monocytes and neutrophils \n\nThe relationship between chronic stress and its concomitant activation of the HPA axis, and dysfunction of the immune system is unclear; studies have found both immunosuppression and hyperactivation of the immune response.\n\nThe HPA axis is involved in the neurobiology of mood disorders and functional illnesses, including anxiety disorder, bipolar disorder, insomnia, posttraumatic stress disorder, borderline personality disorder, ADHD, major depressive disorder, burnout, chronic fatigue syndrome, fibromyalgia, irritable bowel syndrome, and alcoholism. Antidepressants, which are routinely prescribed for many of these illnesses, serve to regulate HPA axis function.\n\nExperimental studies have investigated many different types of stress, and their effects on the HPA axis in many different circumstances. Stressors can be of many different types—in experimental studies in rats, a distinction is often made between \"social stress\" and \"physical stress\", but both types activate the HPA axis, though via different pathways. Several monoamine neurotransmitters are important in regulating the HPA axis, especially dopamine, serotonin and norepinephrine (noradrenaline). There is evidence that an increase in oxytocin, resulting for instance from positive social interactions, acts to suppress the HPA axis and thereby counteracts stress, promoting positive health effects such as wound healing.\n\nThe HPA axis is a feature of mammals and other vertebrates. For example, biologists studying stress in fish showed that social subordination leads to chronic stress, related to reduced aggressive interactions, to lack of control, and to the constant threat imposed by dominant fish. Serotonin (5HT) appeared to be the active neurotransmitter involved in mediating stress responses, and increases in serotonin are related to increased plasma α-MSH levels, which causes skin darkening (a social signal in salmonoid fish), activation of the HPA axis, and inhibition of aggression. Inclusion of the amino acid -tryptophan, a precursor of 5HT, in the feed of rainbow trout made the trout less aggressive and less responsive to stress. However, the study mentions that plasma cortisol was not affected by dietary -tryptophan. The drug LY354740 (also known as Eglumegad, an agonist of the metabotropic glutamate receptors 2 and 3) has been shown to interfere in the HPA axis, with chronic oral administration of this drug leading to markedly reduced baseline cortisol levels in bonnet macaques (Macaca radiata); acute infusion of LY354740 resulted in a marked diminution of yohimbine-induced stress response in those animals.\n\nStudies on people show that the HPA axis is activated in different ways during chronic stress depending on the type of stressor, the person's response to the stressor and other factors. Stressors that are uncontrollable, threaten physical integrity, or involve trauma tend to have a high, flat diurnal profile of cortisol release (with lower-than-normal levels of cortisol in the morning and higher-than-normal levels in the evening) resulting in a high overall level of daily cortisol release. On the other hand, controllable stressors tend to produce higher-than-normal morning cortisol. Stress hormone release tends to decline gradually after a stressor occurs. In post-traumatic stress disorder there appears to be lower-than-normal cortisol release, and it is thought that a blunted hormonal response to stress may predispose a person to develop PTSD.\n\nIt is also known that HPA axis hormones are related to certain skin diseases and skin homeostasis. There is evidence shown that the HPA axis hormones can be linked to certain stress related skin diseases and skin tumors. This happens when HPA axis hormones become hyperactive in the brain.\n\nThere is evidence that prenatal stress can influence HPA regulation. In animal experiments, exposure to prenatal stress has been shown to cause a hyper-reactive HPA stress response. Rats that have been prenatally stressed have elevated basal levels and abnormal circadian rhythm of corticosterone as adults. Additionally, they require a longer time for their stress hormone levels to return to baseline following exposure to both acute and prolonged stressors. Prenatally stressed animals also show abnormally high blood glucose levels and have fewer glucocorticoid receptors in the hippocampus. In humans, prolonged maternal stress during gestation is associated with mild impairment of intellectual activity and language development in their children, and with behaviour disorders such as attention deficits, schizophrenia, anxiety and depression; self-reported maternal stress is associated with a higher irritability, emotional and attentional problems.\n\nThere is growing evidence that prenatal stress can affect HPA regulation in humans. Children who were stressed prenatally may show altered cortisol rhythms. For example, several studies have found an association between maternal depression during pregnancy and childhood cortisol levels. Prenatal stress has also been implicated in a tendency toward depression and short attention span in childhood. There is no clear indication that HPA dysregulation caused by prenatal stress can alter adult behavior.\n\nThe role of early life stress in programming the HPA Axis has been well-studied in animal models. Exposure to mild or moderate stressors early in life has been shown to enhance HPA regulation and promote a lifelong resilience to stress. In contrast, early-life exposure to extreme or prolonged stress can induce a hyper-reactive HPA Axis and may contribute to lifelong vulnerability to stress. In one widely replicated experiment, rats subjected to the moderate stress of frequent human handling during the first two weeks of life had reduced hormonal and behavioral HPA-mediated stress responses as adults, whereas rats subjected to the extreme stress of prolonged periods of maternal separation showed heightened physiological and behavioral stress responses as adults.\n\nSeveral mechanisms have been proposed to explain these findings in rat models of early-life stress exposure. There may be a critical period during development during which the level of stress hormones in the bloodstream contribute to the permanent calibration of the HPA Axis. One experiment has shown that, even in the absence of any environmental stressors, early-life exposure to moderate levels of corticosterone was associated with stress resilience in adult rats, whereas exposure to high doses was associated with stress vulnerability.\n\nAnother possibility is that the effects of early-life stress on HPA functioning are mediated by maternal care. Frequent human handling of the rat pups may cause their mother to exhibit more nurturant behavior, such as licking and grooming. Nurturant maternal care, in turn, may enhance HPA functioning in at least two ways. First, maternal care is crucial in maintaining the normal stress hypo responsive period (SHRP), which in rodents, is the first two weeks of life during which the HPA axis is generally non-reactive to stress. Maintenance of the SHRP period may be critical for HPA development, and the extreme stress of maternal separation, which disrupts the SHRP, may lead to permanent HPA dysregulation. Another way that maternal care might influence HPA regulation is by causing epigenetic changes in the offspring. For example, increased maternal licking and grooming has been shown to alter expression of the glutocorticoid receptor gene implicated in adaptive stress response. At least one human study has identified maternal neural activity patterns in response to video stimuli of mother-infant separation as being associated with decreased glucocorticoid receptor gene methylation in the context of post-traumatic stress disorder stemming from early life stress. Yet clearly, more research is needed to determine if the results seen in cross-generational animal models can be extended to humans.\n\nThough animal models allow for more control of experimental manipulation, the effects of early life stress on HPA axis function in humans has also been studied. One population that is often studied in this type of research is adult victims of childhood abuse. Adult victims of childhood abuse have exhibited increased ACTH concentrations in response to a psychosocial stress task compared to healthy controls and subjects with depression but not childhood abuse. In one study, adult victims of childhood abuse that are not depressed show increased ACTH response to both exogenous CRF and normal cortisol release. Adult victims of childhood abuse that are depressed show a blunted ACTH response to exoegenous CRH. A blunted ACTH response is common in depression, so the authors of this work posit that this pattern is likely to be due to the participant's depression and not their exposure to early life stress.\n\nHeim and colleagues have proposed that early life stress, such as childhood abuse, can induce a sensitization of the HPA axis, resulting in particular heightened neuronal activity in response to stress-induced CRF release. With repeated exposure to stress, the sensitized HPA axis may continue to hypersecrete CRF from the hypothalamus. Over time, CRF receptors in the anterior pituitary will become down-regulated, producing depression and anxiety symptoms. This research in human subjects is consistent with the animal literature discussed above.\n\nThe HPA Axis was present in the earliest vertebrate species, and has remained highly conserved by strong positive selection due to its critical adaptive roles. The programming of the HPA axis is strongly influenced by the perinatal and early juvenile environment, or “early-life environment.” Maternal stress and differential degrees of caregiving may constitute early life adversity, which has been shown to profoundly influence, if not permanently alter, the offspring's stress and emotional regulating systems. Widely studied in animal models (e.g. licking and grooming/LG in rat pups), the consistency of maternal care has been shown to have a powerful influence on the offspring's neurobiology, physiology, and behavior. \nWhereas maternal care improves cardiac response, sleep/wake rhythm, and growth hormone secretion in the neonate, it also suppresses HPA axis activity. In this manner, maternal care negatively regulates stress response in the neonate, thereby shaping his/her susceptibility to stress in later life. These programming effects are not deterministic, as the environment in which the individual develops can either match or mismatch with the former's “programmed” and genetically predisposed HPA axis reactivity. Although the primary mediators of the HPA axis are known, the exact mechanism by which its programming can be modulated during early life remains to be elucidated. Furthermore, evolutionary biologists contest the exact adaptive value of such programming, i.e. whether heightened HPA axis reactivity may confer greater evolutionary fitness.\n\nVarious hypotheses have been proposed, in attempts to explain why early life adversity can produce outcomes ranging from extreme vulnerability to resilience, in the face of later stress. Glucocorticoids produced by the HPA axis have been proposed to confer either a protective or harmful role, depending on an individual's genetic predispositions, programming effects of early-life environment, and match or mismatch with one's postnatal environment. The predictive adaptation hypothesis (1), the three-hit concept of vulnerability and resilience (2) and the maternal mediation hypothesis (3) attempt to elucidate how early life adversity can differentially predict vulnerability or resilience in the face of significant stress in later life. These hypotheses are not mutually exclusive but rather are highly interrelated and unique to the individual.\n\n(1) The predictive adaptation hypothesis: this hypothesis is in direct contrast with the diathesis stress model, which posits that the accumulation of stressors across a lifespan can enhance the development of psychopathology once a threshold is crossed. Predictive adaptation asserts that early life experience induces epigenetic change; these changes predict or “set the stage” for adaptive responses that will be required in his/her environment. Thus, if a developing child (i.e., fetus to neonate) is exposed to ongoing maternal stress and low levels of maternal care (i.e., early life adversity), this will program his/her HPA axis to be more reactive to stress. This programming will have predicted, and potentially be adaptive in a highly stressful, precarious environment during childhood and later life. The predictability of these epigenetic changes is not definitive, however – depending primarily on the degree to which the individual's genetic and epigenetically modulated phenotype “matches” or “mismatches” with his/her environment (See: Hypothesis (2)).\n\n(2) Three-Hit Concept of vulnerability and resilience: this hypothesis states that within a specific life context, vulnerability may be enhanced with chronic failure to cope with ongoing adversity. It fundamentally seeks to explicate why, under seemingly indistinguishable circumstances, one individual may cope resiliently with stress, whereas another may not only cope poorly, but consequently develop a stress-related mental illness. The three “hits” – chronological and synergistic – are as follows: genetic predisposition (which predispose higher/lower HPA axis reactivity), early-life environment (perinatal – i.e. maternal stress, and postnatal – i.e. maternal care), and later-life environment (which determines match/mismatch, as well as a window for neuroplastic changes in early programming). (Figure 1)6 The concept of match/mismatch is central to this evolutionary hypothesis. In this context, it elucidates why early life programming in the perinatal and postnatal period may have been evolutionarily selected for. Specifically, by instating specific patterns of HPA axis activation, the individual may be more well equipped to cope with adversity in a high-stress environment. Conversely, if an individual is exposed to significant early life adversity, heightened HPA axis reactivity may “mismatch” him/her in an environment characterized by low stress. The latter scenario may represent maladaptation due to early programming, genetic predisposition, and mismatch. This mismatch may then predict negative developmental outcomes such as psychopathologies in later life.\n\nUltimately, the conservation of the HPA axis has underscored its critical adaptive roles in vertebrates, so, too, various invertebrate species over time. The HPA Axis plays a clear role in the production of corticosteroids, which govern many facets of brain development and responses to ongoing environmental stress. With these findings, animal model research has served to identify what these roles are – with regards to animal development and evolutionary adaptation. In more precarious, primitive times, a heightened HPA axis may have served to protect organisms from predators and extreme environmental conditions, such as weather and natural disasters, by encouraging migration (i.e. fleeing), the mobilization of energy, learning (in the face of novel, dangerous stimuli) as well as increased appetite for biochemical energy storage. In contemporary society, the endurance of the HPA axis and early life programming will have important implications for counseling expecting and new mothers, as well as individuals who may have experienced significant early life adversity.\n\n\n\nConditions:\n\n"}
{"id": "11582240", "url": "https://en.wikipedia.org/wiki?curid=11582240", "title": "Irvin Abell", "text": "Irvin Abell\n\nIrvin Abell (September 13, 1876 – August 28, 1949) was a surgeon from Louisville, Kentucky. \n\nAbell graduated from Louisville Medical College in 1897 and then studied in Germany at the University of Marburg and the University of Berlin. He joined the faculty at Louisville Medical College faculty in 1900 and became professor of surgery when the school merged with the University of Louisville in 1908. He was named to the school's board of trustees in 1935.\n\nAbell was the first Grand Presiding Senior (president) of Phi Chi Medical Fraternity (Southern) in 1897.\n\nAbell was president of the American Medical Association from 1938 to 1939, and also served as president of the American College of Surgeons, Southeastern Surgical Association, and the Kentucky State Medical Association. During World War II he headed the national committee that consulted with the Department of Defense on matters of public health.\n\nHe was buried in Cave Hill Cemetery in Louisville.\n\n"}
{"id": "49504556", "url": "https://en.wikipedia.org/wiki?curid=49504556", "title": "Jeffrey Escoffier", "text": "Jeffrey Escoffier\n\nJeffrey Escoffier (born October 9, 1942) is an American media strategist, writer, editor, and activist. He was the director of health media and marketing for the New York City Department of Health and Mental Hygiene from 1999 to 2015. Escoffier has long been an active participant in the LGBT community in Philadelphia, San Francisco, and New York City.\n\nIn 1972 he co-founded and served on the editorial board of The Gay Alternative (1972-1976), a gay and lesbian cultural magazine. He moved to San Francisco in 1977 where he co-founded the San Francisco Lesbian and Gay History Project. In 1978 he joined the editorial board of \"Socialist Review\", a democratic socialistic journal, and served as its Executive Editor from 1980 to 1988.\n\nIn 1988, Escoffier co-founded \"OUT/LOOK: A National Lesbian and Gay Quarterly\", which was one of the first joint lesbian and gay cultural ventures. Starting in 1990, OUT/LOOK sponsored, under Escoffier's leadership, a series of conferences called OutWrite that brought together over 1,200 LGBT writers from across the U.S. These conferences brought together several notable writers such as Judy Grahn, Allen Ginsberg, Cherrie Moraga, Gore Vidal, Edward Albee, and Essex Hemphill. In the wake of the OutWrite conferences he worked as a literary agent for lesbian and gay authors across the Bay Area.\n\nEscoffier served on the board of the Center for Lesbian and Gay Studies (CLAGS) at the City University of New York from 1992-1995 and then from 2010-2013. He was the Director of the CLAGS Project on Families, Values, and Public School Curriculum.\n\nIn 1995, he joined the NYC Department of Health and Mental Hygiene as the Deputy Director of the Office of Gay and Lesbian Health. In 2000 he became the Director of Health Media and Marketing and held that position until his retirement in August 2015. In that position he supervised the Department's media and public education campaigns on severals topics, such as smoking cessation, HIV prevention and testing, anti-obesity, Ebola, influenza, and immunization.\n\n"}
{"id": "39569344", "url": "https://en.wikipedia.org/wiki?curid=39569344", "title": "Kongo cosmogram", "text": "Kongo cosmogram\n\nThe cosmogram was a core symbol of the Kongo culture. An ideographic religious symbol, the cosmogram was called \"dikenga dia Kongo\" or \"tendwa kia nza-n' Kongo\" in the KiKongo language. Ethnohistorical sources and material culture demonstrate that the Kongo cosmogram existed as a long-standing symbolic tradition within the BaKongo culture before European contact in 1482, and that it continued in use in West Central Africa through the early twentieth century. In its fullest embellishment, this symbol served as an emblematic representation of the Kongo people and summarized a broad array of ideas and metaphoric messages that comprised their sense of identity within the cosmos.\n\nRobert Farris Thompson describes it as thus: \"Coded as a cross, a quartered circle or diamond, a seashell spiral, or a special cross with solar emblems at each ending - the sign of the four moments of the sun is the Kongo emblem of spiritual continuity and renaissance par excellence. In certain rites it is written on the earth, and a person stands upon it to take an oath, or to signify that he or she understands the meaning of life as a process shared with the dead below the river or the sea - the real sources of earthly power and prestige, in Kongo thinking... The intimation, by shorthand geometric statements, of mirrored worlds within the spiritual journey of the sun, is the source and illumination of some of the more important sculptural gestures and decorative signs pertaining to funerary monuments and objects designated for deposit on the surface of funerary tombs, or otherwise connected with funerary ceremonies and the end of life.\"\n"}
{"id": "53081685", "url": "https://en.wikipedia.org/wiki?curid=53081685", "title": "Lauren Zander", "text": "Lauren Zander\n\nLauren Handel Zander was born on February 3, 1970. She is a life coach and is the author of \"Maybe It's You: Cut the Crap. Face Your Fears. Love Your Life.\".\n\nIn the early 2000s, Zander created The Handel Method, a coaching methodology that has been taught in over 35 major universities and institutes of learning including Massachusetts Institute of Technology, Stanford Graduate School of Business, Stanford Medical School, New York University, Columbia University, Yale School of Drama, Wesleyan University, Fordham University, Rutgers University, Middlebury College, Scripps Research Institute, and in the New York City public school system.\n\nZander is the Co-Founder and Chairwoman of Handel Group, an international corporate consulting and private coaching company based in New York City.\n\nZander graduated from George Washington University in 1994 with a bachelor's degree in environmental studies.\n\nZander co-founded Handel Group in 2004 with her sister, Beth Weissenberger. In 2006, Zander began teaching a course based on The Handel Method to students, staff, and alumni at the Massachusetts Institute of Technology.\n\nIn 2010, Zander starred in a television special \"Celebrity Life Coach\" on A&E Biography with actress Sean Young.\n\nIn 2011, Zander presented her coaching methodology at the TEDx Women’s Conference in Amsterdam and was a moderator running the roundtable for the White House's Office of Social Innovation and Civic Participation at Stanford University.\n\nIn 2014, Zander appeared on the \"Dr. Oz\" show to discuss sustainable weight loss and how women can achieve their goals.\n\nInternationally, the Handel Method was taught for the first time in October 2015 in the CEMS Program at Vienna University of Economics and Business.\n\nZander's first book, \"Maybe It's You: Cut the Crap, Face your Fears, Love Your Life\" was published by Hachette Book Group in 2017.\n\nZander is a regular contributor to media outlets, including \"The New York Times\", Forbes \"Self\", \"Women’s Health\", \"Business Insider\", \"Mind Body Green\", and \"The Huffington Post\".\n\nIn 2011, Zander co-authored an article in the \"Harvard Business Review\" with Deborah H. Gruenfeld titled \"Authentic Leadership Can Be Bad Leadership\".\n\nIn 2017, Zander participated as a contributor to a piece on work-life balance for working mothers in \"Self\".\n"}
{"id": "3043713", "url": "https://en.wikipedia.org/wiki?curid=3043713", "title": "Leaf protein concentrate", "text": "Leaf protein concentrate\n\nLeaf protein concentrate (LPC) is a concentrated form of the proteins found in the leaves of plants. It has been examined as a human or animal food source, because it is potentially the cheapest, most abundant source of available protein. Although humans can derive some protein from the direct consumption of leaves as leaf vegetables, the human digestive system would not be able to deal with the enormous bulk of leaves needed to meet dietary protein requirements with leaf vegetables alone.\n\nLPC was first suggested as a human food in the 1960s, but it has not achieved much success, despite early promise. Norman Pirie (1971, 1975), the Copley Medal winner from the UK, reviewed and emphasized the importance of its benefits, which brought the subject forward. The increasing reliance on feedlot based animal rearing to satisfy human appetites for meat has increased demand for cheaper vegetable protein sources. This has recently led to renewed interest in LPC to reduce the use of human-edible vegetable protein sources in animal feed.\n\nLeaf protein is a good source of amino acids, with methionine being a limiting factor. Leaf proteins can also be rich in polyphenols. The challenges that have to be overcome before LPC from Lucerne and Cassava, two high density mono-culture crops, becomes a viable protein source for humans include the high fiber content and other antinutritional factors, such as phytate, cyanide, and tannins. Leaf for Life, a nonprofit organization dedicated to fighting malnutrition through encouraging increased consumption of vegetables and leaf crops, has extensive information on small scale LPC production using numerous plant species that both do not contain substantial concentrations of the anti-nutrients found in Cassava leaves or Lucerne and from which fiber can be removed through low tech processes.\n\nGenerally, LPC is produced by pulping leaves and pressing the juice out, heating the juice to coagulate the protein, and filtering the protein out and drying it.\n\n\n\n"}
{"id": "8743318", "url": "https://en.wikipedia.org/wiki?curid=8743318", "title": "Least distance of distinct vision", "text": "Least distance of distinct vision\n\n[[Image:Focal-length.png|frame|right|The focal point F and focal length \"f\" of a positive (convex) lens, a negative (concave) lens, a concave mirror, and a convex mirror.]\nIn [[optometry]], the least distance of distinct vision (LDDV) or the reference seeing distance (RSD) is the closest someone with \"normal\" vision ([[Snellen chart|20/20 vision]]) can comfortably look at something. In other words, LDDV is the minimum comfortable distance between the naked human eye and a visible object.\n\nmagnifying power (\"M\") of a lens with [[focal length]] (\"f\" in millimeters) when viewed by the naked human eye:\n\n[[Category:Optometry]]\n\nThe power of a lens or curved mirror in Dioptres is calculated by the relationship D=1/F, where D is the power in Dioptres and F is the focal length of the device in metres. A convex lens and a concave mirror have positive focal lengths, and hence their strength in Dioptres is also positive. If the lens is concave or the mirror is convex, the focal length, and hence the power in Dioptres, is negative."}
{"id": "48010748", "url": "https://en.wikipedia.org/wiki?curid=48010748", "title": "List of United Nations Security Council resolutions concerning North Korea", "text": "List of United Nations Security Council resolutions concerning North Korea\n\nThe Security Council of the United Nations (UNSC) has adopted 21 resolutions concerning North Korea. Five resolutions were adopted during the Korean War in the 1950s.\n\nIn 1991, a single resolution was adopted regarding North Korea's accession to membership in the UN. Since then, many resolutions have been adopted in relation to the North Korean missile and nuclear program.\n\nThe UN Security Council toughens the sanctions in response to North Korea's nuclear and missile tests. The sanctions on North Korea are mainly economic sanctions which regulate North Korea's economic activities such as trade with China. The resolutions' sanction is mainly about 'demands North Korea refrain from further nuclear or missile tests and return to the NPT'. Moreover, the sanctions resolutions try to regulate ban the exports of North Korea's natural resources such as coal and iron ore, and prohibit member states' export to North Korea which may contribute to North Korea's further nuclear and missile tests. The UN Security Council tries to urge North Korea for denuclearisation but it has been ineffective to prevent further nuclear and missile tests. Meanwhile, the most severe sanction of the UNSC is found to be a ban on crude oil exports to North Korea but it has not been done yet. In order to proceed the sanctions, the consensus between member states and international society is the first step to be made.\n\n\n"}
{"id": "18756507", "url": "https://en.wikipedia.org/wiki?curid=18756507", "title": "List of most recent executions by jurisdiction", "text": "List of most recent executions by jurisdiction\n\nThis is a list of the most recent legal executions performed by nations and other entities with criminal law jurisdiction over the people present within its boundaries. Extrajudicial executions and killings are not included. In general, executions performed in the territory of a sovereign state when it was a colony or before the sovereign state gained independence are \"not\" included. The colours on the map correspond to and have the same meanings as the colours in the charts.\n\n\n"}
{"id": "15956136", "url": "https://en.wikipedia.org/wiki?curid=15956136", "title": "Mary Lou's Flip Flop Shop", "text": "Mary Lou's Flip Flop Shop\n\nMary Lou's Flip Flop Shop is a children's television series featuring Olympic champion gymnast Mary Lou Retton. It was created to motivate young children to believe in themselves and get moving. The show takes place in a wacky \"Flip Flop Shop,\" and it features 4 \"real\" children per episode and five characters: Jumpy, Mr. Bump, Miss Warble, Professor Blinky, and L.Z. Bones. \n\nJumpy, a green and blue monkey, serves as Mary Lou's silent and energetic sidekick. Mr. Bump, a clumsy yet charming delivery-man, rides around on a noisy bike with a box full of interesting packages. Miss Warble, the singing custodian, constantly keeps watch over the cleanliness of the Flip Flop Shop. Professor Blinky, an owl puppet, never fails to share wise proverbs and stories with the members of the Flip Flop Shop. L.Z. Bones always tries to get out of physical activity and must be persuaded by the others to get up and join in the fun. \n\nThe show was produced by KUHT Houston and was shown on PBS affiliates. In 2008, the program was added to FamilyNet's Saturday morning children's program block.\n\nCast members include:\n\n"}
{"id": "354076", "url": "https://en.wikipedia.org/wiki?curid=354076", "title": "Maternal death", "text": "Maternal death\n\nMaternal death or maternal mortality is defined by the World Health Organization (WHO) as \"the death of a woman while pregnant or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy, from any cause related to or aggravated by the pregnancy or its management but not from accidental or incidental causes.\"\n\nThere are two performance indicators that are sometimes used interchangeably: maternal mortality ratio and maternal mortality rate, which confusingly both are abbreviated \"MMR\". By 2017, the world maternal mortality rate had declined 44% since 1990, but still every day 830 women die from pregnancy or childbirth related causes. According to the United Nations Population Fund (UNFPA) 2017 report, this is equivalent to \"about one woman every two minutes and for every woman who dies, 20 or 30 encounter complications with serious or long-lasting consequences. Most of these deaths and injuries are entirely preventable.\"\n\nUNFPA estimated that 303,000 women died of pregnancy or childbirth related causes in 2015. These causes range from severe bleeding to obstructed labour, all of which have highly effective interventions . As women have gained access to family planning and skilled birth attendance with backup emergency obstetric care, the global maternal mortality ratio has fallen from 385 maternal deaths per 100,000 live births in 1990 to 216 deaths per 100,000 live births in 2015, and many countries halved their maternal death rates in the last 10 years.\n\nAlthough attempts have been made in reducing maternal mortality, there is much room for improvement, particularly in impoverished regions. Over 85% of maternal deaths are from impoverished communities in Africa and Asia. The effect of a mother's death results in vulnerable families. Their infants, if they survive childbirth, are more likely to die before reaching their second birthday.\n\nAccording to a 2003 article in the \"British Medical Bulletin\", maternal death was first defined as \"the death of a woman while pregnant or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy, from any cause related to or aggravated by the pregnancy or its management but not from accidental or incidental causes \" in the tenth revision of the International Classification of Diseases (ICD-10) which was completed in 1992. It is the definition still in use by the World Health Organization (WHO), which defines maternal mortality as \"the death of a woman while pregnant or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy, from any cause related to or aggravated by the pregnancy or its management but not from accidental or incidental causes.\"\n\nThe 2003 article \"Global burden of maternal death and disability\" noted that the definition leaves out a segment of the population. According to the Centers for Disease Control, during the period 1974-75 in Georgia, US, 29% of maternal deaths \"occurred after 42 days of pregnancy termination and 6% occurred after 90 days post-partum.\" This may explain the CDC’s definition, extending the period of consideration “within 1 year of the end of pregnancy.” Adding to the WHO definition, the CDC also mentions that this death can be irrespective of the outcome of the pregnancy.\n\nSevere maternal morbidity or SMM, is an unanticipated acute or chronic health outcome after labor and delivery that detrimentally affects a woman's health. Severe Maternal Morbidity (SMM) includes any unexpected outcomes from labor or delivery that cause both short and long-term consequences to the mother’s overall health. There are nineteen total indicators used by the CDC to help identify SMM, with the most prevalent indicator being a blood transfusion.  Other indicators include an acute myocardial infarction (\"heart attack\"), aneurysm, and kidney failure. All of this identification is done by using ICD-10 codes, which are disease identification codes found in hospital discharge data. Using these definitions that rely on these codes should be used with careful consideration since some may miss some cases, have a low predictive value, or may be difficult for different facilities to operationalize. There are certain screening criteria that may be helpful and are recommended through the American \"College\" of Obstetricians and Gynecologists as well as the Society for Maternal-Fetal Medicine (SMFM). These screening criteria for SMM are for transfusions of four or more units of blood and admission of a pregnant woman or a postpartum woman to an ICU facility or unit.\n\nThe greatest proportion of women with SMM are those who require a blood transfusion during delivery, mostly due to excessive bleeding. Blood transfusions given during delivery due to excessive bleeding has increased the rate of mothers with SMM. The rate of SMM has increased almost 200% between 1993 (49.5 per 100,000 live births) and 2014 (144.0 per 100,000 live births). This can be seen with the increased rate of blood transfusions given during delivery, which increased from 1993 (24.5 per 100,000 live births) to 2014 (122.3 per 100,000 live births).\n\nIn the United States, severe maternal morbidity has increased over the last several years, impacting greater than 50,000 women in 2014 alone. There is no conclusive reason for this dramatic increase. It is thought that the overall state of health for pregnant women is impacting these rates. For example, complications can derive from underlying chronic medical conditions like diabetes, obesity, HIV/AIDs, and high blood pressure. These underlying conditions are also thought to lead to increased risk of maternal mortality.\n\nThe increased rate for SMM can also be indicative of potentially increased rates for maternal mortality, since without identification and treatment of SMM, these conditions would lead to increased maternal death rates. Therefore, diagnosis of SMM can be considered a “near miss” for maternal mortality. With this consideration, several different expert groups have urged obstetric hospitals to review SMM cases for opportunities that can lead to improved care, which in turn would lead to improvements with maternal health and a decrease in the number of maternal deaths.\n\nFactors that increase maternal death can be direct or indirect. In a 2009 article on maternal morbidity, the authors said, that generally, there is a distinction between a direct maternal death that is the result of a complication of the pregnancy, delivery, or management of the two, and an indirect maternal death, that is a pregnancy-related death in a patient with a preexisting or newly developed health problem unrelated to pregnancy. Fatalities during but unrelated to a pregnancy are termed \"accidental\", \"incidental\", or nonobstetrical maternal deaths.\n\nAccording to a study published in the \"Lancet\" which covered the period from 1990 to 2013, the most common causes are postpartum bleeding (15%), complications from unsafe abortion (15%), hypertensive disorders of pregnancy (10%), postpartum infections (8%), and obstructed labour (6%). Other causes include blood clots (3%) and pre-existing conditions (28%). Maternal mortality caused by severe bleeding and infections are mostly after childbirth. Indirect causes are malaria, anaemia, HIV/AIDS, and cardiovascular disease, all of which may complicate pregnancy or be aggravated by it . Risk factors associated with increased maternal death include the age of the mother, obesity before becoming pregnant, other pre-existing chronic medical conditions, and cesarean delivery.\n\nPregnancy-related deaths between 2011 and 2014 in the United States have been shown to have major contributions from non-communicable diseases and conditions, and the following are some of the more common causes related to maternal death: cardiovascular diseases (15.2%.), non-cardiovascular diseases (14.7%), infection or sepsis (12.8%), hemorrhage (11.5%), cardiomyopathy (10.3%), thrombotic pulmonary embolism (9.1%), cerebrovascular accidents (7.4%), hypertensive disorders of pregnancy (6.8%), amniotic fluid embolism (5.5%), and anesthesia complications (0.3%).\n\nAccording to a 2004 WHO publication, sociodemographic factors such as age, access to resources and income level are significant indicators of maternal outcomes. Young mothers face higher risks of complications and death during pregnancy than older mothers, especially adolescents aged 15 years or younger. Adolescents have higher risks for postpartum hemorrhage, puerperal endometritis, operative vaginal delivery, episiotomy, low birth weight, preterm delivery, and small-for-gestational-age infants, all of which can lead to maternal death. The leading cause of death for girls at the age of 15 in developing countries is complication through pregnancy and childbirth. They have more pregnancies, on average, than women in developed countries and it has been shown that 1 in 180 fifteen year old girls in developing countries who become pregnant will die due to complications during pregnancy or childbirth. This is compared to women in developed countries, where the likelihood is 1 in 4900 live births. However, in the United States, as many women of older age continue to have children, trends have seen the maternal mortality rate to rise in some states, especially among women over 40 years old.\n\nStructural support and family support influences maternal outcomes . Furthermore, social disadvantage and social isolation adversely affects maternal health which can lead to increases in maternal death. Additionally, lack of access to skilled medical care during childbirth, the travel distance to the nearest clinic to receive proper care, number of prior births, barriers to accessing prenatal medical care and poor infrastructure all increase maternal deaths.\n\nUnsafe abortion is another major cause of maternal death. According to the World Health Organization in 2009, every eight minutes a woman died from complications arising from unsafe abortions. Complications include hemorrhage, infection, sepsis and genital trauma.\n\nBy 2007, globally, preventable deaths from improperly performed procedures constitute 13% of maternal mortality, and 25% or more in some countries where maternal mortality from other causes is relatively low, making unsafe abortion the leading single cause of maternal mortality worldwide.\n\nAbortions are more common in developed regions than developing regions of the world. It is estimated that 26% of all pregnancies that occur in the world are terminated by induced abortions. Out of these, 41% occur in developed regions and 23% of them occur in developing regions.\n\nUnsafe abortion practices are defined by the WHO as procedures that are “carried out by persons either lacking the necessary skills or in an environment that does not conform to minimal medical standards, or both.\" Using this definition, the WHO estimates that out of the 45 million abortions that are performed each year globally, 19 million of these are considered unsafe. Also, 97% of these unsafe abortions occur in developing countries.\n\nMaternal deaths caused by improperly performed procedures are preventable and contribute 13% to the maternal mortality rate worldwide. This number is increased to 25% in countries where other causes of maternal mortality are low, such as in Eastern European and South American countries. This makes unsafe abortion practices the leading cause of maternal death worldwide.\n\nSocial factors impact a woman’s decision to seek abortion services, and these can include fear of abandonment from the partner, family rejection and lack of employment. Social factors such as these can lead to the consequence of undergoing an abortion that is considered unsafe.\n\nOne proposal for measuring trends and variations in risks to maternal death associated with maternal death is to measure the percentage of induced abortions that are defined unsafe (by the WHO) and by the ratio of deaths per 100,000 procedures, which would be defined as the abortion mortality ratio.\n\nThere are four primary types of data sources that are used to collect abortion-related maternal mortality rates. These four sources are confidential enquiries, registration data, verbal autopsy, and facility-based data sources. A verbal autopsy is a systematic tool that is used to collect information on the cause of death from lay-people and not medical professionals.\n\nConfidential enquires for maternal deaths do not occur very often on a national level in most countries. Registration systems are usually considered the “gold-standard” method for mortality measurements. However, they have been shown to miss anywhere between 30-50% of all maternal deaths. Another concern for registration systems is that 75% of all global births occur in countries where vital registration systems do not exist, meaning that many maternal deaths occurring during these pregnancies and deliveries may not be properly record through these methods. There are also issues with using verbal autopsies and other forms of survey in recording maternal death rates. For example, the family’s willingness to participate after the loss of a loved one, misclassification of the cause of death, and under-reporting all present obstacles to the proper reporting of maternal mortality causes. Finally, an potential issue with facility-based data collection on maternal mortality is the likelihood that women who experience abortion-related complications to seek care in medical facilities. This is due to fear of social repercussions or legal activity in countries where unsafe abortion is common since it is more likely to be legally restrictive and/or more highly stigmatizing. Another concern for issues related to errors in proper reporting for accurate understanding of maternal mortality is the fact that global estimates of maternal deaths related to a specific cause present those related to abortion as a proportion of the total mortality rate. Therefore, any change, whether positive or negative, in the abortion-related mortality rate is only compared relative to other causes, and this does not allow for proper implications of whether abortions are becoming more safe or less safe with respect to the overall mortality of women.\n\nProviding safe services for pregnant women within family planning facilities is applicable to all regions. This is an important fact to consider since abortion is legal in some way in 189 out of 193 countries worldwide. Promoting effective contraceptive use and information distributed to a wider population, with access to high-quality care, can significantly make strides towards reducing the number of unsafe abortions. However, this alone will not eliminate the demand for safe services.\n\nThe four measures of maternal death are the maternal mortality ratio (MMR), maternal mortality rate, lifetime risk of maternal death and proportion of maternal deaths among deaths of women of reproductive years (PM).\n\nMaternal mortality ratio (MMR): the ratio of the number of maternal deaths during a given time period per 100,000 live births during the same time-period. The MMR is used as a measure of the quality of a health care system.\n\nMaternal mortality rate (MMRate): the number of maternal deaths in a population divided by the number of women of reproductive age, usually expressed per 1,000 women.\nLifetime risk of maternal death: refers to the probability that a 15-year-old female will die eventually from a maternal cause if she experiences throughout her lifetime the risks of maternal death and the overall levels of fertility and mortality that are observed for a given population. The adult lifetime risk of maternal mortality can be derived using either the maternal mortality ratio (MMR), or the maternal mortality rate (MMRate). \nProportion of maternal deaths among deaths of women of reproductive age (PM):\nthe number of maternal deaths in a given time period divided by the total deaths among \nwomen aged 15–49 years.\n\nApproaches to measuring maternal mortality includes civil registration system, household surveys, census, reproductive age mortality studies (RAMOS) and verbal autopsies.\n\nThe United Nations Population Fund (UNFPA; formerly known as the United Nations Fund for Population Activities) have established programs that support efforts in reducing maternal death. These efforts include education and training for midwives, supporting access to emergency services in obstetric and newborn care networks, and providing essential drugs and family planning services to pregnant women or those planning to become pregnant. They also support efforts for review and response systems regarding maternal deaths.\n\nAccording to the 2010 United Nations Population Fund report, developing nations account for ninety-nine percent of maternal deaths with the majority of those deaths occurring in Sub-Saharan Africa and Southern Asia. \nGlobally, high and middle income countries experience lower maternal deaths than low income countries. The Human Development Index (HDI) accounts for between 82 and 85 percent of the maternal mortality rates among countries. In most cases, high rates of maternal deaths occur in the same countries that have high rates of infant mortality. These trends are a reflection that higher income countries have stronger healthcare infrastructure, medical and healthcare personnel, use more advanced medical technologies and have fewer barriers to accessing care than low income countries. Therefore, in low income countries, the most common cause of maternal death is obstetrical hemorrhage, followed by hypertensive disorders of pregnancy, in contrast to high income countries, for which the most common cause is thromboembolism.\n\nBetween 1990 and 2015, the maternal mortality ratio has decreased from 385 deaths per 100,000 live births to 216 maternal deaths per 100,000 live births. Some factors that have attributed to the decreased maternal deaths seen between this period are in part to the access that women have gained to family planning services and skilled birth attendance, meaning a midwife, doctor, or trained nurse), with back-up obstetric care for emergency situations that may occur during the process of labor. This can be examined further by looking at statistics in some areas of the world where inequities in women’s access to health care services reflect an increased number of maternal deaths. The high maternal death rates also reflect access to health services between the poor communities compared to women who are rich.\n\nAt a country level, India (19% or 56,000) and Nigeria (14% or 40,000) accounted for roughly one third of the maternal deaths in 2010 . Democratic Republic of the Congo, Pakistan, Sudan, Indonesia, Ethiopia, United Republic of Tanzania, Bangladesh and Afghanistan accounted for between 3 and 5 percent of maternal deaths each. These ten countries combined accounted for 60% of all the maternal deaths in 2010 according to the United Nations Population Fund report. Countries with the lowest maternal deaths were Greece, Iceland, Poland, and Finland.\n\nUntil the early 20th century developed and developing countries had similar rates of maternal mortality. Since most maternal deaths and injuries are preventable, they have been largely eradicated in the developed world.\n\nA lot of progress has been made since the United Nations made the reduction of maternal mortality part of the Millennium Development Goals (MDGs) in 2000. Bangladesh, for example, cut the number of deaths per live births by almost two thirds from 1990 to 2015. However, the MDG was to reduce it by 75%. According to government data, the figure for 2015 was 181 maternal deaths per 100,000 births. The MDG mark was 143 per 100,000. A further reduction of maternal mortality is now part of the Agenda 2030 for sustainable development. The United Nations has more recently developed a list of goals termed the Sustainable Development Goals. The target of the third Sustainable Development Goal (SDG) is to reduce the global maternal mortality rate (MMR) to less than 70 per 100,000 live births by 2030. Some of the specific aims of the Sustainable Development Goals are to prevent unintended pregnancies by ensuring more women have access to contraceptives, as well as providing women who become pregnant with a safe environment for delivery with respectful and skilled care during delivery. This also includes providing women with complications during delivery timely access to emergency services through obstetric care.\n\nThe WHO has also developed a global strategy and goal to end preventable death related to maternal mortality. A major goal of this strategy is to identify and address the causes of maternal and reproductive morbidities and mortalities, as well as disabilities related to maternal health outcomes. The collaborations that this strategy introduces are to address the inequalities that are shown with access to reproductive, maternal, and newborn services, as well as the quality of that care. They also ensure that universal health coverage is essential for comprehensive health care services related to maternal and newborn health. The WHO strategy also implements strengthening health care systems to ensure quality data collection to better respond to the needs of women and girls, as well as ensuring responsibility and accountability to improve the equity and quality of care provided to women.\n\nThere are significant maternal mortality intracountry variations, especially in nations with large equality gaps in income and education and high healthcare disparities. Women living in rural areas experience higher maternal mortality than women living in urban and sub-urban centers because those living in wealthier households, having higher education, or living in urban areas, have higher use of healthcare services than their poorer, less-educated, or rural counterparts. There are also racial and ethnic disparities in maternal health outcomes which increases maternal mortality in marginalized groups.\n\nThe US has the \"highest rate of maternal mortality in the industrialized world.\" In the United States, the maternal death rate averaged 9.1 maternal deaths per 100,000 live births during the years 1979–1986, but then rose rapidly to 14 per 100,000 in 2000 and 17.8 per 100,000 in 2009. In 2013 the rate was 18.5 deaths per 100,000 live births. It has been suggested that the rise in maternal death in the United States may be due to improved identification and misclassification resulting in false positives. The rate has steadily increased to 18.0 deaths per 100,000 live births in 2014. Between 2011 and 2014, there were 7,208 deaths that were reported to the CDC that occurred for women within a year of the end of their pregnancy. Out of this there were 2,726 that were found to be pregnancy-related deaths.\n\nSince 2016, ProPublica and NPR investigated factors that led to the increase in maternal mortality in the United States. They reported that the \"rate of life-threatening complications for new mothers in the U.S. has more than doubled in two decades due to pre-existing conditions, medical errors and unequal access to care.\" According to the Centers for Disease Control and Prevention, c. 4 million women who give birth in the US annually, over 50,000 a year, experience \"dangerous and even life-threatening complications.\"\n\nAccording to a report by the United States Centers for Disease Control and Prevention, in 1993 the rate of Severe Maternal Morbidity, rose from 49.5 to 144 \"per 10,000 delivery hospitalizations\" in 2014, an increase of almost 200 percent. Blood transfusions also increased during the same period with \"from 24.5 in 1993 to 122.3 in 2014 and are considered to be the major driver of the increase in SMM. After excluding blood transfusions, the rate of SMM increased by about 20% over time, from 28.6 in 1993 to 35.0 in 2014.\"\n\nThe past 60 years have consistently shown considerable racial disparities in pregnancy-related deaths. Between 2011 and 2014, the mortality ratio for different racial populations based on pregnancy-related deaths were as follows: 12.4 deaths per 100,000 live births for white women, 40,0 for black women, and 17.8 for women of other races. This shows that black women have between three and four times greater chance of dying from pregnancy-related issues. It has also been shown that one of the major contributors to maternal health disparities within the United States is the growing rate of non-communicable diseases.\n\nIt is unclear why pregnancy-related deaths in the United States have increased. It seems that the use of computerized data servers by the states and changes in the way deaths are coded, with a pregnancy checkbox added to death certificates in many states, have been shown to improve the identification of these pregnancy-related deaths. However, this does not contribute to decreasing the actual number of deaths. Also, errors in reporting of pregnancy status have been seen, which most likely leads to overestimation of the number of pregnancy-related deaths. Again, this does not contribute to explaining why the death rate has increased, but does show complications between reporting and actual contributions to the overall rate of maternal mortality.\n\nEven though 99% of births in the United States are attended by some form of skilled health professional, the maternal mortality ratio in 215 was 14 deaths per 100,000 live births and it has been shown that the maternal mortality rate has been increasing. Also, the United States is not as efficient at preventing pregnancy-related deaths when compared to most of the other developed nations.\n\nThe United States took part in the Millennium Development Goals (MDGs) set forth from the United Nations. The MDGs ended in 2015 but were followed-up in the form of the Sustainable Development Goals starting in 2016. The MDGs had several tasks, one of which was to improve maternal mortality rates globally. Despite their participation in this program as well as spending more than any other country on hospital-based maternal care, however, the United States has still seen increased rates of maternal mortality. This increased maternal mortality rate was especially pronounced in relation to other countries who participated in the program, where during the same period, the global maternal mortality rate decreased by 44%. Also, the United States is not currently on track to meet the Healthy People 2020 goal of decreasing maternal mortality by 10% by the year 2020, and continues to fail in meeting national goals in maternal death reduction. Only 23 states have some form of policy that establishes review boards specific to maternal mortality as of the year 2010.\n\nIn an effort to respond to the maternal mortality rate in the United States, the CDC requests that the 52 reporting regions (all states and New York City and Washington DC) to send death certificates for all those women who have died and may fit their definition of a pregnancy-related death, as well as copies of the matching birth or death records for the infant. However, this request is voluntary and some states may not have the ability to abide by this effort.\n\nThe Affordable Care Act (ACA) provided additional access to maternity care by expanding opportunities to obtain health insurance for the uninsured and mandating that certain health benefits have coverage. It also expanded the coverage for women who have private insurance. This expansion allowed them better access to primary and preventative health care services, including for screening and management of chronic diseases. An additional benefit for family planning services was the requirement that most insurance plans cover contraception without cost sharing. However, more employers are able to claim exemptions for religious or moral reasons under the current administration. Also under the current administration, the Department of Health and Human Services (HHS) has decreased funding for pregnancy prevention programs for adolescent girls.\n\nThose women covered under Medicaid are covered when they receive prenatal care, care received during childbirth, and postpartum care. These services are provided to nearly half of the women who give birth in the United States. Currently, Medicaid is required to provide coverage for women whose incomes are at 133% of the federal poverty level in the United States.\n\nThe death rate for women giving birth plummeted in the twentieth century. The historical level of maternal deaths is probably around 1 in 100 births. Mortality rates reached very high levels in maternity institutions in the 1800s, sometimes climbing to 40 percent of patients (see Historical mortality rates of puerperal fever). At the beginning of the 1900s, maternal death rates were around 1 in 100 for live births. Currently, there are an estimated 275,000 maternal deaths each year. Public health, technological and policy approaches are steps that can be taken to drastically reduce the global maternal death burden. For developing regions, where it has been shown that maternal mortality is greater than in developed nations, antenatal care has increased from 65% in 1990 to 83% in 2012.\n\nIt was estimated that in 2015, a total of 303,000 women died due to causes related to pregnancy or childbirth. The majority of these causes were either severe bleeding, sepsis, eclampsia, labor that had some type of obstruction, and consequences from unsafe abortions. All of these causes are either preventable or have highly effective interventions. Another factor that contributes to the maternal mortality rate that have opportunities for prevention are access to prenatal care for women who are pregnant. Women who do not receive prenatal care are between three and four times more likely to die from complications resulting from pregnancy or delivery than those who receive prenatal care. For women in the United States, 25% do not receive the recommended number of prenatal visits, and this number increases for women among specific demographic populations: 32% for African American women and 41% for American Indian and Alaska Native women.\n\nFour elements are essential to maternal death prevention, according to UNFPA. First, prenatal care. It is recommended that expectant mothers receive at least four antenatal visits to check and monitor the health of mother and fetus. Second, skilled birth attendance with emergency backup such as doctors, nurses and midwives who have the skills to manage normal deliveries and recognize the onset of complications. Third, emergency obstetric care to address the major causes of maternal death which are hemorrhage, sepsis, unsafe abortion, hypertensive disorders and obstructed labour. Lastly, postnatal care which is the six weeks following delivery. During this time, bleeding, sepsis and hypertensive disorders can occur, and newborns are extremely vulnerable in the immediate aftermath of birth. Therefore, follow-up visits by a health worker to assess the health of both mother and child in the postnatal period is strongly recommended.\n\nWomen who have unwanted pregnancies who have access to reliable information as well as compassionate counseling and quality services for the management of any issues that arise from abortions (whether safe or unsafe) can be beneficial in reducing the number of maternal deaths. Also, in regions where abortion is not against the law, then abortion practices need to be safe in order to effectively reduce the number of maternal deaths related to abortion.\n\nMaternal Death Surveillance and Response is another strategy that has been used to prevent maternal death. This is one of the interventions proposed to reduce maternal mortality where maternal deaths are continuously reviewed to learn the causes and factors that led to the death. The information from the reviews is used to make recommendations for action to prevent future similar deaths. Maternal and perinatal death reviews have been in practice for a long time worldwide, and the World Health Organization (WHO) introduced the Maternal and Perinatal Death Surveillance and Response (MPDSR) with a guideline in 2013. Studies have shown that acting on recommendations from MPDSR can reduce maternal and perinatal mortality by improving quality of care in the community and health facilities.\n\nThe decline in maternal deaths has been due largely to improved asepsis, fluid management and blood transfusion, and better prenatal care.\n\nTechnologies have been designed for resource poor settings that have been effective in reducing maternal deaths as well. The non-pneumatic anti-shock garment is a low-technology pressure device that decreases blood loss, restores vital signs and helps buy time in delay of women receiving adequate emergency care during obstetric hemorrhage. It has proven to be a valuable resource. Condoms used as uterine tamponades have also been effective in stopping post-partum hemorrhage.\n\nA public health approach to addressing maternal mortality includes gathering information on the scope of the problem, identifying key causes, and implementing interventions, both prior to pregnancy and during pregnancy, to combat those causes.\n\nPublic health has a role to play in the analysis of maternal death. One important aspect in the review of maternal death and its causes are Maternal Mortality Review Committees or Boards. The goal of these review committees are to analyze each maternal death and determine its cause. After this analysis, the information can be combined in order to determine specific interventions that could lead to preventing future maternal deaths. These review boards are generally comprehensive in their analysis of maternal deaths, examining details that include mental health factors, public transportation, chronic illnesses, and substance use disorders. All of this information can be combined to give a detailed picture of what is causing maternal mortality and help to determine recommendations to reduce their impact. \n\nMany states within the US are taking Maternal Mortality Review Committees a step further and are collaborating with various professional organizations to improve quality of perinatal care. These teams of organizations form a \"perinatal quality collaborative,\" or PQC, and include state health departments, the state hospital association and clinical professionals such as doctors and nurses. These PQCs can also involve community health organizations, Medicaid representatives, Maternal Mortality Review Committees and patient advocacy groups. By involving all of these major players within maternal health, the goal is to collaborate and determine opportunities to improve quality of care. Through this collaborative effort, PQCs can aim to make impacts on quality both at the direct patient care level and through larger system devices like policy. It is thought that the institution of PQCs in California was the main contributor to the maternal mortality rate decreasing by 50% in the years following. The PQC developed review guides and quality improvement initiatives aimed at the most preventable and prevalent maternal deaths: those due to bleeding and high blood pressure. Success has also been observed with PQCs in Illinois and Florida.\n\nSeveral interventions prior to pregnancy have been recommended in efforts to reduce maternal mortality. Increasing access to reproductive healthcare services, such as family planning services and safe abortion practices, is recommended in order to prevent unintended pregnancies. Several countries, including India, Brazil, and Mexico, have seen some success in efforts to promote the use of reproductive healthcare services. Other interventions include high quality sex education, which includes pregnancy prevention and sexually-transmitted infection (STI) prevention and treatment. By addressing STIs, this not only reduces perinatal infections, but can also help reduce ectopic pregnancy caused by STIs. Adolescents are between two and five times more likely to suffer from maternal mortality than a female twenty years or older. Access to reproductive services and sex education could make a large impact, specifically on adolescents, who are generally uneducated in regards to carrying a healthy pregnancy. Education level is a strong predictor of maternal health as it gives women the knowledge to seek care when it is needed.\nPublic health efforts can also intervene during pregnancy to improve maternal outcomes. Areas for intervention have been identified in access to care, public knowledge about signs and symptoms of pregnancy complications, and improving relationships between healthcare professionals and expecting mothers.\n\nAccess to care during pregnancy is a significant issue in the face of maternal mortality. \"Access\" encompasses a wide range of potential difficulties including costs, location of healthcare services, availability of appointments, transportation services, and cultural or language barriers that could inhibit a woman from receiving proper care. For women carrying a pregnancy to term, access to necessary antenatal (prior to delivery) healthcare visits is crucial to ensuring healthy outcomes. These antenatal visits allow for early recognition and treatment of complications, treatment of infections and the opportunity to educate the expecting mother on how to manage her current pregnancy and the health advantages of spacing pregnancies apart. Access to birth at a facility with a skilled healthcare provider present has been associated with safer deliveries and better outcomes. The two areas bearing the largest burden of maternal mortality, Sub-Saharan Africa and South Asia, also had the lowest percentage of births attended by a skilled provider, at just 45% and 41% respectively. Emergency obstetric care is also crucial in preventing maternal mortality by offering services like emergency cesarean sections, blood transfusions, antibiotics for infections and assisted vaginal delivery with forceps or vacuum. In addition to physical barriers that restrict access to healthcare, financial barriers also exist. Close to one out of seven women of child-bearing age have no health insurance. This lack of insurance impacts access to pregnancy prevention, treatment of complications, as well as perinatal care visits. \n\nBy increasing public knowledge about pregnancy, including signs of complications that need addressed by a healthcare provider, this will increase the likelihood of an expecting mother to seek help when it is necessary. Higher levels of education have been associated with increased use of contraception and family planning services as well as antenatal care. Addressing complications at the earliest sign of a problem can improve outcomes for expecting mothers, which makes it extremely important for a pregnant woman to be knowledgeable enough to seek healthcare for potential complications. Improving the relationships between patients and the healthcare system as a whole will make it easier for a pregnant woman to feel comfortable seeking help. Good communication between patients and providers, as well as cultural competence of the providers, could also assist in increasing compliance with recommended treatments.\n\nThe biggest global policy initiative for maternal health came from the United Nations' Millennium Declaration which created the Millennium Development Goals. In 2012, this evolved at the United Nations Conference on Sustainable Development to become the Sustainable Development Goals (SDGs) with a target year of 2030. The SDGs are 17 goals that call for global collaboration to tackle a wide variety of recognized problems. Goal 3 is focused on ensuring health and well-being for people of all ages. A specific target is to achieve a global maternal mortality ratio of less than 70 per 100,000 live births. So far, specific progress has been made in births attended by a skilled provider, now at 80% of births worldwide compared with 62% in 2005. \n\nCountries and local governments have taken political steps in reducing maternal deaths. Researchers at the Overseas Development Institute studied maternal health systems in four apparently similar countries: Rwanda, Malawi, Niger, and Uganda. In comparison to the other three countries, Rwanda has an excellent recent record of improving maternal death rates. Based on their investigation of these varying country case studies, the researchers conclude that improving maternal health depends on three key factors: 1. reviewing all maternal health-related policies frequently to ensure that they are internally coherent; 2. enforcing standards on providers of maternal health services; 3. any local solutions to problems discovered should be promoted, not discouraged.\n\nIn terms of aid policy, proportionally, aid given to improve maternal mortality rates has shrunken as other public health issues, such as HIV/AIDS, have become major international concerns. Maternal health aid contributions tend to be lumped together with newborn and child health, so it is difficult to assess how much aid is given directly to maternal health to help lower the rates of maternal mortality. Regardless, there has been progress in reducing maternal mortality rates internationally.\n\nIn countries where abortion practices are not considered legal, it is necessary to look at the access that women have to high-quality family planning services, since some of the restrictive policies around abortion could impede access to these services. These policies may also affect the proper collection of information for monitoring maternal health around the world.\n\nMaternal deaths and disabilities are leading contributors in women's disease burden with an estimated 275,000 women killed each year in childbirth and pregnancy worldwide. In 2011, there were approximately 273,500 maternal deaths (uncertainty range, 256,300 to 291,700). Forty-five percent of postpartum deaths occur within 24 hours. Ninety-nine percent of maternal deaths occur in developing countries.\n\n\n"}
{"id": "36371547", "url": "https://en.wikipedia.org/wiki?curid=36371547", "title": "Maurice Tweedie", "text": "Maurice Tweedie\n\nMaurice Charles Kenneth Tweedie, British medical physicist and statistician from the University of Liverpool, was born in Reading, England September 30, 1919 and died March 14, 1996. \nHe read physics at the University of Reading and attained a B.Sc. (general) and B.Sc. (special) in physics in 1939 followed by a M.Sc. in physics 1941. He found a career in radiation physics, but his primary interest was in mathematical statistics where his accomplishments far surpassed his academic postings. This included pioneering work with the inverse Gaussian distribution. Arguably his major achievement rests with the definition of a family of exponential dispersion models characterized by closure under additive and reproductive convolution as well as under transformations of scale that are now known as the Tweedie exponential dispersion models.\nAs a consequence of these properties the Tweedie exponential dispersion models are characterized by a power law relationship between the variance and the mean which leads them to become the foci of convergence for a central limit like effect that acts on a wide variety of random data. The range of application of the Tweedie distributions is wide and includes:\n"}
{"id": "3828753", "url": "https://en.wikipedia.org/wiki?curid=3828753", "title": "Medication discontinuation", "text": "Medication discontinuation\n\nMedication discontinuation is the ceasing of a medication treatment for a patient by either the clinician or the patient themself. When initiated by the clinician, it is known as deprescribing. Medication discontinuation is an important medical practice that may be motivated by a number of reasons:\n\nUnlike the prescribing of medications, appropriate discontinuation has not attracted nearly as much attention or interest.\n\nMedications may be stopped in the context of end-of-life care, such as medications that may affect risk factors for future disease. Medications that may be stopped as part of discussions about end-of-life care include antihypertensives, medications for diabetes, and drugs for high cholesterol.\n\nDrug discontinuation may cause rebound effects (return of the symptoms the drug relieved, and that, to a degree stronger than they were before treatment first began) and withdrawal syndromes (symptoms caused by the discontinuation by the drug itself).\n\nDrug discontinuation may be difficult to adjust to, owing to the long term use and the symbolism associated with ceasing medications, such as the decision to stop chemotherapy.\n\nRecent research (Nixon & Vendelø, 2016) shows that General Practitioners (GPs) who actively consider discontinuation, are reluctant to do so, as they experience that the safest decision is to continue prescriptions, rather than discontinue them. In part this is due to the ambiguity about the appropriateness of discontinuing medication. The clinical guidelines available to GPs do not encourage discontinuation of medication, and thus, they offer GPs a weak frame for discontinuation.\n\n"}
{"id": "9630460", "url": "https://en.wikipedia.org/wiki?curid=9630460", "title": "National Digestive Diseases Information Clearinghouse", "text": "National Digestive Diseases Information Clearinghouse\n\nThe National Digestive Diseases Information Clearinghouse (NDDIC) is an information dissemination service of the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK). The NIDDK is part of the National Institutes of Health, which is under the U.S. Department of Health and Human Services.\n\nEstablished in 1980, the Clearinghouse provides information about digestive diseases to people with digestive disorders and to their families, health care professionals, and the public. The NDDIC answers inquiries, develops and distributes publications, and works closely with professional and patient organizations and Government agencies to coordinate resources about digestive diseases.\n\nPublications produced by the Clearinghouse are carefully reviewed by both NIDDK scientists and outside experts.\n\n"}
{"id": "41379268", "url": "https://en.wikipedia.org/wiki?curid=41379268", "title": "National Health Laboratory Service", "text": "National Health Laboratory Service\n\nThe National Health Laboratory Service (NHLS) is a South African national government institution established in 2001. It was created by merging the South African Institute for Medical Research (SAIMR), the National Centre for Occupational Health and the National Institute for Virology. It also absorbed various provincial health department and university-run pathology laboratories.\n\nThe NHLS is the diagnostic pathology service for the public sector in South Africa. A network of 265 laboratories service all public hospitals and clinics in the country.\n\nBesides the network of pathology laboratories operated by the NHLS the institution also has a number of specialist divisions:\n\nThe National Institute for Communicable Diseases (NICD) was created by combining various structures inherited from the NHLS's parent organisations. The former National Institute for Virology was combined with the former SAIMR's specialist laboratories of microbiology, parasitology, and entomology to create a communicable diseases institute with a public health orientation, comparable to the Centers for Disease Control and Prevention of the United States.\n\nThe National Institute for Occupational Health (NIOH) investigates occupational diseases and performs occupational environment analysis through a variety of services which include statutory autopsy, advisory and information services, health hazard evaluation and specialist laboratory services.\n\nThe services and units of the NIOH are:\n\nThe NIOH also manages the National Cancer Registry which is responsible for analysing newly diagnosed cancer cases and to report annual cancer incidence rates. The NCR collects data from public and private histopathology, cytology and haematology laboratories across the whole country.\n\nSouth African Vaccine Producers (SAVP) is a subsidiary of the National Health Laboratory Service (NHLS) responsible for manufacturing vaccines and antivenoms. SAVP's Antivenom Unit is the only manufacturer of antivenom for a number of venomous snakes found in Africa as well as scorpion and spider antivenom.\n\nThe NHLS is involved in training all pathologists in South Africa through its partnerships with all nine of the medical schools at South African universities.\n\nThe NHLS is involved in pathology and health surveillance research through the pathology laboratories it runs at the medical schools. The partnerships with universities concentrate on a few priority areas; HIV/AIDS, tuberculosis, malaria, occupational health, malnutrition prevention, cervical cancer screening and pneumococcal infections.\n\nDuring 2011 the Treatment Action Campaign, an HIV/AIDS non-governmental organization criticized the national government for allowing the financial viability of the NHLS to be threatened due to the failure of the health departments of Gauteng and Kwa-Zulu Natal to pay the NHLS for services rendered. The financial problems continued into 2012.\n"}
{"id": "7183423", "url": "https://en.wikipedia.org/wiki?curid=7183423", "title": "Patient-reported outcome", "text": "Patient-reported outcome\n\nA patient-reported outcome (PRO) is a health outcome directly reported by the patient who experienced it. It stands in contrast to an outcome reported by someone else, such as a physician-reported outcome, a nurse-reported outcome, and so on. PRO methods, such as questionnaires, are used in clinical trials or other clinical settings, to help better understand a treatment's efficacy or effectiveness. The use of digitized PROs, or electronic patient-reported outcomes (ePROs), is on the rise in today's health research setting.\n\nPROs should not be confused with PCOs, or \"patient-centered outcomes\". The latter implies the use of a questionnaire covering issues and concerns that are specific to a patient. Instead, \"patient-reported\" outcomes refers to reporting situations in which only the patient provides information related to a specific treatment or condition; this information may or may not be of concern to the patient.\n\nFurther, PROs should not be confused with PREMs (patient reported experience measures), which focus more on a patient's overall experience versus a focus on specific treatment outcomes. The term PROs is becoming increasingly synonymous with \"patient reported outcome measures\" (PROMs).\n\nPRO is an umbrella term that covers a whole range of potential measurements, but it specifically refers to \"self-reporting\" by the patient. PRO data may be collected via self-administered questionnaires, which the patient completes themselves, or through patient interviews. The latter will only qualify as a PRO, however, if the interviewer is gaining the patient's views and not using the responses to make a professional assessment or judgment of the impact of a treatment on the patient's condition. Thus, PROs are used as a means of gathering patient- rather than clinical- or other outcomes perspectives. The patient-reported perspective can be an important asset in gaining treatment or drug approval.\n\nThere is no incentive for patients to report their outcome data other than to \"pay it forward\" to the community and help the health industry prevent unnecessary suffering in other patients.\n\nA well-designed PRO questionnaire should assess either a single underlying characteristic or, where it addresses multiple characteristics, should be a number of scales that each address a single characteristic. These measurement \"characteristics\" are termed \"constructs\" and the questionnaires used to collect them, termed \"instruments\", \"measures\", \"scales\" or \"tools\". Typically, PRO tools must undergo extensive validation and testing.\n\nA questionnaire that measures a single construct is described as unidimensional. Items (questions) in a unidimensional questionnaire can be added to provide a single scale score. However, it cannot be assumed that a questionnaire is unidimensional simply because the author intended it to be. This must be demonstrated empirically (for example, by confirmatory factor analysis or Rasch analysis). A questionnaire that measures multiple constructs is termed multi-dimensional. A multi-dimensional questionnaire is used to provide a profile of scores; that is, each scale is scored and reported separately. It is possible to create an overall (single summary) score from a multi-dimensional measure using factor analysis or preference-based methods but some may see this as akin to adding apples and oranges together.\n\nQuestionnaires may be generic (designed to be used in any disease population and cover a broad aspect of the construct measured) or condition-targeted (developed specifically to measure those aspects of outcome that are of importance for a people with a particular medical condition).\n\nThe most commonly used PRO questionnaires assess one of the following constructs:\n\nMeasures of symptoms may focus on a range of impairments or on a specific impairment such as depression or pain. Measures of functioning assess activities such as personal care, activities of daily living and locomotor activities. Health-related quality of life instruments are generally multi-dimensional questionnaires assessing a combination of aspects of impairments and/or disability and reflect a patient's health status. In contrast, QoL goes beyond impairment and disability by asking about the patient's ability to fulfill their needs and also about their emotional response to their restrictions.\n\nA new generation of short and easy-to-use tools to monitor patient outcomes on a regular basis has been recently proposed. These tools are quick, effective, and easy to understand, as they allow patients to evaluate their health status and experience in a semi-structured way and accordingly aggregate input data, while automatically tracking their physio-emotional sensitivity. As part of the National Institute of Health's Roadmap Initiative, the Patient-Reported Outcomes Measurement Information System (PROMIS) uses modern advances in psychometrics such as item response theory (IRT) and computerized adaptive testing (CAT) to create highly reliable and validated measurement tools.\n\nIt is essential that a PRO instrument satisfy certain development, psychometric and scaling standards if it is to provide useful information (e.g.). Specifically, measures should have a sound theoretical basis and should be relevant to the patient group with which they are to be used. They should also be reliable and valid (including responsive to underlying change) and the structure of the scale (whether it possesses a single or multiple domains) should have been thoroughly tested using appropriate methodology in order to justify the use of scale or summary scores. The validation of the PRO measures should incorporate not only short-term but also long-term success in order to be able to reflect sustainability of interventions. \nClassic examples of such tools and methods are noted in commonly used oncology tools, such as FACT or EORTC tools.\n\nThese standards must be maintained throughout every target language population. In order to ensure that developmental standards are consistent in translated versions of a PRO instrument, the translated instrument undergoes a process known as Linguistic validation in which the preliminary translation is adapted to reflect cultural and linguistic differences between diverse target populations.\n\nPreference-based PROs can be used for the computation of a quality-adjusted life year. A preference based PRO has an algorithm attached to the PRO instrument which can 'weigh' the outcomes reported by patients according to the preferences for health outcomes of a group of individuals such as the general public or of patient groups. The purpose of this 'weighing' is to make sure that elements of health that are very important receive larger weight when computing sum scores. For example, individuals may consider problems with their mood to be more important than limitations in usual activities. Examples of generic preference-based PROs are the Health Utilities Index and the EQ-5D. Condition-targeted preference-based PROs also exist, but there are some questions regarding their comparability to generic PROs when used for the computation of Quality Adjusted Life Years.\n\nMany of the common generic PRO tools assess health-related quality of life or patient evaluations of health care. For example, the SF-36 Health Survey, SF-12 Health Survey, Profile, the Nottingham Health Profile, the Health Utilities Index, the Quality of Well-Being Scale, the EuroQol (EQ-5D), and the Consumer Assessment of Healthcare Providers and Systems (CAHPS) survey instruments are PRO instruments.\n\nCondition-targeted tools may capture any of the constructs listed above, depending on the purpose for which they were designed. Examples include the Adult Asthma Quality of Life Questionnaire (AQLQ), the Kidney Disease Quality of Life Instrument, National Eye Institute Visual Functioning Questionnaire, Epilepsy Surgery Inventory, Migraine Specific Quality of Life (MSQOL), the Ankylosing Spondylitis Quality of Life questionnaire (ASQoL) and the Seattle Angina Questionnaire (SAQ), to name a few.\n\nThe American Joint Replacement Registry (AJRR) launched their Level III patient-reported outcome (PRO) platform in November 2015 and switched to a new version created and hosted by Ortech Systems in 2016. AJRR imports the PRO data into the AJRR’s Demand Reporting & Electronic Dashboard system. Clinical staff is able to access patient data while having the ability to manage PRO surveys electronically via a secure patient portal. The AJRR Dashboard system can also pull site-specific patient reports and summary results for each PRO measure supported on the AJRR system.\n\nAJRR collaborated with several orthopaedic organizations to identify the specific measures that AJRR should recommend and that may be used as national benchmarks. Even though specific measures are recommended, AJRR understands that some institutions may have in place a long-standing PRO data collection process. Participating hospitals are able to submit and retrieve these alternative measures, but there will not be national benchmarks available for them.\n\nSince 1 April 2009 all providers of care funded by the National Health Service (NHS) in England have been required to provide patient-reported outcome measures (PROMs) in four elective surgical procedures: hip replacement, knee replacement, varicose vein surgery and hernia surgery. Patients are asked to complete a questionnaire before undergoing the surgical procedure; a follow-up questionnaire is then sent to the patient some weeks or months later. Patient participation is, however, not compulsory.\n\nIn December 2013 a team from the London School of Hygiene and Tropical Medicine reviewed the first three years of NHS PROMs data which captured responses from more than 50,000 patients who underwent groin hernia repair, varicose vein surgery or hip or knee replacements. They found \"no grounds to suggest we should start cutting the amount of surgery we are doing\".\n\nPatient-reported outcomes are important in a regulatory context. The US Food and Drug Administration (FDA) has issued formal Guidance to Industry on PROs in label claims and the European Medicines Agency (EMA) has produced a reflection paper on HRQoL. Increasing numbers of regulatory submissions for new drugs provide PRO data to support claims. DeMuro et al. (2013) have reviewed drug approvals for the years 2006–2010. They showed that of 75 drugs approved by both agencies, 35 (47%) had at last one PRO-related claim approved by the EMA compared to 14 (19%) for the FDA. The FDA was more likely to approve claims for symptom reduction, while the EMA approved relatively more claims for improvement in functioning or HrQoL.\n\nOperationalizing success in multi-modal pain therapy is a challenge and is up to now characterized by tremendous heterogeneity. There are efforts to define core sets of patient-relevant outcome variables to be measured in clinical trials in general and for multi-modal pain therapy. Meanwhile, a core outcome measure set based on PROMS was developed with routine data and validated for operationalizing success in multimodal pain therapy. Validation studies suggest also suitability for depicting long-term success in the sense of sustainability of treatment effects.\n\n\n\n"}
{"id": "47700499", "url": "https://en.wikipedia.org/wiki?curid=47700499", "title": "Personal health budgets", "text": "Personal health budgets\n\nProfessor Lord Darzi's review of the NHS in 2008 introduced the idea of personal health budgets (PHBs) in the English National Health Service. Since October 2014 people eligible for NHS Continuing Healthcare were given the legal right to have a personal health budget. NHS England’s Five Year Forward View called for a ‘major expansion’ of the scheme. NHS Choices describes personal health budgets as a way \"to give people with long-term conditions and disabilities greater choice and control over the healthcare and support they receive.\"\n\nA personal health budget may be used by a patient to buy a range of services to meet their health and well-being needs. This might include things not traditionally commissioned by the NHS, or may be used to pay for services like nursing care which are provided by the NHS, but giving them more autonomy in deciding where, how and when their care and support is provided. \n\nA personal health budget can be managed in three different ways:\n\nThe Royal College of General Practitioners produced guidance for its members on the use of Personal health budgets in 2012. PHBs have been mainly used for people with continuing health needs. The RCGP explained that \"At the heart of a personal health budget is a care or support plan – an agreement between the local NHS and the individual that sets out the person’s health needs, the amount of money available to meet those needs and how this money will be spent.\"\n\nBy 2015-16 expenditure on PHBs was estimated at £123 million per year, just over 0.1% of NHS spending. The number of people with a budget was 4,800, so the average package costs £25,600. PHBs have been targeted on people with most significant needs, generally people on continuing care packages.\n\nIn April 2018 the Department of Health and Social Care proposed to increase the scope of the scheme, claiming that personal health budgets had reduced NHS continuing healthcare costs by 17%. At present only people receiving continuing healthcare funding have a right to a personal health budget, though clinical commissioning groups may offer personal health budgets to others. The new groups proposed to be eligible are:\nIt is proposed that those eligible should have an explicit right to receive direct payments.\n\nPersonal budgets have been widely adopted in social care and have been championed by, among others, Liz Kendall, who advocated extending the idea into health services. Simon Duffy, who was the chief executive of the influential social enterprise company In Control, was one of the creators of Self Directed Support which piloted the idea with people with learning disabilities. According to him \"If you don't tell people what the budget is, as a local authority you are forced into a position of planning for them in order to ration, but if you give them a budget, doing the rationing up front, you liberate people and their families to do their own planning, and liberate service providers to do creative planning. Philosophically, this is a shift towards clarity about rights and duties.\" His ideas were formulated in the Deinstitutionalisation of social care, where there has been much greater progress in the development of personal budgets. He was awarded the Albert Medal by Ivan Lewis who was at that time the minister for social care and championed his ideas in the Department of Health.\n\nThe Metropolitan Borough of Barnsley is piloting both individual social care budgets and the Right to Control scheme to provide personal budgets integrated across different services. Martin Farran, who is the council’s executive director of adults and community services and also leads in this area for the Association of Directors of Adult Social Services is keen to extend personalisation into health, particularly mental health. He says this would \"free up a lot of resources for better outcomes and at a lower cost.”\n\nA survey by Pulse in September 2015, based on Freedom of Information requests to Clinical Commissioning Groups was headlined \"Revealed: NHS funding splashed on holidays, games consoles and summer houses\" and generated considerable publicity. An angry 81-year-old widow in Gateshead complained that the NHS “can afford to pay for horse riding lessons but can’t get me to hospital.”\n\nAccording to Pulse the scheme has been used to pay for unevidenced treatments at the expense of long-established services, which are no longer provided. Duffy said PHBs made up a tiny percentage of NHS spending. It was assessed and approved by health professionals. \"You cannot simply turn up to your GP and ask for a summer house.\"... \"There would have been no advantage in people carrying on spending the money exactly as the NHS used to spend the money.\"\n\nA similar scheme was introduced in the Netherlands in 1997. Because the number of personal budget holders increased tenfold between 2002 and 2010, and expenditure increased on average by 23% a year, restrictions were introduced. From 2014, only people who would otherwise have to move to a nursing or residential home could keep their budget or apply for one to enable them to continue living at home. There were reports of fraud and improper behaviour by brokers who handled money on behalf of vulnerable people who were not able to manage the budget themselves.\n\n\n"}
{"id": "1099391", "url": "https://en.wikipedia.org/wiki?curid=1099391", "title": "Polypharmacy", "text": "Polypharmacy\n\nPolypharmacy is the concurrent use of multiple medications by a patient. Polypharmacy is most common in the elderly, affecting about 40% of older adults living in their own homes. About 21% of adults with intellectual disability are also exposed to polypharmacy. Polypharmacy is not necessarily ill-advised, but in many instances can lead to negative outcomes or poor treatment effectiveness, often being more harmful than helpful or presenting too much risk for too little benefit. Therefore, health professionals consider it a situation that requires monitoring and review to validate whether all of the medications are still necessary. Concerns about polypharmacy include increased adverse drug reactions, drug interactions, prescribing cascade, and higher costs. Polypharmacy is often associated with a decreased quality of life, including decreased mobility and cognition.\n\nThe definition of polypharmacy is still debated. Definitions have ranged from two medications at a time to 18, or to more medications than clinically necessary. Five or more concurrent regular medications appears to be the most common definition. Despite the uncertainty around a definition, experts generally agree on the magnitude, potential for harm and potential for reduction in medication regimens for older people.\n\nWhether or not the advantages of polypharmacy (over monotherapy) outweigh the disadvantages or risks depends upon the particular combination and diagnosis involved in any given case. The use of multiple drugs, even in fairly straightforward illnesses, is not an indicator of poor treatment and is not necessarily overmedication. A perfectly legitimate treatment regimen could include, for example, the following: a statin, an ACE inhibitor, a beta-blocker, aspirin, paracetamol and an antidepressant in the first year after a myocardial infarction. Moreover, it is well accepted in pharmacology that it is impossible to accurately predict the side effects or clinical effects of a combination of drugs without studying that particular combination of drugs in test subjects. Knowledge of the pharmacologic profiles of the individual drugs in question does not assure accurate prediction of the side effects of combinations of those drugs; and effects also vary among individuals because of genome-specific pharmacokinetics. Therefore, deciding whether and how to reduce a list of medications (deprescribe) is often not simple and requires the experience and judgment of a practicing physician. However, such thoughtful and wise review is an ideal that too often does not happen, owing to problems such as poorly handled care transitions (poor continuity of care, usually because of siloed information), overworked physicians, and interventionism.\n\nPolypharmacy continues to grow in importance because of aging populations. Many countries are experiencing a fast growth of the older population, 65 years and older. This growth is a result of the baby-boomer generation getting older and an increased life expectancy as a result of ongoing improvement in health care services worldwide.\n\nConsiderations often associated with thoughtful, therapeutic polypharmacy include:\n\nOften certain medications can interact with others in a positive way specifically intended when prescribed together, to achieve a greater effect that any of the single agents alone. This is particularly prominent in the field of anesthesia and pain management – where atypical agents such as antiepileptics, antidepressants, muscle relaxants, NMDA antagonists, and other medications are combined with more typical analgesics such as opioids, prostaglandin inhibitors, NSAIDS and others. This practice of pain management drug synergy is known as an \"analgesia sparing\" effect.\n\nAs another example, in anesthesia (particularly IV anesthesia and general anesthesia) multiple agents are almost always required – including hypnotics or analgesic inducing/maintenance agents such as midazolam or propofol, usually an opioid analgesic such as morphine or fentanyl, a paralytic such as vecuronium, and in inhaled general anesthesia generally a halogenated ether anesthetic such as sevoflurane or desflurane.\n\nThe use of polypharmacy is correlated to the use of potentially inappropriate medications. Potentially inappropriate medications are generally taken to mean those that have been agreed upon by expert consensus, such as by the Beers Criteria. These medications are generally inappropriate for older adults because the risks outweigh the benefits. Examples of these include urinary anticholinergics, which can prevent up to one episode of incontinence every 48 hours on average. However, they can also cause constipation, dry eyes, dry mouth, impaired cognition, and increase the risk of falls.\n\nPolypharmacy is associated with an increased risk of falls in the elderly. Certain medications are well known to be associated with the risk of falls, including cardiovascular and psychoactive medications. The use of four or more of these medicines is known to be associated with a significantly higher, cumulative risk of falls. Although often not practical to achieve, withdrawing all medicines associated with falls risk can halve an individual's risk of future falls.\n\nEvery medication has potential adverse side-effects. With every drug added, there is an additive risk of side-effects. Also, many medications have potential interactions with other substances. 15% of older adults are potentially at risk for a major drug-drug interaction. When a new drug is prescribed, the risk of interactions increases exponentially. Doctors and pharmacists aim to avoid prescribing medications that interact; often, adjustments in the dose of medications need to be made to avoid interactions, such as with warfarin, as it may lose its effect.\n\nPatients at greatest risk for negative polypharmacy consequences include the elderly, psychiatric patients, patients taking five or more drugs concurrently, those with multiple physicians and pharmacies, recently hospitalized patients, individuals with concurrent comorbidities, low educational level, and those with impaired vision or dexterity.\n\nIt is not uncommon for those dependent or addicted to substances to enter or remain in a state of polypharmacy misuse. Note, however, that the term \"polypharmacy\" and its variants generally refer to legal drug use as-prescribed, even when used in a negative or critical context.\n\nMeasures can be taken to limit polypharmacy to its truly legitimate and appropriate needs. This is an emerging area of research, frequently called deprescribing. This reduction in medications has been shown to reduce the number of medications and is safe as it does not significantly alter health outcomes. Clinical pharmacists can perform drug therapy reviews and teach physicians and their patients about drug safety and polypharmacy, as well as collaborating with physicians and patients to correct polypharmacy problems. Similar programs are likely to reduce the potentially deleterious consequences of polypharmacy. Such programs hinge upon patients and doctors informing pharmacists of other medications being prescribed, as well as herbal, over-the-counter substances and supplements that occasionally interfere with prescription-only medication.\n\nPill burden is the number of pills (tablets or capsules, the most common dosage forms) that a patient takes on a regular basis, along with all associated efforts that increase with that number - like storing, organizing, consuming, and understanding the various medications in one's regimen.\n\nHigh pill burden decreases compliance with drug therapy, resulting from the need to take a large quantity of pills or other forms of medication on a regular basis. It also increases the possibility of adverse medication reactions (side effects) and drug-drug interactions. High pill burden has also been associated with an increased risk of hospitalization, medication errors, and increased costs for both the pharmaceuticals themselves and for the treatment of adverse events. Finally, pill burden is a source of dissatisfaction for many patients.\n\nHigh pill burden was once commonly associated with antiretroviral drug regimens to control HIV, but now is more often seen in other patient populations. For instance, adults with multiple common chronic conditions such as diabetes, hypertension, lymphedema, hypercholesterolemia, osteoporosis, constipation, and clinical depression can often be prescribed more than a dozen different medications daily. The adverse reactions of these combinations of drugs are not reliably predictable. Obesity is implicated in many of the aforementioned conditions, and it is not uncommon for a clinically obese patient to receive pharmacologic treatment for all of these. Because chronic conditions tend to accumulate in the elderly, pill burden is a particular issue in geriatrics.\n\nReducing pill burden is recognized as a way to improve medication compliance. This is done through \"deprescribing\", where the risks and benefits are weighed when considering whether to continue a medication. This includes drugs such as bisphosphonates (for osteoporosis) where it is often used indefinitely although there is only evidence to use it for five to ten years.\n\nThe selection of long-acting active ingredients over short-acting ones may also reduce pill burden. For instance, ACE inhibitors are used in the management of hypertension. Both captopril and lisinopril are examples of ACE inhibitors. However, lisinopril is dosed once a day, whereas captopril may be dosed 2-3 times a day. Assuming that there are no contraindications or potential for drug interactions, using lisinopril instead of captopril may be an appropriate way to limit pill burden.\n\nThe most common intervention in polypharmacy patients is deprescribing, which includes the identification and discontinuance of medications when the benefit no longer outweighs the harm. In elderly patients, this can commonly be done as a patient becomes more frail and treatment focus needs to shift from preventative to palliative.\n\nSeveral tools exist to help physicians decide when to deprescribe and what medications can be added to a pharmaceutical regimen. The Beers Criteria and the STOPP/START criteria help identify medications that have the highest risk of adverse drug events (ADE) and drug-drug interactions. PRIMA-eds is an algorithm that takes many patient factors into its calculations and can offer medication and dosing recommendations to the provider. Parameters of PRIMA-eds include diagnoses, medications, ADEs, symptoms, body mass index, vital signs, laboratory results, and medical studies performed in the elderly.\n\nA team approach is a relatively new method gaining popularity due to its effectiveness in managing patient care and obtaining the best outcomes. A team can include a primary provider, pharmacist, nurse, counselor, physical therapist, chaplain and others involved in patient care. Combining the ideas and points of view of the different providers allows a more holistic approach to the health of a patient. In 2013, the United States legislature mandated that every Medicare D patient receive an annual Medication Therapy Management (MTM) review by a team of healthcare professionals.\n\nIt is unclear if specific interventions to improve adequate polypharmacy in older adults have significant clinical results, but they seem to reduce inappropriate prescribing and medication-related problems. High quality evidence is needed to make any conclusions about the effects of such interventions in care homes.\n\n\n"}
{"id": "7020455", "url": "https://en.wikipedia.org/wiki?curid=7020455", "title": "Postmarketing surveillance", "text": "Postmarketing surveillance\n\nPostmarketing surveillance (PMS) (also post market surveillance) is the practice of monitoring the safety of a pharmaceutical drug or medical device after it has been released on the market and is an important part of the science of pharmacovigilance. Since drugs and medical devices are approved on the basis of clinical trials, which involve relatively small numbers of people who have been selected for this purpose – meaning that they normally do not have other medical conditions which may exist in the general population – postmarketing surveillance can further refine, or confirm or deny, the safety of a drug or device after it is used in the general population by large numbers of people who have a wide variety of medical conditions.\n\nPostmarketing surveillance uses a number of approaches to monitor drug and device safety, including spontaneous reporting databases, prescription event monitoring, electronic health records, patient registries, and record linkage between health databases. These data are reviewed to highlight potential safety concerns in a process known as data mining.\n\nHealth Canada is the regulatory body which approves drugs, and it has a division called \"Marketed Health Products Directorate\" (MHPD) which coordinates Canadian postmarketing surveillance.\n\nThe guidance document \"MEDDEV 2.12-1 rev 8\" offers a comprehensive guidance on best practice for \"medical device post-market surveillance\" (materiovigilance). A manufacturer of medical devices is required to report incidents (serious adverse events) to the national competent authority of the member state the company resides in. The Medical Device Regulation (EU) 2017/745 provides in §2 the following definition of post-market surveillance:\n‘post-market surveillance’ means all activities carried out by manufacturers in cooperation with other economic operators to institute and keep up to date a systematic procedure to proactively collect and review experience gained from devices they place on the market, make available on the market or put into service for the purpose of identifying any need to immediately apply any necessary corrective or preventive actions;\n\nThe Medicines and Healthcare Products Regulatory Agency (MHRA) and the Commission on Human Medicines (CHM) jointly operate the Yellow Card Scheme, which was one of the first examples of a pharmacovigilance scheme, aimed at mitigating adverse drug reactions (ADRs).\n\nPostmarketing surveillance is overseen by the Food and Drug Administration (FDA), which operates a system of passive surveillance called MedWatch, to which doctors or the general public can voluntarily report adverse reactions to drugs and medical devices. The FDA also conducts active surveillance of certain regulated products. For example, the FDA may monitor safety and effectiveness of medical devices through either a Post-Approval Study or through a 522 Postmarket Surveillance Study.\n\n"}
{"id": "1018367", "url": "https://en.wikipedia.org/wiki?curid=1018367", "title": "Reclaimed water", "text": "Reclaimed water\n\nReclaimed or recycled water (also called wastewater reuse or water reclamation) is the process of converting wastewater into water that can be reused for other purposes. Reuse may include irrigation of gardens and agricultural fields or replenishing surface water and groundwater (i.e., groundwater recharge). Reused water may also be directed toward fulfilling certain needs in residences (e.g. toilet flushing), businesses, and industry, and could even be treated to reach drinking water standards. This last option is called either \"direct potable reuse\" or \"indirect potable\" reuse, depending on the approach used. Colloquially, the term \"toilet to tap\" also refers to potable reuse.\nReclaiming water for reuse applications instead of using freshwater supplies can be a water-saving measure. When used water is eventually discharged back into natural water sources, it can still have benefits to ecosystems, improving streamflow, nourishing plant life and recharging aquifers, as part of the natural water cycle.\n\nWastewater reuse is a long-established practice used for irrigation, especially in arid countries. Reusing wastewater as part of sustainable water management allows water to remain as an alternative water source for human activities. This can reduce scarcity and alleviate pressures on groundwater and other natural water bodies. \n\nAchieving more sustainable sanitation and wastewater management will require emphasis on actions linked to resource management, such as wastewater reuse or excreta reuse that will keep valuable resources available for productive uses. This in turn supports human wellbeing and broader sustainability.\n\nSimply stated, reclaimed water is water that is used more than one time before it passes back into the natural water cycle. Advances in wastewater treatment technology allow communities to reuse water for many different purposes. The water is treated differently depending upon the source and use of the water and how it gets delivered.\n\nCycled repeatedly through the planetary hydrosphere, all water on Earth is recycled water, but the terms \"recycled water\" or \"reclaimed water\" typically mean wastewater sent from a home or business through a sewer system to a wastewater treatment plant, where it is treated to a level consistent with its intended use.\n\nThe World Health Organization has recognized the following principal driving forces for wastewater reuse: \nWater recycling and reuse is of increasing importance, not only in arid regions but also in cities and contaminated environments.\n\nAlready, the groundwater aquifers that are used by over half of the world population are being over-drafted. Reuse will continue to increase as the world’s population becomes increasingly urbanized and concentrated near coastlines, where local freshwater supplies are limited or are available only with large capital expenditure. Large quantities of freshwater can be saved by wastewater reuse and recycling, reducing environmental pollution and improving carbon footprint. Reuse can be an alternative water supply option.\n\nMost of the uses of water reclamation are non potable uses such as washing cars, flushing toilets, cooling water for power plants, concrete mixing, artificial lakes, irrigation for golf courses and public parks, and for hydraulic fracturing. Where applicable, systems run a dual piping system to keep the recycled water separate from the potable water.\n\nThe main reclaimed water applications in the world are shown below:\n\nDe facto, unacknowledged or unplanned potable reuse refers to a situation where reuse of treated wastewater is, in fact, practiced but is not officially recognized. For example, a wastewater treatment plant from one city may be discharging effluents to a river which is used as a drinking water supply for another city downstream.\n\nUnplanned Indirect Potable Use has existed for a long time. Large towns on the River Thames upstream of London (Oxford, Reading, Swindon, Bracknell) discharge their treated sewage (\"non-potable water\") into the Thames, which supplies water to London downstream. In the United States, the Mississippi River serves as both the destination of sewage treatment plant effluent and the source of potable water.\n\n\nThere are benefits of using recycled water for irrigation, including the lower cost compared to some other sources and consistency of supply regardless of season, climatic conditions and associated water restrictions. When reclaimed water is used for irrigation in agriculture, the nutrient (nitrogen and phosphorus) content of the treated wastewater has the benefit of acting as a fertilizer. This can make the reuse of excreta contained in sewage attractive.\n\nThe irrigation water can be used in different ways on different crops:\n\nIn developing countries, agriculture is increasingly using untreated wastewater for irrigation - often in an unsafe manner. Cities provide lucrative markets for fresh produce, so are attractive to farmers. However, because agriculture has to compete for increasingly scarce water resources with industry and municipal users, there is often no alternative for farmers but to use water polluted with urban waste directly to water their crops.\n\nThere can be significant health hazards related to using untreated wastewater in agriculture. Wastewater from cities can contain a mixture of chemical and biological pollutants. In low-income countries, there are often high levels of pathogens from excreta. In emerging nations, where industrial development is outpacing environmental regulation, there are increasing risks from inorganic and organic chemicals. The World Health Organization, in collaboration with the Food and Agriculture Organization of the United Nations (FAO) and the United Nations Environmental Program (UNEP), has developed guidelines for safe use of wastewater in 2006. These guidelines advocate a ‘multiple-barrier’ approach wastewater use, for example by encouraging farmers to adopt various risk-reducing behaviours. These include ceasing irrigation a few days before harvesting to allow pathogens to die off in the sunlight, applying water carefully so it does not contaminate leaves likely to be eaten raw, cleaning vegetables with disinfectant or allowing fecal sludge used in farming to dry before being used as a human manure.\n\nThe use of reclaimed water to create, enhance, sustain, or augment water bodies including wetlands, aquatic habitats, or stream flow is called \"environmental reuse\". For example, constructed wetlands fed by wastewater provide both wastewater treatment and habitats for flora and fauna.\n\nThe use of reclaimed water to recharge aquifers that are not used as a potable water source.\n\nPlanned potable reuse is publicly acknowledged as an intentional project to recycle water for drinking water. There are two ways in which potable water can be delivered for reuse - \"Indirect Potable Reuse\" (IPR) and \"Direct Potable Reuse\". Both these forms of reuse are described below, and commonly involve a more formal public process and public consultation program than is the case with de facto or unacknowledged reuse. In ‘indirect’ potable reuse applications, the reclaimed wastewater is used directly or mixed with other sources.\n\nDirect potable reuse is also called \"toilet to tap\".\n\nSome water agencies reuse highly treated effluent from municipal wastewater or resource recovery plants as a reliable, drought proof source of drinking water. By using advanced purification processes, they produce water that meets all applicable drinking water standards. System reliability and frequent monitoring and testing are imperative to them meeting stringent controls.\n\nThe water needs of a community, water sources, public health regulations, costs, and the types of water infrastructure in place, such as distribution systems, man-made reservoirs, or natural groundwater basins, determine if and how reclaimed water can be part of the drinking water supply. Some communities reuse water to replenish groundwater basins. Others put it into surface water reservoirs. In these instances the reclaimed water is blended with other water supplies and/or sits in storage for a certain amount of time before it is drawn out and gets treated again at a water treatment or distribution system. In some communities, the reused water is put directly into pipelines that go to a water treatment plant or distribution system.\n\nModern technologies such as reverse osmosis and ultraviolet disinfection are commonly used when reclaimed water will be mixed with the drinking water supply.\n\nIndirect potable reuse (IPR) means the water is delivered to the consumer indirectly. After it is purified, the reused water blends with other supplies and/or sits a while in some sort of storage, man-made or natural, before it gets delivered to a pipeline that leads to a water treatment plant or distribution system. That storage could be a groundwater basin or a surface water reservoir.\n\nSome municipalities are using and others are investigating Indirect Potable Reuse (IPR) of reclaimed water. For example, reclaimed water may be pumped into (subsurface recharge) or percolated down to (surface recharge) groundwater aquifers, pumped out, treated again, and finally used as drinking water. This technique may also be referred to as \"groundwater recharging\". This includes slow processes of further multiple purification steps via the layers of earth/sand (absorption) and microflora in the soil (biodegradation).\n\nIPR or even unplanned potable use of reclaimed wastewater is used in many countries, where the latter is discharged into groundwater to hold back saline intrusion in coastal aquifers. IPR has generally included some type of environmental buffer, but conditions in certain areas have created an urgent need for more direct alternatives.\n\nDirect potable reuse means the reused water is put directly into pipelines that go to a water treatment plant or distribution system. Direct potable reuse may occur with or without “engineered storage” such as underground or above ground tanks.\n\nIn a Direct Potable Reuse (DPR) scheme, water is put directly into pipelines that go to a water treatment plant or distribution system. Direct potable reuse may occur with or without “engineered storage” such as underground or above ground tanks. In other words, DPR is the introduction of reclaimed water derived from urban wastewater after extensive treatment and monitoring to assure that strict water quality requirements are met at all times, directly into a municipal water supply system.\n\nIPR occurs through the augmentation of drinking water supplies with urban wastewater treated to a level suitable for IPR followed by an environmental buffer (e.g. rivers, dams, aquifers, etc.) that precedes drinking water treatment. In this case, urban wastewater passes through a series of treatment steps that encompasses membrane filtration and separation processes (e.g. MF, UF and RO), followed by an advanced chemical oxidation process (e.g. UV, UV+HO, ozone).\n\nWastewater reclamation can be especially important in relation to human spaceflight. In 1998, NASA announced it had built a human waste reclamation bioreactor designed for use in the International Space Station and a manned Mars mission. Human urine and feces are input into one end of the reactor and pure oxygen, pure water, and compost (humanure) are output from the other end. The soil could be used for growing vegetables, and the bioreactor also produces electricity.\n\nAboard the International Space Station, astronauts have been able to drink recycled urine due to the introduction of the ECLSS system. The system costs $250 million and has been working since May 2009. The system recycles wastewater and urine back into potable water used for drinking, food preparation, and oxygen generation. This cuts back on the need for resupplying the space station so often.\n\nWater/wastewater reuse, as an alternative water source, can provide significant economic, social and environmental benefits, which are key motivators for implementing such reuse programmes. Specifically, in agriculture, irrigation with wastewater may contribute to improve production yields, reduce the ecological footprint and promote socioeconomic benefits. These benefits include: \n\nNonpotable reclaimed water is often distributed with a dual piping network that keeps reclaimed water pipes completely separate from potable water pipes.\n\nIn many cities using reclaimed water, it is now in such demand that consumers are only allowed to use it on assigned days. Some cities that previously offered unlimited reclaimed water at a flat rate are now beginning to charge citizens by the amount they use.\n\nFor many types of reuse applications wastewater must pass through numerous sewage treatment process steps before it can be used. Steps might include screening, primary settling, biological treatment, tertiary treatment (for example reverse osmosis), and disinfection.\n\nThere are several technologies used to treat wastewater for reuse. A combination of these technologies can meet strict treatment standards and make sure that the processed water is hygienically safe, meaning free from bacteria and viruses. The following are some of the typical technologies: Ozonation, ultrafiltration, aerobic treatment (membrane bioreactor), forward osmosis, reverse osmosis, advanced oxidation.\n\nWastewater is generally treated to only secondary level treatment when used for irrigation.\n\nA pump station distributes reclaimed water to users around the city. This may include golf courses, agricultural uses, cooling towers, or in land fills.\n\nRather than treating wastewater for reuse purposes, other options can achieve similar effects of freshwater savings: \n\nThe cost of reclaimed water exceeds that of potable water in many regions of the world, where a fresh water supply is plentiful. However, reclaimed water is usually sold to citizens at a cheaper rate to encourage its use. As fresh water supplies become limited from distribution costs, increased population demands, or climate change reducing sources, the cost ratios will evolve also. The evaluation of reclaimed water needs to consider the entire water supply system, as it may bring important value of flexibility into the overall system \n\nReclaimed water systems usually require a dual piping network, often with additional storage tanks, which adds to the costs of the system.\n\n\nReclaimed water is considered safe when appropriately used. Reclaimed water planned for use in recharging aquifers or augmenting surface water receives adequate and reliable treatment before mixing with naturally occurring water and undergoing natural restoration processes. Some of this water eventually becomes part of drinking water supplies.\n\nA water quality study published in 2009 compared the water quality differences of reclaimed/recycled water, surface water, and groundwater. Results indicate that reclaimed water, surface water, and groundwater are more similar than dissimilar with regard to constituents. The researchers tested for 244 representative constituents typically found in water. When detected, most constituents were in the parts per billion and parts per trillion range. DEET (a bug repellant), and caffeine were found in all water types and virtually in all samples. Triclosan (in anti-bacterial soap & toothpaste) was found in all water types, but detected in higher levels (parts per trillion) in reclaimed water than in surface or groundwater. Very few hormones/steroids were detected in samples, and when detected were at very low levels. Haloacetic acids (a disinfection by-product) were found in all types of samples, even groundwater. The largest difference between reclaimed water and the other waters appears to be that reclaimed water has been disinfected and thus has disinfection by-products (due to chlorine use).\n\nA 2005 study titled \"Irrigation of Parks, Playgrounds, and Schoolyards with Reclaimed Water\" found that there had been no incidences of illness or disease from either microbial pathogens or chemicals, and the risks of using reclaimed water for irrigation are not measurably different from irrigation using potable water.\n\nA 2012 study conducted by the National Research Council in the United States of America found that the risk of exposure to certain microbial and chemical contaminants from drinking reclaimed water does not appear to be any higher than the risk experienced in at least some current drinking water treatment systems, and may be orders of magnitude lower. This report recommends adjustments to the federal regulatory framework that could enhance public health protection for both planned and unplanned (or de facto) reuse and increase public confidence in water reuse.\n\nMany humans associate a feeling of disgust with reclaimed water and 13% of a survey group said they would not even sip it.\nNonetheless, the main health risk for potable use of reclaimed water is the potential for pharmaceutical and other household chemicals or their derivatives (Environmental persistent pharmaceutical pollutants) to persist in this water. This would be less of a concern if human excreta was kept out of sewage by using dry toilets or systems that treat blackwater separately from greywater.\n\nTo address these concerns about the source water, reclaimed water providers use multi-barrier treatment processes and constant monitoring to ensure that reclaimed water is safe and treated properly for the intended end use.\n\nThere is debate about possible health and environmental effects. To address these concerns, A Risk Assessment Study of potential health risks of recycled water and comparisons to conventional Pharmaceuticals and Personal Care Product (PPCP) exposures was conducted by the WateReuse Research Foundation. For each of four scenarios in which people come into contact with recycled water used for irrigation - children on the playground, golfers, and landscape, and agricultural workers - the findings from the study indicate that it could take anywhere from a few years to millions of years of exposure to nonpotable recycled water to reach the same exposure to PPCPs that we get in a single day through routine activities.\n\nUsing reclaimed water for non-potable uses saves potable water for drinking, since less potable water will be used for non-potable uses.\n\nIt sometimes contains higher levels of nutrients such as nitrogen, phosphorus and oxygen which may somewhat help fertilize garden and agricultural plants when used for irrigation.\n\nThe usage of water reclamation decreases the pollution sent to sensitive environments. It can also enhance wetlands, which benefits the wildlife depending on that eco-system. It also helps to stop the chances of drought as recycling of water reduces the use of fresh water supply from underground sources. For instance, The San Jose/Santa Clara Water Pollution Control Plant instituted a water recycling program to protect the San Francisco Bay area's natural salt water marshes.\n\nThe main potential risks that are associated with reclaimed wastewater reuse for irrigation purposes, when the treatment is not adequate are the following: \n\nWastewater reuse (planned or unplanned) is an ancient practice, which has been applied since the dawn of human history and is closely connected to the development of sanitation provision.\n\nIn the U.S., the Clean Water Act of 1972 mandated elimination of the discharge of untreated waste from municipal and industrial sources to make water safe for fishing and recreation. The US federal government provided billions of dollars in grants for building sewage treatment plants around the country. Modern treatment plants, usually using oxidation and/or chlorination in addition to primary and secondary treatment, were required to meet certain standards.\n\nLos Angeles County's sanitation districts started providing treated wastewater for landscape irrigation in parks and golf courses in 1929. The first reclaimed water facility in California was built at San Francisco's Golden Gate Park in 1932. The Water Replenishment District of Southern California was the first groundwater agency to obtain permitted use of recycled water for groundwater recharge in 1962.\n\nOrange County is located in Southern California, USA, and houses a classic example in indirect potable reuse. A large-scale artificial groundwater recharge scheme exists in the area, providing a much-needed freshwater barrier to intruding seawater. Part of the injected water consists of recycled water, starting as of 1976 with Water Factory 21, which used RO and high lime to clean the water (production capacity of 19,000 m per day). This plant was decommissioned in 2004 and has since made place for a new project with a higher capacity (265,000 m per day with an ultimate capacity of 492,000 m per day), under the name of Groundwater Replenishment System.\n\n\nThe health and environmental safety conditions under which wastewater may be reused are not specifically regulated at the European Union (EU) level. There are no guidelines or regulations at EU level on water quality for water reuse purposes. In the Water Framework Directive, reuse of water is mentioned as one of the possible measures to achieve the Directive’s quality goals, however this remains a relatively vague recommendation rather than a requirement: Part B of Annex VI refers to reuse as one of the “supplementary measures which Member States within each river basin district may choose to adopt as part of the programme of measures required under Article 11(4)”.\n\nBesides that, Article 12 of the Urban Wastewater Treatment Directive concerning the reuse of treated wastewater states that “treated wastewater shall be reused whenever appropriate”, is not specific enough to promote water reuse and it leaves too much room for interpretation as to what can be considered as an “appropriate” situation to reuse treated wastewater.\n\nDespite the lack of common water reuse criteria at the EU level, several Member States (MS) have issued their own legislative frameworks, regulations, or guidelines for different water reuse applications (e.g. Cyprus, France, Greece, Italy, and Spain).\n\nHowever, after an evaluation carried out by the European Commission (EC) on the water reuse standards of several member states it was concluded that they differ in their approach. There are important divergences among the different standards regarding the permitted uses, the parameters to be monitored, and the limit values allowed. This lack of harmonization among water reuse standards might create some trade barriers for agricultural goods irrigated with reclaimed water. Once on the common market, the level of safety in the producing member states may be not considered as sufficient by the importing countries. The most representative standards on wastewater reuse from European member states are the following: \n\nReclaimed water is not regulated by the Environmental Protection Agency (EPA), but the EPA has developed water reuse guidelines that were most recently updated in 2012. The EPA Guidelines for Water Reuse represents the international standard for best practices in water reuse. The document was developed under a Cooperative Research and Development Agreement between the U.S. Environmental Protection Agency (EPA), the U.S. Agency for International Development (USAID), and the global consultancy CDM Smith. The Guidelines provide a framework for states to develop regulations that incorporate the best practices and address local requirements.\n\n\nWhen there are droughts in Australia interest in reclaimed effluent options increases. Brisbane has been seen as a leader in this trend, and other cities and towns will review the Western Corridor Recycled Water Project once completed.\n\nWhile there are currently no full-scale direct potable reuse schemes operating in Australia, the Australian Antarctic Division is investigating the option of installing a potable reuse scheme at its Davis research base in Antarctica. To enhance the quality of the marine discharge from the Davis WWTP, a number of different, proven technologies have been selected to be used in the future, such as ozonation, UV disinfection, chlorine, as well as UF, activated carbon filtration and RO.\n\nAs of 2010, Israel leads the world in the proportion of water it recycles. Israel treats 80% of its sewage (400 billion liters a year), and 100% of the sewage from the Tel Aviv metropolitan area is treated and reused as irrigation water for agriculture and public works. As of today, all the reclaimed sewage water in Israel is used for agricultural and land improvement purposes.\n\nAn example of direct potable reuse is the case of Windhoek (Namibia, New Goreangab Water Reclamation Plant (NGWRP)), where treated wastewater has been blended with drinking water for more than 40 years. It is based on the multiple treatment barriers concept (i.e. pre-ozonation, enhanced coagulation/dissolved air flotation/rapid sand filtration, and subsequent ozone, biological activated carbon/granular activated carbon, ultrafiltration (UF), chlorination) to reduce associated risks and improve the water quality. The reclaimed wastewater nowadays represent about 14% of the city’s drinking water production.\n\nIn Singapore reclaimed water is called NEWater and is bottled directly from an advanced water purification facility for educational and celebratory purposes. Though most of the reused water is used for high-tech industry in Singapore, a small amount is returned to reservoirs for drinking water.\n\nAt the end of 2002, the programme - successfully branded as NEWater - had garnered a 98 percent acceptance rate, with 82% of respondents indicating that they would drink the reused water directly, another 16% only when mixed with reservoir water. The produced NEWater after stabilization (addition of alkaline chemicals) is in compliance with the WHO requirements and can be piped off to its wide range of applications (e.g. reuse in industry, discharge to a drinking water reservoir). NEWater now makes up around 30% of Singapore’s total use, by 2060 Singapore’s National Water Agency plans to triple the current NEWater capacity as to meet 50% of Singapore’s future water demand.\n\nIn South Africa, the main driver for wastewater reuse is drought conditions.\n\nFor example, in Beaufort West, South Africa’s a direct wastewater reclamation plant (WRP) for the production of drinking water was constructed in the end of 2010, as a result of acute water scarcity (production of 2,300 m per day). The process configuration based on multi-barrier concept and includes the following treatment processes: sand filtration, UF, two-stage RO, and permeate disinfected by ultraviolet light (UV).\n\nThe leaders in use of reclaimed water in the U.S. are Florida and California.\n\nIn a January 2012 U.S. National Research Council report, a committee of independent experts found that expanding the reuse of municipal wastewater for irrigation, industrial uses, and drinking water augmentation could significantly increase the United States’ total available water resources.\n\nOne example is Orange County which is located in Southern California, USA, and houses a classic example in indirect potable reuse. A large-scale artificial groundwater recharge scheme exists in the area, providing a much-needed freshwater barrier to intruding seawater.\n\n"}
{"id": "14492200", "url": "https://en.wikipedia.org/wiki?curid=14492200", "title": "Sertoli cell-only syndrome", "text": "Sertoli cell-only syndrome\n\nSertoli cell-only syndrome (a.k.a. Del Castillo syndrome and germ cell aplasia ) is a disorder characterized by male sterility without sexual abnormality. It describes a condition of the testes in which only Sertoli cells line the seminiferous tubules.\n\nSertoli cell-only syndrome patients normally have normal secondary male features and have normal or small-sized testes.\n\nSertoli cell-only syndrome is likely multifactorial, and is characterized by severely reduced or absent spermatogenesis despite the presence of both Sertoli and Leydig cells. A substantial subset of men with this uncommon syndrome have microdeletions in the Yq11 region of the Y chromosome, an area known as the AZF (azoospermia factor) region. Generally speaking, testosterone and LH levels are normal, but due to lack of inhibin, FSH levels are increased.\nTesticular biopsy would confirm the absence of spermatozoa. Seminal plasma protein TEX101 was proposed for differentiation of Sertoli cell-only syndrome from maturation arrest and hypospermatogenesis. A clinical trial at Mount Sinai Hospital, Canada started testing this hypothesis in 2016.\n\nSertoli cell-only syndrome is like other non-obstructive azoospermia (NOA), cases are managed by sperm retrieval through testicular sperm extraction (TESE), micro-surgical testicular sperm extraction (mTESE), or testicular biopsy. On retrieval of viable sperm this could be used in Intracytoplasmic sperm injection ICSI\n\nIn 1979, Levin described germinal cell aplasia with focal spermatogenesis where a variable percentage of seminiferous tubules contain germ cells. It is important to discriminate between the two types in view of ICSI.\n\nA retrospective analysis performed in 2015 detailed the outcomes of N=148 men with non-obstructive azoospermia and diagnosed Sertoli cell-only syndrome:\n\n\nThis study considers the effect of FSH levels on clinical success, and it excludes abnormal karyotypes. All patients underwent MD-TESE in Iran. Ethnicity and genetic lineage may affect treatment of azoospermia .\n"}
{"id": "8287327", "url": "https://en.wikipedia.org/wiki?curid=8287327", "title": "Soviet hospital ship Armenia", "text": "Soviet hospital ship Armenia\n\nThe Soviet hospital ship \"Armenia\" () was a transport ship operated by the Soviet Union during World War II to carry both wounded soldiers and military cargo. It had originally been built as a passenger ship for operations on the Black Sea.\n\n\"Armenia\" was sunk on 7 November 1941 by German aircraft while evacuating civilians and wounded soldiers from Crimea. It has been estimated that approximately 5,000 to 7,000 people were killed during the sinking, making it one of the deadliest maritime disasters in history. There were only 8 survivors.\n\n\"Armenia\", built in 1928 at Baltic Shipyards in Leningrad (now St. Petersburg), was one of four \"Adzharia\"-class passenger liners specifically designed for use on the Black Sea. They were the first passenger ships to be built in the newly formed Soviet Union. \"Armenia\" was a mid-sized vessel capable of carrying 1,000 tons of cargo as well as about 550 passengers in first-, second-, and third-class accommodations. On short trips it could carry 4-500 more on deck. Her shallow draft (5.5 meters) allowed access to the shallow-water ports of the Crimean Peninsula. Throughout the 1930s she and her sister ships – \"Adzharia\", \"Abkhazia\", and \"Ukraina\" – reliably ferried passengers, mail, and cargo between Black Sea ports such as Odessa, Mariupol, Sevastopol, Yalta, and Batum.\n\nFollowing the invasion of the Soviet Union by German forces on 22 June 1941, \"Armenia\" was requisitioned by the Soviet Navy for use as a transport and hospital ship. By late October 1941 the German Wehrmacht's 11th Army, under General Erich von Manstein, had cut off the Crimean Peninsula, laying siege to Sevastopol. For the Soviets, the only way in or out of the beleaguered city was by sea. In early November \"Armenia\", painted with the large red crosses of a hospital ship, was tasked with removing wounded Russian soldiers, medical personnel, and civilians from Sevastopol.\n\nOn the night of 6/7 November 1941 \"Armenia\" took on thousands of passengers at Sevastopol, amid scenes of chaos. Although the city would end up withstanding the German siege for nine months before falling, at the time enemy seizure appeared imminent. Entire Soviet hospital staffs and civilian officials and their families were taken aboard alongside the thousands of wounded, bound for the town of Tuapse, 250 miles away on the northeastern shore of the Black Sea. After leaving port in the early morning hours of the 7th, \"Armenia\"s captain, Vladimir Plaushevsky, received orders to put in at Yalta, a few miles east of Sevastopol, where the already overloaded ship was to pick up yet more passengers. Here, no attempt was made at registering the embarkees; wounded soldiers and civilians were simply crammed onto the decks. Plaushevsky was eager to get underway while darkness still provided some protection, but was ordered to wait for escorts to arrive. At 7am \"Armenia\" finally departed Yalta, accompanied by two armed boats and two fighter planes.\n\nThe Germans and their Romanian and Italian allies had only a few surface vessels on the Black Sea; as such, it remained essentially under Soviet control throughout the Second World War. However, in the earlier part of the war the Axis had complete air superiority. Over a hundred Soviet merchant ships were sunk, as were dozens of naval vessels. Only the most heavily armed and escorted ships could travel in daylight with reasonable hope of safety; ships caught alone or in port in the western part of the Black Sea were very likely to be attacked.\n\n\"Armenia\"s status as a hospital ship was uncertain. Though her sides and top were painted with large red cross symbols, she had light anti-aircraft armament, had previously transported troops and military stores, and, on the morning of 7 November, was traveling with military escort, inadequate though it was.\n\nAt 11:30am, about 25 miles from Yalta, \"Armenia\" was attacked by a Heinkel He 111 medium bomber of 1.Staffel (Lufttorpedo)/KG 28, which dropped two torpedoes. One torpedo missed; the other scored a direct hit. The ship broke in two and sank within four minutes. Only eight people were rescued.\n\nEven by the lowest estimate of about 5,000 dead, the sinking of \"Armenia\" remains the deadliest maritime disaster in Russian and Soviet history. In terms of loss of life in the sinking of a single ship, it is often listed as third worst in world history, after the sinking of the \"Wilhelm Gustloff\" and the \"Goya\", German naval ships transporting military personnel and civilian refugees, which were torpedoed by Soviet submarines in the Baltic Sea in 1945.\n\nIn 2014, an Australian company GeoResonance claimed to have located the hulk of \"Armenia\" in 2005 at a depth of 520m, using an undisclosed remote sensing technique. However this claim has not been substantiated. All three of her \"Adzharia\"-class sister ships were also lost during the war.\n"}
{"id": "1874234", "url": "https://en.wikipedia.org/wiki?curid=1874234", "title": "Species dysphoria", "text": "Species dysphoria\n\nSpecies dysphoria is the experience of dysphoria associated with the feeling that one's body is of the wrong species. Outside of psychological literature, the term is common within the otherkin and therian communities to describe their experiences.\n\nEarls and Lalumière (2009) describe it as \"the sense of being in the wrong [species'] body...a desire to be an animal\". A term that has also been used is \"transspecies\", described by Phaedra and Isaac Bonewits as \"people who believe themselves to be part animal, or animal souls that have been incarnated in human bodies\".\n\nSpecies dysphoria may include:\n\n\nMany find comfort in a form of transition, whether physical or social. There is little research in surgeries towards the end of looking like another species, amongst what does exist, a 2008 paper by Samuel Poore in \"The Journal of Hand Surgery - American Volume\" details how a wing similar to that of a flightless bird could be constructed from a human arm.\n\nIn a critical discussion of the work of Gerbasi \"et al.\" (2008), Fiona Probyn-Rapsey (2011) proposes that if \"Species Identity Disorder\" were to be treated, it may follow paths towards encouraging desistance, mirroring aims for desistance in the treatment of gender dysphoria in children; perhaps \"redirecting a child’s attention away from cross-dressing as an animal\" or \"limiting the influence of humanimal creatures like stuffed toys\". She proposes alternatively that treatments \"might involve counseling to learn to tolerate “atypical” humanimal development for those bothered by furries [with Species Identity Disorder]\".\n\nThough not all people with species dysphoria have gender dysphoria, and vice versa, overlap exists. Some people experience both gender dysphoria and species dysphoria and consider them to be related in that they believe them to be similar dysphoric experiences.\n\n\"Species dysphoria\" is informally used mainly in psychological literature to compare the experiences of some individuals to those in the transgender community.\n\nIn a 2008 study by Gerbasi \"et al.\", amongst other things pursuing the potential of a condition termed \"Species Identity Disorder\", 46% of people surveyed who identified as being in the furry fandom, answered \"yes\" to the question \"Do you consider yourself to be less than 100% human?\" and 41% answered \"yes\" to the question \"If you could become 0% human, would you?\"\n\nQuestions that Gerbasi states as being deliberately designed to draw parallels with gender dysphoria, specifying \"a persistent feeling of discomfort\" about the human body and the feeling that the person was the \"non-human species trapped in a human body\", were answered \"yes\" by 24% and 29% of respondents, respectively.\n\nThough not all people with species dysphoria are zoophilic, it has been proposed that there is comorbidity in some zoophiles by Beetz (2004), citing Miletski (2002) as evidence of this.\n\nMiletski (2002), in a study contained in their book, involving 67 male and nine female zoophiles, found that 16 (24%) of the men reported that it was ‘‘completely or mostly true’’ that they began having sex with animals because they ‘‘identiﬁed with the animal of [their] gender’’, 0 women reported this, with 1 woman reporting it as \"sometimes true\" against 14 of the men reporting as \"sometimes true\".\n\nA 2003 study by Williams and Weinberg surveyed 114 self identified zoophilic men and wrote that some admitted to \"believing they had animal characteristics or that they felt like they were an animal.\"\n\nIn 2007, Los Angeles artist Micha Cárdenas created \"Becoming Dragon\", a \"mixed-reality performance\" in which a virtual reality experience was created to allow a person to completely experience life through the eyes of a dragon avatar in the virtual world, Second Life. After the performance, Cárdenas reported that \"some of these people call themselves Otherkin, and feel deeply, truly, painfully that they were born as the wrong species, that they are foxes, dragons and horses. I would refer to them as transspecies.\"\n\nJ M Barrie's \"Peter Pan\" has been described as experiencing species dysphoria by Garber (1997).\n\nJean Dutourd's short novel \"Une Tête de Chien\" features a spaniel-headed human protagonist described by Giffney and Hird as suffering from species dysphoria.\n\n"}
{"id": "57056228", "url": "https://en.wikipedia.org/wiki?curid=57056228", "title": "TBPS", "text": "TBPS\n\nTBPS (\"tert\"-butylbicyclophosphorothionate) is a bicyclic phosphate convulsant. It is an extremely potent GABA receptor antagonist.\n\n"}
{"id": "51403022", "url": "https://en.wikipedia.org/wiki?curid=51403022", "title": "Tennessee Department of Health", "text": "Tennessee Department of Health\n\nThe Tennessee Department of Health (TDH) is the primary agency of state government in Tennessee responsible for public health. Its workforce provides a variety of direct and indirect services to residents and visitors in all 95 counties of the state on a 24/7/365 basis.\n\nIn 2014, some 1.4 million of Tennessee's 6.6 million people received direct services in the 89 rural county health departments or the six metropolitan county health departments. Millions more benefits from other indirect services. Each year the department, in collaboration with many communities, creates a state health plan to help provide direction for the department.\n\nThe Commissioner of the Tennessee Department of Health is appointed by the Governor and oversees an enterprise affecting population health across the state. The commissioner, in turn, has an executive leadership team reporting directly to him that manages various areas or responsibilities within the department. The department has had 13 Commissioners since its creation in 1923. The current Commissioner is John J. Dreyzehner, MD, MPH, FACOEM. He has served since 2011.\n\nThe Tennessee Department of Health has seven regional offices located across Tennessee; each of these is responsible for assisting county health departments within their jurisdiction. The TDH commissioner is responsible for appointing both regional and county directors.\n\nThere are six metro health departments in Tennessee, located in the following counties: Davidson, Hamilton, Knox, Madison, Shelby and Sullivan. Though not under the direct supervision of the commissioner of the Tennessee Department of Health, metro health department directors and staff members work collaboratively with TDH regional and TDH central office staff on a variety of public health services and functions. By statute, the Commissioner, in consultation with county mayors, appoints each new county director including metros. By tradition, metro health departments are engaged directly in day-to-day pre-decisional tactical operations through the Health Systems Council structure. The metro health departments serve approximately 40 percent of Tennesseans and are another source of population health innovations that are often identified as best practices and adopted by their rural counterparts statewide.\n\nIn both rural and metro areas, a county board of health, whose members are appointed under their local form of governance and serve varying term lengths without financial compensation, provide oversight of their county's public health department. County boards of health typically include at least one medical doctor. Each county also has a separate and distinct county health council. These positions are also appointed and uncompensated.\n\nMetro and rural county health departments routinely engage with the Tennessee Emergency Management Agency in preparedness exercises for both natural and man-made disasters that may affect population health.\n\nThe Tennessee Department of Health provides staff members for the 24 Health Professional Boards, three committees, two councils and four registries responsible for the licensure and regulation of more than three dozen health and medical professions in Tennessee. Approximately 260,000 individuals are licensed and regulated by the boards. Board members are appointed by the Governor. Tennessee has had a licensing board for health professionals since 1947. Additionally, the Tennessee Office of Health Care Facilities licenses 14 different types of facilities. Currently, more 2,400 facilities are inspected.\n\nThe Tennessee Department of Health maintains a free online information portal for anyone to review licensure status of health and medical professionals:\n\nhttps://apps.health.tn.gov/Licensure/default.aspx\n\nThere is also a free online information portal to review status of licensed health-related facilities in Tennessee: http://www.tn.gov/health/topic/hcf-facilityinspections\n\nTennessee's network of county health departments assures all residents have access to a variety of local health services intended to maintain or improve health. Services include wellchild exams, fluoride varnish applications, immunizations, family planning, control of sexually transmitted diseases, nutrition counseling, the Women, Infants and Children (WIC) program, children's special services, prenatal care, the Help Us Grow Successfully (HUGS) program, Vital Records, Environmental Health Inspection programs and others. In some county health departments, medical staff members are available for diagnosing and treating acute and chronic illnesses; some also provide dental care.\n\nCounty health departments can also assist with those wanting certificates of births and deaths that occur in Tennessee. Records Vital Records pertaining to deaths and marriages are available from the TDH central office in Nashville. \nFor information about genealogy, researchers may also find information at the Tennessee State Library and Archives.\n\nThe Tennessee Department of Health and local county health departments regulate and inspect many locations and establishments. In the case of food, the department partners with the Tennessee Department of Agriculture in licensing and regulating providers. The Tennessee Department of Agriculture (TDA) is responsible for those food service providers that are physically a part of another retail establishment, such as a gas station or convenience store. Establishments, operations and services licensed and regulated by the Tennessee Department of Health include food service providers that are ‘free-standing’ and not associated with other retail operations; hotels, motels and camps; public swimming pools, spas, hot tubs and camps; body art facilities; correctional facilities; and more.\n\nThe department maintains efforts to prevent the spread of vector-borne and zoonotic diseases. Tennessee is home to a number of insects or arthropods that are known carriers of disease. Illnesses such as West Nile Virus, La Cross Encephalitis and Rocky Mountain Spotted Fever, though rare, do occur in the state. Rabies, once a scourge in the state, is exceptionally rare thanks to effective vaccination programs for animals and heightened awareness in humans. The Tennessee Department of Environment of Conservation, once a part of TDH, has been a separate department since 1992, providing a range of environment protection services.\n\nTDH Laboratory Services provide analytical services of medical and environmental testing at its facilities in Nashville and Knoxville.\n\nAs is the case in many states, Tennessee residents are not as healthy as they could be, affecting both the quality and length of life. To address this, the Tennessee Department of Health has put intentional focus on the Big Four:\n\nThe Tennessee Department of Health created and supports services for any Tennessee resident wanting to end an addiction to tobacco called the Tennessee Tobacco Quitline. 1-800-QUIT NOW.\n\nThere is also a service to get assistance with the disease of drug addiction in Tennessee, known as the TN Redline at 1-800-889-9789.\n"}
{"id": "731387", "url": "https://en.wikipedia.org/wiki?curid=731387", "title": "Testicular self-examination", "text": "Testicular self-examination\n\nTesticular self-examination is a medical practice by which external feeling of the testicles can act as a first-warning for testicular cancer.\n\nTesticular cancer is a significant killer of teenage boys and younger men (roughly ages 15–35 or 40), but\ndoctors do not systematically recommend self-examination.\n\nThe testicular self-exam (TSE) is done at home in an effort to screen for testicular cancer (secondary prevention). International literature shows many men do not know how to perform screening for testicular cancer through a self-exam. TSE is recommended to be done while standing and after a warm shower when the scrotum is relaxed and the testes are lower.\n\nAbnormal results of the TSE include the following:\n\nSome signs and symptoms of testicular cancer found during the TSE are common to other disorders of the male urinary tract and reproductive organs, some of which require prompt medical attention to preserve reproductive and urinary function. These include hydrocele testis, a varicocele, a spermatocele, genitourinary system cancers, urinary tract infections, sexually transmitted infections, or testicular torsion.\n\nPractitioners may recommend testicular self-exam (TSE) when the following risk factors are present:\n\nHowever, there is no medical consensus for recommendations on TSE.\nIt is not clear that TSE without presenting symptoms would decrease risk of dying from testicular cancer. Although the intent of screening is to decrease mortality and increase quality of life, the benefit of TSE is uncertain; thus, the US Preventative Services Task Force and the Royal Australasian College of General Practitioners recommend against routine screening while the American Cancer Society recommends TSE for men over the age of 20, and the European Association of Urology recommends TSE for men with risk factors.\n\nTesticular self-examination has generally low rates of practice in part because males are poorly informed, but also because of psychological aversion. Comparatively woman are more diligent in performing breast self-examination than men. A person's likeliness to perform self-examination is related to their fear of developing cancer. In addition to sex there is some reason to believe that socioeconomic factors also relate to frequency of examination.\n\nSometimes, if a young adult male has a spouse or partner, the spouse/partner will perform or assist in the exam, which can be done as a form of sex play and/or foreplay. The spouse or partner often is the one that spots testicular changes without formal screening.\n\n\n"}
{"id": "55551217", "url": "https://en.wikipedia.org/wiki?curid=55551217", "title": "Tetter", "text": "Tetter\n\nTetter refers to any skin condition characterized by reddish vesicular eruptions and intense itching. Common diseases called tetter include:\n"}
{"id": "56341750", "url": "https://en.wikipedia.org/wiki?curid=56341750", "title": "Turpin case", "text": "Turpin case\n\nThe Turpin case is an alleged child abuse and captivity incident discovered in Perris, California, United States, in which David and Louise Turpin allegedly imprisoned their thirteen children for years or even decades. On January 14, 2018, one of the children escaped and contacted police who, on entering the home, found some of the children in a dark, foul-smelling room. The siblings ranged in age from 2 to 29, with 7 of the 13 children being legal adults (ages 18 and up) at the time of the parents' arrest in January.\n\nThe Turpins allegedly shackled, beat and strangled their children, allowing them to eat just once per day and shower just once per year. According to investigators, the older ones were so malnourished that they appeared to be much younger. The eldest, a 29-year-old woman, weighed just 82 pounds (37 kg). Some of the siblings appeared to lack basic knowledge of the world, being unfamiliar with what medicine and police were.\n\nThe couple were arrested and detained but pleaded not guilty to all charges. Various legal charges and court hearings followed in the succeeding months. The case is considered \"extraordinary for numerous reasons\", such as the alleged abuse being done to multiple children by two parents (whereas abuse with only one child victim is more common), and according to Dr. Bernard Gallagher, because \"you don't often get cases of children being tortured, where the abuse seems calculated\".\n\nSuspects David Allen Turpin (born October 17, 1961) and Louise Ann Turpin (born May 24, 1968) were married in 1985 in Pearisburg, Virginia, when David was 23 and Louise was 16 years old. The couple eloped, angering Louise's father, church pastor Wayne Robinette.\n\nDavid, according to his parents, is a computer engineer who graduated from Virginia Tech. In 1979, he graduated from Princeton High School in West Virginia. The school's 1979 yearbook listed him as the treasurer of the Bible Club, co-captain of the Chess Club, and a member of the Science Club and Acapella Choir. Louise's occupation was listed in court documents as a homemaker. The couple are adherents of the Quiverfull movement and Pentecostalism. According to David's parents, the couple kept having children because \"God called on them\" to do so. According to Louise's sister Elizabeth Flores she, Louise, their youngest sister, and their cousin Patricia were sexually abused as kids by their grandfather, and when Elizabeth was in college, she visited the family and David watched her when she used the shower. The children's homeschooling involved memorizing the Bible, and a few tried to memorize it in its entirety. David made about $140,000 per year at Northrop Grumman and had about $150,000 in assets. The Turpins declared bankruptcy in 2011, owing debt between $100,000 and $500,000.\n\nThe couple rented a postal box in Burleson, Texas, from 1986 to 2003. They owned property or had lived in Rio Vista and Fort Worth, and left the area in 2010. After the Turpins moved out of the house, neighbors visited the property and reportedly found feces throughout the residence, beds with ropes tied to them, several dead cats and dogs in a trailer and large piles of garbage around the property. The neighbors did not disclose their findings to any authorities. In their California house, the yard was unkempt with overgrown weeds, prompting a code violation. Neighbors in California reported that on the occasion they would see the children, they would freeze and stay silent when spoken to, \"like children whose only defense was to be invisible.\" They would skip around rather than walk, and appeared malnourished and pale.\n\nThe Turpin children had been planning an escape for more than two years. On January 14, 2018, two of them left the house through a window. One returned home out of fear, but a 17-year-old daughter got away. She was in possession of a cell phone, and, though deactivated, she was able to call 9-1-1. When police met her, she showed officers photos of conditions in the home. Deputies of the Riverside County Sheriff's Department converged on the house, where they found the twelve other siblings, one of whom (aged 22) was shackled to a bed with chains. The deputies suspected that an additional two had also been shackled just prior to their entry into the house. The deputies described the siblings as having a malnourished and dirty appearance and looking to be younger than their ages. They had initially assumed that all 13 in the group were minors, but they later determined that their ages ranged from 2 to 29, with 7 being legal adults as of January 14, 2018.\n\nThe Sheriff's Department said that Louise was \"perplexed\" when deputies entered the residence. They also said, \"The parents were unable to immediately provide a logical reason why their children were restrained in [the manner that they were].\" The 6 minors, ranging from ages 2 to 17, were transported to Riverside County Regional Medical Center, where they were admitted to the pediatrics unit for treatment. Corona Regional Medical Center said that the facility was treating the 7 adult children, ranging from ages 18 to 29, describing them as small and clearly malnourished, but stable, relieved and very friendly. As of late February, the 7 adult children remained at the medical center, while the 6 younger siblings were in the care of two foster homes.\n\nOn January 14, 2018, David and Louise Turpin were arrested at their 163 Muir Woods Road house on suspicion of child endangerment and torture and held at a Riverside County jail on $9 million bail. Some sources reported that bail had been set as high as $12 million each. Police searched the Turpins' property on January 17, taking away black plastic bags of evidence. Hundreds of journals written by the children were recovered from the home. Although their admissibility in court is , they are expected to provide unique insights into the experiences of victims of torture and long-term captivity.\n\nThe Turpins were charged on January 18 with twelve counts of torture, twelve counts of false imprisonment, seven counts of abuse on a dependent adult, and six counts of child abuse. David received an additional charge of a lewd act on a child under 14 years old. If convicted on all counts, the two could be sent to prison for 94 years to life imprisonment. Upon announcing the charges against the Turpins, Riverside County District Attorney Mike Hestrin said, \"The abuse and severe neglect intensified over time and intensified as they moved to California.\" The couple pleaded not guilty to the charges.\n\nIn a brief hearing on January 24, the judge accepted the prosecutors' request for a restraining order forbidding contact between the Turpin parents and their children for a period of three years. The parents are prohibited from coming within of any of their children or establishing electronic contact with them. Both defendants agreed to these restrictions.\n\nOn February 23, Hestrin filed an additional three charges of child abuse against the couple, and one felony assault charge against Louise individually. A Felony Settlement Conference was scheduled for March 23, with a preliminary hearing following on May 14. On May 4, David was charged with eight counts of perjury in relation to affidavits he filed with the California Department of Education between the years 2010–2017, stating that \"the children in the home were receiving a fulltime education in a private day school\". A preliminary hearing date for the couple was scheduled for June 20, 2018.\n\nOn June 21, Riverside County Superior Court Judge Bernard Schwartz ruled that the Turpin parents would face trial for child abuse, false imprisonment, and torture against their children. The couple face 50 charges including several counts of torture, false imprisonment and child abuse. Despite the efforts of defense attorneys to dismiss most of the charges, the judge only dropped a child endangerment charge involving the Turpin's youngest two year old child due to a lack of evidence that the toddler had been abused.\n\nThe Turpins were then ordered to be arraigned in court on August 3, but this was postponed to August 31 due to the Turpins' defense attorneys considering a new motion in the case. On August 31, they were arraigned once more and were ordered to appear in court on October 5 for a trial readiness conference, with the possibility of the Turpins' trial beginning up to 60 days after this. At this hearing, the judge declined the defense's request for Louise Turpin to seek mental health treatment outside of custody for histrionic personality disorder, which she had been diagnosed with since her arrest. Had the judge granted this request, Louise could have been treated for up to two years and had all charges against her dropped.\n\nThe couple's next court appearance, another trial readiness conference, took place on November 30. At this hearing, the judge announced that motions in the case will be heard the week of August 12, 2019, and that the Turpins' trial would officially begin on September 3, 2019. One of the motions in the case is expected to be a request from the Turpins' defence lawyers for the trial to take place outside of Riverside County due to the publicity the case has received.\n\nAfter the arrest, visitors left notes, balloons and flowers at the house for the children. Thieves and vandals have since struck the house, knowing it was unoccupied.\n\nOn January 17, 2018, Louise's sister said that she begged for decades to see her nieces and nephews, even through Skype, but the couple would not let her. Another sister of Louise said she was concerned about the children's weights. Louise Turpin's aunt said, \"With the pictures they put on Facebook, you thought they were one big happy family.\" David's parents said they were \"surprised and shocked\" at the allegations against their son and daughter-in-law. The couple's previous bankruptcy lawyer said that she met with the couple about four or five times in 2011 and described them as \"just very normal.\"\n\nLouise Turpin's sister revealed on the \"Megyn Kelly Today\" show that the couple had experimented with different religions and with swinging.\n\nLouise Turpin's sister Elizabeth and cousin Patricia exposed the childhood abuse that impacted all of them on \"The Dr. Oz Show\" television series, aired on 30 January 2018.\n\nDavid's brother Randy, the president of a bible college, a fiery megachurch preacher and faith healer who went to Disneyland with the family in 2011 expressed interest in adopting the children. He posted it online, writing, “A memory that I will hold to for the rest of my life. It was so great being with you guys.”\n\nNatascha Kampusch, an Austrian woman who was kidnapped and locked in a cellar for eight years, has said that the 13 Turpin children must be allowed to see the parents who allegedly kept them captive, and that the children who have not been named, will need to find a way to either forgive David and Louise Turpin or leave them behind because \"it will help them begin a process where they can cope with the whole situation and get more stable.\"\n\nThe television series \"Dr. Phil\", episode: \"Inside the California 'House of Horrors'\", aired: January 2018, family, neighbors, and friends speak with Dr. Phil concerning the secrets that were allegedly occurring within the home. Kidnap survivor Michelle Knight shares a message for the children.\n\n\"\" ran an episode on May 2, 2018 titled \"\", about a family in Queens, New York based on the Turpin story.\n\n\n"}
{"id": "648954", "url": "https://en.wikipedia.org/wiki?curid=648954", "title": "Visual acuity", "text": "Visual acuity\n\nVisual acuity (VA) commonly refers to the clarity of vision. Visual acuity is dependent on optical and neural factors, i.e., (i) the sharpness of the retinal focus within the eye, (ii) the health and functioning of the retina, and (iii) the sensitivity of the interpretative faculty of the brain.\n\nA common cause of low visual acuity is refractive error (ametropia), or errors in how the light is refracted in the eyeball. Causes of refractive errors include aberrations in the shape of the eyeball or the cornea, and reduced flexibility of the lens. Too high or too low refractive error (in relation to the length of the eyeball) is the cause of nearsightedness (myopia) or farsightedness (hyperopia) (normal refractive status is referred to as emmetropia). Other optical causes are astigmatism or more complex corneal irregularities. These anomalies can mostly be corrected by optical means (such as eyeglasses, contact lenses, laser surgery, etc.).\n\nNeural factors that limit acuity are located in the retina or the brain (or the pathway leading there). Examples for the first are a detached retina and macular degeneration, to name just two. Another common impairment, amblyopia, is caused by the visual brain not having developed properly in early childhood. In some cases, low visual acuity is caused by brain damage, such as from traumatic brain injury or stroke. When optical factors are corrected for, acuity can be considered a measure of neural well-functioning.\n\nVisual acuity is typically measured while fixating, i.e. as a measure of central (or foveal) vision, for the reason that it is highest there. However, acuity in peripheral vision can be of equal (or sometimes higher) importance in everyday life. Acuity declines towards the periphery in an inverse-linear fashion (i.e. the decline follows a hyperbola).\n\nVisual is a measure of the spatial resolution of the visual processing system. VA, as it is sometimes referred to by optical professionals, is tested by requiring the person whose vision is being tested to identify so-called optotypes – stylized letters, Landolt rings, pediatric symbols, symbols for the illiterate, standardized Cyrillic letters in the Golovin–Sivtsev table, or other patterns – on a printed chart (or some other means) from a set viewing distance. Optotypes are represented as black symbols against a white background (i.e. at maximum contrast). The distance between the person's eyes and the testing chart is set so as to approximate \"optical infinity\" in the way the lens attempts to focus (far acuity), or at a defined reading distance (near acuity).\n\nA reference value above which visual acuity is considered normal is called 6/6 vision, the USC equivalent of which is 20/20 vision: At 6 meters or 20 feet, a human eye with that performance is able to separate contours that are approximately 1.75 mm apart. Vision of 6/12 corresponds to lower, vision of 6/3 to better performance. Normal individuals have an acuity of 6/4 or better (depending on age and other factors).\n\nIn the expression 6/x vision, the numerator (6) is the distance in meters between the subject and the chart and the denominator (x) the distance at which a person with 6/6 acuity would discern the same optotype. Thus, 6/12 means that a person with 6/6 vision would discern the same optotype from 12 meters away (i.e. at twice the distance). This is equivalent to saying that with 6/12 vision, the person possesses half the spatial resolution and needs twice the size to discern the optotype.\n\nA simple and efficient way to state acuity is by solving the fraction to a decimal number. 6/6 then corresponds to an acuity (or a Visus) of 1.0 (see \"Expression\" below). 6/3 corresponds to 2.0, which is often attained by well-corrected healthy young subjects with binocular vision. Stating acuity as a decimal number is the standard in European countries, as required by the European norm (EN ISO 8596, previously DIN 58220).\n\nThe precise distance at which acuity is measured is not important as long as it is sufficiently far away and the size of the optotype on the retina is the same. That size is specified as a visual angle, which is the angle, at the eye, under which the optotype appears. For 6/6 = 1.0 acuity, the size of a letter on the Snellen chart or Landolt C chart is a visual angle of 5 arc minutes (1 arc min = 1/60 of a degree). By the design of a typical optotype (like a Snellen E or a Landolt C), the critical gap that needs to be resolved is 1/5 this value, i.e., 1 arc min. The latter is the value used in the international definition of visual acuity:\nAcuity is a measure of visual performance and does not relate to the eyeglass prescription required to correct vision. Instead, an eye exam seeks to find the prescription that will provide the best corrected visual performance achievable. The resulting acuity may be greater or less than 6/6 = 1.0. Indeed, a subject diagnosed as having 6/6 vision will often actually have higher visual acuity because, once this standard is attained, the subject is considered to have normal (in the sense of undisturbed) vision and smaller optotypes are not tested. Emmetropic subjects with 6/6 vision or \"better\" (20/15, 20/10, etc.), may still benefit from an eyeglass correction for other problems related to the visual system, such as astigmatism, ocular injuries, or presbyopia.\n\nVisual acuity is measured by a psychophysical procedure and as such relates the physical characteristics of a stimulus to a subject's percept and his/her resulting responses. Measurement can be by using an eye chart invented by Ferdinand Monoyer, by optical instruments, or by computerized tests like the FrACT.\n\nCare must be taken that viewing conditions correspond to the standard, such as correct illumination of the room and the eye chart, correct viewing distance, enough time for responding, error allowance, and so forth. In European countries, these conditions are standardized by the European norm (EN ISO 8596, previously DIN 58220).\n\nDaylight vision (i.e. photopic vision) is subserved by cone receptor cells which have high spatial density (in the central fovea) and allow high acuity of 6/6 or better. In low light (i.e., scotopic) vision, cones do not have sufficient sensitivity and vision is subserved by rods. Spatial resolution is then much lower. This is due to spatial summation of rods, i.e. a number of rods merge into a bipolar cell, in turn connecting to a ganglion cell, and the resulting unit for resolution is large, and acuity small. Note that there are no rods in the very center of the visual field (the foveola), and highest performance in low light is achieved in near peripheral vision\n\nThe maximum angular resolution of the human eye at a distance of 1 km is typically 30 to 60 cm. This gives an angular resolution of between 0.02 and 0.03 degrees, which is roughly 1.2–1.8 arc minutes per line pair, which implies a pixel spacing of 0.6–0.9 arc minutes.\n6/6 vision is defined as the ability to resolve two points of light separated by a visual angle of one minute of arc, or about 320–386 pixels per inch for a display on a device held 25 to 30 cm from the eye.\n\nThus, visual acuity, or resolving power (in daylight, central vision), is the property of cones.\nTo resolve detail, the eye's optical system has to project a focused image on the fovea, a region inside the macula having the highest density of cone photoreceptor cells (the only kind of photoreceptors existing in the fovea's very center of 300 μm diameter), thus having the highest resolution and best color vision. Acuity and color vision, despite being mediated by the same cells, are different physiologic functions that do not interrelate except by position. Acuity and color vision can be affected independently.\n\nThe grain of a photographic mosaic has just as limited resolving power as the \"grain\" of the retinal mosaic. In order to see detail, two sets of receptors must be intervened by a middle set. The maximum resolution is that 30 seconds of arc, corresponding to the foveal cone diameter or the angle subtended at the nodal point of the eye. In order to get reception from each cone, as it would be if vision was on a mosaic basis, the \"local sign\" must be obtained from a single cone via a chain of one bipolar, ganglion, and lateral geniculate cell each. A key factor of obtaining detailed vision, however, is inhibition. This is mediated by neurons such as the amacrine and horizontal cells, which functionally render the spread or convergence of signals inactive. This tendency to one-to-one shuttle of signals is powered by brightening of the center and its surroundings, which triggers the inhibition leading to a one-to-one wiring. This scenario, however, is rare, as cones may connect to both midget and flat (diffuse) bipolars, and amacrine and horizontal cells can merge messages just as easily as inhibit them.\n\nLight travels from the fixation object to the fovea through an imaginary path called the visual axis. The eye's tissues and structures that are in the visual axis (and also the tissues adjacent to it) affect the quality of the image. These structures are: tear film, cornea, anterior chamber, pupil, lens, vitreous, and finally the retina. The posterior part of the retina, called the retinal pigment epithelium (RPE) is responsible for, among many other things, absorbing light that crosses the retina so it cannot bounce to other parts of the retina. In many vertebrates, such as cats, where high visual acuity is not a priority, there is a reflecting tapetum layer that gives the photoreceptors a \"second chance\" to absorb the light, thus improving the ability to see in the dark. This is what causes an animal's eyes to seemingly glow in the dark when a light is shone on them. The RPE also has a vital function of recycling the chemicals used by the rods and cones in photon detection. If the RPE is damaged and does not clean up this \"shed\" blindness can result.\n\nAs in a photographic lens, visual acuity is affected by the size of the pupil. Optical aberrations of the eye that decrease visual acuity are at a maximum when the pupil is largest (about 8 mm), which occurs in low-light conditions. When the pupil is small (1–2 mm), image sharpness may be limited by diffraction of light by the pupil (see diffraction limit). Between these extremes is the pupil diameter that is generally best for visual acuity in normal, healthy eyes; this tends to be around 3 or 4 mm.\n\nIf the optics of the eye were otherwise perfect, theoretically, acuity would be limited by pupil diffraction, which would be a diffraction-limited acuity of 0.4 minutes of arc (minarc) or 6/2.6 acuity. The smallest cone cells in the fovea have sizes corresponding to 0.4 minarc of the visual field, which also places a lower limit on acuity. The optimal acuity of 0.4 minarc or 6/2.6 can be demonstrated using a laser interferometer that bypasses any defects in the eye's optics and projects a pattern of dark and light bands directly on the retina. Laser interferometers are now used routinely in patients with optical problems, such as cataracts, to assess the health of the retina before subjecting them to surgery.\n\nThe visual cortex is the part of the cerebral cortex in the posterior part of the brain responsible for processing visual stimuli, called the occipital lobe. The central 10° of field (approximately the extension of the macula) is represented by at least 60% of the visual cortex. Many of these neurons are believed to be involved directly in visual acuity processing.\n\nProper development of normal visual acuity depends on a human or an animal having normal visual input when it is very young. Any visual deprivation, that is, anything interfering with such input over a prolonged period of time, such as a cataract, severe eye turn or strabismus, anisometropia (unequal refractive error between the two eyes), or covering or patching the eye during medical treatment, will usually result in a severe and permanent decrease in visual acuity and pattern recognition in the affected eye if not treated early in life, a condition known as amblyopia. The decreased acuity is reflected in various abnormalities in cell properties in the visual cortex. These changes include a marked decrease in the number of cells connected to the affected eye as well as cells connected to both eyes in cortical area V1, resulting in a loss of stereopsis, i.e. depth perception by binocular vision (colloquially: \"3D vision\"). The period of time over which an animal is highly sensitive to such visual deprivation is referred to as the critical period.\n\nThe eye is connected to the visual cortex by the optic nerve coming out of the back of the eye. The two optic nerves come together behind the eyes at the optic chiasm, where about half of the fibers from each eye cross over to the opposite side and join fibers from the other eye representing the corresponding visual field, the combined nerve fibers from both eyes forming the optic tract. This ultimately forms the physiological basis of binocular vision. The tracts project to a relay station in the midbrain called the lateral geniculate nucleus, part of the thalamus, and then to the visual cortex along a collection of nerve fibers called the optic radiation.\n\nAny pathological process in the visual system, even in older humans beyond the critical period, will often cause decreases in visual acuity. Thus measuring visual acuity is a simple test in accessing the health of the eyes, the visual brain, or pathway to the brain. Any relatively sudden decrease in visual acuity is always a cause for concern. Common causes of decreases in visual acuity are cataracts and scarred corneas, which affect the optical path, diseases that affect the retina, such as macular degeneration and diabetes, diseases affecting the optic pathway to the brain such as tumors and multiple sclerosis, and diseases affecting the visual cortex such as tumors and strokes.\n\nThough the resolving power depends on the size and packing density of the photoreceptors, the neural system must interpret the receptors’ information. As determined from single-cell experiments on the cat and primate, different ganglion cells in the retina are tuned to different spatial frequencies, so some ganglion cells at each location have better acuity than others. Ultimately, however, it appears that the size of a patch of cortical tissue in visual area V1 that processes a given location in the visual field (a concept known as cortical magnification) is equally important in determining visual acuity. In particular, that size is largest in the fovea’s center, and decreases with increasing distance from there.\n\nBesides the neural connections of the receptors, the optical system is an equally key player in retinal resolution. In the ideal eye, the image of a diffraction grating can subtend 0.5 micrometre on the retina. This is certainly not the case, however, and furthermore the pupil can cause diffraction of the light. Thus, black lines on a grating will be mixed with the intervening white lines to make a gray appearance. Defective optical issues (such as uncorrected myopia) can render it worse, but suitable lenses can help. Images (such as gratings) can be sharpened by lateral inhibition, i.e., more highly excited cells inhibiting the less excited cells. A similar reaction is in the case of chromatic aberrations, in which the color fringes around black-and-white objects are inhibited similarly.\n\nVisual acuity is often measured according to the size of letters viewed on a Snellen chart or the size of other symbols, such as Landolt Cs or the E Chart.\n\nIn some countries, acuity is expressed as a vulgar fraction, and in some as a decimal number.\n\nUsing the meter as a unit of measurement, (fractional) visual acuity is expressed relative to 6/6. Otherwise, using the foot, visual acuity is expressed relative to 20/20. For all practical purposes, 20/20 vision is equivalent to 6/6. In the decimal system, acuity is defined as the reciprocal value of the size of the gap (measured in arc minutes) of the smallest Landolt C, the orientation of which can be reliably identified. A value of 1.0 is equal to 6/6.\n\nLogMAR is another commonly used scale, expressed as the (decadic) logarithm of the minimum angle of resolution (MAR). The LogMAR scale converts the geometric sequence of a traditional chart to a linear scale. It measures visual acuity loss: positive values indicate vision loss, while negative values denote normal or better visual acuity. This scale is rarely used clinically; it is more frequently used in statistical calculations because it provides a more scientific equivalent for the traditional clinical statement of “lines lost” or “lines gained”, which is valid only when all steps between lines are equal, which is not usually the case.\n\nA visual acuity of 6/6 is frequently described as meaning that a person can see detail from away the same as a person with \"normal\" eyesight would see from 6 metres. If a person has a visual acuity of 6/12, he is said to see detail from away the same as a person with \"normal\" eyesight would see it from away.\n\nHealthy young observers may have a binocular acuity superior to 6/6; the limit of acuity in the unaided human eye is around 6/3–6/2.4 (20/10–20/8), although 6/3 was the highest score recorded in a study of some US professional athletes. Some birds of prey, such as hawks, are believed to have an acuity of around 20/2; in this respect, their vision is much better than human eyesight.\n\nWhen visual acuity is below the largest optotype on the chart, the reading distance is reduced until the patient can read it. Once the patient is able to read the chart, the letter size and test distance are noted. If the patient is unable to read the chart at any distance, he or she is tested as follows:\n\nVarious countries have defined statutory limits for poor visual acuity that qualifies as a disability. For example, in Australia, the Social Security Act defines blindness as:\n\nIn the USA, the relevant federal statute defines blindness as follows:\n\nA person's visual acuity is registered documenting the following: whether the test was for distant or near vision, the eye(s) evaluated and whether corrective lenses (i.e. glasses or contact lenses) were used:\n\nSo, distant visual acuity of 6/10 and 6/8 with pinhole in the right eye will be: DscOD 6/20 PH 6/8. Distant visual acuity of count fingers and 6/17 with pinhole in the left eye will be: DscOS CF PH 16/17. Near visual acuity of 6/8 with pinhole remaining at 6/8 in both eyes with spectacles will be: NccOU 6/8 PH 6/8.\n\n\"Dynamic visual acuity\" defines the ability of the eye to visually discern fine detail in a moving object.\n\nVisual acuity measurement involves more than being able to see the optotypes. The patient should be cooperative, understand the optotypes, be able to communicate with the physician, and many more factors. If any of these factors is missing, then the measurement will not represent the patient's real visual acuity.\n\nVisual acuity is a subjective test meaning that if the patient is unwilling or unable to cooperate, the test cannot be done. A patient who is sleepy, intoxicated, or has any disease that can alter their consciousness or mental status, may not achieve their maximum possible acuity.\n\nIlliterate patients who cannot read letters and/or numbers will be registered as having very low visual acuity if this is not known. Some patients will not tell the examiner that they don't know the optotypes, unless asked directly about it. Brain damage can result in a patient not being able to recognize printed letters, or being unable to spell them.\n\nA motor inability can make a person respond incorrectly to the optotype shown and negatively affect the visual acuity measurement.\n\nVariables such as pupil size, background adaptation luminance, duration of presentation, type of optotype used, interaction effects from adjacent visual contours (or “crowding\") can all affect visual acuity measurement.\n\nThe newborn’s visual acuity is approximately 6/133, developing to 6/6 well after the age of six months in most children, according to a study published in 2009.\n\nThe measurement of visual acuity in infants, pre-verbal children and special populations (for instance, handicapped individuals) is not always possible with a letter chart. For these populations, specialised testing is necessary. As a basic examination step, one must check whether visual stimuli can be fixated, centered and followed.\n\nMore formal testing using preferential looking techniques use \"Teller acuity\" cards (presented by a technician from behind a window in the wall) to check whether the child is more visually attentive to a random presentation of vertical or horizontal gratings on one side compared with a blank page on the other side — the bars become progressively finer or closer together, and the endpoint is noted when the child in its adult carer's lap equally prefers the two sides.\n\nAnother popular technique is electro-physiologic testing using visual evoked (cortical) potentials (VEPs or VECPs), which can be used to estimate visual acuity in doubtful cases and expected severe vision loss cases like Leber's congenital amaurosis.\n\nVEP testing of acuity is somewhat similar to preferential looking in using a series of black and white stripes (sine wave gratings) or checkerboard patterns (which produce larger responses than stripes). Behavioral responses are not required and brain waves created by the presentation of the patterns are recorded instead. The patterns become finer and finer until the evoked brain wave just disappears, which is considered to be the endpoint measure of visual acuity. In adults and older, verbal children capable of paying attention and following instructions, the endpoint provided by the VEP corresponds very well to the psychophysical measure in the standard measurement (i.e. the perceptual endpoint determined by asking the subject when they can no longer see the pattern). There is an assumption that this correspondence also applies to much younger children and infants, though this does not necessarily have to be the case. Studies do show the evoked brain waves, as well as derived acuities, are very adult-like by one year of age.\n\nFor reasons not totally understood, until a child is several years old, visual acuities from behavioral preferential looking techniques typically lag behind those determined using the VEP, a direct physiological measure of early visual processing in the brain. Possibly it takes longer for more complex behavioral and attentional responses, involving brain areas not directly involved in processing vision, to mature. Thus the visual brain may detect the presence of a finer pattern (reflected in the evoked brain wave), but the \"behavioral brain\" of a small child may not find it salient enough to pay special attention to.\n\nA simple but less-used technique is checking oculomotor responses with an optokinetic nystagmus drum, where the subject is placed inside the drum and surrounded by rotating black and white stripes. This creates involuntary abrupt eye movements (nystagmus) as the brain attempts to track the moving stripes. There is a good correspondence between the optokinetic and usual eye-chart acuities in adults. A potentially serious problem with this technique is that the process is reflexive and mediated in the low-level brain stem, not in the visual cortex. Thus someone can have a normal optokinetic response and yet be cortically blind with no conscious visual sensation.\n\nVisual acuity depends upon how accurately light is focused on the retina, the integrity of the eye's neural elements, and the interpretative faculty of the brain. \"Normal\" visual acuity (in central, i.e. foveal vision) is frequently considered to be what was defined by Herman Snellen as the ability to recognize an optotype when it subtended 5 minutes of arc, that is Snellen's chart 6/6 meter, 20/20 feet, 1.00 decimal or 0.0 logMAR. In young humans, the average visual acuity of a healthy, emmetropic eye (or ametropic eye with correction) is approximately 6/5 to 6/4, so \"it is inaccurate to refer to 6/6 visual acuity as \"perfect\" vision\". 6/6 is the visual acuity needed to discriminate two contours separated by 1 arc minute- 1.75 mm at 6 meters. This is because a 6/6 letter, E for example, has three limbs and two spaces in between them, giving 5 different detailed areas. The ability to resolve this therefore requires 1/5 of the letter's total size, which in this case would be 1 minute (visual angle). The significance of the 6/6 standard can best be thought of as the lower limit of normal or as a screening cutoff. When used as a screening test, subjects that reach this level need no further investigation, even though the average visual acuity with a healthy visual system is typically better.\n\nSome people may suffer from other visual problems, such as severe visual field defects, color blindness, reduced contrast, mild amblyopia, cerebral visual impairments, inability to track fast-moving objects, or one of many other visual impairments and still have \"normal\" visual acuity. Thus, \"\"normal\" visual acuity by no means implies normal vision.\" The reason visual acuity is very widely used is that it is easily measured, its reduction (after correction) often indicates some disturbance, and that it often corresponds with the normal daily activities a person can handle, and evaluates their impairment to do them (even though there is heavy debate over that relationship).\n\nNormally, visual acuity refers to the ability to resolve two separated points or lines, but there are other measures of the ability of the visual system to discern spatial differences.\n\nVernier acuity measures the ability to align two line segments. Humans can do this with remarkable accuracy. This success is sometimes regarded as\" hyperacuity\". Under optimal conditions of good illumination, high contrast, and long line segments, the limit to vernier acuity is about 8 arc seconds or 0.13 arc minutes, compared to about 0.6 arc minutes (6/4) for normal visual acuity or the 0.4 arc minute diameter of a foveal cone. Because the limit of vernier acuity is well below that imposed on regular visual acuity by the \"retinal grain\" or size of the foveal cones, it is thought to be a process of the visual cortex rather than the retina. Supporting this idea, vernier acuity seems to correspond very closely (and may have the same underlying mechanism) enabling one to discern very slight differences in the orientations of two lines, where orientation is known to be processed in the visual cortex.\n\nThe smallest detectable visual angle produced by a single fine dark line against a uniformly illuminated background is also much less than foveal cone size or regular visual acuity. In this case, under optimal conditions, the limit is about 0.5 arc seconds or only about 2% of the diameter of a foveal cone. This produces a contrast of about 1% with the illumination of surrounding cones. The mechanism of detection is the ability to detect such small differences in contrast or illumination, and does not depend on the angular width of the bar, which cannot be discerned. Thus as the line gets finer, it appears to get fainter but not thinner.\n\nStereoscopic acuity is the ability to detect differences in depth with the two eyes. For more complex targets, stereoacuity is similar to normal monocular visual acuity, or around 0.6–1.0 arc minutes, but for much simpler targets, such as vertical rods, may be as low as only 2 arc seconds. Although stereoacuity normally corresponds very well with monocular acuity, it may be very poor, or absent, even in subjects with normal monocular acuities. Such individuals typically have abnormal visual development when they are very young, such as an alternating strabismus, or eye turn, where both eyes rarely, or never, point in the same direction and therefore do not function together.\n\nThe eye has acuity limits for detecting motion. Forward motion is limited by the \"subtended angular velocity detection threshold\" (SAVT), and horizontal and vertical motion acuity are limited by lateral motion thresholds. The lateral motion limit is generally below the looming motion limit, and for an object of a given size, lateral motion becomes the more insightful of the two, once the observer moves sufficiently far away from the path of travel. Below these thresholds subjective constancy is experienced in accordance with the Stevens' power law and Weber–Fechner law.\n\nThere is a specific acuity limit in detecting an approaching object's looming motion. This is regarded as the subtended angular velocity detection threshold (SAVT) limit of visual acuity. It has a practical value of 0.0275 rad/s. For a person with SAVT limit of formula_1, the looming motion of a directly approaching object of size , moving at velocity , is not delectable until its distance is\n\nwhere the term is omitted for small objects relative to great distances by small-angle approximation.\n\nIn order to exceed the SVAT, an object of size moving as velocity must be closer than ; beyond that distance, subjective constancy is experienced. The SVAT formula_1 can be measured from the distance at which a looming object is first detected:\n\nwhere the term is omitted for small objects relative to great distances by small-angle approximation.\n\nThe SAVT has the same kind of importance to driving safety and sports as the static limit. The formula is derived from taking the derivative of the visual angle with respect to distance, and then multiplying by velocity to obtain the time rate of visual expansion ().\n\nThere are acuity limits (formula_1) of horizontal and vertical motion as well. They can be measured and defined by the threshold detection of movement of an object traveling at distance and velocity orthogonal to the direction of view, from a set-back distance with the formula\nBecause the tangent of the subtended angle is the ratio of the orthogonal distance to the set-back distance, the angular time rate (rad/s) of lateral motion is simply the derivative of the inverse tangent multiplied by the velocity (). In application this means that an orthogonally traveling object will not be discernible as moving until it has reached the distance\n\nwhere formula_1 for lateral motion is generally ≥ 0.0087 rad/s with probable dependence on deviation from the fovia and movement orientation, velocity is in terms of the distance units, and zero distance is straight ahead. Far object distances, close set-backs, and low velocities generally lower the salience of lateral motion. Detection with close or null set-back can be accomplished through the pure scale changes of looming motion.\n\nThe motion acuity limit affects radial motion in accordance to its definition, hence the ratio of the velocity to the radius must exceed formula_1:\n\nRadial motion is encountered in clinical and research environments, in dome theaters, and in virtual-reality headsets.\n\n\n"}
{"id": "34873589", "url": "https://en.wikipedia.org/wiki?curid=34873589", "title": "William Buckingham Curtis", "text": "William Buckingham Curtis\n\nWilliam Buckingham \"Father Bill\" Curtis (January 17, 1837 – June 30, 1900) was one of the most important proponents of organized athletics in the late 1800s in America. Curtis had a remarkable career as a competitor, official, sports editor, organizer, and administrator. He was known as \"Father Bill\" in the athletic world. The death of Curtis at the age of 63 while climbing Mount Washington brought forth an outpouring of testimonials from the sports world and recognition as a \"father of American amateur athletics\".\n\nCurtis was born in Vermont and was a sickly child, having contracted tuberculosis at about 10 years old. In 1850, his family moved to Chicago, and he soon enrolled in Wabash College where he quickly became a leader in many sports. After a couple of years, he changed enrollments to Bell's Commercial College but continued to focus on athletics. When the Civil War started, Curtis joined the Illinois Volunteers and served until the war was over.\n\nCurtis actively competed from the age of 17 to 43 and included championships in gymnastics, rowing, weightlifting, and sprinting. In 1853, in his first public competition, he won nine events in the games of the Chicago Caledonian Club at the age of 17. In 1860, Curtis and his friend John C. Babcock managed Hubert Ottignon's Metropolitan Gymnasium in Chicago. From 1853 to 1872, Curtis did not lose a race in the 100-yard dash until he eventually lost to Harry Buermeyer. Curtis was also a three-time national champion in the hammer throw. Curtis and Buermeyer were considered the strongest men of their time. Curtis successfully lifted over in a \"back\" lift.\n\nCurtis helped create several amateur clubs around the country. He founded the New York Athletic Club (N.Y.A.C.) with Buermeyer and Babcock in 1868, and he was the first president of the club. In 1872, he opened the Chicago Athletic Club. Around 1880, Curtis founded the Fresh Air Club to encourage members in New York City to have outdoor exercise in rural areas. Curtis helped found the Amateur Athletic Union in 1888, which eventually became the U.S. Olympic Committee. After retiring from athletics, Curtis became the managing editor of New York City's sports newspaper, the \"Spirit of the Times\".\n\nHe died on June 30, 1900, during an ice storm on Mount Washington in New Hampshire. He is buried in Woodlawn Cemetery in the Bronx, New York City.\n\nHis referee services were in high demand by the Intercollegiate Athletic Association. Throughout his life Curtis strove to purify sports of fraud and corruption.\n\nW. B. Curtis was a devotee of speed skating. In the \"Spirit of The Times\", he covered developments in local and international speed-skating and compiled lists of skating records. He was selected as first president of the National Amateur Skating Association in 1884. Curtis was also an enthusiastic recreational skater and led skating tours on lakes and rivers with the Fresh Air Club. He would scout out the route by skating it himself beforehand and write reports in \"The Spirit of the Times\", under the name \"The Pathfinder\".\n\nCurtis was also a regular contributor to the outdoor sports magazine, \"Outing\".\nHe was inducted into the USA Track & Field Hall of Fame in 1979.\n\n"}
