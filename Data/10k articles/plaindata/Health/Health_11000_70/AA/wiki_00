{"id": "24796896", "url": "https://en.wikipedia.org/wiki?curid=24796896", "title": "12-Hydroxyeicosatetraenoic acid", "text": "12-Hydroxyeicosatetraenoic acid\n\n12-Hydroxyeicosatetraenoic acid (12-HETE) is a derivative of the 20 carbon polyunsaturated fatty acid, arachidonic acid, containing a Hydroxyl residue at carbon 12 and a 5\"Z\",\"8Z\",10\"E\",14\"Z\" Cis–trans isomerism configuration (Z=cis, E=trans) in its four double bonds. It was first found as a product of arachidonic acid metabolism made by human and bovine platelets through their 12\"S\"-lipoxygenase (i.e. ALOX12) enzyme(s). However, the term 12-HETE is ambiquous in that it has been used to indicate not only the initially detected \"S\" stereoisomer, 12\"S\"-hydroxy-5\"Z\",\"8Z\",10\"E\",14\"Z\"-eicosatetraenoic acid (12(\"S\")-HETE or 12\"S\"-HETE), made by platelets, but also the later detected \"R\" stereoisomer, 12(\"R\")-hydroxy-5\"Z\",\"8Z\",10\"E\",14\"Z\"-eicosatetraenoic acid (also termed 12(\"R\")-HETE or 12\"R\"-HETE) made by other tissues through their 12\"R\"-lipoxygenase enzyme, ALOX12B. The two isomers, either directly or after being further metabolized, have been suggested to be involved in a variety of human physiological and pathological reactions. Unlike hormones which are secreted by cells, travel in the circulation to alter the behavior of distant cells, and thereby act as Endocrine signalling agents, these arachidonic acid metabolites act locally as Autocrine signalling and/or Paracrine signaling agents to regulate the behavior of their cells of origin or of nearby cells, respectively. In these roles, they may amplify or dampen, expand or contract cellular and tissue responses to disturbances.\n\nIn humans, Arachidonate 12-lipoxygenase (12-LO, 12-LOX, ALO12, or platelet type 12-lipoxygenase) is encoded by the ALOX12 gene and expressed primarily in platelets and skin. ALOX12 metabolizes arachidonic acid almost exclusively to 12(\"S\")-hydroperoxy-5\"Z\",8\"Z\",10\"E\",14\"Z\"-eicosatetraenoic acid (12(\"S\")-HpETE or 12\"S\"-HpETE). Arachidonate 15-lipoxygenase-1 (15-LO-1, 15-LOX-1, ALOX15), which is expressed in far more tissues that ALOX12, metabolizes arachidonic acid primarily to 15(\"S\")-HpETE along with other metabolites of the 15-Hydroxyicosatetraenoic acid family; during this metabolism, however, ALOX15 also forms 12(\"S\")-HpETE as a minor product. Arachidonate 12-lipoxygenase, 12R type, also termed 12RLOX and encoded by the ALOX12B gene, is expressed primarily in skin and cornea; it metabolizes arachidonic acid to 12(\"R\")-HpETE. Cytochrome P450 enzymes convert arachidonic acid to a variety of hydroperoxy, epoxy, and dihydroxy derivatives including racemic mixtures of 12(\"S\")-HpETE and 12(\"R\")-HpETE or 12(\"S\")-HETE and 12(\"R\")-HETE; the \"R\" stereoisomer predominates in these mixtures. The initial 12(\"S\")-HpETE and 12(\"R\")-HpETE products, regardless of their pathway of formation, are rapidly reduced to 12(\"S\")-HETE and 12(\"R\")-HETE, respectively, by ubiquitous cellular peroxidases, including in particular Glutathione peroxidases or, alternatively, are further metabolized as described below.\n\nSub-primate mammals, such as the mouse, rat, rabbit, cow, and pig, express platelet type 12-lipoxygenase but also a leukocyte type 12-lipoxygenase (also termed 12/15-lipoxygenase, 12/15-LOX or 12/15-LO) which is an ortholog of, and metabolically equivalent to, human 15-LO-1 in that it forms predominantly 15(\"S\")-HpETE with 12(\"S\")-HpETE as a minor product. Mice also express an epidermal type 15-lipoxygenase (e-12LO) which has 50.8% amino acid sequence identity to human 15-LOX-2 and 49.3% sequence indetity to mouse Arachidonate 8-lipoxygenase. Mouse e-12LO metabolizes arachidonic acid predominantly to 12(\"S\")-HETE and to a lesser extent 15(\"S\")-HETE.\n\nSub-human primates, although not extensively examined, appear to have 12-lipoxygenase expression patterns that resemble those of sub-primate mammals or humans depending on the closeness of there genetic relateness to these species.\n\nIn human (and mouse) skin epidermis, 12(\"R\")-HpETE is metabolized by Epidermis-type lipoxygenase, i.e. eLOX3 (encoded by the ALOXE3 gene), to two products: a) a specific hepoxilin, 8\"R\"-hydroxy-11\"R\",12\"R\"-epoxy-5\"Z\",9\"E\",14\"Z\"-eicosatetraenoic acid (i.e. 8\"R\"-hydroxy,11\"R\",12\"R\"-epoxy-hepoxilin A3 or 8\"R\"-OH,11\"R\",12\"R\"-epoxy-hepoxilin A3) and b) 12-oxo-5\"Z\",\"8Z\",10\"E\",14\"Z\"-eicosatetraenoic acid (12-oxo-HETE, 12-oxoETE, 12-Keto-ETE, or 12-KETE); 8\"R\"-hydroxy,11\"R\",12\"R\"-epoxy-hepoxilin A3 is further metabolized by soluble Epoxide hydrolase 2 (sEH) to 8\"R\",11\"R\",12\"R\"-trihydroxy-5\"Z\",9\"E\",14\"Z\"-eicosatetraenoic acid. 12(\"R\")-HpETE also spontaneously decomposes to a mixture of hepoxilins and trihydroxy-eicosatetraenoic acids that possess \"R\" or \"S\" hydroxy and epoxy residues at various sites while 8\"R\"-hydroxy,11\"R\",12\"R\"-epoxy-hepoxilin A3 spontaneously decomposes to 8\"R\",11\"R\",12\"R\"-trihydroxy-5\"Z\",9\"E\",14\"Z\"-eicosatetraenoic acid. These decompositions may occur during tissue isolation procedures. Recent studies indicate that the metabolism by ALOXE3 of the \"R\" stereoisomer of 12-HpETE made by ALOX12B and therefore possibly the \"S\" stereoisomer of 12-HpETE made by ALOX12 or ALOX15 is responsible for forming various hepoxilins in the epidermis of human and mouse skin and tongue and possibly other tissues.\n\nHuman skin metabolizes 12(\"S\")-HpETE in reactions strictly analogous to those of 12(\"R\")-HpETE; it metabolized 12(\"S\")-HpETE by eLOX3 to 8\"R\"-hydroxy-11\"S\",12\"S\"-epoxy-5\"Z\",9\"E\",14\"Z\"-eicosatetraenoic acid and 12-oxo-ETE, with the former product then being metabolized by sEH to 8\"R\",11\"S\",12\"S\"-trihydroxy-5\"Z\",9\"E\",14\"Z\"-eicosatetraenoic acid. 12(\"S\")-HpETE also spontaneously decomposes to a mixture of hepoxilins and trihydroxy-eicosatetraenoic acids (trioxillins) that possess \"R\" or \"S\" hydroxy and \"R\",\"S\" or \"S\",\"R\" epoxide residues at various sites while 8\"R\"-hydroxy,11\"S\",12\"S\"-epoxy-hepoxilin A3 spontaneously decomposes to 8\"R\",11\"S\",12\"S\"-trihydroxy-5\"Z\",9\"E\",14\"Z\"-eicosatetraenoic acid.\n\nIn other tissues and animal species, numerous hepoxilins form but the hepoxilin synthase activity responsible for their formation is variable. (Hepoxilin A3 [8\"R/S\"-hydroxy-11,12-epoxy-5\"Z\",9\"E\",14\"Z\"-eicosatrienoic acid] and hepoxilin B3 [10(\"R/S\"-hydroxy-11,12-epxoy-5\"Z\",8\"Z\",14\"Z\"-eicosatrienoic acid] refer to a mixture of Diastereomers and⁄or Enantiomers derived from arachidonic acid.) Cultured RINm5F rat Insulinoma cells convert 12(\"S\")-HpETE to hepoxilin A3 in a reaction that is completely dependent on, and co-localizes with, the cells' leukocyte type 12-LOX; furthermore, recombinant rat and porcine leukocyte type 12-LOX as well as human platelet type 12-LOX metabolize 12(\"S\")-HpETE to hepoxylin A3. However, transfection of HEK293 human embryonic kidney cells with each of the 6 rat lipoxygenases, including rat eLOX3, found that hepoxilin B3 production required eLOX3; furthermore, the development of inflammation-induced tactile pain hypersensitivity (hyperesthesia; tactile Allodynia) in rats required eLOX3-dependent production of hepoxilin B3 by spinal tissue. Thus, the production of hepoxilins from 12(S)-HpETE may result from the intrinsic activity of platelet or leukocyte type 12-LOX's, require eLOX3, or even result from 12(\"S\")-HpETE spontaneous (and perhaps artefactual) decomposition during isolation. The majority of reports on hepoxilin formation have not defined the pathways evolved.\n\nHuman and other mammalian cytochrome P450 enzymes convert 12(\"S\")-HpETE to 12-oxo-ETE.\n\n12-HETE (stereoisomer not determined), 12(\"S\")-HETE, 12-oxo-ETE, hepoxilin B3, and trioxilin B3 are found in the \"sn\"-2 position of phospholipids isolated from normal human epidermis and human psoriatic scales. This indicates that the metabolites are acylated into the \"sn\"-2 position after being formed and/or directly produced by the metabolism of the arachidonic acid at the \"sn\"-2 position of these phospholipids. These acylation reactions may sequester and thereby inactivate or store the metabolites for release during cell stimulation.\n\n12(\"S\")-HETE and 12(\"R\")-HETE are converted to 12-oxo-ETE by microsomal NAD+-dependent 12-hydroxyeicosanoid dehdrogenase in porcine polymophonuclear leukocytes; a similar pathway may be active in rabbit corneal epitheleum, cow corneal epitheleum, and mouse keratinocytes although this pathway has not been described in human tissues.\n\n12-oxo-ETE is metabolised by cytoslic NADH-dependent 12-oxoeicosinoid Δ10-reductase to 12-oxo-5\"Z\",8\"Z\",14\"Z\"-eicosatreienoic acid (12-oxo-ETrE); 12-ketoreductase may then reduce this 12-oxo-ETrE to 12(\"R\")-hydroxy-5\"Z\",8\"Z\",14\"Z\"-eicosatreienoic acid (12(\"R\")-HETrE) and to a lesser extent 12(\"S\")-hydroxy-5\"Z\",8\"Z\",14\"Z\"-eicosatreienoic acid (12(\"S\")-HETrE).\n\nThe G protein-coupled receptor, GPR31, cloned from PC3 human prostate cancer cell line is a high affinity (Kd=4.8 nM) receptor for 12(\"S\")-HETE; GPR31 does not bind 12(\"R\")-HETE and has relatively little affinity for 5(\"S\")-HETE or 15(\"S\")-HETE. GPR31 mRNA is expressed at low levels in several human cell lines including K562 cells (human myelogenous leukemia cell line), Jurkat cells, (T lymphocye cell line), Hut78 cells (T cell lymphoma cell line), HEK 293 cells (primary embryonic kidney cell line), MCF7 cells (mammary adenocarcinoma cell line), and EJ cells (bladder carcinoma cell line). This mRNA appears to be more highly expressed in PC3 and DU145 prostate cancer cell lines as well as in human umbilical vein endothelial cells (HUVEC), human umbilical vein endothelial cells (HUVEC), human brain microvascular endothelial cells (HBMEC), and human pulmonary aortic endothelial cells (HPAC). In PC-3 prostate cancer cells, GPR31 receptor mediates the action of 12(\"S\")-HETE in activating the Mitogen-activated protein kinase kinase/Extracellular signal-regulated kinases-1/2 pathway and NFκB pathway that lead to cell growth and other functions. Studies have not yet determined the role, if any, in GPR31 receptor in the action of 12(\"S\")-HETE in other cell types.\n\nA G protein-coupled receptor for the 5(\"S\"),12(\"R\")-dihydroxy metabolite of aracidonic acid, Leukotriene B4, vis., Leukotriene B4 receptor 2 (BLT2), but not its Leukotriene B4 receptor 1, mediates responses to 12(\"S\")-HETE, 12(\"R\")-HETE, and 12-oxo-ETE in many cell types. Based on the effects of LTB4 receptor antagonists, for example, leukotriene B4 receptor 2 mediates: the rise in cytosolic Ca concentration (a key signal for cell activation) in human neutrophils and the rise in cytosolic Ca concentration and chemotaxis in Chinese hamstery ovarian cells stimlated by 12(\"S\")-HETE, 12(\"R\")-HETE, and/or 12-oxo-ETE; the itch response to 12(\"S\")-HETE and PMN inflammatory infiltration response to 12(\"R\")-HETE triggered by the injection these metabolites into the skin of mice and guinea pigs, respectively; and an in vitro angiogenic response by Human umbilical vein endothelial cells (HUVEC) and in vivo angiogenic response by mice to 12(\"S\")-HETE. The BLT2 receptor, in contrast to the GPR31 receptor, appears to be expressed at a high level in a wide range of tissues including neutrophils, eosinophils, monocytes, spleen, liver, and ovary. However, 12-Hydroxyheptadecatrienoic acid (i.e. 12-(\"S\")-hydroxy-5\"Z\",8\"E\",10\"E\"-heptadecatrienoic acid or 12-HHT), a product made when prostaglandin H2 is metablized to Thromboxane A2 by Thromboxane synthase or spontaneously rearranges non-enzymatically (see 12-Hydroxyheptadecatrienoic acid) is the most potent BLT2 receptor agonist detected to date. To clarify the role of BLT2 versus GPC31 receptors in responses to 12(\"S\")-HETE, and the role(s) of LTB4, 12(\"S\")-HETE, versus 12-HHT in BLT2-mediated responses, it will be necessary to determine: a) if leukotriene B4 interacts with the GPR31 receptor; b) if BLT2 receptor antagonists interfere with the GPR31 receptor; and c) the relative concentrations and availability of LTB4, 12(\"S\")-HETE, and 12-HHT in tissues exhibiting BLT2-dependent responses. Ultimately, both receptors and all three ligands may prove to be responsible for some tissue responses in vivo.\n\n12(\"S\")-HETE and 12(\"R\")-HETE bind to and act as Competitive antagonists of the Thromboxane receptor which mediates the actions of Thromboxane A2 and Prostaglandin H2. This antagonistic activity was responsible for the ability of 12(\"S\")-HETE and 12(\"R\")-HETE to relax mouse mesenteric arteries pre-constricted with a thromboxane A2 mimetic, U46619.\n\n12(\"S\")-HETE binds with high affinity to a 50 kilodalton (Kda) subunit of a 650 kDa cytosolic and nuclear protein complex.\n\n12(\"S\")-HpETE, 12(\"R\")-HETE, racemic mixtures of these 12-HETEs, and/or 12-oxo-ETE stimulate: a) the directed migration (chemotaxis) of human, rat, and rabbit neutrophils as well as rabbit macrophages; b) human neutrophils to adhere to each other (i.e. aggregate) and in cooperation with Tumor necrosis factor alpha or Platelet-activating factor, to release their granule-bound enzymes; c) the binding of human vascular epithelial cells to human monocytes; d) DNA synthesis and mitogenesis in the immortalized human keratinocyte cell line HaCaT; and e) when injected in the skin of human volunteers, the extravasation and local accumulation of circulating blood neutrophils and mononuclear cells.\nThese results suggest these metabolites contribute to the inflammation that occurs as sites where they are formed in abnormal amounts such as in human rheumatoid arthritis, Inflammatory bowel disease, Contact dermatitis, psoriasis, various forms of Ichthyosis including Congenital ichthyosiform erythroderma, and corneal inflammatory diseases. Since BLT2 appears to mediate the responses of leukocytes to 12(\"S\")-HpETE, 12(\"S\")-HETE, 12(\"R\")-HETE, and 12-oxo-ETE but GPR31 is expressed by various other cells (e.g. vascular endothelium) involved in inflammation, the pro-inflammatory actions of 12-HETE in humans may involve both types of G protein-coupled receptors.\n\n12(\"S\")-HpETE and 12(\"S\")-HETE induce itching responses when injected into the skin of mice; this has led to the suggestion that these metabolites contribute to the itching (i.e. clinical pruritus) which accompanies such conditions as atopic dermatitis, contact dermatitis, urticaria, chronic renal failure, and cholestasis. Since it mediates 12(\"S\")-HETE-induced itching in the mouse model, BLT2 rather than GPR31 may mediate human itch in these reactions.\n\n12-HETE (stereoisomer not defined) is the dominant arachidonic acid metabolite in cultured PC3 human prostate cancer cells and its levels in human prostate cancer tissue exceed by >9-fold its levels in normal human prostate tissue. Furthermore, 12(\"S\")-HETE a) increases the expression of Alpha-v beta-5 cell surface adhesion molecule and associated with this the survival of cultured PC3 cells; b) promotes the phosphorylation of retinoblastoma protein to inhibit its tumor suppressor function while promoting the proliferation of cultured PC3 cells; c) stimulates PC3 cells to activate the Mitogen-activated protein kinase kinase/extracellular signal-regulated kinases-1/2 pathway and the NFκB pathways that lead to cell proliferation; d) reverses the apoptosis-inducing (i.e. cell-killing) effect of pharmacologically inhibiting 12-LO in cultured DU145 human prostate cancer cells; e) promotes the induction of cyclooxygenase-1 and thereby the synthesis of this enzyme's growth-promoting arachidonic acid metabolite, PGE2, in cultured PC3 and LNCaP human prostate cancer cells; and f) induces cultured PC3 cells to express Vascular endothelial growth factor (VEGF), a protein that stimulates the formation of the microvasclature which assists in the metastasis of cancer. These results suggest that the 12(\"S\")-HETE made by prostate cancer tissues serves to promote the growth and spread of this cancer. Since it mediates the action of 12(\"S\")-HETE in stimulating cultured PC3 cells to activate the Mitogen-activated protein kinase kinase/Extracellular signal-regulated kinases-1/2 pathway and NFκB pathways, the GPR31 receptor may contribute to the pro-malignant activity of 12(\"S\")-HETE. However, LNCaP and PC3 cells also express BLT2 receptors; in LNCaP cells, BLT2 receptors are positively linked (i.e. stimulate the expression of) to the growth- and metastasis-promoting androgen receptor; in PC3 cells, BLT2 receptors stimulate the NF-κB pathway to inhibit the apoptosis caused by cell detachment from surfaces (i.e. Anoikis; and, in BLT2-overexpressing PWR-1E non-malignant prostate cells, 12(\"S\")-HETE diminish anoikis-induced apoptosis. ith occurs. Thus, the role of 12(\"S\")-HETE in human prostate cancer, if any, may involve its activation of one or both of the GPR31 and BLT2 receptors.\n\nPreclinical laboratory studies analogous to those conducted on the pro-malignant effects of 12(\"S\")-HETE and growth-inhibiting effects of blocking 12-HETE production in cultured prostate cancer cell lines, have implicated 12-HETE (stereoisomer sometimes undefined) in cancer cell lines from various other human tissues including those from the liver, intestinal epithelium, lung, breast, skin (Melanoma, ovary, pancrease, and possibly bladder. These studies implicate the interaction of 12-HETE with BLT2 receptors in intestinal epithelium cancer cells, and BLT2 receptors in breast, ovary, pancreas, and bladder cancer cells. While the studies on these tissues have not been as frequent or diverse as those on prostate cancer cell lines, they are suggested to indicate that 12-HETE contributes to the growth or spread of the corresponding cancer in humans.\n\n12(S)-HETE, 12(\"S\")-HpETE, and with far less potency 12(\"R\")-HETE reduced insulin secretion and caused apoptosis in cultured human pancreatic insulin-secreting Beta cell lines and prepared Pancreatic islets. TNFα, IL-1β, and IFNγ also reduced insulin secretion in cultured human pancreatic INS-1 beta cells, apparently by inducing the expression of NOX1 (NADPH oxidase 1) and thereby to the production of cell-toxic Reactive oxygen species; these cytokine effects were completely dependent on 12-lipoxygenase and mimicked by 12(\"S\")-HETE but not 12(\"R\")-HETE. 12-lipoxygenase-knockout mice (i.e., mice genetically manipulated to remove the Alox12 [i.e. 12-lipoxygenase gene, see lipoxygenase#mouse lipoxygenases) are resistant to a) streptozotocin-induced, b) high fat diet-induced, and c) autoimmune-induced diabetes. Further studies in animal models suggest that the 12\"S\"-HETE made by pancreatic beta cells (or possibly alpha cells or other cell types indigenous to or invading the pancreatic islands) orchestrate a local immune response that results in the injury and, when extreme, death of beta cells. These results suggest that the 12-lipoxygenase-12S-HETE pathway is one factor contributing to immunity-based type I diabetes as well as low insulin output type II diabetes.\n\n12(\"S\")-HETE and 12(\"S\")-HpETE stimulate the dilation of rat mesenteric arteries; 12(\"S\")-HETE stimulates the dilation of coronary microvessels in pigs and the mesenteric arteries of mice, one or more of these three metabolites are implicated in the vasolilation of rat basilar artery, 12(\"R\")-HETE and to a slightly lesser extent 12(\"S\")-HETE constrict the renal artery of dogs and 12-HETE (stereoisomer undetermined) is implicated in the angiotensin II-induced arterial hypertension response of human placenta. The vasodilating effect on mouse mesenteric arteries appears due to 12\"S\"-HETE's ability to act as a Thromboxane receptor antagonist and thereby block the vasoconstricting actions of thromboxane A2. These results indicate that the cited metabolites have dilating or constricting effects that depend on the arterial vascular site and or species of animal examined; their role in human blood pressure regulation is unclear.\n\nExcessive 12-HETE production is implicated in psoriasis.\n\n"}
{"id": "54512669", "url": "https://en.wikipedia.org/wiki?curid=54512669", "title": "Alveoloplasty", "text": "Alveoloplasty\n\nAlveoloplasty is a dental pre-prosthetic procedure performed to smoothen or reshape the jawbone. In this procedure, the bony edges of the alveolar ridge and its surrounding structures is made smooth, redesigned or recontoured so that a well-fitting, comfortable, and esthetic dental prosthesis may be fabricated. This pre-prosthetic surgery prepares the mouth to receive a dental prosthesis.\n\nAfter tooth extraction, the residual crest irregularities, undercuts or bone spicules should be removed, because they may result in an obstruction in placing a prosthetic restorative appliance. Recontouring can be made at the time of extraction or at a later time.\n\nThe simplest form of alveoloplasty can be in the form of a digital compression on the lateral walls of bone after simple tooth extraction, provided that there are no gross bone irregularities. When more irregularities exist, other techniques can be adopted, such as the conservative technique, interseptal ( Dean's) alveoloplasty, Obwegeser's modification of interseptal, alveoloplasty after post extraction and the alveoloplasty performed on edentulous ridges.\n\nA full thickness flap is usually elevated to a point apical to the desired area to be contoured, and according to the amount of bone needed to be removed, a bone file, or a bone rongeur, or a burr under copious irrigation can be used to provide the desired contour. Taking in consideration that lack of irrigation can lead to bone necrosis. When finished, the flap is repositioned and sutured. The alveolar mucosa covering bone should have uniform thickness, density and compressibility to evenly distribute the masticatory forces to the underlying bone.\n"}
{"id": "447382", "url": "https://en.wikipedia.org/wiki?curid=447382", "title": "Bates method", "text": "Bates method\n\nThe Bates method is an alternative therapy aimed at improving eyesight. Eye-care physician William Horatio Bates, M.D. (1860–1931) attributed nearly all sight problems to habitual strain of the eyes, and felt that glasses were harmful and never necessary. Bates self-published a book, \"\", as well as a magazine, \"\", (and earlier collaborated with Bernarr MacFadden on a correspondence course) detailing his approach to helping people relax such \"strain\", and thus, he claimed, improve their sight. His techniques centered on visualization and movement. He placed particular emphasis on imagining black letters and marks, and the movement of such. He also felt that exposing the eyes to sunlight would help alleviate the \"strain\".\n\nDespite continued anecdotal reports of successful results, including well-publicised support by Aldous Huxley, Bates' techniques have not been objectively shown to improve eyesight. His main physiological proposition—that the eyeball changes shape to maintain focus—has consistently been contradicted by observation. In 1952, optometry professor Elwin Marg wrote of Bates, \"Most of his claims and almost all of his theories have been considered false by practically all visual scientists.\" Marg concluded that the Bates method owed its popularity largely to \"flashes of clear vision\" experienced by many who followed it. Such occurrences have since been explained as a contact lens-like effect of moisture on the eye, or a flattening of the lens by the ciliary muscles.\n\nThe Bates method has been criticized not only because there is no good evidence it works, but also because it can have negative consequences for those who attempt to follow it: they might damage their eyes through overexposure of their eyes to sunlight, put themselves and others at risk by not wearing their corrective lenses while driving, or neglect conventional eye care, possibly allowing serious conditions to develop.\n\nAccommodation is the process by which the eye increases optical power to maintain focus on the retina while shifting its gaze to a closer point. The long-standing medical consensus is that this is accomplished by action of the ciliary muscle, a muscle \"within\" the eye, which adjusts the curvature of the eye's crystalline lens. This explanation is based in the observed effect of atropine temporarily preventing accommodation when applied to the ciliary muscle, as well as images reflected on the crystalline lens becoming smaller as the eye shifts focus to a closer point, indicating a change in the lens' shape. Bates rejected this explanation, and in his 1920 book presented photographs that he said showed that the image remained the same size even as the eye shifted focus, concluding from this that the lens was not a factor in accommodation. However, optometrist Philip Pollack in a 1956 work characterized these photographs as \"so blurred that it is impossible to tell whether one image is larger than the other\", in contrast to later photographs that clearly showed a change in the size of the reflected images, just as had been observed since the late nineteenth century.\n\nBates adhered to a different explanation of accommodation that had already been generally disregarded by the medical community of his time. Bates' model had the muscles \"surrounding\" the eyeball controlling its focus. In addition to their known function of turning the eye, Bates maintained, they also affect its shape, elongating the eyeball to focus at the near-point or shortening it to focus at a distance. Commenting on this hypothesis in an interview with WebMD, ophthalmologist Richard E. Bensinger stated \"When we put drops in the eye to dilate the pupil, they paralyze the focusing muscles. The evidence of the anatomical fallacy is that you can't focus, but your eye can move up and down, left and right. The notion that external muscles affect focusing is totally wrong.\" Science author John Grant writes that many animals, such as fishes, accommodate by elongation of the eyeball, \"it's just that humans aren't one of those animals.\"\n\nLaboratory tests have shown that the human eyeball is far too rigid to spontaneously change shape to a degree that would be necessary to accomplish what Bates described. Exceedingly small changes in axial length of the eyeball (18.6–19.2 micrometres) are caused by the action of the ciliary muscle during accommodation. However, these changes are far too small to account for the necessary changes in focus, producing changes of only −0.036 dioptres.\n\nMedical professionals characterize refractive errors such as nearsightedness, farsightedness, astigmatism, and presbyopia (the age-related blurring of near-point vision) as consequences of the eye's shape and other basic anatomy, which there is no evidence that any exercise can alter. Bates, however, believed that these conditions are caused by tension of the muscles surrounding the eyeball, which he believed prevents the eyeball from sufficiently changing shape (per his explanation of accommodation) when gaze is shifted nearer or farther. Bates characterized this supposed muscular tension as the consequence of a \"mental strain\" to see, the relief of which he claimed would instantly improve sight. He also linked disturbances in the circulation of blood, which he said is \"very largely influenced by thought\", not only to refractive errors but also to double vision, crossed-eye, lazy eye, and to more serious eye conditions such as cataracts and glaucoma. His therapies were based on these assumptions.\n\nBates felt that corrective lenses, which he characterized as \"eye crutches\", are an impediment to curing poor vision. In his view, \"strain\" would increase as the eyes adjust to the correction in front of them. He thus recommended that glasses be discarded by anyone applying his method.\n\nIn his writings, Bates discussed several techniques that he claimed helped patients to improve their sight. He wrote \"The ways in which people strain to see are infinite, and the methods used to relieve the strain must be almost equally varied,\" emphasizing that no single approach would work for everyone. His techniques were all designed to help disassociate this \"strain\" from seeing and thereby achieve \"central fixation\", or seeing what is in the central point of vision without staring. He asserted that \"all errors of refraction and all functional disturbances of the eye disappear when it sees by central fixation\" and that other conditions were often relieved as well.\n\nBates suggested closing the eyes for minutes at a time to help bring about relaxation. He asserted that the relaxation could be deepened in most cases by \"palming\", or covering the closed eyes with the palms of the hands, without putting pressure on the eyeballs. If the covered eyes did not strain, he said, they would see \"a field so black that it is impossible to remember, imagine, or see anything blacker\", since light was excluded by the palms. However, he reported that some of his patients experienced \"illusions of lights and colors\" sometimes amounting to \"kaleidoscopic appearances\" as they \"palmed\", occurrences he attributed to his ubiquitous \"strain\" and that he claimed disappeared when one truly relaxed. This phenomenon, however, was almost certainly caused by Eigengrau or \"dark light\". In fact, even in conditions of perfect darkness, as inside a cave, neurons at every level of the visual system produce random background activity that is interpreted by the brain as patterns of light and color.\n\nBates placed importance on mental images, as he felt relaxation was the key to clarity of imagination as well as of actual sight. He claimed that one's poise could be gauged by the visual memory of black; that the darker it appeared in the mind, and the smaller the area of black that could be imagined, the more relaxed one was at the moment. He recommended that patients think of the top letter from an eye chart and then visualize progressively smaller black letters, and eventually a period or comma. But he emphasized his view that the clear visual memory of black \"cannot be attained by any sort of effort\", stating that \"the memory is not the cause of the relaxation, but must be preceded by it,\" and cautioned against \"concentrating\" on black, as he regarded an attempt to \"think of one thing only\" as a strain.\n\nWhile Bates preferred to have patients imagine something black, he also reported that some found objects of other colors easiest to visualize, and thus were benefited most by remembering those, because, he asserted, \"the memory can never be perfect unless it is easy.\" Skeptics reason that the only benefit to eyesight gained from such techniques is \"itself\" imagined, and point out that familiar objects, including letters on an eye chart, can be recognized even when they appear less than clear.\n\nHe thought that the manner of eye movement affected the sight. He suggested \"shifting\", or moving the eyes back and forth to get an illusion of objects \"swinging\" in the opposite direction. He believed that the smaller the area over which the \"swing\" was experienced, the greater was the benefit to sight. He also indicated that it was usually helpful to close the eyes and \"imagine\" something \"swinging\". By alternating actual and mental shifting over an image, Bates wrote, many patients were quickly able to shorten the \"shift\" to a point where they could \"conceive and swing a letter the size of a period in a newspaper\". One who mastered this would attain the \"universal swing\", Bates believed.\n\nPerhaps finding Bates' concepts of \"shifting\" and \"swinging\" too complicated, some proponents of vision improvement, such as Bernarr Macfadden, suggested simply moving the eyes up and down, from side to side, and shifting one's gaze between a near-point and a far-point.\n\nBates believed that the eyes were benefited by exposure to sunlight. He stated that \"persons with normal sight can look directly at the sun, or at the strongest artificial light, without injury or discomfort,\" and gave several examples of patients' vision purportedly improving after sungazingthis is at variance with the well-known risk of eye damage that can result from direct sunlight observation.\n\nBates said that, just as one should not attempt to run a marathon without training, one should not immediately look \"directly\" at the sun, but he suggested that it could be worked up to. He acknowledged that looking at the sun could have ill effects, but characterized them as being \"always temporary\" and in fact the effects of strain in \"response\" to sunlight. He wrote that he had cured people who believed that the sun had caused them permanent eye damage. In his magazine, Bates later suggested exposing only the white part of the eyeball to direct sunlight, and only for seconds at a time, after allowing the sun to shine on closed eyelids for a longer period.\n\nPosthumous publications of Bates' book omitted mention of the supposed benefits from direct sunlight shining on open eyes.\n\nBates' techniques have never been scientifically established to improve eyesight. Several of Bates' techniques, including \"sunning\", \"swinging\", and \"palming\", were combined with healthy changes to diet and exercise in a 1983 randomized controlled trial of myopic children in India. After 6 months, the experimental groups \"did not show any statistically significant difference in refractive status\", though the children in the treatment group \"subjectively … felt relieved of eye strain and other symptoms\".\n\nIn 1967 the British Medical Journal observed that \"Bates […] advocated prolonged sun-gazing as the\ntreatment of myopia, with disastrous results.\"\n\nThe philosopher Frank J. Leavitt has argued that the method Bates described would be difficult to test scientifically due to his emphasis on relaxation and visualization. Leavitt asked \"How can we tell whether someone has relaxed or imagined something, or just thinks that he or she has imagined it?\" In regards to the possibility of a placebo trial, Leavitt commented \"I cannot conceive of how we could put someone in a situation where he thinks he has imagined something while we know that he has not.\"\n\nAfter Bates died in 1931, his methods of treatment were continued by his widow Emily and other associates, some of whom incorporated exercises and dietary recommendations. Most subsequent proponents did not stand by Bates' explanation of how the eye focuses mechanically, but nonetheless maintained that relieving a habitual \"strain\" was the key to improving sight.\n\nMargaret Darst Corbett first met Bates when she consulted him about her husband's eyesight. She became his pupil, and eventually taught his method at her School of Eye Education in Los Angeles. She was of the stated belief that \"the optic nerve is really part of the brain, and vision is nine-tenths mental and one-tenth only physical.\"\n\nIn late 1940, Corbett and her assistant were charged with violations of the Medical Practice Act of California for treating eyes without a licence. At the trial, many of her students testified on her behalf, describing in detail how she had enabled them to discard their glasses. One witness testified that he had been almost blind from cataracts, but that, after working with Corbett, his vision had improved to such an extent that for the first time he could read for eight hours at a stretch without glasses. Corbett explained in court that she was practicing neither optometry nor ophthalmology and represented herself not as a doctor but only as an \"instructor of eye training\". Describing her method she said \"We turn vision on by teaching the eyes to shift. We want the sense of motion to relieve staring, to end the fixed look. We use light to relax the eyes and to accustom them to the sun.\"\n\nThe trial attracted widespread interest, as did the \"not guilty\" verdict. The case spurred a bill in the Californian State Legislature that would have then made such vision education illegal without an optometric or medical licence. After a lively campaign in the media, the bill was rejected.\n\nPerhaps the most famous proponent of the Bates method was the British writer Aldous Huxley. At the age of sixteen Huxley had an attack of keratitis, which, after an 18-month period of near-blindness, left him with one eye just capable of light perception and the other with an unaided Snellen fraction of 10/200. This was mainly due to opacities in both corneas, complicated by hyperopia and astigmatism. He was able to read only if he wore thick glasses and dilated his better pupil with atropine, to allow that eye to see around an opacity in the center of the cornea.\n\nIn 1939, at the age of 45 and with eyesight that continued to deteriorate, he happened to hear of the Bates method and sought the help of Margaret Corbett, who gave him regular lessons. Three years later he wrote \"The Art of Seeing\", in which he related: \"Within a couple of months I was reading without spectacles and, what was better still, without strain and fatigue... At the present time, my vision, though very far from normal, is about twice as good as it used to be when I wore spectacles.\" Describing the process, Huxley wrote that \"Vision is not won by making an effort to get it: it comes to those who have learned to put their minds and eyes into a state of alert passivity, of dynamic relaxation.\" He expressed indifference regarding the veracity of Bates' explanation of how the eye focuses, stating that \"my concern is not with the anatomical mechanism of accommodation, but with the art of seeing.\"\n\nHis case generated wide publicity as well as scrutiny. Ophthalmologist Walter B. Lancaster, for example, suggested in 1944 that Huxley had \"learned how to use what he has to better advantage\" by training the \"cerebral part of seeing\", rather than actually improving the quality of the image on the retina.\n\nIn 1952, ten years after writing \"The Art of Seeing\", Huxley spoke at a Hollywood banquet, wearing no glasses and, according to Bennett Cerf, apparently reading his paper from the lectern without difficulty. In Cerf's words:\nThen suddenly he faltered—and the disturbing truth became obvious. He wasn't reading his address at all. He had learned it by heart. To refresh his memory he brought the paper closer and closer to his eyes. When it was only an inch or so away he still couldn't read it, and had to fish for a magnifying glass in his pocket to make the typing visible to him. It was an agonizing moment.\n\nIn response to this, Huxley wrote \"I often do use magnifying glasses where conditions of light are bad, and have never claimed to be able to read except under very good conditions.\" This underscored that he had not regained anything close to normal vision, and in fact never claimed that he had.\n\n\"Natural vision correction\" or \"natural vision improvement\" continues to be marketed by practitioners offering individual instruction, many of whom have no medical or optometric credentials. Most base their approach in the Bates method, though some also integrate vision therapy techniques. There are also many self-help books and programs, which have not been subjected to randomized controlled trials, aimed at improving eyesight naturally. Purveyors of such approaches argue that they lack the funds to formally test them.\n\nThe heavily advertised \"See Clearly Method\" (of which sales were halted by a court order in November 2006, in response to what were found to be dishonest marketing practices) included \"palming\" and \"light therapy\", both adapted from Bates. The creators of the program, however, emphasized that they did not endorse Bates' approach overall.\n\nIn his 1992 book \"The Bates Method, A Complete Guide to Improving Eyesight—Naturally\", \"Bates method teacher\" Peter Mansfield was very critical of eye care professionals for prescribing corrective lenses, recommending most of Bates' techniques to improve vision. The book included accounts of twelve \"real cases\", but did not report any information about refractive error.\n\nCzech native John Slavicek claims to have created an \"eye cure\" that improves eyesight in three days, borrowing from ancient yogic eye exercises, visualizations from the Seth Material, and the Bates method. Although he has testimonials from his neighbor and others, several of his students indicate that he has greatly exaggerated their cases. Slavicek's self-published manual, \"Yoga for the Eyes\", was rejected by an ophthalmologist who evaluated it, and evinced no interest from the World Health Organization and St. Erik's Eye Foundation in Sweden as he had not conducted double-blind tests.\n\nIn support of the effectiveness of the Bates method, proponents point to the many accounts of people allegedly having improved their eyesight by applying it. While these anecdotes may be told and passed on in good faith, several potential explanations exist for the phenomena reported other than a genuine reversal of a refractive error due to the techniques practiced:\n\nIn 2004 the American Academy of Ophthalmology (AAO) published a review of various research regarding \"visual training\", which consisted of \"eye exercises, muscle relaxation techniques, biofeedback, eye patches, or eye massages\", \"alone or in combinations\". No evidence was found that such techniques could objectively benefit eyesight, though some studies noted changes, both positive and negative, in the visual acuity of nearsighted subjects as measured by a Snellen chart. In some cases noted improvements were maintained at subsequent follow-ups. However, these results were not seen as actual reversals of nearsightedness, and were attributed instead to factors such as \"improvements in interpreting blurred images, changes in mood or motivation, creation of an artificial contact lens by tear film changes, or a pinhole effect from miosis of the pupil.\"\n\nIn 2005 the Ophthalmology Department of New Zealand's Christchurch Hospital published a review of forty-three studies regarding the use of eye exercises. They found that \"As yet there is no clear scientific evidence published in the mainstream literature supporting the use of eye exercises\" to improve visual acuity, and concluded that \"their use therefore remains controversial.\"\n\nA frequent criticism of the Bates method is that it has remained relatively obscure, which is seen as proof that it is not truly effective. Writer Alan M. MacRobert concluded in a 1979 article that the \"most telling argument against the Bates system\" and other alternative therapies was that they \"bore no fruit\". In regards to the Bates method, he reasoned that \"If palming, shifting, and swinging could really cure poor eyesight, glasses would be as obsolete by now as horse-drawn carriages.\"\n\nDiscarding one's corrective lenses, as Bates recommended, or wearing lenses weaker than one's prescribed correction, as some Bates method advocates suggest, poses a potential safety hazard in certain situations, especially when one is operating a motor vehicle. James Randi related that his father, shortly after discarding glasses on the advice of Bates' book, wrecked his car. Bates method teachers often caution that when driving, one should wear the correction legally required.\n\nOne of the greatest potential dangers of faith in the Bates method is that a believer may be disinclined to seek medical advice regarding what could be a sight-threatening condition requiring prompt treatment, such as glaucoma. Also, children with vision problems may require early attention by a professional in order to successfully prevent lazy eye. Such treatment may include exercises, but which are different from those associated with the Bates method, and parents who subscribe to Bates' ideas may delay seeking conventional care until it is too late. It may further be necessary for a child at risk of developing lazy eye to wear the proper correction.\n\n\n"}
{"id": "27855184", "url": "https://en.wikipedia.org/wiki?curid=27855184", "title": "Blood irradiation therapy", "text": "Blood irradiation therapy\n\nBlood irradiation therapy is a procedure in which the blood is exposed to low level red light (often laser light) for therapeutic reasons. Most research on blood irradiation therapy has been conducted in Germany (by UV lamps), and in Russia (in all variants) while smaller-scale research has been performed in other countries such as Britain.\n\nBlood irradiation therapy can be administered in three ways. Extracorporeally, drawing blood out and irradiating it in a special cuvette. This method is used for the ultraviolet (UV) blood irradiation (UVBI) by UV lamps. The laser light is monochromatic, i.e. it has such a wavelength that allows you to bring light into the optical fiber and carry out irradiation intravenously through a catheter in a vein. This method is more simple and effective. Blood irradiation therapy is also administered externally through the skin on the projection of large blood vessels.\n\nIt is not related to the practice of gamma irradiation of blood in transfusion medicine.\n\nHistorically, German doctors were the first to irradiate blood by UV lamps in the 1920s, and UVBI is widely spread in Germany so far. In the USA, this technique was best known during the Second World War.\nIntravenous laser blood irradiation (a more effective and modern variant) was developed experimentally by the Russian researchers, Meshalkin and Sergievskiy, and introduced into clinical practice in 1981. Originally the method was applied in the treatment of cardiovascular abnormalities. but it is now used in the treatment of a wide range of diseases in select countries.\n\nHowever, one must note that such treatment is not financed by health insurance systems and is not used in the European Union and in the USA since the nineteen seventies, and it is not an FDA-authorized procedure. The vastly variable effects do not warrant the use of this invasive technique.\n\nIntravenous or intravascular laser blood irradiation (ILBI) involves the \"in-vivo\" illumination of the blood by feeding low level laser light generated by a 1–3 mW helium–neon laser at a wavelength of 632.8 nm into a vascular channel, usually a vein in the forearm, under the assumption that any therapeutic effect will be circulated through the circulatory system. \nMost often wavelengths of 365, 405, 525 and 635 nm and power of 2.3 mW are used. The technique is widely used at present in Russia, less in Asia, and not extensively in other parts of the world. It is shown that ILBI improves blood flow and its transport activities, therefore, tissue trophism, has a positive effect on the immune system and cell metabolism. This issue is subject to skepticism. There have been some calls to increase research on this topic.\n\nTranscutaneous therapy applies laser light on unbroken skin in areas with large numbers of blood vessels (such as the forearm). Because of the skin acting as a barrier to the blood, absorbing low level laser energy, the power of the laser is often boosted to compensate.\nThe problem can be solved by using pulsed matrix laser light sources.\n\nIt is used only for ultraviolet blood irradiation, that involves drawing blood out through a vein and irradiating it outside of the body. Though promoted as a treatment for cancer, a 1952 review in the \"Journal of the American Medical Association\" and another review by the American Cancer Society in 1970 concluded the treatment was ineffective. Stephen Barrett, writing for Quackwatch, lists ultraviolet blood irradiation therapy as a questionable treatment.\n\n"}
{"id": "30218463", "url": "https://en.wikipedia.org/wiki?curid=30218463", "title": "Bruesewitz v. Wyeth", "text": "Bruesewitz v. Wyeth\n\nBruesewitz v. Wyeth, 562 U.S. 223 (2011), is a United States Supreme Court case that decided whether a section of the National Childhood Vaccine Injury Act of 1986 preempts \"all\" vaccine design defect claims against vaccine manufacturers.\n\nHannah Bruesewitz, the daughter of the main petitioners in the case, received Wyeth's Tri-Immunol DTP vaccine as part of childhood immunizations. The Bruesewitzes claimed that Hannah's seizures and later developmental problems came from the vaccine. They filed suit in the \"Vaccine Court\", a special court within the United States Court of Federal Claims. Their petition was dismissed for failure to prove a link between the vaccine and Hannah's health problems.\n\nThey proceeded to sue in Pennsylvania state court. The case was removed to the local federal court, which held that the claim was preempted by a section of the National Childhood Vaccine Injury Act of 1986. The Third Circuit Court of Appeals affirmed. A petition for a writ of certiorari was granted on March 8, 2010, bringing the case to the Supreme Court.\n\nIn briefings before the Court, both sides argued over the specific language of the statutory provision.\n\nThe case was decided on February 22, 2011. The Court, in a 6-2 opinion by Justice Antonin Scalia, held that the \"plaintiffs design defect claims [were] expressly preempted by the Vaccine Act.\" Thus, the court affirmed laws that vaccine manufacturers are not liable for vaccine-induced injury or death if they are \"accompanied by proper directions and warnings.\"\n\nJustices Sonia Sotomayor and Ruth Bader Ginsburg dissented.\n\n"}
{"id": "56060035", "url": "https://en.wikipedia.org/wiki?curid=56060035", "title": "Cannabis in Vietnam", "text": "Cannabis in Vietnam\n\nCannabis in Vietnam is illegal, but is cultivated within the country. It is known locally as cần sa.\n\nCannabis was probably introduced to Southeast Asia around the 16th century, and used medicinally and in cuisine.\n\nIn 1968 the government of the Republic of Vietnam \"publicly condemned\" the use or trafficking of cannabis, and instructed local chiefs to prevent its cultivation. In 1969, USAID's Office of Public Safety began eradication of cannabis fields, including aerial eradication in the Mekong Delta. The program was popularly resented and also politically unpalatable; in 1971 OPS was advised not to eradicate cannabis in areas controlled by the Hòa Hảo sect, for fear of driving them to join the National Liberation Front.\n\nIn the 1960s, the United States government became concerned with cannabis use by US troops in the Vietnam War. Though alcohol was the drug most commonly used by American troops in the Vietnam War, cannabis was the second-most common. Initially rates of usage among deployed soldiers were comparable to those of their stateside peers, with 29% of troops departing Vietnam in 1967 reporting having ever used marijuana in their lives. A 1976 study however showed that from 1967–1971, the proportion of troops having used marijuana peaked at 34% before stabilizing to 18%, while the number of troops who had used cannabis \"prior\" to deployment stayed around 8%.\n"}
{"id": "1185207", "url": "https://en.wikipedia.org/wiki?curid=1185207", "title": "Central Forensic Science Laboratory", "text": "Central Forensic Science Laboratory\n\nThere are seven central forensic laboratories in India, at Hyderabad, Kolkata, Chandigarh, New Delhi, Guwahati, Bhopal and Pune.\n\nCFSL Hyderabad is a centre of excellence in chemical sciences, CFSL Kolkata (the oldest laboratory in India) in biological sciences and CFSL Chandigarh in physical sciences. The CFSL New Delhi comes under the Central Bureau of Investigation, Delhi, whereas the other laboratories are under the control of the Directorate of Forensic Science Services (DFSS) of the Ministry of Home Affairs. The laboratory in New Delhi is under the control of the Central Bureau of Investigation (CBI) and investigates cases on its behalf. Dr. Rajinder Singh is currently Director of CFSL (CBI) New Delhi. Shri A.K. Ganjoo is the Chief Forensic Scientist, DFSS, New Delhi, Sh. P. Ghosh is Director of CFSL Kolkata, Dr. R.K. Sarin is Director of CFSL Hyderabad and Dr. S.K. Jain is in charge of CFSL Chandigarh. The most important point is it is used for finding forged documents and finger prints also in this lab.\n\n\n"}
{"id": "21365918", "url": "https://en.wikipedia.org/wiki?curid=21365918", "title": "Cirrhosis", "text": "Cirrhosis\n\nCirrhosis is a condition in which the liver does not function properly due to long-term damage. This damage is characterized by the replacement of normal liver tissue by scar tissue. Typically, the disease develops slowly over months or years. Early on, there are often no symptoms. As the disease worsens, a person may become tired, weak, itchy, have swelling in the lower legs, develop yellow skin, bruise easily, have fluid build up in the abdomen, or develop spider-like blood vessels on the skin. The fluid build-up in the abdomen may become spontaneously infected. Other complications include hepatic encephalopathy, bleeding from dilated veins in the esophagus or dilated stomach veins, and liver cancer. Hepatic encephalopathy results in confusion and may lead to unconsciousness.\nCirrhosis is most commonly caused by alcohol, hepatitis B, hepatitis C, and non-alcoholic fatty liver disease. Typically, more than two or three alcoholic drinks per day over a number of years is required for alcoholic cirrhosis to occur. Non-alcoholic fatty liver disease has a number of causes, including being overweight, diabetes, high blood fats, and high blood pressure. A number of less common causes of cirrhosis include autoimmune hepatitis, primary biliary cholangitis, hemochromatosis, certain medications, and gallstones. Diagnosis is based on blood testing, medical imaging, and liver biopsy.\nSome causes of cirrhosis, such as hepatitis B, can be prevented by vaccination. Treatment partly depends on the underlying cause, but the goal is often to prevent worsening and complications. Avoiding alcohol is recommended in all cases of cirrhosis. Hepatitis B and C may be treatable with antiviral medications. Autoimmune hepatitis may be treated with steroid medications. Ursodiol may be useful if the disease is due to blockage of the bile ducts. Other medications may be useful for complications such as abdominal or leg swelling, hepatic encephalopathy, and dilated esophageal veins. In severe cirrhosis, a liver transplant may be an option.\nCirrhosis affected about 2.8 million people and resulted in 1.3 million deaths in 2015. Of these, alcohol caused 348,000, hepatitis C caused 326,000, and hepatitis B caused 371,000. In the United States, more men die of cirrhosis than women. The first known description of the condition is by Hippocrates in the 5th century BCE. The word \"cirrhosis\" is from ; \"kirrhos\" \"yellowish\" and \"-osis\" (-) meaning \"condition\", describing the appearance of a cirrhotic liver.\n\nCirrhosis has many possible manifestations. These signs and symptoms may be either a direct result of the failure of liver cells, or secondary to the resultant portal hypertension. There are also some manifestations whose causes are nonspecific but which may occur in cirrhosis. Likewise, the absence of any signs does not rule out the possibility of cirrhosis. Cirrhosis of the liver is slow and gradual in its development. It is usually well advanced before its symptoms are noticeable enough to cause alarm. Weakness and loss of weight may be early symptoms.\n\nThe following features are as a direct consequence of liver cells not functioning.\n\n\nLiver cirrhosis increases resistance to blood flow and leads to higher pressure in the portal venous system, resulting in portal hypertension. Effects of portal hypertension include:\n\nThere are some changes seen in cirrhosis whose causes are not clearly known. They may also be a sign of other non-liver related causes.\n\nAs the disease progresses, complications may develop. In some people, these may be the first signs of the disease.\n\nLiver cirrhosis has many possible causes; sometimes more than one cause is present in the same person. Globally, 57% of cirrhosis is attributable to either hepatitis B (30%) or hepatitis C (27%). Alcohol consumption is another major cause, accounting for about 20% of the cases.\n\n\nThe liver plays a vital role in synthesis of proteins (for example, albumin, clotting factors and complement), detoxification, and storage (for example, vitamin A). In addition, it participates in the metabolism of lipids and carbohydrates.\n\nCirrhosis is often preceded by hepatitis and fatty liver (steatosis), independent of the cause. If the cause is removed at this stage, the changes are fully reversible.\n\nThe pathological hallmark of cirrhosis is the development of scar tissue that replaces normal parenchyma. This scar tissue blocks the portal flow of blood through the organ, raising the blood pressure and disturbing normal function. Recent research shows the pivotal role of the stellate cell, a cell type that normally stores vitamin A, in the development of cirrhosis. Damage to the hepatic parenchyma (due to inflammation) leads to activation of stellate cells, which increases fibrosis (through production of myofibroblasts) and obstructs hepatic blood flow. In addition, stellate cells secrete TGF-β, which leads to a fibrotic response and proliferation of connective tissue. Furthermore, it secretes TIMP 1 and 2, naturally occurring inhibitors of matrix metalloproteinases, which prevents them from breaking down the fibrotic material in the extracellular matrix.\n\nAs this cascade of processes continues, fibrous tissue bands (septa) separate hepatocyte nodules, which eventually replace the entire liver architecture, leading to decreased blood flow throughout. The spleen becomes congested, which leads to hypersplenism and the spleen's retention of platelets, which are needed for normal blood clotting. Portal hypertension is responsible for the most severe complications of cirrhosis.\n\nThe gold standard for diagnosis of cirrhosis is a liver biopsy, through a percutaneous, transjugular, laparoscopic, or fine-needle approach. A biopsy is not necessary if the clinical, laboratory, and radiologic data suggests cirrhosis. Furthermore, there is a small but significant risk of complications from liver biopsy, and cirrhosis itself predisposes for complications caused by liver biopsy.\n\nThe best predictors of cirrhosis are ascites, platelet count <160,000/mm3, spider angiomata, and a Bonacini cirrhosis discriminant score greater than 7 (as the sum of scores for platelet count, ALT/AST ratio and INR as per table).\n\nThe following findings are typical in cirrhosis:\n\nFibroTest is a biomarker for fibrosis that can be done instead of a biopsy.\n\nOther laboratory studies performed in newly diagnosed cirrhosis may include:\n\nUltrasound is routinely used in the evaluation of cirrhosis. It may show a small and nodular liver in advanced cirrhosis along with increased echogenicity with irregular appearing areas. Other liver findings suggestive of cirrhosis in imaging are an enlarged caudate lobe, widening of the fissures and enlargement of the spleen. An enlarged spleen (splenomegaly), which normally measures less than 11–12 cm in adults, is suggestive of cirrhosis with portal hypertension, in the right clinical context. Ultrasound may also screen for hepatocellular carcinoma, portal hypertension, and Budd-Chiari syndrome (by assessing flow in the hepatic vein). An increased portal vein pulsatility is an indicator of cirrhosis, but may also be caused by an increased right atrial pressure. Portal vein pulsatility can be quantified by pulsatility indices (PI), where an index above a certain cutoff indicates pathology:\n\nCirrhosis is diagnosed with a variety of elastography techniques. Because a cirrhotic liver is generally stiffer than a healthy one, imaging the liver's stiffness can give diagnostic information about the location and severity of cirrhosis. Techniques used include transient elastography, acoustic radiation force impulse imaging, supersonic shear imaging and magnetic resonance elastography. Compared to a biopsy, elastography can sample a much larger area and is painless. It shows a reasonable correlation with the severity of cirrhosis.\n\nOther tests performed in particular circumstances include abdominal CT and liver/bile duct MRI (MRCP).\n\nGastroscopy (endoscopic examination of the esophagus, stomach, and duodenum) is performed in patients with established cirrhosis to exclude the possibility of esophageal varices. If these are found, prophylactic local therapy may be applied (sclerotherapy or banding) and beta blocker treatment may be commenced.\n\nRarely are diseases of the bile ducts, such as primary sclerosing cholangitis, causes of cirrhosis. Imaging of the bile ducts, such as ERCP or MRCP (MRI of biliary tract and pancreas) may aid in the diagnosis.\n\nMacroscopically, the liver is initially enlarged, but with the progression of the disease, it becomes smaller. Its surface is irregular, the consistency is firm, and the color is often yellow (if associated with steatosis). Depending on the size of the nodules, there are three macroscopic types: micronodular, macronodular, and mixed cirrhosis. In the micronodular form (Laennec's cirrhosis or portal cirrhosis), regenerating nodules are under 3 mm. In macronodular cirrhosis (post-necrotic cirrhosis), the nodules are larger than 3 mm. Mixed cirrhosis consists of nodules of different sizes.\n\nHowever, cirrhosis is defined by its pathological features on microscopy: (1) the presence of regenerating nodules of hepatocytes and (2) the presence of fibrosis, or the deposition of connective tissue between these nodules. The pattern of fibrosis seen can depend on the underlying insult that led to cirrhosis. Fibrosis can also proliferate even if the underlying process that caused it has resolved or ceased. The fibrosis in cirrhosis can lead to destruction of other normal tissues in the liver: including the sinusoids, the space of Disse, and other vascular structures, which leads to altered resistance to blood flow in the liver, and portal hypertension.\n\nAs cirrhosis can be caused by many different entities which injure the liver in different ways, cause-specific abnormalities may be seen. For example, in chronic hepatitis B, there is infiltration of the liver parenchyma with lymphocytes. In cardiac cirrhosis there are erythrocytes and a greater amount of fibrosis in the tissue surrounding the hepatic veins. In primary biliary cholangitis, there is fibrosis around the bile duct, the presence of granulomas and pooling of bile. Lastly in alcoholic cirrhosis, there is infiltration of the liver with neutrophils.\n\nThe severity of cirrhosis is commonly classified with the Child-Pugh score. This scoring system uses bilirubin, albumin, INR, the presence and severity of ascites, and encephalopathy to classify patients into class A, B, or C. Class A has a favourable prognosis, while class C is at high risk of death. This system was devised in 1964 by Child and Turcotte, and modified in 1973 by Pugh and others.\n\nMore modern scores, used in the allocation of liver transplants but also in other contexts, are the Model for End-Stage Liver Disease (MELD) score and its pediatric counterpart, the Pediatric End-Stage Liver Disease (PELD) score.\n\nThe hepatic venous pressure gradient, (difference in venous pressure between afferent and efferent blood to the liver) also determines the severity of cirrhosis, although it is hard to measure. A value of 16 mm or more means a greatly increased risk of death.\n\nKey prevention strategies for cirrhosis are population-wide interventions to reduce alcohol intake (through pricing strategies, public health campaigns, and personal counseling), programs to reduce the transmission of viral hepatitis, and screening of relatives of people with hereditary liver diseases.\n\nLittle is known about factors affecting cirrhosis risk and progression. Research has suggested that coffee consumption appears to help protect against cirrhosis.\n\nGenerally, liver damage from cirrhosis cannot be reversed, but treatment can stop or delay further progression and reduce complications. A healthy diet is encouraged, as cirrhosis may be an energy-consuming process. Close follow-up is often necessary. Antibiotics are prescribed for infections, and various medications can help with itching. Laxatives, such as lactulose, decrease the risk of constipation; their role in preventing encephalopathy is limited.\n\nAlcoholic cirrhosis caused by alcohol abuse is treated by abstaining from alcohol. Treatment for hepatitis-related cirrhosis involves medications used to treat the different types of hepatitis, such as interferon for viral hepatitis and corticosteroids for autoimmune hepatitis. Cirrhosis caused by Wilson's disease, in which copper builds up in organs, is treated with chelation therapy (for example, penicillamine) to remove the copper.\n\nRegardless of the underlying cause of cirrhosis, consumption of alcohol and paracetamol, as well as other potentially damaging substances, are discouraged. Vaccination of susceptible patients should be considered for Hepatitis A and Hepatitis B.\nTreating the cause of cirrhosis prevents further damage; for example, giving oral antivirals such as entecavir and tenofovir in patients of cirrhosis due to Hepatitis B prevents progression of cirrhosis. Similarly, control of weight and diabetes prevents deterioration in cirrhosis due to Non-alcoholic steatohepatitis.\n\nIf complications cannot be controlled or when the liver ceases functioning, liver transplantation is necessary. Survival from liver transplantation has been improving over the 1990s, and the five-year survival rate is now around 80%. The survival rate depends largely on the severity of disease and other medical risk factors in the recipient. In the United States, the MELD score is used to prioritize patients for transplantation. Transplantation necessitates the use of immune suppressants (ciclosporin or tacrolimus).\n\nManifestations of decompensation in cirrhosis include gastrointestinal bleeding, hepatic encephalopathy (HE), jaundice or ascites. In patients with previously stable cirrhosis, decompensation may occur due to various causes, such as constipation, infection (of any source), increased alcohol intake, medication, bleeding from esophageal varices or dehydration. It may take the form of any of the complications of cirrhosis listed below.\n\nPeople with decompensated cirrhosis generally require admission to a hospital, with close monitoring of the fluid balance, mental status, and emphasis on adequate nutrition and medical treatment – often with diuretics, antibiotics, laxatives or enemas, thiamine and occasionally steroids, acetylcysteine and pentoxifylline. Administration of saline is avoided, as it would add to the already high total body sodium content that typically occurs in cirrhosis. Life expectancy without liver transplant is low, at most 3 years.\n\nPalliative care is specialized medical care that focuses on providing patients with relief from the symptoms, pain, and stress of a serious illness, such as cirrhosis. The goal of palliative care is to improve quality of life for both the patient and the patient's family and it is appropriate at any stage and for any type of cirrhosis.\n\nEspecially in the later stages, people with cirrhosis experience significant symptoms such as abdominal swelling, itching, leg edema, and chronic abdominal pain which would be amenable for treatment through palliative care. Because the disease is not curable without a transplant, palliative care can also help with discussions regarding the person's wishes concerning health care power of attorney, Do Not Resuscitate decisions and life support, and potentially hospice. Despite proven benefit, people with cirrhosis are rarely referred to palliative care.\n\nSalt restriction is often necessary, as cirrhosis leads to accumulation of salt (sodium retention). Diuretics may be necessary to suppress ascites. Diuretic options for inpatient treatment include aldosterone antagonists (spironolactone) and loop diuretics. Aldosterone antagonists are preferred for people who can take oral medications and are not in need of an urgent volume reduction. Loop diuretics can be added as additional therapy.\n\nIf a rapid reduction of volume is required, paracentesis is the preferred option. This procedure requires the insertion of a plastic tube into the peritoneal cavity. Human albumin solution is usually given to prevent complications from the rapid volume reduction. In addition to being more rapid than diuretics, 4–5 liters of paracentesis is more successful in comparison to diuretic therapy.\n\nFor portal hypertension, nonselective beta blockers such as propranolol or nadolol are commonly used to lower blood pressure over the portal system. In severe complications from portal hypertension, transjugular intrahepatic portosystemic shunting (TIPS) is occasionally indicated to relieve pressure on the portal vein. As this shunting can worsen hepatic encephalopathy, it is reserved for those patients at low risk of encephalopathy. TIPS is generally regarded only as a bridge to liver transplantation or as a palliative measure.\n\nHigh-protein food increases the nitrogen balance, and would theoretically increase hepatic encephalopathy; in the past, this was therefore eliminated as much as possible from the diet. Recent studies show that this assumption was incorrect, and high-protein foods are even \"encouraged\" to maintain adequate nutrition.\n\nThe hepatorenal syndrome is defined as a urine sodium less than 10 mmol/L and a serum creatinine > 1.5 mg/dl (or 24 hour creatinine clearance less than 40 ml/min) after a trial of volume expansion without diuretics.\n\nPeople with ascites due to cirrhosis are at risk of spontaneous bacterial peritonitis.\n\nThis refers to changes in the mucosa of the stomach in people with portal hypertension, and is associated with cirrhosis severity.\n\nCirrhosis can cause immune system dysfunction, leading to infection. Signs and symptoms of infection may be nonspecific and are more difficult to recognize (for example, worsening encephalopathy but no fever).\n\nHepatocellular carcinoma is a primary liver cancer that is more common in people with cirrhosis. People with known cirrhosis are often screened intermittently for early signs of this tumor, and screening has been shown to improve outcomes.\n\nCirrhosis and chronic liver disease were the tenth leading cause of death for men and the twelfth for women in the United States in 2001, killing about 27,000 people each year. The cost of cirrhosis in terms of human suffering, hospital costs, and lost productivity is high. Cirrhosis is more common in men than in women.\n\nEstablished cirrhosis has a 10-year mortality of 34–66%, largely dependent on the cause of the cirrhosis; alcoholic cirrhosis has a worse prognosis than primary biliary cholangitis and cirrhosis due to hepatitis. The risk of death due to all causes is increased twelvefold; if one excludes the direct consequences of the liver disease, there is still a fivefold increased risk of death in all disease categories.\n\nThe word \"cirrhosis\" is a neologism derived from Greek \"kirrhós\" meaning \"yellowish, tawny\" (the orange-yellow colour of the diseased liver) and the suffix \"-osis\", i.e. \"condition\" in medical terminology. While the clinical entity was known before, it was René Laennec who gave it this name (in the same 1819 work in which he also described the stethoscope).\n\n"}
{"id": "30048845", "url": "https://en.wikipedia.org/wiki?curid=30048845", "title": "Community health center", "text": "Community health center\n\nA healthcare center, health center, or community health center is one of a network of clinics staffed by a group of general practitioners and nurses providing healthcare services to people in a certain area. Typical services covered are family practice and dental care, but some clinics have expanded greatly and can include internal medicine, pediatric, women’s care, family planning, pharmacy, optometry, laboratory testing, and more. In countries with universal healthcare, most people use the healthcare centers. In countries without universal healthcare, the clients include the uninsured, underinsured, low-income or those living in areas where little access to primary health care is available.\n\nCommunity Health Centers (CHCs) have existed in Ontario for more than 40 years. The first established CHC in Canada was Mount Carmel Clinic in 1926. Most CHC's consist of an interdisciplinary team of health care providers using electronic health records.\n\nIn Quebec, local community services centres known by their French acronym, CLSC, offer routine health and social services, including consultations with general practitioners with and without an appointment.\n\nIn China there are, as of 2011, 32,812 community health centers and 37,374 township health centers.\n\nThe health center () was the basic community primary healthcare unit of the National Health Service of Portugal, as well as acting as the local public health authority. Usually, each health center covered the area of one of the Portuguese municipalities, but municipalities with over 15 000 habitants could be covered by more than one of these centers. Health centers were staffed with general practitioners, public health physicians, nurses, social workers and administrative personnel.\n\nIn 2008, the more than 300 health centers were aggregated into around 70 health center groups (\"agrupamentos de centros de saúde\") or ACES. Each ACES includes several family and personalized healthcare units, these being now the basic primary health care providers of the Portuguese National Health Service. Besides family health care services, the ACES also include public health, community health and other specialized units, as well as basic medical emergency services.\n\nSome of the ACES were grouped with hospital units into experimental local health units (\"unidades locais de saúde\") or ULS. The ULS are intended to increase the coordination between the primary and the secondary healthcare, through both of these services being provided by the same health unit.\n\nLord Dawson of Penn was commissioned by Lord Addison to produce a report on \"schemes requisite for the systematised provision of such forms of medical and allied services as should... be available for the inhabitants of a given area\". The Interim Report on the Future Provision of Medical and Allied Services was produced in 1920, though no further report ever appeared. The report laid down detailed plans for a network of Primary and Secondary Health Centres, together with detailed architectural drawings of different sorts of centres. By 1939 the term health centre was widely used to refer to new buildings housing local health authority services. The Dawson report was very influential in debates about the National Health Service when it was set up in 1948, but few centres were built because \"it was not practicable for local authorities to establish health centres without the full compliance of general practitioners\" - which was not forthcoming. Far more attention and resources were devoted to hospital services than to primary care. From 1948 to 1974 local authorities were responsible for the building of health centres.\n\nA well known centre was opened at Woodberry Down in October 1952. It had provision for 6 GPs, 2 dentists, a pharmacist and two nurses. It cost about £163,000, which included the cost of a day nursery and child guidance clinic. This was regarded as extravagant and used as an excuse by critics for not building more. Harlow, where 4 centres were built by the new town corporation, was the only community in Britain served exclusively by doctors working from health centres.\n\nThe few centres that were built \"functioned as isolated islands in a sea of General Practitioners generally indifferent to their success\". There were later calls to establish a network of centres to include not only GPs but also dentists and diagnostic facilities. In 1965 there were only 30 health centres in England and Wales, and 3 in Scotland. By 1974 there were 566 in England, 29 in Wales and 59 in Scotland. After the National Health Service Reorganisation Act 1973, responsibility for promoting health centres was transferred to Area Health Authorities and there were renewed calls to establish more Health Centres. It was suggested that these centres could arrange alternative medical care for patients \"when their doctor is off duty, or for emergency calls when he is engaged elsewhere\".\n\nLord Darzi set up a network of Polyclinics in England when he was a minister in 2008. These clinics had some features in common with earlier proposals for health centres, but shared with them considerable resistance from GPs.\n\nA community health center is a not-for-profit, consumer directed healthcare organization that provides access to high quality, affordable, and comprehensive primary and preventive medical, dental, and mental health care. Community health centers have a unique mission of ensuring access for underserved, underinsured and uninsured patients.\n\nIn the U.S., Community Health Centers (CHCs) are neighborhood health centers generally serving Medically Underserved Areas (MUAs) which includes persons who are uninsured, underinsured, low-income or those living in areas where little access to primary health care is available. Largely federally and locally funded, some health clinics are modernized with new equipment and electronic medical records. In 2006, the National Association of Community Health Centers implemented a model for offering free, rapid HIV testing to all patients between the ages of 13 and 64 during routine primary medical and dental care visits.\n\nMedically Underserved Areas/Populations are areas or populations designated by the Health Resources and Services Administration (HRSA) as having: too few primary care providers, high infant mortality, high poverty and/or high elderly population. Health Professional Shortage Areas (HPSAs) are designated by HRSA as having shortages of primary medical care, dental or mental health providers and may be geographic (a county or service area), demographic (low income population) or institutional (comprehensive health center, federally qualified health center or other public facility).\n\n\n"}
{"id": "23356148", "url": "https://en.wikipedia.org/wiki?curid=23356148", "title": "Cutaneous sinus of dental origin", "text": "Cutaneous sinus of dental origin\n\nA cutaneous sinus of dental origin is where a dental infection drains onto the surface of the skin of the face or neck. This is uncommon as usually dental infections drain into the mouth, typically forming a parulis (\"gumboil\").\n\nCutaneous sinuses of dental origin tend to occur under the chin or mandible. Without elimination of the source of the infection, the lesion tends to have a relapsing and remitting course, with healing periods and periods of purulent discharge.\n\nCutaneous sinus tracts may result in fibrosis and scarring which may cause cosmetic concern. Sometimes minor surgery is carried out to remove the residual lesion.\n"}
{"id": "12293417", "url": "https://en.wikipedia.org/wiki?curid=12293417", "title": "Delcath Systems", "text": "Delcath Systems\n\nDelcath Systems, Inc. Delcath Systems, Inc. (NASDAQ: DCTH) is a publicly-traded specialty pharmaceutical and medical device company that develops percutaneous perfusion technologies for the targeted administration of high-dose chemotherapeutic agents to specific organs or regions of the body. Based in Queensbury, New York, the company has an intellectual property portfolio consisting of 28 patents worldwide. Delcath's Percutaneous Hepatic Perfusion (PHP) is currently undergoing Phase II and Phase III trials against tumors in the liver. Delcath has a Cooperative Research and Development Agreement (CRADA) with the National Cancer Institute and has received Fast Track and a Special Protocol Assessment from the Food and Drug Administration for its use of melphalan in treating unresectable liver tumors. PHP, also known as the Delcath System, is tested for the treatment of metastatic melanoma in the liver and for primary liver cancer and metastatic hepatic malignancies from neuroendocrine cancers and adenocarcinomas, as well as patients with melanoma who previously received isolated perfusion. Chemotherapy is usually delivered intravenously, although a number of agents can be administered orally (e.g. specialty drugs, melphalan (trade name Alkeran), busulfan, capecitabine).\n\n\n"}
{"id": "11505496", "url": "https://en.wikipedia.org/wiki?curid=11505496", "title": "Dental Technologists Association", "text": "Dental Technologists Association\n\nThe Dental Technologists Association (DTA) is the professional body representing dental technicians in the United Kingdom.\n\nThe association began life as the Dental Technicians Education and Training Advisory Board (DTETAB) and was established by the General Dental Council (GDC). The remit of DTETAB was to review and make recommendations about the education and training of dental technicians, the suitability of that training and how it would fit with the demands of providing dental services. The other main role for DTETAB was maintaining a Voluntary Register of Dental Technicians, which served as the precursor to the GDC statutory register. The inaugural meeting of the Board took place in January 1986 and was chaired by Margaret Seward (later Dame Margaret Seward). The GDC was only actively involved for the first three years and DTETAB became an independent professional body in 1989 with Colin Lee, a dental technician and laboratory owner from north west England, as chairman. Ray Cox was appointed as registrar and served until his retirement in 1992 when he was succeeded by Sue Adams.\n\nAnticipating a slightly different role post statutory registration, DTETAB became the Dental Technicians Association in 2002. The final name change came in 2006 when the association became the Dental Technologists Association.\n\nThe headquarters of the DTA is based at F13a Kestrel Court, Waterwells Drive, Waterwells Business Park, Gloucester GL2 2AT. The Chief Executive of the association is Sue Adams, who is responsible for the office, administration and support to the council and provides the first point of contact for members.\n\nThe DTA council is the governing body of the association. Delroy Reeves is president, John Stacey is deputy president and former president Tony Griffin is treasurer. Other council members include immediate past president James Green as well as Gregg Clutton, Andy George, Adrian Rollings, Gerrard Starnes and Barry Tivey. \n\nTo advance standards within dental technology for the benefit of the oral healthcare of the United Kingdom by:\n\n\nFellowship of the DTA is awarded for outstanding contribution to dental technology. Recipients should have maintained the dignity of the dental technology profession over a period of time, striven to enhance the recognition, evidence and education of dental technology and have the majority support of the DTA Council. Fellows are entitled to the postnominal FDTA.\n\nThe Technologist is the official DTA journal. Published quarterly, The Technologist aims to raise awareness, educate, engage and showcase information, products and services relevant to members. The Technologist editorial team consists of editor Vikki Harper, editorial assistant Keith Winwood and editorial panel: James Green, Tony Griffin and John Stacey.\n\nArticulate is a bimonthly electronic newsletter and distributed to all DTA members and is also edited by Vikki Harper.\n\n"}
{"id": "28300731", "url": "https://en.wikipedia.org/wiki?curid=28300731", "title": "Dental antibiotic prophylaxis", "text": "Dental antibiotic prophylaxis\n\nDental antibiotic prophylaxis is the administration of antibiotics to a dental patient for prevention of harmful consequences of bacteremia, that may be caused by invasion of the oral flora into an injured gingival or peri-apical vessel during dental treatment. \nThis issue remains a subject under constant revision, with the intention of providing recommendations based on sound scientific evidence. \nCurrently, there are official guidelines for dental antibiotic prophylaxis for the prevention of infective endocarditis and of infection of prosthetic joint. These guidelines are in constant controversy and revisions by various professional committees. In addition, there are various medical conditions for which clinicians recommended antibiotic prophylaxis, although there is no evidence to support this practice. These conditions include renal dialysis shunt, cerebrospinal fluid shunt, vascular graft, immunosuppression secondary to cancer and cancer chemotherapy, systemic lupus erythematosus, and type 1 diabetes mellitus.\n\nIt is of importance to dental patients and practitioners to remain current with regards to the latest recommendations rendered by professional governing bodies such as the American Dental Association (ADA), American Heart Association (AHA) and the American Association of Orthopaedic surgeons (AAOS). Antibiotic prophylaxis is intended to avoid adverse outcomes in certain patients at \"highest risk of postoperative complications.\"Standard antibiotic regimens are routinely prescribed and taken before dental procedures to avoid systemic complications secondary to the transient bacteremia caused by manipulation of the oral tissues. Although the ADA, in collaboration with AHA and AAOS have published guidelines specifying those patients who should receive antibiotic prophylaxis, research continues to further define the role dental treatment may play in causing adverse outcomes in these patients.\nIn the past, bacteremia caused by dental procedures (in most cases due to viridans streptococci, which reside in oral cavity), such as a cleaning or extraction of a tooth was thought to be more clinically significant than it actually was. However, it is important that a dentist or a dental hygienist be told of any heart problems before commencing treatment. Antibiotics are administered to patients with certain heart conditions as a precaution, although this practice has changed in the US, with new American Heart Association guidelines released in 2007, and in the UK as of March 2008 due to new NICE guidelines. Everyday tooth brushing and flossing will similarly cause bacteremia. Although there is little evidence to support antibiotic prophylaxis for dental treatment, the current AHA guidelines are highly accepted by clinicians and patients.\n\n\n"}
{"id": "12665663", "url": "https://en.wikipedia.org/wiki?curid=12665663", "title": "Development of the reproductive system", "text": "Development of the reproductive system\n\nThe development of the reproductive system is a part of prenatal development, and concerns the sex organs. It is a part of the stages of sexual differentiation. Because its location, to a large extent, overlaps the urinary system, the development of them can also be described together as the development of the urinary and reproductive organs.\n\nThe reproductive organs are developed from the intermediate mesoderm. The permanent organs of the adult are preceded by a set of structures which are purely embryonic, and which with the exception of the ducts disappear almost entirely before the end of fetal life. These embryonic structures are the mesonephric ducts (also known as \"Wolffian ducts\") and the paramesonephric ducts, (also known as \"Müllerian ducts\"). The mesonephric duct remains as the duct in males, and the paramesonephric duct as that of the female.\n\nThe mesonephric duct originates from a part of the pronephric duct.\n\nIn the outer part of the intermediate mesoderm, immediately under the ectoderm, in the region from the fifth cervical segment to the third thoracic segment, a series of short evaginations from each segment grows dorsally and extends caudally, fusing successively from before backward to form the pronephric duct. This continues to grow caudally until it opens into the ventral part of the cloaca; beyond the pronephros it is termed the mesonephric duct. Thus, the mesonephric duct remains after the atrophy of the pronephros duct.\n\nIn the male the duct persists, and forms the tube of the epididymis, the vas deferens and the ejaculatory duct, while the seminal vesicle arises during the third month as a lateral diverticulum from its hinder end. A large part of the head end of the mesonephros atrophies and disappears; of the remainder the anterior tubules form the efferent ducts of the testicle; while the posterior tubules are represented by the ductuli aberrantes, and by the paradidymis, which is sometimes found in front of the spermatic cord above the head of the epididymis.\n\nIn the female the mesonephric bodies and ducts atrophy. The nonfunctional remains of the mesonephric tubules are represented by the epoophoron, and the paroöphoron, two small collections of rudimentary blind tubules which are situated in the mesosalpinx.\n\nThe lower part of the mesonephric duct disappears, while the upper part persists as the longitudinal duct of the epoöphoron, called Gartner's duct.\n\nThere are also developments of other tissues from the mesonephric duct that persist, e.g. the development of the suspensory ligament of the ovary.\n\nShortly after the formation of the mesonephric ducts a second pair of ducts is developed; these are the paramesonephric ducts. Each arises on the lateral aspect of the corresponding mesonephric duct as a tubular invagination of the cells lining the abdominal cavity. The orifice of the invagination remains open, and undergoes enlargement and modification to form the abdominal ostium of the fallopian tube. The ducts pass backward lateral to the mesonephric ducts, but toward the posterior end of the embryo they cross to the medial side of these ducts, and thus come to lie side by side between and behind the latter—the four ducts forming what is termed the \"common genital cord\", to distinguish it from the \"genital cords of the germinal epithelium\" seen later in this article. The mesonephric ducts end in an epithelial elevation, the sinus tubercle, on the ventral part of the cloaca between the orifices of the mesonephric ducts. At a later stage the sinus tubercle opens in the middle, connecting the paramesonephric ducts with the cloaca.\n\nIn the male the paramesonephric ducts atrophy, but traces of their anterior ends are represented by the appendix of testis of the male), while their terminal fused portions form the prostatic utricle in the floor of the prostatic urethra. This is due to the production of Anti-Müllerian hormone by the Sertoli cells of the testes.\n\nIn the female the paramesonephric ducts persist and undergo further development. The portions which lie in the \"genital cord\" fuse to form the uterus and vagina. This fusion of the paramesonephric ducts begins in the third month, and the septum formed by their fused medial walls disappears from below upward.\n\nThe parts outside this cord remain separate, and each forms the corresponding Fallopian tube. The ostium of the fallopian tube remains from the anterior extremity of the original tubular invagination from the abdominal cavity.\n\nAbout the fifth month a ring-like constriction marks the position of the cervix of the uterus, and after the sixth month the walls of the uterus begin to thicken. For a time the vagina is represented by a solid rod of epithelial cells. A ring-like outgrowth of this epithelium occurs at the lower end of the uterus and marks the future vaginal fornix. At about the fifth or sixth month the lumen of the vagina is produced by the breaking down of the central cells of the epithelium. The hymen represents the remains of the sinus tubercle .\n\nThe gonads are the precursors of the testes in males and ovaries in females. They initially develop from the mesothelial layer of the peritoneum.\n\nThe ovary is differentiated into a central part, the medulla of ovary, covered by a surface layer, the germinal epithelium. The immature ova originate from cells from the dorsal endoderm of the yolk sac. Once they have reached the gonadal ridge they are called oogonia. Development proceeds and the oogonia become fully surrounded by a layer of connective tissue cells (pre-granulosa cells). In this way, the rudiments of the ovarian follicles are formed. The embryological origin of granulosa cells, on the other hand, remains controversial. Just as in the male, there is a gubernaculum in the female, which pulls it downward, albeit not as much as in males. The gubernaculum later becomes the proper ovarian ligament and the round ligament of the uterus.\n\nThe periphery of the testes are converted into the tunica albuginea. Cords of the central mass run together and form a network which becomes the rete testis, and another network, which develops the seminiferous tubules. Via the rete testis, the seminiferous tubules become connected with outgrowths from the mesonephros, which form the efferent ducts of the testis.\n\nIn short, the descent of the testes consists of the opening of a connection from the testis to its final location at the anterior abdominal wall, followed by the development of the gubernaculum, which subsequently pulls and translocates the testis down into the developing scrotum. Ultimately, the passageway closes behind the testis. A failure in this process can cause indirect inguinal hernia or an infantile hydrocoele.\n\nAfter the separation of the rectum from the dorsal part of the cloaca, the ventral part becomes the primary urogenital sinus. The urogenital sinus, in turn, divides into the superficial definitive urogenital sinus and the deeper anterior vesico-urethral portion.\n\nThe definitive urogenital sinus consists of a caudal cephallic portion and an intermediate narrow channel, the pelvic portion.\n\nThe vesico-urethral portion is the deepest portion, continuous with the allantois. It absorbs the ends of the mesonephric ducts and the associated ends of the renal diverticula, and these give rise to the trigone of urinary bladder and part of the prostatic urethra. The remainder of the vesico-urethral portion forms the body of the bladder and part of the prostatic urethra; its apex is prolonged to the umbilicus as a narrow canal, the urachus, which later is obliterated and becomes the median umbilical ligament of the adult.\n\nThe prostate originally consists of two separate portions, each of which arises as a series of diverticular buds from the epithelial lining of the urogenital sinus and vesico-urethral part of the cloaca, between the third and fourth months. These buds become tubular, and form the glandular substance of the two lobes, which ultimately meet and fuse behind the urethra and also extend on to its ventral aspect. The median lobe of the prostate is formed as an extension of the lateral lobes between the common ejaculatory ducts and the bladder.\n\nSkene's glands in the female urethra are regarded as the homologues of the prostatic glands.\n\nThe bulbourethral glands in the male, and Bartholin's gland in the female, also arise as diverticula from the epithelial lining of the urogenital sinus.\n\nUntil about the ninth week of gestational age the external genitalia of males and females look the same, and follow a common development. This includes the development of a genital tubercle and a membrane dorsally to it, covering the developing urogenital opening, and the development of the labioscrotal fold, also called the urogenital fold, and the labioscrotal swelling.\n\nEven after differentiation can be seen between the sexes, some stages are common, e.g. the disappearing of the membrane. On the other hand, sex-dependent development include further protrusion of the genital tubercle in the male to form the glans of the penis and in the female, the clitoral glans. The urogenital fold evolves into the shaft of the penis in males and the labia minora in females; the labioscrotal swelling evolves into the scrotum in males, and into the labia majora in females.\n\nThere is initially a cloacal membrane, composed of ectoderm and endoderm, reaching from the umbilical cord to the tail, separating the cloaca from the exterior. After the separation of the rectum from the dorsal part of the cloaca, the ventral part of the cloacal membrane becomes the \"urogenital membrane\".\n\nMesoderm extends to the midventral line for some distance behind the umbilical cord, and forms the lower part of the abdominal wall; it ends below in a prominent swelling, the cloacal tubercle, which after the separation of the rectum becomes the genital tubercle. Dorsally to this tubercle the sides aren't really fused. Rather, the urogenital part of the cloacal membrane separates the ingrowing sheets of mesoderm.\n\nThe genital tubercle develops into the primordial phallus, the first rudiment of the penis or clitoris.\n\nThe terminal part of the phallus, representing the future glans becomes solid. The remainder of the phallus, which remains hollow, is converted into a longitudinal groove by the absorption of the urogenital membrane.\n\nThe term genital tubercle, however, still remains, but only refers to the future glans \n\nIn both sexes the phallic portion of the urogenital sinus extends on to the under surface of the cloacal tubercle as far forward as the apex. At the apex the walls of the phallic portion come together and fuse, obliterating the urogenital opening. Instead, a solid plate, the urethral plate, is formed. The remainder of the phallic portion is for a time tubular, and then, by the absorption of the urogenital membrane, it establishes a communication with the exterior. This opening is for a while the primitive urogenital opening, and it extends forward to the corona glandis.\n\nThe following developments occur in both males and females, although a difference in the development between the sexes already can be seen:\n\n\nIn the female, a deep groove forms around the phallus. The sides of it grow dorsalward as the labioscrotal folds, which ultimately form the labia majora in females. The labia minora, in contrast, arise by the continued growth of the lips of the groove on the under surface of the phallus; the remainder of the phallus forms the clitoral glans.\n\nIn the male the pelvic portion of the cloaca undergoes much greater development, pushing before it the phallic portion.\n\nThe labioscrotal folds extend around between the pelvic portion and the anus, and form a scrotal area. During the changes associated with the descent of the testes this scrotal area is drawn out to form the scrotal sacs. The penis is developed from the phallus.\n\nAs in the female, the urogenital membrane undergoes absorption, forming a channel on the under surface of the phallus; this channel extends only as far forward as the corona glandis.\n\nIn the male, by the greater growth of the pelvic portion of the cloaca, a longer urethra is formed, and the primitive opening is carried forward with the phallus, but it still ends at the corona glandis. Later, this opening, which is located on the dorsal side of the penis, closes from behind forward. Meanwhile, the urethral plate of the glans breaks down centrally to form a median groove continuous with the primitive ostium. This groove also closes from behind forward, leaving only a small pipe running in the middle of the penis. Thus, the urogenital opening is shifted forward to the end of the glans.\n\nA.—Diagram of the primitive urogenital organs in the embryo previous to sexual distinction. \n\nB.—Diagram of the female type of sexual organs. \n\nC.—Diagram of the male type of sexual organs. \n\n\n"}
{"id": "24720454", "url": "https://en.wikipedia.org/wiki?curid=24720454", "title": "Doctors of BC", "text": "Doctors of BC\n\nDoctors of BC, formerly known as the British Columbia Medical Association (BCMA), is a professional organization that represents 14,000 physicians, medical residents and medical students in the province of British Columbia. Its goals are to promote a social, economic, and political climate in which members can provide the citizens of BC with the highest standard of health care, while achieving maximum professional satisfaction and fair economic reward. Membership is voluntary.\n\nDoctors of BC represents physicians in negotiations with the BC government. It also advocates for physicians and promotes health and wellness for BC residents. Doctors of BC is a partner with the BC government on the Joint Collaborative Committees (JCCs) which are designed to improve the health care system with the goal of providing quality patient care.\n\nDoctors of BC has a dual-structure governance model that consists of a Representative Assembly to ensure members' views are fully represented and a Board of Directors that has the legal and fiduciary responsibility to manage the affairs of the Association. The annually elected President serves as the primary spokesperson.\n\nDoctors of BC was founded in January 1900 as the BC Medical Association with Dr R.E. McKechnie, a surgeon from Nanaimo, as President. In 2014, the BCMA changed its name to Doctors of BC.\n\nBritish Columbia Medical Journal\n\nThe British Columbia Medical Journal is a peer-reviewed general medical journal covering scientific research, review articles, and updates on contemporary clinical practices written by British Columbian physicians or focused on topics likely to be of interest to them. It is funded through Doctors of BC is editorially independent. It is published 10 times per year.\n\nCanadian Medical Association\n\n"}
{"id": "22074370", "url": "https://en.wikipedia.org/wiki?curid=22074370", "title": "Doreen Norton", "text": "Doreen Norton\n\nDoreen Norton, OBE, FRCN (1 May 1922, Dartford, Kent – 30 December 2007, Worthing, West Sussex) was an English nurse, in the 1950s she used research to show that the best treatment and prevention of bedsores was removing the pressure by turning the patient.\n\nA fellow of the Royal College of Nursing, Norton was regarded as instrumental in changing nursing practices to effectively treat pressure ulcers, a major killer of hospital inpatients.\n"}
{"id": "37830616", "url": "https://en.wikipedia.org/wiki?curid=37830616", "title": "Eco-friendly dentistry", "text": "Eco-friendly dentistry\n\nEco-friendly dentistry (also called environmentally friendly dentistry, green dentistry or sustainable dentistry) is a sustainability and marketing term that refers to the aim of reducing the detrimental impact of dental services on the environment while still being able to adhere to the regulations and standards of the dental industries in their respective countries.\nThere are no official governing agencies that certify an office as meeting eco-friendly standards. Dental offices in the United States of America can be recognised as eco-friendly offices by becoming members of the Eco Dentistry Association. Within England there are audit programmes available from the National Union of Students such as the Green Impact tool. People who want to be involved and discuss sustainable dentistry in a free and open forum are invited to be members at the Centre for Sustainable Healthcare.\n\nThe term eco-friendly dentistry has roots originating from the environmental movement and environmentalism, which, in the Western world, is often perceived as having begun in the 1960s and 1970s. The rise of this movement is often credited to Rachel Carson, conservationist and author of the book Silent Spring. Subsequently, legislation in many countries throughout the world began gaining momentum in the 1970s and continues to the present day.\n\nEco-friendliness also has meaning in another context as a marketing term. It is used by companies to appeal to consumers of goods and services as having a low impact on the environment. Market research has found that an increasing number of consumers purchase goods and services that appeal to the values of environmental philosophy. The dental industry has adopted the concept of eco-friendliness both in a well-meaning, philosophical context and as a marketing term so that patients who subscribe to principles of sustainability can choose to visit these offices. \n\nThe term has been criticised as being used for \"greenwashing\", which is the practice of deceptively promoting a product or service as environmentally friendly. Legislation in countries around the world have Trade Commissions and such to stop companies profiting with baseless claims on their goods and services. Individuals and bodies that work in the dental industry have also subsequently adopted the principles of sustainability and environmentalism and also as an advertisement to patients, clients and consumers. The Eco Dentistry Association is an accreditation organisation in the United States which has proposed outcomes towards becoming more sustainable.\n\nIn 2008, the Eco Dentistry Association was co-founded by Dr. Fred Pockrass and his wife, Ina Pockrass. The Eco Dentistry Association (EDA) provides \"education, standards and connection\" to patients and dentists who practice green dentistry. The EDA aims to help dentists \"come up with safe and reusable alternatives that lower a dentists' operating cost by replacing paper with digital media whenever possible.\" As of February 2011, the EDA has approximately 600 members.. After the inception of the EDA, the dental industry in America saw more dentists and oral surgeons choosing to make their offices environmentally friendly.\n\nIn 2011, The Australian Dental Association implemented a policy of sustainability to provide guidelines to assist in the environmental sustainability of dental offices in Australia. In August 2017 the FDA adopted a sustainability in dentistry policy\n\nThere is a growing amount of scientific information regarding the carbon footprint of the dental industry. These include papers by Duane relating to work carried out in Scotland and more recently England. \n\nRecently, Public Health England published a report on the carbon footprint of NHS England dentistry. The report based on 2014 data provides a number of recommendations for the dental team in England to consider. The report demonstrated the considerable contribution of staff and patient travel to the overall carbon footprint.\n\nTo be environmentally responsible, offices can incorporate the four R's of environmental responsibility. The four R's are: reduce, reuse, recycle & rethink. \n\nHaving a paperless dental office reduces or eliminates the use of paper by going digital. This involves converting patient files, medical histories and other documentation to an electronic system. Going paperless not only makes information sharing easier and accessible but is a great way of keeping personal information secure. This saves money, boosts productivity and saves space as there is no need for any filing cabinets and is a great way of ensuring clinical records are more accurate. \nUsing digital radiography allows to keep all the patients' records in one spot, reduces the amount of radiation exposure and images and clinical photographs can be shared without losing the quality of the image. \n\nIn many countries around the world there are strict mandatory limits on the use of mercury and the levels found in wastewater. Mercury is traditionally used in dental restorations known as amalgam. In October 2013, Australia's Department of the Environment and Energy signed The Minamata Convention in a call for the reduction of amalgam usage by means of nine measures aiming to eventually phase out the use of amalgam. Mercury can be released into the environment when amalgam is placed, finished and polished or removed from a patient mouth and can be either rinsed into sewage systems or disposed of in landfill. By complying with the Australian Dental Association (ADA) Policy 6.11 and the current edition of the International Organization for Standardization ISO11143 Dentistry – Amalgam Separators, reducing the amount of mercury entering the environment by means of installing amalgam separators and traps to collect and separate amalgam waste before it enters the sewage system. Amalgam that is collected from traps is then collected and recycled for reuse. \nWith the phasing out of manual processing of radiographs and switching to digital radiography allows for offices not having to purchase developing liquids and these liquids are harmful to the environment and need to be collected to be disposed of correctly. \n\n<br>• Installing a water meter to monitor water usage.\n<br>• Handwashing sinks with motion-activated taps.\n<br>• Collect the water bills for the last year to benchmark a water usage audit. \n<br>• Place interpretive signs about water conservation in staff rooms, toilets and surgeries. \n<br>• Maintain and repair taps or fittings. \n<br>• Use a non-water-based approach to cleaning where possible. \n<br>• Retro flow controllers in key usage areas \n<br>• Install 4-, 5- or 6-star water efficient appliances where appropriate.\n\nDental practices can recycle paper, cardboard, aluminum and plastics from plastic barriers and other water products contributing to sustainable environmentally friendly practices. Autoclave bags can be separated after opening and the paper and plastic recycled separately.\n\nTo become more eco-friendly or environmentally friendly dental practices can purchase biodegradable products therefore allowing more waste associated with the running of the practice to be recycled. Shredding of paper documents and recycling shredded paper will contribute to sustainable practices.\n\nAccording to the Eco Dentistry Association, eco-friendly dentistry involves taking initiative to build a dental environment by sustainable practices and materials. However, eco-friendly dentistry is not bound by only sustainable manufacturing distribution, waste reduction, pollution prevention, energy conservation, water conservation, patient care and workplace policies. Eco-friendly dentistry is about leadership and innovation. Influencing others around them to part-take in the cause. Eco-friendly dentistry needs to be widespread to make a large impact on the environment and on the dental industry. It is important to create and implement educational programs to enlighten others to follow. Eco-friendly dentistry is about being innovative, discovering new ideas and ways to promote environmentally safe practices.\n\n"}
{"id": "5746828", "url": "https://en.wikipedia.org/wiki?curid=5746828", "title": "Follicle-stimulating hormone receptor", "text": "Follicle-stimulating hormone receptor\n\nThe follicle-stimulating hormone receptor or FSH receptor (FSHR) is a transmembrane receptor that interacts with the follicle-stimulating hormone (FSH) and represents a G protein-coupled receptor (GPCR). Its activation is necessary for the hormonal functioning of FSH. FSHRs are found in the ovary, testis, and uterus.\n\nThe gene for the FSHR is found on chromosome 2 p21 in humans. The gene sequence of the FSHR consists of about 2,080 nucleotides.\n\nThe FSHR consists of 695 amino acids and has a molecular mass of about 76 kDa. Like other GPCRs, the FSH-receptor possesses seven membrane-spanning domains or transmembrane helices.\n\n\nUpon initial binding to the LRR region of FSHR, FSH reshapes its conformation to form a new pocket. FSHR then inserts its sulfotyrosine from the hinge loop into the pockets and activates the 7-helical transmembrane domain. This event leads to a transduction of the signal that activates the G protein that is bound to the receptor internally. With FSH attached, the receptor shifts conformation and, thus, mechanically activates the G protein, which detaches from the receptor and activates the cAMP system.\n\nIt is believed that a receptor molecule exists in a conformational equilibrium between active and inactive states. The binding of FSH to the receptor shifts the equilibrium between active and inactive receptors. FSH and FSH-agonists shift the equilibrium in favor of active states; FSH antagonists shift the equilibrium in favor of inactive states. For a cell to respond to FSH, only a small percentage (~1%) of receptor sites need to be activated.\n\nCyclic AMP-dependent protein kinases (protein kinase A) are activated by the signal chain coming from the G protein (that was activated by the FSH-receptor) via adenylate cyclase and cyclic AMP (cAMP). \n<br>These protein kinases are present as tetramers with two regulatory units and two catalytic units. Upon binding of cAMP to the regulatory units, the catalytic units are released and initiate the phosphorylation of proteins, leading to the physiologic action. The cyclic AMP-regulatory dimers are degraded by phosphodiesterase and release 5’AMP. DNA in the cell nucleus binds to phosphorylated proteins through the cyclic AMP response element (CRE), which results in the activation of genes.\n\nThe signal is amplified by the involvement of cAMP and the resulting phosphorylation. The process is modified by prostaglandins. Other cellular regulators are participate are the intracellular calcium concentration modified by phospholipase, nitric acid, and other growth factors.\n\nThe FSH receptor can also activate the extracellular signal-regulated kinases (ERK). In a \"feedback mechanism\", these activated kinases phosphorylate the receptor. The longer the receptor remains active, the more kinases are activated, the more receptors are phosphorylated.\n\nIn the ovary, the FSH receptor is necessary for follicular development and expressed on the granulosa cells.\n\nIn the male, the FSH receptor has been identified on the Sertoli cells that are critical for spermatogenesis.\n\nThe FSHR is expressed during the luteal phase in the secretory endometrium of the uterus.\n\nFSH receptor is selectively expressed on the surface of the blood vessels of a wide range of carcinogenic tumors.\n\nUpregulation refers to the increase in the number of receptor sites on the membrane. Estrogen upregulates FSH receptor sites. In turn, FSH stimulates granulosa cells to produce estrogens. This synergistic activity of estrogen and FSH allows for follicle growth and development in the ovary.\n\nThe FSHR become desensitized when exposed to FSH for some time. A key reaction of this downregulation is the phosphorylation of the intracellular (or cytoplasmic) receptor domain by protein kinases. This process uncouples Gs protein from the FSHR. Another way to desensitize is to uncouple the regulatory and catalytic units of the cAMP system.\n\nDownregulation refers to the decrease in the number of receptor sites. This can be accomplished by metabolizing bound FSHR sites. The bound FSH-receptor complex is brought by lateral migration to a \"coated pit,\" where such units are concentrated and then stabilized by a framework of clathrins. A pinched-off coated pit is internalized and degraded by lysosomes. Proteins may be metabolized or the receptor can be recycled. Use of long-acting agonists will downregulate the receptor population.\n\nAntibodies to FSHR can interfere with FSHR activity.\n\nSome patients with ovarian hyperstimulation syndrome may have mutations in the gene for FSHR, making them more sensitive to gonadotropin stimulation.\n\nWomen with 46 XX gonadal dysgenesis experience primary amenorrhea with hypergonadotropic hypogonadism. There are forms of 46 xx gonadal dysgenesis wherein abnormalities in the FSH-receptor have been reported and are thought to be the cause of the hypogonadism.\n\nPolymorphism may affect FSH receptor populations and lead to poorer responses in infertile women receiving FSH medication for IVF.\n\nAlfred G. Gilman and Martin Rodbell received the 1994 Nobel Prize in Medicine and Physiology for the discovery of the G Protein System.\n\n\n\n"}
{"id": "7200208", "url": "https://en.wikipedia.org/wiki?curid=7200208", "title": "Football Act 1424", "text": "Football Act 1424\n\nThe Football Act 1424 was passed by the Parliament of Scotland in the reign of James I. It became law on 26 May 1424, one of a set of statutes passed that day; it is recorded as \"James I. 1424 (May 26) c.18\" in the Record Edition of the statutes, and \"James I. Parl. 1-1424 c.17\" in the Duodecimo Edition. The title of the Act was \"Of playing at the fut ball\".\n\nThe Act stated that \"it is statut and the king forbiddis that na man play at the fut ball under the payne of iiij d\" - in other words, playing football was forbidden by the King, and punishable by a fine of four pence.\n\nThe Act remained in force for several centuries, although somewhat unsurprisingly, it fell into disuse. It was finally repealed by the Statute Law Revision (Scotland) Act 1906.\n\nThree further 15th century Acts (in 1457, 1470 and 1490) explicitly prohibit both football and golf (see Golf in Scotland) during \"wappenschaws\" () for archery practice.\n\n\n"}
{"id": "53050024", "url": "https://en.wikipedia.org/wiki?curid=53050024", "title": "Granny dumping", "text": "Granny dumping\n\nGranny dumping (informal) is a term that was introduced in the early 1980s by professionals in the medical and social work fields. Granny dumping is defined by the \"Oxford English Dictionary\" as \"the abandonment of an elderly person in a public place such as a hospital or nursing home, especially by a relative\". It may be carried out by family members who are unable or unwilling to continue providing care due to financial problems, burnout, lack of resources (such as home health or assisted living options), or stress.\n\nThe phenomenon is not new. A practice, known as \"ubasute\", had allegedly existed in Japan centuries ago when senile elders were brought to mountaintops by poor citizens who were unable to look after them. The widespread economic and demographic problems facing Japan have seen it on the rise with relatives dropping off seniors at hospitals or charities. 70,000 (both male and female equally) elderly Americans were estimated to have been abandoned in 1992 in a report issued by the American College of Emergency Physicians. In this same study, ACEP received informal surveys from 169 hospital Emergency Departments and report an average of 8 \"granny dumping\" abandonments per week. According to the New York Times, 1 in 5 people are now caring for an elderly parent and people are spending more time than ever caring for an elderly parent than their own children. Social workers have said that this may be the result of millions of people who are near the breaking point of looking after their elderly parents who are in poor health.\n\nIn the US, granny dumping is more likely to happen in states such as Florida, Texas and California where there are large populations of retirement communities. Congress has attempted to step in by mandating to emergency departments requiring them to see all patients. However, Medicaid is covering less and less of medical bills through reimbursement (in 1989 it was 78% but that number is decreasing) and reduced eligibility. In some cases, the hospitals may not want to take the risk of having a patient who cannot pay so they will attempt to transfer their care to another hospital. According to the Consolidated Omnibus Budget Reconciliation Act of 1985 set into place by Ronald Reagan, a hospital can transfer at the patient's request or providers must sign a document providing why they believe a patient's care should be better served at another facility. With 40% of revenue coming from Medicaid and Medicare a hospital must earn 8 cents per dollar to compensate for the loss of 7 cents per Medicaid/Medicare patients. Hospitals had to pay an additional 2 billion dollars to private payers to cover costs for Medicare/Medicaid patients in 1989.\n\nIncidents of granny dumping can happen before long weekends and may peak before Christmas when families head off on holidays. Caregivers in both Australia and New Zealand report that old people without acute medical problems are dropped off at hospitals. As a result, hospitals and care facilities have to carry an extra burden on their limited resources.\n"}
{"id": "55556540", "url": "https://en.wikipedia.org/wiki?curid=55556540", "title": "Hamish Munro", "text": "Hamish Munro\n\nHamish Nisbet Munro FRSE (1915-1994) was a Scottish biochemist and expert in protein metabolism at Glasgow University and Massachusetts Institute of Technology. He served as President of the American Institute of Nutrition in 1978, and was first Director of the USDA Human Nutrition Research Centre on Ageing at Tufts University. \nMunro was born in Edinburgh on 3 July 1915, the son of a bank clerk. He was educated first at George Watson's College. However, in 1923 his father was transferred to manage the Bank of Scotland branch in the small village of Bonar Bridge in Sutherland, close to his family home in Dornoch. Hamish was then educated in the one-room village school. In 1932 he was Dux (top pupil) of the county of Sutherland. Wishing to study medicine but needing qualifications which could not be provided in his village school he crammed physics and chemistry alone and passed the university entrance examination, but as medicine was oversubscribed, he first studied for a Bachelor of Science degree at Glasgow University. During this course he spent the summers with Edward Provan Cathcart and David Cuthbertson and acquired an interest in metabolism and nutrition. He graduated BSc in 1936 with First Class Honours, publishing that year the first of over 700 scientific papers. He returned to the medical course and qualified MB ChB in September 1939.\n\nHe began as a medical resident at the Victoria Infirmary in Glasgow, and was promoted to Clinical Tutor the following year, a role he held for the duration of the Second World War. In 1945 he left clinical work for a Lecturership in Physiology at Glasgow University, transferring as Senior Lecturer to the newly separate Department of Biochemistry in 1947. The head of the department was James Norman Davidson, but Munro established his own reputation in studies of protein metabolism and nucleic acids, receiving a doctorate (DSc) in 1956, and being appointed full Professor in 1964.[2] In 1956 he was elected a Fellow of the Royal Society of Edinburgh, proposed by Davidson, Cuthbertson, Robert Campbell Garry, and Hugh Garven.. During this time, he completed the first two volumes of his major work, Mammalian Protein Metabolism, with J.D. Allison.\n\nIn 1966 he moved to Boston in America as Professor of Physiological Chemistry at MIT. In this laboratory he continued his studies of protein metabolism, including his major interest in ferritin, but also important studies in RNA polymerases, and methylated histidine as a measure of muscle breakdown. In 1972 he was offered, but declined, directorship of the Dunn Nutrition Centre in Cambridge.[4] From 1977 he joined a US Department of Agriculture task force which led to the construction of the USDA Human Nutrition Research Centre on Ageing at Tufts University, a 15 storey building in downtown Boston. Munro was its first Director from 1982, also serving as Professor of Medicine at Tufts.\n\nHe received many awards, including the Osborne-Mendel Award (1969) and the Borden Award (1978) from the American Institute of Nutrition, the Bristol-Myers Award for Distinguished Achievements in Nutrition (1981), the Rank Prize for Nutrition (1982) and the Corson Medal, Franklin Institute (1987). In 1983 he compiled publications on ferritin into an MD thesis for which he was awarded the Bellahouston Medal by Glasgow University. He was a member of the US National Academy of Sciences from 1974, and had an honorary doctorate from the University of Nancy, France.\n\nAlthough he continued to work until the age of 75, and added to his scientific publications even in his last years, he was increasingly disabled by Parkinson’s disease and died of complications of this in Glasgow on 28 October 1994.\n\nDuring a study of scurvy he met a medical colleague, Dr Edith Little, whom he married in 1946. They had one daughter and three sons, brought up in Glasgow and Boston. After the children had left home, Edith returned to the UK where she pursued her own career as a psychiatrist, but the family remained integrated.\n\n"}
{"id": "21212648", "url": "https://en.wikipedia.org/wiki?curid=21212648", "title": "Health Insurance Institute of Slovenia", "text": "Health Insurance Institute of Slovenia\n\nThe Health Insurance Institute of Slovenia was founded on March 1, 1992, according to the Law on healthcare and health insurance, after declaring independence from Yugoslavia. The Institute is called: Institute for Medical Insurance (Zavod za zdravsteno zavarovanje). It conducts its business as a public institute, bound by statute to provide compulsory health insurance. The Institute's principal task is to provide effective collection (mobilisation) and distribution (allocation) of public funds, in order to ensure the insured persons quality rights arising from the said funds.\n\nHealth insurance is compulsory and voluntary. Compulsory health insurance is provided by the Health Insurance Institute of Slovenia, voluntary health insurance is carried out by other health insurance companies.\n\nThe Institute comprises 10 regional units and 45 branch offices distributed around the territory of Slovenia. The Information Centre and the Directorate complete the Institute's structure. At the end of 2005, the Institute staff numbered regular 929 employees.\nThe Institute is governed by an Assembly, whose members are the (elected) representatives of employers (including the representatives of the Government of the Republic of Slovenia) and employees. The executive body of the Assembly is the Institute Board of Directors.\n\nThe Slovene health insurance card system was introduced, at the national scale, in the year 1999. The system provides the insured persons with a smart card. The card carries the identification number (HIIS number), the card issue number, the name and surname of the card holder, gender, and date of birth. Data links are established between the health care service providers and health insurance providers (the Health Insurance Institute and the two voluntary health insurance providers). Medical records are accessed by a health care professional using a double card reader. The professional's card controls their level of access.\n\n"}
{"id": "35325706", "url": "https://en.wikipedia.org/wiki?curid=35325706", "title": "Health in Armenia", "text": "Health in Armenia\n\nAfter significant decline in earlier decades crude birth rates in Armenia remained at 13.0–14.2 per 1000 people nearly constant in the years 1998–2015. In the same period the crude death rate went from 8.6 to 9.3 per 1000 people. Note that crude rates are not age-adjusted. Life expectancy at birth at 74.8 years was the 4th highest among the post-USSR countries in 2014.\n\nCertified by World Health Organization Armenia was the first in the European region and as of October 2017 is one of 10 countries worldwide (7 of which are islands) which proved to have eliminated mother-to-child HIV transmission.\n\nAccording to WHO data infant mortality rate nearly halved from 2002 to 2015. \n\nUndernourishment at 6.3% in 2014 of population remained nearly unchanged since 2007.\n\nObesity rate is 19.5% in Armenia in 2017, which is lower than in all regional countries and nearly all European countries.\n\nTobacco policy in Armenia is as of February 2018 still very permissive with almost no enforcement of any smoke-restricting laws.\n\nAt the time of independence in 1991 no traces of pre-Soviet healthcare traditions were discernible. The soviet healthcare system was highly centralized. The entire population was guaranteed free medical assistance, regardless of social status, and had access to a comprehensive range of secondary and tertiary care. After independence Armenia was not in a position to continue to fund it. Following the reform programme all the hospitals and polyclinics, rural health units (including village health centers), and health posts from the previous system continue to function. Hospitals which were formerly accountable to the local administration and ultimately to the Ministry of Health are now autonomous and increasingly responsible for their own budgets and management. \n\nA Basic Benefits Package was established in 1999. This provided provides free specific health-care services, including medicines, to vulnerable segments of the population, including children, the elderly and disabled, impoverished people and injured military personnel. Since 2006, primary health care services have been free of charge. \n\nIn 2009 about half the health expenditure in the country came directly out of the pockets of patients at the time of treatment. More than 50% of the national health budget was spent on hospitals. At the local community level the system was weak and in rural areas often non-existent.\n\nVast improvements of health services in Armenia occurred in the twenty-first century, principally easier accessibility to health-care services and an Open Enrollment program which allows Armenians to freely choose their healthcare service provider.\n\nHealth expenditure at 4.5% of GDP in 2014 was the 3rd lowest in post-USSR countries and below the average of the region of Europe and Central Asia, same as in years 2006–2013. Health expenditures as percentage of government spendings were 4th lowest for the same group in 2008–2014, but beat peers in South Caucasus. Health expenditures in per capita terms (at PPP at constant 2005 USD) were nearly permanently 5th lowest in the above group in years 1999–2014. Out-of-pocket health expenditure were 4th highest in the same group in years 2003–2006 and 2010–2014. In 2014 4.3% of health expenditures came from sources outside of Armenia.\n"}
{"id": "48139493", "url": "https://en.wikipedia.org/wiki?curid=48139493", "title": "Indian states and union territories ranked by availability of toilets", "text": "Indian states and union territories ranked by availability of toilets\n\nThis is a list of Indian states and territories ranked by the availability of toilet facilities per household. Figures are from the 2011 Census of India. The list does not include the newly formed states of Telangana and the residual state of Andhra Pradesh in 2014.\n"}
{"id": "2457830", "url": "https://en.wikipedia.org/wiki?curid=2457830", "title": "Judy McBurney", "text": "Judy McBurney\n\nJudith McBurney (19 May 1948 – 1 December 2018) was an Australian actress, voice-over artist and model, best known for several TV series. She was best known as Tania Livingston in \"The Young Doctors\" and Pixie Mason in \" Prisoner\" (also known as \"Prisoner: Cell Block H\").\n\nBefore acting she started a successful career as a model employed by June Dally-Watkins. One of her first acting roles was in late 1969, in a supporting role in Peter Weir's short movie \"Michael\", one of three short movies released under the title \"Three to Go\". It followed by small parts in ABC TV-plays and guest roles in other TV series. Her first leading role was in 1972 as Ella Belairs in ABC's adaption of \"The Cousin from Fiji,\" based on a novel by Norman Lindsay. Another early and memorable role was as Aldith in \"Seven Little Australians\".\n\nIn late 1973, McBurney was cast in the role of key new character Marilyn McDonald in \"Number 96\" but before any of her scenes had gone to air and with about 30 scenes in the can she had to withdraw from the role due to illness. This left her replacement, Frances Hargreaves, to reshoot all of McBurney's scenes.\n\nMcBurney then went into the ongoing role of plain-Jane secretary Jane Fowler in \"The Box\" in late 1975, and later played the brief role of Jodi in \"Number 96\". Subsequently, she became immensely popular playing the part of nurse Tania Livingston in \"The Young Doctors\". She played the role from 1977 until the series ended in 1982. She followed this with another popular character, that of breezy romantic Sandra \"Pixie\" Mason in Prisoner. She played Pixie on a recurring basis from 1983–85. In later years she appeared in a few episodes of \"Always Greener\".\n\nMcBurney was also famous for her voice-over work and also starring in commercials, in particular a very cheeky, controversial ad from the 1980s for Palmolive Gold, in which she features in a bed with fellow \"The Young Doctors\" actor Peter Bensley. McBurney leaves a cake of soap under his pillow, which led to the famous jingle and saying \"Don't wait to be told\".\n\nMcBurney retired from acting in 2002 to become an educated healer. She also went on to teach modelling and acting for young people. McBurney died of cancer on 1 December 2018.\n\n"}
{"id": "43303883", "url": "https://en.wikipedia.org/wiki?curid=43303883", "title": "King v. Burwell", "text": "King v. Burwell\n\nKing v. Burwell, , was a 6-3 decision by the Supreme Court of the United States interpreting provisions of the Patient Protection and Affordable Care Act (ACA). The Court's decision upheld, as consistent with the statute, the outlay of premium tax credits to qualifying persons in all states, both those with exchanges established directly by a state, and those otherwise established by the Department of Health and Human Services.\n\nThe petitioners had argued that the plain language of the statute provided eligibility for tax credits only to those persons in states with state-operated exchanges. The Court rejected this interpretation. Rather, the Court found the disputed clause to be ambiguous, and that it ought to be interpreted in a manner \"that is compatible with the rest of the law.\" The majority opinion stated: \"Congress made the guaranteed issue and community rating requirements applicable in every State in the Nation. But those requirements only work when combined with the coverage requirement and tax credits. So it stands to reason that Congress meant for those provisions to apply in every State as well.\"\n\nKing v. Burwell, Halbig v. Burwell, Pruitt v. Burwell, and Indiana v. IRS were federal lawsuits challenging U.S. Treasury regulation, 26 C.F.R. § 1.36B-2(a)(1), issued under the Patient Protection and Affordable Care Act (ACA). The challengers argued that the ACA allows for certain subsidies only on state-established exchanges, and that the regulation as implemented by the Internal Revenue Service (IRS), providing for subsidies on state-run exchanges as well as federal exchanges, exceeded the authority Congress granted to it. The Competitive Enterprise Institute coordinated and funded the \"King\" and \"Halbig\" lawsuits.\n\nTimothy Jost, a health law professor at the Washington and Lee University School of Law, wrote that if the challenges were successful, approximately 5 million Americans who obtained coverage through federal exchanges could have lost their tax credits and, in all likelihood, their health insurance coverage. According to Jost, the individual and employer mandates might also have \"disappear[ed] or [been] severely undermined\" in states with federal exchanges. Insurers, however, would still have been required to cover all applicants regardless of pre-existing conditions, which could have destabilized the individual insurance markets in states with federal exchanges and could have led to rapid rises in premiums and the possible collapse of one or more of those markets.\n\nThe Urban Institute estimated that a decision in favor of King would have resulted in 8.2 million more uninsured people in 34 states. Government figures released June 2, 2015 (for the period ending March 31, 2015) show that approximately 6.4 million Americans were enrolled in a federal exchange and received a supplement at that time, and thus, presumably would have lost the subsidy had the court found for the plaintiff.\n\nAs of 2015, sixteen states and the District of Columbia had set up their own exchanges. If the subsidies and (in effect) the mandates had been struck down in the other 34 states, many thought that the economic foundation of the ACA would have been undermined, putting the entirety of the legislation at risk. Supporters of the plaintiffs, as well as some politicians, argued that the effects of striking down the subsidies would have been mitigated by government action (including the possibility of states setting up their own exchanges in response to a ruling in favor of the plaintiffs).\n\nThe district court in \"King\", and the district court in \"Halbig\" both ruled against the plaintiffs. However, on July 22, 2014, the Fourth Circuit Court of Appeals in \"King\" and the D.C. Court of Appeals in \"Halbig\" came to opposite conclusions, creating a circuit split. When the D.C. appeals court decided to rehear the case en banc, however, the court vacated its initial ruling, removing the split. On November 7, 2014, the Supreme Court granted certiorari in the \"King\" case. Oral arguments were heard on March 4, 2015, and a decision was handed down on June 25, 2015, with a win for the Obama administration preserving subsidies in states that have not established their own exchange.\n\nThe ACA legislation includes the language \"enrolled in through an Exchange established by the State under 1311\". As implemented by the IRS, ACA regulations use a more broad definition encompassing both the state exchanges and the federal exchanges set up under section 1321. The legislation includes the phrase \"established by the State under 1311\" in nine different locations.\n\nInternal Revenue Code section 36B, enacted as part of the ACA, includes the following provision:\n\nIn the case of an applicable taxpayer, there shall be allowed as a credit against the tax imposed by this subtitle for any taxable year an amount equal to the premium assistance credit amount of the taxpayer for the taxable year.\n\n(2) (a) the monthly premiums for such month for 1 or more qualified health plans offered in the individual market within a State which cover the taxpayer, the taxpayer's spouse, or any dependent (as defined in section 152) of the taxpayer and which were enrolled in through an Exchange established by the State under 1311 [1] of the Patient Protection and Affordable Care Act, [...]\nThe IRS regulation reads:\n\n(a) In general. An applicable taxpayer (within the meaning of paragraph (b) of this section) is allowed a premium assistance amount only for any month that one or more members of the applicable taxpayer's family (the applicable taxpayer or the applicable taxpayer's spouse or dependent)—\n\n(1) Is enrolled in one or more qualified health plans through an Exchange [ . . . ]\nThe IRS defined the term \"Exchange\" as:\n\n[ . . . ] a governmental agency or non-profit entity that meets the applicable standards of this part [part 155 of title 45 of the Code of Federal Regulations] and makes QHPs [qualified health plans] available to qualified individuals and/or qualified employers. Unless otherwise identified, this term includes an Exchange serving the individual market for qualified individuals and a SHOP [Small Business Health Options Program] serving the small group market for qualified employers, regardless of whether the Exchange is established and operated by a State (including a regional Exchange or subsidiary Exchange) or by HHS [the U.S. Department of Health and Human Services].\nIn \"Chevron U.S.A., Inc. v. Natural Resources Defense Council, Inc.\" the Supreme Court ruled that the U.S Congress may delegate regulatory authority to an agency, and that the agency's regulations carry the weight of the law, if the regulations pass the two-part \"Chevron test\".\n\n(1) \"First, always, is the question whether Congress has directly spoken to the precise question at issue. If the intent of Congress is clear, that is the end of the matter; for the court as well as the agency must give effect to the unambiguously expressed intent of Congress.\"\n\n\"If the Court determines Congress has not directly addressed the precise question at issue, the court does not simply impose its own construction of the statute . . . Rather,\n\n(2) [I]f the statute is silent or ambiguous with respect to the specific question, the issue for the court is whether the agency's answer is based on a permissible construction of the statute.\" \"Chevron U.S.A. v. NRDC\", 467 U.S. 837, 842–843 (1984).\n\nThe Fourth Circuit's opinion had ruled that the statutory language was ambiguous and applied the Chevron doctrine, meaning that the IRS's regulatory ruling was given deference. However, in the Supreme Court's majority ruling this test is said to have been failed because \"had Congress wished to assign that question to an agency, it surely would have done so expressly.\" Had the Court applied the Chevron doctrine and deferred to the IRS, a subsequent IRS ruling could have overturned the subsidies being available on the federal exchange.\n\nPlaintiffs argue that Congress intentionally restricted payment of subsidies to state exchanges to induce states into setting up exchanges so their citizens could receive subsidies.\n\nThe government argues that the law intends for federal exchanges to be treated identically to state exchanges (and therefore qualifying individuals are entitled to subsidies whether or not their state has set up an exchange), or, in the alternative, if the law were ambiguous, that the regulation at issue was a permissible interpretation of the law.\n\nLyle Denniston wrote that the parties' positions offer differing views on how to interpret legislation:\n\nThe challengers take the “literal interpretation” approach, although they also have policy reasons for reading the ACA as they do. The Obama administration takes the “broader purpose” approach, contending that Congress would not have set up the insurance program on a basis that is as limited as the challengers contend.\nIn a 2009 paper published in \"The Journal of Law, Medicine & Ethics\", Timothy Jost argued that one way to avoid a commandeering issue with the ACA would be \"by offering tax subsidies for insurance only in states that complied with federal requirements.\" Jost later published an op-ed in the \"Washington Post\" arguing that allowing subsidies for Federal exchanges is \"the only way of reading the statute that makes sense.\" In an article on Forbes, Jost pointed out that his original law journal article proposed \"several alternatives through which Congress could encourage the states to establish exchanges, one of which was to limit the availability of tax credits to states that operate exchanges. The first alternative [proposed] was that Congress ask the states to establish exchanges, but create a federal fallback exchange in the event they failed to do so.\"\n\nOn January 18, 2012, Jonathan Gruber, a Massachusetts Institute of Technology economist who was a consultant on the ACA, said, \"What's important to remember politically about this is if you're a state and you don't set up an exchange, that means your citizens don’t get their tax credits.\" On January 10, 2012, Gruber said, \"... if your governor doesn’t set up an exchange, you're losing hundreds of millions of dollars of tax credits to be delivered to your citizens.\"\n\nGruber has characterized his earlier statements as \"a mistake\", and said that he \"might have been thinking that if the federal backstop wasn't ready by 2014, and states hadn't set up their own exchange, there was a risk that citizens couldn't get the tax credits right away.\" Sarah Kliff of \"Vox\" cited as evidence of Gruber's comments being mistaken the fact that despite speaking \"regularly to dozens of reporters during this period\", he \"never mentioned this idea to any of them\", and that his models always assumed that subsidies would be available on both state and federal exchanges. In a December 2014 Congressional hearing, Gruber characterized his comments as \"reflecting uncertainty about the federal exchange\". The \"King\" plaintiffs, in their briefs filed in December 2014, referred to Gruber's comments as an indication of Congressional intent supporting their position.\n\nThough the challengers in the Supreme Court case have argued that then Nebraska Sen. Ben Nelson, who by insisting that states take the lead in establishing the exchanges, meant that Congress had intended that tax credits go only to qualified recipients in states that had established their own insurance exchanges, Nelson has denied this interpretation in an amicus brief filed with the court, January 28, 2015. In a letter to Sen. Bob Casey who sought Nelson’s view, the former senator wrote, \"I always believed that tax credits should be available in all 50 states regardless of who built the exchange, and the final law also reflects that belief as well\".\n\nOthers have argued that the issue is due to a drafting error during the legislative process. Yale Law School Professor Abbe Gluck said that the unusual maneuver of having the ACA become law through reconciliation required a preliminary version of the bill to become law without the \"usual legislative clean-up process\".\n\nBoth the Senate Finance Committee and the Health Committee drafted their own versions of the ACA. The Finance Committee bill assumed that if a state refused to participate, HHS would contract with private companies to run “state exchanges”. The Health Committee bill provided for federally run fallback exchanges. When the two bills were combined the Finance Committee bill was used as the primary template but the Health Committee bill’s language was used when addressing possible holdout states. The Senate passed the bill and Congressional staff expected to clean up the language at the Conference Committee.\n\nWhen Health Committee Chairman Ted Kennedy died, however, he was replaced with Republican Scott Brown, who had won a special election by promising to filibuster the ACA. Without sixty Democrats, Majority Leader Harry Reid was now deprived of his ability to invoke cloture. House Speaker Nancy Pelosi was then forced to pass the draft Senate version. Later revisions through the reconciliation process were limited to budget related provisions by the Byrd Rule.\n\nPlaintiffs argue that they have standing because, without the subsidies, they would be exempt from the individual mandate because the cost of the cheapest insurance plan exceeded 8% of their income, but, with the subsidies, the subsidized cost was low enough to require plaintiffs to purchase insurance or pay a penalty.\n\nIn February 2015, \"The Wall Street Journal\" and \"Mother Jones\" investigated the four plaintiffs. Two of the plaintiffs were Vietnam War veterans, who would be eligible for free care. Another plaintiff provided the court with a motel address, which was used to calculate the cost of insurance, as well as the amount of subsidies; a different address might result in different amounts that may cause her not to have standing. The fourth plaintiff stated that she made $10,000 per year as a substitute teacher, an income low enough to be exempt from the individual mandate, although the Competitive Enterprise Institute suggested that she might have additional income from other work. The investigations also suggested that some plaintiffs may lack standing because the cheapest available subsidized insurance was over 8% of their income, making them exempt from the individual mandate.\n\nThe Fourth Circuit court unanimously upheld the regulation, saying that the wording in the statute was ambiguous, and that the IRS wording was a reasonable interpretation of the statute:\n\nThe plaintiffs-appellants bring this suit challenging the validity of an Internal Revenue Service (“IRS”) final rule implementing the premium tax credit provision of the Patient Protection and Affordable Care Act (the “ACA” or “Act”). The final rule interprets the ACA as authorizing the IRS to grant tax credits to individuals who purchase health insurance on both state-run insurance “Exchanges” and federally facilitated “Exchanges” created and operated by the Department of Health and Human Services (“HHS”). The plaintiffs contend that the IRS’s interpretation is contrary to the language of the statute, which, they assert, authorizes tax credits only for individuals who purchase insurance on state-run Exchanges. For reasons explained below, we find that the applicable statutory language is ambiguous and subject to multiple interpretations. Applying deference to the IRS’s determination, however, we uphold the rule as a permissible exercise of the agency’s discretion. [...]Rejecting all of the plaintiffs' arguments as to why Chevron deference is inappropriate in this case, for the reasons explained above we are satisfied that the IRS Rule is a permissible construction of the statutory language. We must therefore apply Chevron deference and uphold the IRS Rule...\n\nAlthough the court ruled unanimously for the government, the opinion stated that it \"cannot ignore the common-sense appeal of the plaintiffs’ argument; a literal reading of the statute undoubtedly accords more closely with [the plaintiffs’] position,\" and \"the [government has] the stronger position, although only slightly.\"\n\nOn November 7, 2014, the Supreme Court granted certiorari in the plaintiff's appeal of the 4th Circuit ruling. The decision to grant certiorari was unusual. It was believed that the Supreme Court would not grant certiorari given the lack of a circuit split, instead awaiting further decisions from lower courts before reviewing the issue. University of Michigan Law School Assistant Professor Nicholas Bagley described the decision to grant certiorari as indicating that \"four justices apparently think—or at least are inclined to think—that \"King\" was wrongly decided\".\n\nAlabama, Georgia, Indiana, Nebraska, Oklahoma, South Carolina, and West Virginia joined amicus briefs in support of the challengers. California, Connecticut, Delaware, the District of Columbia, Hawaii, Illinois, Iowa, Kentucky, Maine, Maryland, Massachusetts, Mississippi, New Hampshire, New Mexico, New York, North Carolina, North Dakota, Oregon, Pennsylvania, Rhode Island, Vermont, Virginia, and Washington filed an amicus brief in support of the government; they state in one part that, under the \"Pennhurst\" doctrine, in cooperative federalism legislation passed by Congress, if Congress wishes to impose any conditions on the States, then it must give \"clear notice\" of such conditions; otherwise, the conditions are invalid. They argue that the controlling phrase \"an Exchange established by the State\" is \"buried in two sub-sections,\" which effectively \"'hide[s] elephants in mouseholes,'\" were it to mean that Congress imposed the condition on the states that they must establish their own exchanges or their residents would not receive federal subsidies; they say that because of this, the phrases \"fail the \"Pennhurst\" clear-notice test,\" thereby making the foregoing condition invalid. Numerous individuals and organizations filed amicus briefs in support of both sides.\n\nThe American Public Health Association and the deans of 19 schools of public health filed an amicus brief in support of the government. In the brief, the public health officials estimated that eliminating the premium tax credits in states that use the federal exchange would result in 9,800 additional deaths per year. This figure was based on earlier studies of the impact of the Massachusetts health care reform law on death rates in that state. The brief stated that residents of the 34 states that use the Federal exchange tend to be less healthy and have less access to healthcare than residents of the states that created their own exchanges. The brief argues that eliminating the subsidies will increase this disparity.\n\nOn July 22, 2014, the U.S. Court of Appeals for the D.C. Circuit ruled 2–1 in favor of the plaintiffs.\n\nThe Court of Appeals stated:\n\nAs part of the government's briefs, they argued that none of the plaintiffs had standing to file suit. David Klemencic, one of the plaintiffs, residing in West Virginia was found to have standing under the Administrative Procedure Act (APA). Although West Virginia is geographically in the Fourth Circuit, the APA grants the D.C. Circuit shared jurisdiction over any issue involving a Federal agency based in Washington, D.C.\n\nOn September 4, 2014, the U.S. Court of Appeals for the D.C. Circuit granted the U.S. Secretary of Health's petition for rehearing the case en banc. The order also vacates the previous July 22 judgment.\n\nOn November 12, the Court of Appeals put further proceedings in \"Halbig\" into abeyance pending the Supreme Court's ruling in \"King\".\n\nOn September 9, 2014, in \"Pruitt v. Burwell\", the U.S. District Court for the Eastern District of Oklahoma ruled against the IRS saying \nThe court holds that the IRS Rule is arbitrary, capricious, an abuse of discretion or otherwise not in accordance with law, pursuant to 5 U.S.C. §706(2)(A), in excess of statutory jurisdiction, authority, or limitations, or short of statutory right, pursuant to 5 U.S.C. §706(2)(C), or otherwise is an invalid implementation of the ACA, and is hereby vacated.\nThe government appealed the decision to the Tenth Circuit, and in November 2014, the appeal was placed in abeyance pending the Supreme Court's decision in \"King\". Oklahoma requested that the Supreme Court take up the \"Pruitt\" case before appellate judgment so that the \"Pruitt\" plaintiffs can present their own arguments alongside the \"King\" plaintiffs. The government responded that the Supreme Court should not hear the Oklahoma case, stating that the states could proceed as \"amici curiae\" in the \"King\" case and that granting the Oklahoma case would raise additional jurisdictional concerns not presented in the \"King\" case. The Supreme Court denied certiorari before judgment on January 26, 2015.\n\nIn \"Indiana v. IRS\" the state of Indiana and multiple Indiana school districts are suing the IRS claiming that the employer mandate should not apply to schools or local governments. The IRS argued that the plaintiffs did not have standing to sue, but that argument was rejected and Judge William T. Lawrence in the U.S. District Court for the Southern District of Indiana ruled that the case could proceed. Oral arguments occurred in October 2014 but a ruling has not been issued.\n\nOn June 25, 2015, the Supreme Court issued its ruling, written by Chief Justice Roberts, and joined by Justices Kennedy, Breyer, Ginsburg, Sotomayor, and Kagan, rejecting the challenge to the act. The Court noted that previous attempts to reform health care insurance \"encouraged people to wait until they got sick to buy insurance\" resulting in \"an economic 'death spiral': premiums rose, the number of people buying insurance declined, and insurers left the market entirely.\"\nIt further noted that in 2006 \"Massachusetts discovered a way to make the guaranteed issue and community rating requirements work—by requiring individuals to buy insurance and by providing tax credits to certain individuals to make insurance more affordable.\" and that \"the Affordable Care Act adopts a version of the three key reforms that made the Massachusetts system successful.\"\n\nThe Court found that the \"Chevron\" test \"does not provide the appropriate framework here.\" and also rejected the Court of Appeals approach of deferring to the IRS: \"The tax credits are one of the Act’s key reforms and whether they are available on Federal Exchanges is a question of deep 'economic and political significance'; had Congress wished to assign that question to an agency, it surely would have done so expressly. And it is especially unlikely that Congress would have delegated this decision to the IRS, which has no expertise in crafting health insurance policy of this sort.\" It concluded that it is \"the Court's task to determine the correct reading of Section 36B.\"\n\nCiting \"FDA v. Brown & Williamson Tobacco Corp\", the Court noted that \"when deciding whether the language is plain, the Court must read the words 'in their context and with a view to their place in the overall statutory scheme.\n\nHaving found the text ambiguous, the Court, citing \"United Sav. Assn. of Tex. v. Timbers of Inwood Forest Associates,\" (an opinion written by Justice Scalia) looked \"to the broader structure of the Act to determine whether one of Section 36B's 'permissible meanings produces a substantive effect that is compatible with the rest of the law. It rejected petitioners' interpretation \"because it would destabilize the individual insurance market in any State with a Federal\nExchange, and likely create the very 'death spirals' that Congress designed the Act to avoid.\" The Court observed that the petitioners' interpretation would make the ACA \"operate quite differently in a State with a Federal Exchange. As they see it, one of the Act's three major reforms—the tax credits—would not apply. And a second major reform—the coverage requirement—would not apply in a meaningful way...[W]ithout the tax credits, the coverage requirement would apply to fewer individuals. And it would be a lot fewer...If petitioners are right, therefore, only one of the Act's three major reforms would apply in States with a Federal Exchange.\" Here, the statutory scheme compels us to reject petitioners’ interpretation because it would destabilize the individual insurance market in any State with a Federal Exchange, and likely create the very “death spirals” that Congress designed the Act to avoid.\n\nUnlike the Fourth Circuit, the Court chose not to rely on the IRS interpretation, noting the “economic and political significance” of the question and the IRS's lack of expertise in health insurance policy. By choosing instead to resolve the ambiguous language of the statute by looking at the purpose of the statute as a whole rather than by applying the Chevron doctrine, the Court's decision precludes the possibility of the IRS reversing in the future its decision to have subsidies available on the federally run exchange.\n\nIn a dissent joined by Justices Thomas and Alito, Justice Scalia wrote: \"The Court holds that when the Patient Protection and Affordable Care Act says 'Exchange established by the State it means 'Exchange established by the State or the Federal Government.' That is of course quite absurd, and the Court’s 21 pages of explanation make it no less so.\" He then ridiculed the decision, saying that the Affordable Care Act should be called \"SCOTUScare.\"\n\nScalia further noted that the rest of the ACA carefully distinguishes between exchanges established by states and those established by the federal government through Health and Human Services. Scalia cited differences between where the document identifies how the different exchanges receive funding, authority, and names. Scalia used this to argue that the context of the law does not allow for the phrase \"established by the state\" to mean \"established by the state and federal government.\" He reminds the reader that the purpose of looking at the context of phrases is for \"understanding the terms of the law, not [to make] an excuse for rewriting them.\"\n\nLater, Scalia also pointed out that interpreting the phrase \"by the State\" as \"by the state and federal government\" not only eliminates all meaning from the first phrase, but causes problems of interpretation elsewhere in the ACA. \"The State\" is identified distinctly in the ACA with regards to the formula for calculating tax credits, for screening children for tax credit eligibility, for using a \"secure electronic interface\" for tax credit screening, for other agencies, for directions on operating web sites, and for guidelines around the enrollment of children. Of this, Scalia mentioned that \"[i]t is bad enough for a court to cross out 'by the State' once. But seven times?\"\n\nThe extensive use of the term \"by the State\" also contrasts against the more extensive use of more general terms. \"Clause after clause of the law uses a more general term such as 'Exchange.'\" Scalia pointed out that the court should defer to the specific meaning of this term, and that assuming that the \"by the State\" term is general does not fit appropriate rules of interpretation. Scalia also notes that the ACA knows how to equate unlike terms explicitly, as it declared that \"[a] territory that...establishes...an Exchange...shall be treated as a State.\" The ACA does not have such an equivalency clause for exchanges established by the federal government.\n\n\n"}
{"id": "21917172", "url": "https://en.wikipedia.org/wiki?curid=21917172", "title": "Lago di Val di Noci", "text": "Lago di Val di Noci\n\nLago di Val di Noci is a lake in the Province of Genova, Liguria, Italy.\n"}
{"id": "236329", "url": "https://en.wikipedia.org/wiki?curid=236329", "title": "List of diseases (A)", "text": "List of diseases (A)\n\nThis is a list of diseases starting with the letter \"M\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "563521", "url": "https://en.wikipedia.org/wiki?curid=563521", "title": "List of wars and anthropogenic disasters by death toll", "text": "List of wars and anthropogenic disasters by death toll\n\nThis is a list of wars and anthropogenic disasters by death toll.\nIt covers the name of the event, the location, and the start and end of each event. Some events may belong in more than one category. In addition, some of the listed events overlap each other, and in some cases the death toll from a smaller event is included in the one for the larger event or time period of which it was part.\n\n\"This section lists all armed conflicts whose highest estimated casualties are one hundred thousand or more, this includes deaths of both soldiers and civilians, from causes both directly and indirectly caused by the war, including combat, disease, famine, massacres and genocide.\"\n\n\"This section lists events that entail the mass murder (or death caused by the forced eviction) of individuals on the basis of race, religion, or ethnicity.\"\n\n\"This section lists events that entail the mass killings of political opposition (such as those of certain ideology, class or political persuasion).\"\n\n\"This list is incomplete; please help by adding to it.\" \n\n\"See also: Red Terror (disambiguation), White Terror, and Politicide.\"\n\n\"This section lists deaths caused by poor labor conditions, executions for not performing labor satisfactorily, and deaths caused by mistreatment of the workforce both in transit and at work locations.\"\n\n\"Massacres and unnatural deaths that occurred during wars and were committed or caused by military or quasi-military forces (including terrorism, insurgent forces, and inter-communal violence). They may not particularly target ethnic, religious, or political groups but are usually part of a military strategy that disregards civilian lives, or they may be arbitrary acts of cruelty. Please try to only include events in which the majority of victims were civilians or which are often referred to as atrocities by significant mainstream scholarship.\"\n\n\"This chart includes regimes, empires, etc.\"\n\n\"This section includes famines, and disease that were caused or exacerbated by human action.\"\n\n\"Note: Some of these famines diseases were partially caused by nature.\"\n\n\"Only riots and incidents where at least four people died are listed here.\"\n\"This section lists deaths from the systematic practice of human sacrifice or suicide. For notable individual episodes, see Human sacrifice and mass suicide.\"\n\n\"These are floods and landslides that have been partially caused by humans, for example by failure of dams, levees, seawalls or retaining walls.\"\n\n\n\n\n"}
{"id": "50399439", "url": "https://en.wikipedia.org/wiki?curid=50399439", "title": "Maye Musk", "text": "Maye Musk\n\nMaye Musk (née Haldeman; born April 19, 1948) is a Canadian-South African model and dietitian. Also the mother of Elon Musk, Kimbal Musk, and Tosca Musk, she has been a model for 50 years appearing on the covers of magazines including \"Time\". The \"New York Post\" asserted her self-earned fame by declaring she is \"a star in her own right\".\n\nMaye was born in 1948 in Regina, Saskatchewan, Canada, a twin and one of five children. Her family moved to Pretoria, South Africa, in 1950. Her parents, Winnifred Josephine \"Wyn\" (Fletcher) and Joshua Norman Haldeman, were adventurous and flew the family around the world in a prop plane in 1952. For over ten years the family would spend time roaming the Kalahari desert in search of its fabled Lost City of the Kalahari. Their parents gave slide shows and talks about their journeys, \"My parents were very famous, but they were never snobs,\" Maye said.\n\nAs a young woman, Maye was a finalist in the 1969 Miss South Africa beauty competition. In 1970, she married Errol Musk, a South African engineer she met in high school. They had three children: Elon Musk, Kimbal Musk and Tosca Musk. She earned a Master's degree in dietetics from the University of the Orange Free State in South Africa. She later earned another Master's degree in nutritional science from the University of Toronto.\n\nIn 1979 she divorced Errol. Two years later Elon decided to live with his father. Kimbal joined Elon four years later. After graduating high school, Elon decided to move to Canada, and Maye followed him there with the rest of her children six months later in 1989.\n\nHer modeling career continued in Canada. She has appeared on boxes of Special K cereal, in Revlon ads, in a Beyoncé video, she appeared nude on the cover of \"Time\" magazine for a health issue; also nude on the cover of \"New York\" magazine in 2011 with a fake pregnant belly; she was on the cover of \"Elle Canada\" in 2012; and starred in advertisement campaigns for Target and Virgin America. In 2015, she was signed by IMG Models. In September 2017, she became CoverGirl's oldest spokesmodel at age 69, which one news story reported as \"making history\".\n\nIn addition to modeling she has a business as a dietitian and gives presentations worldwide.\n\n"}
{"id": "33776114", "url": "https://en.wikipedia.org/wiki?curid=33776114", "title": "Medical Decision Making (journal)", "text": "Medical Decision Making (journal)\n\nMedical Decision Making is a peer-reviewed academic journal that publishes papers in the fields of decision-making and medical informatics. Its editor-in-chief is Alan J. Schwartz (University of Illinois at Chicago). It was established in 1981 and is currently published by SAGE Publications on behalf of the Society for Medical Decision Making. A sister open access journal focusing on applications of medical decision making, \"Medical Decision Making Policy & Practice\", began publishing in 2016.\n\n\"Medical Decision Making\" is abstracted and indexed in MEDLINE, Scopus and the Social Sciences Citation Index. According to the \"Journal Citation Reports\", its 2016 impact factor is 2.362, ranking it 10th out of 23 journals in the category \"Medical Informatics\" and 33rd out of 90 journals in the category \"Health Care Sciences & Services\".\n\nThe following persons have been editors-in-chief of the journal:\n\n"}
{"id": "25030772", "url": "https://en.wikipedia.org/wiki?curid=25030772", "title": "Morbilliform", "text": "Morbilliform\n\nThe term morbilliform refers to a rash that looks like measles. The rash consists of macular lesions that are red and usually 2–10 mm in diameter but may be confluent in places. A morbilliform rash is a rose-red flat (macular) or slightly elevated (maculopapular) eruption, showing circular or elliptical lesions varying in diameter from 1 to 3 mm, with healthy-looking skin intervening.\n\nPatients with measles will have the rash but there are other syndromes and infections that will display the same symptom such as patients with Kawasaki disease, meningococcal petechiae or Waterhouse-Friderichsen syndrome, Dengue, Roseola, congenital syphilis, rubella, Echovirus 9, drug hypersensitivity reactions (in particular with certain classes of antiretroviral drugs, such as abacavir and nevirapine, and also the antiepileptic drug phenytoin), or other conditions may also have a morbilliform rash.\n\nOne cause of morbilliform rash is an allergic reaction to transfused blood/blood components. In such a case, the skin lesions would develop within a few hours (Approx. 4hours) of transfusion along with pruritus. The condition may even present with other symptoms, such as conjunctival oedema, oedema in the lips and tongue, and even localised angioedema. On rare occasions, the condition may even escalate to anaphylactic shock where pulmonary restrictions are seen. The associated cause for this is a reaction against an allergen that is seldom identified during testing. Transfusing products with anti-IgA antibodies to IgA-deficient patients has also been a suspected cause for such reactions. Management usually relates to the stoppage of transfusion for around 30minutes, until given antihistamines take effect. Transfusion may even be continued after, if no further progression is seen.\n"}
{"id": "1664254", "url": "https://en.wikipedia.org/wiki?curid=1664254", "title": "Most livable cities", "text": "Most livable cities\n\nThe world's most livable cities is an informal name given to any list of cities as they rank on an annual survey of living conditions. Regions with cities commonly ranked in the top 50 include the United States, Canada, Western Europe, Australia, and New Zealand. Three examples of such surveys are Monocle's \"Most Liveable Cities Index\", the Economist Intelligence Unit's \"Global Liveability Ranking\", and \"Mercer Quality of Living Survey\". Numbeo has the largest statistics and survey data based on cities and countries. Livability rankings may be used by employers assigning hardship allowances as part of job relocation.\n\nThe Economist Intelligence Unit's (EIU) publishes an annual Global Liveability Ranking, which ranks 140 cities for their urban quality of life based on assessments of their stability, healthcare, culture and environment, education and infrastructure.\n\nMelbourne, Australia, had been ranked by the EIU as the world's most livable city for seven years in a row, from 2011 to 2017. Between 2004 and 2010, Vancouver, Canada, was ranked the EIU's most livable city, with Melbourne sharing first place in the inaugural 2002 report. Vancouver has ranked third since 2015, while Vienna, Austria, ranked second until 2018 when it ranked first.\n\nThe Syrian capital, Damascus, was ranked the least livable city of the 140 assessed in 2016.\n\nThe EIU also publishes a Worldwide Cost of Living Survey that compares the cost of living in a range of global cities.\n\nAmerican global human resources and related financial services consulting firm Mercer annually releases its Mercer Quality of Living Survey, comparing 221 cities based on 39 criteria. New York City is given a baseline score of 100 and other cities are rated in comparison. Important criteria are safety, education, hygiene, health care, culture, environment, recreation, political-economic stability, public transport and access to goods and services. The list is intended to help multinational companies decide where to open offices or plants, and how much to pay employees. For nine consecutive years (2009–2017), Mercer ranked Austria's capital Vienna first in its annual \"Quality of Living\" survey, a title the city still held in 2016.\nSince 2006, the lifestyle magazine \"Monocle\" has published an annual list of livable cities. The list in 2008 was named \"The Most Liveable Cities Index\" and presented 25 top locations for quality of life.\n\nImportant criteria in this survey are safety/crime, international connectivity, climate/sunshine, quality of architecture, public transport, tolerance, environmental issues and access to nature, urban design, business conditions, pro-active policy developments and medical care.\n\nThe 2018 Monocle Survey determined the world's most livable city was Munich, followed by Tokyo, Vienna and Zurich. A total of four German cities were on the list of the 25 most livable cities, 15 out of the 25 were European, 3 each from Japan and Australia, and one from North America (Vancouver). No cities from South America, South Asia, or Africa made it into the list.\n\n"}
{"id": "50172792", "url": "https://en.wikipedia.org/wiki?curid=50172792", "title": "National Cashew Day", "text": "National Cashew Day\n\nNational Cashew Day is celebrated annually November 23. This day is to celebrate a popular nut for partying and simple snacking. Cashews are a popular nut and also provide excellent sources of antioxidants and minerals. National Cashew Day is only celebrated nationwide in the United States of America. This unofficial holiday was first observed in 2015.\n\nUnfortunately, there is yet to be a creator or origin found for this holiday.\n\nNational Cashew Day can be celebrated by eating cashews or using them in a favorite recipe. It can also be celebrated by adding #NationalCashewDay on one's social media pages.\n\n"}
{"id": "12300454", "url": "https://en.wikipedia.org/wiki?curid=12300454", "title": "Nepal Red Cross Society", "text": "Nepal Red Cross Society\n\nNepal Red Cross Society (NRCS)(Devnagari: नेपाल रेडक्रस सोसाइटी) is an independent, volunteer-based and nonprofit-humanitarian organization that delivers humanitarian service and support to the vulnerable people in an impartial and neutral manner. It came into being on 4 September 1963.\nNepal Red Cross Society was officially registered in Nepal after Nepal Government acceded to the Geneva Conventions (August 12, 1949). Having been recognized by the International Committee of the Red Cross (ICRC) and affiliated to International Federation of Red Cross and Red Crescent Societies (IFRC) on 1 October 1964.\n\nNRCS has, over the years, grown to be the largest humanitarian organization in Nepal, with its network of District Chapters (DCs) extended in each of the 77 districts of the country. District Chapters receive organizational support from more than 1,508 Sub-Chapters, 5,410 Junior and 865 Youth Red Cross Circles and Co-operation Committees under them. In addition, NRCS has been providing its services from 2 eye hospital, extended eye care centres, 106 blood transfusion centers, 210 ambulance service stations and 12 warehouses within the country.\nNearly after 100 years of establishment of red cross in the world. By observing the need of establishment of the Red Cross in Nepal, in the chairmanship of then Health Minister Dr. Nageshwar Prasad Singh a meeting was called at Singha Durbar, after Nepal Government acceded to the Geneva Conventions. Nepal Red Cross Society came into being on 4 September 1963. Princess Princep Shah of Nepal helped found the Nepal Red Cross and was its first President.\n\n\nNepal Red Cross Society (NRCS) has District Chapters (DC) in each district of the country. Which receives organizational support from Sub-Chapters, Youth/Junior Red Cross Circles and Co-operation Committees under them.\n\nCentral Committees provide guidance to bringing effectiveness in programme having National Network.\n\nThere are five regional committees and other committees related to management and technical areas. District chapter and sub-chapter have separate committees working in local level.\n\nNepal Red Cross Society has Regional Coordination Committees in each 5 development regions.\n\nThe first provincial assembly elections in Nepal was held in two phase on 26 November 2017 and on 7 December 2017. As the government changed its structure through the constitution of Nepal. Nepal Red Cross Society also needed to change its structure as per the government's setup. Thus, NRCS reformed its statute on 47th General Convention of the central assembly held at Biratnagar. As per the new statue NRCS has changed its structure into four levels – local, district, provincial and central levels.\n\nNepal Red Cross Society has district chapters in all 77 districts of the country. These district chapters receive organizational support from Sub-Chapters at local level, Junior/ Youth Red Cross Circles in district level and coordinating committees under them.\n\nSub-Chapters are the local level committees reaching every ward of the district. The working area of sub-chapters are divided into various wards of the Village / Municipality / Metropolitan City. These sub-chapters receives organizational support from Junior/ Youth Red Cross Circles based on schools and coordinating committees at communities.\n\nNRCS has District Chapter in each district within the country. And more than 10 Sub-Chapters are established in each district.\nBlood Transfusion Service of Nepal Red Cross Society was established in the year 1966 i.e. 3 years after the inception of the Society itself. The government of Nepal, in its policy declaration of 1991, has mandated Nepal Red Cross Society as the sole authority in conducting blood programmes in Nepal.\n\nThe Central Blood Transfusion Service Center (CBTSC) in Kathmandu is responsible for management of services in the Kathmandu valley, and for supervising and monitoring the technical standards of the district centers, providing guidance to ensure the collection and supply of safe blood.\n\nFour of the five regions have blood transfusion service centers at Biratnagar, Pokhra, Nepalganj and Chitwan. Those provide local services, and thus there is no distinct functioning management structure at the regional level.\n\nThe district BTSCs are managed by NRCS district chapters. These centres are supposed to comply with guidelines provided under the 1983 NRCS regulations and the 1998 Standard Operating Procedures.\n\nNepal Red Cross Society has been managing eye care services as a key component of health service in Mid-West Region (Province no 6) since 1990s through eye care centers (Surkhet, Dailekh, Jajarkot and Bardia) and outreach program with the support of Swiss Red Cross. NRCS is operating 2 eye hospitals and extended eye care centers within the country.\n\nThe Nepal Red Cross Society has its National Training Center, Human Resource Development Institute (HRDI) at Budol, Banepa, Kavre, Nepal.\n\n"}
{"id": "277315", "url": "https://en.wikipedia.org/wiki?curid=277315", "title": "Opinion poll", "text": "Opinion poll\n\nAn opinion poll, often simply referred to as a poll or a survey, is a human research survey of public opinion from a particular sample. Opinion polls are usually designed to represent the opinions of a population by conducting a series of questions and then extrapolating generalities in ratio or within confidence intervals.\n\nThe first known example of an opinion poll was a local straw poll conducted by \"The Harrisburg Pennsylvanian\" in 1824, showing Andrew Jackson leading John Quincy Adams by 335 votes to 169 in the contest for the United States Presidency. Since Jackson won the popular vote in that state and the whole country, such straw votes gradually became more popular, but they remained local, usually citywide phenomena. In 1916, \"The Literary Digest\" embarked on a national survey (partly as a circulation-raising exercise) and correctly predicted Woodrow Wilson's election as president. Mailing out millions of postcards and simply counting the returns, \"The Literary Digest\" correctly predicted the victories of Warren Harding in 1920, Calvin Coolidge in 1924, Herbert Hoover in 1928, and Franklin Roosevelt in 1932.\n\nThen, in 1936, its 2.3 million \"voters\" constituted a huge sample, but they were generally more affluent Americans who tended to have Republican sympathies. \"The Literary Digest\" was ignorant of this new bias; the week before election day, it reported that Alf Landon was far more popular than Roosevelt. At the same time, George Gallup conducted a far smaller (but more scientifically based) survey, in which he polled a demographically representative sample. Gallup correctly predicted Roosevelt's landslide victory. \"The Literary Digest\" soon went out of business, while polling started to take off.\n\nElmo Roper was another American pioneer in political forecasting using scientific polls. He predicted the reelection of President Franklin D. Roosevelt three times, in 1936, 1940, and 1944. Louis Harris had been in the field of public opinion since 1947 when he joined the Elmo Roper firm, then later became partner.\n\nIn September 1938 Jean Stoetzel, after having met Gallup, created IFOP, the Institut Français d'Opinion Publique, as the first European survey institute in Paris and started political polls in summer 1939 with the question \"Why die for Danzig?\", looking for popular support or dissent with this question asked by appeasement politician and future collaborationist Marcel Déat.\n\nGallup launched a subsidiary in the United Kingdom that, almost alone, correctly predicted Labour's victory in the 1945 general election, unlike virtually all other commentators, who expected a victory for the Conservative Party, led by Winston Churchill.\n\nThe Allied occupation powers helped to create survey institutes in all of the Western occupation zones of Germany in 1947 and 1948 to better steer denazification. By the 1950s, various types of polling had spread to most democracies.\n\nIn long-term perspective, advertising had come under heavy pressure in the early 1930s. The Great Depression forced businesses to drastically cut back on their advertising spending. Layoffs and reductions were common at all agencies. The New Deal furthermore aggressively promoted consumerism, and minimized the value of (or need for) advertising. Historian Jackson Lears argues that \"By the late 1930s, though, corporate advertisers had begun a successful counterattack against their critics.\" They rehabilitated the concept of consumer sovereignty by inventing scientific public opinion polls, and making it the centerpiece of their own market research, as well as the key to understanding politics. George Gallup, the vice president of Young and Rubicam, and numerous other advertising experts, led the way. Moving into the 1940s, the industry played a leading role in the ideological mobilization of the American people for fighting the Nazis and Japanese in World War II. As part of that effort, they redefined the \"American Way of Life\" in terms of a commitment to free enterprise. \"Advertisers,\" Lears concludes, \"played a crucial hegemonic role in creating the consumer culture that dominated post-World War II American society.\"\n\nOpinion polls for many years were maintained through telecommunications or in person-to-person contact. Methods and techniques vary, though they are widely accepted in most areas. Over the years, technological innovations have also influenced survey methods such as the availability of electronic clipboards and Internet based polling. Verbal, ballot, and processed types can be conducted efficiently, contrasted with other types of surveys, systematics, and complicated matrices beyond previous orthodox procedures.\n\nOpinion polling developed into popular applications through popular thought, although response rates for some surveys declined. Also, the following has also led to differentiating results: Some polling organizations, such as Angus Reid Public Opinion, YouGov and Zogby use Internet surveys, where a sample is drawn from a large panel of volunteers, and the results are weighted to reflect the demographics of the population of interest. In contrast, popular web polls draw on whoever wishes to participate, rather than a scientific sample of the population, and are therefore not generally considered professional.\n\nRecently, statistical learning methods have been proposed in order to exploit social media content (such as posts on the micro-blogging platform of Twitter) for modelling and predicting voting intention polls.\n\nPolls can be used in the public relations field as well. In the early 1920s, public relation experts described their work as a two-way street. Their job would be to present the misinterpreted interests of large institutions to public. They would also gauge the typically ignored interests of the public through polls.\n\nA \"benchmark poll\" is generally the first poll taken in a campaign. It is often taken before a candidate announces their bid for office but sometimes it happens immediately following that announcement after they have had some opportunity to raise funds. This is generally a short and simple survey of likely voters.\n\nA \"benchmark poll\" serves a number of purposes for a campaign, whether it is a political campaign or some other type of campaign. First, it gives the candidate a picture of where they stand with the electorate before any campaigning takes place. If the poll is done prior to announcing for office the candidate may use the poll to decide whether or not they should even run for office. Secondly, it shows them where their weaknesses and strengths are in two main areas. The first is the electorate. A \"benchmark poll\" shows them what types of voters they are sure to win, those they are sure to lose, and everyone in-between these two extremes. This lets the campaign know which voters are persuadable so they can spend their limited resources in the most effective manner. Second, it can give them an idea of what messages, ideas, or slogans are the strongest with the electorate.\n\n\"Brushfire polls\" are polls taken during the period between the \"benchmark poll\" and \"tracking polls\". The number of \"brushfire polls\" taken by a campaign is determined by how competitive the race is and how much money the campaign has to spend. These polls usually focus on likely voters and the length of the survey varies on the number of messages being tested.\n\n\"Brushfire polls\" are used for a number of purposes. First, it lets the candidate know if they have made any progress on the ballot, how much progress has been made, and in what demographics they have been making or losing ground. Secondly, it is a way for the campaign to test a variety of messages, both positive and negative, on themselves and their opponent(s). This lets the campaign know what messages work best with certain demographics and what messages should be avoided. Campaigns often use these polls to test possible attack messages that their opponent may use and potential responses to those attacks. The campaign can then spend some time preparing an effective response to any likely attacks. Thirdly, this kind of poll can be used by candidates or political parties to convince primary challengers to drop out of a race and support a stronger candidate.\n\nA \"tracking poll\" or \"rolling poll\" is a poll in which responses are obtained in a number of consecutive periods, for instance daily, and then results are calculated using a moving average of the responses that were gathered over a fixed number of the most recent periods, for example the past five days. In this example, the next calculated results will use data for five days counting backwards from the next day, namely the same data as before, but with the data from the next day included, and without the data from the sixth day before that day.\n\nOver time, a number of theories and mechanisms have been offered to explain erroneous polling results. Some of these reflect errors on the part of the pollsters; many of them are statistical in nature. Others blame the respondents for not giving candid answers (\"e.g.\", the Bradley effect, the Shy Tory Factor); these can be more controversial.\n\nPolls based on samples of populations are subject to sampling error which reflects the effects of chance and uncertainty in the sampling process. Sampling polls rely on the law of large numbers to measure the opinions of the whole population based only on a subset, and for this purpose the absolute size of the sample is important, but the percentage of the whole population is not important (unless it happens to be close to the sample size). The possible difference between the sample and whole population is often expressed as a margin of error - usually defined as the radius of a 95% confidence interval for a particular statistic. One example is the percent of people who prefer product A versus product B. When a single, global margin of error is reported for a survey, it refers to the maximum margin of error for all reported percentages using the full sample from the survey. If the statistic is a percentage, this maximum margin of error can be calculated as the radius of the confidence interval for a reported percentage of 50%. Others suggest that a poll with a random sample of 1,000 people has margin of sampling error of ±3% for the estimated percentage of the whole population.\n\nA 3% margin of error means that if the same procedure is used a large number of times, 95% of the time the true population average will be within the sample estimate plus or minus 3%. The margin of error can be reduced by using a larger sample, however if a pollster wishes to reduce the margin of error to 1% they would need a sample of around 10,000 people. In practice, pollsters need to balance the cost of a large sample against the reduction in sampling error and a sample size of around 500–1,000 is a typical compromise for political polls. (Note that to get complete responses it may be necessary to include thousands of additional participators.)\n\nAnother way to reduce the margin of error is to rely on poll averages. This makes the assumption that the procedure is similar enough between many different polls and uses the sample size of each poll to create a polling average. An example of a polling average can be found here: 2008 Presidential Election polling average. Another source of error stems from faulty demographic models by pollsters who weigh their samples by particular variables such as party identification in an election. For example, if you assume that the breakdown of the US population by party identification has not changed since the previous presidential election, you may underestimate a victory or a defeat of a particular party candidate that saw a surge or decline in its party registration relative to the previous presidential election cycle.\n\nA caution is that an estimate of a trend is subject to a larger error than an estimate of a level. This is because if one estimates the change, the difference between two numbers \"X\" and \"Y,\" then one has to contend with errors in both \"X\" and \"Y\". A rough guide is that if the change in measurement falls outside the margin of error it is worth attention.\n\nSince some people do not answer calls from strangers, or refuse to the answer the poll, poll samples may not be representative samples from a population due to a non-response bias. Response rates have been declining, and are down to about 10% in recent years. Because of this selection bias, the characteristics of those who agree to be interviewed may be markedly different from those who decline. That is, the actual sample is a biased version of the universe the pollster wants to analyze. In these cases, bias introduces new errors, one way or the other, that are in addition to errors caused by sample size. Error due to bias does not become smaller with larger sample sizes, because taking a larger sample size simply repeats the same mistake on a larger scale. If the people who refuse to answer, or are never reached, have the same characteristics as the people who do answer, then the final results should be unbiased. If the people who do not answer have different opinions then there is bias in the results. In terms of election polls, studies suggest that bias effects are small, but each polling firm has its own techniques for adjusting weights to minimize selection bias.\n\nSurvey results may be affected by response bias, where the answers given by respondents do not reflect their true beliefs. This may be deliberately engineered by unscrupulous pollsters in order to generate a certain result or please their clients, but more often is a result of the detailed wording or ordering of questions (see below). Respondents may deliberately try to manipulate the outcome of a poll by e.g. advocating a more extreme position than they actually hold in order to boost their side of the argument or give rapid and ill-considered answers in order to hasten the end of their questioning. Respondents may also feel under social pressure not to give an unpopular answer. For example, respondents might be unwilling to admit to unpopular attitudes like racism or sexism, and thus polls might not reflect the true incidence of these attitudes in the population. In American political parlance, this phenomenon is often referred to as the Bradley effect. If the results of surveys are widely publicized this effect may be magnified - a phenomenon commonly referred to as the spiral of silence.\n\nUse of the plurality voting system (select only one candidate) in a poll puts an unintentional bias into the poll, since people who favor more than one candidate cannot indicate this. The fact that they must choose only one candidate biases the poll, causing it to favor the candidate most different from the others while it disfavors candidates who are similar to other candidates. The plurality voting system also biases elections in the same way.\n\nSome people responding may not understand the words being used, but may wish to avoid the embarrassment of admitting this, or the poll mechanism may not allow clarification, so they may make an arbitrary choice. Some percentage of people also answer whimsically or out of annoyance at being polled. This results in perhaps 4% of Americans reporting they have personally been decapitated or that \"lizardmen are running the Earth\".\n\nIt is well established that the wording of the questions, the order in which they are asked and the number and form of alternative answers offered can influence results of polls. For instance, the public is more likely to indicate support for a person who is described by the operator as one of the \"leading candidates\". This support itself overrides subtle bias for one candidate, as does lumping some candidates in an \"other\" category or vice versa. Thus comparisons between polls often boil down to the wording of the question. On some issues, question wording can result in quite pronounced differences between surveys. This can also, however, be a result of legitimately conflicted feelings or evolving attitudes, rather than a poorly constructed survey.\n\nA common technique to control for this bias is to rotate the order in which questions are asked. Many pollsters also split-sample. This involves having two different versions of a question, with each version presented to half the respondents.\n\nThe most effective controls, used by attitude researchers, are:\n\n\nThese controls are not widely used in the polling industry.. However, as it is important that questions to test the product have a high quality, survey methodologists work on methods to test them. Empirical tests provide insight into the quality of the questionnaire, some may be more complex than others. For instance, testing a questionnaire can be done by:\n\nOne of the criticisms of opinion polls is that societal assumptions that opinions between which there is no logical link are \"correlated attitudes\" can push people with one opinion into a group that forces them to pretend to have a supposedly linked but actually unrelated opinion. That, in turn, may cause people who have the first opinion to claim on polls that they have the second opinion without having it, causing opinion polls to become part of self-fulfilling prophecy problems. It have been suggested that attempts to counteract unethical opinions by condemning supposedly linked opinions may favor the groups that promote the actually unethical opinions by forcing people with supposedly linked opinions into them by ostracism elsewhere in society making such efforts counterproductive, that not being sent between groups that assume ulterior motives from each other and not being allowed to express consistent critical thought anywhere may create psychological stress because humans are sapient, and that discussion spaces free from assumptions of ulterior motives behind specific opinions should be created. In this context, rejection of the assumption that opinion polls show actual links between opinions is considered important.\n\nAnother source of error is the use of samples that are not representative of the population as a consequence of the methodology used, as was the experience of \"The Literary Digest\" in 1936. For example, telephone sampling has a built-in error because in many times and places, those with telephones have generally been richer than those without.\n\nIn some places many people have only mobile telephones. Because pollsters cannot use automated dialing machines to call mobile phones in the United States (because the phone's owner may be charged for taking a call), these individuals are typically excluded from polling samples. There is concern that, if the subset of the population without cell phones differs markedly from the rest of the population, these differences can skew the results of the poll.\n\nPolling organizations have developed many weighting techniques to help overcome these deficiencies, with varying degrees of success. Studies of mobile phone users by the Pew Research Center in the US, in 2007, concluded that \"cell-only respondents are different from landline respondents in important ways, (but) they were neither numerous enough nor different enough on the questions we examined to produce a significant change in overall general population survey estimates when included with the landline samples and weighted according to US Census parameters on basic demographic characteristics.\" \nThis issue was first identified in 2004, but came to prominence only during the 2008 US presidential election. In previous elections, the proportion of the general population using cell phones was small, but as this proportion has increased, there is concern that polling only landlines is no longer representative of the general population. In 2003, only 2.9% of households were wireless (cellphones only), compared to 12.8% in 2006. This results in \"coverage error\". Many polling organisations select their sample by dialling random telephone numbers; however, in 2008, there was a clear tendency for polls which included mobile phones in their samples to show a much larger lead for Obama, than polls that did not.\n\nThe potential sources of bias are:\n\nSome polling companies have attempted to get around that problem by including a \"cellphone supplement\". There are a number of problems with including cellphones in a telephone poll:\n\nAn oft-quoted example of opinion polls succumbing to errors occurred during the 1992 UK general election. Despite the polling organizations using different methodologies, virtually all the polls taken before the vote, and to a lesser extent, exit polls taken on voting day, showed a lead for the opposition Labour party, but the actual vote gave a clear victory to the ruling Conservative party.\n\nIn their deliberations after this embarrassment the pollsters advanced several ideas to account for their errors, including:\n\n\nThe relative importance of these factors was, and remains, a matter of controversy, but since then the polling organizations have adjusted their methodologies and have achieved more accurate results in subsequent election campaigns.\n\nA comprehensive discussion of these biases and how they should be understood and mitigated is included in several sources including Dillman and Salant (1994).\n\nA widely publicized failure of opinion polling to date in the United States was the prediction that Thomas Dewey would defeat Harry S. Truman in the 1948 US presidential election. Major polling organizations, including Gallup and Roper, indicated a landslide victory for Dewey.\n\nIt was widely predicted that Donald Trump would lose the 2016 US presidential election to Former US Secretary of State Hillary Clinton; however, Donald Trump was elected the 45th President of the United States and was inaugurated on January 20, 2017. Many publications, such as FiveThirtyEight, predicted that there was a significant chance of Trump winning.\n\nIn the United Kingdom, most polls failed to predict the Conservative election victories of 1970 and 1992, and Labour's victory in 1974. However, their figures at other elections have been generally accurate. In the 2015 election virtually every poll predicted a hung parliament with Labour and the Conservatives neck and neck when the actual result was a clear Conservative majority. On the other hand, in 2017, the opposite appears to have occurred. Most polls predicted a Conservative landslide, even though in reality the election resulted in a hung parliament with the Conservatives losing their majority.\n\nSocial media today is a popular medium for the candidates to campaign and for gauging the public reaction to the campaigns. Social media can also be used as an indicator of the voter opinion regarding the poll. Some research studies have shown that predictions made using social media signals can match traditional opinion polls.\n\nRecently, concerning the 2016 U.S. presidential election, a major concern has been that of the effect of false stories spreading throughout social media. Recent evidence shows that social media plays a huge role in the supplying of news: 62 percent of US adults get news on social media. This fact makes the issue of fake news being spread throughout it more influential. Other evidence surrounding fake news shows that: the most popular fake news stories were more widely shared on Facebook than the most popular mainstream news stories; many people who see fake news stories report that they believe them; and the most discussed fake news stories tended to favor Donald Trump over Hillary Clinton. As a result of these facts, some have concluded that if not for these stories, Donald Trump may not have won the election over Hillary Clinton.\n\nBy providing information about voting intentions, opinion polls can sometimes influence the behavior of electors, and in his book \"\", Peter Hitchens asserts that opinion polls are actually a device for influencing public opinion. The various theories about how this happens can be split into two groups: bandwagon/underdog effects, and strategic (\"tactical\") voting.\n\nA bandwagon effect occurs when the poll prompts voters to back the candidate shown to be winning in the poll. The idea that voters are susceptible to such effects is old, stemming at least from 1884; William Safire reported that the term was first used in a political cartoon in the magazine \"Puck\" in that year. It has also remained persistent in spite of a lack of empirical corroboration until the late 20th century. George Gallup spent much effort in vain trying to discredit this theory in his time by presenting empirical research. A recent meta-study of scientific research on this topic indicates that from the 1980s onward the Bandwagon effect is found more often by researchers.\n\nThe opposite of the bandwagon effect is the underdog effect. It is often mentioned in the media. This occurs when people vote, out of sympathy, for the party perceived to be \"losing\" the elections. There is less empirical evidence for the existence of this effect than there is for the existence of the bandwagon effect.\n\nThe second category of theories on how polls directly affect voting is called strategic or tactical voting. This theory is based on the idea that voters view the act of voting as a means of selecting a government. Thus they will sometimes not choose the candidate they prefer on ground of ideology or sympathy, but another, less-preferred, candidate from strategic considerations. An example can be found in the United Kingdom general election, 1997. As he was then a Cabinet Minister, Michael Portillo's constituency of Enfield Southgate was believed to be a safe seat but opinion polls showed the Labour candidate Stephen Twigg steadily gaining support, which may have prompted undecided voters or supporters of other parties to support Twigg in order to remove Portillo. Another example is the boomerang effect where the likely supporters of the candidate shown to be winning feel that chances are slim and that their vote is not required, thus allowing another candidate to win.\n\nIn addition, Mark Pickup in Cameron Anderson and Laura Stephenson's \"Voting Behaviour in Canada\" outlines three additional \"behavioural\" responses that voters may exhibit when faced with polling data.\n\nThe first is known as a \"cue taking\" effect which holds that poll data is used as a \"proxy\" for information about the candidates or parties. Cue taking is \"based on the psychological phenomenon of using heuristics to simplify a complex decision\" (243).\n\nThe second, first described by Petty and Cacioppo (1996) is known as \"cognitive response\" theory. This theory asserts that a voter's response to a poll may not line with their initial conception of the electoral reality. In response, the voter is likely to generate a \"mental list\" in which they create reasons for a party's loss or gain in the polls. This can reinforce or change their opinion of the candidate and thus affect voting behaviour.\n\nThird, the final possibility is a \"behavioural response\" which is similar to a cognitive response. The only salient difference is that a voter will go and seek new information to form their \"mental list\", thus becoming more informed of the election. This may then affect voting behaviour.\n\nThese effects indicate how opinion polls can directly affect political choices of the electorate. But directly or indirectly, other effects can be surveyed and analyzed on all political parties. The form of media framing and party ideology shifts must also be taken under consideration. Opinion polling in some instances is a measure of cognitive bias, which is variably considered and handled appropriately in its various applications.\n\nStarting in the 1980s, tracking polls and related technologies began having a notable impact on U.S. political leaders. According to Douglas Bailey, a Republican who had helped run Gerald Ford's 1976 presidential campaign, \"It's no longer necessary for a political candidate to guess what an audience thinks. He can [find out] with a nightly tracking poll. So it's no longer likely that political leaders are going to lead. Instead, they're going to follow.\"\n\nSome jurisdictions over the world restrict the publication of the results of opinion polls, especially during the period around an election, in order to prevent the possibly erroneous results from affecting voters' decisions. For instance, in Canada, it is prohibited to publish the results of opinion surveys that would identify specific political parties or candidates in the final three days before a poll closes.\n\nHowever, most western democratic nations don't support the entire prohibition of the publication of pre-election opinion polls; most of them have no regulation and some only prohibit it in the final days or hours until the relevant poll closes. A survey by Canada's Royal Commission on Electoral Reform reported that the prohibition period of publication of the survey results largely differed in different countries. Out of the 20 countries examined, 3 prohibit the publication during the entire period of campaigns, while others prohibit it for a shorter term such as the polling period or the final 48 hours before a poll closes. In India, the Election Commission has prohibited it in the 48 hours before the start of polling.\n\n\n\n\n"}
{"id": "28488392", "url": "https://en.wikipedia.org/wiki?curid=28488392", "title": "Peeter Mudist", "text": "Peeter Mudist\n\nPeeter Mudist (19 April 1942, Tallinn – 6 December 2013) was an Estonian painter, sculptor, and print-maker whose works have received multiple awards. He was also a member of the Estonian Artists' Union.\n\nBorn in 1942, in Tallinn, Mudist studied painting at the Estonian State Art Institute for 4 years, starting in 1963.\n\nMudist had Parkinson's Disease in his later years. He died on the morning of 6 December 2013.\n\n"}
{"id": "41220165", "url": "https://en.wikipedia.org/wiki?curid=41220165", "title": "Pilu oil", "text": "Pilu oil\n\nPilu oil is an extract from seeds of the pilu tree, the meswak tree, and the mustard tree.\n\nThe pilu tree belongs to the \"Salvadoraceae\" family. The botanic systematic name of the tree is \"Salvadora persica\". Chewing sticks of the pilu tree were used by the Babylonians approximately 7000 years ago; they were later used throughout the Greek and Roman empires, and by ancient Egyptians and Muslims. These chewing sticks are most commonly used in the Middle East and South America, though are also used in parts of Africa and Asia. Chewing sticks are used for oral hygiene, religious and social purposes. The pilu tree is a salt-tolerant shrub or small tree, living in arid zones. It is native to India, Africa and the Middle East.\n\nIn English, the Pilu tree is commonly known as the Toothbrush Tree, Patana Oak, Slow Match Tree, Tummy Wood, and Wild Guava.\n\nIn Indian languages, common names include:\n\n\nThe generic name was given in 1749 in honor of an apothecary of Barcelona, Jaime Salvador y Pedrol (1649-1740), by Dr Laurent Garcin, botanist, traveler and plant collector. The true specimen of this species came, as the specific name indicates, from Persia.\n\nThe Pilu tree is widely distributed in the drier parts of India, Baluchistan, and Ceylon and in the dry regions of West Asia and Egypt. In India, the tree is found abundantly in the states of Gujarat, Haryana, Punjab and Rajasthan. It can be also found in Raigad district of Maharashtra in abundance in marshy areas. In India it grows wild in arid or sandy areas of Punjab and north India and thrives in saline soils, but with a stunted growth.\nhttp://indiabiodiversity.org/observation/show/1757910?postToFB=false\n\"Salvadora persica\" is a large, well-branched evergreen shrub or small tree having soft whitish yellow wood. The bark is of old stems rugose, branches are numerous, drooping, glabrous, terete, finely striate, shining, and almost white. Leaves are somewhat fleshy, glaucous, 3.8–6.3 by 2–3.2 cm in size, elliptic lanceolate or ovate, obtuse, and often mucronate at the apex, the base is usually acute, less commonly rounded, the main nerves are in 5–6 pairs, and the petioles 1.3–2.2 cm long and glabrous. The flowers are greenish yellow in color, in axillary and terminal compound lax panicles 5–12.5 cm long, numerous in the upper axils, pedicels 1.5–3 mm long, bracts beneath the pedicels, ovate and very caduceus. The calyx is 1.25 mm long, glabrous, cleft halfway down, lobes rounded. The corolla is very thin, 3 mm long, deeply cleft, persistent, lobes are 2.5 mm long, oblong, obtuse, and much reflexed. The stamens are shorter than corolla, but exerted, owing to the corolla lobes being reflexed. The drupe is 3 mm in diameter, globose, smooth and becomes red when ripe\n\nThe fruits appear in May in Western areas and in June in central parts. Fruits are pale green to red brown when ripe. It is a globose drupe. The fruits are sweet and edible. The yield of fresh fruit per tree will be 10–15 kg. The yield of dried fruits will be 2–3 kg. The percentage of the oil in seeds is 32-34%. The seed contains a brittle shell on the outer surface, and the kernel accounts for 60% in dried seed, containing 40-43% oil.\n\n‘Sweet Variety’ of Pilu yields 35-44% oil which has a strong odour. The odour is due the presence of Benzyl isothiocyanate in the oil. There is greenish yellow fat called Khakan fat. The purified fat is free from foul odour and has an agreeable taste. It is snow white. The oil is rich in lauric and myristic acids. The fat is used in soap, making up to 20% of the soap itself, and it replaces coconut oil. It is used as a resist in the dyeing industry. The oil is also used in rheumatic infection treatment. It has a high melting point and a disagreeable odour that disappears on purification. The most important aspect of the oil is the presence of a low percentage of C8 and C10 fatty acids that are of great economic significance. The oil is an alternative source of oil for soap and detergent industries.\n\ntable-physical characters of oil\n\nTable-Fatty acid composition of oil\n\nIn India Pilu fat is being used for soap making in the \"unorganised\" or cottage sector of the industry, particularly by the Non-Edible Oil Industry of the Khadi and Village Industries commission. The All India Non-Edible Oil Industry Association, Poona, are also making soap with it. The oil has a bitter and sharp taste and may be used as a purgative diuretic, or tonic seed oil may be applied to the skin in rheumatism.\n\n\n"}
{"id": "32334136", "url": "https://en.wikipedia.org/wiki?curid=32334136", "title": "Plum syndrome", "text": "Plum syndrome\n\nPlum syndrome is a very rare genetic disorder. It is characterized by retinal non-attachment, colobomata, odd facies, eyes set wide, flat face, dislocated hip, abnormal big toe, contractures of the extremities, cleft lip and mono-segmented leucocytes. There may be associated learning difficulties.\n\nNamed by Dr. C. M. Plum.\n"}
{"id": "1135920", "url": "https://en.wikipedia.org/wiki?curid=1135920", "title": "Prodrazvyorstka", "text": "Prodrazvyorstka\n\nProdrazvyorstka (, short for , \"food apportionment\") was a Bolshevik policy and campaign of confiscation of grain and other agricultural products from the peasants at nominal fixed prices according to specified quotas (the noun \"razvyorstka,\" , and the verb \"razverstat' \"refer to the partition of the requested total amount as obligations from the suppliers).\n\nThe term is commonly associated with war communism during the Russian Civil War when it was introduced by the Bolshevik government. However Bolsheviks borrowed the idea from the grain \"razvyorstka\" introduced in the Russian Empire during World War I, in 1916.\n\n1916 saw a food crisis in the Russian Empire. While the harvest was good in Lower Volga Region and Western Siberia, its transportation by railroads collapsed. In addition, the food market was in disarray. Fixed prices for government purchases were unattractive. A decree of November 29, 1916 signed by Aleksandr Rittich of the Ministry of Agriculture) introduced razvyorstka as the collection of grain for defense purposes. The Russian Provisional Government established after the February Revolution of 1917 could not propose any incentives for peasants, and their state monopoly on grain sales failed to achieve its goal.\n\nIn 1918 the center of Soviet Russia found itself cut off from the most important agricultural regions of the country. The reserves of grain ran low, causing hunger among the urban population, where support for the Bolshevik government was strongest. In order to satisfy minimal food needs, the Soviet government introduced strict control over the food surpluses of the prosperous rural households. Since many peasants were extremely unhappy with this policy and tried to resist it, they were branded as \"saboteurs\" of the bread monopoly of the state and advocates of free \"predatory\", \"speculative\" trade. Vladimir Lenin believed that prodrazvyorstka was the only possible way to procure sufficient amounts of grain and other agricultural products for the population of the cities during the civil war.\n\nBefore prodrazvyorstka, Lenin's May 9, 1918 decree (\"О продовольственной диктатуре\") introduced the concept of \"produce dictatorship\". This and other subsequent decrees ordered the forced collection of foodstuffs, without any limitations, and used the Red Army to accomplish this.\n\nA decree of the Sovnarkom introduced prodrazvyorstka throughout Soviet Russia on January 11, 1919. Prodrazvyorstka also extended to Ukraine and Belarus (1919), Turkestan and Siberia (1920). In accordance with the decree of the People's Commissariat for Provisions on the procedures of prodrazvyorstka (January 13, 1919), the amount of different kinds of products designated for collection by the state (some historians call it an outright confiscation) was calculated on the basis of the data on each guberniya's areas under crops, crop capacity and the reserves of past years. Within each guberniya, the collection plan was broken down between uyezds, volosts, villages, and then separate peasant households. The collection procedures were performed by the agencies of the People's Commissariat for Provisions and prodotryads (продовольственный отряд, food brigades) with the help of kombeds (комитет бедноты, committees of the poor) and of local Soviets.\n\nInitially, prodrazvyorstka covered the collection of grain and fodder. During the procurement campaign of 1919–20, prodrazvyorstka also included potatoes and meat. By the end of 1920, it included almost every kind of agricultural product. According to Soviet statistics, the authorities collected 107.9 million poods (1.77 million metric tons) of grain and fodder in 1918–19, 212.5 million poods (3.48 million metric tons) in 1919–20, and 367 million pounds (6.01 million metric tons) in 1920–21.\n\nProdrazvyorstka allowed the Soviet government to solve the important problem of supplying the Red Army and urban population and of providing raw material for different industries. Prodrazvyorstka left its mark on commodity-money relations, since the authorities had prohibited selling of bread and grain. It also influenced many, if not all, aspects of relations between the city and the village and became one of the most important elements of the system of war communism.\n\nAs the Russian Civil War approached its end, prodrazvyorstka lost its actuality, but it had done much damage to the agricultural sector and caused growing discontent among peasants. As the government switched to the NEP (New Economic Policy), a decree of the 10th Congress of the Russian Communist Party (Bolsheviks) in March 1921 replaced prodrazvyorstka with prodnalog (food tax).\n\n\n"}
{"id": "8032176", "url": "https://en.wikipedia.org/wiki?curid=8032176", "title": "Right to food", "text": "Right to food\n\nThe right to food, and its non variations, is a human right protecting the right for people to feed themselves in dignity, implying that sufficient food is available, that people have the means to access it, and that it adequately meets the individual's dietary needs. The right to food protects the right of all human beings to be free from hunger, food insecurity and malnutrition. The right to food does not imply that governments have an obligation to hand out free food to everyone who wants it, or a right to be fed. However, if people are deprived of access to food for reasons beyond their control, for example, because they are in detention, in times of war or after natural disasters, the right requires the government to provide food directly.\n\nThe right is derived from the International Covenant on Economic, Social and Cultural Rights which has 160 state parties as of May 2012. States that sign the covenant agree to take steps to the maximum of their available resources to achieve progressively the full realization of the right to adequate food, both nationally and internationally. In a total of 106 countries the right to food is applicable either via constitutional arrangements of various forms or via direct applicability in law of various international treaties in which the right to food is protected.\n\nAt the 1996 World Food Summit, governments reaffirmed the right to food and committed themselves to half the number of hungry and malnourished from 840 to 420 million by 2015. However, the number has increased over the past years, reaching an infamous record in 2009 of more than 1 billion undernourished people worldwide. Furthermore, the number who suffer from hidden hunger – micronutrient deficiences that may cause stunted bodily and intellectual growth in children – amounts to over 2 billion people worldwide.\n\nWhilst under international law states are obliged to respect, protect and fulfill the right to food, the practical difficulties in achieving this human right are demonstrated by prevalent food insecurity across the world, and ongoing litigation in countries such as India. In the continents with the biggest food-related problems – Africa, Asia and Latin America – not only is there shortage of food and lack of infrastructure but also maldistribution and inadequate access to food.\n\nThe International Covenant on Economic, Social and Cultural Rights recognizes the \"right to an adequate standard of living, including adequate food\", as well as the \"fundamental right to be free from hunger\". The relationship between the two concepts is not straightforward. For example, \"freedom from hunger\" (which General Comment 12 designates as more pressing and immediate) could be measured by the number of people suffering from malnutrition and at the extreme, dying of starvation. The \"right to adequate food\" is a much higher standard, including not only absence of malnutrition, but to the full range of qualities associated with food, including safety, variety and dignity, in short all those elements needed to enable an active and healthy life.\n\nInspired by the above definition, the Special Rapporteur on the Right to Food in 2002 defined it as follows:\nThe right to have regular, permanent and unrestricted access, either directly or by means of financial purchases, to quantitatively and qualitatively adequate and sufficient food corresponding to the cultural traditions of the people to which the consumer belongs, and which ensure a physical and mental, individual and collective, fulfilling and dignified life free of fear.\nThis definition entails all normative elements explained in detail in the General Comment 12 of the ICESCR, which states:\nthe right to adequate food is realized when every man, woman and child, alone or in community with others, have the physical and economic access at all times to adequate food or means for its procurement.\n\nAccording to the Food and Agriculture Organization of the United Nations, the right to food does not imply that governments have an obligation to hand out free food to everyone who wants it. This is a common misconception.\n\nThe right to food is not a right to a minimum ration of calories, proteins and other specific nutrients, or a right to be fed. It is about being guaranteed the right to feed oneself, which requires not only that food is available – that the ratio of production to the population is sufficient – but also that it is accessible – i.e., that each household either has the means to produce or buy its own food. However, if individuals are deprived of access to food for reasons beyond their control, for instance because of an armed conflict, natural disaster or because they are in detention, recognition of the right to life obliges States to provide them with sufficient food for their survival.\nThe former Special Rapporteur on the Right to Food, Jean Ziegler, defined three dimensions to the right to food.\n\n\nFurthermore, any discrimination in access to food, as well as to means and entitlements for its procurement, on the grounds of race, colour, sex, language, age, religion, political or other opinion, national or social origin, property, birth or other status constitutes a violation of the right to food.\n\nRegarding the right to food, the international community also specified commonly agreed on standards, such as in the 1974 World Food Conference, the 1974 International Undertaking on World Food Security, the 1977 Standard Minimum Rules for the Treatment of Prisoners, the 1986 Declaration on the Right to Development, the ECOSOC Resolution 1987/90, the 1992 Rio Declaration on Environment and Development, and the 1996 Istanbul Declaration on Human Settlements.\n\nThere is a traditional distinction between two types of human rights. On the one hand, negative or abstract rights that are respected by non-intervention. On the other hand, positive or concrete rights that require resources for its realisation. However, it is nowadays contested whether it is possible to clearly distinguish between these two types of rights.\n\nThe right to food can accordingly be divided into the negative right to obtain food by one's own actions, and the positive right to be supplied with food if one is unable to access it. The negative right to food was recognised as early as in England’s 1215 Magna Carta which reads that: \"no one shall be ‘amerced’ (fined) to the extent that they are deprived of their means of living.\"\n\nThis section provides an overview of international developments relevant to the establishment and implementation of the right to food from the mid-20th century onwards.\n\n\n“The freedom from want.”\n\n\n\"Everyone has the right to a standard of living adequate for the health and well-being of himself and of his family, including food, clothing, housing and medical care and necessary social services, and the right to security in the event of unemployment, sickness, disability, widowhood, old age or other lack of livelihood in circumstances beyond his control\" (Article 25). \n\n“the right of everyone to an adequate standard of living for himself and his family, including adequate food” (Article 11.1) and “the fundamental right of everyone to be free from hunger” (Article 11.2).\n\n\n\nAmartya Sen won his 1998 Nobel Prize in part for his work in demonstrating that famine and mass starvation in modern times was not typically the product of a lack of food; rather, it usually arose from problems in food distribution networks or from government policies.\n\nThe right to food is protected under international human rights and humanitarian law.\n\nThe right to food is recognized in the 1948 Universal Declaration of Human Rights (Article 25) as part of the right to an adequate standard of living, and is enshrined in the 1966 International Covenant on Economic, Social and Cultural Rights (Article 11). The 2009 Optional Protocol to the International Covenant on Economic, Social and Cultural Rights makes the right to food justiciable at the international level. In 2012, the Food Assistance Convention is adopted, making it the first legally binding international treaty on food aid.\n\nIt is also recognized in many specific international instruments as varied as the 1948 Genocide Convention (Article 2), the 1951 Convention relating to the Status of Refugees (Articles 20 and 23), the 1989 Convention on the Rights of the Child (Articles 24(2)(c) and 27(3)), the 1979 Convention on the Elimination of All Forms of Discrimination Against Women (Articles 12(2)), or the 2007 Convention on the Rights of Persons with Disabilities (Articles 25(f) and 28(1)).\n\nThe right to food is also recognized in regional instruments, such as the 1988 Additional Protocol to the American Convention on Human Rights in the area of Economic, Social, and Cultural Rights or “Protocol of San Salvador” (Article 12), the 1990 African Charter on the Rights and Welfare of the Child, the 2001 African Commission on Human and Peoples' Rights recognition that the right to food falls under the African Charter on Human and Peoples' Rights, and the 2003 Protocol to the African Charter on Human and Peoples’ Rights on the Rights of Women in Africa or \"Maputo Protocol\" (Article 15). There are also such instruments in many national constitutions.\n\nThere are several non-legally binding international human rights instruments relevant to the right to food. They include recommendations, guidelines, resolutions or declarations. The most detailed is the 2004 Right to Food Guidelines. They are a practical tool to help implement the right to adequate food. The Right to Food Guidelines are not legally binding but draw upon international law and are a set of recommendations States have chosen on how to implement their obligations under Article 11 of the International Covenant on Economic, Social and Cultural Rights. Finally, the 1945 Constitution of the United Nations Food and Agriculture Organization provides that:\n\n“the Nations accepting this Constitution, being determined to promote the common welfare by furthering separate and collective action on their part for the purpose of: raising levels of nutrition and standards of living . . . and thus . . . ensuring humanity’s freedom from hunger. . . .” (Preamble).\nIn 1993, the \"International Food Security Treaty\" is developed in USA and Canada.\n\nIn 1998, a Conference on Consensus Strategy on the Right To Food held in Santa Barbara, California, USA with anti-hunger experts from five continents.\n\nIn 2010, a group of national and international organisations create a proposal to replace the European Union Common Agricultural Policy, which is due for change in 2013. The first article of The New Common Food and Agriculture Policy \"considers food as a universal human right, not merely a commodity.\"\n\nState obligations related to the right to food are well-established under international law. By signing the International Covenant on Economic, Social and Cultural Rights (ICESCR) states agreed to take steps to the maximum of their available resources to achieve progressively the full realization of the right to adequate food. They also acknowledge the essential role of international cooperation and assistance in this context. This obligation was reaffirmed by the Committee on Economic, Social and Cultural Rights (CESCR). Signatories to the Right to Food Guidelines also committed to implementing the right to food at a national level.\n\nIn General Comment no. 12, the CESCR interpreted the states' obligation as being of three types: the obligation to respect, protect and to fulfil:\n\n\nThese were again endorsed by states, when the FAO Council adopted the Right to Food Guidelines.\n\nThe ICESCR recognises that the right to freedom from hunger requires international cooperation, and relates to matters of production, the agriculture and global supply. Article 11 states that:\n\n\"The States Parties to the present Covenant... shall take, individually and through international co-operation, the measures, including specific programmes, which are needed: \n(a) To improve methods of production, conservation and distribution of food by making full use of technical and scientific knowledge, by disseminating knowledge of the principles of nutrition and by developing or reforming agrarian systems in such a way as to achieve the most efficient development and utilization of natural resources; \n(b) Taking into account the problems of both food-importing and food-exporting countries, to ensure an equitable distribution of world food supplies in relation to need.\"\nThe implementation of the right to food standards at national level has consequences for national constitutions, laws, courts, institutions, policies and programmes, and for various food security topics, such as fishing, land, focus on vulnerable groups, and access to resources.\n\nNational strategies on the progressive realization of the right to food should fulfill four functions:\n\nThe right to food imposes on all States obligations not only towards the persons living on their national territory, but also towards the populations of other States. The right to food is only realised when both national and international obligations are complied with. On the one hand, is the effect of the international environment and, in particular, climate change, malnutrition and food insecurity. On the other hand, the international community can only contribute if legal frameworks and institutions are established at the national level.\n\nUnder article 2(2) of the ICESCR, governments agreed that the right to food will be exercised without discrimination on grounds of sex, colour, race, age, language, religion, political or other opinion, national or social origin, property, birth or other status. The CESCR stresses the special attention that should be given to disadvantaged and marginalized farmers, including women farmers, in a rural context.\n\nA framework law is a \"legislative technique used to address cross-sectoral issues.\" Framework laws are more specific than a constitutional provision, as it lays down general obligations and principles. However, competent authorities and further legislation which still have to determine specific measures should be taken. The adoption of framework laws was recommended by the Committee on Economic, Social and Cultural Rights as a \"major instrument in the implementation of the national strategy concerning the right to food\". There are ten countries that have adopted and nine countries that are developing framework laws on food security or the right to food. This development is likely to increase in the coming years. Often they are known as food security laws instead of right to food laws, but their effect is usually similar.\n\nAdvantages of framework law includes that the content and scope of the right can be further specified, state and private actor obligations can be spelled out in detail, appropriate institutional mechanisms can be established, and rights to remedies can be provided for. Further advantages of framework laws include: strengthening government accountability, monitoring, helping government officials understand their role, improving access to courts and by providing administrative recourse mechanisms.\n\nHowever, provisions for obligations and remedies in existing framework law is not always very thorough, and it is neither always clear what they add to the justiciability of the right to food.\n\nAs of 2011, the following ten countries have adopted a framework law on food security or the right to food: Argentina, Bolivia, Brazil, Ecuador, El Salvador, Guatemala, Indonesia, Nicaragua, Peru and Venezuela. Moreover, in 2011 the following nine countries were drafting a framework law on food security or the right to food: Honduras, India, Malawi, Mexico, Mozambique, Paraguay, South Africa, Tanzania and Uganda. Finally, El Salvador, Nicaragua and Peru are drafting to update, replace or strengthen their framework law.\n\nThere are various ways in which constitutions can take the right to food or some aspect of it into account. As of 2011, 56 constitutions protect the right to food in some form or another. The three main categories of constitutional recognition are: as an explicit right, as implied in broader human rights or as part of a directive principle. In addition to those, the right can also indirectly be recognised when other human rights are interpreted by a judiciary.\n\nFirstly, the right to food is explicitly and directly recognised as a right in itself or as part of a broader human right in 23 countries. Three different forms can be distinguished.\n\n1. The following nine countries recognise the right to food as a separate and stand-alone right: Bolivia, Brazil, Ecuador, Guyana, Haiti, Kenya, South Africa, in the Interim Constitution of Nepal (as food sovereignty) and Nicaragua (as freedom from hunger).\n\n2. For a specific segment of the population the right to food is recognised in ten countries. Provisions regarding the right to food of children are present in the constitutions of: Brazil, Colombia, Cuba, Guatemala, Honduras, Mexico, Panama, Paraguay, and South Africa. The right to food of indigenous children is protected in the constitution of Costa Rica. Finally, the right to food of detainees and prisoners is additionally recognised in the constitution of South Africa.\n\n3. Five countries recognize the right to food explicitly as part of a human right to an adequate standard of living, quality of life, or development: Belarus, the Congo, Malawi, Moldova and Ukraine, and two recognise it as part of the right to work: Brazil and Suriname.\n\nSecondly, the following 31 countries implicitly recognise the right to food in broader human rights: Armenia, Azerbaijan, Belgium, Bolivia, Burundi, Cambodia, Czech Rep., Congo, Costa Rica, Cyprus, Ecuador, El Salvador, Eq.uatorial Guinea, Eritrea, Ethiopia, Finland, Georgia, Germany, Ghana, Guatemala, Guinea, Kyrgyzstan, Malawi, Netherlands, Pakistan, Peru, Romania, Switzerland, Thailand, Turkey, Venezuela.\n\nThirdly, the following thirteen countries explicitly recognise the right to food within the constitution as a directive principle or goal: Bangladesh, Brazil, Ethiopia, India, Iran, Malawi, Nigeria, Panama, Papua New Guinea, Pakistan, Sierra Leone, Sri Lanka, Uganda.\n\nIn some countries international treaties have a higher status than or equal status to national legislation. Consequently, the right to food may be directly applicable via international treaties if such country is member to a treaty in which the right is recognised. Such treaties include the International Covenant on Economic, Social and Cultural Rights (ICESCR), the Convention on the Elimination of All Forms of Discrimination Against Women (CEDAW) and the Convention on the Rights of the Child (CRC). Excluding countries in which the right to food is implicitly or explicitly recognised in their constitution, the right is directly applicable in at least 51 additional countries via international treaties.\n\nParties to the International Covenant on Economic, Social and Cultural Rights have to do everything to guarantee adequate nutrition, including legislating to that effect. The Covenant has become part of national legislation in over 77 countries. In these countries the provision for the right to food in the Covenant can be cited in a court. This has happened in Argentina (in the case of the right to health). \nHowever, citizens usually cannot prosecute using the Covenant, but can only do so under national law. If a country does not pass such laws a citizen has no redress, even though the state violated the covenant. The implementation of the Covenant is monitored through the Committee on Economic, Social and Cultural Rights. In total, 160 countries have ratified the Covenant. A further 32 countries have not ratified the covenant, although 7 of them did sign it.\n\nBy signing the Optional Protocol to the ICESCR, states recognise the competence of the Committee on Economic, Social and Cultural Rights to receive and consider complaints from individuals or groups who claim their rights under the Covenant have been violated. However, complainants must have exhausted all domestic remedies. The Committee can \"examine\", works towards \"friendly settlement\", in the case of grave or systematic violations of the Covenant, it can \"invite that State Party to cooperate\" and, finally, could \"include a summary account of the results of the proceedings in its annual report\". The following seven countries have ratified the Optional Protocol to the International Covenant on Economic, Social and Cultural Rights: Bolivia, Bosnia and Herzegovina, Ecuador, El Salvador, Mongolia, Slovakia, and Spain. A further 32 countries have signed the optional protocol.\n\nThe Special Rapporteur on the Right to Food, Mr. De Schutter, urged the establishment in law of the right to food, so that it can be translated into national strategies and institutions. Furthermore, he recommended emerging economies to protect the rights of land users, in particular of minority and vulnerable groups. He also advised to support smallholder agriculture in the face of mega-development projects, and to stop soil and water degradation through massive shifts to agroecological practices. Finally, the UN expert suggested adopting a strategy to tackle rising obesity.\n\nThe United Nations' Article 11 on the Right to Adequate Food suggests several implementation mechanisms. The Article acknowledges that the most appropriate ways and means of implementing the right to adequate food will inevitably vary significantly from one State to another. Every State must choose its own approaches, but the Covenant clearly requires that each State party take whatever steps are necessary to ensure that everyone is free from hunger and as soon as possible can enjoy the right to adequate food.\n\nThe Article emphasizes that the right to food requires full compliance with the principles of accountability, transparency, people's participation, decentralization, legislative capacity and the independence of the judiciary. In terms of strategy to implement the right to food, the Article asks that the States should identify and address critical issues in regard to all aspects of the food system, including the food production and processing, food storage, retail distribution, marketing and its consumption. The implementation strategy should give particular attention to the need to prevent discrimination in access to food shops and retail network, or alternatively to resources for growing food. As part of their obligations to protect people’s resource base for food, States should take appropriate steps to ensure that activities of the private business sector and civil society are in conformity with the right to food.\n\nThe Article notes that whenever a State faces severe resource constraints, whether caused by a process of economic adjustment, economic recession, climatic conditions or other factors, measures should be undertaken to ensure that the right to adequate food is especially fulfilled for vulnerable population groups and individuals.\n\nThe idea of the interdependence and indivisibility of all human rights was a founding principle of the United Nations. This was recognised in the 1993 Vienna Declaration and Programme of Action which reads “all human rights are universal, indivisible and interdependent and interrelated.\" The right to food is considered interlinked with the following human rights in particular: right to life, right to livelihood, right to health, right to property, freedom of expression, freedom of information, right to education, freedom of association, and the right to water. Other relevant rights include: the right to work, the right to social security, the right to social welfare, and the right to an adequate standard of living.\n\nFor example, according to the Committee overseeing the implementation of the ICESCR, “the right to water is a prerequisite for the realization of other human rights.” The need to have adequate water in order to have adequate food is in particular evident in the case of peasant farmers. Access to sustainable water resources for agriculture needs to be ensured to realise the right to food. This applies even more strongly to subsistence agriculture.\n\n"}
{"id": "5023510", "url": "https://en.wikipedia.org/wiki?curid=5023510", "title": "Royal Infirmary of Edinburgh", "text": "Royal Infirmary of Edinburgh\n\nThe Royal Infirmary of Edinburgh, or RIE, often (but incorrectly) known as the Edinburgh Royal Infirmary, or ERI, was established in 1729 and is the oldest voluntary hospital in Scotland. The new buildings of 1879 were claimed to be the largest voluntary hospital in the United Kingdom, and later on, the Empire. The hospital moved to a new 900 bed site in 2003 in Little France. It is the site of clinical medicine teaching as well as a teaching hospital for the University of Edinburgh Medical School. It is currently run by NHS Lothian. In 1960, the first kidney transplant performed in the UK was performed at the Royal Infirmary of Edinburgh by surgeon Michael Woodruff. In 1964, the world's first coronary care unit was established at the Royal Infirmary of Edinburgh by Dr. Desmond Julian. It is the only site for liver, pancreas and pancreatic islet cell transplantation and one of two sites for kidney transplantation in Scotland. It is currently the only site for TAVI in Scotland. In 2012 the Emergency Department had 113,000 patient attendances, the highest number in Scotland. On 16 November 2014, the University announced the Royal Infirmary as the location of Scotland's first PET-MRI Scanner.\n\nJohn Munro, President of the Incorporation of Surgeons in 1712, set in motion a project to establish a \"Seminary of Medical Education\" in Edinburgh, of which a General Hospital was an integral part. His son, Alexander Monro \"primus\", by then Professor of Anatomy, circulated an anonymous pamphlet in 1721 on the necessity and advantage of erecting a Hospital for the Sick Poor. In 1725 the Royal College of Physicians of Edinburgh wrote to the stock-holders of the Fishery Company, which was about to be wound up, suggesting that they assign their shares for the purpose of such a hospital. Other donors included many wealthy citizens, most of the physicians and several surgeons, numerous Church of Scotland parishes (at the urging of their Assembly) and the Episcopal meeting houses in Edinburgh.\nThe committee set up by the donors leased \"a house of small rent\" near the College from the University for 19 years. Known, at first, as the Hospital for the Sick Poor, the Physicians' Hospital, or Little House, it was established at the head of Robertson's Close on the site of the building on the corner of South Bridge and Infirmary Street, now marked with a plaque, on 6 August 1729.\n\nA \"gentlewoman\" was engaged as Mistress or House-keeper, and a \"Nurse or Servant\" was hired for the patients, both women to be resident and \"free of the burden of children and the care of a separate family.\" The physicians, who had seen the poor \"gratis\" twice weekly at their college, arranged for one of their number to attend the hospital, to see both inpatients and outpatients. Six Surgeon-Apothecaries (including Alexander Monro) also agreed to attend in turn, and to dispense the medicines prescribed by the physicians from their own shops, also without payment. The first patient, a lady from Caithness suffering from \"chlorosis,\" was discharged recovered after three months. Thirty five patients were admitted in the first year, of whom 19 were cured, 5 recovered, 5 dismissed either as incurable or for irregularities and one died in the hospital (of \"consumption\"). They came from all over Scotland, but mainly from Edinburgh and its environs. Diseases cured included pains, inflammations, agues, ulcers, cancers, palsies, flux, consumption, hysterick disorders and melancholy. A free advice and medicine service for out-patients was very popular, receiving a 1,000 patients by 1754, which presented the hospital with prohibitively high costs and demand. Fundraising began for a new hospital, driven by Monro and Drummond, and the appeal attracted funds from churches throughout Scotland, landed gentry, private individuals, and prominent professionals including physicians, surgeons, merchants and lawyers, as well as donations of labour and building materials.\n\nThe infirmary received a Royal Charter from George II in 1736 which gave it its name of the Royal Infirmary of Edinburgh and commissioned William Adam to design a new hospital on a site close by on what became Infirmary Street. In 1741 the hospital moved the short distance to the not yet completed building which eventually, on its completion in 1745, had 228 beds compared to 4 beds in the Little House. \n\nBy the 1830s the hospital had become short of space and, in 1832, the former Royal High School in nearby High School Yards, built by Alexander Laing in 1777, was converted to a surgical hospital with a new operating theatre built to the east. This was soon found to be inadequate and a new surgical hospital, designed by David Bryce, was built fronting Drummond Street, opening in 1853. The new building was linked to the High School Yards building by an extension to the north.\n\nThe Infirmary Street buildings were demolished in 1884 and replaced with public swimming baths and a school. The ornamental gates and gate piers now front the former surgical hospital on Drummond Street. The four attached Ionic columns on the frontispiece of the hospital were removed and incorporated as a combined column in a monument to the Covenanters who were defeated at the Battle of Rullion Green. This stands outside the entrance to Dreghorn Barracks on Redford Road in the south west of the city.\n\nThe original surgical theatre, which was on the roof of the 1741 building, was re-erected as part of stables in the grounds of Redford House, also on Redford Road. It has since been converted into a house known as Drummond Scrolls taking its name from the large attached carved bracket scrolls, also from the surgical theatre of 1741. The house is category B-listed by Historic Scotland.\n\nSignificant changes came with the introduction of the \"New System\" in 1873. Four years before, Sir Joseph Lister had been appointed as Professor of Surgery to the Royal Infirmary of Edinburgh. Using antiseptics and narcotics he proved to be very successful, thus attracting patients from higher social classes to the hospital. The hospital managers felt the existing nurses were lacking both medical knowledge and appropriate behaviour. They appointed Deputy Surgeon-General Charles Hamilton Fasson as Medical Superintendent. Fasson recruited a group of 17 trained Nightingale Nurses from St. Thomas’s Hospital London. In 1873 Elizabeth Barclay and Angélique Lucille Pringle started building up a system of nursing where the nurses were under the control of the Lady Superintendent of Nurses instead of individual ward doctors. They also introduced a systematic training of nurses, who were, after one year of probation, admitted to the Royal Infirmary of Edinburgh’s Register Book. Accordingly, the Royal Infirmary of Edinburgh had implemented the first Scottish nursing school. Up to the movement into the new buildings 102 probationers had been entered into the Royal Infirmary of Edinburgh’s Registry Book.\n\nIn 1879, at the instruction of the then Lord Provost, Thomas Jamieson Boyd, the infirmary moved to a new location, then in the fresher air of the edge of the city. The site, on Lauriston Place, had been occupied by George Watson's Hospital (a school, known then as a hospital). The school moved a short distance away to the former Merchant Maiden Hospital (another school) in Archibald Place. The original school building, by the same William Adam as the earlier infirmary, was incorporated into the new David Bryce-designed infirmary buildings and the chapel remained in use for the entirety of the infirmary's occupation of the site.\n\nIn the 1920s the hospital needed to expand, and once again George Watson's College was asked to move. An arrangement was reached to acquire the school's site, with the school to remain there until new premises could be built elsewhere. By 1932 the school's new premises in Colinton Road were ready, and the old Archibald Place building was demolished to make way for the Simpson Memorial Pavilion, used primarily as a maternity wing. In 1948, the infirmary was incorporated into the National Health Service (NHS).\n\nIn May 2001, Lothian Health Trust sold the Lauriston Place site for £30 million to Southside Capital Ltd., a consortium comprising Taylor Woodrow, Kilmartin Property Group, and the Bank of Scotland. It has been redeveloped as the Quartermile housing, shopping, leisure and hotel development. Much of the David Bryce infirmary will remain visible, but some infirmary buildings have been demolished. In the build-up to the move to Little France the Royal Charter awarded by George II in 1736 was rediscovered.\n\nA new hospital, sited on a mostly green field site at Little France in the south-east of the city, was procured under a Private Finance Initiative contract in 1998. The new location reflected the need for the hospital to serve not just people living in Edinburgh, but also Midlothian and East Lothian. The new hospital is physically linked to the Chancellor's Building, the main teaching facility for the University of Edinburgh Medical School. The new building, which was designed by Keppie Design and constructed by Balfour Beatty at a cost of £184 million, opened in 2003. The building was built without air conditioning, and portable units are required for the summer months. \n\nThe Little France site initially attracted some controversy in the local media, such as the \"Edinburgh Evening News\", not least because the city's main accident and emergency facilities are some distance from the city centre, and also because the public transport links to the site had been criticised as inadequate. Also Jim and Margaret Cuthbert, economic consultants, unveiled evidence in the Scottish Left Review outlining why the PFI scheme was a poor use of public funds whilst resulting in huge profits for private investors. \n\nIn 2016 the Royal Infirmary of Edinburgh became one of four major trauma centres where specialist services are based as part of a new national major trauma network in Scotland. The Royal Hospital for Sick Children in Edinburgh is being rebuilt on the Little France site beside the Infirmary with an opening scheduled for 2018.\n\n\nThe Royal Infirmary of Edinburgh has often been described in works of fiction, biography and history, and depicted from both the point of view of the sick and those caring for them. The English poet William Ernest Henley e.g. stayed as a patient at the RIE for three years (1873–75). In several poems he portrayed hospital life as well as individual nurses.\n\n\n\n"}
{"id": "3172801", "url": "https://en.wikipedia.org/wiki?curid=3172801", "title": "SWOG", "text": "SWOG\n\nSWOG (formerly the Southwest Oncology Group) is a National Cancer Institute (NCI) supported organization that conducts clinical trials in adult cancers.\n\nComprising more than 4,000 cancer researchers at more than 650 institutions across the United States and Canada, it is one of the largest of the NCI's clinical trial cooperative groups. Member institutions include 24 of the NCI-designated cancer centers, many university medical centers, private institutions, and Community Clinical Oncology Programs.\n\nSWOG is headquartered at the Oregon Health & Science University in Portland, with an operations office in San Antonio, Texas and a statistical center in Seattle, Washington.\nSWOG was created by the NCI in 1956 as the Southwest Cancer Chemotherapy Study Group (SWCCSG) and was headquartered in Houston, Texas. Its primary purpose was to study leukemia, a cancer of the blood which primarily affects children. Then in 1958, the NCI directed the SWCCSG to include the study of adult cancers, and separate administrative divisions were created for pediatric and adult cancers. The group's name was eventually changed to Southwest Oncology Group to reflect its new mission. Although in its early decades most SWOG member institutions were located in the Southwestern United States, the group spread to include members all over the United States and Canada. In 2010, in recognition of this national scope, the group dropped the regional qualifier from its name and adopted the acronym, SWOG, as its official name.\n\nSWOG has all of its protocol-driven cases reviewed at the Quality Assurance Review Center (QARC). As mandated by the National Cancer Institute (NCI), every radiotherapy (RT) department participating in a SWOG study submits their data to QARC for review. QARC is located in Lincoln, Rhode Island and reviews thousands of RT cases per year. The center was founded in 1977 as a not-for-profit health care organization designed to provide quality assurance for CALGB studies. Radiotherapy data from around one-thousand hospitals in both the United States and abroad is reviewed and archived at QARC.\n\nAnother center for quality assurance is the Radiological Physics Center (RPC) in Houston, Texas. The primary responsibility of the RPC is to assure the National Cancer Institute (NCI) and its cooperative groups like SWOG that all participating institutions are following the basic guidelines regarding the physics-related aspects of radiotherapy. Established in 1968, the RPC has consistently received funding from the NCI in order to perform the aforementioned mission.\n\n"}
{"id": "5051569", "url": "https://en.wikipedia.org/wiki?curid=5051569", "title": "Sinus lift", "text": "Sinus lift\n\nMaxillary sinus floor augmentation (also termed sinus lift, sinus graft, sinus augmentation or sinus procedure) is a surgical procedure which aims to increase the amount of bone in the posterior maxilla (upper jaw bone), in the area of the premolar and molar teeth, by lifting the lower Schneiderian membrane (sinus membrane) and placing a bone graft.\n\nWhen a tooth is lost the alveolar process begins to remodel. The vacant tooth socket collapses as it heals leaving an edentulous (toothless) area, termed a \"ridge\". This collapse causes a loss in both height and width of the surrounding bone. In addition, when a maxillary molar or premolar is lost, the floor of the maxillary sinus expands, which further diminishes the thickness of the underlying bone. Overall, this leads to a loss in volume of bone that is available for implantation of dental implants, which rely on osseointegration (bone integration), to replace missing teeth. The goal of the sinus lift is to graft extra bone into the maxillary sinus, so more bone is available to support a dental implant.\n\nWhile there may be a number of reasons for wanting a greater volume of bone in the posterior maxilla, the most common reason in contemporary dental treatment planning is to prepare the site for the future placement of dental implants.\n\nSinus augmentation (sinus lift) is performed when the floor of the sinus is too close to an area where dental implants are to be placed. This procedure is performed to ensure a secure place for the implants while protecting the sinus. Lowering of the sinus can be caused by: Long-term tooth loss without the required treatment, periodontal disease, trauma.\n\nPatients who have the following may be good candidates for sinus augmentation.\n\nIt is not known if using sinus lift techniques is more successful than using short implants for reducing the number of artificial teeth or dental implant failures up to a year after teeth/implant placement.\n\nPrior to undergoing sinus augmentation, diagnostics are run to determine the health of the patient's sinuses. Panoramic radiographs are taken to map out the patient's upper jaw and sinuses. In special instances, a computed tomography or CT scan is taken to measure the sinus's height and width, and to rule out any sinus disease or pathology.\n\nThere are several variations of the sinus lift technique.\n\nThere are multiple ways to perform sinus augmentation. The procedure is performed from inside the patient's mouth where the surgeon makes an incision into the gum, or gingiva. Once the incision is made, the surgeon then pulls back the gum tissue, exposing the lateral boney wall of the sinus. The surgeon then cuts a \"window\" to the sinus, which is exposing the Schneiderian membrane. The membrane is separated from the bone, and bone graft material is placed into the newly created space. The gums are then sutured close and the graft is left to heal for 4–12 months.\n\nThe graft material used can be either an autograft, an allograft, a xenograft, an alloplast (a growth-factor infused collagen matrix), synthetic variants, or combinations thereof. Studies indicate that the mere lifting of the sinus membrane, creation of a void space and blood clot formation might result in new bone owing to the principles of guided bone regeneration. The long-term prognosis for the technique is estimated to 94%.\n\nAs an alternative, sinus augmentation can be performed by a less invasive osteotome technique. There are several variations of this technique and all originate from the original technique of Dr. Tatum, first published by Dr.s Boyne and James in 1980.\n\nDr. Robert B. Summers described a technique that is normally performed when the sinus floor that needs to be lifted is less than 4 mm. This technique is performed by flapping back gum tissue and making a socket in the bone within 1–2 mm short of the sinus membrane. The floor of the sinus is then lifted by tapping the sinus floor with the use of osteotomes. The amount of augmentation achieved with the osteotome technique is usually less than what can be achieved with the lateral window technique.\nA dental implant is normally placed in the socket formed at the time of the sinus lift procedure and left to integrate with bone. Bone integration normally lasts 4 to 8 months. The goal of this procedure is to stimulate bone growth and form a thicker sinus floor, in order to support dental implants for teeth replacement.\n\nDr. Bruschi and Scipioni described a similiar technique (Localized Management of Sinus Floor or L.M.S.F.) that is based on a partial thickness flap procedure. This technique increases the malleability of the crestal bone and uses not the bone directly below the sinus, but rather the bone on the medial wall, and thus can be used in more extreme cases of bone resorption that would normally need to be treated with the lateral wall technique. The healing period is reduced to 1.5 to 3 months. Recently an electrical mallet has been introduced to simplify the application of this and similiar techniques.\n\nThis technique was invented in 1996, by Dr. Leon Chen. There are other methods of this such as the Intralift(tm) from ACTEON that are also clinically proven.\n\nUnlike the traditional methods of sinus lifts, which typically use an osteotomy of the lateral aspect of the maxilla, the Hydraulic Sinus Condensing, or HSC technique, uses an osteotomy on the lateral aspect of the ridge of the maxilla. The HSC technique has shown to have much shorter recovery times than the traditional method. A dental implant is placed at the same time as the sinus lift, also reducing the healing time.\n\nThe gum tissue is flapped to access the underlying bone. An osteotomy (bone removal) is initiated along the ridge. Drilling ceases about 1mm short of the sinus floor. Hydraulic pressure is introduced to the surgical site at this stage, providing just enough force to begin atraumatically dissecting the membrane from the sinus floor. Once the membrane is loosened, the hydraulic pressure is ceased. The membrane at rest is slightly detached from the bone. A bone grafting mixture is then packed through the hole and pushed gently against the membrane. This pressure will slightly raise the sinus, resting it on the newly placed bone.\n\nOnce the initial lift is complete, the surgeon drills a socket in the bone that is sized to the new implant. Bone graft material is added again with pressure to further lift the sinus until it is raised to the proper height for implant placement. The surgeon then places the dental implant into the bone socket and sutures the gums back into place.\n\nOver an 8-year study of 1,557 implants in 1,100 patients using the Hydraulic Sinus Condensing technique, only 8 implants failed, resulting in a 99.99% success rate.\n\nA major risk of a sinus augmentation is that the sinus membrane could be pierced or ripped. Remedies, should this occur, include stitching the tear or placing a patch over it; in some cases, the surgery is stopped altogether and the tear is given time to heal, usually three to six months. Often, the sinus membrane grows back thicker and stronger, making success more likely on the second operation. Although rarely reported, such secondary intervention can also be successful when the primary surgery is limited to elevation of the membrane without the insertion of additional material.\n\nBesides tearing of the sinus membrane, there are other risks involved in sinus augmentation surgery. Most notably, the close relationship of the augmentation site with the sinonasal complex can induce sinusitis, which may chronicize and cause severe symptoms. Sinusitis resulting from maxillary sinus augmentation is considered a Class 1 sinonasal complication according to Felisati classification and should be addressed surgically with a combined endoscopic endonasal and endoral approach. Beside sinusitis, among other procedure related-risks include:\n\n\nIt takes about three to six months for the sinus augmentation bone to become part of the patient's natural sinus floor bone. Up to six months of healing is sometimes left before implants are attempted. However, some surgeons perform both the augmentation and dental implant simultaneously, to avoid the necessity of two surgeries.\n\nThe first maxillary sinus floor augmentation procedure was performed by Oscar Hilt Tatum, Jr. in 1974.\n\nA sinus-lift procedure was first performed by Dr. Hilt Tatum Jr. in 1974 during his period of preparation to begin sinus grafting. The first sinus graft was done by Tatum in February, 1975 in Lee County Hospital in Opelika, Alabama. This was followed by the placement and successful restoration of two endosteal implants. Between 1975–1979, much of the sinus lining elevation was done using inflatable catheters. After this, suitable instruments had been developed to manage the lining elevation from the different anatomical surfaces encountered in sinuses. Tatum first presented the concept at The Alabama Implant Congress in Birmingham, Alabama in 1976 and presented the evolution of technique during multiple podium presentations each year until 1986 when he published an article describing the procedure. Dr. Philip Boyne was introduced to the procedure when he was invited, by Tatum, to be \"The Discusser\" of a presentation on sinus grafting given by Tatum at the annual meeting of The American Academy of Implant Dentistry in 1977 or 1978. Boyne and James authored the first publication on the technique in 1980 when they published case reports of autogenous grafts placed into the sinus and allowed to heal for 6 months, which was followed by the placement of blade implants. This sequence was confirmed by Boyne before the attendees at The Alabama Implant Congress in 1994.\n\nThe slightly higher effectiveness (implant survival) of the lateral sinus lift technique needs to be considered in relation to the substantially higher costs in comparison with the transalveolar sinus lift technique. From a patient perspective the higher invasiviness of the lateral technique will also be an important decision criterion. However, the transalveolar approach is unlikely to be effective in cases of advanced levels of bone reduction at the implant site.\n\n\nEducational Resources\n"}
{"id": "1391881", "url": "https://en.wikipedia.org/wiki?curid=1391881", "title": "Smoking jacket", "text": "Smoking jacket\n\nA smoking jacket is an overgarment designed to be worn while smoking tobacco, usually in the form of pipes and cigars.\n\nThe dinner jacket evolved out of the smoking jacket – essentially a formal wear without tail – following the example of the then Prince, later King Edward VII.\n\nThe classic smoking jacket is a mid thigh-length jacket made from velvet, silk, or both. It has a shawl collar and turn-up cuffs and toggle or button fastenings, or may simply be closed with a tie belt.\n\nIn the 1850s, the \"Gentlemen's Magazine of London\" defined the smoking jacket as a \"kind of short robe de chambre, of velvet, cashmere, plush, merino or printed flannel, lined with bright colours, ornamented with brandebourgs [i.e. Frog (fastening)s], olives or large buttons.\"\n\nIn the 17th century, goods began flowing into Europe from India, Asia and the Americas, bringing in spices, tobacco, coffee, and silks. It became fashionable to be depicted in one's portrait wearing a silk \"robe de chambre\", or dressing gown. One of the earliest mentions of this garment comes from Samuel Pepys, who desired to be depicted in his portrait in a silk gown but could not afford one, so he rented one:\n\nThence home and eat one mouthful, and so to Hale's and there sat until almost quite dark upon working my gowne, which I hired to be drawn (in) it—an Indian gown, and I do see all the reason to expect a most excellent picture of it. —\"Diary\", 30 March 1666\n\nThe short smoking jacket soon evolved from these silk garments. When the Crimean War during the 1850s popularised Turkish tobacco in Britain, smoking gained in popularity. After dinner, a gentleman might put on a smoking jacket and retreat to a smoking room (akin to a den or mancave). The jacket was intended to absorb the smoke from his cigar or pipe and protect his clothing from falling ash.\n\nThe smoking jacket remained a popular accessory into the 20th century. An editorial in \"The Washington Post\" in 1902 gave the opinion that the smoking jacket was \"synonymous with comfort\", while a Pennsylvania newspaper opined in 1908 that it would be \"putting it mildly to say that a new House Coat or Smoking Jacket will give any man reason for elation\".\n\nFamous wearers included Jon Pertwee, Cary Grant, Fred Astaire (who was buried in a smoking jacket), Frank Sinatra, Dean Martin and Hugh Hefner.\n\nSmoking jackets have declined in popularity since the 1950s, though a minority of wearers still exists; \"Playboy\" mogul Hugh Hefner was a notable example. In its January/February 1999 issue, \"Cigar Aficionado\" stated that it was time the smoking jacket be brought back, perhaps as an \"alternative type of formalwear\".\n\nIn Bulgarian, Czech, Danish, Dutch, French, German, Greek, Hungarian, Icelandic, Italian, Polish, Portuguese, Russian, Spanish, Swedish, Turkish, and other European languages, the term \"smoking\" indicates a tuxedo; see Le Smoking.\n\n"}
{"id": "47533598", "url": "https://en.wikipedia.org/wiki?curid=47533598", "title": "Solyman Brown", "text": "Solyman Brown\n\nSolyman Brown (November 17, 1790 – 1876), was a dentist in 19th century was known for his role in creating the first dental school, the first US national dental society and the first US dental journal. He was known as a poet of dentistry.\n\nBrown was born in Litchfield, Connecticut. He studied at Yale College during his college years. He became a minister and a teacher at Litchfield Female Academy. He eventually moved to New York City in 1822. He embraced the doctrines of Emanuel Swedenborg and became a regular preacher at the New Jerusalem Church. In 1832, Solyman decided to practice dentistry after meeting Eleazer Parmly. He married Elizabeth Butler Brown in 1834 and settled in NYC.\n\nBrown worked with well renowned dentists such as Chapin A. Harris and Norman William Kingsley. He was known for his role in establishing the \"American Association of Dental Surgeons\", and being the editor in chief of \"American Journal and Library of Dental Science\".\n\nHe died in 1876 in Dodge Center, Minnesota.\n\nBrown is mostly known for his poem \"Dentologia\" which he wrote in 1833. The poem talked about diseases of teeth and their remedies. Later in life he was involved in fabricating and selling false teeth.\n"}
{"id": "28243188", "url": "https://en.wikipedia.org/wiki?curid=28243188", "title": "SpecTrek", "text": "SpecTrek\n\nSpecTrek is an augmented reality ghost hunting game. The game won second prize in the \"Android Developer Challenge II\" lifestyle category.\n\nSpecTrek was designed to have the user work-out whilst playing the game, the tag line for the game is \"protect the world, stay in shape\". There are three default games to play, short which lasts 15 minutes, medium which lasts for 30 minutes, and long which lasts for 60 minutes. SpecTrek projects ghosts at various locations on a Google map in either a predetermined search radius or a user defined search radius. To play the user must walk to these ghosts, if within range the user can scan and find out what kind of ghost is nearby as well as how far said ghost is from their current position. If the user is unable to reach a ghost, a horn may be blown which makes all nearby ghosts flee and possibly stop within reach of another accessible location.\n\nThe user catches ghosts by tilting their phone to the \"camera-position\". Through the camera the user can scan the ghosts, see the ghosts in augmented reality and of course catch the ghosts.\n"}
{"id": "49364", "url": "https://en.wikipedia.org/wiki?curid=49364", "title": "Turner syndrome", "text": "Turner syndrome\n\nTurner syndrome (TS), also known as 45,X or 45,X0, is a condition in which a female is partly or completely missing an X chromosome. Signs and symptoms vary among those affected. Often, a short and webbed neck, low-set ears, low hairline at the back of the neck, short stature, and swollen hands and feet are seen at birth. Typically, they develop menstrual periods and breasts only with hormone treatment, and are unable to have children without reproductive technology. Heart defects, diabetes, and low thyroid hormone occur more frequently. Most people with TS have normal intelligence. Many have troubles with spatial visualization that may be needed for mathematics. Vision and hearing problems occur more often.\nTurner syndrome is not usually inherited from a person's parents. No environmental risks are known, and the mother's age does not play a role. Turner syndrome is due to a chromosomal abnormality in which all or part of one of the X chromosomes is missing or altered. While most people have 46 chromosomes, people with TS usually have 45. The chromosomal abnormality may be present in just some cells in which case it is known as TS with mosaicism. In these cases, the symptoms are usually fewer and possibly none occur at all. Diagnosis is based on physical signs and genetic testing.\nNo cure for Turner syndrome is known. Treatment may help with symptoms. Human growth hormone injections during childhood may increase adult height. Estrogen replacement therapy can promote development of the breasts and hips. Medical care is often required to manage other health problems with which TS is associated.\nTurner syndrome occurs in between one in 2,000 and one in 5,000 females at birth. All regions of the world and cultures are affected about equally. Generally people with TS have a shorter life expectancy, mostly due to heart problems and diabetes. Henry Turner first described the condition in 1938. In 1964, it was determined to be due to a chromosomal abnormality.\n\nOf the following common symptoms of Turner syndrome, an individual may have any combination of symptoms and is unlikely to have all symptoms.\n\nOther features may include a small lower jaw (micrognathia), cubitus valgus, soft upturned nails, palmar crease, and drooping eyelids. Less common are pigmented moles, hearing loss, and a high-arch palate (narrow maxilla). Turner syndrome manifests itself differently in each female affected by the condition; therefore, no two individuals share the same features.\n\nWhile most of the physical findings are harmless, significant medical problems can be associated with the syndrome. Most of these significant conditions are treatable with surgery and medication.\n\nDespite the excellent postnatal prognosis, 99% of Turner syndrome conceptions are thought to end in miscarriage or stillbirth, and as many as 15% of all spontaneous abortions have the 45,X karyotype. Among cases that are detected by routine amniocentesis or chorionic villus sampling, one study found that the prevalence of Turner syndrome among tested pregnancies was 5.58 and 13.3 times higher, respectively, than among live neonates in a similar population.\n\nThe rate of cardiovascular malformations among patients with Turner syndrome ranges from 17% to 45%. The variations found in the different studies are mainly attributable to variations in noninvasive methods used for screening and the types of lesions that they can characterize. However, it could be simply attributable to the small number of subjects in most studies.\n\nDifferent karyotypes may have differing rates of cardiovascular malformations. Two studies found a rate of cardiovascular malformations of 30% and 38% in a group of pure 45,X monosomy. Considering other karyotype groups, though, they reported a prevalence of 24.3% and 11% in people with mosaic X monosomy, and a rate of 11% in people with X chromosomal structural abnormalities.\n\nThe higher rate in the group of pure 45,X monosomy is primarily due to a difference in the rate of aortic valve abnormalities and coarctation of the aorta, the two most common cardiovascular malformations.\n\nThe most commonly observed are congenital obstructive lesions of the left side of the heart, leading to reduced flow on this side of the heart. This includes bicuspid aortic valve and coarctation (narrowing) of the aorta. More than 50% of the cardiovascular malformations of individuals with Turner syndrome in one study were bicuspid aortic valves or coarctation of the aorta (usually preductal), alone or in combination.\n\nOther congenital cardiovascular malformations, such as partial anomalous venous drainage and aortic valve stenosis or aortic regurgitation, are also more common in Turner syndrome than in the general population. Hypoplastic left heart syndrome represents the most severe reduction in left-sided structures.\n\nUp to 15% of adults with Turner syndrome have bicuspid aortic valves, meaning only two, instead of three, parts to the valves in the main blood vessel leading from the heart are present. Since bicuspid valves are capable of regulating blood flow properly, this condition may go undetected without regular screening. However, bicuspid valves are more likely to deteriorate and later fail. Calcification also occurs in the valves, which may lead to a progressive valvular dysfunction as evidenced by aortic stenosis or regurgitation.\n\nWith a rate from 12.5% to 17.5% (Dawson-Falk et al., 1992), bicuspid aortic valve is the most common congenital malformation affecting the heart in this syndrome. It is usually isolated, but it may be seen in combination with other anomalies, particularly coarctation of the aorta.\n\nBetween 5% and 10% of those born with Turner syndrome have coarctation of the aorta, a congenital narrowing of the descending aorta, usually just distal to the origin of the left subclavian artery (the artery that branches off the arch of the aorta to the left arm) and opposite to the ductus arteriosus (termed \"juxtaductal\"). Estimates of the prevalence of this malformation in patients with Turner syndrome range from 6.9 to 12.5%. A coarctation of the aorta in a female is suggestive of Turner syndrome and suggests the need for further tests, such as a karyotype.\n\nThis abnormality is a relatively rare congenital heart disease in the general population. The prevalence of this abnormality also is low (around 2.9%) in Turner syndrome. However, its relative risk is 320 in comparison with the general population. Strangely, Turner syndrome seems to be associated with unusual forms of partial anomalous venous drainage.\n\nIn a patient with Turner syndrome, these left-sided cardiovascular malformations can result in an increased susceptibility to bacterial endocarditis. Therefore, prophylactic antibiotics should be considered when procedures with a high risk of endocarditis are performed, such as dental cleaning.\n\nTurner syndrome is often associated with persistent hypertension, sometimes in childhood. In the majority of Turner syndrome patients with hypertension, no specific cause is known. In the remainder, it is usually associated with cardiovascular or kidney abnormalities, including coarctation of the aorta.\n\nTwo studies have suggested aortic dilatation in Turner syndrome, typically involving the root of the ascending aorta and occasionally extending through the aortic arch to the descending aorta, or at the site of previous coarctation of the aorta repair.\n\n\nWhether aortic root diameters that are relatively large for body surface area but still well within normal limits imply a risk for progressive dilatation remains unproven.\n\nThe prevalence of aortic root dilatation ranges from 8.8 to 42% in patients with Turner syndrome. Even if not every aortic root dilatation necessarily goes on to an aortic dissection (circumferential or transverse tear of the intima), complications such as dissection, aortic rupture resulting in death may occur. The natural history of aortic root dilatation is still unknown, but it is linked to aortic dissection and rupture, which has a high mortality rate.\n\nAortic dissection affects 1 to 2% of patients with Turner syndrome. As a result, any aortic root dilatation should be seriously taken into account, as it could become a fatal aortic dissection. Routine surveillance is highly recommended.\n\nCardiovascular malformations (typically bicuspid aortic valve, coarctation of the aorta, and some other left-sided cardiac malformations) and hypertension predispose to aortic dilatation and dissection in the general population. Indeed, these same risk factors are found in more than 90% of patients with Turner syndrome who develop aortic dilatation. Only a small number of patients (around 10%) have no apparent predisposing risk factors. The risk of hypertension is increased three-fold in patients with Turner syndrome. Because of its relation to aortic dissection, blood pressure must be regularly monitored and hypertension should be treated aggressively with an aim to keep blood pressure below 140/80 mmHg. As with the other cardiovascular malformations, complications of aortic dilatation is commonly associated with 45,X karyotype.\n\nThe exact role that these risk factors play in the process leading to rupture is unclear. Aortic root dilatation is thought to be due to a mesenchymal defect as pathological evidence of cystic medial necrosis has been found by several studies. The association between a similar defect and aortic dilatation is well established in such conditions such as Marfan syndrome. Also, abnormalities in other mesenchymal tissues (bone matrix and lymphatic vessels) suggests a similar primary mesenchymal defect in patients with Turner syndrome. However, no evidence suggests that patients with Turner syndrome have a significantly higher risk of aortic dilatation and dissection in absence of predisposing factors. So, the risk of aortic dissection in Turner syndrome appears to be a consequence of structural cardiovascular malformations and hemodynamic risk factors rather than a reflection of an inherent abnormality in connective tissue. The natural history of aortic root dilatation is unknown, but because of its lethal potential, this aortic abnormality needs to be carefully followed.\n\nNormal skeletal development is inhibited due to a large variety of factors, mostly hormonal. The average height of a woman with Turner syndrome, in the absence of growth hormone treatment, is (140 cm). Patients with Turner's mosaicism can reach normal average height.\n\nThe fourth metacarpal bone (fourth toe and ring finger) may be unusually short, as may the fifth.\n\nDue to inadequate production of estrogen, many of those with Turner syndrome develop osteoporosis. This can decrease height further, as well as exacerbate the curvature of the spine, possibly leading to scoliosis. It is also associated with an increased risk of bone fractures.\n\nAbout one-third of all women with Turner syndrome have one of three kidney abnormalities:\n\n\nSome of these conditions can be corrected surgically. Even with these abnormalities, the kidneys of most women with Turner syndrome function normally. However, as noted above, kidney problems may be associated with hypertension.\n\nApproximately one-third of all women with Turner syndrome have a thyroid disorder. Usually it is hypothyroidism, specifically Hashimoto's thyroiditis. If detected, it can be easily treated with thyroid hormone supplements.\n\nWomen with Turner syndrome are at a moderately increased risk of developing type 1 diabetes in childhood and a substantially increased risk of developing type 2 diabetes by adult years. The risk of developing type 2 diabetes can be substantially reduced by maintaining a healthy weight.\n\nTurner syndrome does not typically cause intellectual disability or impair cognition. However, learning difficulties are common among women with Turner syndrome, particularly a specific difficulty in perceiving spatial relationships, such as nonverbal learning disorder. This may also manifest itself as a difficulty with motor control or with mathematics. While it is not correctable, in most cases it does not cause difficulty in daily living. Most Turner syndrome patients are employed as adults and lead productive lives.\n\nAlso, a rare variety of Turner syndrome, known as \"Ring-X Turner syndrome\", has about a 60% association with intellectual disability. This variety accounts for around 2–4% of all Turner syndrome cases.\n\nWomen with Turner syndrome may experience adverse psychosocial outcomes. Research shows a possible association between age at diagnosis and increased substance use and depressive symptoms. \n\nWomen with Turner syndrome are almost universally infertile. While some women with Turner syndrome have successfully become pregnant and carried their pregnancies to term, this is very rare and is generally limited to those women whose karyotypes are not 45,X. Even when such pregnancies do occur, there is a higher than average risk of miscarriage or birth defects, including Turner syndrome or Down syndrome. Some women with Turner syndrome who are unable to conceive without medical intervention may be able to use IVF or other fertility treatments.\n\nUsually, estrogen replacement therapy is used to spur the growth of secondary sexual characteristics at the time when puberty should onset. While very few women with Turner syndrome menstruate spontaneously, estrogen therapy requires a regular shedding of the uterine lining (\"withdrawal bleeding\") to prevent its overgrowth. Withdrawal bleeding can be induced monthly, like menstruation, or less often, usually every three months, if the patient desires. Estrogen therapy does not make a woman with nonfunctional ovaries fertile, but it plays an important role in assisted reproduction; the health of the uterus must be maintained with estrogen if an eligible woman with Turner Syndrome wishes to use IVF (using donated oocytes).\n\nEspecially in mosaic cases of Turner syndrome that contains Y-chromosome (e.g. 45,X/46,XY) due to the risk of development of ovarian malignancy (most common is gonadoblastoma) gonadectomy is recommended. Turner syndrome is characterized by primary amenorrhoea, premature ovarian failure (hypergonadotropic hypogonadism), streak gonads and infertility (however, technology (especially oocyte donation) provides the opportunity of pregnancy in these patients). Failure to develop secondary sex characteristics (sexual infantilism) is typical.\n\nAs more women with Turner syndrome complete pregnancy thanks to modern techniques to treat infertility, it has to be noted that pregnancy may be a risk of cardiovascular complications for the mother. Indeed, several studies had suggested an increased risk for aortic dissection in pregnancy. The influence of estrogen has been examined but remains unclear. It seems that the high risk of aortic dissection during pregnancy in women with Turner syndrome may be due to the increased hemodynamic load rather than the high estrogen rate. Of course, these findings are important and need to be remembered while following a pregnant patient with Turner syndrome.\n\nTurner syndrome is caused by the absence of one complete or partial copy of the X chromosome in some or all the cells. The abnormal cells may have only one X (monosomy) (45,X) or they may be affected by one of several types of partial monosomy like a deletion of the short p arm of one X chromosome (46,X,del(Xp)) or the presence of an isochromosome with two q arms (46,X,i(Xq)) Turner syndrome has distinct features due to the lack of pseudoautosomal regions, which are typically spared from X-inactivation. In mosaic individuals, cells with X monosomy (45,X) may occur along with cells that are normal (46,XX), cells that have partial monosomies, or cells that have a Y chromosome (46,XY). The presence of mosaicism is estimated to be relatively common in affected individuals (67–90%).\n\nIn the majority of cases where monosomy occurs, the X chromosome comes from the mother. This may be due to a nondisjunction in the father. Meiotic errors that lead to the production of X with p arm deletions or abnormal Y chromosomes are also mostly found in the father. Isochromosome X or ring chromosome X on the other hand are formed equally often by both parents. Overall, the functional X chromosome usually comes from the mother.\n\nIn most cases, Turner syndrome is a sporadic event, and for the parents of an individual with Turner syndrome the risk of recurrence is not increased for subsequent pregnancies. Rare exceptions may include the presence of a balanced translocation of the X chromosome in a parent, or where the mother has 45,X mosaicism restricted to her germ cells.\n\nTurner syndrome may be diagnosed by amniocentesis or chorionic villus sampling during pregnancy.\n\nUsually, fetuses with Turner syndrome can be identified by abnormal ultrasound findings (\"i.e.\", heart defect, kidney abnormality, cystic hygroma, ascites). In a study of 19 European registries, 67.2% of prenatally diagnosed cases of Turner syndrome were detected by abnormalities on ultrasound. 69.1% of cases had one anomaly present, and 30.9% had two or more anomalies.\n\nAn increased risk of Turner syndrome may also be indicated by abnormal triple or quadruple maternal serum screen. The fetuses diagnosed through positive maternal serum screening are more often found to have a mosaic karyotype than those diagnosed based on ultrasonographic abnormalities, and\nconversely, those with mosaic karyotypes are less likely to have associated ultrasound abnormalities.\n\nTurner syndrome can be diagnosed postnatally at any age. Often, it is diagnosed at birth due to heart problems, an unusually wide neck or swelling of the hands and feet. However, it is also common for it to go undiagnosed for several years, typically until the girl reaches the age of puberty/adolescence and she fails to develop properly (the changes associated with puberty do not occur). In childhood, a short stature can be indicative of Turner syndrome.\n\nA test called a karyotype, also known as a chromosome analysis, analyzes the chromosomal composition of the individual. This is the test of choice to diagnose Turner syndrome.\n\nAs a chromosomal condition, there is no cure for Turner syndrome. However, much can be done to minimize the symptoms. For example:\n\n\n\nTurner syndrome occurs in between one in 2000 and one in 5000 females at birth.\n\nApproximately 99 percent of fetuses with Turner syndrome spontaneously terminate during the first trimester. Turner syndrome accounts for about 10 percent of the total number of spontaneous abortions in the United States.\n\nThe syndrome is named after Henry Turner, an endocrinologist from Illinois, who described it in 1938. In Europe, it is often called Ullrich–Turner syndrome or even Bonnevie–Ullrich–Turner syndrome to acknowledge that earlier cases had also been described by European doctors.\n\nThe first published report of a female with a 45,X karyotype was in 1959 by Dr. Charles Ford and colleagues in Harwell near Oxford, and Guy's Hospital in London. It was found in a 14-year-old girl with signs of Turner syndrome.\n\n\n"}
{"id": "30744914", "url": "https://en.wikipedia.org/wiki?curid=30744914", "title": "Ultrasonix Medical Corporation", "text": "Ultrasonix Medical Corporation\n\nUltrasonix Medical Corporation is a sonography equipment company specializing in high quality medical ultrasound equipment. Founded in 2000, the company developed the first Open Research Ultrasound Platform called OpenSonix. In 2009, Ultrasonix introduced the SonixTouch Ultrasound System, a leading system configurable for emerging Ultrasound Applications.\n\n"}
{"id": "21567206", "url": "https://en.wikipedia.org/wiki?curid=21567206", "title": "William Stephens (Dean of Winchester)", "text": "William Stephens (Dean of Winchester)\n\nThe Very Reverend William Richard Wood Stephens DD, FSA was Dean of Winchester in the late 19th and early 20th centuries.<br> He was educated at Balliol College, Oxford and graduated in 1862. Ordained in 1865, he began his career with a curacy in Staines. In 1866 he became the curate of Purley, Berkshire. On 31 August 1869 he married Charlotte Jane Hook, the youngest daughter of Walter Farquhar Hook, the dean of Chichester. On the recommendation of the dean of Chichester in 1870, he became Vicar of Mid Lavant, a Lecturer at Chichester Theological College and Rector of Woolbeding Then in 1895 he was elevated to the Deanery at Winchester, a post he held until his death.\n\nStephens was known for his philanthropy, spending his own money to have the church at Mid Lavant restored. He provided funds for the rebuilding of the chancel at Woolbeding and contributed to the repairs of the roof at Winchester Cathedral. He also spent a lot of time showing visitors around the cathedral and explaining its history. In 1895 he was recognised for his interest in history when he was elected FSA.\n\nIn 1902 Stephens attended a mayoral banquet, in Winchester, where he consumed some oysters. Unfortunately the beds, in Emsworth where the oysters were sourced, had been contaminated with raw sewage. Consequently, many of the guests, including Stephens, contracted food poisoning. His death in Winchester deanery, on 22 December 1902, about six weeks after the banquet, was attributed to eating Emsworth oysters. He was buried in the graveyard of Winchester Cathedral on 27 December 1902.\n\n"}
