{"id": "58501382", "url": "https://en.wikipedia.org/wiki?curid=58501382", "title": "2018 Australian strawberry contamination", "text": "2018 Australian strawberry contamination\n\nAn ongoing food safety crisis in which numerous punnets of strawberries grown in Queensland and Western Australia were found to be contaminated with needles began in September 2018. Queensland Police reported that by November 2018, there had been 186 reports of contamination nationally.\n\nOn 9 September 2018, several days prior to any official announcement of contamination, a Facebook user posted a warning about Berry Obsession strawberries purchased from Woolworths Strathpine Centre in Moreton Bay, north of Brisbane. The user reported that his friend had swallowed part of a needle and was in the emergency department at hospital. A second victim had called Woolworths on 11 September. The affected strawberries were not withdrawn and the contamination first publicly reported until 12 September. In the following days, dozens of contaminated punnets of strawberries grown in Queensland and Western Australia were discovered in NSW, South Australia, Victoria and Tasmania.\n\nSome cases are believed to be hoaxes. A South Australian man was arrested on September 21 after faking a contamination, and will face court in November.\n\nAs of 16 September 2018, the following brands have been identified by authorities as being involved in the contamination:\n\nStrawberries grown in Queensland:\n\nStrawberries grown in Western Australia:\n\nPolice and state health authorities have recommend consumers either dispose of affected brands or return to place of purchase. They also recommend consumers cut up other brands before eating. Queensland chief health officer Dr Jeannette Young stated that \"If they do have any strawberries it would be safest to dispose of them.\"\n\nBoth Coles and Aldi removed all strawberries from their shelves, but planned to restock after 18 September. Woolworths had removed affected brands only. Woolworths later removed sewing needles from sale as a temporary measure. On 23 September, needles were found in \"Australian Choice\" brand strawberries sold in Auckland, New Zealand.\n\nThe Queensland Strawberry Growers Association initially stated they had \"reason to suspect\" a \"disgruntled\" former packing employee was responsible for the contamination. Queensland Police Acting Chief Superintendent Terry Lawrence subsequently cast doubt upon that theory, stating \"This was an earlier comment by the Strawberry Growers Association, it's something we don't subscribe to\". Adrian Schultz, the vice-president of the Queensland Strawberry Growers Association described the contamination as an act of \"commercial terrorism\". Tony Holl, a strawberry grower from Western Australia told the ABC that he believed someone had a \"vendetta\" against the strawberry industry, suggesting otherwise the contamination could be a \"terrorist act\".\n\nThe ABC reported on Thursday that \"Police believe they have contained the threat and assured consumers would be able to safely buy strawberries again from Thursday, when stock is replaced.\"\n\nAs of 19 September, Queensland Police have more than 100 officers, including 60 detectives, working on the investigation into the contamination. However, by 15 October Queensland Police scaled this back to one full time detective amid a lack of clear leads.\n\nOn 15 September 2018, Queensland Premier Annastacia Palaszczuk announced a $100,000 reward for information leading to the arrest and conviction of anyone responsible for the sabotage. On 18 September, Palaszczuk announced a $1 million assistance package for the strawberry industry in the state, telling State Parliament that \"This past week, Queensland has been the victim of an ugly, calculated and despicable crime.\" Also on 18 September, Premier of Western Australia Mark McGowan announced a $100,000 reward for information leading to a prosecution. \n\nThe two largest food retailers in New Zealand, Woolworths NZ and Foodstuffs announced they would be removing Australian-grown strawberries from their shelves. \n\nOn 18 September 2018, Senator Bridget McKenzie, the Minister for Regional Services, issued a media statement, describing the contamination as \"deliberate sabotage\" and urging consumers to \"exercise caution and cut up their fruit before consumption.\"\n\nAs of 17 September 2018, a consumer-level food recall of any affected brands has yet to be issued. Instead, only a \"trade recall\" has been issued. Food Standards Australia New Zealand describes a trade recall as a recall \"conducted when the food has not been sold directly to consumers. It involves recovery of the product from distribution centres and wholesalers\". Professor Melissa Fitzgerald, a food safety expert at the University of Queensland, told the ABC she was \"surprised\" there had not been a consumer-level recall given the products had been sold to individual customers. The recall was not mentioned on supermarket or government food safety websites—something Professor Fitzgerald would have expected. Professor Fitzgerald said \"I would expect people to go to the websites for information...People may be quite surprised not to find any information on the websites where they normally would.\" Professor Fitzgerald criticised the delay between the first incident on 9 September and the public warning on 12 September.\n\nQueensland Strawberry Industry Development Officer Jennifer Rowling accused \"some authoritative spokespeople\" of mishandling the response to the incident. She also criticised a \"sometimes hysterical media\" and accused them of costing agricultural businesses millions of dollars. Rowling insisted only three brands had been affected and said \"All other reported cases have either been copycats or unsubstantiated claims.\"\n\nOn 15 September, Suncoast Harvest farm of the Sunshine Coast announced on Facebook that they were ceasing growing strawberries for the remainder of the year, resulting in job losses for 100 workers. Some growers started ordering and installing metal detectors to protect their strawberries from contamination.\n\nSome farms have had to dispose of strawberries in response to the crisis. Donnybrook Berries of Queensland, one of the brands affected, dumped truckloads of berries, sharing the resulting video footage which went viral with over a million views in a day. One Queensland farm burned off 500,000 strawberry plants deemed unsellable, as it was cheaper than harvesting.\n\nOn 11 November 2018, My Ut Trinh, a 50-year-old farm supervisor, was arrested in Brisbane and charged with seven counts of contaminating goods, relating to one of the initial cases of contamination involving the Berry Licious brand. Ms Trinh, who was born in Vietnam but came to Australia as a refugee 20 years ago by boat, worked at the Berrylicious/Berry Obsession fruit farm north of Brisbane as a picking supervisor.\n\nOn 17 September 2018, a 62-year-old woman suffering from a mental illness was cautioned after she allegedly contaminated a banana with a metal object at a supermarket in Maryborough, Queensland. Queensland Police stated the incident was not linked to the contamination crisis.\n\nOn 18 September 2018, New South Wales Police announced needles had been discovered in bananas and apples in two separate incidents in the Sydney area. Detective superintendent Daniel Doherty reminded the public of the importance to report incidents of contamination and \"to be vigilant and exercise caution when buying strawberries and other fruit in NSW.\" Doherty confimed police were investigating \"more than 20 incidents in New South Wales, but nationally, the number is much higher.\" The apple was part of a pack of six, purchased from a Woolworths at The Ponds.\n\nOn 20 September 2018, New South Wales Police reported that a young boy had been arrested after admitting to hiding sewing needles in strawberries as part of what police consider to be \"a prank\". The current penalty in New South Wales for the deliberate contamination of food is up to 10 years in jail, however Australia's Prime Minister Scott Morrison has stated that \"the government plans to increase the maximum jail term to 15 years.\n\nAlso on 20 September, a customer in West Gosford on the New South Wales Central Coast discovered a needle inside a mango.\n\nOn 11 November 2018, Newshub announced that a 50 year old woman in Auckland, New Zealand, was arrested for contaminating strawberries with needles. In the wake of the needle crisis, Foodstuffs NZ made the decision to halt the distribution of Australian strawberries in all their stores including New World, Pak'nSave and Four Square. Countdown said that strawberries grown in New Zealand had not been affected, but it was in contact with New Zealand and Australian authorities.\n\n"}
{"id": "51021305", "url": "https://en.wikipedia.org/wiki?curid=51021305", "title": "Alexander Gillespie", "text": "Alexander Gillespie\n\nAlexander Gillespie FRSE, FRCSEd (21 March 1776 – 1 September 1859) was a Scottish surgeon. He is one of the few persons to have served two non-consecutive periods as President of the Royal College of Surgeons of Edinburgh. He was President from 1810 to 1812 and again from 1818 to 1820.\n\nAlexander Gillespie was born in Ayr, the son of Dr Thomas Gillespie, an Ayrshire physician, and his wife Jean Gillespie (née Thomson). He became a Licentiate of the Royal College of Surgeons of Edinburgh in 1794 and a Fellow in 1803. \n\nHe obtained the degree of MD from the University of St Andrews Medical School in 1822, some 28 years after he began to practise medicine. This was common practice at this time with the St Andrews degrees being conferred on the basis of a written testimonial and payment of a fee, without the need for the candidate visiting St Andrews \nLike many practitioners of his day he combined surgery with general practice. He had appointments as surgeon to a number of institutions including the Lock Hospital in Surgeons' Square, the Edinburgh Lunatic Asylum and Gillespie's Hospital for the Elderly. \nHe was surgeon to Donaldson's Hospital, a school which had family connections. His sister Jane Gillespie (1770–1828) married James Donaldson (1751–1830), who bequeathed a large part of his estate to establish Donaldson's Hospital, founded to maintain and educate poor children. There is an insight into Alexander Gillespie's practice in his son's obituary which observes that Alexander Gillespie 'worked his easy largely aristocratic practice with a light hand...'\n\nIn 1806 he is listed as a member of the Royal Highland Agricultural Society. In 1812 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were Thomas Allan, James Russell and Ninian Imrie.\n\nIn 1818 he was living at 30 York Place in Edinburgh's New Town, a handsome Georgian terraced townhouse. Post Office Directories continue to show him here for many years but his given occupation changes from surgeon to doctor in 1834, suggesting that he had given up surgery in favour of general practice.\n\nHe died in Edinburgh on 1 September 1859 at 45 Castle Street and is buried in Dalry Cemetery.\n\nOn 1 June 1812 he had married Eliza Mary Shirriff. \nTheir son, James Donaldson Gillespie (1824-1891) graduated MD from the University of Edinburgh, gained the FRCSEd diploma, became a surgeon in the Royal Infirmary of Edinburgh and was President of the Royal College of Surgeons of Edinburgh between 1869 and 1871.\n"}
{"id": "25861838", "url": "https://en.wikipedia.org/wiki?curid=25861838", "title": "Association of Mental Health Providers", "text": "Association of Mental Health Providers\n\nThe Association of Mental Health Providers, known until May 2017 as Mental Health Providers Forum (MHPF), is a registered charity based in London and the representative body for voluntary and community sector mental health organisations in England and Wales, working nationally and regionally to influence practice and policy. It aims to improve the range and quality of mental health services by increasing the involvement of the voluntary sector in delivering them, working in partnership with the wider sector and government agencies. Specific projects include the promotion of innovation in the sector, evidencing best practice to achieve the best outcomes for individuals and supporting recovery.\n\nThe Association leads a collaboration of voluntary mental health organisations including the Centre for Mental Health, National Survivor User Network for Mental Health (NSUN), the Mental Health Foundation, Mind and Rethink Mental Illness in the VCSE Health and Wellbeing Alliance with the Department of Health, NHS England and Public Health England.\n\nThe Association's membership consists of voluntary sector organisations providing mental health services in England. The current Chief Executive (since April 2012) is Kathy Roberts, who has a background in health and social care and the voluntary and community sector. She succeeded Dr Ian McPherson, previously Director of the National Mental Health Development Unit. The first Chief Executive of the Forum was Judy Weleminsky who led the organisation from December 2005 to January 2011.\n\nThe Association has increased its work to bring the views of mental health service providers into national policy and strategy discussions, working closely with members to inform and implement the 2011 \"No health without mental health\" cross-government mental health strategy. An area of focus from 2014 has been housing provision for people with mental health needs. In September 2014 AMHP published a report outlining successful housing models and a national forum is now established.\n\nIn 2008 The Association, with Triangle Consulting, worked with service users and The Association's members to develop the Mental Health Recovery Star, which was recommended by the Department of Health New Horizons programme and has been developed for use in adult mental health services. The Recovery Star is a key-working tool that enables staff to support individuals they work with to understand their recovery and plot their progress. It is also an outcomes tool that enables organisations to measure and assess the effectiveness of the services they deliver. It is now being used by many mental health trusts in England.\n\n\nOther UK mental health charities:\n\nGeneral:\n\n"}
{"id": "25119384", "url": "https://en.wikipedia.org/wiki?curid=25119384", "title": "Association of Nurses in AIDS Care", "text": "Association of Nurses in AIDS Care\n\nAssociation of Nurses in AIDS Care (ANAC) is a national nursing organization in the United States which specializes in the care of individuals infected with HIV. It is based in Akron, Ohio and was founded in 1987.\n\nIts official journal is the \"Journal of the Association of Nurses in AIDS Care\" (JANAC), which is published on behalf of ANAC by Elsevier.\n"}
{"id": "3617797", "url": "https://en.wikipedia.org/wiki?curid=3617797", "title": "Bratsk Reservoir", "text": "Bratsk Reservoir\n\nBratsk Reservoir (, \"Bratskoye Reservoir\") is a reservoir on the Angara River, located in Irkutsk Oblast, Russia. It is named after the city of Bratsk, the largest city adjacent to the reservoir. It has a surface area of and a maximum volume of 169.27 × 10 litres (37.2 × 10 gallons).\n\nThe concrete dam of the Bratsk hydroelectric plant was completed in 1967. It is high and long. The Baikal Amur Mainline railroad runs along the top of the dam. At the time of its inauguration, the reservoir was the largest artificial lake in the world. Its electrical power capacity is 4,500 MW.\n\nThe epic construction of the Bratsk Dam is the subject of a large eponymous poem by Yevgeny Yevtushenko.\nMuch later (1976), the impact of the reservoir construction on the life of the villagers upstream, many of whom had to be relocated from the flooded areas, or lost some the best lands of their collective farms, became the motive of Valentin Rasputin's novel \"Farewell to Matyora\".\n"}
{"id": "26775452", "url": "https://en.wikipedia.org/wiki?curid=26775452", "title": "Canadian AIDS Society", "text": "Canadian AIDS Society\n\nThe Canadian AIDS Society (CAS) is a national coalition of community-based AIDS service organizations across Canada. Registered as a charity since 1988, CAS has a mandate to improve the response to HIV/AIDS in Canada across all sectors of society, and to support people and communities living with HIV/AIDS.\n\nCAS was established after the first two national conferences on HIV/AIDS in Montreal (1985) and Toronto (1986) to act as a national umbrella organization for AIDS service organizations in Canada. The original founding member organizations were:\n\n\nBefore CAS became a registered charitable organization with paid staff in Ottawa in 1988, a voluntary secretariat steered the organization operating out of the AIDS Network of Edmonton office.\n\nCAS's objective is to strengthen the response to HIV/AIDS in Canada and enrich the lives of people and communities living with, and affected by, HIV/AIDS.\n\nCAS represents AIDS service organizations across Canada in almost every Province. A full list of member organizations is available on CAS's website.\n\nCAS collaborates with other nationally-focused organizations the address HIV/AIDS in Canada. These partners include:\n\n\nCAS is governed by a board of directors that includes two representatives from each region of Canada, one of whom must by HIV-positive. Additionally, CAS has two at-large board of director seats for one man and one woman to ensure gender diverse representation at the board level. There is also one board of director seat reserved for an HIV-positive young person.\nCAS has historically been funded by the Federal government through Health Canada and/or the Public Health Agency of Canada, however since the Liberals reorganized funding priorities for HIV and Hepatitis-C in 2016 CAS has received no federal funding. This change in funding, and the general under-funding of HIV/AIDS service organizations in Canada more broadly, resulted in sharp criticism of the Liberal government, particularly by CAS's national partner, the Canadian HIV/AIDS Legal Network. \n\nFrom 1991-2001 CAS ran an annual AIDS Awareness Week campaign with support from various governmental, corporate, and charitable organizations including Health and Welfare Canada, the Public Health Agency of Canada, the Canadian Hemophilia Society, Levi Strauss & Co., and others. AIDS Awareness Week took place in October, but after this project came to an end when the Public Health Agency of Canada stopped funding for it, CAS's awareness campaigns largely revolved around World AIDS Day of each year. Each year's AIDS Awareness Week campaign, was organized around a theme. Ads were often developed by Ottawa creative agency McMillan and appeared in wide-circulation magazines and periodicals including MacLean's, L'actualité, and The Globe and Mail.\nThe Canadian AIDS Memorial Quilt was a project initiated by AIDS activists in Halifax, NS who organized a tour of the American AIDS Memorial Quilt throughout Canada in 1989 which resulted in the creation of hundreds of Canadian quilt panels. In 1992, the activists that began the project passed stewardship of the quilt onto CAS, but in 1994 The NAMES project | le projet des NOMS - Canada was established with Lawrence Eisener as the founding Executive Director. John Mactavish and John Stinson were also involved in the establishing of the organization which took over stewardship of the quilt. The NAMES project - Canada maintained and grew the quilt from 400 panels in 1993 to over 600 panels today. The quilt was response to the AIDS epidemic affecting the country at the time. Central goals of the quilt were: 1) To commemorate the lives of Canadians who had died of AIDS-related illness(es); 2) to raise funds for community-based AIDS service organization. In 2013 The NAMES project - Canada dissolved and once again CAS became the steward of the quilt. A new digital version of the quilt was launched by CAS in 2018 and is available online at quilt.ca.\n\nSince 1988,Canadian AIDS Society participate in numerous walks to raise money for the organization and to raise awareness on HIV/AIDS.\n\nOn June 27, 2017 CAS launched a national initiative to encourage people to be tested for HIV. Over forty community organizations participated across all regions of Canada by offering free HIV tests to anyone who wanted one. National Testing Day in Canada, much like its American counterpart that was established in 1995, encourages the use of Point-of-Care Testing (POCT) where participants can get their results in a few minutes as opposed two-weeks later. A 2016 study on the efficacy of National Testing Day in the United States indicates that the program successfully tested more people from priority populations and identified more previously undiagnosed HIV-positive people than during control weeks.\n\nListed below are some of the Canadian AIDS Society’s publications in chronological order. They include historical research, medical studies, tool kits, community involvement resources, and reports. The older publications are linked for historical reference and up to date research should always be consulted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "56205965", "url": "https://en.wikipedia.org/wiki?curid=56205965", "title": "Cannabis in the Maldives", "text": "Cannabis in the Maldives\n\nCannabis in the Maldives is illegal.\n\nPer the United Nations Economic and Social Commission for Asia and the Pacific, drug availability was first recorded in the 1970s in the Maldives. Cannabis usage was noted among tourists, who may have introduced it to the country.\n\nThe Yearbook of the United Nations 2003 noted that cannabis was illicitly cultivated in every country of South Asia, with the exception of the Maldives.\n"}
{"id": "43612592", "url": "https://en.wikipedia.org/wiki?curid=43612592", "title": "Community Options", "text": "Community Options\n\nCommunity Options, Inc. is a 501(c)3 national nonprofit organization. that provides housing and employment supports to people with intellectual disabilities, developmental disabilities and traumatic brain injury. The organizations headquarters are located in Princeton, NJ. In 2015 Community Options was named the fourth-largest nonprofit organization based in New Jersey.\n\nThe organization was founded in 1989 by Robert Stack, the President and CEO, along with a small group of disability advocates to create residential and employment supports for individuals with disabilities. In 2008 the organization supported over 1,400 people with disabilities. Community Options, Inc. also advocates for the de-institutionalization movement. The organization states that all people with disabilities should live in community-based settings and be released from out-of-state-run institutions \n\nRobert Stack was born in McKeesport, Pennsylvania in 1955. He attended St. Fidelis Seminary in Butler, Pennsylvania for the priesthood. During this time, he volunteered to work with children with significant disabilities every Saturday and Wednesday afternoon. He dropped out of the Seminary and after graduating from the University of Dayton (Dayton, Ohio), went to Kent State (Kent, Ohio) to study Philosophy. In 1979 he left to work for the State of New York at the Suffolk Developmental Center as a Recreational Aid in the institution. He opened the first two state-operated group homes on the grounds of the institution. He was hired as the Executive Assistant to the Director of the Division of Mental Retardation in 1981. He wrote the original white paper converting the Division of Mental Retardation to the Division of Developmental Disabilities. It was the first time New Jersey government recognized Autism as a treatable and fundable disability.\n\nStack was youngest Executive Director of United Cerebral Palsy for the entire state in 1984. He founded UCP of Mercer (now enable) UCP of Morris and UPC of Atlantic and Cape May County. He was appointed as the Deputy Director over all the institutions for NJ by Eddie C. Moore in 1987. Stack closed the first institution in New Jersey in Middlesex County.\n\nAfter the death of Eddie Moore, Stack became a consultant and in 1988 formed an exploratory group to look at the lack of services for people with significant disabilities in NJ, Texas, NY, PA and Tennessee. He incorporated Community Options with friends from both University of Dayton and Kent State in 1989. In 1990 Stack introduced John F. Kennedy Jr. to the Rehabilitation Conference participants for over 2,000 in Albany, New York.\n\nIn 1992, Community Options opened their first four group homes in New Jersey. From 1995 to 2011, group homes were opened in six other states including Pennsylvania, Texas, New York, South Carolina, Kentucky and Tennessee. \nThe New Jersey House of Representatives honored Community Options for their 25 years of service on February 10, 2014\nThe 48th Governor of New Jersey Thomas Kean was honored by Community Options on May 8, 2014 with the presentation of The Betty Pendler Award for all his years of service dedicated to people with disabilities\n\nCupid’s Chase 5K is an annual fundraising race presented by Community Options, Inc. It takes place every year on the Saturday of Valentine’s Day weekend. Cupid’s Chase 5K began in Princeton University’s Jadwin Gymnasium in 2009. In 2014, Cupid’s Chase took place in 24 cities across the country.\n\nCommunity Options operates several businesses in the United States that employs people with disabilities.\nCommunity Options opened \"Vaseful\", a nonprofit flower shop, in 1999. Employees are responsible for credit card processing, phone orders, shipping, distribution, inventory management and floral arranging.\nIn 2008, Community Options opened \"Presents of Mind\", a nonprofit gift store. Employees' duties include retail management, point-of-sale software, customer relations skills and store layout. \nThe \"Daily Plan It\" is an office and conference space for local businesses. Employees are responsible for customer service and facilities maintenance. The first Daily Plan It opened in Princeton, NJ in 1997. \nCommunity Options and the \"Parents Group\" opened \"The Red Ribbon Academy\" in 2013. The Red Ribbon Academy medical day program provides medical, therapeutic and recreational supports to people with severe developmental disabilities.\n\nCommunity Options hosts an annual national conference to discuss services to people with developmental disabilities and traumatic brain injury.\nGuest speakers in the past have included Jennifer Velez, Fran Harris, Judy Woodruff, Sean Stephenson, Colleen Wieck and Lou Greenwald. Topics include employment, business development, accessible housing, program development and employee hiring and retention.\n\nEach spring, Community Options selects a distinguished person who exemplifies the mission of Community Options as the Betty Pendler Award recipient. Betty Pendler was a member of the board of directors for the AHRC in New York City. Pendler was committed to people with disabilities, their families, and those who served them, in addition to raising a daughter with a developmental disability. Pendler died in 2001. \nPast recipients of the Betty Pendler Award are: Frank Zak (1991); Jane Olson (1992); Krystal Odell (1993); Karl Pfister (1994); Fran Curley (1995); Dan Fairchild (1996); David Norcross (1997); Patty Moore (1998); Peter Dulligan (1999); Madeleine Will (2000); Greg Hritz (2001); Bright Rajaratnam (2002); Mario Saenz (2003); Karen Hensley (2004); Matt Greller (2005); Reggie Shell (2006); Marge Brown (2007); Svetlana Repic-Qira (2008); Lisa Smith (2009); Jessica Guberman (2010); Joseph Kyrillos (2011); Christopher Dixon (2012); Brian Dion (2013); Thomas H. Kean (2014); Elaine Katz (2015); Robert and Gail Balph Gordon (2016); and Nikki Haley (2017).\n"}
{"id": "26286630", "url": "https://en.wikipedia.org/wiki?curid=26286630", "title": "Design effect", "text": "Design effect\n\nIn statistics, the design effect (or estimates of unit variance) is an adjustment used in some kinds of studies, such as cluster randomised trials, to allow for the design structure. The adjustment inflates the variance of parameter estimates, and therefore their standard errors, which is necessary to allow for correlations among clusters of observations. It is similar to the variance inflation factor and is used in sample size calculations. The term was introduced by Leslie Kish in 1965.\n\nFor a cluster randomised trial with \"m\" observations in each cluster and intra-cluster correlation of formula_1, the design effect. \"D\", is given by:\n\nFormally, the design effect is the ratio of two theoretical variances for an estimator:\n"}
{"id": "43987559", "url": "https://en.wikipedia.org/wiki?curid=43987559", "title": "Ebola virus epidemic in Liberia", "text": "Ebola virus epidemic in Liberia\n\nAn epidemic of Ebola virus disease occurred in Liberia from 2014 to 2015, along with the neighbouring countries of Guinea and Sierra Leone. The first cases of virus were reported by late March 2014. The Ebola virus, a biosafety level four pathogen, is an RNA virus discovered in 1976.\n\nBefore the outbreak of the Ebola epidemic the country had 50 doctors for its population of 4.3 million. The country's health system was seriously weakened by a civil war that ended in 2003.\n\nResearchers generally believe that a two-year-old boy, later identified as Emile Ouamouno, who died in December 2013 in the village of Meliandou, Guéckédou Prefecture, Guinea, was the index case of the current Ebola virus disease epidemic. His mother, sister, and grandmother then became ill with similar symptoms and also died. People infected by those initial cases spread the disease to other villages. Although Ebola represents a major public health issue in sub-Saharan Africa, no cases had ever been reported in West Africa and the early cases were diagnosed as other diseases more common to the area. Thus, the disease had several months to spread before it was recognized as Ebola.\n\nOn 30 March 2014, Liberia confirmed its first two cases of Ebola virus disease in Foya, Lofa County.\n\nBy 23 April thirty-four cases and six deaths from Ebola in Liberia were recorded. By 17 June sixteen people had succumbed to the disease in the country. The initial cases were thought to be malaria, an extremely common disease in Liberia, and thus leading to doctors being infected with the Ebola virus.\n\nBy 17 June the first deaths occurred in Monrovia from Ebola when seven patients died from the disease. Among them were a nurse along with other members of her household. At the time there were about 16 cases reported in Liberia in total. The nurse was treated at Redemption Hospital, a free state-run health care facility in New Kru Town west of Monrovia.\n\nOn 2 July the head surgeon of Redemption Hospital died from the disease. He was treated at the JFK Medical Center in Monrovia. Following his death Redemption Hospital was shut down, and patients were either transferred or referred to other facilities in the area. By 21 July four nurses at Phebe Hospital in Bong County contracted the disease. On 27 July Dr. Samuel Brisbane, one of Liberia's top doctors, succumbed to Ebola. A doctor from Uganda also died from the disease. Two U.S. health care workers, one a doctor (Dr. Kent Brantly) and the other a nurse were also infected with the disease. Both of them missionaries, they were medically evacuated from Liberia to the USA for treatment where they made a full recovery.\n\nBy 28 July most border crossings had been closed, with medical checkpoints set up at the remaining ports and quarantines in some areas. Arik Air suspended all flights between Nigeria and Liberia. On 30 July, Liberia shut down its schools in an attempt to prevent the outbreak from spreading.\n\nOn the first weekend of August, locals quarreled with a burial team trying to bury 22 bodies. The police were summoned and order was restored. On 4 August, the Liberian government ordered all corpses of those who died to be cremated. The body of a patient who died from Ebola is highly contagious in the days following the death. At the time there were 156 recorded deaths from the disease in Liberia.\n\nOn 11 August, the Ivorian government announced the suspension of all flights to and from countries affected by Ebola. Ten days later, it decided to close its borders as well with Guinea and Liberia, the two countries most affected by Ebola. On 27 August, wild dogs were seen eating the corpses that had not been collected for burial. A packs of dogs were observed digging up bodies and eating them in Liberia. One study indicated that dogs may eat at Ebola-infected carcasses and may become carriers of the disease.\n\nOn 18 August, a mob of residents from West Point, an impoverished area of Monrovia, descended upon a local Ebola clinic to protest its presence. The protesters turned violent, threatening the caretakers, removing the infected patients, and looting the clinic of its supplies, including blood-stained bed sheets and mattresses. Police and aid workers expressed fear that this would lead to mass infections of Ebola in West Point.\n\nOn 19 August, the Liberian government quarantined the entirety of West Point and issued a curfew statewide. Violence again broke out on 22 August after the military fired on protesting crowds. An inquiry into the incident found the security forces at fault, stating they \"fired with complete disregard for human life\". The quarantine blockade of the West Point area was lifted on 30 August. The Information Minister, Lewis Brown, said that this step was taken to ease efforts to screen, test, and treat residents.\n\nBy 1 September, Ivory Coast announced the opening of humanitarian corridors with its two affected neighboring countries.\n\nOn 20 September, Liberia opened a new 150-bed treatment unit clinic in Monrovia. At the opening ceremony of the Old Island Clinic on Bushrod Island six ambulances were already waiting with suspected Ebola patients. More patients were waiting by the clinic after making their way on foot with the help of relatives. Two days later 112 beds were already filled with 46 patients testing positive for Ebola, while the rest were admitted for observation. This expanded the number of beds in the city beyond 240. Its capacity was exceeded within 24 hours with a shortage of staff and logistics to take care of a patient with correct precautions in place. One source says it opened on 21 September 2014 with a 100-bed capacity. As of 23 September, there had been 3,458 total cases, 1,830 deaths, and 914 lab confirmed cases according to the World Health Organization.\n\nBy late September there were three clinics in Monrovia. Despite this patients waiting to be treated died outside as the clinics had run out of space to treat the increasing number of patients.<ref name=\"ebola/11118025\"></ref> If patients could not get a bed in the clinic they sometimes waited in holding centers until a bed opened up. There were additional cases in Monrovia where the bodies were dumped into the river. One woman used trash bags to protect herself as she cared for four other family members ill with Ebola. Her father became ill in late July, but they could not find a place of treatment for him and ended up doing home-care.\n\nOn 25 September Liberia's chief medical official went on a self-enforced quarantine after her assistant died from the disease, fearing she might have been accidentally exposed to the virus. By 29 September it was announced she tested negative for Ebola and government officials praised her for following the self quarantine protocol.\n\nOn 28 September Ivory Coast resumed flights to Liberia which had been suspended since 11 August 2014, after WHO's critique for tending to economically strangle the affected nations. From the beginning of the crisis, WHO has discouraged closing the borders with afflicted countries.\n\nOn 30 September a cameraman was tested positive for Ebola in a Texas hospital after contracting the disease before traveling back to the United States from Liberia. He covered the Ebola outbreak for NBC News (see 2014 Ebola virus cases in the United States). Following this the Liberian government enacted strict restrictions on journalistic coverage aimed at protecting patients' privacy.\n\nIn early October Ellen Johnson Sirleaf, the President of Liberia, continued requesting more aid to fight the disease. On 2 October a new 60-bed clinic was opened in Kenema. By then the outbreak was described as being out of control and an exponential growth in cases was seen. The focus shifted to slowing the outbreak down. A key element was the fact that the health care establishments were overwhelmed thus leading to those infected being turned away from treatment centers. This eventually led to the infection of others taking care of sick or dying patients at home.\n\nBy 3 October at least eight Liberian soldiers died after contracting the disease from a female visitor. On 3 October more medical supplies arrived from Germany. On that same day GERLIB opened up an Ebola isolation ward at its 48-bed facility in Paynesville (Monrovia).\nOn 10 October all journalists were banned from entering Ebola clinics.\n\nOn 14 October a hundred U.S. troops arrived in Liberia, bringing the total to 565 to aid in the fight against the deadly disease. On 16 October, U.S. President Obama authorized, via executive order, the use of National Guard and reservists in Liberia. A report of 15 October indicates that Liberia may need 80,000 more body bags and about 1 million protective suits for the next six months.\n\nIn October the WHO pushed for its 70-70-60 plan to control the outbreak. By 19 October it was reported that 223 health care workers had been infected with Ebola, and 103 of them had died in Liberia.\n\nOn 19 October the President of Liberia apologized to the Mayor of Dallas for the Liberian national that brought the disease to the United States. By 19 October only one area in Liberia, Grand Gedeh, has yet to report an Ebola case. 14 out of the 15 districts have reported cases. The disease has been noted to be spreading in Monrovia, the nation's capital with over one million inhabitants. Monrovia was particularly affected with 305 new cases reported in the week ending 19 October.\n\nBy 5 November Liberia had 6,525 cases (including 1,627 probable, 2,447 suspected cases) and 2,697 deaths.\nThe 5 November WHO situation report noted that \"There appears to be some evidence of a decline at the national level in Liberia, although new case numbers remain high in parts of the country.\" A report by CDC released on 14 November, based on data collected from Lofa county, indicates that there has been a genuine reduction in new infections. This is credited to an integrated strategy combining isolation and treatment with community behaviour change including safe burial practices, case finding and contract tracing – this strategy might serve as a model to implement in other affected areas to accelerate control of Ebola. On 13 November the Liberian President announced the lifting of the state of emergency in the country following the decrease in the number of new cases in the country. The decline in Liberia cases is contradicted in the latest reports from WHO with 439 new cases reported between 23 and 28 November.\n\nOn 4 December it was reported that President Sirleaf banned all rallies and gatherings in Monrovia before the senatorial election, fearing that the risk of the Ebola virus spreading may be increased.\n\nOn 13 January 2015, the Liberian government announced that new cases of Ebola in Liberia were now restricted to only two of its counties: Grand Cape Mount County and Montserrado County. On 28 January, the ELWA-3 Ebola treatment centre in Monrovia was partially dismantled. When the centre opened in August it had been swamped with patients, even needing to turn some away, but according to staff it was now down to only two patients. The MSF field coordinator said that as of that date Liberia was down to only five confirmed cases in all of Liberia. On 30 January, Liberia extended school reopenings by two weeks. On 10 February the U.S. military indicated it would end its relief mission. On 20 February, Liberia opened its land borders.\n\nIn the first week of March the World Health Organization announced that Liberia had released its last Ebola patient after going a week without any new cases of the virus being reported. If the country reports no new cases for 42 days, it will be declared Ebola-free according to the WHO.\n\nOn 5 March Tolbert Nyeswah, the assistant health minister of Liberia, reported that the country have released their last confirmed case of Ebola from a Chinese-staffed treatment centre. Beatrice Yardoldo was the last confirmed case and has been treated since 18 February. No new cases were reported for two weeks. On 20 March Dr. Moses Massaquoi, leader of the Clinton Health Access Initiative in Liberia, reported a new confirmed case in the country. The patient developed symptoms on 15 March, and was tested positive on 20 March. Subsequently the patient died on 27 March.\n\nThe countdown restarted on 28 March following the burial of the last casualty. The country was officially declared Ebola-free on 9 May after 42 days passed with no new cases of Ebola being reported. As of May 2015, the country remained on high alert against recurrence of the disease.\n\nThree months passed with no new reports of cases. However, on 29 June Liberia reported that the body of a 17-year-old youth, who had been treated for malaria, tested positive for Ebola. The patient was from Nedowein, a village in Margibi County near the capital Monrovia's international airport.\n\nThe WHO announced the male youth had been in close contact with at least 102 people with no recent history of traveling. Contact tracing followed with visitors from affected areas and those attending his funeral. On 1 July a second case was confirmed. By 2 July a third new case was confirmed leading to the possibility that they might have been infected with the Ebola virus lurking in animal meat according to researchers. All three cases may be linked to a dog meat meal they shared.\n\n\"Today, 3 September 2015, WHO declares Liberia free of Ebola virus transmission in the human population. Forty-two days have passed since the second negative test on 22 July 2015 of the last laboratory-confirmed case. Liberia now enters a 90-day period of heightened surveillance…\"\n\nWHO commends the Government of Liberia and its people on the successful response to this recent re-emergence. It is in full accord with government calls for sustained vigilance…\" \"Part of WHO statement, 3 September 2015\"\nAfter two months of going Ebola-free, on 20 November a new case was confirmed when a 15-year-old boy was diagnosed with Ebola<ref name=\"Liberia New Case 20/11/2015 LA Times\">http://www.latimes.com/world/africa/la-fg-ebola-case-in-liberia-20151120-story.html</ref> and two family members subsequently tested positive as well. Health officials were concerned because the child had not recently travelled or been exposed to someone with Ebola and the WHO stated, \"we believe that this is probably again, somehow, someone who has come in contact with a virus that had been persisting in an individual, who had suffered the disease months ago.\" Two members of the US CDC were sent to the country to help to ascertain the cause of the new cases. The infected boy died on 24 November, and on 3 December the two remaining cases were released after recovering from the virus. The 42-day countdown toward Liberia being declared Ebola-free, for the third time, started on 4 December 2015 and ended on 14 January 2016 when Liberia was declared Ebola-free. On 16 December, WHO reaffirmed that the cases in Liberia were the result of re-emergence of the virus in a previously infected person, and there is speculation that the boy may have been infected by an individual who became infectious once more due to pregnancy which may have weakened her immune system. On 18 December, the WHO indicated that it still considers Ebola in West Africa a public health emergency, though progress has been made.\n\nAfter having completed the 42 day time period, Liberia was declared free from the virus on 14 January 2016, effectively ending the outbreak started in neighbouring Guinea 2 years ago. Liberia will however have a 90-day period of heightened surveillance which was scheduled to conclude on 13 April 2016,\n\nOn 1 April it was reported that a new Ebola fatality has occurred in Liberia, and on 3 April a second case was reported in Monrovia. On 4 April it was reported that 84 individuals were under observation due to contact with the 2 confirmed cases of the virus. On 7 April Liberia confirmed three new cases since the virus resurfaced. A total of 97 contacts, including 15 healthcare workers are currently being monitored. The index case of the new flare up was reported to be the wife of a patient who died from the virus in Guinea. She traveled to Monrovia after the funeral of her husband but succumbed to the disease. The national Incident Management System (IMS) was immediately reactivated to coordinate the response to this flare-up and the counties enhanced the surveillance and prevention for a quick detention and interruption of transmission in case of eventual importation of cases from Monrovia. On 9 June, after 42 days, the country was declared Ebola-free.\n\nOn 20 September 2014, Liberia opened the 150-bed Old Island Clinic on Bushrod Island in Monrovia. Another clinic in Monrovia is a 160-bed facility staffed and run by Médecins Sans Frontières. On 25 November China opened a \"state of the art clinic\" outside Monrovia. The 100-bed clinic was mostly staffed by Chinese medical and other personnel.\n\nJFK (John F. Kennedy Medical Center) is another treatment center, and could hold 35 beds but expanded to 75 because of the increasing demand for beds. On 10 November, the U.S. opened the first of 17 Ebola treatment facilities it is building for Liberia, in Tubmanburg.\n\nSanitation is a major struggle in most parts of Monrovia. There are four public toilets in the West Point area in Monrovia, an area with 70,000 inhabitants. The beach and river surrounding West Point area are often used as a lavatory. The Mesurado River is a source of drinking water, and the fish from the water are a primary source of food for many.\n\nIt has been reported that body-collection teams, dispatched to collect the body of a suspected Ebola death, accepted bribes to issue falsified death certificates to family members. Due to the stigma of Ebola some families did not want to admit that their relative died from Ebola. The body of the deceased would then be left with relatives for a traditional funeral.\n\nDuring the 10-day Liberian government quarantine of the West Point slum in Monrovia, residents were able to leave the quarantine area by bribing soldiers and police officers. A journalist inside West Point told a local radio station that Liberian soldiers and police were seen \"daily soliciting monies from those being quarantined in the area to escape\". The journalist reported that \"many of those even suspected of having the disease were given free passage to leave West Point for Monrovia city center.\"\n\nIn October it was noted that many of the Ebola deaths and those dying were not being reported to health authorities. While the epidemic had been accelerating, the number of bodies being collected was falling. \"Very, very few of those dying in the community are being brought forward,\" said Cokie van der Velde, who organized the collections of bodies with Médecins Sans Frontières.\n\nVan der Velde announced that the main crematorium in Monrovia was running at full capacity in Monrovia. It was cremating 80 bodies at its mass pyre per day. In early October, the number of cremations drastically decreased to 30 or 40 per day. Van der Velde said, \"That means they’re being kept hidden and buried in secret\". Traditional funeral rituals are a risk factor in the spread of Ebola, as the body is at its most contagious stage post-mortem.\n\nBy late October it was reported that many beds in Liberian Ebola treatment centers were empty due to people no longer reporting suspected Ebola cases to health authorities. The assistant Liberian health minister announced at the time that an assessment of Ebola treatment units discovered that out of the 742 beds only 351 were occupied by patients. The non-reporting is believed to be due to a policy decision in August to cremate all bodies of suspected Ebola victims in Monrovia. Cremation was against local culture of a traditional burial. The cremation order came after people in Monrovia's neighborhoods resisted the burial of hundreds of Ebola victims near their homes.\n\nOn 6 August 2014, President Sirleaf, in an emergency announcement, informed absent government ministers and civil service leaders to return to their duties in Liberia. In late August Sirleaf dismissed 10 government officials, including deputy ministers in the central government who refused to return to work. The benefits and pay for nearly twenty other high-ranking officials who refused to return were halted.\n\nIn mid-November President Sirleaf reshuffled the country's cabinet in response to widespread criticism of the government's heavy-handed yet ineffective response to the Ebola crisis. George Warner, previously the head of civil service, would replace Walter Gwenigale as health minister. Sirleaf commented Gwenigale had her \"full confidence\" and will continue as an adviser.\n\nA black market for the blood of Ebola survivors has been reported in Liberia. Buyers of the blood hope to gain immunity or recovery via a blood transfusion. These transfusions have been noted as posing a risk for the transmission of HIV/AIDS, malaria and other blood-borne diseases. \"This has the potential to divert time and resources originally allocated to control Ebola\", according to a US military report.\n\n\nOn 5 August 2014 the Brothers Hospitallers of St. John of God confirmed that Brother Miguel Pajares had been infected after volunteering in Liberia. The Spanish military assisted with his transfer on 6 August. Authorities stated he would be treated in the 'Carlos III' hospital in Madrid. This attracted controversy, amid questions as to the authorities' ability to guarantee no risk of transmission. Brother Pajares died from the virus on 12 August.\n\nKent Brantly, a physician and medical director in Liberia for the aid group Samaritan’s Purse, and co-worker Nancy Writebol were infected while working in Monrovia. Both were flown to the United States at the beginning of August for further treatment. On 21 August, Brantly and Writebol recovered and were discharged.\n\nA French volunteer health worker working for MSF in Liberia contracted Ebola there and was flown to France on 18 September 2014. French Health Minister Marisol Touraine stated the nurse would receive the experimental drug favipiravir. By 4 October she had recovered and was released from hospital.\n\nAfter a news-network's cameraman came down with Ebola, he was evacuated to the U.S. and the rest of the crew also returned and went into quarantine.\n\n\n"}
{"id": "711157", "url": "https://en.wikipedia.org/wiki?curid=711157", "title": "Extensive farming", "text": "Extensive farming\n\nExtensive farming or extensive agriculture (as opposed to intensive farming) is an agricultural production system that uses small inputs of labor, fertilizers, and capital, relative to the land area being farmed.\n\nExtensive farming most commonly refers to sheep and cattle farming in areas with low agricultural productivity, but can also refer to large-scale growing of wheat, barley, cooking oils and other grain crops in areas like the Murray-Darling Basin in Australia. Here, owing to the extreme age and poverty of the soils, yields per hectare are very low, but the flat terrain and very large farm sizes mean yields per unit of labour are high. Nomadic herding is an extreme example of extensive farming, where herders move their animals to use feed from occasional rainfalls.\n\nExtensive farming is found in the mid-latitude sections of most continents, as well as in desert regions where water for cropping is not available. The nature of extensive farming means it requires less rainfall than intensive farming. The farm is usually large in comparison with the numbers working and money spent on it. In 1957, most parts of Western Australia had pastures so poor that only one sheep to the square mile could be supported \n\nJust as the demand has led to the basic division of cropping and pastoral activities, these areas can also be subdivided depending on the region's rainfall, vegetation type and agricultural activity within the area and the many other parentheses related to this data.\n\nExtensive farming has a number of advantages over intensive farming:\n\n\nExtensive farming can have the following problems:\n\n\nExtensive farming may produce more methane and nitrous oxide per kg of milk than intensive farming. A 1995 study compared of a modern dairy farm in Wisconsin with one in New Zealand in which the animals grazed extensively. Using total farm emissions per kg milk produced as a parameter, the researchers showed that production of methane from belching was higher in the New Zealand farm, while carbon dioxide production was higher in the Wisconsin farm. Output of nitrous oxide, a gas with an estimated global warming potential 310 times that of carbon dioxide was also higher in the New Zealand farm. Methane from manure handling was similar in the two types of farm. The explanation for the finding relates to the different diets used on these farms, being based more completely on forage (and hence more fibrous) in New Zealand and containing less concentrate than in Wisconsin. Fibrous diets promote a higher proportion of acetate in the gut of ruminant animals, resulting in a higher production of methane that must be released by belching. When cattle are given a diet containing some concentrates (such as corn and soybean meal) in addition to grass and silage, the pattern of ruminal fermentation alters from acetate to mainly propionate. As a result, methane production is reduced. Capper et al. compared the environmental impact of US dairy production in 1944 and 2007. They calculated that the carbon \"footprint\" per billion kg (2.2 billion lb) of milk produced in 2007 was 37 percent that of equivalent milk production in 1944.\n\n"}
{"id": "8670534", "url": "https://en.wikipedia.org/wiki?curid=8670534", "title": "Family Planning Association", "text": "Family Planning Association\n\nFPA (Family Planning Association) is a UK registered charity (number 250187) working to enable people to make informed choices about sex and to enjoy sexual health. It is the national affiliate for the International Planned Parenthood Federation in the United Kingdom. It celebrated its 80th anniversary in 2010. Its motto is \"Talking sense about sex\".\n\nFPA was founded in 1930 when five birth control societies merged to form the National Birth Control Council (NBCC). Charles Vickery Drysdale FRSE was critical within its foundation. Its stated purpose was \"that married people may space or limit their families and thus mitigate the evils of ill health and poverty\". The NBCC changed its name to the National Birth Control Association (NBCA) in 1931, and then to the Family Planning Association (FPA) in 1939. Since 1998 it has been known as FPA.\n\nThe FPA was heavily invested in guaranteeing and standardising the various contraceptive methods it prescribed using modern science and medicine. From its inception, the association invested heavily in developing and implementing tests for chemical efficacy and safety and rubber quality. These findings were published after 1937 in its annual Approved List of contraceptives.\n\nOriginally only offering a service to married couples, during the 1950s FPA clinics began to offer pre-marital advice to women, although proof, such as a letter from a vicar or family doctor, was often required before contraceptive supplies were provided.\n\nDuring the 1960s, social and sexual attitudes changed dramatically. The combined pill was first prescribed in FPA clinics in 1961 and within ten years was being used by over one million women. This highly reliable method brought a new sense of sexual freedom to men and women.\n\nBy 1970, FPA clinics were offering advice and treatment, without restriction. In 1974, FPA handed their network of over 1,000 clinics to the NHS when contraception became free for all. Family planning is still part of the health service.\n\nThe organisation's first administrator was Margaret Pyke OBE. Following Pyke's death in 1967, Jean Medawar took over as chairwoman.\n\nAs of 2010, the President of FPA is Baroness Gould of Potternewton; Vice Presidents include Jacqui Lait, Joan Ruddock MP, and Baroness Tonge and the charity is supported by a number of patrons. Additionally FPA is steered by a Board of Trustees of 12; the Chair is Dr Val Day and the Vice Chair is Paul Woodward. In January 2008, Julie Bentley became the Chief Executive, taking over from Anne Weyman OBE, who previously led the organisation for 11 years.\n\nIn October 2012, Dr Audrey Simpson OBE became Chief Executive of FPA taking over from Julie Bentley, who previously led the organisation for five years.\n\nFPA aims to improve the public's knowledge of sexual health. The organisation runs training courses and projects for professionals, grandparents, parents, carers and young people, and provides an information and press service to communicate sexual health information more widely.\n\nFPA runs an enquiry service providing confidential information and advice on contraception; common sexually transmitted infections; pregnancy choices; abortion and planning a pregnancy. The enquiry service is made up of its helpline and Web Enquiry Service (Ask WES).\n\nFPA also provides clinic details of contraception, sexual health and genitourinary medicine (GUM) clinics and sexual assault referral centres. In Northern Ireland, where abortion is difficult to obtain, FPA offers an unplanned pregnancy counselling service.\n\nFPA is also funded by the Department of Health (England) to provide a wide range of booklets on individual methods of contraception, common sexually transmitted infections, pregnancy choices, abortion and planning a pregnancy. These are distributed freely across England to sexual health services and GPs.\n\nCampaigning is a core part of the work of FPA. It played a role in obtaining the provision of free contraception on the NHS across the UK and campaigns around abortion to preserve consumer rights and choices. In May 2008, FPA and other pro-choice groups prevented a reduction of the 24-week time limit for abortion, which was debated in the House of Commons. The organisation is now trying to modernise abortion laws throughout the UK.\n\nIn 2010, FPA celebrated 80 years and rebranded with a new logo to reach more people with sexual health and sex and relationships information. During the same year it also founded an Achievers' Club to recognise people who have made significant contributions to improving the sexual health of the UK. \"My contraception tool\", an online tool to help people choose contraception, was launched by FPA and Brook. In October 2010, FPA also held the first all-Ireland conference on abortion for medical practitioners.\n\nAt the Charity Awards 2010, \"All about us\", an FPA CD-ROM for people with learning disabilities, won the Disability category.\n\nDue to the emotive nature of some topics addressed by FPA - for example, abortion - the charity has been criticised by a number of religious, political and pro-life groups. These groups generally protest on the grounds that all foetuses have a right to life; that sexual health education leads to promiscuity; that contraception is against the teachings of the Bible; and for other similar reasons. In the early years of the charity, objects were thrown at clinics and volunteers were threatened. At the present time, opponents frequently hold protests outside the FPA Belfast office.\n\n"}
{"id": "52099863", "url": "https://en.wikipedia.org/wiki?curid=52099863", "title": "Frances G. Wickes", "text": "Frances G. Wickes\n\nFrances Wickes (born Frances Gillespy, Lansingburgh, New York, August 28, 1875 – Peterborough, New Hampshire, May 5, 1967) was a psychologist and writer.\n\nA graduate of Columbia University, Wickes was a teacher, writer and playwright for children and teenagers in New York but later became interested in becoming a Jungian therapist, especially for artists, and visited Zurich several times after meeting Carl Jung in 1920s, with whom Wickes maintained a correspondence.\n\nWickes kept a diary of dreams and made conferences, especially at the Analytical Psychology Club of New York. Wickes had a husband, Thomas Wickes (divorced in 1910 and died about 1947) and a son, Eliphalet Wickes (1906–1926). Wickes lived also in California and Alaska.\n\nJung wrote the preface to her second book on the psychological world of children (1927), where Wickes supported the autonomous presence of the child in the collective unconscious, according to the idea of a participation mystique, which Lucien Lévy-Bruhl in 1910 had theorized to exist within primitive societies, Wickes's comparing a child to an individual in training and giving more place to intuition and feeling than attention to the real or rational. The book was translated into German, French, Dutch, Italian and Greek.\n\nIn coming decades Wickes helped found \"Spring\", which bills itself as the oldest Jungian journal, and lectured at various branches of the Jung Institutes.\n\nAmong Wickes's correspondents are preserved letters to Muriel Rukeyser (1913–1980), Henry Murray, Eudora Welty, Mary Louise Peebles (1833–1915), Martha Graham, Lewis Mumford, Thomas Mann, May Sarton, Robert Edmond Jones (1887–1954) and William McGuire (1917–2009). At death without heirs $1–1/2 million of her $2-million estate was given to the C. G. Jung Institute of San Francisco and the rest to the Frances G. Wickes Foundation (1955–1974).\n\n\n\n"}
{"id": "53227269", "url": "https://en.wikipedia.org/wiki?curid=53227269", "title": "HPV-associated oropharyngeal cancer awareness and prevention", "text": "HPV-associated oropharyngeal cancer awareness and prevention\n\nHuman papilloma virus (HPV)-associated oropharyngeal cancer awareness and prevention is a vital concept from a public and community health perspective.\n\nHPV is the sexually transmitted virus that is known to be the cause of genital warts. There are currently more than 100 different strains of HPV, half of which can cause genital infections. It is worth noting here that although it is not usually the HPV strains that cause genital warts that are associated with the oropharyngeal cancers, they are transmitted the same way through oral-genital sexual contact, and consumers should protect themselves accordingly and adhere to a routine health and dental screening schedule to monitor and maintain their health status. \n\nLearning and utilizing self-examinations of your body, checking for changes or abnormalities in your mouth while performing your oral hygiene as well as routine checks of your genitals after showering to note changes or abnormalities can enhance your awareness of changes in your health status. According to the Centers for Disease Control and Prevention, most of all sexually active people will acquire at least one HPV infection in their lifetime. It is important that consumers seek routine dental examinations as a part of their health care to allow for dentists and primary care providers to screen them for this type of cancer, as early detection can mean all the difference in treatments. Oral cavity and oropharyngeal cancers can occur anywhere in the mouth, but occur most often in the tongue, tonsils, oropharynx (back of the throat), gums and the floor of the mouth. According to the U.S. Preventative Services Task Force, \"Oropharyngeal cancer is difficult to visualize and is usually located at the base of the tongue (the back third of the tongue), the soft palate (the back part of the roof of the mouth), the tonsils, and the side and back walls of the throat\" and requires a thorough exam by a dental provider or specialist preferably. Men are twice as likely than women to have oral cancer, regardless of race, as African-Americans and Caucasians are equally likely to develop oral cancer.\n\nMany people do not realize that some cancers can be linked to viral illnesses, such as human papilloma virus (HPV). Oropharyngeal cancer has been linked in some cases to HPV. The most common type of oral and oropharyngeal cancer is squamous cell carcinoma...and this kind is indicated in about 90% of oral and oropharyngeal cancers. Approximately 63% of oropharyngeal squamous cell carcinomas each year are associated with HPV infection. Most cases of HPV go undetected and clear up on their own without the patient ever knowing they had contracted it. However, there are 13 HPV strains that are known to potentially cause cancer According to the CDC, during an analysis of reported data from national cancer and epidemiological registries between the years of 2008 and 2012, of 30,700 cases of cancer that were estimated to be caused by HPV, 24,600 were found to be caused by strains HPV 16 and HPV 18, which are strains that are preventable as they are covered in the vaccinations that are available to guard against HPV. 3,800 were also found to be caused by 5 other preventable strains. More specifically, 9500 cases of oropharyngeal cancers were caused by the same preventable strains, while 900 more were caused by the other preventable strains.\n\nVaccination is recommended for females and males alike, as early as age 9 and up to 26 years of age. Pre-teens can get their HPV vaccination at the same time as other scheduled vaccinations and boosters for convenience, as they are able to be safely administered together. Parents should speak to their child's pediatrician regarding their recommendations for vaccinations. Pregnant women are recommended to wait until after they deliver to receive the vaccination. Anyone with allergy concerns especially to yeast should consult their physician. Until recently, Gardasil (bivalent, quadrivalent, and 9-valent versions) and Cervarix brands have been available; however, after May 2017, only Gardasil 9 (9-valent) will be available for vaccination in the U.S. 9-valent simply refers to the 9 strains that the vaccine helps to protect against that are known to be associated with cancers.\n\nThe World Health Organization(WHO) says between 30% and 50% of cancer deaths could be prevented by avoiding alcohol and tobacco products, especially concurrent use. The WHO also indicates there are about \"529,000 new cases of cancers of the oral cavity and pharynx each year, and more than 300,000 deaths\" \"The American Cancer Society’s most recent estimates for oral cavity and oropharyngeal cancers in the United States are for 2017...(are) that about 49,670 people will get oral cavity or oropharyngeal cancer...(and) an estimated 9,700 people will die of these cancers.\" In summary, consumers need to take charge of their own health and be accountable. Go see your doctor and dentist for regular exams, perform self-examinations to look for changes in your body, follow the recommended vaccination guidelines, practice safer sex, and avoid alcohol and tobacco products for a healthier you.\n"}
{"id": "26650669", "url": "https://en.wikipedia.org/wiki?curid=26650669", "title": "Han Feng", "text": "Han Feng\n\nHan Feng (韩锋), born in Qinzhou, Guangxi, People's Republic of China, was a member of the Communist Party of China and a senior official of the China National Tobacco Corporation. In early 2010, he was fired and expelled from the party due to corruption and his love life being exposed on the internet from his diary. The scandal soon became one of the most popular stories in Chinese cyberspace. Despite the scandal and arrest, most netizens overwhelmingly believe Han Feng is a good CPC official.\n\nHan originally attended Guangxi University and studied Chinese language. After graduation he went to Shangsi County and was selected as Qinzhou office secretary. After 1988 he became the tobacco secretary of the Guanxi region. Ten years later he became a director of the China National Tobacco Corporation. He married one of his classmates who also worked in a tobacco bureau. On February 26, 2003 he was made head director of its Laibin municipal bureau. In 2009 he was transferred to the Guangxi tobacco monopoly bureau as a director of the sales division.\n\nHan Feng kept a Pepysian diary with entries covering his daily activities from January 1, 2007 to June 10, 2008 with a total of more than 500 entries. It featured graphical sex accounts with mistresses between bribes and banquets. At the time he was the director of the Laibin tobacco monopoly bureau.\n\nAccording to Guangzhou-based Southern Metropolis Weekly (南都周刊)., the diary first appeared online on November 23, 2009. After a four-month cycle of repeated postings and deletions, by the end of February 2010, the news finally broke across almost every major Chinese web portal.\n\nOn March 13, 2010 Han Feng was arrested and expelled from the Communist party. After investigation, Han, at the age of 53, was removed from his tobacco bureau division chief post for allegedly accepting bribes totaling 482,000 yuan. He also accepted an apartment worth 300,000 yuan. The diary itself documented 275,000 yuan in just under two years. The bribes were taken between 2002 and 2010.\n\nA star blogger, Han Han wrote a post titled \"Han Feng is a good cadre\" which commented on the online leak of the diary. The blogger said that Han Feng was a good official since the amount of bribes and number of sexual relationships he had was no comparison to other Communist party officials. He then conducted an online survey and concluded that 96% of 210,000 voters with independent IP addresses felt Han Feng was a good official.\n\nOther responses from Tianya Club said the diary is a great text. It has done a greater job than any recent literary work or news reports in vividly presenting the typical life of a Chinese official, and the real situation of officialdom as well as male-female relationships in second- and third-tier cities in China. Others responded that the Chinese people's requirements for officials are generally low. Others said this \"double-faced\" life is not unique to Han Feng, but all 80 characters in the diary. Political commentator Qin Geng said \"Han Feng is surely not the worst official in China, that he could even be considered to be one of the good ones. That is the most fearful part of the story\".\n\n"}
{"id": "53031520", "url": "https://en.wikipedia.org/wiki?curid=53031520", "title": "Health impact of asbestos", "text": "Health impact of asbestos\n\nAll types of asbestos fibers are known to cause serious health hazards in humans. Amosite and crocidolite are considered the most hazardous asbestos fiber types; however, chrysotile asbestos has also produced tumors in animals and is a recognized cause of asbestosis and malignant mesothelioma in humans, and mesothelioma has been observed in people who were occupationally exposed to chrysotile, family members of the occupationally exposed, and residents who lived close to asbestos factories and mines.\n\nDuring the 1980s and again in the 1990s it was suggested at times that the process of making asbestos cement could \"neutralize\" the asbestos, either via chemical processes or by causing cement to attach to the fibers and changing their physical size; subsequent studies showed that this was untrue, and that decades-old asbestos cement, when broken, releases asbestos fibers identical to those found in nature, with no detectable alteration.\nExposure to asbestos in the form of fibers is always considered dangerous. Working with, or exposure to, material that is friable, or materials or works that could cause release of loose asbestos fibers, is considered high risk. However, in general, people who become ill from inhaling asbestos have been regularly exposed in a job where they worked directly with the material.\n\nAccording to the National Cancer Institute, \"A history of asbestos exposure at work is reported in about 70 percent to 80 percent of all cases. However, mesothelioma has been reported in some individuals without any known exposure to asbestos.\" A paper published in 1998, in the American Journal of Respiratory and Critical Care Medicine, concurs, and comments that asbestosis has been reported primarily in asbestos workers, and appears to require long-term exposure, high concentration for the development of the clinical disease. There is also a long latency period (the time taken between harmful contact and emergence of the actual resulting illness) of about 12 to 20 years, and potentially up to 40 years.\n\nThe most common diseases associated with chronic exposure to asbestos are asbestosis and mesothelioma.\n\nAccording to OSHA, \"there is no 'safe' level of asbestos exposure for any type of asbestos fiber. Asbestos exposures as short in duration as a few days have caused mesothelioma in humans. Every occupational exposure to asbestos can cause injury or disease; every occupational exposure to asbestos contributes to the risk of getting an asbestos related disease.\"\n\nDiseases commonly associated with asbestos include:\n\n\nAsbestos exposure becomes an issue if asbestos containing materials become airborne, such as due to deterioration or damage. Building occupants may be exposed to asbestos, but those most at risk are persons who purposely disturb materials, such as maintenance or construction workers. Housekeeping or custodial employees may be at an increased risk as they may potentially clean up damaged or deteriorated asbestos containing materials without knowing that the material contains asbestos. Asbestos abatement or remediation workers and emergency personnel such as firefighters may also become exposed. Asbestos-related diseases have been diagnosed in asbestos workers' family members, and in residents who live close to asbestos mines or processing plants.\n\nCurrently in the United States, several thousand products manufactured and/or imported today still contain asbestos. In many parts of the industrialized world, particularly the European Union, asbestos was phased out of building products beginning in the 1970s with most of the remainder phased out by the 1980s. Even with an asbestos ban in place, however, asbestos may be found in many buildings that were built and/or renovated from the late 1800s through the present day.\n\nResidential building materials containing asbestos include a variety of products, such as: stipple used in textured walls and ceilings; drywall joint filler compound; asbestos contaminated vermiculite, vinyl floor tile; vinyl sheet flooring; window putty; mastic; cement board; asbestos cement pipes and flues; furnace tape; and stucco. Asbestos is widely used in roofing materials, mainly corrugated asbestos cement roof sheets and asbestos shingles sometimes called transite. Other sources of asbestos-containing materials include fireproofing and acoustic materials.\n\nA fiber cannot be identified or ruled out as asbestos, either using the naked eye or by simply looking at a fiber under a regular microscope. The most common methods of identifying asbestos fibers are by using polarized light microscopy (PLM) or transmission electron microscopy (TEM). PLM is less expensive, but TEM is more precise and can be used at lower concentrations of asbestos.\n\nIf asbestos abatement is performed, completion of the abatement is verified using visual confirmation and may also involve air sampling. Air samples are typically analyzed using phase contrast microscopy (PCM). PCM involves counting fibers on a filter using a microscope. Airborne occupational exposure limits for asbestos are based on using the PCM method.\n\nThe American Conference of Governmental Industrial Hygienists has a recommended Threshold Limit Value (TLV) for asbestos of 0.1 fibers/mL over an 8-hour shift. OSHA in the United States and occupational health and safety regulatory jurisdictions in Canada use 0.1 fibers/mL over an 8-hour shift as their exposure limits.\n\nAsbestos can be found naturally in the air outdoors and in some drinkable water, including water from natural sources. Even nonoccupationally exposed members of the human population have tens to hundreds of thousands of asbestos fibers per gram of dry lung tissue, equivalent to millions of fibers in each lung.\n\nAsbestos from natural geologic deposits is known as \"naturally occurring asbestos\" (NOA). Health risks associated with exposure to NOA are not yet fully understood, and current US federal regulations do not address exposure from NOA. Many populated areas are in proximity to shallow, natural deposits which occur in 50 of 58 California counties and in 19 other US states. In one study, data was collected from 3,000 mesothelioma patients in California and 890 men with prostate cancer, a malignancy not known to be related to asbestos. The study found a correlation between the incidence of mesotheliomas and the distance a patient lived from known deposits of rock likely to include asbestos; the correlation was not present when the incidence of prostate cancer was compared with the same distances. The risk of mesothelioma declined by 6% for every that an individual had lived away from a likely asbestos source.\n\nPortions of El Dorado County, California are known to contain natural amphibole asbestos formations at the surface. The USGS studied amphiboles in rock and soil in the area in response to an EPA sampling study and subsequent criticism of the EPA study. The EPA study was refuted by its own peer reviewers and never completed or published. The study found that many amphibole particles in the area meet the counting rule criteria used by the EPA for chemical and morphological limits, but do not meet morphological requirements for commercial-grade-asbestos. The executive summary pointed out that even particles that do not meet requirements for commercial-grade-asbestos may be a health threat and suggested a collaborative research effort to assess health risks associated with \"Naturally Occurring Asbestos.\"\n\nHowever, the main criticism pointed at EPA was that their testing was conducted in small isolated areas of El Dorado where there were no amphibole asbestos deposits, thus the language regarding amphibole, nonfibrous \"particles\". Actual surface amphibole deposits in residential areas were ignored for testing purposes. Because of this, no final findings were published by ATSDR.\n\nA great deal of Fairfax County, Virginia was also found to be underlaid with tremolite. The county monitored air quality at construction sites, controlled soil taken from affected areas, and required freshly developed sites to lay of clean, stable material over the ground.\n\nGlobally, samples collected from Antarctic ice indicate chrysotile asbestos has been a ubiquitous contaminant of the environment for at least 10,000 years. Snow samples in Japan have shown ambient background levels are one to two orders of magnitude higher in urban than in rural areas. Higher concentrations of airborne asbestos fibers are reported in urban areas where there is more ACM (asbestos containing materials) and mechanisms of release (vehicles braking and weathering of asbestos cement materials); concentrations in the range of 1–20 ng/m have been reported. Fibers longer than 5μm are rarely found in rural areas. Ambient concentrations using TEM analysis have been based on mass measurements.\n\nStanton and Layard hypothesized in 1977–78 that toxicity of fibrous materials is \"not\" initiated by \"chemical\" effects; that is, any trigger-effects of asbestos must presumably be \"physical\", such as mechanical damage which might disrupt normal cell activity—especially mitosis.\n\nThere is experimental evidence that very slim fibers (<60 nm, <0.06 μm in breadth) tangle destructively with chromosomes (being of comparable size). This is likely to cause the sort of mitosis disruption expected in cancer.\n\nIndividual asbestos fibers are invisible to the unaided human eye because their size is about 3–20 µm wide and can be as slim as 0.01 µm. Human hair ranges in size from 17 to 181 µm in breadth. Fibers ultimately form because when these minerals originally cooled and crystallized, they formed by the polymeric molecules lining up parallel with each other and forming oriented crystal lattices. These crystals thus have three cleavage planes, and in this case, there are two cleavage planes which are much weaker than the third. When sufficient force is applied, they tend to break along their weakest directions, resulting in a linear fragmentation pattern and hence a fibrous form. This fracture process can keep occurring and one larger asbestos fiber can ultimately become the source of hundreds of much thinner and smaller fibers.\n\nWhen fibers or asbestos structures from asbestos containing materials (ACM) become airborne, the process is called primary release. Primary release mechanisms include abrasion, impaction, fallout, air erosion, vibration, and fire damage. Secondary release occurs when settled asbestos fibers and structures are resuspended as a result of human activities. In unoccupied buildings or during unoccupied periods, fiber release typically occurs by fallout or is induced by vibration or air erosion.\n\nFriability of a product containing asbestos means that it is so soft and weak in structure that it can be broken with simple finger crushing pressure. Friable materials are of the most initial concern because of their ease of damage. The forces or conditions of usage that come into intimate contact with most non-friable materials containing asbestos are substantially higher than finger pressure.\n\nSmoking has a supra-additive effect in increasing the risk of lung cancer in those exposed to asbestos. Studies have shown an increased risk of lung cancer among smokers who are exposed to asbestos compared to nonsmokers.\n\n\"For additional chronological citations, see also, List of asbestos disease medical articles\"\n\nEarly concern in the modern era on the health effects of asbestos exposure can be found in several sources. Among the earliest were reports in Britain. The annual reports of the Chief Inspector of Factories in 1898 included a report from Lucy Deane which stated that asbestos had \"easily demonstrated\" health risks.\n\nAt about the same time, what was probably the first study of mortality among asbestos workers was reported in France. While the study describes the cause of death as chalicosis, a generalized pneumoconiosis, the circumstances of the employment of the fifty workers whose death prompted the study suggest that the root cause was asbestos or mixed asbestos-cotton dust exposure.\n\nAwareness of asbestos-related diseases can be found in the early 1900s, when London doctor H. Montague Murray conducted a post mortem exam on a young asbestos factory worker who died in 1899. Dr. Murray gave testimony on this death in connection with an industrial disease compensation hearing. The post-mortem confirmed the presence of asbestos in the lung tissue, prompting Dr. Murray to express as an expert opinion his belief that the inhalation of asbestos dust had at least contributed to, if not actually caused, the death of the worker.\n\nThe record in the United States was similar. Early observations were largely anecdotal in nature and did not definitively link the occupation with the disease, followed by more compelling and larger studies that strengthened the association. One such study, published in 1918, noted:\n\nWidespread recognition of the occupational risks of asbestos in Britain was reported in 1924 by a Dr. Cooke, a pathologist, who introduced a case description of a 33-year-old female asbestos worker, Nellie Kershaw, with the following: \"Medical men in areas where asbestos is manufactured have long suspected the dust to be the cause of chronic bronchitis and fibrosis ...\" Dr. Cooke then went on to report on a case in 1927 involving a 33-year-old male worker who was the only survivor out of ten workers in an asbestos carding room. In the report he named the disease \"asbestosis\".\n\nDr. Cooke's second case report was followed, in the late 1920s, by a large public health investigation (now known as the Merewether report after one of its two authors) that examined some 360 asbestos-textile workers (reported to be about 15% of the total comparable employment in Britain at the time) and found that about a quarter of them suffered from pulmonary fibrosis. This investigation resulted in improved regulation of the manufacturing of asbestos-containing products in the early 1930s. Regulations included industrial hygiene standards, medical examinations, and inclusion of the asbestos industry into the British Workers' Compensation Act.\n\nThe first known U.S. workers' compensation claim for asbestos disease was in 1927. In 1930, the first reported autopsy of an asbestosis sufferer was conducted in the United States and later presented by a doctor at the Mayo Clinic, although in this case the exposure involved mining activities somewhere in South America.\n\nIn 1930, the major asbestos company Johns-Manville produced a report, for internal company use only, about medical reports of asbestos worker fatalities. In 1932, a letter from U.S. Bureau of Mines to asbestos manufacturer Eagle-Picher stated, in relevant part, \"It is now known that asbestos dust is one of the most dangerous dusts to which man is exposed.\"\n\nIn 1933, Metropolitan Life Insurance Co. doctors found that 29% of workers in a Johns-Manville plant had asbestosis. Likewise, in 1933, Johns-Manville officials settled lawsuits by 11 employees with asbestosis on the condition that the employees' lawyer agree to never again \"directly or indirectly participate in the bringing of new actions against the Corporation.\" In 1934, officials of two large asbestos companies, Johns-Manville and Raybestos-Manhattan, edited an article about the diseases of asbestos workers written by a Metropolitan Life Insurance Company doctor. The changes downplayed the danger of asbestos dust. In 1935, officials of Johns-Manville and Raybestos-Manhattan instructed the editor of \"Asbestos\" magazine to publish nothing about asbestosis. In 1936, a group of asbestos companies agreed to sponsor research on the health effects of asbestos dust, but required that the companies maintain complete control over the disclosure of the results.\n\nIn 1942, an internal Owens-Corning corporate memo referred to \"medical literature on asbestosis ... scores of publications in which the lung and skin hazards of asbestos are discussed.\" Testimony given in a federal court in 1984 by Charles H. Roemer, formerly an employee of Unarco, described a meeting in the early 1940s between Unarco officials, J-M President Lewis H. Brown and J-M attorney Vandiver Brown. Roemer stated, \"I'll never forget, I turned to Mr. Brown, one of the Browns made this crack (that Unarco managers were a bunch of fools for notifying employees who had asbestosis), and I said, 'Mr. Brown, do you mean to tell me you would let them work until they dropped dead?' He said, 'Yes. We save a lot of money that way.'\" In 1944, a Metropolitan Life Insurance Company report found 42 cases of asbestosis among 195 asbestos miners.\n\nIn 1951, asbestos companies removed all references to cancer before allowing publication of research they sponsored. In 1952, Dr. Kenneth Smith, Johns-Manville medical director, recommended (unsuccessfully) that warning labels be attached to products containing asbestos. Later, Smith testified: \"It was a business decision as far as I could understand ... the corporation is in business to provide jobs for people and make money for stockholders and they had to take into consideration the effects of everything they did and if the application of a caution label identifying a product as hazardous would cut into sales, there would be serious financial implications.\"\n\nIn 1953, National Gypsum's safety director wrote to the Indiana Division of Industrial Hygiene, recommending that acoustic plaster mixers wear respirators \"because of the asbestos used in the product.\" Another company official noted that the letter was \"full of dynamite\" and urged that it be retrieved before reaching its destination. A memo in the files noted that the company \"succeeded in stopping\" the letter, which \"will be modified.\"\n\nThrough the 1970s, asbestos was used to fireproof roofing and flooring, for heat insulation, and for a variety of other purposes. The material was used in fire-check partitioning and doors on North Sea Oil Production Platforms and Rigs.\n\nDuring the mid-to late 1980s, public health concern focused on potential asbestos fiber exposures of building occupants and workers in buildings containing asbestos containing building materials (ACBM) and their risks of developing lung cancer or mesothelioma. As a consequence, the Health Effects Institute (Cambridge, MA) convened a panel to evaluate the lifetime cancer risk of general building occupants as well as service workers.\n\nThe United States remains one of the few developed countries to not completely ban asbestos. While it is banned its use for certain items, it is legal for use in products such as clothing, pipeline wraps, vinyl floor tiles, millboards, cement pipes, disk brake pads, gaskets and roof coatings.\n\nIn 1989 the EPA issued the Asbestos Ban and Phase Out Rule but in 1991, asbestos industry supporters challenged and overturned the ban in a landmark lawsuit: \"Corrosion Proof Fittings v. the Environmental Protection Agency\". Although the case resulted in several small victories for asbestos regulation, the EPA ultimately did not put an end to asbestos use. This ruling leaves many consumer products that can still legally contain trace amounts of asbestos. For a clarification of products which legally contain asbestos, read the EPA's clarification statement.\n\nIn 2010, Washington State banned asbestos in automotive brakes starting in 2014. The Occupational Safety and Health Administration (OSHA), has set limits of 100,000 fibers with lengths greater than or equal to 5 µm per cubic meter of workplace air for eight-hour shifts and 40-hour work weeks.\n\nIn Canada, asbestos is not presently banned, though its use has declined since the mid-1970s and early 1980s. Products containing asbestos are regulated by the Asbestos Products Regulation (SOR 2007/260). On December 16, 2016, parliament stated that as of 2018, all use of asbestos will be totally banned.\n\nIn the United Kingdom, blue and brown asbestos materials were banned outright in 1985 while the import, sale and second hand reuse of white asbestos was outlawed in 1999. The 2012 Control of Asbestos Regulations state that owners of non-domestic buildings (e.g., factories and offices) have a \"duty to manage\" asbestos on the premises by making themselves aware of its presence and ensuring the material does not deteriorate, removing it if necessary. Employers, e.g. construction companies, whose operatives may come into contact with asbestos must also provide annual asbestos training to their workers.\n\nIn 1984, the import of raw amphibole (blue and brown) asbestos into New Zealand was banned. In 2002 the import of chrysotile (white) asbestos was also banned. In 2015 the government announced that the importation of asbestos would be completely banned with very limited exceptions (expected to be applied to replacement parts for older machines) that would be reviewed on a case-by-case basis.\n\nNorth-west of Nelson, in the Upper Takaka Valley is New Zealand's only commercially harvested asbestos mine. A low-grade Chrysotile was mined here from 1908 to 1917 but only 100 tons was washed and taken out by packhorse. A new power scheme enabled work to renew and between 1940 and 1949, 40 tons a month was mined by the Hume Company. This continued to 1964, when, due to the short length of its fibre, the limited commercial viability forced mining to cease.\n\nThe use of crocidolite (blue) asbestos was banned in 1967, while the use of amosite (brown) asbestos continued in the construction industry until the mid-1980s. It was finally banned from building products in 1989, though it remained in gaskets and brake linings until 31 December 2003, and cannot be imported, used or recycled.\n\nAsbestos continues to be a problem. Two out of three homes in Australia built between World War II and the early 1980s still contain asbestos.\n\nThe union that represents workers tasked with modifying electrical meter boxes at residences stated that workers should refuse to do this work until the boxes have been inspected for asbestos, and the head of the Australian Council of Trade Unions (ACTU) has called on the government to protect its citizens by ridding the country of asbestos by 2030.\n\nHandlers of asbestos materials must have a B-Class license for bonded asbestos and an A-Class license for friable asbestos.\n\nThe town of Wittenoom, in Western Australia was built around a blue asbestos mine. The entire town continues to be contaminated, and has been disincorporated, allowing local authorities to remove references to Wittenoom from maps and roadsigns.\n\nDespite the mining and use of asbestos reaching the country’s Supreme Court, Brazil is the world’s third-largest producer and exporter of chrysotile asbestos. São Paulo State law 12.684/07 prohibits the use of any product which utilizes asbestos but many buildings are still constructed of products containing asbestos. As a result, it is estimated that up to 15,000 Brazilians die each year of exposure to asbestos.\n\nRevelations that hundreds of workers had died in Japan over the previous few decades from diseases related to asbestos sparked a scandal in mid-2005. Tokyo had, in 1971, ordered companies handling asbestos to install ventilators and check health on a regular basis; however, the Japanese government did not ban crocidolite and amosite until 1995, and a near complete ban with a few exceptions on asbestos was implemented in 2006, with the remaining exceptions being removed in March 2012 for a full-fledged ban.\n\nIn May 1997, the manufacture and use of crocidolite and amosite, commonly known as blue and brown asbestos, were fully banned in South Korea. In January 2009, a full-fledged ban on all types of asbestos occurred when the government banned the manufacture, import, sale, storage, transport or use of asbestos or any substance containing more than 0.1% of asbestos. In 2011, South Korea became the world's sixth country to enact an asbestos harm aid act, which entitles any Korean citizen to free lifetime medical care as well as monthly income from the government if he or she is diagnosed with an asbestos-related disease.\n\nUse of all types of asbestos has been banned in Singapore since 1989. Currently, only \"removal\" of asbestos-containing materials is allowed in Singapore and the Ministry of Manpower must be notified before work commences.\n\nA complete ban on asbestos in Turkey went into effect in 2011.\n\nFinland\n\nThe manufacture and import of asbestos containing building materials was banned from 1993 onwards, while the selling and use of such materials was banned from 1994. \n\nIn a 1998 paper, medical historian Peter Bartrip examines why awareness and legislation appear to have lagged unduly, compared to evidence of the risks of asbestos. The paper concludes by agreeing with a previous paper ('Asbestos: a chronology of its origins and health effects', British Journal of Independent Medicine, 1990) and the 1930 report of Edward Mereweather (a factory medical inspector involved in the legislative investigations of the time), that despite theories suggesting a coverup and historical evidence that could be cobbled together after the fact, it is more likely that the issue was one of hindsight.\n\nAccording to Bartrip, Mereweather's 1930 report identified six relevant issues:\n\nThere had been earlier discussion, notably a few brief comments by Factory Inspectors Adelaide Anderson and Edgar Collis during 1898–1911, described by Bartrip as minor reports of no great substance in otherwise very large reports about factory workers. As a result, he concludes that between 1898 and the late 1920s, all that can be said is that, \"the dangers of the material were slowly beginning to be appreciated\". As of 1927, the Senior Medical Inspector had reported that the effect of asbestos dust inhalation \"was as yet imperfectly understood\".\n\nA second paper, by Gee & Greenberg, noted additional factors:\n\nLitigation related to asbestos is regarded as one of the largest litigation cases in legal history in terms of duration, claim size, and scope. Factors responsible for this include:\n\n, trends indicate that the worldwide rate at which people are diagnosed with asbestos-related diseases will likely increase through the next decade. Analysts have estimated that the total cost of asbestos litigation in the USA alone is over $250 billion.\n\nIn the United Kingdom, more people died in 2011 from asbestos-related causes (4721) than in all types of traffic and transport accidents combined, and new reported cases were estimated at 2126.\n\nIn the United States, asbestos litigation is the longest, most expensive mass tort in U.S. history, involving more than 8,400 defendants and 730,000 claimants as of 2002 according to the RAND Corporation, and at least one defendant reported claim counts in excess of $800,000 in 2006.\n\nThe federal legal system in the United States has dealt with numerous counts of asbestos-related suits, which often included multiple plaintiffs with similar symptoms. In 1999 there were 200,000 related cases pending in the federal court system of the United States. Further, it is estimated that within the next 40 years, the number of cases may increase to 700,000. These numbers help explain how there are thousands of current pending cases. Litigation of asbestos materials has been slow. Companies sometimes counter saying that health issues do not currently appear in their worker or workers, or sometimes are settled out of court.\n\nThe volume of the asbestos liability has concerned manufacturers and insurers and reinsurers. The amounts and method of allocating compensation have been the source of many court cases, and government attempts at resolution of existing and future cases.\n\nIn 1999 the United States considered but did not enact the Fairness in Asbestos Compensation Act. Between 1981 and the present, many asbestos companies have filed for bankruptcy. While companies filed for bankruptcy, this limited payouts to those who were actually affected by the material. Christopher Edley, Jr. commented what the 1999 act ultimately would have done if passed would be to \"limit punitive damages that seek retribution for the decisions of long-dead executives for conduct that took place decades ago.\"\n\nLitigation exists outside the United States in England, Scotland, Ireland, the Netherlands, France, Italy, and Japan among other nations (though the amounts awarded in these countries are not as large as in the US). See the companion article for further information.\n\nIn Australia a significant and controversial case was brought against the industrial building materials company James Hardie, which had mined and sold asbestos related products for many years.\n\nAsbestos regulation critics include the asbestos industry and JunkScience.com owner Steven Milloy. Critics argue that the outright banning of dangerous products by government regulation is inferior to keeping the products while innovating ways to prevent the lethal effects. They argue that the product benefits are too important to ignore; instead of banning the products, ways should be found to eliminate risks to those who work with the products.\n\nSome criticisms were subsequently discredited. An example is the discredited suggestion by Dixy Lee Ray and others that the Space Shuttle \"Challenger\" disintegrated because the maker of O-ring putty was pressured by the EPA into ceasing production of asbestos-laden putty. However, the putty used in \"Challenger\"s final flight contained asbestos, and failures in the putty were not responsible for the failure of the O-ring that led to loss of the shuttle.\n\nAsbestos was also used in the first forty floors of the World Trade Center north tower causing an airborne contamination among lower Manhattan after the towers collapsed in the September 11 attacks, causing Steven Milloy of the libertarian Cato Institute to suggest that the World Trade Center towers could still be standing or at least would have stood longer had a 1971 ban not stopped the completion of the asbestos coating above the 64th floor. This was not considered in the National Institute of Standards and Technology's report on the towers' collapse, on the basis that all fireproofing materials, regardless of their construction, are required to obtain a fire-resistance rating prior to installation, and all fiber-based lightweight commercial spray fireproofing materials are vulnerable to the dispersive effects of high speed/high energy impacts, as these are outside the fire testing upon which all ratings are based. Therefore, asbestos would have made little or no difference in preventing the towers' collapse, if used as fireproofing, and upon collapse any asbestos, however used, would still have been largely dispersed into the air within the massive dust cloud.\n\n"}
{"id": "15430615", "url": "https://en.wikipedia.org/wiki?curid=15430615", "title": "Holozoic nutrition", "text": "Holozoic nutrition\n\nHolozoic nutrition (Greek: \"holo\"-whole ; \"zoikos\"-of animals) is a type of heterotrophic nutrition that is characterized by the internalization (ingestion) and internal processing of gaseous, liquid or solid food particles. Protozoa, such as amoebas, and most of the free living animals,such as animals, exhibit this type of nutrition. \n\nIn Holozoic nutrition the energy and organic building blocks are obtained by ingesting and then digesting other organisms or pieces of other organisms, including blood and decaying organic matter. This contrasts with holophytic nutrition, in which energy and organic building blocks are obtained through photosynthesis or chemosynthesis, and with saprozoic nutrition, in which digestive enzymes are released externally and the resulting monomers (small organic molecules) are absorbed directly from the environment.\n\nThere are several stages of holozoic nutrition, which often occur in separate compartments within an organism (such as the stomach and intestines):\n"}
{"id": "2080429", "url": "https://en.wikipedia.org/wiki?curid=2080429", "title": "Human tooth development", "text": "Human tooth development\n\nTooth development or odontogenesis is the complex process by which teeth form from embryonic cells, grow, and erupt into the mouth. For human teeth to have a healthy oral environment, all parts of the tooth must develop during appropriate stages of fetal development. Primary (baby) teeth start to form between the sixth and eighth week of prenatal development, and permanent teeth begin to form in the twentieth week. If teeth do not start to develop at or near these times, they will not develop at all, resulting in Hypodontia or Anodontia.\n\nA significant amount of research has focused on determining the processes that initiate tooth development. It is widely accepted that there is a factor within the tissues of the first pharyngeal arch that is necessary for the development of teeth.\n\nThe tooth germ is an aggregation of cells that eventually forms a tooth. These cells are derived from the ectoderm of the first pharyngeal arch and the ectomesenchyme of the neural crest. The tooth germ is organized into three parts: the enamel organ, the dental papilla and the dental sac or follicle.\n\nThe \"enamel organ \" is composed of the outer enamel epithelium, inner enamel epithelium, stellate reticulum and stratum intermedium. These cells give rise to ameloblasts, which produce enamel and become a part of the reduced enamel epithelium (REE) after maturation of the enamel. The location where the outer enamel epithelium and inner enamel epithelium join is called the cervical loop. The growth of cervical loop cells into the deeper tissues forms Hertwig Epithelial Root Sheath, which determines the root shape of the tooth. During tooth development there are strong similarities between keratinization and amelogenesis. Keratin is also present in epithelial cells of tooth germ and a thin film of keratin is present on a recently erupted tooth (Nasmyth's membrane or enamel cuticle).\n\nThe \"dental papilla\" contains cells that develop into odontoblasts, which are dentin-forming cells. Additionally, the junction between the dental papilla and inner enamel epithelium determines the crown shape of a tooth. Mesenchymal cells within the dental papilla are responsible for formation of tooth pulp.\n\nThe \"dental sac or follicle\" gives rise to three important entities: cementoblasts, osteoblasts, and fibroblasts. Cementoblasts form the cementum of a tooth. Osteoblasts give rise to the alveolar bone around the roots of teeth. Fibroblasts are involved developing the periodontal ligament which connect teeth to the alveolar bone through cementum.\n\nNGF-R is present in the condensing ectomesenchymal cells of the dental papilla in the early cap stage tooth germ and plays multiple roles during morphogenetic and cytodifferentiation events in the tooth. There is a relationship between tooth agenesis and absence of the peripheral trigeminal nerve (see Hypodontia).\n\nAll stages (bud, cap, bell, crown), growth and morphogenesis of the teeth are regulated by a protein called sonic hedgehog.\n\nVarious phenotypic inputs modulate the size of the teeth.\n\nParathyroid hormone is required for tooth eruption.\n\nThe following tables present the development timeline of human teeth. Times for the initial calcification of primary teeth are for weeks \"in utero\". Abbreviations: wk = weeks; mo = months; yr = years.\n\nTooth development is commonly divided into the following stages: the initiation stage, the bud stage, the cap stage, the bell stage, and finally maturation. The staging of tooth development is an attempt to categorize changes that take place along a continuum; frequently it is difficult to decide what stage should be assigned to a particular developing tooth. This determination is further complicated by the varying appearance of different histologic sections of the same developing tooth, which can appear to be different stages.\n\nOne of the earliest signs in the formation of a tooth that can be seen microscopically is the distinction between the vestibular lamina and the dental lamina. The dental lamina connects the developing tooth bud to the epithelial layer of the mouth for a significant time. This is regarded as the initiation stage.\n\nThe bud stage is characterized by the appearance of a tooth bud without a clear arrangement of cells. The stage technically begins once epithelial cells proliferate into the ectomesenchyme of the jaw. Typically, this occurs when the fetus is around 8 weeks old. The tooth bud itself is the group of cells at the periphery of the dental lamina.\n\nAlong with the formation of the dental lamina, 10 round epithelial structures, each referred to as a bud, develop at the distal aspect of the dental lamina of each arch. These correspond to the 10 primary teeth of each dental arch, and they signify the bud stage of tooth development. Each bud is separated from the ectomesenchyme by a basement membrane. Ectomesenchymal cells congregate deep to the bud, forming a cluster of cells, which is the initiation of the condensation of the ectomesenchyme. The remaining ectomesenchymal cells are arranged in a more or less haphazardly uniform fashion.\n\nThe first signs of an arrangement of cells in the tooth bud occur in the cap stage. A small group of ectomesenchymal cells stops producing extracellular substances, which results in an aggregation of these cells called the dental papilla. At this point, the tooth bud grows around the ectomesenchymal aggregation, taking on the appearance of a cap, and becomes the enamel (or dental) organ covering the dental papilla. A condensation of ectomesenchymal cells called the dental sac or follicle surrounds the enamel organ and limits the dental papilla. Eventually, the enamel organ will produce enamel, the dental papilla will produce dentin and pulp, and the dental sac will produce all the supporting structures of a tooth, the periodontium.\n\nThe bell stage is known for the histodifferentiation and morphodifferentiation that takes place. The dental organ is bell-shaped during this stage, and the majority of its cells are called stellate reticulum because of their star-shaped appearance. The bell stage is divided into the \"early bell stage\" and the \"late bell stage\". Cells on the periphery of the enamel organ separate into four important layers. Cuboidal cells on the periphery of the dental organ are known as outer enamel epithelium (OEE). The columnar cells of the enamel organ adjacent to the enamel papilla are known as inner enamel epithelium (IEE). The cells between the IEE and the stellate reticulum form a layer known as the stratum intermedium. The rim of the enamel organ where the outer and inner enamel epithelium join is called the \"cervical loop\". \n\nIn summary, the layers in order of innermost to outermost consist of dentin, enamel (formed by IEE, or 'ameloblasts', as they move outwards/upwards), inner enamel epithelium and stratum intermedium (stratified cells that support the synthetic activity of the inner enamel epithelium) What follows is part of the initial 'enamel organ', the center of which is made up of stellate reticulum cells that serve to protect the enamel organ. This is all encased by the OEE layer.\n\nOther events occur during the bell stage. The dental lamina disintegrates, leaving the developing teeth completely separated from the epithelium of the oral cavity; the two will not join again until the final eruption of the tooth into the mouth.\n\nThe crown of the tooth, which is influenced by the shape of the inner enamel epithelium, also takes shape during this stage. Throughout the mouth, all teeth undergo this same process; it is still uncertain why teeth form various crown shapes—for instance, incisors versus canines. There are two dominant hypotheses. The \"field model\" proposes there are components for each type of tooth shape found in the ectomesenchyme during tooth development. The components for particular types of teeth, such as incisors, are localized in one area and dissipate rapidly in different parts of the mouth. Thus, for example, the \"incisor field\" has factors that develop teeth into incisor shape, and this field is concentrated in the central incisor area, but decreases rapidly in the canine area. \n\nThe other dominant hypothesis, the \"clone model\", proposes that the epithelium programs a group of ectomesenchymal cells to generate teeth of particular shapes. This group of cells, called a clone, coaxes the dental lamina into tooth development, causing a tooth bud to form. Growth of the dental lamina continues in an area called the \"progress zone\". Once the progress zone travels a certain distance from the first tooth bud, a second tooth bud will start to develop. These two models are not necessarily mutually exclusive, nor does widely accepted dental science consider them to be so: it is postulated that both models influence tooth development at different times.\n\nOther structures that may appear in a developing tooth in this stage are enamel knots, enamel cords, and enamel niche.\n\nHard tissues, including enamel and dentin, develop during the next stage of tooth development. This stage is called the crown, or maturation stage, by some researchers. Important cellular changes occur at this time. In prior stages, all of the IEE cells were dividing to increase the overall size of the tooth bud, but rapid dividing, called mitosis, stops during the crown stage at the location where the cusps of the teeth form. The first mineralized hard tissues form at this location. At the same time, the IEE cells change in shape from cuboidal to columnar and become preameloblasts. The nuclei of these cells move closer to the stratum intermedium and away from the dental papilla as they become polarized.\n\nThe adjacent layer of cells in the dental papilla suddenly increases in size and differentiates into odontoblasts, which are the cells that form dentin. Researchers believe that the odontoblasts would not form if it were not for the changes occurring in the IEE. As the changes to the IEE and the formation of odontoblasts continue from the tips of the cusps, the odontoblasts secrete a substance, an organic matrix, into their immediate surrounding. The organic matrix contains the material needed for dentin formation. As odontoblasts deposit organic matrix termed predentin, they migrate toward the center of the dental papilla. Thus, unlike enamel, dentin starts forming in the surface closest to the outside of the tooth and proceeds inward. Cytoplasmic extensions are left behind as the odontoblasts move inward. The unique, tubular microscopic appearance of dentin is a result of the formation of dentin around these extensions.\n\nAfter dentin formation begins, the cells of the IEE secrete an organic matrix against the dentin. This matrix immediately mineralizes and becomes the initial layer of the tooth's enamel. Outside the dentin are the newly formed ameloblasts in response to the formation of dentin, which are cells that continue the process of enamel formation; therefore, enamel formation moves outwards, adding new material to the outer surface of the developing tooth.\n\nEnamel formation is called amelogenesis and occurs in the crown stage (advanced bell stage) of tooth development. \"Reciprocal induction\" governs the relationship between the formation of dentin and enamel; dentin formation must always occur before enamel formation. Generally, enamel formation occurs in two stages: the secretory and maturation stages. Proteins and an organic matrix form a partially mineralized enamel in the secretory stage; the maturation stage completes enamel mineralization.\n\nIn the secretory stage, ameloblasts release enamel proteins that contribute to the enamel matrix, which is then partially mineralized by the enzyme alkaline phosphatase. This mineralized phase occurs very early around the 3rd or 4th month of pregnancy. This marks the first appearance of enamel in the body. Ameloblasts make enamel at the location of where the cusps of the teeth are located. Enamel grows outwards, away from the center of the tooth.\n\nIn the maturation stage, the ameloblasts transport some of the substances used in enamel formation out of the enamel. Thus, the function of ameloblasts changes from enamel production, as occurs in the secretory stage, to transportation of substances. Most of the materials transported by ameloblasts in this stage are proteins used to complete mineralization. The important proteins involved are amelogenins, ameloblastins, enamelins, and tuftelins. By the end of this stage, the enamel has completed its mineralization.\n\nA residue may form on newly erupted teeth of both dentitions that may leave the teeth extrinsically stained. This green-gray residue, Nasmyth membrane, consists of the fused tissue of the reduced enamel epithelium and oral epithelium, as well as the dental cuticle placed by the ameloblasts on the newly formed outer enamel surface. Nasmyth membrane then easily picks up stain from food debris and is hard to remove except by selective polishing. The child’s supervising adults may need reassurance that it is only an extrinsic stain on a child’s newly erupted teeth.\n\nPatients with osteopetrosis display enamel abnormalities, suggesting that the a3 gene mutation found in V-ATPases also plays a role in the development of hypomineralized and hypoplastic enamel. \n\nDentin formation, known as dentinogenesis, is the first identifiable feature in the crown stage of tooth development. The formation of dentin must always occur before the formation of enamel. The different stages of dentin formation result in different types of dentin: mantle dentin, primary dentin, secondary dentin, and tertiary dentin.\n\nOdontoblasts, the dentin-forming cells, differentiate from cells of the dental papilla. They begin secreting an organic matrix around the area directly adjacent to the inner enamel epithelium, closest to the area of the future cusp of a tooth. The organic matrix contains collagen fibers with large diameters (0.1–0.2 μm in diameter). The odontoblasts begin to move toward the center of the tooth, forming an extension called the odontoblast process. Thus, dentin formation proceeds toward the inside of the tooth. The odontoblast process causes the secretion of hydroxyapatite crystals and mineralization of the matrix. This area of mineralization is known as mantle dentin and is a layer usually about 150 μm thick.\n\nWhereas mantle dentin forms from the preexisting ground substance of the dental papilla, primary dentin forms through a different process. Odontoblasts increase in size, eliminating the availability of any extracellular resources to contribute to an organic matrix for mineralization. Additionally, the larger odontoblasts cause collagen to be secreted in smaller amounts, which results in more tightly arranged, heterogeneous nucleation that is used for mineralization. Other materials (such as lipids, phosphoproteins, and phospholipids) are also secreted.\n\nSecondary dentin is formed after root formation is finished and occurs at a much slower rate. It is not formed at a uniform rate along the tooth, but instead forms faster along sections closer to the crown of a tooth. This development continues throughout life and accounts for the smaller areas of pulp found in older individuals. Tertiary dentin, also known as reparative dentin, forms in reaction to stimuli, such as attrition or dental caries.\n\nCementum formation is called cementogenesis and occurs late in the development of teeth. Cementoblasts are the cells responsible for cementogenesis. Two types of cementum form: cellular and acellular.\n\nAcellular cementum forms first. The cementoblasts differentiate from follicular cells, which can only reach the surface of the tooth's root once Hertwig's Epithelial Root Sheath (HERS) has begun to deteriorate. The cementoblasts secrete fine collagen fibrils along the root surface at right angles before migrating away from the tooth. As the cementoblasts move, more collagen is deposited to lengthen and thicken the bundles of fibers. Noncollagenous proteins, such as bone sialoprotein and osteocalcin, are also secreted. Acellular cementum contains a secreted matrix of proteins and fibers. As mineralization takes place, the cementoblasts move away from the cementum, and the fibers left along the surface eventually join the forming periodontal ligaments.\n\nCellular cementum develops after most of the tooth formation is complete and after the tooth occludes (in contact) with a tooth in the opposite arch. This type of cementum forms around the fiber bundles of the periodontal ligaments. The cementoblasts forming cellular cementum become trapped in the cementum they produce.\n\nThe origin of the formative cementoblasts is believed to be different for cellular cementum and acellular cementum. One of the major current hypotheses is that cells producing cellular cementum migrate from the adjacent area of bone, while cells producing acellular cementum arise from the dental follicle. Nonetheless, it is known that cellular cementum is usually not found in teeth with one root. In premolars and molars, cellular cementum is found only in the part of the root closest to the apex and in interradicular areas between multiple roots.\n\nThe periodontium, which is the supporting structure of a tooth, consists of the cementum, periodontal ligaments, gingiva, and alveolar bone. Cementum is the only one of these that is a part of a tooth. Alveolar bone surrounds the roots of teeth to provide support and creates what is commonly called a \"socket\". Periodontal ligaments connect the alveolar bone to the cementum, and the gingiva is the surrounding tissue visible in the mouth.\n\nCells from the dental follicle give rise to the periodontal ligament (PDL). Specific events leading to the formation of the periodontal ligament vary between deciduous (baby) and permanent teeth and among various species of animals. Nonetheless, formation of the periodontal ligament begins with ligament fibroblasts from the dental follicle. These fibroblasts secrete collagen, which interacts with fibers on the surfaces of adjacent bone and cementum. \n\nThis interaction leads to an attachment that develops as the tooth erupts into the mouth. The occlusion, which is the arrangement of teeth and how teeth in opposite arches come in contact with one another, continually affects the formation of periodontal ligament. This perpetual creation of periodontal ligament leads to the formation of groups of fibers in different orientations, such as horizontal and oblique fibers.\n\nAs root and cementum formation begin, bone is created in the adjacent area. Throughout the body, cells that form bone are called osteoblasts. In the case of alveolar bone, these osteoblast cells form from the dental follicle. Similar to the formation of primary cementum, collagen fibers are created on the surface nearest the tooth, and they remain there until attaching to periodontal ligaments.\n\nLike any other bone in the human body, alveolar bone is modified throughout life. Osteoblasts create bone and osteoclasts destroy it, especially if force is placed on a tooth. As is the case when movement of teeth is attempted through orthodontics using bands, wires, or appliances, an area of bone under compressive force from a tooth moving toward it has a high osteoclast level, resulting in bone resorption. An area of bone receiving tension from periodontal ligaments attached to a tooth moving away from it has a high number of osteoblasts, resulting in bone formation. Thus, the tooth or teeth are slowly moved along the jaw so as to achieve a dentition that works in harmony. In this way, the width of the space between the alveoli and the root is kept about the same.\n\nThe connection between the gingiva and the tooth is called the dentogingival junction. This junction has three epithelial types: gingival, sulcular, and junctional epithelium. These three types form from a mass of epithelial cells known as the epithelial cuff between the tooth and the mouth.\n\nMuch about gingival formation is not fully understood, but it is known that hemidesmosomes form between the gingival epithelium and the tooth and are responsible for the \"primary epithelial attachment\". Hemidesmosomes provide anchorage between cells through small filament-like structures provided by the remnants of ameloblasts. Once this occurs, junctional epithelium forms from reduced enamel epithelium, one of the products of the enamel organ, and divides rapidly. This results in the perpetually increasing size of the junctional epithelial layer and the isolation of the remnants of ameloblasts from any source of nutrition. As the ameloblasts degenerate, a gingival sulcus is created.\n\nFrequently, nerves and blood vessels run parallel to each other in the body, and the formation of both usually takes place simultaneously and in a similar fashion. However, this is not the case for nerves and blood vessels around the tooth, because of different rates of development.\n\nNerve fibers start to near the tooth during the cap stage of tooth development and grow toward the dental follicle. Once there, the nerves develop around the tooth bud and enter the dental papilla when dentin formation has begun. Nerves never proliferate into the enamel organ.\n\nBlood vessels grow in the dental follicle and enter the dental papilla in the cap stage. Groups of blood vessels form at the entrance of the dental papilla. The number of blood vessels reaches a maximum at the beginning of the crown stage, and the dental papilla eventually forms in the pulp of a tooth. Throughout life, the amount of pulpal tissue in a tooth decreases, which means that the blood supply to the tooth decreases with age. The enamel organ is devoid of blood vessels because of its epithelial origin, and the mineralized tissues of enamel and dentin do not need nutrients from the blood.\n\nTooth eruption occurs when the teeth enter the mouth and become visible. Although researchers agree that tooth eruption is a complex process, there is little agreement on the identity of the mechanism that controls eruption. Some commonly held theories that have been disproven over time include: (1) the tooth is pushed upward into the mouth by the growth of the tooth's root, (2) the tooth is pushed upward by the growth of the bone around the tooth, (3) the tooth is pushed upward by vascular pressure, and (4) the tooth is pushed upward by the cushioned hammock. The cushioned hammock theory, first proposed by Harry Sicher, was taught widely from the 1930s to the 1950s. This theory postulated that a ligament below a tooth, which Sicher observed under a microscope on a histologic slide, was responsible for eruption. Later, the \"ligament\" Sicher observed was determined to be merely an artifact created in the process of preparing the slide.\n\nThe most widely held current theory is that while several forces might be involved in eruption, the periodontal ligaments provide the main impetus for the process. Theorists hypothesize that the periodontal ligaments promote eruption through the shrinking and cross-linking of their collagen fibers and the contraction of their fibroblasts.\n\nAlthough tooth eruption occurs at different times for different people, a general eruption timeline exists. Typically, humans have 20 primary (baby) teeth and 32 permanent teeth. Tooth eruption has three stages. The first, known as deciduous dentition stage, occurs when only primary teeth are visible. Once the first permanent tooth erupts into the mouth, the teeth are in the mixed (or transitional) dentition. After the last primary tooth falls out of the mouth—a process known as exfoliation—the teeth are in the permanent dentition.\n\nPrimary dentition starts on the arrival of the mandibular central incisors, usually at eight months, and lasts until the first permanent molars appear in the mouth, usually at six years. The primary teeth typically erupt in the following order: (1) central incisor, (2) lateral incisor, (3) first molar, (4) canine, and (5) second molar. As a general rule, four teeth erupt for every six months of life, mandibular teeth erupt before maxillary teeth, and teeth erupt sooner in females than males. During primary dentition, the tooth buds of permanent teeth develop below the primary teeth, close to the palate or tongue.\n\nMixed dentition starts when the first permanent molar appears in the mouth, usually at six years, and lasts until the last primary tooth is lost, usually at eleven or twelve years. Permanent teeth in the maxilla erupt in a different order from permanent teeth on the mandible. Maxillary teeth erupt in the following order: (1) first molar (2) central incisor, (3) lateral incisor, (4) first premolar, (5) second premolar, (6) canine, (7) second molar, and (8) third molar. Mandibular teeth erupt in the following order: (1) first molar (2) central incisor, (3) lateral incisor, (4) canine, (5) first premolar, (6) second premolar, (7) second molar, and (8) third molar. Since there are no premolars in the primary dentition, the primary molars are replaced by permanent premolars. If any primary teeth are lost before permanent teeth are ready to replace them, some posterior teeth may drift forward and cause space to be lost in the mouth. This may cause crowding and/or misplacement once the permanent teeth erupt, which is usually referred to as malocclusion. Orthodontics may be required in such circumstances for an individual to achieve a straight set of teeth.\n\nThe permanent dentition begins when the last primary tooth is lost, usually at 11 to 12 years, and lasts for the rest of a person's life or until all of the teeth are lost (edentulism). During this stage, third molars (also called \"wisdom teeth\") are frequently extracted because of decay, pain or impactions. The main reasons for tooth loss are decay and periodontal disease.\n\nImmediately after the eruption enamel is covered by a specific film: Nasmyth's membrane or 'enamel cuticle', structure of embryological origin is composed of keratin which gives rise to the enamel organ.\n\nAs in other aspects of human growth and development, nutrition has an effect on the developing tooth. Essential nutrients for a healthy tooth include calcium, phosphorus, and vitamins A, C, and D. Calcium and phosphorus are needed to properly form the hydroxyapatite crystals, and their levels in the blood are maintained by Vitamin D. Vitamin A is necessary for the formation of keratin, as Vitamin C is for collagen. Fluoride, although not a nutrient, is incorporated into the hydroxyapatite crystal of a developing tooth and bones. The dental theory is the low levels of fluoride incorporation and very mild fluorosis makes the tooth more resistant to demineralization and subsequent decay.\n\nDeficiencies of nutrients can have a wide range of effects on tooth development. In situations where calcium, phosphorus, and vitamin D are deficient, the hard structures of a tooth may be less mineralized. A lack of vitamin A can cause a reduction in the amount of enamel formation. \n\nFluoride ingestion has been noted to delay eruption of teeth for as much as a year or more from the accepted eruption dates since the initial 1940s fluoridation trials. Researchers theorize that the delay is a manifestation of fluoride's depressing impact on thyroid hormones. The delay in eruption has been suggested as the reason for the apparent difference in decay among the youngest children. Fluoride ingestion during tooth development can lead to a permanent condition known as fluorosis with varying levels of severity, the result of fluoride's interference with the normal osteoblast development. \n\nUndiagnosed and untreated celiac disease often causes dental enamel defects and can be the only manifestation of the disease, in absence of gastrointestinal symptoms or malabsorption signs.\n\nBisphenol A (BPA) is a hormone-disrupting chemical that has been implicated in having negative effects on human health, including, but not limited to, fetal development. As shown in animal studies which mimic human enamel, the mother’s consumption of products with BPA during pregnancy can lead to the child’s tooth development being obstructed. Those children are shown to be prone to incisor and first molar hypomineralization, a weakened state of the enamel. Additionally, it is most important for mother’s to avoid BPA during pregnancy, but also avoid BPA-use in the child’s products up to five months of age.\n\nAnodontia is a complete lack of tooth development, and hypodontia is a lack of some tooth development. Anodontia is rare, most often occurring in a condition called Hypohidrotic ectodermal dysplasia, while hypodontia is one of the most common developmental abnormalities, affecting 3.5–8.0% of the population (not including third molars). The absence of third molars is very common, occurring in 20–23% of the population, followed in prevalence by the second premolar and lateral incisor. Hypodontia is often associated with the absence of a dental lamina, which is vulnerable to environmental forces, such as infection and chemotherapy medications, and is also associated with many syndromes, such as Down syndrome and Crouzon syndrome.\n\nHyperdontia is the development of extraneous teeth. It occurs in 1–3% of Caucasians and is more frequent in Asians. About 86% of these cases involve a single extra tooth in the mouth, most commonly found in the maxilla, where the incisors are located. Hyperdontia is believed to be associated with an excess of dental lamina.\n\nDilaceration is an abnormal bend found on a tooth, and is nearly always associated with trauma that moves the developing tooth bud. As a tooth is forming, a force can move the tooth from its original position, leaving the rest of the tooth to form at an abnormal angle. Cysts or tumors adjacent to a tooth bud are forces known to cause dilaceration, as are primary (baby) teeth pushed upward by trauma into the gingiva where it moves the tooth bud of the permanent tooth.\n\nEnamel hypoplasia or hypomineralization is a defect of the teeth caused by a disturbance in the formation of the organic enamel matrix, clinically visible as enamel defects. It may be caused by nutritional factors, some diseases (such as undiagnosed and untreated celiac disease, chicken pox, congenital syphilis), hypocalcemia, fluoride ingestion, birth injury, preterm birth, infection or trauma from a deciduous tooth.\n\nSome systemic conditions may cause delayed tooth development, such as nutritional factors, endocrine disorders (hypothyroidism, hypopituitarism, hypoparathyroidism, pseudohypoparathyroidism), undiagnosed and untreated celiac disease, anemia, prematurity, low birth weight, renal failure, heavy metal intoxication or tobacco smoke, among others.\n\nRegional odontodysplasia is rare, but is most likely to occur in the maxilla and anterior teeth. The cause is unknown; a number of causes have been postulated, including a disturbance in the neural crest cells, infection, radiation therapy, and a decrease in vascular supply (the most widely held hypothesis). Teeth affected by regional odontodysplasia nevAmelogenesis imperfecta is an autosomal dominant disease characterized by a defect in dental enamel formation. Teeth are often free of enamel, small, misshapen, and tinted brown. The cause of these deformities is due to a mutation in enamel in expression. Dental patients with this disease should be especially cautious and visit their dentist frequently.\nNatal and neonatal teeth are an anomaly that involves teeth erupting in a newborn infant’s mouth earlier than usual. The incidence ranges from 1:2,000 to 1:3,500 births. Natal teeth are more frequent, approximately three times more common than neonatal teeth. Some authors reported a higher prevalence in females than males. The most common location is the mandibular region of the central incisors. Natal teeth and neonatal teeth are associated with genetics, developmental abnormalities and certain recognized syndromes. Additional names for this condition include precocious dentition, baby teeth, and milk teeth.\n\n\n\n"}
{"id": "50592", "url": "https://en.wikipedia.org/wiki?curid=50592", "title": "Hyperlexia", "text": "Hyperlexia\n\nHyperlexia is a syndrome characterized by a child's precocious ability to read. It was initially identified by Norman E. Silberberg and Margaret C. Silberberg (1967), who defined it as the precocious ability to read words without prior training in learning to read, typically before the age of 5. They indicated that children with hyperlexia have a significantly higher word-decoding ability than their reading comprehension levels. Children with hyperlexia also present with an intense fascination for written material at a very early age.\n\nHyperlexic children are characterized by having average or above-average IQs, and word-reading ability well above what would be expected given their age. First named and scientifically described in 1967 (Silverberg and Silverberg), it can be viewed as a superability in which word recognition ability goes far above expected levels of skill. Some hyperlexics, however, have trouble understanding speech. Some experts believe that most children with hyperlexia, or perhaps even all of them, lie on the autism spectrum. However, one expert, Darold Treffert, proposes that hyperlexia has subtypes, only some of which overlap with autism. Between 5 and 20 percent of autistic children have been estimated to be hyperlexic.\n\nHyperlexic children are often fascinated by letters or numbers. They are extremely good at decoding language and thus often become very early readers. Some hyperlexic children learn to spell long words (such as \"elephant\") before they are two years old and learn to read whole sentences before they turn three.\nAn fMRI study of a single child showed that hyperlexia may be the neurological opposite of dyslexia.\n\nThe word hyperlexia is derived from the Greek terms \"hyper\" (\"over\") and \"léxis\" (\"diction\", \"word\").\n\nAlthough hyperlexic children usually learn to read in a non-communicative way, several studies have shown that they can acquire reading comprehension and communicative language after the onset of hyperlexia. They follow a different developmental trajectory relative to neurotypical individuals, with milestones being acquired in a different order. Despite hyperlexic children's precocious reading ability, they may struggle to communicate. Often, hyperlexic children will have a precocious ability to read but will learn to speak only by rote and heavy repetition, and may also have difficulty learning the rules of language from examples or from trial and error, which may result in social problems. Their language may develop using echolalia, often repeating words and sentences. Often, the child has a large vocabulary and can identify many objects and pictures, but cannot put their language skills to good use. Spontaneous language is lacking and their pragmatic speech is delayed. Hyperlexic children often struggle with Who? What? Where? Why? and How? questions. Between the ages of 4 and 5 years old, many children make great strides in communicating.\n\nThe social skills of a child with hyperlexia often lag tremendously. Hyperlexic children often have far less interest in playing with other children than do their peers.\n\nIn one paper, Darold Treffert proposes three types of hyperlexia. Specifically:\n\nA different paper by Rebecca Williamson Brown, OD proposes only two types of hyperlexia. These are:\n\nIn studies in Cantonese and Korean, subjects were able to read non-words in their native orthography without a delay relative to the speed with which they read real words in their native orthography. There is a delay noted with exception words in English, including the examples: chaos, unique, and enough. These studies also illustrate difficulties in understanding what it is that they are reading. The findings suggest that non-hyperlexic readers rely more heavily on word semantics in order to make inferences about word meaning.\n\nThe Cantonese study distinguish homographs and determine the readings for rarely used characters. In this study, the subject also made errors of phonetic analogy and regularization of sound. The authors of the study, Weekes, Wong, Iao, To, and Su, suggest that the two routes model for reading Chinese characters may be in effect for hyperlexics. The two routes model describes understanding of Chinese characters in a purely phonetic sense and the understanding of Chinese characters in a semantic sense.\n\nThe semantics deficit is also illustrated in the study of Korean hyperlexics through a priming experiment. Non-hyperlexic children read words primed with a related image faster than non-primed words while hyperlexics read them at the same pace. Lee Sunghee and Hwang Mina, the authors of the Korean study, also found that hyperlexics have fewer errors in non-word reading than non-hyperlexics. They suggest that this may be because of an imbalance in the phonological, orthographical, and semantic understandings of the subjects’ native language and writing system, in this case, Hangul. This combination of the parts of linguistics is known as connectionist theory, in which non-words are distinguished from words by differences in interaction between phonology, orthography, and semantics.\n\nIn the Lee and Hwang study, the subjects scored lower on general language test and vocabulary tests than the average for their age groups. Literacy education in South Korea involves teaching students entire words, rather than starting with the relationship between phonemes and letters in Hangul, despite evidence that letter name knowledge is useful for learning to read words that have not been taught. The results suggest that hyperlexics are able to obtain the relations between letters (or the smallest unit of the writing system) and their phonemes without knowing the names.\n\nComprehension difficulties can also be a result of hyperlexia. Semantics and comprehension both have ties to meaning. Semantics relates to the meaning of a certain word while comprehension is the understanding of a longer text. In both studies, interpretation and meaning based tests proved difficult for the hyperlexic subjects. In the Weeks study, the subject was unable to identify characters based on the logographic aspect of the writing system, and in the Lee and Hwang study, priming was ineffective in decreasing reading times for hyperlexics.\n\nAlthough it is generally associated with autism, a 69-year-old woman appears to have been made hyperlexic because of a \"cerebral infarction in the left anterior cingulate cortex and corpus callosum\".\n\n"}
{"id": "7962104", "url": "https://en.wikipedia.org/wiki?curid=7962104", "title": "International Federation of Medical Students' Associations", "text": "International Federation of Medical Students' Associations\n\nThe International Federation of Medical Students' Associations (IFMSA) is a non governmental organization representing associations of medical students. It was founded in May 1951 and currently maintains 135 member organisations from 125 countries on six continents.\n\nThe International Federation of Medical Students' Associations was one of the numerous international student organizations set up directly after the end of the Second World War. The first meeting that saw the establishment of the Federation was held in Copenhagen, Denmark in May 1951. The first members of this new organization were England, Austria, the Federal Republic of Germany, Finland, Norway, Sweden, the Netherlands, Switzerland and Denmark. London saw the first General Assembly of IFMSA in July 1952. The meeting had a total of thirty participants from ten countries.\n\nStarting from the exclusively European founding organizations the Federation has expanded to include 135 members from all over the world.\n\nIFMSA has always focused on student mobility and exchange as well as conference and workshop organization. The first conferences were the Student International Clinical Conferences, which were quite successful in the 1950s. Various summer schools have been organized through the years, starting in 1963 in Denmark, the UK and Scandinavia. Other conferences have discussed medical education, drugs and AIDS and HIV issues. In the 1960s projects were organized to help less advantaged students in developing countries the Book Aid project, which sought to send medical books from wealthier nations and the Equipment Appeal, which promoted the shipping of surplus medical equipment to these countries.\n\nThe 1970s medical students saw a need for the decentralization of IFMSA. To this aim, IFMSA contributed to the creation of regional medical student organizations in Africa and Asia. Subsequently, regional vice-presidents were elected for six regions as a way of promoting regionalization but this structure was abandoned after a few years.\n\nIn the early 1980s IFMSA issued a number of resolutions and declarations on topics ranging from Medical Education to the Prevention of Nuclear War to Primary Health Care. In the late 80's there was a push towards organizing projects that would be able to make a change locally and thus the Village Concept Project idea was born after collaboration with other international student organizations. 1986 also saw the start of the Leadership Training Programs in collaboration with World Health Organization. These training programs are still active today.\n\nOfficial relations with WHO started back in 1969, when the collaboration resulted in the organization of a symposium on \"Programmed Learning in Medical Education\", as well as immunology and tropical medicine programs. In the following years, IFMSA and WHO collaborated in the organization of a number of workshops and training programs. IFMSA has been collaborating with UNESCO since 1971. Since 2007 IFMSA has been an official supporting organization of HIFA2015 (Healthcare Information For All by 2015).\n\nThe main activities of IFMSA are student exchanges and different projects. Around 10 000 medical students each year participate in international medical student exchanges, both professional and academic (research). IFMSA organizes projects, programs, seminars and workshops on areas of public health, medical education, reproductive health and human rights and peace.\n\nAll activities of the IFMSA are linked to one of its six standing committees, which are:\n\nTo be a forum for medical students throughout the world to discuss topics related to health, education and medicine; to formulate policies from such discussions and to carry out appropriate activities;\nTo promote humanitarian ideals and medical ethics amongst medical students;\nTo act as a mechanism for medical students' professional and scientific exchange and projects;\nTo be a body through which cooperation and contacts with other international organizations are established;\nTo act as a mechanism for member organizations to raise funds for projects recognized by the IFMSA.\n\nThe goal of the Federation is to serve society and medical students all over the world through its member organisations by:\n\nTo be able to work together, IFMSA has created an organizational structure, which facilitates the flow of ideas and enables the development of activities.\n\nIFMSA is composed of medical students' associations from 125 different countries, from which are formed 135 members of IFMSA and are called National Member Organizations (NMO's). All the activities of IFMSA are organized by the NMO's. Each NMO has its own identity. Most NMO's have Local Committees at the medical schools in their country. The Local Committees coordinate and organize IFMSA activities. Through these Local Committees the NMO's are in direct contact with the medical students.\n\nIFMSA is a federation, which respects the autonomy of its members, the NMO's. The NMO's can decide which activities they take part in and what new activities should be developed.\n\nIFMSA as an umbrella organization embraces 135 national and local organisations in 125 countries on six continents.\n\nThe decision making process is in the hands of the General Assembly, the executive process is in the hands of the International Board and the controlling power is in the hands of the Supervising Council.\n\nThe General Assembly (GA) is composed of representatives of all NMO's, and meets twice a year in March and August. The General Assembly decides the activities of IFMSA, the regulations, and the management and elects the Team of Officials and the Supervising Council.\n\nThe General Assembly elects the Team of Officials (TO) each year. It is composed of the Executive Board (EB), Standing Committee Directors, Liaison Officers and Regional Directors. The Executive Board is responsible for the daily management of the Federation and deals with issues such as fundraising, marketing, external relations, finances, administration, development and support to National Member Organisations.\n\nStanding Committee Directors are assigned to one field of activities. They coordinate the Standing Committee, which carries out these activities. They give support to national and local officers, prepare the meetings of the Standing Committee and are responsible for development of new activities.\n\nLiaison Officers maintain the contacts with important external relations and represent IFMSA towards those organizations. The Regional Directors are responsible of each of the IFMSA 5 Regions.\n\nThe Supervising Council is elected by the General Assembly to evaluate and supervise the work of the IFMSA officials and undertakes actions in case problems arise.\n\n\nTwice a year, the delegations of the NMO's of IFMSA get together at the IFMSA General Assemblies. The March Meeting and the August Meeting bring together several hundred (600–900) medical students from all around the world. During the seven days of the meetings, the delegates discuss matters of the Federation and make valuable contacts for their organizations.\n\nThe General Assembly remains the highest decision making body of the Federation although the actual program of the meeting has developed into a mixture of training, planning and evaluation sessions in between the legislative Plenary Sessions. The work in the Standing Committees of IFMSA is the essence for most delegates: signing contracts, presenting and planning projects are all in a day's work for the National Officers from the participating organizations. An extensive Training and Resource Development component has been integrated in the program, providing the participants with new skills needed in their work, but seldom found in the regular University curriculum: Lobbying Skills, Group Dynamics and Strategic Planning to name a few.\n\nWith IFMSA being much more than just exchanges, the Project Fair has evolved to extend presentations of local, national and international projects outside the Standing Committees. Round Table Discussions enable the student delegates to discuss currently relevant topics with invited experts and short lectures are frequently given by representatives from, for example, different UN agencies.\n\nA special Financial Committee is elected at each General Assembly to audit the budget and finances of the Federation. Other legislative proceedings at the meetings include adoption of reports from the Executive Board members, IFMSA Officials and IFMSA projects, and adoption of official IFMSA Policy Statements. Guidelines on which areas IFMSA should focus on and other important decisions for the future of the Federation are prepared by the Presidents of the National Member Organizations together with the Executive Board and brought for approval to the Plenary Sessions.\n\nHosting a General Assembly is an honor, but also a great trial for the Organizing Committee. The entire process is run by voluntary students, from the fundraising to the implementation, and shows that the power of IFMSA indeed lies at the local level. As with any international meeting, the social program is important in providing relaxation between the long hours of work, and giving the hosts an opportunity to show their culture and creativity to their foreign friends.\n\nIn addition to the General Assemblies, the National Member Organizations are also getting together for Regional Meetings. These meetings follow a similar agenda as General Assemblies, and are organized yearly for each region. They allow more students of the Region to gather, because the meeting is always in the Region, and the agenda is focused on regionally relevant topics.\n\nThe IFMSA won on October 3, 2014 the Health Systems Award - Civil Organization. This award recognizes the organization that best employs social media to engage civil society in health systems-related dialogue. It recognizes the work the Federation has been doing all around the world to connect and engage with health communities around the world so to shape the sustainable and healthy future we want.\n\n"}
{"id": "50250624", "url": "https://en.wikipedia.org/wiki?curid=50250624", "title": "Jan van Bemmel", "text": "Jan van Bemmel\n\nJan H. van Bemmel (born 17 November 1938) is a Dutch former professor of medical informatics at the Vrije Universiteit Amsterdam and the Erasmus University Rotterdam. He was rector magnificus of the Erasmus University Rotterdam between 2000 and 2003.\n\nVan Bemmel was born in Rotterdam on 17 November 1938. He studied physics at Delft University of Technology, where he obtained a degree in 1963. He subsequently started working for the Medical-Physical Institute of the Netherlands Organisation for Applied Scientific Research (TNO). At the institute he was head of the workgroup of biomedical signalanalysis. Six years later he earned his Doctor title at the faculty of physics and mathematics of the Radboud University Nijmegen. His thesis concerned challenges of signal processing applied to fetal electrocardiography.\n\nIn 1973 Van Bemmel stopped working for TNO and was appointed as professor of medical informatics at the Vrije Universiteit Amsterdam. In 1987 Van Bemmel became professor of medical informatics at the Erasmus University Rotterdam. He served as \"rector magnificus\" of the University between 2000 and 2003.\n\nHis research has amongst other topics focused on biomedical signal and image analysis, medical information systems and electronic health records. He started his work on the latter during the 1980s and later expanded into the signal and image analytics. The American Medical Informatics Association has called him \"instrumental in the development of medical informatics as a discipline\". He served as President of the International Medical Informatics Association from 1998 to 2001.\n\nVan Bemmel was elected a member of the Royal Netherlands Academy of Arts and Sciences in 1987. He became a corresponding member of the National Academy of Medicine in 1991.\n"}
{"id": "43312937", "url": "https://en.wikipedia.org/wiki?curid=43312937", "title": "Late preterm infant", "text": "Late preterm infant\n\nLate preterm infants are infants born at a gestational age between weeks and weeks. They have higher morbidity and mortality rates than term infants (gestational age ≥37 weeks) due to their relative physiologic and metabolic immaturity, even though they are often the size and weight of some term infants. \"Late preterm\" has replaced \"near term\" to describe this group of infants, since near term incorrectly implies that these infants are \"almost term\" and only require routine neonatal care.\n\nIn 2005, late-preterm births accounted for more than 70% of all preterm births (<37 weeks’ gestation), or approximately 377,000 infants. In fact, much of the increase in the preterm birth rate in recent years can be attributed to increases in late-preterm births.\n\nSeveral important factors that may predispose late-preterm infants to medical conditions associated with immaturity:\n\nAt 34–35 weeks, the brain weight is only about that of a full-term baby. This may lead to an increased risk of:\n\nLate preterm infants have immature gastrointestinal function and feeding difficulties that predispose them to in increase in enterohepatic circulation, decreased stool frequency, dehydration, and hyperbilirubinemia. Feeding during the birth hospitalization may be transiently successful, but not sustained after discharge. Feeding difficulties are associated with relatively low oromotor tone, function, and neural maturation also predispose these infants to dehydration and hyperbilirubinemia.\n\nLate Preterm Infants have an increased risk of being underweight and stunted at 12 and 24 months of age versus term infants.\n\nProper nutrition is essential for normal growth, optimal neurologic and cognitive development, immune protection, and long term health.\n\nThe last trimester of pregnancy the fetus is expressing active amino acid transport, calcium, lipid transfer, and glucose facilitated diffusion. Delivery of the premature infant requires higher energy expenditure, but with inadequate intake the infant will have negative nitrogen balance. There are higher needs for Calcium, Phosphorus, and Vitamin D.\n\nFor every 10 kcal/kg increase in energy intake in the first week of life, there is a 4.6 points increase in MDI (Mental Development Index) at 18 months. For every 1 g/kg increase in protein intake in the first week of life, 8.2 point increase in MDI at 18 months.\n\n\nFactors such as hemodynamic stability, severe IUGR, respiratory, abdominal exam, whether feeding cues are present, and stable glucose could all effect the timing of nutrition. Some preterm infants will be NPO (nil per os). If infants are unable to start oral or enteral intake intravenous fluids may begin with amino acids or total parenteral nutrition.\n\nAccording to the American Academy of Pediatrics section on breastfeeding recommendations are all infants should receive human milk.\n\nUse caution when fortifying single nutrients to prevent alteration of protein/energy ratio. Center for Disease Control (CDC) recommends that sterile formulas and fortifiers be used when mom is not available. Powdered formula and HMF may be contaminants. Start with the mom's diet during breastfeeding. Mom should be eating adequate calories, protein, B vitamins and DHA.\n\nColostrum production can range from 26-56 mL the first day to 113-185 mL for day two. Although colostrum production is not voluminous, it can still meet the needs of the newborn.\n\n\n\n"}
{"id": "2382281", "url": "https://en.wikipedia.org/wiki?curid=2382281", "title": "MD–PhD", "text": "MD–PhD\n\nThe Doctorate of Medicine and of Philosophy (MD–PhD) is a dual doctoral degree for physician–scientists, combining the vocational training of the Doctor of Medicine degree with the research expertise of the Doctor of Philosophy degree. The degree is granted by medical schools often through the Medical Scientist Training Program or other non-MSTP MD–PhD programs. The National Institutes of Health currently provides 43 medical schools with Medical Scientist Training Program grants that support the training of students in MD–PhD programs at these institutions through tuition and stipend allowances. These programs are often competitive, with some admitting as few as two students per academic year. The MCAT score and GPA of MD–PhD matriculants are often higher than MD only matriculants.\n\nMD–PhD typically require or prefer candidates who have had a background in research, either under a professor as an undergraduate or have taken at least one gap year to work in a laboratory setting. The application process in addition to a personal statement required for MD-only applications also require two additional essays to describe why an applicant wants to pursue an MD–PhD and an essay describing their research background.\n\nIn the United States, MD–PhD degrees can be obtained through dual-degree programs offered at some medical schools. The idea for an integrated training program began at Case Western Reserve University School of Medicine in 1956 and quickly spread to other research medical schools. In 1964, through the Chief of the Office of Program Planning and Evaluation Herbert H. Rosenberg, Ph.D., the National Institutes of Health (NIH) developed a grant to underwrite some universities' MD–PhD programs. This funding was distributed through the Medical Scientist Training Program (MSTP). There are also non-MSTP funded dual-degree programs (e.g., the Medical Scholars Program at the University of Illinois at Chicago, which receives funding through endowment funds, research assistantships, teaching assistantships, and extramural fellowships). Non-MSTP funded dual degree programs have more flexibility and can extend to degrees other than the PhD (e.g., JD and MBA degrees).\n\nAdmission to a dual degree program is not a requirement to receive MD and PhD degrees. An individual has the option to complete each degree separately through single-degree programs. However, the student is responsible for all medical school tuition and does not receive a stipend during their MD education. Furthermore, since the PhD training is not streamlined with the medical training, students will often take additional years to complete their PhD.\n\nA PhD may also be obtained by physicians during the residency training period. This combined research and graduate-level medical education are offered by a minority of residency programs. This additional education typically extends the residency period by three to four years.\n\nUpon matriculating in an MD–PhD program, students will often follow a 2-PhD-2 plan. In this system, students will complete the pre-clinical curriculum of their medical school (2 years), transition into PhD graduate training (3–5 years), and then finally complete clinical rotations (2 years).\n\nUpon receiving the MD–PhD dual degree, physician-scientists may choose a variety of career paths. The most common continues to be residency training with additional laboratory training. However, a physician–scientist may also elect to refuse residency training, thereby having a career essentially akin to a conventional PhD scientist. A physician–scientist may also elect to work in the private sector with no further formal academic clinical nor research training.\n\nSome MD–PhD programs (all MSTPs) cover all medical school tuition, provide a stipend, and cover health insurance expenses.\n\nCandidates with MD–PhD dual degrees are favorably looked upon in most residency programs.\n\nGraduates with an MD–PhD degree are generally qualified for a variety of careers in medicine and medical research. On top of graduating debt free, graduates generally have to stay in the medical field to pay off large debts if they decide to quit medicine and find work as a researcher or in academia. Like residency, MD–PhD graduates are often given preference in any research or academia job applications.\n\nThe vast majority (over 80%) of MD–PhD graduates eventually choose to enter academia, government, or industry where medical research is a central component of their duties. According to a FASEB study conducted in 2000, graduates of NIH-funded MSTPs make up just 2.5% of medical school graduates each year, but after graduation, account for about one third of all NIH research grants awarded to physicians. Many MD–PhD graduates also practice clinical medicine in their field of expertise.\n\n\n"}
{"id": "37890504", "url": "https://en.wikipedia.org/wiki?curid=37890504", "title": "Medical device connectivity", "text": "Medical device connectivity\n\nMedical device connectivity is the establishment and maintenance of a connection through which data is transferred between a medical device, such as a patient monitor, and an information system. The term is used interchangeably with biomedical device connectivity or biomedical device integration. By eliminating the need for manual data entry, potential benefits include faster and more frequent data updates, diminished human error, and improved workflow efficiency.\n\nMedical devices may be connected on wireless and wired networks. Wireless networks, including Wi-Fi, Wireless Medical Telemetry Service, and Bluetooth, provide more ubiquitous coverage of connectivity, allowing uninterrupted monitoring of patients in transit. Wired networks are fast, stable, and highly available. Wired networks are usually more costly to install at first and require ongoing costs for maintenance, but allow connectivity of the organization in a closed environment.\n\nAdherence to Standards ensures interoperability within a network of medical devices. In most cases, the clinical environment is heterogenous; devices are supplied by a variety of vendors, allowing for different technologies to be utilized. Achieving interoperability can be difficult, as data format and encryption varies among vendors and models. \nThe following standards enable interoperability between connected medical device.\n\n\nRegulatory organizations and industrial associations, such as Integrating the Healthcare Enterprise (IHE) initiative and Continua Health Alliance, are working towards standardized vendor-neutral device integration systems. The IHE provides a single set of internationally harmonized medical device informatics and interoperability standards as a unitary reference point for the industry. The IHE collaborates with Continua Health Alliance regarding data exchange protocol and device specializations.\n\nThe IHE Patient Care Device (PCD) Technical Framework Volumes 1-3 defines the established standards profiles, such as the integration, transaction and semantic content profiles respectively for complete, enterprise-wide integration and interoperability of health information systems. \nSeveral profiles have applications in medical device connectivity including the following:\n\n\nHospitals have many different makes and models of medical devices. Each department has different types of devices, and rarely does an entire hospital run the same brand device. Because of the large number of devices, and the varying formats that data is exchanged (RS-232, HL7, Bluetooth, WiFi), Medical Device Integration software has become a critical component to integrating this vital patient data. Software such as Picis Hawkeye connects with virtually any device via HL7, Serial (RS-232), Bluetooth, WiFi, etc., and shares this data to any other software platform across a hospital network. This allows hospitals to continue to use their older devices, in a more modern network.\n\nPatient confidentiality can be compromised when the device data is transmitted to the wrong electronic health record. A positive patient identification at the point of care can be ensured through bar-code identifiers and radiofrequency identifiers. \n\nSecurity issues may arise in medical networking for many reasons. The following is a list of security challenges particular to medical devices:\n\n"}
{"id": "9894752", "url": "https://en.wikipedia.org/wiki?curid=9894752", "title": "Mental distress", "text": "Mental distress\n\nMental distress (or psychological distress) is a term used, both by some mental health practitioners and users of mental health services, to describe a range of symptoms and experiences of a person's internal life that are commonly held to be troubling, confusing or out of the ordinary.\n\nMental distress has a wider scope than the related term mental illness. Mental illness refers to a specific set of medically defined conditions. A person in mental distress may exhibit some of the symptoms described in psychiatry, such as: anxiety, confused emotions, hallucination, rage, depression and so on without actually being ‘ill’ in a medical sense.\n\nLife situations such as: bereavement, stress, lack of sleep, use of drugs or alcohol, assault, abuse or accident can induce mental distress. This may be something which resolves without further medical intervention, though people who endure such symptoms longer term are more likely to be diagnosed with mental illness. This definition is not without controversy as some mental health practitioners would use the terms mental distress and mental illness interchangeably.\n\nSome users of mental health services prefer the term mental distress in describing their experience as they feel it better captures that sense of the unique and personal nature of their experience, while also making it easier to relate to, since everyone experiences distress at different times. The term also fits better with the social model of disability.\n\nThe social disparities associated with mental health in the Black community have remained constant over time. According to the Office of Minority Health, black people comprise 12.9% of the U.S. population, yet they are 30% more likely than European Americans to report serious psychological distress. Moreover black people are more likely to have Major Depressive Disorder, and communicate higher instances of intense symptoms/disability. For this reason, researchers have attempted to examine the sociological causes and systemic inequalities which contribute to these disparities in order to highlight issues for further investigation. Nonetheless, much of the research on the mental well-being of black people are unable to separate race, culture, socioeconomic status, ethnicity, or behavioural and biological factors. According to Hunter and Schmidt (2010), there are three distinct beliefs embraced by black people which speak to their socio-cultural experience in the United States: a perception of racism, stigma associated with mental illness, and the importance of physical health. According to Raymond Depaulo, M.D., African Americans are less likely to report depression due to heavy social stigma within their community and culture. All of these social aspects of mental health can create a lot of distress. Therefore, discrimination within the healthcare community and larger society, attitudes related to mental health, and general physical health contribute largely to the mental well-being of black people .\nMental health in the black community is a major issue that is often overlooked due to lack of information, stigmas and socio-economic factors. These factors have been the cause of suicide rates between teens ages 10-14 rising 233% within the last 15 years. Community based issues have also caused black people to be 20% more likely to experience serious mental health conditions than the general population. Among the stigmas within the black people community, there is also a major discord between mental health providers and their black patients in regards to proper diagnosis and treatment. It is an issue that is increasingly getting worse, and one that demands that more be done to fight the problems that these mental health issues cause within the black community.\n\nThere are also disparities with mental health when it comes to black women. One of the reasons why Black women tend to hesitate when it comes to mental health support and treatment is the aura of the Strong Black Woman schema or S.B.W. According to Watson and Hunter, Various scholars have traced the origins of the SBW race-gender schema to slavery and have suggested that the schema persists because of the struggles that African-American women continue to experience, such as financial hardship…racism, and sexism. Watson and Hunter state that Black women, due to the strong black woman schema have a tendency to handle tough and difficult situations alone. \n\nComparable to their adult counterparts, black adolescents experience mental health disparities. The primary reasons for this have been stipulated to be discrimination, inadequate treatment, and underutilization of mental health services, though black youth have been shown to have higher self-esteem than their white counterparts. Similarly, children of immigrants, or second-generation Americans, often encounter barriers to optimal mental well-being. Discrimination and its effects on mental health are evident in adolescents’ ability to achieve in school and overall self-esteem. Researchers are unable to pinpoint exact causes for black teenagers’ underutilization of mental health services. One study attributed this to using alternative methods of support instead of formal treatments. Moreover, Black youth described other means of support, such as peers and spiritual leaders. This demonstrates that black teens are uncomfortable disclosing personal matters to formal supports. It is difficult to decipher if this is cultural or a youth-related issue, as most teens do not choose to access formal supports for their mental health needs.\n\n"}
{"id": "36814787", "url": "https://en.wikipedia.org/wiki?curid=36814787", "title": "Multifocal intraocular lens", "text": "Multifocal intraocular lens\n\nMultifocal and Accommodating intraocular lenses are artificial intraocular lenses (IOLs) that are designed to provide focus of both distance and near objects, in contrast to monofocal intraocular lenses which only have one focal point and correct distance vision. The issue of restoring accommodation following cataract surgery or through refractive lens exchange is becoming an increasingly important topic in ophthalmology.\n\nIntraocular lenses that correct presbyopia are divided into two main categories:\n\n\nMonofocal lenses are standard lenses used in cataract surgery. People who have a multifocal intraocular lens after their cataract is removed may be less likely to need additional glasses compared with people who have standard monofocal lenses. People receiving multifocal lenses may experience more visual problems, such as glare or haloes (rings around lights), than with monofocal lenses.\n\nPeople receiving accommodative intraocular lenses had improvements in near vision but these improvements were small and reduced over time. People who received accommodative intraocular lenses may have a higher risk of thickening and clouding of the tissue behind the intraocular lenses (posterior capsule opacification) but there is some uncertainty around this finding.\n"}
{"id": "50673280", "url": "https://en.wikipedia.org/wiki?curid=50673280", "title": "Nap pods", "text": "Nap pods\n\nNap pods are special chairs, often used in corporate/workplace environments and universities, that allow people to nap. They are available at JFK airport, Google, and Zappos. Washington State University also offers some.\n"}
{"id": "51173740", "url": "https://en.wikipedia.org/wiki?curid=51173740", "title": "Narcosobrinos affair", "text": "Narcosobrinos affair\n\nThe \"Narcosobrinos\" affair (Spanish for \"Narconephews\") is the situation of events that surrounded two nephews of Venezuelan President Nicolás Maduro and his wife Cilia Flores who were arrested for narcotics trafficking. The nephews, Efraín Antonio Campo Flores and Francisco Flores de Freitas, were arrested on 10 November 2015 by the United States Drug Enforcement Administration in Port-au-Prince, Haiti after attempting to transport 800 kilograms of cocaine into the United States. A year later on November 18, 2016, the two nephews were found guilty, with the cash allegedly destined to \"help their family stay in power\". On 14 December 2017, the two were sentenced to 18 years of imprisonment.\n\nThe word \"Narcosobrinos\" is the word \"narco\", meaning \"drug dealer\", followed by \"sobrinos\", which translates to \"nephews\". Its translation could therefore be \"drug dealer nephews\". The term derived from the media which focused on the relation of drug dealing allegations to President Maduro's nephews.\n\nAccording to Jackson Diehl, Deputy Editorial Page Editor of \"The Washington Post\", the Bolivarian government of Venezuela shelters \"one of the world's biggest drug cartels\". There have been allegations of former president Hugo Chávez being involved with drug trafficking. In May 2015, \"The Wall Street Journal\" reported from United States officials that drug trafficking in Venezuela increased significantly with Colombian drug traffickers moving from Colombia to Venezuela due to pressure from law enforcement. One United States Department of Justice official described the higher ranks of the Venezuelan government and military as \"a criminal organization\", with high ranking Venezuelan officials being accused of drug trafficking. Those involved with investigations stated that Venezuelan government defectors and former traffickers had given information to investigators and that details of those involved in government drug trafficking were increasing. Anti-drug authorities have also accused some Venezuelan officials of working with Mexican drug cartels.\n\nAt a presentation at the XXXII International Conference on Drugs in 2015, commander of the United States Southern Command General John Kelly stated that though relations with other Latin American nations countering drug trafficking has been good, Venezuela was not as cooperative and that \"there's a lot of cocaine leaving Venezuela to the world market\". General Kelly also stated that almost all shipments of cocaine using aircraft comes out of Venezuela and that since 2013 to early-2014, the route of drug trafficking aircraft has changed from heading to Central America to primarily traveling through Caribbean islands. The \"Narcosobrinos\" incident happened at a time when multiple high-ranking members of the Venezuelan government were being investigated for their involvement of drug trafficking, including Walter Jacobo Gavidia, Cilia Flores' son who is a Caracas judge, former National Assembly President Diosdado Cabello, and Governor of Aragua State Tarek El Aissami.\n\nCampo Flores and Flores de Freitas were allegedly involved in illicit activities such as drug trafficking and possibly financially assisted President Maduro's presidential campaign in the 2013 Venezuelan presidential election and potentially for the 2015 Venezuelan parliamentary elections. One informant stated that the two would often fly out of Terminal 4 of Simon Bolivar Airport, a terminal reserved for the president.\n\nThe nephews and DEA informants met on multiple occasions in Haiti, Honduras and Venezuela while every meeting \"produced an audio recording plus three to seven videos\". Campo and Flores planned to ship cocaine supplied by the Revolutionary Armed Forces of Colombia (FARC) to the United States and sought for assistance with their plans. On 3 October 2015, a confidential DEA informant known as CW-1 and his employee \"El Flaco\" were contacted by a Venezuelan contact known as \"Hamudi\" who introduced Campo and Flores to the informant. The next day on 4 October 2015, the two flew from Venezuela to San Pedro Sula, Honduras, with nephews stating that they would use their connections to send narcotics on legal flights from Caracas, Venezuela to Roatán, Honduras, knowing that their relation to the president \"would open doors for the smuggling operation\". \nIn late-October, CS-1, who presented himself as a Mexican drug boss and CS-2, presenting himself as an associate of CS-1, flew to Caracas, Venezuela to meet with the nephews. Around 23 October 2015, CS-1 and CS-2 met with the nephews, with Campo stating that he was \"the one in charge\" and that \"we're at war with the United States ... with Colombia ... with the opposition\". Campo also reassured the two informants that the cocaine shipments would not be tracked by law enforcement because the plane would \"depart from here as if ... some from our family were on the plane\". Days later on 26 October 2015, Campo stated that the two were to ship cocaine and were seeking to raise about $20 million, explaining that CW-1 would be paid about $900,000 to receive the cocaine in Honduras. The next day on 27 October 2015, Campo and Flores presented a kilogram of cocaine to CS-1 and CS-2 to show its purity, with the informants believing that the purity was between 95%-97%. On 5 November 2015, informant CS-3 met with co-defendant Roberto de Jesus Soto Garcia to plan on how to receive the cocaine in Honduras. Soto explained the schedule at the Juan Manuel Gálvez International Airport in Roatán, Honduras and stated that the load of cocaine would then be \"arranged with all of those inside of the airport\".\n\nOn 10 November 2015, Campo Flores and Flores de Freites were flown into Port-au-Prince, Haiti by two Venezuelan military personnel accompanied by two presidential honor guards carrying more than 800 kilograms of cocaine destined for New York City. The jet was a Cessna Citation 500 that belonged to Lebanese Venezuelan businessmen Majed and Khaled Khalil Majzoun, who were linked to old projects of the Hugo Chávez government and close to high ranking Venezuelan politician Diosdado Cabello. CS-1 met with the nephews at a restaurant of a hotel near Toussaint Louverture International Airport and was supposed to pay them $5 million for the cocaine. CS-1 then left into the bathroom and the Haitian Brigade de Lutte contre le Trafic de Stupéfiants (BLTS) and DEA agents raided the restaurant after identifying themselves, apprehending the nephews. The BLTS personnel wore fatigues and vests that read \"POLICE\" so they would be able to be identified as well. Campos and Flores were later turned over to the DEA and read their Miranda rights after boarding the DEA plane, being flown directly to Westchester County Airport in White Plains, New York in order to face an immediate trial. \n\nThe two were interviewed separately on the DEA plane. Campo stated on the DEA plane that he was the adopted step son of President Maduro and that he grew up in the Maduro household while being raised by Flores. He was also shown a picture of a man with a kilo package of cocaine replying \"That's me\" and when asked what was in the package he said \"You know what it is\". The two men possessed Venezuelan diplomatic passports but did not have diplomatic immunity according to former head of DEA international operations Michael Vigil. A later raid of Efraín Antonio Campo Flores' \"Casa de Campo\" mansion and yacht in the Dominican Republic revealed an additional 280 lbs of cocaine and 22 lbs of heroin, with 176 lbs of the drugs found in the home while the remainder was discovered in his yacht.\n\nDue to the extradition process, New York courts could not apprehend those who assisted the nephews on their way to Haiti, though a pilot was later arrested. It was also stated by those close to the case that there are more sealed warrants linked to the incident.\n\nOn 28 October 2016, a Honduran man, Roberto de Jesús Soto García, was arrested in Honduras and extradited to the United States. According to authorities, Soto García was responsible for transporting drug shipments from Juan Manuel Gálvez International Airport. Soto García allegedly provided information about the port and was supposed to take the drugs from the nephews into the United States.\n\nIn June 2016, Yazenky Antonio Lamas Rondón, the pilot of the plane which transported the cocaine and the two nephews, was arrested at the El Dorado International Airport in Bogota, Colombia after the DEA and Interpol put out a warrant for his arrest. According to the DEA, Lamas Rondón piloted over 100 flights over the span of a decade from Venezuela which trafficked various drugs throughout Latin America. He is also believed to be involved with the Cartel of the Suns, a group of corrupt drug trafficking Venezuelan officials.\n\nThe nephews originally plead not guilty to the charges of conspiring to transport cocaine into the United States, with the two facing up to life in prison. In trial papers filed on 1 July 2016, the nephews stated that they were not informed of their rights when detained, attempting to suppress their statements that they made to DEA agents after their arrest. However, on 22 July 2016, their statements made to DEA agents were filed as exhibit by the United States Attorney Office in Manhattan, with the two men confessing to their conspiracy to traffic cocaine into the United States that was supposed to be supplied by the Colombian guerilla group, the Revolutionary Armed Forces of Colombia (FARC). The pair hoped to make $20 million through multiple drug shipments. A confidential informant posing as a leader of the Sinaloa cartel confessions testified that Efrain Campo planned to finance Cilia Flores' congressional campaign.\n\nOn November 18, 2016, the jury reached a verdict finding the two nephews guilty of attempting to traffic drugs into the United States.\n\nTwo witnesses that allegedly observed the nephews were murdered shortly before and after their arrest, raising concerns that the drug trafficking operation was larger that suspected. Two weeks before the nephews were arrested, the Venezuelan known as \"Hamudi\" who introduced the nephews to CW-1 was murdered by FARC suppliers. Weeks after the arrests in December 2015, CW-1 was murdered as well.\n\nIt is thought to be that the nephews were not \"the brains\" of the trafficking attempt but were working under the Cartel of the Suns. The murdering of witnesses was a possible way to cover possible involvement by Venezuelan officials. In the United States, the punishment for killing a witness is a federal offense punishable by up to life in prison or execution.\n\nInternational media focused on the events surrounding the nephews and their trial while Venezuelan media was largely censored from revealing that the two were related to the President Maduro and his wife. Venezuelan media organizations like Globovisión and \"Últimas Noticias\" only mentioned that \"two Venezuelans\" were charged with drug trafficking without showing any relation to the president's family, raising accusations of self-censorship. Social media, which is popular in Venezuela, was used by journalists as a way to allow Venezuelans to bypass censorship and provide updates about the situation surrounding the president's nephews.\n\nAccording drug trafficking expert to Bruce Bagley of the University of Miami, \"The nephews are just the tip of the iceberg ... Corruption is rampant in power circles in Venezuela. This case suggests a culture that drug trafficking is routine and daily fare for someone with contacts in the presidential palace\", with Bagley further stating that \"With their connections, they felt they would skate through ... They made a mistake because when the DEA heard their names they targeted them\".\n\nAfter Maduro's nephews were apprehended by the Drug Enforcement Administration for the illegal distribution of cocaine on November 10, 2015, Maduro posted a statement on Twitter criticizing \"attacks and imperialist ambushes\" which was viewed by many media outlets as being directed towards the United States. President Maduro's wife, Cilia Flores, accused the United States of kidnapping her nephews and said that she had proof that they were kidnapped by the DEA. Diosdado Cabello, a senior official in Maduro's government who has been accused of drug trafficking himself, was also quoted as saying the arrests were a \"kidnapping\" by the United States.\n\nRoberto de Jesús Soto Garcia, a Honduran man who provided assistance to the smugglers, has been linked to Venezuela's Vice President Tareck El Aissami.\n\nFollowing the arrest of the nephews, Associated Press correspondent Hannah Dreier was detained by SEBIN agents. She was then interrogated, being threatened with being beheaded in a way like ISIL did to James Foley, saying they would release her for a kiss. They later stated that they wanted to exchange Dreier for President Maduro's nephews.\n"}
{"id": "41367474", "url": "https://en.wikipedia.org/wiki?curid=41367474", "title": "National Obesity Forum", "text": "National Obesity Forum\n\nThe National Obesity Forum is a British independent professional organisation which campaigns for a more interventional approach to obesity. It was established in May 2000.\n\nIn June 2012 the Forum complained that GPs in England were rewarded financially through the Quality and Outcomes Framework for recording the number of obese patients - yet not for doing anything about it.\n\nIn February 2013 the Chairman Dr David Haslam called for stomach surgery to be offered to obese children.\n\n\n"}
{"id": "16711453", "url": "https://en.wikipedia.org/wiki?curid=16711453", "title": "Neurotoxic shellfish poisoning", "text": "Neurotoxic shellfish poisoning\n\nNeurotoxic shellfish poisoning (NSP) is caused by the consumption of shellfish contaminated by breve-toxins or brevetoxin analogs.\n\nSymptoms in humans include vomiting and nausea and a variety of neurological symptoms such as slurred speech. No fatalities have been reported but there are a number of cases which led to hospitalization.\n\n"}
{"id": "33915918", "url": "https://en.wikipedia.org/wiki?curid=33915918", "title": "Office of Minority Health", "text": "Office of Minority Health\n\nThe Office of Minority Health (OMH) was created in 1986 and is one of the most significant outcomes of the 1985 Secretary's Task Force \"Report on Black and Minority Health\", also known as the \"Heckler Report\". The Heckler report \"was a landmark effort in analyzing and synthesizing the present state of knowledge [in 1985] of the major factors that\ncontribute to the health status of Blacks, Hispanics, Asian/Pacific Islanders, and Native Americans.\" The Office the Heckler Report established is dedicated to improving the health of racial and ethnic minority populations through the development of health policies and programs that will help eliminate health disparities. OMH was reauthorized by the Patient Protection and Affordable Care Act of 2010 (P.L. 111-148). \n\nHow OMH works: OMH works in partnership with communities and organizations in the public and private sectors. These collaborations support a systems approach for eliminating health disparities, national planning to identify priorities, and coordinated responses through focused initiatives. OMH provides funding to state offices of minority health, multicultural health, and health equity; community and faith-based organizations, institutions of higher education, tribes and tribal organizations; and other organizations dedicated to improving health.\n\nThe purpose is to improve nationwide cohesion and coordination of strategies and actions to eliminate health disparities and achieve health equity. The NPA has five goals:\nincreasing awareness;\nstrengthening leadership at all levels;\nimproving health and healthcare outcomes;\nimproving cultural and linguistic competence; and\nimproving data availability, and coordination, utilization, and diffusion of research and evaluation outcomes.\n\nThe OMH Resource Center is a one stop shop for minority health literature, research, and referrals. The center also provides technical assistance to community organizations on HIV/AIDS.\n\nCultural and Linguistic Competency: OMH is committed to culturally and linguistically competent systems that will ensure the needs of minority communities are integrated and addressed within health-related programs across the nation.\n\n"}
{"id": "26439208", "url": "https://en.wikipedia.org/wiki?curid=26439208", "title": "Oral sedation dentistry", "text": "Oral sedation dentistry\n\nOral sedation dentistry is a medical procedure involving the administration of oral sedative drugs, generally to facilitate a dental procedure and reduce patients fear and anxiety related to the experience. Oral sedation is one of the available methods of conscious sedation dentistry, along with inhalation sedation (nitrous oxide) and conscious intravenous sedation. Drugs which can be used for sedation include diazepam, triazolam, zaleplon, lorazepam, and hydroxyzine.\n\nDental patients with generalized anxiety, belonephobia (fear of needles and sharp instruments), prior dental trauma, or generalized fear of the dentist can take oral medication in order to reduce their anxieties. A variety of single and incremental dose protocols are used to medicate the patient as early as the day before treatment. Medication additionally helps reduce memory or the sights and smells of the dental office to avoid recall of any trauma. The sedative effect allows more dentistry to be completed in fewer appointments as well as allowing complex procedures to be performed in less time.\n\n\nDrug characteristics must be carefully considered when choosing the appropriate medication. Matching factors, such as where the drug is metabolized and any underlying diseases requires a good medical history and thorough knowledge of the drug's characteristics. All risk factors, individual characteristics, and procedural complexities can be taken into account in order to provide the best drug choice.\n\nCourses in Oral Sedation Dentistry are offered across North America at various dental schools and private organizations. The largest for-profit provider of dental sedation continuing education in North America is the Dental Organization for Conscious Sedation. Specialty courses are being taught in pediatric sedation, ACLS, IV Sedation and emergency preparedness. The largest not-for-profit organization that provides educational courses in oral sedation as well as IV sedation and general anesthesia is the American Dental Society of Anesthesiology.\n\nEducation and training requirements to administer sedation vary by state. The ADA has set forth general sedation guidelines that have been adopted or modified by many states. There are typically dosing limitations involved with anxiolysis, usually a single dose the day of treatment not to exceed the maximum recommended dose (MRD) of the medication in order to achieve the intended level of sedation. However, these sedation laws vary from state to state. Anxiolysis protocols are designed to treat healthy ASA I & II patients ages 18 and up for one to four hours of treatment. Some states now require a permit even for nitrous oxide administration and/or anxiolysis.\n\nThe main advantages of oral sedation versus other sedation methods are:\n\n\n"}
{"id": "5418620", "url": "https://en.wikipedia.org/wiki?curid=5418620", "title": "Partridge Island (Saint John County)", "text": "Partridge Island (Saint John County)\n\nPartridge Island is a Canadian island located in the Bay of Fundy off the coast of Saint John, New Brunswick, within the city's Inner Harbour.\n\nThe island is a provincial historic site and was designated a National Historic Site of Canada in 1974. It lies on the west side of the mouth of the Saint John River.\n\nDuring the American Revolution, in 1780, six British troops from Major Timothy Hierlihy's corps, under the command of Lieut. Wheaton, attacked eight American privateers in a house they were occupying on Partridge island. The British killed three of the privateers and the other five were taken prisoner.\n\nPartridge Island was first established as a quarantine station and pest house in 1785 by the Saint John Royal Charter, which also set aside the island for use as a navigational aid station and a military post. Its first use as a quarantine station was not until 1816. A hospital was constructed on the island in 1830. \n\nThe island received its largest influx of immigrants in the 1840s during the \"Great Famine\", also known as the \"Irish Potato Famine,\" when a shortage of potatoes occurred due to potato blight striking Ireland's staple crop. The famine caused millions to starve to death or otherwise emigrate, mainly to North America. During the famine, some 30,000 immigrants were processed by the island's visiting and resident physicians, with 1,196 dying at Partridge Island and the adjacent city of Saint John during the Typhus epidemic of 1847. During the 1890s there were over 78,000 immigrants a year being examined or treated on the island.\n\nA memorial to the Irish immigrants of the mid-1840s was set up on the island in the 1890s, but by World War One it had deteriorated. In 1926 the Saint John City Cornet Band approached Saint John contractor George McArthur who agreed to lead a campaign to build a suitable monument. The Celtic Cross memorial to the Irish dead of 1847 was dedicated in 1927. This was restored and rededicated in 1985. In the early and mid-1980s, memorials were built by the Saint John Jewish Community, the Loyal Orange Lodge, the Partridge Island Research Project, and the Partridge Island & Harbour Heritage Inc., a company that was registered in 1988 and dissolved in 2004. The memorials were dedicated to the Protestant, Catholic and Jewish immigrants buried in the six island graveyards. A monument was also dedicated to all of the Irish dead from 1830 to the 1920s.\n\nThe island's folklore begins with the Mi'kmaq Nation, who referred to the island as \"Quak'm'kagan'ik\" meaning \"a piece cut out.\" This name is in reference to the belief that the island was created when Glooscap smashed the dam that \"Big Beaver\" had built. At the Reversing Falls Rapids a piece of the dam was swept in the rush of water to the mouth of the harbour where it came to rest to form the island. This version of the legend dates to the early 20th century. The 19th century version refers to Partridge Island in Minas Basin in Nova Scotia.\n\nFollowing the arrival of the American Loyalists from the American Revolutionary War in 1783, and the formation of the city of Saint John, there was the need for a lighthouse to aid shipping. A light station was erected on Partridge Island and began operating in 1791. It was only the third light station to have been built in British North America. A signal station was soon located on the island and it was used for many years to alert the harbour to vessels approaching from the Bay of Fundy. The island's light and signal station were both established in 1791.\n\nThe island was Saint John's principal military fortification from 1800 until 1947. It was the only Saint John fortification to be used during all periods of Saint John's military activity. There are still visible remains of the Royal Artillery gun battery of 1812, and of both the First and Second World Wars.\n\nThe island was also home to dozens of island families over the years, from lightkeepers such as Captain Samuel Duffy, James Wilson, Albert Smith, Charles Mitchell and Thomas Furness, to hospital staff such as Doctors George and William Harding, hospital stewards Thomas McGowan, Fred and Jim Hargrove, and teachers for the island's school such as Jean MacCullum and Forbes Elliott. \n\nBoat tours to the island operated from 1982 until 1995 when the island's small museum closed. Public access is now restricted. There have been numerous books written about the island as well as video documentaries.\n\nAmbitions to turn Partridge Island into a tourist site have been ongoing. In 2014, the federal government set aside $200,000 for a feasibility study which would assess the cost of repairing the breakwater and creating a walkway that would cross to the island as well as annual operation and maintenance costs. The study found that it would cost between $27-$40 million to create a path to the island. \n\nWayne Long, MP for Saint John, has proposed that a wharf be built at the site and that boat tours would go to and from it. Long said in 2017, \"The time for action is now\" about creating access to the historic island. Long estimates that the wharf would cost only $5 million, which is a sharp reduction from the cost of a walkway. \n\nBefore opening to the public, a clean-up of the island's significant soil contaminates would have to be done. All of the remaining buildings on the site have been vandalized or burned. Of the six graveyards, the 19th century graveyard was almost completely obliterated by the military during World War II. Less than three dozen graves remain. Instead of remaining a well-kept national historic site, Partridge Island has become the \"haunted, dangerous rite of passage for New Brunswick’s wasted youth.\" Many young people from the local area go to the island to party or to vandalise, although it is illegal to cross the breakwater.\n\n\n"}
{"id": "55485685", "url": "https://en.wikipedia.org/wiki?curid=55485685", "title": "Permanent Representative of Belarus to the United Nations", "text": "Permanent Representative of Belarus to the United Nations\n\nThis is a list of the Permanent Representatives of Belarus to the United Nations. Permanent Representative is a head of the permanent mission of Belarus to the United Nations. The current office holder is Valentyn Rybakov, since August 7, 2017.\n\n26 June 1945 was signed the Charter of the United Nations and came into force on 24 October 1945. The Byelorussian Soviet Socialist Republic was among the first countries that signed the United Nations Charter, becoming a founding member of the United Nations among 51 countries.\n\nUntil 1958 the permanent mission of Byelorussia was led by the Minister of Foreign Affairs rather than the permanent representative.\n\nSince Belarusine's independence in August 1991, membership in the United Nations is a priority of Belarusine's foreign policy.\n\nThe permanent mission of Belarus consists of permanent representative, three counsellors (one of which is a permanent representative deputy) and a military adviser. The permanent mission also includes about 10 various secretaries and attaches.\n"}
{"id": "10055524", "url": "https://en.wikipedia.org/wiki?curid=10055524", "title": "Person-fit analysis", "text": "Person-fit analysis\n\nPerson-fit analysis is a technique for determining if the person's results on a given test are valid.\n\nThe purpose of a person-fit analysis is to detect item-score vectors that are unlikely given a hypothesized test theory model such as item response theory, or unlikely compared with the majority of item-score vectors in the sample. An item-score vector is a list of \"scores\" that a person gets on the items of a test, where \"1\" is often correct and \"0\" is incorrect. For example, if a person took a 10-item quiz and only got the first five correct, the vector would be {1111100000}.\n\nIn individual decision-making in education, psychology, and personnel selection, it is critically important that test users can have confidence in the test scores used. The validity of individual test scores may be threatened when the examinee's answers are governed by factors other than the psychological trait of interest - factors that can range from something as benign as the examinee dozing off to concerted fraud efforts. Person-fit methods are used to detect item-score vectors where such external factors may be relevant, and as a result, indicate invalid measurement.\n\nUnfortunately, person-fit statistics only tell if the set of responses is likely or unlikely, and cannot prove anything. The results of the analysis might look like an examinee cheated, but the ability to prove it by returning to when the test was administered is not possible. This limits its practical applicability on an individual scale. However, it might be useful on a larger scale; if most examinees at a certain test site or with a certain proctor have unlikely responses, an investigation might be warranted.\n\n"}
{"id": "2514195", "url": "https://en.wikipedia.org/wiki?curid=2514195", "title": "Phoebe Gloeckner", "text": "Phoebe Gloeckner\n\nPhoebe Louise Adams Gloeckner (born December 22, 1960), is an American cartoonist, illustrator, painter, and novelist.\n\nGloeckner worked prolifically as a medical illustrator since 1988, and her training is evident in her paintings and comics art, which are highly detailed and often prominently feature the human body. Her first prominent work in fiction publishing, a series of illustrations for the RE/Search edition of J. G. Ballard's novel \"The Atrocity Exhibition\", used clinical images of internal anatomy, sex, and physical trauma in ambiguous and evocative combinations.\n\nHer early comics work, in the form of short stories published in a variety of underground anthologies including \"Wimmen's Comix\", \"Weirdo\", \"Young Lust\", and \"Twisted Sisters\", and in the tabloid zine, RE/Search (numbered volumes), was sporadic and rarely seen until the 1998 release of the collection \"A Child's Life and Other Stories\". This was followed by her 2002 novel \"\", which revisited the troubled life of the young character (usually referred to as \"Minnie Goetze\") previously featured in some of her comics, this time in an unusual combination of prose, illustration, and short comics scenes.\n\nHer novel and many of her short stories are semi-autobiographical, a frequent cause of comment due to their depiction of sex, drug use, and childhood traumas; however, Gloeckner has stated that she regards them as fiction. Sexual content led to \"A Child's Life and Other Stories\" being banned from the public library in Stockton, California, after it was checked out by an 11-year-old reader. The mayor of Stockton called the book \"a how-to book for pedophiles.\" The graphic novel was also classified as pornography and refused entry by customs officials in both France and England.\n\nLess controversial, and actually intended for children, is the book \"Weird Things You Can Grow\", published by Random House, and books in the series beginning with \"Tales too Funny to be True\" published by HarperCollins, for which she did the illustrations.\n\nA film version of \"The Diary of a Teenage Girl\" premiered at the 2015 Sundance Film Festival. The film was acquired by Sony Pictures Classics at the festival. Adapted and directed by Marielle Heller, it is based on Heller's earlier stage adaptation. The film stars Alexander Skarsgard as Monroe, Kristen Wiig as Charlotte, and Bel Powley as the main character, Minnie Goetze. Heller developed the script at the Sundance Institute's Sundance Feature Film Program Lab. The film won \"Best First Feature\" at the 2016 Spirit Awards.\n\nGloeckner briefly taught courses at Suffolk Community College and SUNY Stony Brook.\n\nGloeckner is an Associate Professor at the University of Michigan Stamps School of Art & Design, a position she has held since 2010.\n\nIn recognition of her contributions to the comic art form, Comics Alliance listed Gloeckner as one of twelve women cartoonists deserving of lifetime achievement recognition.\n\nGloeckner was born in Philadelphia, Pennsylvania. Her mother was a librarian and her father, David Gloeckner, was a commercial illustrator. Her father's family was Quaker and she attended Quaker schools when she was young. She has a younger sister.\n\nGloeckner's parents divorced when she was 4 years old. In 1972, when she was 11 or 12 years old, her mother remarried and the family moved to San Francisco. She attended several Bay Area schools, including The Hamlin School for Girls, The Urban School of San Francisco, Lick-Wilmerding High School, and The Independent Learning School. She was a boarding student at Castilleja (in Palo Alto) for a year, but returned to San Francisco to live with her mother, her mother's boyfriend, and her sister, when she was 14.\n\nGloeckner began cartooning at the age of 12. Because her mother was dating Robert Armstrong, a cartoonist in Robert Crumb's band Cheap Suit Serenaders, she met many San Francisco underground comics figures who had a profound influence upon her, including Robert Crumb, Aline Kominsky, Bill Griffith, Terry Zwigoff, and Diane Noomin.\n\nGloeckner attended San Francisco State University from 1980 to 1985, where she was a pre-med student and studied French and art. She spent the 1983-1984 academic year in Université d’Aix-Marseille studying art, French, and biology, and from 1984 to 1985 spent about six months studying Czech and literature at Charles University in Prague. She has an M.A. in Biomedical Communications from University of Texas Southwestern Medical Center at Dallas, which she received in 1988. The degree was in medical illustration. Her 1987 dissertation was on the \"Semiotic Analysis of Medical Illustration,\" in which she studies narrative devices used in medical and surgical illustration.\n\nGloeckner became interested in medical illustration through her maternal grandfather, an antique dealer who collected and sold old books, and her paternal grandmother, Dr. Louise Carpenter Gloeckner, who was a physician in Philadelphia and was the first woman to be elected vice president of the American Medical Association.\n\nIn 1986, Gloeckner married Czech artist Jakub Kalousek. They later divorced. She has two daughters, Audrey \"Fina\" Gloeckner-Kalousek and Persephone Gloeckner-Kalousek.\n\n\n\n\n\n\n\n\n\n"}
{"id": "55659970", "url": "https://en.wikipedia.org/wiki?curid=55659970", "title": "Prevention of mental disorders", "text": "Prevention of mental disorders\n\nPrevention of mental disorders are measures that try to decrease the chances of a mental disorder occurring. A 2004 WHO report stated that \"prevention of these disorders is obviously one of the most effective ways to reduce the [disease] burden.\"\nThe 2011 European Psychiatric Association (EPA) guidance on prevention of mental disorders states \"There is considerable evidence that various psychiatric conditions can be prevented through the implementation of effective evidence-based interventions.\"\nA 2011 UK Department of Health report on the economic case for mental health promotion and mental illness prevention found that \"many interventions are outstandingly good value for money, low in cost and often become self-financing over time, saving public expenditure\".\nIn 2016, the National Institute of Mental Health re-affirmed prevention as a research priority area.\n\nParenting may affect the child's mental health, and evidence suggests that helping parents to be more effective with their children can address mental health needs.\n\nAssessing parenting capability has been raised in child protection and other contexts. Delaying of potential very young pregnancies could lead to better mental health causal risk factors such as improved parenting skills and more stable homes, and various approaches have been used to encourage such behaviour change. Some countries run conditional cash transfer welfare programs where payment is conditional on behaviour of the recipients. Compulsory contraception has been used to prevent future mental illness.\n\nUse of cognitive behavioral therapy (CBT) with people at risk has significantly reduced the number of episodes of generalized anxiety disorder and other anxiety symptoms, and also given significant improvements in explanatory style, hopelessness, and dysfunctional attitudes. In 2014 the UK National Institute for Health and Care Excellence (NICE) recommended preventive CBT for people at risk of psychosis.\nAs of 2018, some health providers now advocate pre-emptive use of CBT to prevent worsening of mental illnesses.\n\nSahaja meditators scored above control groups for emotional well-being and mental health measures on SF-36 ratings, leading to proposed use for mental illness prevention, although this result could be due to meditators having other characteristics leading to good mental health, such as higher general self care.\n\nAn 8 week mindfulness course given to students was found to reduce the number subsequently needing treatment for mental illness by 60%, although the study was not of large size and commented that the effect could be due to 'non-specific effects', as the control group had received no attention at all, rather than an alternative intervention.\n\nA review found that a number of studies have shown that internet- and mobile-based interventions can be effective in preventing mental disorders.\n\nFor depressive disorders, when people participated in interventions, some studies show the number of new cases is reduced by 22% to 38%. These interventions included CBT. Such interventions also save costs. Depression prevention continues to be called for.\n\nFor anxiety disorders, \n\nIn those at high risk there is tentative evidence that psychosis incidence may be reduced with the use of CBT or other types of therapy. In 2014 the UK National Institute for Health and Care Excellence (NICE) recommended preventive CBT for people at risk of psychosis.\n\nThere is also tentative evidence that treatment may help those with early symptoms. Antipsychotic medications are not recommended for preventing psychosis.\n\nFor schizophrenia, one study of preventative CBT showed a positive effect and another showed neutral effect.\n\nThere has been an historical trend among public health professionals to consider targeted programmes. However identification of high risk groups can increase stigma, in turn meaning that the targeted people do not engage. Thus policy recommends universal programs, with resources within such programs weighted towards high risk groups.\n\nUniversal prevention (aimed at a population that has no increased risk for developing a mental disorder, such as school programs or mass media campaigns) need very high numbers of people to show effect (sometimes known as the \"power\" problem). Approaches to overcome this are (1) focus on high-incidence groups (e.g. by targeting groups with high risk factors), (2) use multiple interventions to achieve greater, and thus more statistically valid, effects, (3) use cumulative meta-analyses of many trials, and (4) run very large trials.\n\n\nHistorically prevention has been a very small part of the spend of mental health systems. For instance the 2009 UK Department of Health analysis of prevention expenditure did not include any apparent spend on mental health. The situation is the same in research.\n\nHowever more recently some prevention programmes have been proposed or implemented. Prevention programmes can include public health policies to raise general health, creating supportive environments, strengthening communities, developing personal skills, and reorienting services.\n\n\nIn India the 1982 National Mental health Programme included prevention, but implementation has been slow, particularly of prevention elements.\n\nIt is already known that home visiting programs for pregnant women and parents of young children can produce replicable effects on children's general health and development in a variety of community settings. Similarly positive benefits from social and emotional education are well proven. Research has shown that risk assessment and behavioral interventions in pediatric clinics reduced abuse and neglect outcomes for young children. Early childhood home visitation also reduced abuse and neglect, but results were inconsistent.\n\nPrevention programs can face issues in (i) ownership, because health systems are typically targeted at current suffering, and (ii) funding, because program benefits come on longer timescales than the normal political and management cycle. Assembling collaborations of interested bodies appears to be an effective model for achieving sustained commitment and funding.\n"}
{"id": "55729", "url": "https://en.wikipedia.org/wiki?curid=55729", "title": "Pure Food and Drug Act", "text": "Pure Food and Drug Act\n\nThe was the first of a series of significant consumer protection laws which was enacted by Congress in the 20th century and led to the creation of the Food and Drug Administration. Its main purpose was to ban foreign and interstate traffic in adulterated or mislabeled food and drug products, and it directed the U.S. Bureau of Chemistry to inspect products and refer offenders to prosecutors. It required that active ingredients be placed on the label of a drug’s packaging and that drugs could not fall below purity levels established by the United States Pharmacopeia or the National Formulary. \"The Jungle\" by Upton Sinclair was an inspirational piece that kept the public's attention on the important issue of unsanitary meat processing plants that later led to food inspection legislation. Sinclair quipped, \"I aimed at the public's heart and by accident I hit it in the stomach,\" as readers demanded and got the pure food law.\n\nThe Pure Food and Drug Act of 1906 was a key piece of Progressive Era legislation, signed by President Theodore Roosevelt on the same day as the Federal Meat Inspection Act. Enforcement of the Pure Food and Drug Act was assigned to the Bureau of Chemistry in the U.S. Department of Agriculture which was renamed the U.S. Food and Drug Administration (FDA) in 1930. The Meat Inspection Act was assigned to what is now known as the Food Safety and Inspection Service, which remains in the U.S. Department of Agriculture. The first federal law regulating foods and drugs, the 1906 Act's reach was limited to foods and drugs moving in interstate commerce. Although the law drew upon many precedents, provisions, and legal experiments pioneered in individual states, the federal law defined \"misbranding\" and \"adulteration\" for the first time and prescribed penalties for each. The law recognized the U.S. Pharmacopeia and the National Formulary as standards authorities for drugs, but made no similar provision for federal food standards. The law was principally a \"truth in labeling\" law designed to raise standards in the food and drug industries and protect the reputations and pocketbooks of honest businessmen.\n\nUnder the law, drug labels, for example, had to list any of 10 ingredients that were deemed \"addictive\" and/or \"dangerous\" on the product label if they were present, and could not list them if they were not present. Alcohol, morphine, opium, and cannabis were all included on the list of these \"addictive\" and/or \"dangerous\" drugs. The law also established a federal cadre of food and drug inspectors that one Southern opponent of the legislation criticized as \"a Trojan horse with a bellyful of inspectors and other employees.\" Penalties under the law were modest, but an under-appreciated provision of the Act proved more powerful than monetary penalties. Goods found in violation of various areas of the law were subject to seizure and destruction at the expense of the manufacturer. That, combined with a legal requirement that all convictions be published as Notices of Judgment, proved to be important tools in the enforcement of the statute and had a deterrent effect upon would-be violators. Deficiencies in this original statute, which had become noticeable by the 1920s, led to the replacement of the 1906 statute with the Federal Food, Drug, and Cosmetic Act which was enacted in 1938 and signed by President Franklin Roosevelt. This act, along with its numerous amendments, remains the statutory basis for federal regulation of all foods, drugs, biological products, cosmetics, medical devices, tobacco, and radiation-emitting devices by the U.S. Food and Drug Administration.\n\nIt took 27 years to the 1906 statute, during which time the public was made aware of many problems with foods and drugs in the U.S. Muckraking journalists, such as Samuel Hopkins Adams, targeted the patent medicine industry with its high-alcoholic content patent medicines, soothing syrups for infants with opium derivatives, and \"red clauses\" in newspaper contracts providing that patent medicine ads (upon which most newspapers of the time were dependent) would be withdrawn if the paper expressed support for food and drug regulatory legislation. The Chief Chemist of the Bureau of Chemistry, Dr. Harvey Washington Wiley, captured the country's attention with his hygienic table studies, which began with a modest Congressional appropriation in 1902. The goal of the table trial was to study the human effects of common preservatives used in foods during a period of rapid changes in the food supply brought about by the need to feed cities and support an industrializing nation increasingly dependent on immigrant labor. Wiley recruited young men to eat all their meals at a common table as he added increased \"doses\" of preservatives including borax, benzoate, formaldehyde, sulfites, and salicylates. The table trials captured the nation's fancy and were soon dubbed \"The Poison Squad\" by newspapers covering the story. The men soon adopted the motto \"Only the Brave dare eat the fare\" and at times the publicity given to the trials became a burden. Though many results of the trial came to be in dispute, there was no doubt that formaldehyde was dangerous and it disappeared quickly as a preservative. Wiley himself felt that he had found adverse effects from large doses of each of the preservatives and the public seemed to agree with Wiley. In many cases, most particularly with ketchup and other condiments, the use of preservatives was often used to disguise insanitary production practices. Although the law itself did not proscribe the use of some of these preservatives, consumers increasingly turned away from many products with known preservatives.\n\nThe 1906 statute regulated food and drugs moving in interstate commerce and forbade the manufacture, sale, or transportation of poisonous patent medicines. The Act arose due to public education and exposés from public interest guardians such as Upton Sinclair and Samuel Hopkins Adams, social activist Florence Kelley, researcher Harvey W. Wiley, and President Theodore Roosevelt.\n\nThe 1906 Act paved the way for the eventual creation of the Food and Drug Administration (FDA) and is generally considered to be that agency's founding date, though the agency existed before the law was passed and was not named FDA until later. \"While the Food and Drug act remains a foundational law of the FDA mission, it's not the law that created the FDA. [Initially,] the Bureau of Chemistry (the precursor to the FDA) regulated food safety. In 1927, the Bureau was reorganized into the Food, Drug, and Insecticide Administration and the Bureau of Chemistry and Soils. The FDIA was renamed the FDA in 1930.\"\n\nThe law itself was largely replaced by the much more comprehensive Federal Food, Drug, and Cosmetic Act of 1938.\n\nThe Pure Food and Drug Act was initially concerned with ensuring products were labeled correctly. Later efforts were made to outlaw certain products that were not safe, followed by efforts to outlaw products which were safe but not effective. For example, there was an attempt to outlaw Coca-Cola in 1909 because of its excessive caffeine content; caffeine had replaced cocaine as the active ingredient in Coca-Cola in 1903. In the case \"United States v. Forty Barrels and Twenty Kegs of Coca-Cola\", the judge found that Coca-Cola had a right to use caffeine as it saw fit, although Coca-Cola eventually lost when the government appealed to the Supreme Court. It reached a settlement with the United States government to reduce the caffeine amount.\n\nIn addition to caffeine, the Pure Food and Drug Act required that drugs such as alcohol, cocaine, heroin, morphine, and cannabis, be accurately labeled with contents and dosage. Previously many drugs had been sold as patent medicines with secret ingredients or misleading labels. Cocaine, heroin, cannabis, and other such drugs continued to be legally available without prescription as long as they were labeled. It is estimated that sale of patent medicines containing opiates decreased by 33% after labeling was mandated. The Pure Food and Drug Act of 1906 is cited by drug policy reform advocates such as James P. Gray as a successful model for re-legalization of currently prohibited drugs by requiring accurate labels, monitoring of purity and dose, and consumer education.\n\n"}
{"id": "1393169", "url": "https://en.wikipedia.org/wiki?curid=1393169", "title": "Quality-adjusted life year", "text": "Quality-adjusted life year\n\nThe quality-adjusted life year or quality-adjusted life-year (QALY) is a generic measure of disease burden, including both the quality and the quantity of life lived. It is used in economic evaluation to assess the value for money of medical interventions. One QALY equates to one year in perfect health. If an individual's health is below this maximum, QALYs are accrued at a rate of less than 1 per year. To be dead is associated with 0 QALYs. QALYs can be used to inform personal decisions, to evaluate programs, and to set priorities for future programs.\n\nThe QALY is a measure of the value of health outcomes. It assumes that health is a function of length of life and quality of life, and combines these values into a single index number. To determine QALYs, one multiplies the utility value associated with a given state of health by the years lived in that state. A year of life lived in perfect health is worth 1 QALY (1 year of life × 1 Utility value). A year of life lived in a state of less than perfect health is worth less than 1 QALY; for example, 1 year of life lived in a situation with utility 0.5 (e.g. bedridden, 1 year × 0.5 Utility) is assigned 0.5 QALYs. Similarly, half a year lived in perfect health is equivalent to 0.5 QALYs (0.5 years × 1 Utility). Death is assigned a value of 0 QALYs, and in some circumstances it is possible to accrue negative QALYs to reflect health states deemed \"worse than dead.\"\n\nThe \"weight\" (utility) values between 0 and 1 are usually determined by methods such as those proposed in the Journal of Health Economics:\n\nAnother way of determining the weight associated with a particular health state is to use standard descriptive systems such as the EuroQol Group's EQ-5D questionnaire, which categorises health states according to five dimensions: mobility, self-care, usual activities (e.g. work, study, homework or leisure activities), pain/discomfort and anxiety/depression.\n\nData on medical costs are often combined with QALYs in cost-utility analysis to estimate the cost-per-QALY associated with a health care intervention. This parameter can be used to develop a cost-effectiveness analysis of any treatment. This incremental cost-effectiveness ratio (ICER) can then be used to allocate healthcare resources, often using a threshold approach.\n\nIn the United Kingdom, the National Institute for Health and Care Excellence, which advises on the use of health technologies within the National Health Service, has since at least 2013 used \"£ per QALY\" to evaluate their utility.\n\nThe concept of the QALY is credited to work by Klarman et al. (1968), Fanshel and Bush (1970), and Torrance et al. (1972) who suggested the idea of length of life adjusted by indices of functionality or health. A 1976 article by Zeckhauser and Shepard contained the first known occurrence in print of the term \"Quality Adjusted Life Years\". QALYs were later promoted through medical technology assessments conducted by the US Congress Office of Technology Assessment.\n\nThen, in 1980, Pliskin et al. proposed a justification of the construction of the QALY indicator using the multiattribute utility theory: if a set of conditions pertaining to agent preferences on life years and quality of life are verified, then it is possible to express the agent's preferences about couples (number of life years/health state), by an interval (Neumannian) utility function. This utility function would be equal to the product of an interval utility function on \"life years\", and an interval utility function on \"health state\".\n\nAccording to Pliskin et al., the QALY model requires utility independent, risk neutral, and constant proportional tradeoff behaviour. Because of these theoretical assumptions, the meaning and usefulness of the QALY is debated. Perfect health is difficult, if not impossible, to define. Some argue that there are health states worse than being dead, and that therefore there should be negative values possible on the health spectrum (indeed, some health economists have incorporated negative values into calculations). Determining the level of health depends on measures that some argue place disproportionate importance on physical pain or disability over mental health.\n\nThe method of ranking interventions on grounds of their cost per QALY gained ratio (or ICER) is controversial because it implies a quasi-utilitarian calculus to determine who will or will not receive treatment. However, its supporters argue that since health care resources are inevitably limited, this method enables them to be allocated in the way that is approximately optimal for society, including most patients. Another concern is that it does not take into account equity issues such as the overall distribution of health states – particularly since younger, healthier cohorts have many times more QALYs than older or sicker individuals. As a result, QALY analysis may undervalue treatments which benefit\nthe elderly or others with a lower life expectancy. Also, many would argue that all else being equal, patients with more severe illness should be prioritised over patients with less severe illness if both would get the same absolute increase in utility. \n\nAs early as 1989, Loomes and McKenzie recommended that research be conducted concerning the validity of QALYs. In 2010, with funding from the European Commission, the European Consortium in Healthcare Outcomes and Cost-Benefit Research (ECHOUTCOME) began a major study on QALYs as used in health technology assessment. Ariel Beresniak, the study's lead author, was quoted as saying that it was the \"largest-ever study specifically dedicated to testing the assumptions of the QALY.\" In January 2013, at its final conference, ECHOUTCOME released preliminary results of its study which surveyed 1361 people \"from academia\" in Belgium, France, Italy and the UK. The researchers asked the subjects to respond to 14 questions concerning their preferences for various health states and durations of those states (e.g., 15 years limping versus 5 years in a wheelchair). They concluded that \"preferences expressed by the respondents were not consistent with the QALY theoretical assumptions\" that quality of life can be measured in consistent intervals, that life-years and quality of life are independent of each other, that people are neutral about risk, and that willingness to gain or lose life-years is constant over time. ECHOUTCOME also released \"European Guidelines for Cost-Effectiveness Assessments of Health Technologies,\" which recommended not using QALYs in healthcare decision making. Instead, the guidelines recommended that cost-effectiveness analyses focus on \"costs per relevant clinical outcome.\"\n\nIn response to the ECHOUTCOME study, representatives of the National Institute for Health and Care Excellence, the Scottish Medicines Consortium, and the Organisation for Economic Co-operation and Development made the following points. First, QALYs are better than alternative measures. Second, the study was \"limited.\" Third, problems with QALYs were already widely acknowledged. Fourth, the researchers did not take budgetary constraints into consideration. Fifth, the UK's National Institute for Health and Care Excellence uses QALYs that are based on 3395 interviews with residents of the UK, as opposed to residents of several European countries. Finally, people who call for the elimination of QALYs may have \"vested interests.\"\n\nThe UK Medical Research Council and others are exploring improvements to or replacements for QALYs. Among other possibilities are extending the data used to calculate QALYs (e.g., by using different survey instruments); \"using well-being to value outcomes\" (e.g., by developing a \"well-being-adjusted life-year\"; and by value outcomes in monetary terms. Furthermore, researchers are studying whether there should be a discount rate for QALYs, and if so whether the rate should be the same as or lower than the rate for costs. In 2018 HM Treasury set a discount rate of 1.5% for QALYs which is lower than the discount rate for other costs and benefits, because the QALY is a direct utility measure.\n\n"}
{"id": "727467", "url": "https://en.wikipedia.org/wiki?curid=727467", "title": "Rosalyn Sussman Yalow", "text": "Rosalyn Sussman Yalow\n\nRosalyn Sussman Yalow (July 19, 1921 – May 30, 2011) was an American medical physicist, and a co-winner of the 1977 Nobel Prize in Physiology or Medicine (together with Roger Guillemin and Andrew Schally) for development of the radioimmunoassay (RIA) technique. She was the second woman (the first being Gerty Cori), and the first American-born woman, to be awarded the Nobel Prize in physiology or medicine.\n\nRosalyn Sussman Yalow was born in the Bronx, New York, the daughter of Clara (née Zipper) and Simon Sussman, and was raised in a Jewish household. She went to Walton High School (Bronx), New York City. After high school, she attended the all-female, tuition-free Hunter College, where her mother hoped she would learn to become a teacher. Instead, Yalow decided to study physics. Yalow knew how to type, and was able to get a part-time position as a secretary to Dr. Rudolf Schoenheimer, a leading biochemist at Columbia University's College of Physicians and Surgeons. She did not believe that any respectable graduate school would admit and financially support a woman, so she took another job as a secretary to Michael Heidelberger, another biochemist at Columbia, who hired her on the condition that she studied stenography. She graduated from Hunter College in January 1941.\n\nA few years later, she received an offer to be a teaching assistant in physics at the University of Illinois at Urbana-Champaign. She received this offer partially because World War II had just begun and many men went off to fight, and the University opted to offer women scholarship to avoid being shut down. At the University of Illinois at Urbana-Champaign, she was the only woman among the department's 400 members, and the first since 1917. Yalow earned her Ph.D. in 1945. The next summer, she took two tuition-free physics courses under government auspices at New York University.\n\nShe married fellow student Aaron Yalow, the son of a rabbi, in June 1943. They had two children, Benjamin and Elanna Yalow, and kept a kosher home. Yalow did not believe in “balancing her career with her home life” and instead incorporated her home life wherever she could in her work life. Yalow did, however, viewed the traditional roles of a homemaker as a priority, and devoted herself to traditional duties associated with motherhood and being a wife.\n\nThroughout her career, she tended to shun feminist organizations, but still, she advocated for including more women in science. While she believed the reason she had certain opportunities in Physics was because of the war, she thought that the reason that the number of women in this field decreased after the war due to a lack of interest. Yalow saw the feminist movement as a challenge to her traditional beliefs and thought that it encouraged women not to fulfill their duties to become mothers and wives.\n\nThe month after graduating from Hunter College in January 1941, Rosalyn Sussman Yalow was offered a teaching assistantship in the physics department of the University of Illinois at Urbana-Champaign. Gaining acceptance to the physics graduate program in the College of Engineering at the University of Illinois was one of the many hurdles she had to overcome as a woman in her field. Powerful male figures controlled opportunities for training, recognition, promotion, and many aspects of development in the field of science, and especially physics.\n\nWhen Yalow entered the university in September 1941, she was the only woman in the faculty, which comprised 400 professors and teaching assistants. She was the first woman in more than two decades to attend or teach at this engineering college. Yalow credited her position at the prestigious graduate school to the shortage of male candidates during World War II. Being surrounded by gifted men made her aware of a wider world in science. They recognized her talent, they encouraged her, and they supported her. They were in a position to help her succeed.\n\nYalow felt that other women in her field did not like her because of her ambition. Other women saw her curiosity as abandoning the only acceptable path for a woman in science at the time, becoming a high school science teacher, but Yalow wanted to be a physicist. During her time at the University of Illinois, she took extra undergraduate courses to increase her knowledge because she wanted to do original experimental research in addition to her regular teaching duties.\n\nFor years Yalow faced criticism from women at work but she never quit nor turned her back on other young women, if she believed they had the potential to become real scientists. She never became an advocate for women's organizations in the field of science. She was even quoted as saying, \"It bothers me that there are now organizations for women in science, which means they think they have to be treated differently from the men. I don't approve.\" Although girls and young women found a role model in her after she won her Nobel, Yalow was not a champion for improving women's treatment or representation in science.\n\nYalow's first job after teaching and taking classes at the University of Illinois at Champaign-Urbana graduate school was as an assistant electrical engineer at Federal Telecommunications Laboratory. She, again, found herself to be the only woman employee. In 1946, she returned to Hunter College to teach physics and consequently influenced many women, most notably a young Mildred Dresselhaus: Yalow was responsible for steering the future \"Queen of Carbon Science\" away from primary school teaching and into a research career. She remained a physics lecturer from 1946 to 1950, although by 1947, she began her long association with the Veterans Administration (VA) by becoming a consultant to the Bronx VA Hospital.\n\nThe VA wanted to establish research programs to explore medical uses of radioactive substances. By 1950, Yalow had equipped a radioisotope laboratory at the Bronx VA Hospital and decided to leave teaching to finally devote her attention to full-time research. There she collaborated with Solomon Berson to develop radioimmunoassay (RIA), a radioisotope tracing technique that allows the measurement of tiny quantities of various biological substances in human blood as well as a multitude of other aqueous fluids. Originally used to study insulin levels in diabetes mellitus, the technique has since been applied to hundreds of other substances – including hormones, vitamins and enzymes – all too small to detect previously.\n\nDespite its huge commercial potential, Yalow and Berson refused to patent the method. In 1968, Yalow was appointed Research Professor in the Department of Medicine at Mount Sinai Hospital, where she later became the Solomon Berson Distinguished Professor at Large.\n\nYalow was awarded a Fulbright fellowship to Portugal, which is an American scholarship program of competitive, merit-based grants that sponsor participants for exchanges in all areas of endeavor, including the sciences, business, academe, public service, government and the arts.\n\nIn 1961, Yalow won the Eli Lilly Award of the American Diabetes Association, which provides scholarships for up to 100 scholars to attend Scientific Sessions, the world’s largest scientific and medical conference focused on diabetes and its complications. Additionally, it provides the education and training for these scholars to serve as faculty for professional education programs and to clinically manage the disease.\n\nA year later, she was awarded the Gairdner Foundation International Award, which recognizes the world’s most creative and accomplished biomedical scientists who are advancing humanity.\n\nThe same year, Yalow was awarded the American College of Physicians Award, which recognizes excellence and distinguished contributions by individuals to internal medicine.\n\nIn 1972, Yalow was awarded the William S. Middleton Award for Excellence in Research, which is the highest honor awarded annually by the Biomedical Laboratory Research and Development Service to senior bio biomedical research scientists in recognition of their outstanding scientific contributions and achievements, pertaining to the healthcare of Veterans.\n\nAlso in 1972, she was given the Koch Award of the Endocrine Society, which awards individuals for their dedication to excellence in research, education and clinical practice in the field of endocrinology.\n\nIn 1975, Yalow and Berson (who had died in 1972) were awarded the American Medical Association Scientific Achievement Award, which is a gold medallion award presented to individuals on special occasions in recognition of their outstanding work in scientific achievement.\n\nThe following year she became the first female recipient and first nuclear physicist of the Albert Lasker Award for Basic Medical Research. Established by Albert and Mary Lasker in 1945, the award is intended to celebrate scientists who have made fundamental biological discoveries and clinical advances that improve human health.\n\nIn 1977, Yalow was the sixth individual woman (seventh overall, considering Marie Curie's two wins), and first American-born woman, to win the Nobel Prize in a scientific field. She was also the second woman in the world to win in the physiology or medicine category (the first being Gerty Cori). She was so honored, together with Roger Guillemin and Andrew V. Schally, for her role in devising the radioimmunoassay (RIA) technique. By measuring substances in the human body, the screening of the blood of donors for such diseases as hepatitis was made possible.[17] Radioimmunoassay (RIA) can be used to measure a multitude of substances found in tiny quantities in fluids within and outside of organisms (such as viruses, drugs and hormones). The list of current possible uses is endless, but specifically, RIA allows blood-donations to be screened for various types of hepatitis. The technique can also be used to identify hormone-related health problems. Further, RIA can be used to detect in the blood many foreign substances including some cancers. Finally, the technique can be used to measure the effectiveness of dose levels of antibiotics and drugs.\n\nIn 1978, Yalow was elected a Fellow of the American Academy of Arts and Sciences, which provides an opportunity for an early-career professional with training in science or engineering to learn about a career in public policy and administration.\n\nIn 1986, Yalow was awarded the A. Cressy Morrison Award in Natural Sciences of the N.Y. Academy of Sciences, which is offered by Mr. Abraham Cressy Morrison to individuals with superlative papers on a scientific subject within the field of The New York Academy of Sciences and its Affiliated Societies.\n\nIn 1988, Yalow received the National Medal of Science, which is given to American individuals who deserve the highest honor in science and technology.\n\nIn 1993, Yalow was inducted into the National Women's Hall of Fame.\n\n\n\n"}
{"id": "28948681", "url": "https://en.wikipedia.org/wiki?curid=28948681", "title": "Santé Mentale et Exclusion Sociale", "text": "Santé Mentale et Exclusion Sociale\n\nSanté Mentale et Exclusion Sociale - Europa (SMES-Europa), or Mental Health and Social Exclusion - Europa (MHSE-Europa) is an international non-profit organization that helps people suffering from both mental illness and extreme poverty. SMES-Europa, the Europe-wide organization, was founded in 1992.\n\nThe European SMES project began in Rome as a result of the first European seminar on the appalling neglect and abandonment homeless people living in extremely poor health & social conditions. The poverty and social exclusion are not a fatality, but a dramatic structural phenomenon of our society and constitutes a political challenge for all the Europe.\nLoneliness, sickness, mental illness, drug abuse and/or alcoholism, may be the cause or the results of this state of alienation and exclusion from society, which in some cases the same social context is responsible in beginning of the process that risk to become irreversible and which could eventually lead to a complete break with society.\n\n\n\nSMES helps:\n\n"}
{"id": "1549518", "url": "https://en.wikipedia.org/wiki?curid=1549518", "title": "School social worker", "text": "School social worker\n\nA school social worker provides counseling and psycho-social services to children and adolescents in schools at both micro and macro levels. Social workers work as mental health experts, leaders of social and emotional development within the school community, family-school liaisons, and program development experts. They aim to address student issues by working with an ecological, systems approach with parents, the school, and the community. They also work with crisis intervention, group treatment, child neglect and abuse identification and reporting, integrating services to culturally and economically diverse populations, and working on education policy issues. Most school social workers in the United States hold a Master of Social Work degree and have specialized training in helping students within the context of local schools.\n\nSchool social work began during the school year 1907–08 and was established simultaneously in New York City, Boston, Chicago and New Haven, Connecticut. At its inception, school social workers were known, among other things, as advocates for new immigrants and welfare workers of equity and fairness for people of lower socioeconomic class' as well as home visitors. These unheralded and extensive process' led to the expansion of school social work services with the encouragement of the community.\n\nBy 1900 over two-thirds of the states had compulsory attendance laws and by 1918, each state had passed compulsory school attendance laws based on the philosophy of inclusion, making school attendance obligatory by rights, and as a privilege of equal opportunity for those with individual differences (including differences in rate of learning). These pupil personnel workers or attendance workers were replaced by Visiting teachers by 1920's, they were later called as School based caseworkers. They made different emphases and methods in their work. E.g. Special schools, Psycho-social assessment and referrals and family based intervention.\n\nA 1917 study of truancy in Chicago supported \"findings that the need for school attendance officers who understood the social ills of the community\" and school social workers were best equipped for that responsibility (Allen-Meares, 1996, p. 25). Mary Richmond, one of the founding figures of social work, devoted an entire chapter to the visiting teacher in her 1922 book on \"What is Social Casework?\" The testing movement influenced school social work growth as well. Through the testing movement, educators were gaining knowledge about individual differences, underscoring the need for some children to attend school, children whose social conditions related to their test scores. Lastly during this time, leaders in the field like Sophonisba Breckinridge, expressed concerns of how school and education would relate to future success and happiness, and expressed the need to connect school and home in order to relate to the needs of children.\n\nLater in the 1920s, with the mental hygiene movement school social work was concerned with treating nervous disorders and behavioral problems in difficult children and prevention of social maladjustment, this was the beginning of therapeutic role for school social workers. During the great depression in the 1930s, like school counseling, school social work also declined. Fair Labor Standards Act in 1938 a progressive movement saw social work efforts to be initiated in the schools, and community settlement programs also have its share that led to its growth.\n\nFrom the 1940–1960 case work and group work in schools had become an established specialty. In 1960, pupil-personnel laws called for a greater emphasis by school social workers on the development of school policies and reforms. School social workers were affected by the governmental reforms and education research. Like school counselors, social workers were now called upon to address student needs while also addressing the sources of student troubles within the school. The school social worker was considered as an expert by then, who could help schools on varying psychosocial issues.\n\nDuring the 1970s, school social work gave more emphasis on family, community, collaborative approach with teachers and others school personnel. In 1975, the United States passed the Education of All Handicapped Children Act (EAHC, P.L. 94-142). It gave special importance to the role of School social work services. The legislation was later renamed as the Individuals with Disabilities Education Act in 1990. In the latter part of the 1970s, inflation was rising at an alarming rate and budget cuts threatened the profession of school social work, especially as many social workers were being replaced by other school personnel claiming similar roles. The National Association of Social Work (NASW) published a newsletter to bring attention to the issue and get responses from practitioners. Through this, NASW conducted research and replicated the findings of others' studies on the roles of school social workers and models of practice, and school social work continued to expand.\n\nIn the 1980s, school social workers were included as \"qualified personnel\" in many pieces of legislation, especially in the Elementary and Secondary School Improvement Amendments of 1988. These led to NASW giving more attention to the profession and more service to meet the needs of the category. NASW's active participation in the profession eventually led to a school social worker credential with exams in 1992. Since then, there has been a trend of integrative collaborative services. In 1994, school social workers were included in American Education Act. In July 1994 64 school social workers from across the USA met in Edwardsville, Illinois and formed the School Social Work Association of America. They drafted the first constitution and by-laws for the organization. In June 2009, a second national organization incorporated, the American Council for School Social Work, after reviewing the direction of the profession and concluding that a stronger, enhanced national voice would benefit the profession.\n\nSchool social work in Germany began in the 1970s. The German term for school social work \"Schulsozialarbeit\". It dealt with helping students with social skills, interpersonal relations, and personal growth. Initially it was an institutionalized form of both school and youth welfare for providing underprivileged children support regarding healthy socialization and adjustment in school for rising above the demands of school settings. German school social workers find solution to problems in school environment and personal ones of students. The German Youth Institute provided the first social work training with school social work concentration. Apart from this the changing social and economic paradigms of the 21st century that affects lives of families as wells as of children rises the importance of school social work in German pedagogy.\n\nSchool social work in India was officially recognized by the Government of India in the 21st century. From the 1970s, school social workers were prominent in elite schools, adopting the American model of school counseling, based on the client or person centered approach of Carl Rogers and others. The main objective was whole welfare of the child. Central Board of Secondary Education refers to school social workers as Health Wellness Teachers (CBSE Circular No.20/2014), while the Integrated Child Protection Scheme (ICPS) strictly enforces requirement of a School Social Worker and School Counselor. The psycho-social service scheme instituted under ICPS in Kerala with the guidance of a child development center (CDC) have contracted social workers for 800 schools to provide the professional services. The services are limited to only teenage girls, and excludes boys from equal rights to access of the program.\n\nFlorence Poole in 1949 described a school social worker as a skilled worker required to determine which needs within the school can be met through school social work service. A school social worker must develop a method of offering the service that will fit with the general organization and structure of the school, and which could be identified as using social work knowledge and skill. They must define the service and their contribution so that the school personnel can accept it as a service that contributes to the major purpose of the school.\n\nThe values that school social work upholds are:\n\n\nThe National Association of Social Workers in the U.S. provides a code of ethics for school social work professionals.\n\nSchool social work is structured around a range of practice models.\n\nJohn Alderson was the first to describe the existed traditional-clinical models. Generally the schools followed \"social change model\" whose major focus was the dysfunctional conditions of the school; the community school model which urged school social workers to employ \"community organization\" methods; and the social interaction model which de-emphasized a specific methodology and required the worker to intervene with the systems interacting with the target system. These were known as the Traditional models. Students who have disabilities are defined as exceptional children by federal and state legislation, including the Individuals with Disability Education Act (P.L. 94-142), the Rehabilitation Act (Section 504) in the United States of America.\n\nIn the clinical model, school social workers work primarily through casework methods supplemented by group methods with students and family members; A greater emphasis is placed on evidence-based practice and promising intervention methods that is supported empirically.\n\nLater school social workers used an approach that draws on components of the existing multidisciplinary models - Social interaction model, focusing on working with students with social and emotional difficulties and their problems in families(parents) and schools with a flexible and dynamic reciprocal interaction. This model is grounded on systems theory and transactional systems perspective. This model was an answer to organize the methodological diversity inherent in the role, rather than limiting to individual change or systems change.\n\nLela B. Costin, in 1973 developed this model. It focuses on the school, community, and student and the interactions among the three. In this model, school social workers serves as mediators, negotiators, consultants, and advocates for students and school personnel, listening to student grievances. They also set up informal groups for students, teachers, and other school personnel. This model also focuses on evaluation by a school social worker of the characteristics of students, the school, and community conditions and their relational affect on the availability and quality of educational opportunities to specific target groups (students with chemical dependency, disabilities, and so on). They are grounded in social learning theory and systems theory.\n\nThis model is grounded on the ecological systems theory. This was developed by Frey and Dupper (2005) and Germain (2006). The model promotes view of person and environment as a unitary interacting system in which each constantly affects and shapes the other. This model attends the complexities of the person as well as the environment by engaging progressive forces in people and situational assets, and impinging the removal of environmental obstacles for growth and adaptive functioning. This model leads to an effecting dynamic change.\n\nThe role of school social workers continues to expand as the knowledge-base and the level of student need grows or the recognition of opportunities to address student need. Two examples of this role expansion include functional behavior assessment, an efficient, empirically - supported, and amenable approach to undesirable school behavior that can be accomplished in a classroom collaboration model with teachers (Waller, 2008) and a leadership role in helping schools become foundational in promoting the mental health of children and adolescents in a manner similar to the role that schools already play in promoting physical health. Indeed, the roles played by School Social Workers has grown so substantially as a direct result of student needs, consultation, education, and collaboration with other school personnel (e.g. Waller, 2008) is a practice which is only destined to grow as a means of insufficient resources being used to their greatest advantage.\n\nA survey published in 1989 by school social work experts categorized five job function dimensions.\n\n\nFurther research on these roles revealed other important areas that are frequently addressed - Consultation and teamwork; needs assessment and program evaluation; Social work interventions with systems; developmental programs management. A role where school social work falls short is in the range of administering diagnostic psychological tests. School Social Work Association of America identifies general roles like psycho-social assessment, developmental psycho-education, student and family counseling, early intervention for risk behaviors, therapeutic behavioral intervention for academic success, personality development, recreational therapies, yearly assessment, and case management for identifying students in need of help and to promote systematic change within a school system (not to stratify students into groups, and thus stratify the students' opportunities because of this)), consultation for special issues, crisis intervention and conflict resolution. Social worker deal with stressful situations. Some situations might be more complex than others since every family brings different problems.\n\nStates regulate school social work practice in different ways. Approximately 33 jurisdictions license or certify school social workers. Most require a master's degree in social work (MSW), but a smaller number of states also license Bachelors of Social Work (holders of the BSW degree). The National Association of Social Workers with 150,000 members also offers a Certified School Social Work Specialist (C-SSWS) Certificate in school social work revised from the 1992 School Social Work Credential Exam. It does not replace any license or certification that individual states require of school social workers.\n\nThe Council on Social Work Education (CSWE) is the national accrediting body for social work education at the BSW and MSW levels. It specifies foundational social work program components, but social work specialties areas are defined by the individual accredited MSW programs. \"Social work education is grounded in the liberal arts and contains a coherent, integrated professional foundation in social work practice from which an advanced practice curriculum is built at the graduate level.\"(CSWE, Educational Programs and Accreditation Standards)\n\nSchool social workers work to promote student learning and well-being, address academic and non-academic barriers to learning, develop comprehensive and cohesive academic and social supports, and understand and apply diverse frameworks for evidence-based practice and program development for the educational process to work the fullest extent.\n\nMajor associations in North America include the School Social Work Association of America, the American Council for School Social Work, and the Canadian Association of School Social Workers and Attendance Counsellors.\n\nSchool social work journals have been published across the globe including the \"School Social Work Journal\" sponsored by the Illinois Association of School Social Workers, the Journal of School Social Work (JSSW) from Chennai, India and the Canadian Journal of School Psychology from SAGE Publications, Canada.\n\n\n"}
{"id": "5348493", "url": "https://en.wikipedia.org/wiki?curid=5348493", "title": "Secondary treatment", "text": "Secondary treatment\n\nSecondary treatment is a treatment process for wastewater (or sewage) to achieve a certain degree of effluent quality by using a sewage treatment plant with physical phase separation to remove settleable solids and a biological process to remove dissolved and suspended organic compounds. After this kind of treatment, the wastewater may be called as secondary-treated wastewater.\n\nSecondary treatment is the portion of a sewage treatment sequence removing dissolved and colloidal compounds measured as biochemical oxygen demand (BOD). Secondary treatment is traditionally applied to the liquid portion of sewage after primary treatment has removed settleable solids and floating material. Secondary treatment is typically performed by indigenous, aquatic microorganisms in a managed aerobic habitat. Bacteria and protozoa consume biodegradable soluble organic contaminants (e.g. sugars, fats, and organic short-chain carbon molecules from human waste, food waste, soaps and detergent) while reproducing to form cells of biological solids. Biological oxidation processes are sensitive to temperature and, between 0 °C and 40 °C, the rate of biological reactions increase with temperature. Most surface aerated vessels operate at between 4 °C and 32 °C.\n\nPrimary treatment of sewage by quiescent settling allows separation of floating material and heavy solids from liquid waste. The remaining liquid usually contains less than half of the original solids content and approximately two-thirds of the BOD in the form of colloids and dissolved organic compounds. Where nearby water bodies can rapidly dilute this liquid waste, primary treated sewage may be discharged so natural biological decomposition oxidizes remaining waste.\n\nThe city of San Diego used Pacific Ocean dilution of primary treated effluent into the 21st century.\n\nThe United States Environmental Protection Agency (EPA) defined secondary treatment based on the performance observed at late 20th-century bioreactors treating typical United States municipal sewage. Secondary treated sewage is expected to produce effluent with a monthly average of less than 30 mg/l BOD and less than 30 mg/l suspended solids. Weekly averages may be up to 50 percent higher. A sewage treatment plant providing both primary and secondary treatment is expected to remove at least 85 percent of the BOD and suspended solids from domestic sewage. The EPA regulations describe stabilization ponds as providing treatment equivalent to secondary treatment removing 65 percent of the BOD and suspended solids from incoming sewage and discharging approximately 50 percent higher effluent concentrations than modern bioreactors. The regulations also recognize the difficulty of meeting the specified removal percentages from combined sewers, dilute industrial wastewater, or Infiltration/Inflow.\n\nWhere natural waterways are too small to rapidly oxidize primary treated sewage, the liquid may be used to irrigate sewage farms until suburban property values encourage secondary treatment methods requiring less acreage. Glacial sand deposits allowed some northeastern United States cities to use intermittent sand filtration until more compact secondary treatment bioreactors became available.\n\nBiological nutrient removal is regarded by some sanitary engineers as secondary treatment and by others as tertiary treatment. The differentiation may also differ from one country to another.\n\nThe purpose of tertiary treatment (also called \"advanced treatment\") is to provide a final treatment stage to further improve the effluent quality before it is discharged to the receiving environment (sea, river, lake, wet lands, ground, etc.). Tertiary treatment may include biological nutrient removal (alternatively, this can be classified as secondary treatment), disinfection and removal of micropollutants, such as environmental persistent pharmaceutical pollutants.\n\nProcess upsets are temporary decreases in treatment plant performance caused by significant population change within the secondary treatment ecosystem. Conditions likely to create upsets include for example toxic chemicals and unusually high or low concentrations of organic waste BOD providing food for the bioreactor ecosystem.\n\nWaste containing biocide concentrations exceeding the secondary treatment ecosystem tolerance level may kill a major fraction of one or more important ecosystem species. BOD reduction normally accomplished by that species temporarily ceases until other species reach a suitable population to utilize that food source, or the original population recovers as biocide concentrations decline.\n\nWaste containing unusually low BOD concentrations may fail to sustain the secondary treatment population required for normal waste concentrations. The reduced population surviving the starvation event may be unable to completely utilize available BOD when waste loads return to normal. Dilution may be caused by addition of large volumes of relatively uncontaminated water such as stormwater runoff into a combined sewer. Smaller sewage treatment plants may experience dilution from cooling water discharges, major plumbing leaks, firefighting, or draining large swimming pools.\n\nA similar problem occurs as BOD concentrations drop when low flow increases waste residence time within the secondary treatment bioreactor. Secondary treatment ecosystems of college communities acclimated to waste loading fluctuations from student work/sleep cycles may have difficulty surviving school vacations. Secondary treatment systems accustomed to routine production cycles of industrial facilities may have difficulty surviving industrial plant shutdown. Populations of species feeding on incoming waste initially decline as concentration of those food sources decrease. Population decline continues as ecosystem predator populations compete for a declining population of lower trophic level organisms.\n\nHigh BOD concentrations initially exceed the ability of the secondary treatment ecosystem to utilize available food. Ecosystem populations of aerobic organisms increase until oxygen transfer limitations of the secondary treatment bioreactor are reached. Secondary treatment ecosystem populations may shift toward species with lower oxygen requirements, but failure of those species to use some food sources may produce higher effluent BOD concentrations. More extreme increases in BOD concentrations may drop oxygen concentrations before the secondary treatment ecosystem population can adjust, and cause an abrupt population decrease among important species. Normal BOD removal efficiency will not be restored until populations of aerobic species recover after oxygen concentrations rise to normal.\n\nMeasures creating uniform wastewater loadings tend to reduce the probability of upsets. Fixed-film or attached growth secondary treatment bioreactors are similar to a plug flow reactor model circulating water over surfaces colonized by biofilm, while suspended-growth bioreactors resemble a continuous stirred-tank reactor keeping microorganisms suspended while water is being treated. Secondary treatment bioreactors may be followed by a physical phase separation to remove biological solids from the treated water. Upset duration of fixed film secondary treatment systems may be longer because of the time required to recolonize the treatment surfaces. Suspended growth ecosystems may be restored from a population reservoir. Activated sludge recycle systems provide an integrated reservoir if upset conditions are detected in time for corrective action. Sludge recycle may be temporarily turned off to prevent sludge washout during peak storm flows when dilution keeps BOD concentrations low. Suspended growth activated sludge systems can be operated in a smaller space than fixed-film trickling filter systems that treat the same amount of water; but fixed-film systems are better able to cope with drastic changes in the amount of biological material and can provide higher removal rates for organic material and suspended solids than suspended growth systems.\n\nWastewater flow variations may be reduced by limiting stormwater collection by the sewer system, and by requiring industrial facilities to discharge batch process wastes to the sewer over a time interval rather than immediately after creation. Discharge of appropriate organic industrial wastes may be timed to sustain the secondary treatment ecosystem through periods of low residential waste flow. Sewage treatment systems experiencing holiday waste load fluctuations may provide alternative food to sustain secondary treatment ecosystems through periods of reduced use. Small facilities may prepare a solution of soluble sugars. Others may find compatible agricultural wastes, or offer disposal incentives to septic tank pumpers during low use periods.\n\nA great number of secondary treatment processes exist, see List of wastewater treatment technologies. The main ones are explained below.\n\nIn older plants and those receiving variable loadings, trickling filter beds are used where the settled sewage liquor is spread onto the surface of a bed made up of coke (carbonized coal), limestone chips or specially fabricated plastic media. Such media must have large surface areas to support the biofilms that form. The liquor is typically distributed through perforated spray arms. The distributed liquor trickles through the bed and is collected in drains at the base. These drains also provide a source of air which percolates up through the bed, keeping it aerobic. Biofilms of bacteria, protozoa and fungi form on the media’s surfaces and eat or otherwise reduce the organic content. The filter removes a small percentage of the suspended organic matter, while the majority of the organic matter supports microorganism reproduction and cell growth from the biological oxidation and nitrification taking place in the filter. With this aerobic oxidation and nitrification, the organic solids are converted into biofilm grazed by insect larvae, snails, and worms which help maintain an optimal thickness. Overloading of beds may increase biofilm thickness leading to anaerobic conditions and possible bioclogging of the filter media and ponding on the surface.\n\nRotating biological contactors (RBCs) are robust mechanical fixed-film secondary treatment systems capable of withstanding surges in organic load. RBCs were first installed in Germany in 1960 and have since been developed and refined into a reliable operating unit. The rotating disks support the growth of bacteria and micro-organisms present in the sewage, which break down and stabilize organic pollutants. To be successful, micro-organisms need both oxygen to live and food to grow. Oxygen is obtained from the atmosphere as the disks rotate. As the micro-organisms grow, they build up on the media until they are sloughed off due to shear forces provided by the rotating discs in the sewage. Effluent from the RBC is then passed through a secondary clarifier where the sloughed biological solids in suspension settle as a sludge.\n\nActivated sludge is a common suspended-growth method of secondary treatment. Activated sludge plants encompass a variety of mechanisms and processes using dissolved oxygen to promote growth of biological floc that substantially removes organic material. Biological floc is an ecosystem of living biota subsisting on nutrients from the inflowing primary clarifier effluent. These mostly carbonaceous dissolved solids undergo aeration to be broken down and either biologically oxidized to carbon dioxide or converted to additional biological floc of reproducing micro-organisms. Nitrogenous dissolved solids (amino acids, ammonia, etc.) are similarly converted to biological floc or oxidized by the floc to nitrites, nitrates, and, in some processes, to nitrogen gas through denitrification. While denitrification is encouraged in some treatment processes, denitrification often impairs the settling of the floc causing poor quality effluent in many suspended aeration plants. Overflow from the activated sludge mixing chamber is sent to a secondary clarifier where the suspended biological floc settles out while the treated water moves into tertiary treatment or disinfection. Settled floc is returned to the mixing basin to continue growing in primary effluent. Like most ecosystems, population changes among activated sludge biota can reduce treatment efficiency. Nocardia, a floating brown foam sometimes misidentified as \"sewage fungus\", is the best known of many different fungi and protists that can overpopulate the floc and cause process upsets. Elevated concentrations of toxic wastes including pesticides, industrial metal plating waste, or extreme pH, can kill the biota of an activated sludge reactor ecosystem.\n\nOne type of system that combines secondary treatment and settlement is the cyclic activated sludge (CASSBR), or sequencing batch reactor (SBR). Typically, activated sludge is mixed with raw incoming sewage, and then mixed and aerated. The settled sludge is run off and re-aerated before a proportion is returned to the headworks.\n\nThe disadvantage of the CASSBR process is that it requires a precise control of timing, mixing and aeration. This precision is typically achieved with computer controls linked to sensors. Such a complex, fragile system is unsuited to places where controls may be unreliable, poorly maintained, or where the power supply may be intermittent. Extended aeration package plants use separate basins for aeration and settling, and are somewhat larger than SBR plants with reduced timing sensitivity.\n\nPackage plants may be referred to as \"high charged\" or \"low charged\". This refers to the way the biological load is processed. In high charged systems, the biological stage is presented with a high organic load and the combined floc and organic material is then oxygenated for a few hours before being charged again with a new load. In the low charged system the biological stage contains a low organic load and is combined with flocculate for longer times.\n\nMembrane bioreactors (MBR) are activated sludge systems using a membrane liquid-solid phase separation process. The membrane component uses low pressure microfiltration or ultrafiltration membranes and eliminates the need for a secondary clarifier or filtration. The membranes are typically immersed in the aeration tank; however, some applications utilize a separate membrane tank. One of the key benefits of an MBR system is that it effectively overcomes the limitations associated with poor settling of sludge in conventional activated sludge (CAS) processes. The technology permits bioreactor operation with considerably higher mixed liquor suspended solids (MLSS) concentration than CAS systems, which are limited by sludge settling. The process is typically operated at MLSS in the range of 8,000–12,000 mg/L, while CAS are operated in the range of 2,000–3,000 mg/L. The elevated biomass concentration in the MBR process allows for very effective removal of both soluble and particulate biodegradable materials at higher loading rates. Thus increased sludge retention times, usually exceeding 15 days, ensure complete nitrification even in extremely cold weather.\n\nThe cost of building and operating an MBR is often higher than conventional methods of sewage treatment. Membrane filters can be blinded with grease or abraded by suspended grit and lack a clarifier's flexibility to pass peak flows. The technology has become increasingly popular for reliably pretreated waste streams and has gained wider acceptance where infiltration and inflow have been controlled, however, and the life-cycle costs have been steadily decreasing. The small footprint of MBR systems, and the high quality effluent produced, make them particularly useful for water reuse applications.\n\nAerobic granular sludge can be formed by applying specific process conditions that favour slow growing organisms such as PAOs (polyphosphate accumulating organisms) and GAOs (glycogen accumulating organisms). Another key part of granulation is selective wasting whereby slow settling floc-like sludge is discharged as waste sludge and faster settling biomass is retained. This process has been commercialized as Nereda process.\n\nAerated lagoons are a low technology suspended-growth method of secondary treatment using motor-driven aerators floating on the water surface to increase atmospheric oxygen transfer to the lagoon and to mix the lagoon contents. The floating surface aerators are typically rated to deliver the amount of air equivalent to 1.8 to 2.7 kg O/kW·h. Aerated lagoons provide less effective mixing than conventional activated sludge systems and do not achieve the same performance level. The basins may range in depth from 1.5 to 5.0 metres. Surface-aerated basins achieve 80 to 90 percent removal of BOD with retention times of 1 to 10 days. Many small municipal sewage systems in the United States (1 million gal./day or less) use aerated lagoons.\n\nPrimary clarifier effluent was discharged directly to eutrophic natural wetlands for decades before environmental regulations discouraged the practice. Where adequate land is available, stabilization ponds with constructed wetland ecosystems can be built to perform secondary treatment separated from the natural wetlands receiving secondary treated sewage. Constructed wetlands resemble fixed-film systems more than suspended growth systems, because natural mixing is minimal. Constructed wetland design uses plug flow assumptions to compute the residence time required for treatment. Patterns of vegetation growth and solids deposition in wetland ecosystems, however, can create preferential flow pathways which may reduce average residence time. Measurement of wetland treatment efficiency is complicated because most traditional water quality measurements cannot differentiate between sewage pollutants and biological productivity of the wetland. Demonstration of treatment efficiency may require more expensive analyses.\n\n\n\n"}
{"id": "33026432", "url": "https://en.wikipedia.org/wiki?curid=33026432", "title": "Soul, Mind, Body Medicine", "text": "Soul, Mind, Body Medicine\n\nSoul, Mind, Body Medicine: A Complete Soul Healing System for Optimum Health and Vitality is a self-help book written by spiritual healer Zhi Gang Sha which provides a controversial interpretation of Traditional Chinese medicine and quantum physics. Published in 2006, within three weeks of its release the book was placed in the top five of \"The New York Times\" Best Seller list.\n\n"}
{"id": "40018674", "url": "https://en.wikipedia.org/wiki?curid=40018674", "title": "Stereopsis recovery", "text": "Stereopsis recovery\n\nStereopsis recovery, also recovery from stereoblindness, is the phenomenon of a stereoblind person gaining partial or full ability of stereo vision (stereopsis).\n\nRecovering stereo vision as far as possible has long been established as an approach to the therapeutic treatment of stereoblind patients. Treatment aims to recover stereo vision in very young children, as well as in patients who had acquired but lost their ability for stereopsis due to a medical condition. In contrast, this aim has normally not been present in the treatment of those who missed out on learning stereopsis during their first few years of life. In fact, the acquisition of binocular and stereo vision was long thought to be impossible unless the person acquired this skill during a critical period in infancy and early childhood. This hypothesis normally went unquestioned and has formed the basis for the therapeutic approaches to binocular disorders for decades. It has been put in doubt in recent years. In particular since studies on stereopsis recovery began to appear in scientific journals and it became publicly known that neuroscientist Susan R. Barry achieved stereopsis well into adulthood, that assumption is in retrospect considered to have held the status of a scientific dogma.\n\nVery recently, there has been a rise in scientific investigations into stereopsis recovery in adults and youths who have had no stereo vision before. While it has now been shown that an adult \"may\" gain stereopsis, it is currently not yet possible to predict how likely a stereoblind person is to do so, nor is there general agreement on the best therapeutic procedure. Also the possible implications for the treatment of children with infantile esotropia are still under study.\n\nIn cases of acquired strabismus with double vision (diplopia), it is long-established state of the art to aim at curing the double vision and at the same time recovering a patient's earlier ability for stereo vision. For example, a patient may have had full stereo vision but later had diplopia due to a medical condition, losing stereo vision. In this case, medical interventions, including vision therapy and strabismus surgery, may remove the double vision and recover the stereo vision which had temporarily been absent in the patient.\n\nAlso when children with congenital (infantile) strabismus (e.g. infantile esotropia) receive strabismus surgery within the first few years or two of their life, this goes along with the hope that they may yet develop their full potential for binocular vision including stereopsis.\n\nIn contrast, in a case where a child's eyes are straightened surgically after the age of about five or six years and the child had no opportunity to develop stereo vision in early childhood, normally the clinical expectation is that this intervention will lead to cosmetic improvements but not to stereo vision. Conventionally, no follow-up for stereopsis was performed in such cases.\n\nFor instance, one author summarized the accepted scientific view of the time with the words: \"Stereopsis will never be obtained unless amblyopia is treated, the eyes are aligned, and binocular fusion and function are achieved before the critical period for stereopsis ends. Clinical data suggest that this occurs before 24 months of age,[…] but we do not know exactly when it occurs, because crucial pieces of basic science information are missing.\" For purposes of illustration, reference is made to a book of doctors' handouts for patients, written for the general public and published in 2002, which summarizes the limitations in the terms in which they, at the time, were fully accepted as medical state of the art as follows: \"If an adult has a childhood strabismus that was never treated, it is too late to improve any amblyopia or depth perception, so the goal may be simply cosmetic – to make the eyes appear to be properly aligned – though sometimes treatment does enlarge the extent of side vision.\" It has only been accepted very recently that the therapeutic approach was based on an unquestioned notion that has, since, been referred to as \"myth\" or \"dogma\".\n\nRecently, however, stereopsis recovery is known to have occurred in a number of adults. While this has in some cases occurred after visual exercises or spontaneous visual experiences, recently also the medical community's view of strabismus surgery has become more optimistic with regard to outcomes in terms of binocular function and possibly stereopsis. As one author states:\nScientific investigations on residual neural plasticity in adulthood now also include studies on the recovery of stereopsis. Now it is a matter of active scientific investigation under which conditions and to which degree binocular fusion and stereo vision can be acquired in adulthood, especially if the person is not known to have had any preceding experience of stereo vision, and how outcomes may depend on the patient's history of therapeutic interventions.\n\nStereopsis recovery has been reported to have occurred in a few adults as a result of either medical treatments including strabismus surgery and vision therapy, or spontaneously after a stereoscopic 3D cinema experience.\n\nThe most renowned case of regained stereopsis is that of neuroscientist Susan R. Barry, who had had alternating infantile esotropia with diplopia, but no amblyopia, underwent three surgical corrections in childhood without achieving binocular vision at the time, and recovered from stereoblindness in adult age after vision therapy with optometrist Theresa Ruggiero. Barry's case has been reported on by neurologist Oliver Sacks. Also David H. Hubel, winner of the 1981 Nobel Prize in Physiology or Medicine with Torsten Wiesel for their discoveries concerning information processing in the visual system, commented positively on her case. In 2009, Barry published a book \"Fixing My Gaze: A Scientist's Journey into Seeing in Three Dimensions\", reporting on her own and several other cases of stereopsis recovery.\n\nIn her book \"Fixing my Gaze\", Susan Barry gives a detailed description of her surprise, elation and subsequent experiences when her stereo vision suddenly set in.\n\nHubel wrote of her book:\nHer book includes reports of further persons who have had similar experiences with stereopsis recovery. Barry cites the personal experiences of several persons, including a man who was an artist and described his experience of seeing with stereopsis as \"that he could see one hundred more times negative space\", a woman who had been amblyopic before seeing in 3D described how empty space now \"looks and feels palpable, tangible—alive!\", a woman who had been strabismic since age two and saw in 3D after taking vision therapy and stated that \"The coolest thing is the feeling you get being “in the dimension”\", a woman who felt quite alarmed at the experience of suddenly seeing roadside trees and signs looming towards her, and two women who experienced an abrupt onset of stereo vision with a wide-angled view of the world, the first stating: \"I was able to take in so much more of the room than I did before\" and the second: \"It was very dramatic as my peripheral vision suddenly filled in on both sides\".\n\nCommon to Barry and at least one person on whom she had reported is the finding that also their mental representation of space changed after having acquired stereo vision: that even with one eye closed the feeling is to see \"more\" than seeing with one eye closed before recovering stereopsis.\n\nApart from Barry, another formerly stereoblind adult whose acquired ability for stereopsis has received media attention is neuroscientist Bruce Bridgeman, professor of psychology and psychobiology at University of California Santa Cruz, who had grown up nearly stereoblind and acquired stereo vision spontaneously in 2012 at the age of 67, when watching the 3D movie Hugo with polarizing 3D glasses. The scene suddenly appeared to him in depth, and the ability to see the world in stereo stayed with him also after leaving the cinema.\n\nThere is a growing recent body of scientific literature on investigations into the recovery of stereopsis in adults which started to appear shortly before Oliver Sacks' \"The New Yorker\" publication drew public attention to Barry's discovery. A number of scientific publications have systematically assessed patients' post-surgical stereopsis, whereas other studies have investigated the effects of eye training procedures.\n\nCertain conditions are known to be a prerequisite for stereo vision, for instance, that the amount of horizontal deviation, if any is present, needs to be small. In several studies it has been recognized that surgery to correct strabismus can have the effect of improving binocular function. One of these studies, published in 2003, explicitly concluded: \"We found that improvement in binocularity, including stereopsis, can be obtained in a substantial portion of adults.\" That article was published together with a discussion of the results among peers in which the scientific and social implications of the medical treatment were addressed, for example concerning the long-term relevancy of stereopsis, the importance of avoiding diplopia, the necessity of predictable outcomes, and psychosocial and socioeconomic relevance.\n\nAmong the investigations into post-surgical stereopsis is a publication of 2005 that reported on a total of 43 adults over 18 years of age who had surgical correction after having lived with from constant-horizontal strabismus for more than 10 years with no previous surgery or stereopsis, with visual acuity of 20/40 or more also in the deviating eye; in this group, stereopsis was present in 80% of exotropes and 31% of esotropes, with the recovery of stereopsis and stereoacuity being uncorrelated to the number of years the deviation had persisted. A study that was published 2006 included, aside an extensive review of investigations on stereopsis recovery of the last decades, a re-evaluation of all those patients who had had congenital or early-onset strabismus with a large constant horizontal divergence and had undergone strabismus surgery in the years 1997–1999 in a given clinic, excluding those who had a history of neurologic or systemic diseases or with organic retinal diseases. Among the resulting 36 subjects aged 6–30 years, many had regained binocular vision (56% according to an evaluation with Bagolini striated glasses, 39% with Titmus test, 33% with Worth 4-dot test, and 22% with Random dot E test) and 57% had stereoacuity of 200 sec of arc of better, leading to the conclusion that some degrees of stereopsis can be achieved even in cases of infantile or early-childhood strabism. Another study  found that some chronically strabismic adults with good vision could recover fusion and stereopsis by means of surgical alignment.\n\nIn contrast, in a study in which a group of 17 adults and older children of at least 8 years of age, all of whom received strabismus surgery and post-operative evaluation after long-standing untreated infantile esotropia, most showed binocular fusion when tested with Bagolini lenses and an increased visual field, but none demonstrated stereo fusion or stereopsis.\n\nStereoacuity is limited by the visual acuity of the eyes, and in particular by the visual acuity of the weaker eye. That is, the more a patient's vision of any one of the two eyes is degraded compared to the 20/20 vision standard, the lower are the prospects of improving or re-gaining stereo vision, unless visual acuity itself were improved by other means. Strabismus surgery itself does not improve visual acuity.\n\nOrthoptic exercises have proven to be effective for reducing symptoms in patients with convergence insufficiency and decompensating exophoria by improving the near-point convergence of the eyes that is necessary for binocular fusion.\n\nExperiments on monkeys, published 2007, revealed improvements in stereoacuity in monkeys who, after having been raised with binocular deptivation through prisms for the first two years, were exposed to extensive psychophysical training. Their stereo vision recovered in part, but remained far more limited than that of normally raised monkeys.\n\nScientists at the University of California, Berkeley have stated that perceptual learning appears to play an important role. One investigation, published 2011, reported on a study on human stereopsis recovery using perceptual learning which was inspired by Barry's work. In this study, a small number of stereoblind subjects who had initially been stereoblind or stereoanomalous recovered stereopsis using perceptual learning exercises. Alongside the scientific assessment of the extent of recovery, also the subjective outcomes are described: \"After achieving stereopsis, our observers reported that the depth “popped out,” which they found very helpful and joyful in their everyday life. The anisometropic observer GD noticed “a surge in depth” one day when shopping in a supermarket. While playing table tennis, she feels that she is able to track a ping-pong ball more accurately and therefore can play better. Strabismic observer AB is more confident now when walking down stairs because she can judge the depth of the steps better. Strabismics AB, DP, and LR, are able to enjoy 3D movies for the first time, and strabismic GJ finds it easier to catch a fly ball while playing baseball.\" In a follow-up study, the authors of this study pointed out that the stereopsis that was recovered following perceptual learning was more limited in resolution and precision compared to normal subjects' stereopsis. Dennis M. Levi was awarded the 2011 Charles F. Prentice Medal of the American Academy of Optometry for this work.\nThere have been several attempts to make use of modern technology for enhanced binocular eye training, in particular for treating amblyopia and interocular suppression. In some cases these modern techniques have improved patients' stereoacuity. Very early technology-enhanced vision therapy efforts have included the cheiroscope, which is a haploscope in which left- and or right-eye images can be blended into view over a drawing pad, and the subject may be given a task such as to reproduce a line image presented to one eye. However, historically these approaches were not developed much further and they were not put to widespread use. Recent systems are based on dichoptic presentation of the elements of a video game or virtual reality such that each eye receives different signals of the virtual world that the player's brain must combine in order to play successfully.\n\nOne of the earliest systems of this kind has been proposed by a research group in the University of Nottingham with the aim of treating amblyopia, using virtual reality masks or commercially available 3D shutter glasses. The group also has worked to develop perceptual learning training protocols that specifically target the deficit in stereo acuity to allow the recovery of normal stereo function even in adulthood.\n\nAnother system of dichoptic presentation for binocular vision therapy has been proposed by researchers of the Research Institute of the McGill University Health Centre. Using a modified puzzle video game \"Tetris\", the interocular suppression of patients with amblyopia was successfully treated with dichotomic training in which certain parameters of the training material were systematically adapted during the course of four weeks. Clinical supervision of such procedures is required to ensure that double vision does not occur. Most of the patients who underwent this treatment gained improved visual acuity of the weaker eye, and some also showed increased stereoacuity. Another study performed at the same institute showed that dichoptic training can be more effective in adults than the more conventional amblyopia treatment of an eye patch. For this investigation, 18 adults played the game Tetris for one hour each day, half of the group wearing eye patches and the other half playing a dichoptic version of the game. After two weeks, the group who played dichoptically showed a significant improvement of vision in the weaker eye and in stereopsis acuity; the eye patch group had moderate improvements, which increased substantially after they, too, were given the dichoptic training afterwards. Dichoptic-based perceptual learning therapy, presented by means of a head-mounted display, is amenable also to amblyopic children, as it improves both the amblyopic eye's visual acuity and the stereo function. The researchers at McGill University have shown that one to three weeks of playing a dichoptic video game for one to two hours on a hand-held device \"can improve acuity and restore binocular function, including stereopsis in adults\". Furthermore, it has been suggested that these effect can be enhanced by anodal transcranial direct current stimulation (tDCS).\n\nTogether with Levi of the University of California, Berkeley, scientists at the University of Rochester have made further developments in terms of virtual reality computer games which have shown some promise in improving both monocular and binocular vision in human subjects.\n\nGame developer James Blaha, who developed his own crowd-funded version of a dichoptic VR game for the Oculus Rift together with Manish Gupta and is continuing to experiment with the game, experienced steropsis for the first time using his game. In 2011, two cases of adults with anisometropic amblyopia were reported whose visual acuity and stereoacuity improved due to learning-based therapies.\n\nThere are indications that the suppression of binocularity in amblyopic subjects is due to a suppression mechanism that prevents the amblyopic brain from learning to see. It has been suggested that desuppression and neuroplasticity may be favored by specific conditions that are commonly associated with perceptual learning tasks and video game playing such as a heightened requirement of attention, a prospect of reward, a feeling of enjoyment and a sense of flow.\n\nHealth insurances always review therapies in terms of clinical effectiveness in view of existing scientific literature, benefit, risk and cost. Even if individual cases of recovery exist, a treatment is only considered effective under this point of view if there is sufficient likelihood that it will predictably improve outcomes.\n\nIn this context, medical coverage policy of the global health services organization Cigna \"does not cover vision therapy, optometric training, eye exercises or orthoptics because they are considered experimental, investigational or unproven for any indication including the management of visual disorders and learning disabilities\" based on a bibliographic review published by Cigna which concludes that \"insufficient evidence exists in the published, peer-reviewed literature to conclude that vision therapy is effective for the treatment of any of the strabismic disorders except preoperative prism adaptation for acquired esotropia\". Similarly, the U.S. managed health care company Aetna offers vision therapy only in contracts with supplemental coverage and limits its prescriptions to a number of conditions that are explicitly specified in a list of vision disorders.\n\n"}
{"id": "20115793", "url": "https://en.wikipedia.org/wiki?curid=20115793", "title": "The Dartmouth Institute for Health Policy and Clinical Practice", "text": "The Dartmouth Institute for Health Policy and Clinical Practice\n\nThe Dartmouth Institute for Health Policy and Clinical Practice (TDI) is an organization within Dartmouth College \"dedicated to improving health care through education, research, policy reform, leadership improvement, and communication with patients and the public.\" It was founded in 1988 by John Wennberg as the Center for the Evaluative Clinical Sciences (CECS); a reorganization in 2007 led to TDI's current structure.\nDr. Elliott S. Fisher, MD, MPH, was named Director of The Dartmouth Institute on 1 April 2013. An internationally recognized leader in health services research and health policy, Dr. Fisher is the James W. Squire Professor of Medicine and Community and Family Medicine at the Geisel School of Medicine at Dartmouth. He is also the co-director of the Dartmouth Atlas for Health Care. Dr. Fisher is a member of the Institute of Medicine at the National Academy of Sciences. \n\nThe institute provides a graduate-level education program involving elements of both Dartmouth's Graduate Arts and Sciences Programs and the Geisel School of Medicine. It grants Masters in Public Health degrees as well as Master of Science and Doctor of Philosophy in Health Policy and Clinical Science degrees. The institute is located at One Medical Center Drive, WTRB, Level 5 on the Dartmouth Hitchcock Hospital campus, Lebanon, NH. The institute's largest policy product is the Dartmouth Atlas of Health Care, which documents unwarranted variation in the American health care system.\n"}
{"id": "30006354", "url": "https://en.wikipedia.org/wiki?curid=30006354", "title": "Tomotaka Tasaka", "text": "Tomotaka Tasaka\n\nBorn in Hiroshima Prefecture, he began working at Nikkatsu's Kyoto studio in 1924 and eventually came to prominence for a series of realist, humanist films made at Nikkatsu's Tamagawa studio in the late 1930s such as \"Robō no ishi\" and \"Mud and Soldiers\", both of which starred Isamu Kosugi. His war film, \"Five Scouts\", was screened in the competition at the 6th Venice International Film Festival.\n\nTasaka was a victim of the atomic bombing of Hiroshima and spent many years recovering. He eventually resumed directing and won the best director prize at the 1958 Blue Ribbon Awards for \"A Slope in the Sun\", which starred Yūjirō Ishihara.\n\nHis brother, Katsuhiko Tasaka, was also a film director, and his wife, Hisako Takihana, was an actress.\n\n\n"}
{"id": "50806866", "url": "https://en.wikipedia.org/wiki?curid=50806866", "title": "Why Am I?", "text": "Why Am I?\n\nWhy Am I?: The Science of Us (also known as Predict My Future: The Science of Us) is a 2016 New Zealand documentary series about the Dunedin Multidisciplinary Health and Development Study, a long-running cohort study following 1037 people born in Dunedin, New Zealand during 1972 and 1973. The study revealed the result of the combined effects of hereditary (genes) and environment (upbringing) on how people turn out.\n\nThe series of four sixty minute episodes was made by Razor Films of Auckland, New Zealand, and screened on TV One from 31 May to 21 June 2016, with all four episodes available online on TVNZ On Demand.\n\nThe series follows the study and information it has provided in almost every field of medical and social development including respiratory and cardiovascular health, addictions, obesity, sexual health, cognitive neuroscience, psychiatry, genetics and criminology and the effects of nature and nurture on health and behaviour.\n\n"}
{"id": "19073965", "url": "https://en.wikipedia.org/wiki?curid=19073965", "title": "William Thomas Frederick Davies", "text": "William Thomas Frederick Davies\n\nLieutenant-Colonel William Thomas Frederick Davies CMG DSO (13 August 1860 – 24 June 1947) was a South African surgeon, army officer and politician.\n\nDavies trained at Guy's Hospital in London. In the South African War he served as Surgeon-Major with the Imperial Light Horse and was awarded the Distinguished Service Order (DSO). From 1914 to 1915, he commanded the 2nd Imperial Light Horse in German South-West Africa, where he was wounded. From 1915 to 1917, he was a member of the South African House of Assembly, for which he was appointed Companion of the Order of St Michael and St George (CMG) in the 1920 New Year Honours. \n\nIn 1917, he joined the Royal Army Medical Corps and remained with the corps until 1919, serving as Surgeon Specialist at the General Military Hospital, Colchester. Returning home, he became President of the South African Medical Council.\n\n"}
{"id": "41845071", "url": "https://en.wikipedia.org/wiki?curid=41845071", "title": "Ștefan Stoica (politician)", "text": "Ștefan Stoica (politician)\n\nŞtefan Stoica (1976 – 4 February 2014) was a Romanian politician. He was a member of the Senate from 2012 to 2014.\n\nA physician by profession, he was a member of the Democratic Liberal Party before switching to the People's Party – Dan Diaconescu, and he was elected while belonging to that party, for a seat in Ialomița County. In October 2013, he defected to the Social Democratic Party.\n\nStoica died of cancer at the age of 37.\n"}
