{"id": "43318121", "url": "https://en.wikipedia.org/wiki?curid=43318121", "title": "ADIME", "text": "ADIME\n\nADIME, or Assessment, Diagnosis, Intervention, and Monitoring/Evaluation, is a process used to ensure high quality nutrition care to patients and clients from nutrition professionals, such as Registered Dietitians (RD) or Registered Dietitian Nutritionist (RDN). ADIME is used as a means of charting patient progress and to encourage a universal language amongst nutrition professionals.\n\nThe ADIME process consists of four steps:\n"}
{"id": "34807342", "url": "https://en.wikipedia.org/wiki?curid=34807342", "title": "Aasandha", "text": "Aasandha\n\nAasandha (Dhivehi: އާސަންދަ) is the universal health insurance scheme of the Maldives. It is managed in a public-private partnership with Allied Insurance Company of the Maldives and began its services on the 1 January 2012. \n\n"}
{"id": "44559447", "url": "https://en.wikipedia.org/wiki?curid=44559447", "title": "Abortion in Bahrain", "text": "Abortion in Bahrain\n\nAbortion in Bahrain is legal upon request, upon authorization by a panel of physicians. By the Penal Code of 1976, abortion is only illegal in Bahrain when it is self-induced which subjects the pregnant woman to up to six months in prison, or when it is performed without the woman's consent, which merits up to ten years' imprisonment.\n\nThe United Nations reported an abortion rate of 11.1 abortions per 1000 women aged 15–44 .\n"}
{"id": "44198247", "url": "https://en.wikipedia.org/wiki?curid=44198247", "title": "Abortion in Bangladesh", "text": "Abortion in Bangladesh\n\nAbortion in Bangladesh is illegal under most situations, but menstrual regulation is often used as a substitute. Bangladesh is still governed by the penal code from 1860, where induced abortion is illegal unless the woman in danger.\n\nHistorically, abortion has been prevalent, especially during the years following the Bangladesh Liberation War. For example, in 1972, the law allowed for abortion for those women who has been raped during the war. In 1976, the Bangladesh National Population Policy unsuccessfully attempted to legalize abortion in the first trimester.\n\nSince 1979, menstrual regulation has been the favored alternative to induced abortion, and it is legally permitted because pregnancy cannot be established. In 2012, the Drug Administration for Bangladesh legalised the combination of mifepristone and misoprotol for medical abortion.\n\nPart of the family planning program in Bangladesh since 1979, menstrual regulation is a procedure that uses manual vacuum aspiration to make it impossible to be pregnant after missing a period. It is simple and can be done with inexpensive equipment. Its procedure also goes without the use of anaesthesia.\n\nA study about menstrual regulation in 2013 studied 651 consenting women from 10 different facilities in Bangladesh, who were seeking menstrual regulation and were about 63 days or less late of their menstrual cycle. They were given about 200 mg of mifepristone, followed later by 800 mg of misoprostol. The researchers found that 93% of the women had evacuated the uterus without the use of the surgical intervention, and 92% of the women were satisfied with the pills and the rest of the treatment.\n\nAlthough menstrual regulation centers are centralized and free of charge, many women still lack access due to socioeconomic barriers and social stigma. Centers charge additional fees if the pregnancy is beyond 10 weeks, and many women are unaware of menstrual regulation or face male opposition to the procedure. As a result, some women turn to illegal abortions.\n\nAn abortion can be legally performed by a physician in a hospital, if it is necessary to save the life of the mother. A person who performs an abortion under any other circumstances, including a woman who self-aborts, can be punished by a fine and imprisonment.\n\nMenstrual regulation allows a woman to terminate within 10 weeks of her last period, but unsafe methods to terminate pregnancy are widespread. In response, a hotline was created for women to get information about fertility control, including menstrual regulation.\n\nAccording to an article by the Guttmacher Institute, which studied the rural district of Matlab, illegal abortion is becoming increasingly prevalent despite the availability of safer methods of fertility control.\n\nA study by Mizanur Rahman and Julie DaVanzo showed that between 2000 and 2008, a woman was more likely to die from the complications of unsafe abortion than from childbirth itself, and that death rates from childbirth were similar of the death rates for the complications of menstrual regulation.\n\nAnother study in Matlab found that between 1982 and 1998, abortion about 35 times more prevalent among unmarried adolescent girls than among married adolescent girls, and it was much higher among who were less than 18 years of age and those who passed or had more than primary education.\n\n, the national rate that women participate in menstrual regulation as a post-contraceptive way to control their fertility was 10 per 1,000 women aged 15–49. The national rate for induced abortion was 29 per 1,000 women in the same age interval. The United Nations estimated that in 2000, the abortion rate was 4.0 abortions per 1,000 women aged 15–44.\n\nOnly 42% of facilities that were expected to provide menstrual regulation services actually did so. Of the Union Health and Family Welfare Centres, which are especially relied upon in rural areas, half provided these services. According to Guttmacher, about 27% of women (about 105,000) are turned away annually. In addition, in 2014, about 50% of ever-married Bangladeshi women had not heard of menstrual regulation.\n\nIn 2014, it was estimated that between 523,808 and 769,269 abortions occurred per year in Bangladesh. \n"}
{"id": "34113938", "url": "https://en.wikipedia.org/wiki?curid=34113938", "title": "Act on Health Sector Database", "text": "Act on Health Sector Database\n\nThe Act on Health Sector Database, also known as Act on Health Sector Database, No. 139/1998, the Health Sector Database Act and in media by other colloquial names, was a 1998 act of the Icelandic Parliament which allowed the Icelandic government to grant a license to a private company for the creation of a national biological database to store health information which could be used for research. The act was noted for boldly introducing policy related to biobanks and was the subject of controversy.\n\ndeCODE genetics did most of the lobbying for the act and was the beneficiary of the license to create the database.\n\nThe passing of this act spurred international discussion about what policies were already in place and what differences in policy existed among biobanks.\n\nThe establishment of a national database for all Icelandic citizens raised discussion about the nature of the informed consent process for the project.\n\n"}
{"id": "727904", "url": "https://en.wikipedia.org/wiki?curid=727904", "title": "Alexandre Yersin", "text": "Alexandre Yersin\n\nAlexandre Emile Jean Yersin (22 September 1863 – 1 March 1943) was a Swiss and naturalized French physician and bacteriologist. He is remembered as the discoverer of the bacillus responsible for the bubonic plague or pest, which was later named in his honour (\"Yersinia pestis\").\n\nYersin was born in 1863 in Aubonne, Canton of Vaud, Switzerland, to a family originally from France. From 1883 to 1884, Yersin studied medicine at Lausanne, Switzerland; and then at Marburg, Germany and Paris (1884–1886). In 1886, he entered Louis Pasteur's research laboratory at the École Normale Supérieure, by invitation of Emile Roux, and participated in the development of the anti-rabies serum. In 1888 he received his doctorate with a dissertation titled \"Étude sur le Développement du Tubercule Expérimental\" and spent two months with Robert Koch in Germany. He joined the recently created Pasteur Institute in 1889 as Roux's collaborator and discovered with him the diphtheric toxin (produced by the \"Corynebacterium diphtheriae\" bacillus).\n\nTo practice medicine in France, Yersin applied for and obtained French nationality in 1888. Soon afterwards (1890), he left for French Indochina in Southeast Asia as a physician for the \"Messageries Maritimes\" company, on the Saigon-Manila line and then on the Saigon-Haiphong line. He participated in one of the Auguste Pavie missions. In 1894 Yersin was sent by request of the French government and the Pasteur Institute to Hong Kong, to investigate the Manchurian pneumonic plague epidemic.\n\nThere, in a small hut (according to \"Plague\" by Wendy Orent) since he was denied access to English hospitals at his arrival, he made his greatest discovery: that of the pathogen which causes the disease. Dr Kitasato Shibasaburō, also in Hong Kong, had identified a bacterium several days earlier. There is controversy whether this was the same pneumococci or a mix of the two. Because Kitasato's initial reports were vague and somewhat contradictory, some give Yersin sole credit for the discovery. However, a thorough analysis of the morphology of the organism discovered by Kitasato has determined that \"we are confident that Kitasato had examined the plague bacillus in Hong Kong in late June and early July 1894\", only days after Yersin announced his own discovery on 20 June. Therefore, Kitasato \"should not be denied this credit\". The plague bacillus develops better at lower temperatures, so Yersin's less well-equipped lab turned out to be an advantage in the race with Kitasato, who used an incubator. Therefore, although at first named “Kitasato-Yersin bacillus” by the scientific community, the microbe will later assume only the latter’s name because the one identified by Kitasato, a type of streptococcus, cannot be found in the lymphatic glands. Yersin was also able to demonstrate for the first time that the same bacillus was present in the rodent as well as in the human disease, thus underlining the possible means of transmission. This important discovery was communicated to the French Academy of Sciences in the same year, by his colleague Emile Duclaux, in a classic paper titled \"La peste bubonique à Hong-Kong\".\n\nFrom 1895 to 1897, Yersin further pursued his studies on the bubonic plague. In 1895 he returned to the Institute Pasteur in Paris and with Émile Roux, Albert Calmette and Amédée Borrel, prepared the first anti-plague serum. In the same year, he returned to Indochina, where he installed a small laboratory at Nha Trang to manufacture the serum (in 1905 this laboratory became a branch of the Pasteur Institute). Yersin tried the serum received from Paris in Canton and Amoy, in 1896, and in Bombay, India, in 1897, with disappointing results. Having decided to stay in his country of adoption, he participated actively in the creation of the Medical School of Ha Noi in 1902, and was its first director, until 1904.\n\nYersin tried his hand at agriculture and was a pioneer in the cultivation of rubber trees (\"Hevea brasiliensis\") imported from Brazil into Indochina. For this purpose, he obtained in 1897 a concession from the government to establish an agricultural station at Suoi Dau. He opened a new station at Hon Ba in 1915, where he tried to acclimatize the quinine tree (\"Cinchona ledgeriana\"), which was imported from the Andes in South America by the Spaniards, and which produced the first known effective remedy for preventing and treating malaria (a disease which prevails in Southeast Asia to this day).\n\nAlexandre Yersin is well remembered in Vietnam, where he was affectionately called Ông Năm (Mr. Nam/Fifth) by the people.\n\nOn 8 January 1902, Yersin was accredited to be the first Headmaster of Hanoi Medical University by the Governor-General of French Indochina, Paul Doumer. Following the country's independence, streets named in his honor kept their designation and his tomb in Suoi Dau was graced by a pagoda where rites are performed in his worship. His house in Nha Trang is now the Yersin Museum, and the epitaph on his tombstone describes him as a \"Benefactor and humanist, venerated by the Vietnamese people\". In Ha Noi, a French lycée has his name. A private university founded in 2004 in Da Lat was named \"Yersin University\" in his honour.\n\nIn 1934 he was nominated honorary director of Pasteur Institute and a member of its Board of Administration. He died during World War II at his home in Nha Trang, in 1943.\n\nDr Yersin was credited for finding the site for the town of Dalat (300 km northeast of Saigon) in 1893. Because of the high altitude and European-like climate, Dalat soon became an R&R spot for French officers. There was a high school named after him which was built in the 1920s, the Lycée Yersin, aka Grand Lycée (grade 6 to 12), the Petit Lycée (elementary to grade 5) and a university named for him which was built in the 2000s.\n\nWhile in Hong Kong, Yersin was helped in his research by an Italian priest of the PIME order, Bernardo Viganò (1837–1901). He provided dead bodies and assisted him in his quest for a remedy for the plague.\n\n\n\n\n"}
{"id": "16656819", "url": "https://en.wikipedia.org/wiki?curid=16656819", "title": "Alfredo Avelín", "text": "Alfredo Avelín\n\nAlfredo Avelín (1 May 1927 – 26 January 2012) was an Argentine politician, physician and author. He served as Governor of his province of San Juan and as a member of the Argentine Senate and Chamber of Deputies.\n\nAvelín was born in San Juan Province to parents of Lebanese descent. He earned a medical degree at the University of Córdoba, and later founded the Colegio Médico de la Provincia. Entering politics, he founded the Renewal Crusade party. He became Mayor of the provincial capital, San Juan, in 1958, and at the age of 31.\n\nHe was ultimately elected to the Argentine Chamber of Deputies in 1989, and to the Argentine Senate, in 1992.\n\nOn 16 May 1999, Avelín was elected as governor of his province with 55%, heading the list of the Alliance for Work, Justice and Education, which would win the presidency of Argentina later the same year. He defeated the incumbent Governor, Jorge Escobar. His period in office was marked by economic turbulence and public unrest in San Juan, not unlike the country as a whole; by 2001, provincial employees were not being paid, and the province was insolvent.\n\nFollowing the downfall of the government of President de la Rua, Avelín was a fierce opponent of the austerity measures proposed by the International Monetary Fund, saying, \"The only thing lacking for us is to pull down the Argentine flag and replace it with the IMF's.\" In 2002, Avelín was impeached and deposed as governor by a majority of provincial deputies following massive demonstrations.\n\nAvelín considered running again for the governorship in 2007 against the incumbent, José Luis Gioja. His daughter, Nancy Avelín, who also served as a senator, was eventually the candidate for the Renewal Crusade that year.\n\nAlfredo Avelín died on 26 January 2012.\n\n"}
{"id": "2739229", "url": "https://en.wikipedia.org/wiki?curid=2739229", "title": "Audiometer", "text": "Audiometer\n\nAn audiometer is a machine used for evaluating hearing acuity. They usually consist of an embedded hardware unit connected to a pair of headphones and a test subject feedback button, sometimes controlled by a standard PC. Such systems can also be used with bone vibrators, to test conductive hearing mechanisms. \n\nAudiometers are standard equipment at ENT (ear, nose, throat) clinics and in audiology centers. \nAn alternative to hardware audiometers are software audiometers, which are available in many different configurations.\nScreening PC-based audiometers use a standard computer.\nClinical PC-based audiometers are generally more expensive than software audiometers, but are much more accurate and efficient. They are most commonly used in hospitals, audiology centers and research communities. These audiometers are also used to conduct industrial audiometric testing. Some audiometers even provide a software developer's kit that provides researchers with the capability to create their own diagnostic tests.\n\nAn audiometer typically transmits recorded sounds such as pure tones or speech to the headphones of the test subject\nat varying frequencies and intensities, and records the subject's responses to produce an audiogram of threshold sensitivity, or speech understanding profile.\n\nMedical grade audiometers are usually an embedded hardware unit controlled from a PC. Software audiometers which run on a PC are also commercially available, but their accuracy and utility for evaluating hearing loss is questionable due to lack of a calibration standard.\n\nThe most common type of audiometer generates pure tones, or transmits parts of speech.\nAnother kind of audiometer is the Bekesy audiometer, in which the subject follows a tone of increasing and decreasing amplitude as the tone is swept through the frequency range by depressing a button when the tone is heard and releasing it when it cannot be heard, crossing back and forth over the threshold of hearing. Bekesy audiometry typically yields lower thresholds and standard deviations than pure tone audiometry.\n\nAudiometer requirements and the test procedure are specified in IEC 60645, ISO 8253, and ANSI isoS3.6 standards.\n\n\n"}
{"id": "39961287", "url": "https://en.wikipedia.org/wiki?curid=39961287", "title": "Baroda Development Screening Test", "text": "Baroda Development Screening Test\n\nBaroda Developmental Screening Test is a screening test for motor-mental assessment of infants, developed from Bayley Scales of Infant Development. It is meant to be used by child psychologists rather than physicians.\nIt can be applied up to 30 month of age.\nkit is commercially available for it.\n\nThe test was developed by Promila Phatak in 1991 at Department of Child Development, University of Baroda.\n\nOther tests used for developmental surveillance of a child:\n"}
{"id": "52459702", "url": "https://en.wikipedia.org/wiki?curid=52459702", "title": "Cannabis in Sierra Leone", "text": "Cannabis in Sierra Leone\n\nCannabis in Sierra Leone is illegal, but is widely cultivated and consumed in the country, and exported to neighboring countries and to Europe. Cannabis is known locally as diamba.\n\nCannabis is believed to have become commonly cultivated in Sierra Leone well before it became widespread in West Africa. Midwives used it as anaesthesia for childbirth, and fishermen used it to deal with their difficult labors. An 1851 journal article reported that cannabis had been \"long in use\" in the interior of Sierra Leone, and claimed that cannabis seeds were brought to the colony by \"Congoes captured by one of our cruisers.\" Sierra Leoneon sailors and stevedores played a role in disseminating cannabis regionally, spreading the use of the drug to Ghana and Gambia.\n\nCannabis was first banned in Sierra Leone in 1920, during the country's British colonial period; cannabis was included in that year as an addendum to the nation's 1913 Opium Act.\n\nThe 2011 \"International Narcotics Control Strategy Report\" noted that cannabis is widely cultivated in Sierra Leone, to the degree that the national government was concerned that cannabis may be crowding out subsistence farming and threatening food security.\n\nA doctor stated to Sierra Leone's Truth and Reconciliation Committee in 2003: \"Cannabis sativa is so commonly used or abused in Sierra Leone... that I don't think people consider it a crime any more to use it... As you can see, it is grown nearly everywhere in Sierra Leone today. You can get it anytime, anywhere, either for free or for a low fee.\"\n"}
{"id": "9001425", "url": "https://en.wikipedia.org/wiki?curid=9001425", "title": "Capital punishment in Italy", "text": "Capital punishment in Italy\n\nThe use of capital punishment in Italy has been banned since 1889, with the exception of the period 1926-1947, encompassing the rule of Fascism in Italy and the early restoration of democracy. Before the unification of Italy in 1860, capital punishment was performed in almost all pre-unitarian states, except for Tuscany, where it was historically abolished in 1786. It is currently out of use as a result of the adoption of the current constitution, and defunct as of 1 January 1948.\n\nIn Italy, the first pre-unitarian state to abolish the death penalty was the Grand Duchy of Tuscany as of November 30, 1786, under the reign of Pietro Leopoldo, who later became Leopold II, Holy Roman Emperor. So Tuscany was the first modern European state in the world to do away with torture and capital punishment.\nHowever, the death penalty was sanctioned in the codes of law of all the other pre-unitarian states, therefore when the Kingdom of Italy was proclaimed in 1860, legislation was divided, since the death penalty was legal in all of Italy except for Tuscany.\n\nAfterwards, the death penalty was definitively abolished in the Penal Code in 1889 with the almost unanimous approval of both Houses of Parliament under suggestion of Minister Zanardelli. However executions in Italy had not been carried out since 1877, when King Umberto I granted a general pardon (royal decree of pardon of January 18, 1878). Ironically, as a result of this pardon, Gaetano Bresci could not be sentenced to death after he assassinated Umberto I in 1900. The death penalty was still present in military and colonial penal codes.\n\nIn 1926, it was reintroduced by dictator Benito Mussolini to punish those who made an attempt on the king, the queen, the heir apparent or the Prime Minister as well as for espionage and armed rebellion. The Rocco Code (1930, in force from July 1, 1931) added more crimes to the list of those punishable with the death penalty, and reintroduced capital punishment for some common crimes. It was used sparsely, however; until the outbreak of war in 1940, a total of nine executions were carried out, allegedly not for political offenses, followed by another 17 until Italy's surrender in July 1943 (compared to almost 80,000 legal executions in Nazi Germany, including courts martial).\n\nThe last people executed for civil crimes were three Sicilian robbers, also convicted of murder, who battered and threw into a well ten people (while still alive) on a farm near Villarbasse (province of Turin) in 1945. The president, Enrico de Nicola, declined to pardon them, and they were executed by a firing squad on March 4, 1947 at Basse di Stura riverside, in the suburbs of Turin. This was the last execution in Italy.\n\nThe Italian Constitution, approved on December 27, 1947 and in force since January 1, 1948, completely abolished the death penalty for all common military and civil crimes during peacetime. This measure was implemented by the legislative decree 22/48 of January 22, 1948 (provision of coordination as a consequence of the abolishment of capital punishment). The death penalty was still in force in Italy in the military penal code, only for high treason against the Republic or for crimes perpetrated in war theatres (though no execution ever took place) until law 589/94 of October 13, 1994 abolished it completely from there as well, and substituted it with the maximum penalty of the civil penal code (imprisonment for life). In 2007 a constitutional amendment was adopted. Article 27 of Italian Constitution was changed to fully ban the death penalty.\n\nPrior to abolition, the death penalty was sanctioned in article 21 of the Italian penal code. It stated that \"Death penalty is to be carried out by shooting inside a penitentiary or in any other place suggested by the Ministry of Justice. The execution is not public, unless the Ministry of Justice determines otherwise.\"\n\nA draft law to ratify the 13th Protocol of the European Convention on Human Rights had been approved by the Senate on October 9, 2008 (it was approved earlier by the Chamber of Deputies on September 24). It was ratified on March 3, 2009.\n\nFewer than half of Italians approved of the 2006 execution of Saddam Hussein. Italy proposed the UN moratorium on the death penalty, which urges states to establish a moratorium on executions with a view toward abolition and urged states around the world to approve it. The former Italian Foreign Minister Massimo D'Alema also stated that the next step was to work on abolishing the death penalty.\n\nThe 2008 European Values Study (EVS) found that 62.6% of respondents in Italy said that the death penalty can never be justified, while only 4.8% said it can always be justified.\n\n"}
{"id": "1557627", "url": "https://en.wikipedia.org/wiki?curid=1557627", "title": "Cross-sectional study", "text": "Cross-sectional study\n\nIn medical research and social science, a cross-sectional study (also known as a cross-sectional analysis, transverse study, prevalence study) is a type of observational study that analyzes data from a population, or a representative subset, \"at a specific point in time\"—that is, cross-sectional data.\n\nIn economics, cross-sectional studies typically involve the use of cross-sectional regression, in order to sort out the existence and magnitude of causal effects of one or more independent variables upon a dependent variable of interest at a given point in time. They differ from time series analysis, in which the behavior of one or more economic aggregates is traced through time.\n\nIn medical research, cross-sectional studies differ from case-control studies in that they aim to provide data on the entire population under study, whereas case-control studies typically include only individuals with a specific characteristic, with a sample, often a tiny minority, of the rest of the population. Cross-sectional studies are descriptive studies (neither longitudinal nor experimental). Unlike case-control studies, they can be used to describe, not only the odds ratio, but also absolute risks and relative risks from prevalences (sometimes called \"prevalence risk ratio\", or PRR). They may be used to describe some feature of the population, such as prevalence of an illness, or they may support inferences of cause and effect. Longitudinal studies differ from both in making a series of observations more than once on members of the study population over a period of time.\n\nCross-sectional studies involve data collected at a defined time. They are often used to assess the prevalence of acute or chronic conditions, but cannot be used to answer questions about the causes of disease or the results of intervention. Cross-sectional data cannot be used to infer causality because temporality is not known. They may also be described as censuses. Cross-sectional studies may involve special data collection, including questions about the past, but they often rely on data originally collected for other purposes. They are moderately expensive, and are not suitable for the study of rare diseases. Difficulty in recalling past events may also contribute bias.\n\nThe use of routinely collected data allows large cross-sectional studies to be made at little or no expense. This is a major advantage over other forms of epidemiological study. A natural progression has been suggested from cheap cross-sectional studies of routinely collected data which suggest hypotheses, to case-control studies testing them more specifically, then to cohort studies and trials which cost much more and take much longer, but may give stronger evidence. In a cross-sectional survey, a specific group is looked at to see if an activity, say alcohol consumption, is related to the health effect being investigated, say cirrhosis of the liver. If alcohol use is correlated with cirrhosis of the liver, this would support the hypothesis that alcohol use may be associated with cirrhosis.\n\nRoutine data may not be designed to answer the specific question. \n\nRoutinely collected data does not normally describe which variable is the cause and which the effect. Cross-sectional studies using data originally collected for other purposes are often unable to include data on confounding factors, other variables that affect the relationship between the putative cause and effect. For example, data only on present alcohol consumption and cirrhosis would not allow the role of past alcohol use, or of other causes, to be explored.\n\nMost case-control studies collect specifically designed data on all participants, including data fields designed to allow the hypothesis of interest to be tested. However, in issues where strong personal feelings may be involved, specific questions may be a source of bias. For example, past alcohol consumption may be incorrectly reported by an individual wishing to reduce their personal feelings of guilt. Such bias may be less in routinely collected statistics, or effectively eliminated if the observations are made by third parties, for example taxation records of alcohol by area.\n\nCross-sectional studies can contain individual-level data (one record per individual, for example, in national health surveys). However, in modern epidemiology it may be impossible to survey the entire population of interest, so cross-sectional studies often involve secondary analysis of data collected for another purpose. In many such cases, no individual records are available to the researcher, and group-level information must be used. Major sources of such data are often large institutions like the Census Bureau or the Centers for Disease Control in the United States. Recent census data is not provided on individuals, for example in the UK individual census data is released only after a century. Instead data is aggregated, usually by administrative area. Inferences about individuals based on aggregate data are weakened by the ecological fallacy. Also consider the potential for committing the \"atomistic fallacy\" where assumptions about aggregated counts are made based on the aggregation of individual level data (such as averaging census tracts to calculate a county average). For example, it might be true that there is no correlation between infant mortality and family income at the city level, while still being true that there is a strong relationship between infant mortality and family income at the individual level. All aggregate statistics are subject to compositional effects, so that what matters is not only the individual-level relationship between income and infant mortality, but also the proportions of low, middle, and high income individuals in each city. Because case-control studies are usually based on individual-level data, they do not have this problem.\n\nIn economics, cross-sectional analysis has the advantage of avoiding various complicating aspects of the use of data drawn from various points in time, such as serial correlation of residuals. It also has the advantage that the data analysis itself does not need an assumption that the nature of the relationships between variables is stable over time, though this comes at the cost of requiring caution if the results for one time period are to be assumed valid at some different point in time.\n\nAn example of cross-sectional analysis in economics is the regression of money demand—the amounts that various people hold in highly liquid financial assets—at a particular time upon their income, total financial wealth, and various demographic factors. Each data point is for a particular individual or family, and the regression is conducted on a statistical sample drawn at one point in time from the entire population of individuals or families. In contrast, an intertemporal analysis of money demand would use data on an entire country's holdings of money at each of various points in time, and would regress that on contemporaneous (or near-contemporaneous) income, total financial wealth, and some measure of interest rates. The cross-sectional study has the advantage that it can investigate the effects of various demographic factors (age, for example) on individual differences; but it has the disadvantage that it cannot find the effect of interest rates on money demand, because in the cross-sectional study at a particular point in time all observed units are faced with the same current level of interest rates.\n\n\n"}
{"id": "48258076", "url": "https://en.wikipedia.org/wiki?curid=48258076", "title": "DFB Sports Court", "text": "DFB Sports Court\n\nThe DFB Sports Court (German: DFB-Sportgericht) is a regulatory body in the German Football Association (, DFB) and may adopt different sanctions on clubs and players.\n\nTogether with the Bundesliga, the DFB Sports Court was founded in 1963. It hears cases of misconduct by individual players, clubs or spectators. The court is a separate authority, responsible for the national and regional leagues. The judges and staff are volunteers.\n\nThe structure resembles that of a normal court. The court is composed of a chief judge, a deputy and 28 assessors. The chief judge and the deputy are elected by the DFB-Bundestag. Chief judge is Hans Eberhard Lorenz.\n\nThe DFB Sports Court convenes when serious rule violations occur. This starts directly after a red card is given. The court determines the sentence depending on the hardness of the fouls or unsportsmanlike conduct. If the clubs in question agree, the sports court creates a written statement. Only 20% of procedures end with a hearing.\n\nAppeals against decisions from the DFB Sports Court can be presented to the DFB Federal Court.\n\n\n"}
{"id": "37445789", "url": "https://en.wikipedia.org/wiki?curid=37445789", "title": "Disability and poverty", "text": "Disability and poverty\n\nThe world's poor are significantly more likely to have or incur a disability within their lifetime compared to more financially privileged populations. The rate of disability within impoverished nations is notably higher than that found in more developed countries. Though no one explanation entirely accounts for this connection, recently there has been a substantial amount of research illustrating the cycle by which poverty and disability are mutually reinforcing. Physical, cognitive, mental, emotional, sensory, or developmental impairments independently or in tandem with one another may increase one's likelihood of becoming impoverished, while living in poverty may increase one's potential of having or acquiring special needs in some capacity.\n\nA multitude of studies have been shown to demonstrate a significant rate of disability among individuals living in poverty. People with disabilities were shown by the World Bank to comprise 15 to 20 percent of the poorest individuals in developing countries. Former World Bank President James Wolfensohn has stated that this connection reveals a link that should be broken. He stated, “People with disabilities in developing countries are over-represented among the poorest people. They have been largely overlooked in the development agenda so far, but the recent focus on poverty reduction strategies is a unique change to rethink and rewrite that agenda.” The link between disability and development has been further stressed by Judith Heumann, the World Bank's first advisor for international disability rights, who indicated that of the 650 million people living with disabilities today eighty percent live in developing countries. Additionally, some research investigations with proved social impact are opening venues that lead to establish enabling factors to break the cycle of deprivation faced by poor people with disabilities. According to the United Kingdom Department for International Development, 10,000 individuals with disabilities die each day as a result of extreme poverty, showing that the connection between these two constructs is especially problematic and deep-seated. This connection is also present in developed countries, with the Disability Funders Network reporting that in the United States alone those with disabilities are twice as likely to live below the poverty line than those without special needs.\n\nAccording to the World Bank, “Persons with disabilities on average as a group experience worse socioeconomic outcomes than persons without disabilities, such as less education, worse health outcomes, less employment, and higher poverty rates.” Researchers have demonstrated that these reduced outcomes may be attributed to a myriad of institutional barriers and other factors. Furthermore, the prevalence of disabilities in impoverished populations has been predicted to follow a cyclical pattern by which those who live in poverty are more likely to acquire a disability and those who have a disability are more likely to become impoverished.\n\nExperts from the United Kingdom Disabled Persons Council attribute the connection between disability and poverty to many systemic factors that promote a “vicious circle.” Statistics affirm the mutually reinforcing nature of special needs and low socioeconomic status, showing that people with disabilities are significantly more likely to become impoverished and people who are impoverished are significantly more likely to become disabled. Barriers presented for those with disabilities can lead individuals to be deprived of access to essential resources, such as opportunities for education and employment, thus causing them to fall into poverty. Likewise, poverty places individuals at a much greater risk of acquiring a disability due to the general lack of health care, nutrition, sanitation, and safe working conditions that the poor are subject to.\n\nExperts assert that this cycle is perpetuated mainly by the lack of agency afforded to those living in poverty. The few options available to the poor often necessitate that these individuals put themselves in harms way, consequently resulting in an increase in the acquisition of preventable impairments. Living in poverty is also shown to decrease an individual's access to preventative health services, which results in an increase in the acquisition of potentially preventable disabilities. In a study by Oxfam, the organization found that well over half of the instances of childhood blindness and hearing impairment in Africa and Asia were considered preventable or treatable. Another estimate released by Oxfam provides further evidence of this vicious circle, finding that 100 million people living in poverty suffer from impairments acquired due to malnutrition and lack of proper sanitation.\n\nDiscrimination\nPrejudice held against individuals with disabilities, otherwise termed ableism, is shown to be a significant detriment to the successful outcomes of persons in this population. According to one study following the lives of children with disabilities in South Africa, the children in the sample described \"discrimination from other metal children and adults in the community as their most significant daily problem.\"\n\nAdditional forms of discrimination may lead disability to be more salient in already marginalized populations. Women and individuals belonging to certain ethnic groups who have disabilities have been found to more greatly suffer from discrimination and endure negative outcomes. Some researchers attribute this to what they believe is a “double rejection” of girls and women who are disabled on the basis of their sex in tandem with their special needs. The stereotypes that accompany both of these attributes lead females with disabilities to be seen as particularly dependent upon others and serve to amplify the misconception of this population as burdensome. In a study done by Oxfam, the societal consequences of having a disability while belonging to an already marginalized population were highlighted, stating, “A disabled women suffers a multiple handicap. Her chances of marriage are very slight, and she is most likely to be condemned to a twilight existence as a non-productive adjunct to the household of her birth… it is small wonder that many disabled female babies do not survive.” Additionally, women with disabilities are particularly susceptible to abuse. A 2004 UN survey in Orissa, India, found that every women with disabilities in their sample had experienced some form of physical abuse. This double discrimination is also shown to be prevalent in more industrialized nations. In the United States, for example, 72 percent of women with disabilities live below the poverty line. The intensified discrimination individuals with disabilities may face due to their sex is especially important to consider when taking into account that, according to the Organisation for Economic Co-operation and Development, women report higher incidences of disability than men. Furthermore, the connection between disability and poverty holds particular significance for the world's women, with females accounting for roughly 70 percent of all individuals living in poverty.\n\nInstitutional discrimination also exists as there are policies existing in organizations that result in inequality between a disabled person and non-disabled person. Some of these organizations systematically ignore the needs of disabled people and some interfere in their lives as a means of social control.\n\nAnother reason individuals living with disabilities are often impoverished is the high medical costs associated with their needs. One study, conducted in villages in South India, demonstrated that the annual cost of treatment and equipment needed for individuals with disabilities in the area ranged from three days of income to upwards of two years' worth, with the average amount spent on essential services totaling three months worth of income. This figure does not take into account the unpaid work of caregivers who must provide assistance after these procedures and the opportunity costs leading to a loss of income during injury, surgery, and rehabilitation.\nStudies reported by medical anthropologists Benedicte Ingstad and Susan Reynolds Whyte have also shown that access to medical care is significantly impaired when one lacks mobility. They report that in addition to the direct medical costs associated with special needs, the burden of transportation falls most heavily on those with disabilities. This is especially true for the rural poor whose distance from urban environments necessitates extensive movement in order to obtain health services. Due to these barriers, both economic and physical, it is estimated that only 2 percent of individuals with disabilities have access to adequate rehabilitation services.\n\nThe inaccessibility of health care for those living in poverty has a substantial impact on the rate of disability within this population. Individuals living in poverty face higher health risks and are often unable to obtain proper treatment, leading them to be significantly more likely to acquire a disability within their lifetime. Financial barriers are not the only obstacles those living in poverty are confronted with. Research shows that matters of geographic inaccessibility, availability, and cultural limitations all provide substantial impediments to the acquisition of proper care for the populations of developing countries. Sex-specific ailments are particularly harmful for women living in poverty. The World Health Organization estimates that each year 20 million women acquire disabilities due to complications during pregnancy and childbirth that could be significantly mitigated with proper pre-natal, childbirth, and post-natal medical care.\nOther barriers to care are present in the lack of treatments developed to target diseases of poverty. Experts assert that the diseases most commonly affecting those in poverty attract the least research funding. This discrepancy, known as the 10/90 gap, reveals that only 10 percent of global health research focuses on conditions that account for 90 percent of the global disease burden. Without a redistribution in research capital, it is likely that many of the diseases known to cause death and disability in impoverished populations will persist.\n\nResearchers assert that institutional barriers play a substantial role in the incidence of poverty in those with disabilities.\n\nPhysical environment may be a large determinant in one's ability to access ladders of success or even basic sustenance. Professor of urban planning Rob Imrie concluded that most spaces contain surmountable physical barriers that unintentionally create an “apartheid by design,” whereby individuals with disabilities are excluded from areas because of the inaccessible layout of these spaces. This \"apartheid\" has been seen by some, such as the United Kingdom Disabled Persons Council, as especially concerning with regard to public transportation, education and health facilities, and perhaps most relevantly places of employment. Physical barriers are also commonly found in the home, with those in poverty more likely to occupy tighter spaces inaccessible to wheelchairs. Beyond physical accessibility, other potential excluding agents include a lack of Braille, sign language and shortage of audio tape availability for those who are blind and deaf.\n\nThe roots of unemployment are speculated to begin with discrimination at an early age. UNESCO reports that 98 percent of children with disabilities in developing countries are denied access to formal education. According to the World Bank, at least 40 million children with disabilities do not receive an education thus barring them from obtaining knowledge essential to gainful employment and forcing them to grow up to be financially dependent upon others. This is also reflected in a finding obtained by the World Development Report that 77 percent of persons with disabilities are illiterate. This statistic is even more jarring for women with disabilities, with the United Nations Development Program reporting that the global literacy rate for this population is a mere 1 percent. This may be attributed to the fact that, according to the World Health Organization, boys with disabilities are significantly more likely to receive an education than similarly abled girls. Beyond simply the skills obtained, experts such as former World Bank advisor Judith Heumann speculate that the societal value of education and the inability of schools to accommodate special needs children substantially contributes to the discrimination of these individuals. It is important to note that the deprivation of education to individuals with special needs may not be solely an issue of discrimination, but an issue of resources. Children with disabilities often require specialized educational resources and teaching practices largely unavailable in developing countries.\n\nSome sociologists have found a number of barriers to employment for individuals with disabilities. These may be seen in employer discrimination, architectural barriers within the workplace, pervasive negative attitudes regarding skill, and the adverse reactions of customers. According to sociologist Edward Hall, \"More disabled people are unemployed, in lower status occupations, on low earnings, or out of the labour market altogether, than non-disabled people.\" The International Labour Organization estimates that roughly 386 million of the world's working age population have some form of disability, however, up to eighty percent of these employable individuals with disabilities are unable to find work. Statistics show that individuals with disabilities in both industrialized and developing countries are generally unable to obtain formal work. In India, only 100,000 of the country's 70 million individuals with disabilities are employed. In the United States, 14.3 of a projected 48.9 million people with disabilities were employed, with two-thirds of those unemployed reporting that they were unable to find work. Similarly in Belgium, only 30 percent of persons with disabilities were able to find gainful employment. In the United Kingdom, 45 percent of adults with disabilities were found to live below the poverty line. Reliable data on the rate of unemployment for persons with disabilities has yet to be determined in most developing countries.\n\nSociologists Colin Barnes and Geof Mercer demonstrated that this exclusion of persons with disabilities from the paid labor market is a primary reason why the majority of this population experiences far greater levels of poverty and are more reliant on the financial support of others. In addition to the economic gains associated with employment, researchers have shown that participation in the formal economic sector reduces discrimination of persons with disabilities. One anthropologist who chronicled the lives of persons with disabilities in Botswana noted that individuals who were able to find formal employment “will usually obtain a position in society equal to that of non-disabled citizens.” Because the formal workplace is such a social space, the exclusion of individuals with disabilities from this realm is seen by some sociologists to be a significant impediment to social inclusion and equality.\n\nEquity in employment has been strategized by some, such as sociologists Esther Wilder and William Walters, to depend on heightened awareness of current barriers, wider use of assistive technologies that can make workplaces and tasks more accessible, more accommodating job development, and most importantly deconstructing discrimination.\n\nCreating inclusive employment that better facilitates the participation of individuals with disabilities is demonstrated to have a significantly positive impact on not only the lives of these individuals, but also the economies of nations who implement such measures. The International Labour Organization estimates that the current exclusion of employable individuals with special needs is costing countries possible gains of 1 to 7 percent of their GDP.\n\nThe relationship between disability and poverty is seen by many to be especially problematic given that it places those with the greatest needs in a position where they have access to the fewest resources. Researchers from the United Nations and the Yale School of Public Health refer to the link between disability and poverty as a manifestation of a self-fulfilling prophecy where the assumption that this population is a drain of resources leads society to deny them access to avenues of success. Such exclusion of individuals on the basis of their special needs in turn denies them the opportunity to make meaningful contributions that disprove these stereotypes. Oxfam asserts that this negative cycle is largely due to a gross underestimation of the potential held by individuals with disabilities and a lack of awareness of the possibilities that each person may hold if the proper resources were present.\n\nThe early onset of preventable deaths has been demonstrated as a significant consequence of disability for those living in poverty. Researchers show that families who lack adequate economic agency are unable to care for children with special medical needs, resulting in preventable deaths. In times of economic hardship studies show families may divert resources from children with disabilities because investing in their livelihood is often perceived as an investment caretakers cannot afford to make. Benedicte Ingstad, an anthropologist who studied families with a member with disabilities, asserted that what some may consider neglect of individuals with disabilities “was mainly a reflection of the general hardship that the household was living under.\" A study conducted by Oxfam found that the rejection of a child with disabilities was not uncommon in areas of extreme poverty. The report went on to show that neglect of children with disabilities was far from a deliberate choice, but rather a consequence of a lack of essential resources. The study also demonstrated that services necessary to the well being of these children “are seized upon” when they are made available. The organization thus concludes that if families had the capacity to care for children with special needs they would do so willingly, but often the inability to access crucial resources bars them from administering proper care.\n\nInitiatives on the local, national, and transnational levels addressing the connection between poverty and disability are exceedingly rare. According to the UN, only 45 countries throughout the world have anti-discrimination and other disability-specific laws. Additionally, experts point to the Western world as a demonstration that the association between poverty and disability is not naturally dissolved through the development process. Instead, a conscious effort toward inclusive development is seen by theorists, such as Disability Policy expert Mark Priestley, as essential in the remediation process.\n\nDisability rights advocate James Charlton asserts that it is crucial to better incorporate the voices of individuals with disabilities into the decision making process. His literature on disability rights made popular the slogan, “Nothing about us without us,” evidencing the need to ensure those most affected by policy have an equitable hand in its creation. This need for agency is an issue particularly salient for those with special needs who are often negatively stereotyped as dependent upon others. Furthermore, many who are part of the disability rights movement argue that there is too little emphasis on aid designed to eliminate the physical and social barriers those with disabilities face. The movement asserts that unless these obstacles are rectified, the connection between disability and poverty will persist.\n\nEmployment is seen as a critical agent in reducing stigma and increasing capacity in the lives of individuals with disabilities. The lack of opportunities currently available is shown to perpetuate the vicious cycle, causing individuals with disabilities to fall into poverty. To address these concerns many recent initiatives have begun to develop more inclusive employment structures. One example of this is the Ntiro Project for Supported and Inclusive Employment. Located in South Africa, the project aims to eliminate the segragationist models prevalent in the country through coordinated efforts between districts, NGOs, and community organizations. The model stresses education and pairs individuals with intellectual disabilities with mentors until they have developed the skills necessary to perform their roles independently. The program then matches individuals with local employers. This gradualist model ensures that people who may have been deprived of the resources necessary to acquire essential skills are able to build their expertise and enter the workforce.\n\nThe United Nations has been at the forefront of initiating legislation that aims to deter the current toll disabilities take on individuals in society, especially those in poverty. In 1982 the UN published the World Programme of Action Concerning Disabled Persons, which explicitly states \"Particular efforts should be made to integrate the disabled in the development process and that effective measures for prevention, rehabilitation and equalization of opportunities are therefore essential.\" This doctrine set stage for the UN Decade of the Disabled Person from 1983 to 1992, where, at its close, the General Assembly adopted the Standard Rules of the Equalization of Opportunities for Persons with Disabilities. The Standard Rules encourages states to remove social, cultural, economic, educational, and political barriers that bar individuals with disabilities from participating equally in society. Proponents claim that these movements on behalf of the UN helped facilitate more inclusive development policy and brought disability rights to the forefront.\n\nCritics assert that the relationship between disability and poverty may be overstated. Cultural differences in the definition of disability, bias leading to more generous estimates on behalf of researchers, and the variability in incidences that are not accounted for between countries are all speculated to be part of this mischaracterization. These factors lead some organizations to conclude that the projection asserting 10 percent of the global population belongs to the disabled community is entirely too broad. Speculation over the projection of a 10 percent disability rate has led other independent studies to collect varying results. The World Health Organization updated their estimate to 4 percent for developing countries and 7 percent for industrialized countries. USAID maintains the initial 10 percent figure, while the United Nations works off half of that rate with a projection of 5 percent. The percentage of the world's population with disabilities remains a highly contested matter.\n\nThe argument that development should be channeled to better the agency of individuals with disabilities has been contested on several grounds. First, critics argue that development is enacted to harness potential that most individuals in this population do not possess. Second, the case that health care costs for many persons with special needs are simply too great to be shouldered by the government or NGO's has been made, especially with regard to emerging economies. Furthermore, there is no guarantee that investing in an individual's rehabilitation will result in substantial change in their agency. Lastly is the proposition of priorities. It is argued that most countries in need of extensive development must focus on health ails such as infant mortality, diarrhea, and malaria that are widespread killers not limited to a specific population.\n\nCritique with respect to potential solutions has also been made. In regards to implementing change through policy, critics have noted that the weak legal standing of United Nations' documents and the lack of resources available to aid in their implementation have resulted in a struggle to achieve the goals set forth by the General Assembly. Other studies have shown that policy on a national level has not necessarily equated to marked improvements within these countries. One such example is the United States where sociologists Esther Wilder and William Walters purport that “the employment of disabled individuals has increased only marginally since the Americans with Disabilities Act was passed.” The smaller than anticipated impact of the ADA and other policy-based initiatives is seen as a critical flaw in legislation. This is because many issues surrounding disability, namely employment discrimination, are generally reconciled through the legal system necessitating that individuals engage in the often expensive process of litigation.\n\n"}
{"id": "6044476", "url": "https://en.wikipedia.org/wiki?curid=6044476", "title": "Disease theory of alcoholism", "text": "Disease theory of alcoholism\n\nThe modern disease theory of alcoholism states that problem drinking is sometimes caused by a disease of the brain, characterized by altered brain structure and function.\n\nThe American Medical Association (AMA) declared that alcoholism was an illness in 1956. In 1991, the AMA further endorsed the dual classification of alcoholism by the International Classification of Diseases under both psychiatric and medical sections.\n\nAlcoholism is a chronic problem. However, if managed properly, damage to the brain can be stopped and to some extent reversed.  In addition to problem drinking, the disease is characterized by symptoms including an impaired control over alcohol, compulsive thoughts about alcohol, and distorted thinking. Alcoholism can also lead indirectly, through excess consumption, to physical dependence on alcohol, and diseases such as cirrhosis of the liver.\n\nThe risk of developing alcoholism depends on many factors, such as environment. Those with a family history of alcoholism are more likely to develop it themselves (Enoch & Goldman, 2001); however, many individuals have developed alcoholism without a family history of the disease. Since the consumption of alcohol is necessary to develop alcoholism, the availability of and attitudes towards alcohol in an individual's environment affect their likelihood of developing the disease. Current evidence indicates that in both men and women, alcoholism is 50–60% genetically determined, leaving 40-50% for environmental influences.\n\nIn a review in 2001, McLellan et al. compared the diagnoses, heritability, etiology (genetic and environmental factors), pathophysiology, and response to treatments (adherence and relapse) of drug dependence vs type 2 diabetes mellitus, hypertension, and asthma. They found that genetic heritability, personal choice, and environmental factors are comparably involved in the etiology and course of all of these disorders, providing evidence that drug (including alcohol) dependence is a chronic medical illness.\n\nAccording to the theory, genes play a strong role in the development of alcoholism.\n\nTwin studies, adoption studies, and artificial selection studies have shown that a person's genes can predispose them to developing alcoholism. Evidence from twin studies show that concordance rates for alcoholism are higher for monozygotic twins than dizygotic twins—76% for monozygotic twins and 61% for dizygotic twins. However, female twin studies demonstrate that females have much lower concordance rates than males. Reasons for gender differences may be due to environmental factors, such as negative public attitudes towards female drinkers. Twin studies suggest that males are more likely to have a genetic predisposition for alcoholism. However, this does not suggest that a male who does have a genetic predisposition will become an alcoholic. Sometimes the individual may never encounter an environmental trigger that leads to alcoholism.\n\nAdoption studies also suggest a strong genetic tendency towards alcoholism. Studies on children separated from their biological parents demonstrates that sons of alcoholic biological fathers were more likely to become alcoholic, even though they have been separated and raised by non alcoholic parents. Female show similar results, but to a lesser degree.\n\nIn artificial selection studies, specific strains of rats were bred to prefer alcohol. These rats preferred drinking alcohol over other liquids, resulting in a tolerance for alcohol and exhibited a physical dependency on alcohol. Rats that were not bred for this preference did not have these traits. Upon analyzing the brains of these two strains of rats, it was discovered that there were differences in chemical composition of certain areas of the brain. This study suggests that certain brain mechanisms are more genetically prone to alcoholism.\n\nThe convergent evidence from these studies present a strong case for the genetic basis of alcoholism.\n\nHistorians debate who has primacy in arguing that habitual drinking carried the characteristics of a disease. Some note that Scottish physician Thomas Trotter was the first to characterize excessive drinking as a disease, or medical condition.\n\nOthers point to American physician Benjamin Rush (1745–1813), a signatory to the United States Declaration of Independence — who understood drunkenness to be what we would now call a \"loss of control\" — as possibly the first to use the term \"addiction\" in this sort of meaning.\nRush argued that \"habitual drunkenness should be regarded not as a bad habit but as a disease\", describing it as \"a palsy of the will\". Rush expounded his views in a book published in 1808. His views are described by Valverde and by Levine:\n\nSwedish physician Magnus Huss coined the term \"alcoholism\" in his book \"Alcoholismus chronicus\" (1849). Some argue he was the first to systematically describe the physical characteristics of habitual drinking and claim that it was a disease. However, this came decades after Rush and Trotter wrote their works, and some historians argue that the idea that habitual drinking was a diseased state emerged earlier.\n\nGiven this controversy, the best one can say is that the idea that habitual alcohol drinking was a disease had become more acceptable by the middle of the nineteenth century, although many writers still argued it was a vice, a sin, and not the purview of medicine but of religion.\n\nBetween 1980 and 1991, medical organizations, including the AMA, worked together to establish policies regarding their positions on the disease theory. These policies were developed in 1987 in part because third-party reimbursement for treatment was difficult or impossible unless alcoholism were categorized as a disease. The policies of the AMA, formed through consensus of the federation of state and specialty medical societies within their House of Delegates, state, in part: \"The AMA endorses the proposition that drug dependencies, including alcoholism, are diseases and that their treatment is a legitimate part of medical practice.\"\n\nIn 1991, the AMA further endorsed the dual classification of alcoholism by the International Classification of Diseases under both psychiatric and medical sections.\n\nThe disease theory is often interpreted as implying that problem drinkers are incapable of returning to 'normal' problem free drinking, and therefore that treatment should focus on total abstinence. Some critics have used evidence of controlled drinking in formerly dependent drinkers to dispute the disease theory of alcoholism.\n\nThe first major empirical challenge to this interpretation of the disease theory followed a 1962 study by Dr. D. L. Davies. Davies' follow-up of 93 problem drinkers found that 7 of them were able to return to \"controlled drinking\" (less than 7 drinks per day for at least 7 years). Davies concluded that \"the accepted view that no alcohol addict can ever again drink normally should be modified, although all patients should be advised to aim at total abstinence\"; After the Davies study, several other researchers reported cases of problem drinkers returning to controlled drinking.\n\nIn 1976, a major study commonly referred to as the RAND report, published evidence of problem drinkers learning to consume alcohol in moderation. The publication of the study renewed controversy over how people suffering a disease which reputedly leads to uncontrollable drinking could manage to drink controllably. Subsequent studies also reported evidence of return to controlled drinking. Similarly, according to a 2002 National Institute on Alcohol Abuse and Alcoholism (NIAAA) study, about one of every six (18%) of alcohol dependent adults in the U.S. whose dependence began over one year previously had become \"low-risk drinkers\" (less than 14 drinks per week and 5 drinks per day for men, or less than 7 per week and 4 per day for women). This modern longitudinal study surveyed more than 43,000 individuals representative of the U.S. adult population, rather than focusing solely on those seeking or receiving treatment for alcohol dependence. \"Twenty years after onset of alcohol dependence, about three-fourths of individuals are in full recovery; more than half of those who have fully recovered drink at low-risk levels without symptoms of alcohol dependence.\" \n\nHowever, many researchers have debated the results of the smaller studies. A 1994 followup of the original 7 cases studied by Davies suggested that he \"had been substantially misled, and the paradox exists that a widely influential paper which did much to stimulate new thinking was based on faulty data.\" The most recent study, a long-term (60 year) follow-up of two groups of alcoholic men by George Vaillant at Harvard Medical School concluded that \"return to controlled drinking rarely persisted for much more than a decade without relapse or evolution into abstinence.\" Vaillant also noted that \"return-to-controlled drinking, as reported in short-term studies, is often a mirage.\"\n\nThe second RAND study, in 1980, found that alcohol dependence represents a factor of central importance in the process of relapse. Among people with low dependence levels at admission, the risk of relapse appears relatively low for those who later drank without problems. But the greater the initial level of dependence, the higher the likelihood of relapse for nonproblem drinkers. The second RAND study findings have been strengthened by subsequent research by Dawson \"et al.\" in 2005 which found that severity was associated positively with the likelihood of abstinent recovery and associated negatively with the likelihood of non-abstinent recovery or controlled drinking. Other factors such as a significant period of abstinence or changes in life circumstances were also identified as strong influences for success in a book on Controlled Drinking published in 1981.\n\nAs part of a harm reduction strategy, provision of small amounts of alcoholic beverages to homeless alcoholics at homeless shelters in Toronto and Ottawa reduced government costs and improved health outcomes.\n\nIn 1988, the US Supreme Court upheld a regulation whereby the Veterans' Administration was able to avoid paying benefits by presuming that primary alcoholism is always the result of the veteran's \"own willful misconduct.\" The majority opinion written by Justice Byron R. White echoed the District of Columbia Circuit's finding that there exists \"a substantial body of medical literature that even contests the proposition that alcoholism is a disease, much less that it is a disease for which the victim bears no responsibility\". He also wrote: \"Indeed, even among many who consider alcoholism a \"disease\" to which its victims are genetically predisposed, the consumption of alcohol is not regarded as wholly involuntary.\" However, the majority opinion stated in conclusion that \"this litigation does not require the Court to decide whether alcoholism is a disease whose course its victims cannot control. It is not our role to resolve this medical issue on which the authorities remain sharply divided.\" The dissenting opinion noted that \"despite much comment in the popular press, these cases are not concerned with whether alcoholism, simplistically, is or is not a \"disease.\"\"\n\nThe American Bar Association \"affirms the principle that dependence on alcohol or other drugs is a disease.\"\n\nAlcoholism is a disease with a known pathology and an established biomolecular signal transduction pathway which culminates in ΔFosB overexpression within the D1-type medium spiny neurons of the nucleus accumbens; when this overexpression occurs, ΔFosB induces the addictive state.\n\nIn 2004, the World Health Organization published a detailed report on alcohol and other psychoactive substances entitled \"Neuroscience of psychoactive substance use and dependence\". It stated that this was the \"first attempt by WHO to provide a comprehensive overview of the biological factors related to substance use and dependence by summarizing the vast amount of knowledge gained in the last 20-30 years. The report highlights the current state of knowledge of the mechanisms of action of different types of psychoactive substances, and explains how the use of these substances can lead to the development of dependence syndrome.\" The report states that \"dependence has not previously been recognized as a disorder of the brain, in the same way that psychiatric and mental illnesses were not previously viewed as being a result of a disorder of the brain. However, with recent advances in neuroscience, it is clear that dependence is as much a disorder of the brain as any other neurological or psychiatric illness.\"\n\nThe American Society of Addiction Medicine and the American Medical Association both maintain extensive policy regarding alcoholism. The American Psychiatric Association recognizes the existence of \"alcoholism\" as the equivalent of alcohol dependence. The American Hospital Association, the American Public Health Association, the National Association of Social Workers, and the American College of Physicians classify \"alcoholism\" as a disease.\n\nIn the US, the National Institutes of Health has a specific institute, the National Institute on Alcohol Abuse and Alcoholism (NIAAA), concerned with the support and conduct of biomedical and behavioral research on the causes, consequences, treatment, and prevention of alcoholism and alcohol-related problems. It funds approximately 90 percent of all such research in the United States. The official NIAAA position is that \"alcoholism is a disease. The craving that an alcoholic feels for alcohol can be as strong as the need for food or water. An alcoholic will continue to drink despite serious family, health, or legal problems. Like many other diseases, alcoholism is chronic, meaning that it lasts a person's lifetime; it usually follows a predictable course; and it has symptoms. The risk for developing alcoholism is influenced both by a person's genes and by his or her lifestyle.\"\n\nSome physicians, scientists and others have rejected the disease theory of alcoholism on logical, empirical and other grounds. Indeed, some addiction experts such as Stanton Peele are outspoken in their rejection of the disease model, and other prominent alcohol researchers such as Nick Heather have authored books intending to disprove the disease model.\n\nSome critics of the disease model argue alcoholism still involves choice, not total loss of control, and stripping alcohol abusers of their choice, by applying the disease concept, is a threat to the health of the individual; the disease concept gives the substance abuser an excuse. A disease cannot be cured by force of will; therefore, adding the medical label transfers the responsibility from the abuser to caregivers. Inevitably the abusers become unwilling victims, and just as inevitably they take on that role. They argue that the disease theory of alcoholism exists only to benefit the professionals' and governmental agencies responsible for providing recovery services, and the disease model has not offered a solution for those attempting to stop abusive alcohol and drug use.\n\nThese critics hold that by removing some of the stigma and personal responsibility the disease concept actually increases alcoholism and drug abuse and thus the need for treatment. This is somewhat supported by a study which found that a greater belief in the disease theory of alcoholism and higher commitment to total abstinence to be factors correlated with increased likelihood that an alcoholic would have a full-blown relapse (substantial continued use) following an initial lapse (single use). However, the authors noted that \"the direction of causality cannot be determined from these data. It is possible that belief in alcoholism as a loss-of-control disease predisposes clients to relapse, or that repeated relapses reinforce clients' beliefs in the disease model.\"\n\nOne study found that only 25 percent of physicians believed that alcoholism is a disease. The majority believed alcoholism to be a social or psychological problem instead of a disease.\n\nA survey of physicians at an annual conference of the International Doctors in Alcoholics Anonymous reported that 80 percent believe that alcoholism is merely bad behavior instead of a disease.\n\nThomas R. Hobbs says that \"Based on my experiences working in the addiction field for the past 10 years, I believe many, if not most, health care professionals still view alcohol addiction as a willpower or conduct problem and are resistant to look at it as a disease.\"\n\nAlcoholics Anonymous says that \"Some professionals will tell you that alcoholism is a disease while others contend that it is a choice\" and \"some doctors will tell you that it is in fact a disease.\"\n\nLynn Appleton says that \"Despite all public pronouncements about alcoholism as a disease, medical practice rejects treating it as such. Not only does alcoholism not follow the model of a 'disease,' it is not amenable to standard medical treatment.\" She says that \"Medical doctors' rejection of the disease theory of alcoholism has a strong basis in the biomedical model underpinning most of their training\" and that \"medical research on alcoholism does not support the disease model.\"\n\n\"Many doctors have been loath to prescribe drugs to treat alcoholism, sometimes because of the belief that alcoholism is a moral disorder rather than a disease,\" according to Dr. Bankole Johnson, Chairman of the Department of Psychiatry at the University of Virginia. Dr Johnson's own pioneering work has made important contributions to the understanding of alcoholism as a disease.\n\nCertain medications including opioid antagonists such as naltrexone have been shown to be effective in the treatment of alcoholism, although research has not yet demonstrated long-term efficacy.\n\nFrequency and quantity of alcohol use are not related to the presence of the condition; that is, people can drink a great deal without necessarily being alcoholic, and alcoholics may drink minimally or infrequently.\n\n"}
{"id": "31559443", "url": "https://en.wikipedia.org/wiki?curid=31559443", "title": "Domestic violence and pregnancy", "text": "Domestic violence and pregnancy\n\nPregnancy when coupled with domestic violence is a form of intimate partner violence (IPV) where health risks may be amplified. Abuse during pregnancy, whether physical, verbal or emotional, produces many adverse physical and psychological effects for both the mother and fetus. Domestic violence during pregnancy is categorized as abusive behavior towards a pregnant woman, where the pattern of abuse can often change in terms of severity and frequency of violence. Abuse may be a long-standing problem in a relationship that continues after a woman becomes pregnant or it may commence in pregnancy. Although female-to-male partner violence occurs in these settings, the overwhelming form of domestic violence is perpetrated by men against women. Pregnancy provides a unique opportunity for healthcare workers to screen women for domestic violence though a recent review found that the best way in which to do this is unclear. Reducing domestic violence in pregnancy should improve outcomes for mothers and babies though more good quality studies are needed to work out effective ways of screening pregnant women.\n\nDomestic abuse can be triggered by pregnancy for a number of reasons. Pregnancy itself can be used a form of coercion and the phenomenon of preventing an intimate partner's reproductive choice is referred to as reproductive coercion. Studies on birth control sabotage performed by males against female partners have indicated a strong correlation between domestic violence and birth control sabotage. Pregnancy can also lead to a hiatus of domestic violence when the abuser does not want to harm the unborn child. The risk of domestic violence for pregnant women is greatest immediately after childbirth.\n\nDomestic violence can increase a woman's chances of becoming pregnant and the number of children she has, both because the woman may be coerced into sex and because she may be prevented from using birth control. A correlation has been shown between large families and domestic violence. Whereas previously it was thought that having many children and the resultant stress of large families increased likelihood domestic violence, it has been shown that the violence commonly predates the births.\n\nBirth control sabotage, or reproductive coercion, is a form of coercion where someone manipulates another person's use of birth control - weakening efforts to prevent an unwanted pregnancy. Replacing birth control pills with fakes, puncturing condoms, and threats and violence are examples of prevention of an individual's attempt to avoid pregnancy. Pregnancy-promoting behavior of abusive male partners is one method of domestic violence and is associated with unwanted pregnancy, particularly in adolescents. Reproductive coercion itself is a form of domestic violence because it results from unwanted sexual activity and hinders a woman’s ability to control her body. Forced pregnancy can also be a form of financial abuse when a woman becomes trapped in a relationship because the pregnancy has led to economic dependence for new mothers.\n\nUnintended pregnancies are 2 to 3 times more likely to be associated with abuse than intended pregnancies. Research among adolescent populations shows females who experience IPV use condoms at low rates and are fearful of negotiating the use of condoms. In a study of sexually experienced women 15-19 in Uganda, surveys found that fourteen percent of women’s first sexual intercourse had been coerced. Of those fourteen percent, the women were far more likely to be having unprotected sex without the use of modern contraceptives and to have had unintended pregnancies within the last six months compared to women who had not been sexually coerced. In Egypt, over 80% of rural women believe that beatings are sometimes justified and one of the most common reasons given as a just cause for beatings is refusing a man sex. This affects the ability of women to protect themselves from unwanted sexual contact and the consequences of sexual intercourse, such as pregnancy and sexually transmitted infections.\n\nA study conducted by the Center for Impact Research on young mothers classified birth control sabotage into two categories: verbal and behavioral. Verbal sabotage is verbal or emotional pressure not to use birth control, or pressure to become pregnant. Behavioral sabotage is the use of force to prevent the use of birth control, or to have unprotected sexual intercourse.\n\nIn most cases, domestic violence can be prompted by or intensified by pregnancy, but in some cases domestic violence ends during pregnancy because the abuser makes a conscious effort to not harm the fetus.\n\nDomestic violence does not always increase during pregnancy and can even lead to a hiatus in violence. This phenomenon can provide protection for both the woman and child. Because this can lead to decreased violence, some women use pregnancy as a means of protection against domestic abuse. Since abuse generally restarts after the pregnancy ends, women may get pregnant intentionally to prevent violence. However, since women who have been abused before getting pregnant are more likely to experience violence during pregnancy, this is not a reliable means of protection.\n\nAlthough pregnancy can be a protective period for some women, either in terms of a hiatus of pre-existing violence, for others it is a risk period during which abuse may begin or escalate. Women with violent partners have a hard time protecting themselves from unintended pregnancy and sexual violence can directly lead to pregnancy. Studies consistently indicate that domestic violence is more common in large families. However, international studies show that 25% of women are abused for the first time during pregnancy.\n\nIn one study conduct by Campbell \"et al.\", women were asked to speculate on why they thought they were abused during their pregnancies. The answers were categorized into four categories:\n\nThere are many dangerous effects that violence during pregnancy can cause for both the mother and child. A violent pregnancy is considered high risk because verbal, emotional, and physical abuse all lead to adverse health consequences for both the mother and fetus. Violence during pregnancy has been associated with miscarriage, late prenatal care, stillbirth, preterm birth, fetal injury (including bruising, broken and fractured bones, stab wounds and low birth weight. Violence during pregnancy also leads to additional risks for the mother such as increased mental health problems, suicide attempts, worsening of chronic illness, injury, substance abuse, anxiety, stress, chronic pain, and gynecological problems. Women battered during pregnancy were more frequently and severely beaten throughout the course of their relationship compared to women who were not abused during pregnancy. IPV also accounts for a large portion of maternal mortality. Homicide is the second leading cause of injury related deaths in pregnant and post-partum women in the United States and a study conducted in hospital in India found that 16% of all deaths during pregnancy were a result of partner violence. Studies have also found a correlation between domestic violence and increased use of abortion. Pregnant abused women are less likely to report abuse or leave their abuser because of added financial and housing security concerns.\n\nCertain women are more likely to be abused during pregnancy than others. Women who have been abused before getting pregnant are at higher risk of violence during pregnancy. Abuse is not restricted to a specific socio-economic or demographic group of women or to a specific period in a woman’s reproductive life.\n\nIn general, the rate of physical violence during pregnancy decreases as household income increases. Women whose total household income was less than $16,000 were much more likely to experience physical or sexual violence during pregnancy than women with a total household income over $16,000.\n\nPartner violence in a relationship increases the chances of unintended pregnancy. A Canadian study that outlined causes of physical abuse identified “social instability” (e.g. low age, unmarried, lower level of education, and unemployment) as a trigger for violence and used unplanned pregnancies as an example. This suggests that partner violence can lead to increased unintended pregnancies which, in turn, increases physical abuse. Younger women are statistically more susceptible to reproductive coercion and this may be due to less experience in relationships and, for minors, less access to doctor’s appointments and emergency contraception. Adolescents are especially at risk and teenage pregnancy is correlated with increased rates of domestic violence. Young women with older boyfriends are more likely to experience domestic violence. Women who experience physical violence from their husbands are less likely to use contraception and more likely to have an unwanted pregnancy.\n\nA study done on reporting rates of domestic violence concluded that a woman’s risk of physical and sexual violence during pregnancy is under-reported and underestimated. Each year, over 324,000 pregnant women are victims of domestic violence in the United States. A number of countries have sought to statistically estimate the number of adult women who have experienced domestic violence during pregnancy:\n\nIncidence rates are higher for teenagers. The incidence rate for low-income, teen mothers is as high as 38%.\n"}
{"id": "4245684", "url": "https://en.wikipedia.org/wiki?curid=4245684", "title": "Dorothy Tennov", "text": "Dorothy Tennov\n\nDorothy Tennov (August 29, 1928 – February 3, 2007) was an American psychologist who, in her 1979 book \"Love and Limerence – the Experience of Being in Love\" introduced the term \"limerence\". During her years of research into romantic love experiences, she obtained thousands of personal testimonies from questionnaires, interviews, and letters from readers of her writing, in an attempt to support her hypothesis that a distinct and involuntary psychological state occurs identically among otherwise normal persons across cultures, educational level, gender, and other traits. Tennov emphasized that her data consist entirely of verbal reports by volunteers who reported their love experiences.\n\nDorothy Tennov was born in Montgomery County, Alabama. She received her bachelor's degree from Brooklyn College and a Ph.D. from the University of Connecticut. She was a professor of psychology at the University of Bridgeport for twenty years. In addition to being a professor of psychology she was also a student of the philosophy of science.\n\nShe had three sons: Randall Hoffman (d. Nov. 19, 1994), Ace Hoffman and Daniel Hoffman. Since 1986 she lived in Millsboro, Delaware, where she lectured at the local senior learning academy and worked as a volunteer at the nursing home. She was passionate about music, especially classical. She could play the piano, which she did in a local church. She volunteered at the community theatre, Possum Point Players. Tennov died in Harbeson, Delaware at the age of 78 in 2007.\n\nShe was an author of three published nonfiction books, including \"Love and Limerence,\" \"Psychotherapy: The Hazardous Cure,\" and \"Super Self: A woman's guide to self-management.\" Among her other writings were a prize-winning play about life in a nursing home, reviews of books on scientific subjects, presentations at scientific meetings, and essays. Her television credits included a PBS interview with the late French novelist and essayist, Simone de Beauvoir and appearance in a 1998 BBC documentary, \"The Evolution of Desire\". She participated in Internet discussions on scientific and political topics while conducting research for a forthcoming book in which she planned more fully to analyze the methodologies and philosophies of the human sciences.\n"}
{"id": "31821270", "url": "https://en.wikipedia.org/wiki?curid=31821270", "title": "EMBRACE Healthcare Reform Plan", "text": "EMBRACE Healthcare Reform Plan\n\nThe Expanding Medical and Behavioral Resources with Access to Care for Everyone (EMBRACE) plan is a healthcare system reform proposal introduced by a group called Healthcare Professionals for Healthcare Reform (HPfHR). The plan incorporates elements of private health insurance, single-payer and fee-for-service models in one comprehensive system. It has been referred to as a \"Single System\" healthcare system. First published in the Annals of Internal Medicine in April 2009, the plan got some early discussion in the healthcare community, but appeared to have come out too late to have had any impact in the development of the Patient Protection and Affordable Care Act (PPACA), the 111th Congress’ landmark health insurance reform legislation. A book outlining the EMBRACE plan in more detail was authored in 2016 by Dr. Gilead Lancaster, a cofounder of HPfHR. \n\nIn 2007 HPfHR was established in an effort to advise politicians on healthcare issues from the point of view of healthcare professionals. They felt that the only effective way to fix the American healthcare system was with a complete overhaul based on science-based guidelines, also known as evidence-based medicine.\n\nThe group identified five important parts of the American healthcare system that they felt needed to be addressed in their new system. These included inefficiencies in medical offices and hospitals due to a cumbersome insurance and reimbursement system; coverage of the entire United States population for basic healthcare services while preserving the quality and feel of the current delivery system of healthcare; promotion and integration of scientifically validated diagnostic and therapeutic modalities into the system so it becomes the driving force of the healthcare system; and depoliticizing healthcare and allowing for a more manageable way to finance it. In addition, the group felt that it was important that the plan was completely portable throughout the country and did not depend on income, age or employment status.\n\nThe EMBRACE system would require a comprehensive reorganization of the entire United States healthcare system, but would attempt to preserve important elements of the current infrastructure. Current Procedural Terminology (CPT) and International Statistical Classification of Diseases and Related Health Problems (ICD) codes that are currently being used to report services and determine reimbursement to doctors, hospitals and other care providers would be maintained. There would also be an attempt to allow doctors and other healthcare providers to keep private offices and clinics as independent businesses.\n\nThe new system would change 4 fundamental things: It would classify diseases and their therapies into 3 distinct tiers, separate private insurance from public insurance but keep them in the same system, create a politically quasi-independent ‘healthcare board’ funded by Congress to supervise the U. S. healthcare system, and develop a simplified web-based electronic billing and reimbursement system. These fundamental reforms would change many other aspects of the current healthcare system. For example, healthcare coverage would be completely portable from job to job and from state to state and would not be tied to employment.\n\nEMBRACE would establish 3 tiers of diagnoses and treatments founded on evidence-based medicine (EBM), and its funding will be tier-specific and separate:\n\nThe base level (Tier 1) would cover all medical, surgical and psychiatric therapies shown to be life saving, life sustaining and/or preventative and would cover the entire population “from cradle to grave” without registration, deductibles or fee payments. It would also be completely portable and independent of employment status, economic status, race, gender or pre-existing conditions.\n\nFunding of Tier 1 services would be overseen by a healthcare board (see below) that is in turn funded by Congress. The method of raising this revenue could be similar to the present funding of Medicare (e.g. Federal Insurance Contributions Act tax) and Medicaid. Since there will be no requirement for employer-based insurance under EMBRACE, payroll taxes (indexed to salary), a tax on businesses based on the number of employees (and their wages) or a combination of these could also be considered.\n\nTier 2 would cover all conditions affecting quality of life and their therapies. In addition, this tier will include all services of Tier 1 conditions and treatments that do not have sufficient evidence for a Tier 1 indication.\n\nPrivate insurance carriers would be invited to cover Tier 2 services through a menu of plans developed by the Board that is similar to the Medigap Plans A to N now offered through the Centers for Medicare & Medicaid Services. Although each insurance carrier does not have to offer all the plans listed on the menu, the plans that are offered by the insurance carrier must cover all the services stipulated by the Board. This assures that consumers (whether state governments, unions, employers or individuals) can compare the price of the plans and can be confident of the scope of their coverage. In addition, if an insurance provider offers a specific plan in one state, it will be required to offer it in all other states; assuring portability of all tier 2 coverage. Except for these two stipulations, the private insurance provider will be free to set their fee (on an individual basis), set deductibles and co-pays and even deny coverage. \nThe Tier 2 plans can be broad (covering most Tier 2 services) or can be customized for specific groups: a geriatric plan that covers extended care facilities but not fertility care, a heavy laborer plan that includes chiropractic therapy, or a Workman’s Compensation plan purchased by employers, employees or unions.\n\nTier 3 would apply to all medical and surgical issues considered luxury or cosmetic (examples are Lasik surgery or Botox treatments). Funding for Tier 3 would not be covered under this system (as is true in the current system) and all bills would go to the patient. However, billing would still be made through the web-based universal billing form discussed below.\n\nPharmaceuticals will have similar Tier assignments as medical coverage: Tier 1 would be formulations and therapies that have good evidence-based data for treatment or prevention of Tier 1 illnesses and would mostly be paid by public funds or be heavily subsidized. Tier 2 would apply to those drugs and therapies that enhance quality of life or have not yet had adequate evidence for effectiveness for a particular condition. These Tier 2 pharmaceuticals would be covered by private insurance or out of pocket. Tier 3 would be for “luxury” items and would likely be ‘out of pocket’.\n\nThe entire health system would be overseen by a healthcare panel known as “The Board”. Although the details of the exact composition of the Board has not been discussed in detail by HPfHR, it would be composed of physicians and other healthcare professionals, public health experts, economists specializing in health care, business representatives, insurance representatives, representatives from the pharmaceutical industry and representatives of patients. This Board’s mission would be to promote the health of Americans in a socially responsible and economically sound way. Similar to the “Federal Health Board” proposed by Tom Daschle, it would be a quasi-independent organization resembling the Federal Reserve, which it is hoped would make it less beholden to political pressures. It would be headed by a chairperson who would be appointed to a 10-year term by the president and require Senate confirmation. \nThe Board would have oversight of a significantly revised Center for Medicare & Medicaid Services, and input into the Food and Drug Administration and the National Institutes of Health. It would use the already established Diagnosis-related group (DRG), Ambulatory Payment Classification (APC) and International Classification of Diseases (ICD) codes. The Board would decide which diagnoses and services are covered by Tier 1, 2 or 3 based on the medical importance (using evidence-based data such as practice guidelines developed by expert medical panels, Cochrane Library database reviews and other sources), public health considerations and economic impact. This would be updated periodically as more evidence and research becomes available. \nWhen evidence is not available, the Board would have the option to commission the National Institutes of Health and the Food and Drug Administration to direct research focused specifically to use in the Tier assignments. \nAmong the prerequisites to the implementation of this system would be delineation of the specific relationships between the Board and existing agencies within the Department of Health and Human Services, in particular the Food and Drug Administration and the National Institutes of Health. Some reorganization of these government agencies might be warranted to optimize inter-agency interactions. \nTo address local variations in health and social concerns, the health Board would establish several local health-boards (possibly in each state). These local branches would not only handle local health issues, but may be used to establish peer review boards to hear ethical and malpractice issues.\n\nTo simplify claim submissions by healthcare providers (physicians, and hospitals), a “Universal Reimbursement Form” would be created by the Board and would be implemented electronically using a web-based tool available to hospitals and physician offices. This Universal Reimbursement Form (URF) will be the only form of billing for all providers, will be internet-based and will be simple to use. It will transmit data to a “Central Billing System”, which will decide if the condition/service is Tier 1, Tier 2 or Tier 3. Tier 1 services will be reimbursed directly to the provider. Tier 2 services will trigger a search (by the computer) for insurance coverage; if insurance is found the insurance carrier would be billed, if not the patient would be billed. Bills for Tier 3 would be sent directly to the patient. \nTo help in cases where there is some question about which tier a particular service will be charged, there will be a “Billing Inquiry” feature on the Central Billing System available to providers and consumers that allows inquiries of tier assignment in advance. \nAlthough the CBS will be secured with encryption and other anti-hacking devices, the internet platform that the URF is based on will be open-sourced and available for entrepreneurial development. Similar to the open sourced platform of the iPhone, the URF platform would allow for the development of “Health Information Technology” on a single fully interactive web-based platform.\n\nThe budget for the EMBRACE system will be determined by the United States Congress, with one comprehensive bill a year that will fund the entire public healthcare system in the United States. Because the Healthcare Board will have to justify the budget, Congress will continue to have full control on expenditures for the healthcare system.\n\n"}
{"id": "382766", "url": "https://en.wikipedia.org/wiki?curid=382766", "title": "Edwin Chadwick", "text": "Edwin Chadwick\n\nSir Edwin Chadwick KCB (24 January 1800 – 6 July 1890) was an English social reformer who is noted for his leadership in reforming the Poor Laws in England and instituting major reforms in urban sanitation and public health. A disciple of Utilitarian philosopher Jeremy Bentham, he was most active between 1832 and 1854; after that he held minor positions, and his views were largely ignored. Chadwick pioneered the use of scientific surveys to identify all phases of a complex social problem, and pioneered the use of systematic long-term inspection programmes to make sure the reforms operated as planned.\n\nEdwin Chadwick was born on 24 January 1800 at Longsight, Manchester, to James Chadwick. His mother died when he was still a young child, yet to be named. His father, James Chadwick, tutored the scientist John Dalton in music and botany and was considered an advanced liberal politician, thus exposing young Edwin to political and social ideas. His grandfather, Andrew Chadwick, had been a close friend of the Methodist theologian John Wesley.\n\nHe began his education at a small school in Lancashire and moved to a boarding school in Stockport, where he studied until he was 10. When his family moved to London in 1810, Chadwick continued his education with the help of private tutors, his father and a great deal of self-teaching.\n\nHis father remarried in the early 1820s and Edwin's younger half-brother was baseball icon Henry Chadwick, born in 1824.\n\nAt 18, he decided to pursue a career in law and undertook an apprenticeship at an attorney's office. In 1823, he enrolled in law school at The Temple in London. On 26 November 1830 he was called to the bar, becoming a barrister, also known as a court lawyer.\n\nCalled to the bar without independent means, he sought to support himself by literary work such as his work on \"Applied Science and its Place in Democracy\", and his essays in the \"Westminster Review\", mainly on different methods of applying scientific knowledge to the practice of government. He became friends with two of the leading philosophers of the day, John Stuart Mill and Jeremy Bentham. Bentham engaged him as a literary assistant and left him a large legacy. He also became acquaintances with Thomas Southwood Smith, Neil Arnott, and James Kay-Shuttleworth, all doctors.\n\nFrom his exposure to social reform and under the influence of his friends, he began to devote his efforts to sanitary reform. In 1832, Chadwick began on his path to make improvements with sanitary and health conditions.\n\nIn 1832, he was employed by the Royal Commission appointed to inquire into the operation of the Poor Law, and in 1833, he was made a full member of that commission. Chadwick and Nassau William Senior drafted the famous report of 1834, recommending the reform of the old law. Under the 1834 system, individual parishes were formed into Poor Law Unions, and each Poor Law Union was to have a union workhouse. Chadwick favoured a more centralised system of administration than the one adopted, and he felt the Poor Law reform of 1834 should have provided for the management of poor law relief by salaried officers controlled from a central board, with the boards of guardians acting merely as inspectors.\n\nIn 1834, he was appointed secretary to the Poor Law commissioners. Unwilling to administer an act of which he was largely the author in any way other than as he thought best, he found it hard to get along with his superiors. The disagreement, among others, contributed to the dissolution of the Poor Law Commission in 1847. His chief contribution to political controversy was his belief in entrusting certain departments of local affairs to trained and selected experts instead of to representatives, elected on the principle of local self-government.\n\nFollowing a serious outbreak of typhus in 1838, Chadwick convinced the Poor Law Board that an enquiry was required, and this was initially carried out by his doctor friends Arnot and Southwood Smith, assisted by another doctor from Manchester, James Kay Shuttleworth. This was the first time in British history that doctors were employed to look at the conditions which might contribute to ill health in the population. Chadwick sent questionaires to every Poor Law Union, and talked to surveyors, builders, prison governors, police officers and factory inspectors to obtain additional data about the lives of the poor. He edited the information himself, and prepared it for publication. His \"Report on The Sanitary Condition of the Labouring Population of Great Britain\", begun in 1839 and published in 1842, was researched and published at his own expense, and became the best-selling publication produced by the Stationery Office to date. A supplementary report was also published in 1843.\n\nHe employed John Roe, the surveyor for the district of Holborn and Finsbury who had invented the egg-shaped sewer, to conduct experiments on the most efficient ways to construct drains, the results of which were incorporated into the report, and the summary included eight points, including the absolute necessity of better water supplies and of a drainage system to remove waste, as ways to diminish premature mortality. Evidence given by Dr Dyce Guthrie convinced Chadwick that every house should have a permanent water supply, rather than the intermittent supplies from standpipes that were often provided. The report caught the public imagination, and the government had to set up a Health of Towns Commission to consider the issues and recommend legislation. Its chairman was the Duke of Baccleuch, and there were thirteen members, including then engineers Robert Stephenson and William Cubitt. Chadwick acted as secretary in an unofficial capacity, and seems to have dominated the proceedings.\n\nThe Commission took evidence from Robert Thom, who had designed a water supply system for Greenock, Thomas Wicksteed, who was the engineer for the East London Waterworks Company, and Thomas Hawksley from the Trent Waterworks, Nottingham. These confirmed his ideas about constant water supplies, and he developed a model which he called the \"venous and arterial system.\" Each house would have a constant water supply, and water-closets would ensure that soil was discharged into egg-shaped sewers, to be carried away and spread on the land as manure, preventing rivers from becoming polluted. Followed the publication of the Commission's report, the Health of Towns Association was formed and various city-based branches were created. Chadwick later helped to ensure that the Waterworks Clauses Act 1847 became law, to limit the profits that water supply companies could make to 10 percent, and requiring them to comply with reasonable demands for water. This included a constant supply of wholesome water for houses, and a supply for cleansing sewers and watering streets.\n\nChadwick wanted to see his ideas implemented over a wide area, and set about forming a company to supply water to towns, to ensure their drainage and cleansing, and to use the refuse for agricultural production. It was to have the grandiose title \"The British, Colonial and Foreign Drainage, Water Supply and Towns Improvement Company\", with an initial capital of £1 million, but it was the time of the Railway Mania, and he struggled to raise the finance against such competition. The company when it was eventually registered became the more modest Towns Improvement Company. Railways continued to dominate the money market, and the company was wound up after just three years. Chadwick understood that both water supply and drainage were important, since replacing earth-closets with water closets resulted in cesspools overflowing and making sanitary conditions worse, unless there were sewers to carry the waste away. This let to a rift forming between him and Hawksley, who had initially worked closely with him but who later took on water supply projects which did not include any requirement for drainage.\n\nChadwick's report led to the Public Health Act 1848, which was the first instance of the British government taking responsibility for the health of its citizens. It had been introduced by Lord Morpeth in 1847, with the aim or requiring every town to supply water to every house, which they could do by building their own waterworks or by liaising with water companies, and to undertake drainage, sewerage and street paving projects. Most of its grand aims were considerably watered down by the time it became law, but it established a Central Board of Health, whose members were Lord Morpeth, Lord Shaftesbury, and Chadwick. They were later joined by Southwood Smith, who acted as a medical advisor. Districts could pay for a survey from an inspector employed by the Board, and could then proceed without the need to obtain a costly local Act to authorise the work. Chadwick chose all of the inspectors himself, ensuring that they shared his views on glazed sewer pipes, constant water supply and arterial drainage. They worked enthusiastically, ensuring that districts considered comprehensive schemes for water supply, drainage and sewerage.\n\nRatepayers in a district could request an inspector to attend if ten percent of them signed a petition. The Board could also hold an enquiry if the death rate of a district exceeded 23 per 1000, but gauged the level of local support, and formed a local Board if there was support, and withdrew tactfully if there was opposition. By 1853, they had received requests for inspections from 284 towns, and 13 combined water supply, sewerage and drainage schemes had been completed under the legislation. Relationships with engineers were not always easy, with J M Rendel calling Chadwick's ideas \"sanitary humbug\" and Robert Stephenson stating that he hated the ideas of pipes, rather than brick-built sewers. Even Joseph Bazalgette, who went on to design London's trunk sewer network, spoke out against glazed sewer pipes. As time went by, there was growing opposition to what was seen as central control by the Board. The rift between Chadwick and Hawksley had become open hostility, and Hawksley made serious complaints against Chadwick and one of the Board's inspectors to a Select Committee of the House of Commons in 1853. Opposition from engineers increased, and a \"Private Enterprise Society\" was formed by Hawksley and James Simpson, with the intent of bringing down the Board. Shaftsbury, Chadwick and Southwood Smith all had to resign in 1854, and the Board did not last for long afterwards. Chadwick lived to see his position vindicated, when the Local Government Board was established in 1871, which led to the formation of the Ministry of Health. Since then, it has been widely acknowledged that public health is a matter of government responsibility, working through local authorities.\n\nIn 1851, Chadwick made a recommendation that a single authority should take over the nine separate water supply companies that operated in London. Although this idea was rejected by the Government, the Metropolitan Water Supply Act 1852 forced the companies to move the intakes for their water to locations above Teddington weir, to filter water before it was used for domestic supply, to cover filtered water reservoirs, and to ensure that there was a constant water supply to all users by 1857. This was not achieved until 1899, and Chadwick's recommendation for a single authority was eventually implemented in 1902, with the formation of the Metropolitan Water Board.\nIn 1852, Chadwick conversed with Lewis Llewelyn Dillwyn in relation to the construction of a sewerage system in Swansea.\n\nChadwick's efforts were acknowledged by at least one health reformer of the day: William James Erasmus Wilson dedicated his 1854 book \"Healthy Skin\" to Chadwick \"In admiration of his strenuous and indefatigable labors in the cause of Sanitary Reform\".\n\nHe corresponded with Florence Nightingale on methodology. He encouraged her to write up her research into the book \"Notes on Nursing.\" He promoted it among well placed intellectuals, making her more visible.\n\nChadwick was a commissioner of the Metropolitan Commission of Sewers in London from 1848 to 1849. He was also a commissioner of the General Board of Health from its establishment in 1848 to its abolition in 1854, when he retired on a pension. He occupied the remainder of his life in voluntary contributions to sanitary, health and economic questions.\n\nIn January 1884, he was appointed as the first president of the Association of Public Sanitary Inspectors, now the Chartered Institute of Environmental Health. Its head office, in Waterloo, London, is named Chadwick Court, in his honour.\n\nWhile he is well known for his work with the Poor Law and with sanitation, he also contributed to other areas of public policy. These included tropical hygiene, criminal justice institutions, policy regarding funerals and burials in urban areas, school architecture, utilisation of sewage, military sanitation and the education of paupers. He was involved in investigations into child labour in factories, the organisation of the police force, drunkenness, the treatment of labourers on the railways, the building and maintenance of roads, organisation of the civil service, and various aspects of education. Most of these benefitted from his use of statistical methods to collect and organise data, and of the use of anecdotal evidence to support his conclusions. He was involved in a number of organisations, including the British Association for the Advancement of Science, the Institute of Civil Engineers, the National Association for the Promotion of Social Science, the Royal Society of Arts, the Royal Statistical Society, the London Debating Society and the Political Economy Club.\n\nIn recognition of his public service, he was knighted in 1889. He served in his post until his death, at 90, in 1890, at East Sheen, Surrey.\n\nChadwick is remembered at the London School of Hygiene & Tropical Medicine where his name appears among the names of 23 pioneers of public health and tropical medicine chosen to be honoured when the School was built in 1929.\nHe is also commemorated at Heriot Watt University in Edinburgh, Scotland for the engineering building.\n\nAccording to Priti Joshi in 2004 the evaluation of his career has drastically changed since the 1950s:\n\nSuch views are not universally held. Thus Eckland and Price in their assessment of Chadwick's economic policies in 2012 can write:\n\n\n\n\n"}
{"id": "912558", "url": "https://en.wikipedia.org/wiki?curid=912558", "title": "Essiac", "text": "Essiac\n\nEssiac is an herbal tea promoted as an alternative treatment for cancer and other illnesses. There is no evidence it is beneficial to health, and it may be harmful.\n\nThe exact composition of Essiac is unclear, but it reportedly contains burdock (\"Arctium lappa\"), sheep sorrel (\"Rumex acetosella\"), slippery elm bark (\"Ulmus rubra\"), and Indian rhubarb (\"Rheum officinale\") or turkey rhubarb (\"Rheum palmatum\"). Some formulations may also contain watercress, blessed thistle, red clover, and kelp. From the 1920s through the 1970s, Essiac was promoted as a cancer treatment by Rene Caisse, a Canadian nurse, who claimed that it had been given to her by an Ontario Ojibwa patient she treated. However, this has never been substantiated. There is no evidence that Essiac is a Native American or First Nations remedy, and there are multiple factors that indicate the formula is not from any Native American culture. Several of the plants in the mixture (burdock, sheep sorrel, Indian rhubarb and turkey rhubarb) are not indigenous to the Americas, and were not growing in the wilds of Northern Ontario during the time Caisse began prescribing this tea. The name \"Essiac\" is Caisse's name spelled backwards. Today, Essiac is often sold with apparatus (such as bottles and infusers) for making the tea, and is sometimes promoted with untrue claims that scientific studies have shown it to be effective.\n\nIn 1977, Caisse sold the Essiac formula and trademark rights to Respirin Corporation (a Canadian company and predecessor in title to Essiac Products Inc.), which attempted to commercialize the product. However, the company was unable to show any efficacy of Essiac against cancer. Repeated laboratory tests showed that Essiac failed to slow tumor growth and, in large doses, killed test animals. In a number of studies, Essiac actually \"increased\" the rate of cancer growth. As a result, both the U.S. and Canadian governments refused to approve Essiac as a medical treatment. Essiac was instead marketed by Essiac Products Inc. and others as a dietary supplement, subject to much looser regulation and not required to show any proof of effectiveness.\n\nEssiac's purported effect on cancer has been reviewed by several major medical and scientific bodies, including the U.S. Food and Drug Administration (FDA), the National Cancer Institute, and the American Cancer Society. The American Cancer Society states that \"Reviews of medical records of people who have been treated with Essiac do not support claims that this product helps people with cancer live longer or that it relieves their symptoms.\" The NCI states \"Essiac and Flor Essence have not reported clear evidence of an anticancer effect\", and the FDA described Essiac as a \"Fake Cancer 'Cure' Consumers Should Avoid\". Researchers at Memorial Sloan-Kettering Cancer Center have written that Essiac continues to be a popular cancer therapy despite unsubstantiated claims of its effectiveness.\n\nCancer Research UK also notes that there is \"no scientific evidence that Essiac can help to treat cancer or control its symptoms\" and cautions that \"Essiac may interact with some types of cancer treatment so it is very important to tell your doctor if you are thinking of taking Essiac.\"\n\n\n"}
{"id": "19478067", "url": "https://en.wikipedia.org/wiki?curid=19478067", "title": "European Academy of Allergy and Clinical Immunology", "text": "European Academy of Allergy and Clinical Immunology\n\nThe European Academy of Allergy and Clinical Immunology (EAACI) is an association of clinicians, researchers and allied health professionals, dedicated to improving the health of people affected by allergic diseases. \n\nWith nearly 10,000 members from 121 countries and over 50 National Allergy Societies, EAACI is the primary source of expertise in Europe for all aspects of allergy.\n\nAllergic and immunologic diseases (such as asthma, rhinitis, eczema and occupational allergy, food and drug allergy, severe anaphylactic reactions, autoimmune disorders, and immunodeficiencies) represent current crippling or life threatening conditions, and are a cause of worldwide concern. EAACI’s mission is to provide the most efficient platform for scientific communication and education in the field of allergy and immunology, ultimately striving to ease the lives of patients suffering from these diseases.\n\nIn pursuit of our mission, we seek to accomplish the following aims:\n\n\n\n\n\n\nEAACI is a non-profit organisation. Income is derived from fees; congress and meetings profits are devoted to current activities, research and travel, awards, and initiatives of interest to members.\n\nEAACI was founded in 1956 in Florence, its Constitution and By-Laws were registered in Utrecht in 1957. Since then, it has grown to become the largest medical association in Europe in the field of allergy and clinical immunology. \n\nThis is the flagship meeting of the academy and the world’s largest congress specializing in the field of allergy and clinical immunology*. A wide range of sessions, including interactive formats, provide optimal learning for all tastes. Attracting in the region of 7000-8000 participants and experts from across the globe, the congress is a key annual event to interact with peers, leading scientists and practitioners over five days.\n\nThese mid-size events address particular key areas in the field of allergy, and provide knowledge sharing and training using various session formats, attracting delegates and speakers from around the world. Due to the attendee size of between 250 -1500 participants, depending on the topic area, these meetings also provide an excellent platform for networking with peers over a three day period.\n\nThese training courses focus on a specific topic within allergy and are built to offer a solid foundation in this area. They are therefore attractive to experienced individuals who are new to the particular field, for fellows in training and EAACI Junior Members, as well as professionals with a particular interest in the chosen topic. Limited group sizes of around 120 international participants enable great dialogue throughout the typically three day courses.\n\nEAACI makes available to all members multiple resources namely: journals (Allergy, PAI and CTA), books, bibliographic updates, position papers, newsletter, guidelines and an open access EAACI media library.\n"}
{"id": "46454308", "url": "https://en.wikipedia.org/wiki?curid=46454308", "title": "Feeling Nuts Movement", "text": "Feeling Nuts Movement\n\nThe Feeling Nuts movement is a social media campaign created by Check One Two in London, to encourage young men to regularly check their testicles for early signs of testicular cancer. The campaign went viral when the public and celebrities began sharing pictures and videos of crotch grabs using the hashtag #feelingnuts and became an annual TV event on Channel 4. The campaign involves both social media and television.\n\nFeeling Nuts was created by brothers and founders of Salter Brothers Entertainment, Simon and Andrew Salter after being inspired by Wendy Gough, a mother who lost her son to testicular cancer in 1998 simply because he was too embarrassed to discuss it. Gough has conducted awareness talks in schools ever since her son's death which left a lasting impression on the Salter brothers. They realised that raising money wasn’t necessary to stop men dying of testicular cancer and therefore created a new currency through social awareness. In an interview with \"The Times\", the brothers stated that \"young guys don’t want to listen to charities lecturing them so we took a different approach\" creating the social movement organisation Check One Two and devising its first campaign 'Feeling Nuts'.\n\nIn an interview with \"The Huffington Post\" Andrew explained: \"Our mission is to empower people to spread awareness of this preventable cancer and change behavior to check themselves regularly. We firmly believe that this social movement, designed to save lives, will get the nation talking about this taboo subject and by using entertainment around the #feelingnuts campaign, ultimately, save lives.\"\n\nOn 10 April 2014, Check One Two launched the hashtag #FeelingNuts to promote the Testicular Cancer Awareness Month and encouraged men to regularly check themselves for testicular cancer.\n\nOn 12 June 2014, the Check One Two YouTube channel uploaded a video titled \"Are You #FeelingNuts\", challenging viewers to share pictures of themselves grabbing their crotch with the hashtag #FeelingNuts to raise awareness about testicular cancer.\n\nThe tipping point for Feeling Nuts arrived in August 2014 when the brothers dropped their trousers in Oxford Street in London and took a selfie grabbing their crotches, receiving 4,864 retweets and 13,507 favourites on Twitter. Using #feelingnuts, they started a monthly call to action for men to check themselves regularly and then issued an online challenge to boyband 5 Seconds Of Summer and Richard Branson's son Sam Branson, who took it up. Soon more celebrities started to challenge each other including Hugh Jackman, William Shatner, Ricky Gervais, chef Jamie Oliver and super model Cara Delevingne.\n\nIn October 2014 the campaign went viral and social media monitoring company Brandwatch estimated that within 3 months, the campaign had reached at least 750 million people globally in addition to thousands of media stories.\n\nOn 17 September 2014 it was announced that the brothers had joined forces with Channel 4 to launch an annual 90-minute comedy TV event called \"The Feeling Nuts Comedy Night\" designed to raise awareness of testicular cancer. The inaugural show premiered on 24 October 2014 and was hosted by comedian Jack Whitehall as well as featuring comedy sketches from Cara Delevingne, Jimmy Carr, One Direction and musical performances from James Corden and Rizzle Kicks. The show also reunited Martin Clunes and Neil Morrissey for a new Men Behaving Badly sketch for the first time in over 15 years. The Feeling Nuts Comedy Night inaugural event was a success, easily beating and over doubling the channel’s slot average.\n"}
{"id": "20726599", "url": "https://en.wikipedia.org/wiki?curid=20726599", "title": "Food and Fuel Control Act", "text": "Food and Fuel Control Act\n\nThe Food and Fuel Control Act, , also called the Lever Act or the Lever Food Act was a World War I era US law that among other things created the United States Food Administration and the Federal Fuel Administration.\n\nThe act was a very controversial piece of legislation. The act was sponsored by Rep. Asbury F. Lever, a Democrat from South Carolina. President Wilson urged its passage as a wartime emergency measure. Some opposed the authority that would rest in the person of the \"Food Administrator.\" Others opposed language that empowered the president to limit or prohibit the use of agricultural products in the production of alcoholic beverages, thereby establishing a form of national prohibition. Senators proposed alternatives, including a prohibition on the production of whiskey alone for the duration of the war. Republican Senator Henry Cabot Lodge objected to the language that authorized the president to \"use any agency or agencies, to accept the services of any person without compensation, to cooperate with any person or persons in relation to the processes, methods, activities of and for the production manufacture, procurement, storage, distribution, sale, marketing, pledging, financing, and consumption of necessaries which are declared to be affected with a public interest.\" Wilson also had to fight off the proposal of Massachusetts Republican Senator John W. Weeks to establish instead a Joint Committee on the Conduct of the War.\n\nIts official name was \"An Act to Provide Further for the National Security and Defense by Encouraging the Production, Conserving the Supply, and Controlling the Distribution of Food Products and Fuel\" and became law on August 10, 1917. It banned the production of \"distilled spirits\" from any produce that was used for food.\n\nIn 1918, faced with complaints from farmers that the Food Administration created under the Act had set the minimum price of wheat too low, Congress passed an amendment increasing that level from $2.20 to $2.40 per bushel. The President's veto out of concerns about inflation and the impact on the British, is credited with producing disastrous results for Democrats in the 1918 elections in the states of the grain belt.\n\nOn August 18, 1919, after the end of hostilities, President Wilson asked Congress to extend the life of the Act to allow his administration to address widespread and dramatic increases in the prices of commodities. He requested amendments to include clothing and to set increased penalties for profiteering. Opponents delayed passage for months while berating the administration for its failure to control prices and then granted the authority the President requested in October. In the House of Representatives, the President's chief critic complained of the administration's priorities: \"Where there is one man in a thousand who cares a rap about the League of Nations, there are nine hundred and ninety-nine who are vitally and distressingly concerned about the high cost of living.\" The Department of Justice launched 179 prosecutions under the amended Act in the first two months following its passage.\n\nThe Act, an emergency wartime measure, was designed to expire at the end of World War I or shortly thereafter. It created two agencies, the Food Administration and the Fuel Administration.\n\nTo head the Food Administration, the President named Herbert Hoover who had handled Belgian relief at the beginning of the war and had coordinating food and fuel supplies since May 1917 on Wilson's personal authority. As United States Food Administrator he had the authority to fix food prices, license distributors, coordinate purchases, oversee exports, act against hoarding and profiteering, and encourage farmers to grow more crops. He emphasized the needs of America's allies, both those under arms and the civilian populations, for American produce. He encouraged American households to consume less meat and bread.\n\nWilson issued a proclamation in January 1918 calling upon Americans to demonstrate their patriotism by following Hoover's guidelines. There were voluntary \"meatless Tuesdays\" and \"sweetless Saturdays.\" Tuesdays and Saturdays were \"porkless.\" Both Mondays and Wednesday would be \"wheatless.\" Compliance was voluntary, though the baking industry, including hotels and restaurants, was limited to the production of war bread and rolls called \"victory bread.\" Initially it was made from at least 5% of grains other than wheat and that amount increased to 20% by February 24. His agency asked households to pledge their support and some 13 million of 18 million did so. Hoover's call for the conservation of the nation's produce emphasized voluntary compliance:\n\nChildren were organized into the \"United States School Garden Army.\" When eating apples, Boy Scouts were urged to be \"patriotic to the core.\" Citizens were encouraged to grow \"victory gardens\" of vegetables in their backyards and vacant lots. Slogans like \"By all means, save the beans\" became popular. The Food Administration also fixed the price of a bushel of wheat, the price floor being $2 a bushel and the ceiling at $2.20. One of its posters said: \"FOOD WILL WIN THE WAR; DON’T WASTE IT.\" By the end of 1918, about one-fourth of all American production was diverted to the war effort.\nThe Fuel Administration under Harry Garfield, the President of Williams College, directed efforts to save coal. Nonessential factories were closed, and the Federal government had complete control over all aspects of the coal industry including production, pricing, sale, shipment, and distribution. Although the Act also included oil and natural gas, it gave the government less authority over those energy sources and no ability to control the price of oil and gas. Copying the methods of the Food Administration, citizens were encouraged to save fuel with \"gasless Sundays,\" \"heatless Mondays,\" and \"lightless nights.\"\n\nGarfield's most dramatic action was an attempt to speed fuel to eastern ports where ships were idled for lack of fuel. On January 17, 1918, he order the closing of all factories east of the Mississippi. That accomplished his goal, but exposed the Wilson administration to criticism both from its usual opponents and members of the President's own party.\n\nOn February 4, 1918, Garfield announced rules to govern the distribution of fuel oil that defined priority classes starting with railroads, then exports to the American armed forces, exports to America's allies in the war, hospitals, and several other classes.\n\nIn November 1919, Attorney General A. Mitchell Palmer sought and won an injunction against a strike in the coal industry under the Act. He claimed the President authorized the action, following a meeting with the severely ill President Wilson in the presence of his doctor. Samuel Gompers, President of the American Federation of Labor, protested that President and members of his Cabinet had provided assurances when the Act was passed that it would not be used to prevent strikes by labor unions. He provided detailed accounts of his negotiations with representatives of the administration, especially Secretary of Labor William B. Wilson. He also argued that the end of hostilities, even in the absence of a signed treaty, should have invalidated any attempts to enforce the Act's provisions.\n\nAt one point Palmer asserted that the entire Cabinet had backed his request for an injunction. That infuriated Secretary of Labor Wilson who had opposed Palmer's plan and supported Gompers' view of the President's promises when the Act was under consideration. The rift between the Attorney General and the Secretary of Labor was never healed, which had consequences the next year when Palmer's attempts to deport radicals were frustrated by the Department of Labor.\n\nPalmer used the Act again in April 1919 against 38 of the leaders of a walkout by railroad workers.\n\nThe amended Act's attempt to limit profits was found unconstitutional in February 1920 by a federal court that found its language \"vague, indefinite, and uncertain.\" The Supreme Court struck down the provisions of the Act that allowed the Food Administrator to set maximum prices and fine those who violated the levels he set in 1921. That same year, the Supreme Court upheld the Act's imposition of rent control in the District of Columbia, which had not been repealed along with the bulk of the Act.\n\nThe work of the Fuel Administration ended in May 1919. The activities of the Food Administration declined quickly after the Armistice of 11 November 1918 and all but disappeared by July 1920.\n\nThe Act of August 10, 1917, as amended, was repealed along with a number of other authorized-for-wartime measures in a joint resolution of Congress on March 3, 1921 by effectively declaring the wartime emergency still in effect at the time as formally over.\n\nCourt cases brought under the Act, both before and after its repeal, continued to work their way through the courts.\n\nWilliam C. Mullendore, \"History of the United States Food Administration\" (Stanford, 1941)\n\n"}
{"id": "7096518", "url": "https://en.wikipedia.org/wiki?curid=7096518", "title": "Geatnjajávri", "text": "Geatnjajávri\n\nGeatnjajávri is a lake in Berlevåg Municipality in Finnmark county, Norway. The lake is located just north of the border with Tana Municipality, along the road between the villages of Berlevåg and Båtsfjord. The Norwegian County Road 890 crosses over the lake on a causeway and bridge on the western side of the lake. The lake has a dam on the northern end and the water eventually flows out into the river Kongsfjordelva.\n\n"}
{"id": "15962284", "url": "https://en.wikipedia.org/wiki?curid=15962284", "title": "Global Alliance for Improved Nutrition", "text": "Global Alliance for Improved Nutrition\n\nThe Global Alliance for Improved Nutrition (GAIN) is an independent non-profit foundation based in Geneva, Switzerland. GAIN was developed at the UN 2002 Special Session of the General Assembly on Children.\n\nGAIN is an organization driven by the vision of a world without malnutrition. To achieve its goal, GAIN mobilises public-private partnerships and provides financial and technical support to deliver nutritious foods to those people most at risk of malnutrition.\n\nGAIN works with diverse partners – governments, UN agencies, non-governmental organizations, and businesses worldwide. As of 2015, GAIN reached an estimated 900 million people including 330 million women, adolescent girls and children 6–59 months:\n51% of these were in Africa, 44% in Asia and 5% in the rest of the world.\n\nGAIN's collective impact approach in the nutrition sector has been recognised by the Stanford Social Innovation Review as a model of collaboration that achieves large scale progress in the face of the urgent and complex problems of our time. The \"Harvard Business Review\" has also recognized GAIN's innovation in pushing businesses to develop nutritious food products for the base of the pyramid.\n\nGAIN supports market-based nutrition solutions in nutrition interventions areas including: large scale food fortification; maternal, infant and young child nutrition; and agriculture and nutrition.\n\n\nGAIN's program portfolio is based on proven and cost-effective approaches. Programs support large-scale food fortification, multi-nutrient supplements, nutritious foods for mothers and children, and enhancement of the nutritional content of agriculture products.\n\nGAIN's Large Scale Food Fortification Program aims to increase sustainable consumption of staple foods and condiments fortified with essential vitamins and minerals among populations at large in target countries. Key goals include increasing coverage of key micronutrients (vitamin A, iodine, iron, zinc, folic acid) to more than 500 million women and children and reduction of key deficiencies by 20–30 percent. Projects fortify staple foods and condiments including vegetable oil, maize meal, rice, wheat flour, vegetable oil, salt, sugar, soy sauce and fish sauce – relying on business to fortify products and governments to establish appropriate legislation and regulation. The program supports innovative ways of reaching the hard to reach through market-based channels and public distribution.\n\nGAIN's efforts in this area support in-country production and distribution of high-quality, affordable, low-cost fortified foods for young children which complement breast milk, and development and improved access to nutritious foods for pregnant and breastfeeding women. The focus is placed in particular on the development of adequate delivery models to ensure access to, and demand for and use of, products by target populations.\n\nSupporting the production, processing and use of more nutritious agricultural crops and enhancing the quality of staple foods (through improved seeds, fertilizers and milling and storage practices) are two strategies for bringing better nutrition to vulnerable populations in a sustainable way. Taken together these efforts can also increase effective yields for poor farmers and improve their health and livelihoods.\n\nEnsuring that the food system works to improve nutritional value means more than simply increasing production and making sure there is enough food to feed the world's population. Nutritional quality needs to become a measurable objective in agricultural programs, and embedded within the pillars of food security. Strategies for integrating health, nutrition and agriculture are reflected in the Scaling Up Nutrition movement, and are gaining momentum both nationally and in global thinking.\n\nA critical component of GAIN's efforts has been positioning nutrition as central to the global health and development platform, as poor nutrition is clearly an impediment for achieving the Sustainable Development Goals. GAIN supports enhanced advocacy efforts at the national, regional and international levels, to enhance the policy environment to reach scale, raise the profile of nutrition and ultimately increase human and financial resources to address malnutrition.\n\nIn 2011, as part of its expanded advocacy program, GAIN launched Future Fortified, a global campaign to improve the nutrition of mothers and children through support for GAIN programs and related global nutrition advocacy efforts.\n\nGAIN is also a strong supporter of the Scaling Up Nutrition movement (SUN) and, together with the World Food Programme, is a co-convener of the SUN Business Network. GAIN joined with partners to establish the 1,000 Days Partnership and supports the implementation of its programs.\n\nMarket-based approaches to nutrition are an essential part of the way GAIN does business. Over 90 percent of food and beverages in the developed world, and increasingly in emerging markets too, are produced or delivered by the private sector. GAIN engages with the private sector to improve efficiency and effectiveness of markets.\n\nThe SUN Business Network (SBN) co-convened by GAIN and the UN World Food Programme – is one of the four global networks that support SUN countries – along with UN, Civil Society and Donor Networks. The SBN was established to bring business together behind the SUN Movement and its aim to ensure that all people realise their right to good food and nutrition. In 2015,the SBN surpassed its target of 99 companies,with 160 now making public commitments to improving nutrition,and tracking their progress annually.These commitments range from increasing reach in\nbroadcasting mobile phone nutrition messages,all the way to providing 60 million people each year with fortified staple foods.In total,commitments from member companies amount to reaching 125 million consumers every year by 2020.The SBN is now supporting ten SUN countries to develop national platforms and strategies to engage business in country-led national nutrition strategies.\n\nThe Amsterdam Initiative against Malnutrition (AIM) is a coalition of the Dutch Ministry of Foreign Affairs, Unilever, DSM, AkzoNobel, Wageningen University, ICCO and GAIN that aims to work with others to end malnutrition in Africa by 2015 through initially targeting six countries: Kenya, Tanzania, South Africa, Ethiopia, Ghana and Mozambique. In Kenya, AIM has initiated a milk fortification project and a market study to get insight into consumer attitudes towards fortified milk. AIM partners have also identified distribution channels for nutritious foods that reach base of the pyramid populations, including safe water kiosks, milk bars, and school feeding programs. The initiative also began supporting the Kenya Nutritionists and Dieticians Institute, to create demand for nutrition through strengthening capacity development, policy and advocacy, and public engagement. Going forward, AIM will increase focus on market insight and BOP consumer aspirations to ensure nutritious products are not only accessible and affordable, but also in high demand.\n\nThe GAIN Nordic Partnership is a multi-sector platform with an ambition to facilitate scalable and inclusive business models that enhance the nutritional value of food in developing countries. It was established in 2014 by the five founding partners: Arla Foods Ingredients, Tetra Pak, DanChurchAid, the Confederation of Danish Industry and GAIN. The platform brings together Nordic companies, civil society, academia and the public sector in a forum for collaboration, action and knowledge sharing. Together, we co-create solutions that address undernutrition but at the same time are affordable, tasty and attractive for low income consumers living on a budget of a few dollars a day. The first focus area of the GAIN Nordic Partnership is the development of sustainable initiatives along the dairy value chain in Ethiopia and East Africa. The goal is to reach low-income consumers with an income of US$2 to US$5 a day.\n\nThe Access to Nutrition Index, which started in 2009, tracks how well the food and beverage industry provides nutritious products to consumers. The methodology was developed by GAIN, and is jointly funded by GAIN, the Bill & Melinda Gates Foundation, and the Wellcome Trust. The index aims to increase consumers' access to more nutritious products and ultimately contribute to addressing the serious global problems of both undernutrition and obesity. It will allow food and beverage companies to benchmark their performance on nutrition against their peers, and it will serve as a platform that provides stakeholders – from investors to consumers and policymakers – with information that they can use to inform their decisions and their programs. The index also seeks to promote nutrition as an investible theme within the investment community.\n\nGAIN currently employs 120+ professionals in Africa, Asia, Europe and North America, who work on various projects in more than 30 countries. Headquartered in Geneva, Switzerland. GAIN has country offices in Abuja (Nigeria), Addis Ababa (Ethiopia), Copenhagen (Denmark), Dar Es Salaam (Tanzania) Dhaka (Bangladesh), Islamabad (Pakistan), Jakarta (Indonesia), Kabul (Afghanistan), Maputo\n(Mozambique), Nairobi (Kenya), New Delhi (India).\n\nGAIN has representative offices in London (United Kingdom), Ottawa (Canada), Utrecht (The Netherlands) and Washington D.C. (USA).\n\nGAIN receives funding from a number of organizations including: the Bill & Melinda Gates Foundation (BMGF), The Children's Investment Fund Foundation (CIFF), the Department for International Development (DFID), Agence Française du Développement (AFD), GiveWell, Good Ventures, Goldsmith Foundation, Department of Foreign Affairs, Trade and Development, Canada (DFATD), Ministry of Foreign Affairs of the Government of the Netherlands, the GSM Association (GSMA), the Inter-American Development Bank (IDB), the Irish Aid, the Karl Pedersen og Hustrus Industrifond, the Liverpool School of Tropical Medicine, the Micronutrient Initiative (MI), the United Nations Children's Fund (UNICEF), the United States Agency for International Development (USAID) and the United Nations World Food Programme (WFP).\n\nGAIN's total expenditure during FY2014–2015 (prior to grant accounting adjustments) was US$53,134,922.\n\nGAIN's Board of Directors is GAIN's decision making body, which provides overall strategy and direction. The Chair of the GAIN Board is Vinita Bali. The Vice Chair of the Board of Directors is Joachim von Braun, Director of the Center for Development Research (ZEF) and Professor for Economic and Technological Change at the University of Bonn, Germany. \nOther Board members include:\n\n\nEx officio:\n\n\n\n\n"}
{"id": "36447827", "url": "https://en.wikipedia.org/wiki?curid=36447827", "title": "Healthcare in Tripura", "text": "Healthcare in Tripura\n\nHealthcare in Tripura features a universal health care system run by the state government. The Constitution of India charges every state with \"raising of the level of nutrition and the standard of living of its people and the improvement of public health as among its primary duties\". Ministry of Health & Family Welfare of the Government of Tripura is responsible for healthcare administration in the state.\n\nThe health care infrastructure is divided into three tiers — the primary health care network, a secondary care system comprising district and sub-divisional hospitals and tertiary hospitals providing specialty and super specialty care.\n\nFollowing table illustrates some health care indicators of the state, compared to the national indicator, as of 2010. These data are based on Sample Registration System of Office of the Registrar General and Census Commissioner, India.\n\nAs of 2010–11, there are 17 hospitals, 11 rural hospitals and community health centres, 79 primary health centres, 635 sub-centres/dispensaries, 7 blood banks and 7 blood storage centres in the state. Homeopathic and Ayurvedic styles of medicine are also popular in the state. The number of beds in the four district of the state available for patients are as follows:\n\nAlthough the state government is trying to promote family welfare and birth control, the target achievement in birth control measures has remained limited; none of the birth control methods achieved 50% of the target in 2010–11. Immunisation programs have been more successful, most immunisation programs achieved nearly 70% of the target.\n\nNational Family Health Survey-3 revealed that 20% of the residents of Tripura do not generally use government health facilities, and prefers private medical sector. This is overwhelmingly less compared to the national level, where 65.6% do not rely on government facilities. As with the average of India, Tripura residents also cite poor quality of care as the most frequent reason for non-reliance over public health sector. Other reasons include distance of the public sector facility, long waiting time, and inconvenient hours of operation.\n"}
{"id": "29950780", "url": "https://en.wikipedia.org/wiki?curid=29950780", "title": "History of medical regulation in the United Kingdom", "text": "History of medical regulation in the United Kingdom\n\nThe aim of medical regulation is to ensure that medicine is only practised by qualified and suitable people. The history of regulating doctors in the UK dates back around 600 years. The earliest licensing procedures were administered by the Church, with professional associations and universities also playing a role. Modern regulation of doctors is carried out by the General Medical Council.\n\nThe earliest reference to medical regulation in the UK dates from 1421, when physicians petitioned parliament to ask that nobody without appropriate qualifications be allowed to practise medicine. The doctors said that unqualified practitioners caused \"great harm and slaughter of many men\".\n\nDespite agreement in principle from parliament, little more appeared to happen until 1511, when a statute placed regulation of the medical profession in the hands of the bishops. John Raach wrote that \"the Church was apparently considered the one institution whose influence was extensive and potent enough to be effective in suppressing quacks and licensing the members of the medical profession\". Raach further suggested that as a learned profession, medicine \"could not be relegated to regulation by the average county official\". Clerics, often the most highly educated members of society, were better suited to the task. Medicine and religion were also closely entwined: healing had long been associated with the supernatural, while the events of birth and death involved both medics and clerics.\n\nThe purpose of the 1511 statute was to eliminate unqualified practitioners, and to that end it provided for a financial reward for those who reported them.\n\nIn 1518, the College of Physicians was founded and took over licensing of doctors in London. The College was founded by physicians themselves, meaning that in London the licensing of medicine was in the hands of the profession, rather than the bishop. Various disputes arose between the College, universities, and bishops over their authority to license and recognise each other's qualifications.\n\nAs doctors often covered large areas, crossing diocesan boundaries, they often required licenses from several bishops. At some point – it is unclear precisely when – archbishops were empowered to issue licenses for multiple dioceses. In the early seventeenth century, nearly a quarter of doctors received their licenses from archbishops.\n\nApothecaries Act 1815\n\nThe Apothecaries Act introduced compulsory apprenticeship and formal qualifications for apothecaries, in modern terms general practitioners, under the license of the Society of Apothecaries.\n\nThe Medical Act 1858 marks the start of the modern period of medical regulation in the UK. The purpose of the Act was to create the body now known as the General Medical Council – then known as The General Council of Medical Education and Registration of the United Kingdom. Explaining its purpose, the Act says \"it is expedient that Persons requiring Medical Aid should be enabled to distinguish qualified from unqualified Practitioners\".\n\nThe Act created the position of Registrar of the General Medical Council – an office still in existence today – whose duty is to keep up-to-date records of those registered to practise medicine and to make them publicly available.\n\nThe 1950 Medical Act introduced disciplinary boards and a right of appeal to the General Medical Council. It formally renamed the Council to the name that had informally been used for some time: the General Medical Council. It also introduced a compulsory year of training for doctors after their university qualification, a training position which has developed into the current Foundation House Officer role.\n\nSumming up the Act, the \"British Medical Journal\" wrote, \"In future, the GMC will be possessed of wider powers, improved machinery, and a better status, all serving to ensure the continued and enhanced confidence of the profession and the public alike.\"\n\nThe Medical Act 1983 provides the current statutory basis for the General Medical Council's functions. The Council is also bound by laws that implement a European directive on mutual recognition of professional qualifications from European Economic Area countries.\n\nWarren, Michael D. (2000) A Chronology of State Medicine, Public Health, Welfare and Related Services in Britain, 1066-1999. Faculty of Public Health Medicine of the Royal Colleges of Physicians of the United Kingdom.\n"}
{"id": "25690682", "url": "https://en.wikipedia.org/wiki?curid=25690682", "title": "Humam Khalil Abu-Mulal al-Balawi", "text": "Humam Khalil Abu-Mulal al-Balawi\n\nHumam Khalil Abu-Mulal al-Balawi (25 December 1977 – 30 December 2009) was a Jordanian doctor and a triple agent suicide bomber loyal to Islamist extremists who carried out the Camp Chapman attack, a suicide attack against a CIA base near Khost, Afghanistan on 30 December 2009.\n\nAn Afghan security official gave al-Balawi's name as Hamman Khalil Abu Mallal al-Balawi. The Arab newspaper \"The National\" referred to him as Homam Khaleel Mohammad Abu Mallal. He also used the alias Abu Dujana al-Khurasani or Dujjanah al Kharassani when writing for jihadi websites. Hajj Yacoub, a self-proclaimed spokesman for the Pakistani Taliban, identified him as Hamman Khalil Mohammed.\n\nAl-Balawi was born in Kuwait on 25 December 1977. He grew up in a middle-class family of nine other children, including an identical twin brother, and lived in Kuwait until Iraq's 1990 invasion of the country, when the family moved to Jordan. He graduated with honors from an Amman high school.\n\nAl-Balawi studied medicine for six years in Turkey at Istanbul University and graduated in 2002. He also received medical training at the University of Jordan Hospital and at the Islamic hospital run by Jordan’s Islamic Brotherhood in Amman. He was married to Dafinah Bairak (Defne Bayrak), a Turkish journalist and translator, with whom he had two children. They lived in the lower-income Amman suburb of Jabal Nuzhah.\n\nAl-Balawi had a history of supporting violent Islamist causes. He was tagged by the National Intelligence Organization of Turkey as having a relation with the Great Eastern Islamic Raiders' Front. It is not clear whether this information was shared with other intelligence organizations. According to the SITE Intelligence Group, which monitors extremist websites, he was a well-known contributor to al-Hesbah, an online forum run by Islamist extremists. He also ran his own Islamist blog.\n\nAl-Balawi was arrested by the Jordanian security service in late 2007 and was believed to have been transformed into a double agent loyal to the U.S. and to Jordan. According to Western government officials, al-Balawi had been recruited by Jordan's General Intelligence Directorate and taken to Afghanistan. The Jordanian intelligence service is one of the CIA's closest allies in the Middle East.\n\nAccording to intelligence officials, al-Balawi had been invited to FOB Chapman after claiming to have information related to senior al-Qaeda leader Ayman al-Zawahiri. He was not closely searched because of his perceived value as someone who could infiltrate the ranks of senior al-Qaeda leaders. The CIA had come to trust the informant, and the Jordanian spy agency vouched for him, according to officials.\n\nAccording to a Jordanian report, al-Balawi was an \"informant, who offered dangerous and important information which the authorities said they had to take seriously\", but not recruited by the CIA or Jordanian intelligence. He was \"only a trusted source who went onto the base without inspection\" the official said.\n\nAl-Balawi appeared in a video released after his death and was shown saying that the attack was carried out in revenge for the 2009 killing of the Pakistani Taliban leader Baitullah Mehsud.\n\nIn his last statement issued by the Al Qaeda's media wing As-Sahab, he revealed that Jordanian intelligence was co-operating with the CIA to kill or capture senior Al-Qaeda and other militant group leaders. He further claimed that the Jordanian Intelligence Directorate assisted the CIA in killing Imad Mughniyah, a senior Hizbullah militant killed in Lebanon, and Abdullah Azzam, senior Afghan jihad leader, as well as assisting them to eliminate Abu Musab Zarqawi, who was the head of al-Qaeda in Iraq.\n\nAfter Al-Balawi's death, his wife Defne Bayrak was interrogated for almost five hours by Istanbul Security Directorate (Turkish police). According to the leaked information, the first question asked during the interrogation was how they had met each other to which she replied that they met in a chat room on a website that she accessed to learn Arabic. It is also said that CIA officials brought a file, containing information on al-Balawi and questions to ask during interrogation, and gave it to Istanbul Anti-Terror Branch Directorate. However, Istanbul Security Directorate denied any CIA involvement. Later, she gave extensive interviews to \"Newsweek Turkey\" and CNN. She also complained to The Association of Human Rights and Solidarity of Oppressed People in Turkey about being constantly bothered by reporters.\n\nHer main point during interviews was that al-Balawi never really worked for CIA or Jordanian intelligence, wasn't their agent, and only used them by pretending to work for them. In the interviews al-Balawi comes out as someone who was obsessed about Jihad and felt guilty for not doing anything despite constantly writing on the subject. He was very affected by the occupation of \"Islamic lands\" by United States. She said that he had wanted to go to the conflict areas before but was unable because Jordanian intelligence strictly controls access of suspicious people to these areas. According to her he wasn't tortured during the 3 day arrest and was given a Quran to read but was prohibited from reading it out loud. She says it is probable that it was during interrogation that he convinced the intelligence agencies and gained easy exit to Pakistan. She denies knowing anything about his connection to intelligence services but admits concealing from his parents that he was in Pakistan and not in Turkey. She also expressed great pride in her husband's suicide attack.\n\n\n"}
{"id": "21496038", "url": "https://en.wikipedia.org/wiki?curid=21496038", "title": "Lactation", "text": "Lactation\n\nLactation describes the secretion of milk from the mammary glands and the period of time that a mother lactates to feed her young. The process can occur with all post-pregnancy female mammals, although it predates mammals. In humans the process of feeding milk is also called \"breastfeeding\" or \"nursing\". Newborn infants often produce some milk from their own breast tissue, known colloquially as witch's milk.\n\nIn most species, milk comes out of the mother's nipples; however, the monotremes, egg-laying mammals, lack nipples and release milk through ducts in the abdomen. In only one species of mammal, the Dayak fruit bat, is milk production a normal male function.\n\n\"Galactopoiesis\" is the maintenance of milk production. This stage requires prolactin. Oxytocin is critical for the \"milk let-down reflex\" in response to suckling. Galactorrhea is milk production unrelated to nursing. It can occur in males and females of many mammal species as result of hormonal imbalances such as hyperprolactinaemia.\n\nThe chief function of a lactation is to provide nutrition and immune protection to the young after birth. In almost all mammals, lactation induces a period of infertility (in humans, lactational amenorrhea), which serves to provide the optimal birth spacing for survival of the offspring.\n\nFrom the eighteenth week of pregnancy (the second and third trimesters), a woman's body produces hormones that stimulate the growth of the milk duct system in the breasts:\n\n\nIt is also possible to induce lactation without pregnancy. Protocols for inducing lactation are called the Goldfarb protocols. Using birth control pills to mimic the hormone levels of pregnancy, then discontinuing the birth control, followed by use of a double electric breast pump for 15 minute sessions at regular 2-3 hour intervals (100+ minutes total per day)_ helps induce milk production.\n\nDuring the latter part of pregnancy, the woman's breasts enter into the \"Secretory Differentiation\" stage. This is when the breasts make colostrum (see below), a thick, sometimes yellowish fluid. At this stage, high levels of progesterone inhibit most milk production. It is not a medical concern if a pregnant woman leaks any colostrum before her baby's birth, nor is it an indication of future milk production.\n\nAt birth, prolactin levels remain high, while the delivery of the placenta results in a sudden drop in progesterone, estrogen, and HPL levels. This abrupt withdrawal of progesterone in the presence of high prolactin levels stimulates the copious milk production of \"Secretory Activation\".\n\nWhen the breast is stimulated, prolactin levels in the blood rise, peak in about 45 minutes, and return to the pre-breastfeeding state about three hours later. The release of prolactin triggers the cells in the alveoli to make milk. Prolactin also transfers to the breast milk. Some research indicates that prolactin in milk is greater at times of higher milk production, and lower when breasts are fuller, and that the highest levels tend to occur between 2 a.m. and 6 a.m.\n\nOther hormones—notably insulin, thyroxine, and cortisol—are also involved, but their roles are not yet well understood. Although biochemical markers indicate that Secretory Activation begins about 30–40 hours after birth, mothers do not typically begin feeling increased breast fullness (the sensation of milk \"coming in the breast\") until 50–73 hours (2–3 days) after birth.\n\nColostrum is the first milk a breastfed baby receives. It contains higher amounts of white blood cells and antibodies than mature milk, and is especially high in immunoglobulin A (IgA), which coats the lining of the baby's immature intestines, and helps to prevent pathogens from invading the baby's system. Secretory IgA also helps prevent food allergies. Over the first two weeks after the birth, colostrum production slowly gives way to mature breast milk.\n\nThe hormonal endocrine control system drives milk production during pregnancy and the first few days after the birth. When the milk supply is more firmly established, autocrine (or local) control system begins.\n\nDuring this stage, the more that milk is removed from the breasts, the more the breast will produce milk. Research also suggests that draining the breasts more fully also increases the rate of milk production. Thus the milk supply is strongly influenced by how often the baby feeds and how well it is able to transfer milk from the breast. Low supply can often be traced to:\n\nThis is the mechanism by which milk is transported from the breast alveoli to the nipple. Suckling by the baby stimulates the paraventricular nuclei and supraoptic nucleus in the hypothalamus, which signals to the posterior pituitary gland to produce oxytocin. Oxytocin stimulates contraction of the myoepithelial cells surrounding the alveoli, which already hold milk. The increased pressure causes milk to flow through the duct system and be released through the nipple. This response can be conditioned e.g. to the cry of the baby.\n\nMilk ejection is initiated in the mother's breast by the act of suckling by the baby. The milk ejection reflex (also called let-down reflex) is not always consistent, especially at first. Once a woman is conditioned to nursing, let-down can be triggered by a variety of stimuli, including the sound of any baby. Even thinking about breastfeeding can stimulate this reflex, causing unwanted leakage, or both breasts may give out milk when an infant is feeding from one breast. However, this and other problems often settle after two weeks of feeding. Stress or anxiety can cause difficulties with breastfeeding. The release of the hormone oxytocin leads to the \"milk ejection\" or \"let-down reflex\". Oxytocin stimulates the muscles surrounding the breast to squeeze out the milk. Breastfeeding mothers describe the sensation differently. Some feel a slight tingling, others feel immense amounts of pressure or slight pain/discomfort, and still others do not feel anything different.\n\nA poor milk ejection reflex can be due to sore or cracked nipples, separation from the infant, a history of breast surgery, or tissue damage from prior breast trauma. If a mother has trouble breastfeeding, different methods of assisting the milk ejection reflex may help. These include feeding in a familiar and comfortable location, massage of the breast or back, or warming the breast with a cloth or shower.\n\nThis is the mechanism by which milk is transported from the breast alveoli to the nipple. Suckling by the baby innervates slowly-adapting and rapidly-adapting mechanoreceptors that are densely packed around the areolar region. The electrical impulse follows the spinothalamic tract, which begins by innervation of fourth intercostal nerves. The electrical impulse then ascends the posterolateral tract for one or two vertebral levels and synapses with second-order neurons, called tract cells, in the posterior dorsal horn. The tract cells then decussate via the anterior white commissure to the anterolateral corner and ascend to the supraoptic nucleus and paraventricular nucleus in the hypothalamus, where they synapse with oxytocinergic third-order neurons. The somas of these neurons are located in the hypothalamus, but their axon and axon terminals are located in the infundibulum and pars nervosa of the posterior pituitary, respectively. The oxytocin is produced in the neuron's soma in the supraoptic and paraventricular nuclei, and is then transported down the infundibulum via the hypothalamo-neurohypophyseal tract with the help of the carrier protein, neurophysin I, to the pars nervosa of the posterior pituitary, and then stored in Herring bodies, where they are stored until the synapse between second- and third-order neurons.\n\nFollowing the electrical impulse, oxytocin is released into the bloodstream. Through the bloodstream, oxytocin makes its way to myoepithelial cells, which lie between the extracellular matrix and luminal epithelial cells that also make up the alveoli in breast tissue. When oxytocin binds to the myoepithelial cells, the cells contract. The increased intra-aveolar pressure forces milk into the lactiferous sinuses, into the lactiferous ducts (a study found that lactiferous sinuses may not exist. If this is true then milk simply enters the lactiferous ducts), and then out the nipple.\n\nA surge of oxytocin also causes the uterus to contract. During breastfeeding, mothers may feel these contractions as \"afterpains\". These may range from period-like cramps to strong labour-like contractions and can be more severe with second and subsequent babies.\n\nIn humans, induced lactation and relactation have been observed frequently in some cultures, and demonstrated with varying success in adoptive mothers. It appears plausible that the possibility of lactation in women (or females of other species) who are not biological mothers does confer an evolutionary advantage, especially in groups with high maternal mortality and tight social bonds. The phenomenon has been also observed in most primates, in some lemurs, and in dwarf mongooses.\n\nLactation can be induced in humans by a combination of physical and psychological stimulation, by drugs, or by a combination of those methods. Some couples may stimulate lactation outside of pregnancy for sexual purposes.\n\nRare accounts of male lactation (as distinct from galactorrhea) exist in historical medical and anthropological literature, although the phenomenon has not been confirmed by more recent literature.\n\nCharles Darwin recognized that mammary glands seemed to have developed specifically from cutaneous glands, and hypothesized that they evolved from glands in brood pouches of fish, where they would provide nourishment for eggs. The latter aspect of his hypothesis has not been confirmed; however, more recently the same mechanism has been postulated for early synapsids.\n\nAs all mammals lactate, lactation must have evolved before the last common ancestor of all mammals, which places it at a minimum in the Middle or Late Triassic when monotremes diverged from therians. O. T. Oftedal has argued that therapsids evolved a proto-lacteal fluid in order to keep eggs moist, an adaption necessitated due to diapsids parchment shelled eggs which are more vulnerable to evaporation and dehydration than the mineralized eggs produced by some sauropsids. This protolacteal fluid became a complex, nutrient-rich milk which then allowed a decline in egg size by reducing the dependence on a large yolk in the egg. The evolution of lactation is also believed to have resulted in the more complex dentition seen in mammals, as lactation would have allowed the prolonged development of the jaw before the eruption of teeth.\n\nDuring early evolution of lactation, the secretion of milk was through pilosebaceous glands on mammary patches, analogous to the areola, and hairs on this patch transported the nourishing fluids to the hatchlings as is seen in monotremes. Later the development of the nipple rendered mammary hairs obsolete.\n\nAnother well known example of nourishing young with secretions of glands is the crop milk of columbiform birds. Like in mammals, this also appears to be directed by prolactin. Other birds such as flamingos and penguins utilize similar feeding techniques.\n\nThe discus fish (\"Symphysodon\") is known for (biparentally) feeding their offspring by epidermal mucus secretion. A closer examination reveals that, as in mammals and birds, the secretion of this nourishing fluid may be controlled by prolactin. Similar behavior is seen in at least 30 species of cichlids.\n\nLactation is also the hallmark of adenotrophic viviparity - a breeding mechanism developed by some insects, most notably tsetse flies. The single egg of the tsetse develops into a larva inside the uterus where it is fed by a milky substance secreted by a milk gland inside the uterus. The cockroach species \"Diploptera punctata\" is also known to feed their offspring by milky secretions.\n\n\"Toxeus magnus\", an ant-mimicking jumping spider species of Southeast Asia, also lactates. It nurses its offspring for about 38 days, although they are able to forage on their own after 21 days. Blocking nursing immediately after birth resulted in complete mortality of the offspring, whereas blocking it 20 days after birth resulted in increased foraging and reduced survival. This form of lactation may have evolved from production of trophic eggs.\n\n\n"}
{"id": "50126231", "url": "https://en.wikipedia.org/wiki?curid=50126231", "title": "Malaysian Journal of Nutrition", "text": "Malaysian Journal of Nutrition\n\nThe Malaysian Journal of Nutrition is a triannual peer-reviewed medical journal published by the Nutrition Society of Malaysia. It was established in 1995 and covers nutrition science. The editor-in-chief is Khor Geok Lin.\n\nThe journal is abstracted and indexed in Index Medicus/PubMed/MEDLINE, and Scopus.\n"}
{"id": "1377715", "url": "https://en.wikipedia.org/wiki?curid=1377715", "title": "Margaret Drummond (mistress)", "text": "Margaret Drummond (mistress)\n\nMargaret Drummond (c. 1475 – 1501) was a daughter of John Drummond, 1st Lord Drummond, and a mistress of King James IV of Scotland. She was a great-great-great-great-niece of the Margaret Drummond who was King David II's second queen.\n\nHer death has been the subject of a very persistent romantic legend.\n\nShe was definitely the mistress of James IV during 1496-97, and possibly as early as 1495. Records show her living at Stirling Castle from 3 June 1496, and from 30 October to March 1497 at Linlithgow Palace. Her presence, and a previous similar arrangement for another mistress in the royal houses, was also noted by the Spanish ambassador Pedro de Ayala. Ayala later wrote of James IV:\"When I arrived, he was keeping a lady with great state in a castle. He visited her from time to time. Afterwards he sent her to the house of her father, who is a knight, and married her [to a third party]. He did the same with another lady, by whom he had had a son.\"\nHowever, the king had a number of mistresses in his time, and this relationship seems to have been shorter than those he had with either Marion Boyd or Janet Kennedy.\n\nMargaret and James IV had a daughter, Margaret Stewart. She married firstly John Gordon, Lord Gordon, and secondly Sir John Drummond of Innerpeffray.\n\nIt is definitely known that in 1501 she died of food poisoning, along with her sisters Eupheme and Sibylla, while staying at Drummond Castle. As a general rule, claims of poisoning made in relation to a historical figure who died after a sudden illness should be treated with caution, but in this case, with three people who presumably died shortly after eating the same meal, the contemporary judgement should be accepted. The three sisters are buried together in Dunblane Cathedral, their graves can still be seen in front of the altar. This did not cause a great deal of suspicion at the time; standards of food hygiene are unlikely to have been very good then, and cases of accidental food poisoning have happened in any period.\n\nAfter her death the king paid for masses to be said for her soul, and continued to support their daughter.\n\nIt has been widely suggested in more recent years that Margaret Drummond was murdered, either by English agents or by pro-English elements in the Scottish nobility. Many believe that James IV was planning to or had already secretly married Drummond, and her death was necessary in order to allow or force the King to marry the English princess Margaret Tudor, daughter of Henry VII of England and Elizabeth of York. The (comparatively recent) plaque on her grave in Dunblane Cathedral claims that she was commonly believed to be \"privately married\" to the king, and that she was murdered by Scottish nobles who supported the English marriage.\n\nFurthermore, the \"Marriage of the Rose and Thistle\", as the poet William Dunbar described it, brought about the Union of the Crowns exactly 100 years later, as it enabled their great-grandson James VI of Scotland to claim the English throne upon the death of Elizabeth I through his descent from Henry VII.\n\nHad James IV married Margaret Drummond instead of Margaret Tudor, the Union of the Crowns might never have taken place and Scotland might have remained an independent country. This idea has been the theme of numerous historical novels and popular histories.\n\nSerious historians are skeptical of the theory. It is not supported by the contemporary evidence, and originates in a history of the Drummond family written by Viscount Strathallan in 1681. Her death was probably a case of accidental food poisoning, a common cause of death at that time. The idea that James had to be pressured to marry Margaret Tudor is dubious. As Scotland was the less important and poorer country, it is more likely that James IV pressured Henry VII to give him his daughter. It is also clear that negotiations for the marriage had been taking place before Margaret Drummond died.\n\n\n\n"}
{"id": "16585663", "url": "https://en.wikipedia.org/wiki?curid=16585663", "title": "Matsuo Fujimoto", "text": "Matsuo Fujimoto\n\nOn August 1, 1951, a dynamite charge was set in the house of a functionary who supported the segregation of leper patients. The police arrested Matsuo Fujimoto and he was detained at a leper colony, Kikuchi Keifuen Sanatorium, in Kumamoto Prefecture. He escaped from the hospital on June 16, 1952. On July 6, the functionary was murdered. The police announced that Matsuo had killed him, and shot and captured him on July 12. Although there was none of the victim's blood on his dagger, the police extracted a confession from Matsuo.\n\nFujimoto's trial was abnormal, taking place in a special isolated court because of his condition. His first lawyers agreed with the prosecutors, and his supporters, including Yasuhiro Nakasone, viewed his trials as unfair. Kumamoto district court sentenced him to death on August 29, 1953, and he was eventually executed by hanging on September 14, 1962, after Kunio Nakagaki signed his death warrant. \n\nLater, when Japanese policy against lepers was criticised as unethical, the case came under review. In March 2005, a verification committee established by the Ministry of Health, Labour and Welfare concluded that \"Fujimoto's case did not even come close to satisfying the constitutional requirements.\"\n\n"}
{"id": "3770891", "url": "https://en.wikipedia.org/wiki?curid=3770891", "title": "Medical laboratory scientist", "text": "Medical laboratory scientist\n\nA medical laboratory scientist (MLS), also traditionally referred to as a clinical laboratory scientist (CLS), is a healthcare professional who performs chemical, hematological, immunologic, histopathological, cytopathological, microscopic, and bacteriological diagnostic analyses on body fluids such as blood, urine, sputum, stool, cerebrospinal fluid (CSF), peritoneal fluid, pericardial fluid, and synovial fluid, as well as other specimens. Medical laboratory scientists work in clinical laboratories at hospitals, reference labs, biotechnology labs and non-clinical industrial labs.\n\nEducational and licensing requirements vary by country due to differing scopes of practice and legislative differences.\n\nIn Australia, medical laboratory scientists complete a four-year undergraduate degree program in medical laboratory science or Master of Medical Laboratory science . These programs should be accredited by the Australian Institute of Medical Scientists (AIMS).\n\nIn Canada, three-year college or technical school programs are offered that include seven semesters, two of them comprising an unpaid internship. The student graduates before taking a standard examination (such as the Canadian Society for Medical Laboratory Science, or CSMLS, exam) to be qualified as a medical laboratory technologist. Many MLTs go on to receive a bachelor of science degree after they are certified, but a few university programs affiliated with a college MLT program to allow students to graduate with both MLT certification and a degree such as the University of New Brunswick's Bachelor of Medical Laboratory Sciences program.\n\nCanada is currently experiencing an increasing problem with staffing shortages in medical laboratories.\n\nIn New Zealand, a medical laboratory scientist must complete a bachelor's degree in medical laboratory science or biological or chemical science recognized by the Medical Sciences Council of New Zealand. Once they graduate they must have worked at least six months under supervision, be registered with the Medical Sciences Counsel of New Zealand, and hold a current Annual Practicing Certificate.\n\nIn Ghana, a doctor of medical laboratory scientist (MLS.D) is a professional with a six (6) years professional doctorate degree in medical laboratory science, the medical laboratory scientist (MLS) has four (4) years bachelor's degree in medical laboratory science and the technicians has three (3) years diploma in medical laboratory science.\n\nThe curriculum for the programme include clinical rotations, where the students get hands-on experiences in each discipline of the laboratory and performs diagnostic testing in a functioning laboratory under supervision.\n\nIn Pakistan National Institute of Health (NIH) Islamabad is the pioneer in Laboratory Sciences, College of Medical Lab Technology, (CMLT), NIH, Islamabad offers 2 years F.Sc in Medical Lab Technology (MLT), Previously 2 Years B.Sc (MLT) that was discontinued and replaced by 4 years Bachelor Program in Medical Lab Sciences. University of Health Sciences, Lahore also offering 4 year Bachelor program in Medical Lab Sciences through approved colleges. University of Lahore, University of Faisalabad, University of Sargodha and Superior University Lahore offering 5-years Doctor of Medical Lab Sciences (DMLS) Program; Eligibility criteria for 4 years BS Medical Lab Sciences and 5 years Doctor of Medical Lab Sciences (DMLS) is F.Sc Pre-Medical. \nIn the United States, a medical laboratory scientist (MLS), medical technologist (MT), or a clinical laboratory scientist (CLS) typically earns a bachelor's degree in medical laboratory science, clinical laboratory science, or medical technology. Other routes include attaining a degree in biomedical science or in a life / biological science (biology, biochemistry, microbiology, etc.), in which a case certification from an accredited training program is also required.\n\nCommon comprehensive Medical laboratory scientist degree programs are set up in a few different ways. \nThe core curriculum in medical technology generally comprises 20 credits in clinical chemistry, 20 credits in hematology, and 20 credits in clinical microbiology.\n\nDuring clinical rotations, the student experiences hands-on learning in each discipline of the laboratory and performs diagnostic testing in a functioning laboratory under supervision. With limited or no compensation, a student in the clinical phase of training usually works 40 hours per week for 20 to 52 weeks. Some programs in the United States have had the time students spend completing their clinical rotation reduced due to staffing shortages. For example, in 2015, the MLS program at the University of Minnesota reduced the clinical rotation portion of the program from 22 weeks to 12 weeks.\n\nIn the United States, a two year academic program (associate's degree) qualifies the graduate to work as a medical laboratory technician (MLT). MLTs receive training more exclusively in laboratory sciences without the basic science coursework often required by MLS programs; however, there are many MLT training programs that require substantial basic didactic science course work prior to entry into a clinical practicum. Although the didactic coursework may be less for the MLT, the clinical practicum, in many cases, is similar to that of the MLS student's. This equates to MLTs who are well equipped to enter the work force with relevant and knowledge based practical application. The shorter training time may be attractive to many students, but there are disadvantages to this route. MTs, MLSs and CLSs usually earn higher salaries and have more responsibilities than MLTs. In 2018, medical laboratory technicians earned an average salary of $51,219, while medical laboratory scientists earned a salary of $67,888. An added disadvantage for MLTs is that some institutions will only employ MLSs, although that practice is starting to change due to recent efforts in cost reduction, and due to staffing shortages. \n\nIn practice, the term \"medical laboratory technician\" may apply to persons who are trained to operate equipment and perform tests, usually under the supervision of the certified medical technologist or laboratory scientist. Depending on the state where employment is granted, the job duties between MLSs and MLTs may or may not be similar. For example, in Florida, an MLT may only perform highly complex testing while under the direct supervision of a clinical laboratory technologist, a clinical laboratory supervisor, or a clinical laboratory director. This may make it impractical for an MLT to lawfully work in a Florida blood bank. California has similar restrictions on MLTs. To accommodate California's restrictions, the American Association of Bioanalysts (AAB) developed a separate certification examination for California licensure. However, this exam does not include material covering the areas of immunohematology or microscopy. Although the typical entry-level academic requirement for most MLTs is an associate degree, a 60 credit certificate program exists through military training programs; such as the U.S. Army's 68K military occupational specialty. \n\nAs in other countries, staffing shortages have become a major issue in many clinical laboratories in the United States. Due to several factors, including boomer retirement, and inadequate recruitment and retention efforts, the medical laboratory workforce is shrinking. For the decade 2010-2020, workforce needs are expected to grow by 13%. This translates into about 11,300 positions per year that will need to be filled, with only about 5000 new graduates per year coming out of various programs. By 2025, it is estimated that the shortage of medical laboratory professionals will reach 98,700 in the U.S.\n\nIn the United Kingdom (UK) there are two varieties of registered healthcare scientist in hospitals - Clinical Scientists and Biomedical Scientists (BMS). There is a strict and formal post graduate training programme for both careers followed by statutory registration for each with the Health & Care Professions Council UK (HCPC):[1], for the safety and assurance of the customers - the patients. They are two similar but distinct careers with parallel but different training paths and different entry requirements.\n\nThe role of Clinical Scientists is to improve the health and well-being of patients and the public by practising alongside doctors, nurses, and other health and social care professionals in the delivery of healthcare. Their aim is to provide expert scientific and clinical advice to clinician colleagues, to aid in the diagnosis, treatment and management of patient care.\n\nExamples of the type of work they undertake include:\n\n\nTrainee Clinical Scientist posts are advertised nationally, usually between November and February on the Clinical Scientists Recruitment webpages where application forms may be obtained and electronic submission of applications can be made. These posts are for the approved Pre-registration Training Programme, designed to prepare entrants for higher professional qualifications, further clinical training and eventual Consultant responsibility.\n\nClinical Scientist training involves enrolment of graduates (1st or 2nd class honours degree or better is essential due to the high competition for limited training places) into an intensive 3-year training scheme leading to certification and eventual registration before starting the higher career structure. The basic qualification for becoming a Clinical Biochemist, Clinical Immunologist or Clinical Microbiologist is a good Honours degree in an appropriate subject: for Clinical Biochemistry, that subject might be Biochemistry or Chemistry (or another life science subject which contains a substantial Biochemistry component); for Clinical Immunology, that subject might be any life science degree with an immunology component; for Clinical Microbiology that subject might be any life science degree with a microbiology component.\n\nAlthough not essential, some candidates will apply with higher degrees in an to attempt to improve their chances of selection for training and several universities currently offer MSc courses in Clinical Biochemistry, Immunology and Microbiology which have been approved by the ACB or the AHCS. Full-time and 'sandwich' courses are available, and further information may be obtained from individual programmes, although the level of financial support provided varies, and should be clarified at interview. Some entrants to the profession will already have obtained a PhD, and the training and research experience that this provides is invaluable to the work of the Clinical Scientist. In larger Departments, there may be opportunities to study for a research degree after entering the profession and acquiring registration, but since this has to be fitted in with other responsibilities, it may take some years to complete. It should be clearly understood that the major role of the profession is patient care and that research, management and all the other aspects will come as side issues and not be the predominating factor in the career path. The work of Biomedical Scientists and Clinical Scientists have impact on the diagnosis and treatment of almost every patient admitted to hospitals in the United Kingdom.\n\nThe United Kingdom is facing a shortage of qualified Clinical and Biomedical Scientists. The Royal College of Pathologists and the Royal College of Physicians have pointed out the need for increased government funding for medical training programs to prevent diagnostic facilities and medical infrastructure from being overwhelmed.\n\nThere are currently three major certification agencies in the United States of America for clinical laboratory scientists. They are the American Association of Bioanalysts (AAB), the American Medical Technologists (AMT), and the American Society for Clinical Pathology (ASCP). All three national accrediting agencies will certify scientists in the clinical laboratory as generalist (chemistry, hematology, immunology, immunohematology/blood bank, and microbiology). The American Association of Bioanalysts and the American Medical Technologists certifications continue to use the traditional designation Medical Technologist (MT), while the American Society for Clinical Pathology has adopted the designation of Medical Laboratory Scientist (MLS). Regardless of terminology, these highly qualified individuals serve as scientists in the clinical laboratory.\n\nThere are two other organizations that have previously provided proficiency examinations to clinical laboratory scientist. The first, is the US Department of Health and Human Services. The second, is the National Credentialing Agency for Laboratory Personnel (NCA). The NCA was absorbed by the American Society for Clinical Pathology in 2009 and promptly dissolved.\n\nIn the United States, the Clinical Laboratory Improvement Amendments (CLIA '88) define the level of qualification required to perform tests of various complexity. Clinical Laboratory Scientists, Medical Technologists and Medical Laboratory Scientists are near the highest level of qualification among general testing personnel and are usually qualified to perform the most complex clinical testing including HLA testing (also known as tissue typing) and blood type reference testing. Provider Performed Microscopy, or PPM (doctorate or master's level health provider) and Cytology have additional requirements.\n\nIn addition to the national certification, 12 states (California, Florida, Georgia, Hawaii, Louisiana, Montana, Nevada, North Dakota, Rhode Island, Tennessee, West Virginia and New York) and Puerto Rico also require a state license. Puerto Rico, in order to provide the state license, requires either a local board certification with a state examination, or any of both the ASCP and the NCA. Minnesota, Texas, Illinois, Massachusetts, Michigan, Vermont, Washington, New Jersey, Iowa, Utah, Ohio, South Carolina, Wyoming, Pennsylvania, Virginia, South Dakota, Delaware, Missouri, and Alaska are currently attempting to obtain licensure. All states require documentation from a professional certification agency before issuing a state certification. A person applying for state certification may also be expected to submit fingerprints, education and training records, and competency certification. Some states also require completion of a specified number of continuing education contact hours prior to issuing or renewing a license.\n\nSome states recognize another state's license if it is equal or more stringent, but currently California does not recognize any other state license.\n\nIn the United Kingdom all clinical scientists and biomedical scientists have had to be registered with the Health & Care Professions Council (HCPC) in order to work unsupervised, to develop through the careers grades of their profession and to use the protected titles of \"Clinical Scientist\" or \"Biomedical Scientist\". The HCPC registers nearly 200,000 healthcare professionals[3] and while success in an approved degree course from an accredited University is sufficient for all other professions, both clinical scientists and biomedical scientists have post graduate training and no approved degree courses. Autonomous assessment of applicants in these two professions with subsequent certification for successful ones, is the only approved UK route to registration for them.\n\n\"Clinical Scientist\", just as \"Biomedical Scientist\", is a protected title under the law (there is a £5000 fine for transgressors who fraudulently use the title without being registered by the state). The HCPC can strike people off the register for malpractice in just the same way as for doctors with the General Medical Council (GMC).\n\nThose who are working in \"Trainee\" positions in the profession are permitted to use the title with an appropriate caveat, for example – \"Pre-registration Clinical Scientist\", Trainee Clinical Scientist, etc. Alternatively some may use titles specific to the discipline they train in, such as Trainee Clinical Biochemist\", \"Clinical Immunologist in Training\" or “ Pre-Registrant Clinical Microbiologist” which is also perfectly acceptable since it is not implying the protected \"Clinical Scientist\" title of fully qualified and registered practitioners. It is against the law to formally work with the title of “Clinical Scientist” without professional registration[4].\n\nMany Medical Laboratory Scientists are \"generalists\", skilled in most areas of the clinical laboratory. However some are specialists, qualified by unique undergraduate education or additional training to perform more complex analyses than usual within a specific field. Specialties include clinical biochemistry, hematology, coagulation, microbiology, bacteriology, toxicology, virology, parasitology, mycology, immunology, immunohematology (blood bank), histopathology, histocompatibility, cytopathology, genetics, cytogenetics, electron microscopy, and IVF labs. Medical Technologists specialty may use additional credentials, such as \"SBB\" (Specialist in Blood Banking) from the American Association of Blood Banks, \"SM\" (Specialist in Microbiology) from the American Society for Microbiology, \"SC\" (Specialist in Chemistry) from the American Association for Clinical Chemistry, or \"SH\" (Specialist in Hematology) from the American Society for Clinical Pathology (ASCP). These additional notations may be appended to the base credential, for example, \"MLS(ASCP)SBB\". Additional information can be found in the ASCP Procedures for Examination & Certification.\n\nAndrology Laboratory Scientist, Embryology Laboratory Scientist, and Molecular Diagnostics Technologist certifications are provided by the American Association of Bioanalysts; those with the certifications are classified as ALS(AAB), ELS(AAB), and MDxT(AAB) respectively. Certified Histocompatibility Associate, Certified Histocompatibility Technologist, Certified Histocompatibility Specialist, and Diplomate of the ABHI are titles granted by the American Board of Hisocompatibility and Immunogenetics after meeting education and experience requirements and passing the required examination; those individuals would hold the credentials CHA(ABHI), CHT(ABHI), CHS(AHBI), and D(ABHI) upon passing the corresponding examination.\n\nIn the United States, Medical Laboratory Scientists can be certified and employed in infection control. These professionals monitor and report infectious disease findings to help limit iatrogenic and nosocomial infections. They may also educate other healthcare workers about such problems and ways to minimize them.\n\nIn the United Kingdom the number of Clinical Scientists in a pathology discipline are typically greater, where less medically qualified pathologists train as consultants. Clinical Biochemistry, Clinical Immunology and Genomic Medicine are specialities with an abundance of UK Clinical Scientists, and where the role is well established. Infection services in the United Kingdom are generally undertaken by medically qualified Microbiologists, who may have overall responsibility for laboratory services in addition to Infection Prevention and Control responsibilities, and may be required to contribute to ward rounds and patient clinics. Therefore, the Royal College of Pathologists and Royal College of Physicians have developed Combined Infection Training[10], that medical trainees gain a much more patient focused experience, and undertake Physician examinations in addition to Pathology training. The end result of this is that several regional medical deaneries no longer permit Medical Doctors to train in Microbiology or Virology as single disciplines, and instead advocate dual-specialisation as Infectious Disease/Microbiology or Infectious Disease/Virology [11]. Simultaneously the expansion of higher specialist scientist trainees in microbiology mean that many of the laboratory and scientific responsibilities of medical doctors may be taken on my Clinical Scientists, and medical doctors will instead be expected to perform a much more patient facing role. The exception in Microbiology is the sub-discipline of Virology, which is well suited to the expertise of clinical scientists due to reliance on cutting edge scientific methods, increasing use of specialised genetic technologies, and a technical understanding of virus biology, with a reduced emphasis on patient management compared with Microbiology as a whole[12].\n\nIt is therefore likely that many patients in UK hospitals may come into contact with Clinical Scientists working in a patient facing speciality, who may be confused with medical doctors due to the complex nature of their role.\n\nAs in many healthcare professions, a Medical Laboratory Scientist may pursue higher education to advance or further specialize in their career.\n\nIn the United Kingdom The Modernising Scientific Careers (MSC) programme sets out for the first time a comprehensive training and career framework for the whole healthcare science workforce inclusive of the more than 50 different scientific professional specialisms. In its conception it aimed to provide a coherent framework that was accessible, affordable and designed specifically to both capture scientific and technological advances and to provide improved outcomes for patients, the service and professionals. A key aspect of the framework from the start was the formalisation of training to develop talented clinical scientists to undertake quality assured Higher Specialist Scientist Training (HSST) programmes to prepare them for roles as Consultant Clinical Scientists. It is envisaged that Consultant Clinical Scientists will work synergistically and in partnership with their medical colleagues and within multiprofessional clinical teams to support clinical scientific practice aimed at quality improvement, innovation and world-class outcomes for patients. This scientific expertise and leadership will provide important benefits and added value to patients and to the service as it moves forward through the 21st century. This will bring to fruition the vision of science and realise the potential of scientific and technological advances for both translational and personalised medicine.\n\nTraining through the Higher Specialist Scientist Training pathway is discipline specific. For life science disciplines (Immunology, Microbiology, Virology, Haematology, Biochemistry) the training curriculum and formal examinations are administered by the Royal College of Pathologists. The life science training pathway for Clinical Scientists follows a similar pathway to that undertaken by medically qualified specialist registrars in pathology. Clinical Scientists are therefore the only discipline of non-medical healthcare professionals examined by a Medical Royal College. Clinical Scientists who attain both part 1 examination certification and part 2 certification are awarded Fellowship of the Royal College of Pathologists (FRCPath) and are deemed to have the knowledge and expertise expected of a consultant level scientist. Consultant Clinical Scientist posts generally require candidates to have completed FRCPath qualification to be eligible.\n\nAll Clinical Scientists regardless of seniority or specialisation may have other responsibilities including academic appointments, responsibilities as clinical lead for a pathology service, or may have wider hospital responsibilities such as Directorship of Infection Prevention and Control, or responsibility for the hospital's Research and Development strategy. Junior clinical scientists may become involved in academic research, working towards award of a Ph.D. or DClinSci\n\nMedical laboratory scientists work in all areas of the clinical laboratory, including blood banking, chemistry, hematology, immunology, histology and microbiology . They perform a full range of laboratory tests – from simple prenatal blood tests to more complex tests to uncover diseases such as HIV/AIDS, diabetes, and cancer. They are also responsible for confirming the accuracy of test results, and reporting laboratory findings to pathologists and other physicians. The information that a medical laboratory scientist gives to the doctor influences the medical treatment a patient will receive. Medical laboratory scientists operate complex electronic equipment, computers, and precision instruments costing millions of dollars.\n\nMedical Laboratory Scientist analyzes human fluid samples using techniques available to the clinical laboratory, such as manual white blood cell differentials/counts, bone marrow counts, analysis via microscopy, and advanced analytical equipment. Medical laboratory scientists assist doctors and nurses in choosing the correct lab tests and ensure proper collection methods. Medical laboratory scientists receive the patient specimens, analyze the specimens and report results. A pathologist may confirm a diagnostic result, but often the medical laboratory scientist is responsible for interpreting and communicating critical patient results to the physician.\n\nMedical laboratory scientists must recognize anomalies in their test results and know how to correct problems with the instrumentation. They monitor, screen, and troubleshoot analyzers featuring the latest technology available on the market. The MLS performs equipment validations, calibrations, quality controls, \"STAT\" or run-by-run assessment, statistical control of observed data, and recording normal operations. To maintain the integrity of the laboratory process, the medical laboratory scientist recognizes factors that could introduce error and rejects contaminated or sub-standard specimens, as well as investigates discrepant results.\n\nA typical laboratory performs hundreds of different tests with a number of methodologies. Common tests performed by medical laboratory scientists are complete blood count (CBC), comprehensive metabolic panel (CMP), electrolyte panel, liver function tests (LFT), renal function tests (RFT), thyroid function test (TFT), urinalysis, coagulation profile, lipid profile, blood type, semen analysis (for fertility and post-vasectomy studies), serological studies and routine cultures. In some facilities that have few phlebotomists, or none at all, (such as in rural areas) medical laboratory scientists may perform phlebotomy on patients, as this skill is part of the clinical training.\n\nBecause medical laboratory scientists are skilled in diverse scientific disciplines, employment outside of the medical laboratory is common. Many MLS are employed in government positions such as the FDA, USDA, non-medical industrial laboratories, and manufacturing. The practical experience required to obtain the bachelor's degree in medical technology give the MLS a unique understanding of the inter-relationship between microbiological and chemical testing and the resulting clinical manifestations in clinical, scientific, and industrial settings.\n\nIn the United Kingdom and the United States, senior laboratory scientists, who are typically post-doctoral scientists, take on significantly greater clinical responsibilities in the laboratory. In the United States these scientists may function in the role of clinical laboratory directors, while in the United Kingdom they are known as consultant clinical scientists.\n\nThough clinical scientists have existed in the UK National Health Service for ~60 years, the introduction of formally trained and accredited consultant level clinical scientists is relatively new, and was introduced as part of the new Modernising Scientific Careers framework.\n\nConsultant clinical scientists are expected to provide expert scientific and clinical leadership alongside and, at the same level as, medical consultant colleagues. While specialists in healthcare science will follow protocols, procedures and clinical guidelines, consultant clinical scientists will help shape future guidelines and the implementation of new and emerging technologies to help advance patient care.\n\nA Medical Laboratory Scientist's role is to provide accurate laboratory results in a timely manner. An estimated 70 percent of all decisions regarding a patient's diagnosis and treatment, hospital admission and discharge are based on laboratory test results.\n\nin the United Kingdom, Healthcare Scientists including Clinical Scientists may intervene throughout entire care pathways from diagnostic tests to therapeutic treatments and rehabilitation. Although this workforce comprises approximately 5% of the healthcare workforce in the UK, their work underpins 80% of all diagnoses and clinical decisions made.\n\nThe informal abbreviations of job titles may be a source of confusion. In the United States Medical Laboratory Scientist (ASCP) and Medical Technologists (AMT) or (AAB) are often called \"med techs\" (based on the era in which they were known as \"medical technologists\"), but this shorthand term is shared by other healthcare employees, including pharmacy techs, radiographers (also known as radiologic technologists), and respiratory therapists.\n\nIn the United States there is a formal distinction between an MLT and a MT/MLS. Often, MT/MLS have at least a bachelor's degree, while MLT have an associate degree. However, due to grandfathering rules and certification requirements between the boards of registry, some MT/MLS may only have an associate degree. Scientists and technologists generally earn a higher income than technicians, have more responsibilities, and have more opportunities for advancement.\n\nIn the United Kingdom, there are defined training pathways leading to professional registration as either a Clinical Scientist, or as a Biomedical Scientist. The role descriptions for these healthcare scientists are very different, where clinical scientists generally undertake non-routine research and development, as well as improving and providing clinical service using scientific expertise. Biomedical Scientists in the United Kingdom are similar to the role of MLT and MT/CLS described above, and have similar regulatory requirements for professional regulation. Clinical Scientists in the United Kingdom may struggle with a lack of professional recognition. This is in part due to the myriad job titles used to describe them including Clinical Physiologists, Medical Physicists, and Clinical Biochemists, which generally mean the public and other healthcare workers assume Clinical Scientists to be medically qualified doctors, due to the sometimes complex nature of the role.\n\n\n\n"}
{"id": "47677024", "url": "https://en.wikipedia.org/wiki?curid=47677024", "title": "Mental illness in fly-in fly-out workers", "text": "Mental illness in fly-in fly-out workers\n\nFly-in fly-out (FIFO) work practices in Australia occur amongst various professions primarily associated within the resources industry as well as medical and related health services. Following the recession of the 1980s, Australia has experienced a resources boom that has seen thousands of families impacted by FIFO work. The FIFO lifestyle often sees workers on a scheduled roster flying to remote locations. Workers live in serviced accommodation, working long days.\n\nWhile working in the mining and resource sector is financially rewarding, the type of lifestyle it leads is far different than the life workers have at home. As a result of this type of work, there is an impact on individuals, couples and family units that can account for the emotional health and well-being seen in workers.\n\nAustralia is one of the leading mining nations in the world, with large scale extraction of mineral sands, brown coal, nickel, zinc, lead, and uranium.\n\nIncreases in worldwide demand for resources have resulted in Australia's annual mining production has more than doubling in the 20 years up to 2008. There are approximately 365 operating mines in Australia, and as for the employment of up to 269,000 people, the mining and resource industry contributes 121.5 billion dollars to the economy. Thus plays a significant role to Australia’s wealth.\n\nMental health has been described by the World Health Organisation (WHO) as “a state of wellbeing in which every individual realises his or her potential, can cope with normal stresses of life, can work productively and fruitfully, and is able to make a contribution to her or his community” (2014). It is acknowledged that people working in rural and remote mining and resource operations confront psychological and emotional demands that will create unique challenges for both men and women. The key mental health issues across the resource mining sector includes feelings such as isolation and loneliness, due to the remoteness of living on-site and from family and friends. Stress, anxiety and depression are major factors which are likely to influence employment performance and antisocial conduct. This is predicted to get worse for some people during the transition period from home to work, and can potentially increase the risk of self-harm and suicide.\n\nAlong with the stresses of being away from home, other stresses FIFO workers experience include:\n\n\nThe FIFO lifestyle is based upon a roster, typically a fortnight on and one week off. However, more remote mining sites require month on and month off rosters, attributable to the incremented time and costs of flying to and from remote areas. As expected, FIFO workers are implemented work long shifts, usually ranging from 12- up to 18-hour shifts.\n\nAs reported by Meredith, Rush & Robinson (2014) it is notable that the longer work length than to time on leave has more deleterious effects on the workers' wellbeing and can lead to a vulnerable mental state. Physical health allows for a positive mental health as well as the capability to handle the demands of FIFO work. The length of 12 hour or more shifts with short breaks, make is arduous for workers to relax and involve themselves in effective coping strategies such as getting enough sleep, exercising and socialising, in dealing with daily stresses. In conclusion occupational fatigue may occur as frequent recovery is confined.\n\nOn-site mining workplace culture is considered a problem in reinforcing positive mental health and seeking support behavior. Unfortunately many workers do not seek formal help due to the general outlook held by the wider population of fear and stigma in seeking support for mental health issues.\n\nMale dominancy significantly contributes to the mining workplace culture, where females may experience tensions fitting within the FIFO civilisation. The masculine culture consequently effects the relationships with other workers, negative feelings suppression or bullying behavior that is a negative subside of poor mental health. Commendations include creating a fair and just workplace environment for both men and women, for instance The Australian Mines and Metal Sodality are taking action to improve the amount of women to 20% by 2020.\n\nWorkers who do not seek emotional support may experience adverse impacts and result in poor work output and increased isolation. An organisational culture is vital, in order to proposer relevant approaches for workers in acknowledging when to seek help.\n\nOver the past 20 years FIFO had become a prevalent mining industry practice, however on the subject matter there is a scarcity of Australian research. According to Arnold (1995) studies resulted in FIFO being problematic for some families, in an analysis of the impacts on the lifestyle and families of workers. The Australian FIFO personnel indicate the benefits of leisure, access to services and facilities swell as friends and extended family from relatively high earnings from working in the industry. However, observation conducted by Gillies et al. (1997) involved surveying fifteen Australian FIFO operations and out of a study sample of 227 employees, a total of 30% employees stated their families were not in favour of the FIFO lifestyle. Additionally results included 25% of employees believed their family relationships had been earnestly disadvantaged by the FIFO employment. \n\nThe structure of rostered schedules and continuous cycles such as four weeks away and one week home, the long separations and short homecomings provoke an increased amount of conflict between work and home. In accordance to Torkington, Larkins & Sen Gupta (2011) “ Miners reported that when they were away their partner described: being upset or lonely; the stresses of dealing with busy roles, such as parenting, alone; the challenge of changing routines; and having to managing practical tasks (e.g. mechanical repairs), which fell outside their normal role.” (2011). As a result, the family structure is altered, especially for those with younger children.\n"}
{"id": "24456774", "url": "https://en.wikipedia.org/wiki?curid=24456774", "title": "Millennium Cohort Study", "text": "Millennium Cohort Study\n\nThe Millennium Cohort Study (MCS) is a longitudinal survey conducted by the Centre for Longitudinal Studies (CLS) at the University of London, following the lives of a sample of about 18,818 babies born in the UK in the year 2000–2001. \n\nThe MCS is the fourth longitudinal birth cohort study conducted in the UK. Its aim is to create a multi-purpose data-set that describes the diversity of backgrounds into which children are born in the beginning of the 21st century. The information collected includes topics such as child development, social stratification and family life in order to identify possible advantages and disadvantages that the children are facing.\n\nThe survey is conducted in different sweeps with the first one concentrating on the circumstances of the pregnancy and birth as well as the first few months of life. This first part of the survey is also important to record the socio-economic background of the family into which the child is born. The second sweep took place when the children were about 3 years of age and the main focus was on continuity and change in the family as well as the parenting environment to extract information about the child’s development. In the third sweep in 2006, the children were at the age of starting primary school. The fourth sweep took place in 2008, the fifth was in 2012 and the sixth in 2015. The following sweep was planned for 2018.\n\nThe MCS is funded mainly by the Economic and Social Research Council (ESRC) and various government departments, such as the Department for Children, Schools and Families, the Department of Health (United Kingdom) (DH) and the Department for Work and Pensions (DfWP). The Scottish Government, the Welsh Assembly Government and the Northern Ireland Executive have also contributed to fund the survey.\n\nHeather Joshi was director of the survey from 2000 to 2011. Emla Fitzsimons was the director from 2013.\n\nThe sample is structured by geographical clusters, allowing certain areas with significant ethnic minorities (in England), high levels of child poverty, as well as Scotland, Wales and Northern Ireland to be overrepresented.\n\nBy 2016 the MCS findings have already been included in over 700 journal articles, books, etc.\n\n"}
{"id": "12270384", "url": "https://en.wikipedia.org/wiki?curid=12270384", "title": "Nigerian Red Cross Society", "text": "Nigerian Red Cross Society\n\nThe Nigerian Red Cross Society (NRCS) was founded in 1960 and it has its headquarters in Abuja.\n\nIt has over 500,000 volunteers and 300 permanent employees.\nThe Nigerian Red Cross Society was established by an Act of Parliament in 1960 and became the 86th Member – National Society of the League of Red Cross and Red Crescent Societies (Now International Federation of Red Cross and Red Crescent Societies) on 4 February 1961.\n\nIts mandate originates from:\n\n- The Movement Statutes\n- Main Movement Resolutions\n- RCRC Statutes and Byelaws\n- Geneva Conventions and\n- Nigerian Red Cross Act 1960\n\nIts driving principles are humanity, impartiality, neutrality, independence, voluntary service, unity and universality.\n\n"}
{"id": "22095947", "url": "https://en.wikipedia.org/wiki?curid=22095947", "title": "Nonpuerperal mastitis", "text": "Nonpuerperal mastitis\n\nThe term nonpuerperal mastitis describes inflammatory lesions of the breast (mastitis) that occur unrelated to pregnancy and breastfeeding.\n\nIt is sometimes equated with duct ectasia, but other forms can be described.\n\n\"Duct ectasia\" in the literal sense (literally: duct widening) is a very common and thus rather unspecific finding, increasing with age. However, in the way in which the term is mostly used, duct ectasia is an inflammatory condition of the larger-order lactiferous ducts. It considered likely that the condition is associated with aseptic (chemical) inflammation related to the rupture of ducts or cysts. It is controversial whether duct dilation occurs first and leads to secretory stasis and subsequent periductal inflammation or whether inflammation occurs first and leads to an inflammatory weakening of the duct walls and then stasis. When the inflammation is complicated by necrosis and secondary bacterial infection, breast abscesses may form. Subareolar abscess, also called Zuska's disease (only nonpuerperal case), is a frequently aseptic inflammation and has been associated with squamous metaplasia of the lactiferous ducts.\n\nThe duct ectasia—periductal mastitis complex affects two groups of women: young women (in their late teens and early 20s) and perimenopausal women. Women in the younger group mostly have inverted nipples due to squamous metaplasia that lines the ducts more extensively compared to other women and produces keratin plugs which in turn lead to duct obstruction and then duct dilation, secretory stasis, inflammation, infection and abscess. This is not typically the case for women in the older group; in this group, there is likely a multifactorial etiology involving the balance in estrogen, progesterone and prolactin.\n\nTreatment of mastitis and/or abscess in nonlactating women largely the same as that of lactational mastitis, generally involving antibiotics treatment, possibly surgical intervention by means of fine-needle aspiration and/or incision and drainage and/or interventions on the lactiferous ducts (for details, \"see also\" the articles on treatment of mastitis, of breast abscess and of subareolar abscess). Additionally, an investigation for possible malignancy is needed, normally by means of mammography, and a pathological investigation such as a biopsy may be necessary to exclude malignant mastitis. Although no \"causal\" relation with breast cancer has been established, there appears to be an increased statistical risk of breast cancer, warranting a long-term surveillance of patients diagnosed with non-puerperal mastitis.\n\nNonpuerperal breast abscesses have a higher rate of recurrence compared to puerperal breast abscesses. There is a high statistical correlation of nonpuerperal breast abscess with diabetes mellitus (DM). On this basis, it has recently been suggested that diabetes screening should be performed on patients with such abscess.\n\nCharacteristic for granulomatous mastitis are multinucleated giant cells and epithelioid histiocytes around lobules. Often minor ductal and periductal inflammation is present. The lesion is in some cases very difficult to distinguish from breast cancer.\n\nComedo mastitis is a very rare form similar to granulomatous mastitis but with tissue necrosis. Because it is so rare it may be sometimes confused with comedo carcinoma of the breast although the conditions appear to be completely unrelated.\n"}
{"id": "16063653", "url": "https://en.wikipedia.org/wiki?curid=16063653", "title": "Nutrition transition", "text": "Nutrition transition\n\nNutrition transition is the shift in dietary consumption and energy expenditure that coincides with economic, demographic, and epidemiological changes. Specifically the term is used for the transition of developing countries from traditional diets high in cereal and fiber to more Western pattern diets high in sugars, fat, and animal-source food.\n\nThe nutrition transition model was first proposed in 1993 by Barry Popkin, and is the most cited framework in literature regarding the nutrition transition, although it has been subject to some criticism for being overly simplified. Popkin posits that two other historic transitions affect and are affected by nutritional transition. The first is the demographic transition, whereby a pattern of high fertility and high mortality transforms to one of low fertility and low mortality. Secondly, an epidemiological transition occurs, wherein a shift from a pattern of high prevalence of infectious diseases associated with malnutrition, and with periodic famine and poor environmental sanitation, to a pattern of high prevalence of chronic and degenerative diseases associated with urban-industrial lifestyles is shown. These concurrent and dynamically influenced transitions share an emphasis on the ways in which populations move from one pattern to the next. \nPopkin used five broad patterns to help summarize the nutrition transition model. While these patterns largely appear chronological, it is important to note that they are not restricted to certain periods of human history and still characterize certain geographic and socioeconomic subpopulations. The first pattern is that of collecting food, a characterization of hunter-gatherers, whose diets were high in carbohydrates and low in fat, especially saturated fat. The second pattern is defined by famine, a marked scarcity and reduced variation of the food supply. The third pattern is one of receding famine. Fruits, vegetables, and animal protein consumption increases, and starchy staples become less important in the diet. The fourth pattern is one of degenerative diseases onset by a diet high in total fat, cholesterol, sugar, and other refined carbohydrates and low in polyunsaturated fatty acids and fiber. This pattern is often accompanied by an increasingly sedentary lifestyle. The fifth pattern, and most recently emerging pattern, is characterized by a behavioral change reflective of a desire to prevent or delay degenerative diseases. Recent and rapid changes seen in developing countries from the second and third pattern to the fourth is the common focus of nutrition transition research and desire for policy that would emphasize a healthier overall diet characterizes the shift from the fourth to the fifth pattern.\n\nThe nutrition transition has much of its roots in economic factors related to the development of a nation or subpopulations within a nation. It was once believed that current nutrition transition was endemic only to industrialized nations like the United States, but increasing research has indicated that not only is nutrition transition occurring most rapidly in low- and middle-income developing countries, the stress of its effects stands to burden the poorest populations of these countries the most as well.\nThis shift is attributable to many causes. Globalization has played a large role in altering the access and availability of foods in formerly undeveloped nations. Demographic shifts from rural to urban areas are central to this as well as the liberalization of food markets, global food marketing, and the emergence of transnational food companies in developing countries. All these forces of globalization are creating lifestyle changes that contribute to the nutrition transition. Technological advancements are making previously arduous labor less difficult and thus altering energy expenditure that would have helped offset the caloric increases in the diet. Daily tasks and leisure are also affected by technological advancements and contributing to greater rates of inactivity. The aforementioned increases in calorie are due to increased consumption of edible oils, animal-source foods, caloric sweeteners, accompanied by reduced consumption of grains and fruits and vegetables. These changes play into human biological preferences seen across the world. Socioeconomic factors also play an important role as do cultural values tied to appearance and status.\n\nThe current nutrition transition seen in the emerging markets of Asia, Latin America, the Middle East, North Africa, and urban areas of sub-Saharan Africa is largely a product of globalization. International food trade, investment, commercialization and marketing are drastically impacting the availability of and access to energy-dense, but nutrient-deficient foods causing the aforementioned shift from traditional diet. Another byproduct of globalization has been a marked demographic transition in these countries from rural areas to urban ones. Urban populations are more susceptible to current trends in nutrition transition because of the improved transportation, commercial food distribution and marketing, less labor-intensive-occupations, and changes in household eating habits and structure.\nThe liberalization of food markets has had a drastic effect on consumption patterns across the globe. Liberalization and commercialization of domestic agricultural markets are opening up food trading since this is needed to compete in the world market. This had led to changes in the types of food produced, and increases in amounts of food imported into developing countries, which affects the relative availability and prices of different foods. \nFood demand is being shaped by increases in income and urbanization. As these rapidly developing nations continue to accrue high incomes per capita, their food spending is increasing as well. They elect to use these higher incomes on more calorically-dense foods that are sweeter and higher in fats. For example, in China, for the same extra dollar of income, an average Chinese person is purchasing higher calorie food today than that person would have done for the same extra yuan in 1990. Rapid urbanization has also shaped food demand globally. The demographic transition from rural areas to urban populations is a well documented byproduct of globalization and technological advancements. This is because agro-food systems have replaced local subsistence farming in many rural areas. \nThe supply of food is directly sculpted by increasing demand in these areas with growing income. Urbanization is increasing access to new foods and therefore altering the supply chain. This is why transnational food companies have grown so rapidly over the past few decades. These companies are making processed and fast foods much cheaper and more widely available through the growth of transnational supermarkets and chain restaurants. Food is not only easier to obtain in urban areas; it is also cheaper and less time-consuming to acquire which creates an imbalance between energy intake and output. Their advertising and promotional strategies have a strong effect on consumer choices and desire. Foreign direct investment is also stimulating processed food sales in these supermarkets by lowering prices and creating incentives for advertising and promotion. A large proportion of this advertising is for energy-dense processed foods and is being directed at children and youth.\nTechnological and transportation advancements are reducing the barriers that once limited global food trade. These techniques are critical to facilitating the production and distribution needed in a global market. Better preservation techniques are helping to reduce waste which contributes to lower prices for consumers. Technology is creating higher yields which also reduce prices.\n\nThe forces of globalization are strongly influencing many lifestyle changes in developing countries. Major changes in economic structures from agrarian economies to industrialized economies are reducing physical activity levels in occupations around the world. Even in agricultural work, gas-powered technologies are helping reduce the energy expenditure needed to perform pertinent farming tasks. These reduced activity levels are not just seen in the workplace, but in homes as well. Daily tasks that were once laborious engagements are now much easier with the help of technological advancements, with examples being appliances such as washing machines, refrigerators, and stoves. Also, recent leaps in the efficiency of food production (canning, refrigeration, freezing, and packaging being a few of the most notable) and improvements in cookware, such as the introduction of improved metal stoves which use fossil fuels and microwave ovens, have helped reduce domestic efforts greatly.\n\nLeisure is being greatly impacted as well. Activities such as playing sports outside are being replaced with television watching and computer games. Decreasing physical leisure activities can also be contributed to urbanization wherein access to fields needed to play such games as soccer are not available due to such dense populations and their subsequent demand for land. Other important lifestyle changes fueling the nutrition transition relate to the composition of diets. These dietary shifts have been mentioned previously several times but deserve greater scrutiny. Diets rich in legumes, other vegetables, and coarse grains are disappearing in all regions and countries. Taking their place are diets characterized by fat-rich edible and vegetable oils, cheap animal-source foods high in fat and protein, and artificially sweetened foods high in sugar and refined carbohydrates. Consumption of caloric beverages such as soda represented 21% of all calorie intake in Mexico from 1996 to 2002. Processes of globalization that have influenced food markets have made these products much cheaper, flavorful, and easier to produce which has in turn driven up their demand. So while globalization and the accompanying economic development has created higher levels of food security for developing countries, the ongoing trend of eating in a more Western fashion has caused increased rates of adverse health and childhood obesity.\n\nThe desires for these new diets and lifestyles are very understandable from a biological and psychosocial perspective. For example, humans have an innate preference for sweets dating back to hunter-gatherer populations. These sweets signaled a good source of energy for hunter-gatherers that were not food secure. This same concept also relates to human predisposition for energy-dense fatty foods. These foods were needed to sustain long journeys and provided a safety net for times of famine. Humans also desire to eliminate physical exertion. This can explain the shift to more sedentary lifestyles from occupational, domestic, and leisurely activities that were previously much more physical taxing. \nSocioeconomic and cultural influences also contribute to lifestyle changes associated with nutrition transition. The transfer of tastes by means of tourism and open food trade has introduced developing nations to foods previously enjoyed only by industrialized countries. Global food advertising and promotion has only further cemented these dietary changes. Additionally some cultures view obese body types in high regard as they relate them to power, beauty and affluence. Several studies suggest that socioeconomic status contributes greatly to nutrition transition wherein there is a lack of healthy food alternatives completely or a lack of affordable healthy food alternatives.\n\nWhile increased food security is a major benefit of global nutrition transition, there are a myriad of coinciding negative health and economic consequences. Rates of obesity are soaring across the world and recent trends suggest that incidences of overnutrition in coming decades will overtake that of undernutrition in the developing world. As well there will be a marked epidemiological shift from infectious disease to degenerative, noncommunicable disease, NCDs in these countries. As it stands now these countries face a unique paradox in having to deal with both over- and undernutrition, a dual burden of malnutrition, that will inevitably be accompanied by both infectious and noncommunicable diseases, a dual burden of disease. The economic impact will be enormous as well. In addition to reduced productivity, the health systems of these countries stand to face a tremendous burden.\n\nThe foremost health outcome of the global nutrition transition will be an increased prevalence of obesity across the world. Obesity prevalence in developing countries increased from 2.3% in 1988 to 19.6% in 1998. Incidences are highest among women and children, indicating health inequities across global populations. Obesity is strongly linked to degenerative, NCDs such as coronary heart disease, diabetes, stroke, and hypertension. WHO estimates place NCDs as the principal global cause of morbidity and mortality, and global prevalence of chronic diseases is projected to increase substantially over the next 2 decades in developing countries. Between 1990 and 2020, mortality from cardiovascular diseases, CVDs, in developing countries is expected to increase 120% for women and 137% for men compared to 29 and 49% respectively in industrialized countries. In many of the countries facing epidemics of overnutrition, there is still widespread undernutrition.\n\nDeemed as a developmental challenge of epidemic proportions, the double burden of disease (DBD) is an emerging global health challenge, that exists predominately in low-to-middle income countries. More specifically, the DBD refers to the dual burden of communicable and non-communicable diseases(NCD). Today, over 90 per cent of the world’s disease burden occurs in developing regions, and most are attributed to communicable diseases. Communicable diseases are infectious diseases that “can be passed between people through proximity, social contact or intimate contact.” Common diseases in this category include whooping cough or tuberculosis, HIV/AIDs, malaria, influenza (the flu), and mumps. \nAs low-to-middle income countries continue to develop, the types of diseases that affecting populations within these countries shifts primarily from infectious diseases, such as diarrhea and pneumonia, to primarily non-communicable diseases, such as cardiovascular disease, cancer and obesity. This shift is increasingly being referred to as the risk transition. Thus, as globalization and the proliferation of pre-packaged foods continues, traditional diets and lifestyles are changing in many developing countries. As such, it is becoming increasingly common to see low-to-middle income countries battle with century old issues such as food insecurity and under nutrition, in addition to emerging health epidemics such as chronic heart disease, hypertension, stroke, and diabetes. Diseases once characteristic of industrialized nations, are increasingly becoming health challenges of epidemic proportions in many low-to-middle income countries.\n\nThe economic impact of these rising rates and dual burdens of disease looks to be tremendous. Disability, decreased quality of life, greater use of health care facilities, and increased absenteeism are strong associated with obesity. With inadequate resources, poorly construed health systems, and a general lack of expertise to address the burden of infective diseases, the disease burden for low-to-middle countries is exacerbated by the rising rate of non-communicable diseases. This is often attributed to the fact that these countries by nature have ill-health systems that possess inadequate resources to detect and prevent many non-communicable diseases.” Social constructs within these countries often amplify the risk of the double burden, as inequality, gender, and other social determinants often have a role to play in disparate access and allocation of health services and resources. If current trends are maintained, the World Health Organization predicts that low-and-middle income countries will be unable to support the burden of disease within the foreseeable future.\n\nCountries worldwide have made several, varied efforts to address the consequences of the nutrition transition. These policies target the food environment, governance, food system, or education and can be generally classified into the following categories:\n\nNutrition education intends to facilitate healthy behavioral changes, at the individual level. Dietary guidelines, specifically, promote public awareness of nutritional needs. Over 60 countries in the Global North and South have established national dietary guidelines.\n\nNutrition labeling for food packages and in restaurants may encourage consumers to choose healthier foods. Nutrition labeling has been emphasized as important in influencing food choices and potentially reducing the intake of fat, sugar, and sodium.\n\nSchools are viewed as a primary target of intervention for implementing nutrition-related policies. Children and adolescents are particularly vulnerable to exposure to unhealthy foods before, during, and after school. Children are more susceptible to developing early obesity and are likely to remain obese throughout adulthood. School policies are varied and specific to the political, economic, and social climates of a place. They can focus on increasing nutritional standards, promoting active lifestyles, regulating school meal programs, and banning the sale of certain foods and beverages in and around schools.\n\nFood marketing, via several media outlets - television, the Internet, packaging, popular culture - has been an effective strategy for influencing and changing consumers' food choices, particularly among children. Several studies have indicated the association between exposure to food advertising and food choices and beliefs. The impact of advertising has led to support for government level regulation of food marketing. Countries have implemented voluntary or mandatory restrictions on advertisements of unhealthy food products. Food companies are also urged to implement responsible food marketing strategies. Efforts by corporations should reverse drivers of food consumption, including convenience, low cost, good taste, and nutritional knowledge. Recommendations include downsizing packaging, reducing serving sizes, and recreating formulas to decrease caloric content.\n\nStemming from the success of taxation of tobacco products in reducing tobacco usage, policy makers and researchers have adopted a parallel approach for reducing obesity. The WHO supported economic policies as a method of influencing food prices and promoting healthy eating in public spaces (cite, 2008). Tax policies, in the form of sin taxes or Pigovian taxes, generally target unhealthy food and drink products, including the \"fat tax,\" \"junk food tax,\" and of particular popularity, the tax on sugar-sweetened beverages (SSBs). Taxation is intended to combat obesity by increasing the price of SSBs and unhealthy foods and in turn, reducing their consumption, as well as generating revenue that may be used towards obesity prevention programs or promotion of fruit and vegetable consumption. However, the effectiveness of taxation remains under scrutiny - economists argue that taxes are inefficient for combating obesity and can result in greater losses for consumers.\n\nThe literature suggests that it may be ideal for governments to adopt a holistic policy approach to address the obesity epidemic, given the associated social conditions. \"Policy package\" recommendations have been a supported framework for preventing obesity and diet-related non-communicable diseases worldwide because they are adaptable to country-specific circumstances. For example, the NOURISHING framework summarizes key avenues for action and policy but is flexible to suit a range of national and local contexts. The World Health Organization has called for governments to have multi-faceted interventions, focusing on food security, food safety, healthy lifestyle, and nutrition. Given the scope of the pandemic but the diverse place-based trends and risk factors, appropriate and adequate intervention calls for policy change across multiple levels - population and individual - and the need for international collaboration. At the same time, evaluations of programs and initiatives on their impact on obesity are necessary to both enhance efficacy of existing interventions and provide a foundation for future interventions.\n\nCase studies for individual nations are plentiful. The BRICS countries are specifically studied in great depth because of their rapidly transitioning economies, but more slowly developing nations are well studied too.\n\nCase studies in the United States and United Kingdom are particularly bountiful.\n\nReports based in Latin America, Asia, the Middle East, North Africa, and developed areas of sub-Saharan Africa can be found in a wide range of academic literature.\n\nWorldwide, Aboriginal populations have experienced radical changes in diet. Traditional diets and food intakes have been replaced by diets consisting of foods high in fat, sugar and salt. This change in diet is related to the life-style changes during the last century: for example, Hunter-gatherer communities became more settled, and traditional food gathering methods changed. The nutrition transition has been linked to increased rates of non-communicable diseases amongst Aboriginal populations. Industrialization introduced a less complicated way to access food; a protein rich diet was replaced by white bread, processed food and sugary beverages.\n\nTraditional food of First Nations included burbot filet (or muscle) and moose liver. Food consumption provided essential fats (i.e., fatty acids) and proteins that played a key medicinal role in the prevention and reduction of obesity and obesity-related diseases.\n\n"}
{"id": "32679414", "url": "https://en.wikipedia.org/wiki?curid=32679414", "title": "Patrick Blair (surgeon)", "text": "Patrick Blair (surgeon)\n\nPatrick Blair, M.D. (c.1680–1728) was a Scottish surgeon and botanist, a Fellow of the Royal Society from 1712.\n\nHe was born at Dundee, where he practised as a doctor. He was introduced to Hans Sloane by Charles Preston in 1705. Being a nonjuror and Jacobite, he was imprisoned as a suspect at the time of the 1715 Jacobite Rising. He subsequently moved to London, then settled at Boston, Lincolnshire.\n\nIn 1706 Blair dissected and mounted the bones of an elephant, and contributed a description, under the title of \"Osteographia Elephantina\", to the Royal Society of London, published in 1713. He delivered some discourses before the Royal Society on the sexes of flowers.\n\nBlair published \"Miscellaneous Observations on the Practice of Physick, Anatomy, and Surgery\" in 1718, \"Botanick Essay\" in 1720, and \"Pharmaco-botanologia\" in 1723–8, which ended with the letter H. His \"Botanick Essays\" were his major work. In them he expounded the progress of the classification of plants up to his time, and the then novel views as to the sexual characters of flowering plants, adding his own observations.\n\n"}
{"id": "43336040", "url": "https://en.wikipedia.org/wiki?curid=43336040", "title": "Paul Connett", "text": "Paul Connett\n\nPaul Connett is a prominent water fluoridation critic, executive director of the Binghamton, New York based \"Fluoride Action Network\" (FAN), one of the largest organizations opposing water fluoridation worldwide. Connett has been invited by environmental organizations opposing fluoridation to come and lecture on the subject in fluoridating countries such as Canada, Israel, Australia and New Zealand. Connett has stated \"It’s politics that is interfering with science in this issue...It’s a matter of political will, and you cannot change political will if you don’t get the people. We must involve the people.\"\n\nConnett is a graduate of Cambridge University. He holds a Ph.D. in chemistry from Dartmouth College. After teaching chemistry and toxicology for 23 years at St. Lawrence University, Canton, NY, he retired from his full professorship. He is currently also the director of the \"American Environmental Health Studies Project\" (AEHSP).\n\nIn 2004, Connett published the paper \"50 Reasons to Oppose Fluoridation\". In 2010 he coauthored; \"The Case against Fluoride: How Hazardous Waste Ended Up in Our Drinking Water and the Bad Science and Powerful Politics That Keep It There\" along with Dr. James Beck and Dr. H. Spedding Micklem.\nHe also wrote the book in 2013; \"The Zero Waste Solution\". and assisted the city of Naples in pursuing its zero waste strategy.\n"}
{"id": "35330387", "url": "https://en.wikipedia.org/wiki?curid=35330387", "title": "Refugee children", "text": "Refugee children\n\nNearly half of all refugees are children, and almost one in three children living outside their country of birth is a refugee. These numbers encompass children whose refugee status has been formally confirmed, as well as children in refugee-like situations.\n\nIn addition to facing the direct threat of violence resulting from conflict, forcibly displaced children also face various health risks, including: disease outbreaks and long-term psychological trauma, inadequate access to water and sanitation, nutritious food, and regular vaccination schedules. Refugee children, particularly those without documentation and those who travel alone, are also vulnerable to abuse and exploitation. Although many communities around the world have welcomed them, forcibly displaced children and their families often face discrimination, poverty, and social marginalization in their home, transit, and destination countries. Language barriers and legal barriers in transit and destination countries often bar refugee children and their families from accessing education, healthcare, social protection, and other services. Many countries of destination also lack intercultural supports and policies for social integration. Such threats to safety and well-being are amplified for refugee children with disabilities.\n\nThe Convention on the Rights of the Child, the most widely ratified human rights treaty in history, includes four articles that are particularly relevant to children involved in or affected by forced displacement:\nStates Parties to the Convention are obliged to uphold the above articles, regardless of a child's migration status. As of November 2005, a total of 192 countries have become States Parties to the Convention. Somalia and the United States are the only two countries that have not ratified it.\n\nThe United Nations 1951 Convention on the Status of Refugees is a comprehensive and rigid legal code regarding the rights of refugees at an international level and it also defines under which conditions a person should be considered as a refugee and thus be given these rights. The Convention provides protection to forcibly displaced persons who have experienced persecution or torture in their home countries. For countries that have ratified it, the Convention often serves as the primary basis for refugee status determination, but some countries also utilize other refugee definitions, thus, have granted refugee status not based exclusively on persecution. For instance, the African Union has agreed on a definition at the 1969 Refugee Convention, that also accommodates people affected by external aggression, occupation, foreign domination, and events seriously disturbing public order. South Africa has granted refugee status to Mozambicans and Zimbabweans following the collapse of their home countries’ economies.\n\nOther international legal tools for the protection refugee children include two of the Protocols supplementing the United Nations Convention against Transnational Organized Crime which reference child migration:\n\nAdditionally the International Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families covers the rights of the children of migrant workers in both regular and irregular situations during the entire migration process.\n\nRefugee experiences can be categorized into three stages of migration: home country experiences (pre-migration), transit experiences (transmigration), and host country experiences (post-migration). However, the large majority of refugees do not travel into new host countries, but remain in the transmigration stage, living in refugee camps or urban centres waiting to be able to return home.\n\nThe pre-migration stage refers to home country experiences leading up to and including the decision to flee. Pre-migration experiences include the challenges and threats children face that drive them to seek refuge in another country. Refugee children migrate, either with their families or unaccompanied, due to fear of persecution on the premise of membership of a particular social group, or due to the threat of forced marriage, forced labor, or conscription into armed forces. Others may leave to escape famine or in order to ensure the safety and security of themselves and their families from the destruction of war or internal conflict.\nA 2016 report by UNICEF found that, by the end of 2015, five years of open conflict in the Syrian Arab Republic had forced 4.9 million Syrians out of the country, half of which were children. The same report found that, by the end of 2015, more than ten years of armed conflict in Afghanistan had forced 2.7 million Afghans beyond the country's borders; half of the refugees from Afghanistan were children. During times of war, in addition to being exposed to violence, many children are abducted and forced to become soldiers. According to an estimate, 12,000 refugee children have been recruited into armed groups within South Sudan. War itself often becomes a part of the child's identity, making reintegration difficult once he or she is removed from the unstable environment.\n\nExamples of children's pre-migration experiences: \n\nIn general, children may also cross borders for economic reasons, such as to escape poverty and social deprivation, or some children may do so to join other family members already settled in another State. But it is the involuntary nature of refugees' departure that distinguishes them from other migrant groups who have not undergone forced displacement. Refugees, and even more so their children, are neither psychologically nor pragmatically prepared for the rapid movement and transition resulting from events outside their control. Any direct or witnessed forms of violence and sexual abuse may characterize refugee children's pre-migration experiences.\n\nThe transmigration period is characterized by the physical relocation of refugees. This process includes the journey between home countries and host countries and often involves time spent in a refugee camp. Children may experience arrest, detention, sexual assault, and torture during their translocation to the host country. Children, particularly those who travel on their own or become separated from their families, are likely to face various forms of violence and exploitation throughout the transmigration period. The experience of traveling from one country to another is much more difficult for women and children, because they are more vulnerable to assaults and exploitation by people they encounter at the border and in refugee camps.\n\nSmuggling, in which a smuggler illegally moves a migrant into another country, is a pervasive issue for children travelling both with and without their families. While fleeing their country of origin, many unaccompanied children end up travelling with traffickers who may attempt to exploit them as workers. Including adults, sex trafficking is more prevalent in Europe and Central Asia, whereas in East Asia, South Asia, and the Pacific labour trafficking is more prevalent.\n\nMany unaccompanied children fleeing from conflict zones in Moldova, Romania, Ukraine, Nigeria, Sierra Leone, China, Afghanistan or Sri Lanka are forced into sexual exploitation. Especially vulnerable groups include girls belonging to single-parent households, unaccompanied children, children from child-headed households, orphans, girls who were street traders, and girls whose mothers were street traders. While refugee boys have been identified as the main victims of exploitation in the labor market, refugee girls aged between 13 and 18 have been the main targets of sexual exploitation. In particular, the number of young Nigerian women and girls brought into Italy for exploitation has been increasing: it was reported that 3,529 Nigerian women, among them underage girls, arrived by sea between January and June 2016. Once they reached Italy, these girls worked under conditions of slavery, for periods typically ranging from three to seven years.\n\nChildren may be detained in prisons, military facilities, immigration detention centers, welfare centers, or educational facilities. While detained, migrant children are deprived of a range of rights, such as the right to physical and mental health, privacy, education, and leisure. And many countries do not have a legal time limit for detention, leaving some children incarcerated for indeterminate time periods. Some children are even detained together with adults and subjected to a harsher, adult-based treatment and regimen.\n\nIn North Africa, children travelling without legal status are frequently subjected to extended periods of immigration detention. Children held in administrative detention in Palestine only receive a limited amount of education, and those held in interrogation centers receive no education at all. In two of the prisons visited by Defense for Children International Palestine, education was found to be limited to two hours a week. It has also been reported that child administrative detainees in Palestine do not receive sufficient food to meet their daily nutritional requirements.\n\nDocumented cases of child detention are available for more than 100 countries, ranging from the highest to the lowest income nations. Even so, a growing number of countries, including both Panama and Mexico, prohibit the detention of child migrants. And Yemen has adopted a community-driven approach, using small-group alternative care homes for child refugees and asylum-seekers, as a more age-appropriate way of detention. In the United States unaccompanied children are placed in single purpose non-secure “children’s shelters” for immigration violations, rather than in juvenile detention facilities. However, this change has not ended the practice of administrative detention entirely.\nAlthough there is commitment by the Council of Europe to work toward ending the detention of children for migration control purposes, asylum-seeking and migrant children and families often undergo detention experiences that conflict with international commitments.\n\nSome refugee camps operate at levels below acceptable standards of environmental health; overcrowding and a lack of wastewater networks and sanitation systems are common.\n\nHardships of a refugee camp may also contribute to symptoms following a refugee child's discharge from a camp. A small number of Cuban refugee children and adolescents, who were detained in a refugee camp, were assessed months after their release, and it was found that 57 percent of the youth exhibited moderate to severe posttraumatic stress disorder (PTSD) symptoms. Unaccompanied girls at refugee camps may also face harassment or assault from camp guards and fellow male refugees. In addition to having poor infrastructure and limited support services, there are a few refugee camps that can present danger to refugee children and families by housing members of armed forces. Also, at a few refugee camps, militia forces may try to recruit and abduct children.\n\nThe third stage, host country experiences, is the integration of refugees into the social, political, economic, and cultural framework of the host country society. The post-migration period involves adaptation to a new culture and re-defining one's identity and place in the new society. This stress can be exacerbated when the children arrive in the host country and are expected to adapt quickly to a new setting.\n\nIt is only a minority of refugees who travel into new host countries and who are allowed to start a new life there. Most refugees are living in refugee camps or urban centres waiting to be able to return home. For those who are starting a new life in a new country there are two options:\nAsylum seekers are people who have formally applied for asylum in another country and who are still waiting for a decision on their status. Once they have received a positive response from the host government, they will legally be considered as refugees. Refugees, like citizens of the host country, have the rights to education, health, and social services, whereas asylum seekers do not.\n\nFor instance, the majority of refugees and migrants who arrived in Europe in 2015 through mid-2016 were accommodated in overcrowded transit centers and informal settlements, where privacy and access to education and health services were often limited. In some accommodation centers in Germany and Sweden, where asylum seekers stayed until their claims were processed, separate living spaces for women, as well as sex-separated latrines and shower facilities, were unavailable.\n\nUnaccompanied children face particular difficulties throughout the asylum process. They are minors who are separated from their families once they reach the host country, or minors who decide to travel from their home countries to a foreign country without a parent or guardian. More children are traveling alone, with nearly 100,000 unaccompanied children in 2015 filing claims for asylum in 78 countries. Bhabha (2004) argues that it is more challenging for unaccompanied children than adults to gain asylum, as unaccompanied children are usually unable to find appropriate legal representation and stand up for themselves during the application process. In Australia, for instance, unaccompanied children, who usually do not have any kind of legal assistance, must prove beyond any reasonable doubt that they are in need of the country's protection. Many children do not have the necessary documents for legal entry into a host country, often avoiding officials due to fear of being caught and deported to their home countries. Without documented status, unaccompanied children often face challenges in acquiring education and healthcare in many countries. These factors make them particularly vulnerable to hunger, homelessness, and sexual and labor exploitation. Displaced youth, both male and female, are vulnerable to recruitment into armed groups. Unaccompanied children may also resort to dangerous jobs to meet their own survival needs. Some may also engage in criminal activity or drug and alcohol abuse. Girls, to a larger extent than boys, are vulnerable to sexual exploitation and abuse, both of which can have far-reaching effects on their physical and mental health.\n\nThird country resettlement refers to the transfer of refugees from the country they have fled to another country that is more suitable to their needs and that has agreed to grant them permanent settlement. Currently the number of places available for resettlement is less than the number needed for children for whom resettlement would be most appropriate. Some nations have prioritized children at risk as a category for resettlement:\n\nThe United States established its Unaccompanied Refugee Minor Program in 1980 to support unaccompanied children for resettlement. The Office of Refugee Resettlement (ORR) by the Department of Homeland Security currently works with state and local service providers to provide unaccompanied refugee children with resettlement and foster care services. This service is guaranteed to unaccompanied refugee minors until they reach the age of majority or until they are reunited with their families.\n\nSome European nations have established programs to support the resettlement and integration of refugee children. The European countries admitting the most refugee children in 2016 via resettlement were the United Kingdom (2,525 refugee children), Norway (1,930), Sweden (915), and Germany (595). Together, these accounted for 66% of the child resettlement admissions to all of Europe. The United Kingdom also established a new initiative in 2016 to support the resettlement of vulnerable refugee children from the Middle East and North Africa, regardless of family separation status. It was reported in February 2017 that this program has been partially suspended by the government; the program would no longer accept refugee youth with \"complex needs,\" such as those with disabilities, until further notice.\nRefugee children without caretakers have a greater risk of exhibiting psychiatric symptoms of mental illnesses following traumatic stress. Unaccompanied refugee children display more behavioral problems and emotional distress than refugee children with caretakers. Parental well-being plays a crucial role in enabling resettled refugees to transition into a new society. If a child is separated from his/her caretakers during the process of resettlement, the likelihood that he/she will develop a mental illness increases.\n\nThis section covers health throughout the different stages of the refugee experience.\n\nRefugee children arriving in the United States often come from countries with a high prevalence of undernutrition. Nearly half of a sample of refugee children who arrived to the American state of Washington, the majority of which were from Iraq, Somalia, and Burma, were found to have at least one form of malnutrition. In the under five age range refugee children had significantly higher rates of wasting syndrome and stunted growth, as well as a lower prevalence of obesity, in comparison to low-income non-refugee children.\n\nHowever, some time after they arrived in the United States and Australia, many refugee children demonstrated an increasing rate of overnutrition. An Australian study, assessing the nutritional status of 337 sub-Saharan African children aged between three and 12 years, found that the prevalence rate for overweight amongt refugee children was 18.4%. The prevalence rate of overweight and obesity among refugee children in Rhode Island, increased from 17.3% at initial measurement at first arrival to 35.4% at measurement three years after.\n\nBut the nutritional profiles of refugee children also often vary by their country of origin. A study involving Syrian refugee children in Jordanian refugee camps found them to be on average more likely overweight than acutely malnourished. The low prevalence of acute malnutrition among them was attributed, at least partly, to UNICEF's infant and child feeding interventions, as well as to the distribution of food vouchers by the World Food Programme (WFP).\n\nAmong newly arrived refugees in Washington state, significantly higher rates of obesity were observed among Iraqi children, whereas higher rates of stunting were found among Burmese and Somali children. The latter also had higher rates of wasting. Such variation in the nutrition profiles of refugee children may be explained by the variance in refugees' location and time in transition.\n\nCommunicable diseases are a pervasive issue faced by refugee children in camps and other temporary settlements. Governments and organizations are working to address a number of them, such as measles, rubella, diarrhea, and cholera. Refugee children often arrive in the United States from countries with a high prevalence of infectious disease.\n\nMeasles has been a major cause of child deaths in refugee camps and among internally displaced people; measles also exacerbates malnutrition and vitamin A deficiency. Some countries, such as Kenya, have developed preventative, detective, and curative programs to specifically target measles within the refugee children population. Kenya has reached over 20 million children with a measles and rubella immunization campaign carried out at the national level in May 2016. In 2017 the Kenya Ministry of Health even reported a routine vaccination coverage of 95 percent in the Dadaab refugee camp. As of April 2017, in response to the first confirmed cases of measles in the camp, UNICEF and UNHCR have collaborated with the Kenya Ministry of Health to swiftly implement an integrated measles vaccination program in Dadaab. The campaign, which has been targeting children aged six to 14 years, also includes screening, treatment referrals for cases of malnutrition, vitamin A supplementation, and deworming.\n\nDiarrhea, acute watery diarrhea, and cholera can also put children's lives at risk. Countries, such as Bangladesh, have identified the introduction and development of proper sanitation habits and facilities as potential solutions to these medical conditions. A 2008 study comparing refugee camps in Bangladesh reported that camps with sanitation facilities had cholera rates of 16%, whereas camps without such facilities had cholera rates that were almost three times higher. In a single week in 2017, 5,011 cases of diarrhea in refugee camps in Cox's Bazar in Bangladesh were reported. In response, UNICEF started a year-long cholera vaccination campaign in October 2017, targeting all children in the camps. At health centers in the refugee camps, UNICEF has been screening for potential cholera cases and providing oral rehydration salts. Community-based health workers are also going around the camps to share information on the risks of acute watery diarrhea, the cholera vaccination campaign, and the importance and necessity of good hygiene practices.\n\nDuring all points of the refugee experience, refugee children are often at risk of developing several noncommunicable diseases and conditions, such as lead poisoning, obesity, type 2 diabetes, and pediatric cancer.\n\nMany refugee children come to their host countries with elevated blood lead levels; others encounter lead hazards once they have resettled. A study published in January 2013 found that the blood lead levels of refugee children who had just arrived to the state of New Hampshire were more than twice as likely to be above 10 µg/dL as the blood lead levels of children born in the United States. Evidence from the Centers for Disease Control and Prevention (CDC) in the United States also found that nearly 30% of 242 refugee children in New Hampshire developed elevated blood lead levels within three to six months of their arrival to the United States, even though their levels were not found to be elevated at initial screening. A more recent study reported that refugee children in Massachusetts were 12 times more likely to have blood lead levels over 20 µg/dL a year after an initial screening than non-refugee children of the same age and living in the same communities.\n\nA study analyzing the medical records of former refugees residing in Rochester, New York between 1980 and 2012 demonstrated that former child refugees may be at increased risk of obesity, type 2 diabetes, and hypertension following resettlement.\n\nMany Afghan children lack access to urban diagnosis centers in Pakistan; those who do have access have been found to have various types of cancer. It is also estimated that, within Turkey's Syrian refugee population, 60 to 100 children are diagnosed with cancer each year. Overall, the incidence rate of pediatric cancers among Turkey's Syrian refugee population was similar to that of Turkish children. The study additionally noted, however, that most refugee children affected by cancer were diagnosed when the tumor was already at an advanced stage. This could indicate that refugee children and their families often face obstacles such as poor prognoses, language barriers, financial problems, and social problems in adapting to a new setting.\n\nTraditionally, the mental health of children experiencing conflict is understood in terms of either post-traumatic stress disorder (PTSD) or toxic stress. Prolonged and constant exposure to stress and uncertainty, characteristic of a war environment may result in toxic stress that children express with a change in behavior that may include anxiety, self-harm, aggressiveness or suicide. A 2017 study conducted in Syria by Save the Children determined that 84% of all adults and most children considered ongoing bombing and shelling to be the main psychological stressor, while 89% said that children were more fearful as the war progressed, and 80% said that children had become more aggressive. These stressors are leading causes of the symptoms described above, which lead to diagnosis of PTSD and toxic stress, among other mental conditions. These issues may then be further exacerbated by a forced migration to a foreign country, and the beginning of the process of refugee status determination.\n\nRefugee children are extremely vulnerable during migration and resettlement, and may experience long-term pathological effects, due to \"disrupted development time.\" Psychoanalysts of refugee health have proposed that refugee children experience mourning for their culture and countries, despite the fact that the war-torn state of their homes is unsafe. This sudden loss of familiarity places children at a greater risk for mental dysfunction. In addition, studies have shown that refugee children show a higher vulnerability to stress when separated from their families. Studies from treatment facilities and small community samples have confirmed that refugee youth are at higher risk for psychopathologic disorders, including post-traumatic stress disorder, depression, conduct disorder, and problems resulting from substance abuse. However, it is important to note that other large-scale community surveys have found that the rate of psychiatric disorder among immigrant youth is not higher than that of native-born children. Nonetheless, experiments have shown that these adverse outcomes can be prevented through adequate protective factors, such as social support and intimacy. Additionally, effective adaptation strategies, such as absorption in work and creation of pseudofamilies, have led to successful coping in refugees. Many refugee populations, particularly Southeast Asian, undergo a secondary migration to larger communities of kinfolk from their countries of origin, which serve as social support networks for refugees. Research has shown that family reunification, formation of new social groups, community groups, and social services and professional support have contributed to successful resettlement of refugees.\n\nRefugees can be stigmatized if they encounter mental health deficiencies prior to and during their resettlement into a new society. Differences between parental and host country values can create a rift between the refugee child and his/her new society. Less exposure to stigmatization lowers the risk of refugee children developing PTSD.\n\nCognitive and structural barriers make it difficult to determine the medical service utilization rates and patterns of refugee children. A better understanding of these barriers will help improve mental healthcare access for refugee children and their families.\n\nMany refugees develop a mistrust of authority figures due to repressive governments in their country of origin. Fear of authority and a lack of awareness regarding mental health issues prevent refugee children and their families from seeking medical help. Certain cultures use informal support systems and self-care strategies to cope with their mental illnesses, rather than rely upon biomedicine. Language and cultural differences also complicate a refugee's understanding of mental illness and available healthcare.\n\nOther factors that delay refugees from seeking medical help are: \n\nA broad spectrum of translation services are available to all refugees, but only a small number of those services are government-sponsored. Community health organizations provide a majority of translation services, but there are a shortage of funds and available programs. Since children and adolescents have a greater capacity to adopt their host country's language and cultural practices, they are often used as linguistic intermediaries between service providers and their parents. This may result in increased tension in family dynamics where culturally sensitive roles are reversed. Traditional family dynamics in refugee families disturbed by cultural adaptation tend to destabilize important cultural norms, which can create a rift between parent and child. These difficulties cause an increase of depression, anxiety and other mental health concerns in culturally-adapted adolescent refugees.\n\nRelying on other family members or community members has equally problematic results where relatives and community members unintentionally exclude or include details relevant to comprehensive care. Healthcare practitioners are also hesitant to rely on members of the community because it is breaches confidentiality. A third party present also reduces the willingness of refugees to trust their healthcare practitioners and disclose information. Patients may receive a different translator for each of their follow-up appointments with their mental healthcare providers, which means that refugees need to recount their story via multiple interpreters, further compromising confidentiality.\n\nCulturally competent care exists when healthcare providers have received specialized training that helps them to identify the actual and potential cultural factors informing their interactions with refugee patients. Culturally competent care tends to prioritize the social and cultural determinants contributing to health, but the traditional Western biomedical model of care often fails to acknowledge these determinants.\n\nTo provide culturally competent care to refugees, mental healthcare providers should demonstrate some understanding of the patient's background, and a sensitive commitment to relevant cultural manners (for example: privacy, gender dynamics, religious customs, and lack of language skills). The willingness of refugees to access mental healthcare services rests on the degree of cultural sensitivity within the structure of their service provider.\n\nThe protective influence exercised by adult refugees on their child and adolescent dependents makes it unlikely that young adult-accompanied refugees will access mental healthcare services. Only 10-30 percent of youth in the general population, with a need for mental healthcare services, are currently accessing care. Adolescent ethnic minorities are less likely to access mental healthcare services than youth in the dominant cultural group.\n\nParents, caretakers and teachers are more likely to report an adolescent's need for help, and seek help resources, than the adolescent. Unaccompanied refugee minors are less likely to access mental healthcare services than their accompanied counterparts. Internalizing complaints (such as depression and anxiety) are prevalent forms of psychological distress among refugee children and adolescents.\n\nAdditional structural deterrents for refugees:\n\nStructural deterrents for healthcare professionals:\n\nThe World Association of Girl Guides and Girl Scouts (WAGGGS) and Family Health International (FHI) have designed and piloted a peer-centered education program for adolescent refugee girls in Uganda, Zambia, and Egypt. The goal of the program was to reach young women who were interested in being informed about reproductive health issues. The program was split into three age-specific groups: girls aged seven to 10 learned about bodily changes and anatomy; girls aged 11 to 14 learned about sexually transmitted diseases; girls aged 15 and older focused on tips to ensure a healthy pregnancy and to properly care for a baby. According to qualitative surveys, increased self-esteem and greater use of health services among the program's participants were the largest benefits of the program.\n\nThis section covers education throughout the different stages of the refugee experience. The report, \"Left Behind: Refugee Education in Crisis,\" compares UNHCR sources and statistics on refugee education with data on school enrollment around the world provided by UNESCO, the United Nations Educational, Scientific, and Cultural Organization. The report notes that, globally, 91 percent of children attend primary school. For all refugees, that figure is at 61 percent. Specifically in low-income countries, less than 50 percent of refugees are able to attend primary school. As refugee children get older, school enrollment rates drop: only 23 percent of refugee adolescents are enrolled in secondary school, versus the global figure of 84 percent. In low-income countries, nine percent of refugees are able to go to secondary school. Across the world, enrollment in tertiary education stands at 36 percent. For refugees, the percentage remains at one percent.\n\nAdapting to a new school environment is a major undertaking for refugee children who arrive in a new country or refugee camp. Education is crucial for the sufficient psychosocial adjustment and cognitive growth of refugee children. Due to these circumstances, it is important that educators consider the needs, obstacles, and successful educational pathways for children refugees.\n\nGraham, Minhas, and Paxton (2016) note in their study that parents' misunderstandings about educational styles, teachers' low expectations and stereotyping tendencies, bullying and racial discrimination, pre-migration and post-migration trauma, and forced detention can all be risk factors for learning problems in refugee children. They also note that high academic and life ambition, parents' involvement in education, a supportive home and school environment, teachers' understanding of linguistic and cultural heritage, and healthy peer relationships can all contribute to a refugee child's success in school.\n\nSchools in North America lack the necessary resources for supporting refugee children, particularly in negotiating their academic experience and in addressing the diverse learning needs of refugee children. Complex schooling policies that vary by classroom, building and district, and procedures that require written communication or parent involvement intimidate the parents of refugee children. Educators in North America typically guess the grade in which refugee children should be placed because there is not a standard test or formal interview process required of refugee children.\nThe ability to enroll in school and continue one's studies in developing countries is limited and uneven across regions and settings of displacement, particularly for young girls and at the secondary levels. The availability of sufficient classrooms and teachers is low and many discriminatory policies and practices prohibit refugee children from attending school. Educational policies promoting age-caps can also be harmful to refugee children.\n\nMany refugee children face legal restrictions to schooling, even in countries of first asylum. This is the case especially for countries that have not signed the 1951 Refugee Convention or its 1967 Protocol. In countries where they lack official refugee status, refugee children are unable to enroll in national schools. In Kuala Lumpur, Malaysia, unregistered refugee children described being hesitant to go to school, due to risk of encountering legal authorities at school or while on the way to and from school.\n\nStudent-teacher ratios are very high in most refugee schools, and in some countries, these ratios are nearly twice the UNCHR guideline of 40:1. Although global policies and standards for refugee settings endorse child-centered teaching methods that promote student participation, teacher-centered instruction often predominates in refugee classrooms. Teachers lecture for the majority of the time, offering few opportunities for students to ask questions or engage in creative thinking. In eight refugee-serving schools in Kenya, for example, lecturing was the primary mode of instruction.\n\nRefugee children who live in large urban centers in North America have a higher rate of success at school, particularly because their families have greater access to additional social services that can help address their specific needs. Families who are unable to move to urban centers are at a disadvantage.\n\nAcculturation stress occurs in North America when families expect refugee youth to remain loyal to ethnic values while mastering the host culture in school and social activities. In response to this demand, children may over-identify with their host culture, their culture of origin, or become marginalized from both. Insufficient communication due to language and cultural barriers may evoke a sense of alienation or \"being the other\" in a new society. The clash between cultural values of the family and popular culture in mainstream Western society leads to the alienation of refugee children from their home culture.\n\nMany Western schools do not address diversity among ethnic groups from the same nation or provide resources for specific needs of different cultures (such as including halal food in the school menu). Without successfully negotiating cultural differences in the classroom, refugee children experience social exclusion in their new host culture. The presence of racial and ethnic discrimination can have an adverse effect on the well-being of certain groups of children and lead to a reduction in their overall school performance. For instance, cultural differences place Vietnamese refugee youth at a higher risk of pursuing disruptive behaviour. Contemporary Vietnamese American adolescents are prone to greater uncertainties, self-doubts and emotional difficulties than other American adolescents. Vietnamese children are less likely to say they have much to be proud of, that they like themselves as they are, that they have many good qualities, and that they feel socially accepted.\n\nClasses for refugees, more often than not, are taught in the host-country language. Refugees in the same classroom may also speak several different languages, requiring multiple interpretations; this can slow the pace of overall instruction. Refugees from the Democratic Republic of Congo living in Uganda, for example, had to transition from French to English. Some of these children were placed in lower-level classes due to their lack of English proficiency. Many older children therefore had to repeat lower-level classes, even if they had already mastered the content. Using the language of one ethnic group as the instructional language may threaten the identity of a minority group.\n\nThe content of the curriculum can also act as a form of discrimination against refugee children involved in the education systems of first asylum countries. Curricula often seem foreign and difficult to understand to refugees who are attending national schools alongside host-country nationals. For instance, in Kakuma refugee camp in Kenya, children described having a hard time understanding concepts that lacked relevance to their lived experiences, especially concepts related to Kenyan history and geography. Similarly, in Uganda, refugee children from the Democratic Republic of Congo studying together with Ugandan children in government schools did not have opportunities in the curriculum to learn the history of their home country. The teaching of one-sided narratives, such as during history lessons, can also threaten the identity of students belonging to minority groups.\n\nAlthough high-quality education helps refugee children feel safe in the present and enable them to be productive in the future, some do not find success in school. Other obstacles may include:\n\nNorth American schools are agents of acculturation, helping refugee children integrate into Western society. Successful educators help children process trauma they may have experienced in their country of origin while supporting their academic adjustment. Refugee children benefit from established and encouraged communication between student and teacher, and also between different students in the classroom. Familiarity with sign language and basic ESL strategies improves communication between teachers and refugee children. Also, non-refugee peers need access to literature that helps educate them on their refugee classmates experiences. Course materials should be appropriate for the specific learning needs of refugee children and provide for a wide range of skills in order to give refugee children strong academic support.\n\nEducators should spend time with refugee families discussing previous experiences of the child in order to place the refugee child in the correct grade level and to provide any necessary accommodations School policies, expectations, and parent's rights should be translated into the parent's native language since many parents do not speak English proficiently. Educators need to understand the multiple demands placed on parents (such as work and family care) and be prepared to offer flexibility in meeting times with these families.\n\nTeachers can make the transition to a new school easier for refugee children by providing interpreters. Schools meet the psychosocial needs of children affected by war or displacement through programs that provide children with avenues for emotional expression, personal support, and opportunities to enhance their understanding of their past experience. Refugee children benefit from a case-by-case approach to learning, because every child has had a different experience during their resettlement. Communities where the refugee populations are bigger should work with the schools to initiate after school, summer school, or weekend clubs that give the children more opportunities to adjust to their new educational setting.\n\nBicultural integration is the most effective mode of acculturation for refugee adolescents in North America. The staff of the school must understand students in a community context and respect cultural differences. Parental support, refugee peer support, and welcoming refugee youth centers are successful in keeping refugee children in school for longer periods of time. Education about the refugee experience in North America also helps teachers relate better with refugee children and understand the traumas and issues a refugee child may have experienced.\n\nRefugee children thrive in classroom environments where all students are valued. A sense of belonging, as well as ability to flourish and become part of the new host society, are factors predicting the well-being of refugee children in academics. Increased school involvement and social interaction with other students help refugee children combat depression and/or other underlying mental health concerns that emerge during the post-migration period.\n\nImplemented by UNICEF from 2012 to 2016 and funded by the Government of the Netherlands, Peacebuilding, Education, and Advocacy (PBEA) was a program that tested innovative education solutions to achieve peacebuilding results. The PBEA program in Kenya's Dadaab refugee camp aimed to strengthen resilience and social cohesion in the camp, as well as between refugees and the host community. The initiative was composed of two parts: the Peace Education Programme (PEP), an in-school program taught in Dadaab's primary schools, and the Sports for Development and Peace (SDP) program for refugee adolescents and youth. There was anecdotal evidence of increased levels of social cohesion from participation in PEP and potential resilience from participation in SDP.\n\nPeace education for refugee children may also have limitations and its share of opponents. Although peace education from past programs involving non-refugee populations reported to have had positive effects, studies have found that the attitudes of parents and teachers can also have a strong influence on students' internalization of peace values. Teachers from Cyprus also resisted a peace education program initiated by the government. Another study found that, while teachers supported the prospect of reconciliation, ideological and practical concerns made them uncertain about the effective implementation of a peace education program.\n\nChildren with disabilities frequently suffer physical and sexual abuse, exploitation, and neglect. They are often not only excluded from education, but also not provided the necessary supports for realizing and reaching their full potential.\n\nIn refugee camps and temporary shelters, the needs of children with disabilities are often overlooked. In particular, a study surveying Bhutanese refugee camps in Nepal, Burmese refugee camps in Thailand, Somali refugee camps in Yemen, the Dadaab refugee camp for Somali refugees in Kenya, and camps for internally displaced persons in Sudan and Sri Lanka, found that many mainstream services failed to adequately cater to the specific needs of children with disabilities. The study reported that mothers in Nepal and Yemen have been unable to receive formulated food for children with cerebral palsy and cleft palates. The same study also found that, although children with disabilities were attending school in all surveyed countries, and refugee camps in Nepal and Thailand have successful programs that integrate children with disabilities into schools, all other surveyed countries have failed to encourage children with disabilities to attend school. Similarly, Syrian parents consulted during a four-week field assessment conducted in northern and eastern Lebanon in March 2013 reported that, since arriving in Lebanon, their children with disabilities had not been attending school or engaging in other educational activities. In Jordan, too, Syrian refugee children with disabilities identified lack of specialist educational care and physical inaccessibility as the main barriers to their education.\n\nLikewise, limited attention is being given to refugee children with disabilities in the United Kingdom. It was reported in February 2017 that its government has decided to partially suspend the Vulnerable Children's Resettlement Scheme, originally set to resettle 3,000 children with their families from countries in the Middle East and North Africa. As a result of this suspension, no youth with complex needs, including those with disabilities and learning difficulties, would be accepted into the program until further notice.\n\nCountries may often overlook refugee children with disabilities with regards to humanitarian aid, because data on refugee children with disabilities are limited. Roberts and Harris (1990) note that there is insufficient statistical and empirical information on disabled refugees in the United Kingdom. While it was reported in 2013 that 26 percent of all Syrian refugees in Jordan had impaired physical, intellectual, or sensory abilities, such data specifically for children do not exist.\n\n\n"}
{"id": "6563970", "url": "https://en.wikipedia.org/wiki?curid=6563970", "title": "Rodrigo Roncero", "text": "Rodrigo Roncero\n\nRodrigo Roncero, also known as \"RoRo\" (born 16 February 1977, in Buenos Aires), is a retired Argentine former rugby union player. The last team in which he played was Stade Français in the Top 14. He has also played for Gloucester Rugby in the Guinness Premiership from 2002 to 2004. Roncero also played for Argentina, usually as a prop. Whilst at Gloucester he was a replacement in the 2003 Powergen Cup Final in which Gloucester defeated Northampton Saints.\n\nLike his team mate Felipe Contepomi, Rodrigo is a qualified doctor.\n\nRoncero made his first appearance for Argentina on 15 September 1998 in a match against Japan. He made three appearances in 2002 as well. Roncero was included in a mid-week Pumas side to play South Africa A, that Marcelo Loffreda was using to determine part of his squad for the 2003 Rugby World Cup. He was included in the Pumas' 2003 Rugby World Cup squad, playing in matches against Namibia and Romania.\n\nHe was capped five times for Argentina in 2004, including in a match against the All Blacks. Roncero played another five times for Argentina in 2005. He also played in Argentina's 2006 mid-year series against Wales and the All Blacks.\n\nIn 2007 he was selected to join the Argentina squad for the 2007 Rugby World Cup which succeed in gaining Argentina's highest world cup finish of third place.\n\nHe played five internationals in 2008 and five in 2009.\n\nIn the issue of Rugby World magazine published in December 2009 (and dated January 2010), the columnist Stephen Jones ranked Roncero as the ninth best player in the world at the time of his writing.\n\nRodrigo was part of Argentine squad for the 2011 Rugby World Cup in New Zealand.\n\nHe has developed a reputation as one of the world game's most destructive scrummagers. On 6 October 2012, Roncero played his last game in international rugby against the Wallabies during The Rugby Championship in his country.\n\n"}
{"id": "5560256", "url": "https://en.wikipedia.org/wiki?curid=5560256", "title": "Sisters of St. Francis of Maryville", "text": "Sisters of St. Francis of Maryville\n\nThe Sisters of St. Francis of Maryville (postnominal initials: S.S.M.) was a Roman Catholic religious congregation for women based in Maryville, Missouri, which followed the Rule of the Third Order of St. Francis. Their ministry was primarily one of medical care, to which end they founded hospitals in Missouri and Oklahoma.\n\nThe congregation was founded in 1894 when Mother Mary Augustine Giesen led six other sisters to Maryville from the St. Louis, Missouri motherhouse of the Sisters of St. Mary. They then became independent of that congregation, with Mother Augustine as the first Superior General.\n\nThey founded St. Francis Hospital, the only hospital in the town and one of only two hospitals in the vast Platte Purchase area of northwest Missouri north of St. Joseph, Missouri (the other hospital is in Fairfax, Missouri).\n\nIn 1947, the order built its motherhouse, with its landmark yellow steeple, on a bluff overlooking the One Hundred and Two River. In 1963 they opened the Mount Alverno Academy for high school girls on land adjacent to the motherhouse. The high school closed in 1971.\n\nIn 1985 the congregation merged back with its parent congregation of the Sisters of St. Mary to form the Franciscan Sisters of Mary. The headquarters was moved to St. Louis.\n\nConcerned Maryville residents seeking to preserve the landmark motherhouse tower sought various uses for it. In 1995 the Missouri Department of Corrections bought the grounds with the motherhouse and school for the minimum security Maryville Treatment Center which began operations in 1996.\n\nThe congregation's hospitals now are operated as the SSM Health Care Institute.\n\n\n"}
{"id": "34291646", "url": "https://en.wikipedia.org/wiki?curid=34291646", "title": "Smoking in Norway", "text": "Smoking in Norway\n\nSmoking in Norway is banned indoors in public buildings and aboard aircraft or other means of public transport. In addition, it is also illegal to smoke in outdoor locations that are close to children's schools and hospitals, also its illegal to advertise, promote or sponsor any tobacco products to the public; however, this law does not apply to tobacconist shops which are allowed to advertise tobacco related products. \n\nThe legal age to buy tobacco is 18 years in Norway, but 10%-12% of 15-year-olds smoke daily or weekly, and 31% of adults smoke daily or occasionally. The overall proportion of smokers is decreasing. \n\nSweden was the only European country to achieve the World Health Organization goal of less than 20% daily smoking prevalence among adults by year 2000. \nIn Norway in 2008, approximately 17% of adult men used snus daily or occasionally, while 4% of adult women used snus daily or occasionally. In secondary schools in 2000-2004, 21% of boys and 4% of girls used snus daily or occasionally. Many people both smoke and use snus.\n\nThe proportion of smokers is higher among immigrants to Norway than among ethnic Norwegians. The highest proportion of smokers can be found among immigrants from Turkey, Iran, Vietnam and Pakistan.\n\n"}
{"id": "45363494", "url": "https://en.wikipedia.org/wiki?curid=45363494", "title": "Solvatten", "text": "Solvatten\n\nSOLVATTEN is a combined portable water treatment and solar water heater system that has been designed for use at the household level in the developing world. The device uses natural UV radiation to treat water, and units are capable of rendering highly contaminated water drinkable (as defined by the WHO safe drinking water standards.) in a few hours, provided there is sufficient sunlight.\n\nSOLVATTEN incorporates three water treatment processes: filtration, pasteurisation and UV sterilisation. Under optimal conditions, the device can eliminate all pathogenic material in 10-litres of water within 2 hours, allowing for multiple batches of water to be treated in a given day.\n\nThe device is typically used in situations where water resources are scarce and prone to contamination, but it has also been applied in disaster relief scenarios. The device and its inventor, Petra Wadström, have both achieved recognition through a series of national and international awards.\n\nEach SOLVATTEN unit consists of two five-litre containers that have transparent surfaces and can be filled with water. The transparent surfaces of the containers are made from a plastic that allows the penetration of a large portion of UV light, specifically UV-B which is highly effective at destroying microorganisms. Water is poured in through an opening that houses a filter of 35 microns to remove larger particles\n\nOnce filled, the unit is placed in direct sunlight, which simultaneously heats the water and exposes it to ultraviolet radiation. The combination of these is an effective means of purifying water and, depending on conditions, the water will be free of pathogenic material in 2–6 hours. The water is heated to between 55 and 75 °C, making it suitable for a number of other household and hygiene purposes, such as hand washing, bathing and domestic cleaning.\n\nA study has shown that the use of SOLVATTEN is associated with health improvements in communities that are dependent upon contaminated water resources. It showed that it eliminated 100% of pathogenic material from water sourced from contaminated wells, whilst its use was also associated with as 60% reduction in diarrhoeal infections in children under five years old.\n\nThe study, conducted in Mali, showed that households using SOLVATTEN reduced their combustion of firewood and charcoal by an average of 0.5 and 0.9 kg/day respectively. This had additional economic benefits for the user, with average savings of 15.8 USD/month. A separate research exercise calculated the social return on investment (SROI) of SOLVATTEN, which accounts for extra-financial value (e.g. environmental and social value not reflected in conventional financial accounts) relative to resources invested. In that study, the SROI was calculated as being 1:26, meaning that for every unit of currency invested in SOLVATTEN, 26 were created\n\nSOLVATTEN was designed by the Swedish inventor and environmentalist Petra Wadström. Wadström has been recognised as one of Sweden’s most prestigious modern entrepreneurs and environmentalists, whilst the SOLVATTEN device itself has been acknowledged as a significant innovation. Below is a list of some of the awards received by both Wadström and her invention.\n"}
{"id": "49966987", "url": "https://en.wikipedia.org/wiki?curid=49966987", "title": "The Firm (brand)", "text": "The Firm (brand)\n\nThe FIRM (stylized as The FIRM) is a brand of exercise videos and equipment owned by Gaiam. It is best known for popularizing a hybrid of aerobic exercise and weight training.\n\nIn 1979, Anna Benson founded the exercise studio The Body Firm (later The FIRM Studios) in Charleston and Columbia, South Carolina. She recruited her sister Cynthia Benson and husband Mark Henriksen under the Meridian Films label to co-produce a series of exercise videos filmed there between 1986 and 1994. It quickly established itself as a competitor to personalities like Jane Fonda.\n\nA lawsuit brought against Meridian Films in 2001 over royalties forced The FIRM to be sold to GoodTimes Entertainment. Gaiam purchased GoodTimes in 2005 and subsequently expanded The FIRM, focusing on women's fitness.\n\nAnna Benson started Fitness Favorites, which became the official online store for the original videos after her death in 2009. After her death, Anna's son became owner of the Classic The FIRM and has released Anna's 'classic' DVDs from VHS format. The FIRM Studios was renamed The Flex Body/The FLEX in 2015 and is owned by Emily Welsh. In 2018, Anna Benson's Fitness Favorites renamed The FIRM (classic) to Anna L. Benson's The Body Firm. Now, Fitness Favorites is also streaming workouts.\n\n"}
{"id": "38866302", "url": "https://en.wikipedia.org/wiki?curid=38866302", "title": "Top of the Lake", "text": "Top of the Lake\n\nTop of the Lake is a mystery drama television series created and written by Jane Campion and Gerard Lee, and directed by Campion and Garth Davis. It aired in 2013, and the sequel, entitled \"Top of the Lake: China Girl\", in 2017. It marks Campion's first work for television since \"An Angel at My Table\" in 1990. \n\nSeason 1 follows Detective Robin Griffin (Elisabeth Moss) and deals with her investigation of the disappearance of a pregnant 12-year-old girl in New Zealand. Season 2, \"China Girl\", is set in Sydney four years later, as Detective Griffin investigates the death of an unidentified Asian girl found at Bondi Beach. \n\n\"Top of the Lake\" was co-produced for BBC Two in the UK, BBC UKTV in Australia and New Zealand, and Sundance Channel in the United States. It has been generally very well received.\n\nElisabeth Moss plays the central role of Robin Griffin, a Sydney detective specialising in sexual assault, in both series. Additional cast members are as follows:\n\n\n\n\n\n\n\nDavid Wenham returns as Al in one episode. Kip Chapman, Jacqueline Joe, Byron Coll and Cohen Holloway also reprise their \"Top of the Lake\" roles in a flashback sequence, with Mark Leonard Winter appearing as Johnno.\n\nActress Jennifer Ehle auditioned for the role of GJ, which went to Holly Hunter. Jane Campion originally offered the role of Robin to Anna Paquin, who had worked with her on \"The Piano\" (1993). Paquin declined due to her pregnancy, and the role went to American Elisabeth Moss.\n\nThe series was originally intended as a co-production with the Australian Broadcasting Corporation. But after Moss was cast as Robin, the network pulled their funding before production began, citing a prior agreement to put an Australian actress in the lead. Australian-based channel UKTV, owned by BBC Worldwide, filled the funding gap left by the ABC. Philippa Campbell was the New Zealand-based producer.\n\nFilming took 18 weeks and was shot entirely on location in Queenstown and Glenorchy, in Otago, on the South Island of New Zealand. While Queenstown is referred to during the series, Glenorchy doubles as the fictitious town of Laketop. The scenes in the women's commune were filmed at Moke Lake.\n\nIn early 2013, co-creator Jane Campion said that \"Top of the Lake\" comes to a distinct ending, and there would be no additional series. Despite this, it was announced in October 2014 that the series had been renewed for a second season. \"China Girl\" began shooting on location in Sydney, Australia in December 2015.\n\nCampion returned as co-writer and co-director. Gerard Lee returned as co-writer. The original co-director, Garth Davis, was replaced by Ariel Kleiman due to scheduling conflicts. Philippa Campbell returned as producer. Actress Nicole Kidman joined the cast for \"China Girl\", which is the second time she has worked with Campion. Kidman \"plays an Australian mother, Julia, whose story dovetails with that of Detective Robin Griffin\", played by Elisabeth Moss. Christie, a fan of Campion's \"The Piano\", joined the cast after sending a letter through a mutual friend.\n\n\"Top of the Lake\" screened in its entirety at the January 2013 Sundance Film Festival, in a single seven-hour session with one intermission and a break for lunch. This was the first such screening in the history of the festival. \"Top of the Lake\" was additionally shown at the 63rd Berlin International Film Festival.\n\nThe US premiere was on the Sundance Channel on 18 March 2013, in Australia on BBC UKTV on 24 March 2013, and in New Zealand on 25 March, also on BBC UKTV.\n\n\"China Girl\" was screened in its entirety at the May 2017 Cannes Film Festival. In the UK, it premiered on BBC Two on 27 July 2017, and the entire series was released on the BBC iPlayer immediately afterwards. In the US, it premiered in September 2017, on Sundance TV, and each episode will be available on Hulu the day after its screening on SundanceTV. In Canada, \"China Girl\" premiered on 25 October 2017, on CBC Television.\n\nReviews of \"Top of the Lake\" have been positive, referring to the series as \"masterfully made\", \"beautiful\", \"mysterious\", \"riveting\", and \"a masterpiece\". It received a score of 86 out of 100 from Metacritic and a score of 93 per cent from Rotten Tomatoes.\n\nThere were also some less positive reviews. Mike Hale of \"The New York Times\" criticized the \"elaborately introduced plotlines\" and described Tui's disappearance as \"less a story element than a metaphor for the kind of armed resistance to male hegemony that constitutes the central idea of Ms Campion’s body of work.\"\n\n\"Top of the Lake\" was lauded by feminist critics for its explicit effort to analyse rape culture as well as its radical construction of narratives entirely foregrounding the experiences of single women.\n\n"}
{"id": "23582220", "url": "https://en.wikipedia.org/wiki?curid=23582220", "title": "Uterotubal junction", "text": "Uterotubal junction\n\nThe uterotubal junction or utero-tubal junction is the connection between the endometrial cavity of the uterus and the fallopian tube (uterine tube) at the proximal tubal opening, the beginning of the intramural part of the fallopian tube. Histologically, the endometrial epithelium changes over to the ciliated tubal epithelium.\n\nPatency of the utero-tubal junction is necessary for normal reproduction. The tubes can get blocked here by infection (salpingitis) and surgical intervention may be necessary.\nMouse studies have indicated that selective passage of individual spermatozoa may occur at this junction, with abnormal morphology being identified as a significant selection criterion, leading to predominantly normal sperm passing towards the ovum. Absence of the protein calmegin has also been suggested as a critical factor for reliable sperm passage.\n\nThe utero-tubal junction is accessible by hysteroscopy and the entry point for tubal cannulation and falloposcopy. Contraceptive methods have been developed to block the utero-tubal junction.\n"}
{"id": "55945310", "url": "https://en.wikipedia.org/wiki?curid=55945310", "title": "Voluntary Assisted Dying Act 2017 (Victoria)", "text": "Voluntary Assisted Dying Act 2017 (Victoria)\n\n<!--**************************************************************\n"}
{"id": "42758135", "url": "https://en.wikipedia.org/wiki?curid=42758135", "title": "World Health Academy", "text": "World Health Academy\n\nThe World Health Academy (WHA) is an international non-governmental organization specializing in global health. It serves as a representative body for physicians worldwide, developing health policy, advancing continuing education programs, and speaking as a unified voice for medicine in international advocacy. The WHA is headquartered in Florence, Italy and led by Robert A. Schwartz, of Rutgers University.\n\nThe WHA seeks to achieve optimal health for all people across borders, uniting the medical field to harness the collective expertise and power of its members. As the medical profession’s global and authoritative voice, the WHA assists governments and national medical associations in jointly tackling health problems and improving the welfare of their citizens.\n\nThe \"International Journal of Medicine\" is an open access peer-reviewed scientific journal published by the World Health Academy. It covers primary research and secondary research from any discipline within medicine. The \"IJM\" Editorial Board consist of Nobel Prize laureates, Lasker Award recipients, and other prominent persons. \"Dermatologic Therapy\", a peer-reviewed scientific journal published by Wiley-Blackwell, is also an official journal of the WHA.\n\n\n"}
