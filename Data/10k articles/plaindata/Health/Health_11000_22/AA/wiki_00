{"id": "38036756", "url": "https://en.wikipedia.org/wiki?curid=38036756", "title": "100K Pathogen Genome Project", "text": "100K Pathogen Genome Project\n\nThe 100K Pathogen Genome Project was launched in July 2012 by Bart Weimer (UC Davis) as an academic, public, and private partnership. It aims to sequence the genomes of 100,000 infectious microorganisms to create a database of bacterial genome sequences for use in public health, outbreak detection, and bacterial pathogen detection. This will speed up the diagnosis of foodborne illnesses and shorten infectious disease outbreaks.\n\nThe 100K Pathogen Genome Project is a public-private collaborative project to sequence the genomes of 100,000 infectious microorganisms. The 100K Genome Project will provide a roadmap for developing tests to identify pathogens and trace their origins more quickly.\n\nPartners announced in the launch of the project were UC Davis, Agilent Technologies, and the US Food and Drug Administration, with the US Centers for Disease Control and Prevention and the US Department of Agriculture noted as collaborators. As the project has proceeded, the partnership has evolved to include or replace these founding partners. The 100K Pathogen Genome Project was selected by the IBM/Mars Food Safety Consortium for metagenomic sequences.\n\nThe 100K Pathogen Genome Project is conducting high-throughput next-generation sequencing (NGS) to investigate the genomes of targeted microorganisms, with whole genome sequencing to be carried out on a small number of microorganisms for use as a reference genome. Most bacterial strains will be sequenced and assembled as draft genomes; however, the project has also produced closed genomes for a variety of enteric pathogens in the 100K bioproject. Data from this project is also available for download at the 100K Pathogen Genome Project website.\n\nThis strategy enables worldwide collaboration to identify sets of genetic biomarkers associated with important pathogen traits. This five-year microbial pathogen project will result in a free, public database containing the sequence information for each pathogen's genome. The completed gene sequences will be stored in the National Institutes of Health (NIH)'s National Center for Biotechnology Information (NCBI)'s public database. Using the database, scientists will be able to develop new methods of controlling disease-causing bacteria in the food chain.\n\n"}
{"id": "4641407", "url": "https://en.wikipedia.org/wiki?curid=4641407", "title": "AIDS Clinical Trials Group", "text": "AIDS Clinical Trials Group\n\nThe AIDS Clinical Trials Group network (ACTG) is one of the largest HIV clinical trials organizations in the world, playing a major role in setting standards of care for HIV infection and opportunistic diseases related to HIV and AIDS in the United States and the developing world. The ACTG is composed of, and directed by, leading clinical scientists in HIV/AIDS therapeutic research. The ACTG is funded by the Department of Health and Human Services, National Institutes of Health through the National Institute of Allergy and Infectious Diseases.\n\nThrough innovative studies of the treatment of HIV-1 infection and its complications, ACTG research focuses on:\n\n\nThe ACTG has been pivotal in providing the data necessary for the approval of therapeutic agents, as well as treatment and prevention strategies, for many opportunistic infections and malignancies.\n\n"}
{"id": "7756492", "url": "https://en.wikipedia.org/wiki?curid=7756492", "title": "Abstinence, be faithful, use a condom", "text": "Abstinence, be faithful, use a condom\n\nAbstinence, be faithful, use a condom, also known as the ABC strategy or abstinence-plus sex education, also known as abstinence-based sex education, is a sex education policy based on a combination of \"risk avoidance\" and harm reduction which modifies the approach of abstinence-only sex education by including education about the value of partner reduction safe sex and birth control methods. Abstinence-only sex education is strictly to promote the sexual abstinence until marriage, and does not teach about safe sex or contraceptives. The abstinence-based sex education program is meant to stress abstinence and include information on safe sex practices. In general terms, this strategy of sex education is a compromise between abstinence-only education and comprehensive sex education. The ABC approach was developed in response to the growing epidemic of HIV/AIDS in Africa, and to prevent the spread of other sexually transmitted diseases. This approach has been credited by some with the falling numbers of those infected with AIDS in Uganda, Kenya and Zimbabwe, among others. From 1990 to 2001 the percentage of Ugandans living with AIDS fell from 15% to between 5 and 6%. This fall is believed to result from the employment of the ABC approach, especially reduction in the number of sex partners, called \"Zero-Grazing\" in Uganda.\n\nAbstinence-based sex education can include issues of human relationships, the basic biology of human reproduction, safe sex methods and contraceptives, HIV/AIDS information, and masturbation in place of sex. It recommends sexual abstinence outside marriage as an ideal, having only a single long-term sexual partner. The use of condoms and other safe sex practices is advocated only if it is not possible to remain with a single sexual partner. Advocating this ideal, whilst pragmatically dealing with the fact that abstinence only sex education is ineffective by itself, has made the ABC approach popular with many African governments and relief agencies.\n\nThe ABC approach has notably been used in African countries. Versions of this approach have been used for abstinence-only sex education in Uganda, among others. Its positive impact has been confirmed by a 2009 Stanford University survey.\n\nAbstinence, be faithful, use a Condom consists of three components:\n\nStarting primarily in the 1980s and 1990s, the popularity of the \"abstinence plus\" sex education program grew into a common method of teaching students, in the United States, about sexuality. The program understood that it would not be possible to stop all teenagers from having sex, but still stressed that abstinence is the only guaranteed way of avoiding unwanted pregnancies and contraction of STDs. The most important message to teens from the program comes from Joycelyn Elders, President Clinton's first Surgeon General, that \"if they have a baby there is an 80% likelihood they'll be poor, ignorant, and slaves for the rest of their lives.\" In 1997 the Department of Health and Human Services established a \"Girl Power!\" campaign focusing on girls ages 9–14 which encouraged abstinence as a form of empowerment. For those who would reject abstinence, or have early sex forced upon them, abstinence-only messages provided little to no hope to prevent pregnancy and spread of disease. To account for this, some states included information about contraceptives in their sex education programs along with encouragement for students to be abstinent. Of the states that, in 1995, required abstinence education, fourteen also included the use of contraception within the curriculum.\n\nIn September 1995, Hawaii passed the Abstinence-Based Education Policy. (Policy # 2110). It is meant to support abstinence and help develop skills to continue abstinence, help teens who have had sexual intercourse to abstain from further, and to provide teens with information on contraceptives and methods for preventing sexually transmitted diseases and pregnancy.\n\nIn 1996, the federal government increased funding for state abstinence education as an inclusion of the Welfare Reform Act, creating Title V, and the abstinence-only-until-marriage program. The increase in abstinence funding created incentives for states to maintain these rulings, which over 20 states have.\n\nIn 2006 and 2007, $176 million was given annually to abstinence-only programs, whose central message was to delay sexual activity until marriage, preventing the inclusion of information about safe-sex practices or contraception.\n\nOn June 30, 2009, funding was cut off for abstinence-only education programs, with no further fund allocated in the 2010 FY budget, and the push for evidence-based sex education and teen pregnancy prevention programs. However, on September 29, 2009 funding was restored to abstinence-only programs, as well as leaving the decision of what sex education program would be taught in public schools up to the individual states\n\nThe state of Texas, where the law states that comprehensive sex education is allowed to be taught, but 94% of school districts have been reported to use abstinence-only sex education, developed an abstinence-plus sex education program called \"Big Decisions\", which not only emphasized abstinence to high school students, but integrated education about condom use and contraception, contradicting the majority of the school districts' decisions to maintain an abstinence-only education program\n\nUganda has been touted as the success story of the ABC strategy, with a significant reduction of HIV/AIDS infection rates and a push towards sexual behavior change. This shift in sexual behavior began in the mid 1980s, when AIDS was a global disease, resulting in the reduction of multiple sexual partners for an individual, and the increase of condom use However, there is debate over whether the ABC strategy was the sole factor that contributed to the decline in HIV/AIDS infection. There has also been debate and discourse over the emphasis of one tenet of ABC sex education over the other. Currently there is the debate over the emphasis of \"A\" over \"C\" and vis versa. Uganda's HIV/AIDS and sex education program has a greater focus on the use of condoms in preventing HIV/AIDS, however, religious institutions emphasize abstinence. The success of the ABC strategy in sex education is in part due to the fact that the branches of this strategy are \"proximate determinants of HIV infection\", also known as ways of reducing or even avoiding the risk of infection.\n\nLike Uganda, there is a push to change the sexual behaviors, however, this message is being emphasized by religious leaders rather than state officials. The three tenets of ABC are broken down in Malawi into groups. Abstinence is emphasized for individuals who are young and unmarried, who hear messages about AIDS regularly; Be faithful is emphasized with married couples whose religious leader \"polices\" their sexual behavior; Condom use is a contested area with religious leaders due to the fact that religious openly denounce condom use, however some leaders have privately consulted individuals on condom use, resulting in many members doing so. However, religious leaders do not comment on the risks and issues of AIDS, which can potentially put an entire congregation at risk.\n\nMany communities that have adopted the ABC strategy of sex education have used this strategy of sex education in congruence with religious or ideological influences. In Malawi, religious leaders influence and emphasize the use this strategy\n\nHowever, in the United States, ideologies influence the use of ABC strategy. In this case, political ideologies influence the use of various sex education programs. For example, conservatism had previously taught young women that sex was about danger, not pleasure, focusing on an abstinence-only education program, reinforcing the foundational conservative ideas of fear, and that society is fragile and human nature is inherently bad. Political ideologies have long been influences in the debate over sex education programs, pushing political agendas.\n\nGlobally, the debate of sex education divides liberals and conservatives. In many countries, children do not receive proper sex education, and discourage the act of sex itself\n\nThe usefulness of the ABC approach is highly debated. The three elements are interpreted differently by different actors and critics argue that often abstinence and faithfulness are unduly promoted over condoms and other measures such as education, female empowerment and making available modern antiviral drugs. For example, the U.S. President's Emergency Plan for AIDS Relief under President George W Bush has been criticized for seeming to prioritize \"A\" and \"B\" over \"C\" within its funding criteria. \"C\" activities may only be directed at \"high-risk\" groups, and not to the general population. However, donor funding has always been allocated overwhelmingly to condoms, reflecting clear US and European policy priorities, including under George W Bush.\n\nCritics argue that in many countries women are frequently infected by their unfaithful husbands while being faithfully married, and thus women who follow the recommendations of ABC promoters face an increased risk of HIV infection. Condoms, needles, and negotiation is a proposed alternative approach as is \"SAVE\" (Safer practices, Available medication, Voluntary testing & counseling and Empowerment through education).\n\nCritics furthermore allege that the strategy overlooks the epidemic's social, political, and economic causes and \"vulnerable populations\", e.g. sex workers and \"those who lack the ability to negotiate safe sex\" as well as risk groups such as homosexuals and intravenous drug users. However, most infections in Africa occur outside these vulnerable groups, and ABC was a US donor policy only for the \"generalized\" epidemics in Africa. Murphy et al. found that Uganda's ABC approach empowered women. \"Remarkably, in the 2000–2001 Uganda DHS, 91 percent of women said they could refuse sex with their husbands if they knew their husbands had STIs, a somewhat higher percentage than in several other African countries\" \n\nCritics also argue that using the word \"abstinence,\" then teaching about safe sex and contraceptives, can be contradictory.\n\nThere is also the argument of the gendered presentation of ABC success stories. Research has indicated that the power roles in which men and women fall in the gender dynamic of relationships, as well as sexual double standards, sexual violence, and harmful cultural practices affect a greater number of women when trying to implement HIV/AIDS prevention through individual decision making. Critiques of geographic location are also relevant in the success of ABC success. Migration patterns within a population affect both men and women where men who migrate are more likely to contract the infection and bring it back and infect their female partner, whose greatest risk of contracting HIV is from their husbands extramarital sexual encounters, but women are also seen contracting the disease outside of their primary relationship, focusing the ABC strategy on morality and \"static individualized behavior\".\n\nHow sex education should be taught in public schools in the United States has been a topic for debate since the sexual revolution. A majority of the debate is focused on whether or not there should be a comprehensive sex education program or an abstinence-only program.\n\nAbstinence-only sex education programs have been proven to be ineffective and there is little evidence showing that abstinence-only, or abstinence-only-until-marriage programs lower risky sexual behavior in teens\n\nThere is also evidence to support a parental push for earlier sex education for students, starting in elementary school rather than middle school. Along with the push for earlier sex education, there is the call for age-appropriate sex education. This means that for elementary school students, the sex education they receive will be tailored to their age. A study conducted in 2013 found that parents who pushed for earlier sex education settled on the sex education elementary school students receive will be 89% communication skills, 65% human anatomy/reproductive information, 61% abstinence, 53% HIV/STD infection information, and 52% gender/sexual orientation issues. Parents in this study also supported a greater emphasis on sex education in middle schools and an increase in the teaching of the same topics from elementary school as well as a greater emphasis on birth control and condom use\n\nPope Benedict XVI has criticized some harm reduction policies with regards to HIV/AIDS, saying that \"if the soul is lacking, if Africans do not help one another, the scourge [of HIV] cannot be resolved by distributing condoms; quite the contrary, we risk worsening the problem\". Whilst this position has been widely denounced for misrepresenting and oversimplifying the role of condoms in preventing infections, there has been expert scientific opinion and evidence supporting it. \nArchbishop Gabriel Charles Palmer-Buckle of Accra has stated that \"the Catholic Church [offers] three methods to help solve this problem of AIDS in Africa: \"A\", abstain; \"B\", be faithful; \"C\", chastity, which is in consonance with traditional African values. Those Planned Parenthood people are only talking about condoms. By the way, they know full well that the condoms devoted to Africa are sub-standard.\" There are no reliable sources that condoms distributed in Africa are inferior to those elsewhere in the world.\n\nA major proponent of the ABC approach, author and member of Presidential Advisory Council on HIV and Aids in 2003-2007, Harvard University's Edward C Green, stated “Advocates of the ABCs often use the term to mean a primary emphasis on abstinence/delay of sexual debut and faithfulness/partner reduction, with condom use being a secondary but necessary strategy for those who do not or cannot practice abstinence or fidelity.\" Furthermore, Green in his seminal book Rethinking AIDS Prevention (Praeger 2003) argued that the success in Uganda, where prevalence feel 21% to 6% between 1989-2003 was largely due to the \"B\" of the ABC approach, fidelity or reduction in multiple partners. This conclusion was validated and expanded to underscore the dangers of concurrent sexual partners by Helen Epstein, in The Invisible Cure \n\n"}
{"id": "22570610", "url": "https://en.wikipedia.org/wiki?curid=22570610", "title": "Active mobility", "text": "Active mobility\n\nActive mobility, active travel, active transport or active transportation, is a form of transport of people and sometimes goods, that only uses the physical activity of the human being for the locomotion. The most known forms of active mobility are walking or cycling, though other mobility means such as the skateboard, kick scooter or roller skates are also a form of active mobility. In certain latitudes and elevations, practical transportation may also include cross-country skiing and snowshoeing, perhaps only in the winter season. \n\nThe academical literature evidences that public policies which promote active mobility tend to increase health indicators by increasing the levels of physical fitness and reducing the rates of obesity and diabetes, whilst also reducing the consumption of fossil fuels and consequent particulates, Nitrous Oxide and Carbon emissions.\n\nSerious health and environmental problems, especially global climate change due to fossil fuel usage and the continued increase in obesity, are a current political concern. A House of Commons of the United Kingdom Health Committee report into Obesity in 2004 recommended promoting and facilitating cycling and walking as key components of an integrated anti-obesity strategy, suggesting \"physical activity incorporated into the fabric of everyday life\". Public Health England estimated in 2016 that in the UK, physical inactivity directly contributes to one in six deaths every year. The PHE report notes that including walking and cycling to daily routines is the most effective way to increase physical activity and reduce levels of obesity, as well as prevent cardiovascular disease, type 2 diabetes, cancer and several mental illnesses, including depression. \n\nStudies have shown that the recent global increase in levels of obesity can be attributed to the decrease in physical activity by children and adults. This is also as a result of an increase in more sedentary forms of leisure (TV, video games) and to low levels of walking and cycling. Correlational studies have shown that across socio-economic groupings in the UK caloric intake does not vary significantly, whereas activity levels and BMI do, and are closely correlated to each other.\n\nThe US Centers for Disease Control has also recommended increasing access to active transportation.\n\nIn response to the high level of sedentarian life-style and automobile usage, which have negative environmental and health effects, a recent movement has emerged led by public health and environmental campaigners to advocate for stronger policies and practices that promote active travel, and make cycling and walking safer and more attractive. The intention being that these modes could, in many instances, replace car usage for everyday journeys to school, shops, public services, etc. To facilitate this would require local planning and highway authorities to invest in ensuring safe routes are available to these destinations (danger from other road traffic is frequently cited as the primary reason for not cycling). In many areas, the current focus of development for cycling infrastructure is on isolated leisure trails, resulting in highly fragmented cycle routes and pavements/sidewalks, which do not link effectively to everyday destinations. Governments can take the initiative to establish active transportation programs and plans, such as California's Active Transportation Program (ATP), Portland, Oregon's Regional Active Transportation Plan, Fort Worth, Texas ATP and San Diego County ATP, as well as Active and Sustainable School Transportation (ASST) planning and implementation, such as in Hamilton, Ontario.\n\nIn 2012 Polis, a network of European cities and regions, published a position paper that calls upon European institutions and other European actors to take action, to ensure that the promotion of health benefits of active travel are maximised in all relevant European policies and programmes. Recommendation are based on references in European policy documents to improving health through active travel which should form the basis of shared objectives, policies, work programmes and investment to increase levels of walking and cycling.\n\nSpecific recommendations include:\n\n\nIn the United Kingdom, for example, over 50% of car journeys are under 5 km, which in theory, could be replaced by active mobility means. \n\nIn 2008, the UK Association of Directors of Public Health, with Sustrans, CTC, The Ramblers and other agencies, launched a Call to Action on Active Travel. This sets out a number of clear goals for local planning and highway authorities. If achieved, these goals could mark an effective response to the steadily increasing problem of obesity, and also help reduce carbon emissions. However, the process for monitoring progress towards these goals (beyond anecdotal good-news stories) and holding councils to account for their performance, is weak, and threatens to undermine progress towards the goals:\n\n\nThe \"Active Travel (Wales) Act 2013\" became law in Wales in 2013. The Act requires local authorities to continuously improve facilities and routes for pedestrians and cyclists and to prepare maps identifying current and potential future routes for their use. It also requires new road schemes (including road improvement) to consider the needs of pedestrians and cyclists at design stage.\n\n\n"}
{"id": "48101601", "url": "https://en.wikipedia.org/wiki?curid=48101601", "title": "Agricultural safety and health", "text": "Agricultural safety and health\n\nAgricultural safety and health is an aspect of occupational safety and health in the agricultural workplace. It specifically addresses the health and safety of farmers, farm workers, and their families.\n\nContrary to perceived belief and notions of work in the agricultural landscape, agriculture is one of the most dangerous industries in the US, with a variety of factors causing injuries and death in the workplace. Many of the injuries, long-term or short, prevalent in the occupation are hearing loss, musculoskeletal disorders, respiratory diseases, poisoning from pesticides and chemicals, reproductive issues, and many other ailments. These injuries are caused mainly by loud noises from machinery, stress from transporting heavy objects, gases and fumes like methane and from chemicals, and other various causes, respectively.\n\nAccording to a 2011 study by the National Agricultural Statistics Service, a major cause of injury and deaths in the workplace comes from farm machinery/vehicles, specifically tractor overturns. The National Institute for Occupational Safety and Health (NIOSH) has stressed that while the industry produces a necessary product needed by everyone, the industry is diminishing in prevalence and only recently, has been viewed as an occupation needed of further knowledge and development of safety measures and standards. Following a 1988 national conference held at Ohio State University and University of Ohio, the report \"Agriculture at risk\" was published in the \"Journal of Agricultural Safety and Health\". This report was funded by NIOSH, who assisted with the initiation of research and implementation regarding agricultural health issues, illnesses, and safety measures in the workplace.\n\nThe demographics behind agricultural work has been changing throughout the years, with the rise of more private, family owned farm industries, as well as the increasing prevalence of young farm workers. Young farm workers are at a greater risk than older or adult farm workers of injury, but this issue is not addressed to the full extent due to nonexistence in the statistics of the Bureau of Labor, which only regards those of age 16 and older. Increase of young farm workers could be attributed to the rising trend of \"return to farming\". Not only young workers, but women farm workers are increasing at a steady rate as the lead operators of the farm industries.\n\nWhile the exact timeline of the history behind agricultural occupational safety and health is not set, there are major dates, through the years, that led the way of increasing the importance of agricultural safety measures. The initial steps in enacting laws to improve agricultural safety measures occurred in 1878 in the Threshing Machines Act. Later on in the timeline, the Agriculture Poisonous Substances Act and Agriculture Safety, Health, and Welfare Provisions Act were enacted in 1952 and 1956, respectively, to address regulations of the use of chemicals as well as stressing safety and sanitation measures, especially for young children farm workers. The most recent event has occurred in 2008 in which the Pesticide Safety Directorate (PSD) is now under the Health and Safety Executive (HSE), with the Department for Environment, Food, and Rural Areas (DEFRA) being the previous department to oversee the PSD.\n\nThe agriculture industry is one of the most dangerous occupations and has led to thousands of deaths due to work-related injuries in the US. In 2011 the fatality rate for farmworkers was 7 times higher than that of all the workers in the private industry, a difference of 24.9 deaths for every 100,000 people as opposed to 3.5 deaths for every 100,000 people in the private industry. The National Institute for Occupational Safety and Health (NIOSH) estimated that 374 farmers and farmworkers died due to a work-related injury in 2012, tractor overturns being the number one cause death. An average of 113 youth between the ages of 16–19 years die annually from agriculture related injuries (1995-2002). About 167 farmworkers each day are affected by a lost-work-time injury in which 5% of them suffer from permanent damage. Non-fatal injuries that farmworkers are at high risk for include work-related lung problems, hearing loss due to noise, skin diseases, various cancers due to exposure to certain chemicals as well as prolonged exposure to the sun.\n\nUnlike other industries that impose labor laws and occupational safety and health regulations in the workplace, agriculture deals with diverse production, large labor force and an array of environmental conditions that makes it difficult to address.\n\nIn the United Kingdom, the first law towards agriculture safety started in 1878 with the Threshing Machines Act 1878. As the industry mechanized, the Chaff-Cuttings Machine (Accidents) Act 1897 provided slightly more statutory protection. In 1947, the Health, Welfare, and Safety in Non-Industrial Employment Hours of Employment of Juveniles: Report by a Committee of Enquiry was presented to the Parliament, leading to two acts. These two acts, the Agriculture (Poisonous Substances) Act 1952, and the Agriculture (Safety, Health, and Welfare Provisions) Act 1956. The Agriculture (Poisonous Substances) Act 1952, which protected employees against risks of poisoning, while the Agriculture (Safety, Health and Welfare Provisions) Act 1956 gave workers and children health protection and safeguards. However, it was repealed and modified in 1975 by the Agriculture (Safety, Health and Welfare Provisions) Act 1956 (Repeals and Modifications) Regulations 1975.\n\nIn the United States of America, the Occupational Safety and Health Administration (OSHA) overviews agricultural safety. as with all OSHA standards, it is covered by Section 5(a)(1) and Section 5(a)(2), which requires employers to \"furnish to each of his employees employment and a place of employment which are free from recognized hazards that are causing or are likely to cause death or serious physical harm to his employees\" and to comply with occupational safety and health standards promulgated under this act.\" Agriculture Safety is covered by the Agriculture (29 CFR 1928), which mostly covers farm equipment and operation, and the General Industry (29 CFR 1910) standards, which defines workplace safety for all industries. In addition to that, there are 28 OSHA-approved State Plans that have standards that are \"at least as effective as OSHA's and may have different or more stringent requirements\".\n\nIllnesses and injuries regarding agriculture can vary from one farm to another, according to which industry or sector the farm specializes in. This becomes more apparent as the environment of each farm is different according to these specialties, which in turn poses different areas of risk factors leading to injuries and ailments. In all though, there are similarities in the risk factors and illnesses that agricultural workers face on a day to day basis as in injuries from machinery, large animals, pesticides, respiratory illness-causing factors, musculoskeletal disorders, hearing loss, reproductive issues regarding women, and many more. The most common illnesses seem to be musculoskeletal disorders, pesticide poisoning, and respiratory diseases.\n\nMusculoskeletal disorders can arise from a number of factors, but the main causes seem to be from livestock and large machinery/equipment. Machinery are usually devoid of safety measures and pose a greater threat, since agricultural workers fix and operate these machines themselves for utilization in the fields. These machines may also be run while repairs are under way, causing even more instances of potential injuries. Bending, twisting, and stretching motions that are apparent when operating these equipment causes much back and neck strain, leading to more exacerbated conditions over time. This is in regards to not only agricultural workers operating machinery, but also workers in the fields, who experience pain and strains in the wrists, back, hips, and knees. Livestock can also pose a threat to the musculoskeletal systems of the body due to their large weight and varied behavior, possibly leading to kicking and unintentional blows by the animal to the agricultural worker. No matter the cause, it is apparent that musculoskeletal disorders are common in the agricultural industry and need to be addressed to treat the ailment as quickly as possible. In a study regarding agricultural work safety climate of approximately 300 North Carolina migrant farmworkers, 40% had musculoskeletal ailments and many reported numerous days working while ill. Many underestimated the safety measures they were able to obtain and thought that risks were inevitable due to the risks involved in agriculture. Musculoskeletal disorders are categorized into acute or first time injuries and chronic long-term ailments according to many acute injuries.\n\nChemical and pesticide use causes many health ailments as in chemical or pesticide poisoning and reproductive issues in women. Some common chemicals used in agriculture are bipyridyls, organophosphates, and carbamates. Exposure to pesticides can occur through not just the process of making pesticides, but also in other agricultural tasks like harvesting crop or irrigating fields. Part of the problem arises in the lack of more strict safety measures as well as utilization of personal protective equipment, but the use of pesticides requires care in itself of set safety measures. The United States' use of pesticides resides heavily in agriculture, with 75% being used in this industry. In a study by the California Department of Pesticide Regulation and the SENSOR- Pesticides program, regarding pesticide poisoning incidence rates out of 3,271 cases, 402 individuals had medium severity illnesses from pesticide exposure with high severity cases being rare and lower severity being more common. From this study, the pesticides that were deemed to cause most cases of diseases were cholinesterase, pyrethroids, inorganic compounds, and dithiocarbamates. Insecticides, specifically cholinesterase inhibitors (N-methyl carbamates and organophosphates) cause a majority of the illnesses in 54% of the individuals in the case. Some of the more common symptoms seen in pesticide poisoning were nervous system impairments, headaches, gastrointestinal issues, respiratory impairments, skin impairments, inflammations, and many more.\n\nRecently, agricultural respiratory diseases have been rising from development of animal production facilities causing toxic fumes to permeate the area. Much of the irritants involved in causing these diseases are ammonia, organic dust, hydrogen sulfide, bacterial microorganisms, mold, and various hydrocarbons.\n\nIncidents that occur involving farm machinery are a risk to children under the age of 13 and those who are employed in any way must be properly trained by their employer in order to use the machinery. According to the law, children under 13 cannot drive any farm machinery, and children under 16 cannot operate machinery that is equipped with mechanisms that could seriously injure them. Even if there is an adult present within the machine, children under 13 cannot sit within the cab of an agricultural vehicle. As for children or adults who are not there for labor, use of a trailer may be allowed under the condition that the trailer is in good condition, has good seating, is fitted with guard railing, and supervision is supplied at all times.\n\nChildren are also prohibited from being within the vicinity of a potentially dangerous animal. Without competent supervision, animals can exhibit behaviors that may severely injure a child in the area. Children are exposed to potential danger in the farm, so areas that chemicals are stored, reservoirs, slurry pits, grain bins, and that contain farm machinery, must be properly enclosed with fencing or locked for precaution.\n\nNonfatal injuries in agriculture that occur may permanently disable, injure, or kill youths over 13 on the farm in the United States. An estimation of $1 billion annually has been spent on nonfatal injuries to youths in agriculture. The National Institute of Occupational Safety and Health has created the Childhood Agricultural Injury Prevention Initiative for young workers and their employers to reduce the amount of injures on the job. 33,000 children have farm-related injuries each year in the United States, and 113 who are less than 20 years of age have died from farm-related injuries between 1995 and 2002.\n\nIn 1991, the Surgeon General's Conference in Des Moines, Iowa, was held to bring awareness to the risks that children face in production agriculture. A session titled \"Intervention: Safe Behaviors Among Adults and Children,\" emphasized the need for children's injury prevention. This conference was followed by the Childhood Agricultural Injury Prevention symposium in Marshfield, Wisconsin, that sought to propose policies, further education on agricultural injuries among children, and demonstrate relevant research on the topic. As a result of the symposium, the National Committee for Childhood Agricultural Injury Prevention (NCCAIP) was formed. From this, the National Action Plan was finalized over 16 months, and it addressed agricultural injuries that occurred to children.\n\nHandling livestock involves a risk of injury. Large livestock, especially, have the ability to crush the handler, and without proper training and competent workers, unrestrained cattle can seriously injure workers, visitors, and even vets. Proper handling facilities that are kept in working order are suggested when dealing with larger livestock. A suitable race and crush may be helpful, but makeshift equipment has more hazards and risk of injury. When keeping bulls, most accidents occur because of the lack of precaution when handling the bull. Because bulls are temperamental, training will be helpful when exposing the bull to others. Bulls who are willing to be trained can be taught to associate people will their needs, such as feeding, exercise, and grooming. Training will then make a less hazardous workplace if the bull is docile. At 10 months, it is suggested to ring bulls, and the ring must be inspected regularly. Competent workers and proper handling will avoid fatal injuries.\n\nExposure to animal with diseases are a risk for agricultural workers. Zoonosis are diseases that are transmitted from infected animals to humans. Most zoonotic diseases are caused by organisms such as parasites, bacteria, prions, fungi, protozoa, and viruses, which reside within the animal but can be pathogens to humans. Direct or indirect transmission from feces or bodily fluids can result in infection, and consuming animal products that are contaminated can also become a cause of infection. Because of their close contact with animals, farmers, veterinarians, ranchers, and other agricultural workers are more at risk for contracting zoonotic diseases.\n\n\n\n"}
{"id": "216861", "url": "https://en.wikipedia.org/wiki?curid=216861", "title": "Agroecosystem", "text": "Agroecosystem\n\nAn agroecosystem is the basic unit of study in agroecology, and is somewhat arbitrarily defined as a spatially and functionally coherent unit of agricultural activity, and includes the living and nonliving components involved in that unit as well as their interactions.\n\nAn agroecosystem can be viewed as a subset of a conventional ecosystem. As the name implies, at the core of an agroecosystem lies the human activity of agriculture. However, an agroecosystem is not restricted to the immediate site of agricultural activity (e.g. the farm), but rather includes the region that is impacted by this activity, usually by changes to the complexity of species assemblages and energy flows, as well as to the net nutrient balance. Traditionally an agroecosystem, particularly one managed intensively, is characterized as having a simpler species composition and simpler energy and nutrient flows than \"natural\" ecosystem. Likewise, agroecosystems are often associated with elevated nutrient input, much of which exits the farm leading to eutrophication of connected ecosystems not directly engaged in agriculture.\n\nSome major organizations are hailing farming within agro ecosystems as the way forward for mainstream agriculture. Current farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility. According to a report by the International Water Management Institute and the United Nations Environment Programme, there is not enough water to continue farming using current practices; therefore how critical water, land, and ecosystem resources are used to boost crop yields must be reconsidered. The report suggested assigning value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests, as well addressing inequities that sometimes result when such measures are adopted, such as the reallocation of water from poor to rich, the clearing of land to make way for\nmore productive farmland, or the preservation of a wetland system that limits fishing rights.\n\nForest gardens are probably the world's oldest and most resilient agroecosystem. Forest gardens originated in prehistoric times along jungle-clad river banks and in the wet foothills of monsoon regions. In the gradual process of a family improving their immediate environment, useful tree and vine species were identified, protected and improved whilst undesirable species were eliminated. Eventually superior foreign species were selected and incorporated into the family's garden.\n\nOne of the major efforts of disciplines such as agroecology is to promote management styles that blur the distinction between agroecosystems and \"natural\" ecosystems, both by decreasing the impact of agriculture (increasing the biological and trophic complexity of the agricultural system as well as decreasing the nutrient inputs/outflow) and by increasing awareness that \"downstream\" effects extend agroecosystems beyond the boundaries of the farm (e.g. the Corn Belt agroecosystem includes the hypoxic zone in the Gulf of Mexico). In the first case, polyculture or buffer strips for wildlife habitat can restore some complexity to a cropping system, while organic farming can reduce nutrient inputs. Efforts of the second type are most common at the watershed scale. An example is the National Association of Conservation Districts' Lake Mendota Watershed Project, which seeks to reduce runoff from the agricultural lands feeding into the lake with the aim of reducing algal blooms. \n\n"}
{"id": "7529374", "url": "https://en.wikipedia.org/wiki?curid=7529374", "title": "Alan Whiteside", "text": "Alan Whiteside\n\nAlan Walter Whiteside OBE (born in Nairobi, Kenya, on 18 March 1956) is a South African academic, researcher and professor at the Balsillie School of International Affairs and professor emeritus at the University of KwaZulu-Natal. He is well known for his innovative work in the field of the social impacts of HIV and AIDS.\n\nWhiteside obtained a bachelor's degree in 1978, and an MA (Development Economics) in 1980, from the School of Development Studies at the University of East Anglia. He then obtained a D.Econ. from the University of Natal (now University of KwaZulu-Natal) in 2003. Whiteside joined the university as a research fellow in 1983, and in 1998 founded the Health Economics and HIV/AIDS Research Division (HEARD). He held a chair at the University of KwaZulu-Natal, where he was executive director of HEARD. Currently, Whiteside has a position at the Balsillie School of International Affairs and Wilfrid Laurier University School of International Policy and Governance. He holds a Center for International Governance Innovation Chair in Global Health Policy. \n\nWhiteside started his career as a freelance journalist and teacher in Mbabane, Swaziland, before joining the Botswana Ministry of Finance and Development Planning as an Overseas Development Institute Fellow and a planning officer/economist in 1980.\nHe was a research fellow, senior research fellow and associate professor at the Economic Research Unit at the University of Natal, from 1983 to 1997. Whiteside was an associate and director for Capricorn Africa Economic Associates, in Mbabane, Swaziland, between 1988 and 1998.\nIn 1998 he founded the Health Economics and HIV/AIDS Research Division (HEARD) and he was subsequently promoted to professor. He was a senior research fellow at the Department for International Development(DFID) from 2009 to 2012.\n\nIn 2012 he was appointed CIGI Chair in Global Health at the Balsillie School of International Affairs.\n\nWhiteside has written many peer-reviewed articles, and has authored several books and numerous papers relating to HIV and AIDS. Of particular importance was his 2003 paper, (with fellow academic Alex de Waal), ‘New variant famine: AIDS and the food crisis in southern Africa’, in The Lancet. Among his books are the popular ‘AIDS the Challenge for South Africa’ (with Clem Sunter, 2000), and influential academic volumes such as ‘HIV/AIDS: A Very Short Introduction’, (Oxford University Press, 2008), and 'AIDS in the Twenty-First Century: Disease and Globalization' (with Tony Barnett, 2006).\nWhiteside’s work has encompassed lecturing, mentorship and training. He developed the successful ‘Planning for HIV/AIDS’ training workshop for the Overseas Development Group at the University of East Anglia. He continues to mentor and supervise many students, academics and researchers in the field. He has worked with many international donors philanthropies and the corporate sector.\n\nWhiteside was appointed Officer of the Order of the British Empire (OBE) in the 2015 New Year Honours for services to science and strategic interventions to curb HIV/AIDS.\n\nWhiteside contributes to international policy through engagement with governments, the UN system and donors. He was on the International AIDS Vaccine Initiative's Policy Advisory Committee, SIDA/Norad Regional AIDS Team Reference Group, and International AIDS Society Governing Council.\n\nHe is a member of the Waterford Kamhlaba (a United World College) Governing Council and a Trustee of the Waterford School Trust.\n\nWhiteside is a member of the Academy of Science of South Africa\n\nBetween 2003 and 2006 he was appointed by Kofi Annan to the Commission on HIV/AIDS and Governance in Africa and is Treasurer of the International AIDS Society Governing Council\n\nWhiteside serves on the editorial boards of several journals, including:\n\n\n"}
{"id": "21499348", "url": "https://en.wikipedia.org/wiki?curid=21499348", "title": "Alternative treatments used for the common cold", "text": "Alternative treatments used for the common cold\n\nAlternative treatments used for the common cold include numerous home remedies and alternative medicines. Scientific research regarding the efficacy of each treatment is generally nonexistent or inconclusive. Current best evidence indicates prevention, including hand washing and neatness, and management of symptoms.\n\nMany believe that cold temperature can cause and therefore prevention of exposure to cold temperature can prevent the common cold. Evidence does not support this association.\n\nVitamin C was identified in the early part of the previous century and there was much interest in its possible effects on various infections including the common cold. A few controlled trials on the effect of vitamin C on the common cold were carried out already in the 1940s, but the topic became particularly popular after 1970, when Linus Pauling, a double Nobel laureate, wrote a best-selling book \"Vitamin C and the Common Cold\". Pauling's book led to great interest in the topic among lay people, but also among academic circles. After Pauling's book, a number of controlled trials were carried out. However, the interest disappeared after the middle of 1970s apparently because of the publication of two reviews and one primary study, which all concluded that vitamin C does not influence the common cold. However, the three papers were later shown to be erroneous.\n\nAccording to the Cochrane review on vitamin C and the common cold, 1 g/day or more of vitamin C does not influence common cold incidence in the general community. However, in five randomized double-blind placebo-controlled trials with participants who were under heavy short-term physical stress (three of the trials were with marathon runners), vitamin C halved the incidence of colds. In the dose of 1 g/day or more, vitamin C shortened the duration of colds in adults by 8% and in children by 18%. Vitamin C also decreased the severity of colds.\n\nA systematic review by the Cochrane Collaboration, last updated in 2014, examines twenty-four randomized controlled trials studying various echinacea preparations for prevention and treatment of the common cold. Echinacea showed no benefit over placebo for prevention. Evidence for treatment was inconsistent. Reported side effects were rare.\n\n2007 meta-analyses conclude that there is some evidence that echinacea may reduce either the duration or severity of the common cold, but results are not consistent.\n\nUse of echinacea preparations is not currently recommended.\n\nIn the twelfth century, Moses Maimonides wrote, \"Chicken soup ... is recommended as an excellent food as well as\nmedication.\"\nSince then, there have been numerous reports in the United States that chicken soup alleviates the symptoms of the common cold. Even usually staid medical journals have published tongue-in-cheek articles on the alleged medicinal properties of chicken soup.\n\nA 2013 Cochrane review found tentative evidence of benefit with \"Pelargonium sidoides\" for the symptoms of the common cold; however, the quality of the evidence was very poor.\n\nMany people believe that steam inhalation reduces cold symptoms. There is no evidence suggesting that steam inhalation is effective for treating the common cold. There have been reports of children being badly burned by accidentally spilling the water used for steam inhalation.\n\nEvidence does not support a relationship between cold temperature exposure or a \"chill\" (feeling of coldness) and the common cold.\n\nZinc is tentatively linked to a shorter length of symptoms.\n"}
{"id": "32116125", "url": "https://en.wikipedia.org/wiki?curid=32116125", "title": "Amblyaudia", "text": "Amblyaudia\n\nAmblyaudia (amblyos- blunt; audia-hearing) is a term coined by Dr. Deborah Moncrieff from the University of Pittsburgh to characterize a specific pattern of performance from dichotic listening tests. Dichotic listening tests are widely used to assess individuals for binaural integration, a type of auditory processing skill. During the tests, individuals are asked to identify different words presented simultaneously to the two ears. Normal listeners can identify the words fairly well and show a small difference between the two ears with one ear slightly dominant over the other. For the majority of listeners, this small difference is referred to as a \"right-ear advantage\" because their right ear performs slightly better than their left ear. But some normal individuals produce a \"left-ear advantage\" during dichotic tests and others perform at equal levels in the two ears. Amblyaudia is diagnosed when the scores from the two ears are significantly different with the individual's dominant ear score much higher than the score in the non-dominant ear \nResearchers interested in understanding the neurophysiological underpinnings of amblyaudia consider it to be a brain based hearing disorder that may be inherited or that may result from auditory deprivation during critical periods of brain development. Individuals with amblyaudia have normal hearing sensitivity (in other words they hear soft sounds) but have difficulty hearing in noisy environments like restaurants or classrooms. Even in quiet environments, individuals with amblyaudia may fail to understand what they are hearing, especially if the information is new or complicated. Amblyaudia can be conceptualized as the auditory analog of the better known central visual disorder amblyopia. The term “lazy ear” has been used to describe amblyaudia although it is currently not known whether it stems from deficits in the auditory periphery (middle ear or cochlea) or from other parts of the auditory system in the brain, or both. A characteristic of amblyaudia is suppression of activity in the non-dominant auditory pathway by activity in the dominant pathway which may be genetically determined and which could also be exacerbated by conditions throughout early development.\n\nChildren with amblyaudia experience difficulties in speech perception, particularly in noisy environments, sound localization, and binaural unmasking (using interaural cues to hear better in noise) despite having normal hearing sensitivity (as indexed through pure tone audiometry). These symptoms may lead to difficulty attending to auditory information causing many to speculate that language acquisition and academic achievement may be deleteriously affected in children with amblyaudia. A significant deficit in a child's ability to use and comprehend expressive language may be seen in children who lacked auditory stimulation throughout the critical periods of auditory system development. A child suffering from amblyaudia may have trouble in appropriate vocabulary comprehension and production and the use of past, present and future tenses. Amblyaudia has been diagnosed in many children with reported difficulties understanding and learning from listening and adjudicated adolescents are at a significantly high risk for amblyaudia (Moncrieff, et al., 2013, Seminars in Hearing).\nFamilies report the presence of amblyaudia in several individuals, suggesting that it may be genetic in nature. It is possible that abnormal auditory input during the first two years of life may increase a child’s risk for amblyaudia, although the precise relationship between deprivation timing and development of amblyaudia is still unclear. Recurrent ear infections (otitis media) are the leading cause of temporary auditory deprivation in young children. During ear infection bouts, the quality of the signal that reaches the auditory regions of the brains of a subset of children with OM is degraded in both timing and magnitude. When this degradation is asymmetric (worse in one ear than the other) the binaural cues associated with sound localization can also be degraded. Aural atresia (a closed external auditory canal) also causes temporary auditory deprivation in young children. Hearing can be restored to children with ear infections and aural atresia through surgical intervention (although ear infections will also resolve spontaneously). Nevertheless, children with histories of auditory deprivation secondary to these diseases can experience amblyaudia for years after their hearing has been restored.\n\nAmblyaudia is a deficit in binaural integration of environmental information entering the auditory system. It is a disorder related to brain organization and function rather than what is typically considered a “hearing loss” (damage to the cochlea). It may be genetic or developmentally acquired or both. When animals are temporarily deprived of hearing from an early age, profound changes occur in the brain. Specifically, cell sizes in brainstem nuclei are reduced, the configuration of brainstem dendrites are altered and neurons respond in different ways to sounds presented to both the deprived and non-deprived ears (in cases of asymmetric deprivation). This last point is particularly important for listening tasks that require inputs from two ears to perform well. There are multiple auditory functions that rely on the computation of well calibrated inputs from the two ears. Chief among these is the ability to localize sound sources and separate what we want to hear from a background of noise. In the brainstem, the auditory system compares the timing and levels of sounds between the two ears to encode the location of sound sources (sounds that originate from our right as opposed to left side are louder and arrive earlier in our right ear). This ability to separate sound sources not only helps us locate the trajectories of moving objects, but also to separate different sound sources in noisy environments.\n\nAn electrophysiologic study demonstrated that children with amblyaudia (referred to then as a \"left-ear deficit\") were less able to process information from their non-dominant ears when competing information is arriving at their dominant ears. The N400-P800 complex showed a strong and highly correlated response from the dominant and non-dominant ears among normal children while the response from children with amblyaudia was uncorrelated and indicated an inability to separate information arriving at the non-dominant ear from the information arriving at the dominant ear. The same children also produced weaker fMRI responses from their non-dominant left ears when processing dichotic material in the scanner.\n\nA clinical diagnosis of amblyaudia is made following dichotic listening testing as part of an auditory processing evaluation. Clinicians are advised to use newly developed dichotic listening tests that provide normative cut-off scores for the listener's dominant and non-dominant ears. These are the Randomized Dichotic Digits Test and the Dichotic Words Test. Older dichotic listening tests that provide normative information for the right and left ears can be used to supplement these two tests for support of the diagnosis (). If performance across two or more dichotic listening tests is normal in the dominant ear and significantly below normal in the non-dominant ear, a diagnosis of amblyaudia can be made. The diagnosis can also be made if performance in both ears is below normal but performance in the non-dominant ear is significantly poorer, thereby resulting in an abnormally large asymmetry between the two ears. Amblyaudia is emerging as a distinct subtype of auditory processing disorder (APD).\n\nA number of computer-based auditory training programs exist for children with generalized Auditory Processing Disorders (APD). In the visual system, it has been proven that adults with amblyopia can improve their visual acuity with targeted brain training programs (perceptual learning). A focused perceptual training protocol for children with amblyaudia called Auditory Rehabilitation for Interaural Asymmetry (ARIA) was developed in 2001 which has been found to improve dichotic listening performance in the non-dominant ear and enhance general listening skills. ARIA is now available in a number of clinical sites in the U.S., Canada, Australia and New Zealand. It is also undergoing clinical research trials involving electrophysiologic measures and activation patterns acquired through functional magnetic resonance imaging (fMRI) techniques to further establish its efficacy to remediate amblyaudia.\n\n\n"}
{"id": "213070", "url": "https://en.wikipedia.org/wiki?curid=213070", "title": "Artificial insemination", "text": "Artificial insemination\n\nArtificial insemination (AI) is the deliberate introduction of sperm into a female's cervix or uterine cavity for the purpose of achieving a pregnancy through in vivo fertilization by means other than sexual intercourse. It is a fertility treatment for humans, and is common practice in animal breeding, including dairy cattle (see Frozen bovine semen) and pigs.\n\nArtificial insemination may employ assisted reproductive technology, sperm donation and animal husbandry techniques. Artificial insemination techniques available include intracervical insemination and intrauterine insemination. The beneficiaries of artificial insemination are women who desire to give birth to their own child who may be single, women who are in a lesbian relationship or women who are in a heterosexual relationship but with a male partner who is infertile or who has a physical impairment which prevents full intercourse from taking place. Intracervical insemination (ICI) is the easiest and most common insemination technique and can be used in the home for self-insemination without medical practitioner assistance. Compared with natural insemination (i.e., insemination by sexual intercourse), artificial insemination can be more expensive and more invasive, and may require professional assistance.\n\nSome countries have laws which restrict and regulate who can donate sperm and who is able to receive artificial insemination, and the consequences of such insemination. Some women who live in a jurisdiction which does not permit artificial insemination in the circumstance in which she finds herself may travel to another jurisdiction which permits it.\n\nThe first reported case of artificial insemination by donor occurred in 1884: Dr. William H. Pancoast, a professor in Philadelphia, took sperm from his \"best looking\" student to inseminate an anesthetized woman. The woman was not informed about the procedure, unlike her infertile husband. The case was reported 25 years later in a medical journal. The sperm bank was developed in Iowa starting in the 1920s in research conducted by University of Iowa medical school researchers Jerome Sherman and Raymond Bunge.\n\nIn the 1980s, direct intraperitoneal insemination (DIPI) was occasionally used, where doctors injected sperm into the lower abdomen through a surgical hole or incision, with the intention of letting them find the oocyte at the ovary or after entering the genital tract through the ostium of the fallopian tube.\nThe sperm used in artificial insemination may be provided by either the woman's husband or partner (partner sperm) or by a known or anonymous sperm donor (see sperm donation (donor sperm)).\n\nIf the procedure is successful, the woman will conceive and carry a baby to term in the normal manner. A pregnancy resulting from artificial insemination is no different from a pregnancy achieved by sexual intercourse. In all cases, the woman is the biological mother of any child produced by AI, and the male whose sperm is used is the biological father.\n\nThere are multiple methods used to obtain the semen necessary for artificial insemination. Some methods require only men, while others require a combination of a male and female. Those that require only men to obtain semen are masturbation or the aspiration of sperm by means of a puncture of the testicle and epididymus. Methods of collecting semen that involve a combination of a male and female include interrupted intercourse, intercourse with a 'collection condom', or the post-coital aspiration of the semen from the vagina.\n\nThere are a number of reasons why a woman with a male partner would use artificial insemination to achieve pregnancy. For example, a woman's immune system may be rejecting her partner's sperm as invading molecules. Women who have issues with the cervix – such as cervical scarring, cervical blockage from endometriosis, or thick cervical mucus – may also benefit from artificial insemination, since the sperm must pass through the cervix to result in fertilization.\n\nIn the case of heterosexual couples who are finding it difficult to conceive, before artificial insemination is turned to as the solution, doctors will require an examination of both the male and female involved in order to remove any and all physical hindrances that are preventing them from naturally achieving a pregnancy. The couple is also given a fertility test to determine the motility, number, and viability of the male's sperm and the success of the female's ovulation. From these tests, the doctor may or may not recommend a form of artificial insemination.\n\nTiming is critical, as the window and opportunity for fertilization is little more than twelve hours from the release of the ovum. To increase the chance of success, the woman's menstrual cycle is closely observed, often using ovulation kits, ultrasounds or blood tests, such as basal body temperature tests over, noting the color and texture of the vaginal mucus, and the softness of the nose of her cervix. To improve the success rate of AI, drugs to create a stimulated cycle may be used, but the use of such drugs also results in an increased chance of a multiple birth.\n\nSperm can be provided fresh or washed. The washing of sperm increases the chances of fertilization. Pre- and post-concentration of motile sperm is counted. Sperm from a sperm bank will be frozen and quarantined for a period, and the donor will be tested before and after production of the sample to ensure that he does not carry a transmissible disease. For fresh shipping, a semen extender is used.\n\nIf sperm is provided by a private donor, either directly or through a sperm agency, it is usually supplied fresh, not frozen, and it will not be quarantined. Donor sperm provided in this way may be given directly to the recipient woman or her partner, or it may be transported in specially insulated containers. Some donors have their own freezing apparatus to freeze and store their sperm.\n\nSemen used is used either fresh, raw, or frozen. Where donor sperm is supplied by a sperm bank, it will always be quarantined and frozen, and will need to be thawed before use. When an ovum is released, semen is introduced into the woman's vagina, uterus or cervix, depending on the method being used.\n\nSperm is occasionally inserted twice within a 'treatment cycle'.\n\nIntracervical insemination (ICI) is painless and is the easiest and most common insemination technique. It closely replicates the ejaculation of semen by the penis into the vagina during intercourse.\n\nICI involves the introduction of unwashed or raw semen into the vagina at the entrance to the cervix, usually by means of a needleless syringe. \n\nIt is the simplest type of artificial insemination, and unwashed or raw semen is normally used, but semen supplied by a donor through a sperm bank which has been prepared for IUI use may also be used. The procedure is commonly used in home, self-insemination and practitioner insemination procedures, and for insemination where semen is provided by private donors.\n\nIn order to perform an ICI insemination, air must be expelled from a needle-less syringe which is then filled with semen which has been allowed to liquify. Any further enclosed air must be removed by gently pressing the plunger forward. The woman lies on her back and the syringe is then inserted into the vagina. Care is optimal when inserting the syringe, so that the tip is as close to the entrance to the cervix as possible. A vaginal speculum may be used for this purpose. The plunger is then slowly pushed forward and the semen in the syringe is gently emptied deep into the vagina. It is important that the syringe is emptied slowly for best results. The syringe may be left in place for several minutes before removal. The woman can bring herself to orgasm so that the cervix 'dips down' into the pool of semen, again replicating closely vaginal intercourse, and this may improve the success rate. The woman is advised to lie still for about half-an-hour to improve the success rate.\n\nOne insemination during a cycle is usually sufficient. Additional inseminations may not improve the chances of a pregnancy.\n\nOrdinary sexual lubricants should not be used in the process, but special fertility or 'sperm-friendly' lubricants can be used for increased ease and comfort.\n\nWhen performed at home without the presence of a professional, aiming the sperm in the vagina at the neck of the cervix may be more difficult to achieve and the effect may be to 'flood' the vagina with semen, rather than to target it specifically at the entrance to the cervix. This procedure is therefore sometimes referred to as intravaginal insemination (IVI). Sperm supplied by a sperm bank will be frozen and must be allowed to thaw before insemination. The sealed end of the straw itself must be cut off and the open end of the straw is usually fixed straight on to the tip of the syringe, allowing the contents to be drawn into the syringe. Sperm from more than one straw can generally be used in the same syringe. Where fresh semen is used, this must be allowed to liquefy before inserting it into the syringe, or alternatively, the syringe may be back-loaded.\n\nA conception cap, which is a form of conception device, may be inserted into the vagina following insemination and may be left in place for several hours. Using this method, a woman may go about her usual activities while the cervical cap holds the semen in the vagina close to the entrance to the cervix. Advocates of this method claim that it increases the chances of conception. One advantage with the conception device is that fresh, non-liquefied semen may be used. The partner or donor may ejaculate straight into the cap and this can be immediately inserted into the vagina. Other methods may be used to insert semen into the vagina notably involving different uses of a conception cap. This may, for example, be inserted filled with sperm which does not have to be liquefied. The male may therefore ejaculate straight into the cap. Alternatively, a specially designed conception cap with a tube attached may be inserted empty into the vagina after which liquefied semen is poured into the tube. These methods are designed to ensure that donor or partner semen is inseminated as close as possible to the cervix and that it is kept in place there to increase the chances of conception.\n\nIntrauterine insemination (IUI) involves injection of washed sperm into the uterus with a catheter. If unwashed semen is used, it may elicit uterine cramping, expelling the semen and causing pain, due to content of prostaglandins. (Prostaglandins are also the compounds responsible for causing the myometrium to contract and expel the menses from the uterus, during menstruation.) Resting on the table for fifteen minutes after an IUI is optimal for the woman to increase the pregnancy rate.\n\nUnlike ICI, intrauterine insemination normally requires a medical practitioner to perform the procedure. A female under 30 years of age has optimal chances with IUI; for the man, a TMS of more than 5 million per ml is optimal. In practice, donor sperm will satisfy these criteria. A promising cycle is one that offers two follicles measuring more than 16 mm, and estrogen of more than 500 pg/mL on the day of hCG administration. A short period of ejaculatory abstinence before intrauterine insemination is associated with higher pregnancy rates. However, GnRH agonist administration at the time of implantation does not improve pregnancy outcome in intrauterine insemination cycles according to a randomized controlled trial.\n\nIUI is a more efficient method of artificial insemination than ICI and, because of its generally higher success rate, is usually the insemination procedure of choice for single women and lesbians using donor semen in a fertility centre and who are less likely to have fertility issues of their own. Enabling the sperm to be inserted directly into the womb will produce a better chance of conceiving.\n\nIt is also a method used by couples using donor sperm in a fertility centre.\n\nIUI can be used in conjunction with controlled ovarian hyperstimulation (COH). Still, advanced maternal age causes decreased success rates; women aged 38–39 years appear to have reasonable success during the first two cycles of ovarian hyperstimulation and IUI. However, for women aged over 40 years, there appears to be no benefit after a single cycle of COH/IUI. Medical experts therefore recommend considering in vitro fertilization after one failed COH/IUI cycle for women aged over 40 years.\n\nA double intrauterine insemination theoretically increases pregnancy rates by decreasing the risk of missing the fertile window during ovulation. However, a randomized trial of insemination after ovarian hyperstimulation found no difference in live birth rate between single and double intrauterine insemination.\n\nIntrauterine tuboperitoneal insemination (IUTPI) involves injection of washed sperm into both the uterus and fallopian tubes. The cervix is then clamped to prevent leakage to the vagina, best achieved with a specially designed double nut bivalve (DNB) speculum. The sperm is mixed to create a volume of 10 ml, sufficient to fill the uterine cavity, pass through the interstitial part of the tubes and the ampulla, finally reaching the peritoneal cavity and the Pouch of Douglas where it would be mixed with the peritoneal and follicular fluid. IUTPI can be useful in unexplained infertility, mild or moderate male infertility, and mild or moderate endometriosis. In non-tubal sub fertility, fallopian tube sperm perfusion may be the preferred technique over intrauterine insemination.\n\nIntratubal insemination (ITI) involves injection of washed sperm into the fallopian tube, although this procedure is no longer generally regarded as having any beneficial effect compared with IUI. ITI however, should not be confused with gamete intrafallopian transfer, where both eggs and sperm are mixed outside the woman's body and then immediately inserted into the fallopian tube where fertilization takes place.\n\nThe pregnancy or success rates for artificial insemination are 10 to 15% per menstrual cycle using ICI, and 15–20% per cycle for IUI. In IUI, about 60 to 70% have achieved pregnancy after 6 cycles.\n\nHowever, these pregnancy rates may be very misleading, since many factors, including the age and health of the recipient, have to be included to give a meaningful answer, e.g. definition of success and calculation of the total population. For couples with unexplained infertility, unstimulated IUI is no more effective than natural means of conception.\n\nThe pregnancy rate also depends on the total sperm count, or, more specifically, the total motile sperm count (TMSC), used in a cycle. The success rate increases with increasing TMSC, but only up to a certain count, when other factors become limiting to success. The summed pregnancy rate of two cycles using a TMSC of 5 million (may be a TSC of ~10 million on graph) in each cycle is substantially higher than one single cycle using a TMSC of 10 million. However, although more cost-efficient, using a lower TMSC also increases the average time taken to achieve pregnancy. Women whose age is becoming a major factor in fertility may not want to spend that extra time.\n\nThe number of samples (ejaculates) required to give rise to a child varies substantially from person to person, as well as from clinic to clinic. However, the following equations generalize the main factors involved:\n\nFor intracervical insemination:\n\nThe pregnancy rate increases with increasing number of motile sperm used, but only up to a certain degree, when other factors become limiting instead.\n\nWith these numbers, one sample would on average help giving rise to 0.1–0.6 children, that is, it actually takes on average 2–5 samples to make a child.\n\nFor intrauterine insemination, a \"centrifugation fraction\" (\"f\") may be added to the equation:\n\nOn the other hand, only 5 million motile sperm may be needed per cycle with IUI (\"n\"=5 million)\n\nThus, only 1–3 samples may be needed for a child if used for IUI.\n\nOne of the key issues arising from the rise of dependency on assisted reproductive technology (ARTs) is the pressure placed on couples to conceive; 'where children are highly desired, parenthood is culturally mandatory, and childlessness socially unacceptable'.\n\nThe medicalization of infertility creates a framework in which individuals are encouraged to think of infertility quite negatively. In many cultures donor insemination is religiously and culturally prohibited, often meaning that less accessible \"high tech\" and expensive ARTs, like IVF, are the only solution.\n\nAn over-reliance on reproductive technologies in dealing with infertility prevents many – especially, for example, in the \"infertility belt\" of central and southern Africa – from dealing with many of the key causes of infertility treatable by artificial insemination techniques; namely preventable infections, dietary and lifestyle influences. \n\nIf good records are not kept, the offspring when grown up risk accidental incest.\n\nAn Anglican writer says that, \"To achieve union but not children by means of contraceptives and to achieve children but not union by means of artificial insemination are both equally wrong.\" Heterosexual intercourse is viewed by the Catholic Church as an act meant to be experienced only by married couples; it is viewed as a physical representation of the spiritual unity of marriage between a husband and wife. According to the \"Catechism of the Catholic Church\", artificial insemination \"dissociates the sexual act from the procreative act. The act which brings the child into existence is no longer an act by two persons giving themselves to one another, but one that 'entrusts the life and identity of the embryo into the power of doctors and biologists and establishes the domination of technology over the origin and destiny of the human person. Such a relationship of domination is, in itself, contrary to the dignity and equality that must be common to parents and children'\".\n\nSome countries restrict artificial insemination in a variety of ways. For example, some countries do not permit AI for single women, and some Muslim countries do not permit the use of donor sperm. As of May 2013, the following European countries permit medically assisted AI for single women:\n\nAI is useful for pets, livestock, endangered species, and animals in zoos or marine parks difficult to transport. \n\nIt may be used for many reasons, including to allow a male to inseminate a much larger number of females, to allow use of genetic material from males separated by distance or time, to overcome physical breeding difficulties, to control the paternity of offspring, to synchronise births, to avoid injury incurred during natural mating, and to avoid the need to keep a male at all (such as for small numbers of females or in species whose fertile males may be difficult to manage).\n\nSemen is collected, extended, then cooled or frozen. It can be used on site or shipped to the female's location. If frozen, the small plastic tube holding the semen is referred to as a \"straw\". To allow the sperm to remain viable during the time before and after it is frozen, the semen is mixed with a solution containing glycerol or other cryoprotectants. An \"extender\" is a solution that allows the semen from a donor to impregnate more females by making insemination possible with fewer sperm. Antibiotics, such as streptomycin, are sometimes added to the sperm to control some bacterial venereal diseases. Before the actual insemination, estrus may be induced through the use of progestogen and another hormone (usually PMSG or Prostaglandin F2α).\n\nThe first viviparous to be artificially fertilized was a dog. The experiment was conducted with success by the Italian Lazzaro Spallanzani in 1780. Another pioneer was the Russian Ilya Ivanov since 1899. In 1935 Suffolk sheep diluted semen was sent from Cambridge by plane to Krakow, Poland, and international research joint (Prawochenki from Poland, Milovanoff from USSR, Hammond from Cambridge, Walton from Scotland, and Thomasset from Uruguay).\n\nModern artificial insemination was pioneered by John O. Almquist of the Pennsylvania State University. His improvement of breeding efficiency by the use of antibiotics (first proven with penicillin in 1946) to control bacterial growth, decreasing embrionic mortality and increase fertility, and various new techniques for processing, freezing and thawing of frozen semen significantly enhanced the practical utilization of AI in the livestock industry, and earned him the 1981 Wolf Foundation Prize in Agriculture. Many techniques developed by him have since been applied to other species, including that of the human male.\n\nArtificial insemination is used in many non-human animals, including sheep, horses, cattle, pigs, dogs, pedigree animals generally, zoo animals, turkeys and even honeybees. \n\nArtificial insemination of farm animals is very common in today's agriculture industry in the developed world, especially for breeding dairy cattle (75% of all inseminations). Swine are also bred using this method (up to 85% of all inseminations). It provides an economical means for a livestock breeder to improve their herds utilizing males having very desirable traits.\n\nAlthough common with cattle and swine, AI is not as widely practised in the breeding of horses. A small number of equine associations in North America accept only horses that have been conceived by \"natural cover\" or \"natural service\" – the actual physical mating of a mare to a stallion – the Jockey Club being the most notable of these, as no AI is allowed in Thoroughbred breeding. Other registries such as the AQHA and warmblood registries allow registration of foals created through AI, and the process is widely used allowing the breeding of mares to stallions not resident at the same facility – or even in the same country – through the use of transported frozen or cooled semen.\n\nIn modern species conservation, semen collection and artificial insemination is used also in birds. In 2013 scientist of the Justus-Liebig-University of Giessen, Germany, from the working group of Michael Lierz, Clinic for birds, reptiles, amphibians and fish, developed a novel technique for semen collection and artificial insemination in parrots producing the world's first macaw by assisted reproduction.\n\n\n"}
{"id": "55269640", "url": "https://en.wikipedia.org/wiki?curid=55269640", "title": "Belarus at the Deaflympics", "text": "Belarus at the Deaflympics\n\nBelarus has been participating at the Deaflympics since 1993 and has earned a total of 87 medals.\n\nBelarus yet to compete at the Winter Deaflympic Games.\n\n\n"}
{"id": "3726299", "url": "https://en.wikipedia.org/wiki?curid=3726299", "title": "Blunt trauma", "text": "Blunt trauma\n\nBlunt trauma, blunt injury, non-penetrating trauma or blunt force trauma is physical trauma to a body part, either by impact, injury or physical attack. The latter is usually referred to as blunt force trauma. Blunt trauma is the initial trauma, from which develops more specific types such as contusions, abrasions, lacerations, and/or bone fractures. Blunt trauma is contrasted with penetrating trauma, in which an object such as a projectile or knife enters the body.\n\nBlunt abdominal trauma (BAT) comprises 75% of all blunt trauma and is the most common example of this injury. The majority occurs in motor vehicle accidents, in which rapid deceleration may propel the driver into the steering wheel, dashboard, or seatbelt causing contusions in less serious cases, or rupture of internal organs from briefly increased intraluminal pressure in the more serious, dependent on the force applied. It is important to note that initially there may be little in the way of overt clinical signs to indicate that serious internal abdominal injury has occurred, making assessment more challenging and requiring a high degree of clinical suspicion.\n\nThere are two basic physical mechanisms at play with the potential of injury to intra-abdominal organs: compression and deceleration. The former occurs from a direct blow, such as a punch, or compression against a non-yielding object such as a seat belt or steering column.\nThis force may deform a hollow organ thereby increasing its intra-luminal or internal pressure, leading to rupture. Deceleration, on the other hand, causes stretching and shearing at the points at which mobile structures, such as the bowel, are anchored. This can cause tearing of the mesentery of the bowel, and injury to the blood vessels that travel within the mesentery. Classic examples of these mechanisms are a hepatic tear along the ligamentum teres and injuries to the renal arteries.\n\nWhen blunt abdominal trauma is complicated by 'internal injury', the liver and spleen (see blunt splenic trauma) are most frequently involved, followed by the small intestine.\n\nIn rare cases, this injury has been attributed to medical techniques such as the Heimlich Maneuver, attempts at cardiopulmonary resuscitation and manual thrusts to clear an airway. Although these are rare examples, it has been suggested that they are caused by applying unnecessary pressure when administering such techniques. Finally, the occurrence of splenic rupture with mild blunt abdominal trama in those convalescing from infectious mononucleosis is well reported.\n\nIn all but the most obviously trivial injuries, the first concern is to exclude anything that might be quickly or immediately life-threatening. This is resolved by ascertaining that the subject's airway is open and competent, that breathing is unlabored, and that circulation—i.e. pulses that can be felt—is present. This is sometimes described as the \"A, B, C's\"—Airway, Breathing, and Circulation—and is the first step in any resuscitation or triage. Then, the history of the accident or injury is amplified with any medical, dietary (timing of last oral intake) and past history, from whatever sources such as family, friends, previous treating physicians that might be available. This method is sometimes given the mnemonic \"SAMPLE\". The amount of time spent on diagnosis should be minimized and expedited by a combination of clinical assessment and appropriate use of technology, such as diagnostic peritoneal lavage (DPL), or bedside ultrasound examination (FAST) before proceeding to laparotomy if required. If time and the patient's stability permits, CT examination may be carried out if available. Its advantages include superior definition of the injury, leading to grading of the injury and sometimes the confidence to avoid or postpone surgery. Its disadvantages include the time taken to acquire images, although this gets shorter with each generation of scanners, and the removal of the patient from the immediate view of the emergency or surgical staff.\n\nRecently, criteria have been defined that might allow patients with blunt abdominal trauma to be discharged safely without further evaluation. The characteristics of such patients would include:\nTo be considered low risk, patients would need to meet all low-risk criteria.\n\nIn the US, the majority of contact-collision injuries, usually blunt trauma, should have been witnessed in high school or collegiate games where the athletic training staff are trained to keep their eyes on the play. This may allow some departure from Advanced Trauma Life Support guidelines in the initial assessment, although the principles always apply. The major priority then becomes separating contusions and musculo-tendinous injuries from injuries to solid organs and the gut and recognizing potential for developing blood loss, and reacting accordingly. Blunt injuries to the kidney from helmets, shoulder pads, and knees are also described in American football, association football, martial arts, and all-terrain vehicle accidents.\n\nIn every case where the presumption of internal injury has been sufficient to trigger the diagnostic steps outlined above, intravenous access will be established and crystalloid solutions and/or blood will be administered at rates sufficient to maintain the circulation. Thereafter, further treatment will depend on the grade of organ damage estimated by the prior investigations and will vary from close observation with the ability to intervene quickly, or surgery, open or laparoscopic. In the case of blunt abdominal trauma, there is no shown benefit from surgery unless bleeding is present.\n\n"}
{"id": "53187118", "url": "https://en.wikipedia.org/wiki?curid=53187118", "title": "Bundesverband der Pharmazeutischen Industrie", "text": "Bundesverband der Pharmazeutischen Industrie\n\nThe Bundesverband der Pharmazeutischen Industrie (BPI) founded after WWII with headquarters in Berlin is a German Non-profit association and trade group for Small and medium-sized enterprises in the pharmaceutical industry. it represented 250 classic pharmaceutical companies, pharmaceutical service providers, biotech companies, herbal medicines and homeopathy/anthroposophy with altogether approximately 78,000 employees. BPI´s focus has been on political consulting and public relations on the EU-level in order to enhance development in the national and international health care systems.\n\nThe Bundesverband der Pharmazeutischen Industrie (BPI) was founded after WWII with headquarters in Berlin as an Eingetragener Verein. It is the German industry association or trade group for a broad spectrum of companies in the pharmaceutical industry. BPI works\n\nLarge pharmaceutical companies in Germany are represented by the Verband Forschender Arzneimittelhersteller (\"Association of Research-Based Pharmaceutical Companies\", vfa)\n\nAnother pharmaceutical industry association in Germany, the Bundesverband der Arzneimittelhersteller (\"German Medicines Manufacturers' Association\", BAH), has about 450 company members including 150 traditional pharmaceutical manufacturers and about 150 other members with a business interest in healthcare, such as publishers and polling organizations.\n\nThe philosophy of BPI begins with the health of man and works towards enhancing the health care system to become more future-oriented. The company works towards this goal by aiding and consulting in several aspects of pharmaceutical and medical device development and improving market chances.\n\nBPI´s focus has been on political consulting and public relations on the EU-level. BPI headquarters is located in Berlin. There is also an office in Brussels. \nThe BDI offers competencies in:\nAs of 2017, the BPI represented about 250 German Large to medium to small pharmaceutical companies and biotech companies with altogether about 78,000 employees. \n\nAmong the BPI's members are:\nIn 2008, the BPI was found to run a portal claiming to represent self-help support groups, while it propagated pharma industry interests.\n\nIn 2010, the BPI and the vfa were listed as the most powerful lobby groups for the pharma sector in Germany.\n\n"}
{"id": "56063121", "url": "https://en.wikipedia.org/wiki?curid=56063121", "title": "Cannabis in Cuba", "text": "Cannabis in Cuba\n\nCannabis in Cuba is illegal.\n\nCannabis was introduced to Cuba as a textile crop in 1793, but planters on the island found sugar to be a more lucrative crop. \n\nIn 1949, prior to the Cuban Revolution, a journal noted that most of the cannabis found in Cuba was imported from Mexico, but it was increasingly grown on the island, and was receiving attention in medical, juridical, and police publications.\n"}
{"id": "58186869", "url": "https://en.wikipedia.org/wiki?curid=58186869", "title": "Chicago Medical Society", "text": "Chicago Medical Society\n\nThe Chicago Medical Society is the medical society for Cook County, Illinois, United States. It was founded in 1850.\n"}
{"id": "1084968", "url": "https://en.wikipedia.org/wiki?curid=1084968", "title": "Decontamination foam", "text": "Decontamination foam\n\nDecontamination foam (known commonly as Decon foam) is a spray-on cleaning solution that, due to its physical properties, has a longer residence time on contaminated surfaces than regular liquids and thus provides efficient decontamination of biological and chemical contaminants (e.g. chemical warfare agents, anthrax spores or other toxic industrial materials (TIM)). The composition of decontamination foams is essentially water and a surfactant, creating an aqueous film forming foam that various reactive chemicals are then added, diminishing the amount of contaminants adhering to a surface and forming less hazardous products. Common reactants are hydrogen peroxide and quaternary ammonium complexes (QACs).\n\nIt is designed for use in emergency situations involving areas containing large numbers of possibly contaminated people e.g. at conventions, airports, concerts etc.\n\nThe two main benefits of decontamination foam over liquid decontaminants (chlorine, decontamination solutions, etc.) are its effectiveness on non horizontal surfaces and its high air to liquid ratio. Other decontaminants are difficult to apply to walls and ceilings due to poor adhesion, however decon foam is much better at adhering to surfaces, which increases the amount of time for the decontamination reaction to take place. Additionally, the high air to liquid ratio allows the foam to be used without over-applying the decontaminant. This high ratio also allows a small amount of liquid to cover a relatively large area in the event of a major contamination.\n\nThe LAX airport soap cannon can spray large quantities of decon foam on crowds of potentially exposed persons, e.g. to decon an entire planeload of people who might be victims of a nuclear, chemical, or biological agent release.\n\nDecon foam often comes in multiple bottles, that, when mixed, combine to form the decontamination solution. The bottles should be kept separate until needed as the foam may begin to lose effectiveness after mixing. After these bottles are mixed together, the foam can be applied by spraying it on a contaminated area or by manual application.\n\nDecon foam is also useful as an area denial weapons medium for crowd control, e.g. to make the ground slippery, as well as to reduce visibility (as a fog), such as with the CASCAD product.\n\n"}
{"id": "6548985", "url": "https://en.wikipedia.org/wiki?curid=6548985", "title": "Diana Zuckerman", "text": "Diana Zuckerman\n\nDiana M. Zuckerman (born 16 June 1950) is an American health policy analyst who focuses on the implications of policies for public health and patients’ health. She is an expert on national health policy, particularly in women's health and the safety and effectiveness of medical products. She is the President of the National Center for Health Research (formerly National Research Center for Women & Families) and the Cancer Prevention and Treatment Fund.\n\nZuckerman earned her B.A. in psychology from Smith College and then obtained a Ph.D in psychology from The Ohio State University in 1977. At Yale Medical School she was a post-doctoral fellow in epidemiology and public health from 1979 to 1980.\n\nShe was on the faculty at Vassar College and Yale University, and directed a longitudinal study of college students as director of the Seven College Study at Harvard University, publishing books and articles on the impact of media on children, the impact of religion on the health of the elderly, and how women's life experiences influence their mental and physical health. She left academia in 1983 when she was selected as a Fellow in the American Association for the Advancement of Science Congressional Science Fellowship program.\n\nFrom 1985 to 1993 she worked at the U.S. Congress in a House subcommittee where she was responsible for a dozen Congressional oversight investigations on health and social policy, including political manipulation of government grants to prevent child abuse, lack of safeguards for infertility treatments, financial conflicts of interest among National Institutes of Health (NIH) grant recipients, and the lack of safety studies on breast implants. Information from the hearings received widespread public health, government, and media attention, resulting in several policy and regulatory changes, including the U.S. Food and Drug Administration (FDA) requiring implant manufacturers to submit safety studies for the first time.\n\nIn 1993, Zuckerman joined the staff of the Senate Veterans Affairs Committee and began an investigation that resulted in the first Congressional hearings focused on the possible causes of Gulf War syndrome. In 1995 she was a senior policy advisor in the Clinton Administration. From 1996, she undertook leading roles in non-profit organizations, including, from 1999, presidency of the National Research Center for Women & Families (renamed the National Center for Health Research in 2014) and The Cancer Prevention and Treatment Fund. In 2010, she was inducted into the Women in Medicine International Hall of Fame by the American Medical Women's Association.\n\nHer work focuses on improving the quality of medical products and healthcare in the United States. She has been highly critical of scientific and medical research paid for by companies, who then use this to promote their products, as well as the lack of media coverage on independently funded research that challenges industry-funded research. She has said:\n\nIn February 2011, Zuckerman and colleagues Paul Brown and Dr. Steven Nissen published a study in the peer-reviewed journal \"Archives of Internal Medicine\", which evaluated the FDA’s recalls of devices that the agency considered potentially deadly or otherwise very high risk. Based on FDA data, the authors determined that most of the devices that were high-risk recalls had never been studied in clinical trials prior to FDA approval, and that the FDA needed to use more stringent criteria for implanted medical devices and those used to diagnose serious illnesses, and an editorial in the same issue agreed.\nIn April 2011, Zuckerman presented the results of the study at a hearing of the U.S. Senate Senate Special Committee on Aging.\n\nZuckerman is the author of five books, several book chapters, and dozens of articles in medical and academic journals, and in newspapers across the country. Her policy work has resulted in news coverage on all the major TV networks, including ABC, CBS, NBC, CNN, Fox News, public television, \"60 Minutes\", \"20/20\", National Public Radio, and in major U.S. print media such as \"The New York Times\", \"The Washington Post\", \"The Washington Times\", \"Los Angeles Times\", \"Boston Globe\", \"USA Today\", \"Detroit Free Press\", \"New York Daily News\", \"Newsweek\", \"Time\", \"U.S. News and World Report\", \"Family Circle\", \"The New Yorker\", \"Glamour\", \"Self\", as well as many other newspapers, magazines, and radio programs.\n\nZuckerman has appeared before numerous FDA advisory committees to testify as a medical expert on a variety of drugs and devices.\nOn December 12, 2013, Zuckerman testified regarding the type-2 diabetes drug dapagliflozin (Farxiga), saying: \"My concern about this drug is that there are just too many unanswered questions – and those unanswered questions are frightening ones. That was true when the FDA rejected this application for approval 2 years ago, and it is still true today.\" She cited studies that highlighted the risks that the drug poses, as well as methodological concerns such as a lack of diversity in clinical trials.\nOn November 14, 2012, Zuckerman made a statement at a Capitol Hill press conference about the VALID Compounding Act. She stated that loopholes in the FDA law, including compounding pharmacies, have allowed hundreds of people to get sick with meningitis and several to die. The VALID Compounding Act is designed to close the loopholes that are so harmful to patients.\n\nZuckerman frequently writes articles regarding medical drugs and devices, as well as public health policy. Recent articles include:\n"}
{"id": "49492645", "url": "https://en.wikipedia.org/wiki?curid=49492645", "title": "Disability in Japan", "text": "Disability in Japan\n\nIn Japan, a person with a disability is defined as: \"a person whose daily life or life in society is substantially limited over the long term due to a physical disability or mental disability\". Japan ratified the United Nations Convention on the Rights of Persons with Disabilities (CRPD) on 20 January 2014.\n\nIn 1998 the government estimated that there were 5,753,000 people with disabilities in Japan, constituting about 4.8% of the total population. The totals of the three legally defined categories were: 3,170,000 physically disabled; 413,000 intellectually disabled; and 4,170,000 have psychiatric disabilities. The physically disabled category was made up of people with the following impairments: 1,657,000 (56.5%) limb impairments; 305,000 (10.4%) are visually impaired; 350,000 (11.9%) have hearing impairments; while 621,000 (21.2%) had \"internal disabilities\" such as heart disease.\n\nJapan signed the CRPD in September 2007, but took until January 2014 to ratify it. The long delay was due to various legislative amendments and policy reforms that had to be in place before ratification.\n\nJapan made its Paralympic debut by hosting the 1964 Games in Tokyo. The country has participated in every subsequent edition of the Summer Paralympics, and in every edition of the Winter Paralympics since the first in 1976. It has hosted the Paralympic Games twice, with Tokyo hosting the 1964 Summer Games, and Nagano the 1998 Winter Paralympics. Tokyo is scheduled to host the Summer Games again in 2020.\n\n"}
{"id": "49121387", "url": "https://en.wikipedia.org/wiki?curid=49121387", "title": "Disabled Iranian veterans", "text": "Disabled Iranian veterans\n\nDisabled Iranian veterans, called janbaz (, literally \"those who were willing to lose their lives\") in Iran, mostly constitute the disabled veterans of the Iran–Iraq war. According to Mohammad Esfandiari, director of communications and public relations of Iran’s Martyrs and Disabled Veterans’ Organization, there are 548,499 disabled veterans of the Iran–Iraq War living in Iran as of June 2014, a number which includes the victims of Iraq's chemical weapon attacks on Iran. The latter are called \"chemical \"janbaz\"\" ().\n\nAfter Iraqi chemical attacks against Iranian soldiers and civilians, from 1983 to 1988, the number of people suffering injuries, including respiratory (42%), ocular (39%) and skin complications (25%) was more than 3,400 – a number which increased to at least 45,000 twenty years later, \"due to the occurrence of late respiratory complications of mustard gas exposure.\" \"The latency period can be as much as 40 years\" and \"So almost every day there are new cases — 30 years after the war,\" said Shahriar Khateri, the co-founder of the Society for Chemical Weapons Victims Support. According to Farhad Hashemnezhad in 2002, at least 20 percent of the patients were \"civilians who didn’t think they were close enough to be exposed.\" This large number of chemically affected veterans has made Iran the world’s largest laboratory for the study of the effects of chemical weapons. According to a declassified CIA report, as a result of Iraq’s repeated use of nerve agents and toxic gases in the 1980s, Iran suffered more than 50,000 casualties mostly by mustard gas used in dusty, liquid and vapor forms packed into bombs and artillery shells which were then fired at the front lines and beyond, at targets such as hospitals. The number of registered chemically affected veterans was 70,000 by 2014, according to Shahriar Khateri, Iran’s leading expert on chemical weapons victims. \"awareness could have saved lives,\" Khateri said. Doctors estimate that the final toll of Iraq’s chemical weapons could be as high as 90,000, equal to the total deaths from all toxic gases in World War I.\n\nDuring the Iran–Iraq War, mustard gas was used by Iraq against Iran, and it was the \"first time ever that nerve agents such as sarin and tabun were employed.\" Experiencing the outcomes of the chemical weapons, chemical warfare veterans believe that the younger generations should be instructed that \"war is not a computer game.\" \"We want to show how painful the consequences are. We don't want revenge. We just want to show what happens so it won't happen again,\" said Saadi, injured by Iraqi mustard gas.\n\nThe number of mentally disabled veterans including those who suffer from PTSD is increasing some of them increasingly showing signs of mental issues and the mental health conditions have worsened with age in others.\nIn Tehran alone there are very few wheelchair-accessible ramps, elevators and parking spots\nand the problem is more serious in smaller cities. This is while over 5,000 disabled Iranian veterans, mostly living in the capital, are restricted to wheelchairs for mobility. According to the Tehran Metro Group many more elevators are needed in Tehran’s subway system. To provide better services for disabled veterans, the Iranian government has announced that around $5 million has been dedicated to constructing ramps and wheelchair-accessible paths throughout Tehran. No similar official plans are dedicated for other cities in Iran. \n\nIn Tehran, chemical weapons victims are often referred to the Sasan Hospital.\n\nThe Tehran Peace Museum plans to focus on the enduring human consequences of Iran–Iraq War and serves as a centre for surviving victims of the war, especially chemical warfare veterans attacked by Saddam Hussein's forces.\n\nIn addition to numerous laws and regulations the Iranian government has passed to address disability related issues, the Disability Protect Act, including 16 articles\nproviding legal protections for disabled persons in areas such as public building access, education, housing and finance, has been Iran’s\nmost progressive and comprehensive legislation concerning disabled persons which was passed in 2003.\n\nThe \"Janbazan\" Foundation is created by Iran government for the assistance of Iranian disabled veterans and for giving them special treatment. They also receive services such as financial loan from Foundation of Martyrs and Veterans Affairs.\n\nVeterans are presented with awards and are thanked for their service on formalities such as ceremonies which are plentiful and widely covered in the Iranian media, particularly on two annual national occasions, namely Disabled Veterans’ Day in May and Sacred Defence Week (Sept. 20–27), which commemorates the commencement of the Iran–Iraq War.\n\nAccording to the supreme leader of Iran, Ali Khamenei, \"disabled war veterans are images of the war crimes of big powers who encouraged former Iraqi dictator Saddam Hussein to invade Iran.\"\n\n"}
{"id": "53307248", "url": "https://en.wikipedia.org/wiki?curid=53307248", "title": "Eastern Association for the Surgery of Trauma", "text": "Eastern Association for the Surgery of Trauma\n\nThe Eastern Association for the Surgery of Trauma is a 501(c)(3) medical association of American trauma surgeons. It has over 2,000 members who meet at an annual four-day conference. Its official journal is the \"Journal of Trauma and Acute Care Surgery\".\n"}
{"id": "1684595", "url": "https://en.wikipedia.org/wiki?curid=1684595", "title": "Effective therapeutic regimen management", "text": "Effective therapeutic regimen management\n\nReadiness for enhanced therapeutic regimen management is a NANDA approved nursing diagnosis which is defined as \"A pattern of regulating and integrating into daily living a program(s) for treatment of illness and its sequelae that is sufficient for meeting health-related goals and can be strengthened.\" It was introduced at the 15th NANDA conference in 2002.\n\n"}
{"id": "19771404", "url": "https://en.wikipedia.org/wiki?curid=19771404", "title": "Galilee Medical Center", "text": "Galilee Medical Center\n\nGalilee Medical Center (, \"HaMerkaz HaRefu'i LaGalil\"), abbreviated GMC, is a hospital located in the coastal city of Nahariya and is the second largest hospital in northern Israel (after Rambam Hospital in Haifa). It was established in 1956.\n\nThe hospital located on the outskirts of Nahariya, three kilometers from the city center, serving half a million residents of the western Galilee, from Karmiel to the coast.\n\nSince its modest beginning as a small maternity hospital, The Galilee Medical Center has grown into a 722-bed facility. The emergency department receives about 400 people every day and the number of hospitalizations is about 60,000 a year. Approximately 420 physicians practice in this government owned hospital, while the total number of employees is about 2200. The hospital staff is a reflection of the multi-ethnic demography of the Western Galilee, consisting of Jews, Muslims, Christians, Druze and others. In 2007, the Western Galilee Hospital was the first to appoint an Arab Israeli, Dr. Masad Barhoum, as its director.\n\nLocated in close proximity to Israel's northern border with Lebanon, the hospital's main area of expertise is trauma: IDF soldiers, Israeli civilians, UN personnel and civilians from neighboring Arab countries.\n\nBefore 2000, approximately one-third of the patients in the ophthalmology department were Lebanese citizens who crossed the border through the Good Fence and received treatment free of charge.\n\nIn summer of 2006, during the 2006 Lebanon War, the hospital handled the largest number of casualties in Israel. During the month-long war, some 1,800 civilians and 300 airlifted IDF soldiers were treated there. During this time, the hospital took a direct hit that destroyed an outer wall and eight rooms. Since then, the hospital has built an underground emergency department, partly funded by overseas donors.\n\nSince the beginning of the Syrian war in 2013, the Galilee Medical Center has treated more than 1600 victims of this ongoing civil carnage, making it one of the leading institutions in Israel experienced in treating complex war injuries \n\n"}
{"id": "17163431", "url": "https://en.wikipedia.org/wiki?curid=17163431", "title": "History of malaria", "text": "History of malaria\n\nThe history of malaria stretches from its prehistoric origin as a zoonotic disease in the primates of Africa through to the 21st century. A widespread and potentially lethal human infectious disease, at its peak malaria infested every continent, except Antarctica. Its prevention and treatment have been targeted in science and medicine for hundreds of years. Since the discovery of the parasites which cause it, research attention has focused on their biology, as well as that of the mosquitoes which transmit the parasites.\n\nReferences to its unique, periodic fevers are found throughout recorded history beginning in the first millennium BCE in Greece and China.\n\nFor thousands of years, traditional herbal remedies have been used to treat malaria. The first effective treatment for malaria came from the bark of cinchona tree, which contains quinine. After the link to mosquitos and their parasites were identified in the early twentieth century, mosquito control measures such as widespread use of the insecticide DDT, swamp drainage, covering or oiling the surface of open water sources, indoor residual spraying and use of insecticide treated nets was initiated. Prophylactic quinine was prescribed in malaria endemic areas, and new therapeutic drugs, including chloroquine and artemisinins, were used to resist the scourge. Today, artemisinin is present in every remedy applied in treatment of malaria. After introducing artemisinin as a cure administered together with other remedies, the mortality in Africa went down by a half.\n\nMalaria researchers have won multiple Nobel Prizes for their achievements, although the disease continues to afflict some 200 million patients each year, killing more than 600,000.\n\nMalaria was the most important health hazard encountered by U.S. troops in the South Pacific during World War II, where about 500,000 men were infected. According to Joseph Patrick Byrne, \"Sixty thousand American soldiers died of malaria during the African and South Pacific campaigns.\"\n\nAt the close of the 20th century, malaria remained endemic in more than 100 countries throughout the tropical and subtropical zones, including large areas of Central and South America, Hispaniola (Haiti and the Dominican Republic), Africa, the Middle East, the Indian subcontinent, Southeast Asia, and Oceania. Resistance of Plasmodium to anti-malaria drugs, as well as resistance of mosquitos to insecticides and the discovery of zoonotic species of the parasite have complicated control measures.\n\nThe first evidence of malaria parasites was found in mosquitoes preserved in amber from the Palaeogene period that are approximately 30 million years old. Human malaria likely originated in Africa and coevolved with its hosts, mosquitoes and non-human primates. Malaria protozoa are diversified into primate, rodent, bird, and reptile host lineages. Humans may have originally caught \"Plasmodium falciparum\" from gorillas. \"P. vivax\", another malarial \"Plasmodium\" species among the six that infect humans, also likely originated in African gorillas and chimpanzees. Another malarial species recently discovered to be transmissible to humans, \"P. knowlesi\", originated in Asian macaque monkeys. While \"P. malariae\" is highly host specific to humans, there is some evidence that low level non-symptomatic infection persists among wild chimpanzees.\n\nAbout 10,000 years ago, malaria started having a major impact on human survival, coinciding with the start of agriculture in the Neolithic revolution. Consequences included natural selection for sickle-cell disease, thalassaemias, glucose-6-phosphate dehydrogenase deficiency, Southeast Asian ovalocytosis, elliptocytosis and loss of the Gerbich antigen (glycophorin C) and the Duffy antigen on the erythrocytes, because such blood disorders confer a selective advantage against malaria infection (balancing selection). The three major types of inherited genetic resistance (sickle-cell disease, thalassaemias, and glucose-6-phosphate dehydrogenase deficiency) were present in the Mediterranean world by the time of the Roman Empire, about 2000 years ago.\n\nMolecular methods have confirmed the high prevalence of \"P. falciparum\" malaria in ancient Egypt. The Ancient Greek historian Herodotus wrote that the builders of the Egyptian pyramids (circa 2700–1700 BCE) were given large amounts of garlic, probably to protect them against malaria. The Pharaoh Sneferu, the founder of the Fourth dynasty of Egypt, who reigned from around 2613–2589 BCE, used bed-nets as protection against mosquitoes. Cleopatra VII, the last Pharaoh of Ancient Egypt, similarly slept under a mosquito net. However, whether the mosquito nets were used for the purpose of malaria prevention, or for more mundane purpose of avoiding the discomfort of mosquito bites, is unknown. The presence of malaria in Egypt from circa 800 BCE onwards has been confirmed using DNA-based methods.\n\nMalaria became widely recognized in ancient Greece by the 4th century BCE, and is implicated in the decline of many city-state populations. The term \"μίασμα\" (Greek for miasma): \"stain, pollution\", was coined by Hippocrates of Kos who used it to describe dangerous fumes from the ground that are transported by winds and can cause serious illnesses. Hippocrates (460–370 BCE), the \"father of medicine\", related the presence of intermittent fevers with climatic and environmental conditions and classified the fever according to periodicity: Gk.:\"tritaios pyretos\" / L.:\"febris tertiana\" (fever every third day), and Gk.:\"tetartaios pyretos\" / L.:\"febris quartana\" (fever every fourth day).\n\nThe Chinese \"Huangdi Neijing\" (The Inner Canon of the Yellow Emperor) dating from ~300 BCE – 200 CE apparently refers to repeated paroxysmal fevers associated with enlarged spleens and a tendency to epidemic occurrence.\nAround 168 BCE, the herbal remedy \"Qing-hao\" () (\"Artemisia annua\") came into use in China to treat female hemorrhoids (\"Wushi'er bingfang\" translated as \"Recipes for 52 kinds of diseases\" unearthed from the Mawangdui).\n\"Qing-hao\" was first recommended for acute intermittent fever episodes by Ge Hong as an effective medication in the 4th-century Chinese manuscript \"Zhou hou bei ji fang\", usually translated as \"Emergency Prescriptions kept in one's Sleeve\". His recommendation was to soak fresh plants of the artemisia herb in cold water, wring it out and ingest the expressed bitter juice in its raw state.\n\n'Roman fever' refers to a particularly deadly strain of malaria that affected the Roman Campagna and the city of Rome throughout various epochs in history. An epidemic of Roman fever during the fifth century AD may have contributed to the fall of the Roman empire. The many remedies to reduce the spleen in Pedanius Dioscorides's \"De Materia Medica\" have been suggested to have been a response to chronic malaria in the Roman empire. Some so-called \"vampire burials\" in late antiquity may have been performed in response to malaria epidemics. For example, some children who died of malaria were buried in the necropolis at Lugnano in Teverina using rituals meant to prevent them from returning from the dead. Modern scholars hypothesize that communities feared that the dead would return and spread disease.\n\nIn 835, the celebration of Hallowmas (All Saints Day) was moved from May to November at the behest of Pope Gregory IV, on the \"practical grounds that Rome in summer could not accommodate the great number of pilgrims who flocked to it\", and perhaps because of public health considerations regarding Roman Fever, which claimed a number of lives of pilgrims during the sultry summers of the region.\n\nDuring the Middle Ages, treatments for malaria (and other diseases) included blood-letting, inducing vomiting, limb amputations and trepanning. Physicians and surgeons in the period used herbal medicines like belladonna to bring about pain relief in afflicted patients.\n\nThe name malaria derived from \"mal aria\" ('bad air' in Medieval Italian). This idea came from the Ancient Romans who thought that this disease came from the horrible fumes in the swamps. The word malaria has its roots in the miasma theory, as described by historian and chancellor of Florence Leonardo Bruni in his \"Historiarum Florentini populi libri XII\", which was the first major example of Renaissance historical writing:\n\nAvuto i Fiorentini questo fortissimo castello e fornitolo di buone guardie, consigliavano fra loro medesimi fosse da fare. Erano alcuni a' quali pareva sommamente utile e necessario a ridurre lo esercito, e massimamente essendo affaticato per la infermità e per la mala ariae per lungo e difficile campeggiare nel tempo dell'autunno e in luoghi infermi, e vedendo ancora ch'egli era diminuito assai per la licenza conceduta a molti pel capitano di potersi partire: perocchè, nel tempo che eglino erano stati lungamente a quello assedio, molti, o per disagio del campo o per paura d'infermità, avevano domandato e ottenuto licenza da lui (Acciajuoli 1476).\n\nAfter the Florentines had conquered this stronghold, after putting good guardians on it they were discussing among themselves how to proceed. For some of them it appeared most useful and necessary to reduce the army, more so as it was extremely stressed by disease and bad air, and due to the long-lasting and difficult camps in unhealthy places during the autumn. They (the Florentines) further considered that the army was reduced in numbers due to the leave permits granted to many soldiers by their officers. In fact, during the siege, many soldiers had asked and obtained leave permits due to the camp hardships and fear of illness [translated from medieval Italian, Toscanic dialect].\n\nThe coastal plains of southern Italy fell from international prominence when malaria expanded in the sixteenth century. At roughly the same time, in the coastal marshes of England, mortality from \"marsh fever\" or \"tertian ague\" (\"ague\": via French from medieval Latin \"acuta\" (\"febris\"), acute fever) was comparable to that in sub-Saharan Africa today. William Shakespeare was born at the start of the especially cold period that climatologists call the \"Little Ice Age\", yet he was aware enough of the ravages of the disease to mention it in eight of his plays.\n\nMedical accounts and ancient autopsy reports state that tertian malarial fevers caused the death of four members of the prominent Medici family of Florence . These claims have been confirmed with more modern methodologies.\n\nMalaria was not referenced in the \"medical books\" of the Mayans or Aztecs. European settlers and the West Africans they enslaved likely brought malaria to the Americas in the 16th century.\n\nIn the book \"\", the author Charles Mann cites sources that speculate that the reason African slaves were brought to the British Americas was because of their immunity to malaria. Britain did not have large numbers of African slaves, there were plenty of unemployed workers who could come as indentured servants. In the area above the Mason–Dixon line, the malaria protozoa did not fare well, the English-speaking indentured servant proved more profitable as he would work toward his freedom and hence worked with less supervision and coercion. As malaria spread, places such as the tidewater of Virginia and South Carolina which had previously been habitable by white people became endemic with malaria. Small white landholders were at a disadvantage to plantation owners, as they risk complete economic ruin when they were sick, while plantation owners relied on more malaria resistant West African slaves. Malaria caused huge losses to British forces in the South during the revolutionary war as well as to Union forces during the Civil War. Malaria also helped weaken the Native American population and make them more susceptible to other diseases.\n\nSpanish missionaries found that fever was treated by Amerindians near Loxa (Peru) with powder from Peruvian bark (later established to be from any of several trees of genus \"Cinchona\"). It was used by the Quechua Indians of Peru to reduce the shaking effects caused by severe chills. Jesuit Brother Agostino Salumbrino (1561–1642), who lived in Lima and was an apothecary by training, observed the Quechua using the bark of the cinchona tree for that purpose. While its effect in treating malaria (and hence malaria-induced shivering) was unrelated to its effect in controlling shivering from cold, it was nevertheless effective for malaria. The use of the “fever tree” bark was introduced into European medicine by Jesuit missionaries (Jesuit's bark). Jesuit Bernabé de Cobo (1582–1657), who explored Mexico and Peru, is credited with taking cinchona bark to Europe. He brought the bark from Lima to Spain, and then to Rome and other parts of Italy, in 1632. Francesco Torti wrote in 1712 that only “intermittent fever” was amenable to the fever tree bark. This work finally established the specific nature of cinchona bark and brought about its general use in medicine.\n\nIt would be nearly 200 years before the active principles, quinine and other alkaloids, of cinchona bark were isolated. Quinine, a toxic plant alkaloid, is, in addition to its anti-malarial properties, an effective muscle relaxant, as the modern use for nocturnal leg cramps suggests (corroborating its use for shivering by the Peruvian Indians).\n\nIn 1717, the dark pigmentation of a postmortem spleen and brain was published by the epidemiologist Giovanni Maria Lancisi in his malaria text book \"De noxiis paludum effluviis eorumque remediis\". This was one of the earliest reports of the characteristic enlargement of the spleen and dark color of the spleen and brain which are the most constant post-mortem indications of chronic malaria infection. He related the prevalence of malaria in swampy areas to the presence of flies and recommended swamp drainage to prevent it.\n\nIn the nineteenth century, the first drugs were developed to treat malaria and parasites were first identified as its source.\n\nFrench chemist Pierre Joseph Pelletier and French pharmacist Joseph Bienaimé Caventou separated in 1820 the alkaloids cinchonine and quinine from powdered fever tree bark, allowing for the creation of standardized doses of the active ingredients. Prior to 1820, the bark was simply dried, ground to a fine powder and mixed into a liquid (commonly wine) for drinking.\n\nAn English trader, Charles Ledger, and his Amerindian servant spent four years collecting cinchona seeds in the Andes in Bolivia, highly prized for their quinine but whose export was prohibited. Ledger managed to get seeds out; in 1865, the Dutch government cultivated 20,000 trees of the \"Cinchona ledgeriana\" in Java (Indonesia). By the end of the nineteenth century, the Dutch had established a world monopoly over its supply.\n\nIn 1834, in British Guiana, a German physician, Carl Warburg, invented an antipyretic medicine: 'Warburg's Tincture'. This secret, proprietary remedy contained quinine and other herbs. Trials were made in Europe in the 1840s and 1850s. It was officially adopted by the Austrian Empire in 1847. It was considered by many eminent medical professionals to be a more efficacious antimalarial than quinine. It was also more economical. The British Government supplied Warburg's Tincture to troops in India and other colonies.\n\nIn 1876, methylene blue was synthesized by German chemist Heinrich Caro. Paul Ehrlich in 1880 described the use of \"neutral\" dyes – mixtures of acidic and basic dyes for the differentiation of cells in peripheral blood smears. In 1891 Ernst Malachowski and Dmitri Leonidovich Romanowsky independently developed techniques using a mixture of Eosin Y and modified methylene blue (methylene azure) that produced a surprising hue unattributable to either staining component: a shade of purple. Malachowski used alkali-treated methylene blue solutions and Romanowsky used methylene blue solutions which were molded or aged. This new method differentiated blood cells and demonstrated the nuclei of malarial parasites. Malachowski's staining technique was one of the most significant technical advances in the history of malaria.\n\nIn 1891, Paul Guttmann and Ehrlich noted that methylene blue had a high affinity for some tissues and that this dye had a slight antimalarial property. Methylene blue and its congeners may act by preventing the biocrystallization of heme.\n\nIn 1848, German anatomist Johann Heinrich Meckel recorded black-brown pigment granules in the blood and spleen of a patient who had died in a mental hospital. Meckel was thought to have been looking at malaria parasites without realizing it; he did not mention malaria in his report. He hypothesized that the pigment was melanin. The causal relationship of pigment to the parasite was established in 1880, when French physician Charles Louis Alphonse Laveran, working in the military hospital of Constantine, Algeria, observed pigmented parasites inside the red blood cells of malaria sufferers. He witnessed the events of exflagellation and became convinced that the moving flagella were parasitic microorganisms. He noted that quinine removed the parasites from the blood. Laveran called this microscopic organism \"Oscillaria malariae\" and proposed that malaria was caused by this protozoan. This discovery remained controversial until the development of the oil immersion lens in 1884 and of superior staining methods in 1890–1891.\n\nIn 1885, Ettore Marchiafava, Angelo Celli and Camillo Golgi studied the reproduction cycles in human blood (Golgi cycles). Golgi observed that all parasites present in the blood divided almost simultaneously at regular intervals and that division coincided with attacks of fever. In 1886 Golgi described the morphological differences that are still used to distinguish two malaria parasite species \"Plasmodium vivax\" and \"Plasmodium malariae\". Shortly after this Sakharov in 1889 and Marchiafava & Celli in 1890 independently identified \"Plasmodium falciparum\" as a species distinct from \"P. vivax\" and \"P. malariae\". In 1890, Grassi and Feletti reviewed the available information and named both \"P. malariae\" and \"P. vivax\" (although within the genus \"Haemamoeba\".) By 1890, Laveran's germ was generally accepted, but most of his initial ideas had been discarded in favor of the taxonomic work and clinical pathology of the Italian school. Marchiafava and Celli called the new microorganism \"Plasmodium\". \"H. vivax\" was soon renamed \"Plasmodium vivax\". In 1892, Marchiafava and Bignami proved that the multiple forms seen by Laveran were from a single species. This species was eventually named \"P. falciparum\". Laveran was awarded the 1907 Nobel Prize for Physiology or Medicine \"in recognition of his work on the role played by protozoa in causing diseases\".\n\nDutch physician Pieter Pel first proposed a tissue stage of the malaria parasite in 1886, presaging its discovery by over 50 years. This suggestion was reiterated in 1893 when Golgi suggested that the parasites might have an undiscovered tissue phase (this time in endothelial cells). Pel in 1896 supported Golgi's latent phase theory.\n\nThe establishment of the scientific method from about the mid-19th century on demanded testable hypotheses and verifiable phenomena for causation and transmission. Anecdotal reports, and the discovery in 1881 that mosquitos were the vector of yellow fever, eventually led to the investigation of mosquitoes in connection with malaria.\n\nAn early effort at malaria prevention occurred in 1896 in Massachusetts. An Uxbridge outbreak prompted health officer Dr. Leonard White to write a report to the State Board of Health, which led to a study of mosquito-malaria links and the first efforts for malaria prevention. Massachusetts state pathologist, Theobald Smith, asked that White's son collect mosquito specimens for further analysis, and that citizens add screens to windows, and drain collections of water.\n\nBritain's Sir Ronald Ross, an army surgeon working in Secunderabad India, proved in 1897 that malaria is transmitted by mosquitoes, an event now commemorated via World Mosquito Day. He was able to find pigmented malaria parasites in a mosquito that he artificially fed on a malaria patient who had crescents in his blood. He continued his research into malaria by showing that certain mosquito species (\"Culex fatigans\") transmit malaria to sparrows and he isolated malaria parasites from the salivary glands of mosquitoes that had fed on infected birds. He reported this to the British Medical Association in Edinburgh in 1898.\n\nGiovanni Battista Grassi, professor of Comparative Anatomy at Rome University, showed that human malaria could only be transmitted by \"Anopheles\" (Greek \"anofelís\": good-for-nothing) mosquitoes. Grassi along with coworkers Amico Bignami, Giuseppe Bastianelli and Ettore Marchiafava announced at the session of the Accademia dei Lincei on 4 December 1898 that a healthy man in a non-malarial zone had contracted tertian malaria after being bitten by an experimentally infected \"Anopheles claviger\" specimen.\n\nIn 1898–1899, Bastianelli, Bignami and Grassi were the first to observe the complete transmission cycle of \"P. falciparum\", \"P. vivax\" and \"P. malaria\" from mosquito to human and back in \"A. claviger\".\n\nA dispute broke out between the British and Italian schools of malariology over priority, but Ross received the 1902 Nobel Prize for Physiology or Medicine for \"his work on malaria, by which he has shown how it enters the organism and thereby has laid the foundation for successful research on this disease and methods of combating it\".\n\nWilliam Henry Perkin, a student of August Wilhelm von Hofmann at the Royal College of Chemistry in London, unsuccessfully tried in the 1850s to synthesize quinine in a commercial process. The idea was to take two equivalents of N-allyltoluidine () and three atoms of oxygen to produce quinine () and water. Instead, Perkin's mauve was produced when attempting quinine total synthesis via the oxidation of N-allyltoluidine. Before Perkin's discovery, all dyes and pigments were derived from roots, leaves, insects, or, in the case of Tyrian purple, molluscs.\n\nQuinine wouldn't be successfully synthesized until 1918. Synthesis remains elaborate, expensive and low yield, with the additional problem of separation of the stereoisomers. Though quinine is not one of the major drugs used in treatment, modern production still relies on extraction from the cinchona tree.\n\nRelapses were first noted in 1897 by William S. Thayer, who recounted the experiences of a physician who relapsed 21 months after leaving an endemic area. He proposed the existence of a tissue stage. Relapses were confirmed by Patrick Manson, who allowed infected \"Anopheles\" mosquitoes to feed on his eldest son. The younger Manson then described a relapse nine months after his apparent cure with quinine.\n\nAlso, in 1900 Amico Bignami and Giuseppe Bastianelli found that they could not infect an individual with blood containing only gametocytes. The possibility of the existence of a chronic blood stage infection was proposed by Ronald Ross and David Thompson in 1910.\n\nThe existence of asexually-reproducing avian malaria parasites in cells of the internal organs was first demonstrated by Henrique de Beaurepaire Aragão in 1908.\n\nThree possible mechanisms of relapse were proposed by Marchoux in 1926 (codice_1) parthenogenesis of macrogametocytes: (codice_2) persistence of schizonts in small numbers in the blood where immunity inhibits multiplication, but later disappears and/or (codice_3) reactivation of an encysted body in the blood. James in 1931 proposed that sporozoites are carried to internal organs, where they enter reticuloendothelial cells and undergo a cycle of development, based on quinine's lack of activity on them. Huff and Bloom in 1935 demonstrated stages of avian malaria that transpire outside blood cells (exoerythrocytic). In 1945 Fairley \"et al.\" reported that inoculation of blood from a patient with \"P. vivax\" may fail to induce malaria, although the donor may subsequently exhibit the condition. Sporozoites disappeared from the blood stream within one hour and reappeared eight days later. This suggested the presence of forms that persist in tissues. Using mosquitoes rather than blood, in 1946 Shute described a similar phenomenon and proposed the existence of an 'x-body' or resting form. The following year Sapero proposed a link between relapse and a tissue stage not yet discovered. Garnham in 1947 described exoerythrocytic schizogony in \"Hepatocystis (Plasmodium) kochi\". In the following year Shortt and Garnham described the liver stages of \"P. cynomolgi\" in monkeys. In the same year a human volunteer consented to receive a massive dose of infected sporozoites of \"P. vivax\" and undergo a liver biopsy three months later, thus allowing Shortt \"et al.\" to demonstrate the tissue stage. The tissue form of \"Plasmodium ovale\" was described in 1954 and that of \"P. malariae\" in 1960 in experimentally infected chimpanzees.\n\nThe latent or dormant liver form of the parasite (hypnozoite), apparently responsible for the relapses characteristic of \"P. vivax\" and \"P. ovale\" infections, was first observed in the 1980s. The term \"hypnozoite\" was coined by Miles B. Markus while a student. In 1976, he speculated: \"If sporozoites of \"Isospora\" can behave in this fashion, then those of related Sporozoa, like malaria parasites, may have the ability to survive in the tissues in a similar way.\" In 1982, Krotoski \"et al\" reported identification of \"P. vivax\" hypnozoites in liver cells of infected chimpanzees.\n\nIn the early twentieth century, before antibiotics, patients with tertiary syphilis were intentionally infected with malaria to induce a fever; this was called malariotherapy. In 1917, Julius Wagner-Jauregg, a Viennese psychiatrist, began to treat neurosyphilitics with induced \"Plasmodium vivax\" malaria. Three or four bouts of fever were enough to kill the temperature-sensitive syphilis bacteria (\"Spirochaeta pallida\" also known as \"Treponema pallidum\"). \"P. vivax\" infections were then terminated by quinine. By accurately controlling the fever with quinine, the effects of both syphilis and malaria could be minimized. While about 15% of patients died from malaria, this was preferable to the almost-certain death from syphilis. Therapeutic malaria opened up a wide field of chemotherapeutic research and was practised until 1950. Wagner-Jauregg was awarded the 1927 Nobel Prize in Physiology or Medicine for his discovery of the therapeutic value of malaria inoculation in the treatment of dementia paralytica.\n\nHenry Heimlich advocated malariotherapy as a treatment for AIDS, and some studies of malariotherapy for HIV infection have been performed in China. The United States Centers for Disease Control and Prevention does not recommend the use of malariotherapy for HIV.\n\nIn 1881, Dr. Carlos Finlay, a Cuban-born physician of Scottish ancestry, theorized that yellow fever was transmitted by a specific mosquito, later designated \"Aedes aegypti\". The theory remained controversial for twenty years until confirmed in 1901 by Walter Reed. This was the first scientific proof of a disease being transmitted exclusively by an insect vector, and demonstrated that control of such diseases necessarily entailed control or eradication of its insect vector.\n\nYellow fever and malaria among workers had seriously delayed construction of the Panama Canal. Mosquito control instituted by William C. Gorgas dramatically reduced this problem.\n\nJohann \"Hans\" Andersag and colleagues synthesized and tested some 12,000 compounds, eventually producing Resochin as a substitute for quinine in the 1930s. It is chemically related to quinine through the possession of a quinoline nucleus and the dialkylaminoalkylamino side chain. Resochin (7-chloro-4- 4- (diethylamino) – 1 – methylbutyl amino quinoline) and a similar compound Sontochin (3-methyl Resochin) were synthesized in 1934. In March 1946, the drug was officially named Chloroquine. Chloroquine is an inhibitor of hemozoin production through biocrystallization. Quinine and chloroquine affect malarial parasites only at life stages when the parasites are forming hematin-pigment (hemozoin) as a byproduct of hemoglobin degradation.\nChloroquine-resistant forms of \"P. falciparum\" emerged only 19 years later. The first resistant strains were detected around the Cambodia‐Thailand border and in Colombia, in the 1950s. In 1989, chloroquine resistance in \"P. vivax\" was reported in Papua New Guinea. These resistant strains spread rapidly, producing a large mortality increase, particularly in Africa during the 1990s.\n\nSystematic screening of traditional Chinese medical herbs was carried out by Chinese research teams, consisting of hundreds of scientists in the 1960s and 1970s. Qinghaosu, later named artemisinin, was cold-extracted in a neutral milieu (pH 7.0) from the dried leaves of \"Artemisia annua\".\n\nArtemisinin was isolated by pharmacologist Tu Youyou (Nobel Prize in Physiology or Medicine, 2015). Tu headed a team tasked by the Chinese government with finding a treatment for choloroquine-resistant malaria. Their work was known as Project 523, named after the date it was announced – 23 May 1967. The team investigated more than 2000 Chinese herb preparations and by 1971 had made 380 extracts from 200 herbs. An extract from qinghao (\"Artemisia annua\") was effective but the results were variable. Tu reviewed the literature, including \"Zhou hou bei ji fang\" (A handbook of prescriptions for emergencies) written in 340 BC by Chinese physician Ge Hong. This book contained the only useful reference to the herb: \"A handful of qinghao immersed with two litres of water, wring out the juice and drink it all.\" Tu's team subsequently isolated a nontoxic, neutral extract that was 100% effective against parasitemia in animals. The first successful trials of artemisinin were in 1979.\n\nArtemisinin is a sesquiterpene lactone containing a peroxide group, which is believed to be essential for its anti-malarial activity. Its derivatives, artesunate and artemether, have been used in clinics since 1987 for the treatment of drug-resistant and drug-sensitive malaria, especially cerebral malaria. These drugs are characterized by fast action, high efficacy and good tolerance. They kill the asexual forms of \"P. berghei\" and \"P. cynomolgi\" and have transmission-blocking activity. In 1985, Zhou Yiqing and his team combined artemether and lumefantrine into a single tablet, which was registered as a medicine in China in 1992. Later it became known as “Coartem”. Artemisinin combination treatments (ACTs) are now widely used to treat uncomplicated \"falciparum\" malaria, but access to ACTs is still limited in most malaria-endemic countries and only a minority of the patients who need artemisinin-based combination treatments receive them.\n\nIn 2008 White predicted that improved agricultural practices, selection of high-yielding hybrids, microbial production, and the development of synthetic peroxides would lower prices.\n\nEfforts to control the spread of malaria suffered a major setback in 1930: entomologist Raymond Corbett Shannon discovered imported disease-bearing \"Anopheles gambiae\" mosquitoes living in Brazil (DNA analysis later revealed the actual species to be \"A. arabiensis\"). This species of mosquito is a particularly efficient vector for malaria and is native to Africa. In 1938, the introduction of this vector caused the greatest epidemic of malaria ever seen in the New World. However, complete eradication of \"A. gambiae\" from northeast Brazil and thus from the New World was achieved in 1940 by the systematic application of the arsenic-containing compound Paris green to breeding places, and of pyrethrum spray-killing to adult resting places.\n\nAustrian chemist Othmar Zeidler is credited with the first synthesis of DDT (DichloroDiphenylTrichloroethane) in 1874. The insecticidal properties of DDT were identified in 1939 by chemist Paul Hermann Müller of Geigy Pharmaceutical. For his discovery of DDT as a contact poison against several arthropods, he was awarded the 1948 Nobel Prize in Physiology or Medicine. In the fall of 1942, samples of the chemical were acquired by the United States, Britain and Germany. Laboratory tests demonstrated that it was highly effective against many insects.\n\nRockefeller Foundation studies showed in Mexico that DDT remained effective for six to eight weeks if sprayed on the inside walls and ceilings of houses and other buildings. The first field test in which residual DDT was applied to the interior surfaces of all habitations and outbuildings was carried out in central Italy in the spring of 1944. The objective was to determine the residual effect of the spray upon \"Anopheline\" density in the absence of other control measures. Spraying began in Castel Volturno and, after a few months, in the delta of the Tiber. The unprecedented effectiveness of the chemical was confirmed: the new insecticide was able to eradicate malaria by eradicating mosquitoes. At the end of World War II, a massive malaria control program based on DDT spraying was carried out in Italy. In Sardinia – the second largest island in the Mediterranean – between 1946 and 1951, the Rockefeller Foundation conducted a large-scale experiment to test the feasibility of the strategy of \"species eradication\" in an endemic malaria vector. Malaria was effectively eliminated in the United States by the use of DDT in the National Malaria Eradication Program (1947–52). The concept of eradication prevailed in 1955 in the Eighth World Health Assembly: DDT was adopted as a primary tool in the fight against malaria.\n\nIn 1953, the World Health Organization (WHO) launched an antimalarial program in parts of Liberia as a pilot project to determine the feasibility of malaria eradication in tropical Africa. However, these projects encountered difficulties that foreshadowed the general retreat from malaria eradication efforts across tropical Africa by the mid-1960s.\n\nDDT was banned in the US in 1972, after the discussion opened in 1962 by \"Silent Spring\", written by American biologist Rachel Carson, which launched the environmental movement in the West. The book catalogued the environmental impacts of indiscriminate DDT spraying and suggested that DDT and other pesticides cause cancer and that their agricultural use was a threat to wildlife. The U.S. Agency for International Development supports indoor DDT spraying as a vital component of malaria control programs and has initiated DDT and other insecticide spraying programs in tropical countries.\n\nOther insecticides are available for mosquito control, as well as physical measures, such as draining the wetland breeding grounds and the provision of better sanitation. Pyrethrum (from the flowering plant \"Chrysanthemum\" [or \"Tanacetum\"] \"cinerariaefolium\") is an economically important source of natural insecticide. Pyrethrins attack the nervous systems of all insects. A few minutes after application, the insect cannot move or fly, while female mosquitoes are inhibited from biting. The use of pyrethrum in insecticide preparations dates to about 400 BCE. Pyrethrins are biodegradable and break down easily on exposure to light. The majority of the world's supply of pyrethrin and \"Chrysanthemum cinerariaefolium\" comes from Kenya. The flower was first introduced into Kenya and the highlands of Eastern Africa during the late 1920s. The flowers of the plant are harvested shortly after blooming; they are either dried and powdered, or the oils within the flowers are extracted with solvents.\n\nUntil the 1950s, screening of anti-malarial drugs was carried out on avian malaria. Avian malaria species differ from those that infect humans. The discovery in 1948 of \"Plasmodium berghei\" in wild rodents in the Congo and later other rodent species that could infect laboratory rats transformed drug development. The short hepatic phase and life cycle of these parasites made them useful as animal models, a status they still retain. \"Plasmodium cynomolgi\" in rhesus monkeys (\"Macaca mulatta\") were used in the 1960s to test drugs active against \"P. vivax\".\n\nGrowth of the liver stages in animal-free systems was achieved in the 1980s when pre-erythrocytic \"P. berghei\" stages were grown in wI38, a human embryonic lung cell line (cells cultured from one specimen). This was followed by their growth in human hepatoma line HepG2. Both \"P. falciparum\" and \"P. vivax\" have been grown in human liver cells; partial development of \"P. ovale\" in human liver cells was achieved; and \"P. malariae\" was grown in chimpanzee and \"monkey\" liver cells.\n\nThe first successful continuous malaria culture was established in 1976 by William Trager and James B. Jensen, which facilitated research into the molecular biology of the parasite and the development of new drugs. By using increasing volumes of culture medium, \"P.falciparum\" was grown to higher parasitemia levels (above 10%).\n\nThe use of antigen-based malaria rapid diagnostic tests (RDTs) emerged in the 1980s. In the twenty-first century Giemsa microscopy and RDTs became the two preferred diagnostic techniques. Malaria RDTs do not require special equipment and offer the potential to extend accurate malaria diagnosis to areas lacking microscopy services.\n\n\"Plasmodium knowlesi\" has been known since the 1930s in Asian macaque monkeys and as experimentally capable of infecting humans. In 1965 a natural human infection was reported in a U.S. soldier returning from the Pahang Jungle of the Malaysian peninsula.\n\n\n"}
{"id": "23495775", "url": "https://en.wikipedia.org/wiki?curid=23495775", "title": "Huntington's Disease Outreach Project for Education at Stanford", "text": "Huntington's Disease Outreach Project for Education at Stanford\n\nThe Huntington's disease Outreach Project for Education at Stanford (HOPES) is a student-run project at Stanford University dedicated to making scientific information about Huntington's disease (HD) more readily accessible to patients and the public. Initiated by Professor William H. Durham in 2000, HOPES is a team of faculty members and undergraduate students at Stanford that surveys the rapidly growing scientific and clinical literature on Huntington's disease. They then present this information in a web resource that reflects the current scientific understanding of HD.\n\nThe HOPES website provides information about topics including the causes and symptoms of HD, existing drugs and supplements that may help HD patients, recent advances in HD research and lifestyle choices for managing HD. Articles summarize and synthesize recent research on HD for a non-technical audience. The website is designed for people of all ages and scientific backgrounds. Material ranges from interactive articles about basic genetics, written for children, to more comprehensive topics in molecular neuroscience, such as the potential for stem cells to treat or cure HD.\n\nIn June 2008 HOPES was honored with the first annual “Giving a Voice to HD” award from the Huntington's Disease Society of America (HDSA), which recognizes an individual or group who has helped to raise awareness about HD in the community.\nIn October 2018, HOPES was honored with the Community Advocate Award at the 12th Annual San Francisco Team Hope Walk\n\n"}
{"id": "37806911", "url": "https://en.wikipedia.org/wiki?curid=37806911", "title": "Interact Advocates for Intersex Youth", "text": "Interact Advocates for Intersex Youth\n\ninterACT or interACT Advocates for Intersex Youth, formerly known as Advocates for Informed Choice, is a 501(c)(3) nonprofit organization using innovative strategies to advocate for the legal and human rights of children born with intersex traits. The organization was founded in 2006 and formally incorporated on April 12, 2010.\n\ninterACT was founded in 2006 in Cotati, California. The organization is now based in Sudbury, Massachusetts. The board of directors includes Arlene Baratz, , Georgiann Davis, Emily Doskow, , Julie Greenberg, , Eric Lohman, Lynnell Stephani Long, Mani Mitchell, Karen Walsh, and Reid Williams. Staff members include Kimberly Zieselman, , Executive Director, and Anne Tamar-Mattis, , Legal Director.\n\ninterACT uses innovative strategies to advocate for the legal and human rights of children born with intersex traits, including media work and the development of youth leadership, in addition to strategic litigation. Issues of focus are informed consent, insurance, identity documents, school accommodation, discrimination, medical records retrieval, adoption, military service, medical privacy, refugee asylum, and wider international human rights.\n\nIn 2014, following testimony by then staff member Pidgeon Pagonis, Anne Tamar-Mattis was published on medical interventions as torture in healthcare settings, in a book by the Center for Human Rights & Humanitarian Law at American University Washington College of Law. In 2016, the United Nations Committee Against Torture asked the United States government to comment on reports of intersex medical interventions on infants and children, following submission of a report by interACT. As part of its submission, interACT stated that it is \"unaware of any jurisdiction in the U.S. that enforces its own FGM laws in cases where the girl undergoing clitoral cutting has an intersex trait\".\n\nIn July 2017, Human Rights Watch and interACT published a major report on medically unnecessary surgeries on intersex children, \"I Want to Be Like Nature Made Me\", based on interviews with intersex persons, families and physicians. The report found that \"Intersex people in the United States are subjected to medical practices that can inflict irreversible physical and psychological harm on them starting in infancy, harms that can last throughout their lives.\" The report calls for a ban on \"surgical procedures that seek to alter the gonads, genitals, or internal sex organs of children with atypical sex characteristics too young to participate in the decision, when those procedures both carry a meaningful risk of harm and can be safely deferred.\"\n\nOn May 14, 2013, interACT, The Southern Poverty Law Center, and pro bono counsel for the private law firms of Janet, Jenner & Suggs and Steptoe & Johnson LLP filed a lawsuit against South Carolina Department of Social Services (SCDSS), Greenville Health System, Medical University of South Carolina and individual employees for performing an irreversible and medically unnecessary surgery on an infant who was in the state's care at the time of the surgery.\n\nThe defendants sought to dismiss the case and seek a defense of qualified immunity, but these were denied by the District Court for the District of South Carolina. In January 2015, the Court of Appeals for the Fourth Circuit reversed this decision and dismissed the complaint, stating that, \"it did not 'mean to diminish the severe harm that M.C. claims to have suffered' but that a reasonable official in 2006 did not have fair warning from then-existing precedent that performing sex assignment surgery on sixteen-month-old M.C. violated a clearly established constitutional right.\" The Court did not rule on whether or not the surgery violated M.C.'s constitutional rights. State suits were subsequently filed. In July 2017, it was reported that the case had been settled out of court by the Medical University of South Carolina for $440,000, without admission of liability.\n\ninterACT Youth is a program for intersex youth, run by intersex youth. All members between 14 and 25 years old, have intersex traits, and are in a place where they are ready to speak out about their experiences. interACT Youth works to provide tomorrow's scholars and activists a platform for their vital perspectives. A product of this work entitled \"What We Wish Our Doctors Knew\" was the first of its kind: Intersex youth talking back to medical providers and caregivers. InterACT Youth is funded in part by Ms. Foundation and Liberty Hill Foundation.\n\nIn 2013, the then youth leadership coordinator, Pidgeon Pagonis, testified for interACT before the Inter-American Commission on Human Rights about the medical interventions they were subjected to as an intersex child, alongside Latin Americans Mauro Cabral, Natasha Jiménez and Paula Machado.\n\ninterACT has worked with MTV on the program \"Faking It\", notable for providing the first intersex main character in a television show, and television's first intersex character played by an intersex actor. In 2017, interACT began working with Belgian-born model Hanne Gaby Odiele to tackle social taboos and unnecessary surgeries.\n\ninterACT published a media guide on covering intersex issues in January 2017.\n\nHaving historically used the current clinical terminology of disorders of sex development, interACT issued a strong statement favoring the term intersex in 2016, citing increasing acceptance and public awareness.\n\nThe Interface Project is a tax-exempt 501(c)(3) nonprofit operating under the fiscal sponsorship of interACT. Founded in 2012, and currently curated by Jim Ambrose, The Interface Project features stories of people born with intersex traits - or variations of sex anatomy - under the banner: \"No Body Is Shameful\" ®.\n\n"}
{"id": "5152675", "url": "https://en.wikipedia.org/wiki?curid=5152675", "title": "Inverse care law", "text": "Inverse care law\n\nThe inverse care law is the principle that the availability of good medical or social care tends to vary inversely with the need of the population served. Proposed by Julian Tudor Hart in 1971, the term has since been widely adopted. It is a pun on inverse-square law, a term and concept from physics.\n\nThe law states that:\n\n\"The availability of good medical care tends to vary inversely with the need for it in the population served. This ... operates more completely where medical care is most exposed to market forces, and less so where such exposure is reduced.\" \n\nHart later paraphrased his argument: \"To the extent that health care becomes a commodity it becomes distributed just like champagne. That is rich people gets lots of it. Poor people don’t get any of it.\"\n\nThe Inverse Care Law is a key issue in the debate about health inequality. As Frank Dobson put it when he was Secretary of State for Health: \"Inequality in health is the worst inequality of all. There is no more serious inequality than knowing that you'll die sooner because you're badly off.\" \n\n\n\n"}
{"id": "12767417", "url": "https://en.wikipedia.org/wiki?curid=12767417", "title": "Laya Healthcare", "text": "Laya Healthcare\n\nLaya Healthcare is a health insurance company in Ireland. Its headquarters are in Little Island, County Cork. It is regulated by the Health Insurance Authority.\n\nBUPA Ireland entered the private medical insurance market in Ireland in 1997 ending the Vhi Healthcare monopoly which had existed for almost 40 years following the opening of the health insurance market in Ireland in 1994. In December 2006, BUPA Ireland announced that it would be leaving the Irish market as the High Court had left the Risk Equalisation scheme unchallenged. BUPA Ireland argued it could not afford to make a cash transfer to Vhi Healthcare as required under the Risk Equalisation Scheme. In February 2007 the QUINN group acquired BUPA Ireland and renamed it QUINN-healthcare.\n\nAs a new insurer, QUINN-healthcare was exempt from the Risk Equalisation payments for three years which was the normal concession granted to any new market entrant. In 1994 the Supreme Court ruled that the Governments Risk Equalisation specifications were incompatible with the \"Health Insurance Act 1994\" and must be set aside. But the \"Health Insurance (Miscellaneous Provisions) Bill December 2008\" proposed a new scheme of levies on all health insurance members to be used to fund a new system of tax credits for those over 50 who have private medical insurance.\n\nIn late 2009, the future of the entity came into question, as its holder, the QUINN group, fell into administration, and was looking to sell off its insurance assets in the UK and Ireland. A plan was considered by the Irish Government to buy QUINN-healthcare and merge it with Vhi Healthcare, but was not acted on.\n\nOn 23 December 2011 Dónal Clancy, Managing Director of QUINN-healthcare announced that a senior management bid enabled by Swiss Re as the re-insurer was successfully concluded with the administrators. Bruce Hodkinson of Swiss Re, confirmed that Elips Life (a wholly owned subsidiary of Swiss Re) would be the underwriter.\n\nThe company was re-branded in May 2012 as Laya Healthcare. Laya means \"Looking After You Always\". Laya Healthcare is currently the second largest health insurer in Ireland.\n\nOn 1 April 2015, AIG acquired Laya Healthcare.\n\n"}
{"id": "17249184", "url": "https://en.wikipedia.org/wiki?curid=17249184", "title": "Life-giving Spring", "text": "Life-giving Spring\n\nThe Mother of God of the Life-giving Spring or Life-giving Font (Greek: \"Ζωοδόχος Πηγή,\" \"Zoodochos Pigi\", Russian: \"Живоносный Источник\") is an epithet of the Holy Theotokos that originated with her revelation of a sacred spring (, \"hagiasma\") in Valoukli, Constantinople, to a soldier named Leo Marcellus, who later became Byzantine Emperor Leo I (457-474). Leo built the historic Church of St. Mary of the Spring over this site, which witnessed numerous miraculous healings over the centuries, through her intercessions, becoming one of the most important pilgrimage sites in Greek Orthodoxy. Thus the term \"Life-giving Font\" became an epithet of the Holy Theotokos and she was represented as such in iconography.\n\nThe feast day of the Life-giving Spring is celebrated on Bright Friday in the Orthodox Church, and in those Eastern Catholic Churches which follow the Byzantine Rite. Additionally, the icon of the Theotokos the \"Life-giving Spring\" is commemorated on April 4 / 17 in Slavic Orthodox churches.\n\nOutside the Imperial City of Constantinople, near the Golden Gate (\"Porta Aurea\") used to be found a grove of trees. A shrine was located there with a spring of water, which from early times had been dedicated to the Theotokos. Over time, the grove had become overgrown and the spring became fetid.\n\nThe traditional account surrounding the feast of the Life-Giving Spring is recorded by Nikephoros Kallistos Xanthopoulos, the last of the Greek ecclesiastical historians, who flourished around 1320. It begins with a miracle that occurred involving a soldier named Leo Marcellus, the future Byzantine Emperor Leo I. On April 4, 450, as Leo was passing by the grove, he came across a blind man who had become lost. Leo took pity on him, led him to the pathway, seated him in the shade and began to search for water to give the thirsty man. Leo heard a voice say to him, \"Do not trouble yourself, Leo, to look for water elsewhere, it is right here!\" Looking about, he could see no one, and neither could he see any water. Then he heard the voice again, \n\n\"Leo, \"Emperor\", go into the grove, take the water which you will find and give it to the thirsty man. Then take the mud [from the stream] and put it on the blind man's eyes... And build a temple [church] here ... that all who come here will find answers to their petitions.\"\n\nLeo did as he was told, and when the blind man's eyes were anointed he regained his sight.\n\nAfter his accession to the throne, the Emperor erected a magnificent church on this site, dedicated to the Theotokos, and the water continued to work miraculous cures, as well as resurrections from the dead, through the intercession of the Theotokos, and therefore it was called \"The Life-Giving Spring.\"\n\nHistorians Procopius and Cedrenus state that Emperor Justinian erected a new church, larger than the first, in the last years of his reign (559-560), utilizing materials that had remained after the erection of the Hagia Sophia. After the erection of the sanctuary, the Byzantines named the Gate that was situated outside the walls of Theodosius II \"Gate of the Spring\" ().\n\nAfter the Fall of Constantinople in 1453, the church was torn down by the Turks, and the stones used to build a mosque of Sultan Bayezid. Only a small chapel remained at the site of the church. Twenty-five steps led down to the site of the spring, surrounded by a railing. In 1547 the French humanist Petrus Gyllius noted that the church no longer existed, but that ailing people continued to visit the spring of holy water.\n\nAs a result of the Greek War of Independence of 1821, even the little chapel was destroyed and the spring was left buried under the rubble.\n\nIn 1833 the reforming Ottoman Sultan Mahmud II gave permission for the Christians to rebuild the church. When the foundations of the original church were discovered during the course of construction, the Sultan issued a second firman permitting not only the reconstruction of the small chapel, but of a large church according to the original dimensions. Construction was completed on December 30, 1834, and the Ecumenical Patriarch, Constantius II consecrated the church on February 2, 1835, celebrating with 12 bishops and an enormous flood of the faithful.\n\nOn September 6, 1955, during the anti-Greek Istanbul Pogrom, the church was one of the targets of the fanatic mob. The building was burned to the ground while the abbot was lynched, and 90-year-old Archimandrite Chrisanthos Mantas was assassinated by the mob.\n\nAnother small chapel has been rebuilt on the site, but the church has not yet been restored to its former size. The spring still flows to this day and is considered by the faithful to have wonderworking properties.\n\nThe feast day is observed on Bright Friday, that is, the Friday following Pascha (Easter). It is the only feast day which may be celebrated during Bright Week, as all other commemorations which happen to fall during this time are usually transferred to another day. The propers (hymns and prayers) of the feast are combined with the Paschal hymns, and there is often a Lesser Blessing of Waters performed after the Divine Liturgy on Bright Friday.\n\nThere is also a commemoration of the Icon of the Theotokos the \"Life-giving Spring\" observed on April 4 (Julian Calendar) / April 17 (Gregorian Calendar).\n\nThis type of icon spread throughout the Orthodox world, particularly in places where a spring was believed to be sacred.\n\nIn old Russia, continuing Greek traditions, there was a custom to sanctify springs that were located near churches, dedicate them to the Holy Mother, and paint icons of her under the title \"The Life Giving Spring\".\n\nA similar revelation of the Theotokos occurred in Estonia in the 16th century. The Pühtitsa Convent is located on a site where, according to a 16th-century legend, near the local village of Kuremäe, a shepherd witnessed a divine revelation of the Theotokos near a spring of water that is to this day venerated as holy and is famous for many miracles and healings. The icon, which was painted much later, is known as the \"Pühtitsa icon of the Mother of God \"To the spring\"\" (: Пюхтицкая икона Божией Матери «У источника»).\n\nIn the 9th century, Joseph the Hymnographer gave the title \"Zoodochos Pege\" (\"Life-giving Spring\") to a hymn (\"Theotokion\") for the Mother of God for the first time.\n\nApolytikion (Tone 3)\n\nKontakion (Plagal of Tone 4)\n\n"}
{"id": "5290678", "url": "https://en.wikipedia.org/wiki?curid=5290678", "title": "List of bean-to-bar chocolate manufacturers", "text": "List of bean-to-bar chocolate manufacturers\n\nThis is a list of companies which produce chocolate, not chocolates. That is, they process cocoa beans into a product in-house, rather than merely melting chocolate from another manufacturer.\n\nSome are large companies that own the entire process for economic reasons; others aim to control the whole process to improve quality, working conditions, or environmental impact.\n\n\n\n"}
{"id": "36674001", "url": "https://en.wikipedia.org/wiki?curid=36674001", "title": "List of submerged places in France", "text": "List of submerged places in France\n\nThis is a list of submerged places in France. Sometimes, entire villages are submerged under the waters of a reservoir. When the level of waters is low, the structures can emerge and be visible.\n\nVillages and municipalities under the waters of a reservoir. Sometimes, a new village or municipality is created with the same name in another location:\n\n\nLandmarks which are partially or completely under the waters of a reservoir. Some of them may be visible under some circumstances:\n\n\nSome landmarks have been moved partially or completely to another locations and saved from being submerged:\n\n"}
{"id": "4982305", "url": "https://en.wikipedia.org/wiki?curid=4982305", "title": "Mental disorders and gender", "text": "Mental disorders and gender\n\nGender is correlated with the prevalence of certain mental disorders, including depression, anxiety and somatic complaints. Major depression is twice as common in women. The lifetime prevalence rate of alcohol dependence is more than twice as high in men, and men are more than three times as likely to be diagnosed with antisocial personality disorder. There are no marked gender differences in the diagnosis rates of disorders like schizophrenia and bipolar disorder.\n\nSigmund Freud postulated that women were more prone to neurosis because they experienced aggression towards the self, which stemmed from developmental issues. Freud's postulation is countered by the idea that societal factors, such as gender roles, may play a major role in the development of mental illness. When considering gender and mental illness, one must look to both biology and social/cultural factors to explain areas in which men and women develop different mental illnesses. Societal patriarchy and gender roles have adverse effects on the psychological perceptions of both men and women. These gender roles may include the pressure on men not to show their emotions and the fact that women, on average, have lower self-esteem and sense of control than men. When certain factors, such as work outside the domestic sphere, are controlled, women and men tend to experience a full range of mental illnesses at approximately equal rates. In some cases when such factors were controlled, women showed lower rates of mental illness on the whole.\n\nThe object relations theory postulates that because women are mostly responsible for parenting, mothers emphasize the importance of relationships to their daughters while pushing their sons into independence. Sarah Rosenfield uses this theory to argue that males and females develop different types of symptoms when they are mentally ill. Men tend to display externalized symptoms, expressing problematic emotions in outward behavior. Women tend to develop internalized symptoms, where problematic feelings are directed towards the self. In accordance with the internalized-externalized dichotomy, women are more commonly diagnosed with mental illnesses such as anxiety, depression, and phobias. Once thought to be more common in women, borderline personality disorder has been found to be equally prevalent among both men and women. Men more commonly experience substance abuse, antisocial disorders, and violence. Both men and women are more likely to be institutionalized if their diagnosis is not typical of their gender (Martha Lang, 2006).\n\nIntimate partner violence (IPV) is a particularly gendered issue. Data collected from the National Violence Against Women Survey (NVAWS) of women and men aged 18–65 found that women were significantly more likely than men to experience physical and sexual IPV.\n\nThere have been numerous studies conducted linking the experience of being a survivor of domestic violence to a number of mental health issues, including post-traumatic stress disorder, anxiety, depression, substance dependence, and suicidal attempts. Humphreys and Thiara (2003) assert that the body of existing research evidence shows a direct link between the experience of IPV and higher rates of self-harm, depression, and trauma symptoms. The NVAWS survey found that physical IPV was associated with an increased risk of depressive symptoms, substance dependence problems, and chronic mental illness.\n\nA study conducted in 1995 of 171 women reporting a history of domestic violence and 175 reporting no history of domestic violence confirmed these hypotheses. The study found that the women with a history of domestic violence were 11.4 times more likely to suffer dissociation, 4.7 times more likely to suffer anxiety, 3 times as likely to suffer from depression, and 2.3 times more likely to have a substance abuse problem.\nThe same study noted that several of the women interviewed stated that they only began having mental health issues when they began to experience violence in their intimate relationships.\n\nIn a similar study, 191 women who reported at least one event of IPV in their lifetime were tested for PTSD. 33% of the women tested positively were lifetime PTSD, and 11.4% tested positive for current PTSD.\n\nAnother study found that in a group of women in a psychiatric inpatient hospital ward, women who were survivors of domestic violence were twice as likely to suffer depression as those were not. All twenty of the women interviewed fit into a pattern of symptoms associated with trauma-based mental health disorders. Six of the women had attempted suicide. Moreover, the women spoke openly of a direct connection between the IPV they suffered and their resulting mental disorders.\n\nThe direct psychological effects of IPV may contribute directly to the development of these disorders. In Humphreys' and Thiara's study, 60% of the women interviewed feared for their life, 69% feared for their emotional wellbeing, and 60% feared for their mental health. Some of the women discussed an undermining of their self-esteem, as well as an \"overwhelming fear and erosion of their sense of safety.\" Johnson and Ferraro (2000) refer to this overwhelming fear as \"intimate terrorism,\" decimating a women's sense of security and contributing to a worsening psychological state.\n\nHumphreys and Thiara (2003) refer to these consequential mental disorders as \"symptoms of abuse\". That sentiment is echoed by some survivors who don't feel comfortable identifying with loaded diagnoses such as depression or PTSD.\n\nScholars studying the effects of IPV on mental health note the necessity of screening for domestic violence in primary care patients. Roberts, Williams, Lawrence and Raphael (2008) conclude from their study that women who presented to doctors with somatic symptoms including abdominal pain and headaches are often subjected to a variety of investigative tests, without domestic violence ever being raised as a potential cause.\n\nMcLeer, Anwar, Herman and Maquiling (1989) find from their research that when protocol for detection of IPV was introduced, the rate of identification of battered women increased from 5.6% to 30% in one year. When followed up eight years later, only 7.7% were identified.\n\nRamsay, Richardson, Carter, Davidson and Feder (2002) found in a systematic review that about half to three-quarters of women in primary care responding to surveys believe that routine screening for domestic violence is acceptable, and a higher proportion is shown among women with a history of having been abused. In contrast, only a minority of nurses and doctors favored screening, shown in the results of two surveys.\n\nIntroduction of discussion of domestic violence into standard primary care may have revealing effects in itself. In one case, a domestic violence victim states that \"the introduction of the words 'domestic violence' to me, as hard as they were to swallow, were like a fresh light on my situation because it made me think differently about things.\"\n\nThiara and Turner (2000) find other obstacles to treatment of IPV survivors include physicians' belief that the abuser requires treatment rather than survivor, a sense of feeling unqualified to treat IPV survivors with the appropriate sensitivity and effectiveness (and therefore sidestepping questions that could lead to IPV disclosure), and overt victim-blaming.\n\nSurvivors themselves note the need for \"immediate and flexible services\" including 24/7 hotlines, as well as non-judgmental and victim-trusting approaches from physicians.\n\nAnother barrier to seeking treatment is that a threat often used against women IPV victims is that they are \"crazy\", and seeking treatment may seem to a survivor like a confirmation of this sentiment.\n\nEven when women have sought treatment, there are still problematic elements to existing treatment services. In mixed-sex hospitals, there have been worrying reports that suggest a lack of safety for women survivors.\n\nIt is crucial to adopt an intersectional approach when considering how to treat survivors. In Humphreys' and Thiara's study, they found that \"black and minority ethnic women were significantly more likely than white women to suffer substantial problems both emotionally and materially.\" This intersectional approach can include a push for more female doctors, as well as people of color in the medical profession.\n\nPosttraumatic stress disorder (PTSD) is among the most common responses a person can have in response to a traumatic event. Research has found that women have elevated rates of PTSD compared to men. According to epidemiological studies, women are two to three times are likely as men to develop PTSD. The lifetime prevalence of PTSD is reported to be about 10-12% in women and 5-6% in men. Women are also four times as likely to develop chronic PTSD compared to men. Additionally, women are at greater risk for developing PTSD despite exposure to fewer traumatic events than men. There are even observed differences in the type of symptoms endorsed by men and women. Women are more likely to endorse specific sub-clusters of symptoms, such as re-experiencing symptoms (e.g. flashbacks) and hyper-arousal symptoms. These differences have been found to be persistent across cultures. \n\nWhile PTSD is perhaps the most well-known psychological response to a trauma, depression can also develop following exposure to traumatic events. Women are twice as likely as men to experience depression; the lifetime prevalence of major depressive disorder (MDD) is 21.3% in women and 12.7% in men. \n\nSome have theorized that the observed gender differences are due to women being exposed to more high impact-traumas, such as sexual assault. Epidemiological studies have found that men are more likely to have PTSD as a result of experiencing combat, war, accidents, nonsexual assaults, disaster or fire, and witnessing death or injury. Meanwhile, women are more likely to have PTSD attributable to rape, sexual assault, sexual molestation, and childhood sexual abuse. However, despite the theorized explanation that gender differences were due to differential exposure rates to high impact traumas such as sexual assaults, a meta-analysis found that when excluding instances of sexual assault or abuse, women remained at greater for developing PTSD. Additionally, it has been found that when only looking at those who have experienced sexual assaults, women remained approximately twice as likely as men to develop PTSD. Thus, it is likely that exposure to specific traumatic events such as sexual assault only partially accounts for the observed gender differences in PTSD.\n\nResearch suggest that gender differences in exposure to traumatic events have serve as an explanation for observed differences in MDD. It has been indicated in research that women have been found to have two times the rate of sexual assault as men when sexual assault is defined as being pressured or forced into unwanted sexual contact. A history of sexual assault is thought to be related to increased rates of depression. For example, studies of survivors of childhood sexual assault found that the rates of childhood sexual assault ranged from 7-19% for women and 3-7% for men. This gender discrepancy in childhood sexual assault in turn contributed to 35% of the observed gender difference in adult depression. It is also thought that the increased likelihood of adverse traumatic experiences in childhood may also help to explain the observed gender difference in MDD. Studies show that women have been found to have an increased risk of experiencing traumatic events in childhood, especially childhood sexual abuse. This increased risk of experiencing traumatic events in childhood has been associated with an increased risk of developing depression, making women more sensitive to developing depression than men.\n\nBiological differences is a proposed mechanism contributing to observed gender differences in PTSD. Dysregulation of the hypothalamic-pituitary-adrenal (HPA) axis has been proposed for both men and women. The HPA helps to regulate an individual’s stress response by changing the amount of stress hormones released into the body, such as cortisol. However, a meta-analysis found that women have greater dysregulation than men; women have been found to have lower circulating cortisol concentrations compared to healthy controls, where men did not have this difference in cortisol. It is also thought that gender differences in threat appraisal might contribute to observed gender differences in PTSD as well by contributing to HPA dysregulation. Women are reported to be more likely to appraise events as stressful and to report higher perceived distress in response to traumatic events compared to men, potentially leading to an increased dysregulation of the HPA in women than in men. However, research looking at potential biological explanations for gender differences in PTSD is in its infancy, and further research is needed before conclusions can be drawn.\n\nAs with PTSD, there has also been some evidence of a biological difference between men and women that may contribute to the observed gender difference. Expanding on the research concerning the HPA and PTSD, one existing hypothesis is that women are more likely than men to have a dysregulated HPA in response to a traumatic event, like in PTSD. This dysregulation may occur as a result of the increased likelihood of women experiencing a traumatic event, as traumatic events have been known to contribute to HPA dysregulation. Differences in stress hormone levels can influence moods due to the negative effect of high cortisol concentrations on biochemicals that regular mood such as serotonin. Research has found that people with MDD have elevated cortisol levels in response to stress and that low serotonin levels are related to the development of depression. Thus, it is possible that a dysregulation in the HPA, when combined with the increased history of traumatic events, may contribute to the gender differences seen in depression.\n\nFor PTSD, genders differences in coping mechanisms has been proposed as a potential explanation for observed gender differences in PTSD prevalence rates. Studies have found that women tend to respond differently to stressful situations than men. For example, men are more likely than women to react using the fight-or-flight response. Additionally, men are more likely to use problem-focused coping, which is known to decrease the risk of developing PTSD when a stressor is perceived to be within an individual’s control. Women, meanwhile, are thought to use emotion-focused, defensive, and palliative coping strategies. As well, women are more likely to engage in strategies such as wishful thinking, mental disengagement, and the suppression of traumatic memories. These coping strategies have been found in research to correlate with an increased likelihood of developing PTSD. Women are more likely to blame themselves following a traumatic event than men, which has been found to increase an individual’s risk of PTSD. In addition, women have been found to be more sensitive to a loss of social support following a traumatic event than men. A variety of differences in coping mechanisms and use of coping mechanisms may likely play a role in observed gender differences in PTSD.\n\nThese described differences in coping mechanisms are in line with a preliminary model of sex-specific pathways to PTSD. The model, proposed by Christiansen and Elklit, suggests that there are sex differences in the physiological stress response. In this model, variables such as dissociation, social support, and use of emotion-focused coping may be involved in the development and maintenance of PTSD in women, whereas physiological arousal, anxiety, avoidant coping, and use of problem-focused coping may be more likely to be related to the development and maintenance of PTSD in men. However, this model is only preliminary and further research is needed.\n\nFor more about gender differences in coping mechanisms, see the Coping (psychology) page.\n\n\n"}
{"id": "32689929", "url": "https://en.wikipedia.org/wiki?curid=32689929", "title": "Mi-Case", "text": "Mi-Case\n\nMi-Case is a suite of web-based case management and operations management software provided by CASMACO Ltd. (trading as Mi-Case). Mi-Case is used in public and commercial sector organisations including criminal justice and behavioural health agencies to underpin a multi-agency/institutional approach to offender management, correctional services, drug and alcohol abuse, policing, probation, healthcare and the court system. The Mi-Case Upstream range is used by the upstream oil and gas industry to support incident management as well as risk and root cause analysis in production.\n\nMi-Case was launched in early 2006 by an international consulting and software firm, Mi-Services, headquartered in UK and US. Mi-Case was born out of the work done with UK Drug Intervention Programmes (DIPs), Drug Action Teams and treatment providers to assist them and other types of agencies and initiatives in their operations: to increase case management, demonstrate performance and enable best practice when sharing (offender/patient) data. The company was acquired later in the same year by a larger global consulting and systems integration company, Business & Decision. Since March 2012 it has operated as Casmaco Ltd., which is a majority owned subsidiary of Business & Decision Group, serving the USA, UK and continental European markets.\n\nMi-Case is a suite of web-based case management and operations management including: Mi-Case Criminal Justice, Mi-Case Behavioural Healthcare and Mi-Case Oil and Gas range of solutions. \nThe Mi-Case Criminal Justice and Behavioural Health have been designed to enable clients to be tracked through their cycle of interventions and maintain a continuity of treatment and/or attention. \n\nMi-Case is developed for:\n\nMi-Case can track and record key client information in relation to:\n\nThe Mi-Case approach to web based data sharing has also been applied to the commercial sector and is used by upstream oil and gas organisations to manage off shore / on shore data sharing on production equipment and shift management.\n\nMi-Case was originally developed using Active Server Pages but was later upgraded to Microsoft .NET. It sits on an Oracle database and has been upgraded to later versions of the database broadly as they have become available or widely used. Mi-Case uses 128 bit encryption of data being transmitted across the internet as part of its data security approach.\n\nMi-Case is used by 30 customers in the UK, most of which are directly linked to the UK Home Office directives and UK National Health Service (NHS) initiatives.\n\nIn February 2009 Business & Decision was awarded a $12.5m contract by the Maryland Department of Public Safety and Correctional Services (DPSCS) to implement Mi-Case to replace numerous information silos and legacy applications managing ‘offender case management’. The first three phases of this project went live in 2011. By fall 2012 Mi-Case is scheduled to provide all DPSCS agencies with accurate and timely access to integrated data captured throughout the lifecycle of an offender from arrest, booking, pre-trial, incarceration, and through supervision in the community. The solution also allows for information sharing with local law enforcement and other criminal justice partners. Mi-Case is used by all of DPSCS' divisions to manage the interaction with offenders in accordance with the State’s requirements of best practice and to obtain management information and reports.\n\n"}
{"id": "21703096", "url": "https://en.wikipedia.org/wiki?curid=21703096", "title": "Mike Aamodt", "text": "Mike Aamodt\n\nMichael G. Aamodt (born September 1, 1957) is an American industrial and organizational psychology professor at Radford University who has published more than 50 professional journal articles and presented more than 100 papers at professional conferences. \n\nAamodt is actively involved in SHRM and is the 2009 president of the New River Valley chapter of SHRM as well as the advisor for the Radford University Chapter. He received his B.A. in psychology from Pepperdine University in Malibu, California and both his M.A. and Ph.D. from the University of Arkansas. He has three nieces, Emily Aamodt, Kitty Aamodt, and Liz Aamodt who reside in California.\n\n\n\n\n"}
{"id": "37393679", "url": "https://en.wikipedia.org/wiki?curid=37393679", "title": "No Leprosy Patients in Our Prefecture Movement", "text": "No Leprosy Patients in Our Prefecture Movement\n\nThe , or No Leprosy Patients in Our Prefecture Movement, was a government funded Japanese public health and social movement which began between 1929 and 1934. Its mission was to systematically eliminate leprosy, (Hansen's disease), a readily transmissible, previously incurable, chronic infectious disease caused by M. leprae, from each prefecture in Japan. This was to be achieved by caring for those afflicted by the disease in government funded sanatoriums.\n\nIn 1927, the Japanese government planned the dissolution of leprosy communities (leper colonies). The district welfare officers of Aichi Prefecture, Mamoru Uchida and Soichiro Shiotani, studied the conditions of the communities of the Honmyoji Temple in Kumamoto Prefecture. Six patients wished to enter the Kyushu Sanatorium, (later Kikuchi Keifuen Sanatorium). However, they were refused by Matsuki Miyazaki, the director of the sanatorium. Uchida and Shiotani brought the patients to Kensuke Mitsuda at the Nagashima Aiseien Sanatorium. Together, the welfare officers and Mitsuda initiated the movement. In 1931, the concept was made law. In the same year, the Empress Teimei founded the Leprosy Prevention Association. Eiichi Shibusawa was its president. The birthday of the empress, around 25 June, marked the beginning of an annual Leprosy Prevention Week. In 1952, at the time of the death of the empress, the name of the Leprosy Prevention Association was changed to Tofu Kyokai.\nThe governor of each prefecture raised funds for the building of leprosy sanatoriums. The movement and its slogans, for example, \"donate 10-tsubo houses (33.058 square meters) to sanatoriums\", were publicised in newspapers, radio, film advertisements, and through religious groups, schools and other organisations. For example, a Jodo Shinshu school founded an association called Otani Komyokai to popularise the movement.\n\nPublic interest in the movement varied between prefectures and over time. The people of Tottori Prefecture were most supportive of the movement. Kiyotatsu Tatsuda, the governor of Tottori prefecture, raised funds for the movement, invited Mitsuda to give lectures about the movemen and built six houses in the Airakuen sanatorium to accommodate leprosy patients from Tottori Prefecture. Fukuoka Prefecture, Yamaguchi Prefecture, Miyagi Prefecture, Toyama Prefecture, Okayama Prefecture, Saitama Prefecture, Aichi Prefecture and Mie Prefecture were also supportive.\n\nThe forced hospitalisation increased the leprosy stigma of the patients, their families and their neighbourhoods. Some patients were transferred beyond their own neighbourhoods, increasing their isolation. The conditions in sanatoriums suffered from overcrowding. Food ran short. In 1936, riots occurred and some patients escaped.\n\nMasako Ogawa was a Japanese physician who worked at the Nagashima Aiseien sanatorium. In 1938, she wrote the book, \"Spring in a Small Island\" which later became a film. She wrote of her experiences in persuading leprosy sufferers in remote areas to be hospitalised. Some criticised Ogawa for accelerating the \"No leprosy patients our in prefecture\" movement and giving an impression that leprosy was to be feared. \nOn 31 December 1947, the Japanese government's Ministry of Interior Affairs, which had been responsible leprosy control, was abolished. After World War II, welfare officers were less involved because their role had induced stigma. Responsibility for control of leprosy was transferred to the prefectures. The names of patients previously reported to the head of police stations were reported to governors of prefectures. In November 1947, the Ministry of Welfare commented on the \"No leprosy patients in our prefecture\" movement. It stated that the elimination of leprosy was important for the building of a cultural state, and therefore, should be accomplished. Hospitalization should be commenced with the most contagious patients. \nIn 1949, the government advised that training courses for physicians and technicians should begin; physical examination of all citizens should occur and patients should be hospitalised, even on rumour. \nIn 1952, Matsuo Fujimoto, an hospitalised leprosy sufferer of Kumamoto, was tried, found guilty and executed for murder. His execution was contentious because some people felt he was being unfairly treated because he was a leper. \nBy 1955, the government bodies responsible for control of leprosy included the ministry of welfare; the prefectures; the departments of public health and medicine; the section of tuberculosis prevention and the national sanatoria. Working for the movement were medical schools and physicians, news media, women's groups, schools and religious groups. Physicians who lectured for the movement included Kensuke Mitsuda, Fumio Hayashi (doctor), Isamu Tajiri and Mamoru Uchida.\n\nThe following apology was issued by the Ohtani Sect of Jōdo Shinshu Buddhism:\n\nIn 2001, when the leprosy prevention law was ruled unconstitutional, the Prime Minister, the Minister of Welfare, and the National Diet published statements of apology to leprosy patients and their families. Several prefectural governors made apologies at public sanatoriums.\n\nIn 2001, compensation to patients hospitalised between 1960 and 1998 was legislated. The compensation varied between 8,000,000 yen to 14,000,000 yen per person.\n\n\n\n"}
{"id": "4397294", "url": "https://en.wikipedia.org/wiki?curid=4397294", "title": "Nursing Personnel Convention, 1977", "text": "Nursing Personnel Convention, 1977\n\nNursing Personnel Convention, 1977 is an International Labour Organization Convention.\n\nIt was established in 1977, with the preamble stating:\nHaving decided upon the adoption of certain proposals with regard to employment and conditions of work and life of nursing personnel...\n\nAs of 2013, the convention had been ratified by 41 states.\n\n"}
{"id": "1048389", "url": "https://en.wikipedia.org/wiki?curid=1048389", "title": "Pirbright Institute", "text": "Pirbright Institute\n\nThe Pirbright Institute (formerly the Institute for Animal Health) is a research institute in Surrey, England, dedicated to the study of infectious diseases of farm animals. It forms part of the UK government's Biotechnology and Biological Sciences Research Council (BBSRC). The Institute employs scientists, vets, PhD students and operations staff.\n\nIt began in 1914 to test cows for tuberculosis. More buildings were added in 1925. Compton was established by the Agricultural Research Council in 1937. Pirbright became a research institute in 1939 and Compton in 1942. The Houghton Poultry Research Station at Houghton, Cambridgeshire was established in 1948. In 1963 Pirbright became the Animal Virus Research Institute and Compton became the Institute for Research on Animal Diseases. The Neuropathogenesis Unit (NPU) was established in Edinburgh in 1981. This became part of the Roslin Institute in 2007.\n\nIn 1987, Compton, Houghton and Pirbright became the Institute for Animal Health, being funded by BBSRC. Houghton closed in 1992, operations at Compton are being rapidly wound down with the site due to close in 2015.\n\nThe Edward Jenner Institute for Vaccine Research was sited at Compton until October 2005.\n\nSignificant investment (over £170 million) is taking place at Pirbright with the development of new world class laboratory and animal facilities. The Institute has been known as \"The Pirbright Institute\" since October 2012.\n\n\nThe work previously carried out at Compton has either moved out to the university sector, ended or has been transferred to the Pirbright Site. The Compton site currently carries out work on endemic (commonplace) animal diseases including some Avian Viruses and a small amount of Bovine Immunology whilst Pirbright works on exotic (unusual) animal diseases (usually caused by virus outbreaks). Pirbright has National and International Reference Laboratories of diseases.\n\n25% of its income comes from a core grant from the BBSRC of around £11m. Around 50% comes from research grants from related government organisations, such as DEFRA, or industry and charities (such as the Wellcome Trust). The remaining 25% comes from direct payments for work carried out.\n\nThe Pirbright Institute carries out research, diagnostics and surveillance viruses carried by animals, such as foot-and-mouth disease virus (FMDV), African swine fever, bluetongue, lumpy skin disease and avian and swine flu farm animals. Understanding of viruses comes from molecular biology.\n\nIt carries out surveillance activities on farm animal health and disease movement in the UK.\n\nThe Institute had two sites at:\n\n\n"}
{"id": "20879301", "url": "https://en.wikipedia.org/wiki?curid=20879301", "title": "Postgraduate training in general dentistry", "text": "Postgraduate training in general dentistry\n\nThere are two forms of institution-based training for general dentistry available for dental school graduates in Australia: \n\nThere are three forms of institution-based training for general dentistry available for dental school graduates in Canada: \n\nThere are two forms of institution-based training for general dentistry available for dental school graduates in the United States:\n\nIn the UK, most postgraduate training in dentistry are specialized. Some universities offer postgraduate degrees in general dentistry, with an emphasis on clinical research.\n\nAll of the programs below can be 1-year programs with an option available to continue for a second year, or they may be a two-year program from the start. They allow the new dentist to further hone his or her skills in most of the traditionally defined disciplines of dentistry while at the same time increasing one's speed and refining one's techniques. These programs also afford trainees the opportunity to learn from the attending dentists who serve a supervisory role, something generally unavailable in private practice.\n\nWhile a GPR is a hospital-based program, an AEGD is usually not and the differences between the two types of programs are generally a result of this distinction. AEGDs are usually based in postgraduate dental school clinics. Both types of programs afford the trainee with a larger patient pool than he or she was exposed to in dental school as an undergraduate; while dental students will typically treat 2 or 3 patients a day in multiple-hour-long sessions, these postgraduate programs are constructed so that trainees may see anywhere from 8-15 patients a day, or even more. They emphasize restorative dentistry, fixed and removable prosthodontics, orofacial pain, and dental implants.\n\nBoth DO and GDR programs are 1-year long commitments and are usually based in a hospital setting. These programs provide a dentist with a wide range of experiences including oral surgery, oral pathology, oral medicine, and treating medically compromised patients.\n\nPrograms will often emphasize the importance of managing comprehensive dental treatment plans and adjusting them based on the patient's medical condition. During training, residents may be faced with the task of managing patients that require dentistry in a hospital setting due to a compromised medical condition. Medical management of dental patients may be emphasized in weekly grand rounds and rotations through anesthesia, internal medicine, and the hospital emergency department. Some programs also provide rotations in family medicine and otolaryngology. These rotations not only increase the trainee's knowledge and experience, but also allow physicians, resident or attending, to see how dentistry and medicine are related, permitting a better referral relationship in future practices. This relationship is best demonstrated in tumor boards where both medical and dental residents discuss and treatment plan head and neck cancer patients. GPR residents may also become familiar with performing dental/oral surgical procedures in the operating room and managing the patient's stay while in the hospital. Rotation through the dental specialties increases the resident's ability to handle situations in private practice without referral to a specialist.\n\nThese programs are usually 1-year in length and are designed very similar in structure to an AEGD program. Emphasis is placed on restorative dentistry, fixed prosthodontics, removable prosthodontics, orofacial pain, and dental implants.\n\nEmphasis is on enhanced general practice skills to allow for broader patient care to underserviced areas. Clinical experience is usually obtained from the residency clinic in the department of dentistry, and during rotations including off site rural experience.\n\nThe goal of the program is to provide the recent dental graduate with a broad multidisciplinary approach to the clinical practice of dentistry. The postgraduate trainees are thus under the constant supervision and guidance of members of the attending staff. Experience is gained in general practice and specialties. The specialties represented are: Endodontics, Oral and Maxillofacial Surgery, Oral Medicine, Orthodontics, Periodontics, Prosthodontics and Pediatric Dentistry. \n\nIn general, GPR, DO/GDR, and MPT programs pay higher stipends than do AEGD, SEGD, and GradDipClinDent programs; this is because the former residents take call and answer consults. While on call, the residents are expected to manage dental as well as some head and neck trauma reporting to the ER in accordance with hospital guidelines\n\n"}
{"id": "4671451", "url": "https://en.wikipedia.org/wiki?curid=4671451", "title": "Quentin Collins", "text": "Quentin Collins\n\nQuentin Collins is the name of several characters featured in the 1966–1971 ABC cult TV Gothic horror-soap opera \"Dark Shadows\". All variations of the character have been played by actor David Selby.\n\nThe first Quentin Collins is actually the third one shown in the TV series. This version of Quentin was first introduced in episode #1109 in a storyline commonly referred to as the \"1840 flashback\".\n\nIn the 1840 storyline, Quentin Collins was one of two brothers living at the Gothic mansion known as Collinwood Mansion in the fictional town of Collinsport, Maine. Born in 1808, as the favorite son of his mentally troubled father Daniel Collins, Quentin was the head of the Collins Family, and stood to inherit the entire family fortune. This position brought him into frequent conflict with his scheming brother, Gabriel (Christopher Pennock). Quentin was married to a woman named Samantha Drew (Virginia Vestoff), with whom he had a son named Tad (David Henesy).\n\nAround 1839, Quentin had an affair with Joanna Mills. After he broke off with her, he took Tad on an ocean voyage, on which Quentin became close friends with a fellow passenger, Gerard Stiles (James Storm). His wife Samantha eventually discovered the affair and secretly murdered Joanna, making it look as if she had killed herself.\n\nQuentin was interested in the study of witchcraft. He was not part of a witches coven, but is eventually framed by Gerard (who was possessed by the spirit of the 17th-century warlock Judah Zachery) and sentenced to execution. Quentin also fancied himself an amateur inventor and used his knowledge of the occult and supernatural powers to develop a device that he called his \"Stairway Through Time\". He persistently labored to build his staircase in a cellar room at Collinwood. It was Quentin's belief that ascending the staircase could open a dimensional portal through which individuals could view and even interact with the future. What Quentin never realized, however, was that his staircase also created a divergent parallel reality (This functioned as the genesis for several storylines which took place in parallel universes). Quentin's devotion towards his work gave Gerard the means by which to manipulate and ultimately betray him.\n\nIn early 1840, Quentin, Tad, and Gerard Stiles had been off at sea for several months. Gerard comes back to Collinwood with the news for Quentin's wife, that her husband and son Tad had been lost at sea.\n\nWhen Samantha thought that her husband and son were dead, she began spending more and more time with Gerard. Over time, they fell in love, and he asked her to marry. On the day they got married, Quentin returned. He was shocked to discover that his wife had thought him dead, and had married his best friend.\n\nWhile his wife decided whom she wanted, either him or Gerard; Quentin began having an affair with his son's governess, Daphne Harridge (Kate Jackson), the sister of Quentin's former lover, Joanna Mills.\n\nGerard meanwhile had fallen prey to the spirit of a disembodied warlock named Judah Zachary. Judah wanted revenge against the entire Collins family for sentencing him to death for witchcraft in the late 17th century. Judah's spirit took possession of Gerard Stiles and used him as a secret weapon against the Collins family. Working alongside a disreputable mortician named Lamar Trask (Jerry Lacy), who was the son of the 1790s witch hunter Reverend Trask (also played by Lacy), Gerard convinced the local authorities that Quentin was a Satanist and that he had used his knowledge of the dark arts to murder a young woman named Lorna Bell, as well as his own brother-in-law, Randall Drew. Quentin was arrested on charges of Witchcraft and murder, and taken to jail. When his ailing father heard the news concerning Quentin's arrest, he changed his will, leaving the entire Collins family fortune to Gerard Stiles.\n\nQuentin was convicted of witchcraft and sentenced to death by beheading. However, with the aid of Valerie Collins (Lara Parker), and his cousins Desmond (John Karlen) and Barnabas (Jonathan Frid), Quentin escaped being beheaded. At the last minute, Gerard was shot. At the same moment, the head of Judah Zachary burned up in front of the judge in the case. With its destruction, Gerard was free from the possession and confessed that Quentin was innocent, admitting that Judah was responsible for all the deaths. After asking for forgiveness, Gerard died in Quentin's arms. Quentin having been cleared of all charges, eloped with Daphne Harridge. The two left Collinsport forever to make a new life for themselves.\n\nThe second Quentin Collins was actually the first one featured in the series, and is arguably the more popular of the two versions among fans, owing primarily to the fact that he is a werewolf. (When Quentin became a werewolf, the character was played by Alex Stevens.) He was born in 1870, and had three siblings; two brothers, the eldest Edward (Louis Edmonds), the youngest Carl (John Karlen), and older sister Judith Collins (Joan Bennett). He had loved two children like his own, his niece and nephew, Jamison (Henesy) and Nora (Denise Nickerson). This version of Quentin was the great-nephew of the first Quentin Collins and whose lineage includes several supporting characters peppered throughout the series, including his great-grandchildren Amy (Nickerson), Chris, and Tom Jennings (Don Briscoe).\n\nThis Quentin Collins originally appeared in December 1968 as an angry, silent, malevolent spirit haunting the playroom in the west wing of Collinwood (along with the ghost of his lover Beth Chavez). Quentin and Beth's spirits befriended the children David Collins (Henesy) and his friend and distant cousin, Amy Jennings. David strongly resembled Jamison Collins. Eventually, they took possession of both children and used them to serve their own interests.\n\nIn an effort to save his family, the former vampire known as Barnabas Collins, used the cosmological principles of the I Ching to send his spirit backwards through time to the year 1897 (Unfortunately for Barnabas, this meant becoming a vampire once again as such was his physical state during that time period). This series of episodes is commonly referred to as the \"1897 flashback\".\n\nLike the first Quentin, the second also was secretly involved in the occult and practiced witchcraft. He was a member of a witches coven headed by his best friend, the Collins family lawyer Evan Hanley (Humbert Allen Astredo).\n\nIn 1897, Quentin had earned himself a reputation of being the black sheep of the family – due largely in part to his marriage and subsequent abuse of a girl named Jenny (Marie Wallace). Quentin's affair with his brother Edward's wife Laura (Diana Millay), and his running off to Egypt with Laura resulted in Jenny Collins going insane. While Quentin was away, Jenny had two babies, a boy and a girl. Along with Beth Chavez, Edward and Judith Collins conspired to keep Jenny hidden from the rest of the family by imprisoning her inside of the tower room at Collinwood. Edward and Judith wanted to keep any knowledge of the babies from their father and the rest of the world. To that end they gave Jenny's children to Ms. Filmore in Collinsport, who would raise the children as her own. Amy, Chris, and Tom Jennings are descendant from the female child (Lenore Filmore).\n\nWhen Quentin returned from Egypt in the spring of 1897, his wife was nowhere to be found. He turned his attention to his grandmother Edith Collins (Isabella Hoopes), who was ill and near death. Quentin hoped that she would leave the Collins wealth and control of Collinwood to him. When she dies, in an attempt to make that hope a reality, Quentin had Jamison Collins steal the will from the lining of Edith Collins' coffin. The will left everything to Edith's favorite, Edward. Quentin was only guaranteed a room at Collinwood for the rest of his life, but given no money. So Quentin went so far as to have Evan Hanley get Sandor Racosi (Thayer David) to make a fake will.\n\nQuentin was being haunted by the ghost of Edith, and thought that Barnabas Collins was plotting against him. Being suspicious of his mysterious cousin from England, and believing he would try to interfere in his plans, Quentin and Hanley did a black magic ritual asking for help to defeat their enemies. Using the innocence of his young nephew Jamison as a focal point, they manage to summon a powerful spirit. The ritual resurrected from Hell the powerful witch Angelique (Lara Parker). But due to her own interests in Barnabas, Angelique wouldn't help.\n\nBarnabas Collins stole the fake will, and had Sandor now under his vampiric control to change it, leaving everything to Judith Collins. Angelique warned Barnabas about Quentin being out to destroy him. Quentin had Jamison steal Barnabas’s cane which he then uses in a ritual to cause Barnabas great pain. Had Barnabas been mortal instead of a vampire, the ritual would have killed him.\n\nSoon after, Quentin's insane wife Jenny escaped from the tower room and stabbed Quentin to death. Quentin possesses the body of Jamison. Barnabas, not wanting to fail at his mission to save the future Collins family from Quentin's ghost, asked Angelique to restore Quentin to life. However, Angelique was jealous of Barnabas latest love interest Rachel Drummond. So she turned Quentin into a zombie, and had him try to take Rachel back to the grave with him.\n\nDue to Jamison's strange behavior of acting like Quentin, Edward Collins decides to send Jamison and Nora to the Worthington Hall Boarding School operated by Reverend Gregory Trask. Barnabas tried to resurrect Quentin, but the ritual failed because Quentin wouldn't leave Jamison's body. Yet, Barnabas was able to save Rachel. Reverend Trask's prayer was apparently answered when Quentin was restored to life, and Jamison was no longer possessed by Quentin, although Angelique was apparently the one that actually used her powers to do it for her own reasons.\n\nAfter returning from the dead, Quentin had increased paranoia about Jenny trying to kill him again. It was at this time that Magda (Grayson Hall) and Sandor Racosi (Thayer David) discovered that Jenny was still alive. It turns out that she was Magda's sister, a gypsy that had run away from home and married Quentin. None of the Collins family had known that she was a gypsy, so it was a shock to Judith, Edward, and the rest of the family.\n\nMagda threatened to curse Quentin if he did anything more to hurt her sister. Quentin, however, was more worried about Jenny hurting him or his beloved Beth Chavez. When Jenny catches Quentin and Beth kissing, she attacks him with a knife. In self-defense he kills Jenny. With the help of his brother Edward, in order to protect the Collins name, Jenny's death is covered up as an accidental fall down the stairs. Magda finds a button from Quentin's suit in Jenny's hand, and can easily tell that Jenny didn't die from a fall, but was strangled. So she makes plans to curse Quentin.\n\nQuentin, believing that he was already cursed, and thinking he was haunted by the ghost of Jenny, went to Magda and tried to persuade her to lift the curse. His sister Judith gave him $10,000, in return for Quentin signing away his right to live at Collinwood. Magda and Sandor took the money, and said the curse was lifted, having a drink to the agreement. Then Magda threw the money back at him and informed Quentin that the drink he had just consumed contained the curse.\n\nQuentin soon discovered that he had inherited a curse that transformed him into a werewolf whenever the moon was full. He had no memory of what he did as a werewolf. The transformation itself was painful and unbearable for Quentin. He was tormented by the blood of innocent victims he found on himself the morning after. Barnabas, sympathetic to Quentin's plight, tried to help him overcome the curse, but to no avail. He did, however, succeed in altering Quentin’s destiny, preventing the creation of the future timeline where Quentin’s spirit plagued Collinwood.\n\nQuentin felt hopeless and contemplated suicide as the only way out. But Magda, having discovered that she had not only cursed Quentin, but also her sister Jenny's children for all eternity, decided to try to help find a cure for Quentin. Magda had heard of a magical hand possessed by a gypsy tribe controlled by King Johnny Romano. Magda went away to find the magical hand of Count Petofi and stole it. She returned to Collinwood, with the hand, in hopes of lifting the werewolf curse from Quentin and his first-born male descendants. However, the hand's powers were uncontrollable, and it did as it pleased.\n\nThen the powerful warlock Count Petofi, and his servant Aristede arrived in Collinsport, looking for his all-powerful hand. After causing much trouble for the Collins family by possessing Jamison Collins with his own spirit, Petofi pressured Barnabas and Quentin to yield the magical hand. Barnabas and Quentin felt compelled to do so or risk Jamison dying, and with him the entire future of the Collins family. So they gave Petofi back his hand.\n\nReverend Gregory Trask had become suspicious of Quentin and his strange behavior. He discovered Quentin with chains in his room, and Quentin confessed that he was the werewolf, and wanted to be killed. Trask, wanting to make sure Quentin was the werewolf, took him to a cell in the basement, and waited for the moon. Yet, Quentin didn't change that night. Upon arriving back at his room, he met Count Petofi, who showed Quentin his portrait, and that it had changed into the werewolf instead of him. Petofi said he would expect the debt of curing Quentin to be paid at some point in the future.\n\nNot only did the portrait cure Quentin of lycanthropy, but he was also rendered immortal due to the efforts of the artist Charles Delaware Tate (Roger Davis), whose powers were a gift of Count Petofi. In a pastiche of Oscar Wilde's \"The Picture of Dorian Gray\", at the request of Count Petofi, Tate painted a portrait of Quentin Collins. So long as the portrait remained intact, Quentin would remain forever young, and the portrait would change into the werewolf instead of Quentin himself.\n\nQuentin soon discovered that the price Count Petofi expected to be paid was his now immortal body. Count Petofi wanted to escape the gypsies, who wanted his powerful hand for themselves. Petofi used his powers to swap bodies with Quentin. Using Barnabas Collins' method of time travel, the I Ching wands, Petofi travelled 70 years into the future. In Quentin's body he would be forever young, and free from any threat from his gypsy enemies. However, something went wrong with Petofi's spell. The powers returned to the hand in his old body. Quentin used the powers of the hand to reverse the body swap. Before Petofi could swap bodies again, Quentin left Collinsport. Petofi apparently died in a fire at Tate's studio before he could possess Barnabas Collins' body, whom he realized also existed in the future.\n\nQuentin was still alive and wandering the streets of Collinsport by the year 1969. An amnesiac calling himself Grant Douglas, he reunited with Barnabas Collins and soon regained his true memories. He worked closely alongside Barnabas and his trusted companion Dr. Julia Hoffman (Grayson Hall), and aided them against the likes of Angelique and the Cult of the Leviathans.\n\nDuring this time, Quentin discovered that his lycanthropy had passed along his family line, affecting his descendant Chris Jennings (Chris' twin brother, Tom, ironically enough became a vampire).\n\nIn 1971, Metro-Goldwyn-Mayer released \"Night of Dark Shadows\", a sequel to the 1970 film \"House of Dark Shadows\". Although both films presumably share continuity with one another, they exist independently of the television series. Once again, David Selby resumed the role of Quentin Collins as well as that of his character’s ancestor, Charles Collins. In \"Night of Dark Shadows\", Quentin Collins is an artist who inherits Collinwood after the passing of the last remaining family member, Elizabeth Collins Stoddard.\n\nAlong with his wife Tracy (Kate Jackson), Quentin becomes the victim of Angelique – a witch who died over a century ago. Angelique's spirit operates in tandem with that of her 19th century lover, Charles Collins and slowly subverts Quentin's will. Before long, Quentin's soul is consumed by the evil of Charles Collins and he begins stalking his wife and friends.\n\nBased on a stage play performed at a \"Dark Shadows\" convention, \"Return to Collinwood\" is an audio drama written by David Selby's son, Jamison Selby and Jim Pierson centered around Quentin Collins return to Collinwood. It starred most of the original cast, including David Selby (Quentin Collins), Kathryn Leigh Scott (Maggie Evans), John Karlen (Willie Loomis), Nancy Barrett (Carolyn), Lara Parker (Angelique), and many more.\n\nIn 2006, Big Finish Productions continued the \"Dark Shadows\" saga with an original series of audio dramas, starring most of the original cast, with the addition of David Selby's son Jamison, and other talented voice actors. Like \"Return to Collinwood\", Quentin Collins is the central character, and the first season dealt with Quentin's return to Collinwood, and his attempts to re-establish the Collins family.\n\nThe first season comprises four discs, featuring David Selby (Quentin Collins), Lara Parker (Angelique), Kathryn Leigh Scott (Maggie Evans), and John Karlen (Willie Loomis). More information and online ordering can be found at .\n\nMore recently, the character has appeared in a series of audio plays produced by Big Finish Productions.\n\nSeason 1\n\nSeason 2\n\nDramatic readings\n\n\n\n"}
{"id": "17619560", "url": "https://en.wikipedia.org/wiki?curid=17619560", "title": "Registered Dental Nurse", "text": "Registered Dental Nurse\n\nIn the United Kingdom, a Registered Dental Nurse (RDN) works as part of a dental team in a variety of clinical and non-clinical settings. From 30 July 2008, all qualified Dental Nurses in the UK must be registered with the General Dental Council (GDC) to continue working legally and must hold the NEBDN National Diploma or another recognised qualification. \"Grandparenting\" arrangements were in place 2006–08 to allow unqualified dental nurses to register on the basis of experience.\n\nThe most up-to-date recognised qualifications that lead to registering with the GDC are specified by the General Dental Council.\n\nDental hospitals and further education colleges run courses on a full-time and part-time basis.\n\nA small proportion of student Dental Nurses start their career in a hospital. They will attend lectures, usually at a school of dental nursing several times a week. Their practical experience is gained from placements on specialists clinics within the hospital, before qualifying.\n\nThe British Association of Dental Nurses is the professional organisation representing dental nurses in the UK – whether qualified or unqualified, working in General Practice, hospital, community, the armed forces, industry, practice management or reception. This Association supports dental nurses themselves and represents the interests of Dental Nurses at all levels. BADN Executive Committee is made up of dental nurses elected by the members.\n\nThe National Dental Nursing Conference, held each year, provides an opportunity for members of the dental team to meet with colleagues, socialise and further their professional education. The Conference is held in a different location each year. Current BADN members receive a discount on Conference registration fees.\n\nBADN members in all categories have access to the digital quarterly journal “The British Dental Nurses’ Journal”. In addition, members have access to the legal helpline, as well as information and support, a members only area of the BADN website, and a range of discounts and special offers. \n\nFull membership is available with or without indemnity cover. Student Associate e-membership is also available to Student Dental Nurses on or awaiting a place on a recognised course.\n\n\n"}
{"id": "4272623", "url": "https://en.wikipedia.org/wiki?curid=4272623", "title": "René Rachou", "text": "René Rachou\n\nRené Rachou was a Brazilian physician and researcher on malaria who was the director of the Institute of Malariology of the Oswaldo Cruz Institute in Rio de Janeiro. He also worked with the Pan-American Health Organization. The Institute was moved to Belo Horizonte in 1955, and, after his death, in 1965, it was renamed Centro de Pesquisas René Rachou in his honor.\n\n"}
{"id": "30003939", "url": "https://en.wikipedia.org/wiki?curid=30003939", "title": "Retinal vasculitis", "text": "Retinal vasculitis\n\nRetinal vasculitis is inflammation of the vascular branches of the retinal artery, caused either by primary ocular disease processes, or as a specific presentation of any systemic form of vasculitis such as Behçet's disease, sarcoidosis, multiple sclerosis, or any form of systemic nectrozing vasculitis such as temporal arteritis, polyarteritis nodosa, and granulomatosis with polyangiitis, or due to lupus erythematosus, or rheumatoid arthritis. Eales disease, pars planitis, birdshot retinochoroidopathy (autoimmune bilateral posterior uveitis), and Fuchs heterochromic iridocyclitis (FHI) can also cause retinal vasculitis. Infectious pathogens such as \"Mycobacterium tuberculosis\", visceral larva migrans (\"Toxocara canis\" & \"Toxocara cati\") can also cause retinal vasculitis.\n\nRetinal vasculitis presents as painless, decrease of visual acuity (blurry vision), visual floaters, scotomas (dark spot in vision), decreased ability to distinguish colors, and metamorphopsia (distortion of images such as linear images).\n\n\nRetinal vasculitis is very rare as the only presenting symptom. Often there is sufficient systemic evidence to help the physician decide between any one of the aforementioned possible systemic diseases. For those patients who present with only vasculitis of the retinal vessels, great investigative effort (Chest X-ray, blood test, urinary analysis, vascular biopsy, ophthalmology assessment, etc.) should be undertaken to ensure that a systemic disease is not the hidden culprit.\nOphthalmic examination may reveal neovascularization (creation of new vessels in the retina), retinal vessel narrowing, retinal vessel cuffing, retinal hemorrhage, or possible vitritis (inflammation of the vitreous body) or choroiditis (inflammation of the choroid).\n"}
{"id": "12792961", "url": "https://en.wikipedia.org/wiki?curid=12792961", "title": "Sampada Gramin Mahila Sanstha", "text": "Sampada Gramin Mahila Sanstha\n\nSampada Gramin Mahila Sanstha (SANGRAM) is a voluntary organization that was co-founded by activist Meena Seshu. It works at the grassroots level with activists, volunteers and paid workers. It is slowly gaining importance as a practical training ground for other NGO’s and GO’s interested in working on HIV/AIDS in a rural context. SANGRAM started its work with women in prostitution and sex work from South Maharashtra and North Karnataka in 1992 and has since fanned out among diverse populations. SANGRAM is based in Sangli district, which has the highest incidence of HIV/AIDS in Maharashtra after Mumbai. \n\nOne settlement in one town: from this small beginning in 1992, the peer education programme has grown to span six districts in Maharashtra and the border areas of North Karnataka. About 120 peer educators drop off 350,000 condoms to 5,500 women each month. These women include devadasis, streetwalkers, and housewives in sex work, \"flying\" sex workers working in different locations, brothel keepers, PLHA etc.\n\nThe locations that the peer education programme spans are as diverse as the women themselves. They range from small hutments to sturdy homes in industrial centres like Karad, where household women turn to prostitution and sex work on market days. They include textile towns like Ichalkaranji, popularly known as the Manchester of India, and truck stops like Pethnaka on national highway no 4, where women from nearby villages work from midnight to dawn. They cover \"dhabas\" and cloth cabins, brothels and lodges.\n\nUnlike programmes that view sex workers as carriers of HIV, SANGRAM’s peer education programme sees a woman in prostitution and sex work as an individual who can be empowered to become an agent of change for herself and her community. This vision is based on two underlying premises:\n\n\nIt frames HIV within a context of sexuality, gender and rights. For instance, condoms are viewed as life-saving equipment that women in prostitution and sex work must have access to – by right. Workers are trained on issues such as law, inheritance, property rights and other gendered issues related to HIV.\n\nWhen the peer education programme began in 1992, it was run by SANGRAM. Since 1996, the peer intervention has been run by VAMP, a collective of women in prostitution and sex work. As part of its responsibilities, VAMP runs and manages the peer intervention in the six districts where it began: Sangli, Satara, Kolhapur, Solapur, Bagalkot, Belgaum. “Once VAMP was formed, we actually decided to close down SANGRAM.” But the women perceived it as abandonment. “They would tell us, 'Because we are sex workers, you want us to form our own organization and work on our own,' recalls SANGRAM staff.\n\nVAMP members – all of whom are women in prostitution and sex work – said they felt comfortable managing the community on their own, but needed help with back-office work: writing proposals, managing accounts, dealing with the Charity Commissioner etc. “We will manage the community at all levels and only come to you in a crisis,” VAMP members told SANGRAM staff. It was then decided that the two organizations should collaborate – with each bringing its strengths and meeting the other’s needs.\n\nWomen in prostitution and sex work are champions at orally keeping track of money and handling salary and other disbursals – but putting it on paper is another story. “The women are scared of paperwork.” “They need to learn that they don’t need to be scared – but this is an alien space for them.” An earlier attempt to train the women did not work. “The energy dipped.” Now, for each project that VAMP runs, its members select a co-ordinator who will put their words on paper.\n\nReading, writing, and doing paperwork is a need that women in prostitution and sex work often need help with, simply because many of them have never been to school. But not being able to read or write does not mean that they cannot think. Working from its philosophy of building capacities, SANGRAM proposed a unique solution to the paperwork problem:\n\n\nSANGRAM web site\n"}
{"id": "48429203", "url": "https://en.wikipedia.org/wiki?curid=48429203", "title": "Sandow Chiktan", "text": "Sandow Chiktan\n\nSandow is the highest village of Chiktan block, situated away in the east of district headquarter Kargil Ladakh (\"Jammu and Kashmir \"). Sandow is surrounded by villages-Lamsoo in the west, Yokma Kharboo and Shakar in the south and Lhalung Batalik in the north. Sandow falls under Shakar constituency zone. The village is from Kargil town, located on 34.55 latitude and 76.38 longitude and at an altitude of above mean sea level. The village was settled some around 300 years before, on a syncline of a complex range of Himalaya.\n\nSandow has very cool climate with temperature dropping to in winter. Very cool normal wind makes life harsh there in winter. In summer temperature rises to 20. Three main seasons i.e. spring, autumn and winter are very cold. In winter snowfall of over 1 feet will be there which is the main source of water for the summer. There is a large like namely Shashi Lake 2 km above the village which stores snow water melted in spring and early summer to ensure continuous water supply for the whole summer.\n\nAgriculture is very important and main work of the villagers. Crops like Barley, wheat and pea are cultivated. Agricultural works begins in mid-April until August. There are canals to water fields as there used to be very less rainfall. Until a few years back all agricultural works like ploughing, threshing, grinding to flour are used to be done by peoples with help of animals but now machines replaced grinding and threshing.\n\nSandow is in between mountains and is very rugged. Small agricultural fields are made in the mountains alongside the stream. Soil here is not productive, basically soil is very new, formed by degradation and erosion of mountains peoples have made the soil cultivable by fediing the soil with local manures whole village and field are in slope of mountains.\n\nThere is a middle school in the village. Literacy rate of the village is above 80% and most of the students are pursuing their higher studies in various parts of India. A secondary school is located away from the village in another village Shakar.\n\nThere are about 40 households in the villages and population is roughly 400.\n\nShashi Lake (popularly known as Shashi Tso) is one of the unexplored lakes of Kargil located 08 km from village Sandow. It is a beautiful artificial lake located in upper mountain range of Shashi at an altitude of 4,200 metres (14,000 ft) surrounded by mountains on all sides. The lake is snow fed so its volume changes with seasons reaching at peak volume in June. The lake remains in frozen state for half of the year. The Lake is seasonal and it gets filled up in Months of May–June when snow starts to melt in the surrounding mountains. It is famous for its scenic beauty, cool weather, fresh icy water and adventures like Kayaking and trekking. It is base camp for trekking to Barbanchanla peak which is the highest peak of Shakar-Chiktan. The area surrounding the lake are grazing grounds for livestock of surrounding villagers and wild animals like fox, deer, ibex, wolves etc. could be seen around the lake drinking water.\n\nThis lake was constructed about 60 years back by the villagers of twin village of Sandow and Yokma Kharboo with the purpose of permanent water source for the agricultural land in the two villages which lies along the stream originating from the lake. A wall was constructed on lower side of the lake to stop the water and regulatory water gateway was made to allow water to flow in case overfilled. A required amount of water leak from the base of the wall which makes it independent, requiring no human to regulate the flowing water level from the lake.The wall was constructed with a slope varying from 40 degrees at bottom to 60 degrees at top and one can easily walk even on the slanting wall of the lake.It was constructed without use of any cement, concrete and iron and this adds to the wonder of visitors.\n\nThe Lake is connected with road to Sandow and lalung and one can reach there through both the roads from Kargil. Through Sandow it is connected to National highway (NH1) at Khangral and Lotsum. It is 65 km from district headquarter Kargil and 200 km from Leh. From last few years (after linking it with road to Sandow) thousands of peoples are flooding to the lake for picnic in summer mainly in the month of July and August. Nowadays it has become a favourite picnic spot for peoples of the region and school children. 2015 alone saw a footfall of more than 10 thousand with locals mainly staying a day long while most foreign tourists stayed for many days and explored the surrounding areas like Trekking to Barbanchan la and trip to mysterious Api Strinjangmo-e-Baho (roughly translated as Old Spider's cave).\n\nIn 2015 the lake drew attention from administration when due to overfilling and resulting leakages damaging low lying areas.\n"}
{"id": "37992863", "url": "https://en.wikipedia.org/wiki?curid=37992863", "title": "Scotland's Rural College", "text": "Scotland's Rural College\n\nScotland's Rural College (SRUC) is a higher education institution that combines education, consulting and research in Scotland. It focuses on agriculture. It was founded in October 2012 through a merger of Barony College, Elmwood College, Oatridge College and the Scottish Agricultural College (SAC).\n\nThe institution will work towards gaining the status of a university college with degree awarding powers. Until such time, the initialism SRUC is given as the organisation's name to avoid a reference to university college status. SRUC is a registered charity under Scottish law. Degrees are currently awarded by The University of Edinburgh and The University of Glasgow depending on the campus.\n\nSRUC students study land-based courses on sites throughout Scotland from further education to PhD level. The organisation’s Research Division carries out research in the agriculture and rural sector while the Consulting Division, SAC Consulting, works with more than 12,000 clients in rural businesses and associated industries. The college has six campuses across Scotland while the Consulting arm has 26 offices located both in Scotland and in the north of England, as well as eight veterinary surveillance centres. SRUC’s Research Division operates in six research centres, and SRUC also runs eight farms for both research and educational purposes.\n\nScotland’s Rural College‘s heritage stretches back more than 100 years through many smaller institutions which have merged. The current organisation came into being on 1 October 2012 after land-based colleges Barony, Elmwood and Oatridge merged with the Scottish Agricultural College (SAC). \n\nBefore it became a college the 300 acre Barony estate had a varied existence. It was an elegant home, a home for the elderly, a wartime army training camp and, up until 1947, a prisoner of war camp. In 1949 Dumfries County Council Education Department purchased the estate with the purpose of turning it into an agricultural school. The Barony Farm School opened in 1953, with a class 46 boys of 14 to 15 years of age.\n\nDay release classes in general agriculture and agricultural engineering began 1962, and ten years later the school became the Barony Agricultural College. The 70s and 80s saw the range of courses on offer expand to include National Certificates in agriculture, fish farming, forestry, countryside rangers, horticulture, animal care, veterinary nursing and equine studies. By this time most students at the college were studying full-time.\n\nA new teaching block complete with a large sports hall, a multigym and a bar was opened in 1992. The new millennium brought massive investment in Animal Care and Veterinary Nursing, an Equine Unit and a Forestry Technology Centre. The Dairy Technology Centre was opened in 2006, complete with a robotic milking system.\n\nWhile Elmwood College did not officially open its doors until 1972, its foundations were laid 20 years earlier holding classes in the local school and cricket club before Fife County Council Education Committee acquired some land and erected a Nissen hut. This was followed by the purchase of the adjacent property of Elmwood House, Gardens and Greenhouses in 1953 for the sum of £2,300.\n\nIn 1956 the first day release classes in Scotland for agricultural and horticultural apprentices commenced at Elmwood Agricultural Centre. Elmwood College continued to expand during the early 1960s and this culminated in the construction of a new building, completed in 1972. By then Elmwood had also acquired Stratheden Hospital Farm. The College was officially opened in 1972 by Sir Hector Munro.\n\nElmwood is well known for its golf education and construction of the golf course began in 1995 with special attention being given to both the quality of the course and consideration of the local environment. Opened in 1997 the Elmwood course was Geo Certified in 2013; this environmental plaudit is only awarded to courses which prove they are working as sustainably as possible.\n\nThree agricultural colleges were created in the east, north and west of Scotland around the close of the 19th century. They fulfilled a critical need to transfer the growing scientific knowledge of agricultural issues like soil condition, drainage, use of manures and animal diseases, to farmers and the general public. In 1899 the agricultural department of the Glasgow and West of Scotland Technical College amalgamated with the Scottish Dairy Institute and formed the West of Scotland Agricultural College. Originally based in Glasgow, the organisation began moving to Ayrshire when in 1927 the Auchincruive estate was left to the College by John Hannah of Girvan Mains.\n\nThe Edinburgh and East of Scotland College of Agriculture was formed in 1901 and grew so rapidly its premises in the city’s George Square had to be expanded in 1904. In 1913 they formed a Joint Committee on Research in Animal Breeding with the University of Edinburgh.\n\nNorth of Scotland College of Agriculture (NOSCA) began in 1904 in Aberdeen through grants from ten benefactors including most of the local councils nearby. In 1914 it moved to the Craibstone Estate, which is still a campus today.\n\nIn 1990 these three major agricultural colleges merged to form the Scottish Agricultural College (SAC). SAC’s three main divisions offered research, education and consultancy.\n\nSRUC has six campuses located throughout Scotland, each offering varied land-based education courses.\n\nThe Aberdeen Campus is based on Craibstone Estate about outside Aberdeen in the north east of Scotland. As well as halls of residence and a library, the campus also boasts many sporting opportunities with an Astroturf pitch, a gym, sauna and steam room and a golf course. Courses on offer in Aberdeen include agriculture, organic farming and countryside and environmental management.\n\nThe Ayr campus is shared with students from the University of the West of Scotland. The £81 million facility was opened in September 2011 and was awarded the internationally recognised BREEAM Excellence rating for its environmentally friendly design. As well as student accommodation, the campus has a library and a diverse range of sporting activities, including climbing and horse riding, are available. Courses on offer in Ayr include Agricultural Bioscience and Green Technology.\n\nBarony Campus is set in a working estate in Dumfries and Galloway in south west Scotland. As well as the usual student facilities such as library and accommodation, the campus is home to the Scottish Dairy technology Centre and the Scottish Forestry Technology Centre. Courses on offer at Barony include animal care, equine studies and forestry and arboriculture.\n\nEdinburgh campus - Located on the south side of the capital this campus is shared with the University of Edinburgh. This allows students to access the university’s facilities, both academic and recreational, as well as SRUC’s. As well as libraries and cafes the campus also has sporting amenities and is just a ten-minute bus journey from the city centre. Courses on offer in Edinburgh include horticulture, applied animal science and rural resource management.\n\nSRUC Elmwood is based in Cupar, a small town in Fife approximately nine miles from St Andrews. As well having as a professional golf course, students have the opportunity to play badminton, table tennis and football, or work out in the gym. Courses on offer at Elmwood include conservation, greenkeeping and gamekeeping.\n\nSituated in West Lothian, SRUC’s Oatridge Campus is set on a large estate which includes a working farm. As well as a student accommodation and a library, there is also a nine-hole golf course, and the campus is home to the Scottish National Equestrian Centre (SNEC). Courses on offer at Oatridge include farriery and forge work, and land-based engineering.\n\n, the combined college had around 8,000 students and 1,500 staff. In April 2016, after two years with a principal, SRUC appointed Professor Wayne Powell to the position.\n\nThe further education and degree programmes at Scotland’s Rural College are grouped into six main departments: Agriculture and Business Management, Animal and Equine, Engineering, Science and Technology, Environment and Countryside, Horticulture and Landscape, and Sport and Tourism.\n\nStudents can study at all levels – from access courses and vocational studies, through undergraduate programmes covering HNC, HND and undergraduate degree courses, to taught postgraduate programmes and PhDs.\n\nSRUC Research seeks to address major challenges posed by the growing human population, and increasing demand for food, in a world with a potentially dramatically changing climate and with growing pressure on its natural resources. SRUC Research aims to benefit the rural economy and rural communities and enhance their environment.\n\nSRUC’s Research Division is divided into four interdisciplinary research groups; each of which are devoted to different, often overlapping, areas of land-based research.\n\n\n"}
{"id": "51478996", "url": "https://en.wikipedia.org/wiki?curid=51478996", "title": "Sylva Macharová", "text": "Sylva Macharová\n\nSylva Macharová (23 June 1893 – 19 January 1968) was a Czech nurse who was one of the first trained nurses in Prague. She headed the first nursing school in the country between 1923 and 1931. She was one of the inaugural recipients of the Florence Nightingale Medal. After a break to raise her family, Macharová returned to nursing after World War II, working at a military hospital until 1949. Thereafter, she was appointed Head of the Rehabilitation Department in the private clinic of Professor Jirásek.\n\nSylva Macharová was born on 23 June 1893 in Vienna, which was part of the Austro-Hungarian Empire at that time. Her father was the Czech poet Josef Svatopluk Machar, who at the time of her birth was working as a bank clerk. After attending the Lyceum in Hradec Králové for her secondary schooling, Macharová enrolled in 1913 in the Hospital School of Nursing in Vienna. She graduated in 1915 and went to Prague, becoming one of the first licensed nurses of the city.\n\nMacharová, who spoke Czech, English, and German, began working in a sanatorium in Podolí. Soon thereafter, she became a scrub nurse in the clinic of Professor Kukuly. Around 1918, Macharová went to Zlín at the request of Tomáš Baťa, founder of Bata Shoes, who was the mayor of Zlín at that time. He wanted her to investigate why they had cases of wound suppuration and she was able to discover the cause. In 1920, the International Committee of the Red Cross awarded the Florence Nightingale Medal for the first time, and Macharová was one of two Czech recipients, the other being Irene Metekickova. The award was designed to recognize those who exhibited exemplary performance of nursing duties.\n\nIn 1923, Macharová became the first headmistress of the Czechoslovakian School of Nursing, taking over from the three American Red Cross nurses who had initially developed the theoretical and practical training for the school. Simultaneously, she became the director of the German Nursing School in the city. During her tenure, the school expanded rapidly, but maintained its dedication to professionalism. Macharová developed internship opportunities and workshops to improve skill and she sought to integrate languages in both schools.\n\nIn 1931, Macharová married Alfonse Novacek, a veterinarian. They moved to Moravské Budějovice, where Macharová raised two sons, until 1938. At that time, the family returned to Prague because Marcharová's father was ill. In 1946, she returned to nursing, working at the , which was often referred to as the Střešovická Hospital, due to its nearness to Střešovice. She worked in the neurosurgery unit until 1949, when for political reasons she was forced to resign. That same year, she was hired as the she head of the Rehabilitation Department in the private clinic of Professor Jiráska, where she worked until her retirement in 1957.\n\nMacharová died after a lengthy bout with cancer on 19 January 1968.\n\n"}
{"id": "1551797", "url": "https://en.wikipedia.org/wiki?curid=1551797", "title": "Texas Medication Algorithm Project", "text": "Texas Medication Algorithm Project\n\nThe Texas Medication Algorithm Project (TMAP) is a controversial decision-tree medical algorithm, the design of which was based on the expert opinions of mental health specialists. It has provided and rolled out a set of psychiatric management guidelines for doctors treating certain mental disorders within Texas' publicly funded mental health care system, along with manuals relating to each of them. The algorithms commence after diagnosis and cover pharmacological treatment (hence \"Medication Algorithm\").\n\nTMAP was initiated in the fall of 1997 and the initial research covered around 500 patients.\n\nTMAP arose from a collaboration that began in 1995 between the Texas Department of Mental Health and Mental Retardation (TDMHMR), pharmaceutical companies, and the University of Texas Southwestern. The research was supported by the National Institute of Mental Health, the Robert Wood Johnson Foundation, the Meadows Foundation, the Lightner-Sams Foundation, the Nanny Hogan Boyd Charitable Trust, TDMHMR, the Center for Mental Health Services, the Department of Veterans Affairs, the Health Services Research and Development Research Career Scientist Award, the United States Pharmacopoeia Convention Inc. and Mental Health Connections.\n\nNumerous companies that invent and develop antipsychotic medications provided use of their medications and furnished funding for the project. Companies did not participate in the production of the guidelines. \n\nIn 2004 TMAP was mentioned as an example of a successful project in a paper regarding implementing mental health screening programs throughout the United States, by the President George W. Bush's New Freedom Commission on Mental Health, which looks to expand the program federally. The President had previously been Governor of Texas, in the period when TMAP was implemented. Similar programs have been implemented in about a dozen States, according to a 2004 report in the \"British Medical Journal\".\n\nSimilar algorithms with similar prescribing advice have been produced elsewhere, for instance at the Maudsley Hospital, London.\n\n"}
{"id": "12098786", "url": "https://en.wikipedia.org/wiki?curid=12098786", "title": "Tim Ferriss", "text": "Tim Ferriss\n\nTimothy Ferriss (born July 20, 1977) is an American author, entrepreneur, and public speaker.\n\nHe has written a number of self-help books on the \"4-hour\" theme, some of which have appeared on the \"New York Times\", \"Wall Street Journal\", and \"USA Today\" bestseller lists, starting with \"The 4-Hour Workweek\".\n\nFerriss is also an angel investor and advisor to Facebook, Twitter, StumbleUpon, Evernote, and Uber, among other companies.\n\nFerriss grew up in East Hampton, New York and graduated from St. Paul's School, Concord, New Hampshire. He received a degree in East Asian Studies from Princeton University in 2000. After graduation, Ferriss worked in sales at a data storage company. Ferriss began building his own Internet business, BrainQUICKEN, while still employed at the company.\n\nIn 2001, Ferriss founded BrainQUICKEN and sold the company to a London-based private equity firm in 2010. He said \"The 4-Hour Workweek\" was based on this period.\n\nIn December 2008, Ferriss had a pilot on the History Channel called Trial by Fire, where he had one week to attempt to learn a skill normally learned over the course of many years. In the pilot episode he practiced yabusame, the Japanese art of horseback archery.\n\nIn December 2013, his television series \"The Tim Ferriss Experiment\" debuted on HLN. Although there were 13 episodes shot, only a portion of those were shown on television. The show was released in its entirety on iTunes.\n\nIn November 2013, Ferriss began an audiobook publishing venture, Tim Ferriss Publishing. The first book published was \"Vagabonding\" by Rolf Potts. Other books include \"The Obstacle Is The Way\" by Ryan Holiday, \"Daily Rituals\" by Mason Currey, and \"What I Learned Losing A Million Dollars\" by Jim Paul and Brendan Moynihan.\n\nFerriss is an angel investor and advisor to startups.\n\nHe has invested or advised in startups such as StumbleUpon, Posterous, Evernote, DailyBurn, Shopify, Reputation.com, Trippy, and TaskRabbit. He is a pre-seed money advisor to Uber, a company co-founded by Garrett Camp, the founder of StumbleUpon, which Ferriss also advises.\n\nIn 2013, Ferriss raised $250,000 in under an hour to invest in Shyp by forming a syndicate on AngelList. Ferriss ended up raising over $500,000 through his backers and Shyp raised a total of $2.1 million. In 2018, Shyp shut down and laid off all its employees.\n\n\"The New York Times\" listed Ferriss among their \"Notable Angel Investors\" while CNN said he was \"one of the planet's leading angel investors in technology.\"\n\nIn 2015, Ferriss declared a long vacation from new investing. He cited the stress of the work and a feeling his impact was \"minimal in the long run\", and said he planned to spend time on his writing and media projects.\n\nFerriss has written five books, \"The 4-Hour Workweek\", \"The 4-Hour Body\", \"The 4-Hour Chef\", \"Tools of Titans\", and \"Tribe of Mentors.\"\n\nAs of 2016, \"The Tim Ferriss Show\", a podcast, has had over 80 million downloads. It covers topics ranging from personal and character development, to exercise routines, acting, venture capital and metaphysics.\n\nFerriss is the host of the 2017 TV show \"Fear(Less) with Tim Ferriss \", in which he interviews people.\n\n\"The New Yorker\" has described Ferriss as this generation's self-help guru, comparing him to personalities of similar influence in earlier times, such as Napoleon Hill, Norman Vincent Peale, Stephen Covey and Spencer Johnson. \"Wired\" called Ferriss “The Superman of Silicon Valley”. \"The New York Times\" said Ferriss was \"somewhere between [retired General Electric chairman and CEO] Jack Welch and a Buddhist monk.\" In 2011, \"Newsweek\" declared Ferriss \"The World's Best Guinea Pig\".\n\nHe has been named among \"Newsweek\"s Digital Power Index 100 as the seventh most powerful online personality, \"Fortune\"s \"40 Under 40\", one of \"Fast Company\"<nowiki>'</nowiki>s \"Most Innovative Business People of 2007\", and a Henry Crown Fellow by the Aspen Institute.\n\nBoth \"The 4-Hour Body\" and \"The 4-Hour Workweek\" are in the \"10 Most Highlighted Books of All Time\" on Amazon Kindle.\n\nIn 2008, Ferriss was named \"Wired\"<nowiki>'</nowiki>s \"Greatest Self-Promoter of All Time.\"\n\nIn 2007, Ferriss was a charity advocate and a member of the National Advisory Council of the public school nonprofit DonorsChoose. His projects and donations have raised more than $250,000 for underfunded public school teachers and classroom projects, and his campaigns, such as dedicating his birthday to raising funds and heading LitLiberation to increase literacy worldwide, have impacted more than 60,000 students. In March 2016, Ferriss flash funded 145 school projects posted on DonorsChoose.org in coordination with Stephen Colbert.\n\nIn October 2014, BUILD Boston, a not-for-profit organization that uses entrepreneurship to equip Boston youth for high school and college success, honored Ferriss with the annual BUILDer Award for Innovation and Entrepreneurship for his work in education reform.\n\nFerriss is also on the advisory board of the non-profit QuestBridge, along with LinkedIn Founder Reid Hoffman, David Sze, and others. QuestBridge was created in 2003 to connect disadvantaged students with elite colleges and helps place over 2,000 students per year.\n\nFerriss has raised funds for psychedelic studies at institutions such as Johns Hopkins University School of Medicine. In 2016, Ferriss donated $100,000 to research psychedelic drugs for the treatment of major depression.\n\nIn 2017 Ferriss moved to Austin, Texas after living 17 years in Silicon Valley. In an interview with Business Insider, he blamed \"intellectual smugness\" and \"closed-mindedness masquerading as opened-mindedness\" for the move. \"For the last few years I've witnessed what appears to be the forming of an echo chamber that is even more hermetically sealed than it usually is in Silicon Valley,\" he said.\n\nFerriss has stated that prior to his writing career he won in the weight class at the 1999 USAWKF national Sanshou (Chinese kickboxing) championship through a process of shoving opponents out of the ring and by dramatically dehydrating himself before weigh in, and then rehydrating before the fight in order to compete several classes below his actual weight – a practice known as weight cutting.\n\nFerriss is a practitioner of the Transcendental Meditation technique.\n"}
{"id": "32487739", "url": "https://en.wikipedia.org/wiki?curid=32487739", "title": "Video self-modeling", "text": "Video self-modeling\n\nVideo self-modeling (VSM) is a form of observational learning in which individuals observe themselves performing a behavior successfully on video, and then imitate the targeted behavior. VSM allows individuals to view themselves being successful, acting appropriately, or performing new tasks. Peter Dowrick, a key researcher in the development of self-modeling, described two forms of VSM, \"feedforward\" and \"self-review\". \"Self-review\" involves someone with a relatively well developed skill watching examples of best performance. A good example of this is the procedure used by Laura Wilkinson, gold medal platform diver, prior to every meet. In an interview after her gold medal performance, she was asked how she prepares for competition. She said that she watches a video that consists of her best dives along with encouragement from family and coaches. Self-review is mainly used in sports training as a form of visual imaging. Feedforward, on the other hand, is used with people who do not have a skill or when a new skill is emerging. Thus, feedforward is the method most often used in instructional or clinical settings. Because Feedforward involves new skills or behaviors performed by the viewer, it usually requires some degree of video editing to make it appear that the viewer is performing in an advanced manner. The term feedforward can be contrasted with the more traditional term \"feedback\" as it relates to receiving information about performance. Feedback allows people to see how they are doing. Feedforward allows them to see how they could be performing; a future self. Feedforward is mainly used in education and therapy circles and mainly with children with disabilities. It has been found to be especially effective with children with autism who tend to be visual learners and who seem to attend better to monitors than to live models.\n\nResearch across an array of behaviors and many types of disabilities has been ongoing since 1970 with very positive results. Researchers report that changes occur rapidly, there is good maintenance, and that changes generalize across settings. Adoption by educators and therapists has lagged behind the research possibly because of the technology requirements for editing videos. The advent of user-friendly editing software such as iMovie and Movie Maker has gone a long way in solving that problem. Another reason that the use of VSM seems to be gathering momentum is an article that appeared in 2007 by Scott Bellini and Jennifer Akulian. These researchers conducted a meta-analyses of all forms of video modeling and concluded that both peer and self-modeling met the Council for Exceptional Children's requirements for research-based methods.\nSelf-modeling has several attributes that sets it apart as a good tool to use with children with disabilities. It uses only positive imagery which makes it fit well into most positive behavior support plans. Effects of VSM should be seen almost immediately. If change is not seen after two or three viewings, one can move quickly to an alternate intervention. Finally, there appears to be no real down-side to the method. No damage is done if it does not work for an individual. As Buggey states in his book, the only tangible outcome when no results are seen is that the person ends up with a flattering video of him or herself.\nIn his book, \"Seeing is Believing\", Tom Buggey lists three major ways video footage can be collected and compiled into a feedforward video:\n\n\nAlthough VSM has shown overwhelming success with a range of ages and types of disabilities it has had problematic results with children under 4 yrs and with persons with very severe cognitive disabilities (although it has been successful with children with severe autism). Users must use caution to not depict behavior that is far beyond the viewer's ability. Showing skills beyond the reach of individuals can cause frustration and work counter to the purpose of VSM. Both speech/language and physical or occupational therapists need to be consulted when the skills needing to be addressed fall within their areas of expertise. (update: 2017. Studies with children under the age of 4 were restricted to the training of social skills. However, The Dept. of Education in Minnesota (MDOE) funded a training project for its Birth to 3 caregivers in 2013. Participants were trained in the use of VSM and were required to record and report results to a MDOE supervisor. Over 90 personnel were trained and 87 submitted results. All but 3 reported positive results. The skills addressed were mainly functional and language-based. Because none of these cases involved training social interaction skills, it was hypothesized that use of VSM with children under 4 [as young as 2 yrs. 1 mo.] was practical for certain behaviors.)\n\nUsers should also be aware of the differences between \"self-modeling\" and \"self-observation\". While self-modeling involves edited videos depicting only positive imagery, self-observation involves watching raw, unedited footage of behavior. The classic example of self-observation is watching game films in sports. Much can be gained by using self-observation; however, there is a risk that if behaviors viewed are too negative (e.g. a lineman missing blocking assignments 60% of the time) it could adversely affect self-confidence, and thus the performance of the viewer. Buggey suggests that use of self-observation with children with disabilities should be used with extreme caution.\n\nMany self-modeling researchers point to Albert Bandura's studies on social learning as key to the understanding of the effectiveness of VSM. Bandura made two fundamental findings that relate directly to self modeling. The first is that the best models are those as close to the viewer as possible in all attributes including ability. You cannot get closer than when the model and viewer are the same person with only a slight change in ability. The other finding involves self-efficacy, the belief that one can succeed at a task. Bandura found that the higher the belief in success, the higher the success rate. Self-modeling allows children to see themselves succeeding, and increases self-efficacy (as long as the new behavior is attainable and developmentally appropriate).\n\nOne of the reasons VSM may work so well with social behaviors may have been uncovered by Thomas Kehle and colleagues. While working with children with emotional disorders who had had VSM intervention, they noticed that clients had difficulty remembering negative behaviors during exit interviews. They hypothesized that these individual were not only getting new memories based on VSM experiences, but they were also supplanting memories of the old behaviors. Their hypothesis was substantiated in the 2002 study. Their findings are preliminary and there have not been any published replications, but if substantiated, it raises both interesting methodological and ethical issues.\n\nAny behavior that can be observed, and thus filmed, can be a subject of a self-modeling video. In their meta-analyes article, Bellini and Akulian identified behaviors that were addressed in studies they evaluated. These include:\n\n"}
{"id": "17494210", "url": "https://en.wikipedia.org/wiki?curid=17494210", "title": "Virgin (title)", "text": "Virgin (title)\n\nThe title Virgin (Latin \"Virgo\", Greek ) is an honorific bestowed on female saints and blesseds in both the Eastern Orthodox Church and the Roman Catholic Church.\n\nChastity is one of the seven virtues in Christian tradition, listed by Pope Gregory I at the end of the 6th century. In 1 Corinthians, Saint Paul suggests a special role for virgins or unmarried women () as more suitable for \"the things of the Lord\" ().\nIn 2 Corinthians 11:2, Paul alludes to the metaphor of the Church as Bride of Christ by addressing the congregation \n\"I have espoused you to one husband, that I may present you as a chaste virgin to Christ\".\n\nIn the theology of the Church Fathers, the prototype of the sacred virgin is Mary, the mother of Jesus, consecrated by the Holy Spirit at Annunciation. \nAlthough not stated in the gospels, the perpetual virginity of Mary was widely upheld as a dogma by the Church Fathers from the 4th century. \n\nIn the hagiography of Christian martyrs of the late 1st to early 4th centuries, virgin martyrs \nare Christian virgins, often persecuted for their refusal to enter a worldly marriage after having vowed to keep their virginity.\nThe historicity of these early saints cannot be established, the dates given are from hagiographical tradition.\n\nPost-Nicean Virgin martyrs:\n\nThe first known formal consecration is that of Saint Marcellina, dated AD 353, mentioned in \"De Virginibus\" by her brother, Saint Ambrose. Another early consecrated virgin is Saint Genevieve (c. 422 – c. 512).\n\nSaint Margaret of Hungary (1242–1270) is noted as a nun and virgin, as she received \na separate consecration as a virgin in spite of already having taken monastic vows; this was done in order to dissuade her father, king Béla IV of Hungary, from trying to have her vows rescinded by the pope for the purposes of a political marriage.\n\nAccording to Raymond of Capua, Catherine of Siena (c. 1347–1380) at the age of twenty-one (c. 1368) experienced what she described in her letters as a \"Mystical Marriage\" with Jesus, later a popular subject in art as the \"Mystic marriage of Saint Catherine\".\n\nSaint Thérèse of Lisieux (1873–1897), canonized in 1925.\n\nThe tradition of the rite of consecration dates back to the 4th century. The rite for virgins living in the world has been reintroduced under Pope Paul VI in 1970. The reintroduction of the rite of consecration of virgins for women living in the world was notably campaigned for by Anne Leflaive (1899–1987), who had been consecrated as a virgin in 1924, and who campaigned for the formal recognition of the rite of consecration during the 1920s to 1960s.\n\nThe number of consecrated virgins ranges in the thousands. While the Holy See does not keep official statistics, estimates derived from diocesane records range at around 5,000 consecrated virgins worldwide as of 2018.\n\n\n"}
{"id": "39530108", "url": "https://en.wikipedia.org/wiki?curid=39530108", "title": "Water privatization in France", "text": "Water privatization in France\n\nWater privatization in France (\"Gestion déléguée des services publics d'eau potable\") - more accurately called public-private partnerships for drinking water supply - goes back to the mid-19th century when cities signed concessions with private water companies for the supply of drinking water. As of 2010, according to the Ministry of Environment 75% of water and 50% of sanitation services in France are provided by the private sector, primarily by two firms, Veolia Water and Suez Environnement. In 1993 the \"Loi Sapin\" strengthened competition in the sector by limiting the duration of contracts to 20 years, among others. In 2010 the lease contracts for Paris with Suez Environnement and Veolia Water expired and the water system returned to public management.\n\nThe typical form of delegated management (Gestion Déléguée) from the public to the private sector is through a lease contract (\"Affermage\") or a concession contract. A lease contract is of shorter duration (10–15 years) and the responsibility to finance most of the infrastructure remains with the municipality. A concession contract is of longer duration (20–30 years) and the Concessionnaire is in charge of mobilizing financial resources. In both cases, the municipality or the Intermunicipal utility fixes the water and sanitation tariff and remains the owner of the infrastructure. Other forms of delegated management include the \"Régie intéressée\" and the \"Gérance\", less common forms of private sector participation under which the private sector takes fewer risks.\n\nThere are three large private French water companies:\n\nThe private Société des Eaux de Marseille (SEM), half owned by Veolia Environnement and half owned by SUEZ, provides Marseille with water and wastewater services. \n\nPrivate sector participation in water supply and sanitation has a long tradition in France and has provided many benefits to municipalities, such as lower levels of public debt. However, it is not without its critics. A comparative assessment of public and private service provision is complicated by the absence of a mandatory national performance benchmarking system. Therefore, supporters and opponents of private sector participation often find it hard to provide objective figures to back up their respective positions.\n\nAccording to the Ministry of Environment, in 1992 tariffs by private providers were 22% higher than for public providers. After competition in the sector has been strengthened through the \"Loi Sapin\" in 1993, the difference declined to 13% in 1998, according to the Ministry of Environment. No matter how high the difference is, these assessments do not compare apples with apples. For example, public companies do not have to pay for the acquisition of public land. Also, they are not subject to corporate income tax (\"taxe professionnelle\") or property tax (\"redevance pour occupation du domaine public\"). Both factors reduce the costs of public companies compared to private companies without providing an economic benefit, since the higher costs of private companies are channeled back to the public treasury. In addition, a study by the research institute INRA showed that the municipalities with the most difficult conditions tend to delegate service provision to the private sector, thus biasing the comparison of tariff levels. Among the conditions driving municipalities to seek private sector participation are the classification in an ecologically sensitive zone implying stricter wastewater treatment standards and a lower population density implying a longer network per customer. \n\nA 2003 report by the French Supreme Audit Agency (\"Cour des Comptes\") on water and sanitation in France covering the period 1995-2002 confirms that in 1999, six years after the Loi Sapin strengthened competition, 85% of contracts were still renewed with the incumbent. The \"Cour des Comptes\" noted that many municipalities, including some large ones, do not have the capacity to control the private sector contracts, in particular unjustified increases of certain fees. The municipalities do not use the numerous legal instruments at their disposal to better control the lease contracts they sign. According to critics, the three large private water companies are in a stronger negotiation position than the municipalities, thus leading to a lack of meaningful competition and regulatory capture.\n\nThe annual financial reports submitted by the private enterprises to the municipalities are often not very transparent. For example, the \"Cour des Comptes\" noted that these reports cannot be compared to the financial projections submitted during contract negotiation, because they are established on different bases.\n\nIn some cases, private water companies also use accounting tricks to increase their profit margin. The \"Cour des Comptes\" noted that revenues from ancillary activities, such as the sale of bulk water to neighboring municipalities or electricity sales from hydropower production, are sometimes omitted from the financial reports to the municipalities. Furthermore, some operators cover fees for a “renewal guarantee” without fully reinvesting the proceeds. These revenues thus constitute a net gain for the operator at the end of the contract. Also, water companies are allowed to carry out works through their own subsidiaries without selecting them according to the local government regulations for competitive bidding. Finally, large utilities can manipulate transfer prices, thus making their finances even more intransparent to municipal regulators.\n\nPrivate water utilities have been used as a vehicle for financing election campaigns and other political activities, leading to corrupt practices despite several laws passed to prevent corruption in the 1990s, such as the \"Loi Sapin\". Some municipal associations, such as in Grenoble, have sued private operators leading to the cancellation of contracts and to a prison sentence against the mayor of Grenoble in 1996. \n\n"}
