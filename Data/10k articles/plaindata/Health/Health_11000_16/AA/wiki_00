{"id": "8160346", "url": "https://en.wikipedia.org/wiki?curid=8160346", "title": "Ablynx", "text": "Ablynx\n\nAblynx is a subsidiary of biopharmaceutical company Sanofi engaged in the discovery and development of nanobodies, based in Science Park Zwijnaarde, Ghent.\n\nIn November 2001, Ablynx was established as a spin-off of the Vlaams Instituut voor Biotechnologie (VIB) and the Free University of Brussels (VUB). Seed financing of was provided by Gimv.\n\nIn July 2015 Merck and Ablynx expanded their 18-month-old immuno-oncology collaboration by four years, generating a potential in milestone payments for Ablynx. In 2015, Novo Nordisk announced it would collaborate with Ablynx, using its nanobody technology to develop at least one new drug candidate, generating a potential $400 million for the company.\n\nIn January 2018, Reuters reported that Novo Nordisk had offered to acquire Ablynx for $3.1 billion - having made an unreported offer in mid December for the company. However the Ablynx board rejected this offer the same day, saying that the price undervalued the business.\n\nIn January 2018, they were acquired by Sanofi for $4.8 billion.\n\n\n"}
{"id": "53298524", "url": "https://en.wikipedia.org/wiki?curid=53298524", "title": "Abul K. Abbas", "text": "Abul K. Abbas\n"}
{"id": "18769343", "url": "https://en.wikipedia.org/wiki?curid=18769343", "title": "Academic health science centre", "text": "Academic health science centre\n\nAn academic health science centre (AHSC; also known as an academic health sciences centre, an academic health science(s) system, an academic health science(s) partnership or an academic medical centre) is a partnership between two or more universities and healthcare providers focusing on research, clinical services, education and training. AHSCs are intended to ensure that medical research breakthroughs lead to direct clinical benefits for patients. The organisational structures that comprise an AHSC can take a variety of forms, ranging from simple partnerships to, less frequently, fully integrated organisations with a single management board.\n\nThere are AHSCs operating in a number of countries including Australia, Canada, the Republic of Ireland, Japan, the Netherlands, Qatar, Singapore, Sweden, the United Kingdom and the United States.\n\nIn Australia, AHSCs are also referred to as Advanced Health Research & Translation Centres (AHRTCs). AHSCs currently in operation in Australia include:\n\nIn Canada, AHSCs are also referred to as Academic Healthcare Organizations. AHSCs currently in operation in Canada include:\n\nIn 2007 a review of healthcare in London led by Professor Lord Darzi, \"A Framework for Action\", recommended the creation of a number of AHSCs. In October 2007 Imperial College Healthcare became the first AHSC to be established in the UK when the Imperial College London Faculty of Medicine merged with the Hammersmith Hospital and St Mary's NHS trusts. Four more AHSCs have subsequently been established in the UK and one is planned.\n\nFunding mainly comes from NHS and work was already \"in hand to identify the funding\" when expressions of interest were solicited. \nWhen contracts were signed with NHS in 2013, AHSCs shared among themselves around £60 million of funding.\n\nWith a clear purpose, structure and approach of individual AHSCs is a matter for local decision especially with the contrasting approaches adopted as well as the differences in opinions voiced out by network founders.\n\nThe following AHSCs are currently in operation in the UK:\n\n\nAHSCs currently in operation in the United States include:\n\n\nAHSCs currently in operation in other parts of the world include:\n"}
{"id": "18797385", "url": "https://en.wikipedia.org/wiki?curid=18797385", "title": "Aggregate Spend", "text": "Aggregate Spend\n\nAggregate Spend is the process used in the United States to aggregate and monitor the total amount spent by healthcare manufacturers on individual healthcare professionals and organizations (HCP/O) through payments, gifts, honoraria, travel and other means. Also often referred to as the Physician Payments Sunshine Act, this initiative is a growing body of federal and state legislations intended to collectively address all or some of the following goals:\n\nOrganizations monitored include pharmaceutical, biotechnology and, in some states, medical device organizations.\n\nOn September 6, 2007, Senator Chuck Grassley (R-Iowa) introduced the Physician Payments Sunshine Act of 2007 (S. 2029). In March 2008, Rep. Peter DeFazio (D-Oregon) and Rep. Pete Stark (D-California) introduced a slightly different companion bill in the House of Representatives. (H.R. 5605). These bills were reintroduced in 111th Congress as the Physician Payments Sunshine Act of 2009 (S. 301 and H.R. 3138), again by Senator Chuck Grassley and in the House of Representatives by Rep. Baron Hill (D-Indiana). The bills all aimed to replace the differing state legislations with a single law, common to all 50 states. According to Ashley Glacel, the press secretary for the Senate Aging Committee, whose chairman, Herb Kohl, co-sponsored the bill, the Senate bill is more expansive because it also include Medical Device makers.\n\nThe bills would amend the Social Security Act \"to provide for transparency in the relationship between physicians and manufacturers of drugs, devices, or medical supplies for which payment is made under Medicare, Medicaid, or SCHIP.\" The bill proposed that each quarter, beginning on January 1, 2008, companies or their agents which manufacture drugs, medical devices, or medical supplies would be required to disclose all payments over $25 in value made \"to a physician, or to an entity that a physician is employed by, has tenure with, or has an ownership interest in\".\n\nThe bill would also require manufacturers to provide details on the date, value and nature of the payment, such as whether it was for \"food, entertainment, or gifts\", \"trips or travel\", \"a product or other item provided for less than market value\", \"participation in a medical conference, continuing medical education, or other educational or informational program or seminar, provision of materials related to such a conference or educational or informational program or seminar, or remuneration for promoting or participating in such a conference or educational or informational program or seminar\", \"product rebates or discounts\", \"consulting fees or honoraria\" or \"any other economic benefit\". Companies would be required to submit a summary report in electronic format. The proposed penalties for breaches were \"not less than $10,000, but not more than $100,000\", for each such failure.\n\nThe proposed federal law would undermine a stronger Vermont law if passed, according to state officials and advocacy groups. The reporting threshold under the proposed federal law is $500 - much higher than the $25 threshold found in a similar Vermont law passed five years ago. If passed, the federal bill would preempt the state law.\n\nIn May 2008, the Pharmaceutical Research and Manufacturers of America stated that they supported a revised version of the bill, but only on condition of \"the continued inclusion of the provision that preempts state law\". In a media statement, the PhRMA president, Billy Tauzi,n stated that \"PhRMA believes that preempting local and state marketing reporting or disclosure laws that have been enacted or are pending avoids a confusing myriad of local, state and federal requirements that confuse patients accessing the information and are overly burdensome and costly for those required to report.\"\n\nThe federal bill was finally passed on March 21, 2010, as a provision under the Patient Protection and Affordable Care (PPAC) Act (https://www.cms.gov/LegislativeUpdate/downloads/PPACA.pdf), and several states — including California, Massachusetts, Minnesota, Maine, District of Columbia, West Virginia, Vermont and Nevada — have already passed their versions of the Sunshine Law. The federal law was due to go into effect from January 1, 2012, with the earliest reports (covering January - December 2012) mandated on or before March 31, 2013. The penalties range from $10,000 to $100,000 for each violation, and can go up to $1 million.\n\nIn February 2013 the planned dates for implementation were changed to: earliest reports to cover August - December 2013; submission by March 31, 2014.\n\nIn February 2014 CMS (The Centers for Medicare & Medicaid Services) advised the planned submission dates and what would be submitted were changed. In essence this was because the required registration process, for those who would submit data and attest to the accuracy of that data, was not ready for use, nor were the supporting systems, people and processes for receiving the data\n\nOn February 18, Open Payments registration and data submission for applicable manufacturers and applicable GPOs opened with a two-phased approach for the first reporting year of the new program:\nPhase 1 (February 18 through March 31) includes user registration in CMS’ Enterprise Portal (the gateway to CMS’ Enterprise Identity Management system (EIDM)) and submission of corporate profile information and summary aggregate 2013 (August - December) payment data.\nPhase 2 (begins in May and extends for no fewer than 30 days) includes industry registration in the Open Payments system, submission of detailed 2013( August - December) payment data, and legal attestation to the accuracy of the data.\n\nAfter Phase 2 submission is complete, physicians and teaching hospitals will have the opportunity to register with OpenPayments and view the transactions reported under their name, prior to it being made available to the public. During this review period, any reported transactions may be disputed by the recipient. If a transfer of value is disputed, it will still be publicized, but remain flagged as disputed, until the dispute has been resolved.\n\nAggregate Spend compliance has been affected by individual state law compliance, which requires healthcare manufacturers to address and collect distinct spend types to comply with disclosure requirements at the HCP/O aggregate level. Minnesota, West Virginia, Vermont, California, Nevada, and Washington D.C. all have some type of gift-giving limit or disclosure law. Starting in July 2009, Massachusetts and Vermont Gift Ban Law became active with bans of $5,000 and $10,000 per violation respectively. Other states are evaluating similar options as well. On June 29, 2011, the Maine legislature passed House Paper No. 530 which was subsequently signed into law by Governor LePage on July 8, 2011, effectively repealing Maine’s aggregate spend reporting requirements (22 MRSA §2698-A)\n\n\n\n"}
{"id": "22273290", "url": "https://en.wikipedia.org/wiki?curid=22273290", "title": "Alain Afflelou", "text": "Alain Afflelou\n\nAlain Afflelou is a French optician and businessman, born 1 January 1948 in Mascara, French Algeria, to a father working in trade. He is the founder of his eponymous brand for optics. After having lived in Geneva and Paris, he was living between Paris and Biarritz before settling in London.\n\nAlain Afflelou is also the name of the French retail chain specializing in optical services and sales of the Alain Afflelou brand of eyewear. Created in 1972 in Le Bouscat by Alain Afflelou (the original shop still exists), as of 2007 the company had over 894 stores, 201 of which are in Spain.\n\nIn 2006, Bridgepoint Capital, a UK investment firm, acquired a majority stake in the company.\n\nIn 2010, Alain Afflelou Jr., opened his first boutique in Milan with Luigi J. Zubizarreta, the son of the wealthy businessman Giuseppe Zubizarreta. Alain Afflelou Jr., is the associate of Luigi J. Zubizarreta since 2005.\n\nOn 30 April 2012 Bridgepoint Capital announced a period of exclusivity has been offered to Lion Capital with the intention of reaching an agreement between Alain Afflelou and Bridgepoint for Lion to acquire the business.\n"}
{"id": "18450716", "url": "https://en.wikipedia.org/wiki?curid=18450716", "title": "Alcohol use and sleep", "text": "Alcohol use and sleep\n\nAlcohol (also known formally as ethanol), found in alcoholic beverages, can exacerbate sleep disturbances. During abstinence, sleep disruption is one of the greatest predictors of relapse.\n\nModerate alcohol consumption 30–60 minutes before bedtime results in disruptions in sleep maintenance and sleep architecture that are mediated by blood alcohol levels. Disruptions in sleep maintenance are most marked once alcohol has been completely metabolized from the body. Under conditions of moderate alcohol consumption where blood alcohol levels average 0.06–0.08% and decrease 0.01–0.02% per hour, an alcohol clearance rate of 4–5 hours would coincide with disruptions in sleep maintenance in the second half of an 8-hour sleep episode. In terms of sleep architecture, moderate doses of alcohol facilitate \"rebounds\" in rapid eye movement (REM) and stage 1 sleep; following suppression in REM and stage 1 sleep in the first half of an 8-hour sleep episode, REM and stage 1 sleep increase well beyond baseline in the second half. Moderate doses of alcohol also increase slow wave sleep (SWS) in the first half of an 8-hour sleep episode. Enhancements in REM sleep and SWS following moderate alcohol consumption are mediated by reductions in glutamatergic activity by adenosine in the central nervous system. In addition, tolerance to changes in sleep maintenance and sleep architecture develops within 3 days of alcohol consumption before bedtime.\n\nLow doses of alcohol (one 360.0 ml (13 imp fl oz; 12 US fl oz) beer) are sleep-promoting by increasing total sleep time and reducing awakenings during the night. The sleep-promoting benefits of alcohol dissipate at moderate and higher doses of alcohol (two 12 oz. beers and three 12 oz. beers, respectively). Previous experience with alcohol also determines whether or not alcohol is a \"sleep promoter\" or \"sleep disrupter.\" Under free-choice conditions, in which subjects chose between drinking alcohol or water, inexperienced drinkers were sedated while experienced drinkers were stimulated following alcohol consumption. In insomniacs, moderate doses of alcohol improve sleep maintenance.\n\nSleepiness influences the severity of alcohol consumption. Conditions of sleep deprivation encourage more episodes of alcohol consumption. Increased alcohol consumption during the winter months for Northern climate residents is attributed to escalations in fatigue.\n\nSleep and hormonal disruptions following withdrawal from chronic alcohol consumption are the greatest predictors of relapse. During abstinence, recovering alcoholics have attenuated melatonin secretion in the beginning of a sleep episode, resulting in prolonged sleep latencies. Escalations in cortisol and core body temperatures during the sleep period contribute to poor sleep maintenance. Abstinent alcoholics tend to have lighter, more fragmented sleep than normal control subjects. Research indicates that it may take as long as one to two years for sleep to return to normal in abstinent alcoholics and that for some it may never return to normal.\n\n"}
{"id": "16886106", "url": "https://en.wikipedia.org/wiki?curid=16886106", "title": "Alexander Fleming Biomedical Sciences Research Center", "text": "Alexander Fleming Biomedical Sciences Research Center\n\nThe Biomedical Sciences Research Center \"Alexander Fleming\" is a non-profit research organisation based in Vari, Athens, Greece. The Center is named after the scientist Alexander Fleming.\n\nSince the beginning of its operations in 1998, the Center develops basic as well as translational and applied research programs at the cutting edge of modern biomedical sciences. Currently the Center hosts 14 research groups distributed in 4 Institutes (Immunology, Molecular Oncology, Molecular Biology and Genetics, Cellular & Developmental Biology). Over the short period since its establishment, BSRC Al. Fleming has gained extensive visibility in the European science arena.\n\nFleming researchers have established transgenic animal models for rheumatoid arthritis, inflammatory bowel disease and multiple sclerosis and these models have served as a basis for multiple collaborations with the international biopharmaceutical industry in the evaluation of novel therapeutic compounds, or as tools for collaborative R&D.\n\nThe Center is equipped with state-of-the-art Core Units which include: an Expression Profiling Facility, a Flow Cytometry Facility, a Protein Chemistry Lab, a Transgenics and gene targeting Unit, and a BioIT Unit, all of which serve internal collaborations, as well as external partners in academia and industry. The Center also runs an Innovation and Enterprise Unit that facilitates the protection and exploitation of the Center’s research and technologies.\n\nFleming operates its own Animal House, which can house up to 20,000 mice and has its own complete mouse histopathology unit. Fleming’s Animal house (certified with ISO 9001) provides husbandry of animals and services to the biomedical research community since 2001. It covers an area of approximately 600 m2 within the Center and is equipped with highly automated systems that provide the best possible conditions for mouse reproduction and maintenance. Its main activity is the reproduction and maintenance of mice stocks either of inbred strains or genetically engineered mice, such as transgenic and knockout mouse lines, as well as chemically induced mutants developed by Fleming researchers. The Animal House has a capacity to house more than 20,000 mice and is currently the largest Mouse Unit in Greece in terms of number and variety of mice. The Facility became a full member of EMMA in 2009.\n\nMain areas of research: Functional genomics and proteomics; Molecular and cellular immunology; Animal models of human disease; Transcriptional and post-transcriptional mechanisms of gene regulation; DNA repair; Stem Cell differentiation; Epigenetics; Learning and memory; ECM biology.\nFields of excellence: Molecular mechanisms of disease (inflammation, cancer, metabolic syndrome, CNS disorders)\n\n\n"}
{"id": "998103", "url": "https://en.wikipedia.org/wiki?curid=998103", "title": "Bioequivalence", "text": "Bioequivalence\n\nBioequivalence is a term in pharmacokinetics used to assess the expected in vivo biological equivalence of two proprietary preparations of a drug. If two products are said to be bioequivalent it means that they would be expected to be, for all intents and purposes, the same.\n\nBirkett (2003) defined bioequivalence by stating that, \"two pharmaceutical products are bioequivalent if they are pharmaceutically equivalent and their bioavailabilities (rate and extent of availability) after administration in the same molar dose are similar to such a degree that their effects, with respect to both efficacy and safety, can be expected to be essentially the same. Pharmaceutical equivalence implies the same amount of the same active substance(s), in the same dosage form, for the same route of administration and meeting the same or comparable standards.\"\n\nFor The World Health Organization (WHO) \"two pharmaceutical products are bioequivalent if they are pharmaceutically equivalent or pharmaceutical alternatives, and their bioavailabilities, in terms of rate (Cmax and tmax) and extent of absorption (area under the curve), after administration of the same molar dose under the same conditions, are similar to such a degree that their effects can be expected to be essentially the same\" .\n\nThe United States Food and Drug Administration (FDA) has defined bioequivalence as, \"the absence of a significant difference in the rate and extent to which the active ingredient or active moiety in pharmaceutical equivalents or pharmaceutical alternatives becomes available at the site of drug action when administered at the same molar dose under similar conditions in an appropriately designed study.\"\n\nIn determining bioequivalence, for example, between two products such as a commercially available Brand product and a potential to-be-marketed Generic product, pharmacokinetic studies are conducted whereby each of the preparations are administered in a cross-over study to volunteer subjects, generally healthy individuals but occasionally in patients. Serum/plasma samples are obtained at regular intervals and assayed for parent drug (or occasionally metabolite) concentration. Occasionally, blood concentration levels are neither feasible or possible to compare the two products (e.g. inhaled corticosteroids), then pharmacodynamic endpoints rather than pharmacokinetic endpoints (see below) are used for comparison. For a pharmacokinetic comparison, the plasma concentration data are used to assess key pharmacokinetic parameters such as area under the curve (AUC), peak concentration (\"C\"), time to peak concentration (\"T\"), and absorption lag time (\"t\"). Testing should be conducted at several different doses, especially when the drug displays non-linear pharmacokinetics.\n\nIn addition to data from bioequivalence studies, other data may need to be submitted to meet regulatory requirements for bioequivalence. Such evidence may include:\n\n\nThe World Health Organization considers two formulation bioequivalent if the 90% confidence interval for the ratio multisource (generic) product/comparator lie within 80.00-125.00% acceptance range for AUC and C. For high variable finished pharmaceutical products, the applicable acceptance range for C can be 69.84-143.19% .\n\nIn Australia, the Therapeutics Goods Administration (TGA) considers preparations to be bioequivalent if the 90% confidence intervals (90% CI) of the rate ratios, between the two preparations, of \"C\" and AUC lie in the range 0.80–1.25. \"T\" should also be similar between the products.\n\nThere are tighter requirements for drugs with a narrow therapeutic index and/or saturable metabolism – thus no generic products exist on the Australian market for digoxin or phenytoin for instance.\n\nAccording to regulations applicable in the European Economic Area two medicinal products are bioequivalent if they are pharmaceutically equivalent or pharmaceutical alternatives and if their bioavailabilities after administration in the same molar dose are similar to such a degree that their effects, with respect to both efficacy and safety, will be essentially the same. This is considered demonstrated if the 90% confidence intervals (90% CI) of the ratios for AUC and \"C\" between the two preparations lie in the range 80–125%.\n\nThe FDA considers two products bioequivalent if the 90% CI of the relative mean C, AUC and AUC of the test (e.g. generic formulation) to reference (e.g. innovator brand formulation) should be within 80% to 125% in the fasting state. Although there are a few exceptions, generally a bioequivalent comparison of Test to Reference formulations also requires administration after an appropriate meal at a specified time before taking the drug, a so-called \"fed\" or \"food-effect\" study. A food-effect study requires the same statistical evaluation as the fasting study, described above.\n\nWhile the FDA maintains that approved generic drugs are equivalent to their branded counterparts, bioequivalence problems have been reported by physicians and patients for many drugs. Certain classes of drugs are suspected to be particularly problematic because of their chemistry. Some of these include chiral drugs, poorly absorbed drugs, and cytotoxic drugs. In addition, complex delivery mechanisms can cause bioequivalence variances. Physicians are cautioned to avoid switching patients from branded to generic, or between different generic manufacturers, when prescribing anti-epileptic drugs, warfarin, and levothyroxine.\n\nMajor issues were raised in the verification of bioequivalence when multiple generic versions of FDA-approved generic drug were found not to be equivalent in efficacy and side effect profiles. In 2007, two providers of consumer information on nutritional products and supplements, ConsumerLab.com and The People's Pharmacy, released the results of comparative tests of different brands of bupropion. The People's Pharmacy received multiple reports of increased side effects and decreased efficacy of generic bupropion, which prompted it to ask ConsumerLab.com to test the products in question. The tests showed that some generic versions of Wellbutrin XL 300 mg didn't perform the same as the brand-name pill in laboratory tests. The FDA investigated these complaints and concluded that the generic version is equivalent to Wellbutrin XL in regard to bioavailability of bupropion and its main active metabolite hydroxybupropion. The FDA also said that coincidental natural mood variation is the most likely explanation for the apparent worsening of depression after the switch from Wellbutrin XL to Budeprion XL. After several years of denying patient reports, in 2012 the FDA reversed this opinion, announcing that \"Budeprion XL 300 mg fails to demonstrate therapeutic equivalence to Wellbutrin XL 300 mg.\" The FDA did not test the bioequivalence of any of the other generic versions of Wellbutrin XL 300 mg, but requested that the four manufacturers submit data on this question to the FDA by March 2013. As of October 2013, the FDA has made determinations on the formulations from some manufacturers not being bioequivalent.\n\nIn 2004, Ranbaxy was revealed to have been falsifying data regarding the generic drugs they were manufacturing. As a result, 30 products were removed from US markets and Ranbaxy paid $500 million in fines. The FDA investigated many Indian drug manufacturers after this was discovered, and as a result at least 12 companies have been banned from shipping drugs to the US.\n\n\n"}
{"id": "23970827", "url": "https://en.wikipedia.org/wiki?curid=23970827", "title": "Cantel Medical Corporation", "text": "Cantel Medical Corporation\n\nCantel Medical Corporation is a company which produces and sells medical equipment. The company is based in Little Falls, NJ. The company has three subsidiaries:\n"}
{"id": "21368113", "url": "https://en.wikipedia.org/wiki?curid=21368113", "title": "Colleges of Medicine of South Africa", "text": "Colleges of Medicine of South Africa\n\nThe Colleges of Medicine of South Africa (CMSA) is the custodian of the quality of medical care in South Africa. It is unique in the world in that its 27 constituent Colleges represent all the disciplines of medicine and dentistry. One route to specialisation in South Africa, is via a Fellowship conferred by the College. See Medical education in South Africa; Dental degree: South Africa.\n\nCMSA was founded and financed in 1954 by members of the medical profession, and was registered as a non-profit making company in 1955. It has facilities in Cape Town and Johannesburg, including lecture venues and committee and reception rooms, and an office in Durban.\n\nThe specialist qualifications obtained through The CMSA are known as Fellowships, a designation similar to that used in various countries. These are recognised by the Health Professions Council of South Africa as acceptable for specialist registration and are known throughout the world. A qualification was designed especially for family practitioners and is called Membership of the College of Family Practitioners. This is registerable as an additional qualification.\n\nFor those medical and dental practitioners who do not wish to specialise, the CMSA offers Higher Diploma and Diploma qualifications which are registered by the HPCSA as additional qualifications. There is also an additional qualification for those who obtain a Certificate in one of the subspecialities.\n\n\n"}
{"id": "773959", "url": "https://en.wikipedia.org/wiki?curid=773959", "title": "Diatomaceous earth", "text": "Diatomaceous earth\n\nDiatomaceous earth () – also known as D.E., diatomite, or kieselgur/kieselguhr – is a naturally occurring, soft, siliceous sedimentary rock that is easily crumbled into a fine white to off-white powder. It has a particle size ranging from less than 3 μm to more than 1 mm, but typically 10 to 200 μm. Depending on the granularity, this powder can have an abrasive feel, similar to pumice powder, and has a low density as a result of its high porosity. The typical chemical composition of oven-dried diatomaceous earth is 80–90% silica, with 2–4% alumina (attributed mostly to clay minerals) and 0.5–2% iron oxide.\n\nDiatomaceous earth consists of fossilized remains of diatoms, a type of hard-shelled protist (chrysophytes). It is used as a filtration aid, mild abrasive in products including metal polishes and toothpaste, mechanical insecticide, absorbent for liquids, matting agent for coatings, reinforcing filler in plastics and rubber, anti-block in plastic films, porous support for chemical catalysts, cat litter, activator in blood clotting studies, a stabilizing component of dynamite, a thermal insulator, and a soil for potted plants and trees like bonsai.\n\nEach deposit of diatomaceous earth is different, with varying blends of pure diatomaceous earth combined with other natural clays and minerals. The diatoms in each deposit contain different amounts of silica, depending on the age of the deposit. The species of diatom may also differ among deposits. The species of diatom is dependent upon the age and paleo-environment of the deposit. In turn, the shape of a diatom is determined by its species.\n\nMany deposits throughout British Columbia, Canada, such as Red Lake Earth, are from the Miocene epoch and contain a species of diatom known as \"Melosira granulata\". These diatoms are approximately 12 to 13 million years old and have a small globular shape. \nA deposit containing diatoms from this age can provide many more benefits than that of an older deposit. For example, diatoms from the Eocene age (approximately 40 to 50 million years old) are not as effective in their ability to absorb fluids because older diatoms recrystallize, their small pores becoming filled with silica.\n\nDiatomite forms by the accumulation of the amorphous silica (opal, SiO·nHO) remains of dead diatoms (microscopic single-celled algae) in lacustrine or marine sediments. The fossil remains consist of a pair of symmetrical shells or frustules.\n\nIn 1836 or 1837, German peasant Peter Kasten discovered diatomaceous earth (German: \"Kieselgur\") when sinking a well on the northern slopes of the Haußelberg hill, in the Lüneburg Heath in North Germany.\n\n\nThe deposits are up to thick and are all of freshwater diatomaceous earth.\n\nUntil the First World War almost the entire worldwide production of diatomaceous earth was from this region.\n\nIn Germany, diatomaceous earth was also extracted at Altenschlirf on the Vogelsberg (Upper Hesse) and at Klieken (Saxony-Anhalt).\n\nThere is a layer of diatomaceous earth up to thick in the nature reserve of Soos in the Czech Republic.\n\nDeposits on the isle of Skye, off the west coast of Scotland, were mined until 1960.\n\nIn Colorado and in Clark County, Nevada, United States, there are deposits that are up to several hundred metres thick in places. Marine deposits have been worked in the Sisquoc Formation in Santa Barbara County, California near Lompoc and along the Southern California coast. Additional marine deposits have been worked in Maryland, Virginia, Algeria and the MoClay of Denmark. Freshwater lake deposits occur in Nevada, Oregon, Washington and California. Lake deposits also occur in interglacial lakes in the eastern United States, in Canada and in Europe in Germany, France, Denmark and the Czech Republic. The worldwide association of diatomite deposits and volcanic deposits suggests that the availability of silica from volcanic ash may be necessary for thick diatomite deposits.\n\nSometimes diatomaceous earth is found on the surfaces of deserts. Research has shown that the erosion of diatomaceous earth in such areas (such as the Bodélé Depression in the Sahara) is one of the most important sources of climate-affecting dust in the atmosphere.\n\nThe siliceous frustules of diatoms accumulate in fresh and brackish wetlands and lakes. Some peats and mucks contain a sufficient abundance of frustules that they can be mined. Most of Florida’s diatomaceous earths have been found in the muck of wetlands or lakes. The American Diatomite Corporation, from 1935 to 1946, refined a maximum of 145 tons per year from their processing plant near Clermont, Florida. Muck from several locations in Lake County, Florida was dried and burned (calcined) to produce the diatomaceous earth.\n\nThe commercial deposits of diatomite are restricted to Tertiary or Quaternary periods. Older deposits from as early as the Cretaceous Period are known, but are of low quality.\n\nDiatomaceous earth is available commercially in several formats:\n\nIn 1866, Alfred Nobel discovered that nitroglycerin could be made much more stable if absorbed in diatomite. This allows much safer transport and handling than nitroglycerin in its raw form. He patented this mixture as dynamite in 1867; the mixture is also called guhr dynamite.\n\nThe Celle engineer Wilhelm Berkefeld recognized the ability of the diatomaceous earth to filter, and he developed tubular filters (known as filter candles) fired from diatomaceous earth. During the cholera epidemic in Hamburg in 1892, these Berkefeld filters were used successfully. \nOne form of diatomaceous earth is used as a filter medium, especially for swimming pools. It has a high porosity because it is composed of microscopically small, hollow particles. Diatomaceous earth (sometimes referred to by trademarked brand names such as Celite) is used in chemistry as a filtration aid, to filter very fine particles that would otherwise pass through or clog filter paper. It is also used to filter water, particularly in the drinking water treatment process and in fish tanks, and other liquids, such as beer and wine. It can also filter syrups, sugar, and honey without removing or altering their color, taste, or nutritional properties.\n\nThe oldest use of diatomite is as a very mild abrasive and, for this purpose, it has been used both in toothpaste and in metal polishes, as well as in some facial scrubs.\n\nDiatomite is of value as an insecticide, because of its abrasive and physico-sorptive properties. The fine powder adsorbs lipids from the waxy outer layer of the exoskeletons of many species of insects; this layer acts as a barrier that resists the loss of water vapour from the insect's body. Damaging the layer increases the evaporation of water from their bodies, so that they dehydrate, commonly fatally. \n\nArthropods die as a result of the water pressure deficiency, based on Fick's law of diffusion. This also works against gastropods and is commonly employed in gardening to defeat slugs. However, since slugs inhabit humid environments, efficacy is very low. Diatomaceous earth is sometimes mixed with an attractant or other additives to increase its effectiveness. \n\nThe shape of the diatoms contained in a deposit has not been proven to affect their functionality when it comes to the adsorption of lipids; however, certain applications, such as that for slugs and snails, do work best when a particular shaped diatom is used, suggesting that lipid adsorption is not the whole story. For example, in the case of slugs and snails large, spiny diatoms work best to lacerate the epithelium of the mollusk. Diatom shells will work to some degree on the vast majority of animals that undergo ecdysis in shedding cuticle, such as arthropods or nematodes. It also may have other effects on lophotrochozoans, such as mollusks or annelids.\n\nMedical-grade diatomite has been studied for its efficacy as a deworming agent in cattle; in both studies cited the groups being treated with diatomaceous earth did not fare any better than control groups. It is commonly used in lieu of boric acid, and can be used to help control and possibly eliminate bed bug, house dust mite, cockroach, ant and flea infestations. \n\nDiatomaceous earth is widely applied for insect control in grain storage.\n\nIn order to be effective as an insecticide, diatomaceous earth must be uncalcinated (i.e., it must not be heat-treated prior to application) and have a mean particle size below about 12 µm (i.e., food grade – see below).\n\nAlthough considered to be relatively low-risk, pesticides containing diatomaceous earth are not exempt from regulation in the United States under the Federal Insecticide, Fungicide, and Rodenticide Act and must be registered with the Environmental Protection Agency.\n\nIts thermal properties enable it to be used as the barrier material in some fire resistant safes. It is also used in evacuated powder insulation for use with cryogenics. Diatomaceous earth powder is inserted into the vacuum space to aid in the effectiveness of vacuum insulation. It was used in the Classical AGA Cookers as a thermal heat barrier.\n\nDiatomaceous earth also finds some use as a support for catalysts, generally serving to maximize a catalyst's surface area and activity. For example, nickel can be supported on the materialthe combination is called Ni–Kieselguhrto improve its activity as a hydrogenation catalyst.\n\nNatural freshwater diatomaceous earth is used in agriculture for grain storage as an anticaking agent, as well as an insecticide. It is approved by the Food and Drug Administration as a feed additive to prevent caking.\n\nSome believe it may be used as a natural anthelmintic (dewormer), although studies have not shown it to be effective. Some farmers add it to their livestock and poultry feed to prevent the caking of feed. \"Food Grade Diatomaceous Earth\" is widely available in agricultural feed supply stores.\n\nFreshwater diatomite can be used as a growing medium in hydroponic gardens.\n\nIt is also used as a growing medium in potted plants, particularly as bonsai soil. Bonsai enthusiasts use it as a soil additive, or pot a bonsai tree in 100% diatomaceous earth. In vegetable gardening it is sometimes used as a soil conditioner, because like perlite, vermiculite, and expanded clay, it retains water and nutrients, while draining fast and freely, allowing high oxygen circulation within the growing medium.\n\nNatural dried, not calcinated diatomaceous earth is regularly used in livestock nutrition research as a source of acid insoluble ash (AIA), which is used as an indigestible marker. By measuring the content of AIA relative to nutrients in test diets and feces or digesta sampled from the terminal ileum (last third of the small intestine) the percentage of that nutrient digested can be calculated using the following equation:\n\nNatural freshwater diatomaceous earth is preferred by many researchers over chromic oxide, which has been widely used for the same purpose, the latter being a known carcinogen and, therefore, a potential hazard to research personnel.\n\nSpent diatomaceous earth from the brewing process can be added to ceramic mass for the production of red bricks with higher open porosity.\n\n\nCertain species of bacteria in oceans and lakes can accelerate the rate of dissolution of silica in dead and living diatoms; by using hydrolytic enzymes to break down the organic algal material.\n\nThe Earth's climate is affected by dust in the atmosphere, so locating major sources of atmospheric dust is important for climatology. Recent research indicates that surface deposits of diatomaceous earth play an important role. Research shows that significant dust comes from the Bodélé depression in Chad, where storms push diatomite gravel over dunes, generating dust by abrasion.\n\nInhalation of \"crystalline\" silica is harmful to the lungs, causing silicosis. \"Amorphous\" silica is considered to have low toxicity, but prolonged inhalation causes changes to the lungs. Diatomaceous earth is mostly amorphous silica, but contains some crystalline silica, especially in the saltwater forms. In a study of workers, those exposed to natural D.E. for over 5 years had no significant lung changes, while 40% of those exposed to the calcined form had developed pneumoconiosis. Today's common D.E. formulations are safer to use as they are predominantly made up of amorphous silica and contain little or no crystalline silica.\n\nThe crystalline silica content of D.E. is regulated in the United States by the Occupational Safety and Health Administration (OSHA), and there are guidelines from the National Institute for Occupational Safety and Health setting maximum amounts allowable in the product (1%) and in the air near the breathing zone of workers, with a recommended exposure limit at 6 mg/m over an 8-hour workday. OSHA has set a permissible exposure limit for diatomaceous earth as 20 mppcf (80 mg/m/%SiO). At levels of 3000 mg/m, diatomaceous earth is immediately dangerous to life and health.\n\nIn the 1930s, long-term occupational exposure among workers in the cristobalite D.E. industry who were exposed to high levels of airborne crystalline silica over decades were found to have an increased risk of silicosis.\n\nToday, workers are required to use respiratory-protection measures when concentrations of silica exceed allowable levels.\n\nDiatomite produced for pool filters is treated with high heat (calcination) and a fluxing agent (soda ash), causing the formerly harmless amorphous silicon dioxide to assume its crystalline form.\n\n\n\n"}
{"id": "31240336", "url": "https://en.wikipedia.org/wiki?curid=31240336", "title": "Disability Pride Parades", "text": "Disability Pride Parades\n\nDisability Pride Parades are parades held to celebrate disabled people.\n\nDisability Pride Parades seek to change the way people think about and define \"disability\", to end the stigma of disability, and to promote the belief that disability is a natural and beautiful part of human diversity in which people living with disabilities can take pride.\n\nDisability Pride Parades also usually coincide with Disability Pride Week in the communities where they are held.\n\nThe first Disability Pride Day was held in Boston, MA in 1990. The featured speaker was Karen Thompson, author of \"Why Can't Sharon Kowalski Come Home?\" The Boston Disability Pride Parade was held again in 1991, but has not been held since. It ended with the death of lead organizer, Diana Viets, and with the move of co-organizer Catherine Odette to Madison, WI.\n\nThe first Chicago Disability Pride Parade was the first Disability Pride Parade in the United States after the Boston-based parades of the 1990s. It was held on July 18, 2004.\n\nOn July 12, 2015 New York City held its first Disability Pride Parade; Tom Harkin and Mary LeDonne (daughter of Mike LeDonne, Founder/President of Disability Pride NYC ) were its grand marshals.\n\n\nDisability Pride Parades have been held many times in many places across the United States.\nThe first Chicago Disability Pride Parade was the first such parade in the United States after the Boston-based parades of the 1990s. It was held on July 18, 2004, and another Disability Pride Parade has been held in Chicago each subsequent July. The first Chicago parade was funded with $10,000 in seed money that Sarah Triano received in 2003 as part of the Paul G. Hearne Leadership award from the American Association of People with Disabilities.<ref name=\"http://www.itodaynews.com\"></ref> According to Triano, fifteen hundred people attended the parade. Yoshiko Dart was the parade marshal. The most recent Disability Pride Parade in Chicago was held July 23, 2016.\n\nThe Chicago Disability Pride Parade has a theme and a grand marshal each year.\n\nAfter attending the Philadelphia Disability Pride Parade in 2011, a group of NYC area based disability activists began brainstorming about how to make a Disability Pride Parade happen in NYC. Jazz musician Mike LeDonne, who had previously formed Disability Pride NYC, Inc. and was in the process of filing for non-profit status, joined with these activists to realize their shared dream. This group, together with support from the Mayor's Office for People With Disabilities (MOPD) held the first annual Disability Pride Parade on July 12, 2015, on the 25th anniversary of the signing of the ADA (Americans with Disabilities Act). Some of the seed money for the parade was raised from a Jazz concert called Jazz Legends Play For Disability Pride put on by Mike in which many of the biggest names in Jazz donated their talent for the night. Almost 4,000 people showed up for the first parade which culminated with a celebration featuring the talents of the disability community. There are 2 or 3 Grand Marshals each year and Mayor Bill De Blasio declared July to be Disability Pride Month in New York City.\n\n"}
{"id": "17069844", "url": "https://en.wikipedia.org/wiki?curid=17069844", "title": "Duck call", "text": "Duck call\n\nA duck call may be either the sound-imitation process by which a hunter lures waterfowl, or the actual tool which the person uses to do so. Early duck call tools were basic woodwind instruments, while later innovations are constructed of rubber and plastic, and allow the hunter to adjust the volume and tone of the calls with reeds. Today's duck calls usually fall into three main categories: a single, double, or triple reed call with many variations. The goal of a duck call is to sound like a realistic live duck.\n\nAs a tool, a duck call is like a traditional whistle made to emulate the sound of a duck. Early duck calls were simple woodwind instruments with a barrel, a sounding board and a reed. Hunters would use the air from their diaphragms into the call while saying \"hut\", \"wuit\", or \"oak\" to make the single quack. In order for a hunter to make the feeding call, the hunter must say a quick repetition of \"tiki-tika\" or \"duga-duga\". With the improvement of calls and calling techniques the more experienced callers do not use their voice to perform their techniques. The more experienced callers simply push the air from their diaphragms with no words to be spoken into the call. \n\nThe two most common high quality duck call materials are wood and acrylic. The key difference between wood and acrylic duck calls is that acrylic calls are much louder and carry much further than wood duck calls. This is important for different types of duck hunting. Acrylic calls are preferred in large, open spaces for a call to reach out and find the ducks. Most acrylic calls are single reed calls, because single reed calls have the capability of getting much louder than double reed calls. When hunting in timber or swampy area, a wood call is preferable. Another key difference is that acrylic is known to show less sound variance over the temperature ranges expected in a duck blind.\n\nWood expands and contracts with the changing temperature and humidity levels. It is still the preferred call, as it is more time consuming to manufacture and design. However decorative duck calls can be quite elaborate and expensive.\n\nBefore the mass production and popularization of duck calls hunters used their own voices (mouth calling) or used call ducks or duck decoys. This dates back to 1678, but it is believed that the use of call ducks originated in the Far East. Hunters would feed wild tame ducks and trap them, using their calls to attract wildfowl.\n\nNon-patent duck calls may have been made as early as 1850, but the first patent was awarded to Elam Fisher in 1870. In 1863, Fred Allen had created external duck calls, but did not have a patent. Allen's calls were barrel calls with straight tone boards and curved reeds. His most unusual call was the “Allen Nickel-Plated Duck Caller” which was made of metal but froze to the hunters' lips and had to be re-made using wood. Fisher was famous for the production of his Tongue Pincher Duck Calls which were made of two pieces of curved wood facing each other with a metal reed sandwiched between them and a holding device (usually a band) holding it all together. Tongue Pinchers had limited tone range and often cut the hunters' tongues and mouths. Fred Allen was the first to sell his duck calls commercially and began advertisements in 1880. David Fuller was the first to receive the patent for the goose call in 1885 and impacted the duck hunting world in 1903 with his combination goose/duck call. This call had a screw that retracted from inside the barrel which changed the sounds that were produced. In the early 1890s a blacksmith by the name of Victor Glodo Jr. began producing duck calls, he is credited with the barrel shape, his signature copper reeds and using the “duck wing” checkered pattern. Glodo Jr.'s most well-known production is the Reelfoot Lake Style duck call.\n\nFrom 1900-1910 many modifications were made to duck calls such as; the straight tone boards were replaced with curved ones, wedges were replaced with groove and cork locking systems, and there began to be production of duck calls made out of materials other than wood such as rubber and acrylic.\n\nPhillip Sanford Olt from Pekin, IL was one of the most widely distributed duck calls in the 1900s. He began his duck call company in 1904 with his Arkansas Style duck call which has a straight reed and curved tone board. Most made today still use this same technique. Olt received a patent in 1905 for the first adjustable tone duck call. It was made of a strong hard rubber-plastic that allowed hunters to change the volume and tone of the call. In 1950, Olt developed the Model D-2 Keyhole, which became incredibly popular. Many hunters would modify it to change the sounds and eventually this led to the production of the Model D-2 Cutdown (also nicknamed “Black Stick of Death”). Olt's calls were mostly made of rubber and wood, but he did produce some acrylic models.\n\nThrough 1920-1930s several new duck calls were produced using different styles or techniques to produce different kinds of sounds. Mid-1920s a man named Charles Ditto produced a duck call called the Eureka Model. It was made of two parts and contained a hard rubber insert and brass reed. Mid-1930 through the 1950s, father and son Clarence and Dudley Faulk produced some of the first plastic duck calls as well as several wooden ones. In 1935 using live ducks was outlawed, in the United States and the demand for duck calls increased incredibly.\n\nBy the 1950s custom call makers were showing up everywhere with the ability to engrave duck calls, include different colors or patterns and generally just make duck calls more showy/fancy.\n\nIn the early 1940s George Yentzen and his partner Cowboy Fernandez designed, invented, and patented the first double and triple reed duck calls from Black Walnut wood. The first double reed patent was submitted in March 7, 1946 by Yentzen, but did not get approved until September 3, 1950, because only war relief related patents were approved up until 1950. Yentzen died in 1959 prompting Cowboy to take over and start the Sure-Shot Game Call Company, with the most popular Yentzen Double Reed Duck Call. Cowboy won the World Championship and was also crowned the Champion of Champions in Stuttgart, Arkansas in 1959. He was the first Texan to win the two prestigious titles, and marked the first time the World Championship of Duck Calling using a double reed duck call. The Yentzen and Sure-Shot Game Calls dominated commercial markets for the next 20 years and beyond.\n\nIn 1972, Phil Robertson created his famous Duck Commander Call, and started his Duck Commander Company in 1973. His product and name have become increasingly famous since his son, Willie Robertson, turned the company from a family business into a multimillion-dollar empire. Their name and product became even more famous after their show, Duck Dynasty, aired on March 21, 2012.\n\nIn 1980, the rock band Genesis used duck calls, listed on the sleeve as 'duck', to trigger sounds on a Yamaha CS-80 polyphonic synthesiser when recording their album 'Duke'.\n\nThe two most popular styles of duck calls are the Arkansas style and the Louisiana style. The style is determined by the shape of the tone board and the effect it has on the reed material.\n\nThere are many different schools of thought on what technique is best for imitating the sounds that ducks make; however, every duck call should be able to create four basic calls: the quack, the feed call, and the comeback or hail call.\nThe quack is a short, sharp note that is the most used in waterfowl hunting. The feed is a sequence of rapid short notes of varying pitch that imitate ducks eating. The hail call or comeback call are the loudest and longest notes, typically used to attract the attention of far away ducks.\n\nThe most prevalent and hunted duck in the United States, the mallard, makes the well known \"quack\" sound many associate with ducks. Other species make many different sounds, ranging from high-pitched whistles to very low, grunt-like quacks. There are calls for almost all species of ducks. Pintails, teal, wood ducks, gadwall, diving ducks and other ducks including the calls of both the male, or drake and the female, or hen.\n\nIn many species, the call of the drake (male) is different from that of the hen (female). Mallard drakes make a lower pitch, longer quack than the hen mallard. This call is often used while feeding and when a mallard drake is landing. It gives the other birds a heads up. The quack of a mallard drake requires voice and is replicated by humming into a special whistle-like call. \nIn teals, the drakes make a call of short bursts of a high pitch whistle. The \"teet! (pause) teet! (pause) teet!-teet!\" or any other order of repetition. This call can be made by blowing short bursts of air into the whistle.\n\nThe majority of duck sounds such as quacking people have heard and are familiar with comes from female, or hen, mallards. Hen mallards are extremely vocal and this is probably why the number one call for duck hunting in North America is a hen mallard call. By making your call sound similar to that of a hen mallard, almost all other ducks will respond to it, and have their curiosity piqued by the calling.\n\n"}
{"id": "19265789", "url": "https://en.wikipedia.org/wiki?curid=19265789", "title": "Ear disease", "text": "Ear disease\n\nEar disease is a subfield of otolaryngology addressing the pathology of the ear.\n\nTwo of the major categories are otitis and hearing disorders. However, not all hearing disorders are due to structures of the ear.\n"}
{"id": "16527578", "url": "https://en.wikipedia.org/wiki?curid=16527578", "title": "Eddie Tagoe", "text": "Eddie Tagoe\n\nEddie Tagoe is a Ghanaian actor best known in the U.K. for playing \"Presuming Ed\" in the 1987 film production of \"Withnail and I\", a role which he resumed in 2000 in a stage production of the same work. Internationally, he may be better recognized for his brief appearance in the 1981 hit movie, \"Raiders of the Lost Ark\". Billed only as the \"Messenger Pirate\", his character was sent to find Indiana Jones in advance of Nazis boarding the ship on which Jones was travelling. Initially unable to find Jones, he was instructed by the captain to look again, and immediately replied, \"I found him!\", pointing to Jones swimming to the Nazi submarine.\n\nTagoe had a significantly larger part in his film debut, \"Who Is Killing the Great Chefs of Europe?\" (1978), and as \"Chocolate Mousse\" in the 1984 farce, \"Top Secret!\". He appeared in various other roles such as Sgt. Gwambe in \"\" (1985), as well as \"The Dogs of War\" (1980), \"Pink Floyd The Wall\" (1982) and \"Spaghetti House\" (1982). Tagoe also appeared in episodes of a number of British television series, including \"Legacy of Murder\", \"Prospects\", and \"The Bill\", becoming \"a well-known face on British television as an actor\".\n\nThe son of Ghanaian Chieftain Asafoatshe Ayah Tagoe, Eddie Tagoe travelled to London to study reflexology prior to pursuing acting. He then received a grant from the government of Ghana to study at the Royal Academy of Dramatic Art in London. In 1995, however, he returned to his reflexology career when he was recruited to serve as team reflexologist for Newcastle United F.C.\n"}
{"id": "46210785", "url": "https://en.wikipedia.org/wiki?curid=46210785", "title": "Erna Takazawa", "text": "Erna Takazawa\n\nErna Takazawa (born 1988) is the first fully qualified optometrist of Samoa. In 2014, she was awarded as one of the inaugural recipients the Queen's Young Leader Award.\n\nErna Fumi Rebecca Fotuofaamanuiaga Takazawa was born 21 December 1988 in Japan but raised in Samoa. She graduated with First Class Honors with degree in optometry from the University of Auckland in 2011 and was licensed in 2012. She is the first fully qualified optometrist on the island and began working immediately at the newly established Samoa Vision Centre.\n\nTakazawa also works in conjunction with the National Health Service and SENESE, a support program for special needs children. Her work has led to free eye care services for children and the elderly and a reduction in cost of eyewear for adults. In 2013, she screened over 200 athletes with the Special Olympics.\n\nIn 2014, Takazawa's work was recognized when she was awarded the Queen's Young Leader Award. The annual award is given to selected citizens throughout the Commonwealth who work to better lives in their communities.\n"}
{"id": "3311580", "url": "https://en.wikipedia.org/wiki?curid=3311580", "title": "European Food Safety Authority", "text": "European Food Safety Authority\n\nThe European Food Safety Authority (EFSA) is the agency of the European Union (EU) that provides independent scientific advice and communicates on existing and emerging risks associated with the food chain. EFSA was established in February 2002, is based in Parma, Italy, and has a budget for 2016 of €79.5 million, and a total staff of 447. \n\nThe work of EFSA covers all matters with a direct or indirect impact on food and feed safety, including animal health and welfare, plant protection and plant health and nutrition. EFSA supports the European Commission, the European Parliament and EU member states in taking effective and timely risk management decisions that ensure the protection of the health of European consumers and the safety of the food and feed chain. EFSA also communicates to the public in an open and transparent way on all matters within its remit.\n\nBased on a regulation of 2002, the EFSA is composed of four bodies:\n\n\nThe Management Board sets the budget, approves work programmes, and is responsible for ensuring that EFSA co-operates successfully with partner organisations across the EU and beyond. It is composed of fourteen members appointed by the Council of the European Union in consultation with the European Parliament from a list drawn up by the European Commission, plus one representative of the European Commission.\n\nThe Executive Director is EFSA's legal representative and is responsible for day-to-day administration, drafting and implementing work programmes, and implementing other decisions adopted by the Management Board. They are appointed by the Management Board.\n\nThe Advisory Forum advises the Executive Director, in particular in drafting a proposal for the EFSA's work programmes. It is composed of representatives of national bodies responsible for risk assessment in the Member States, with observers from Norway, Iceland, Switzerland and the European Commission.\n\nThe Scientific Committee and its Scientific Panels provide scientific opinions and advice, each within their own sphere of competence, and are composed of independent scientific experts. The number and names of the Scientific Panels are adapted in the light of technical and scientific development by the European Commission at EFSA's request. The independent scientific experts are appointed by the Management Board upon a proposal from the Executive Director for three-year terms.\n\nThe scientific output of the European Food Safety Authority is published in the \"EFSA Journal\", an open-access, online scientific journal. This concerns risk assessment in relation to food and feed and includes nutrition, animal health and welfare, plant health and plant protection.\n\nEFSA has been criticised for their alleged overregulation, as well as allegations of \"frequent conflicts of interest\", some of them undeclared.\n\nEFSA has also been criticised by the NGO CHEM Trust for misrepresenting the results of their expert committee's report on Bisphenol A (BPA) in January 2015. EFSA claimed in the abstract, press release and briefing that Bisphenol A 'posed no risk' to health, when the expert report actually stated the risk was 'low' when considering aggregate exposure (beyond just food). EFSA later modified the abstract to correct this error, though the press release remains unchanged. EFSA have argued that use of 'no health concern' in their press release and Bisphenol A briefing is to ensure these materials are accessible, though this rationale is disputed by CHEM Trust.\n\n\n"}
{"id": "2604726", "url": "https://en.wikipedia.org/wiki?curid=2604726", "title": "Exercise hypertension", "text": "Exercise hypertension\n\nExercise hypertension is an excessive rise in blood pressure during exercise. Many of those with exercise hypertension have spikes in systolic pressure to 250 mmHg or greater.\n\nA rise in systolic blood pressure to over 200 mmHg when exercising at 100 W is pathological and a rise in pressure over 220 mmHg needs to be controlled by the appropriate drugs.\n\nSimilarly, in healthy individuals the response of the diastolic pressure to 'dynamic' exercise (e.g. walking, running or jogging) of moderate intensity is to remain constant or to fall slightly (due to the improved blood flow), but in some individuals a rise of 10 mmHg or greater is found.\n\nRecent work at Johns Hopkins involving a group of athletes aged 55 to 75 with mild hypertension has found a correlation of those with exercise hypertension to a reduced ability of the major blood vessels to change in size in response to increased blood flow (probably due to impaired function of the endothelial cells in the vessel walls). This is to be differentiated from stiffness of the blood-vessel walls, which was not found to be correlated with the effect.\n"}
{"id": "16590097", "url": "https://en.wikipedia.org/wiki?curid=16590097", "title": "Figure Reasoning Test", "text": "Figure Reasoning Test\n\nFigure Reasoning Test (FRT) is an intelligence test created by John Clifford Daniels.\n\nIt is used by a few Mensa chapters in Europe for their admissions tests.\n"}
{"id": "17720767", "url": "https://en.wikipedia.org/wiki?curid=17720767", "title": "Galactosialidosis", "text": "Galactosialidosis\n\nGalactosialidosis is a lysosomal storage disease. This condition is rare and most cases have been in the juvenile/adult group of patients. An infantile form has been described.\n\nIt is associated with cathepsin A. This disease is due to mutations in the CTSA gene which encodes the protective protein/cathepsin A (PPCA). This in turn leads to a secondary deficiency of beta-galactosidase (GLB1) and neuraminidase 1 (NEU1).There are three distinct CTSA isoforms.\n\nA prenatal diagnosis was made by Kleijer et al. in 1979 by measuring beta-galactosidase and neuraminidase activities in cultured amniotic fluid cells.\n\n"}
{"id": "35324684", "url": "https://en.wikipedia.org/wiki?curid=35324684", "title": "Health in the Republic of the Congo", "text": "Health in the Republic of the Congo\n\nThe Republic of the Congo face a number of ongoing health challenges.\n\nPublic expenditure health was at 8.9% of the GDP in 2004, whereas private expenditure on health related costs was at 1.3% of private income. Health expenditure was at US$30 per capita in 2004. There were 20 doctors per 100,000 persons in the early 2000s (decade).\n\nThe 2014 CIA estimated average life expectancy in the Republic of the Congo was 58.52 years.\n\nThe entire population of the Republic of the Congo is at high risk of malaria and transmission is intense all year round. The annual reported number of malaria cases in 2012 was 117,640 with 623 deaths.\n\nYellow fever is also endemic to the Congo.\n\nThe 2013 HIV prevalence is at 3.4% among 15- to 49-year-olds. \n\nA large proportion of the population is undernourished. \n"}
{"id": "5156803", "url": "https://en.wikipedia.org/wiki?curid=5156803", "title": "Health surveillance", "text": "Health surveillance\n\nHealth surveillance may refer to:\n\n"}
{"id": "26064260", "url": "https://en.wikipedia.org/wiki?curid=26064260", "title": "Hirvijärvi Reservoir", "text": "Hirvijärvi Reservoir\n\nHirvijärvi Reservoir () is medium-sized lake in the Lapuanjoki main catchment area. It is located in the region Southern Ostrobothnia, in the municipality of Seinäjoki, Finland.\n\n"}
{"id": "31469779", "url": "https://en.wikipedia.org/wiki?curid=31469779", "title": "Illicit drug use in Ireland", "text": "Illicit drug use in Ireland\n\nIllicit drug use in Ireland & Northern Ireland has been growing since the mid-1970s. The use by young people of psychedelic drugs, including LSD and cannabis, was recognized at that time. Opiate abuse was uncommon until the 1980s, following events in the opium production centres of Afghanistan and Iran. Government task forces and private programmes were formed to tackle increased opiate abuse. Dublin and Ballymena have been centres of increased heroin use and preventative efforts. Studies confirmed significant opiate use in the 1990s, when action to reduce harm caused by drug use became favoured. Programmes focussed on controlling the spread of HIV, seen as a greater social threat than drug abuse itself.\n\nHeroin use in Ireland has always centred on Dublin, and to a lesser extent Cork city. Heroin abuse became a major problem in inner-city Dublin in the late 1970s. Earlier, there was no evidence of anything more than isolated use of heroin. In December 1968, the Minister for Health, Seán Flanagan, established a working party to investigate the extent of drug abuse at the time and to advise the government. Their research, reported in 1971, could not find any evidence of significant use of heroin, which they attributed to the difficulty of obtaining supplies at the time. Drug use was limited mostly to cannabis and LSD. These drugs were seen as part of student sub-culture; Dr. Hugh Byrne, a TD debating what was to be the 1977 Misuse of Drugs Act, described Trinity College Dublin as \"a nest and a hive for the production of LSD [...] leaflets containing the formula of LSD have been freely sold around the campus\". He blamed this activity on foreign students in areas of \"advanced study\"\n\nThe main treatment centre for drug users was at Jervis Street Hospital. The National Drug Advisory and treatment Centre was founded there in 1969. In 1973, the Coolmine therapeutic community was founded as a voluntary body to provide a structure for people to \"maintain a drug-free existence\".\n\nIn 1979, there was a dramatic increase in the supply of heroin to Western Europe, usually attributed to the fall of the Shah in Iran and the Soviet invasion of Afghanistan. This marked the start of an epidemic in inner-city Dublin.\n\nThe number of heroin users in Dublin continued to grow in the early 1980s. The 1983 Bradshaw Report found that in north central Dublin, 10% of 15- to 24-year-olds had used heroin in the previous year; the figure was 12% for 15- to 19-year-olds, and 13% for females of the same age group. The report also confirmed Dublin as a centre for heroin use, with only three or four heroin users in Cork and Galway.\n\nFollowing this report, the government created a Special Governmental Task Force on Drug Abuse in April 1983. Their report recommended funding community facilities in deprived areas, but this was at odds with government policy at the time, so the report went unpublished. The government's position was that drug abusers were victims of their own choices, rather than their socio-economic circumstances. The Misuse of Drugs Act 1984 was enacted to provide for tougher punishments than the 1977 Act.\n\nThe 1980s also saw the rise of community groups which organised themselves to rid their local areas of drugs. Priests, politicians and even Provisional IRA members took part in residents' associations in areas of Dublin such as Fatima Mansions, the Hardwicke Street flats, St. Teresa's Gardens, and Dolphin House. Groups met to name and shame drug dealers, giving them the choice either to stop dealing or leave the area. Actions broadened to include patrols by residents, checkpoints to search vehicles for drugs, forced evictions, and other vigilante actions. These local groups got together and adopted a constitution in February 1984, naming themselves \"Concerned Parents Against Drugs\".\n\nThe Drug Treatment Centre Board moved to Trinity Court in 1988 following the closure of Jervis Street hospital.\n\nThe most significant event of the decade was the arrival of the HIV/AIDS epidemic to Ireland. The first diagnosed case of AIDS was in 1982. Early cases before 1987 were found in homosexual men, this soon spread to intravenous drug users, overtaking cases amongst homosexual men. A survey by the Department of Health in 1986 found that 30% of intravenous drug users were HIV positive.\n\nThere were an estimated 13,460 opiate users in Ireland in 1996. The HIV/AIDS epidemic in Ireland was most active among intravenous drug users. Treatment in centres such as Trinity Court required a commitment from the patient to achieve abstinence from drugs. In light of the HIV epidemic, this policy was revised in 1992 to one of harm reduction. This different approach recognised that the harms of drug use, such as the spread of HIV, were of a greater danger to society than drug use itself. Harm reduction was implemented in the form of methadone maintenance and needle exchange programmes.\n\nThe first needle exchange opened in 1989 and there were about eleven others by the end of the 1990s. There are now plans to offer needle exchange services at pharmacies.\nHead shops did exist legally in Ireland, and were reported by authorities to be opening at a rate of one per week in January 2010. Some of the shops were open 24 hours a day, serving through a hatch at night. The legality of the shops was discussed in Seanad Éireann that month, with an all-party motion being passed requesting the Government to introduce legislation to regulate the sale of products. One head shop in Roscommon received objections from residents two weeks after opening for business that month.\n\nHead shops received a lot of media attention in 2010, with one doctor describing on the television programme, \"Prime Time\", patients of his who suffered hallucinations, anxiety and psychosis after experiencing \"legal highs\" party powders from head shop substances. Politicians weighed in, with Chris Andrews in favour of outlawing head shops while Jim McDaid said this would be a \"huge mistake\" which would allow illegal street dealers to thrive. There was controversy and irony when a judge renowned for his strict anti-drug sentencing discovered that a premises he had rented to a business in Naas contained a head shop, and evicted the operator.\n\nA Dublin head shop exploded and caught fire on 12 February 2010, engulfing a neighbouring building in fire and the surrounding streets and quays in smoke, causing Capel Street to be closed for the day. The blaze levelled two other businesses including a sex shop, as one of Dublin's busiest streets was evacuated. A second head shop burned down on 16 February 2010 in Dublin. On 10 March 2010, two pipe bombs were found outside two separate head shops in Athlone, and Garda bomb disposal experts closed two main streets in the town. The attacks were later traced to disgruntled drug dealers.\n\nAnother burned down on 11 March 2010 in Sligo, and an adult shop also caught fire. On 16 April 2010 in Dundalk, County Louth, a head shop was set alight in a petrol bomb attack. The county is home to then Minister for Justice Dermot Ahern and hours later plans for legislation for regulation of head shops got underway.\n\nOn 28 March 2010, vigilante group Republican Action Against Drugs (RAAD) claimed responsibility for planting an explosive device outside a head shop in Letterkenny, County Donegal. It was made safe by the security forces. RAAD issued a statement that it was the \"first and only warning\" the shop would receive; the head shop closed shortly afterwards.\n\nMany head shop products became illegal in Ireland on 23 August 2010 when the new \"Criminal Justice (Psychoactive Substances) Act 2010\" became law. The Act empowered Gardaí to seek court orders to close head shops suspected of selling drug-like products, with the onus on the owners to prove they are not doing so.\n\nFollowing this legislation, the number of head shops declined dramatically from 112 to just 12.\n\nFollowing a decision by the Court of Appeal on the tenth of March 2015, concerning the powers of the Government to control substances which are harmful to human health under Section 2 (2) of the Act of the Misuse of Drugs Act 1977, possession of many drugs including ecstasy, magic mushrooms, so called head shop drugs and other new psychoactive drugs became legal in Ireland. The following day new legislation was introduced reinstating the ban on the previously legalised drugs. However, owing to technical reasons the ban could not come into law until the following day. Consequently, for one day (eleventh of March 2015) many headshop type drugs and an assortment of other synthetic drugs including amphetamine, MDMA, Khat and ketamine were legal to possess in Ireland.\n\n"}
{"id": "37328190", "url": "https://en.wikipedia.org/wiki?curid=37328190", "title": "Jerry Lonecloud", "text": "Jerry Lonecloud\n\nJerry Lonecloud (July 4, 1854 – April 16, 1930) was an entertainer, ethnographer and medicine man for the Mi'kmaq people in Nova Scotia. He wrote the first Mi'kmaq memoir, which is entitled \"Tracking Dr. Lonecloud:Showman to Legend Keeper\". Historian Ruth Holmes Whitehead wrote, \"Ethnographer of the Micmac nation could rightly have been his epitaph, his final honour.\"\n\nIn the 1880s, Lonecloud adopted the name \"Dr. Lonecloud\" during his career with American Medicine Shows and Wild West Shows, including John Healy and Charles Bigelow’s Kickapoo Indian medicine, Buffalo Bill Cody’s Wild West Show and the Kiowa Medicine Show. \n\nAs an ethnographer he worked extensively with historian and archivist Harry Piers.\n\nHe and his family were living at Tuff's Cove in Dartmouth during the Halifax Explosion. Two of his daughters were killed and he lost an eye. He died in Halifax on April 16, 1930.\n\n\n"}
{"id": "53508117", "url": "https://en.wikipedia.org/wiki?curid=53508117", "title": "Khalil Bin Ebrahim Hassan", "text": "Khalil Bin Ebrahim Hassan\n\nKhalil Bin Ebrahim Hassan () is a Bahraini surgeon and diplomat who has been the first Ambassador of the Kingdom of Bahrain to Japan (since 2005) and was also the country's health minister.\n\nKhalil Bin Ebrahim Hassan graduated from Damascus University Medical School in 1972. Throughout the rest of the decade he worked in the United Kingdom before returning to Bahrain and working there in several hospitals during the 1980s. During the 1990s, Hassan was a professor in a university before being appointed minister of health of Bahrain in 2002. He was appointed the kingdom's ambassador to Japan in 2005, when Bahrain first opened an embassy in the country.\n\nHe is married and has two daughters and three sons. His hobbies include golf, photography, tennis, squash, swimming, traveling.\n"}
{"id": "35230534", "url": "https://en.wikipedia.org/wiki?curid=35230534", "title": "Maternal health in Uganda", "text": "Maternal health in Uganda\n\nThe health of a mother impacts the family and even the entire community. Her ability and access to receive necessary healthcare largely determines health outcomes for herself and her baby. Like many developing countries, Uganda has high maternal mortality rates, which is often reflective of access to health care services. Even when health care services are available, they are often understaffed and low on supplies which can also have an effect. Traditionally, Ugandan women seek to handle birth on their own as it is a time when they can use their own power and make their own decisions which can also be a factor in such a high maternal mortality rate. Many women report mistreatment from healthcare personnel as an additional reason to avoid seeking professional care during pregnancy and labor. A study also found that a majority of Ugandan women lack health literacy and in turn seek care in more traditional or homeopathic ways. Malaria is also a substantial issue. Pregnant women and their newborn babies are particularly susceptible to complications related to malaria, which is endemic in Uganda. This is also an issue that needs to be addressed in order to improve maternal mortality in Uganda.\n\nThe World Health Organization defines health as the enjoyment of the highest attainable standard of health and is one of the fundamental rights of every human being. Amartya K. Sen, a feminist economist, adds to that as he said that health is among the most important conditions of human life and a critically significant constituent of human capabilities which we have reason to value. It has been found that healthier nations, or those with a greater life expectancy and lower infant mortality, see greater economic growth and prosperity. The argument has also been made the other way that economic growth contributes to healthier nations. It's not just the overall availability of resources that improves health, but the access by the public to those resources. Sen argues that health only improves during economic prosperity if there is a shift in resource allocation towards health and education, equitable distribution of income, and extensive employment programs to decrease the unemployment rate.\n\nSen writes, \"The factors that can contribute to health achievements and failures go well beyond health care, and include many influences of very different kinds, varying from (genetic) propensities, individual incomes, food habits and lifestyle, on the one hand, to the epidemiological environment and work conditions, on the other...We have to go well beyond the delivery and distribution of health care to get an adequate understanding of health achievement and capability.\"\n\nUNICEF found that healthy children need healthy mothers. A woman in Sub-Saharan Africa has a 1 in 16 chance of dying in childbirth. The report found that at least 20% of the burden of disease in children below the age of 5 is related to poor maternal health and nutrition, as well as quality of care at delivery and during the newborn period. Yearly, 8 million babies die before or during delivery or in the first week of life. Further, many children are tragically left motherless each year. These children are 10 times more likely to die within two years of their mothers' death. It has also been found that the health of the mother vastly effects the health of all of her children. The health of our mothers vastly impacts the health and success of our future generations.\n\nThe World Health Organization (WHO) defines maternal health as the health of women during pregnancy, childbirth and the postpartum period. According to estimates from UNICEF, Uganda’s maternal mortality ratio, the annual number of deaths of women from pregnancy-related causes per 100,000 live births, stands at 435 after allowing for adjustments. Women die as a result of complications during and following pregnancy and childbirth and the major complications include severe bleeding, infections, unsafe abortion and obstructed labor.\n\nUganda is slow in its progress in the fifth goal of improving maternal health in its Millennium Development Goals. With the 2015 target for maternal mortality ratio at 131 per 100,000 births and proportion of births attended by skilled health personnel set at 100%, Uganda has a long battle in reaching its intended goals. Moreover, the methodology used and the sample sizes implemented by the Uganda Demographic Health Survey (UDHS) do not allow for precise estimates of maternal mortality. This suggests that the estimates collated are erroneous and it is conceivable that the actual rates could be much higher than those reported.\nHigh maternal mortality rates persist in Uganda due to an overall low use of contraceptives, limited capacity of health facilities to manage abortion/miscarriage complications and prevalence of HIV/AIDS among pregnant women. Despite malaria being one of the leading causes of morbidity in pregnant women, prevention and prophylaxis services are not well established.\n\nThe Human Development Report ranked 183 countries based on a variety of criteria. Uganda ranked 161 out of 183 countries. It's high ranking put it is under the Low Human Development category. The Human Development Index (HDI) is a composite index that measures the achievement of countries in three basic dimensions of human development-a long and healthy life, knowledge, and a decent standard of living. Uganda received a value of 0.446. The closer the number is to one, the better the country is in regards to human development. Norway is ranked at number one with a HDI of 0.943. The next variable that was taken into consideration is life expectancy at birth in years. Uganda's life expectancy is 54.1 years. The mean number of years in school is the average number of years of education received by people ages 25 and older. In Uganda the mean is only 4.7. Expected years of schooling is the number of years of schooling that a child of school entrance can expect to receive if prevailing patterns of age-specific enrollment rates persist throughout the child's life. I Uganda, the expected years of schooling is 10.8. The Gross National Income (GNI0 is the aggregate income of an economy generated by its production and its ownership of factors of production, less the incomes paid for the use of factors of production owned by the rest of the world, converted to international dollars using purchasing power parity rates divided by the midyear population. In Uganda the GNI is $1,124.00. The GNI per capita rank minus the HDI rank: Difference in ranking by GNI per capita and by the HDI. In Uganda this measure is 7. The last measure is the Nonincome HDI, which is the value of the HDI computed from the life expectancy and education indicators only. In Uganda this measure is 0.506.\n\nThe Gender Inequality Index (GDI) is a composite measure that reflects inequality in achievements between women and men in three categories: reproductive health, empowerment, and the labor market. Out of 183 countries, Uganda ranks 11th. Their maternal mortality is 430 deaths per 100,000 live births. The fertility rate adolescents (ages 15–19) is 149.9 per 1,000. Women currently hold 37.2 seats in parliament. Only a meager 9.1% of the female population has a secondary education or higher. 78.3% of women are participating in the labor force. Only 24% of women who are in their childbearing years use some sort of contraceptive method. 94% of women are receiving at least 1 antenatal visit, but only 42% are giving birth with the aid of a skilled attendant. The fertility rate in Uganda is 5.9 children per female. Based on the information given in these tables there appears to be a correlation between the high maternal mortality ratio and high fertility rate which could be associated with low contraceptive use. The low percentage of women giving birth with a skilled birth attendant could also be associated with a high maternal mortality ratio.\n\nUganda is an African country within the Sub-Saharan African Region. When comparing the rates of Uganda to the region Uganda has a much lower maternal mortality ratio, 430 versus 619 deaths per 100,000. Sub-Saharan Africa fertility rate is 119 per 1,000-which is significantly less than Uganda. This is suggesting that when compared to other nations in the same region, Uganda has more teen pregnancies and births. However, the region averages only 19 seats in parliament, which is much lower than Uganda. 22.2% of women in the region have at least a secondary education. This is significantly higher than Uganda suggesting that higher education for women is more difficult to achieve in Uganda compared to the average number in the region. 62.9% of women in Sub-Saharan Africa are actively participating in the labor force. Uganda seems to have greater participation from women than the region's average. Uganda is the average when it comes to contraceptive use in the region suggesting this is an issue of that entire region. 73.6% of women attend at least one antenatal care visit in the region. Uganda's is higher, suggesting more Uganda women take advantage of healthcare services than other women in the region. Uganda is also the average for the region in percent of births done with the aid of a skilled attendant also suggesting this is a regional issue. The fertility rate in Sub-Saharan Africa is 4.8, which is a lot lower than the fertility rate in Uganda.\n\nLow secondary education rates, low access to health care services, low use of contraceptives coupled high fertility rates is contributing to a high percentage of maternal mortality in Uganda. However, we do see a high labor force participation which could potentially lead increase empowerment for women and an increase in monetary gain to spend on health services. However, currently the high labor force participation doesn't seem to be aiding Uganda women in regards to maternal health. Uganda's extremely low percentage of women with at least a secondary education could also be attributing to a higher fertility rate and less desirable birth outcomes, as education affects health literacy and identifying the need for assistance from a skilled birth attendant. Studies have shown how the poor health of women can have a negative impact on economic achievement for the country as a whole. Low economic development then constrains the capability of women to achieve better health.\n\nAlmost all women in developing countries have at least four antenatal care visits, are attended to by a skilled health worker during childbirth, and receive postpartum care. In contrast, only 47% of Ugandan women receive antenatal care coverage and only 42% of births are attended by skilled health personnel. Among the poorest 20% of the population, the share of births attended by skill health personnel was 29% in 2005/2006 as compared to 77% among the wealthiest 20% of the population. The case of Jennifer Anguko, a popular elected official who bled slowly to death in the maternity ward in a major hospital, aptly exemplifies the poor state of maternal health care that is provided to women, even in major urban healthcare facilities.\n\nDespite the national policy of promoting maternal health through promoting informed choice, service accessibility and improved quality of care through the national Safe Motherhood Programme (SMP), it remains a challenge to the Ugandan government as to how it would achieve its 2015 Millennium Development Goals of reducing maternal mortality rates and 100% births attended to by skilled health personnel. In order to achieve future economic growth, it is vital that the population remains healthy.\n\nA study was done in 2007 in 54 districts and 553 health facilities in Uganda to determine availability of emergency obstetric care and its related maternal deaths. The study found that few of these units had running water; electricity or a functional operating theater. However, having these items was shown to have a protective effect on maternal deaths.\n\nThe availability of midwives had the highest protective effect, reducing the case fatality rate by 80%. This study found that while 97.2% of health facilities were expected to have emergency obstetric care services, few had provided these services. This is the most likely explanation for the high health facility-based maternal death rate of 671 per 100,000 live births in Uganda in 2007. The study concluded that addressing health system issues, particularly among human resources, and increasing access to emergency obstetric care could reduce maternal mortality.\n\nThere is not only the issue of lack of resources in healthcare services, there are also cultural barriers in women seeking professional care. Ugandan women adhere to very traditional birthing practices and believe that pregnancy is a test of endurance and maternal death is merely a sad but normal event. This cultural view also hinders the chances of women seeking professional maternal care. In the Kiboga community it is evident that pregnancy and childbirth were one of the major areas where women still command power and status, which they would strive to keep to enhance their status within the household and community.\n\nOne major issue in regards to maternal health is access to quality emergency obstetric care and the many barriers Ugandan women face to gain access to such care. A needs assessment of emergency obstetric care was carried out in 197 health facilities in 19 out of 5 health districts in Uganda, covering 38% of the total population. The study found in 2005, that there was a large number of missing signal functions at health facilities and an urgent need to improve the availability of emergency obstetric care. It was found that the improvement of care begins by improving district health workers' skills in emergency obstetric care so that they can effectively manage and treat obstetric complications. Part of this training includes hospital staff on how to properly manage data systems to better monitor and evaluate program implementation. An effective advocacy tool, known as REDUCE, has already been developed for Uganda to stimulate policy dialog and strategic planning. The REDUCE tool uses computer models to estimate the human and economic consequences of maternal mortality, and generates data that can be used to create arguments for giving higher priority to maternal mortality reduction in creating policies, strategy development, and resource allocation.\n\nAnother factor preventing women from seeking emergency obstetric care among Ugandan women is the cultural desire for Ugandan women to 'protect their own integrity.' A common birthing practice is for women to give birth completely alone and the individual is the one who decides if outside help is needed. It has also been found that women feel that they have the most power and control during the birthing process, which is something they often lack in other aspects of their lives. Women are considered to be strong and independent if they can handle the birthing process by themselves. These beliefs often lead to very dangerous circumstances as the women often delay assistance, which sometimes costs their life or the life of the baby. Oftentimes one of these main issues is obstructed labor. When women realize labor is not progressing normally they first seek female friends or traditional birth attendants. This can result in a further delay in seeking medical attention from someone who is trained to handle such complications.\n\nA study conducted in 2001 found that one common remedy used for obstructed labor in home births was herbs. As high as 80% of childbirths used herbs. Ugandan culture also sees the birthing process as a woman's affair and therefore oftentimes there is little male involvement. Transportation is also another issue in Uganda. Most families do not own personal cars and cannot afford taxi fares. More remote and rural areas cannot be reached by car but must be reached using a motorcycle. These conditions are not ideal in transporting a woman in labor, so women tend to choose to stay home during labor. Women also avoid healthcare facilities as this also inhibits their own integrity. They feel that they are powerless in a hospital, have little say in decisions, and know little about procedures being done to them. There is also a lack of medical supplies in Ugandan hospitals and healthcare is run on a fee for service basis. Many women do not have the funds necessary to both travel to a hospital and pay for hospital services and supplies. Or if they do have the funds, hospitals could also be out of supplies. This further discourages them from giving birth in a healthcare setting.\n\nThere is an urgent need to educate both men and women on the risks of having home or solitary childbirths that aren't assisted by a skilled attendant. A study conducted in 2011, suggests that birthing outcomes would improve if men were intimately involved in the process and could assist in making decisions regarding obstructed labor or other complications. The integrity of women could be maintained if health workers were more compassionate and more able to support women and provide understandable information during labor. There is also a great need to improve both access and quality of healthcare offered to the masses in Uganda.\n\nIn 2003, a study was conducted in Hoima, Uganda, whose aim was to examine the reasons why, when faced with complications of pregnancy or delivery, women continued to choose high-risk options leading to severe morbidity and mortality. The study found that women considered the use of primary health units and the referral hospital when complication occurred as a last resort. Women reported that a lack of skilled staff, complaints of abuse, neglect, and poor treatment in the hospital, and poorly understood reasons for procedures, plus health workers’ views that women are ignorant, also explain why many women consider going to a hospital for delivery as a last resort.\n\nMany women do not utilize healthcare services because they do not understand reasons for procedures. Health literacy is also a large issue among women in Uganda. It not only affects birthing outcomes but also information on reproductive care. Another study conducted in 2012, found that many Ugandan women rely on myths, rumors, and misconceptions that discourage them from using reproductive health services, particularly family planning.\n\nAnother study conducted in 2011 found that even those pregnant women who attended antenatal classes had very little knowledge of danger signs during pregnancy. An association was found between birthing preparedness and knowledge of danger signs. The most common birth preparedness practice was saving money to facilitate referral in case of complications. While only 68% of women in the study had attended at least four antenatal care visits during their last pregnancy. Only 19% of women in the study could indicate at least three danger signs. This shows that a considerable share of those who seek professional care are not receiving or retaining vital information. It was found that women appear to be unaware of the risk they take by subjecting themselves to prolonged labor. The study found that among women who went to antenatal visits 40% had not been advised where to deliver and the staff were allegedly unfriendly. There also seems to be an association between having a delivery by skilled birth attendants and being under the age of twenty. It is hoped that it is becoming more culturally acceptable to give birth with the assistance of a skilled attendant than it is among the older generations. The study concluded that every woman should be made aware of the likelihood of complications during pregnancy, childbirth, labour, and the postpartum periods. There has also been success using mobile phones to provide health information in HIV programs in Uganda. Since mobile phones are becoming more and more common, it appears this could also be a good route for antenatal education.\n\nIn rural areas, conceiving pregnant women seek the help of traditional birth attendants (TBAs) due to difficulty in accessing formal health services and also high transportation or treatment costs. TBAs are trusted as they embody the cultural and social life of the community. However, the TBAs’ lack of knowledge and training and the use of traditional practices have led to risky medical procedures resulting in high maternal mortalities.\n\nIn 2006 it was found that some rural areas of Uganda up to 90% of the population uses traditional medicine for day to day healthcare needs. The World Health Organization estimates that 80% of the developing world uses traditional medicinal practices. It was found that over 80% of child births that are conducted at home use herbal remedies in the Bushenyi district of Uganda. Over seventy five plants have been recorded for use to induce labor and some of these plants could be oxytocic. The danger lies in levels of dosage as to whether or not the plants could potentially bring harm to the mother and baby. These medicinal herbs are often used because Ugandans cannot afford western pharmaceuticals. These herbal remedies are also socially and culturally accepted. In Uganda reproductive health issues such as maternal mortality and morbidity, account for the number one disease burden. Perinatal and maternal-related conditions account for 20.4%, malaria 15.4%, acute lower respiratory infections 10.5%, AIDS 9.1%, and diarrhea 8.4%. These conditions account for over 60% of the total burden.\n\nIn Uganda, it is viewed that a woman who had died in childbirth is equated to a soldier who had died during a war. Maternal death is considered a natural phenomenon and encourages the use of herbs, while undermining safe birthing practices with a skilled birth attendant. Women in Uganda are generally more disadvantaged than men. There tends to be a patriarchal order of communities. Therefore, women have a very limited control of resources and in most cases have poorer health. The study found that dosing and toxicity levels need to be monitored in the use of medicinal herbs during labor. There is a need for further field and laboratory research to establish appropriate dosage levels. In 2006, the maternal mortality rate in Uganda is 506 deaths per 100,00 live births. There is a need for health provision programs, safe motherhood programs, health policies in reproductive health care, and collaborative approaches involving traditional medicinal practitioners such as traditional birth attendants. It is possible that a lack of knowledge on plant species used to induce labor and speed up childbirth could be one of the main factors that contribute to high maternal mortality in Uganda.\n\nMalaria is a leading cause of morbidity and mortality in Uganda. It is especially lethal among pregnant women and children under five. The mortality rate for all ages is estimated at 32.1% in 2004. A study in Mukono, Uganda, determined that the most effective delivery system of intermittent preventive treatment (IPTp) for pregnant women was that education was a factor in health seeking behaviors. Those who were a part of the study accessed IPTp early and most of them adhered to the two doses of SP. Women experienced a reduction in malaria episodes, anemia, parasitaemia and low birth weight. While these results cannot be attributed to the intervention alone, after controlling for age, education, parity, and occupation, there were still significant differences for parasitaemia, reported malaria episodes and birth weight, indicating the importance of access and adherence to IPTp.\n\n"}
{"id": "27289540", "url": "https://en.wikipedia.org/wiki?curid=27289540", "title": "Maternal physiological changes in pregnancy", "text": "Maternal physiological changes in pregnancy\n\nMaternal physiological changes in pregnancy are the adaptations during pregnancy that a woman’s body undergoes to accommodate the growing embryo or fetus. These physiologic changes are entirely normal, and include behavioral (brain), cardiovascular (heart and blood vessel), hematologic (blood), metabolic, renal (kidney), posture, and respiratory (breathing) changes. Increases in blood sugar, breathing, and cardiac output are all expected changes that allow a pregnant woman’s body to facilitate the proper growth and development of the embryo or fetus during the pregnancy. The pregnant woman and the placenta also produce many other hormones that have a broad range of effects during the pregnancy.\n\nPregnant women experience numerous adjustments in their endocrine system that help support the developing fetus. The fetal-placental unit secretes steroid hormones and proteins that alter the function of various maternal endocrine glands. Sometimes, the changes in certain hormone levels and their effects on their target organs can lead to gestational diabetes and gestational hypertension.\n\nLevels of progesterone and estrogen rise continually throughout pregnancy, suppressing the hypothalamic axis and subsequently the menstrual cycle. The progesterone is first produced by the corpus luteum and then by the placenta in the second trimester. Women also experience increased human chorionic gonadotropin (β-hCG), which is produced by the placenta.\n\nThe placenta also produces human placental lactogen (hPL), which stimulates maternal lipolysis and fatty acid metabolism. As a result, this conserves blood glucose for use by the fetus. It can also decrease maternal tissue sensitivity to insulin, resulting in gestational diabetes.\n\nThe pituitary gland grows by about one-third as a result of hyperplasia of the lactrotrophs in response to the high plasma estrogen. Prolactin, which is produced by the lactrotrophs increases progressively throughout pregnancy. Prolactin mediates a change in the structure of the breast mammary glands from ductal to lobular-alveolar and stimulates milk production.\n\nFetal skeletal formation and then later lactation challenges the maternal body to maintain their calcium levels. The fetal skeleton requires approximately 30 grams of calcium by the end of pregnancy. The mother's body adapts by increasing parathyroid hormone, leading to an increase in calcium uptake within the gut as well as increased calcium reabsorption by the kidneys. Maternal total serum calcium decreases due to maternal hypoalbuminemia, but the ionized calcium levels are maintained.\n\nTotal cortisol increases to three times of non-pregnant levels by the third trimester. The increased estrogen in pregnancy leads to increase corticosteroid-binding globulin production and in response the adrenal gland produces more cortisol. The net effect is an increase of free cortisol. This contributes to insulin resistance of pregnancy and possibly striae. Despite the increase in cortisol, the pregnant mom does not exhibit Cushing syndrome or symptoms of high cortisol. One theory is that high progesterone levels act as an antagonist to the cortisol.\n\nThe adrenal gland also produces more aldosterone, leading to an eight-fold increase in aldosterone. Women do not show signs of hyperaldosterone, such as hypokalemia, hypernatremia, or high blood pressure.\n\nThe adrenal gland also produces more androgens, such as testosterone, but this is buffered by estrogen's increase in sex-hormone binding globulin (SHBG). SHBG binds avidly to testosterone and to a lesser degree DHEA.\n\nThe thyroid enlarges and may be more easily felt during the first trimester. The increased in kidney clearance during pregnancy causes more iodide to be excreted and causes relative iodine deficiency and as a result an increase in thyroid size. Estrogen-stimulated increase in thyroid-binding globulin (TBG) leads to an increase in total thyroxine (T4), but free thyroxine (T4) and triiodothyronine (T3) remain normal.\n\nA woman's breasts grow during pregnancy, usually 1 to 2 cup sizes and potentially several cup sizes. A woman who wore a C cup bra prior to her pregnancy may need to buy an F cup or larger bra while nursing. A woman's torso also grows and her bra band size may increase one or two sizes. An average of 80% of women wear the wrong bra size, and mothers who are preparing to nurse can benefit from a professional bra fitting from a lactation consultant. Once the baby is born up to about 50–73 hours after birth, the mother will experience her breasts filling with milk (sometimes referred to as “the milk coming in”). Once lactation begins, the woman's breasts swell significantly and can feel achy, lumpy and heavy (which is referred to as engorgement). Her breasts may increase in size again by an additional 1 or 2 cup sizes, but individual breast size may vary depending on how much the infant nurses from each breast. A regular pattern of nursing is generally established after 8–12 weeks, and a woman's breasts will usually reduce in size, but may remain about 1 cup size larger than prior to her pregnancy. Changes in breast size during pregnancy may be related to the sex of the infant, as mothers of female infants have greater changes in breast size than mothers of male infants.\n\nMany people and even medical professionals mistakenly think that breastfeeding causes the breasts to sag (referred to as \"ptosis\"). As a result, some new parents are reluctant to nurse their infants. In February 2009, Cheryl Cole told British Vogue that she hesitated to breastfeed because of the effect it might have on her breasts. \"I want to breastfeed,\" she said, \"but I’ve seen what it can do, so I may have to reconsider.\" In actuality, breastfeeding is not considered to be a major contributor to ptosis of the breasts. In fact, the biggest factors affecting ptosis are cigarette smoking, a woman's body mass index (BMI), her number of pregnancies, her breast cup size before pregnancy, and age.\n\nThe heart adapts to the increased cardiac demand that occurs during pregnancy in many ways. Cardiac output increases throughout early pregnancy, and peaks in the third trimester, usually to 30-50% above baseline. Estrogen mediates this rise in cardiac output by increasing the pre-load and stroke volume, mainly via a higher overall blood volume (which increases by 40–50%). The heart rate increases, but generally not above 100 beats/ minute. Total systematic vascular resistance decreases by 20% secondary to the vasodilatory effect of progesterone. Overall, the systolic and diastolic blood pressure drops 10–15 mm Hg in the first trimester and then returns to baseline in the second half of pregnancy. All of these cardiovascular adaptations can lead to common complaints, such as palpitations, decreased exercise tolerance, and dizziness.\nUterine enlargement beyond 20 weeks' size can compress the inferior vena cava, which can markedly decrease the return of blood into the heart or preload. As a result, healthy pregnancy patients in a supine position or prolonged standing can experience symptoms of hypotension.\n\nDuring pregnancy the plasma volume increases by 40-50% and the red blood cell volume increases only by 20–30%. These changes occur mostly in the second trimester and prior to 32 weeks gestation. Due to dillution, the net result is a decrease in hematocrit or hemoglobin, which are measures of red blood cell concentration. Erythropoietin, which stimulates red blood cell production, increases throughout pregnancy and reaches approximately 150 percent of their pregnancy levels at term. The slight drop in hematocrit or hemoglobin is most pronounced at the end of the second trimester and slowly improves when reaching term.\n\nThe effect of pregnancy on platelet count is unclear, with some studies demonstrating a mild decline in platelet count and other studies that show no effect. The white blood cell count increases with occasional appearance of myelocytes or metamyelocytes in the blood. During labor, there is a rise in leukocyte count.\n\nA pregnant woman will also become hypercoagulable, leading to increased risk for developing blood clots and embolisms, such as deep vein thrombosis and pulmonary embolism. Women are 4-5 times more likely to develop a clot during pregnancy and in the postpartum period than when they are not pregnant. Hypercoagulability in pregnancy likely evolved to protect women from hemorrhage at the time of miscarriage or childbirth. In third world countries, the leading cause of maternal death is still hemorrhage. In the United States 2011-2013, hemorrhage made up of 11.4% and pulmonary embolisms made up of 9.2% of all pregnancy-related deaths.\n\nThe increase risk of clots can be attributed to several things. Plasma levels of pro-coagulantion factors increased markedly in pregnancy, including: von Willebrand Factor, fibrinogen, factor VII, factor VIII, and factor X. Both the production of prostacyclin (an inhibitor of platelet aggregation) and thromboxane (an inducer of platelet aggregation and a vasoconstrictor) are increased, but overall there is an increase in platelet reactivity which can lead to a predisposition to clots. There is also increased blood stasis due to the compression of the vena cava by the enlargening uterus. Many factors have been shown to increase the risk of clots in pregnancy, including baseline thrombophillia, cesarean section, preeclampsia, etc. Clots usually develop in the left leg or the left iliac/ femoral venous system. Recently, there have been several case reports of May-Thurner Syndrome in pregnancy, where the right common iliac artery compresses the below left common iliac vein.\n\nEdema, or swelling, of the feet is common during pregnancy, partly because the enlarging uterus compresses veins and lymphatic drainage from the legs.\n\nDuring pregnancy, both protein metabolism and carbohydrate metabolism are affected. One kilogram of extra protein is deposited, with half going to the fetus and placenta, and another half going to uterine contractile proteins, breast glandular tissue, plasma protein, and haemoglobin.\n\nAn increased requirement for nutrients is given by fetal growth and fat deposition. Changes are caused by steroid hormones, lactogen, and cortisol.\n\nMaternal insulin resistance can lead to gestational diabetes. Increased liver metabolism is also seen, with increased gluconeogenesis to increase maternal glucose levels.\n\n Some degree of weight gain is expected during pregnancy. The enlarging uterus, growing fetus, placenta, amniotic fluid, normal increase in body fat, and increase in water retention all contribute weight gain during pregnancy. The amount of weight gain can vary from to over . In the United States, the range of weight gain that doctors generally recommend is to , less if the woman is overweight, more (up to ) if the woman is underweight.\n\nNutritionally, pregnant women require a caloric increase of 300 kcal/day and an increase in protein to 70 or 75 g/day. There is also an increased folate requirement from 0.4 to 0.8 mg/day (important in preventing neural tube defects). On average, a weight gain of is experienced.\n\nAll patients are advised to take prenatal vitamins to compensate for the increased nutritional requirements. The use of Omega 3 fatty acids supports mental and visual development of infants. Choline supplementation of research mammals supports mental development that lasts throughout life.\n\nProgesterone causes many changes to the genitournary system. A pregnant woman may experience an increase in the size of the kidneys and ureter due to the increase blood volume and vasculature. Later in pregnancy, the woman might develop physiological hydronephrosis and hydroureter, which are normal. Progesterone causes vasodilatation and increased blood flow to the kidneys, and as a result glomerular filtration rate (GFR) commonly increases by 50%, returning to normal around 20 weeks postpartum. The increased GFR increases the excretion of protein, albumin, and glucose. The increased GFR leads to increased urinary output, which the woman may experience as increased urinary frequency. Progesterone also causes decreased motility of the ureters, which can lead to stasis of the urine and hence an increased risk of urinary tract infection.\n\nPregnancy alters the vaginal microbiota with a reduction in species/genus diversity.\nPhysiological hydronephrosis may appear from six weeks.\n\nChanges in the gastrointestinal (GI) system during pregnancy are caused by the enlarging uterus and hormonal changes of pregnancy. Anatomically, the intestine and stomach are pushed up from their original positions by the enlarging uterus. While there aren’t any intrinsic changes in the sizes of the GI organs, the portal vein increases in size due to the hyperdynamic state of pregnancy. Elevated levels of progesterone and estrogen mediate most of the functional changes of the GI system during pregnancy. Progesterone causes smooth muscle relaxation which slows down GI motility and decreases lower esophageal sphincter (LES) tone. The resulting increase in intragastric pressure combined with lower LES tone leads to the gastroesophageal reflux commonly experienced during pregnancy.\n\nThe increased occurrence of gallstones during pregnancy is due to inhibition of gallbladder contraction (as result of increased smooth muscle relaxation mediated by progesterone) and reduced biliary transportation of bile (mediated by estrogen) which results in cholestasis of pregnancy.\n\nNausea and vomiting of pregnancy, commonly known as “morning sickness”, is one of the most common GI symptoms of pregnancy. It begins between the 4 and 8 weeks of pregnancy and usually subsides by 14 to 16 weeks. The exact cause of nausea is not fully understood but it correlates with the rise in the levels of human chorionic gonadotropin, progesterone, and the resulting relaxation of smooth muscle of the stomach. Hyperemesis gravidarum, which is a severe form of nausea and vomiting of pregnancy can lead to nutritional deficiencies, weight loss, electrolytes imbalance and is one of the leading causes of hospitalization in the first trimester of pregnancy.\n\nConstipation is another GI symptom that is commonly encountered during pregnancy. It is associated with the narrowing of the colon as it gets pushed by the growing uterus found adjacent it leading to mechanical blockade. Reduced motility in the entire GI system as well as increased absorption of water during pregnancy are thought to be contributing factors.\n\nDietary cravings and dietary as well as olfactory avoidance of certain types of food are common in pregnancy. Although the exact mechanisms of these symptoms are not fully explained, it is thought that dietary cravings may arise from the thought that certain foods might help relieve nausea. Pica, which is the intense craving for unusual materials such as clay and ice has also been reported in pregnancy.\n\nHemorrhoids and gingival disease are two common pregnancy associated physical findings involving the gastrointestinal system. Hemorrhoids arise as a result of constipation and venous congestion that are common in pregnancy. Gingival disease is thought to be related to gum softening and edema (swelling from fluid collection) that is mostly observed in pregnancy. The mechanism and reason for the gingival changes are poorly understood.\n\nThe fetus inside a pregnant woman may be viewed as an unusually successful allograft, since it genetically differs from the woman. In the same way, many cases of spontaneous abortion may be described in the same way as maternal transplant rejection.\n\nNeuromechanical adaptations to pregnancy refers to the change in gait, postural parameters, as well as sensory feedback, due to the numerous anatomical, physiological, and hormonal changes women experience during pregnancy. Such changes increase their risk for musculoskeletal disorders and fall injuries. Musculoskeletal disorders include lower-back pain, leg cramps, and hip pain. Pregnant women fall at a similar rate (27%) to women over age of 70 years (28%). Most of the falls (64%) occur during the second trimester. Additionally, two-thirds of falls are associated with walking on slippery floors, rushing, or carrying an object. The root causes for these falls are not well known. However, some factors that may contribute to these injuries include deviations from normal posture, balance, and gait.\n\nThe body's posture changes as the pregnancy progresses. The pelvis tilts and the back arches to help keep balance. Poor posture occurs naturally from the stretching of the woman's abdominal muscles as the fetus grows. These muscles are less able to contract and keep the lower back in proper alignment. The pregnant woman has a different pattern of gait. The step lengthens as the pregnancy progresses, due to weight gain and changes in posture. On average, a woman's foot can grow by a half size or more during pregnancy. In addition, the increased body weight of pregnancy, fluid retention, and weight gain lowers the arches of the foot, further adding to the foot's length and width. The influences of increased hormones such as estrogen and relaxin initiate the remodeling of soft tissues, cartilage and ligaments. Certain skeletal joints such as the pubic symphysis and sacroiliac widen or have increased laxity.\n\nThe addition of mass, particularly around the torso, naturally changes a pregnant mother's center of mass (COM). The change in COM requires pregnant mothers to adjust their bodies to maintain balance.\n\nTo positionally compensate the additional load due to the pregnancy, pregnant mothers often extend their lower backs. As the fetal load increases, women tend to arch their lower backs, specifically in the lumbar region of their vertebral column to maintain postural stability and balance. The arching of the lumbar region is known as lumbar lordosis, which recovers the center of mass into a stable position by reducing hip torque. According to a study conducted by Whitcome, et al., lumbar lordosis can increase from an angle of 32 degrees at 0% fetal mass (i.e. non-pregnant women or very early in pregnancy) to 50 degrees at 100% fetal mass (very late in pregnancy). Postpartum, the angle of the lordosis declines and can reach the angle prior to pregnancy. Unfortunately, while lumbar lordosis reduces hip torque, it also exacerbates spinal shearing load, which may be the cause for the common lower back pain experienced by pregnant women.\n\nGiven the demands of fetal loading during pregnancy and the importance of producing offspring to the fitness of human beings, one can imagine that natural selection has had a role in selecting a unique anatomy for the lumbar region in females. It turns out that there are sex differences in the lumbar vertebral column of human males and females, which ultimately helps mitigate some of the discomfort due to the fetal load in females. There are 5 vertebrae in the lumbar region for both males and females. However, the 3 lower vertebrae of a female's lumbar region are dorsally wedged while for males, only the lower 2 of the lumbar region are dorsally wedged. When a female arches her lower back, such as during fetal loading, having an extra dorsally wedged vertebra lessens the shearing force. This lumbar sexual dimorphism in humans suggests high natural selection pressures have been acting to improve maternal performance in posture and locomotion during pregnancy.\n\nIf natural selection has acted on the lumbar region of \"Homo sapiens\" to create this sexual dimorphism, then this sort of trait should also be apparent in the genus \"Australopithecus\", hominins that have been known to be habitually bipedal for at least 2 million years after the earliest bipedal hominins. Currently there are 2 nearly complete australopith lumbar segments; one has three dorsally wedged vertebrae in the lumbar region while the other has two. An explanation for these findings is that the first one is a female, while the latter is a male. This sort of evidence supports the notion that natural selection has played a dimorphic role in designing the anatomy of the vertebral lumbar region.\n\nThe weight added during the progression of pregnancy also affects the ability to maintain balance.\n\nPregnant women have a decreased perception of balance during quiet standing, which is confirmed by an increase in anterior-posterior (front to back) sway. This relationship heightens as pregnancy progresses and significantly decreases postpartum. To compensate for the decrease in balance stability (both actual and perceived), stance width increases to maintain postural stability.\n\nUnder dynamic postural stability, which can be defined as the response to anterior (front) and posterior (back) translation perturbations, the effects of pregnancy are different. Initial sway, total sway, and sway velocity (see figure for description of variables) are significantly less during the third trimester than during the second trimester and when compared to non-pregnant women. These biomechanical characteristics are possible reasons why falls are more prevalent during the second trimester during pregnancy.\n\nAdditionally, the time it takes for pregnant women (any stage of pregnancy) to react to a translational disturbance is not significantly different than that of non-pregnant women. This alludes to some sort of stability mechanism that allow pregnant women to compensate for the changes they experience during pregnancy.\n\nGait in pregnant women often appear as a “waddle” – a forward gait that includes a lateral component. However, research has shown that the forward gait alone remains unchanged during pregnancy. It has been found that gait parameters such as gait kinematics, (velocity, stride length, and cadence) remain unchanged during the third trimester of pregnancy and 1 year after delivery. These parameters suggest that there is no change in forward movement. There is, though, a significant increases in kinetic gait parameters, which may be used to explain how gait motion remains relatively unchanged despite increase in body mass, width and changes in mass distribution about the waist during pregnancy. These kinetic gait parameters suggest an increased use of hip abductor, hip extensor, and ankle plantar flexor muscle groups. To compensate for these gait deviations, pregnant women often make adaptations that can result in musculoskeletal injuries. While the idea of \"waddling\" cannot be dispensed, these results suggest that exercise and conditioning may help relieve these injuries.\n\nThere are many physiologic changes that occur during pregnancy that influence respiratory status and function. Progesterone has noticeable effects on respiratory physiology, increasing minute volume (the amount of air breathed in and out of the lungs in 1 minute) by 40% in the first trimester via an increase in tidal volume alone, as the respiratory rate does not change during pregnancy. As a result, carbon dioxide levels in the blood decrease and the pH of the blood becomes more alkaline (i.e. the pH is higher and more basic). This causes the maternal kidneys to excrete bicarbonate to compensate for this change in pH. The combined effect of the decreased serum concentrations of both carbon dioxide and bicarbonate leads to a slight overall increase in blood pH (to 7.44 compared to 7.40 in the non-pregnant state) . If an arterial blood gas (ABG) specimen is drawn on a pregnant person, it would therefore reveal respiratory alkalosis (from the decrease in serum carbon dioxide mediated by the lungs) with a compensatory metabolic acidosis (from the decrease in serum bicarbonate mediated by the kidneys).\n\nAs the uterus and fetus continue to enlarge over time, the diaphragm progressively becomes more upwardly displaced. This causes less space to be available for lung expansion in the chest cavity, and leads to a decrease in expiratory reserve volume and residual volume. This culminates in a 20% decrease in functional residual capacity (FRC) during the course of the pregnancy.\n\nOxygen consumption increases by 20% to 40% during pregnancy, as the oxygen demand of the growing fetus, placenta, and increased metabolic activity of the maternal organs all increase the pregnant person's overall oxygen requirements. This increase in oxygen consumption paired with the decrease in FRC can potentially mean that pregnant people with pre-existing and/or comorbid asthma, pneumonia, or other respiratory issues may be more prone to disease exacerbation and respiratory decompensation during pregnancy.\n\n"}
{"id": "33153324", "url": "https://en.wikipedia.org/wiki?curid=33153324", "title": "Mathematical Biosciences Institute", "text": "Mathematical Biosciences Institute\n\nThe Mathematical Biosciences Institute (MBI) is an institution of higher learning affiliated with the Ohio State University in Columbus, Ohio. MBI receives major funding from the National Science Foundation. The institute offers a vigorous program of research and education, and fosters the growth of an international community of researchers in the mathematical biosciences.\n\nUnder the leadership of founding director Avner Friedman, MBI opened its doors in September 2002, holding its first workshop, hosting its first visiting researchers, and starting its first cohort of postdocs in that month. MBI holds 10–12 scientific workshops each year, and hosts about 25 postdoctoral and visiting researchers in residence at any given time. Through its collective events and programs, MBI draws over 1000 visits by researchers in the broadly defined area of mathematical biology throughout the year. MBI’s long term planning is overseen by its Directorate and its Board of Trustees, while its scientific activities are overseen by its Directorate and its Scientific Advisory Committee.\n\nThe mission of MBI is: \nTo support this mission, MBI programs are designed to reinforce and build upon existing research efforts in the mathematical biosciences, and to inspire and accelerate the expansion of the community and its intellectual growth. These include emphasis year programs, current topic workshops, education programs, and research projects. The administrative and governance structure of the MBI are designed to support the mission of the Institute.\n\nMBI organizes Emphasis Semesters consisting of three or four week-long workshops. Emphasis Semesters are organized around selected themes which have included Mathematical Neuroscience, Cancer and Its Environment, and Analysis of Complex Data in Biological Systems. Outside of Emphasis Semesters, Current Topic Workshops focus on emerging topics in the mathematical biosciences. Most of the Institute's programs are conducted on The Ohio State University campus, but MBI also sponsors conferences and workshops at its academic Institute Partners. MBI accepts proposals for future programs.\n\nMBI postdoctoral fellows engage in an integrated program of tutorials, working seminars, workshops, and interactions with their mathematical and bioscience mentors. These activities are geared toward providing the tools to pursue an independent research program with an emphasis on collaborative research in the mathematical biosciences.\n\nMBI has a program of support for visitors to spend an extended period of time in residence. During their time at the institute, which can range from a few weeks to many months, visitors can focus on their research while benefiting from participation in MBI workshops and seminars and collaborating with others in the MBI community. Visitors also participate in the Visiting Lecturer Program through which they can share their research.\n\nEarly Career Awards are aimed at non-tenured scientists who have continuing employment and who hold a doctorate in any of the mathematical, statistical and computational sciences, or in any of the biological, medical, and related sciences. Award winners are supported to spend a period of time in residence at MBI.\n\nThe MBI Summer Undergraduate Research Program aims to give outstanding undergraduate students the opprtunity to conduct meaningful research in the mathematical biosciences. MBI works with partner institutions to facilitate an eight-week Research Experience for Undergraduates (REU), supported by the National Science Foundation. Students also participate in a mathematical biology bootcamp at MBI and present their completed research at the Undergraduate Capstone Conference.\n\nMBI co-sponsors a rotating annual summer school with the National Institute for Mathematical and Biological Synthesis (NIMBioS) and Centre for Applied Mathematics in Bioscience and Medicine (CAMBAM). The school brings together graduate students in mathetmatics, biology, and related fields to engage in a focused course of study on a current topic in mathematical biology.\n\nPast programs include the workshop for Women Advancing Mathematical Biology, Workshop for Young Researchers in Mathematical Biology, Blackwell Tapia Conference, and the Science Sundays lecture series.\n\nThe MBI Institute Partner (IP) program promotes the involvement of the international math biosciences community in MBI programs. Institute Partners receive direct benefits and opportunities enabling them to support, guide and participate in MBI research and education programs.\n\n\n"}
{"id": "8593281", "url": "https://en.wikipedia.org/wiki?curid=8593281", "title": "Median alveolar cyst", "text": "Median alveolar cyst\n\nThe median alveolar cyst is a rare cyst, occurring in the bony alveolus between the maxillary central incisors. It is distinguished from a periapical cyst by the fact that adjacent teeth are vital.\n\nTreatment is by enucleation, or surgical removal.\n"}
{"id": "3106121", "url": "https://en.wikipedia.org/wiki?curid=3106121", "title": "Moscow uprising of 1648", "text": "Moscow uprising of 1648\n\nThe Moscow uprising of 1648 (Russian: Соляной бунт, Московское восстание 1648), sometimes known as the salt riot, started because of the government's replacement of different taxes with a universal salt tax for the purpose of replenishing the state treasury after the Time of Troubles. This drove up the price of salt, leading to violent riots in the streets of Moscow. The riot was an early challenge to the reign of Alexei I, eventually resulting in the exile of Alexei's advisor Boris Morozov.\n\nThe taxes fell mostly onto artisans and serfs who were unable to pay the increased price. Furthermore, many townsmen and boyars developed ways to evade taxation, thus placing an even higher burden on those less able to cheat the system. This created resentment among the townspeople, expediting their desire for tax reform. The addition of the salt tax, which increased the price of salt, hit hardest of all because salted fish was an important part of the Russian diet at the time.\n\nA second major complaint came from the poorer landed boyars who wanted to reclaim escaped serfs. Serfs fled their estates due to cruelty from their masters, but more frequently because of bad soil. In the Northern reaches of the kingdom, the ground stayed frozen for most of the year leading to weaker yields when compared to fields on Southern estates. Richer boyars enticed agriculturally minded peasants off of the small estates with the promise of better soil and stronger crops. Boyar livelihood and land holding status depended almost entirely on the productivity of their land. When laborers left, productivity invariably dropped, threatening the landed status of the boyar and leading to discontent among the elite. Before the uprising, a statute of limitations constricted the amount of time boyars had to reclaim \"lost souls.\" The lesser boyars wanted this policy rescinded so that they could reclaim serfs at any point, thus securing their landed status. The riot solidified serfdom in Russia by lifting the repatriation time limit, binding serfs to an estate with more permanence.\n\nBesides taxation, Muscovites were fed up with widespread corruption at the local scale. The worst offender was Levontii Stephanovich Pleshcheyev, the governor of Moscow. In their petition, the people claimed, \"...that from him the taxpaying community suffered heavy taxes and they were groundlessly charged with all sorts of robberies and thefts of his, Levontii's, instruction.\" Among the Tsar's advisors, Boris Morozov, the man who orchestrated the bureaucratization of the government, kindled outrage among the populace. Russians were strongly tied to tradition and feared they would lose their long cherished personal connection to the Tsar. While the Tsar remained ever pure in the eyes of the people, popular opinion held that his advisors exercised an evil influence over him. As the rioters told Alexei I, Morozov and his cronies are turning \"your Tsarist Majesty against the people, and the people against your Tsarist Majesty.\" They resented Morozov for usurping power from the divinely appointed Alexei and for changing the established system.\n\nAll these problems came to a head on 1 June 1648, upon Alexei I's return to Moscow from the Troitse-Sergiyeva Lavra monastery. A crowd of Muscovites surrounded the Tsar and complained about the boyars and prikaz officials. Instead of hearing the petition, the royal bodyguards started dispersing the crowd, pushing them away from the Tsar. This unexpected reaction caused a major outbreak of anger among the people. On 2 June the insurgents burst into the Moscow Kremlin and demanded the surrender of Leontii Pleshcheyev (head of Zemsky Prikaz and Moscow police department), Duma diak Nazar Chistoy (salt tax initiator), boyar Boris Morozov (actual head of government) and his brother-in-law Pyotr Trakhaniotov (head of Cannon Prikaz). Morozov commanded the Streltsy (musketeers) to drive the rioters out of the Kremlin, but they refused. When not acting as the Tsar's bodyguards, the musketeers held artisanal jobs in Moscow. This conflict of interest led them to side with the plight of the townsmen, stating that they, \"...did not want to stand in antagonistic relations with the crowd for the sake of the traitor and tyrant Pleshcheyev.\" The people would not hear the Tsar's heartfelt pleas to spare Pleshceyev and, on June 3, Alexei surrendered the official. In their fervor, the crowd did not wait for Pleshcheyev to be executed instead, \"...they cuggeled him so black and blue and with axes they cut him asunder like a fish, the pieces they let lie naked here and there.\" The rebels set fire to the White City and Kitai-gorod. They burned between 15,000 and 24,000 houses; between 1700 and 2000 people died in the riot. The rioters split into two groups to target the most hated boyars, diaks, okolnichys, and merchants, killing Nazar Chistoy as he begged for mercy. When rumors spread that Morozov's men had started the fires to antagonize the rioters, the boyar head hunt gained greater momentum.\n\nOn 6 June, after receiving a promised salary increase, the Streltsy withdrew from their active role in the riot. On June 11, Alexei managed to convince the people to allow Morozov to be exiled to the Kirillo-Belozersky monastery in Siberia. As the ashes settled, and half of Moscow lay in ruin, the riot gradually dissipated. Soon, however, the provincial nobility, big merchants, and top townsmen seized the initiative and came out with a petition demanding the convocation of the zemsky sobor, or Assembly of the Land, to discuss salary distribution, time limits for recovering escaped serfs, and other legalities. However, the Assembly lacked the voices of the serfs, leading to the institutionalism of serfdom instead of granting them concessions. Upon Morozov's removal, Alexei appointed a new boyar group led by Prince Yakov Cherkassky and boyar Nikita Romanov. They began distributing money, lands and souls to the dvoryane and made a few concessions to the remaining rebels, including the postponement of collection of arrears on 12 June. The government's measures widened the split among the rebels, leading to the arrest and execution of many of the leaders of the uprising on July 3. On 22 October, Boris Morozov secretly returned to Moscow under Alexei's order, and resumed his position as the head of the Russian government, relieving Nikita Romanov of the post. Thus, the immediate outcomes of the riot reversed themselves, and the old order became solidified in an official legal code.\n\nThe uprising in Moscow sparked sporadic riots elsewhere in Russia. Most of these happened in southwestern fortress towns where the population consisted of runaway serfs and people of low birth. They enlisted into state service in order to better their lot within society and feared unfavorable reforms by the government. Changes in military organization and obligation could result in their social regression placing them back into indentured servitude.\nThe most significant outcome of the riot was the Assembly of the Land. Through it, a legal code formed that would be used for centuries to come. Representatives of nearly all social levels codified many of the reforms Alexei's administration had been implementing since the beginning of his reign. Significantly, the Sobornoye Ulozheniye made escape virtually impossible for serfs. In order to spread the ratified laws through the country, Alexei installed the first major printing press in Muscovy, a heretofore unseen invention in Russia.\n"}
{"id": "16462572", "url": "https://en.wikipedia.org/wiki?curid=16462572", "title": "National Health Service", "text": "National Health Service\n\nThe National Health Service (NHS) is the name used for each of the public health services in the United Kingdom – the NHS in England, NHS Scotland, NHS Wales, and the affiliated Health and Social Care (HSC) in Northern Ireland – as well as a commonly used term to describe them collectively. They were established together in 1948 as one of the major social reforms following the Second World War. The founding principles were that services should be comprehensive, universal and free at the point of delivery. Each service provides a comprehensive range of health services, free at the point of use for people ordinarily resident in the United Kingdom, apart from dental treatment and optical care. (The English NHS also requires patients to pay prescription charges with a range of exemptions from these charges.)\n\nDr Somerville Hastings, President of the Socialist Medical Association, successfully proposed a resolution at the 1934 Labour Party Conference that the party should be committed to the establishment of a State Health Service.\n\nConservative MP and Health Minister, Henry Willink, first proposed the National Health Service in 1944 with the publication of a White Paper \"A National Health Service\" which was widely distributed in full and short versions as well as in newsreel by Henry Willink himself. (White Paper – A National Health Service) Henry Willink's National Health Service received cross party support and became Westminster legislation for England and Wales from 1946 and Scotland from 1947, and the Northern Ireland Parliament's 1947 Public Health Services Act. (NHS Wales was split from NHS (England) in 1969 when control was passed to the Secretary of State for Wales before transferring to the Welsh Executive and Assembly under devolution in 1999.)\n\nCalls for a \"unified medical service\" can be dated back to the Minority Report of the Royal Commission on the Poor Law in 1909, but it was following the 1942 Beveridge Report's recommendation to create \"comprehensive health and rehabilitation services for prevention and cure of disease\" that cross-party consensus emerged on introducing a National Health Service of some description. When Clement Attlee's Labour Party won the 1945 election he appointed Aneurin Bevan as Health Minister. Bevan then embarked upon what the official historian of the NHS, Charles Webster, called an \"audacious campaign\" to take charge of the form the NHS finally took. The NHS was born out of the ideal that good healthcare should be available to all, regardless of wealth. Although being freely accessible regardless of wealth maintained Henry Willink's principle of free healthcare for all, Conservative MPs were in favour of maintaining local administration of the NHS through existing arrangements with local authorities fearing that an NHS which owned hospitals on a national scale would lose the personal relationship between doctor and patient. Conservative MPs voted in favour of their amendment to Bevan's Bill to maintain local control and ownership of hospitals and against Bevan's plan for national ownership of all hospitals. The Labour Government defeated Conservative amendments and went ahead with the NHS as it remains today; a single large national organisation (with devolved equivalents) which forces the transfer of ownership from local authority and voluntary hospitals to the new NHS. Bevan's principle of ownership with no private sector involvement has been diluted with future Labour Governments which implemented large scale financing arrangements with private builders in Private Finance Initiatives and joint ventures. (kingsfund, July 2013)\n\nAt its launch by Bevan on 5 July 1948 it had at its heart three core principles: That it meet the needs of everyone, that it be free at the point of delivery, and that it be based on clinical need, not ability to pay.\n\nThree years after the founding of the NHS, Bevan resigned from the Labour government in opposition to the introduction of charges for the provision of dentures and glasses. The following year, Winston Churchill's Conservative government introduced prescription charges. These charges were the first of many controversies over reforms to the NHS throughout its history.\n\nFrom its earliest days, the cultural history of the NHS has shown its place in British society reflected and debated in film, TV, cartoons and literature. The NHS had a prominent slot during the directed by Danny Boyle, being described as \"the institution which more than any other unites our nation\".\n\nEach of the UK's health service systems operates independently, and is politically accountable to the relevant government: the Scottish Government; Welsh Government; Northern Ireland Executive; and the UK Government, responsible for England's NHS. NHS Wales was originally part of the same structure as that of England until powers over the NHS in Wales were firstly transferred to the Secretary of State for Wales in 1969 and thereafter, in 1999, to the Welsh Assembly as part of Welsh devolution. Some functions may be routinely performed by one health service on behalf of another. For example, Northern Ireland has no high-security psychiatric hospitals and depends on hospitals in Great Britain, routinely at Carstairs hospital in Scotland for male patients and Rampton Secure Hospital in England for female patients. Similarly, patients in North Wales use specialist facilities in Manchester and Liverpool which are much closer than facilities in Cardiff, and more routine services at the Countess of Chester Hospital. There have been issues about cross-border payments.\n\nTaken together, the four National Health Services in 2015–16 employed around 1.6 million people with a combined budget of £136.7 billion. In 2014 the total health sector workforce across the UK was 2,165,043. This broke down into 1,789,586 in England, 198,368 in Scotland, 110,292 in Wales and 66,797 in Northern Ireland. In 2017, there were 691,000 nurses registered in the UK, down 1,783 from the previous year. However, this is the first time nursing numbers have fallen since 2008.\n\nAlthough there has been increasing policy divergence between the four National Health Services in the UK, it can be difficult to find evidence of the effect of this on performance since, as Nick Timmins says: \"Some of the key data needed to compare performance – including data on waiting times – is defined and collected differently in the four countries.\" Statistics released in December 2017 showed that, compared with 2012/3, 9% fewer patients in Scotland were waiting more than four hours in accident and emergency, whereas in England the number had increased by 155%.\n\nUK residents are not charged for most medical treatment though NHS dentistry does have standard charges in each of the four national health services in the UK. In addition, most patients in England have to pay charges for prescriptions though some are exempted. \n\nAneurin Bevan in considering the provision of NHS services to overseas visitors wrote, in 1952, that it would be \"unwise as well as mean to withhold the free service from the visitor to Britain. How do we distinguish a visitor from anybody else? Are British citizens to carry means of identification everywhere to prove that they are not visitors? For if the sheep are to be separated from the goats both must be classified. What began as an attempt to keep the Health Service for ourselves would end by being a nuisance to everybody.\" \n\nThe provision of free treatment to non-UK-residents, formerly interpreted liberally, has been increasingly restricted, with new overseas visitor hospital charging regulations introduced in 2015.\n\nCitizens of the EU holding a valid European Health Insurance Card and persons from certain other countries with which the UK has reciprocal arrangements concerning health care can get emergency treatment without charge.\n\nThe NHS is free at the point of use, for general practitioner (GP) and emergency treatment not including admission to hospital, to non-residents. People with the right to medical care in European Economic Area (EEA) nations are also entitled to free treatment by using the European Health Insurance Card. Those from other countries with which the UK has reciprocal arrangements also qualify for free treatment. Since 6 April 2015, non-EEA nationals who are subject to immigration control must have the immigration status of indefinite leave to remain at the time of treatment and be properly settled, to be considered ordinarily resident. People not ordinarily resident in the UK are in general not entitled to free hospital treatment, with some exceptions such as refugees.\n\nPeople not ordinarily resident may be subject to an interview to establish their eligibility, which must be resolved before non-emergency treatment can commence. Patients who do not qualify for free treatment are asked to pay in advance or to sign a written undertaking to pay, except for emergency treatment.\n\nPeople from outside the EEA coming to the UK for a temporary stay of more than six months are required to pay an immigration health surcharge at the time of visa application, and will then be entitled to NHS treatment on the same basis as a resident. This includes overseas students with a visa to study at a recognised institution for 6 months or more, but not visitors on a tourist visa. In 2016 the surcharge was £200 per year, with exemptions and reductions in some cases. It is to increase to £400 in 2018. The discounted rate for students and those on the Youth Mobility Scheme will increase from £150 to £300.\n\nFrom 15 January 2007, anyone who is working outside the UK as a missionary for an organisation with its principal place of business in the UK is fully exempt from NHS charges for services that would normally be provided free of charge to those resident in the UK. This is regardless of whether they derive a salary or wage from the organisation, or receive any type of funding or assistance from the organisation for the purposes of working overseas. This is in recognition of the fact that most missionaries would be unable to afford private health care and those working in developing countries should not effectively be penalised for their contribution to development and other work.\n\nThose who are not ordinarily resident (including British citizens who may have paid National Insurance contributions in the past) are liable to charges for services.\n\nThere are some other categories of people who are exempt from the residence requirements such as specific government workers and those in the armed forces stationed overseas.\n\nSee also Immigration health surcharge.\n\nThe systems are 98.8% funded from general taxation and National Insurance contributions, plus small amounts from patient charges for some services. About 10% of GDP is spent on health and most is spent in the public sector. The money to pay for the NHS comes directly from taxation. The 2008/9 budget roughly equates to a contribution of £1,980 per person in the UK.\n\nWhen the NHS was launched in 1948 it had a budget of £437 million (roughly £9 billion at today’s prices). In 2008/9 it received over 10 times that amount (more than £100 billion). In 1955/6 health spending was 11.2% of the public services budget. In 2015/6 it was 29.7%. This equates to an average rise in spending over the full 60-year period of about 4% a year once inflation has been taken into account. Under the Blair government spending levels increased by around 6% a year on average. Since 2010 spending growth has been constrained to just over 1% a year. Many minor procedures may no longer be available from 2019 and the real reason may be to cut costs.\n\nSome 60% of the NHS budget is used to pay staff. A further 20% pays for drugs and other supplies, with the remaining 20% split between buildings, equipment, training costs, medical equipment, catering and cleaning. Nearly 80% of the total budget is distributed by local trusts in line with the particular health priorities in their areas. Since 2010, there has been a cap of 1% on pay rises for staff continuing in the same role. Unions representing doctors, dentists, nurses and other health professionals have called on the government to end the cap on health service pay, claiming the cap is damaging the health service and damaging patient care. The pay rise is likely to be below the level of inflation and to mean a real-terms pay cut. \nThe House of Commons Library did research showing that real-terms NHS funding per head will fall in 2018–19, and stay the same for two years afterwards. \n\nThere appears to be support for higher taxation to pay for extra spending on the NHS as an opinion poll in 2016 showed that 70% of people were willing to pay an extra penny in the pound in income tax if the money were ringfenced and guaranteed for the NHS. Two thirds of respondents to a King's Fund poll favour increased taxation to help finance the NHS.\n\nThe Guardian has said that GPs face excessive workloads throughout Britain, and that this puts the GP's health and that of their patients at risk. The Royal College of Physicians did a survey of doctors in England, Wales, Scotland and Northern Ireland. Two thirds of doctors surveyed maintained patient safety had deteriorated during the year to 2018, 80% feared they would be unable to provide safe patient care in the coming year while 84% felt increased pressure on the NHS was demoralising the workforce. Jane Dacre said, “We simply cannot go through this [a winter when the NHS is badly overstretched] again. It is not as if the situation was either new or unexpected. As the NHS reaches 70, our patients deserve better. Somehow, we need to move faster towards a better resourced, adequately staffed NHS during 2018 or it will happen again.” At a time when the NHS is short of doctors foreign doctors are forced to leave the UK due to visa restrictions. A study found that a fifth of doctors had faced bullying from seniors in the previous year due to pressure at work.\n\nThe NHS is underresourced compared to health provision in other developed nations. A King’s Fund study of OECD data from 21 nations, revealed that the NHS has among the lowest numbers of doctors, nurses and hospital beds per capita in the western world. Nurses within the NHS maintain that patient care is compromised by the shortage of nurses and the lack of experienced nurses with the necessary qualifications. According to a YouGov poll, 74 percent of the UK public believes there are too few nurses. The NHS performs below average in preventing deaths from cancer, strokes and heart disease. Staff shortages at histology departments are delaying diagnosis and start of treatment for cancer patients. Some cancer patients stop getting follow up treatment when they are still at risk of dying from cancer. Joyce Robins of 'Patient Concern' said, it was “terrifying that cancer patients are being abandoned like this. This is such a life-changing disease and to think that after recovering you’re on your own is very scary. People should be getting the full follow-up they deserve at the time when they are still at high-risk.” According to Breast Cancer Care 72% of NHS trusts across the UK do not provide dedicated specialist nurses for patients with incurable breast cancer. Patients feel abandoned. Samia al Qadhi of Breast Cancer Care stated, \"After this life-changing diagnosis, patients continue to be abandoned without ongoing specialist support they need to manage complex treatment and debilitating side effects, like chronic pain and fatigue.\" \n\nDeath rates for babies at birth and during the month following birth were higher.<ref name=guardian25/6/2018>NHS 'worse than average in treating eight common causes of death' \"The Guardian\"</ref> Infant mortality in England and Wales rose two years running up to 2018. The Royal College of Paediatrics and Child Health claims it is 30% over the median rate for 15 EU nations, together with Australia, Canada and Norway (the EU15+), if the present trend in England and Wales continues the difference will increase.\n\n62% of Intensive Care Units function below normal because there are not enough nurses, a survey of ICU consultants by the Faculty of Intensive Care Medicine (FICM) stated. The survey found the 210 intensive care units throughout the UK were short of 12 nurses each on average and nurses are vital caring for critically ill patients.\n\nPrescriptions for drugs to help patients stop smoking fell by 75% in England by 40% in Scotland and by two thirds in Wales over ten years to 2018. Combining medication with support has been found to help smokers quit most effectively and is three times more effective than leaving smokers to try on their own. The combination is recommended by the National Institute for Health and Care Excellence (Nice). Lack of funding is blamed. \n\nTheresa May is under pressure from MP's of both the main political parties to increase funding for the NHS and for social care, also to consider tax rises to achieve this. 98 signatories to a letter maintain the NHS, public health and social care are “overstretched, poorly integrated and no longer able to keep pace with rising demand and the cost pressures of new drugs and technologies”. Without action, patients will experience a serious further decline in services.” One possibility is a NHS tax where the money would be earmarked for the NHS. 61% of voters favour higher taxes to pay for improvements to the NHS. The NHS is a major concern for voters and consensus for finding more money exists.\n\nAccording to a BMA poll 4 out of 5 doctors think quality and safety of patient care is threatened by underfunding. 3 in 4 doctors polled believe financial targets have higher priority than patient care, doctors maintain more staff and better IT systems could improve their working environment. Chaand Nagpaul of the BMA said, 'We know the NHS has been systematically and scandalously starved of resources for years. It lacks doctors, it lacks nurses, it lacks beds. It’s not just the channel that separates us from our European neighbours, but a vast funding gap equating to 35,000 hospital beds or 10,000 doctors. (...) A health service of gaps and stopgaps where two out of three juniors report holes in their rota and one third of GP practices have long-term vacancies. It’s the new norm. It’s a new low. (...) All this is inevitably affecting patient safety, with bed occupancy in some trusts running up to 100% – well above recommended safe limits of 85%. Is it safe for patients who should be admitted in an emergency to suffer ambulance delays of several hours with some not surviving the wait as reported last winter? Is it safe to work in an understaffed environment of perpetual rota gaps? Is it safe to manage patients in car parks because the hospital has no space, or to treat patients on trolleys in corridors rather than the facilities of a ward? Is it safe for GPs to spend just 10 minutes with patients with four or more complex problems? The prime minister’s belated and desperately needed announcement of increased NHS funding after years of denial is a positive step. But the investment is still well short of what’s needed and we need it now. We will continue to campaign to be at parity with our European neighbours. Meanwhile, it’s crucial that this money is delivered to treat patients and attract and retain staff.' In the worst cases patients waited over 24 hours for an ambulance. A poll by the Royal College of Physicians found the majority of doctors fear their hospital will be unable to provide safe care for patients in the winter 2018/2019.\n\nAmyas Morse of the National Audit Office also maintains spending on the NHS should provide substantially more than has been promised. Morse would like the NHS’s to expand into a “bigger and better” and “fully developed” healthcare provider that would be able to give better care to Britain’s ageing and growing population and the 15 million patients with at least one chronic health problem like diabetes, cancer, heart or lung issues, dementia or depression. The 2018 British Isles heat wave also created a situation where patients are treated in corridors because there is no room for them in wards, patients were sent away from the hospital where they first arrived because that hospital was too busy. Chris Hopson of NHS Providers said, “The increased pressure we’ve seen in many places over the summer is a symptom of the health and care system running at boiling point all year round. The NHS is struggling to cope and that shows just how important it will be to invest the right amount of extra NHS funding in frontline services like A&E capacity”.\n\nThe plan to exit the European Union will affect physicians from EU countries, about 11% of the physician workforce. Many of these physicians are considering leaving the UK if Brexit happens, as they have doubts that they and their families can live in the country. A survey suggests 60% are considering leaving. Record numbers of EU nationals (17,197 EU staff working in the NHS which include nurses and doctors) left in 2016. The figures, put together by NHS Digital, led to calls to reassure European workers over their future in the UK. EU nurses registering to work in the UK are down 96% since the Brexit vote aggravating shortages of nurses. Janet Davies of the Royal College of Nursing, said, “We rely on the contributions of EU staff and this drop in numbers could have severe consequences for patients and their families. Our nursing workforce is in a state of crisis. Across our health service, from A&E to elderly care, this puts patients at serious risk.” 3,962 nurses and midwives from the European Economic Area (EEA) left in 2017 and 2018. With reduced numbers of nurses patient mortality increases, in 2018 there are 40,000 unfilled nursing vacancies just in England EU nurses are badly needed to prevent the nursing situation getting worse.\n\nIn June 2018 the Royal College of Physicians calculated that medical training places need to be increased from 7,500 to 15,000 by 2030 to take account of part-time working among other factors. At that time there were 47,800 consultants working in the UK of which 15,700 were physicians. About 20% of consultants work less than full-time.\n\nSince bursaries for students studying to become nurses have stopped apprenticeships have decreased by over a third in the three years to 2018 leading to fears over how the NHS will be staffed after BREXIT. European Economic Area workers comprise 15% of dentists, 9.1% of doctors and 5.5% of nurses and midwives. There have been efforts to increase the number of British nurses and doctors, however this takes time. Therefore, “continued migration across the NHS is vital to maintain service levels”.\n\nIn England and Scotland cancer wards and children's wards have to close because the hospital cannot attract sufficient qualified doctors and nurses to run the wards safely. Cancer patients and child patients are having to travel very long distances to get treatment and their relatives must travel far to visit the patients. In wards which have not closed staff sometimes work under stress due to staff shortages. Brexit is likely to aggravate these problems.\n\nA study by the King's Fund, Health Foundation, Nuffield Trust and the Institute for Fiscal Studies to mark the NHS 70th anniversary concluded that the main weakness of the NHS was health care outcomes. Mortality for cancer, heart attacks and stroke, was higher than average among comparable countries. The NHS does well at protecting people from heavy financial costs when they are ill. Waiting times are about the same and the management of longterm illness is better than in other comparable countries. Efficiency is good, with low administrative costs and high use of cheaper generic medicines. Twenty-nine hospital trusts and boards out of 157 have not hit any waiting time target in the year 2017-2018.\n\nThere is also concern that a disorderly Brexit may compromise patients' access to vital medicines. Many medical organisations are diverting resources from patient care to managing a possible worst case Brexit scenario. Doctors' and nurses' organisations both say Brexit is bad for the nation's health. Paul Willims said, “Instead of the £350m a week for the NHS we were promised by the Brexiters, we have had cuts and closures as the NHS loses staff and struggles with budgets that are limited by the Brexit economic squeeze. If Brexit actually happens, it seems certain it will only make things worse – with new drug treatments, investment in research and sustainable funding all under threat.”\n\nWithdrawal from the EU could potentially cause a wide range of problems. Radioisotopes for treating cancer patients could be harder to obtain. Skilled medical professional could find it harder to emigrate to the UK. Collaborating with the rest of Europe on medical research could become harder. A separate regulatory system for medicines in the UK could lead to delays of up to two years before UK patients can receive new life saving drugs. In the opinion of the BMA a continued relationship between the UK and the EU is highly desirable. A no-deal Brexit could be catastrophic for patients, health workers and health services and UK health. Among other problems reciprocal arrangements for health care in the UK and the EU would be unclear. A large majority of doctors and nurses believe Brexit will make the NHS worse. Staff shortages concern doctors and nurses, who also fear longer waiting times and funding cuts that Brexit could cause. 85% said that the NHS needs overseas nurses and doctors, and 90% said training UK nationals to replace them would take a long time. A high proportion of NHS trusts have made no preparations for Brexit.\n\nSocial care will cost more in future according to research by Liverpool University, University College London, and others and higher investment are needed. Professor Helen Stokes-Lampard of the Royal College of GPs said, “It’s a great testament to medical research, and the NHS, that we are living longer – but we need to ensure that our patients are living longer with a good quality of life. For this to happen we need a properly funded, properly staffed health and social care sector with general practice, hospitals and social care all working together – and all communicating well with each other, in the best interests of delivering safe care to all our patients.”\n\nPatients needing a wheelchair for less than 6 months are subject to a postcode lottery and frequently do not get one. According to the Red Cross, spending more money on wheelchairs would save the NHS money otherwise patients stay in hospital longer. It also leaves patients isolated, trapped in their homes, unable to get to work. Affected patients include those reovering from an operation, those with broken bones and patients receiving end of life care. Jon Ashworth said, “Restricting access to wheelchairs or mobility aids has proven negative mental and physical impacts on patients’ health, wellbeing and sense of independence. The new health secretary should therefore make every effort to end this unacceptable postcode lottery in provision.” The Red Cross surveyed 139 NHS wheelchair services and 114 said they could not provide short-term wheelchairs. Most public services say they have insufficient funding to supply needed wheelchairs. Some patients who got home without mobility needed expensive home visits and the health of others deteriorated due to lack of mobility. Mike Adamson of the Red Cross \nsaid statutory provision of short-term wheelchairs \"should be a no-brainer. They reduce recovery time, boost independence and would ultimately save money for both the NHS and social care.\"\n\nPatients have to wait excessively long for mental health care. The Royal College of Psychiatrists found some must wait up to thirteen years for the right care. Wendy Burn of the Royal College of Psychiatrists said, “It is a scandal that patients are waiting so long for treatment. The failure to give people with mental illnesses the prompt help they need is ruining their lives.” Even patients who are suicidal or who have attempted suicide are sometimes denied treatment. Patients are told they are not ill enough or waiting lists are too long. During very long waits for treatment one in three patients deteriorate, they may become unemployed or get divorced. One in four patients throughout the UK, wait over three months to see an NHS mental health professional. 6% wait at least a year. The human cost of long waits for treatment are impossible to calculate. The NHS is trying to address long delays for mental health treatment but staff shortages, notably shortages of mental health nurses frustrate this.\n\nThe National Audit Office found mental health provision for children and young people will not meet gowing demand, despite promises of increased funding. Even if promises to provide £1.4bn more for the sector are kept, there will be “significant unmet need” due to staff shortages, inadequate data and failure to control spending by NHS clinical commissioning groups. Currently one-quarter of young people needing mental health services can get NHS help. The Department of Health and Social Care hopes to raise the ratio to 35%. Efforts to improve mental health provision could reveal previously unmet demand. Meg Hillier of the select committee on public accounts said, “The government currently estimates that less than a third of children and young people with a diagnosable mental health condition are receiving treatment. But the government doesn’t understand how many children and young people are in need of treatment or how funding is being spent locally. The government urgently needs to set out how departments, and national and local bodies, are going to work together to achieve its long-term ambition.” Amyas Morse said, “Current targets to improve care are modest and even if met would still mean two-thirds of those who need help are not seen. Rising estimates of demand may indicate that the government is even further away than it thought.”\n\nChildren suspected of having ADHD are subject to a postcode lottery. In some areas diagnosis is prompt. In more areas there is a wait of months or even up to two years while children's school performance and life chances suffer.\n\nOne out of seven NHS hospital operations are cancelled just before they should happen, often due to insufficient beds, staff or operating theatres. Delays cause patients pain and distress when they wait longer than expected for surgery and the NHS is short of the resources it needs to function properly. Research published in the British Journal of Anaesthesia also revealed patients were often refused surgery at a late stage because patients who came to the hospital through A&E were considered in more urgent need. Too few beds in high-dependency and intensive care units cause cancellations. Patients who will need either type of facility after surgery are three times more likely to face cancellation than other groups waiting for surgery. 31% of cancellations were through lack of beds, 12.7% through lack of available operating theatres, 2.3% through equipment difficulties and 2.2% due to staff not being available. In 2017 18647 children's operations were cancelled, cancellations included those for broken bones, breast cancer and acute tonsillitis. This was a 58% increase from 2011/12, when 11,821 operations were cancelled, 117,936 were cancelled during the 8 years to 2018. Dr Dougal Hargreaves of the Royal College of Paediatrics and Child Health said, “We want to see an NHS that is tailored and responsive to the needs of infants, children and young people, ensuring that they get the care they need, when they need it – operations or otherwise. This will be hugely difficult to achieve, however, without significant expansion in the child health workforce.” The largest number of cancelled operations from 2011/12 to 2017/18 was 46,151, at Great Ormond Street hospital NHS foundation trust.\n\nIn 2018, British Prime Minister Theresa May announced that NHS in England would receive a 3.4% increase in funding which would allow it to receive an extra £20bn a year in real terms funding by 2024. There is concern that a high proportion of this money will go to service NHS debts rather than for improved patient care. There are calls for the government to write off the NHS debt. Saffron Cordery of NHS Providers said that hospitals needed help to do their work without being up in deficit, as two-thirds were in the year to 2018. Some expressed doubt over whether May could carry out this proposed increase in funding. The next day, Health Secretary Jeremy Hunt backed the extra £20bn annual increase in NHS funding and responded to criticism by stating that taxation would be used to carry out the funding and that details would be revealed when the next budget is unveiled in November. The Institute for Fiscal Studies has stated a 5% real-terms increase was needed for real change. Paul Johnson of the IFS pointed out the 3.4% was greater than recent increases, but less than the long-term average. Health experts maintain the money will “help stem further decline in the health service, but it’s simply not enough to address the fundamental challenges facing the NHS, or fund essential improvements to services that are flagging.” Inflation may erode the real value of this funding increase.\n\n\n"}
{"id": "8830624", "url": "https://en.wikipedia.org/wiki?curid=8830624", "title": "National minimum dataset", "text": "National minimum dataset\n\nIn health informatics, a national minimum dataset is a database of health encounters held by a central repository.\n\n\"Minimum\" implies that the data fields will be only those required to aggregate information for the purposes of administering the health system in the particular country and for reporting information required as a member country of WHO.\n\n"}
{"id": "37941157", "url": "https://en.wikipedia.org/wiki?curid=37941157", "title": "Nimtala crematorium", "text": "Nimtala crematorium\n\nNimtala crematorium or Nimtala burning ghat or Nimtala ghat is a crematory located at Beadon Street, Kolkata, West Bengal, India.\n\nThe burning ghat came up in 1827. In 2010 the central government of India rejuvenated and upgraded the crematorium which cost . Rabindranath Tagore was cremated here (north Calcutta burning ghat). There is a Rabindranath Tagore Memorial in the crematorium compound which was beautified in the 2010 project.\n\nThe ghat has also represented in popular literature, in the famous Sahitya Akademi Award winning Malayalam novel by K. R. Meera, Aarachaar Nimtala Crematorium is a significant part of the plot.\n\nLegend has it, that atleast one deceased must be cremated there or else all the funeral pyres catch fire and incinerate. It is also believed that water cannot extinguish those pyres, if such an incident ever occurs. \n\n\n"}
{"id": "4909324", "url": "https://en.wikipedia.org/wiki?curid=4909324", "title": "Pelvic pain", "text": "Pelvic pain\n\nPelvic pain is pain in the area of the pelvis. Acute pain is more common than chronic pain. If the pain lasts for more than six months, it is deemed to be chronic pelvic pain. It can affect both women and men.\n\nCommon causes in include: endometriosis in women, bowel adhesions, irritable bowel syndrome, and interstitial cystitis. The cause may also be a number of poorly understood conditions that may represent abnormal psychoneuromuscular function.\n\nUrologic chronic pelvic pain syndrome (UCPPS) is an umbrella term adopted for use in research into pain syndromes associated with the male and female pelvis. It is not intended for use as a clinical diagnosis. The hallmark symptom for inclusion is chronic pain in the pelvis, pelvic floor or external genitalia, although this is often accompanied by lower urinary tract symptoms (LUTS).\n\nChronic pelvic pain in men is referred to as chronic prostatitis/chronic pelvic pain syndrome (CP/CPPS) and is also known as \"chronic nonbacterial prostatitis\". Men in this category have no known infection, but do have extensive pelvic pain lasting more than 3 months.\n\nMany different conditions can cause pelvic pain including:\n\n\n\nThe diagnostic workup begins with a careful history and examination, followed by a pregnancy test. Some women may also need bloodwork or additional imaging studies, and a handful may also benefit from having surgical evaluation.\n\nThe absence of visible pathology in chronic pain syndromes should not form the basis for either seeking psychological explanations or questioning the reality of the patient’s pain. Instead it is essential to approach the complexity of chronic pain from a psychophysiological perspective which recognises the importance of the mind-body interaction. Some of the mechanisms by which the limbic system impacts on pain, and in particular myofascial pain, have been clarified by research findings in neurology and psychophysiology.\n\nIn chronic pelvic pain there are no standard diagnostic tests in males; diagnosis is by exclusion of other disease entities.\n\nChronic pelvic pain (category IIIB) is often misdiagnosed as chronic bacterial prostatitis and needlessly treated with antibiotics exposing the patient to inappropriate antibiotic use and unnecessarily to adverse effects with little if any benefit in most cases. Within a Bulgarian study, where by definition all patients had negative microbiological results, a 65% adverse drug reaction rate was found for patients treated with ciprofloxacin in comparison to a 9% rate for the placebo patients. This was combined with a higher cure rate (69% v 53%) found within the placebo group.\n\nMany women will benefit from a consultation with a physical therapist, a trial of anti-inflammatory medications, hormonal therapy, or even neurological agents.\n\nA hysterectomy is sometimes performed.\n\nSpinal cord stimulation has been explored as a potential treatment option for some time, however there remains to be consensus on where the optimal location of the spinal cord this treatment should be aimed. As the innervation of the pelvic region is from the sacral nerve roots, previous treatments have been aimed at this region; results have been mixed. Spinal cord stimulation aimed at the mid- to high-thoracic region of the spinal cord have produced some positive results.\n\nMultimodal therapy is the most successful treatment option in chronic pelvic pain, and includes physical therapy, myofascial trigger point release, relaxation techniques, α-blockers, and phytotherapy. The UPOINT diagnostic approach suggests that antibiotics are not recommended unless there is clear evidence of infection.\n\nMost women, at some time in their lives, experience pelvic pain. As girls enter puberty, pelvic or abdominal pain becomes a frequent complaint.\nChronic pelvic pain is a common condition with rate of dysmenorrhoea between 16.8—81%, dyspareunia between 8—21.8%, and noncyclical pain between 2.1—24%.\nAccording to the CDC, Chronic pelvic pain (CPP) accounted for approximately 9% of all visits to gynecologists in 2007. In addition, CPP is the reason for 20—30% of all laparoscopies in adults. Pelvic girth pain is frequent during pregnancy.\n\nIn the pursuit of better outcomes for people, problems have been found in current procedures for the treatment of chronic pelvic pain (CPP). These relate primarily with regard to the conceptual dichotomy between an ‘organic’ genesis of pain, where the presence of tissue damage is presumed, and a ‘psychogenic’ origin, where pain occurs despite a lack of damage to tissue. CPP literature in medicine and psychiatry reflects a paradigm where unproblematically observable ‘organic’ processes are causally and sequentially explained, despite evidence in favour of a possible model which accounts for the “complex role played by meaning and consciousness” in the experience of pain. While in the literature of causal mechanisms reference is made to ‘subjective’ aspects of pain, current models do not provide a means through which these aspects may be accessed or understood. Without interpretive or ‘subjective’ approaches to the pain experienced by patients, medical understandings of CPP are fixed within ‘organic’ sequences of the “purely object” body conceptually separated from the patient. Despite the prevalence of this wider understanding of the biological genesis of pain, alternate diagnosis and treatments of CPP in multidisciplinary settings have shown high success rates for people for whom ‘organic’ pathology has been unhelpful.\n\nIn 2007, the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK), part of the United States National Institutes of Health, began using UCPPS as a term to refer to chronic pelvic pain syndromes, mainly interstitial cystitis/bladder pain syndrome (IC/BPS) in women and chronic prostatitis/chronic pelvic pain syndrome (CP/CPPS) in men.\n\n"}
{"id": "3442784", "url": "https://en.wikipedia.org/wiki?curid=3442784", "title": "Pharmacy and Therapeutics", "text": "Pharmacy and Therapeutics\n\nPharmacy and Therapeutics (P&T) is a committee at a hospital or a health insurance plan that decides which drugs will appear on that entity's drug formulary. The committee usually consists of healthcare providers involved in prescribing, dispensing, and administering medications, as well as administrators who evaluate medication use. They must weigh the costs and benefits of each drug and decide which ones a person and the most efficacy per dollar. This is one aspect of pharmaceutical policy. P&T committees utilize an evidence-based approach to drive change within health systems/plans by changing existing policies and bringing up-to-date research to support medical decision-making.\n\nThe role of the P&T committee has evolved significantly over time, starting as a forum for discussion of drug use in hospitals. The 3 basic categories of issues that the committee deals with are: \n\n\nGiven the increasing numbers of specialty medications, P&T committees are expected to make decisions on whether or not these medications, which may not be covered, should be utilized in the clinic. Comparative effectiveness or head to head clinical trials data is critical to making informed decisions for the health system, this responsibility falls on the P&T committee. \n\nThe P&T committee is in charge of deciphering clinical trials and generating drug monographs to ensure that the selected medications are utilized effectively. When considering novel therapies; the committee uses drugs currently on their formularies that treat the same disease state to determine the efficacy of the new therapy. The committee is also involved in addressing and reviewing severity of adverse events with associated agents.\n\nThe pharmacy and therapeutics committee is multidisciplinary and composed of healthcare providers who are involved in the use of medications, and they are responsible for determining the drug formulary of their institution. The committee may include physicians and midlevel providers from a variety of specialties, as well as administrators and quality assurance staff. Members are expected to be capable of conducting study design, evaluating scientific articles, and providing professional insight to support the decision making process. The P&T Committee typically meet on a quarterly basis to discuss new formulary changes, however this could vary between institutions.\n\nWhen new drugs come to market, the pharmacy and therapeutics committee evaluates the drug's clinical and economic benefits and drawbacks to determine whether the drug will be added to the organization's formulary. Most generic drugs are available on formularies, except in cases where that class of drugs are no longer considered safe and efficacious. Medications under formulary restrictions may be used only by specific physicians, patient care area or disease states.\n\n"}
{"id": "44181761", "url": "https://en.wikipedia.org/wiki?curid=44181761", "title": "Primary juvenile glaucoma", "text": "Primary juvenile glaucoma\n\nPrimary juvenile glaucoma is glaucoma that develops due to ocular hypertension and is evident either at birth or within the first few years of life. It is caused due to abnormalities in the anterior chamber angle development that obstruct aqueous outflow in the absence of systemic anomalies or other ocular malformation.\n\nThe typical infant who has congenital glaucoma usually is initially referred to an ophthalmologist because of apparent corneal edema. The commonly described triad of epiphora (excessive tearing), blepharospasm and photophobia may be missed until the corneal edema becomes apparent.\nTwo of the more commonly encountered disorders that may be associated with congenital glaucoma are Aniridia and Sturge-Weber syndrome.\n\nSome pedigrees suggest inherited primary congenital is autosomal dominant but three major autosomal recessive loci have been identified:\n\nThe diagnosis is clinical. The intraocular pressure (IOP) can be measured in the office in a conscious swaddled infant using a Tonopen or hand-held Goldmann tonometer. Usually, the IOP in normal infants is in the range of 11-14 mmHg. Buphthalmos and Haab's striae can often be seen in case of congenital glaucoma.\n\nCorneal cloudiness may have a myriad of causes. Corneal opacity that results from hereditary dystrophies is usually symmetric.\nCorneal enlargement may result from megalocornea, a condition in which the diameter of the cornea is larger than usual and the eye is otherwise normal.\n\nThe preferred treatment of congenital glaucoma is surgical not medical. The initial procedures of choice are goniotomy or trabeculotomy if the cornea is clear, and trabeculectomy ab externo if the cornea is hazy. The success rates are similar for both procedures in patients with clear corneas. Trabeculectomy and shunt procedures should be reserved for those cases in which goniotomy or trabeculotomy has failed. Cyclophotocoagulation is necessary in some intractable cases but should be avoided whenever possible because of its potential adverse\neffects on the lens and the retina.\n\nIn the United States, the incidence of primary congenital glaucoma is about one in 10,000 live births. Worldwide, the incidence ranges from a low of 1:22,000 in Northern Ireland to a high of 1:2,500 in Saudi Arabia and 1:1,250 in Romania. In about two-thirds of cases, it is bilateral. The distribution between males and females varies with geography. In North America and Europe it is more common in boys, whereas in Japan it is more common in girls.\n\n\n\n\n"}
{"id": "1522083", "url": "https://en.wikipedia.org/wiki?curid=1522083", "title": "Professional and Linguistic Assessments Board", "text": "Professional and Linguistic Assessments Board\n\nThe Professional and Linguistic Assessments Board (PLAB) test provides the main route for International Medical Graduates (IMGs) to demonstrate that they have the necessary skills and knowledge to practise medicine in the United Kingdom (UK). PLAB is a two part assessment that overseas doctors (or international medical graduates), from outside the European Economic Area and Switzerland, usually need to pass before they can legally practise medicine in the UK. It is conducted by the General Medical Council of the United Kingdom. The test is designed to assess the depth of knowledge and level of medical and communication skills possessed by the international medical graduates. The PLAB blueprint sets out what candidates are expected to demonstrate in the test and beyond.\n\nThe PLAB test has 2 parts:\n\nPart 1 : Consists of a multiple choice format examination paper with 180 SBA's (One Hundred Eighty Single Best Answer questions) lasting 3 hours, This part is conducted in a number of countries including Bangladesh, Egypt, India, Pakistan, Nigeria and Sri Lanka.\n\nPart 2 : Consists of an objective structured clinical examination (OSCE). This Part is available in the cities of Manchester, Cambridge, and London, United Kingdom. It consists of 18 clinical stations. All the stations are eight minutes long, plus two minutes reading time. The standard of both parts of the PLAB exam is set at the level of competence of a doctor at the start of Foundation Year 2 (F2) in the Foundation Programme.\n\n"}
{"id": "27325000", "url": "https://en.wikipedia.org/wiki?curid=27325000", "title": "Project GABRIEL", "text": "Project GABRIEL\n\nProject GABRIEL refers to an investigation by the United States Atomic Energy Commission to gauge the impact of radioactive fallout resulting from nuclear warfare. It surmised that the radioactive isotope strontium-90 (Sr-90) presented the greatest hazard to life globally. This resulted in the commissioning of Project SUNSHINE, which sought to examine the levels of Sr-90 in human tissues and bones (with a special interest in infants) gathered from around the world.\n\n During the Cold War era, there was an escalation of the atmosphere testing of nuclear weapons. After the atomic bomb in 1945, testing continued and the scale increased with the first hydrogen bomb in 1952. Soon after the United States tested the hydrogen bomb, the USSR followed, in 1953. The mushroom clouds that occurred from the explosions released radioactive isotopes in mass quantities.\nThe first comprehensive study of this problem began in spring 1949 with a one-man project called GABRIEL, conducted by Nicholas M. Smith Jr. at Oak Ridge National Laboratory. Smith produced his first report in 1949. Project GABRIEL was revived in mid-1951 because bombs that were dropped had brought up concerns people had about the dangers of strontium-90. United States Atomic Energy Commission (AEC) was interested by GABRIEL's report though they said it was lacking in hard data and needed independent confirmation of the tests.\nAfter reviewing Project GABRIEL in 1953, it was given first priority status. The secret project would define \"practical limits\" for using atomic weapons. A task team was assembled and the codename used was \"Project HORN.\" In 1954, AEC argued that fallout was harmless because there wasn't enough evidence to prove that fallout would harm humans, animals, or crops. The AEC campaign persuaded the public that the worldwide fallout was harmless. This claim was later disputed when scientists announced publicly that there was no safe level of radiation. This was confirmed in a confidential report by a geneticist for the AEC. By 1954 and the Castle Bravo incident, it was obvious that radioactive fallout was dangerous to humans. United States Atomic Energy Commission Division of Biology and Medicine dealt with efforts directed towards experimental and field studies and the correlation of data dealing with Project GABRIEL.\nThe RAND Corporation, Laboratories at Columbia University, AEC's New York office, the University of Chicago, an exclusive group of scientists, UCLA, and the United States Air Force were all involved in collection and testing of samples from around the world for radioactive fallout.\n\nAfter the hazards of strontium-90 became evident, the next step was to focus on impact and damage per detonation. Smith's tests focused primarily on how many atomic weapons could potentially be detonated before radioactive contamination of air, water and soil became a long-term effect on crops, animals and humans worldwide. In 1949, Smith estimated that it would take 3,000 Hiroshima-sized detonations in a single growing season to see if it have an effect on people who ate crops in affected areas. In 1951, Smith repeated this study with new information from the previous two detonations. With the new information, he then calculated that 10,000 Hiroshima-sized detonations would be needed before the long-term hazards became serious.\nThe testing was done with bones, urine and tissue samples collected worldwide. These samples were all tested for nuclear fallout, yet were falsely studied under the guise of nutritional importance and naturally occurring radon. It was determined that Sr-90 is a \"bone-seeker,\" depositing in bones and marrow after ingestion. Civilian prisoners were considered for certain radiation testing, mainly Utah State prison inmates. One document revealed tests done on the bones of a stillborn baby showed that strontium-90 levels were 36% higher than the average 55% of other stillborn.\n\nProject GABRIEL had opened a wide range of questions about formation, transformation, fallout and biological hazards due to bomb debris. GABRIEL supported work in research projects that might apply to the side effects of nuclear war. It was the sole support of the major research effort of Project SUNSHINE, which tested biological damage from radioactive fallout of Sr-90. By 1954 Project GABRIEL included about 70 investigations supported by the Division of Biology and Medicine. At a summer conference that was hosted by the RAND Corporation the estimate of detonations was revised and increased to 25,000 megatons worth of damage. Project Sunshine was led by radiation physicist Willard Libby on July 21, 1953. Libby realized GABRIEL lacked data in other aspects of fallout, examined carbon-14 and developed radiocarbon dating. The Project GABRIEL report by the AEC was issued in 1954, while the RAND Corporation issued their report on Project SUNSHINE in 1953. Both Project GABRIEL and SUNSHINE played a direct role in the reorganization of the AEC's Division of Biology and Medicine in 1957.\n"}
{"id": "54118834", "url": "https://en.wikipedia.org/wiki?curid=54118834", "title": "RMIT School of Health and Biomedical Sciences", "text": "RMIT School of Health and Biomedical Sciences\n\nThe RMIT School of Health and Biomedical Sciences is an Australian tertiary education school within the College of Science Engineering and Health of RMIT University. It was created in 2016 from the former schools of Health Sciences, Life and Physical Sciences and Medical Sciences.\n\n\n"}
{"id": "23392257", "url": "https://en.wikipedia.org/wiki?curid=23392257", "title": "Rescue: Special Ops", "text": "Rescue: Special Ops\n\nRescue: Special Ops is an Australian television drama series that first screened on the Nine Network in 2009. Filmed in and around Sydney, the program is produced by Southern Star Group with the assistance of Screen Australia and the New South Wales Government.\n\nThe series focuses on a team of experienced professional paramedics who specialise in rescue operations. It premiered on Sunday 2 August 2009, and the season finale of the first season aired on Sunday 25 October. A second season screened from 28 June 2010. The third and final season consisting of 22 episodes screened from 30 May 2011. The Nine Network has confirmed it will not be renewing \"Rescue: Special Ops\" for a fourth season.\n\n\"Rescue: Special Ops\" follows the work of a team of experienced paramedics involved in complex search and rescue operations.\n\nThe job brings them face to face with life and death situations every day but just like anyone else, they juggle life, love and career. Brothers Dean Gallagher and Chase Gallagher are competitive alpha males who are part of the Special Ops team.\n\n\n\n\n\n"}
{"id": "40513485", "url": "https://en.wikipedia.org/wiki?curid=40513485", "title": "Responsible drug use", "text": "Responsible drug use\n\nResponsible drug use maximizes the benefits and reduces the risk of negative impact on the lives of both the user and others. For illegal psychoactive drugs that are not diverted prescription controlled substances, some critics believe that illegal recreational use is inherently irresponsible, due to the unpredictable and unmonitored strength and purity of the drugs and the risks of addiction, infection, and other side effects.\n\nNevertheless, harm-reduction advocates claim that the user can be responsible by employing the same general principles applicable to the use of alcohol: avoiding hazardous situations, excessive doses, and hazardous combinations of drugs; avoiding injection; and not using drugs at the same time as activities that may be unsafe without a sober state. Drug use can be thought of as an activity that can be simultaneously beneficial but risky, similar to driving a car, skiing, skydiving, surfing, or mountain climbing, the risks of which can be minimized by using caution and common sense. These advocates also point out that government action (or inaction) makes responsible drug use more difficult, such as by making drugs of known purity and strength unavailable.\n\nDuncan and Gold argue that to use controlled and other drugs responsibly, a person must adhere to a list of principles. They and others argue that drug users must:\n\nSome proposed ethical guidelines include:\n\nDuncan and Gold suggested that responsible drug use involves responsibility in three areas: situational responsibilities, health responsibilities, and safety-related responsibilities. Among situational responsibilities they included concerns over the possible situations in which drugs might be used legally. This includes the avoidance of hazardous situations; not using when alone; nor using due to coercion or when the use of drugs itself is the sole reason for use. Health responsibilities include: avoidance of excessive doses or hazardous combinations of drugs; awareness of possible health consequences of drug use; avoiding drug-using behaviors than can potentially lead to addiction; and not using a drug recreationally during periods of excessive stress. Safety-related responsibilities include: using the smallest dose necessary to achieve the desired effects; using only in relaxed settings with supportive companions; avoiding the use of drugs by injection; and not using drugs while performing complex tasks or those where the drug might impair one's ability to function safely.\n\nResponsible drug use is emphasized as a primary prevention technique in harm-reduction drug policies. Harm-reduction policies were popularized in the late 1980s although they began in the 1970s counter-culture where users were distributed cartoons explaining responsible drug use and consequences of irresponsible drug use.\n\nDrug use and users are often not considered socially acceptable; they are often marginalized socially and economically.\n\nDrug use may affect work performance; however, drug testing should not be necessary if this is so, as a user's work performance would be observably deficient, and be grounds in itself for dismissal. In the case of discriminate use of amphetamine, similar drugs and some other stimulants, work capacity actually increases, which in itself raises additional ethical considerations.\n\nIllegality causes supply problems, and artificially raises prices. The price of the drug soars far above the production and transportation costs. Purity and potency of many drugs is difficult to assess, as the drugs are illegal. Unscrupulous and unregulated middle men are drawn, by profit, into the industry of these valuable commodities. This directly affects the users ability to obtain and use the drugs safely. Drug dosaging with varying purity is problematic. Drug purchasing is problematic, forcing the user to take avoidable risks. Profit motivation rewards illegal sellers adding a cutting agent to drugs, diluting them; when a user, expecting a low dose, procures \"uncut\" drugs, an overdose can result.\n\nThe morality of buying certain illegal drugs is also questioned given that the trade in cocaine, for instance, has been estimated to cause 20,000 deaths a year in Colombia alone. Increasing Western demand for cocaine causes several hundred thousand people to be displaced from their homes every year, indigenous people are enslaved to produce cocaine and people are killed by the land mines drug cartels place to protect their coca crops. However, the majority of deaths currently caused by the illegal drug trade can only take place in a situation in which the drugs are illegal and some critics blame prohibition of drugs and not their consumption for the violence surrounding them. The illegality of drugs in itself may also cause social and economic consequences for those using them, and legal regulation of drug production and distribution could alleviate these and other dangers of illegal drug use.\n\nHarm reduction as applied to drug use began as a philosophy in the 1980s aiming to minimize HIV transmission between intravenous drug users. It also focused on condom usage to prevent the transmission of HIV through sexual contact.\n\nHarm reduction worked so effectively that researchers and community policy makers adapted the theory to other diseases to which drug users were susceptible, such as Hepatitis C.\n\nHarm reduction seeks to minimize the harms that can occur through the use of various drugs, whether legal (e.g. ethanol (alcohol), caffeine and nicotine), or illegal (e.g. heroin and cocaine). For example, people who inject illicit drugs can minimize harm to both themselves and members of the community through proper injecting technique, using new needles and syringes each time, and through proper disposal of all injecting equipment. Smoking a 700 mg tobacco cigarette or cannabis joint (with the attendant heat shock, carbon monoxide, and combustion toxins) can be avoided by serving individual 25 mg \"single tokes\" in a miniature pipe or using a vaporizer.\n\nOther harm reduction methods have been implemented with drugs such as crack cocaine. In some cities, peer health advocates (Weeks, 2006) have participated in passing out clean crack pipe mouthpiece tips to minimize the risk of Hepatitis A, B and C and HIV due to sharing pipes while lips and mouth contain open sores. Also, a study by Bonkovsky and Mehta reported that, just like shared needles, the sharing of straws used to \"snort\" cocaine can spread blood diseases such as Hepatitis C.\n\nThe responsible user therefore minimizes the spread of blood-borne viruses such as hepatitis C and HIV in the wider community.\n\nThe provision of supervised injection sites, also referred to as safe injection sites, operates under the premise of harm reduction by providing the injection drug user with a clean space and clean materials such as needles, sterile water, alcohol swabs, and other items used for safe injection.\n\nVancouver, British Columbia opened a SiS called Insite in its poorest neighbourhood, the Downtown Eastside. Insite was opened in 2003 and has dramatically reduced many harms associated with injection drug use. The research arm of the site, run by The Centre of Excellence for HIV/AIDS has found that SiS leads to increases in people entering detox and addiction treatment without increasing drug-related crime. As well, it reduces the littering of drug paraphernalia (e.g., used needles) on the street and reduces the number of people injecting in public areas. The program is attracting the highest-risk users, which has led to less needle-sharing in the Downtown Eastside community, and in the 453 overdoses which occurred at the facility, health care staff have saved every person.\n\nIn the Netherlands, where drug use is considered a social and health-related issue and not a law-related one, the government has opened clinics where drug users may consume their substances in a safe, clean environment. Users are given access to clean needles and other paraphernalia, monitored by health officials and are given the ability to seek help from drug addiction.\n\nDue to the project's initial success in reducing mortality ratios and viral spread amongst injection drug users, other projects have been started in Switzerland, Germany, Spain, Australia, Canada and Norway. France, Denmark and Portugal are also considering similar actions.\n\nAs drugs are very prevalent in festival culture more and more consider taking measures for responsible usage there. Some festival organizers have chosen to provide services meant to inform about responsible drug use and testing drugs for the disposal of dangerously laced ones. As a result, some have reported a significant reduction of the workload of festival's medics, welfare team and police officers.\n\n\n\n\n\n"}
{"id": "12343059", "url": "https://en.wikipedia.org/wiki?curid=12343059", "title": "Royan Institute", "text": "Royan Institute\n\nRoyan Institute is an Iranian clinical, research and educational institute dedicated to biomedical, translational and clinical researches, stem cell research and infertility treatment. It is a public non-profitable organization affiliated to Academic Center for Education, Culture and Research. It was established in 1991 by the late Dr. Saeid Kazemi Ashtiani as a research institute for Reproductive Biomedicine and infertility treatments. In 1998 this institute was approved by Ministry of Health as Cell Based Research Center with over 46 scientific members and 186 lab technicians.\n\nRoyan consists of three research institutes, each focused on different fields of research:\n\nThe institute has had close collaborations with other leading Iranian research centers as Institute of Biochemistry and Biophysics (IBB), NRCGEB, and the Hematology-Oncology and Stem Cell Transplantation Research Center at Shariati Hospital in Tehran.\n\nRoyan publishes the Cell Journal with an impact factor of 1.636 (2016).\n\nRoyan Institute for Stem Cell Biology and Technology (RI-SCBT) first, as the \"Department of Stem Cells\" was established in 2002 to advance researches on biology and technology of embryonic stem cells, induced pluripotent stem cells, germ line stem cells, adult stem cells, cancer stem cells, and cord blood stem cells.\n\nThe Institute is providing a comprehensive and coordinated \"bench to bedside\" approach to regenerative medicine, including a greater understanding of fundamental biology of stem cells, developmental biology, tissue engineering programs, the development of translational research of stem cell therapeutics, and administration of new cell therapies approaches that can restore tissue function to patients.\n\nThe RI-SCBT consists of three departments and one center including Department of Stem Cells and Developmental Biology, Department of Biological Engineering, Department of Regenerative Medicine, and Cell Therapy Center.\n\nIran has some of the most liberal laws on stem cell research in the world. The institute founded a Department of Stem Cells in 2002 to establish human embryonic stem cell lines and Induced Pluripotent Stem Cells (iPSC), and to study differentiation into different kinds of cells including cardiomyocytes, beta cells, and neural cells. Researchers claimed a live birth of a cloned sheep in 2006, inviting foreign observers to verify the claim.\n\nRoyan Institute for Reproductive Biomedicine (RI-RB), founded in 1991, consists of six departments and one clinic actively working on different aspects of infertility and the development of new methods for infertility treatment.\n\nIt aims to improve the population’s health through infertility treatments and to research different aspects of infertility and its treatment in order to increase the success rate alongside improving embryo health.\n\n\n\n"}
{"id": "1826989", "url": "https://en.wikipedia.org/wiki?curid=1826989", "title": "Self-healing", "text": "Self-healing\n\nSelf-healing refers to the process of recovery (generally from psychological disturbances, trauma, etc.), motivated by and directed by the patient, guided often only by instinct. Such a process encounters mixed fortunes due to its amateur nature, although self-motivation is a major asset. The value of self-healing lies in its ability to be tailored to the unique experience and requirements of the individual. The process can be helped and accelerated with introspection techniques such as Meditation. \n\nHistorically, communities of Color in the United States and around the World have attempted to use different modes (i.e. scholarship, art, and community gathering) to create self-healing as a way to combat the daily trauma of living in a racialized society.\n\nFrantz Fanon wrote about the subjectivity and objectivity paradox inherent in Blackness in his book Black Skin, White Masks. He describes feeling \"infinite\" but being subjected to the standards reserved for someone who is crippled. At the end of the fifth chapter, he writes that he weeps at the \"crossroads between Nothingness and Infinity.\" His scholarship is meant to provide a lesson to White readers: Black people are also human beings. However, his intention in writing the fifth chapter was no doubt a hope to heal himself in offering up his struggle so that those who mistreated him and his people could reach a place of understanding. In sharing his personal trauma, he offered a way for Black people to connect over shared struggle and commiserate, but also brought his plight to the attentions of a White audience, who, if they could empathize, could lighten the racial load of the Black people around them and could hopefully, someday, ease Fanon's own mental load.\n\nLangston Hughes is an example of how Black people have used art to self-heal from racial trauma. His poem, \"Theme for English B\" details his struggle with completing a writing assignment about truth for a class. He is only able to complete the assignment when he acknowledges the stratified differences between him, his other classmates, and his professor. As he writes, \"I am the only colored student in my class.\" His poem is a way to package his trauma so that he can use it for something constructive in the hopes that it will ultimately heal some of his pain.\n\nHarriet's Apothecary is a NYC-based organization that seeks to create self-healing for communities of color through different healing based events. Their work takes place across the US. They host vendors, and offer reiki-healing, massages, food, and yoga among many other different \"stations\" as a way to combat racial trauma. They do community building work to address poverty (because racism in the United States has left communities of color in disproportionate levels of poverty compared to their White counterparts). To gain entrance to most, if not all of their events, their policy is pay-what-you-can.\n\nSelf-healing is the ultimate phase of Gestalt Therapy.\n\nSelf-healing may refer to automatic, homeostatic processes of the body that are controlled by physiological mechanisms inherent in the organism. Disorders of the spirit and the absence of faith can be self-healed.\n\nIn a figurative sense, self-healing properties can be ascribed to systems or processes, which by nature or design tend to correct any disturbances brought into them. Such as the regeneration of the skin after a cut or scrape, or of an entire limb. The injured party (the living body) repairs the damaged part by itself.\n\nBeyond the innate restorative capacities of the physical body, there are many factors of psychological nature that can influence self-healing. Hippocrates, considered by many to be the father of medical treatment, observed: \"The physician must be ready, not only to do his duty himself, but also to secure the co-operation of the patient, of the attendants and of externals.\"\n— Hippocrates.\n\nSelf-healing may also be achieved through deliberately applied psychological mechanisms. These approaches may improve the psychological and physical conditions of a person. Research confirms that this can be achieved through numerous mechanisms, including relaxation, breathing exercises, fitness exercises, imagery, Meditation, Yoga, qigong, t'ai chi, biofeedback, and various forms of psychotherapy, among other approaches.\n\nVarieties of mechanisms for self-healing have been proposed, including:\n\n\nAnother phrase that often includes self-healing is self-help. In 2013 Kathryn Schulz examined it as \"an $11 billion industry\".\n\nTwelve-step programs support individuals recovering from dysfunctional families and addictive/compulsive behaviors.\n\n"}
{"id": "39348721", "url": "https://en.wikipedia.org/wiki?curid=39348721", "title": "Semiotics of agriculture", "text": "Semiotics of agriculture\n\nSemiotics is the study of signs and meaning-making. According to Daniel Chandler, in his work Semiotics: The Basics, “semiotics involves the study not only of what we refer to as 'signs' in everyday speech, but of anything which 'stands for' something else.” The visual semiotics of agriculture are the observations of the images (namely photographs) surrounding agriculture. This article will look at the symbolic, icon, and indexical modes of visual agricultural images.\n\nEmploying the framework of semiotics to understand the visual semiotics of agriculture, this article discusses the visual signs of agriculture in terms of photographs and images selected from various studies. According to Paul Martin Lester, semiotics helps to identify the symbols used in images to determine their meanings for society as a whole.\n\nImages of agriculture were examined in Sara King’s thesis work in conjunction with Dr. Emily Rhodes, titled Impressions of Agriculture: Using Semiotics to Decode Agricultural Images. Jennifer Norwood observed similar patterns in her work: A semiotic analysis of biotechnology and food safety photographs, and used these to discuses images associated with agriculture.\n\nAgricultural images, according to King and Rhodes, when viewed as semiotic signs of agriculture are viewed on a scale. People with high exposure to agriculture (those with agricultural backgrounds such as FFA, family farms, etc.) tend to associate agricultural images with day-to-day tasks (such as operating farm equipment or getting things ready to plant) and objects such as muddy boots or farm tools. Those with low exposure to agriculture (those who grew up in suburban or urban areas) tend to associate the images of agriculture with agricultural or open-space landscapes and finished agricultural food products in grocery stores. There are also those with a medium exposure to agriculture, who view agricultural images as somewhere in between industrial and open-space settings of agriculture.\n\nIn another study, conducted by Leslie Edgar and Tracy Rutherford titled: A Semiotic Analysis of a Texas Cooperative Extension Marketing Packet, agricultural images of this marketing packet revealed five repeating themes that included diversity, relationships, messages portrayed, and stereotypes.\n\nAccording to a study titled: “The Stuff You Need Out Here”: A Semiotic Case Study Analysis of an Agricultural Company’s Advertisements by Emily B. Rhoades and Tracy Irani, agricultural images used in advertisements and in the media typically follow ideological views held by the American public. These ideologies contain stereotypes of those who live in rural areas and those who are directly involved in agriculture or agricultural production. Many of these stereotypes portray farmers in a negative way.\n\nCharles Peirce identified three different types of signs: symbolic, iconic, and indexical modes of signs. Symbolic modes of signs are signs that do not resemble the signified; they are arbitrary or purely conventional. An example of a symbolic signs would be the alphabet. Iconic modes of signs are those in which the signifier is perceived as resembling the signified. Examples of iconic signs include portraits and scale-models. Indexical signs are those in which the signifier is not arbitrary but is directly related to the signified. Examples of indexical signs include thunder and footprints.\n\nAgricultural images in are typically classified in the iconic and indexical modes of Peirce's modes of signs. Images associated with agricultural typically depict open-space landscapes, farm machinery, farm or ranch workers, agricultural food products, farm clothing, corn, cows, and farm tools.\n\n"}
{"id": "97264", "url": "https://en.wikipedia.org/wiki?curid=97264", "title": "Serket", "text": "Serket\n\nSerket (also known as Serqet, Selket, Selqet, or Selcis) is the goddess of fertility, nature, animals, medicine, magic, and healing venomous stings and bites in Egyptian mythology, originally the deification of the scorpion.\n\nScorpion stings lead to paralysis and Serket's name describes this, as it means \"(she who) tightens the throat\", however, Serket's name also can be read as meaning \"(she who) causes the throat to breathe\", and so, as well as being seen as stinging the unrighteous, Serket was seen as one who could cure scorpion stings and the effects of other venoms such as snakebite.\n\nIn the art of ancient Egypt, Serket was shown as a scorpion (a symbol found on the earliest artifacts of the culture such as from Naqada III) or, as a woman with a scorpion on her head. Although Serket does not appear to have had any temples, she had a sizable number of priests in many communities. \n\nOne of the most dangerous species of scorpion, the Deathstalker (Leiurus quinquestriatus) resides in North Africa, and its sting may kill, so Serket was considered a highly important goddess, and sometimes she was considered by pharaohs to be their patron. Her close association with the early rulers implies that she was their protector, notably Scorpion I and Scorpion II.\n\nAs the protector against venom and snakebite, Serket often was said to protect the deities from Apep, the great snake-demon of evil, sometimes being depicted as the guard when Apep was captured.\n\nAs many of the venomous creatures of Egypt could prove fatal, Serket also was considered a protector of the dead, particularly being associated with venoms and fluids causing stiffening. She was thus said to be the protector of the tents of embalmers, and of the canopic jar associated with venom—the jar of the intestine—which was deified later as Qebehsenuef, one of the four sons of Horus, who were her sons by one of the two Horuses (Heru-pa-khered (Horus the Younger) or Her-ur (Horus the Elder)).\n\nAs the guard of one of the canopic jars and a protector, Serket gained a strong association with Neith, Isis, and Nephthys, who also performed similar functions. Eventually, Serket began to be identified with Isis, sharing imagery and parentage, until finally, Serket became said to be merely an aspect of Isis, whose cult had become very dominant.\n\nIt has been suggested that Serket's identification with a scorpion may be a misinterpretation of the determinative of her name and animal associated with her, and that could refer not to a scorpion, but rather to a waterscorpion (Nepidae). According to this hypothesis, Serket is referred to as \"She who gives breath\" because of the way waterscorpions seem to breathe underwater. The appearance of a waterscorpion must have made it be associated with the scorpion, therefore the use of the goddess for curing scorpion stings and other venomous creatures, or, maybe exactly because she \"causes to breathe\", not for the physical similarities of the creatures.\n"}
{"id": "55361258", "url": "https://en.wikipedia.org/wiki?curid=55361258", "title": "Shrimad Rajchandra Love and Care", "text": "Shrimad Rajchandra Love and Care\n\nShrimad Rajchandra Love and Care (SRLC) is a non-governmental organisation based in Gujarat, India. It was established by Jain spiritual leader Rakesh Jhaveri in 2003. SRLC provides medical, educational and humanitarian services, for universal upliftment.. In addition to India, the organization carries out its service related activities through 50 centres in North America, Europe, Asia, the Middle East and Australia.\n\nShrimad Rajchandra Love and Care has a holistic approach in carrying out its mission. They carry out volunteer based services and programmes under a 10 care programme. \n\nIn 2016, SRLC launched the Shrimad Rajchandra Organ Donation Programme in which Pujya Gurudevshri Rakeshbhai, along with more than 3000 people pledged to donate their organs at an event in Mumbai. In 2017, he was awarded certificates of appreciation in spreading awareness of organ donation in India and across the world by National Organ & Tissue Transplant Organisation and the Zonal Transport Coordination Centre, Mumbai.\n\nShrimad Rajchandra Love and Care has been participating in the Standard Chartered Mumbai Marathon since 2011. It has been the highest fund-raising NGO for seven consecutive years collecting approximately 77.26 lakh in 2011, 96.92 lakh in 2012, 1.38 crore in 2013, 1.5 crore in 2014, 1.6 crore in 2015, 2.56 crore in 2016, 3.83 crore in 2017, 3.46 crore in 2018.\n\nShrimad Rajchandra Hospital is a charitable multi-speciality hospital opened by Pujya Gurudevshri Rakeshbhai in 2004. It is located in Dharampur, Gujarat. The hospital provides medical care at nominal rates and often free for the tribal population of the surrounding areas. There are health awareness camps and mobile programmes initiated by the hospital to educate the rural population. \n\nIt is a 55 bed charitable hospital classified as a First Referral Unit (FRU) in the Valsad district by the Government of Gujarat. with allopathic and homeopathic dispensaries with a 24-hour cardiac ambulance service and a neonatal intensive care unit. The hospital partners with local NGOs such as Bhansali Trust, Jasoda Narottam Public Charity Trust (JNPCT), BAIF, Nandigram Trust, Action Research Community Health (ARCH) to assist the population.\n\nSRH is registered under ADIP scheme of Government of India and conducts camps in remote areas to supply aids and appliances to the physically challenged underprivileged individuals, free of cost.\n\nA new multispecialty 250-bed hospital is planned to meet the increasing needs of the residents of the area. Fundraising for this charitable hospital has been a global initiative managed through the play Yugpurush along with the Mumbai Marathon.\n\n"}
{"id": "53941597", "url": "https://en.wikipedia.org/wiki?curid=53941597", "title": "Surfing in New Zealand", "text": "Surfing in New Zealand\n\nNew Zealand is a popular surfing destination, with a long history of the sport and a varied coastline with locations suitable for all types of surfing. The West coast is notably consistent, with big swells and high winds, whereas the east coast is dominated by cyclone season swells; the North island is notably warmer than the South, but less consistent; mean temperatures range from 7°C to 20°C, depending on location and time of year. Winter is more consistent than Summer, with a southeasterly swell. The climate of New Zealand is varied, so different surf conditions are encountered across the islands.\n\nSurfing was a part of Māori culture before the arrival of European settlers in the 19th century. The practice was called \"whakahekeheke\", and was carried out using a variety of craft, including boards, or \"kopapa\", and even bags of kelp (\"poha\"). The influence of Christian missionaries led to a noted decline in surfing.\n\nIt was later revived following a tour of New Zealand by Hawai'ian surfer Duke Kahanamoku in 1915, when he gave demonstrations to locals. By the 1920s and 1930s, New Zealanders were surfing using solid wooden boards.\n\nSurfing was utilized in the Surf Lifesaving movement, which used heavy hollow longboards to paddle through the surf. Imported magazines in the 1950s contained plans for longboards, which were improved upon to incorporate features such as a rocker and fin. These boards helped attracted younger members to lifesaving.\n\nUp until this point, surfing consisted of riding the wave in a straight line perpendicular to the beach. In 1958, two American lifeguards, Bing Copeland and Rick Stoner, came to stay at Piha Surf Lifesaving Club and introduced the concept of surfing across the face of the wave on a smaller board. Copeland and Stoner also helped locals to make copies of their boards, introducing modern surfing and surfboards to New Zealand. These new surfing techniques put more emphasis on the surf conditions, causing surfers to lease in search of better locations and conditions, resulting in a decline in the number of surfing lifesavers.\n\nIn 1963 the first National Surfing Championships was held at Mt Maunganui, followed by the establishment of the New Zealand Surf Riders' Association. In 1966, New Zealand sent its first representative team to the World Surfing Championships in San Diego. By the late 1960s, more surfboard builders were setting up business, using improved technology which resulted in shorter boards. This allowed for greater speed in executing turns and cutbacks. Conflict between surfers and the New Zealand Surf Lifesaving Association over the safety of surfing in close proximity to swimmers was partially resolved by the introduction of \"surf lanes\" and leg ropes.\n\nSurfing New Zealand is the governing body for the sport of surfing in New Zealand. It was established in 1963, and is involved in the organization of competitions, the development of local training programmes and the education of surfing coaches and judges.\n\nWater sports were popular in Māori culture, and considered important for ensuring children were comfortable in water.\n\nThere are a number of modern surfing clubs and championships which cater to Māori surfers. The \"Auahi Kore Māori Titles\", held since 1992, is a national surfing championship open to Māori contestants in eight categories. The \"Auahi Kore Aotearoa Māori Surfing Team\" compete in the Oceania Surfing Cup, an international surfing championship for indigenous nations of the South Pacific.\n\nThe practice of searching for better surf conditions led to the establishment of certain cities and towns as surfing centres, with a distinctive surfing culture. These include Raglan, Mt Maunganui and Gisborne.\n\nRaglan is particularly known for its consistent surf, and many people visit the small town to learn surfing from one of the local surf schools. Green Wave Raglan is currently the top-rated surf school in the country.\n\nIn 1963 there were approximately 300 surfers in the country, but this number grew to an estimated 15,000 by 1967, and estimated 240,000 today.\n\n\n"}
{"id": "3325923", "url": "https://en.wikipedia.org/wiki?curid=3325923", "title": "Winterbottom's sign", "text": "Winterbottom's sign\n\nWinterbottom's sign is seen in the early phase of African trypanosomiasis, a disease caused by the parasites \"Trypanosoma brucei rhodesiense\" and \"Trypanosoma brucei gambiense\" which is more commonly known as African sleeping sickness. Dr. Anthony Martinelli describes Winterbottom's sign as the swelling of lymph nodes (lymphadenopathy) along the back of the neck, in the posterior cervical chain of lymph nodes, as trypanosomes travel in the lymphatic fluid and cause inflammation. \n\nIt may be suggestive of cerebral infection.\n\nThe term Winterbottom's sign derives from descriptions of the posterior cervical lymphadenopathy associated with African trypanosomiasis made by a slave trader using the sign to weed out the ill.\n\n"}
