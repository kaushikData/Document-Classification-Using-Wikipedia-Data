{"id": "31952575", "url": "https://en.wikipedia.org/wiki?curid=31952575", "title": "Angiolathyrism", "text": "Angiolathyrism\n\nAngiolathyrism-\"Angio\" = Blood vessel, Lathyrism = disease due to Lathyrus Sativus. It is commonly associated with osteolathyrism and neurolathyrism and is caused by over consumption of Lathyrus sativus, also known as grasspea. The main chemical responsible is β-Aminopropionitrile, which prevents collagen cross-linking, thus making the blood vessel, especially the tunica media, weak. This can result in Cystic medial necrosis or a picture similar to Marfan syndrome. The damaged vessels are at an increased risk of dissection.\n"}
{"id": "16266979", "url": "https://en.wikipedia.org/wiki?curid=16266979", "title": "Ashes to Ashes (book)", "text": "Ashes to Ashes (book)\n\nAshes to Ashes: America's Hundred-Year Cigarette War, the Public Health, and the Unabashed Triumph of Philip Morris, written by Richard Kluger and published by Alfred A. Knopf in 1996, won the 1997 Pulitzer Prize for General Non-Fiction. \n"}
{"id": "3939286", "url": "https://en.wikipedia.org/wiki?curid=3939286", "title": "Associate of Science in Nursing", "text": "Associate of Science in Nursing\n\nAn Associate of Science in Nursing (ASN) is a tertiary education nursing degree which typically take 2–3 years to complete. In the United States, this type of degree is usually awarded by community colleges or similar nursing schools. Some four year colleges also offer this degree. Students awarded an Associate of Science in Nursing are qualified to sit for the NCLEX-RN and apply for licensure as a Registered Nurse.\n\nStudents enrolled in an Associate of Science in Nursing program would take courses in nursing, anatomy, physiology, microbiology, chemistry, nutrition, psychology and other social and behavioral sciences. The curriculum will also require supervised clinical experience.\n\nSome hospital-based nursing schools that granted diplomas altered their curriculum to offer associate degrees.\n\n"}
{"id": "41165890", "url": "https://en.wikipedia.org/wiki?curid=41165890", "title": "Australian Army Veterinary Corps", "text": "Australian Army Veterinary Corps\n\nThe Australian Army Veterinary Corps (AAVC) was a corps of the Australian Army which was formed in 1909 to replace the veterinary department of the Commonwealth Military Forces. Following the establishment of a number of permanent artillery batteries and a remount department to supply them with horses, a permanent section of the AAVC was formed in 1911. Responsibilities included veterinary care of horses and the training of farriers and non-commissioned officers in shoeing, horse care and veterinary first aid. During the First World War 120 officers of the AAVC served overseas with the Australian Imperial Force. However, due to the effect of mechanisation there was only a limited role for specialised veterinary services during the Second World War. The corps was disbanded in 1946.\n"}
{"id": "55504612", "url": "https://en.wikipedia.org/wiki?curid=55504612", "title": "Body image law", "text": "Body image law\n\nBody image law is the developing area of law that, according to Dr Marilyn Bromberg of the University of Western Australia Law School and Cindy Halliwell, a law student at Deakin University, \"encompasses the bills, laws and government actions (such as establishing parliamentary inquiries and creating policies) that may help to improve the body image of the general public, and particularly of young people\".<ref name=\"mb/ch\"></ref> Among the reasons for implementing law in this area is to prevent the images of unhealthily thin women causing poor body image which can, along with other factors, lead to an eating disorder.\n\nThe Israeli government passed a body image law in 2012 which became operational the following year. The law requires models to have a minimum body mass index to work and if an image was photoshopped to make the model appear thinner, it must have a warning. The warning must state that the image was modified and it must take up at least seven percent of the image. Breaches can result in a civil lawsuit. \n\nThe French Government passed a similar law in 2015 which came into effect in 2017. This law requires that models provide their employers with a \"medical certificate, valid for up to two years, confirming their general physical well-being and the fact that they are not excessively underweight.\" The BMI of models older than 16 will also be taken into consideration, when determining their overall health. \n\nIn contrast to the Israeli law, breaching it attracts criminal sanctions. Additionally, any photo that has been digitally altered must be labeled as such; failure to label these photos will result in a \"fine of 37,500 euros, or more than $41,000,\" and hiring a model without the verified medical certificate and requirements \"carries a fine of €75,000 and six months in jail.\" The law dictates that digitally altered images must be labeled \"applies only to advertising, not to editorial images in magazines or newspapers.\" \n\nThe Greater London Authority banned advertisements that promote unhealthy body image on Transport for London public transport in 2016.<ref name=\"ap/mo\"></ref> Similarly, Trondheim in Norway banned advertisements that promote unhealthy body image in public places in 2017. \n\nThe Australian Government's position in this area is that it is up to industry to solve the problem of poor body image. Likewise, the previous Labor Government created a non-binding Voluntary Industry Code of Conduct on Body Image.\n"}
{"id": "27218831", "url": "https://en.wikipedia.org/wiki?curid=27218831", "title": "British Safety Council", "text": "British Safety Council\n\nThe British Safety Council, a Registered Charity founded by James Tye in 1957, is one of the world's leading Health and Safety organisations alongside the likes of IOSH & IIRSM unlike these the Council's members are mostly companies. Safety Practitioners the world over use the services and training they provide.The London based charity provides training in over 50 countries.\n\nThe British Safety Council is also a partner in the development of the Occupational Safety & Health Consultants Register scheme (OSHCR), a centrally held register of registered health and safety consultants within the United Kingdom.\n\nThe British Safety Council is a government-regulated awarding body and, as such, must comply with a wide range of conditions set by the regulator so that rigour and consistency in the awarding of qualifications is maintained.\nThey are also the only UK awarding body to offer a complete suite of health and safety qualifications from Entry Level to Level 6 in the national framework.\n"}
{"id": "52551225", "url": "https://en.wikipedia.org/wiki?curid=52551225", "title": "Cannabis in Lithuania", "text": "Cannabis in Lithuania\n\nCannabis in Lithuania is illegal for recreational and medical purposes, but production of industrial hemp is permitted.\n\nIn 2013 Lithuania was the only country in the European Union which banned the cultivation of industrial hemp, and in September that year by a vote of 66–16 the government elected to legalize industrial hemp, pending the president's signature and to take effect at the start of 2014.\n\nIn 2016, protestors planning an April 20th event at Parliament were targeted by the Prosecutor General's Office, which believed that permitting the event might \"violate the public interest\".\n"}
{"id": "8876355", "url": "https://en.wikipedia.org/wiki?curid=8876355", "title": "Delta Dental", "text": "Delta Dental\n\nDelta Dental is the largest dental plan system in the United States. The Delta Dental Plans Association is composed of 39 independent Delta Dental member companies operating in all 50 states, the District of Columbia and Puerto Rico. These member companies provide coverage to 68 million people, enrolled in over 120,000 groups. While many of the Delta Dental member companies and Delta Dental Plans Association (DDPA) are non-profit organizations, a few of the member companies have for-profit segments.\n\nDelta Dental member companies serve more than one third of the estimated 166 million Americans with dental insurance.\n\nDelta Dental's roots go back to 1954 with the formation of dental service corporations in California, Oregon and Washington, which were created by dentists who recognized the need to increase access to oral health care. Led by Washington Dental Service (WDS), Delta Dental began by providing dental benefit programs for organized labor unions. WDS underwent a rebranding campaign in 2013 and now goes by the name Delta Dental of Washington.\n\nDelta Dental Plans Association (DDPA) was created in 1966 to bring together these local state service organizations and coordinate dental benefit programs for customers with employees in multiple states. A year later, the first multi-state program was sold by WDS to the International Association of Machinists. WDS ceded the administration for enrollees in other states to other Delta Dental member companies and contracted with the Blue Cross and Blue Shield Association for administration in those states without a Delta Dental affiliate organization.\n\nCoverage was provided this way until the late 1980s when Delta Dental of California won the bid for the Office of the Civilian Health and Medical Program of the Uniformed Services (OCHAMPUS) program. Delta Dental member companies agreed to share its provider data so the administration of this very large account could be centralized, with Delta Dental of California sharing the administrative income and risk. The OCHAMPUS program led to the creation of the National Provider File (NPF), which was made available for commercial accounts in 1990 via Delta USA – providing Delta Dental coverage to organizations with employees and subscribers located in multiple states.\n\nIf individuals have dental insurance through their employer, the Delta Dental member company in the state of their headquarters usually will handle coverage. Individual Delta Dental insurance is available in several states either through that state's member company or the state or federal health care exchange. Other individual dental insurance options are available through specialty groups such as AARP and military retiree, federal employee and veterans' groups.\n\n"}
{"id": "33395486", "url": "https://en.wikipedia.org/wiki?curid=33395486", "title": "Diagnosis of malaria", "text": "Diagnosis of malaria\n\nThe mainstay of malaria diagnosis has been the microscopic examination of blood, utilizing blood films. Although blood is the sample most frequently used to make a diagnosis, both saliva and urine have been investigated as alternative, less invasive specimens. More recently, modern techniques utilizing antigen tests or polymerase chain reaction have been discovered, though these are not widely implemented in malaria endemic regions. Areas that cannot afford laboratory diagnostic tests often use only a history of subjective fever as the indication to treat for malaria.\n\nThe most economic, preferred, and reliable diagnosis of malaria is microscopic examination of blood films because each of the four major parasite species has distinguishing characteristics. Two sorts of blood film are traditionally used. Thin films are similar to usual blood films and allow species identification because the parasite's appearance is best preserved in this preparation. Thick films allow the microscopist to screen a larger volume of blood and are about eleven times more sensitive than the thin film, so picking up low levels of infection is easier on the thick film, but the appearance of the parasite is much more distorted and therefore distinguishing between the different species can be much more difficult. With the pros and cons of both thick and thin smears taken into consideration, it is imperative to utilize both smears while attempting to make a definitive diagnosis.\n\nFrom the thick film, an experienced microscopist can detect parasite levels (or parasitemia) as few as 5 parasites/µL blood. Diagnosis of species can be difficult because the early trophozoites (\"ring form\") of all four species look similar and it is never possible to diagnose species on the basis of a single ring form; species identification is always based on several trophozoites.\n\nA new system, by www.foldscope.com provides a $1 paper microscope and centrifuge that can be deployed to rural areas in the third world.\n\n\"Plasmodium malariae\" and \"P. knowlesi\" (which is the most common cause of malaria in South-east Asia) look very similar under the microscope. However, \"P. knowlesi\" parasitemia increases very fast and causes more severe disease than \"P. malariae\", so it is important to identify and treat infections quickly. Therefore, modern methods such as PCR (see \"Molecular methods\" below) or monoclonal antibody panels that can distinguish between the two should be used in this part of the world.\n\nFor areas where microscopy is not available, or where laboratory staff are not experienced at malaria diagnosis, there are commercial antigen detection tests that require only a drop of blood. Immunochromatographic tests (also called: Malaria Rapid Diagnostic Tests, Antigen-Capture Assay or \"Dipsticks\") have been developed, distributed and fieldtested. These tests use finger-stick or venous blood, the completed test takes a total of 15–20 minutes, and the results are read visually as the presence or absence of colored stripes on the dipstick, so they are suitable for use in the field. The threshold of detection by these rapid diagnostic tests is in the range of 100 parasites/µl of blood (commercial kits can range from about 0.002% to 0.1% parasitemia) compared to 5 by thick film microscopy. One disadvantage is that dipstick tests are qualitative but not quantitative – they can determine if parasites are present in the blood, but not how many.\n\nThe first rapid diagnostic tests were using \"Plasmodium\" glutamate dehydrogenase as antigen.\nPGluDH was soon replaced by \"Plasmodium\" lactate dehydrogenase (pLDH). Depending on which monoclonal antibodies are used, this type of assay can distinguish between different species of human malaria parasites, because of antigenic differences between their pLDH isoenzymes. Antibody tests can also be directed against other malarial antigens such as the \"P. falciparum\" specific HPR2.\n\nModern rapid diagnostic tests for malaria often include a combination of two antigens such as a \"P. falciparum\". specific antigen e.g. histidine-rich protein II (HRP II) and either a \"P. vivax\" specific antigen e.g. \"P. vivax\" LDH or an antigen sensitive to all \"plasmodium\" species which affect humans e.g. pLDH. It should be noted that such tests do not have a sensitivity of 100% and where possible, microscopic examination of blood films should also be performed.\n\nMolecular methods are available in some clinical laboratories and rapid real-time assays (for example, QT-NASBA based on the polymerase chain reaction) are being developed with the hope of being able to deploy them in endemic areas.\n\nPCR (and other molecular methods) is more accurate than microscopy. However, it is expensive, and requires a specialized laboratory. Moreover, levels of parasitemia are not necessarily correlative with the progression of disease, particularly when the parasite is able to adhere to blood vessel walls. Therefore, more sensitive, low-tech diagnosis tools need to be developed in order to detect low levels of parasitemia in the field.\n\nAnother approach is to detect the iron crystal byproduct of hemoglobin that is found in malaria parasites feasting on red blood cells, but not found in normal blood cells. It can be faster, simpler and precise than any other method. \nResearchers at Rice University have published a preclinical study of their new tech that can detect even a single malaria-infected cell among a million normal cells. They claim it can be operated by nonmedical personal, produce zero false-positive readings, and it doesn’t need a needle or any damage done.\n\nMultiple recent studies have documented malaria overdiagnosis as a persistent issue globally, but especially in African countries. Overdiagnosis results in over-inflation of actual malaria rates reported at the local and national levels. Health facilities tend to over-diagnose malaria in patients presenting with symptoms such as fever, due to traditional perceptions such as \"any fever being equivalent to malaria\" and issues related to laboratory testing (for example high false positivity rates of diagnosis by unqualified personnel ). Malaria overdiagnosis leads to under management of other fever-inducing conditions, over-prescription of antimalarial drugs and exaggerated perception of high malaria endemicity in regions which are no longer endemic for this infection.\n\nAreas that cannot afford laboratory diagnostic tests often use only a history of subjective fever as the indication to treat for malaria. Using Giemsa-stained blood smears from children in Malawi, one study showed that when clinical predictors (rectal temperature, nailbed pallor, and splenomegaly) were used as treatment indications, rather than using only a history of subjective fevers, a correct diagnosis increased from 2% to 41% of cases, and unnecessary treatment for malaria was significantly decreased.\n\nFever and septic shock are commonly misdiagnosed as severe malaria in Africa, leading to a failure to treat other life-threatening illnesses. In malaria-endemic areas, parasitemia does not ensure a diagnosis of severe malaria, because parasitemia can be incidental to other concurrent disease. Recent investigations suggest that malarial retinopathy is better (collective sensitivity of 95% and specificity of 90%) than any other clinical or laboratory feature in distinguishing malarial from non-malarial coma.\n\nQuantitative buffy coat (QBC) is a laboratory test to detect infection with malaria or other blood parasites. The blood is taken in a QBC capillary tube which is coated with acridine orange (a fluorescent dye) and centrifuged; the fluorescing parasites can then be observed under ultraviolet light at the interface between red blood cells and buffy coat. This test is more sensitive than the conventional thick smear, however it is unreliable for the differential diagnosis of species of parasite.\n\nIn cases of extremely low white blood cell count, it may be difficult to perform a manual differential of the various types of white cells, and it may be virtually impossible to obtain an automated differential. In such cases the medical technologist may obtain a buffy coat, from which a blood smear is made. This smear contains a much higher number of white blood cells than whole blood.\n"}
{"id": "57127165", "url": "https://en.wikipedia.org/wiki?curid=57127165", "title": "Disability in Malaysia", "text": "Disability in Malaysia\n\nDisability in Malaysia refers to the people with disability in Malaysia (Malay: Orang Kurang Upaya or OKU). As of 2007, there are 197,519 people in Malaysia with various category of disability. The country is a state party to the United Nations' Convention on the Rights of Persons with Disabilities, having signed on 8 April 2008 and ratified the treaty on 19 July 2010.\n\nThe Department of Social Welfare Malaysia lists disabilities into seven categories, which are hearing, visual, speaking, physical, learning, mental and multiple disabilities.\n\nIn 2008, the Parliament of Malaysia passed the Persons with Disabilities Act (PWDA) to ensure access for disabled people to public facilities, transportation, recreation, leisure and sport services.\n\n\n"}
{"id": "1491529", "url": "https://en.wikipedia.org/wiki?curid=1491529", "title": "Edible protein per unit area of land", "text": "Edible protein per unit area of land\n\nEdible protein per unit area of land is a measure of agricultural productivity. This measure for various major foodstuffs is shown in the chart below. Values are expressed for one calendar year. Biological values and usable protein values have been added to the chart to show the true relative value of each foodstock for human consumption. Usable protein values are determined by the biological value (BV) of each foodstuff and represent the amount of protein that is fully digested by humans, it is calculated as follows:\n\nSoybeans produce at least two times more usable protein per unit area than any other major vegetable or grain crop, except for hemp which can produce up to . They produce 5 to 10 times more protein per unit area than land set aside for grazing animals to make milk, and up to 15 times more protein per unit area than land set aside for meat production.\nSelected averages as computed in the 1970s.\n\n\n"}
{"id": "54429695", "url": "https://en.wikipedia.org/wiki?curid=54429695", "title": "Elisabet Helsing", "text": "Elisabet Helsing\n\nDr Elisabet Helsing DrMedSci (born 1940) is a Norwegian nutritional physiologist.\n\nHelsing attended the University of Oslo, and subsequently wrote on and campaigned for natural breastfeeding.\n\nShe joined the World Health Organization in 1984, working at their regional office for Europe on matters related to nutrition, until 1996.\n\nShe served as president of the 8th European Nutrition Conference; and as president of the Federation of European Nutrition Societies from 1999 to 2003.\n\nShe was a founder of the Norwegian support group for mothers, , whose first meeting was held in her house in 1968, and which inspired the Swedish equivalent. She is an honorary member of both.\n\nShe was given the Mediterranean Diet Foundation's first Grande Covian Award in 1996, and received the Norwegian King's Medal of Merit in Gold, for services to the people in 2003.\n"}
{"id": "52464947", "url": "https://en.wikipedia.org/wiki?curid=52464947", "title": "Ella Mae Romig", "text": "Ella Mae Romig\n\nElla Mae Romig (1871–1936) was the wife of Moravian missionary Joseph Herman Romig. Using her nursing skills, Romig ran the Moravian mission, including clinical services except surgery, in Bethel, Alaska alone and with her husband when he was not at remote missions. \n\nElla Mae Romig was born December 28, 1871 to Albert Fisher and Marietta Struck Ervin of Wilkes-Barre, Pennsylvania. She was their second of their three children. Her family were prominent members of the Moravian church, based in Pennsylvania, including her great-grandfather who fought in the American Revolution. In 1894, Ella graduated from the Hahnemann University Nursing School, where she would eventually work. While working at Hahnemann University, she met Joseph Herman Romig during his senior year. They were married April 30th, 1896.\n\nElla and her family arrived at the Bethel Mission in 1896. At the mission, Ella was responsible for hosting visitors and ensuring that the logistics of the mission ran smoothly. There were frequent visitors to the mission, as it was one of the few, spread out settlements for travelers to stay at and Ella was always an excellent host. Ella took her housekeeping task very seriously, and always ensured that the hospitality and care of the mission was superb. Her husband would frequently leave the mission for long periods of time, so she would take over all aspects of the medicinal practices except major surgeries. She often was the only one at the mission with the medical knowledge necessary to save lives. Ella left Alaska in 1903, but returned June 21, 1904 to develop a church hospital at the Carmel Mission in Western Alaska. She moved to California with her family in the fall of 1905. During her time at the mission, Ella actively went against the practices of the Moravian Church and allowed her children to adopt Yup'ik customs and dress, including changing their diet to a mostly wild game and fish diet. \n\nIn 1936, Ella had a stroke, which ultimately lead to her death on January 1st, 1937. Ella was initiated into the Pioneer Women of Alaska as well as the Daughters of the Revolution.\n\nIn 1997, her journals were published as \"When the Geese Come: The Journals of a Moravian Missionary Ella Mae Ervin Romig, 1898-1905, Southwest Alaska\" by the University of Alaska Press edited by Phylis Movius.\n"}
{"id": "43479167", "url": "https://en.wikipedia.org/wiki?curid=43479167", "title": "First Nations nutrition experiments", "text": "First Nations nutrition experiments\n\nThe First Nations nutrition experiments were a series of experiments run in Canada by Department of Pensions and National Health (now Health Canada) in the 1940s and 1950s. The experiments involved nutrient-poor isolated communities such as those in The Pas and Norway House in northern Manitoba and in residential schools and were designed to discover relative importance and optimum levels of the then-newly discovered vitamins. The deaths connected with the experiments have been described as 'genocide', but this is not (yet) a mainstream view.\n\n"}
{"id": "43622905", "url": "https://en.wikipedia.org/wiki?curid=43622905", "title": "G. William Morgan", "text": "G. William Morgan\n\nG. William Morgan, also known as George William Morgan, health physicist and founding member of the Health Physics Society. Morgan held key health physics positions at Oak Ridge National Laboratory, the Manhattan Project and the Atomic Energy Commission. Morgan was instrumental in developing the regulations that we know today as I0 CFR 20, the Standards for Protection against Radiation.\n\nWhen G. William Morgan died in 1984, his will established a substantial fund for the Health Physics Society. The will required that interest from the fund be utilized to have internationally recognized experts present papers at the Society's meetings.\n\nMichael C. O'Riordan from the United Kingdom's National Radiation Protection Board was the first international expert to receive support from the HPS through the Morgan Fund. O'Riordan presented \"Radon in Albion\" as part of the Indoor Radon Session at the 1989 HPS Annual Meeting in Albuquerque, New Mexico.\n\nMorgan was a Charter member of the Health Physics Society and during the Society's formative years was an active member. He began his health physics career at Oak Ridge National Laboratory as part of the Manhattan Project. He later joined the Atomic Energy Commission and was instrumental in the development of the preliminary regulations that became I0 CFR 20, Standards for Protection against Radiation.\nHe was a great champion of education and helped establish the AEC Health Physics Fellowship Program. After retirement, Morgan became successful in the real estate, but maintained an interest in health physics.\nThe Health Physics Society's Presidents Emeritus Committee has responsibility for the selection of the international expert who will be supported by the G. William Morgan Trust Fund each year. The endowment fund was established in 1984 with $100,000 and during the period 2000-2005 the fund grew to $200,000.\n\n\n\n\n"}
{"id": "19164518", "url": "https://en.wikipedia.org/wiki?curid=19164518", "title": "Health Physics Society", "text": "Health Physics Society\n\nThe Health Physics Society (HPS) is a nonprofit scientific professional organization whose mission is excellence in the science and practice of radiation safety. It is based in the United States and the specific purposes of the society's activities include encouraging research in radiation science, developing standards, and disseminating radiation safety information. Society members are involved in understanding, evaluating, and controlling potential risks from radiation relative to the benefits.\n\nThe Society was formed in 1955, with an organizational meeting in June, 1955 at Ohio State University Columbus, Ohio. As of 2013, the membership consists of approximately 5,500 scientists, physicians, engineers, and other professionals. The headquarters are in McLean, VA. The society is an affiliate of the American Institute of Physics.\n\nIt publishes \"Health Physics\" since 1958, a peer reviewed scientific journal; \"Health Physics News\" for material of interest to members, and \"Operational Radiation Safety\". It operates a public information website, \"Radiation Answers\", and has begun a series of special publications (The first, \"Radiation and Risk: Expert Perspectives\" was published on March 2012.)\n\nIt holds its annual meeting in July, and a mid-year meeting in January or February.\n\nThe society's archives are held at the University of Tennessee, Knoxville.\n\nThe society has 37 geographically based chapters, all in the United States, except for one chapter in the Georgian Republic and one in Taiwan; there are 8 Sections on special interests.\n\n\nThe First Annual Meeting was held in 1956 at the University of Michigan. \nThe 57th Annual Meeting of the society was held at Sacramento, California on July 22–26, 2012. \nThe 58th was held July 7–11, 2013, in Madison, Wisconsin; the 59th was held in Baltimore in July 2014 and the 60th in Indianapolis in 2015.\n\nThe 47th, \"Nuclear Power Radiation Safety: Learning from the Past to Protect the Future\" will be held 9–12 February 2014, Baton Rouge, Louisiana.\n\nIn the midyear issue of Science the announcement came of the formation new national scientific organization for health physicists at a 3-day Health Physics Conference at Ohio State University in Columbus, Ohio on 14 June 1955. The organization was temporarily named \"Health Physics Society\", and Karl Z. Morgan of the Health Physics Division of Oak Ridge National Laboratory was elected interim president. Other interim officers were: \n\nTalks regarding the formation of a professional society had been ongoing for several years. The health physicists had decided to form an independent organization rather than attach to an existing group.<br>\nDirectors of the Health Physics Society included: \n\nWilliam Edward Nolan, Jr. was born on 12 July 1914 in Boston, Massachusetts and died on 27 September 1985 in Livermore, California at the age of 71. Nolan and his wife Vivian are buried at Saint Michael’s Cemetery, Livermore, Alameda County, California. His father was William Edward Nolan and worked on the railroad. His mother’s name was Mary Handrahan. Nolan had five brothers and sisters.\n\nNolan met his wife while she was undergoing a physical examination for entry into the United States Navy Nurse Corps. Nolan took the x-ray as part of her physical exam. Nolan married Dorothy Vivian on 2 August 1945. Nolan served in the US Navy during World War II and achieved the rank of Pharmacists Mate 2nd Class, PhM2. Dorothy served as an Lieutenant (junior grade) LTJG officer in the US Navy Nurse Corps during World War II. In 1956 Nolan first moved to El Cerrito, California and finally settled in Livermore, California and worked in the Donner Lab, Lawrence Berkeley National Laboratory at the University of California, Berkeley, California.\n\nIn 1945 Nolan started work at Lawrence Berkeley National Laboratory (LBL) as a laboratory technician. For a time he worked at Lawrence Livermore National Laboratory and returned to LBL in 1970 and worked with Luis Walter Alvarez in Group A as the group program administrator. The work with Alvarez was productive and several patents were awarded for projects from optical lenses and systems to particle detectors. Nolan teamed with H. Wade Patterson to conduct a survey of dental facilities in the San Francisco, California area. A portable ionization chamber and DuPont type 552 and type 558 film-based radiation monitoring devices were utilized to collect the data.\n\nNolan also worked at Site 300 at Livermore. Nolan was involved with the tests at Bikini Atoll and Johnston Atoll in the Pacific Ocean. In 1970, Louis Alvarez completed a study of the pyramids at Giza near Cairo, Egypt. Alvarez published a report titled: \"Search for Hidden Chambers in the Pyramids\" and credited Nolan for his assistance with the project.\n\nNolan retired from LBL after 33 years of service in 1977. Nolan died on 27 September 1985 from sprue known today as Celiac disease.\n\n\n\n"}
{"id": "56079139", "url": "https://en.wikipedia.org/wiki?curid=56079139", "title": "Health in Luxembourg", "text": "Health in Luxembourg\n\nA new measure of expected human capital calculated for 195 countries from 1990 to 2016 and defined for each birth cohort as the expected years lived from age 20 to 64 years and adjusted for educational attainment, learning or education quality, and functional health status was published by the Lancet in September 2018. Luxembourg had the eighth highest level of expected human capital with 25 health, education, and learning-adjusted expected years lived between age 20 and 64 years. \n\nHealthcare in Luxembourg\n"}
{"id": "71088", "url": "https://en.wikipedia.org/wiki?curid=71088", "title": "Hibernation", "text": "Hibernation\n\nHibernation is a state of inactivity and metabolic depression in endotherms. Hibernation refers to a season of heterothermy characterized by low body temperature, slow breathing and heart rate, and low metabolic rate. It is most commonly observed during the winter months. Although traditionally reserved for \"deep\" hibernators such as rodents, the term has been redefined to include animals such as bears and is now applied based on active metabolic suppression rather than any absolute decline in body temperature. Many experts believe that the processes of daily torpor and hibernation form a continuum and utilize similar mechanisms. The equivalent during the summer months is aestivation.\n\nOften associated with low temperatures, the function of hibernation is to conserve energy when sufficient food is unavailable. To achieve this energy saving, an endothermic animal decreases its metabolic rate and thereby its body temperature. Hibernation may last days, weeks, or months depending on the species, ambient temperature, time of year, and the individual's body condition. Before entering hibernation, animals need to store enough energy to last through the duration of their dormant period, possibly as long as the entire winter. Larger species become hyperphagic, eating a large amount of food and storing the energy in fat deposits. In many small species, food caching replaces eating and becoming fat.\n\nSome species of mammals hibernate while gestating young, which are born either while the mother hibernates or shortly afterwards. For example, female polar bears go into hibernation during the cold winter months in order to give birth to their offspring. The pregnant mothers significantly increase their body mass prior to hibernation, and this increase is further reflected in the weight of the offspring. The fat accumulation enables them to provide a sufficiently warm and nurturing environment for their newborns. During hibernation, they subsequently lose 15–27% of their pre-hibernation weight by using their stored fats for energy.\n\nTrue hibernation is restricted to endotherms; ectotherms by definition cannot hibernate because they cannot actively down-regulate their body temperature or their metabolic rate. Still, many ectothermic animals undergo periods of dormancy which are sometimes confused with hibernation. Some reptile species are said to brumate, but possible similarities between brumation and hibernation are not firmly established. Many insects, such as the wasp \"Polistes exclamans\", exhibit periods of dormancy which have often been referred to as hibernation, despite their ectothermy.\n\nObligate hibernators are animals that spontaneously, and annually, enter hibernation regardless of ambient temperature and access to food. Obligate hibernators include many species of ground squirrels, other rodents, mouse lemurs, European hedgehogs and other insectivores, monotremes, marsupials, and even butterflies such as the small tortoiseshell. These species undergo what has been traditionally called \"hibernation\": a physiological state wherein the body temperature drops to near ambient (environmental) temperature, and heart and respiration rates slow drastically.\n\nThe typical winter season for obligate hibernators is characterized by periods of torpor interrupted by periodic, euthermic arousals, during which body temperatures and heart rates are restored to more typical levels. The cause and purpose of these arousals is still not clear; the question of why hibernators may return periodically to normal body temperatures has plagued researchers for decades, and while there is still no clear-cut explanation, there are multiple hypotheses on the topic. One favored hypothesis is that hibernators build a \"sleep debt\" during hibernation, and so must occasionally warm up to sleep. This has been supported by evidence in the Arctic ground squirrel. Other theories postulate that brief periods of high body temperature during hibernation allow the animal to restore its available energy sources or to initiate an immune response.\n\nHibernating Arctic ground squirrels may exhibit abdominal temperatures as low as , maintaining sub-zero abdominal temperatures for more than three weeks at a time, although the temperatures at the head and neck remain at or above.\n\nHistorically there was a question of whether or not bears truly hibernate since they experience only a modest decline in body temperature (3–5 °C) compared with the much larger decreases (often 32 °C or more) seen in other hibernators. Many researchers thought that their deep sleep was not comparable with true, deep hibernation, but recent research has refuted this theory in captive black bears.\n\nUnlike obligate hibernators, facultative hibernators only enter hibernation when either cold-stressed, food-deprived, or both. A good example of the differences between these two types of hibernation can be seen in prairie dogs: the white-tailed prairie dog is an obligate hibernator and the closely related black-tailed prairie dog is a facultative hibernator.\n\nWhile hibernation has long been studied in rodents, namely ground squirrels, no primate or tropical mammal was known to hibernate until the discovery of hibernation in the fat-tailed dwarf lemur of Madagascar, which hibernates in tree holes for seven months of the year. Malagasy winter temperatures sometimes rise to over , so hibernation is not exclusively an adaptation to low ambient temperatures. The hibernation of this lemur is strongly dependent on the thermal behaviour of its tree hole: if the hole is poorly insulated, the lemur's body temperature fluctuates widely, passively following the ambient temperature; if well insulated, the body temperature stays fairly constant and the animal undergoes regular spells of arousal. Dausmann found that hypometabolism in hibernating animals is not necessarily coupled with low body temperature.\n\nHibernating bears are able to recycle their proteins and urine, allowing them both to stop urinating for months and to avoid muscle atrophy.\n\nHistorically, Pliny the Elder believed swallows hibernated, and ornithologist Gilbert White documented anecdotal evidence in his 1789 book \"The Natural History of Selborne\" that indicated the belief was still current in his time. It is now understood that the vast majority of bird species typically do not hibernate, instead utilizing torpor. One known exception is the common poorwill (\"Phalaenoptilus nuttallii\"), for which hibernation was first documented by Edmund Jaeger.\n\nBecause they cannot actively down-regulate their body temperature or metabolic rate, ectothermic animals (including fish, reptiles, and amphibians) cannot hibernate. However, they can experience decreased metabolic rates associated with colder environments and/or low oxygen availability (hypoxia) and exhibit dormancy. It was once thought that basking sharks settled to the floor of the North Sea and became dormant, but research by Dr. David Sims in 2003 dispelled this hypothesis, showing that the sharks actively traveled huge distances throughout the seasons, tracking the areas with the highest quantity of plankton. Epaulette sharks have been documented to be able to survive for long periods of time without oxygen and at temperatures of up to as a means to survive in their shoreline habitat, where water and oxygen levels vary with the tide. Other animals able to survive long periods with no or very little oxygen include goldfish, red-eared sliders, wood frogs, and bar-headed geese. However, the ability to survive hypoxic or anoxic conditions is not the same as, nor closely related, to endotherm hibernation.\n\nHibernation induction trigger (HIT) is somewhat of a misnomer. Although research in the 1990s hinted at the ability to induce torpor in animals by injection of blood taken from a hibernating animal, further research has been unable to reproduce this phenomenon. Despite the inability to induce torpor, there are substances in the blood of hibernators that can lend protection to organs for possible transplant. Researchers were able to prolong the life of an isolated pig's heart with a HIT. This may have potentially important implications for organ transplant, as it could allow organs to survive for up to 18 hours outside the human body. This would be a great improvement from the current 6 hours.\n\nThe supposed HIT is a mixture derived from blood serum, including at least one opioid-like substance. DADLE is an opioid that in some experiments has been shown to have similar functional properties.\n\nResearchers have studied how to induce hibernation in humans. The ability to hibernate would be useful for a number of reasons, such as saving the lives of seriously ill or injured people by temporarily putting them in a state of hibernation until treatment can be given.\n\n\n\n"}
{"id": "1555251", "url": "https://en.wikipedia.org/wiki?curid=1555251", "title": "History of salt", "text": "History of salt\n\nSalt, also referred to as table salt or by its chemical formula NaCl, is an ionic compound made of sodium and chloride ions. All life has evolved to depend on its chemical properties to survive. It has been used by humans for thousands of years, from food preservation to seasoning. Salt's ability to preserve food was a founding contributor to the development of civilization. It helped to eliminate dependence on seasonal availability of food, and made it possible to transport food over large distances. However, salt was often difficult to obtain, so it was a highly valued trade item, and was considered a form of currency by certain peoples. Many salt roads, such as the via Salaria in Italy, had been established by the Bronze age.\n\nAll through history, availability of salt has been pivotal to civilization. In Britain, the suffix \"-wich\" in a placename means it was once a source of salt, as in Sandwich and Norwich. The Natron Valley was a key region that supported the Egyptian Empire to its north, because it supplied it with a kind of salt that came to be called by its name, natron. Today, salt is almost universally accessible, relatively cheap, and often iodized. There have been reports as to the value of salt in historical times, however it has never been more valuable than gold.\n\nSalt comes from two main sources: sea water and the sodium chloride mineral halite (also known as rock salt). Rock salt occurs in vast beds of sedimentary evaporite minerals that result from the drying up of enclosed lakes, playas, and seas. Salt beds may be up to 350 m thick and underlie broad areas. In the United States and Canada extensive underground beds extend from the Appalachian basin of western New York through parts of Ontario and under much of the Michigan basin. Other deposits are in Texas, Ohio, Kansas, New Mexico, Nova Scotia, and Saskatchewan. In the United Kingdom underground beds are found in Cheshire and around Droitwich. Salzburg, Austria, was named \"the city of salt\" for its mines. High-quality rock salt was cut in medieval Transylvania, Maragmureş and Southern Poland (Wieliczka). Tuzla in Bosnia and Herzegovina was named in Hungarian Só (salt) from the twelfth century on and later \"place of salt\" by Turks.\n\nSalt is extracted from underground beds either by mining or by solution mining using water to dissolve the salt. In solution mining the salt reaches the surface as brine, from which the water is evaporated leaving salt crystals.\n\nSolnitsata, the earliest known town in Europe, was built around a salt production facility. Located in present-day Bulgaria, archaeologists believe the town accumulated wealth by supplying salt throughout the Balkans.\n\nSalt was of high value to the Jews, Greeks, Tamils the Chinese, Hittites and other peoples of antiquity. Aside from being a contributing factor in the development of civilization, salt was also used in the military practice of salting the earth by various peoples, beginning with the Assyrians.\nIn the early years of the Roman Republic, with the growth of the city of Rome, roads were built to make transportation of salt to the capital city easier. An example was the Via Salaria (originally a Sabine trail), leading from Rome to the Adriatic Sea. The Adriatic, having a higher salinity due to its shallow depth, had more productive solar ponds compared with those of the Tyrrhenian Sea, much closer to Rome. The word \"salary\" comes from the Latin word for salt. The reason for this is unknown; a persistent modern claim that the Roman Legions were sometimes paid in salt is baseless.\nDuring the late Roman Empire and throughout the Middle Ages salt was a precious commodity carried along the salt roads into the heartland of the Germanic tribes. Caravans consisting of as many as forty thousand camels traversed four hundred miles of the Sahara bearing salt to inland markets in the Sahel, sometimes trading salt for slaves: Timbuktu was a huge salt and slave market.\n\nSalt in Chinese history was both a driver of technological development and a stable source of revenue for the imperial government.\n\nIn the Old Testament, Mosaic law calls for salt to be added to all burnt animal sacrifices (Lev. 2:13) and compares the priestly covenant between God and the kohen patrilineal descendants of Ahron to salt.\n\nThe Book of Ezra (550 BC to 450 BC) associated accepting salt from a person with being in that person's service. In Ezra 4:14, the adversaries of Ezra and company, in their letter of complaint to Artaxerxes I of Persia explain their loyalty to the King. When translated, it is either stated literally as \"because we have eaten the salt of the palace\" or more figuratively as \"because we have maintenance from the king\".\n\nSalt is used as a metaphor in the Bible. In the New Testament, Matthew 5:13, Jesus said, \"You are the salt of the earth\". He added that if the salt loses its flavor, it is good for nothing but to be trampled. In addition, the preservative quality of salt is in view here to show how the disciples were called to preserve the society and the world around them from moral decay. On another occasion, according to the Gospels, Jesus commanded his followers to \"...have salt within them.\"\n\nIn Luke 14:34-35 Jesus concludes a series of parables on the cost of following him with the parable of spent salt. It seems that those who follow him are to be like the salt. From this we learn that those who follow him should expect to be spent, as chunks of salt after much use. Furthermore, they should prepare to be useful until the end, for the long haul. In this parable, it is good to be used as salt and bad to become useless salt. This illustration ties in with the two preceding ones (Luke 14:28-33) of counting the cost: the disciples must prepare, by counting the cost, to be salty for as long as they are needed.\n\nSalt has played a prominent role in determining the power and location of the world's great cities. Liverpool rose from just a small English port to become the prime exporting port for the salt dug in the great Cheshire salt mines and thus became the entrepôt for much of the world's salt in the 19th century.\n\nSalt created and destroyed empires. The salt mines of Poland led to a vast kingdom in the 16th century, only to be demolished when Germans brought in sea salt (which most of the world considered superior to rock salt). Venice fought and won a war with Genoa over salt. However, Genoese Christopher Columbus and Giovanni Caboto would later destroy the Mediterranean trade by introducing the New World to the market.\n\nCities, states and duchies along the salt roads exacted heavy duties and taxes for the salt passing through their territories. This practice even caused the formation of cities, such as the city of Munich in 1158, when the then Duke of Bavaria, Henry the Lion, decided that the bishops of Freising no longer needed their salt revenue.\n\nThe \"gabelle\"—a hated French salt tax—was enacted in 1286 and maintained until 1790. Because of the gabelles, common salt was of such a high value that it caused mass population shifts and exodus, attracted invaders and caused wars.\n\nIn American history, salt has been a major factor in outcomes of wars. In the Revolutionary War, the British used Loyalists to intercept Revolutionaries' salt shipments and interfere with their ability to preserve food. During the War of 1812, salt brine was used to pay soldiers in the field, as the government was too poor to pay them with money. Before Lewis and Clark set out for the Louisiana Territory, President Jefferson in his address to Congress mentioned a mountain of salt, 180 miles long and 45 wide, supposed to lie near the Missouri River, which would have been of inconceivable value, as a reason for their expedition.\n\nDuring India's independence movement, Mohandas Gandhi organized the Salt Satyagraha protest to demonstrate against the British salt tax.\n\n\"Wich\" and \"wych\" are names associated (but not exclusively) with brine springs or wells in England. Originally derived from the Latin \"vicus\", meaning \"place\", by the 11th century use of the 'wich' suffix in placenames was associated with places with a specialised function including that of salt production. Several English places carry the suffix and are historically related to salt, including the four Cheshire 'witches' of Middlewich, Nantwich, Northwich and Leftwich (a small village south of Northwich), and Droitwich in Worcestershire. Middlewich, Nantwich, Northwich and Droitwich are known as the \"Domesday Wiches\" due to their mention in the Domesday Book, \"an indication of the significance of the salt-working towns in the economy of the region, and indeed of the country\". Salt was very important to Europe because it was hard to trade with Africa and they needed to produce it themselves.\n\nMonopolies over salt production and trade were essential aspects of government revenue in imperial China and most of the 20th century.\n\nDuring modern times, it became more profitable to sell salted food than pure salt. Thus sources of food to salt went hand in hand with salt making. The British controlled saltworks in the Bahamas as well as North American cod fisheries. The search for oil in the late 19th and early 20th centuries used the technology and methods pioneered by salt miners, even to the degree that they looked for oil where salt domes were located.\n\nOn an industrial scale, salt is produced in one of two principal ways: the evaporation of salt water (brine) or by mining. Evaporation can either be solar evaporation or using some heating device.\n\nIn the correct climate (one for which the ratio of evaporation to rainfall is suitably high) it is possible to use solar evaporation of sea water to produce salt. Brine is evaporated in a linked set of ponds until the solution is sufficiently concentrated by the final pond so that the salt crystallizes on the pond's floor.\n\nOne of the traditional methods of salt production in more temperate climates is using open pans. In open-pan production, salt brine is heated in large, shallow open pans. The earliest examples of this date back to prehistoric times and the pans were made of either a type of ceramic called briquetage, or lead. Later examples were made from iron. This change coincided with a change from wood to coal for the purpose of heating the brine. Brine would be pumped into the pans and concentrated by the heat of the fire burning underneath. As crystals of salt formed, these would be raked out and more brine added.\n\nThe open pan salt works has effectively been replaced with a closed pan system where the brine solution is evaporated under a partial vacuum.\n\nIn the second half of the 19th century, industrial mining and new drilling techniques made the discovery of more and deeper deposits possible, increasing mine salt's share of the market. Although mining salt was generally more expensive than extracting it from brine via solar evaporation of seawater, the introduction of this new source reduced the price of salt due to a reduction of monopolization. Extraction of salt from brine is still heavily used; for example, vacuum salt produced by British Salt in Middlewich has 57% of the UK market\n\nThe earliest systematic exposition of the different kinds of salts, its uses, and the methods of its extraction was published in China around 2700 BCE. Hippocrates encouraged his fellow healers to use salt water to heal various ailments by immersing their patients in sea water. The ancient Greeks continued this, and in 1753, English author and physician Dr. Charles Russel published \"The Uses of Sea Water\".\n\n\n"}
{"id": "3840014", "url": "https://en.wikipedia.org/wiki?curid=3840014", "title": "Illegal sports", "text": "Illegal sports\n\nAn illegal sport is any sport that is illegal in one or more jurisdictions due to the violent or dangerous nature of the sport. Well-known illegal sports, such as cockfighting and dogfighting, are barred on the basis of animal abuse.\n\nIllegal sports are controversial due to the dangerous aspects attributed to them and the pain they can inflict on humans or animals. They also are controversial due to the perceived nature of some of them — notably dogfighting — as being savage sports.\n\nCockfighting is a gambling and spectator sport where roosters fight, frequently to the death, in rings, while players place bets on which rooster will win. Often, sharp implements are attached to the legs of the birds, inflicting massive injuries and pain. The birds used for cockfighting sometimes are given stimulant drugs to enhance their fighting ability and make them more aggressive.\n\nAccording to the Humane Society of the United States, cockfighting is illegal (at least a misdemeanor) in all fifty US states. It is classified as a felony in 39 states. Notable states that have less severe laws are Alabama, Hawaii, Idaho, and Mississippi (misdemeanor punishment for cockfighting; no punishment for possessing cock or being a spectator); South Carolina, South Dakota, Utah, and Kentucky (misdemeanor punishment for cockfighting, no punishment for possessing cocks, misdemeanor punishment for being a spectator).\n\nGovernor Frank Keating of Oklahoma said when outlawing cockfighting in his state that \"Cockfighting is cruel, it promotes illegal gambling and it is simply embarrassing to Oklahoma to be seen as one of only a tiny handful of locations outside of the third world where this activity is legal.\" Since there is no reliable data on the status of cockfighting in the third world, it is assumed that cockfighting is largely legal, unpopular, or laws against it are unenforced amongst these nations.\n\nDog fighting is a practice, illegal in many jurisdictions, where two dogs, often a molosser breed, are put into an area to fight and sometimes kill each other. Dog fighting has been reported as far back as AD 43 when the Romans invaded Britain. Both sides employed fighting dogs, and out of their wartime use grew a sport, which achieved great popularity, particular in Britain and later the United States.\n\nDogfighting can involve high stakes, and carries with it the same sociological dangers of other gambling, and particularly illegal gambling, activities.\n\nThe American Society for the Prevention of Cruelty to Animals focuses heavily on the issue on dog fighting. There are various levels of dog fighting. There is “street” level, which means that the dogfights are informal because strict rules and regulations are absent from the matches. Another level is “hobbyists”, which are fights that are formally organized. They are mainly scheduled for income and speculators. The final level of dog fighting is “professional.” At the professional level, owners usually have more that fifty fighting dogs and carefully examine the specific breed, lineage, and winning history of each dog.\n\nDog fighting is illegal in every U.S. state and in many countries around the world (Britain, where it was quite popular, banned it as far back as the 1830s), although enforcement in other countries is frequently lax or nonexistent. Dog fighting is a felony in all states except Idaho and Wyoming, where it's a misdemeanor. It is a felony to possess dogs for fighting except in the states of New York, Texas, West Virginia, and Wyoming.\n\nStreet racing is the frequently illegal racing of motor vehicles on public roads and highways. These high-speed races, usually with untrained drivers, can result in fatal crashes that have the capacity to inflict damage on innocent people not participating in the race. In 2006, California state highway patrol issued 697 citations for \"speed contests\". There is no official statistic kept on street racing deaths. Street racing can become an addicting habit for many drivers.\n\nBASE jumping is a form of skydiving from buildings. The sport is illegal in almost all cities, because the jumper risks seriously injuring himself and pedestrians or motorists when he lands. In many cases, BASE jumpers illegally access the high points from which they're jumping by breaking and entering or trespassing. Two BASE jumpers were arrested in St. Petersburg, Russia after jumping off of the Cathedral of the Apostles St. Peter and St. Paul in November 2011.\n"}
{"id": "50721436", "url": "https://en.wikipedia.org/wiki?curid=50721436", "title": "J. Pauly &amp; Sohn", "text": "J. Pauly &amp; Sohn\n\nJ. Pauly & Sohn now known as Pauly Beds is an one of the oldest bedding companies in the world. It was the sole official supplier of beds to the Austrian Empire for several generations.\n\nJosef Pauly established the company in Vienna, in 1838 when he received a license to make beds and mattresses from the emperor, Ferdinand I of Austria (17931875). The name of the new company was \"J. Pauly & Sohn\". In 1878, Pauly received an Imperial and Royal Warrant of Appointment as a Purveyor to the Imperial and Royal Court of Emperor Franz Joseph I and his wife Empress Elisabeth of Austria, (known as \"Sissi\").\n\nIn Austria, the handcrafting of beds and mattresses is a traditional industry. In the 18th century in Austria, there were ten recognised masters of this profession. During the reign of Leopold I of Belgium (17901865), the industry was regulated through a system of royal privileges and a strictly controlled trade union. \n\nThe master craftsmen of beds and mattresses also handcrafted suitcases and saddles. This was because many of the expensive materials such as horsetail hair, wool, cotton and leather used in production were the same. The master craftsmen, especially those of Vienna, were well known and exported their goods to Greece, Turkey and other countries in the Middle East.\n\nDuring the 19th century, there was an increase in demand for luxury products of this sort. The 1873 Vienna World's Fair provided a showcase for the local artisans. During the Fair, Josef Pauly received the first \"Golden Cross for Industrial Merit\". \n\nThe year 1873 was the fiftieth anniversary of the founding of Pauly and Sohn. Josef Pauly appeared in a Vienna publication honouring distinguished industrialists.\n\nBy 1891, the company had received ten awards, including the \"Medal d'amission\" (\"medal of rightful admission\") in Brussels in 1876.\n\nAccording to a 1903 Vienna newspaper, J. Pauly & Sohn was considered \"one of the oldest and most respected companies in the bedding industry\" as, at the time of publication, \"it had been 31 years since Pauly had obtained the honorary title of K. & K. Purveyor of the Empire and was successfully delivering beds and furniture internationally\".\n\nAn Imperial and Royal Purveyor (\"k.u.k. Hoflieferant\") was, in Austria-Hungary, a trader of products or services who had a special permit, by imperial privilege, to deliver its goods or services to the court in Vienna. This privilege allowed the suppliers to advertise publicly and exempted them from paying tax. The company received the title only if it was a leader in its industry in quality. The title was therefore a seal of approval of “highest” class, the highest honor that a business could get back then.\n\nJ. Pauly & Sohn was the first company in Austria to produce handcrafted down feather duvets. They made beds and relaxation furniture, bedroom furniture, mirrors, living room furniture, and furniture for green houses and patios. They offered complete furnishings, beds for children, baby cribs, travel beds and a large variety of bed linen, headboards and products for sleep. In their mattresses and furniture they used materials such as horsehair (a moisture resistant material used at the time in saddles and carriages seats), cotton, wool, springs and silk.\n\nJosef Pauly was succeeded by his son, also named \"Josef Pauly\". By 1900, the Pauly family had been handcrafting beds for four generations. Nevertheless, the First World War, the fall of the monarchy in 1918, the Great Depression and eventually the Second World War created constant challenges for the company. The last family owner was Dorothea Henning after which the company was acquired by an investor group.\n\n"}
{"id": "57532235", "url": "https://en.wikipedia.org/wiki?curid=57532235", "title": "J. Richard Udry", "text": "J. Richard Udry\n\nJ. Richard Udry (1929 – July 29, 2012) was an American sociologist and demographer, known for his work on the biological and sociological factors affecting human behavior. He was Kenan Distinguished Professor of maternal and child health in the University of North Carolina at Chapel Hill (UNC) Gillings School of Global Public Health and professor of sociology in the UNC College of Arts and Sciences. He joined the faculty at UNC from 1965, and remained there for the rest of his career. He also directed UNC's Carolina Population Center (CPC) from 1977 to 1992. He is known for designing the National Longitudinal Study of Adolescent to Adult Health (originally known as the National Longitudinal Study of Adolescent Health), which he also secured funding for and directed from 1994 to 2004. He served as president of the Population Association of America in 1994, and served two terms as president of the Society for the Study of Social Biology.\n\nIn the popular media, he is known for a 1970 study he conducted which concluded that, contrary to popular belief, the Northeast blackout of 1965 had no effect on the number of births in New York City. The study was cited, for example, in a 2009 MSNBC article about claims that the United States' birth rate had increased nine months after the 2008 United States presidential election.\n\n"}
{"id": "19571474", "url": "https://en.wikipedia.org/wiki?curid=19571474", "title": "Limbe Provincial Hospital", "text": "Limbe Provincial Hospital\n\nLimbe Provincial Hospital (also Limbe Regional Hospital and Mile 1 Hospital) is a 200-bed hospital in the Southwest Province of Cameroon and is the principal referral hospital for the region.\n\nThe hospital was built ca. 1940 and was accorded the status of a Provincial Hospital in 1972. Since 2008, the Provinces of Cameroon officially have been called \"Regions\", and the \"Provincial Hospitals\" are now called \"Regional Hospitals\". The hospital's charter was to:\n\nIn the late 1990s and early 21st century, the hospital was suffering from major structural and organizational problems. By 2004, the attendance rate had dropped by 50%. Moreover, research revealed that there was a lack of knowledge among hospital staff. However, under the new directorship of Dr. Thompson Kinge (appointed 2005) the hospital has seen some major improvements. In July 2008, Nigerian Consul to the North West and South West provinces of Cameroon, Dr. Kenneth Nsor Nsor, awarded Kinge with an award for “recognition of outstanding meritorious services and good management.” Nsor praised Dr. Kinge for rehabilitating the hospital, and for providing quality care to the large Nigerian community in Limbe. Cameroonian journalist Aimé Potabo also credits Kinge for resuscitating the hospital, and praises the improvements of the last few years.\n\nThe hospital has received support from Cameroonians both within the country and abroad. Moreover, the Programme Germano-Camerounais de Santé has initiated training workshops in Cameroon, at least one of which (a workshop on sterilization) has been attended by technical staff from Limbe Provincial Hospital.\n\nLike hospitals in many parts of the Third World, Limbe Regional Hospital still suffers from several problems including lack of certain hospital supplies and equipment such as ECG machines and incubators. The hospital administration is working hard to raise money in order to solve these problems, which negatively affect the population of the Southwest Province.\n\nIn 2008, a partnership agreement has been concluded between the Regional Hospital Limbe and the Faculty of Medicine of the University of Rostock in Germany. This partnership is financed by the \"Gesellschaft für technische Zusammenarbeit (GTZ)\", the Society for Technical Cooperation, which belongs to the German Ministry of Economic Cooperation. It is part of the ESTHER network (\"Ensemble pour une Solidarité Thérapeutique Hospitalier En Réseau\") of the European Union. The goal of this partnership is improving care for patients with HIV and AIDS. Through this ongoing ESTHER cooperation, the laboratory has been upgraded (introduction of PCR technology), and physicians as well as laboratory technicians have received training in Rostock / Germany. In addition, joint Continuing Medical Education workshops have been conducted in Limbe. (see also: http://www.edennewspaper.com/index.php?option=com_content&view=article&id=11509:451&catid=55:health&Itemid=166 )\n\nThe hospital offers units for radiology, surgery, gynaecology and obstetrics, dental surgery, ophthalmology, pediatrics,physiotherapy, maternity and general medicine.\n\nBefore 2000, practically no research had been done at Limbe Provincial Hospital. In the last few years, however, several research studies have been based at the hospital. A few examples are:\n\n\n\n\nLimbe Provincial Hospital is located in the coastal town of Limbe, in the Southwest Province of Cameroon. The Southwest Province is one of only two Anglophone provinces in Cameroon; the other eight provinces are Francophone. Although there is no university in Limbe, there is a university in the nearby town of Buea (about 30 minutes drive). The University of Buea, which was founded in 1997, has a strong Health Sciences Department.\n\nLimbe Provincial Hospital is referred to by locals as \"Mile 1 Hospital\" since it is exactly one mile away from the Atlantic Ocean. Heading out from the hospital down the hill and towards the ocean, one will soon reach \"Half Mile\", the town center of Limbe. Continuing straight ahead, one will reach an area called \"Down Beach\", which is right by the ocean. Moving away from the ocean from the hospital, one passes other parts of Limbe (\"Mile 2, Mile 3, Mile 4,\" etc.) and eventually leaves the town. A few miles later, one reaches the town of Mutengene. Continuing straight at Mutengene, one will arrive in Douala, the economic capital of Cameroon, in about one hour. Turning left at Mutengene, one will soon come to Mile 17 ( from the Ocean) in the town of Buea. Continuing on the road through Buea one will reach Nigeria.\n"}
{"id": "32855794", "url": "https://en.wikipedia.org/wiki?curid=32855794", "title": "List of ponds in the Lower Harz Pond and Ditch System", "text": "List of ponds in the Lower Harz Pond and Ditch System\n\nThe ponds of the Lower Harz Pond and Ditch System consist of around 20 small and larger reservoirs. The majority were laid out roughly from the beginning of the 17th century between the Upper Lude, Großer Auerberg, Straßberg, Neudorf and Silberhütte. Several of the pond barrages are classified as dams. The dam of the Gräfiggründer Teich is also the second oldest reservoir in Germany.\n\nThe ponds are located in the mining fields of Straßberg, Silberhütte, Birnbaum and Neudorf.\n\n\n"}
{"id": "26478244", "url": "https://en.wikipedia.org/wiki?curid=26478244", "title": "Maged N. Kamel Boulos", "text": "Maged N. Kamel Boulos\n\nMaged N. Kamel Boulos is a British health informatician, scientist and Professor of Digital Health currently based at the Alexander Graham Bell Centre of Digital Health, University of the Highlands and Islands, having worked before that at the University of Plymouth, at the University of Bath and at City University London. He is particularly known for his research into Geographic Information Systems (GIS) applications in health and healthcare, which received wide news media coverage. He is credited with coining the phrases 'online consumer geoinformatics services' and 'wikification of GIS by the masses' in 2005, when neogeography and virtual globes were still very new.\n\nKamel Boulos is Founder and Editor-in-Chief of the Open Access, MEDLINE-indexed \"International Journal of Health Geographics\", published by BioMed Central since 2002. He is Co-Chair of WG IV/4: Virtual Globes and Context-Aware Visualisation/Analysis within the International Society for Photogrammetry and Remote Sensing (ISPR) Commission IV Geodatabases and Digital Mapping, 2008-2012.\n\n\n\n"}
{"id": "52810315", "url": "https://en.wikipedia.org/wiki?curid=52810315", "title": "Minna Weizmann", "text": "Minna Weizmann\n\nMinna Weizmann (1889–?) was a Russian Jewish doctor who served in Syria and Palestine during World War I. Weizmann also served as a spy for Germany.\n\nWeizmann was born in present-day Motal, Belarus (then part of the Russian Empire) to a prominent family. Her parents were Oizer and Rachel Czermerinsky Weizmann. Weizmann's most famous sibling was Chaim Weizmann, the President of Zionist Organization and the first President of Israel.\n\nWeizmann attended medical school in Berlin. She was a socialist who hated the czarist government. In 1913, she immigrated to Palestine, where she became one of the few female physicians. In 1914, she met Curt Prufer, who recruited her to be a spy in British Egypt for Germany. Weizmann was caught during one of her missions in Italy and sent back to Egypt. If she had been convicted of spying for Germany, she would have been imprisoned or killed. Possibly due to the prominence of her family, or due to her good reputation in Egypt, she was allowed to return to Russia.\n\nWeizmann survived the war and continued her medical service with the Zionist women's organization, Hadassah.\n"}
{"id": "1984280", "url": "https://en.wikipedia.org/wiki?curid=1984280", "title": "Monsanto Canada Inc v Schmeiser", "text": "Monsanto Canada Inc v Schmeiser\n\nMonsanto Canada Inc v Schmeiser [2004] 1 S.C.R. 902, 2004 SCC 34 is a leading Supreme Court of Canada case on patent rights for biotechnology, between a Canadian canola farmer, Percy Schmeiser, and the agricultural biotechnology company Monsanto. The court heard the question of whether Schmeiser's intentionally growing genetically modified plants constituted \"use\" of Monsanto's patented genetically modified plant cells. By a 5-4 majority, the court ruled that it did. The case drew worldwide attention and is widely misunderstood to concern what happens when farmers' fields are accidentally contaminated with patented seed. However, by the time the case went to trial, all claims of accidental contamination had been dropped; the court only considered the GM canola in Schmeiser's fields, which Schmeiser had intentionally concentrated and planted. Schmeiser did not put forward any defence of accidental contamination.\n\nThe biotechnology company Monsanto developed and patented a glyphosate-resistant gene for the canola plant which has the effect of producing canola that is resistant to glyphosate. Monsanto marketed the seed as Roundup Ready Canola. Farmers using the system are able to control weed competition using Roundup, while avoiding damage to the Roundup-resistant crops. Users are required to enter into a formal agreement with Monsanto, which specifies that new seed must be purchased every year, the purchase price of which includes a licensing fee to use the patent rights. Roundup Ready Canola was introduced in Canada in 1996, and by 1998, it accounted for 25% of the country's canola area.\n\nAs established in the original Federal Court trial decision, Percy Schmeiser, a canola breeder and grower in Bruno, Saskatchewan, first discovered Roundup-resistant canola in his crops in 1997. He had used Roundup herbicide to clear weeds around power poles and in ditches adjacent to a public road running beside one of his fields, and noticed that some of the canola which had been sprayed had survived. Schmeiser then performed a test by applying Roundup to an additional to of the same field. He found that 60% of the canola plants survived. At harvest time, Schmeiser instructed a farmhand to harvest the test field. That seed was stored separately from the rest of the harvest, and used the next year to seed approximately 1,000 acres (4 km²) of canola.\n\nAt the time, Roundup Ready canola was in use by several farmers in the area. Schmeiser claimed that he did not plant the initial Roundup Ready canola in 1997, and that his field of custom-bred canola had been accidentally contaminated. While the origin of the plants on Schmeiser's farm in 1997 remains unclear, the trial judge found that with respect to the 1998 crop, \"none of the suggested sources [proposed by Schmeiser] could reasonably explain the concentration or extent of Roundup Ready canola of a commercial quality\" ultimately present in Schmeiser's 1998 crop.\n\nIn 1998, Monsanto learned that Schmeiser was growing a Roundup-resistant crop and approached him to sign a license agreement to their patents and to pay a license fee. Schmeiser refused, maintaining that the 1997 contamination was accidental and that he owned the seed he harvested, and he could use the harvested seed as he wished because it was his physical property. Monsanto then sued Schmeiser for patent infringement, filing its case in Canadian federal court on August 6, 1998. Negotiations to settle the matter collapsed on August 10, 1999, leading Schmeiser to file a countersuit against Monsanto for $10 million for libel, trespass, and contaminating his fields.\n\nRegarding the question of patent rights and the farmer's right to use seed taken from his fields, Monsanto said that because they hold a patent on the gene, and on canola cells containing the gene, they have a legal right to control its use, including the intentional replanting of seed collected from plants with the gene which grew accidentally. Schmeiser insisted on his \"farmer's rights\" to do anything he wished with seeds harvested from any plants grown on his field - including plants from seeds that were accidentally sown - and that this tangible property right overrides Monsanto's patent rights.\n\nCanadian law does not mention any such \"farmer's rights\"; the court held that the farmer's right to save and replant seeds is simply the right of a property owner to use his or her property as he or she wishes, and hence the right to use the seeds is subject to the same legal restrictions on use rights that apply in any case of ownership of property, including restrictions arising from patents in particular. The court wrote: \"Thus a farmer whose field contains seed or plants originating from seed spilled into them, or blown as seed, in swaths from a neighbour's land or even growing from germination by pollen carried into his field from elsewhere by insects, birds, or by the wind, may own the seed or plants on his land even if he did not set about to plant them. He does not, however, own the right to the use of the patented gene, or of the seed or plant containing the patented gene or cell.\"\n\nBeginning with the lead-up to the initial Federal Court trial, the case drew widespread public attention and media coverage. The contest was portrayed by some as a classic David-and-Goliath confrontation between small farmer and Monsanto, while others portrayed it as theft of the results of years of research and development. Environmental groups and anti-genetic engineering activists championed Schmeiser's cause and he spoke on the case around the world. Others depicted the case as a contest between a large biotechnology company and an equally large and well funded anti-biotechnology industry and raised concerns that the facts and context of the case was being misrepresented by Schmeiser, environmental groups and anti-genetic engineering activists.\n\n\"Monsanto v. Schmeiser\" was portrayed as being part of the process of legally defining the bounds of new biotechnologies, including genetic engineering and ownership of higher lifeforms. The case was frequently connected with that of the so-called Harvard mouse, where in 2002 the Canadian Supreme Court had rejected a patent for a special breed of mouse developed for research by Harvard University. The Canadian Harvard mouse case was a precedent-setting case in Canada with regard to the right to own higher lifeforms, where the Canadian ruling went against findings in the US and Europe, where the Harvard mouse patent was upheld. The Canadian Supreme Court eventually took pains to point out that the Monsanto v Schmeiser case focused on genes in seeds, and not on higher life forms; it was \"the first in which the top court of any country has ruled on patent issues involving plants and seed genes.\"\n\nThe issues of patent infringement and \"farmer's rights\" were settled, in Monsanto's favour, at the trial before the Federal Court of Canada and upheld at the appeal level before the Federal Court of Appeal. Both courts found that a key element in Mr. Schmeiser's patent infringement in his 1998 crop was that he knew or ought to have known the nature of the glyphosate-resistant seed he saved and planted.\n\nThe case was initially tried on June 5, 2000, in the Federal Court of Canada, at Saskatoon, Saskatchewan.\n\nAll claims relating to Roundup Ready canola in Schmeiser's 1997 canola crop were dropped prior to trial and the court only considered the canola in Schmeiser's 1998 fields. Regarding his 1998 crop, Schmeiser did not put forward any defence of accidental contamination. The evidence showed that the level of Roundup Ready canola in Mr. Schmeiser's 1998 fields was 95-98%. Evidence was presented indicating that such a level of purity could not occur by accidental means. On the basis of this the court found that Schmeiser had either known \"or ought to have known\" that he had planted Roundup Ready canola in 1998. Given this, the question of whether the canola in his fields in 1997 arrived there accidentally was ruled to be irrelevant. Nonetheless, at trial, Monsanto was able to present evidence sufficient to persuade the Court that Roundup Ready canola had probably not appeared in Schmeiser's 1997 field by such accidental means (paragraph 118). The court said it was persuaded \"on the balance of probabilities\" (the standard of proof in civil cases, meaning \"more probable than not\" i.e. strictly greater than 50% probability) that the Roundup Ready canola in Mr. Schmeiser's 1997 field had not arrived there by any of the accidental means, such as spillage from a truck or pollen travelling on the wind, that Mr. Schmeiser had proposed.\n\nIn the public arena, Schmeiser supporters argued that his account still leaves open the possibility that the harvesting and replanting of Roundup Ready canola from the sprayed region was accidental and resulted from a miscommunication between Schmeiser and his farmhand, or from a failure of Schmeiser to have the presence of mind to instruct his farmhand to avoid taking canola seed for replanting from the sprayed region. Supporters of Monsanto argued that an oversight of this nature is not plausible, especially in light of Schmeiser's claims regarding the extent to which he considered Roundup Ready canola undesirable in his fields and the importance he claims to have placed on the continued survival of his own strain of canola, and in light of his having been notified prior to planting his 1998 crop that Monsanto believed he had grown Roundup Ready canola in 1997. Legally, an oversight of this nature is not a defence against patent infringement, and was therefore irrelevant. Patents are civil law, and the presence or absence of \"guilty intent\" is not a factor in determining patent infringement. On this point, the Federal Court of Appeal noted that accidental genetic contamination of a crop beyond a farmer's control should be an exception to the rule that intent is not an issue in patent disputes.\n\nThe Court's ruling concluded:\n\nThe case was then heard by the Federal Court of Appeal at Saskatoon, Saskatchewan, beginning May 15, 2002. The Federal Court of Appeal upheld the ruling of the trial judge.\n\nThe Federal Court of Appeal in particular stressed the importance of the finding that Schmeiser had knowingly used the seed, in their decision to find Schmeiser in infringement of the patent, and noted that in a case of accidental contamination or a case where the farmer knew of the presence of the gene but took no action to increase its prevalence in his crop, a different ruling could be possible (see paragraphs 55-58 of the appeal ruling). No damages were assessed against Percy Schmeiser, the private individual. Only Mr. Schmeiser's farming corporation, Schmeiser Enterprises Ltd., was held liable, as Mr. Schmeiser had acted in his capacity as director of the corporation.\n\nLeave was requested of the Supreme Court of Canada to hear the case. This was granted in May, 2003, and the trial began on January 20, 2004. The issue before the Supreme Court was whether Schmeiser's planting and cultivation of genetically modified canola constituted \"use\" of Monsanto's patented invention of genetically modified canola cells.\n\nIntervening on Schmeiser’s behalf were a consortium of six non-government organizations (Council of Canadians; Action Group on Erosion, Technology and Concentration; Sierra Club; National Farmers Union; Research Foundation for Science, Technology and Ecology; and the International Center for Technology Assessment) and the Attorney General of Ontario.\n\nSchmeiser's principal defense at trial was that as he had not applied Roundup herbicide to his canola he had not used the invention. This argument was rejected; the court said that the patent granted for the invention did not specify the use of Roundup as part of the invention, and thus there was no basis for introducing the requirement that Roundup had to be used in order for the invention to be used. That is, a patent prohibits unauthorized use of an invention in any manner, not merely unauthorized use for its intended purpose.\n\nThe Court considered the question of whether knowingly (or, where one ought to have known) planting and cultivating genetically modified canola constitutes \"use\" of Monsanto's patented invention of genetically modified canola cells, even if the crop is not treated with Roundup and the presence of the gene affords no advantage to the farmer. The court ruled in favour of Monsanto, holding that his use of the patented genes and cells was analogous to the use of a machine containing a patented part: \"It is no defense to say that the thing actually used was not patented, but only one of its components.\" (Supreme Court Decision, Paragraph 78) The court also held that by planting genetically modified Roundup resistant canola, Schmeiser made use of the \"stand-by\" or insurance utility of the invention. That is, he left himself the option of using Roundup on the crop should the need arise. This was considered to be analogous to the installation of patented pumps on a ship: even if the pumps are never actually switched on, they are still used by being available for pumping if the need arises.\n\nOn May 21, 2004, the Supreme Court ruled 5-4 in favor of Monsanto. Schmeiser won a partial victory, where the court held that he did not have to pay Monsanto his profits from his 1998 crop, since the presence of the gene in his crops had not afforded him any advantage and he had made no profits on the crop that were attributable to the invention. The amount of profits at stake was relatively small, C$19,832; however, by not having to pay damages, Schmeiser was also saved from having to pay Monsanto's legal bills, which amounted to several hundred thousand dollars and exceeded his own.\n\nThe majority was written by McLachlin C.J. with Major, Binnie, Deschamps and Fish JJ. concurring.\n\nThe Court dismissed the argument that \"use\" of patented cells or genes applied only in the context of their isolated form. Nor does the fact that Schmeiser did not use Roundup herbicide on his crops preclude \"use\" of the gene. Even though the plants propagate without human intervention the realities of modern agriculture mean there is always human intervention in the growth of plants and thus farming is a method of \"use\" of plant genes.\n\nThe Court ruled that Schmeiser deprived Monsanto of its monopoly on the special canola plant by storing and planting the Roundup Ready canola seeds pursuant to his commercial interests. Thus, Schmeiser is considered to have infringed section 42 of the Patent Act. The Court, however, disagreed with the damages given by the trial judge as there was no profit directly resulting from the invention itself.\n\nIn the ruling, the court made it clear that patent infringement was the sole consideration, and concerns related to genetic engineering in agriculture were not within the scope of the case:\n\nArbour J., writing for Iacobucci, Bastarache, and LeBel JJ., dissented in part. The reasoning of the dissent closely follows that of the majority in \"Harvard College v. Canada (Commissioner of Patents)\" that concluded that though a company can patent products and processes, they cannot patent higher forms of life such as the whole plant itself. That is, \"the plant cell claim cannot extend past the point where the genetically modified cell begins to multiply and differentiate into plant tissues, at which point the claim would be for every cell in the plant\" (para. 138), which would extend the patent too far. The patent can only be for the founder plant and not necessarily its offspring.\n\nThe courts at all three levels noted that the case of accidental contamination beyond the farmer's control was not under consideration but rather that Mr. Schmeiser's action of having identified, isolated and saved the Roundup-resistant seed placed the case in a different category. The appellate court also discussed a possible intermediate scenario, in which a farmer is aware of contamination of his crop by genetically modified seed, but tolerates its presence and takes no action to increase its abundance in his crop. The court held that whether such a case would constitute patent infringement remains an open question but that it was a question that did not need to be decided in the Schmeiser case.(Paragraph 57 of the Appeals Court Decision)\n\nThe ruling did increase the protection available to biotechnology companies in Canada, a situation which had been left open with the Harvard mouse decision, where it was determined that a \"higher lifeform\", such as an animal, or by extension a plant, cannot be patented. This put Canada at odds with the other G8 countries where the patent had been granted. In Monsanto vs. Schmeiser, it was determined that protection of a patented gene or cell extends to its presence in a whole plant, even while the plant itself, as a higher lifeform, cannot be patented. This majority view, based on the precedent of mechanical devices, was central to the Supreme Court's decision, and put the onus on the Canadian Parliament to make distinctions between machines and lifeforms as it saw fit.\n\nIn 2005, a \"documentary theatre\" production dramatizing the court battle, entitled \"Seeds\", by Annabel Soutar, was staged in Montreal, Quebec. The dialogue was derived entirely verbatim from various archival sources.\n\nThe case is widely cited or referenced by the anti-GM community in the context of a fear of a company claiming ownership of a farmer’s crop based on the inadvertent presence of GM pollen grain or seed. \"The court record shows, however, that it was not just a few seeds from a passing truck, but that Mr Schmeiser was growing a crop of 95–98% pure Roundup Ready plants, a commercial level of purity far higher than one would expect from inadvertent or accidental presence. The judge could not account for how a few wayward seeds or pollen grains could come to dominate hundreds of acres without Mr Schmeiser’s active participation, saying ‘. . .none of the suggested sources could reasonably explain the concentration or extent of Roundup Ready canola of a commercial quality evident from the results of tests on Schmeiser’s crop’\" - in other words, even if the original presence of Monsanto seed on his land in 1997 was inadvertent, the crop in 1998 was entirely purposeful.\n\n"}
{"id": "469690", "url": "https://en.wikipedia.org/wiki?curid=469690", "title": "National Water Carrier of Israel", "text": "National Water Carrier of Israel\n\nThe National Water Carrier of Israel (, \"HaMovil HaArtzi\") is the largest water project in Israel. Its main task is to transfer water from the Sea of Galilee in the north of the country to the highly populated center and arid south and to enable efficient use of water and regulation of the water supply in the country. Up to of water can flow through the carrier each hour, totalling 1.7 million cubic meters in a day.\n\nMost of the water works in Israel are combined with the National Water Carrier, the length of which is about . The carrier consists of a system of giant pipes, open canals, tunnels, reservoirs and large scale pumping stations. Building the carrier was a considerable technical challenge as it traverses a wide variety of terrains and elevations.\n\nEarly plans were made before the establishment of the state of Israel but detailed planning started only after nascent Israel's formation in 1948. The construction of the project, originally known as the Jordan Valley Unified Water Plan, started in 1953, during the planning phase, long before the detailed final plan was completed in 1956. The project was designed by Tahal and constructed by Mekorot. It was started during the tenure of Prime Minister David Ben-Gurion, but was completed in June 1964 under Prime Minister Levi Eshkol, at a cost of about 420 million Israeli lira (at 1964 values). The National Water Carrier was inaugurated in 1964, with 80% of its water being allocated to agriculture and 20% for drinking water. As time passed however, increasing amounts were consumed as drinking water, and by the early 1990s, the National Carrier was supplying half of the drinking water in Israel. It was forecast that by the year 2010 80% of the National Carrier will be directed more at providing drinking water. The reasons for the increased demand for drinking water was twofold. Firstly, Israel saw rapid population growth, primarily in the center of the country which increased demands for water. Furthermore, as the standard of living in the country rose, there was increased domestic water use. As a result of the 1994 Israel-Jordan Treaty of Peace, among other items, Israel agreed to transfer 50 million cubic metres of water annually to Jordan.\n\nNowadays water from the Sea of Galilee supplies approximately 10% of Israel's drinking water needs. In recent years the Israeli government has undertaken extensive investments in water reclamation and desalination infrastructure in the country, while promoting water conservation. This has lessened the country's reliance on the National Water Carrier and has allowed it to significantly reduce the amount of water pumped from the Sea of Galilee annually in an effort to restore and improve the lake's ecological environment, especially in face of persistent severe droughts affecting the lake's intake basin in recent years. It is expected that in 2016 only about of water will be drawn from the lake for Israeli domestic water consumption, down from more than pumped annually a decade earlier.\n\nWater first enters the National Water Carrier through a several hundred meter long pipeline which is submerged under the northern part of Sea of Galilee. The water passes into a reservoir on the shore and then travels on to a pumping station. The pipeline is made up of nine pipes which are joined by an internal cable threaded through them. Each of these pipes includes twelve concrete pipes, each five meters long and three meters wide. As these pipes were cast, they were encased in steel pipes, sealed at the ends and floated out onto the lake. A winged star-shaped cap is mounted in a vertical section of the underwater pipe to allow water to be taken in from all directions.\n\nWater travels to the Sapir Pumping Station on the shore of the lake where four horizontal pumps lift the water into three pipes which subsequently join to form the \"pressure pipe\", a long steel pressure resistant pipe which raises the water from -213 meters below sea level to +44 meters. From here, the water flows into the Jordan Canal, an open canal. This runs along a mountainside for most of its route. When full, the water in the canal is deep and flows purely by gravity apart from where two deep wadis intersect the course of the canal (Nahal Amud and Nahal Tzalmon). To overcome these obstacles, water is carried through steel pipes shaped like an inverted siphon.\n\nThe canal transfers the water into the Tzalmon Reservoir, a 1 hm operational reservoir in the Nahal Tzalmon valley. Here, the second pumping station in the course of the Water Carrier is located, the Tzalmon Pumping Station which is designed to lift water an additional . Water then enters the Ya’akov Tunnel which is long and 3 meters in diameter. This flows under hills near the village of Eilabun and transfers the water from the Jordan Canal to the open canal crossing which crosses the Beit Netofa Valley – the Beit Netofa Canal. The Beit Netofa Canal takes the water 17 kilometers and was built with an oval base due to the clay soil through which it runs. The width of the canal is 19.4 meters, the bottom is 12 meters wide and it is 2.60 meters deep, with the water flowing through it at a height of 2.15 meters.\n\nThe advanced Eshkol Water Filtration Plant, completed in 2007-2008 by Mekorot, the fourth largest in the world, is located at the southwestern edge of the Beit Netofa Valley. The water first passes through two large reservoirs. The first of these is a sedimentation pond, holding about 1.5 million m³ of water, which allow suspended matter in the water to settle to the bottom, thus cleaning the water. The second reservoir is separated from the sedimentation pond by a dam and has a capacity of 4.5 million m³. Here the inflow of water from the pumping stations and open canals is regulated against the outflow into the closed pipeline. The amount allowed through depends on water demand. A special canal bypasses the reservoirs allowing water to travel straight through the carrier. Before entering the closed pipeline, final tests are performed on the water in the carrier, with chemicals added to bring the water to drinking standards. At the end of the filtration process the water enters the 108\" Pipeline, which transports it 86 km to the Yarkon-Negev system near the city of Rosh HaAyin to the east of Tel Aviv and Petah Tikva.\n\nThe initial idea of a National Water Carrier followed the proposal of several solutions for the water problems of Palestine put forward before the establishment of Israel in 1948. Early ideas appeared in the 1902 book Altneuland by Theodore Herzl in which he talked about utilizing the sources of the Jordan River for irrigation purposes and channeling sea water for producing electricity from the Mediterranean Sea near Haifa through the Beit She'an and Jordan valleys to a canal which ran parallel to the Jordan River and the Dead Sea.\n\nAn earlier water development scheme was proposed by Walter C. Lowdermilk in his book Palestine, Land of Promise, published in 1944; it was developed with human and financial assistance from the American Zionist Emergency Council. The book became a bestseller, and important in swaying the debate within the Truman administration concerning immigrant absorptive capacity and the Negev as part of Israel. His book served as the basis for a detailed water resource plan which was prepared by James Hayes, an engineer from the USA, who proposed utilizing all water sources in Israel (2 km³ per annum) for irrigation and the production of electricity. This would involve diverting part of the Litani River water to the Hasbani River. This water which would be further transported by a dam and canal to the area south of Tel Hai, from where it would be \"dropped\" to produce electricity. Water would also be carried from Tel Hai to the Beit Netofa Valley which would become a national water reservoir, of about one billion cu.m. volume (one quarter of the Sea of Galilee's volume). An electricity generating station would be located at the reservoir's outlet, from where the water would flow into an open canal to Rafiah, which, whilst travelling south would collect water from wadis and streams, including the waters of the Yarkon River. Hayes also asserted that the Yarmouk River would be channeled into Lake Kinneret, in order to prevent a rise in its salinity which could come about as a result of the diversion of the River Jordan, and that a joint Israeli-Jordanian dam about 5 km east of kibbutz Sha'ar HaGolan would be constructed. The Hayes plan was designed to be implemented in two stages over a 10-year period, but never materialised due to its economic infeasibility and lack of cooperation by Jordan.\n\nEric Johnston, the water envoy of US President Dwight Eisenhower between 1954–1957, developed another water plan for Israel, which became known as the Johnston Plan. In this, water from the Jordan River and Yarmuk River would be divided between Israel (40%), Jordan (45%) and Syria and Lebanon (15%). Each country would keep its right to utilize the water flowing within its borders, if it caused no harm to a neighboring country. Whilst this plan was accepted as fair by Arab water experts, it later floundered as a result of increasing tensions in the region, but was later seriously considered by Arab leaders\n\nSince its construction, the resulting diversion of water from the Jordan River has been a source of tension with Syria and Jordan. In 1964, Syria attempted construction of a Headwater Diversion Plan that would have prevented Israel from using a major portion of its water allocation, sharply reducing the capacity of the carrier. This project and Israel's subsequent physical attack on those diversion efforts in 1965 were factors which played into regional tensions culminating in the 1967 Six-Day War. In the course of the war, Israel captured from Syria the Golan Heights, which contain some of the sources of the Sea of Galilee.\n\n\n"}
{"id": "510370", "url": "https://en.wikipedia.org/wiki?curid=510370", "title": "Neurofeedback", "text": "Neurofeedback\n\nNeurofeedback (NFB), also called neurotherapy or neurobiofeedback, is a type of biofeedback that uses real-time displays of brain activity—most commonly electroencephalography (EEG), to teach self-regulation of brain function. Typically, sensors are placed on the scalp to measure activity, with measurements displayed using video displays or sound.\n\nNeurofeedback is a type of biofeedback that measures brain waves to produce a signal that can be used as feedback to teach self-regulation of brain function. Neurofeedback is commonly provided using video or sound, with positive feedback for desired brain activity and negative feedback for brain activity that is undesirable. Related technologies include hemoencephalography biofeedback (HEG) and functional magnetic resonance imaging (fMRI) biofeedback.\n\nClinical guidelines on neurofeedback as a treatment for ADHD are mixed. Biofeedback is graded by the American Academy of Pediatrics with their Level 2 evidence-based treatment for ADHD. The NICE guideline for ADHD leaves the efficacy of biofeedback an open question (p. 412). In page 202 states \"Biofeedback has been employed as a non-invasive treatment for children with ADHD since the 1970s but is probably not used as a significant intervention in UK clinical practice\". However this is unsurprising since in the UK, NICE evaluates whether treatments should be recommended on the basis of the cost of a quality-adjusted life year. SIGN guideline no 112 in page 24 mentions \"Neurofeedback is presently considered to be an experimental intervention in children and young people with ADHD/HKD. There are no standardised interventions\". Institute for Clinical Systems Improvement guideline on Diagnosis and Management of Attention Deficit Hyperactivity Disorder in Primary Care for School-Age Children and Adolescents in page 41 mentions neurofeedback lacks enough research evidence for efficacy in ADHD.\n\nOverall research into neurofeedback is considered to have been limited and of low quality, although others have disagreed.\n\nIt has been argued there is some indication on the effectiveness of biofeedback for ADHD but that it is not conclusive: several studies have yielded positive results, however the best designed ones have either shown absent or reduced effects. Other experts have proposed that standard neurofeedback protocols for ADHD, such as theta/beta, SMR and slow cortical potentials neurofeedback are well investigated and have demonstrated specificity. No serious adverse side effects from neurofeedback have been reported.\n\nQEEG has been used to develop EEG models of ADHD. According to this model, persons with ADHD often have too many slow theta brain waves (associated with relaxation) and not enough fast beta wave activity (associated with mental focus). Neurofeedback therapies for ADHD generally attempt to increase the production of betawaves and decrease the number of slower brain waves. This can be accomplished by allowing the patient to view their levels of brain waves on a screen and attempt to alter them, or by integrating brain waves into a video game.\n\nResearch shows neurofeedback may be a potentially useful intervention for a range of brain-related conditions. It has been used for pain, addiction, aggression, anxiety, autism, depression, Schizophrenia, epilepsy, headaches, insomnia, Tourette syndrome, and brain damage from stroke, trauma, and other causes.\n\nIt is also used to treat other less well known disorders, such as Auditory Processing Disorder and working memory deficit.\n\nThe applications of neurofeedback to enhance performance extend to the arts in fields such as music, dance, and acting. A study with conservatoire musicians found that alpha-theta training benefitted the three music domains of musicality, communication, and technique. Historically, alpha-theta training, a form of neurofeedback, was created to assist creativity by inducing hypnagogia, a “borderline waking state associated with creative insights”, through facilitation of neural connectivity. Alpha-theta training has also been shown to improve novice singing in children. Alpha-theta neurofeedback, in conjunction with heart rate variability training, a form of biofeedback, has also produced benefits in dance by enhancing performance in competitive ballroom dancing and increasing cognitive creativity in contemporary dancers. Additionally, neurofeedback has also been shown to instil a superior flow state in actors, possibly due to greater immersion while performing.\n\nHowever, randomized control trials have found that neurofeedback training (using either sensorimotor rhythm or theta/beta ratio training) did not enhance performance on attention-related tasks or creative tasks. It has been suggested that claims made by proponents of alpha wave neurofeedback training techniques have yet to be validated by randomized, double-blind, controlled studies, a view which even some supporters of alpha neurofeedback training have also expressed.\n\nIn 1924, the German psychiatrist Hans Berger connected a couple of electrodes (small round discs of metal) to a patient's scalp and detected a small current by using a ballistic galvanometer. During the years 1929-1938 he published 14 reports about his studies of EEGs, and much of our modern knowledge of the subject, especially in the middle frequencies, is due to his research. Berger analyzed EEGs qualitatively, but in 1932 G. Dietsch applied Fourier analysis to seven records of EEG and became the first researcher of what later is called QEEG (quantitative EEG).\n\nLater, Joe Kamiya popularized neurofeedback in the 1960s when an article about the alpha brain wave experiments he had been conducting was published in \"Psychology Today\" in 1968. Kamiya’s experiment had two parts. In the first part, a subject was asked to keep his eyes closed and when a tone sounded to say whether he thought he was in alpha. He was then told whether he was correct or wrong. Initially the subject would get about fifty percent correct, but some subjects would eventually develop the ability to better distinguish between states. In the second part of the study, subjects were asked to go into alpha when a bell rang once and not go into the state when the bell rang twice. Once again some subjects were able to enter the state on command. Alpha states were connected with relaxation, and alpha training had the possibility to alleviate stress and stress-related conditions.\n\nDespite these claims, the universal correlation of high alpha density to a subjective experience of calm cannot be assumed. Alpha states do not seem to have the universal stress-alleviating power indicated by early observations. At one point, Martin Orne and others challenged the claim that alpha biofeedback actually involved the training of an individual to voluntarily regulate brainwave activity. James Hardt and Joe Kamiya, then at UC San Francisco's Langley Porter Neuropsychiatric Institute published a paper that supported biofeedback.\n\nIn the late sixties and early seventies, Barbara Brown, one of the most effective popularizers of Biofeedback, wrote several books on biofeedback, making the public much more aware of the technology. The books included \"New Mind New Body\", with a foreword from Hugh Downs, and \"Stress and the Art of Biofeedback\". Brown took a creative approach to neurofeedback, linking brainwave self-regulation to a switching relay which turned on an electric train.\n\nThe work of Barry Sterman, Joel F. Lubar and others has been relevant on the study of beta training, involving the role of sensorimotor rhythmic EEG activity. This training has been used in the treatment of epilepsy, attention deficit disorder and hyperactive disorder. The sensorimotor rhythm (SMR) is rhythmic activity between 12 and 16 hertz that can be recorded from an area near the sensorimotor cortex. SMR is found in waking states and is very similar if not identical to the sleep spindles that are recorded in the second stage of sleep.\n\nFor example, Sterman has shown that both monkeys and cats who had undergone SMR training had elevated thresholds for the convulsant chemical monomethylhydrazine. These studies indicate that SMR may be associated with an inhibitory process in the motor system.\n\nWithin the last 5–10 years, neurofeedback has taken a new approach in taking a look at deep states. Alpha-theta training has been tried with patients with alcoholism, other addictions as well as anxiety. This low frequency training differs greatly from the high frequency beta and SMR training that has been practiced for over thirty years and is reminiscent of the original alpha training of Elmer Green and Joe Kamiya. Beta and SMR training can be considered a more directly physiological approach, strengthening sensorimotor inhibition in the cortex and inhibiting alpha patterns, which slow metabolism. Alpha-theta training, however, derives from the psychotherapeutic model and involves accessing of painful or repressed memories through the alpha-theta state. The alpha-theta state is a term that comes from the representation on the EEG.\n\nA recent development in the field is a conceptual approach called the Coordinated Allocation of Resource Model (CAR) of brain functioning which states that specific cognitive abilities are a function of specific electrophysiological variables which can overlap across different cognitive tasks. The activation database guided EEG biofeedback approach initially involves evaluating the subject on a number of academically relevant cognitive tasks and compares the subject's values on the QEEG measures to a normative database, in particular on the variables that are related to success at that task.\n\nThe Association for Applied Psychophysiology and Biofeedback (AAPB) is a non-profit scientific and professional society for biofeedback and neurofeedback. The International Society for Neurofeedback and Research (ISNR) is a non-profit scientific and professional society for neurofeedback. The Biofeedback Federation of Europe (BFE) sponsors international education, training, and research activities in biofeedback and neurofeedback.\n\nThe Biofeedback Certification International Alliance (formerly the Biofeedback Certification Institute of America) is a non-profit organization that is a member of the Institute for Credentialing Excellence (ICE). BCIA certifies individuals who meet education and training standards in biofeedback and neurofeedback and progressively recertifies those who satisfy continuing education requirements. BCIA offers biofeedback certification, neurofeedback (also called EEG biofeedback) certification, and pelvic muscle dysfunction biofeedback certification. BCIA certification has been endorsed by the Mayo Clinic, the Association for Applied Psychophysiology and Biofeedback (AAPB), the International Society for Neurofeedback and Research (ISNR), and the Washington State Legislature.\n\nThe BCIA didactic education requirement includes a 36-hour course from a regionally accredited academic institution or a BCIA-approved training program that covers the complete Neurofeedback Blueprint of Knowledge and study of human anatomy and physiology. The Neurofeedback Blueprint of Knowledge areas include: I. Orientation to Neurofeedback, II. Basic Neurophysiology and Neuroanatomy, III. Instrumentation and Electronics, IV. Research, V. Psychopharmalogical Considerations, VI. Treatment Planning, and VII. Professional Conduct.\n\nApplicants may demonstrate their knowledge of human anatomy and physiology by completing a course in biological psychology, human anatomy, human biology, human physiology, or neuroscience provided by a regionally accredited academic institution or a BCIA-approved training program or by successfully completing an Anatomy and Physiology exam covering the organization of the human body and its systems.\n\nApplicants must also document practical skills training that includes 25 contact hours supervised by a BCIA-approved mentor designed to them teach how to apply clinical biofeedback skills through self-regulation training, 100 patient/client sessions, and case conference presentations. Distance learning allows applicants to complete didactic course work over the internet. Distance mentoring trains candidates from their residence or office. They must recertify every 4 years, complete 55 hours of continuing education (30 hours for Senior Fellows) during each review period or complete the written exam, and attest that their license/credential (or their supervisor’s license/credential) has not been suspended, investigated, or revoked.\n\nIn 2010, a study provided some evidence of neuroplastic changes occurring after brainwave training. Half an hour of voluntary control of brain rhythms led in this study to a lasting shift in cortical excitability and intracortical function. The authors observed that the cortical response to transcranial magnetic stimulation (TMS) was significantly enhanced after neurofeedback, persisted for at least 20 minutes, and was correlated with an EEG time-course indicative of activity-dependent plasticity.\n\n\n\n"}
{"id": "24052424", "url": "https://en.wikipedia.org/wiki?curid=24052424", "title": "Norwegian Cancer Society", "text": "Norwegian Cancer Society\n\nThe Norwegian Cancer Society () is a non-profitmaking organisation in Norway.\n\nIt was established as \"Norsk Forening til Kreftens Bekjempelse\" in 1938, and took the current name when it merged with \"Landsforeningen mot Kreft\" in 1948. Its purpose is to help patients with cancer, increase awareness of cancer and to fund cancer research. The society funds the annual King Olav V's Prize for Cancer Research.\n\nThe Secretary general is Anne Lise Ryel, and the board of directors consists of Gunn-Elin Aasprong Bjørneboe (chair), Carl Otto Løvenskiold (deputy chair), Tone Nordøy, Wenche Frogn Sellæg, Jostein Christian Dalland, Tord Dale, Lars A. Akslen, Grete Wennes, Anine Kierulf and Else Støring. The organizational headquarters are in Kongens gate 6 in Oslo.\n\n"}
{"id": "543329", "url": "https://en.wikipedia.org/wiki?curid=543329", "title": "Optician", "text": "Optician\n\nAn optician, or \"dispensing optician\", is a technical practitioner who designs, fits and dispenses corrective lenses for the correction of a person's vision. Opticians determine the specifications of various ophthalmic appliances that will give the necessary correction to a person's eyesight. Some registered or licensed opticians also design and fit special appliances to correct cosmetic, traumatic or anatomical defects. These devices are called shells or artificial eyes. Other registered or licensed opticians manufacture lenses to their own specifications and design and manufacture spectacle frames and other devices.\n\nCorrective ophthalmic appliances may be contact lenses, spectacles lenses, low vision aids or ophthalmic prosthetics to those who are partially sighted. The appliances are mounted either on the eye as contact lenses or mounted in a frame or holder in front of the eye as spectacles or as a monocle.\n\nOpticians may work in any variety of settings such as joint practice, hospitals, laboratories, eye care centers or retail stores. However, registered opticians have to meet standards of practice and training, commit to ongoing education, hold professional liability insurance and are held to these standards by their respective regulating bodies.\n\nA fully credentialed optician in the United States is college educated in Optical Science and is known as an Ophthalmic Optician® (O.O.) and they are credentialed by the Society to Advance Opticianry (SAO). To achieve this nationally registered title an optician must achieve a combination of a college education, American Board of Opticianry and National Contact Lens Examiners advanced certifications, or maintain their state license in both eyewear dispensing and contact lens fitting when applicable. In the United Kingdom, an ophthalmic optician is also known as an optometrist and is regulated by the General Optical Council under the Opticians Act 1989.\n\nLike many health care providers, opticians are regulated professionals in certain countries. The profession is often regulated by optician-specific agencies, as in Canada and some states of the U.S., or jointly with optometry such as the New Zealand Optometrist and Dispensing Opticians Board or the United Kingdom General Optical Council. Opticians may work independently or dependently with an optometrist or ophthalmologist although some opticians may work in an optical laboratory as a laboratory technical optician. Opticians convert a prescription for the correction of a refractive error into an ophthalmic lens or some other device, such as reading aids or telescopic lenses.\n\nThe first known artistic representation of eyeglasses was painted by Tommaso da Modena in 1352. He did a sequence of frescoes of brothers efficiently reading or replicating manuscripts; one holds a magnifying glass while the other has glasses suspended on his nose. Once Tommaso had established the example, other painters positioned spectacles on the noses of many of subjects, almost certainly as a representation of wisdom and respect.\n\nOne of the most noteworthy developments in spectacle production in the 15th century was the introduction of concave lenses for the myopic or nearsighted. Pope Leo X, who was very myopic, wore concave spectacles when hunting and professed they enabled him to see clearer than his cohorts.\n\nThe first spectacles utilized quartz lenses since optical glass had not been developed. The lenses were set into bone, metal and leather mountings, frequently fashioned like two small magnifying glasses with handles riveted together and set in an inverted V shape that could be balanced on the bridge of the nose. The use of spectacles extended from Italy to Germany, Spain, France and Portugal.\n\nFrom their inception, eyeglasses posed a dilemma that wasn't solved for almost 350 years: how to keep them on the bridge of the nose without falling. Spanish spectacle makers of the 17th century experimented with ribbons of silk that could be attached to the frames and then looped over the ears. Spanish and Italian missionaries carried the new models to spectacle wearers in China. The Chinese attached little ceramic or metal weights to the strings instead of making loops. In 1730 a London optician named Edward Scarlett perfected the use of rigid sidepieces that rested atop the ears. This perfection rapidly spread across the continent.\nIn 1752 James Ayscough publicized his latest invention, spectacles with double hinged side pieces. These became very popular and appear more often than any other kind in paintings and prints of the period. Lenses were fabricated of tinted glass as well as clear. Ayscough felt that the clear glass lenses gave an unpleasant glare. In Spain in 1763 Pablo Minguet recommended turquoise, green, or yellow lenses but not amber or red.\n\nEuropeans, in particular the French, were self-conscious about the use of eyeglasses. Parisian aristocrats used reading aids only in private. The gentry of England and France used a \"perspective glass” or monocular which could be concealed from view easily. In Spain, however, spectacles were popular amongst all classes since they considered eyeglasses made them look more important and dignified.\n\nFar-sighted or aging colonial Americans imported spectacles from Europe. Spectacles were primarily for the affluent and literate colonists, who required a valuable and precious appliance. Benjamin Franklin in the 1780s developed the bifocal. Bifocal lenses advanced little in the first half of the 19th century. The terms bifocal and trifocal were introduced in London by John Isaac Hawkins, whose trifocals were patented in 1827. In 1884 B. M. Hanna was granted patents on two forms of bifocals which become commercially standardized as the \"cemented\" and \"perfection\" bifocals. Both had the serious faults of ugly appearance, fragility, and dirt-collection at the dividing line. At the end of the 19th century the two sections of the lens were fused instead of cemented At the turn of the 20th century, there was a considerable increase in the use of bifocals.\nBetween 1781 and 1789, silver spectacles with sliding extension temples were being fabricated in France; however it was not until the 19th century that they gained extensive popularity. John McAllister of Philadelphia began fabricating spectacles with sliding temples containing looped ends which were much easier to use with the then-popular wigs. The loops supplement the inadequacy of stability, by allowing the addition of a cord or ribbon which could be tied behind the head, thus holding the eyeglasses firmly in place.\n\nIn 1826, William Beecher moved to Massachusetts from Connecticut to establish a jewellery-optical manufacturing shop. The first ophthalmic pieces he fabricated were silver spectacles, which were later followed by blue steel. In 1869 the American Optical Company was incorporated and acquired the holdings of William Beecher. In 1849 J. J. Bausch immigrated to the United States from Germany. He had already served an apprenticeship as an optician in his native land and had found work in Berne. His reimbursement for the labor on a complete pair of spectacles was equal to six cents. Mr. Bausch encountered difficult times in America from 1849 until 1861, at which time war broke out. When the war prevented import of eyeglass frames, demand for his hard rubber frames skyrocketed. Continuous expansion followed and the large Bausch and Lomb Company was formed.\n\nThe monocle, which was first called an \"eye-ring\", was initially introduced in England in the early 19th century; although it had been developed in Germany during the 18th century. A young Austrian named studied optics in London and took the monocle idea back to Germany with him. He started making monocles in Vienna about 1814 and the fashion spread and took particularly strong roots in Germany and in Russia. The first monocle wearers were upper-class gentlemen, which may account for the aura of arrogance the monocle seemed to confer on the wearer. After World War I, the monocle fell into disrepute, its downfall in the allied sphere hastened, no doubt, by its association with the German military.\n\nThe lorgnette, two lenses in a frame the user held with a lateral handle, was another 18th-century development (by Englishman George Adams). The lorgnette almost certainly developed from the scissors-glass, which was a double eyeglass on a handle. Given that the two branches of the handle came together under the nose and looked as if they were about to cut it off, they were known as binocles-ciseaux or scissors glasses. The English altered the size and form of the scissors-glasses and produced the lorgnette. The frame and handle were often artistically embellished, given that they were used mostly by women and more often as a piece of jewellery than as a visual aid. The lorgnette maintained its popularity with ladies of fashion, who chose not to wear spectacles. The lorgnette maintained its popularity to the end of the 19th century.\n\nPince-nez are believed to have appeared in the 1840s, but in the latter part of the century there was a great upsurge in the popularity of the pince-nez for both men and women. Gentlemen wore any style which suited them—heavy or delicate, round, or oval, straight, or drooping—usually on a ribbon, cord, or chain about the neck or attached to the lapel. Ladies more often than not wore the oval rimless style on a fine gold chain which could be reeled automatically into a button-size eyeglass holder pinned to the dress. Whatever the disadvantage of the pince-nez, it was convenient.\n\nIn the 19th century, the responsibility of choosing the correct lens lay, as it always had, with the customer. Even when the optician was asked to choose, it was often on a rather casual basis. Spectacles were still available from travelling salesmen.\nSpectacles with large round lenses and tortoise shell frames became the fashion around 1914. The enormous round spectacles and the pince-nez continued to be worn in the twenties. In the thirties there was increased emphasis on style in glasses with a variety of spectacles available. Meta Rosenthal wrote in 1938 that the pince-nez was still being worn by dowagers, headwaiters, old men, and a few others. The monocle was worn by only a minority in the United States. Sunglasses, however, became very popular in the late '30s.\n\nOpticians use a variety of equipment to fit, adjust and dispense eyewear, contact lenses and low vision aids.\n\nThe dispensing of eyewear requires the use of a focimeter, or lensometer, to verify the correct prescription in a pair of eyeglasses, properly orient and mark uncut lenses, and to confirm the correct mounting of lenses in spectacle frames. Certain lensometers also have the ability to examine contact lenses.\n\nThe parameters appraised by a lensometer are the sphere, cylinder, axis, add, and in some cases, prism. The lensometer is also used to check the accuracy of progressive lenses, and is often capable of marking the lens center and various other measurements critical to proper performance of the lens.\n\nAnother indispensable piece of equipment is a pupilometer. A pupilometer is a tool for more accurately measuring interpupillary distance (IPD or PD). It is used for fitting eyeglasses so that the lenses are centered in the visual axis. This is the most common nomenclature. A pupilometer may be manually operated, or may be digital. Pupilometers may also be used to verify a PD measurement taken by hand with a millimeter ruler. A pupilometer is best suited for better assurance in fitting progressive lenses, and other specialty lenses, since even tiny errors cause eye strain. Pupilometer apps have also been developed for smart phones and tablets.\n\nThe fitting and dispensing of contact lenses requires the use of additional equipment, all with very specific purposes. A keratometer is a diagnostic instrument for measuring the curvature of the anterior surface of the cornea, particularly for assessing the extent and axis of astigmatism. It was invented by the French ophthalmologist Samuel Hankins in 1880. Opticians, like ophthalmologists and optometrists, also use a slit-lamp/bio-microscope to examine the anterior segment, or frontal structures and posterior segment, of the human eye, which includes the eyelid, sclera, conjunctiva, iris, natural crystalline lens, and cornea. The binocular slit-lamp examination provides stereoscopic magnified view of the eye structures in detail, enabling anatomical diagnoses to be made for a variety of eye conditions.\n\nWhile a patient is seated in the examination chair, he rests his chin and forehead on a support to steady the head. Using the biomicroscope, the optician then proceeds to examine the patient's eye. A fine strip of paper, stained with fluorescein, a fluorescent dye, may be touched to the side of the eye; this stains the tear film on the surface of the eye to aid examination. The dye is naturally rinsed out of the eye by tears. Adults need no special preparation for the test; however children may need some preparation, depending on age, previous experiences, and level of trust.\n\nThe list of equipment used by an optician is extensive and is often specified in jurisdiction specific Professional Standards of Practice. The standards of the College of Opticians of British Columbia serve as an example.\n\nAll provinces in Canada require opticians to complete formal training and education in opticianry and then must pass competency examinations prior to receiving governmental licensure. Some provinces (Ontario and Quebec) require a single optician's license that includes both the dispensing of eyeglasses and contact lenses, while the other provinces have two separate licenses, one each for eyeglasses and contact lens dispensing.\n\nRecent changes to the British Columbia Opticians regulations allow qualified opticians in that province to test a persons vision and prepare an assessment of the corrective lenses required for a client. Using the results of the assessment an optician is able to prepare and dispense eyeglasses or contact lenses. Opticians in Alberta are also permitted, under certain conditions, to refract and prepare and dispense eyeglasses and contact lenses\n\nEach Canadian province has its own regulatory College or Board that provides registration or licensure to its opticians. The Regulatory body (\"often known as a ‘College’ but separate from, and not to be confused with, an educational institute\") has a government mandate to protect the public. This includes enforcement of provincial statutes (Opticians Act) and public awareness campaigns.\n\nThe National Association of Canadian Opticianry Regulators (NACOR) is an organization of all the provincial opticianry regulatory bodies in Canada (except Quebec). NACOR also administers Canada’s national opticianry examination(s). Since 2001, all jurisdictions (except Quebec) have agreed to and signed, the Mutual Recognition Agreement among Opticianry Regulators that ensures labour mobility to all opticians across the entire nation without need for further examination. All provinces (with the exception of Quebec) require individuals to achieve a passing mark in a national examination as a requirement of licensure as an optician.\n\nDespite the non participation of Quebec in National initiatives, Canadian opticians who relocate to Quebec are able to register and practice in that province provided that meet certain language requirements.\n\nMost Canadian provinces have their own provincial opticianry associations that look after the interests of their members at the provincial level, such as advocacy. Some provincial regulatory agencies have a dual role or purpose and also serve as the association for that province. In addition to protecting their member's interests, provincial associations also undertake public interest initiatives such as providing vision screening for children in schools, or organizing professional development seminars.\n\nEstablished in 1989, the Opticians Association of Canada is a national organization of all provincial Opticianry Associations in Canada. The role of the OAC is to advocate for the various interests of opticians on a national basis.\n\nAs a prerequisite for registration in any province of Canada opticians are required to complete a course at one of the NACOR accredited teaching institutions. Persons from an international jurisdiction may apply to a provincial regulatory agency for an assessment of equivalency of their education. Such applications are not unreasonably denied.\n\nIn Ghana, opticians are trained at the Optical Technician Training Institute (OTTI) at Oyoko in the Ashanti Region. The training programme is structured into a 3 year-diploma program. The diploma programme replaced the 2-year certificate program. In the new 3-year diploma program, one semester each of duration of about 4 months, is spent out on attachment every academic year. Presently, arrangements are being made for an affiliation to be built with the Department of Optometry, KNUST. Currently there are about 100 opticians in the country.\n\nOpticians or Dispensing Opticians are regulated by the General Optical Council (GOC). A dispensing optician advises on, fits and supplies the most appropriate spectacles after taking account of each patient’s visual, lifestyle and vocational needs. Dispensing opticians also play an important role in fitting contact lenses and advising and dispensing low vision aids to those who are partially sighted and in advising on and dispensing to children where appropriate.\n\nThe Association of British Dispensing Opticians (ABDO) is the qualifying body for dispensing opticians in the United Kingdom (UK). The Fellow of British Dispensing Opticians (FBDO) is the base qualification for UK dispensing opticians. This qualification has been awarded level 6 status (equivalent to BSc) by Ofqual Welsh Assembly Government and Council for Curriculum Examinations and Assessment(CCEA). Additional qualifications are Contact Lenses and Low Vision, have been assessed at level 7 (equivalent to an MSc).\n\nIn the United States, an optician, through testing, may be certified by the American Board of Opticianry (ABO) to fill the prescription ordered by an ophthalmologist or optometrist. Note: The ABO Exam is not nationally recognized and does not indicate a license to practice as an optician. In roughly half the states, licensing is not a requirement to make or dispense eyewear. Many eye doctors do their own dispensing, and it is frequent for eye clinics to have an optician on their premises; or, conversely, for large optical chains to have optometrists in offices on their premises.\n\nSome opticians learn their skills through formal training programs. Professional technical schools and two-year colleges offer programs in opticianry. Two-year programs usually grant an associate degree. One-year programs offer a certificate. Training usually includes courses in optical math, optical physics, and tools and equipment use. Other opticians can apprentice to learn the required skills. Many formal education programs will accept hours worked as an apprentice to supplement or replace course credits, as well.\n\n\n"}
{"id": "54904694", "url": "https://en.wikipedia.org/wiki?curid=54904694", "title": "Pain management in children", "text": "Pain management in children\n\nPain management in children is the assessment and treatment of pain in infants and children.\n\nUsually, acute pain has an obvious cause and is expected to last for a few days or weeks. It is usually managed with medication and non-pharmacological treatment to provide comfort. Acute pain is an indication for needed assessment, treatment and prevention. While a child is experiencing pain, physiological consequences can jeopardize healing and recovery. Unrelieved pain can cause alkalosis and hypoxemia that result from rapid, shallow breathing. This shallow breathing can lead to the accumulation of fluid in the lungs, taking away the ability to cough. Pain can cause an increase in blood pressure and heart rate, putting stress on the heart. Pain also increases the release of anti-inflammatory steroids that reduce the ability to fight infection, increase the metabolic rate and affect healing. Another harmful outcome of acute pain is an increase in sympathetic effects such as the inability to urinate. Pain can also slow the gastrointestinal system.\n\nInadequate pain management in children can lead to psycho-social consequences, including lack of interest in food, apathy, sleep problems, anxiety, avoidance of discussions about health, fear, hopelessness and powerlessness. Other consequences include extended hospital stays, high re-admission rates and longer recovery.\n\nExamples of harmful consequences of unrelieved pain include:\n\nNeuropathic pain is associated with nerve injuries or abnormal sensitivities to touch or contact. Though neuropathic pain is relatively uncommon in children compared to adults, greater awareness of these conditions is on the rise. Some causes may include past surgeries and amputations, \"autoimmune and degenerative neuropathies\", and injury to the spinal cord.\n\nSymptoms may include a tingling, shooting, prickling or burning sensation. The pain can be intermittent or continual and is often exacerbates in the evenings.\n\nNeuropathic pain can be peripheral or central. Peripheral neuropathic pain refers to disturbance in the function of peripheral nerves while central neuropathic pain refers to nerves in the central nervous system.\n\nThough not FDA-approved to address pain in children, anticonvulsants such as gabapentin and pregabalin have been used in severe neuropathic impairment (SNI). Other options are serotonin-norepinephrine reuptake inhibitors (SNRIs). Despite limited studies in children (limited to those with depression), SNRIs such as Venlafaxine have been shown to be effective.\n\nCancer pain in children may be caused by the cancer itself or the side effects of treatment. Tumors can cause pain in two different ways, either by the physical pressure it places on organs or by occluding normal bodily functions. Treatment such as surgery and injections can also lead to significant pain for the patient. If untreated, the pain can suppress the immune, interfere with sleep, and increase the chance of depression. Many different health care professionals will manage the child's pain and are referred to as the patient's palliative care team, these include oncologists, anesthesiologists, neurologists, surgeons, psychiatrists, and pharmacists. Hospitals may also hire individuals who specialize in music or art therapy, these therapies include acupuncture, biofeedback, massage therapy, and hypnosis. \n\nTreatment of the cancer pain is tailored to the child based on age, treatment, and side effects. The goal is to achieve sufficient background control of pain and minimize any acute exacerbation of severe pain. Often times medications such as non-steroidal anti-inflammatory drugs (NSAIDs), acetaminophen, or opiates are used to manage the pain. Additionally, non-pharmacological can also be used to manage the child's pain, this includes distracting the child, massages, acupuncture, heat/cold therapy, exercise, and quality sleep. \n\nChronic pain in children is unresolved pain that affects activities of daily living and may result in a significant amount of missed school days. Chronic pain is present for long periods of time and is characterized as mild to severe. Chronic pain has also been described as the pain experienced when the child reports a headache, abdominal pain, back pain, generalized pain or combination of these. Chronic pain can develop from disease or injury and can occur simultaneously with acute pain. Children who experience chronic pain can have psychological effects. Caring for a child in pain may cause distress to the caregiver, may cause costs due to healthcare or lost wages from time off work, and may stop caregivers from leaving the house.\n\nAssessment of pain in children depends on the cooperation and developmental stage of the child. Some children cannot assist in their assessment because they have not matured enough cognitively, emotionally, or physically. The following sections list signs of distress and possible pain in children by age group:\n\n\n\nToddlers show signs of distress and possible pain by:\n\nSchool-age children show signs of distress and possible pain by:\n\nAdolescents show signs of distress and possible pain:\n\nAlthough pain is subjective and can occur in a continuous spectrum of intensities, there are assessment tools that compare pain levels over time. This kind of assessment incorporates pain scales and requires a high enough developmental level for the child to respond to questions. A verbal response is not always necessary to quantify pain.\n\nA pain scale measures a patient's pain intensity and other features. Pain scales can be based on observational (behavioral) or physiological data, as well as self-report. Self-report is considered primary and should be obtained if possible. Pain measurements help determine the severity, type, and duration of pain. They are also used in diagnosis, to determine a treatment plan, and to evaluate the effectiveness of treatment. Pain scales are available for neonates, infants, children, adolescents, adults, seniors, and persons with impaired communication. Pain assessments are often regarded as \"the 5th vital sign\".\n\nThe causes of pain in children are similar to the causes in adults.\n\nPain can be experienced in many ways and depends on the following factors in each child:\n\nClinicians responsible for a child monitor the child frequently in tertiary care centers (hospitals). Pharmacological and non-pharmacological treatments are used to manage the pain. Parents or caregivers are also requested to provide their own pain assessments. At the beginning of pharmacological treatment, clinicians monitor the child for adverse reactions to the medications. The levels of some medications are monitored to ensure that the child is not over-medicated and does not receive toxic levels of any drug. The levels also indicate whether there would be enough drug in the blood to be effective in managing the pain. Medications are metabolized differently between children of the same age. Factors that influence the levels of medications controlling pain include the height, weight, and body surface of the child, as well as any other illnesses. Some medications may have a paradoxical effect in children, which is an effect that is the opposite of the expected effect. Clinicians monitor for this and any other reactions to medication.\n\nPost-procedural treatment in children is primarily prescription opioids. Morphine is effective and relatively safe, and is often used with moderate to severe pain. Codeine and tramadol should be avoided especially in children younger than 12 years old since metabolism varies due to genetic differences between individuals. However, other interventions include medications classified as non-opioid analgesics, which are useful in post surgical treatment. For example, acetaminophen or ibuprofen can be used as a non-opioid analgesics. Unlike acetaminophen, ibuprofen has anti-inflammatory property which can be useful for pain in inflammatory conditions. Aspirin is not used in pediatric population due to its association with Reye's syndrome.\n\nBecause children process information differently from adults, treatment centers for children often use atraumatic measures to reduce anxiety and stress. Examples include:\n\nDepending on the source of pain, there are many non-pharmacological options to be considered. Also, depending on the age of the child, different approaches may be more suitable.\n\nNon-pharmacological methods to manage discomfort during immunizations include putting sugar on a pacifier, comforting the child during and after the injection, chest-to-chest hugging, and letting the child choose the injection site. Other non-pharmacological treatments that have been found to be effective include:\n\n\nNon-pharmacological treatment for pain associated with venipuncture in children includes hypnosis and distraction. These treatments reduced self-reported pain and when combined with cognitive-behavioural therapy (CBT) the reduction of pain was even greater. Other interventions have not been found to be effective and these are suggestion, blowing out air, and distraction with parent coaching did not differ from control for pain and distress.\n\nBreastfeeding during painful procedures has been found to be more effective in controlling pain than placebo or positioning. Breastmilk or 'sugar' water has a similar effect, though studies in preterm infants have yet to be done. Skin-to-skin care (kangaroo care) is thought be effective for pain control during painful procedures.\n\nFor children and adolescents who experience chronic pain- behavioral treatment, relaxation training, cognitive behavioral therapy (CBT) and acupuncture have been proven to be effective for some patients.\n\nAcute pain, chronic pain, neuropathic pain and recurrent pain in children is most often managed with medication. Most of these medications are analgesics. These include acetaminophen, NSAIDs, local anesthetics, opioids, and medications for neuropathic pain. The most effective approach to pain management in children is to provide pain control around the clock instead of providing pain relief as needed. Regional anesthesia is also effective and recommended whenever possible. It is important to use caution when administering opioids to neonates and young infants. There is a higher risk for apnea and hypoventilation in this population, due to their decreased breathing response.\n\nChronic pain is treated with a variety of medications and non-pharmacological interventions. The World Health Organization recommends using a two step treatment approach based on the level of pain in children. The first step explains mild pain treatment, while the second step considers moderate to severe pain. Opioids, such as morphine, is an example of a drug of choice for moderate-severe pain in children with medical illnesses. Some side effects of opioid use can include cognition deficits, dependence, altered mood, and disturbances of endocrine development.\n\nNon-pharmacological treatment for children to help relieve periodic pain includes counseling and behavior modification therapy.The American Association of Pediatrics states that psychological interventions, such as relaxation and cognitive strategies, have strong evidence for pain management.\n\nThe approach to acute pain should take into account the severity of the pain. Non-opioid analgesics, such as acetaminophen and NSAIDs, can be used alone to treat mild pain. For moderate to severe pain, it is optimal to use a combination of multiple agents, including opioid and non-opioid agents. \n\nA panel, including the American Pain Society and American Society of Regional Anesthesia and Pain Medicine, recommends multimodal analgesia, which they define as a combination of pharmacological agents and non-pharmacological techniques to treat postoperative pain. A significant benefit of this technique is that non-opioid analgesics used in combination with opioids can decrease the amount of opioids required and reduce the risk of opioid-related side effects. Medications can be delivered as needed or around-the-clock depending on the patient’s needs. For children, intravenous patient-controlled analgesia (IV-PCA) an be used when parenteral administration is preferred. IV-PCA allows for consistent opioid levels, which can be a better alternative to scheduled intramuscular injections. In addition, studies have shown that children as young as 6 years old can use the IV-PCA correctly.\n\nFor projected moderate to severe pain, analgesics can be used on a regular schedule for the first 36 to 48 hours after a dental procedure. NSAIDs are preferred over other analgesics to treat acute mild to moderate postoperative pain, due to the inflammatory component of dental pain. Alternating between the NSAIDs and acetaminophen in combination is another option. In addition, these two agents are considered equivalent or better than opioids for managing musculoskeletal pain, which includes dental pain.\n\nCancer pain is managed differently in children. Typically, medical history, physical examinations, age and overall health of the child are evaluated. The type of cancer may influence decisions about pain management. The extent of the cancer, and the tolerance of the child to specific medications, procedures or therapies are also taken into account, as well as the preferences of the parent or caregiver. Medications used to treat cancer pain include non-steroidal anti-inflammatory drugs (NSAIDs) and opioids.\n\n\n\nThe frequency of prescribing for these pain medications has more than doubled from 1990 to 2010 with 20-50% of adolescents who complain of headache, back pain, or joint pain receiving a prescribed opioid.\n\nBefore an adolescent or young adult is prescribed opioids, they should be screened for risk factors for opioid drug abuse. Prescription drug monitoring programs (PDMs) are now available in 37 states and 11 states have programs in development.\n\nThe use of opioids can result in a number of complications in children, including respiratory depression and risk for unintended overdose and opioid misuse later in life. These risks can be reduced by the use of NSAIDs (e.g. ibuprofen) and paracetamol. Using NSAIDs and paracetamol can be opioid-sparing, leading to less opioid use in situations such as peri-operative pain management. The efficacy and safety of opioids in children have not been established.\n\nOther side effects of opioids are constipation, fatigue, and disorientation. Children can develop opioid tolerance, where larger doses are needed to have the same effect. Tolerance occurs earlier in children than in adults, especially with prolonged use. When tolerance to opioids develop, it takes a larger dose of the opioid to achieve the same analgesic effect. Non-pharmacological treatments have few side effects.\n\nThe indications that treatment is needed are not always clear for children because of poor assessments and the tendency to undertreat pain..\n\n\n"}
{"id": "409699", "url": "https://en.wikipedia.org/wiki?curid=409699", "title": "Persistent genital arousal disorder", "text": "Persistent genital arousal disorder\n\nPersistent genital arousal disorder (PGAD), previously called persistent sexual arousal syndrome, is spontaneous, persistent, unwanted and uncontrollable genital arousal in the absence of sexual stimulation or sexual desire, and is typically not relieved by orgasm. Instead, multiple orgasms over hours or days may be required for relief.\n\nPGAD occurs in women. A similar disorder in men is called priapism. PGAD is rare and is not well understood. The literature is inconsistent with the nomenclature. It is distinguished from hypersexuality, which is characterized as heightened sexual desire.\n\nIn 2003, \"persistent genital arousal\" was considered for inclusion with regard to the International Consultation on Sexual Medicine (ICSM). In 2009, \"persistent genital arousal dysfunction\" was included in its third edition. PGAD is not included in the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM-5) or the International Classification of Diseases (ICD-10), which may be due to the disorder requiring further research.\n\nThe condition has been characterized by a researcher as being a term with no scientific basis. There is concern that the title may be misleading because, since the genital arousal is unwanted, it is dubious to characterize it as arousal.\n\nOther researchers have suggested that the disorder be renamed \"persistent genital vasocongestion disorder (PGVD)\" or \"restless genital syndrome (ReGS).\"\n\nPhysical arousal caused by PGAD can be very intense and persist for extended periods, days or weeks at a time. Symptoms include pressure, pain, irritation, clitoral tingling, throbbing, vaginal congestion, vaginal contractions, and sometimes spontaneous orgasms. Pressure, discomfort, pounding, pulsating, throbbing or engorgement may include the clitoris, labia, vagina, perineum, or the anus. The symptoms may result from sexual activity or from no identified stimulus, and are not relieved by orgasm except for cases where multiple orgasms over hours or days allow for relief. The symptoms can impede on home or work life. Women may feel embarrassment or shame, and avoid sexual relationships, because of the disorder. Stress can make the symptoms worse.\n\nResearchers do not know the cause of PGAD, but assume that it has neurological, vascular, pharmacological, and psychological causes. Tarlov cysts have been speculated as a cause. PGAD has been associated with clitoral priapism, and is sometimes considered to be the same as priapism in men. It is also similar to vulvodynia, in that the causes for both are not well understood, both last for a long time, and women with either condition may be told that it is psychological rather than physical. It has been additionally associated with restless legs syndrome (RLS), but a minority of women with PGAD have restless legs syndrome.\n\nIn some recorded cases, the syndrome was caused by or can cause a pelvic arterial-venous malformation with arterial branches to the clitoris. Surgical treatment was effective in this instance.\n\nBecause PGAD has only been researched since 2001, there is little documenting what may cure or remedy the disorder. Treatment may include extensive psychotherapy, psycho-education, and pelvic floor physical therapy. In one case, serendipitous relief of symptoms was concluded from treatment with varenicline, a treatment for nicotine addiction.\n\nPGAD is very rare. Although online surveys have indicated that hundreds of women may have PGAD, documented case studies have been limited to about 22. No population data on the disorder exists.\n\nThe earliest references to PGAD may be Greek descriptions of hypersexuality (previously known as \"satyriasis\" and \"nymphomania\"), which confused persistent genital arousal with sexual insatiability. While PGAD involves the absence of sexual desire, hypersexuality is characterized as heightened sexual desire.\n\nThe term \"persistent sexual arousal syndrome\" was coined by researchers Leiblum and Nathan in 2001. In 2006, Leiblum renamed the condition to \"persistent genital arousal disorder\" to indicate that genital arousal sensations are different from those that result from true sexual arousal. The rename was also considered to give the condition a better chance of being classified as a dysfunction.\n\n\n"}
{"id": "92790", "url": "https://en.wikipedia.org/wiki?curid=92790", "title": "Pinga", "text": "Pinga\n\nIn Inuit mythology, Pinga (\"the one who is [up on] high\") was a goddess of the hunt, fertility and medicine. She was also the psychopomp, bringing souls of the newly dead to Adlivun, the underworld. \n"}
{"id": "24008878", "url": "https://en.wikipedia.org/wiki?curid=24008878", "title": "Public health insurance option", "text": "Public health insurance option\n\nThe public health insurance option, also known as the public insurance option or the public option, is a proposal to create a government-run health insurance agency that would compete with other private health insurance companies within the United States. The public option is not the same as publicly funded health care, but was proposed as an alternative health insurance plan offered by the government. The public option was initially proposed for the Patient Protection and Affordable Care Act, but was removed after Sen. Joe Lieberman (I-CT) threatened a filibuster.\n\nThe public option was featured in three bills considered by the United States House of Representatives in 2009: the proposed Affordable Health Care for America Act (), which was passed by the House in 2009, its predecessor, the proposed America's Affordable Health Choices Act (), and a third bill, the Public Option Act, also referred to as the \"Medicare You Can Buy Into Act\", (). In the first two bills, the public option took the form of a Qualified Health Benefit Plan competing with similar private insurance plans in an internet-based exchange or marketplace, enabling citizens and small businesses to purchase health insurance meeting a minimum federal standard. The Public Option Act, in contrast, would have allowed all citizens and permanent residents to buy into a public option by participating in the public Medicare program. Individuals covered by other employer plans or by state insurance plans such as Medicare would have not been eligible to obtain coverage from the exchange. The federal government's health insurance plan would have been financed entirely by premiums without subsidy from the Federal government, although some plans called for government seed money to get the programs started.\n\nPresident Barack Obama promoted the idea of the public option while running for election in 2008. Following his election, Obama downplayed the need for a public health insurance option, including calling it a \"sliver\" of health care reform, but still campaigned for the option up until the health care reform was passed.\n\nUltimately, the public option was removed from the final bill. While the United States House of Representatives passed a public option in their version of the bill, the public option was voted down in the Senate Finance Committee and the public option was never included in the final Senate bill, instead opting for state-directed health insurance exchanges. Critics of the removal of the public option accused President Obama of making an agreement to drop the public option from the final plan, but the record showed that the agreement was based on vote counts rather than backroom deals, as substantiated by the final vote in the Senate.\n\nIn January 2013, Representative Jan Schakowsky and 44 other U.S. House of Representatives Democrats introduced , the \"Public Option Deficit Reduction Act\", which would amend the Affordable Care Act to create a public option. The bill would set up a government-run health insurance plan with premiums 5% to 7% percent lower than private insurance. The Congressional Budget Office estimated it would reduce the United States public debt by $104 billion over 10 years. Representative Schakowsky reintroduced the bill as in January 2015, where it gained 35 cosponsors.\n\nIn the run-up to the 2016 Democratic National Convention, the Democratic Platform Committee approved a plank supporting the addition of a public option onto the Affordable Care Act. The decision was seen as a compromise measure between the Hillary Clinton campaign who during the 2016 presidential primaries advocated for keeping and reforming the ACA, and the Bernie Sanders campaign who advocated for repealing and replacing the ACA with a single-payer Medicare for All program. The Clinton campaign stated shortly before the plank was added that as president Clinton would \"pursue efforts to give Americans in every state in the country the choice of a public-option insurance plan\", while Bernie Sanders applauded the decision to \"see that all Americans have the right to choose a public option in their health care exchange, which will lower the cost of healthcare\". The call was echoed by President Obama, who in an article for the American Medical Association stated that Congress \"should revisit a public plan to compete alongside private insurers in areas of the country where competition is limited.\"\n\nThe purpose behind the public option was to make more affordable health insurance for uninsured citizens who are either unable to afford the rates of or are rejected by private health insurers. Supporters argued that a government insurance company could successfully lower its rates by using greater leverage than private industry when negotiating with hospitals and doctors, as well as paying the employees of the public option insurance company salaries as opposed to paying based on individual medical procedures.\n\nSupporters of a public plan, such as \"Washington Post\" columnist E. J. Dionne, argue that many places in the United States have monopolies in which one company, or a small set of companies, control the local market for health insurance. \"Economist\" and \"New York Times\" columnist Paul Krugman also wrote that local insurance monopolies exist in many of the smaller states, accusing those who oppose the idea of a public insurance plan as defenders of local monopolies. He also argued that traditional ideas of beneficial market competition do not apply to the insurance industry given that insurers mainly compete by risk selection, claiming that \"[t]he most successful companies are those that do the best job of denying coverage to those who need it most.\"\n\nEconomist and former US Secretary of Labor Robert Reich argued that only a \"big, national, public option\" can force insurance companies to cooperate, share information, and reduce costs while accusing insurance and pharmaceutical companies of leading the campaign against the public option.\n\nMany Democratic politicians were publicly in favor of the public option for a variety of reasons. President Obama continued campaigning for the public option during the debate. In a public rally in Cincinnati on September 7, 2009, President Obama said: \"I continue to believe that a public option within the basket of insurance choices would help improve quality and bring down costs.\" The President also addressed a Joint Session of Congress on September 9, 2009, reiterating his call for a public insurance option, saying that he had \"no interest in putting insurance companies out of business\" while saying that the public option would \"have to be self-sufficient\" and succeed by reducing overhead costs and profit motives. Democratic Representative Sheila Jackson-Lee, who represents the 18th congressional district in Houston, believed that a \"vigorous public option\" would be included in the final bill and would \"benefit the state of Texas.\"\n\nThe final bill, the Patient Protection and Affordable Care Act, included provisions to open health insurance exchanges in each state by October 1, 2013. As the Act requires Americans to purchase health insurance, the federal government will offer subsidies to Americans with income levels up to four times the federal poverty level.\n\nAn alternative proposal is to subsidize private, non-profit health insurance cooperatives to get them to become large and established enough to possibly provide cost savings Democratic politicians such as Howard Dean were critical of abandoning a public option in favor of co-ops, raising questions about the ability of the cooperatives to compete with existing private insurers. Paul Krugman also questioned the ability of cooperatives to compete.\n\nWhile politically difficult, some politicians and observers have argued for a single-payer system. A bill, the United States National Health Care Act, was first proposed by Representative John Conyers in 2003 and has been perennially proposed since, including during the debate on the public option and the Patient Protection and Affordable Care Act. President Obama has come out against a single-payer reform at this time, stating in the joint session of Congress that \"it makes more sense to build on what works and fix what doesn't, rather than try to build an entirely new system from scratch.\" Obama had previously expressed that he is a proponent of a single payer universal health care program during an AFL-CIO conference in 2003.\n\nA number of alternatives to the public option were proposed in the Senate. Instead of creating a network of statewide public plans, Senator Olympia Snowe proposed a \"trigger\" in which a plan would be put into place at some point in the future in states that do not have more than a certain number of private insurance competitors. Senator Tom Carper has proposed an \"opt-in\" system in which state governments choose for themselves whether or not to institute a public plan. Senator Chuck Schumer has proposed an \"opt-out\" system in which state governments would initially be part of the network but could choose to avoid offering a public plan.\n\nIn January 2013, Representative Jan Schakowsky and 44 other U.S. House of Representatives Democrats introduced , the \"Public Option Deficit Reduction Act\", which would amend the 2010 Affordable Care Act to create a public option. The bill would set up a government-run health insurance plan with premiums 5% to 7% percent lower than private insurance, with the Congressional Budget Office estimating a reduction in the United States public debt by $104 billion over 10 years.\n\nBoth before and after passage in the House, significant controversy surrounded the Stupak–Pitts Amendment, added to the bill to prohibit coverage of abortions – with limited exceptions – in the public option or in any of the health insurance exchange's private plans sold to customers receiving federal subsidies. In mid-November, it was reported that 40 House Democrats would not support a final bill containing the Amendment's provisions. The Amendment was abandoned after a deal was struck between Representative Bart Stupak and his voting bloc would vote for the bill as written in exchange for the signing of Executive Order 13535.\n\nRepublican House Minority Whip Eric Cantor has argued that a public plan would compete unfairly with private insurers and drive many of them out of business.\n\nMichael F. Cannon, a senior fellow of the libertarian CATO Institute, has argued that the federal government can hide inefficiencies in its administration and draw away consumers from private insurance even if the government offers an inferior product. A study by the Congressional Budget Office found that profits accounted for only about 4 or 5 percent of private health insurance premiums, and Cannon argued that the lack of a profit motive reduces incentives to eliminate wasteful administrative costs.\n\nDr. Robert E. Moffit of the Heritage Foundation has argued that a public plan in competition in private plans would likely be used as a \"dumping ground\" for families and individuals with higher than average health risks. This, in his view, would lead to costs that business should pay being passed onto the taxpayer.\n\nMarcia Angell, M. D., Senior Lecturer in the Department of Social Medicine at Harvard Medical School and former Editor-in-Chief of the \"New England Journal of Medicine\", believes that the result of a public option would be more \"under-55's\" opting to pay the fine rather than purchase insurance under a public option scenario, instead advocating lowering the Medicare age to 55.\n\nThe chief executive of Aetna, Ron Williams, argued against the public option based on issues of fairness. On the \"News Hour with Jim Lehrer\", Williams noted that a public option creates a situation where \"you have in essence a player in the industry who is a participant in the market, but also is a regulator and a referee in the game\". He said, \"we think that those two roles really don't work well.\"\n\nPublic polling consistently showed majority support for a public option. A July 2009 survey by the Quinnipiac University Polling Institute found that 28% of Americans would like to purchase a public plan while 53% would prefer to have a private plan. It also stated that 69% would support its creation in the first place. Survey USA estimated that the majority of Americans (77%) feel that it is either \"Quite Important\" or \"Extremely Important\" to \"give people a choice of both a public plan administered by the federal government and a private plan for their health insurance\" in August 2009. A Rasmussen Reports poll taken on August 17–18 stated that 57% of Americans did not support the current health care bill being considered by Congress that did not include a public option, a change from their findings in July 2009. A NBC News/\"Wall Street Journal\" poll, conducted August 15–17, found that 47% of Americans opposed the idea of a public option and 43% expressed support. A Pew Research Center report published on October 8, 2009 stated that 55% of Americans favor a government health insurance plan to compete with private plans. The results were very similar to their polling from July, which found 52% support. An October 2009 \"Washington Post\"/ABC poll showed 57% support, a \"USA Today\"/Gallup survey described by a \"USA Today\" article on October 27 found that 50% of Americans supported a government plan proposal, and a poll from November 10 and 11 by Angus Reid Public Opinion found that 52% of Americans supported a public plan. On October 27, journalist Ray Suarez of \"The News Hour with Jim Lehrer\" noted that \"public opinion researchers say the tide has been shifting over the last several weeks, and now is not spectacularly, but solidly in favor of a public option.\"\n\nBetween October 28 and November 13, 2009, Democratic Senator Dick Durbin's campaign organization polled Americans to rank their support for various forms of the \"public option\" currently under consideration by Congress for inclusion in the final health care reform bill. The 83,954 respondents assigned rankings of 0 to 10. A full national option had the most support, with an 8.56 average, while no public option was least favored, with a 1.10 average.\n\nA survey designed and conducted by Drs. Salomeh Keyhani and Alex Federman of Mount Sinai School of Medicine done over the summer of 2009 found that 73% of doctors supported a public option. A survey reported by the \"New England Journal of Medicine\" in September, based on a random sample of 6,000 physicians from the American Medical Association, stated that \"it seems clear that the majority of U.S. physicians support using both public and private insurance options to expand coverage.\"\n\nConversely, an IBD/TIPP poll of 1,376 physicians showed that 45% of doctors \"would consider leaving or taking early retirement\" if Congress passes the health care plan wanted by the White House and Democrats. This poll also found that 65% of physicians oppose the White House and Democratic version of health reform. Statistician and polling expert Nate Silver has criticized that IBD/TIPP poll for what he calls its unusual methodology and bias and for the fact that it was incomplete when published as responses were still coming in.\n\n"}
{"id": "4350222", "url": "https://en.wikipedia.org/wiki?curid=4350222", "title": "Radiation Protection Convention, 1960", "text": "Radiation Protection Convention, 1960\n\nRadiation Protection Convention, 1960 is an International Labour Organization Convention to restrict workers from exposure of ionising radiation and to prohibit persons under 16 engaging in work that causes such exposure. (Article 6)\n\nIt was established in 1960, with the preamble stating:\nHaving decided upon the adoption of certain proposals with regard to the protection of workers against ionising radiations...\n\nArticle 2. This Convention applies to all activities involving exposure of workers to ionising radiation in the course of their work.\n\nArticle 5. Every effort shall be made to restrict the exposure of workers to ionising radiation to the lowest protectable level.\n\nArticle 12 imposes undergoing further medical examinations at appropriate intervals, and Article 13 imposes the employer shall take any necessary remedial action on the basis of the technical findings and the medical advice.\n\nAs of July 2013, the convention has been ratified by 50 states.\n\n"}
{"id": "412735", "url": "https://en.wikipedia.org/wiki?curid=412735", "title": "Rheumatic fever", "text": "Rheumatic fever\n\nRheumatic fever (RF) is an inflammatory disease that can involve the heart, joints, skin, and brain. The disease typically develops two to four weeks after a streptococcal throat infection. Signs and symptoms include fever, multiple painful joints, involuntary muscle movements, and occasionally a characteristic non-itchy rash known as erythema marginatum. The heart is involved in about half of the cases. Damage to the heart valves, known as rheumatic heart disease (RHD), usually occurs after repeated attacks but can sometimes occur after one. The damaged valves may result in heart failure, atrial fibrillation and infection of the valves.\nRheumatic fever may occur following an infection of the throat by the bacterium \"Streptococcus pyogenes\". If the infection is untreated rheumatic fever can occur in up to three percent of people. The underlying mechanism is believed to involve the production of antibodies against a person's own tissues. Due to their genetics, some people are more likely to get the disease when exposed to the bacteria than others. Other risk factors include malnutrition and poverty. Diagnosis of RF is often based on the presence of signs and symptoms in combination with evidence of a recent streptococcal infection.\nTreating people who have strep throat with antibiotics, such as penicillin, decreases the risk of developing rheumatic fever. In order to avoid antibiotic misuse this often involves testing people with sore throats for the infection, which may not be available in the developing world. Other preventive measures include improved sanitation. In those with rheumatic fever and rheumatic heart disease, prolonged periods of antibiotics are sometimes recommended. Gradual return to normal activities may occur following an attack. Once RHD develops, treatment is more difficult. Occasionally valve replacement surgery or valve repair is required. Otherwise complications are treated as per normal.\nRheumatic fever occurs in about 325,000 children each year and about 33.4 million people currently have rheumatic heart disease. Those who develop RF are most often between the ages of 5 and 14, with 20% of first-time attacks occurring in adults. The disease is most common in the developing world and among indigenous peoples in the developed world. In 2015 it resulted in 319,400 deaths down from 374,000 deaths in 1990. Most deaths occur in the developing world where as many as 12.5% of people affected may die each year. Descriptions of the condition are believed to date back to at least the 5th century BCE in the writings of Hippocrates. The disease is so named because its symptoms are similar to those of some rheumatic disorders.\n\nThe disease typically develops two to four weeks after a throat infection. Symptoms include: fever, painful joints with those joints affected changing with time, involuntary muscle movements, and occasionally a characteristic non-itchy rash known as erythema marginatum. The heart is involved in about half of the cases. Damage to the heart valves usually occurs only after multiple attacks but may occasionally occur after a single case of RF. The damaged valves may result in heart failure and also increase the risk of atrial fibrillation and infection of the valves.\n\nRheumatic fever is a systemic disease affecting the connective tissue around arterioles, and can occur after an untreated strep throat infection, specifically due to group A streptococcus (GAS), \"Streptococcus pyogenes\". It is believed to be caused by antibody cross-reactivity. This cross-reactivity is a type II hypersensitivity reaction and is termed molecular mimicry. Usually, self reactive B cells remain anergic in the periphery without T cell co-stimulation. During a streptococcal infection, mature antigen-presenting cells such as B cells present the bacterial antigen to CD4+T cells which differentiate into helper T cells. Helper T cells subsequently activate the B cells to become plasma cells and induce the production of antibodies against the cell wall of Streptococcus. However the antibodies may also react against the myocardium and joints, producing the symptoms of rheumatic fever. \"S. pyogenes\" is a species of aerobic, cocci, gram-positive bacteria that are non-motile, non-spore forming, and forms chains and large colonies.\n\n\"S. pyogenes\" has a cell wall composed of branched polymers which sometimes contain M protein, a virulence factor that is highly antigenic. The antibodies which the immune system generates against the M protein may cross-react with heart muscle cell protein myosin, heart muscle glycogen and smooth muscle cells of arteries, inducing cytokine release and tissue destruction. However, the only proven cross-reaction is with perivascular connective tissue. This inflammation occurs through direct attachment of complement and Fc receptor-mediated recruitment of neutrophils and macrophages. Characteristic Aschoff bodies, composed of swollen eosinophilic collagen surrounded by lymphocytes and macrophages can be seen on light microscopy. The larger macrophages may become Anitschkow cells or Aschoff giant cells. Rheumatic valvular lesions may also involve a cell-mediated immunity reaction as these lesions predominantly contain T-helper cells and macrophages.\n\nIn rheumatic fever, these lesions can be found in any layer of the heart causing different types of carditis. The inflammation may cause a serofibrinous pericardial exudate described as \"bread-and-butter\" pericarditis, which usually resolves without sequelae. Involvement of the endocardium typically results in fibrinoid necrosis and wart formation along the lines of closure of the left-sided heart valves. Warty projections arise from the deposition, while subendocardial lesions may induce irregular thickenings called MacCallum plaques.\n\nChronic rheumatic heart disease (RHD) is characterized by repeated inflammation with fibrinous repair. The cardinal anatomic changes of the valve include leaflet thickening, commissural fusion, and shortening and thickening of the tendinous cords. It is caused by an autoimmune reaction to Group A β-hemolytic streptococci (GAS) that results in valvular damage. Fibrosis and scarring of valve leaflets, commissures and cusps leads to abnormalities that can result in valve stenosis or regurgitation. The inflammation caused by rheumatic fever, usually during childhood, is referred to as rheumatic valvulitis. About half of patients with rheumatic fever develop inflammation involving valvular endothelium. The majority of morbidity and mortality associated with rheumatic fever is caused by its destructive effects on cardiac valve tissue. The pathogenesis of RHD is complex and not fully understood, but it is known to involve molecular mimicry and genetic predisposition that lead to autoimmune reactions.\n\nMolecular mimicry occurs when epitopes are shared between host antigens and \"Streptococcus\" antigens. This causes an autoimmune reaction against native tissues in the heart that are incorrectly recognized as \"foreign\" due to the cross-reactivity of antibodies generated as a result of epitope sharing. The valvular endothelium is a prominent site of lymphocyte-induced damage. CD4+ T cells are the major effectors of heart tissue autoimmune reactions in RHD. Normally, T cell activation is triggered by the presentation of bacterial antigens. In RHD, molecular mimicry results in incorrect T cell activation, and these T lymphocytes can go on to activate B cells, which will begin to produce self-antigen-specific antibodies. This leads to an immune response attack mounted against tissues in the heart that have been misidentified as pathogens. Rheumatic valves display increased expression of VCAM-1, a protein that mediates the adhesion of lymphocytes. Self-antigen-specific antibodies generated via molecular mimicry between human proteins and streptococcal antigens up-regulate VCAM-1 after binding to the valvular endothelium. This leads to the inflammation and valve scarring observed in rheumatic valvulitis, mainly due to CD4+ T cell infiltration.\n\nWhile the mechanisms of genetic predisposition remain unclear, a few genetic factors have been found to increase susceptibility to autoimmune reactions in RHD. The dominant contributors are a component of MHC class II molecules, found on lymphocytes and antigen-presenting cells, specifically the DR and DQ alleles on human chromosome 6. Certain allele combinations appear to increase RHD autoimmune susceptibility. Human leukocyte antigen (HLA) class II allele DR7 (HLA-DR7) is most often associated with RHD, and its combination with certain DQ alleles is seemingly associated with the development of valvular lesions. The mechanism by which MHC class II molecules increase a host's susceptibility to autoimmune reactions in RHD is unknown, but it is likely related to the role HLA molecules play in presenting antigens to T cell receptors, thus triggering an immune response. Also found on human chromosome 6 is the cytokine TNF-α which is also associated with RHD. High expression levels of TNF-α may exacerbate valvular tissue inflammation, contributing to RHD pathogenesis. Mannose-binding lectin (MBL) is an inflammatory protein involved in pathogen recognition. Different variants of MBL2 gene regions are associated in RHD. RHD-induced mitral valve stenosis has been associated with MBL2 alleles encoding for high production of MBL. Aortic valve regurgitation in RHD patients has been associated with different MBL2 alleles that encode for low production of MBL. Other genes are also being investigated to better understand the complexity of autoimmune reactions that occur in RHD.\n\nModified Jones criteria were first published in 1944 by T. Duckett Jones, MD. They have been periodically revised by the American Heart Association in collaboration with other groups. According to revised Jones criteria, the diagnosis of rheumatic fever can be made when two of the major criteria, or one major criterion plus two minor criteria, are present along with evidence of streptococcal infection: elevated or rising antistreptolysin O titre or DNAase. Exceptions are chorea and indolent carditis, each of which by itself can indicate rheumatic fever. An April 2013 review article in the \"Indian Journal of Medical Research\" stated that echocardiographic and Doppler (E & D) studies, despite some reservations about their utility, have identified a massive burden of rheumatic heart disease, which suggests the inadequacy of the 1992 Jones' criteria. E & D studies have identified subclinical carditis in patients with rheumatic fever, as well as in follow-ups of rheumatic heart disease patients who initially presented as having isolated cases of Sydenham's chorea. Signs of a preceding streptococcal infection include: recent scarlet fever, raised antistreptolysin O or other streptococcal antibody titre, or positive throat culture.\n\n\n\nRheumatic fever can be prevented by effectively treating strep throat with antibiotics.\n\nIn those who have previously had rheumatic fever, antibiotics in a preventative manner are occasionally recommended. As of 2017 the evidence to support long term antibiotics in those with underlying disease is poor.\n\nThe American Heart Association suggests that dental health be maintained, and that people with a history of bacterial endocarditis, a heart transplant, artificial heart valves, or \"some types of congenital heart defects\" may wish to consider long-term antibiotic prophylaxis.\n\nThe management of rheumatic fever is geared toward the reduction of inflammation with anti-inflammatory medications such as aspirin or corticosteroids. Individuals with positive cultures for strep throat should also be treated with antibiotics.\n\nAspirin is the drug of choice and should be given at high doses.\n\nOne should watch for side effects like gastritis and salicylate poisoning. In children and teenagers, the use of aspirin and aspirin-containing products can be associated with Reye's syndrome, a serious and potentially deadly condition. The risks, benefits, and alternative treatments must always be considered when administering aspirin and aspirin-containing products in children and teenagers. Ibuprofen for pain and discomfort and corticosteroids for moderate to severe inflammatory reactions manifested by rheumatic fever should be considered in children and teenagers.\n\nNo vaccines are currently available to protect against \"S. pyogenes\" infection, although research is underway to develop one. Difficulties in developing a vaccine include the wide variety of strains of \"S. pyogenes\" present in the environment and the large amount of time and people that will be needed for appropriate trials for safety and efficacy of the vaccine.\n\nPeople with positive cultures for \"Streptococcus pyogenes\" should be treated with penicillin as long as allergy is not present. The use of antibiotics will not alter cardiac involvement in the development of rheumatic fever. Some suggest the use of benzathine benzylpenicillin.\n\nMonthly injections of long-acting penicillin must be given for a period of five years in patients having one attack of rheumatic fever. If there is evidence of carditis, the length of therapy may be up to 40 years.\nAnother important cornerstone in treating rheumatic fever includes the continual use of low-dose antibiotics (such as penicillin, sulfadiazine, or erythromycin) to prevent recurrence.\n\nWhile corticosteroids are often used, evidence to support this is poor. Salicylates are useful for pain.\n\nSteroids are reserved for cases where there is evidence of an involvement of the heart. The use of steroids may prevent further scarring of tissue and may prevent the development of sequelae such as mitral stenosis.\n\nSome patients develop significant carditis which manifests as congestive heart failure. This requires the usual treatment for heart failure: ACE inhibitors, diuretics, beta blockers, and digoxin. Unlike typical heart failure, rheumatic heart failure responds well to corticosteroids.\n\nAbout 33 million people are affected by rheumatic heart disease with an additional 47 million having asymptomatic damage to their heart valves. As of 2010 globally it resulted in 345,000 deaths, down from 463,000 in 1990.\n\nIn Western countries, rheumatic fever has become fairly rare since the 1960s, probably due to the widespread use of antibiotics to treat streptococcus infections. While it has been far less common in the United States since the beginning of the 20th century, there have been a few outbreaks since the 1980s. The disease is most common among Indigenous Australians (particularly in central and northern Australia), Māori, and Pacific Islanders, and is also common in Sub-Saharan Africa, Latin America, the Indian Subcontinent, the Middle East, and North Africa.\n\nRheumatic fever primarily affects children between ages 5 and 17 years and occurs approximately 20 days after strep throat. In up to a third of cases, the underlying strep infection may not have caused any symptoms.\n\nThe rate of development of rheumatic fever in individuals with untreated strep infection is estimated to be 3%. The incidence of recurrence with a subsequent untreated infection is substantially greater (about 50%). The rate of development is far lower in individuals who have received antibiotic treatment. Persons who have suffered a case of rheumatic fever have a tendency to develop flare-ups with repeated strep infections.\n\nThe recurrence of rheumatic fever is relatively common in the absence of maintenance of low dose antibiotics, especially during the first three to five years after the first episode. Recurrent bouts of rheumatic fever can lead to valvular heart disease. Heart complications may be long-term and severe, particularly if valves are involved. In countries in Southeast-Asia, sub-saharan Africa, and Oceania, the percentage of people with rheumatic heart disease detected by listening to the heart was 2.9 per 1000 children and by echocardiography it was 12.9 per 1000 children.\n\n"}
{"id": "46382018", "url": "https://en.wikipedia.org/wiki?curid=46382018", "title": "Richard James Arthur Berry", "text": "Richard James Arthur Berry\n\nProfessor Richard James Arthur Berry FRSE FRCSE (1867–1962) was a British-born surgeon and anatomist who was well-known in Australia. He was author of several internationally recognised books in his field.\n\nBerry was born on 30 May 1867, in Upholland in Lancashire, the son of Jane Barlow and James Berry, a coal-merchant. His father died before he was born and he was largely raised by his grandfather. He was educated at small private schools in Southport, before winning a place at the University of Cambridge. However, he did not take up his place at the university, instead taking an apprenticeship with a firm of shipbrokers in Liverpool. In May 1886 he entered the University of Edinburgh to study medicine, graduating with an MBChM in 1891.\n\nBerry then took up a role of House Surgeon under Thomas Annandale at Edinburgh Royal Infirmary on Lauriston Place. In the same year Berry was elected President of the Royal Medical Society of Edinburgh. On receipt of his MD in 1894 he had written a prize-winning thesis on the Vermiform appendix.\n\nIn 1895 Berry was elected a Fellow of the Royal College of Surgeons of Edinburgh and the following year began to lecture in anatomy at the University of Edinburgh. In 1897 he was elected a Fellow of the Royal Society of Edinburgh.\n\nIn December 1905 Berry was accepted for a role as Professor of Anatomy at Melbourne University and travelled over with his wife in February 1906 to replace Sir Harry Brookes Allen in his role of Head of Anatomy. The style of teaching was revolutionised by Berry. He taught until 1929. He also served as Honorary Psychiatrist at Melbourne Children’s Hospital.After settling into his new role he became interested in studies of the skulls of the aboriginals. His collection of skulls and bones was rediscovered in 2003. From this he developed a further interest in the skulls of mentally deficient children. He was a proponent of eugenics, supporting the killing of \"the grosser types of our mental defectives\".\n\nIn 1923 a new Anatomy Department was opened at Melbourne University, and was nicknamed ‘’Berry’s Folly’’’ on account of what was thought to be its over-size, but this proved to be prudent foresight once class sizes swelled after the Second World War.\n\nFrom 1925 to 1929 Berry was the Dean of the Faculty. He strongly advocated a closer physical relationship between the university and the hospital. However this met with opposition from Sir James Barrett. However, in 1927 he toured hospitals of North America with Sir Stanley Argyle the Premier of Victoria, and this ultimately led to the plan being adopted.\n\nIn 1929 Berry unexpectedly resigned and returned to Britain to take up the role as Head of Medical Services at Stoke Park Mental Hospital near Bristol in England. He also then took chairmanship of the Burden Mental Research Trust. He represented Queensland and New South Wales in his membership of the council of the British Medical Association. He continued studies into mental deficiency until 1940.\n\nIn 1959, Sir William Upjohn persuaded Melbourne University to grant Berry the title of Professor emeritus, and Berry, by then virtually blind, returned to receive this honour. \n\nBerry died on 30 September 1962 at Clifton, Bristol.\n\nOn 7 August 1900 Berry married Beatrice Catherine Brighouse (d.1949), daughter of Sir Samuel Brighouse, whom he had met through his hobby of cycling and mountain climbing. His daughter Beatrice married Professor Ian Maxwell of Melbourne.\n\nBerry's portrait, by Justus Jorgensen, hangs in the Anatomy Department of Melbourne University. The building housing the anatomy department at Melbourne University was named the Richard Berry Building in his honour. The building was later used by the Melbourne's mathematics department. Following protests about Berry's racist and eugenicist views, the name of the building was changed in March 2017 to honour recently deceased mathematician Peter Hall instead.\n\n"}
{"id": "21688801", "url": "https://en.wikipedia.org/wiki?curid=21688801", "title": "Roger Birkman", "text": "Roger Birkman\n\nRoger Winfred Birkman (February 1, 1919 – March 26, 2014) was an American organizational psychologist. He was the creator of The Birkman Method, a workplace psychological assessment. Birkman received his Ph.D. in psychology in 1961 from the University of Texas at Austin. He was the founder and Chairman of the Board of Birkman International, Inc.\n\nBirkman began his studies in psychology at the University of Houston before enlisting in the United States Army Air Corps and becoming a B-17 bomber pilot. He became interested in the exploration of individual psychological differences while serving during World War II. His experiences with the impact that perceptions, and misperceptions, among his crew and fellow pilots had on performance led him to the study of social psychology. He began developing The Birkman Method in the late 1940s while working with a group of scientists at the University of Houston surveying psychological instruments for pilot selection by the Air Force.\n\nBirkman sought to create a single instrument that would measure self-concepts, social expectations, stress behaviors, and occupational interests valuable to both work and life. By 1951, he had completed his first iteration called the \"Test of Social Comprehension.\" A self-report questionnaire eliciting responses about perception of self, social context, and occupational opportunities. Scales were empirically developed by comparing self-report item results with descriptions of likes, dislikes and behaviors provided by third parties. The Birkman Method took its final form in his 1961 doctoral dissertation. His work aligns with the social psychology theories of the organizational psychologist Kurt Lewin.\n\nBirkman authored two books: \"True Colors\" (1995) and \"A Man of Understanding\" (2002). He was certified as a Licensed Psychologist in the State of Texas and was a member of the American Psychological Association, the Southwestern Psychological Association, Society for Industrial & Organizational Psychology, and the Texas Psychological Association.\n\nHe died in his sleep on March 26, 2014 at the age of 95.\n\nThe Birkman Method is an online personality, social perception, and occupational interest assessment consisting of ten scales describing occupational preferences (Interests), 11 scales describing “effective behaviors” (Usual behaviors) and 11 scales describing interpersonal and environmental expectations (Needs or Expectations). A corresponding set of 11 scale values was derived to describe \"less than effective\" behaviors (Stress behaviors). Occupational profiling consists of 22 job families with more than 200 corresponding job titles all connected to O*Net.\n\nThe construction and comparative analysis of the Birkman Method is designed to provide insight into what specifically drives a person’s behavior, with the goal of creating greater choice and more self-responsibility. It attempts to measure social behaviors, underlying expectations of interpersonal and task actions, potential stress reactions to unmet expectations, occupational preferences and organizational strengths. It is empirically supported by reliability and validity studies, including studies using classical test theory (CTT) and item response theory (IRT). The Birkman Method has 3 different types of assessments available.\n\n\n\n"}
{"id": "7786459", "url": "https://en.wikipedia.org/wiki?curid=7786459", "title": "Salman Akhtar", "text": "Salman Akhtar\n\nSalman Akhtar (born 31 July 1946, Uttar Pradesh) is a psychoanalyst practicing in the United States. He is an author and Professor of Psychiatry and Human Behavior at Jefferson Medical College in Philadelphia. He was born into a Muslim family in Khairabad, Uttar Pradesh to Jan Nisar Akhtar, a Bollywood film songwriter and Urdu poet, and singer Safia Akhtar, a teacher and writer. He is the brother veteran poet and film lyricist Javed Akhtar. His son Kabir Akhtar is an American television director and Emmy-nominated editor.\n\nAfter receiving his M.D. at Aligarh Muslim University's Medical School in India, he did his internship at Maulana Azad Medical College of the University of Delhi in India. He moved to the United States in 1973 and did his residency and psychiatric training at the University of Virginia Medical Center, and then obtained psychoanalytic training from the Philadelphia Psychoanalytic Institute. Currently, he is Professor of Psychiatry & Human Behavior at Jefferson Medical College and psychiatrist an at the Jefferson University Hospital as well as a Training and Supervising Analyst at the Psychoanalytic Center of Philadelphia. He has served on the editorial boards of the \"International Journal of Psychoanalysis\" and the \"Journal of the American Psychoanalytic Association\". His more than 300 publications, 12 of which are A+ (or 4%), include 13 books. He has also served as the Film Review Editor for the \"International Journal of Psychoanalysis\", and is currently serving as the Book Review Editor for the \"International Journal of Applied Psychoanalytic Studies\". He also has published seven collections of poetry and serves as a Scholar-in-Residence at the Inter-Act Theatre Company in Philadelphia.\n\n"}
{"id": "23819678", "url": "https://en.wikipedia.org/wiki?curid=23819678", "title": "Shipping holiday", "text": "Shipping holiday\n\nIn United States agriculture, a shipping holiday is a fruit and vegetable marketing order feature that prohibits the commercial shipping of the regulated commodity during periods following certain holidays when demand is historically low, such as the several days after Thanksgiving and Christmas.\n"}
{"id": "307128", "url": "https://en.wikipedia.org/wiki?curid=307128", "title": "Shiroka Polyana", "text": "Shiroka Polyana\n\nShiroka Polyana () is a reservoir lake situated in the Western Rhodopes mountains in Bulgaria.\n\nThe lake is situated 30 km south of Batak, on the road to Dospat. It is located 1500 meters above sea level amidst a forest of old pine trees. Different legends are told about the lake.\nThe shape of Shiroka Poliana reservoir is unique. Viewed from the ring-road it seems that the lake is composed of numerous small lakes with separate walls. This illusion is due to the indented relief of the lake bed that consists of several mountain gullies.\n\nAlthough the banks of the lake are far from the road and comparatively hard to access, it attracts many visitors and sports fishermen because of the abundance of fish, including grey mullet, European perch, and trout.\n"}
{"id": "747279", "url": "https://en.wikipedia.org/wiki?curid=747279", "title": "Snellen chart", "text": "Snellen chart\n\nA Snellen chart is an eye chart that can be used to measure visual acuity. Snellen charts are named after the Dutch ophthalmologist Herman Snellen, who developed the chart in 1862. Many ophthalmologists and vision scientists now use an improved chart known as the LogMAR chart.\n\nSnellen developed charts using symbols based in a 5×5 unit grid. The experimental charts developed in 1861 used abstract symbols. Snellen's charts published in 1862 used alphanumeric capitals in the 5×5 grid. The original chart shows A, C, E, G, L, N, P, R, T, 5, V, Z, B, D, 4, F, H, K, O, S, 3, U, Y, A, C, E, G, L, 2.\n\nThe normal Snellen chart is printed with eleven lines of block letters. The first line consists of one very large letter, which may be one of several letters, for example E, H, or N. Subsequent rows have increasing numbers of letters that decrease in size. A person taking the test covers one eye from 6 metres or 20 feet away, and reads aloud the letters of each row, beginning at the top. The smallest row that can be read accurately indicates the visual acuity in that specific eye.\nThe symbols on an acuity chart are formally known as \"optotypes\".\nIn the case of the traditional Snellen chart, the optotypes have the appearance of block letters, and are intended to be seen and read as letters. They are not, however, letters from any ordinary typographer's font. They have a particular, simple geometry in which:\nOnly the ten letters C, D, E, F, L, N, O, P, T, Z are used in the common Snellen chart. The perception of five out of six letters (or similar ratio) is judged to be the Snellen fraction.\nWall-mounted Snellen charts are inexpensive and are sometimes used for approximate assessment of vision, e.g. in a primary-care physician's office. Whenever acuity must be assessed carefully (as in an eye doctor's examination), or where there is a possibility that the examinee might attempt to deceive the examiner (as in a motor vehicle license office), equipment is used that can present the letters in a variety of randomized patterns.\nBS 4274-1:1968(British Standards Institution) \"Specification for test charts for determining distance visual acuity\" was replaced by BS 4274-1:2003 \"Test charts for clinical determination of distance visual acuity — Specification\". It states that \"the luminance of the presentation shall be uniform and not less than 120 cd/m. Any variation across the test chart shall not exceed 20 %.\"\nAccording to BS 4274-1:2003 only the letters C, D, E, F, H, K, N, P, R, U, V, and Z should be used for the testing of vision based upon equal legibility of the letters.\n\nVisual acuity = Distance at which test is made / distance at which the smallest optotype identified subtends an angle of five arcminutes.\n\nSnellen defined “standard vision” as the ability to recognize one of his optotypes when it subtended 5 minutes of arc. Thus the optotype can only be recognized if the person viewing it can discriminate a spatial pattern separated by a visual angle of one minute of arc.\n\nOutside of the United States, the standard chart distance is , and normal acuity is designated \"6/6\". Other acuities are expressed as ratios with a numerator of 6. Some clinics do not have 6-metre eye lanes available, and either a half-size chart subtending the same angles at , or a reversed chart projected and viewed by a mirror is used to achieve the correct sized letters.\n\nIn the most familiar acuity test, a Snellen chart is placed at a standard distance: 6 metres. At this distance, the symbols on the line representing \"normal\" acuity subtend an angle of five minutes of arc, and the thickness of the lines and of the spaces between the lines subtends one minute of arc. This line, designated 6/6 (or 20/20), is the smallest line that a person with normal acuity can read at a distance of 6 metres.\n\nThree lines above, the letters have twice the height of those letters on the 6/6 (or 20/20 in the US) line. If this is the smallest line a person can read, the person's acuity is \"6/12\" (\"20/40\"), meaning that this person needs to approach to a distance of to read letters that a person with normal acuity could read at . In an even more approximate manner, this person could be said to have \"half\" the normal acuity of 6/6.\n\nAt exactly 6 metres' distance from the patient, the letters on the 6/6 line shall subtend 5 minutes of arc (such that the individual limbs of the letters subtend 1 minute of arc), which means that the chart should be sized such that these letters are 8.73 mm tall and the topmost (6/60) \"E\" should be 87.3 mm tall. Putting it another way, the eye should be at a distance 68.76 times the height of the top (6/60) letter. The formula is\n\nformula_1\n\nwhere formula_2 is the optotype height or width (which are the same due to the optotype being on a square grid), formula_3 is the distance from eye to chart, and formula_4 is the angle subtended by the optotype, which is 5 arcminutes as specified by Snellen. Another calculation for United States clinics using 20-foot chart distances (slightly more than 6 m), and using a 17 mm model eye for calculations, and a letter which subtends 5 minutes of arc, gives a vertical height of the 20/20 letter to be 8.75 mm.\n\nAcuity charts are used during many kinds of vision examinations, such as \"refracting\" the eye to determine the best eyeglass prescription. During such examinations, acuity ratios are never mentioned.\n\nThe largest letter on an eye chart often represents an acuity of 6/60 (20/200), the value that is considered \"legally blind\" in the US. Some individuals with moderate myopia may not be able to read the large E without glasses, but have no problem reading the 6/6 (20/20) line or 6/4.5 (20/15) line with glasses. By contrast, legally blind individuals have a visual acuity of 6/60 (20/200) or less when using the best corrective lens.\n\nTo ensure adequate illumination of the Snellen charts, various medical device manufacturers had developed Snellen chart products with backlight or projection.\n\nSince computer monitors typically have good lighting for reading and LCD/LED monitors have high DPI between 96 to 480, they are suitable for displaying optotypes. Commonly digital chart products support randomizing optotypes displayed to prevent patients from cheating through memorizing the chart. In Google Play and App Store (iOS), there are Snellen chart apps for smart phones and tablets.\n\n\n"}
{"id": "893392", "url": "https://en.wikipedia.org/wiki?curid=893392", "title": "Strategic health authority", "text": "Strategic health authority\n\nStrategic health authorities (SHA) were part of the structure of the National Health Service in England between 2002 and 2013. Each SHA was responsible for enacting the directives and implementing fiscal policy as dictated by the Department of Health at a regional level. \n\nIn 2002, the existing regional health authorities were renamed and merged to form 28 new strategic health authorities. On 12 April 2006, Patricia Hewitt, Secretary of State for Health, announced that, following an NHS consultation which ended on 22 March 2006, the SHAs were to be reorganized, reducing to ten in number. This was expected to produce substantial financial savings. \n\nStrategic health authorities and primary care trusts were abolished on 31 March 2013 as part of the Health and Social Care Act 2012. Facilities owned by SHAs were transferred to NHS Property Services.\n\nEach SHA area contained various NHS trusts which took responsibility for running or commissioning local NHS services, and the SHA was responsible for strategic supervision of these services. The types of trust included: \nThe SHAs had the board and governance structures common to all NHS trusts. \n\nThe ten SHAs established on 1 July 2006, and abolished on 31 March 2013, were:\n\nThese SHAs are coterminous with government office regions, except that the large South East England region is divided into two: South Central and South East Coast.\n\n\n† known as the 'Coventry, Warwickshire, Herefordshire and Worcestershire SHA until 2004.\n\nThe London boundaries were:\n\n\nThese SHAs were replaced by a single London SHA in 2006.\n\n"}
{"id": "40963539", "url": "https://en.wikipedia.org/wiki?curid=40963539", "title": "The Banyan (NGO)", "text": "The Banyan (NGO)\n\nThe Banyan is an non-governmental organization based in Chennai, India was founded in 1993 by Vandana Gopikumar and Vaishnavi Jayakumar to cater to the mentally ill and homeless women in the city. Over the past two decades, has expanded to offer a range of comprehensive mental health solutions for men and women who are either homeless, or living in a state of abject poverty.\n\nThis includes emergency care and critical time interventions offered across 2 hospital based settings, and one shelter for homeless persons with psychosocial disabilities, operated in collaboration with the Corporation of Chennai under the National Urban Livelihoods Mission (NULM), reaching out to 162 individuals in distress at any given point of time. Over 3400 individuals have accessed these services since 1993.\n\nThe Banyan's NALAM programme offers well-being oriented outpatient clinics and community based mental health care in both urban and rural areas. These clinics provide free of charge and mental health and social care services, address distress, common mental disorders and severe mental disorders. They are offered across 15 service access points, reaching out to a population of approximately 7 lakhs. NALAM has reached out to over 10,000 individuals and has a current active registry of 2000 individuals.\n\nFor those who experience high support needs, The Banyan offers a range of inclusive living options, over 200 individuals with moderate to severe mental health issues, today live in communities & homelike environments with supportive services across 6 districts in 3 States (Tamil Nadu, Kerala, Maharashtra).\n\nThe Banyan's sister organisation - The Banyan Academy of Leadership in Mental Health (BALM), was founded in 2007, and works towards consolidating findings and experiences from The Banyan, and disseminating it across multiple stakeholders within the mental health and development spaces. BALM works on building practice based evidence and contributing to a knowledge base, that aids in addressing treatment and care gaps, particularly in the context of vulnerable individuals and groups. It uses capacity building, education programmes (diploma, masters’, and PhD programmes) in collaboration with the Tata Institute of Social Sciences (TISS), research, collaborative work (with civil society organisations, local governments, disability movements, mental health user-caregiver groups, and Central and State Government) and construction of think tanks, as strategies to influence progressive policies and plans.\n\nAn inclusive and humane world that promotes capabilities, equity and justice.\n\nEnabling access to health and mental health care for persons living in poverty and homelessness through comprehensive creative clinical and social care approaches embedded in a well being paradigm. The needs of those who live in the margins are our collective responsibility.\n\nThe Banyan's chapter in Trichy is led by a pair of sisters, who have experienced homelessness, poverty and mental illness first hand. One of the sisters was rescued by The Banyan several years ago, and after recovering was able to find her way back home. Their personal experiences gave them the passion to form a chapter of The Banyan in their home community in Kovandakurichi (Trichy).\n\nThey have built a programme that provides long-term care for 32 women through the Home Again approach.\n\nThe Banyan's Kerala Chapter is led by Mr. Salih PM, a social worker and social entrepreneur who has been with The Banyan for over 10 years. He leads a team of 10 mental health professionals that run our Kerala Chapter, currently housing 22 residents through the Home Again programme.\n\nIn Kerala, The Banyan is also working in collaboration with Tata Institute of Social Sciences, The Hans Foundation and The Government of Kerala on reorienting the social architecture of institutional mental health care. The project, known as \"Snehakoodu\" aims to identify long-stay patients in the government mental health centres in Trissur, Trivandrum and Calicut and facilitate their return to their homes or into inclusive living long-term care options, such as the Home Again program.\n\nThe Banyan chapter in Maharashtra is a collaboration with Tata Institute of Social Sciences Field Action Programme and the Integrated Rural Health and Development Project, Tata Trusts and the Government of Maharashtra. The project entails the facilitation of exit pathways out of institutionalised care for persons who have been incarcerated within mental health facilities for extended periods of time.\n\nThe Banyan rescues homeless individuals with mental illness through referrals and calls. They have 2 hospital based settings that service women and one shelter for homeless men with psychosocial needs run in collaboration with the corporation of Chennai. Combined, these locations serve around 150 individuals at a time.\n\nThe Government of Tamil Nadu - National Health Mission plans to set up multiple new emergency care and recovery centres for individuals with mental illness, both those who are homeless and those who are not. The Banyan will act as capacity building partners on this project, and will also share the protocols and values of their emergency care and recovery services, so that they will be replicated in these new centres.\n\nThe Banyan works with police and local organisations to help their clients reconnect with their families after they have gotten better. They conduct home visits and educate family members on illness management prior to reintegrating the client.\n\nThe aftercare programme ensures continuity of care after an individual has returned home. It functions through a coalition of civil society organisations, Government agencies, and individual functionaries \"(community health workers/peer advocates and activists)\" with capabilities to offer integrated mental health and social care interventions. Besides access to medical and psychological/counselling support, social care facilitation such as problem solving, work placement, access to social entitlements and local community support circles are also essential components of our aftercare programme.\n\nThe Banyan’s NALAM programme is a well-being oriented, community based mental health care programme driven by grassroots workers who offer multi-tiered, multi-interventional packages of mental health care, that range from tracking progress on symptom reduction and securing economic stability, to pursuing personal aspirations. This programme exists across both rural and urban sites, servicing a population of over 1 million across 3 States (Tamil Nadu, Kerala and Maharashtra) and 6 districts through 17 mental health service access points. To date - 10,000 clients have accessed these services, and over 800 children access the youth clubs, pre-adolescent workshops, and tuition centres offered through this same programme.\n\nNALAM Urban serves predominantly low-income areas in the Greater Chennai City Area and has cultivated a grassroots presence across all of the wards (Mogappair, Padi, Padiputhunagar, KK Nagar, Jafarkhanpet, West Saidapet, Choolaimedu, Santhome and Teynampet) it works in.\n\nThe urban clinics are located in Chennai across these locations:\n\n\nNALAM Rural offers inpatient and outpatient programmes that address clinical and social care needs of individuals in need. The NALAM community mobilisers also engage in innovative mental health awareness programmes across local NGOs, youth groups, PHCs and MGNREGs sites.\n\nOutreach clinics are co-located with Government run spaces, such as the Government Primary Health Centre, and the Panchayat office. This is in an attempt to work alongside the Government machinery, and support the implementation of the Government District Mental Health Programme (DMHP).\n\nThe rural clinics are located in Kanchipuram districts across these locations:\n\n\nThis approach will soon be replicated in collaboration with The Government of Tamil Nadu as a means to strengthen the District Mental Health Programme (DMHP)\n\nFor individuals who require long-term care, The Banyan offers supportive care inclusive living long term options. These programmes encourage social inclusion and currently almost 300 individuals are residing in these projects.\n\nThe Banyan’s Clustered Group Home is a pseudo-institutional long-term care home. In it, about 50 women reside across 8 cottages, where women live, work and support one another while contributing to their home and space. In this setting support is provided to address complex long-term physical, psychological and economic needs of the clients. The women who live here spend their days pursuing work and vocational training as well as recreation and leisure activities of their choice. The Clustered Group Home is co-located with the BALM-TISS, a college that offers capacity building masters’, PhD and diploma programs in mental health.\n\nHome Again is a model of care, for individuals with mental illness who require long-term care that fosters choice based, inclusive living spaces through clustered or scattered homes in rural or urban neighbourhoods with a range of supportive services for people. Through this programme’s model, individuals form affinity groups and live together in homes in a community. Together they create a shared space of comfort that mimics a familial environment. Every house of three to five women is staffed by a community worker who facilitates the psychosocial and medical intervention they continue to need and the women are encouraged to engage in a diverse range of work, and embrace leisure, recreation and socialization with the community. This programme is currently active in Tamil Nadu (Chennai, Kovalam & Trichy), Kerala (Mallapuram), and has been implemented in Assam through a partner organisation, Ashadeep.\n\nThe Banyan is currently part of a collaborative project (along with Tata Institute of Social Sciences, The Hans Foundation and The Government of Kerala) in Kerala to facilitate exit options for long-stay patients in government run mental health centres. Many of the long-stay patients who will be identified through this project will be transferred to Home Again homes within Kerala.\n\nAt The Banyan, vocational training and work placement is offered across three verticals - arts & crafts (\"wire and jute products, carpentry, tailoring etc.)\", hospitality services \"(housekeeping, beauty services, laundry, waitstaff etc.),\" and healthcare \"(personal assistants, community based mental health workers, peer advocates, programme managers etc.)\"\n\nThe organisation holds income enhancement and vocational independence as significant factors of importance when it comes to personal recovery. In addition to internal work placements and vocational training, The Banyan also has ties with external organisations to facilitate employment outside of the organisation. They also offer clients small and medium scale grants to federate social cooperatives. 60% of their clients are gainfully employed and productive in work.\n\nThe Banyan and its founders have acquired several accolades since 1993, including the WHO Public Health Champion Award (India), 2017, The Sat Paul Mittal National Award (2007) and University of Pennsylvania Nursing Renfield Foundation Award for Global Women’s Health (2018) among others.\n"}
{"id": "27103951", "url": "https://en.wikipedia.org/wiki?curid=27103951", "title": "Tin poisoning", "text": "Tin poisoning\n\nTin poisoning refers to the toxic effects of tin and its compounds. Cases of poisoning from tin metal, its oxides, and its salts are \"almost unknown\"; on the other hand certain organotin compounds are almost as toxic as cyanide.\n\nTin has no known natural biological role in living organisms. It is not easily absorbed by animals and humans. The low toxicity is relevant to the widespread use of tin in dinnerware and canned food. Nausea, vomiting and diarrhea have been reported after ingesting canned food containing 200 mg/kg of tin. This observation led, for example, the Food Standards Agency in the UK to propose upper limits of 200 mg/kg. A study showed that 99.5% of the controlled food cans contain tin in an amount below that level. However un-lacquered tin cans with food of a low pH, for example, fruits and pickled vegetables, can contain elevated concentrations of tin.\n\nThe toxic effects of tin compounds is based on the interference with the iron and copper metabolism. For example, it affects heme and cytochrome P450, and decreases their effectiveness.\n\nOrganotin compounds can be very toxic. \"Tri-\"n\"-alkyltins\" are phytotoxic and, depending on the organic groups, can be powerful bactericides and fungicides. Other triorganotins are used as miticides and acaricides.\nTributyltin (TBT) was extensively used in marine antifouling paints, until discontinued for leisure craft due to concerns over longer-term marine toxicity in high-traffic areas such as marinas with large numbers of static boats.\n\n"}
{"id": "23889410", "url": "https://en.wikipedia.org/wiki?curid=23889410", "title": "Toiletry bag", "text": "Toiletry bag\n\nA toiletry bag (also called a toiletry kit, ditty bag, doc kit, bathroom kit, sponge bag, toilet bag, toilet kit, body hygiene kit, travel kit, wash bag, or wet pack) is a portable container—usually a pouch with a drawstring or zippered closure—which holds body hygiene and toiletry supplies such as toothbrush and toothpaste, dental floss, cotton swabs, deodorant, nail clippers, tweezers, soap, shaving supplies, hair brush, tampons, contact lenses and supplies, and similar items while travelling and in other circumstances where permanent shelves and cupboards are unavailable or impractical for use.\n\n\"Dopp kit\" is a term particularly in use in the US for toiletry bags. The name derives from the early 20th century leather craftsman Charles Doppelt, whose company designed the case in 1926.\n\nDopp kits became widely known during World War I and World War II when they were issued to soldiers by the US military. The Dopp brand name was purchased by Samsonite in the early 1970s and was acquired by Buxton in 1979.\n\nThe trademark \"Dopp\" was filed by Samsonite for registration at the US Trademark Office on March 24, 1980. The class of goods for which it was applicable was “toilet cases sold empty, briefcases, briefcase type portfolios, sample and catalog cases sold empty, luggage identification tags, and traveling bags”. An additional filing for the trademark \"Dopp Kit\" was made on April 3, 1980 for the class of goods “travel kits, sold empty”. In both of those applications, Samsonite stated that the trademarks were first used in July 1936. The registration for \"Dopp Kit\" was cancelled in March 2003 when no one filed a required (Section 8) Declaration of Continued Use, but the required declarations have been filed for \"Dopp\". The marks were assigned to DHP Limited Partnership (dba Buxton) in 1990; the current owner of the \"Dopp\" trademark is listed as Buxton Acquisition Co., LLC, of Chicopee, Massachusetts.\n"}
{"id": "6970497", "url": "https://en.wikipedia.org/wiki?curid=6970497", "title": "Universal Primary Education", "text": "Universal Primary Education\n\nThe second goal in the United Nations Millennium Development Goal is to achieve Universal Primary Education, more specifically, to \"ensure that by 2015, children everywhere, boys and girls alike will be able to complete a full course of primary schooling.\" Education is vital to meeting all other Millennium Development Goals: \"Educating children gives the next generation the tools to fight poverty and prevent disease, including malaria and AIDS.\"\nDespite the significance of investing in education, the recent report, Fixing the Broken Promise of Education for All: Findings from the Global Initiative on Out-of-School Children—produced by UNESCO Institute for Statistics and UNICEF found that the world has missed this 2015 target of universal primary education, and there are currently 58 million children, of primary school age, out of school worldwide.\n\nSince 1999, there has been great progress towards achieving universal primary enrollment due in large part to a pursuit of the Millennium Development Goals (MDGs) and the Education for All (EFA) The number of primary school age out-of-school children dropped by 42% between 2000 and 2012, despite rapid population growth.\nGreater than half of countries and regions worldwide have a net enrolment rate of more than 95% and either already have or are close to achieving universal primary education. However, despite an increase in enrollment over the past decade, global progress has stalled since 2007, and net enrolment or attendance is less than 80 per cent in about 20 countries. Of the 58 million children out of school:\n\n\nRoughly half of all out-of-school children come from just a few countries, many of them characterized by conflict, instability, and extreme poverty. West and Central Africa is home to one-third of all primary school age out-of-school children, making it the region with the lowest rates of school participation. Challenges to achieving universal primary education are exacerbated in unstable regions, as they have greater difficulty in accessing financial support.\n\nThe barriers which prevent children around the world from obtaining primary level education are diverse and require tailored responses. Children living in conflict-affected areas account for \"just 20% of the world's children of primary school age but 50% of the world's out-of-school children.\" Additionally, inequalities in wealth significantly impact out-of-school rates. In many countries, children from the poorest 20 per cent of the population are less likely to attend school than those who are better off. Despite overall improvements, girls continue to be at a disadvantage as 53%—more than half—of the estimated 58 million primary age out-of-school-children are girls.\n\nLocation contributes to a child's lack of access and attendance to primary education. In certain areas of the world it is more difficult for children to get to school. For example; in high-altitude areas of India, severe weather conditions for more than 7 months of the year make school attendance erratic and force children to remain at home (Postiglione).\n\nIn these remote locations, insufficient school funds contribute to low attendance rates by creating undesirable and unsafe learning environments. In 1996, the General Accounting Office (GAO) reported that poor conditions existed in many rural areas; one out of every two rural schools had at least one inadequate structural or mechanical feature (Lawrence). In these situations where regular school attendance is rare, a low population contributes to the problem. In other locations, large numbers are often the cause of low attendance rates.\n\nDue to population growth, many urban schools have expanded their boundaries making school transportation more complicated. \"For over 50 years the U.S. has been shifting away from small, neighborhood schools to larger schools in lower density areas. Rates of children walking and biking to school have declined significantly over this period\" (Schlossberg). There is evidence to prove that the distance to and from school contributes a child's attendance, or lack thereof. In a study done investigating the relation between location (distance) and school attendance in Mali, about half the villages reported that the school was too far away, causing students not to enroll (Birdsall).\n\nThere is still speculation as to whether primary schools are more accessible in rural or urban areas because situations differ depending on geographic location. In a study done examining the correlation between location and school attendance in Argentina and Panama, researchers found that urban residence was positively correlated with school attendance (De Vos), but another study in a Louisiana school district found that schools with the lowest attendance rates were in metropolitan areas (Moonie).\n\nMore research needs to be done to determine geography's specific effects on attendance, but no matter where you live, there is evidence that location will contribute to a child's access and attendance to education.\n\nGender contributes to a child's lack of access and attendance to education. Although it may not be as an obvious a problem today, gender equality in education has been an issue for a long time. Many investments in girls' education in the 1900s addressed the widespread lack of access to primary education in developing countries (Dowd).\n\nThere is currently a gender discrepancy in education. In 25 countries the proportion of boys enrolling in secondary school is higher than girls by 10% or more, and in five; India, Nepal, Togo, Turkey and Yemen, the gap exceeds 20%. Enrollment is low for both boys and girls in sub-Saharan Africa, with rates of just 27% and 22%. Girls trail respectively behind (Douglas). It is generally believed that girls are often discouraged from attending primary schooling, especially in less developed countries for religious and cultural reasons, but there is little evidence available to support this association. However, there is evidence to prove that the disparity of gender in education is real. Today some 78% of girls drop out of school, compared with 48% of boys (Douglas). A child's gender continues to contribute to access and attendance today.\n\nCosts contribute to a child's lack of acquiescence and attendance to primary education. High opportunity costs are often influential in the decision to attend school. For example; an estimated 121 million children of primary-school age are being kept out of school to work in the fields or at home (UNICEF). For many families in developing countries the economic benefits of no primary schooling are enough to offset the opportunity cost of attending.\n\nBesides the opportunity costs associated with education, school fees can be very expensive, especially for poor households. In rural China, families dedicate as much as a third of their income to school fees (Peverly). Sometimes, the cost gets too expensive and families can't support their children's education anymore, although the statistics disagree. \"China has 108.6 million primary school students, with a 1 percent dropout rate, but experts doubt these figures because the dropout rates in rural areas appear much higher\" (Peverly).\n\nAlthough the relationship between school fees and attendance still isn't perfectly clear (Peverly), there is evidence to prove that cost is a factor that contributes to a child's access and attendance to primary education.\n\nIn developing countries throughout the world the educational context is characterized not by monolingual settings, but rather multilingual situations. Often children are asked to enroll in a primary school where the Medium of Instruction (MI) is not her home language, but rather the language of the government, or another dominant society . Studies throughout the world demonstrate the importance of the MI in determining a child's educational attainment. According to Mehrotra (1988) \"In a situation where the parents are illiterate..., if the medium of instruction in school is a language that is not spoken at home the problems of learning in an environment characterized by poverty are compounded, and the chances of drop-out increase correspondingly. In this context, the experience of the high-achievers has been unequivocal: the mother tongue was used as the medium of instruction at the primary level in all cases. ... There is much research which shows that students learn to read more quickly when taught in their mother tongue. Second, students who have learned to read in their mother tongue learn to read in a second language more quickly than do those who are first taught to read in the second language. Third, in terms of academic learning skills as well, students taught to read in their mother tongue acquire such skills more quickly\". (See also Multilingual Education)\n\nEducation is a crucial factor in ending global poverty. With education, employment opportunities are broadened, income levels are increased and maternal and child health is improved.\n\nIn areas where access, attendance and quality of education have seen improvements, there has also been a slow in the spread of HIV/AIDS and an increase in the healthiness of the community in general. In fact, children of educated mothers are 50% more likely to live past the age of five. Not only does education improve individual and familial health, but it also improves the health of a community. In countries with solid education systems in place, there are lower crime rates, greater economic growth and improved social services.\n\n\"There are approximately 300 million chronically hungry children in the world. One hundred million of them do not attend school, and two thirds of those not attending school are girls. World Food Programme's school feeding formula is simple: food attracts hungry children to school. An education broadens their options, helping to lift them out of poverty.\"–World Food Programme\n\nOne successful method to ensuring that children attend school on a regular basis is through school feeding programs. Many different organizations fund school feeding programs, among them the World Food Programme and the World Bank. The idea of a school feeding program is that children are provided with meals at school with the expectation that they will attend school regularly. School feeding programs have proven a huge success because not only do the attendance rates increase, but in areas where food is scarce and malnutrition is extensive, the food that children are receiving at school can prove to be a critical source of nutrition. School meals have led to improved concentration and performance of children in school. Another aspect of school feeding programs is take home rations. When economic reasons, the need to care for the elderly or a family member suffering from HIV, or cultural beliefs keep a parent from sending their child (especially a female child) to school, these take home rations provide incentives to sending their children to school rather than to work\n\nThis organization promotes education as a basic human right. It motivates people and groups to put public pressure on governments and the international community in order to assure that all children are provided with free, compulsory public education. It brings together major NGOs and Teachers Unions in over 120 countries to work in solidarity towards their vision of universal primary education.\n\nThe Right to Education Project aims to promote social mobilization and legal accountability, looking to focus on the legal challenges to the right to education.\nTo ensure continued relevance and engagement with activists and the academic community the Project also undertakes comparative research to advance an understanding of the right to education.\n\nUNICEF believes that in treating education as a basic human right, it will address the basic inequalities in our society, especially gender inequalities. It advocates high-quality, child-friendly basic education for all, with an emphasis on gender equality and eliminating disparities of all kinds through a range of innovative programs and initiatives. In working with local, national and international partners, UNICEF's work supports the attainment of universal primary education.\n\nThis organization is a confederation of 12 organizations that are dedicated to reducing poverty and eliminating injustices in the world. Oxfam works on a grassroots level in countries around the world to ensure that all people have access to the basic human rights, including education.\n\nThis organization advocates education as a way for individuals to escape poverty. They are running a campaign entitled \"Rewrite the Future\" to encouraging American citizens, in positions of power and wealth, to take action against the injustices in education systems around the world. Save the Children also operates education programs in 30 countries all over the world.\n\nThis United States government organization has volunteers on the ground in 75 countries. Many of the volunteers are working as teachers in rural areas or working to promote and improve access to education in the areas in which they are stationed.\n\nUNESCO works to improve education through projects, advice, capacity-building and networking. UNESCO's Education for All Campaign by 2015 is the driving force in UNESCO's work in the field of education at the moment.\n\nThis organization provides financial and technical assistance to developing countries. Loans and grants from the World Bank provide much of the funding for educational projects around the world, including but not limited to school feeding programs.\n\nChild Aid conducts school- and library-based reading programs in over 50 indigenous villages in Guatemala, where literacy rates are lower than anywhere in Latin America. Through its literacy development programing it trains teachers and librarians, creates and improves community libraries and delivers tens of thousands of children's books annually.\n\nThis organization provides food relief in areas that need it most and is one of the major funders of school feeding programs.\n\nThis organization runs a campaign entitled Education for Rural People in which they work to ensure education for rural people as the key to reduction of poverty, food security and sustainable development.\n\nThis organization is a hub for organizations committed to ending vitamin and mineral deficiencies. GAIN works with other international organizations, governments and the private sector to implement large-scale food fortification programs as well as targeted ones including school feeding projects aimed at the most at risk of malnutrition. Home Page\n\nThe Fast Track Initiative (FTI) was launched in 2002. It was designed as a major initiative to help countries achieve the Millennium Development Goal (MDG) of Universal Primary Education (UPE) by 2015. It was endorsed by the Development Committee of the World Bank as a 'process that would provide quick and incremental technical and financial support to countries that have policies but are not on track to attain Universal Primary Completion by 2015' (World Bank Development Committee, 2003). In 2011, the organization re-committed to achieving education for all children through their transformation into the Global Partnership for Education, reflecting the importance of uniting worldwide to achieve this goal.\n\nThe Walking School Bus (TWSB) was launched in 2015. TWSB empower educational attainment through mutually beneficial partnerships that promote: access, nutrition, and curriculum. TWSB mission is to empower students in developing countries to access education, thereby breaking down cultural and faith-based barriers, creating academic partnerships between both local and international schools and their respective communities, while helping provide sustainable and adequate nutrition for their partnered beneficiaries. TWSB currently work across North America, Uganda, South Africa, Greece, and India. To find out more about their work, visit: http://thewalkingschoolbus.com/\n\nThis Indianapolis, IN based social-profit empowers young people to support their peers in sub-Saharan Africa by raising funds and awareness for school infrastructure projects. They have built seven primary schools in Uganda since their inception in 2005 and are working to support the UN Millennium Development Goal of Universal Primary Education.\n\nSince the Khmer Rouge eliminated a large percentage of educated Cambodians, Cambodia has been lacking educated resources leading to a lower educational level.\n\nThe mission of Teach for America is to address the inadequacies in the United States education system by placing highly qualified college graduates into under resourced schools for a two-year period in an attempt transform these leaders into lifelong advocates of education reform in the United States.\n\nThis is a campaign to empower young people in the United States to stand up and speak out against the inadequacies in the United States education system and to demand change through political activism.\n\nThis organization empowers high potential middle school students from lower income communities to excel in school and at the same time inspires motivated high school and college students to pursue careers in education. It is a six-week summer enrichment program where \"students teach students\" run in more than 30 sites all over the United States.\n\n\nUNICEF Staff Working Papers, New York, UNICEF.\n\n"}
{"id": "10571642", "url": "https://en.wikipedia.org/wiki?curid=10571642", "title": "Vladislav Terzyul", "text": "Vladislav Terzyul\n\nVladyslav Terzyul (; 18 June 1953 in Artyom, Primorsky Krai, Soviet Union – 17 May 2004), was an Ukrainian alpinist, one of the world's premier high-altitude climbers.\n\nHe is said to be one of the few people to have climbed all eight-thousander peaks and the first Ukrainian ever. However this claim is disputed because he did not reach the highest point on Shishapangma (8027m), but instead stopped at Shishapangma Central (8013m).\n\nVladislav Terzyul died descending from the summit of Makalu on May 17, 2004, at an altitude of about 8300 metres.\n\n\n"}
