{"id": "5840460", "url": "https://en.wikipedia.org/wiki?curid=5840460", "title": "Airspace Action on Smoking and Health", "text": "Airspace Action on Smoking and Health\n\nAirspace Action on Smoking and Health (formerly \"Airspace Non-smokers' Rights Society\") is a volunteer-based anti-tobacco organization in the Canadian province of British Columbia.\n\nAirspace Non-smokers' Rights Society was initially founded in Victoria in the late 1970s and successfully lobbied for one of Canada's first ever smoking control by-laws. Dale Jackaman, then Secretary of the Victoria organization, moved to Vancouver in 1984. He and Norm Gillan founded the Vancouver branch of Airspace, and Jackaman was its first Executive Director. Jackaman then amalgamated both the Victoria and Namaimo organizations (run by Errol Povah) and formed what is now the province-wide Airspace organization.\n\nThe Presidents and Executive Directors of Airspace include Jackaman, Deborah Wotherspoon, Jerry Steinberg, Robert Broughton (activist), Heather Mackenzie, and Povah, respectively.\n\nOne aspect of the Airspace organization was its early adoption of computer technology to facilitate its activist activities. Airspace was probably the first organization of its kind to adopt a modem based bulletin board system (BBS). Airspace was also one of the first organizations of this nature to have a web site. Some screen shots from an early version of the site turned up in internal documents from Philip Morris.\n\nThe organization's public persona is the Grim Reaper, used it to create a form of street theatre at tobacco promotions.\n\nIn 1993, the New Democratic Party government of the day announced regulations to prevent the sale of cigarettes to children. Airspace organized a \"compliance check\" (or \"kiddie sting\"). Ten Airspace volunteers escorted five youths aged 12 to 16 who attempted to purchase cigarettes. They visited a total of 65 tobacco retailers, and 43% of them were willing to sell cigarettes to the underaged person. This effort got significant press attention, and Burnaby-Edmonds MLA Fred Randall pressed for stronger regulations, such as stiffer fines and suspensions. The provincial government started doing compliance checks of its own to measure the effectiveness of this effort.\n\nDuring the early 1990s, Airspace staged regular protests in front of Imperial Tobacco's Vancouver office. These protests included an annual \"Modified Christmas Caroling\", using \"Clean Air Carols\". These protests came to an end when Imperial Tobacco (the Canadian branch of BAT) closed the Vancouver office and moved their Western Canada operations to Calgary.\n\n\n"}
{"id": "20824873", "url": "https://en.wikipedia.org/wiki?curid=20824873", "title": "Alison Gopnik", "text": "Alison Gopnik\n\nAlison Gopnik (born June 16, 1955) is an American professor of psychology and affiliate professor of philosophy at the University of California, Berkeley. She is known for her work in the areas of cognitive and language development, specializing in the effect of language on thought, the development of a theory of mind, and causal learning. Her writing on psychology and cognitive science has appeared in \"Science\", \"Scientific American\", \"The Times Literary Supplement\", \"The New York Review of Books\", \"The New York Times\", \"New Scientist\", \"Slate\" and others. Her body of work also includes four books and over 100 journal articles.\n\nShe has frequently appeared on TV and radio including \"The Charlie Rose Show\" and \"The Colbert Report\". \"Slate\" writes of Gopnik, \"One of the most prominent researchers in the field, Gopnik is also one of the finest writers, with a special gift for relating scientific research to the questions that parents and others most want answered. This is where to go if you want to get into the head of a baby.\" Gopnik is a columnist for \"The Wall Street Journal\", sharing the Mind & Matter column with Robert Sapolsky on alternating Saturdays.\n\nGopnik received a B.A., majoring in psychology and philosophy, from McGill University in 1975. In 1980, she received a D.Phil. in experimental psychology from Oxford University. She worked at the University of Toronto before joining the faculty at UC Berkeley in 1988.\n\nGopnik has done extensive work with applying Bayesian networks to human learning and has published and presented numerous papers on the topic. Gopnik says of this work, \"The interesting thing about Bayes nets is that they search out causes rather than mere associations. They give you a single representational structure for dealing both with things that just happen and with interventions--things you observe others doing to the world or things you do to the world. This is important because there is something really special about the way we treat and understand human action. We give it a special status in terms of our causal inferences. We think of human actions as things that you do that are designed to change things in the world as opposed to other events that just take place.\"\n\nJudea Pearl, developer of Bayesian networks, says Gopnik was one of the first psychologists to note that the mathematical models also resemble how children learn. Gopnik's work at Berkeley's Child Study Center seeks to develop mathematical models of how children learn. These models could be used to develop better algorithms for artificial intelligence.\n\nIn April, 2013, Gopnik was inducted into The American Academy of Arts and Sciences. She is, as of 2014, a Fellow of the Cognitive Science Society.\n\nGopnik is an authority on the philosophy of mind and a preeminent developmental psychologist. Gopnik is known for advocating the \"theory theory\" which postulates that the same mechanisms used by scientists to develop scientific theories are used by children to develop causal models of their environment. The \"theory theory\" was explored in \"\"Words, Thoughts, and Theories\",\" co-authored with Andrew N. Meltzoff. Gopnik co-authored with Andrew N. Meltzoff and Patricia K. Kuhl \"\"The Scientist in the Crib: What Early Learning Tells Us About the Mind\".\" The book posits that the cognitive development of children in early life is made possible by three factors: innate knowledge, advanced learning ability, and the evolved ability of parents to teach their offspring. \"\"Causal Learning: Psychology, Philosophy, and Computation\",\" edited with Laura Schulz, explores causal learning and the interdisciplinary work done in furthering the understanding of learning and reasoning.\n\nIn her book \"\"The Philosophical Baby: What Children’s Minds Tell Us about Truth, Love, and the Meaning of Life\",\" Gopnik explores how infants and young children cognitively develop by using processes similar to those used by scientists, including experimenting on their environment. The book explains how an environment maximized for an infant's cognitive development is one that is safe to explore. The book also explores what babies can tell us about love, imagination and identity, as well as considering the broader philosophical significance of care-giving. \"The Philosophical Baby\" has been recognized as a New York Times Extended List Bestseller, a San Francisco Chronicle Bestseller, and an Independent Bookstores Bestseller. It has also received acclaim on the New York Times Editor's Choice list, the San Francisco Chronicle Editors Choice list, and as one of Babble's 50 Best Parenting Books. It has also been recognized as recommended reading by Scientific American.\n\nIn 2009, Gopnik published a paper in \"Hume Studies\" arguing that the historical record regarding the circumstances around David Hume's authoring of \"A Treatise of Human Nature\" are wrong. Gopnik argued that Hume had access to the library of the Royal College at La Flèche, a Jesuit institution that had been founded by Henri IV. At the time Hume was living nearby and working on the \"Treatise\", La Flèche was home to a Jesuit missionary named Charles François Dolu, a learned man who was an expert on different world religions who had visited the French embassy in Siam. In addition, Dolu had met Ippolito Desideri, another Jesuit missionary who had visited Tibet from 1716–1721. Gopnik argues that because of his exposure to Theravada Buddhism, Dolu may form the source of the Buddhist influence on Hume's \"Treatise\". Gopnik cites a number of letters from Hume that mention his time at La Flèche and his meeting with Jesuits from the college. It is from this Buddhist connection through the learning of the Jesuit college that Hume is influenced to deny the ontological reality of the self—which Gopnik links to the Indian Buddhist idea of \"Śūnyatā\" (Emptiness).\n\nThe feature-length documentary film \"The Singularity\" by independent filmmaker Doug Wolens (released at the end of 2012), showcasing Gopnik's work in cognitive development as it relates to computer learning, has been acclaimed as \"a large-scale achievement in its documentation of futurist and counter-futurist ideas\" and \"the best documentary on the Singularity to date.\"\n\nGopnik is the daughter of linguist Myrna Gopnik. She is the firstborn of six siblings who include Blake Gopnik, the \"Newsweek\" art critic, and Adam Gopnik, a writer for \"The New Yorker\". She was formerly married to journalist George Lewinski and has three sons: Alexei, Nicholas, and Andres Gopnik-Lewinski. In 2010, she married computer graphics pioneer Alvy Ray Smith, the co-founder of Pixar.\n\n\n"}
{"id": "32085824", "url": "https://en.wikipedia.org/wiki?curid=32085824", "title": "American Hypnosis Society", "text": "American Hypnosis Society\n\nAmerican Hypnosis Society (AHS) existed from 1965 until 2001 for the purpose of educating interested individuals and mental health professionals in the proper practice of hypnosis. The Society’s stated objective was “..to advance understanding of, and research into, the beneficial use of properly practiced hypnotherapy and to dispel myths regarding its possible misuses.”\n\nThe Society was founded by New York City-based psychiatrist Michael Goldstein, M.D., Ph.D., whose practice was focused on the treatment of aberrant behavior, and the reintegration of social outcasts with their community.\n\nUsing a local training methodology, matching students with experienced instructors on a one-on-one basis the Society's hands on approach gained it quick acceptance in New York and eventually throughout the country . This, coupled with quarterly classroom based training, allowed Society members to learn hypnosis through personal interaction with subjects and knowledgeable professionals.\n\nThe Society offered certification and training recognizing successful students as hypnotists after 200 hours of training and examination and as hypnotherapists after 800 hours of training and 120 hours of supervised client interaction (which time could be accounted against the 800 hours of training). Related class work from a properly accredited institution, such as a college, could be used to offset up to 500 hours of the required class material, if approved by the accreditation panel, but only actual client interaction by a previously certified hypnotherapist could be used for the 120-hour client requirement.\n\nSubmission to examinations and demonstrations of accumulated knowledge were required by students at regular intervals during the education process.\n\nAt its apex, the Society had a membership consisting of over 750 hypnotherapists and mental health professionals, most on the Eastern half of the United States and Canada.\n\nThe Society was eventually disbanded when, on behalf of the Board of Trustees, Secretary Edward J. Herman announced that the Board had discovered that the Treasurer, Patricia O’Neal had embezzled the majority of the Society’s operating capital in the eighteen (18) months after Dr. Goldstein’s death in 1999.\n\nSince the dissolution of the Society, the majority of its former members have migrated to the Society For Psychological Hypnosis (division 30 of the American Hypnosis Society), the National Guild of Hypnotists, or to Dr. Milton Erickson's American Society of Clinical Hypnosis \n\n"}
{"id": "14589581", "url": "https://en.wikipedia.org/wiki?curid=14589581", "title": "American Occupational Therapy Association", "text": "American Occupational Therapy Association\n\nThe American Occupational Therapy Association (AOTA) is the national professional association established in 1917 to represent the interests and concerns of occupational therapy practitioners and students and improve the quality of occupational therapy services. AOTA membership is approximately 63,000 occupational therapists, occupational therapy assistants, and students.\n\nThe National Society for the Promotion of Occupational Therapy was the founding name of the AOTA.\n\n\n"}
{"id": "191295", "url": "https://en.wikipedia.org/wiki?curid=191295", "title": "Aswan Dam", "text": "Aswan Dam\n\nThe Aswan Dam, or more specifically since the 1960s, the Aswan High Dam, is an embankment dam built across the Nile in Aswan, Egypt, between 1960 and 1970. Its significance largely eclipsed the previous Aswan Low Dam initially completed in 1902 downstream. Based on the success of the Low Dam, then at its maximum utilization, construction of the High Dam became a key objective of the government following the Egyptian Revolution of 1952; with its ability to control flooding better, provide increased water storage for irrigation and generate hydroelectricity the dam was seen as pivotal to Egypt's planned industrialization. Like the earlier implementation, the High Dam has had a significant effect on the economy and culture of Egypt.\n\nBefore the High Dam was built, even with the old dam in place, the annual flooding of the Nile during late summer had continued to pass largely unimpeded down the valley from its East African drainage basin. These floods brought high water with natural nutrients and minerals that annually enriched the fertile soil along its floodplain and delta; this predictability had made the Nile valley ideal for farming since ancient times. However, this natural flooding varied, since high-water years could destroy the whole crop, while low-water years could create widespread drought and associated famine. Both these events had continued to occur periodically. As Egypt's population grew and technology increased, both a desire and the ability developed to completely control the flooding, and thus both protect and support farmland and its economically important cotton crop. With the greatly increased reservoir storage provided by the High Aswan Dam, the floods could be controlled and the water could be stored for later release over multiple years.\n\nThe Aswan Dam was designed by the Moscow-based Hydroproject Institute.\n\nThe earliest recorded attempt to build a dam near Aswan was in the 11th century, when the Arab polymath and engineer Ibn al-Haytham (known as \"Alhazen\" in the West) was summoned to Egypt by the Fatimid Caliph, Al-Hakim bi-Amr Allah, to regulate the flooding of the Nile, a task requiring an early attempt at an Aswan Dam. His field work convinced him of the impracticality of this scheme.\n\nThe British began construction of the first dam across the Nile in 1898. Construction lasted until 1902, and the dam was opened on 10 December 1902. The project was designed by Sir William Willcocks and involved several eminent engineers, including Sir Benjamin Baker and Sir John Aird, whose firm, John Aird & Co., was the main contractor.\n\nIn 1952, the Greek-Egyptian engineer Adrian Daninos began to develop the plan of the new Aswan Dam. Although the Low Dam was almost overtopped in 1946, the government of King Farouk showed no interest in Daninos's plans. Instead the Nile Valley Plan by the British hydrologist Harold Edwin Hurst to store water in Sudan and Ethiopia, where evaporation is much lower, was favored. The Egyptian position changed completely with the overthrow of the monarchy, led by the Free Officers Movement including Gamal Abdel Nasser. The Free Officers were convinced that the Nile Waters had to be stored in Egypt for political reasons, and within two months, the plan of Daninos was accepted. Initially, both the United States and the USSR were interested in helping the development of the dam, but this movement happened in the midst of the Cold War, as well as of growing intra-Arab rivalries.\n\nIn 1955, Nasser was trying to portray himself as the leader of Arab nationalism, in opposition to the traditional monarchies, especially the Hashemite Kingdom of Iraq following its signing of the 1955 Baghdad Pact. At that time the U.S. feared that communism would spread to the Middle East, and it saw Nasser as a natural leader of an anticommunist procapitalist Arab League. America and Britain offered to help finance construction of the High Dam, with a loan of $270 million, in return for Nasser's leadership in resolving the Arab-Israeli conflict. While opposed to communism, capitalism, and imperialism, Nasser presented himself as a tactical neutralist, and sought to work with both the U.S. and the USSR for Egyptian and Arab benefit. After a particularly criticized raid by Israel against Egyptian forces in Gaza in 1955, Nasser realized that he could not legitimately portray himself as the leader of pan-Arab nationalism if he could not defend his country militarily against Israel. In addition to his development plans, he looked to quickly modernize his military, and he turned first to the U.S.\nThe American Secretary of State John Foster Dulles and the American President Dwight Eisenhower told Nasser that the U.S. would supply him with weapons only if they were used for defensive purposes and accompanied by American military personnel for supervision and training. Nasser did not accept these conditions, but then he looked to the USSR for support. Although Dulles believed that Nasser was only bluffing and that the USSR would not aid Nasser, he was wrong— the USSR promised Nasser a quantity of arms in exchange for a deferred payment of Egyptian grain and cotton. On 27 September 1955, Nasser announced an arms deal, with Czechoslovakia acting as a middleman for the Soviet support. Instead of attacking Nasser for turning to the Soviets, Dulles sought to improve relations with him. This explains the later offer of December 1955, in which the U.S. and Britain pledged $56 and $14 million respectively towards the construction of the dam.\n\nThough the Czech arms deal actually increased the American willingness to invest at Aswan, Great Britain cited the deal as a reason for reversing its promise of funds. What angered Dulles much more was Nasser's diplomatic recognition of China, which was in direct conflict with Dulles's policy of containment. There are several other reasons why the U.S. decided to withdraw its offer of funding. Dulles believed that the USSR would not fulfill its commitment to help the Egyptians. He was also irritated by Nasser's neutrality and attempts to play both sides of the Cold War. At the time, other western allies in the Middle East, including Turkey and Iraq, were irritated and jealous that Egypt, a persistently neutral country, was being offered so much aid.\n\nIn June 1956, the Soviets offered Nasser $1.12 billion at 2% interest for the construction of the dam. On 19 July the U.S. State Department announced that American financial assistance for the High Dam was \"not feasible in present circumstances.\"\n\nOn 26 July 1956, with wide Egyptian acclaim, Nasser announced the nationalization of the Suez Canal as well as fair compensation for the former owners. Nasser planned on the revenues generated by the canal helping to fund construction of the High Dam. When the Suez War broke out, the United Kingdom, France, and Israel seized the canal and the Sinai, but pressure from the U.S. and the USSR at the United Nations and elsewhere forced them to withdraw.\n\nIn 1958, the USSR went ahead in providing support for the High Dam project.\n\nIn the 1950s, archaeologists began raising concerns that several major historical sites, including the famous temple of Abu Simbel were about to be under water. A rescue operation began in 1960 under UNESCO (for details see below under Effects).\n\nThe Soviets also provided technicians and heavy machinery. The enormous rock and clay dam was designed by the Soviet Hydroproject Institute along with some Egyptian engineers. 25,000 Egyptian engineers and workers contributed to the construction of the dams.\n\nOn the Egyptian side, the project was led by Osman Ahmed Osman's Arab Contractors. The relatively young Osman underbid his only competitor by one-half.\n\nThe Aswan High Dam is long, wide at the base, wide at the crest and tall. It contains of material. At maximum, of water can pass through the dam. There are further emergency spillways for an extra , and the Toshka Canal links the reservoir to the Toshka Depression. The reservoir, named Lake Nasser, is long and at its widest, with a surface area of . It holds of water.\n\nDue to the absence of appreciable rainfall, Egypt's agriculture depends entirely on irrigation. With irrigation, two crops per year can be produced, except for sugar cane which has a growing period of almost one year.\n\nThe high dam at Aswan releases, on average, water per year, of which some are diverted into the irrigation canals.\n\nIn the Nile valley and delta, almost benefit from these waters producing on average 1.8 crops per year. The annual crop consumptive use of water is about . Hence, the overall irrigation efficiency is 38/46 = 0.82 or 82%. This is a relatively-high irrigation efficiency. The field irrigation efficiencies are much less, but the losses are reused downstream. This continuous reuse accounts for the high overall efficiency.\n\nThe following table shows that the equal distribution of irrigation water over the branch canals taking off from the one main irrigation canal, the Mansuriya Canal near Giza, leaves much to be desired:\n\nThe salt concentration of the water in the Aswan reservoir is about , a very low salinity level. At an annual inflow of , the annual salt influx reaches 14 million tons. The average salt concentration of the drainage water evacuated into the sea and the coastal lakes is . At an annual discharge of (not counting the of salt intrusion from the sea and the lakes, see figure \"Water balances\"), the annual salt export reaches 27 million ton. In 1995, the output of salt was higher than the influx, and Egypt's agricultural lands were desalinizing. Part of this could be due to the large number of subsurface drainage projects executed in the last decades to control the water table and soil salinity.\n\nDrainage through subsurface drains and drainage channels is essential to prevent a deterioration of crop yields from waterlogging and soil salinization caused by irrigation. By 2003, more than have been equipped with a subsurface drainage system and approximately of water is drained annually from areas with these systems. The total investment cost in agricultural drainage over 27 years from 1973 to 2002 was about $3.1 billion covering the cost of design, construction, maintenance, research and training. During this period 11 large-scale projects were implemented with financial support from World Bank and other donors.\n\nThe High Dam has resulted in protection from floods and droughts, an increase in agricultural production and employment, electricity production, and improved navigation that also benefits tourism. Conversely, the dam flooded a large area, causing the relocation of over 100,000 people. Many archaeological sites were submerged while others were relocated. The dam is blamed for coastline erosion, soil salinity, and health problems.\n\nThe assessment of the costs and benefits of the dam remains controversial decades after its completion. According to one estimate, the annual economic benefit of the High Dam immediately after its completion was E£255 million, $587 million using the exchange rate in 1970 of $2.30 per E£1): £140 million from agricultural production, £100 million from hydroelectric generation, £10 million from flood protection, and £5 million from improved navigation. At the time of its construction, total cost, including unspecified \"subsidiary projects\" and the extension of electric power lines, amounted to £450 million. Not taking into account the negative environmental and social effects of the dam, its costs are thus estimated to have been recovered within only two years. One observer notes: \"The impacts of the Aswan High Dam (...) have been overwhelmingly positive. Although the Dam has contributed to some environmental problems, these have proved to be significantly less severe than was generally expected, or currently believed by many people.\" Another observer disagreed and he recommended that the dam should be torn down. Tearing it down would cost only a fraction of the funds required for \"continually combating the dam's consequential damage\" and 500,000 hectares of fertile land could be reclaimed from the layers of mud on the bed of the drained reservoir.\n\nPeriodic floods and droughts have affected Egypt since ancient times. The dam mitigated the effects of floods, such as those in 1964, 1973, and 1988. Navigation along the river has been improved, both upstream and downstream of the dam. Sailing along the Nile is a favorite tourism activity, which is mainly done during the winter when the natural flow of the Nile would have been too low to allow navigation of cruise ships. A new fishing industry has been created around Lake Nasser, though it is struggling due to its distance from any significant markets. The annual production was about 35 000 tons in the mid-1990s. Factories for the fishing industry and packaging have been set up near the Lake.\n\nThe dams also protected Egypt from the droughts in 1972–73 and 1983–87 that devastated East and West Africa. The High Dam allowed Egypt to reclaim about 2.0 million feddan (840,000 hectares) in the Nile Delta and along the Nile Valley, increasing the country's irrigated area by a third. The increase was brought about both by irrigating what used to be desert and by bringing under cultivation of 385,000 ha that were previously used as flood retention basins. About half a million families were settled on these new lands. In particular the area under rice and sugar cane cultivation increased. In addition, about 1 million feddan (420,000 hectares), mostly in Upper Egypt, were converted from flood irrigation with only one crop per year to perennial irrigation allowing two or more crops per year. On other previously irrigated land, yields increased because water could be made available at critical low-flow periods. For example, wheat yields in Egypt tripled between 1952 and 1991 and better availability of water contributed to this increase. Most of the 32 km of freshwater, or almost 40 percent of the average flow of the Nile that were previously lost to the sea every year could be put to beneficial use. While about 10 km of the water saved is lost due to evaporation in Lake Nasser, the amount of water available for irrigation still increased by 22 km. Other estimates put evaporation from Lake Nasser at between 10 and 16 cubic km per year.\n\nThe dam powers twelve generators each rated at , with a total of . Power generation began in 1967. When the High Dam first reached peak output it produced around half of Egypt's production of electric power (about 15 percent by 1998), and it gave most Egyptian villages the use of electricity for the first time. The High Dam has also improved the efficiency and the extension of the Old Aswan Hydropower stations by regulating upstream flows.\n\nLake Nasser flooded much of lower Nubia and 100,000 to 120,000 people were resettled in Sudan and Egypt.\nIn Sudan, 50,000 to 70,000 Sudanese Nubians were moved from the old town of Wadi Halfa and its surrounding villages. Some were moved to a newly created settlement on the shore of Lake Nasser called New Wadi Halfa, and some were resettled approximately 700 kilometres south to the semi-arid Butana plain near the town of Khashm el-Girba up the Atbara River. The climate there had a regular rainy season as opposed to their previous desert habitat in which virtually no rain fell. The government developed an irrigation project, called the New Halfa Agricultural Development Scheme to grow cotton, grains, sugar cane and other crops. The Nubians were resettled in twenty five planned villages that included schools, medical facilities, and other services, including piped water and some electrification.\n\nIn Egypt, the majority of the 50,000 Nubians were moved three to ten kilometers from the Nile near Kom Ombo, 45 kilometers downstream from Aswan in what was called \"New Nubia\". Housing and facilities were built for 47 village units whose relationship to each other approximated that in Old Nubia. Irrigated land was provided to grow mainly sugar cane.\n\n22 monuments and architectural complexes that were threatened by flooding from Lake Nasser, including the Abu Simbel temples, were preserved by moving them to the shores of the lake under the UNESCO Nubia Campaign. Also moved were Philae, Kalabsha and Amada.\n\nThese monuments were granted to countries that helped with the works:\n\nThese items were removed to the garden area of the Sudan National Museum of Khartoum:\n\nThe remaining archaeological sites, including the Buhen fort or the cemetery of Fadrus have been flooded by Lake Nasser.\n\nBefore the construction of the High Dam, the Nile deposited sediments of various particle size – consisting of fine sand, silt and clay – on fields in Upper Egypt through its annual flood, contributing to soil fertility. However, the nutrient value of the sediment has often been overestimated. 88 percent of the sediment was carried to the sea before the construction of the High Dam. The nutrient value added to the land by the sediment was only 6,000 tons of potash, 7,000 tons of phosphorus pentoxide and 17,000 tons of nitrogen. These amounts are insignificant compared to what is needed to reach the yields achieved today in Egypt's irrigation. Also, the annual spread of sediment due to the Nile floods occurred along the banks of the Nile. Areas far from the river which never received the Nile floods before are now being irrigated.\n\nA more serious issue of trapping of sediment by the dam is that it has increased coastline erosion surrounding the Nile Delta. The coastline erodes an estimated per year.\n\nBefore the construction of the High Dam, groundwater levels in the Nile Valley fluctuated 8–9 m per year with the water level of the Nile. During summer when evaporation was highest, the groundwater level was too deep to allow salts dissolved in the water to be pulled to the surface through capillary action. With the disappearance of the annual flood and heavy year-round irrigation, groundwater levels remained high with little fluctuation leading to waterlogging. Soil salinity also increased because the distance between the surface and the groundwater table was small enough (1–2 m depending on soil conditions and temperature) to allow water to be pulled up by evaporation so that the relatively small concentrations of salt in the groundwater accumulated on the soil surface over the years. Since most of the farmland did not have proper subsurface drainage to lower the groundwater table, salinization gradually affected crop yields. Drainage through sub-surface drains and drainage channels is essential to prevent a deterioration of crop yields from soil salinization and waterlogging. By 2003, more than 2.0 million have been equipped with a subsurface drainage system at a cost from 1973 to 2002 of about $3.1 billion.\n\nContrary to many predictions made prior to the Aswan High Dam construction and publications that followed, that the prevalence of schistosomiasis (bilharzia) would increase, it did not. This assumption did not take into account the extent of perennial irrigation that was already present throughout Egypt decades before the high dam closure. By the 1950s only a small proportion of Upper Egypt had not been converted from basin (low transmission) to perennial (high transmission) irrigation. Expansion of perennial irrigation systems in Egypt did not depend on the high dam. In fact, within 15 years of the high dam closure there was solid evidence that biharzia was declining in Upper Egypt. S. haematobium has since disappeared altogether. Suggested reasons for this include improvements in irrigation practice. In the Nile Delta, schistosomaisis had been highly endemic, with prevalence in the villages 50% or higher for almost a century before. This was a consequence of the conversion of the Delta to perennial irrigation to grow long staple cotton by the British. This has changed. Large scale treatment programmes in the 1990s using single dose oral medication contributed greatly to reducing the prevalence and severity of S. mansoni in the Delta.\n\nSediment deposited in the reservoir is lowering the water storage capacity of Lake Nasser. The reservoir storage capacity is 162 km, including 31 km dead storage at the bottom of the lake below 147 m above sea level, 90 km live storage, and 41 km of storage for high flood waters above 175m above sea level. The annual sediment load of the Nile is about 134 million tons. This means that the dead storage volume would be filled up after 300–500 years if the sediment accumulated at the same rate throughout the area of the lake. Obviously sediment accumulates much faster at the upper reaches of the lake, where sedimentation has already affected the live storage zone.\n\nBefore the construction of the High Dam, the 50,000 km of irrigation and drainage canals in Egypt had to be dredged regularly to remove sediments. After construction of the dam, aquatic weeds grew much faster in the clearer water, helped by fertilizer residues. The total length of the infested waterways was about 27,000 km in the mid-1990s. Weeds have been gradually brought under control by manual, mechanical and biological methods.\nMediterranean fishing and brackish water lake fishery declined after the dam was finished because nutrients that flowed down the Nile to the Mediterranean were trapped behind the dam. For example, the sardine catch off the Egyptian coast declined from 18,000 tons in 1962 to a mere 460 tons in 1968, but then gradually recovered to 8,590 tons in 1992. A scientific article in the mid-1990s noted that \"the mismatch between low primary productivity and relatively high levels of fish production in the region still presents a puzzle to scientists.\"\n\nA concern before the construction of the High Dam had been the potential drop in river-bed level downstream of the Dam as the result of erosion caused by the flow of sediment-free water. Estimates by various national and international experts put this drop at between 2 and 10 meters. However, the actual drop has been measured at 0.3–0.7 meters, much less than expected.\n\nThe red-brick construction industry, which consisted of hundreds of factories that used Nile sediment deposits along the river, has also been negatively affected. Deprived of sediment, they started using the older alluvium of otherwise arable land taking out of production up to 120 square kilometers annually, with an estimated 1,000 square kilometers destroyed by 1984 when the government prohibited, \"with only modest success,\" further excavation. According to one source, bricks are now being made from new techniques which use a sand-clay mixture and it has been argued that the mud-based brick industry would have suffered even if the dam had not been built.\n\nBecause of the lower turbidity of the water sunlight penetrates deeper in the Nile water. Because of this and the increased presence of nutrients from fertilizers in the water, more algae grow in the Nile. This in turn increases the costs of drinking water treatment. Apparently few experts had expected that water quality in the Nile would actually decrease because of the High Dam.\n\n\n"}
{"id": "46837255", "url": "https://en.wikipedia.org/wiki?curid=46837255", "title": "Avalere Health", "text": "Avalere Health\n\nAvalere Health is a consulting firm headquartered in Washington, D.C., specializing in strategy, policy, and data analysis for life sciences, health plans and providers. The company also publishes research studies on health care issues and the health care reform debate in the United States.\n\nAvalere was founded as The Health Strategies Consultancy LLC in 2000 by Dan Mendelson. The company was renamed Avalere Health in 2005, and in 2008 Mendelson sold a minority interest to ABS Capital Partners, a Baltimore, Maryland–based private equity firm. In September 2015, Avalere was purchased by Inovalon, a technology company providing cloud-based analytics and intervention platforms to the healthcare industry. Avalere continues to operate as a subsidiary with a focus on providing advisory services on market consolidation, cost management, quality improvement, and managed care as well as business intelligence and corporate communication strategies. The firm has also published studies on drug plan coverage and managed care plans in the US.\n\n"}
{"id": "4373182", "url": "https://en.wikipedia.org/wiki?curid=4373182", "title": "Benzene Convention, 1971", "text": "Benzene Convention, 1971\n\nBenzene Convention, 1971 is an International Labour Organization Convention.\n\nIt was established in 1971:\nHaving decided upon the adoption of certain proposals with regard to protection against hazards arising from benzene...\n\nAs of 2013, the convention has been ratified by 38 countries (Montenegro being the most recent in 2006).\n\n"}
{"id": "29665262", "url": "https://en.wikipedia.org/wiki?curid=29665262", "title": "Biodosimetry", "text": "Biodosimetry\n\nBiodosimetry is a measurement of biological response as a surrogate for radiation dose. The International Commission on Radiation Units and Measurements and International Atomic Energy Agency have issued guidance on performing biodosimetry and interpreting data.\n"}
{"id": "16568591", "url": "https://en.wikipedia.org/wiki?curid=16568591", "title": "Canada's Health Care providers, 2007", "text": "Canada's Health Care providers, 2007\n\nCanada's Health Care Providers, 2007 is a reference on the country's health care workforce. It looks at how the health provider landscape has evolved, examines the complexities of health human resources planning and management in the current environment and provides the latest information on supply trends for various health professions.\n\n\n\n\n\n\n\n"}
{"id": "52342989", "url": "https://en.wikipedia.org/wiki?curid=52342989", "title": "Cannabis in Norway", "text": "Cannabis in Norway\n\nCannabis in Norway is illegal for all purposes except medical purposes, for which it has been available through special approval since November 2016.\n\nIn December 2017, the Norwegian Parliament announced the nation would decriminalize personal drug use, including cannabis, providing medical treatment to users rather than fines and imprisonment.\n\nUp to 15 grams is considered an amount for personal use, and is punished with a fine in the case of first-time offenders; possessing more is punished more harshly. Repeat offenders or dealers can face prison charges. The type of fine given for drug offences are of the more serious category, and will appear on a criminal record. Young first-time offenders are routinely compelled to consent to regular drug testing to avoid prosecution. Up to one kilo is punished with up to 2 years in prison. If the amount is larger, the limit is 10 years. Amounts over 80 kilos are punished with sentences of 3 to 15 years, and in very serious cases up to 21 years is permitted.\n"}
{"id": "581220", "url": "https://en.wikipedia.org/wiki?curid=581220", "title": "Contraceptive sponge", "text": "Contraceptive sponge\n\nThe contraceptive sponge combines barrier and spermicidal methods to prevent conception.\n\nThree brands are marketed: Pharmatex, Protectaid and Today. Pharmatex is marketed in France and the province of Quebec; Protectaid in the rest of Canada and Europe; and Today in the United States.\n\nSponges work in two ways. First, the sponge is inserted into the vagina, so it can cover the cervix and prevent any sperm from entering the uterus. Secondly, the sponge contains spermicide.\n\nThe sponges are inserted vaginally prior to intercourse and must be placed over the cervix to be effective.\nSponges provide no protection from sexually transmitted infections. Like condoms, sponges are single-use only.\n\nThe manufacturer of the Today sponge reports effectiveness for prevention of pregnancy of 100% when used correctly and consistently. When packaging directions are not followed for every act of intercourse, effectiveness rates of 84% to 89% are reported. Other sources cite poorer effectiveness rates for women who have given birth: 74% during correct and consistent use, and 68% during typical use.\n\nStudies of Protectaid have found effectiveness rates of 77% to 91%.\n\nStudies of Pharmatex have found perfect use effectiveness rates of over 99% per year. Typical use of Pharmatex results in effectiveness of 81% per year. Sponges may be used in conjunction with another method of birth control such as condoms to increase effectiveness.\n\nTo use the Today sponge, it must be run under water until thoroughly wet, about 2 tablespoons. The water is used as a mechanism to activate the spermicide inside the sponge. No extra spermicide is needed. The Protectaid\nand Pharmatex sponges come ready to use.\n\nThe sponge can be inserted up to 24 hours before intercourse. It must be left in place for at least six hours after intercourse. It should not be worn for more than 30 hours in a row.\n\nThe sponge should never be reused once it has been removed after having sexual intercourse.\n\nThe devices have had periods of unavailability in some markets since being introduced. All three brands are currently available outside their normal marketing areas through internet retailers.\n\nThe Today Sponge was developed beginning in 1976 and introduced in the United States in 1983. Today was removed from the market in 1994 due to manufacturing problems. Following several delays, the Today brand became available again in Canada in March 2003, and in the U.S. in September 2005. After the manufacturer's parent company declared bankruptcy in 2007, production was shut down again, until the new manufacturer, Mayer Laboratories Ltd., reintroduced Today to the U.S. market in 2009.\n\nThe Pharmatex sponge was introduced in France and the Canadian province of Quebec in 1984.\n\nThe Protectaid sponge was introduced in Canada in 1996, and in Europe in 2000.\n\nSponges are a physical barrier, trapping sperm and preventing their passage through the cervix into the reproductive system. The spermicide is an important component of pregnancy prevention; each brand offers a different formula.\n\nThe Today sponge contains 1,000 milligrams (mg) of nonoxynol-9. Protectaid contains 5,000 mg of the F-5 gel, with three active ingredients (6.25 mg of nonoxynol-9, 6.25 mg of benzalkonium chloride, and 25 mg of sodium cholate). Pharmatex contains 60 mg of benzalkonium chloride.\n\nSome people are allergic to the spermicide used in the sponge. Women who use contraceptive sponges have an increased risk of yeast infection and urinary tract infection. Improper use, such as leaving the sponge in too long, can result in toxic shock syndrome.\n\nThe Today sponge contains the spermicide nonoxynol-9, which may contain certain risks for those using the sponge multiple times a day, or for those at risk for HIV. In these cases, nonoxynol-9 can irritate the tissue, which leads to an increased risk of HIV and other sexually transmitted infections.\n\n\n"}
{"id": "13976738", "url": "https://en.wikipedia.org/wiki?curid=13976738", "title": "Corporate Accountability International", "text": "Corporate Accountability International\n\nCorporate Accountability (formerly INFACT, Corporate Accountability International) is a non-profit organization, founded in 1977. Their campaign headquarters are in Boston, Massachusetts and they have offices in Oakland, California, Seattle, Washington, and Bogotá, Colombia. Currently, their most prominent campaign is their climate campaign to kick Big Polluters out of climate policy.\n\nSince 1977 Corporate Accountability has waged a number of high-profile campaigns to protect public health, the environment and democracy from abuse by transnational corporations.\n\nFrom 1977 to 1986 the Infant Formula Campaign and Nestlé Boycott brought about significant reforms in the life-threatening marketing of infant formula in developing countries. The work of Corporate Accountability International and allies contributed to the passage of the World Health Organization's International Code of Marketing of Breast-milk Substitutes in 1981.\n\nFrom 1984 to 1993 the Nuclear Weaponmakers Campaign and General Electric (GE) Boycott helped push industry leader GE out of the nuclear weapons business and exposed the human and environmental costs of the corporation's nuclear weapons production and promotion. The international boycott of GE products cost the company over $19 million in lost medical equipment sales and $100 million in overall sales. Major retail stores including Safeway and Target began stocking light bulbs made by other companies. In 1991, Corporate Accountability International commissioned the Academy Award winning documentary, \"Deadly Deception: General Electric, Nuclear Weapons, and Our Environment\" that juxtaposed \"GE's rosy 'We Bring Good Things To Life' commercials with the true stories of workers and neighbors whose lives have been devastated by the company's involvement in building and testing nuclear bombs.\" In 1993, GE caved under enormous public pressure and moved out of the nuclear weapons business.\n\nIn 1994 Corporate Accountability launched the Challenging Big Tobacco Campaign. In 2003, years of campaigning culminated in the adoption of the world's first public health and corporate accountability treaty—the World Health Organization Framework Convention on Tobacco Control. \nThey spent the fall of 2005 working alongside other organizations to get a number of African countries to ratify the treaty and also gained notice for their attempts to get the US to ratify the Framework Convention on Tobacco Control. In 2009 they gained notice for instigating the removal of tobacco company representatives from a UN-backed meeting on tobacco smuggling. The Challenging Big Tobacco Campaign is currently focused on expanding implementation and enforcement of the Framework Convention on Tobacco Control.\n\nIn 2004 Corporate Accountability launched the Think Outside the Bottle Campaign to promote, protect and ensure public funding for public water systems and challenge corporations who undermine public confidence in tap water. Recently, Corporate Accountability’s Think Outside The Bottle Campaign has garnered international notice. The campaign has been supported by Salt Lake City mayor Rocky Anderson, who has also begun his own “Knock Out Bottled Water” website,\nSan Francisco mayor Gavin Newsom, and more.\nThe campaign also played a major role in the July 2007 decision by PepsiCo to change the label on their Aquafina bottled water to more plainly state it is sourced from public water.\nThe campaign was also featured on NBC Nightly News in October 2007. On World Water Day March 22, 2010 Corporate Accountability released the film Story of Bottled Water with the Story of Stuff project.\n\nIn 2009 Corporate Accountability launched the Value [The] Meal Campaign challenging corporate abuse of food by the fast food industry. The campaign demands to the fast food industry include: stop fast food marketing, promotion and sponsorship that appeals to children and teenagers; stop manipulating public health policy and nutrition science; and provide complete, accurate and non-promotional information about the health risks of fast food. On a similar tack, in April 2010 the nonprofit began calling for the 'retirement' of Ronald McDonald, saying the venerable mascot fuels childhood obesity.\n\nIn 2014, Corporate Accountability launched its climate campaign. It began to organize with people around the world to hold fossil fuel corporations accountable and remove them from the policymaking process. Corporate Accountability's climate campaign has turned what was once an untouchable subject — the fossil fuel industry’s conflicts of interest in climate policy — into a hotly debated issue at the U.N. climate treaty negotiations and in national policymaking \n. Corporate Accountability has also supported policymakers from former President Obama to Massachusetts Attorney General Maura Healey to take decisive action on climate change and hold the fossil fuel industry accountable. \n\nMembers of the campaign advisory board include Frances Moore Lappé author of Diet for a Small Planet, Susan Linn, EdD co-founder and director of The Campaign for a Commercial-Free Childhood, Marion Nestle Ph.D., M.P.H., the Paulette Goddard Professor of Nutrition, Food Studies, and Public Health at New York University, Ronnie Cummins National Director of the Organic Consumers Association, David L. Katz MD, MPH, FACPM, FACP Director and founder of Yale University’s Prevention Research Center, Raj Patel, PHD author of Stuffed and Starved: The Hidden Battle for the World Food System, Scot Quaranda the Campaign Director for Dogwood Alliance, Michele Simon, JD, MPH author of Appetite for Profit: How the Food Industry Undermines Our Health and How to Fight Back, and Judy Wicks founder of Philadelphia’s White Dog Cafe.\n\n"}
{"id": "18584283", "url": "https://en.wikipedia.org/wiki?curid=18584283", "title": "Dana Goldman", "text": "Dana Goldman\n\nDana Paul Goldman is the Leonard D. Schaeffer Chair and director of the University of Southern California Leonard D. Schaeffer Center for Health Policy and Economics and Professor of Public Policy, Pharmacy, and Economics at the USC Sol Price School of Public Policy and USC School of Pharmacy. He is also an adjunct professor of health services and radiology at UCLA, and a managing director and founding partner, along with Darius Lakdawalla and Tomas J. Philipson, at Precision Heath Economics, a health care consulting firm. Previously held positions include the director of the Bing Center for Health Economics, RAND Royal Center for Health Policy Simulation, and UCLA/RAND Health Services Research Postdoctoral Training Program.\n\nGoldman's professional interests include the innovation of health technology, the future of America's elderly population, the design of insurance, and disparities in health outcomes. More recently, his work has focused on medical innovation and regulation, comparative effectiveness and outcomes research, and patient-reported outcomes in emerging markets\n\nGoldman is also the founding co-editor of the Forum for Health Economics and Policy and has been on the editorial board of Health Affairs, B.E. Journals of Economic Analysis and Policy, and the RAND Journal of Economics, among others. He is a health policy advisor to the Congressional Budget Office and, in 2009, was elected a member of the Institute of Medicine. He is also the 2009 recipient of the Eugene Garfield Economic Impact Prize, in recognition of his outstanding research on how medical research impacts the economy.\n\nHe received his B.A. \"summa cum laude\" from Cornell University and a Ph.D. in Economics from Stanford University.\n\nGoldman’s 1997 article, \"Redistributional Consequences of Community Rating\" discusses a study done in California where health insurance premiums were based on community ratings. The Patient Protection and Accountable Care Act (PPACA) has been passed through Congress and implementation has commenced in the U.S. healthcare system. Community rating systems will be the basis for health care premiums in the future exchange system. These ratings, pool people in to demographic groups and charge all members a constant rate. The goal of this system is to prevent medical underwriting and decrease the inequities that occur for clients with higher risks of increased medical utilization present in a risk adjusted system. Goldman et al. conducted a study in California that trialed such clusters of insured clients by pooling at the state level, regional level, and metropolitan level. Results from California’s experiment with such a system conclude that the larger the areas pooled, the greater the transfer of costs.\n\nAnother challenge to community based rating systems is that lower income neighborhoods with lower healthcare utilization subsidize the healthcare cost of higher income communities that tend to have higher healthcare expenditures.\n\nThe health exchange system will not be the only place where community rating systems are utilized. Centers for Medicare and Medicaid Services (CMS) (CMS) plans to initiate a modified rating system starting in January 2014, this may lead other insurance companies following suit.\n\nDue to the results of the California experiment and the widespread use of community based rating systems being initiated, it is imperative that some kind of subsidies be used for low income families. Otherwise, these families may be more likely to avoid insurance due to higher premium costs. This result would be counter intuitive to the goals of implementing the Patient Protection and Affordable Care Act in the U.S. healthcare system.\n\nData from the Centers for Disease Control and Prevention (CDC) revealed that chronic illness affected 133 million people in the United States and accounted for seven out of ten deaths. In relationship to these numbers, the American Society of Health System Pharmacists say Americans spent $307.5 billion on pharmaceuticals in 2010.\n\nResearch by Goldman, Joyce, Escarce, Pace, Soloman, Laouri, Landsman, and Teutsch (2001) studied the purchasing behavior of drugs used to treat eight chronic illnesses: diabetes, high blood pressure, high cholesterol, asthma, depression, allergies, arthritis, and stomach ulcers. This retrospective study presents a strong correlation between co-payment levels and medication use for these chronic illnesses. The study illustrated the change in consumption behaviors based on plan generosity and structure such as coinsurance rates and mandatory generic substitution.\n\nThe study by Goldman et al. (2001) predicts there would be a significant decrease in medication utilization in all of the chronic disease categories examined when co-payments were doubled. However, of note, the researchers discovered that patients respond discriminatorily to changes in co-payment and are less likely to reduce consumption of disease specific medications and will reduce pharmacy spending in other medications.\nGoldman, et al. (2001) exposed the largest decrease in drug spending when co-payments were doubled were in medications to treat arthritis and allergies.\n\nThe study revealed that patients with diabetes decreased their purchase of diabetes drugs the most compared to the other chronic illnesses examined when their co-payments doubled.\n\nThe research by Goldman et al. (2001) reveals two points that could inform public policy related to pharmaceutical expenditures. One, consumption of over-the-counter drugs to treat allergies and arthritis are highly influenced by out of pocket spending. Two, diabetes patients may choose lifestyle behavior changes when faced with higher drug costs.\n\nBefore changing payment structures, more research will be needed to examine adverse health consequences in the chronically ill if pharmaceutical interventions are limited by increasing out of pocket expenses. For instance, emergency department utilization could rise in response to these changes.\n\nThe results of the study by Goldman et al. (2001) could inform public policy on ways to decrease excess drug usage when the benefits are less than the cost of the drug.\n\n\n"}
{"id": "5592849", "url": "https://en.wikipedia.org/wiki?curid=5592849", "title": "Delivery after previous caesarean section", "text": "Delivery after previous caesarean section\n\nIn case of a previous caesarean section a subsequent pregnancy can be planned beforehand to be delivered by either of the following two main methods:\n\nBoth have higher risks than a vaginal birth with no previous caesarean section. There are many issues which affect the decision for planned vaginal or planned abdominal delivery. There is a slightly higher risk for uterine rupture and perinatal death of the child with VBAC than ERCS, but the absolute increased risk of these complications is small, especially with only one previous low transverse caesarean section. 60–80% of women planning VBAC will achieve a successful vaginal delivery, although there are more risks to the mother and baby from an unplanned caesarean section than from an ERCS. Successful VBAC also reduces the risk of complications in future pregnancies than ERCS.\n\nWhere the woman is labouring with a previous section scar (i.e. a planned VBAC in labour), depending on the provider, special precautions may be recommended. These include intravenous access (a cannula into the vein) and continuous fetal monitoring (cardiotocography or CTG monitoring of the fetal heart rate with transducers on the mother's abdomen). Most women in the UK should be counselled to avoid induction of labour if there are no medical reasons for it, as the risks of uterine rupture of the previous scar are increased if the labour is induced. Other intrapartum management options, including analgesia/anesthesia, are identical to those of any labour and vaginal delivery.\n\nFor ERCS, the choice of skin incision should determined by what seems to be most beneficial for the present operation, regardless of the choice of the previous location as seen by its scar, although the vast majority of surgeons will incise through the previous scar to optimise the cosmetic result. Hypertrophic (very thick or unsightly) scars are best excised because it gives a better cosmetic result and is associated with improved wound healing. On the other hand, keloid scars should have their margins left without any incision because of risk of tissue reaction in the subsequent scar.\n\nThe choice of VBAC or ERCS depends on many issues: medical and obstetric indications, maternal choice and availability of provider and birth setting (hospital, birthing center, or home). Some commonly employed criteria include:\n\nAccording to the American Pregnancy Association, 90% of women who have undergone caesarean deliveries are candidates for planned VBAC because there are no obvious antenatal reasons for them as individuals why planned ERCS is safer.\n\n\n\nAccording to ACOG guidelines, the following criteria may reduce the likelihood of VBAC success but should NOT preclude a trial of labour: having two prior caesarean sections, suspected fetal macrosomia at term (fetus greater than 4000-4500 grams in weight), gestation beyond 40 weeks, twin gestation, and previous low vertical or unknown previous incision type, provided a classical uterine incision is not suspected.\n\nThe presence of any of the following practically always mean that ERCS will be performed - but this decision should always be discussed with a senior obstetrician: \n\nVBAC, compared to vaginal birth without a history of Caesarean section, confers an increased risks for placenta previa, placenta accreta, prolonged labor, antepartum hemorrhage, uterine rupture, preterm birth, low birth weight, and stillbirth. However, some risks may be due to confounding factors related to the indication for the first caesarean, rather than due to the procedure itself.\n\nVBAC and ERCS differ in outcomes on many end-points.\n\nThe American Congress of Obstetricians and Gynecologists (ACOG) states that VBAC is associated with decreased maternal morbidity and a decreased risk of complications in future pregnancies than ERCS.\n\nA caesarean section leaves a scar in the wall of the uterus which is considered weaker than the normal uterine wall. A VBAC carries a risk of uterine rupture of 22–74/10,000. There is virtually no risk of uterine rupture in women undergoing ERCS (i.e. a section before the onset of labour). If a uterine rupture does occur, the risk of perinatal death is approximately 6%. Mothers with a previous lower uterine segment cesarean are considered the best candidates for VBAC, as that region of the uterus is under less physical stress during labor and delivery.\n\nA VBAC carries a 2–3/10,000 additional risk of birth-related perinatal death when compared with ERCS. The absolute risk of such birth-related perinatal loss is comparable to the risk for women having their first birth. Planned VBAC carries an 8/10,000 risk of the child developing hypoxic ischaemic encephalopathy, but the long-term outcome of the infant with HIE is unknown and related to many factors.\n\nOn the other hand, attempting VBAC reduces the risk that the child will have respiratory problems after birth such as infant respiratory distress syndrome (IRDS), as rates are estimated at 2–3% with planned VBAC and 3–4% with ERCS.\n\nOf the women who have previously had a Caesarean, only about 8% of them will opt for a VBAC. However, of the 8% that opt for a VBAC, between 75%-80% will successfully give birth vaginally, which is comparable to the overall vaginal delivery rate in the United States in 2010.\n\nThe chance of having a successful VBAC is decreased by the following factors:\nWhen the first four factors are present, successful VBAC is achieved in only 40% of cases. In contrast, in women with a previous caesarean section who have had a subsequent vaginal birth, the chance of a successful vaginal birth again is estimated at 87–90%.\n\nERCS, as compared to VBAC, further increases the risks of complications in future pregnancies. Complications whose risks significantly increase with increasing number of repeated caesarean sections include:\n\nAside from uterine rupture risk, the drawbacks of VBAC are usually minor and identical to those of any vaginal delivery, including the risk of perineal tearing. Maternal morbidity, NICU admissions, length of hospital stay, and medical costs are typically reduced following a VBAC rather than a repeat caesarean delivery.\n\nA VBAC, compared with ERCS, carries around 1% additional risk of either blood transfusion (mainly because of antepartum hemorrhage), postpartum haemorrhage or endometritis.\n\nWhile vaginal births after caesarean (VBAC) are not uncommon today, the rate of VBAC has declined to include less than 10% of births after previous caesarean in the USA. Although caesarean deliveries made up only 5% of births overall in the USA until the mid-1970s, it was commonly believed that for women with previous caesarean sections, \"Once a Caesarean, always a Caesarean\". A consumer-driven movement supporting VBAC changed medical practice and led to soaring rates of VBAC in the 1980s and early 1990s, but rates of VBAC dramatically dropped after the publication of a highly publicized scientific study showing worse outcomes for VBACs as compared to repeat caesarean and the resulting medicolegal changes within obstetrics. In 2010, the National Institutes of Health, U.S. Department of Health and Human Services, and American Congress of Obstetrics and Gynecology all released statements in support of increasing VBAC access and rates.\n\nAlthough caesarean sections made up only 5% of all deliveries in the early 1970s, among women who did have primary caesarean sections, the century-old opinion held, \"Once a caesarean, always a caesarean.\" Overall, cesarean sections became so commonplace that the caesarean delivery rate climbed to over 31% in 2006. A mother-driven movement supporting VBAC changed standard medical practice, and rates of VBAC rose in the 1980s and early 1990s. However, a major turning point occurred in 1996 when one well publicized study in \"The New England Journal of Medicine\" reported that vaginal delivery after previous caesarean section resulted in more maternal complications than a repeat caesarean delivery. The American Congress of Obstetrics and Gynecology subsequently issued guidelines which identified VBAC as a high-risk delivery requiring the availability of an anesthesiologist, an obstetrician, and an operating room on standby. Logistical and legal (professional liability) concerns led many hospitals to enact overt or de facto VBAC bans. As a result, the rate at which VBAC was attempted fell from 26% in the early 1990s to 8.5% in 2006.\n\nIn March 2010, the National Institutes of Health met to consolidate and discuss the overall up-to-date body of VBAC scientific data and concluded, \"Given the available evidence, trial of labor is a reasonable option for many pregnant women with one prior low transverse uterine incision.\". Simultaneously, the U.S. Department of Health and Human Services Agency for Healthcare Research and Quality reported that VBAC is a reasonable and safe choice for the majority of women with prior caesarean and that there is emerging evidence of serious harms relating to multiple caesareans. In July 2010, The American Congress of Obstetricians and Gynecologists (ACOG) similarly revised their own guidelines to be less restrictive of VBAC, stating, \"Attempting a vaginal birth after cesarean (VBAC) is a safe and appropriate choice for most women who have had a prior cesarean delivery, including for some women who have had two previous cesareans.\" and this is also the current position of the Royal College of Obstetricians and Gynaecologists in the UK.\n\nEnhanced access to VBAC has been recommended based on the most recent scientific data on the safety of VBAC as compared to repeat caesarean section, including the following recommendation emerging from the NIH VBAC conference panel in March 2010, \"We recommend that hospitals, maternity care providers, health care and professional liability insurers, consumers, and policymakers collaborate on the development of integrated services that could mitigate or even eliminate current barriers to trial of labor.\" The U.S Department of Health and Human Services' Healthy People 2020 initiative includes objectives to reduce the primary caesarean rate and to increase the VBAC rate by at least 10% each.\n\nThe American Congress of Obstetricians and Gynecologists (ACOG) modified the guidelines on vaginal birth after previous Caesarean delivery in 1999, 2004, and again in 2010. In 2004, this modification to the guideline included the addition of the following recommendation:\n\nBecause uterine rupture may be catastrophic, VBAC should be attempted in institutions equipped to respond to emergencies with physicians immediately available to provide emergency care.\n\nThe recommendation for access to emergency care during trial of labor has, in some cases, had a major impact on the availability of VBACs to birthing mothers in the US. For example, a study of the change in frequency of VBAC deliveries in California after the change in guidelines, published in 2006, found the VBAC rate fell to 13.5% after the change, compared with 24% VBAC rate before the change. The new recommendation has been interpreted by many hospitals as indicating a full surgical team must be standing by to perform a Caesarean section for the full duration of a VBAC woman's labor. Hospitals that prohibit VBACs entirely are said to have a 'VBAC ban'. In these situations, birthing mothers are forced to choose between having a repeat Caesarean section, finding an alternate hospital in which to deliver their babies or attempting delivery outside the hospital setting.\n\nMost recently, enhanced access to VBAC has been recommended based on updated scientific data on the safety of VBAC as compared to repeat caesarean section, including the following recommendation emerging from the NIH VBAC conference panel in March 2010, \"We recommend that hospitals, maternity care providers, health care and professional liability insurers, consumers, and policymakers collaborate on the development of integrated services that could mitigate or even eliminate current barriers to trial of labor.\" The U.S Department of Health and Human Services' Healthy People 2020 initiative includes objectives to reduce the primary cesarean rate and to increase the VBAC rate by at least 10% each.\n\nACOG recommends that obstetricians offer most women with one prior cesarean section with a low-transverse incision a trial of labor (TOLAC) and that obstetricians should discuss the risks and benefits of VBAC with these patients.\n\nThis VBAC success calculator is a useful educational tool (noted by the US Agency for Healthcare Research and Quality) for clinicians who are discussing the risks and benefits of VBAC with their patients.\n\n\n"}
{"id": "45364687", "url": "https://en.wikipedia.org/wiki?curid=45364687", "title": "Face-bow", "text": "Face-bow\n\nA face-bow is a dental instrument used in the field of prosthodontics. Its purpose is to transfer functional and aesthetic components from patient's mouth to the dental articulator. Specifically, it transfers the relationship of maxillary arch and temporomandibular joint to the casts. It records the upper model's (maxilla) relationship to the External Acoustic Meatus, in the hinge axis. It aids in mounting maxillary cast on the articulator.\n\nU-shaped frame - forms the main part of the frame with remaining components attached to it by clamps. Frame extends from the region of TMJ or external acoustic meatus to a distance of 2-3 inches in front of the face.\n\nCondylar rods – are positioned 13 mm anterior to the auditory meatus on the Canto-Tragal line. This placement generally locates the rods within 5 mm of the true centre of the opening hinge axis of the jaw.\n\nBite fork – consist of stem and prongs. Wax material is usually attached to the bite fork, and the bite fork is held in contact with maxillary jaw or mandibular jaw in kinematic face-bow.\n\nLocking device – helps to attach the bite fork to the U-shaped frame.\n\nOrbital pointer with clamp – used as a third reference point. The pointer tip is placed in the contact with infraorbital notch which is 43 mm above the incisal edge of the right incisors.\n\n\nGeorge B. Snow is credited as the inventor of face-bow. In his version of face-bow, he positioned the plaster cast in the articulator in respect to distance of median incisal point from the condyles and all the other points on the occlusal plane. Snow attempted to give the occlusal plane an individual position also in this third dimension : and in order to achieve this he set about as follows. He fixed his bite-fork in the upper occlusion rim in such away that the handle, when the rim was placed in the patient’s mouth. was parallel with a plane extending from the bottom of the glenoid fossa and passing through the anterior nasal spine. This plane cannot be determined directly on a living person; but it approximately corresponds with a line drawn from the upper part of the tragus to the lower edge of the nostril. In American literature, this plane is known as the Bromell plane, in Europe as the Camper plane. Snow then placed the bite-fork horizontally when the casts were mounted in the articulator.\n\n Boucher"}
{"id": "17612590", "url": "https://en.wikipedia.org/wiki?curid=17612590", "title": "Finnish Red Cross", "text": "Finnish Red Cross\n\nThe Finnish Red Cross (FRC, , ) is an independent member of the International Federation of Red Cross and Red Crescent Societies, which is one of the biggest and best-known international organisations in the world and in the field of humanitarian aid. FRC has over 90,000 members and around 45,000 active volunteers in Finland. FRC consists of 12 regional chapters and 550 local branches throughout the country. The current General Secretary is Ms. Kristiina Kumpula. At the end of 2006, the FRC employed 1,168 people, of which 136 worked at the headquarters in Helsinki.\n\nThe Finnish Red Cross is based on volunteering and has branches in almost every municipality in Finland. The organisational structure has three layers: local branches, regional chapters, and the national headquarters. Volunteers are important decision-makers at every level. At the local level, the activities of the branches are determined to a large part by how dedicated their members are.\n\nFinnish Red Cross was founded on May 7, 1877 to care for the wounded and sick soldiers of the Finnish Guard in the Russo–Turkish War. The FRC was recognized by the ICRC in 1920 and became a member of IFRC in 1922, when Finland ratified the Geneva Conventions.\n\nIn 1948 the FRC took the operational responsibility of the blood transfusion services in Finland, which had been operated by the Finnish Scouts since 1935. The FRC transfusion service is a legally independent organization.\n\nThe Blood Transfusion Service complements the national health services and is at the same time an important component of FRC's overall structure.\n\nThe FRC also runs a national AIDS helpline and organises campaigns. In addition, it trains and finds support persons for HIV infected and their families.\n\nFinnish Red Cross Drug and Alcohol programme includes prevention work and early intervention. FRC has volunteers who act as counsellors in schools and other communities in their own expertise. At the summertime they will be on call almost at hundred of festivals discussing intoxicant use.\n\nThe FRC holds a fundraiser known as Hunger Day every autumn, usually in September. The campaign has an annual theme; in 2006 people were reminded how desertification, flooding and storms most affect the children under five years of age. On the day volunteer fundraisers can be seen in front of grocery stores and other public places. In schools school meals can be reduced and served, for example, without salad, bread or milk.\n\nThe FRC trains over 80,000 people yearly on the first aid and CPR courses for the general public and companies. FRC has more than 550 first aid groups around the country with more than 8,000 volunteers. These groups are on duty daily in the majority of public events in Finland. They can also help in search and rescue when requested by the authorities. FRC also coordinates the Voluntary Rescue Service which is formed by over 40 civic organisations and over 30,000 volunteers.\n\nThe FRC has a national emergency group of psychologists which organises and coordinates psycho-social support in situations they are needed, for example after the Jokela school shooting.\n\nFRC's Disaster Relief Fund is used to give assistance in accidents and due to social reasons. Domestic aid was about FIM 3.6 million (USD 955,000) in 2001.\n\nThe organisation also has youth programs and youth based operations. Members between 13–29 years of age compose 10% of all members and operate on all levels of the organisation. \nIt was decided in the Finnish Red Cross 2008 national meeting that youth work will be supported also in the future, and that the goal for 2011 is to gain 5% more youth members.\n\nThe Finnish Red Cross gives international aid – personnel, funds, material – in response to appeals from the International Movement. There are two main sources for funds: FRC's Disaster Relief Fund and the Finnish government. In recent years, the European Union has also provided funds.\n\nDisaster relief is given to victims of war, conflict and natural disasters. Development cooperation improves the capacities and disaster preparedness of newly established sister societies or those with fewer resources. The FRC maintains its own Logistics Centre and International Personnel Reserve in Finland. The Disaster Relief Fund provides funds when needed. Funds are collected continuously and preferably as non-earmarked.\n\nFRC's International Personnel Reserve comprises about 600 professionals trained on FRC's basic and further training courses. Approx. 100 of them work in international assignments every year.\n\nFRC is also a member organisation of the European Council on Refugees and Exiles (ECRE) and Reuter's AlertNet network.\n\n"}
{"id": "59050878", "url": "https://en.wikipedia.org/wiki?curid=59050878", "title": "Health in Anguilla", "text": "Health in Anguilla\n\nHealth in Anguilla is managed by the Health Authority of Anguilla, an agency of the Government of Anguilla.\n\nHealth facilities were devastated in 2017 as a result of Hurricane Irma. Immediate medical support was provided by the Pan American Health Organization and Direct Relief. The British government and charitable donations have contributed towards the rebuilding program. Tim Foy says \"The United Kingdom government is committed to the full rehabilitation of the Princess Alexandra Hospital\".\n\nIn November 2018 it established a charitable foundation, the Health Authority of Anguilla Foundation, which is to be registered in Delaware as a 501(c)(3) organization which will allow tax-free contributions from donors in the USA. This is intended to help close \"the gap between the health needs and desires of the community and the capacity to meet these needs and desires\".\n\nPeople on the island often use health facilities on St. Maarten. Their access to the British NHS is limited. There are concerns about the effect of Brexit on access to medicine and medical treatment.\n\nThe Ministry of Health and Social Development employed Dr. Olufunmike Banks-Devonish as a Clinical Psychologist in November 2018, the first full-time clinical psychologist to be employed by the Anguilla Public Service.\n\nWater supply is the responsibility of the Water Corporation of Anguilla, which entered into a 10 year contract with Seven Seas Water, a subsidiary of AquaVenture Holdings Limited in October 2018 for the supply of 500 thousand gallons per day, due to expand to 750 thousand gallons per day. Testing of water on the island is done by the St. Maarten Laboratory Services. \n"}
{"id": "37943308", "url": "https://en.wikipedia.org/wiki?curid=37943308", "title": "Hearing level", "text": "Hearing level\n\nHearing level is the sound pressure level produced by an audiometer at a specific frequency. It is measured in decibels with reference to audiometric zero. Hearing of speech is considered to be impaired when the hearing level is shifted 25 dB or more.\n"}
{"id": "7584848", "url": "https://en.wikipedia.org/wiki?curid=7584848", "title": "Helen Liebmann", "text": "Helen Liebmann\n\nHelen Liebmann was a founding member (along with Simon Jeffes) of the avant garde music group Penguin Café Orchestra in 1973. A cellist, she studied at the Royal Academy of Music. In addition to playing cello with a number of different ensembles, she is also a practicing music therapist.\n"}
{"id": "4167519", "url": "https://en.wikipedia.org/wiki?curid=4167519", "title": "High-energy visible light", "text": "High-energy visible light\n\nIn ophthalmology, high-energy visible light (HEV light) is high-frequency, high-energy light in the violet/blue band from 400 to 450 nm in the visible spectrum. Despite a lack of concurring scientific evidence, HEV light has sometimes been claimed to be a cause of age-related macular degeneration. Some sunglasses and beauty creams specifically block HEV, for added marketing value.\n\nBlue-light hazard is defined as the potential for a photochemically-induced retinal injury resulting from electromagnetic radiation exposure at wavelengths primarily between 400 and This study has not been done in humans, only inconclusively in some rodent, primate, and in vitro studies. The mechanisms for photochemically-induced retinal injury are caused by the absorption of light by photoreceptors in the eye. Under normal conditions when light hits a photoreceptor, the cell bleaches and becomes useless until it has recovered through a metabolic process called the visual cycle.\n\nAbsorption of blue light, however, has been shown in rats and a susceptible strain of mice to cause a reversal of the process where cells become unbleached and responsive again to light before they are ready. At wavelengths of blue light below 430 nm this greatly increases the potential for oxidative damage. For blue-light circadian therapy, harm is minimized by employing blue light at the near-green end of the blue spectrum. \"1-2 min of 408 nm and 25 minutes of 430 nm are sufficient to cause irreversible death of photoreceptors and lesions of the retinal pigment epithelium. ... The action spectrum of light-sensitive retinal ganglion cells was found to peak at approximately 450 nm, a range with lower damage potential, yet not completely outside the damaging range.\" A 2014 study found that LEDs cause retinal damage even in settings where they are used indirectly, such as household light bulbs.\n\nAn unpublished and non peer-reviewed 2013 in vitro study financed by skin care company Lipo Chemicals using shorter blue band spectrum LED lights claims that prolonged exposure may permanently damage the pigment epithelial cells of the retina. However, according to a specialist, the test conditions were the equivalent of staring at a blue light equivalent to a 100 watt incandescent source from for 12 hours, which is not deemed to be a realistic light exposure.. Sébastien Point and colleagues discussed in some recent peer-reviewed publications the validity of rodent models and conclude that LEDs are not a problem of public health in normal use but that caution must be taken for newborn infants, as their eye collect more light and for specific occupational situations for which observers can look at high power leds for hours (for example LEDs quality control or alternative light therapies). \nA recent study has given more insight about the Blue light hazard: permanent damage to the eye cells, as reported by a research made by a team from Toledo University , especially for children, who are big users of LED screen (smartphones,tablets...). However, the conclusion that LED technology would be more dangerous for retina than Tungsten lamp and fluorescent lamp is debated, as the LED spectrum does not contain potentially retinotoxic violet light (to which a child's lens is highly transparent) in contrary to spectra of fluorescent and incandescent lamps which are rich in violet light.\n\nBlue light within the range 400-450 nm has been reported in a number of studies to be effective as local treatment of eczema and psoriasis, as it purportedly helps dampen the immune response. Recent studies have also shown improvement of facial acne upon exposure to a LED emitting at 414 nm. A combination of exposure to red and blue lights is used more and more in clinical dermatologic therapies. Constructors such as Philips currently develop devices and techniques emitting in the blue visible spectrum to be used in dermatologic therapy.\n\n\n"}
{"id": "38600542", "url": "https://en.wikipedia.org/wiki?curid=38600542", "title": "History of medicine in the United States", "text": "History of medicine in the United States\n\nThe history of medicine in the United States encompasses a variety of periods and approaches to health care in the United States from colonial days to the present, ranging from early folk remedies to the increasing professionalization and managed care of modern medicine.\n\nColonial era health care was based primarily on traditional medicines and traditional cures. Professionalization was very slow before 1750, by which time there were a handful of cities of more than 20,000 population, each of which had physicians trained in England and Scotland, as well as a growing number of locally trained men. Cotton Mather of Boston was the first significant figure in American medicine.\n\nMortality was very high for new arrivals, and high for children in the colonial era. Malaria was deadly to many new arrivals. The disease environment was very hostile to European settlers, especially in all the Southern colonies. Malaria was endemic in the South, with very high mortality rates for new arrivals. Children born in the new world had some immunity—they suffered mild recurrent forms of malaria but survived. For an example of newly arrived able-bodied young men, over one-fourth of the Anglican missionaries died within five years of their arrival in the Carolinas. Mortality was high for infants and small children, especially from diphtheria, yellow fever, and malaria. Most sick people turn to local healers, and used folk remedies. Others relied upon the minister-physicians, barber-surgeons, apothecaries, midwives, and ministers; a few used colonial physicians trained either in Britain, or an apprenticeship in the colonies. There was little government control, regulation of medical care, or attention to public health. By the 18th century, Colonial physicians, following the models in England and Scotland, introduced modern medicine to the cities. This allowed some advances in vaccination, pathology, anatomy and pharmacology.\n\nThere was a fundamental difference in the human infectious diseases present in the indigenous peoples and that of sailors and explorers from Europe and Africa. Some viruses, like smallpox, have only human hosts and appeared to have never occurred on the North American continent before 1492. The indigenous people lacked genetic resistance to such new infections, and suffered overwhelming mortality when exposed to smallpox, measles, malaria, tuberculosis and other diseases. The depopulation occurred years before the European settlers arrived in the vicinity and resulted from contact with trappers.\n\nThe city of New Orleans, Louisiana opened two hospitals in the early 1700s. The first was the Royal Hospital, which opened in 1722 as a small military infirmary, but grew to importance when the Ursuline Sisters took over the management of it in 1727 and made it a major hospital for the public, with a new and larger building built in 1734. The other was the Charity Hospital, which was staffed by many of the same people but was established in 1736 as a supplement to the Royal Hospital so that the poorer classes (who usually could not afford treatment at the Royal Hospital) had somewhere to go.\n\nIn most of the American colonies, medicine was rudimentary for the first few generations, as few upper-class British physicians emigrated to the colonies. The first medical society was organized in Boston in 1735. In the 18th century, 117 Americans from wealthy families had graduated in medicine in Edinburgh, Scotland, but most physicians learned as apprentices in the colonies. In Philadelphia, the Medical College of Philadelphia was founded in 1765, and became affiliated with the university in 1791. In New York, the medical department of King's College was established in 1767, and in 1770, awarded the first American M.D. degree.\n\nSmallpox inoculation was introduced 1716-1766, well before it was accepted in Europe. The first medical schools were established in Philadelphia in 1765 and New York in 1768. The first textbook appeared in 1775, though physicians had easy access to British textbooks. The first pharmacopoeia appeared in 1778. The European populations had a historic exposure and partial immunity to smallpox, but the Native American populations did not, and their death rates were high enough for one epidemic to virtually destroy a small tribe.\n\nPhysicians in port cities realized the need to quarantine sick sailors and passengers as soon as they arrived. Pest houses for them were established in Boston (1717), Philadelphia (1742) Charleston (1752) and New York (1757). The first general hospital was established in Philadelphia in 1752.\n\nIn the colonial era, women played a major role in terms of healthcare, especially regarding midwives and childbirth. Local healers used herbal and folk remedies to treat friends and neighbors. Published housekeeping guides included instructions in medical care and the preparation of common remedies. Nursing was considered a female role. Babies were delivered at home without the services of a physician well into the 20th century, making the midwife a central figure in healthcare.\n\nThe professionalization of medicine, starting slowly in the early 19th century, included systematic efforts to minimize the role of untrained uncertified women and keep them out of new institutions such as hospitals and medical schools.\n\nIn 1849 Elizabeth Blackwell (1821–1910), an immigrant from England, graduated from Geneva Medical College in New York at the head of her class and thus became the first female doctor in America. In 1857, she and her sister Emily, and their colleague Marie Zakrzewska, founded the New York Infirmary for Women and Children, the first American hospital run by women and the first dedicated to serving women and children. Blackwell viewed medicine as a means for social and moral reform, while a younger pioneer Mary Putnam Jacobi (1842-1906) focused on curing disease. At a deeper level of disagreement, Blackwell felt that women would succeed in medicine because of their humane female values, but Jacobi believed that women should participate as the equals of men in all medical specialties.\n\nNursing became professionalized in the late 19th century, opening a new middle-class career for talented young women of all social backgrounds. The School of Nursing at Detroit's Harper Hospital, begun in 1884, was a national leader. Its graduates worked at the hospital and also in institutions, public health services, as private duty nurses, and volunteered for duty at military hospitals during the Spanish–American War and the two world wars.\n\nThe major religious denominations were active in establishing hospitals in many cities. Several Catholic orders of nuns specialized in nursing roles. While most lay women got married and stopped, or became private duty nurses in the homes and private hospital rooms of the wealthy, the Catholic sisters had lifetime careers in the hospitals. This enabled hospitals like St. Vincent's Hospital in New York, where nurses from the Sisters of Charity began their work in 1849; patients of all backgrounds were welcome, but most came from the low-income Catholic population.\n\nInfant mortality was the major component of life expectancy. Infant mortality was lower in America compared to other parts of the world because of better nutrition. The rates were higher in urban areas, and in Massachusetts statewide the rates increased as the state urbanized. Public health provisions involving sanitation, water supplies, and control of tuberculosis started showing effects by 1900. Public health conditions were worse in the South until the 1950s.\n\nsource: U.S. Bureau of the Census, \"Historical Statistics of the United States\" (1976) Series B148\n\nIn the American Civil War (1861–65), as was typical of the 19th century, more soldiers died of disease than in battle, and even larger numbers were temporarily incapacitated by wounds, disease and accidents. Conditions were poor in the Confederacy, where doctors and medical supplies were in short supply. The war had a dramatic long-term impact on American medicine, from surgical technique to hospitals to nursing and to research facilities.\n\nThe hygiene of the training and field camps was poor, especially at the beginning of the war when men who had seldom been far from home were brought together for training with thousands of strangers. First came epidemics of the childhood diseases of chicken pox, mumps, whooping cough, and, especially, measles. Operations in the South meant a dangerous and new disease environment, bringing diarrhea, dysentery, typhoid fever, and malaria. Disease vectors were often unknown. The surgeons prescribed coffee, whiskey, and quinine. Harsh weather, bad water, inadequate shelter in winter quarters, poor sanitation within the camps, and dirty camp hospitals took their toll.\n\nThis was a common scenario in wars from time immemorial, and conditions faced by the Confederate army were even worse. The Union responded by building army hospitals in every state. What was different in the Union was the emergence of skilled, well-funded medical organizers who took proactive action, especially in the much enlarged United States Army Medical Department, and the United States Sanitary Commission, a new private agency. Numerous other new agencies also targeted the medical and morale needs of soldiers, including the United States Christian Commission as well as smaller private agencies such as the Women's Central Association of Relief for Sick and Wounded in the Army (WCAR) founded in 1861 by Henry Whitney Bellows, and Dorothea Dix. Systematic funding appeals raised public consciousness, as well as millions of dollars. Many thousands of volunteers worked in the hospitals and rest homes, most famously poet Walt Whitman. Frederick Law Olmsted, a famous landscape architect, was the highly efficient executive director of the Sanitary Commission.\n\nStates could use their own tax money to support their troops as Ohio did. Following the unexpected carnage at the battle of Shiloh in April 1862, the Ohio state government sent 3 steamboats to the scene as floating hospitals with doctors, nurses and medical supplies. The state fleet expanded to eleven hospital ships. The state also set up 12 local offices in main transportation nodes to help Ohio soldiers moving back and forth. The U.S. Army learned many lessons and in 1886, it established the Hospital Corps. The Sanitary Commission collected enormous amounts of statistical data, and opened up the problems of storing information for fast access and mechanically searching for data patterns. The pioneer was John Shaw Billings (1838-1913). A senior surgeon in the war, Billings built the Library of the Surgeon General's Office (now the National Library of Medicine, the centerpiece of modern medical information systems. Billings figured out how to mechanically analyze medical and demographic data by turning it into numbers and punching onto cardboard cards as developed by his assistant Herman Hollerith, the origin of the computer punch card system that dominated statistical data manipulation until the 1970s.\n\nAfter 1870 the Nightingale model of professional training of nurses was widely copied. Linda Richards (1841 – 1930) studied in London and became the first professionally trained American nurse. She established nursing training programs in the United States and Japan, and created the first system for keeping individual medical records for hospitalized patients.\n\nAfter the American Revolution, the United States was slow to adopt advances in European medicine, but adopted germ theory and science-based practices in the late 1800s as the medical education system changed. Historian Elaine G. Breslaw describes earlier post-colonial American medical schools as \"diploma mills\", and credits the large 1889 endowment of Johns Hopkins Hospital for giving it the ability to lead the transition to science-based medicine. Johns Hopkins originated several modern organizational practices, including \"residency\" and \"rounds\".\n\nAs Campbell (1984) shows, the nursing profession was transformed by World War Two. Army and Navy nursing was highly attractive and a larger proportion of nurses volunteered for service higher than any other occupation in American society.\n\nThe public image of the nurses was highly favorable during the war, as exemplified by such Hollywood films as \"Cry \"Havoc\"\", which made the selfless nurses heroes under enemy fire. Some nurses were captured by the Japanese, but in practice they were kept out of harm's way, with the great majority stationed on the home front. The medical services were large operations, with over 600,000 soldiers, and ten enlisted men for every nurse. Nearly all the doctors were men, with women doctors allowed only to examine patients from the Women's Army Corps.\n\n\n\n\n\n\n\n\n\n"}
{"id": "3973644", "url": "https://en.wikipedia.org/wiki?curid=3973644", "title": "Homogeneously staining region", "text": "Homogeneously staining region\n\nHomogeneously staining regions (HSRs) are chromosomal segments with various lengths and uniform staining intensity after G banding. This type of aberration is also known as Copy Number Gains or Amplification.\n\nAn HSR is one type of change in a chromosome's structure which is frequently observed in the nucleus of human cancer cells. In the region of a chromosome where an HSR occurs, a segment of the chromosome, which presumably contains a gene or genes that give selective advantage to the progression of the cancer, is amplified or duplicated many times. As a result of the duplication this chromosomal segment is greatly lengthened and expanded such that when it is stained with a fluorescent probe specific to the region (Fluorescent in situ hybridization), rather than causing a focal fluorescent signal as in a normal chromosome, the probe \"paints\" a broad fluorescent signal over the whole of the amplified region. It is because of the appearance of this broadly staining region that this chromosomal abnormality was named a homogeneously staining region.\n\n"}
{"id": "147918", "url": "https://en.wikipedia.org/wiki?curid=147918", "title": "Industrial robot", "text": "Industrial robot\n\nAn industrial robot is a robot system used for manufacturing. Industrial robots are automated, programmable and capable of movement on three or more axes.\n\nTypical applications of robots include welding, painting, assembly, pick and place for printed circuit boards, packaging and labeling, palletizing, product inspection, and testing; all accomplished with high endurance, speed, and precision. They can assist in material handling.\n\nIn the year 2015, an estimated 1.64 million industrial robots were in operation worldwide according to International Federation of Robotics (IFR).\n\nThe most commonly used robot configurations are articulated robots, SCARA robots, delta robots and cartesian coordinate robots, (gantry robots or x-y-z robots). In the context of general robotics, most types of robots would fall into the category of robotic arms (inherent in the use of the word \"manipulator\" in ISO standard 1738).\nRobots exhibit varying degrees of autonomy: \n\nThe earliest known industrial robot, conforming to the ISO definition was completed by \n\"Bill\" Griffith P. Taylor in 1937 and published in Meccano Magazine, March 1938. The crane-like device was built almost entirely using Meccano parts, and powered by a single electric motor. Five axes of movement were possible, including \"grab\" and \"grab rotation\". Automation was achieved using punched paper tape to energise solenoids, which would facilitate the movement of the crane's control levers. The robot could stack wooden blocks in pre-programmed patterns. The number of motor revolutions required for each desired movement was first plotted on graph paper. This information was then transferred to the paper tape, which was also driven by the robot's single motor. Chris Shute built a complete replica of the robot in 1997.\nGeorge Devol applied for the first robotics patents in 1954 (granted in 1961). The first company to produce a robot was Unimation, founded by Devol and Joseph F. Engelberger in 1956. Unimation robots were also called \"programmable transfer machines\" since their main use at first was to transfer objects from one point to another, less than a dozen feet or so apart. They used hydraulic actuators and were programmed in \"joint coordinates\", i.e. the angles of the various joints were stored during a teaching phase and replayed in operation. They were accurate to within 1/10,000 of an inch (note: although accuracy is not an appropriate measure for robots, usually evaluated in terms of repeatability - see later). Unimation later licensed their technology to Kawasaki Heavy Industries and GKN, manufacturing Unimates in Japan and England respectively. For some time Unimation's only competitor was Cincinnati Milacron Inc. of Ohio. This changed radically in the late 1970s when several big Japanese conglomerates began producing similar industrial robots.\n\nIn 1969 Victor Scheinman at Stanford University invented the Stanford arm, an all-electric, 6-axis articulated robot designed to permit an arm solution. This allowed it accurately to follow arbitrary paths in space and widened the potential use of the robot to more sophisticated applications such as assembly and welding. Scheinman then designed a second arm for the MIT AI Lab, called the \"MIT arm.\" Scheinman, after receiving a fellowship from Unimation to develop his designs, sold those designs to Unimation who further developed them with support from General Motors and later marketed it as the Programmable Universal Machine for Assembly (PUMA).\n\nIndustrial robotics took off quite quickly in Europe, with both ABB Robotics and KUKA Robotics bringing robots to the market in 1973. ABB Robotics (formerly ASEA) introduced IRB 6, among the world's first \"commercially available\" all electric micro-processor controlled robot. The first two IRB 6 robots were sold to Magnusson in Sweden for grinding and polishing pipe bends and were installed in production in January 1974. Also in 1973 KUKA Robotics built its first robot, known as FAMULUS, also one of the first articulated robots to have six electromechanically driven axes.\n\nInterest in robotics increased in the late 1970s and many US companies entered the field, including large firms like General Electric, and General Motors (which formed joint venture FANUC Robotics with FANUC LTD of Japan). U.S. startup companies included Automatix and Adept Technology, Inc. At the height of the robot boom in 1984, Unimation was acquired by Westinghouse Electric Corporation for 107 million U.S. dollars. Westinghouse sold Unimation to Stäubli Faverges SCA of France in 1988, which is still making articulated robots for general industrial and cleanroom applications and even bought the robotic division of Bosch in late 2004.\n\nOnly a few non-Japanese companies ultimately managed to survive in this market, the major ones being: Adept Technology, Stäub, the Swedish-Swiss company ABB Asea Brown Boveri, the German company KUKA Robotics and the Italian company Comau.\n\nAccuracy and repeatability are different measures. Repeatability is usually the most important criterion for a robot and is similar to the concept of 'precision' in measurement—see accuracy and precision. ISO 9283 sets out a method whereby both accuracy and repeatability can be measured. Typically a robot is sent to a taught position a number of times and the error is measured at each return to the position after visiting 4 other positions. Repeatability is then quantified using the standard deviation of those samples in all three dimensions. A typical robot can, of course make a positional error exceeding that and that could be a problem for the process. Moreover, the repeatability is different in different parts of the working envelope and also changes with speed and payload. ISO 9283 specifies that accuracy and repeatability should be measured at maximum speed and at maximum payload. But this results in pessimistic values whereas the robot could be much more accurate and repeatable at light loads and speeds.\nRepeatability in an industrial process is also subject to the accuracy of the end effector, for example a gripper, and even to the design of the 'fingers' that match the gripper to the object being grasped. For example, if a robot picks a screw by its head, the screw could be at a random angle. A subsequent attempt to insert the screw into a hole could easily fail. These and similar scenarios can be improved with 'lead-ins' e.g. by making the entrance to the hole tapered.\n\nThe setup or programming of motions and sequences for an industrial robot is typically taught by linking the robot controller to a laptop, desktop computer or (internal or Internet) network.\n\nA robot and a collection of machines or peripherals is referred to as a workcell, or cell. A typical cell might contain a parts feeder, a molding machine and a robot. The various machines are 'integrated' and controlled by a single computer or PLC. How the robot interacts with other machines in the cell must be programmed, both with regard to their positions in the cell and synchronizing with them.\n\n\"Software:\" The computer is installed with corresponding interface software. The use of a computer greatly simplifies the programming process. Specialized robot software is run either in the robot controller or in the computer or both depending on the system design.\n\nThere are two basic entities that need to be taught (or programmed): positional data and procedure. For example, in a task to move a screw from a feeder to a hole the positions of the feeder and the hole must first be taught or programmed. Secondly the procedure to get the screw from the feeder to the hole must be programmed along with any I/O involved, for example a signal to indicate when the screw is in the feeder ready to be picked up. The purpose of the robot software is to facilitate both these programming tasks.\n\nTeaching the robot positions may be achieved a number of ways:\n\n\"Positional commands\" The robot can be directed to the required position using a GUI or text based commands in which the required X-Y-Z position may be specified and edited.\n\n\"Teach pendant:\" Robot positions can be taught via a teach pendant. This is a handheld control and programming unit. The common features of such units are the ability to manually send the robot to a desired position, or \"inch\" or \"jog\" to adjust a position. They also have a means to change the speed since a low speed is usually required for careful positioning, or while test-running through a new or modified routine. A large emergency stop button is usually included as well. Typically once the robot has been programmed there is no more use for the teach pendant.\n\n\"Lead-by-the-nose:\" this is a technique offered by many robot manufacturers. In this method, one user holds the robot's manipulator, while another person enters a command which de-energizes the robot causing it to go into limp. The user then moves the robot by hand to the required positions and/or along a required path while the software logs these positions into memory. The program can later run the robot to these positions or along the taught path. This technique is popular for tasks such as paint spraying.\n\n\"Offline programming\" is where the entire cell, the robot and all the machines or instruments in the workspace are mapped graphically. The robot can then be moved on screen and the process simulated. A robotics simulator is used to create embedded applications for a robot, without depending on the physical operation of the robot arm and end effector. The advantages of robotics simulation is that it saves time in the design of robotics applications. It can also increase the level of safety associated with robotic equipment since various \"what if\" scenarios can be tried and tested before the system is activated.[8] Robot simulation software provides a platform to teach, test, run, and debug programs that have been written in a variety of programming languages. \n\n\"Robot simulation\" tools allow for robotics programs to be conveniently written and debugged off-line with the final version of the program tested on an actual robot. The ability to preview the behavior of a robotic system in a virtual world allows for a variety of mechanisms, devices, configurations and controllers to be tried and tested before being applied to a \"real world\" system. Robotics simulators have the ability to provide real-time computing of the simulated motion of an industrial robot using both geometric modeling and kinematics modeling.\n\n\"Others\" In addition, machine operators often use user interface devices, typically touchscreen units, which serve as the operator control panel. The operator can switch from program to program, make adjustments within a program and also operate a host of peripheral devices that may be integrated within the same robotic system. These include end effectors, feeders that supply components to the robot, conveyor belts, emergency stop controls, machine vision systems, safety interlock systems, bar code printers and an almost infinite array of other industrial devices which are accessed and controlled via the operator control panel.\n\nThe teach pendant or PC is usually disconnected after programming and the robot then runs on the program that has been installed in its controller. However a computer is often used to 'supervise' the robot and any peripherals, or to provide additional storage for access to numerous complex paths and routines.\n\nThe most essential robot peripheral is the end effector, or end-of-arm-tooling (EOT). Common examples of end effectors include welding devices (such as MIG-welding guns, spot-welders, etc.), spray guns and also grinding and deburring devices (such as pneumatic disk or belt grinders, burrs, etc.), and grippers (devices that can grasp an object, usually electromechanical or pneumatic). Other common means of picking up objects is by vacuum or magnets. End effectors are frequently highly complex, made to match the handled product and often capable of picking up an array of products at one time. They may utilize various sensors to aid the robot system in locating, handling, and positioning products.\n\nFor a given robot the only parameters necessary to completely locate the end effector (gripper, welding torch, etc.) of the robot are the angles of each of the joints or displacements of the linear axes (or combinations of the two for robot formats such as SCARA). However, there are many different ways to define the points. The most common and most convenient way of defining a point is to specify a Cartesian coordinate for it, i.e. the position of the 'end effector' in mm in the X, Y and Z directions relative to the robot's origin. In addition, depending on the types of joints a particular robot may have, the orientation of the end effector in yaw, pitch, and roll and the location of the tool point relative to the robot's faceplate must also be specified. For a jointed arm these coordinates must be converted to joint angles by the robot controller and such conversions are known as Cartesian Transformations which may need to be performed iteratively or recursively for a multiple axis robot. The mathematics of the relationship between joint angles and actual spatial coordinates is called kinematics. See robot control\n\nPositioning by Cartesian coordinates may be done by entering the coordinates into the system or by using a teach pendant which moves the robot in X-Y-Z directions. It is much easier for a human operator to visualize motions up/down, left/right, etc. than to move each joint one at a time. When the desired position is reached it is then defined in some way particular to the robot software in use, e.g. P1 - P5 below.\n\nMost articulated robots perform by storing a series of positions in memory, and moving to them at various times in their programming sequence. For example, a robot which is moving items from one place to another might have a simple 'pick and place' program similar to the following:\n\n\"Define points P1–P5:\"\n\n\n\"Define program:\"\n\n\nFor examples of how this would look in popular robot languages see industrial robot programming.\n\nThe American National Standard for Industrial Robots and Robot Systems — Safety Requirements (ANSI/RIA R15.06-1999) defines a singularity as “a condition caused by the collinear alignment of two or more robot axes resulting in unpredictable robot motion and velocities.” It is most common in robot arms that utilize a “triple-roll wrist”. This is a wrist about which the three axes of the wrist, controlling yaw, pitch, and roll, all pass through a common point. An example of a wrist singularity is when the path through which the robot is traveling causes the first and third axes of the robot’s wrist (i.e. robot's axes 4 and 6) to line up. The second wrist axis then attempts to spin 180° in zero time to maintain the orientation of the end effector. Another common term for this singularity is a “wrist flip”. The result of a singularity can be quite dramatic and can have adverse effects on the robot arm, the end effector, and the process. Some industrial robot manufacturers have attempted to side-step the situation by slightly altering the robot’s path to prevent this condition. Another method is to slow the robot’s travel speed, thus reducing the speed required for the wrist to make the transition. The ANSI/RIA has mandated that robot manufacturers shall make the user aware of singularities if they occur while the system is being manually manipulated.\n\nA second type of singularity in wrist-partitioned vertically articulated six-axis robots occurs when the wrist center lies on a cylinder that is centered about axis 1 and with radius equal to the distance between axes 1 and 4. This is called a shoulder singularity. Some robot manufacturers also mention alignment singularities, where axes 1 and 6 become coincident. This is simply a sub-case of shoulder singularities. When the robot passes close to a shoulder singularity, joint 1 spins very fast.\n\nThe third and last type of singularity in wrist-partitioned vertically articulated six-axis robots occurs when the wrist's center lies in the same plane as axes 2 and 3.\n\nSingularities are closely related to the phenomena of gimbal lock, which has a similar root cause of axes becoming lined up.\n\nA video illustrating these three types of singular configurations is available here.\n\nAccording to the International Federation of Robotics (IFR) study \"World Robotics 2018\", there were about 2,097,500 operational industrial robots by the end of 2017. This number is estimated to reach 3,788,000 by the end of 2021. For the year 2017 the IFR estimates the worldwide sales of industrial robots with US$16.2 billion. Including the cost of software, peripherals and systems engineering, the annual turnover for robot systems is estimated to be US$48.0 billion in 2017.\n\nChina is the largest industrial robot market, with 137,900 units sold in 2017. Japan had the largest operational stock of industrial robots, with 286,554 at the end of 2015. The biggest customer of industrial robots is automotive industry with 33% market share, then electrical/electronics industry with 32%, metal and machinery industry with 12%, rubber and plastics industry with 5%, food industry with 3%. In textiles, apparel and leather industry, 1,580 units are operational.\n\nEstimated worldwide annual supply of industrial robots (in units):\nThe International Federation of Robotics has predicted a worldwide increase in adoption of industrial robots and they estimated 1.7 million new robot installations in factories worldwide by 2020 [IFR 2017]. Rapid advances in automation technologies (e.g. fixed robots, collaborative and mobile robots, and exoskeletons) have the potential to improve work conditions but also to introduce workplace hazards in manufacturing workplaces. Despite the lack of occupational surveillance data on injuries associated specifically with robots, researchers from the US National Institute for Occupational Safety and Health (NIOSH) identified 61 robot-related deaths between 1992 and 2015 using keyword searches of the Bureau of Labor Statistics (BLS) Census of Fatal Occupational Injuries research database (see info from Center for Occupational Robotics Research). Using data from the Bureau of Labor Statistics, NIOSH and its state partners have investigated 4 robot-related fatalities under the Fatality Assessment and Control Evaluation Program. In addition the Occupational Safety and Health Administration (OSHA) has investigated dozens of robot-related deaths and injuries, which can be reviewed at OSHA Accident Search page. Injuries and fatalities could increase over time because of the increasing number of collaborative and co-existing robots, powered exoskeletons, and autonomous vehicles into the work environment.\n\nSafety standards are being developed by the Robotic Industries Association (RIA) in conjunction with the American National Standards Institute (ANSI). On October 5, 2017, OSHA, NIOSH and RIA signed an alliance to work together to enhance technical expertise, identify and help address potential workplace hazards associated with traditional industrial robots and the emerging technology of human-robot collaboration installations and systems, and help identify needed research to reduce workplace hazards. On October 16 NIOSH launched the Center for Occupational Robotics Research to \"provide scientific leadership to guide the development and use of occupational robots that enhance worker safety, health, and wellbeing.\" So far, the research needs identified by NIOSH and its partners include: tracking and preventing injuries and fatalities, intervention and dissemination strategies to promote safe machine control and maintenance procedures, and on translating effective evidence-based interventions into workplace practice.\n\n\n\n"}
{"id": "19164498", "url": "https://en.wikipedia.org/wiki?curid=19164498", "title": "Karl Z. Morgan", "text": "Karl Z. Morgan\n\nKarl Ziegler Morgan (September 27, 1907 – June 8, 1999), was an American physicist who was one of the founders of the field of radiation health physics. Late in life, after a long career in the Manhattan Project and at Oak Ridge National Laboratory, he became a critic of nuclear power and nuclear weapons production. \n\nBorn in Enochville, North Carolina, Karl Morgan attended Lenoir-Rhyne College (now University) as a freshman and sophomore and then transferred to the University of North Carolina, where he graduated with bachelor's and master's degrees in physics and mathematics. He continued graduate study in physics at Duke University, where he received a PhD degree in 1934 for research into cosmic radiation. He began an academic career as a faculty member at Lenoir Rhyne College, but in 1943 was recruited to work in the secret project to develop an atomic bomb.\n\nInitially at the University of Chicago Metallurgical Laboratory and later in Oak Ridge, Morgan joined a small group of physicists who were interested in the health effects of radiation. \n\nMorgan became director of health physics at Oak Ridge National Laboratory (ORNL), serving from the late 1940s until his retirement in 1972. In 1955 he became the first president of the Health Physics Society, and was editor of the journal \"Health Physics\" from 1955 to 1977. After his retirement from ORNL, he joined the faculty of Georgia Institute of Technology as professor of nuclear energy in the school of nuclear engineering, retiring from that position in 1982, when he became a consulting professor at Appalachian State University.\n\nAfter decades as a \"pillar of the nuclear establishment\", Morgan had a \"change of heart\" about nuclear weapons production and nuclear power. He began to offer court testimony which was friendly to people who said they had been harmed by nuclear weapons and the nuclear power industry. In October 1982, he testified in a lawsuit brought by nearly 1,200 people who accused the government of negligence in atomic weapons testing at the Nevada Test Site in the 1950s, which they said had caused leukemia and other cancers. Morgan, then 75 years old, testified that radiation protection measures in the tests were substandard. \n\nMorgan also testified on behalf of Navajo uranium miners and their survivors, saying government officials had known about mine radiation dangers but had not protected the miners. He also testified in the case of Karen Silkwood against Kerr-McGee.\n\nMorgan's autobiography, \"The Angry Genie: One Man's Walk Through the Nuclear Age\" was published in 1999 by the University of Oklahoma Press. He died in Oak Ridge, Tennessee, on June 8, 1999, apparently from a ruptured aortic aneurysm.\n\nPhD John Cameron, the developer of a more accurate dosimeter in the 1960s, was however a major critic of Morgan's error prone autobiography that was otherwise interesting for its historical detailing of the Manhattan Project's health physics evolution. Cameron goes chapter by chapter of Morgan's generally \"flawed\" anti-nuclear stance, writing a critique in the year 2000 on Morgan's exaggeration of the small risks from exposure.\n\n\n"}
{"id": "45572487", "url": "https://en.wikipedia.org/wiki?curid=45572487", "title": "Lagos State Ministry of Health", "text": "Lagos State Ministry of Health\n\nThe Lagos State Ministry of Health is the state government ministry, charged with the responsibility to plan, devise and implement the state policies on health.\n\n"}
{"id": "200614", "url": "https://en.wikipedia.org/wiki?curid=200614", "title": "Liebeck v. McDonald's Restaurants", "text": "Liebeck v. McDonald's Restaurants\n\nLiebeck v. McDonald's Restaurants, also known as the McDonald's coffee case and the hot coffee lawsuit, was a 1994 product liability lawsuit that became a flashpoint in the debate in the United States over tort reform. Although a New Mexico civil jury awarded $2.86 million to plaintiff Stella Liebeck, a 79-year-old woman who suffered third-degree burns in her pelvic region when she accidentally spilled hot coffee in her lap after purchasing it from a McDonald's restaurant, ultimately Liebeck was only awarded $640,000. Liebeck was hospitalized for eight days while she underwent skin grafting, followed by two years of medical treatment.\n\nLiebeck's attorneys argued that, at , McDonald's coffee was defective, claiming it was too hot and more likely to cause serious injury than coffee served at any other establishment. McDonald's had refused several prior opportunities to settle for less than what the jury ultimately awarded. The jury damages included $160,000 to cover medical expenses and compensatory damages and $2.7 million in punitive damages. The trial judge reduced the final verdict to $640,000, and the parties settled for a confidential amount before an appeal was decided.\n\nThe case was said by some to be an example of frivolous litigation; ABC News called the case \"the poster child of excessive lawsuits\", while the legal scholar Jonathan Turley argued that the claim was \"a meaningful and worthy lawsuit\". In June 2011, HBO premiered \"Hot Coffee\", a documentary that discussed in depth how the \"Liebeck\" case has centered in debates on tort reform.\n\nOn February 27, 1992, Stella Liebeck, a 79-year-old woman from Albuquerque, New Mexico, ordered a 49-cent cup of coffee from the drive-through window of a local McDonald's restaurant located at 5001 Gibson Boulevard Southeast. Liebeck was in the passenger's seat of a 1989 Ford Probe which did not have cup holders. Her grandson parked the car so that Liebeck could add cream and sugar to her coffee. Liebeck placed the coffee cup between her knees and pulled the far side of the lid toward her to remove it. In the process, she spilled the entire cup of coffee on her lap. Liebeck was wearing cotton sweatpants; they absorbed the coffee and held it against her skin, scalding her thighs, buttocks, and groin.\n\nLiebeck was taken to the hospital, where it was determined that she had suffered third-degree burns on six percent of her skin and lesser burns over sixteen percent. She remained in the hospital for eight days while she underwent skin grafting. During this period, Liebeck lost (nearly 20% of her body weight), reducing her to . After the hospital stay, Liebeck needed care for three weeks, which was provided by her daughter. Liebeck suffered permanent disfigurement after the incident and was partially disabled for two years.\n\nLiebeck sought to settle with McDonald's for $20,000 to cover her actual and anticipated expenses. Her past medical expenses were $10,500; her anticipated future medical expenses were approximately $2,500; and her daughter's loss of income was approximately $5,000 for a total of approximately $18,000. Instead, the company offered only $800. When McDonald's refused to raise its offer, Liebeck retained Texas attorney Reed Morgan. Morgan filed suit in New Mexico District Court accusing McDonald's of \"gross negligence\" for selling coffee that was \"unreasonably dangerous\" and \"defectively manufactured\". McDonald's refused Morgan's offer to settle for $90,000. Morgan offered to settle for $300,000, and a mediator suggested $225,000 just before trial, but McDonald's refused these final pre-trial attempts to settle.\n\nThe trial took place from August 8–17, 1994, before New Mexico District Court Judge Robert H. Scott. During the case, Liebeck's attorneys discovered that McDonald's required franchisees to hold coffee at . Liebeck's attorney argued that coffee should never be served hotter than , and that a number of other establishments served coffee at a substantially lower temperature than McDonald's. They presented evidence that coffee they had tested all over the city was all served at a temperature at least 20°F (11°C) lower than what McDonald's served. Liebeck's lawyers also presented the jury with expert testimony that coffee may produce third-degree burns (where skin grafting is necessary) in about 3 seconds and coffee may produce such burns in about 12 to 15 seconds. Lowering the temperature to would increase the time for the coffee to produce such a burn to 20 seconds. Liebeck's attorneys argued that these extra seconds could provide adequate time to remove the coffee from exposed skin, thereby preventing many burns. McDonald's claimed that the reason for serving such hot coffee in its drive-through windows was that those who purchased the coffee typically were commuters who wanted to drive a distance with the coffee; the high initial temperature would keep the coffee hot during the trip. However, it came to light that McDonald's had done research which indicated that customers intend to consume the coffee immediately while driving.\n\nOther documents obtained from McDonald's showed that from 1982 to 1992 the company had received more than 700 reports of people burned by McDonald's coffee to varying degrees of severity, and had settled claims arising from scalding injuries for more than $500,000. McDonald's quality control manager, Christopher Appleton, testified that this number of injuries was insufficient to cause the company to evaluate its practices. He argued that all foods hotter than constituted a burn hazard, and that restaurants had more pressing dangers to worry about. The plaintiffs argued that Appleton conceded that McDonald's coffee would burn the mouth and throat if consumed when served.\n\nA twelve-person jury reached its verdict on August 18, 1994. Applying the principles of comparative negligence, the jury found that McDonald's was 80% responsible for the incident and Liebeck was 20% at fault. Though there was a warning on the coffee cup, the jury decided that the warning was neither large enough nor sufficient. They awarded Liebeck $200,000 in compensatory damages, which was then reduced by 20% to $160,000. In addition, they awarded her $2.7 million in punitive damages. The jurors apparently arrived at this figure from Morgan's suggestion to penalize McDonald's for one or two days' worth of coffee revenues, which were about $1.35 million per day. The judge reduced punitive damages to $480,000, three times the compensatory amount, for a total of $640,000. The decision was appealed by both McDonald's and Liebeck in December 1994, but the parties settled out of court for an undisclosed amount less than $600,000.\n\nThe case is considered by some to be an example of frivolous litigation. ABC News called the case \"the poster child of excessive lawsuits\". Jonathan Turley called the case \"a meaningful and worthy lawsuit\". McDonald's asserts that the outcome of the case was a fluke, and attributed the loss to poor communications and strategy by an unfamiliar insurer representing a franchise. Liebeck's attorney, Reed Morgan, and the Association of Trial Lawyers of America defended the result in \"Liebeck\" by claiming that McDonald's reduced the temperature of its coffee after the suit, although it is not clear whether McDonald's in fact had done so.\n\nDetractors have argued that McDonald's refusal to offer more than an $800 settlement for the $10,500 in medical bills indicated that the suit was meritless and highlighted the fact that Liebeck spilled the coffee on herself rather than any wrongdoing on the company's part. They also argued that the coffee was not defective because McDonald's coffee conformed to industry standards, and coffee continues to be served as hot or hotter today at McDonald's and chains like Starbucks. They further stated that the vast majority of judges who consider similar cases dismiss them before they get to a jury.\n\nLiebeck died on August 5, 2004, at age 91. According to her daughter, \"the burns and court proceedings (had taken) their toll\" and in the years following the settlement Liebeck had \"no quality of life\", and that the settlement had paid for a live-in nurse.\n\nIn \"McMahon v. Bunn Matic Corporation\" (1998), Seventh Circuit Court of Appeals Judge Frank Easterbrook wrote a unanimous opinion affirming dismissal of a similar lawsuit against coffeemaker manufacturer Bunn-O-Matic, finding that hot coffee was not \"unreasonably dangerous\".\n\nIn \"Bogle v. McDonald's Restaurants Ltd.\" (2002), a similar lawsuit in England failed when the court rejected the claim that McDonald's could have avoided injury by serving coffee at a lower temperature.\n\nSince \"Liebeck\", major vendors of coffee, including Chick-Fil-A, Starbucks, Dunkin' Donuts, Wendy's, Burger King, hospitals, and McDonald's have been defendants in similar lawsuits over coffee-related burns.\n\nIn 1994, a spokesman for the National Coffee Association said that the temperature of McDonald's coffee conformed to industry standards. An \"admittedly unscientific\" survey by the \"LA Times\" that year found that coffee was served between , and that two coffee outlets tested, one Burger King and one Starbucks, served hotter coffee than McDonald's.\n\nSince \"Liebeck\", McDonald's has not reduced the service temperature of its coffee. McDonald's policy today is to serve coffee at , relying on more sternly worded warnings on cups made of rigid foam to avoid future liability, though it continues to face lawsuits over hot coffee. The Specialty Coffee Association of America supports improved packaging methods rather than lowering the temperature at which coffee is served. The association has successfully aided the defense of subsequent coffee burn cases. Similarly, as of 2004, Starbucks sells coffee at , and the executive director of the Specialty Coffee Association of America reported that the standard serving temperature is .\n\nOn June 27, 2011, HBO premiered a documentary about tort reform problems titled \"Hot Coffee\". A large portion of the film covered Liebeck's lawsuit. This included news clips, comments from celebrities and politicians about the case, as well as myths and misconceptions, including how many people thought she was driving when the incident occurred and thought that she suffered only minor superficial burns. The film also discussed in great depth how \"Liebeck v. McDonald's Restaurants\" is often used and misused to describe a frivolous lawsuit and referenced in conjunction with tort reform efforts. It contends that corporations have spent millions promoting misconceptions of tort cases in order to promote tort reform. In reality, the majority of damages in the case were punitive due to McDonald's' reckless disregard for the number of burn victims prior to Liebeck.\n\nOn October 21, 2013, \"The New York Times\" published a Retro Report video about the media reaction and an accompanying article about the changes in coffee drinking over 20 years. \"The New York Times\" noted how the details of Liebeck's story lost length and context as it was reported worldwide. An October 25 follow-up article noted that the video had more than one million views and had sparked vigorous debate in the online comments.\n\n\n\n"}
{"id": "23490378", "url": "https://en.wikipedia.org/wiki?curid=23490378", "title": "Ligovsky Pond", "text": "Ligovsky Pond\n\nLigovsky pond () is artificial lake existing in Ligovo, suburb of Saint Petersburg (Russia) in 1716–1941.\n\nIn 1703 Peter I had been based the city of Saint Petersburg and this area became capital suburb.\nIn 1710s emperor has taken part in destiny of settlement - it has enjoined to block the Dudergofka river in 1715.\nOn a dam the watermill which specialised on flour-grinding and felting works has been constructed.\n\nSimultaneously with barrage of Dudergofka river has been dug the Ligovsky channel.\nIt has taken away an essential part of water from Dudergofka and the artificial lake became a source of water for Ligovo.\nAt that time surrounding district represented imperial grange for maintenance of a palace with the foodstuffs; there was a dairy farm, kitchen gardens.\n\nWhen in 1765 owner of district became Grigory Orlov, for it on the bank of lake buildings have been built.\nOn the western coast of a pond were the house with landing stage and economic constructions.\nOn east coast the manour house where Orlov accepted Russian empress Catherine II has been constructed.\n\nAfter 1783, when Grigory Orlov is dead, earths of Ligovo have been inherited by its pupil Natalia Alekseeva, she was married for aide-de-camp of Orlov Friedrich Wilhelm von Buxhoeveden.\nIn 1840s the manor of Buxhoeveden has passed to count G. G. Kushelev-Bezbordko (younger), it has continued useful agricultural activity of count Orlov, and Ligovo became an exemplary agricultural manor.\nHave spent a network of avenues and paths, have created specific hills, have dug out ponds.\nThe house has a connected covered transition to kitchen, to the north from it have organised separate zones — an orchard with greenhouses, stables, the bird's and cattle courtyard.\nLake coast too have improved - by the architect A. Stackenschneider have been constructed terrace, going down from the house to lake, a grotto on love island.\n\nAfter Kushelev-Bezbordko's death the manor gradually fell into decay.\nIn 1857 in Ligovo has come Baltic railway, by the end 1870s vicinities Ligovsky pond became country area; on lake summer residents went for a drive on boats.\nTo October Revolution in 1917 the lake, a dam and a mill were supported as it should be.\nAfter that the mill has stopped, contemporaries recollected that water falls fell from a dam.\n\nThe pond mirror was supported by a dam of a mill till 1941. On 5 December 1941 German armies destroyed the dam on the approach to Leningrad. After the Second World War hydraulic work was not restored. On a lake place the big ravine which has grown with a bush settles down. The territory of the former lake is not built up - it is a memorial zone: in fights on this place was lost more than 700 persons. Near to the former lake the memorial Orthodox church is under construction.\n"}
{"id": "62424", "url": "https://en.wikipedia.org/wiki?curid=62424", "title": "List of pharmaceutical companies", "text": "List of pharmaceutical companies\n\n It is limited to those companies notable enough to have articles in Wikipedia.\n\n"}
{"id": "399978", "url": "https://en.wikipedia.org/wiki?curid=399978", "title": "List of self-help organizations", "text": "List of self-help organizations\n\nThis is a list of self-help organizations.\n\nRecovery programs using Alcoholics Anonymous' twelve steps and twelve traditions either in their original form or by changing only the alcohol-specific references:\n\n\n\n\n"}
{"id": "3209542", "url": "https://en.wikipedia.org/wiki?curid=3209542", "title": "Maxillary first premolar", "text": "Maxillary first premolar\n\nThe maxillary first premolar is one of two teeth located in the upper jaw, laterally (away from the midline of the face) from both the maxillary canines of the mouth but mesial (toward the midline of the face) from both maxillary second premolars. The function of this premolar is similar to that of canines in regard to tearing being the principal action during mastication, commonly known as chewing. There are two cusps on maxillary first premolars, and the buccal (closest to the cheek) cusp is sharp enough to resemble the prehensile teeth found in carnivorous animals. There are no deciduous maxillary premolars. Around 10-11 years of age, the primary molars are shed and the permanent premolars erupt in their place. It takes about 3 years for the adult premolar and its root to fully calcify.\n\nIn the universal system of notation, the permanent maxillary premolars are designated by a number. The right permanent maxillary first premolar is known as \"5\", and the left one is known as \"12\". In the Palmer notation, a number is used in conjunction with a symbol designating in which quadrant the tooth is found. For this tooth, the left and right first premolars would have the same number, \"4\", but the right one would have the symbol, \"┘\", underneath it, while the left one would have, \"└\". The international notation has a different numbering system than the previous two, and the right permanent maxillary first premolar is known as \"14\", and the left one is known as \"24\".\n\n"}
{"id": "4294204", "url": "https://en.wikipedia.org/wiki?curid=4294204", "title": "MedCalc", "text": "MedCalc\n\nMedCalc is a statistical software package designed for the biomedical sciences. It has an integrated spreadsheet for data input and can import files in several formats (Excel, SPSS, CSV, ...).\n\nMedCalc includes basic parametric and non-parametric statistical procedures and graphs such as descriptive statistics, ANOVA, Mann–Whitney test, Wilcoxon test, χ test, correlation, linear as well as non-linear regression, logistic regression, etc.\nSurvival analysis includes Cox regression (Proportional hazards model) and Kaplan–Meier survival analysis.\n\nProcedures for method evaluation and method comparison include ROC curve analysis, Bland–Altman plot, as well as Deming and Passing–Bablok regression.\n\nThe software also includes meta-analysis and sample size calculations.\n\nThe first DOS version of MedCalc was released in April 1993 and the first version for Windows was available in November 1996. On 7 March 2007, version 9.3 obtained the Certified for Windows Vista logo.\nVersion 15.2 introduced a user-interface in English, Chinese (simplified and traditional), French, German, Italian, Japanese, Korean, Polish, Portuguese (Brazilian), Russian and Spanish.\n\n\n\n"}
{"id": "10453294", "url": "https://en.wikipedia.org/wiki?curid=10453294", "title": "Mendelian randomization", "text": "Mendelian randomization\n\nIn epidemiology, Mendelian randomization is a method of using measured variation in genes of known function to examine the causal effect of a modifiable exposure on disease in observational studies. The design was first proposed in 1986 and subsequently described by Gray and Wheatley as a method for obtaining unbiased estimates of the effects of a putative causal variable without conducting a traditional randomised trial. These authors also coined the term \"Mendelian randomization\". The design has a powerful control for reverse causation and confounding, which often impede or mislead epidemiological studies.\n\nAn important focus of observational epidemiology is the identification of modifiable causes of common diseases that are of public health interest. In order to have firm evidence that a recommended public health intervention will have the desired beneficial effect, the observed association between the particular risk factor and disease must imply that the risk factor actually causes the disease.\n\nWell-known successes include the identified causal links between smoking and lung cancer, and between blood pressure and stroke. However, there have also been notable failures when identified exposures were later shown by randomised controlled trials (RCTs) to be non-causal. For instance, it has now been shown that hormone replacement therapy will not prevent cardiovascular disease, as was previously thought, and may have other adverse health effects. The reason for such spurious findings in observational epidemiology is most likely to be confounding by social, behavioural or physiological factors which are difficult to control for and particularly difficult to measure accurately. Moreover, many findings cannot be replicated by RCTs for ethical reasons.\n\n\"Genetics is indeed in a peculiarly favoured condition in that Providence has shielded the geneticist from many of the difficulties of a reliably controlled comparison. The different genotypes possible from the same mating have been beautifully randomised by the meiotic process. A more perfect control of conditions is scarcely possible, than that of different genotypes appearing in the same litter.\" --R.A. Fisher\n\nMendelian randomization is a method that allows one to test for, or in certain cases to estimate, a causal effect from observational data in the presence of confounding factors. It uses common genetic polymorphisms with well-understood effects on exposure patterns (e.g., propensity to drink alcohol) or effects that mimic those produced by modifiable exposures (e.g., raised blood cholesterol). Importantly, the genotype must only affect the disease status indirectly via its effect on the exposure of interest. Because genotypes are assigned randomly when passed from parents to offspring during meiosis, if we assume that choice of mate is not associated with genotype (panmixia), then the population genotype distribution should be unrelated to the confounders that typically plague observational epidemiology studies. In this regard, Mendelian randomization can be thought of as a “natural” randomized controlled trial. Because the polymorphism is the instrument, Mendelian randomization is dependent on genetic association studies having provided good candidate genes for response to risk exposure.\n\nFrom a statistical perspective, MR is an application of the technique of instrumental variables with genotype acting as an instrument for the exposure of interest.\n\nMR is based on a number of assumptions. These include that there is no direct relationship between the instrument and the dependent variable, and that there are no direct paths between the instrument and any potential confounders. In addition to direct effects of the instrument on the disease misleading the analyst, misleading conclusions may also arise in the presence of linkage disequilibrium with unmeasured directly-causal variants, genetic heterogeneity, pleiotropy (often detected as a genetic correlation), or population stratification.\n\n\n"}
{"id": "24417538", "url": "https://en.wikipedia.org/wiki?curid=24417538", "title": "Multiscale Electrophysiology Format", "text": "Multiscale Electrophysiology Format\n\nMultiscale Electrophysiology Format (MEF) was developed to handle the large amounts of data produced by large-scale electrophysiology in human and animal subjects. MEF can store any time series data up to 24 bits in length, and employs lossless range encoded difference compression. Subject identifying information in the file header can be encrypted using 128-bit AES encryption in order to comply with HIPAA requirements for patient privacy when transmitting data across an open network.\n\nCompressed data is stored in independent blocks to allow direct access to the data, facilitate parallel processing and limit the effects of potential damage to files. Data fidelity is ensured by a 32-bit cyclic redundancy check in each compressed data block using the Koopman polynomial (0xEB31D82E), which has a Hamming distance of from 4 to 114 kbits.\n\nA formal specification and source code are available online.\n\n\n"}
{"id": "4059869", "url": "https://en.wikipedia.org/wiki?curid=4059869", "title": "Naas General Hospital", "text": "Naas General Hospital\n\nNaas General Hospital is located in Craddockstown, Naas in County Kildare, near Naas' town center, in Ireland. This General Hospital provides acute services for the population of around 200,000 people in County Kildare and western parts of County Wicklow. The hospital currently has 243 patient beds which include 18 day service beds. The hospital features a 24-hour Emergency Department.\nThe hospital was originally built to serve as a workhouse in the 1840s, then in 1922 around the time of Ireland's independence from Britain it became Naas County Hospital. Then in 1981 the Department of Health began to develop the hospital as a general hospital.\n\nA multi-million euro development of the hospital produced a new building, with 8 wards and 243 beds. Naas General Hospital provides inpatient and outpatient services, 24-hour accident and emergency services, day procedures, radiology, pathology and physical medicine, along with many other services and facilities.\n\n"}
{"id": "11475327", "url": "https://en.wikipedia.org/wiki?curid=11475327", "title": "National Science Advisory Board for Biosecurity", "text": "National Science Advisory Board for Biosecurity\n\nThe National Science Advisory Board for Biosecurity is a panel of experts that report to the Secretary of the United States Department of Health and Human Services. It is tasked with recommending policies on such questions as how to prevent published research in biotechnology from aiding terrorism, without slowing scientific progress. It is composed of non-voting \"ex officio\" and appointed voting members. The current Chair of the NSABB is Samuel L. Stanley, Jr., M.D..\n\nAs of 2017, the \"ex officio\" members were:\n\n"}
{"id": "39646001", "url": "https://en.wikipedia.org/wiki?curid=39646001", "title": "Northern vigor", "text": "Northern vigor\n\nNorthern vigor is an effect seen in certain varieties of produce where varieties of plants grown in northern climates, then moved to southern climates, are hardier, better-producing, and better tasting. This effect has been primarily observed in potatoes, but is also seen in strawberries and garlic. The Saskatchewan Seed Potato Growers Association has trademarked the term \"Northern Vigour\" for use with their potatoes, but the effect is seen in produce grown throughout Canada and the northern United States.\n\nThe exact cause of northern vigor is not known, but there are many theories. Some believe it has to do with the length of the days in northern latitudes, or that it has to do with the combination of cold nights and hot days. Others believe that the cold may kill off any disease that would otherwise affect plants from the south. Still others think that the switch from a colder climate to a warm, less harsh environment makes it easier for the plants to thrive. Researchers in Saskatchewan discovered that tubers raised in the cold and then moved to a warm environment undergo a series of physiological changes that may trigger more vigorous growth.\n"}
{"id": "26019393", "url": "https://en.wikipedia.org/wiki?curid=26019393", "title": "Out from Under: Disability, History and Things to Remember", "text": "Out from Under: Disability, History and Things to Remember\n\nOut from Under: Disability, History and Things to Remember is a traveling exhibition that explores the history of disability within the lives of Canadians.\n\nThe exhibition was produced in collaboration with students, scholars and alumni from the School of Disability Studies at Ryerson University. The exhibition originated from a special topic seminar designed to uncover the hidden history of disability in Canada. Each student identified an artifact to explore that represented a particular moment in Canadian disability history. Thirteen objects were then selected for the exhibition, revealing narratives that pay homage to the resilience, creativity, and the civic and cultural contributions of Canadians with disabilities. Accompanying interpretive texts help confront stereotypical representations of disability such as pity, inspiration or freak, helping to illustrate the complexity of disability struggles for social and political acknowledgement, identity, survival and remembrance.\n\nOut from Under is curated by three faculty members from Ryerson University’s School of Disability Studies.\n\nEach objects’ history is represented by a single word title: Digging, Labouring, Dressing, Fixing, Measuring, Naming, Breathing, Remembering, Packing, Trailblazing, Struggling, Leading and Aspiring.\n\nAmerican Sign Language\nVideo podcast of all exhibit text in American Sign Language.\n\nAudio description and Touch Stations\nAudio podcasts of all exhibit text with detailed audio descriptions and touch stations profiling selected objects.\n\nBraille Booklets\nBraille copies of exhibit text including participant profiles.\n\nA 60-page color catalogue entitled “Out from Under: Disability, History and Things to Remember” (2008) was produced by the School of Disability Studies.\n\nMarch 9 to March 21, 2010, Cultural Olympiad exhibition (UBC Robson Square), Vancouver, Canada\n\nApril 17 to July 13, 2008, Royal Ontario Museum, Toronto, Canada\n\nOctober 25 to October 31, 2007, Abilities Arts Festival (at the Columbus Centre), Toronto, Canada\n\nAccess Award for Disability Issues, 2008.\n\n\n"}
{"id": "38373964", "url": "https://en.wikipedia.org/wiki?curid=38373964", "title": "Pakistan Medical and Dental Council", "text": "Pakistan Medical and Dental Council\n\nThe Pakistan Medical and Dental Council (; abbreviated as PMDC) is a statutory regulatory authority under the Ministry of National Health Services Regulation and Coordination that maintains the official register of medical practitioners within Pakistan. Its chief function is to \"\"establish uniform minimum standard of basic and higher qualifications in medicine and dentistry throughout Pakistan\" by controlling entry to the PMDC register and suspending or removing members when necessary. It also sets the education standards for medical schools in Pakistan along with the Higher Education Commission (Pakistan).\n\nThe Pakistan Medical and Dental Council was initially established as the \"Pakistan Medical Council\" in 1948, by adopting the \"British Indian Medical Council Act 1933\" on the recommendations of the 1947 Pakistan Health Conference. It was later reorganized under the \"Pakistan Medical Council Act 1951\", whereby each province has its own medical council. In 1957, the West Pakistan Medical Council was formed by merging the Sindh Medical Council and Punjab Medical Council. The \"Pakistan Medical Council Ordinance 1962\" established the present-day Pakistan Medical and Dental Council as a statutory body in 1962 and all provincial councils were dissolved. Three amendments were passed thereafter as the \"Medical and Dental Council (Amendment) Act\" in 1973, 1999 and 2012.\n\nPMDC has been given a mandate to establish uniform medical and dental practice across Pakistan by:\n\nAll medical practitioners and students are required to register with PMDC, to legally practice medicine in Pakistan. The guidelines for registration are outlined under \"Chapter IX, Pakistan Registration of Medical and Dental Practitioners Regulations, 2008\".\n\nSeveral corruption allegations and scandals have been associated with PMDC. In particular, irregularities in the registration of medical colleges and allegations of wrongdoing in the accreditation of doctors have also been leveled in judicial probes of PMDC’s affairs. A judicial commission was set up under a court order after allegations of embezzlement in the registration of private colleges in 2013 — the commission was headed by former Lahore High Court judge Shabbar Raza Rizvi.\n\n\n"}
{"id": "16317992", "url": "https://en.wikipedia.org/wiki?curid=16317992", "title": "Plastic soup", "text": "Plastic soup\n\nPlastic soup is an increasingly widely used term referring to pollution of the environment by plastics in general, ranging from large pieces of fishing gear that can entrap marine animals to the microplastics and nanoplastics that result from the breakdown or photodegradation of plastic waste in surface waters, rivers or oceans. \n\nThe most notorious example of this pollution is the Great Pacific Garbage Patch (lots more on the science of the subject there), an area in which marine debris has accumulated in an ocean gyre. It should be pointed out that the density even in that region does not correspond to the public image of floating islands of rubbish; the concentrations are however high enough to be of considerable environmental concern.\n"}
{"id": "19764862", "url": "https://en.wikipedia.org/wiki?curid=19764862", "title": "Pollution in the United States", "text": "Pollution in the United States\n\nAs with many countries pollution in the United States is a concern for environmental organizations, government agencies and individuals.\n\nExamples of land pollution include:\n\nAir pollution is caused predominantly from, burning fossil fuels, cars and much more. Natural sources of air pollution include forest fires, volcanic eruptions, wind erosion, pollen dispersal, evaporation of organic compounds, and natural radioactivity. These natural sources of pollution often soon disperses and thins settling near its locale. However, major natural events such as volcanic activity can convey throughout the air spreading, thinning and settling over continents. Fossil fuel burning for heating, electrical generation, and in motor vehicles are responsible for about 90% of all air pollution in the United States.\n\nIn a report published in the November 12, 2008 online issue of Environmental Science and Technology, researchers found that freshwater pollution by phosphorus and nitrogen costs U.S. government agencies, drinking water facilities and individual Americans at least $4.3 billion annually. Of that, they calculated that $44 million a year is spent just protecting aquatic species from nutrient pollution.\n\n\nThe use of DDT and its consequences as a pollutant is attributed as sparking the environmental movement in the United States.\n\nWorldwide there are numerous environmental organizations attempting to ban the use of polystyrene. One such organization in the U.S. is Californians Against Waste. The city of Berkeley, California, was one of the first cities in the world to ban polystyrene food packaging (called Styrofoam in the media announcements). It was also banned in Portland, Oregon and Suffolk County, New York in 1990. Now, over 20 US cities have banned polystyrene food packaging, including Oakland, California, on Jan 1, 2007. San Francisco introduced a ban on the packaging on June 1, 2007: Board of Supervisors President Aaron Peskin noted: \"This is a long time coming. Polystyrene foam products rely on nonrenewable sources for production, are nearly indestructible and leave a legacy of pollution on our urban and natural environments. If McDonald's could see the light and phase out polystyrene foam more than a decade ago, it's about time San Francisco got with the program.\"\n\nThe overall benefits of the ban in Portland, Oregon have been questioned, as have the general environmental concepts of the use of paper versus polystyrene. The California and New York state legislatures are currently considering bills which would effectively ban expanded polystyrene in all takeout food packaging statewide.\n\nThe United States Environmental Protection Agency (EPA) is an agency of the federal government of the United States charged with protecting human health and with safeguarding the natural environment: air, water, and land. The EPA was proposed by President Richard Nixon and began operation on 2 December 1970, when it was passed by Congress, and signed into law by President Nixon, and has since been chiefly responsible for the environmental policy of the United States.\n\nEnvironmental Justice is defined as \"the fair treatment and meaningful involvement of all people regardless of race, color, sex, national origin, or income with respect to the development, implementation and enforcement of environmental laws, regulations, and policies\" by the United States Environmental Protection Agency. It is a social movement that aims to ensure all citizens have equal rights and opportunities to reside in a safe environment. The movement began in the 1980s as evidence was mounting that companies were targeting minority and low-income communities. Due to the lack of community action among minorities and low-come, corporations found little resistance when applying to build environmentally polluting factories.\n\nOn February 11, 1994, President William Clinton signed Executive Order 12898 “Federal Actions To Address Environmental Justice in Minority Populations and Low-Income Populations”. Its purpose was to create the “Interagency Working Group on Environmental Justice”. It provided directions to the “Working Group” on how to develop and manage an effective system for preventing environmental injustices. The “Working Group” was made up of various heads of federal agencies and tasked with creating guidelines for reporting, tracking, and developing regulations to curb environmental discrimination.\n\nIn 2014, EPA has a strategy known as Plan EJ 2014. It is not, however, a rule or regulation.\n\nThe goals of the plan are to:\n• Protect health in communities over-burdened by pollution\n• Empower communities to take action to improve their health and environment\n• Establish partnerships with local, state, tribal and federal organizations to achieve healthy and sustainable communities.\n\nCommon offenders of environmental discrimination are corporations that build environmentally hazardous sites. These are typically waste processing facilities, energy companies such as coal plants, chemical plants, and manufacturers who use specific chemicals known to be hazardous to both the environment and/or human health. Other industries known for being responsible for negatively impacting the United States include transportation and energy mining and drilling. A list called The Toxic 100 is maintained by the Political Economy Research Institute (PERI), an institute at the University off Massachusetts Amherst, of the United States’ top polluters. PERI uses a formula: Emissions (millions of pounds) x Toxicity x Population Exposure. Population is measured by its proximity to nearby residents, as well as, prevailing winds and height of smokestacks. The data on chemical releases come from the U.S. Environmental Protection Agency's Toxics Release Inventory (TRI).\n\n\n"}
{"id": "600451", "url": "https://en.wikipedia.org/wiki?curid=600451", "title": "Polydipsia", "text": "Polydipsia\n\nPolydipsia is excessive thirst or excess drinking. The word derives from the Greek () \"very thirsty\", which is derived from (, \"much, many\") + (, \"thirst\"). Polydipsia is a nonspecific symptom in various medical disorders. It also occurs as an abnormal behaviour in some non-human animals, such as in birds.\n\nThis symptom is characteristically found in diabetics, often as one of the initial symptoms, and in those who fail to take their anti-diabetic medications or whose condition is poorly controlled. It can also be caused by a change in the osmolality of the extracellular fluids of the body, hypokalemia, decreased blood volume (as occurs during major hemorrhage), and other conditions that create a water deficit. This is usually a result of osmotic diuresis. Diabetes insipidus (\"tasteless\" diabetes, as opposed to diabetes mellitus) can also cause polydipsia. Polydipsia is also a symptom of anticholinergic poisoning. Zinc is also known to reduce symptoms of polydipsia by causing the body to absorb fluids more efficiently (reduction of diarrhea, induces constipation) and it causes the body to retain more sodium; thus a zinc deficiency can be a possible cause. The combination of polydipsia and (nocturnal) polyuria is also seen in (primary) hyperaldosteronism (which often goes with hypokalemia).\nAntipsychotics can have side effects such as dry mouth that may make the patient feel thirsty.\n\nPrimary polydipsia describes excessive thirst and water intake caused in the absence of physiological stimuli to drink. This includes both psychogenic primary polydipsia and non-psychogenic primary polydipsia, such as in patients with autoimmune chronic hepatitis with severely elevated globulin levels.\n\nPsychogenic polydipsia is an excessive water intake seen in some patients with mental illnesses such as schizophrenia, and/or the developmentally disabled. It should be taken very seriously, as the amount of water ingested exceeds the amount that can be excreted by the kidneys, and can on rare occasions be life-threatening as the body's serum sodium level is diluted to an extent that seizures and cardiac arrest can occur.\n\nWhile psychogenic polydipsia is generally not found outside the population of serious mental disorders, there is some anecdotal evidence of a milder form (typically called 'habit polydipsia' or 'habit drinking') that can be found in the absence of psychosis or other mental conditions. The excessive levels of fluid intake may result in a false diagnosis of diabetes insipidus, since the chronic ingestion of excessive water can produce diagnostic results that closely mimic those of mild diabetes insipidus. As discussed in the entry on diabetes insipidus, \"Habit drinking (in its severest form termed psychogenic polydipsia) is the most common imitator of diabetes insipidus at all ages. While many adult cases in the medical literature are associated with mental disorders, most patients with habit polydipsia have no other detectable disease. The distinction is made during the water deprivation test, as some degree of urinary concentration above isosmolar is usually obtained before the patient becomes dehydrated.\" However, prior to a water deprivation test, consideration should be given to a psychiatric consult to see whether it is possible to rule out psychogenic polydipsia or habit polydipsia.\n\nPolydipsia is a symptom (evidence of a disease state), not a disease in itself. As it is often accompanied by polyuria, investigations directed at diagnosing diabetes insipidus and diabetes mellitus can be useful. Blood serum tests can also provide useful information about the osmolality of the body's extracellular fluids. A decrease in osmolality caused by excess water intake will decrease the serum concentration of red blood cells, blood urea nitrogen (BUN), and sodium.\n"}
{"id": "13093214", "url": "https://en.wikipedia.org/wiki?curid=13093214", "title": "Prince Mahidol Award", "text": "Prince Mahidol Award\n\nThe Prince Mahidol Award () is a Thai Royal Family annual award for outstanding achievements in medicine and public health worldwide.\n\nKing Rama IX Bhumibol Adulyadej founded the Prince Mahidol Award Foundation on 1 January 1992 on the occasion of the 100th Anniversary of the birth of his father, Prince Mahidol, initially under the name \"Mahidol Award Foundation\", but since 28 July 1997 as \"Prince Mahidol Award Foundation.\" In addition to the actual award, the Fund also promotes the memory of Prince Mahidol, who is regarded as the father of modern medicine and public health of Thailand. Princess Maha Chakri Sirindhorn is chairman of the Foundation Committee.\n\nThe award is given annually in two categories to international personalities or organizations:\nand consists of:\n\nBetween 1992 and 2014, 70 prizes were awarded, including 32 in medicine and 38 in public health.\n\n\nSource:Awardees\n\n"}
{"id": "36524658", "url": "https://en.wikipedia.org/wiki?curid=36524658", "title": "Ras Abu Jarjur", "text": "Ras Abu Jarjur\n\nRas Abu Jarjur is a reverse osmosis water desalination plant in Bahrain. The plant produces 16.3 million imperial gallons of desalinated drinking water per day. At the time of its commissioning in the mid-1980s it was the largest reverse osmosis plant in the Middle East with a production capacity of 10 million imperial gallons per day.\n"}
{"id": "1188176", "url": "https://en.wikipedia.org/wiki?curid=1188176", "title": "Rat Park", "text": "Rat Park\n\nRat Park was a series of studies into drug addiction conducted in the late 1970s and published between 1978 and 1981 by Canadian psychologist Bruce K. Alexander and his colleagues at Simon Fraser University in British Columbia, Canada.\n\nAlexander's hypothesis was that drugs do not cause addiction, and that the apparent addiction to opiate drugs commonly observed in laboratory rats exposed to them is attributable to their living conditions, and not to any addictive property of the drug itself.\n\nTo test his hypothesis, Alexander built Rat Park, a large housing colony, 200 times the floor area of a standard laboratory cage. There were 16–20 rats of both sexes in residence, food, balls and wheels for play, and enough space for mating. The results of the experiment appeared to support his hypothesis. \n\nThe two major science journals, \"Science\" and \"Nature\", rejected Alexander, Coambs, and Hadaway's first paper, which appeared instead in \"Psychopharmacology\" in 1978. The paper's publication initially attracted no response. Within a few years, Simon Fraser University withdrew Rat Park's funding.\n\nIn Rat Park, the rats could drink a fluid from one of two drop dispensers, which automatically recorded how much each rat drank. One dispenser contained a sweetened morphine solution and the other plain tap water. Morphine solution was sweetened to reduce averse reaction to the taste of morphine; as a control, prior to morphine introduction rats were offered a sweetened quinine solution instead.\n\nAlexander designed a number of experiments to test the rats' willingness to consume the morphine. The Seduction Experiment involved four groups of rats. Group CC was isolated in laboratory cages when they were weaned at 22 days of age, and lived there until the experiment ended at 80 days of age; Group PP was housed in Rat Park for the same period; Group CP was moved from laboratory cages to Rat Park at 65 days of age; and Group PC was moved out of Rat Park and into cages at 65 days of age.\n\nThe caged rats (Groups CC and PC) took to the morphine instantly, even with relatively little sweetener, with the caged males drinking 19 times more morphine than the Rat Park males in one of the experimental conditions. The rats in Rat Park resisted the morphine water. They would try it occasionally—with the females trying it more often than the males—but they showed a statistically significant preference for the plain water. He writes that the most interesting group was Group CP, the rats who were brought up in cages but moved to Rat Park before the experiment began. These animals rejected the morphine solution when it was stronger, but as it became sweeter and more dilute, they began to drink almost as much as the rats that had lived in cages throughout the experiment. They wanted the sweet water, he concluded, so long as it did not disrupt their normal social behavior. Even more significant, he writes, was that when he added a drug called Naloxone, which negates the effects of opioids, to the morphine-laced water, the Rat Park rats began to drink it.\n\nIn another experiment, he forced rats in ordinary lab cages to consume the morphine-laced solution for 57 days without other liquid available to drink. When they moved into Rat Park, they were allowed to choose between the morphine solution and plain water. They drank the plain water. He writes that they did show some signs of dependence. There were \"some minor withdrawal signs, twitching, what have you, but there were none of the mythic seizures and sweats you so often hear about ...\"\n\nAlexander believes his experiments show that animal self-administration studies provide no empirical support for the theory of drug-induced addiction. \"The intense appetite of isolated experimental animals for heroin in self-injection experiments tells us nothing about the responsiveness of normal animals and people to these drugs. Normal people can ignore heroin ... even when it is plentiful in their environment, and they can use these drugs with little likelihood of addiction ... Rats from Rat Park seem to be no less discriminating.\"\n\nTwo major science journals, \"Science\" and \"Nature\", rejected Alexander's results, but they were later accepted and published in \"Pharmacology Biochemistry and Behavior\".\n\nFurther studies showed mixed results. One of those studies found that both caged and \"park\" rats showed a decreased preference for morphine, suggesting a genetic difference. Another found that while social isolation can influence levels of heroin self-administration, isolation is not a necessary condition for heroin or cocaine injections to be reinforcing. \n\nOther studies reinforced the \"Rat Park\" findings showing that environmental enrichment reduced cocaine seeking behavior in mice and that environmental enrichment can eliminate already established addiction-related behaviors. Furthermore, removing mice from enriched environments has been shown to increase vulnerability to cocaine addiction and exposure to complex environments during early stages of life produce dramatic changes in the reward systems of the brain that result in reduced reactivity to drugs of abuse.\n\n\n\n"}
{"id": "11834714", "url": "https://en.wikipedia.org/wiki?curid=11834714", "title": "Saint Peter's University Hospital", "text": "Saint Peter's University Hospital\n\nSaint Peter's University Hospital (SPUH) is a Roman Catholic hospital on Easton Avenue in New Brunswick, New Jersey. The hospital is a member of the Saint Peter's Healthcare System, Inc., a New Jersey nonprofit corporation sponsored by the Roman Catholic Diocese of Metuchen.\n\nSaint Peter's University Hospital is a non-profit, 478-licensed-bed acute care teaching hospital. Saint Peter's has been designated by the state of New Jersey as a Specialty Acute Care Children's Hospital, Regional Perinatal Center, and Stroke Center that operates one of the largest maternity services in New Jersey and in the country.\n\nThe hospital is a major clinical affiliate of Rutgers Biomedical and Health Sciences, providing full-time training to as many as 60 students in their third or fourth years of medical school, and has a clinical affiliation with The Children's Hospital of Philadelphia.\n\nSaint Peter's University Hospital opened in 1907 as a 25-bed hospital on Somerset and Hardenburgh streets in New Brunswick. Saint Peter's moved to its current location at 254 Easton Avenue in New Brunswick in 1929 as a 125-bed facility. In 1959, a three-wing 349-bed addition was constructed. In 1976, a five-story tower containing the emergency department, radiology department, operating and recovery suite, and a 40-bed nursing unit was completed. In 1991 The Women and Children's Pavilion was added. In 1999 the Center for Ambulatory Resources(CARES) building was constructed. New Telemetry, Maternity and Oncology units were completed in 2008.\n\n\n\n"}
{"id": "27681348", "url": "https://en.wikipedia.org/wiki?curid=27681348", "title": "Subjective report", "text": "Subjective report\n\nIn experimental psychology and medical science, a subjective report is information collected from an experimental subject's description of their own experiences, symptoms or histories. Subjective reporting is the act of an individual describing their own subjective experience, following their introspection on physical or psychological effects under consideration. The method of subjective report analysis also encompasses obtaining information from a subject's own recollection, such as verbal case histories, or experiences in the individual's wider daily life.\n\nCollection of subjective reports consists simply of asking the subject to reflect on their own individual experience; subjective report techniques may vary from open-ended interviews to formal questionnaires consisting of specific, response-constrained questions or Likert items, the latter being used in quantitative and qualitative analyses. Whereas evidential, controlled methods of experimentation yield \"objective\" information on processes observable objectively by the experimenter — that is, the experimenter can observe the given effect externally through some experimental means, such as in problem-solving tasks or laboratory tests — analysis via \"subjective\" report obtains the subject's own opinion on a particular task or subject under investigation and allows study of effects outside of the scope of controlled clinical analysis.\n\nFields that rely heavily on subjective report include social psychology; studies of sexuality (the best known of subjective report studies in this field being the \"Kinsey Reports\"); pharmacological trials of psychiatric and analgesic (pain relief) medication; and ethnography, the study of cultures and cultural processes as part of social science. A 2003 experimental study by Coghill \"et al\" determined that subjective reports of pain were capable of being correlated statistically to neuroimaging data, verifying the reports as being, for the most part, sufficiently accurate for clinical purposes. Issues of veracity of subjective reports can, however, be a limiting factor in using them to draw experimental conclusions.\n"}
{"id": "46293766", "url": "https://en.wikipedia.org/wiki?curid=46293766", "title": "ToothPick (company)", "text": "ToothPick (company)\n\nToothPick.com is an online dental appointment booking service for UK users to find, compare and book local dentists. The site lets users search for their nearest dental practices, compare prices and services and book appointments online. The service is free to use for consumers. Both NHS and private practice dentists are listed on the site.\n\nToothpick launched a limited, London-based Beta version in November 2012 and will then be rolled out nationwide in early 2013.\n\nThe company was founded in late 2011 by a practicing dentist, Sandeep Senghera, and Jozef Wallis, a technology entrepreneur.\n\nToothPick has been developed in partnership with Henry Schein. The product is integrated with the company’s dental appointment scheduling software, which is in use by thousands of dentists across the UK.\n\nThe company is backed by UK angel investors including Passion Capital, Pradeep Menon, Paul Edwards, Satish Jayakumar and Michael Stephanblome.\n\nThe company was acquired by WhatClinic, a healthcare comparison site, on November 9, 2015.\n\n"}
{"id": "4580454", "url": "https://en.wikipedia.org/wiki?curid=4580454", "title": "Virtual Physiological Human", "text": "Virtual Physiological Human\n\nThe Virtual Physiological Human (VPH) is a European initiative that focuses on a methodological and technological framework that, once established, will enable collaborative investigation of the human body as a single complex system. The collective framework will make it possible to share resources and observations formed by institutions and organizations, creating disparate but integrated computer models of the mechanical, physical and biochemical functions of a living human body.\n\nVPH is a framework which aims to be descriptive, integrative and predictive. Clapworthy \"et al.\" state that the framework should be descriptive by allowing laboratory and healthcare observations around the world \"to be collected, catalogued, organized, shared and combined in any possible way.\" It should be integrative by enabling those observations to be collaboratively analyzed by related professionals in order to create \"systemic hypotheses.\" Finally, it should be predictive by encouraging interconnections between extensible and scalable predictive models and \"systemic networks that solidify those systemic hypotheses\" while allowing observational comparison.\n\nThe framework is formed by large collections of anatomical, physiological, and pathological data stored in digital format, typically by predictive simulations developed from these collections and by services intended to support researchers in the creation and maintenance of these models, as well as in the creation of end-user technologies to be used in the clinical practice. VPH models aim to integrate physiological processes across different length and time scales (multi-scale modelling). These models make possible the combination of patient-specific data with population-based representations. The objective is to develop a systemic approach which avoids a reductionist approach and seeks not to subdivide biological systems in any particular way by dimensional scale (body, organ, tissue, cells, molecules), by scientific discipline (biology, physiology, biophysics, biochemistry, molecular biology, bioengineering) or anatomical sub-system (cardiovascular, musculoskeletal, gastrointestinal, etc.).\n\nThe initial concepts that led to the Virtual Physiological Human initiative came from the IUPS Physiome Project. The project was started in 1997 and represented the first worldwide effort to define the physiome through the development of databases and models which facilitated the understanding of the integrative function of cells, organs, and organisms. The project focused on compiling and providing a central repository of databases that would link experimental information and computational models from many laboratories into a single, self-consistent framework.\n\nFollowing the launch of the Physiome Project, there were many other worldwide initiatives of loosely coupled actions all focusing on the development of methods for modelling and simulation of human pathophysiology. In 2005, an expert workshop of the Physiome was held as part of the Functional Imaging and Modelling of the Heart Conference in Barcelona where a white paper entitled \"Towards Virtual Physiological Human: Multilevel modelling and simulation of the human anatomy and physiology\" was presented. The goal of this paper was to shape a clear overview of on-going relevant VPH activities, to build a consensus on how they can be complemented by new initiatives for researchers in the EU and to identify possible mid-term and long term research challenges.\n\nIn 2006, the European Commission funded a coordination and support action entitled \"STEP: Structuring The EuroPhysiome\". The STEP consortium promoted a significant consensus process that involved more than 300 stakeholders including researchers, industry experts, policy makers, clinicians, etc. The prime result of this process was a booklet entitled \"Seeding the EuroPhysiome: A Roadmap to the Virtual Physiological Human\". The STEP action and the resulting research roadmap were instrumental in the development of the VHP concept and in the initiation of much larger process that involves significant research funding, large collaborative projects, and a number of connected initiatives, not only in Europe but also in the United States, Japan, and China.\n\nVPH now forms a core target of the 7th Framework Programme of the European Commission, and aims to support the development of patient-specific computer models and their application in personalised and predictive healthcare. The Virtual Physiological Human Network of Excellence (VPH NoE) aims to connect the various VPH projects within the 7th Framework Programme.\n\nVPH-related projects have received substantial funding from the European Commission in order to further scientific progress in this area. The European Commission is insistent that VPH-related projects demonstrate strong industrial participation and clearly indicate a route from basic science into clinical practice. In the future, it is hoped that the VPH will eventually lead to a better healthcare system which aims to produce the following benefits:\n\n\nPersonalized care solutions are a key aim of the VPH, with new modelling environments for predictive, individualized healthcare to result in better patient safety and drug efficacy. It is anticipated that the VPH could also result in healthcare improvement through greater understanding of pathophysiological processes. The use of biomedical data from a patient to simulate potential treatments and outcomes could prevent the patient from experiencing unnecessary or ineffective treatments. The use of in silico (by computer simulation) modelling and testing of drugs could also reduce the need for experiments on animals.\n\nA future goal is that there will be also be a more holistic approach to medicine with the body treated as a single multi-organ system rather than as a collection of individual organs. Advanced integrative tools should further help to improve the European healthcare system on a number of different levels that include diagnosis, treatment and care of patients and in particular quality of life.\n\n\n\n\n"}
{"id": "1475064", "url": "https://en.wikipedia.org/wiki?curid=1475064", "title": "Zygote intrafallopian transfer", "text": "Zygote intrafallopian transfer\n\nZygote intrafallopian transfer (ZIFT) is an infertility treatment used when a blockage in the fallopian tubes prevents the normal binding of sperm to the egg. Egg cells are removed from a woman's ovaries, and in vitro fertilised. The resulting zygote is placed into the fallopian tube by the use of laparoscopy. The procedure is a spin-off of the gamete intrafallopian transfer (GIFT) procedure. \n\nThe average ZIFT cycle takes five weeks-six weeks to complete. First, the female must take a fertility medication clomiphene to stimulate egg production in the ovaries. The doctor will monitor the growth of the ovarian follicles, and once they are mature, the woman will receive an injection containing human chorionic gonadotropins (HCG or hCG). The eggs will be harvested approximately 36 hours later, usually by transvaginal ovum retrieval. After fertilization in the laboratory, the resulting early embryos or zygotes are placed into the woman's fallopian tubes using a laparoscope.\n\nZIFT has been used in infertility situations where at least one of the fallopian tubes is normal and other treatments have failed; however, the need for two interventions and the fact that IVF results are equal or better (as of 2004), leaves few indications for this procedure. Accordingly, the number of ZIFTs performed has been declining.\n"}
