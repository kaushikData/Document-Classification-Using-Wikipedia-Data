{"id": "54876666", "url": "https://en.wikipedia.org/wiki?curid=54876666", "title": "2017 Gorakhpur hospital deaths", "text": "2017 Gorakhpur hospital deaths\n\nA large number of child deaths occurred at the state-run BRD Medical College hospital in Gorakhpur city of Uttar Pradesh, India in 2017. As of 2 September 2017, 1,317 children had died at the hospital in 2017. The 2017 deaths attracted national attention in August, when 325 children died at the hospital after the hospital's piped oxygen supply ran out. The number of child deaths in previous years were 5,850 in 2014; 6,917 in 2015; and 6,121 in 2016.\n\nAcute encephalitis syndrome (AES) was a major cause of the deaths: Till 29 August 2017, 175 children had died because of encephalitis (including 77 in August alone).\n\nMedical negligence arising from the shortage of oxygen supply was discovered to have been a major cause for avoidable deaths. The oxygen supply was cut by the supplier due to long non-payment of dues. The state government had ignored repeated requests for clearing the dues despite warning about supply being cut. One year after the incident, the families of the victims had not been compensated or visited by state government officials.\n\nThe BRD Medical College hospital is one of the biggest government hospitals in Uttar Pradesh to have specialized facility to treat neo-natal and pediatric encephalitis. It has seen a number of child deaths since 1978, when the first encephalitis outbreak in the Gorakhpur region. During 1978-2017, around 25,000 children have died of encephalitis.\n\nAs of 3 September 2017, 1,317 children had died in the hospital during 2017. The number of deaths has sharply declined compared to the previous years.\n\nAs of 2 September 2017, the month of August saw the highest number of deaths (325):\n\nOn 6 April, the hospital's oxygen supplier, Puspha Sales wrote a letter to the Chief Minister, Adityanath, and Health Minister reminding them that the Hospital's dues were unpaid and that the supply would be discontinued unless they were paid. On 3 June 2017, Pushpa Sales wrote to the Principal of the Medical College, Rajiv Mishra, Principal Secretary in the UP government, Anita Bhatnagar Jain, the Director General of Medical Education (DGME) in the UP government, K. K. Gupta, the superintendent in charge at BRD Medical College, the head of the pediatrics department, Dr Mahima Mittal, and the district magistrate, Rajeev Rautela reminding them of the unpaid dues and the fact that Encephalitis patients, who would increase with the rainy season, require 24 hours supply of oxygen. Pushpa Sales wrote 20 letters to Rajiv Mishra about the unpaid dues. Mishra in turn, wrote ten letters to the UP government about the same, and even raised the issue in a video conference with the DGME, principal secretary and district magistrate. On 30 July, Pushpa Sales sent a legal notice to the Principal, giving them until 14 August to clear dues. Oxygen ran out in the plant in the early hours of 11 August.\n\nStarting on 10 August 2017, 30 children died within 48 hours: 17 children in the neo-natal ward, five in the AES (acute encephalitis syndrome) ward and eight in the general ward.\n\nAccording to a 10-page letter written by Dr. Kafeel Khan in prison he received a Whatsapp message on the night of 10 August informing him of the oxygen supply cut. He said that he called the head of department, the principal and acting principal of B.R.D., the district magistrate of Gorakhpur, the chief medical superintendent of Gorakhpur and B.R.D. Medical College, and his other colleagues to inform them of the situation. He said that he also called local oxygen supplier agencies and begged them to immediately arrange for oxygen cylinders at BRD, and called nearby hospitals. He narrated that he went out to buy oxygen cylinders himself. He was able to scrape 250 cylinders together, paying for them himself and promising the suppliers that he would arrange for the rest of the payment soon. He carried some in his car and arranged with the DIG of Police for a truck and manpower from the Armed Border Force to deliver the others.\n\nOxygen supply was restored by 2:15 AM on 13 August.\n\nThe hospital's principal RK Mishra was suspended on 12 August by the Government of Uttar Pradesh for \"negligent behaviour\" after which he resigned. On 13 August, the head of encephalitis ward at the hospital, Kafeel Khan, was removed as the nodal officer. The state government claimed that no deaths had occurred due to oxygen shortage.\n\nA probe report by the Gorakhpur district administration identified oxygen deprivation as the reason for the death of the children admitted to the encephalitis and neonatal wards between 10 and 11 August, but refrained from blaming the Uttar Pradesh government for the tragedy. It charged the doctors, paramedical staff and clerks of the medical institution with criminal negligence, while maintaining that the state government was unaware of the situation. It accused institute principal Rajiv Mishra of failing to alert the medical education and health departments to the impending oxygen crisis despite being aware of it. It demanded strict action against Pushpa Sales for stopping oxygen supply and accused Dr Satish Kumar, a member of the BRD Medical College Hospital’s purchase committee, of going on leave from 11 August without taking Mishra’s permission. Kumar was additionally charged with failing to get a faulty air-conditioner installed at the encephalitis ward repaired on time. It also found overwriting in the log book related to purchase and re-filling of oxygen cylinders.\n\nA three-member technical team appointed by the central government found that 8 of 12 senior resident posts lie vacant, that only three of 31 nurses are trained to handle newborns, that poor parents are forced to buy disposables and consumables, and that the most basic infection control norms like washing hands and use of disinfectants were not being followed.\n\nNine people were arrested in the case until August, 2018. They were:\n\n\nOf these, Manish Bhandari, Dr Kafeel Khan, Dr Satish, Dr Rajiv Misra and Dr Purnima Shukla had been given bail, while the others remained in jail. None of the top government officials who also had responsibility came under investigation.\n\nOn 12 August, Indian Prime Minister Narendra Modi's office said he is \"constantly monitoring\" the situation with Minister of State Health Anupriya Patel and the Union Health Secretary. In his Independenc Day speech on 15 August, Modi expressed sympathy for the affected families, while calling the episode a 'natural calamity'. Siddharth Nath Singh, the Health Minister of Uttar Pradesh, denied a lack of oxygen was the cause of the deaths He said that such deaths are common in August. Uttar Pradesh Chief Minister Yogi Adityanath ordered a probe into the incident. He visited the hospital on 13 August 2017.\n\nThe Indian National Congress (INC) demanded a separate probe monitored by the Supreme Court of India. It also demanded Chief Minister, Adityanath's resignation. However, BJP President, Amit Shah dismissed this demand, saying, that such deaths were common during Congress rule too, and that action would be taken on the basis of investigation, the results of which would be made public.\n\nOn 14 August, Pushpa Sales, the oxygen supplier to the hospital, released a statement that it never stopped the supply of oxygen cylinders despite the outstanding dues. Manish Bhandari, managing director of the company, stated, \"The government must find why there were just around 50 cylinders instead of 400 on that particular day when deaths took place. I suspect a big oxygen cylinder theft or racket, which they must find it out.\"\n\nThe National Human Rights Council (NHRC) issued a notice to the state government, taking \"suo moto\" cognisance and seeking a report from the state government. It said that the incident indicates \"gross callousness\" on the part of the hospital and the state administration, and asked the government what steps were been taken to compensate the families.\n\nThe Samajwadi Party accused the state government of encouraging corrupt practices at the hospital.\n"}
{"id": "6804899", "url": "https://en.wikipedia.org/wiki?curid=6804899", "title": "AIDES", "text": "AIDES\n\nAIDES is a French community-based non-profit organisation that was founded in 1984 by Daniel Defert, following the death of his partner Michel Foucault. The name is a word play on \"aides\" (the plural for \" help\" in French) and AIDS hence the name.\n\nIts aim is to bring people living with HIV/AIDS together with their loved ones and peers into an organised entity dedicated to fighting HIV/AIDS and to defend the rights of people and communities affected by this disease.\n\nAs of 2007, AIDES is active in France in 100 cities with 400 staff members and more than 1000 registered and trained volunteers. It is the largest non-governmental organisation in France working on HIV issues, by number of activists and budget. It is considered one of the main observers of the epidemic in France.\nInternationally, AIDES has developed strong partnerships with fellow community-based NGOs in Africa, in Europe and in Canada (Quebec) to strengthen the role of civil society by sharing best practices and to jointly advocate for global access to care and prevention. It also developed a partnership with the UN Programme on HIV/AIDS\n\nThe name of the organization is a reference to the French noun \"aide\", from the verb \"aider\", 'to help or assist'. The founder chose to pluralize the word because it could be said that there are multiple types of help that the association can provide.\n\n\nOne of their video projects is an AIDS awareness cartoon. The cartoon, produced by Goodby, Silverstein and Partners and presented in the style of 1920's era animation, features the comically exaggerated sexual adventures of a cat named \"Smutley\" and ends with the message, \"He has nine lives. You only have one. Protect yourself.\" Posted on AIDES' YouTube channel on 16 March 2011, it has logged over 1.3 million views as of 3 June 2012.\n\nAIDES released the album \"Message\" in February 2010 with the participation of 33 artists covering various songs as well as 15 radio personalities. Collectif Artistes performs \"If\" credited to Collectif Artistes, namely Daniel Powter, M. Pokora, Caroline Costa, Natasha St. Pier, Justin Nozuka, Sofia Essaidi, Lara Fabian, Anggun, Tom Frager, Christophe Willem, Jenifer, Bob Sinclar, Joachim Garraud. On the other hand, Animateur FM Matinales contribute their comments under the title \"No Comment\". The album also contains 14 covers of songs by individual artists and one new track \"Peace Song\" contributed by Bob Sinclar.\n\nOne AIDES Public Service Announcement, released in 2011, is entitled A Smutley Cartoon - Gettin' Tail, and is a cartoon presented in black and white (with film scratches) and character designs from the 1920s and 1930s \"silent-age of animation\". Smutley is an extremely promiscuous, bowtie-wearing cat. To the musical accompaniment of Joan Jett's Bad Reputation, Smutley is shown entering a rough-looking bar called \"Furballs\". After flipping a coin into a jukebox and attracting the eye of a female turtle, Smutley is shown having sex with her in a run-down bedroom.\nAs a squirrel turns a crank to advance different scenes from right to left, Smutley is further shown having sex with a rabbit in a sauna, a non-morphic (and possibly non-anthropomorphic) goldfish in an elevator, a dolphin (penetrating its blowhole), three seals in an office typing pool, a pig in a BDSM dungeon, four chickens in a sports locker room (also non-morphic or non-anthro), a vixen in the back of a taxi cab (being driven by a monkey), and an elephant in a public library.\nThe cartoon ends with a series of title cards warning the viewer: \"He's got nine lives\", \"YOU only have one\", \"Protect yourself\".\nThe Smutley campaign was created by Los Angeles-based advertising agency Goodby, Silverstein & Partners.\n\n"}
{"id": "42470013", "url": "https://en.wikipedia.org/wiki?curid=42470013", "title": "Abstinence-only sex education in Uganda", "text": "Abstinence-only sex education in Uganda\n\nUganda is one of the few Sub-Saharan African countries that has adopted abstinence-only sex education as an approach of sexual education that emphasizes abstinence from sexual intercourse until marriage as the only option. Abstinence-only sex education does not include joint curriculum covering other options including safe sex practices, family planning, and is espoused as the only sure way to avoid pregnancy and Sexually transmitted infections. Uganda is commonly recognized as an exemplary case of lowering the rate of HIV prevalence Prevalence figures may have also been distorted by the lack of treatment, meaning that the percentage of infected is decreased by disproportionately early deaths. Abstinence-only sex education has been implemented and supported for this cause to a large degree in Uganda, to some controversy. Critics have questioned its effectiveness in lowering HIV/AIDS transmission. They have also highlighted discrimination, gender inequality and social stigma as the outcomes of the program in Uganda.\n\nIn the wake of an increase of HIV prevalence in Uganda since it first appeared in 1982, many programs were introduced to lower incidence of HIV and educate about the disease. Abstinence-only sex education was implemented as one way to quell the spread of HIV/AIDS, in addition to creating a positive behaviour change to the same end over time. As the epidemic became known on the international scene, money and resources from many nations flooded Africa in general and Uganda in particular. Abstinence-only sex education in Uganda in the late 1990s onward was largely financed by the United States and independent faith-based organizations \n\nThe ABC approach was introduced during the US-Ugandan efforts to propose Abstinence-only as a way to lower HIV transmission. ABC is an acronym, A being Abstinence, B being Be Faithful, and C being Condom use. It would seem that this form of abstinence-only includes some education and encouragement of condoms, but the application of ABC is disaggregated for specific populations. The breakdown is Abstinence for people not yet married, Be Faithful for those in marriages, and Condom Use only for people that are already infected or are in a marriage where one person is infected.\n\nABC has evolved since its inception to a program known as AB, released in 2004 as an official abstinence-only education program for Uganda specifically. AB has lessened the emphasis on condoms due to the confusing nature of teaching the two simultaneously and the fear that condoms undermine the message of abstinence. AB is criticized for its lack of follow-through of collecting data on whether participants are abstaining or not. The main source of data collected for AB is the number of AB rallies, organizations and discussions concerning AB, instead of the behavioural changes proposed by AB. Therefore, AB is difficult to monitor as the true success of the programs aims for abstinence and social modifications, but no data is kept.\n\nCondoms, Needles and Negotiations was born out of the necessity of failing marital monogamy. CNN stands for Condoms, Needles and Negotiation. In sub-Saharan Africa, Uganda included, married women in monogamous relationships are at a disadvantage due to the lack of rights within marriage, bargaining power, and domestic violence. The need was discovered as most new HIV cases come from HIV women contracting the disease in their marriage through their husbands. In this way, AB abstinence-only education falls short for protecting the lives of those practicing it. The Condoms part of CNN is two-fold to reduce condom stigma and promote use within the B (Be Faithful) section of AB. Women are still practicing faithfulness, but in order to protect their health, they still need to be able to understand and use condoms. The next N in CNN covers needle exchange, which doesn't relate to abstinence-only sex education. But the third N stands for Negotiation, which again plays a part for individuals practicing faithfulness who could still be harmed. Negotiation informs partners of the risks they might encounter if their partner is not being monogamous and teaches them skills to bargain in their position in the relationship.\n\nIn 2001, Ugandan President Yoweri Museveni introduced his official program for sex education for the youth of Uganda, Presidential Initiative on AIDS Strategy for Communication to Youth, abbreviated as PIASCY. PIASCY mainly targets primary schools, secondary schools and after school youth rallies with materials and instruction (HRW, 29). The main focus of PIASCY is for young people to empower themselves to delay their sexual relations until marriage through abstinence. PIASCY is primarily funded through the United States Agency for International Development and the U.S. Centers for Disease Control and Prevention, both of which provide many other services outside of finances to support the program. In addition, PIASCY has since become a foundation feature in the United States’ President’s Emergency Plan for AIDS relief (PEPFAR).\n\nThe medium for primary school involvement with PIASCY is a combination of manuals, instruction for teachers to relay to students, as well as materials given directly to the students based upon their age/grade. An assembly is normally held twice a month school wide for teachers to give presentations outlined from the curriculum within the PIASCY manuals. The first message is the choice of abstinence and its benefits, but subsequent lessons can include teachings of self-esteem and how to be above peer-pressure. Although other parts of PIASCY include correct condom application and uses, at the primary level, abstinence-only is the majority of the message.\n\nIn 2004, the Ugandan government incorporated PIASCY in secondary schools, in the form of handouts for students as well as teachers. The curriculum is disseminated in the classroom format on a regular basis. The secondary school level information was drafted to include age sensitive subjects such as masturbation, abortion and homosexuality, but was met with strong opposition from many powerful, yet undisclosed, groups in Uganda and the United States. In addition, draft copies of the secondary school handouts contained misleading and inaccurate information about condoms and HIV prevention. Furthermore, condoms were encouraged after marriage as HIV prevention, but did not explain how this differed with unmarried youth. The final drafts did include rudimentary instruction on condoms without the corresponding photos.\n\nPIASCY also has a significant section of their focus targeting youth that are not in school by holding rallies and events after school or outside of school. In Uganda, there are significant amounts of youth that are not enrolled in school, despite the recent movements to educate everyone. Although these events are generally held on a school campus or facility, anyone aged 15–30, student or not can participate. These events generally have speakers who are as diverse as the President of Uganda to religious leaders. In more rural areas, these serve to disseminate important abstinence messages but have been criticized as supporting an agenda of re-election and political messages and including inaccurate information about condoms and abstinence. After the formal rallies and programs, it is made aware that political leaders are around to take questions that youth may have about the current government. PEPFAR, an organization providing funding to PIASCY, outlines promotion of information, prevention and treatment of HIV, and is in contradiction with the political agenda and misleading information sometimes found in the PIASCY after school programs.\n\nIn Uganda, there are a number of faith-based organizations that provide abstinence-only education to youth. Many of these are financed and supported by not only the Ugandan Government, but also the United States. In general, these are based on Christian beliefs, as 60% of Ugandans are Christian.\n\nThe National Youth Forum was founded by one of the nation's largest activists for abstinence-only sex education, first lady Janet Museveni. The primary objective for the National Youth Forum (NYF) is to coordinate assemblies where young boys and girls pledge to stay sexually pure until marriage. More than 70, 000 Ugandan youths have committed to this through NYF. NYF and Museveni advocate abstinence-only education in its most exclusive form, even pushing the notion that condoms are inadequate in preventing HIV and education involving them promote sexual activity among youth. She has gone so far as to propose a nationwide testing of young boys and girls for virginity, although this has not come to pass. Ms. Museveni's organization NYF also obtains funding from the American government. This relationship has been called under scrutiny on the grounds that UNF Proselytizing and the laws of the United States regarding the secular nature of congressional funding.\n\nThe Makerere Community Church is another national leader in the abstinence-only push. Martin Ssempa, who is one of the writers of the Ugandan AB policy, directs the church and foundation. Ssempa leads the fundamentalist Makerere Christian Church and his policies fall in line. He takes a more extreme version of abstinence only sex education, sometimes speaking out directly against condoms and women's rights. His is a more outlying approach to abstinence-only and the lifestyle that he connects with it.\n\nThe Family Life Network is a non-profit organization that teaches abstinence-only techniques alongside value based sexual education. Since 2002, they have reached at least 130,000 students in over 400 schools. A tactic readily put into place is to create abstinence-only in the public discussion is a signing of \"True Love Waits\" cards to pledge abstinence until marriage. Stephen Langa, the executive director, defines the Family Life Network as having four goals: bring back faith in the marriage institution; show the danger of sexual involvement; warn children on the dangers of globalization, such as pornography; and ask children to make a commitment of abstinence. Although Langa does not actively speak out against condom use, his beliefs when questioned contain false information about the protection provided by condoms against sperm and HIV transference. The Family Life Network receives money from foreign and Ugandan donors, but has not in the past received any contributions from the United States or PEPFAR.\n\nLooking at the effectiveness of abstinence-only sex education, it can’t be measured in the knowledge received from the education, but instead the translation to the decisions that stem from the education. Three of these are STI/HIV prevention, the levels of teenagers participating in sexual activity and unplanned pregnancies.\n\nIn terms of lowering new infection rates of STIs and HIV in particular, there is a mixed opinion on the influence of abstinence only. Edward C. Green, frequently consults for USAID has been published remarking that abstinence-only was the main proponent of the lowering of HIV/AIDS infection in Uganda and that condoms were not effective. Furthermore, AIDS specialist Sophie Wacasa-Monacco explains that abstinence has played a very important role in the HIV decline, but only in conjunction with comprehensive education. It is the knowledge about HIV and STIs in general that allow for people to make educated decisions concerning their body and that of a prospective sexual partner. Abstinence is a good outlet to avoid sexual behaviour and therefore sexually transmitted infections, but without the full scope of the situation regarding sexual decisions, the consequences can be unclear and misleading.\n\nAbstinence-only sex education encourages youth to delay their sexual début until marriage. Between the years of 1989 and 1995, Uganda saw large success with the employment of abstinence and subsequent HIV infection rates comparatively to their regional neighbours. In males aged 15–24 years old, there was a decrease from 60% to 23% participation in premarital sex. Similarly, females aged 15–25 saw a decrease from 53% to 16% in 1989 and 1995 respectively. This decrease coincided with a policy called zero-grazing, which emphasizes monogamous relationships and the faithfulness that is implied. Curiously, there were very few abstinence-only education programs in place during this decrease, despite the choice of abstinence from sexual intercourse having a significant decline.\n\nAbstinence-only sex education leaves women socially vulnerable. Women in Uganda have a diminished ability to hold dominion over their bodies. Young women are often coerced into relationships with older men, teachers, or \"sugar daddies\" that provide them with items that would not be accessible otherwise. Biologically speaking, if women or men break their abstinence vow, a woman is twice as likely to contract HIV due to her anatomy. Furthermore, abstinence only sex education doesn't teach ways to avoid HIV/AIDS infection after marriage.\n\nIn Uganda, married women are one of the highest at risk populations. One of the biggest concerns is extra-marital sex that brings HIV into a seronegative relationship, in fact, in Uganda, men are twice as likely to infect their wives in marriage after entering into the relationship as seronegative. In addition, men are less likely to communicate the fact that they are infected with HIV to their partners. For example, of men that are hospitalized for HIV/AIDS, only 12% of the wives knew of their husbands' infection. When seronegative women engage in sexual intercourse with seropositive men, they are twice as likely to contract HIV than men in the same situation. In this respect, abstinence-only sex education neglects protecting anyone that does participate in sexual acts, and especially women, due to the gender power relations in Uganda.\n\nIn abstinence-only sex education, there is not a program within to plan for pregnancy or control birth spacing. As the only tenant is to abstain from sexual relations until marriage, once in marriage this feature is not utilized. In Uganda, there are strict tradition surrounding sex post-partum and while breastfeeding. Postpartum abstinence is a widely held belief and generally practiced. Unfortunately, this often leads to extramarital intercourse. In monogamous relationships, 31% of husbands abstain completely. This remaining 69% look elsewhere for sex, through polygamous relations, sex workers, or casual relationships. This abstinent period is not effective in the realm of preventing HIV from entering a seronegative relationship. In addition, in relationships that don't observe the post-partum abstinence, birth spacing becomes non-existent. In this case, many women find themselves pregnant in quick succession.\n\nAbstinence-only sex education became quite controversial in the examination of HIV/AIDS and pregnancy prevention due to the common link with morals and proselytizing. Often, abstinence-only sex education is taught from a Christian perspective and creates a connection with virginity and self-worth. In many cases, if someone slips from abstaining, they will then view their self-worth as less as they no longer have their virginity, leading to risky sexual behaviour. Many countries have pulled support for any abstinence-only education due to concerns with the idea of HIV prevention information being coupled with missionary work and message.\n\nSupport has also been retracted when abstinence-only sex education was discovered to have been distributing false information regarding condom use and effectiveness. Abstinence-only sex education in Uganda has reportedly distributed the misinformation that there are tiny pores in condoms that allow the HIV virus to pass through, therefore deeming them unsafe to use against infection. It has also been disseminated that condoms are not effective birth control and the chance of getting pregnant is very high. This information that is easily disproven in the Global North, is difficult to combat in Uganda when it comes from political, spiritual and community leaders. This further stigmatizes condom use and lowers the likelihood that people engaging in sexual acts will use them. This misinformation is a large proponent against abstinence-only sex education as the organizations implementing the programs are unreliable or have proven to be in the past.\n\nIn many situations, the choice to abstain is non-existent. Many young girls and some boys are sex workers, selling their bodies to survive. In these high-risk environments, the participants do not have the ability to abstain or enter into a faithful monogamous relationship. Abstinence-only sex education in Uganda has been criticized for ignoring the thousands of citizens that are forced into sexual relations and therefore unprotected against HIV/AIDS. Due to abstinence-only sex education teaching abstinence until marriage as the only option, many sex workers do not know about or use condoms. This creates a breeding ground for HIV transmission. Abstinence-only sex education is woefully discriminatory against lower socioeconomic peoples.\n\nOutside of sex work, there exists young women and men that also cannot abstain do to social or financial pressure from older parties that act as \"sugar mommies\" and \"sugar daddies\". Young men and women engage in sexual relationships in order to obtain items, treats or opportunities that they would not be able to otherwise receive. Abstinence-only sex education in Uganda disregards the existence of these relationships and does not provide any program or policy to protect against HIV/AIDS infection.\n\nAnother criticism levelled at abstinence-only sex education in Uganda is the limiting language and subsequent exclusion that the policy promotes when addressing lesbian, gay and bisexual (LGB) people. Abstinence-only sex education teaches to abstain from sex until marriage to a faithful partner. In Uganda, same-sex marriage is not recognized, and gay people can be penalized for coming out. In this way, some believe that abstinence-only sex education in Uganda discriminates against LGB individuals and does not provide them with adequate tools to combat HIV/AIDS infection.\n\nCritics of abstinence-only sex education have made the point that abstinence-only truly curbs education and a well-rounded knowledge to fight HIV/AIDS infection. The implied mutual exclusivity of abstinence-only harms followers as they are not able to receive as much knowledge as possible. Leaders of the ABC method have highlighted the use of abstinence as a tool among others. The mutual exclusivity that so many abstinence-only teachers espouse does not need to exist. Abstinence can be an aspect of sex education without being the only curriculum in order to support an educated platform against HIV/AIDS infection in Uganda.\n\n"}
{"id": "55098697", "url": "https://en.wikipedia.org/wiki?curid=55098697", "title": "Acid ash hypothesis", "text": "Acid ash hypothesis\n\nThe acid-ash hypothesis is a medical hypothesis which suggests that excessively acidic diets may result in a number of identifiable health effects, including an increased risk of osteoporosis. It has received some attention in the lay community, and has been used to support the fad diet known as the Alkaline diet. According to the hypothesis, acid ash is produced by meat, poultry, cheese, fish, eggs, and grains. Alkaline ash is produced by fruits and vegetables, except cranberries, prunes and plums. Since the acid or alkaline ash designation is based on the residue left on combustion rather than the acidity of the food, foods such as citrus fruits that are generally considered acidic are actually considered alkaline producing in this diet.\n\nRecent systematic reviews have been published which have methodically analyzed the weight of available scientific evidence, and have found no significant evidence to support the acid-ash hypothesis in regard to prevention of osteoporosis. A meta-analysis of studies on the effect of dietary phosphate intake contradicted the expected results under the acid-ash hypothesis with respect to calcium in the urine and bone metabolism. This result suggests use of this diet to prevent calcium loss from bone is not justified. Other meta-analyses which have investigated the effect of total dietary acid intake have also found no evidence that acid intake increases the risk for osteoporosis as would be expected under the acid-ash hypothesis. A review looked at the effects of dairy product intake, which have been hypothesized to increase the acid load of the body through phosphate and protein components. This review found no significant evidence suggesting dairy product intake causes acidosis or increases risk for osteoporosis. A meta-analysis on the effects of alkaline potassium salts on calcium metabolism and bone health found that supplementation with alkaline potassium salts reduces loss of calcium in urine and reduces acid secretion.\n\nThe acid ash hypothesis suggests that diets high in \"acid ash\" (acid producing) elements would cause the body to try to buffer (or counteract) any additional acid load in the body by breaking down bone, leading to weaker bones and increased risk for osteoporosis. Consequently, \"alkaline ash\" (alkaline producing) elements would hypothetically decrease the risk of osteoporosis. This hypothesis has been advanced in a position statement of the Academy of Nutrition and Dietetics, in a publication of the U.S. National Academy of Sciences, as well as other scientific publications, which have stated foods high in potassium and magnesium such as fruits and vegetables may decrease the risk of osteoporosis through increased alkaline ash production. However, this acceptance of the acid-ash hypothesis as a major modifiable risk factor of osteoporosis by these publications was largely made without significant critical review by high quality systematic analysis.\n\nIt has also been speculated that an alkaline diet may have an effect on muscle wasting, growth hormone metabolism or back pain, though there is no conclusive evidence to confirm these hypotheses. Given an aging population, the effects of an alkaline diet on public health may offer some benefits due to its focus on an increase in fresh fruits and vegetables, but there are limited scientific studies on the topic.\n"}
{"id": "46383767", "url": "https://en.wikipedia.org/wiki?curid=46383767", "title": "Agrometeorology", "text": "Agrometeorology\n\nAgrometeorology is the study of weather and use of weather and climate information to enhance or expand agricultural crops and/or to increase crop production. Agrometeorology mainly involves the interaction of meteorological and hydrological factors, on one hand and agriculture, which encompasses horticulture, animal husbandry, and forestry.\n\nIt is an interdisciplinary, holistic science forming a bridge between physical and biological sciences and beyond. It deals with a complex system involving soil, plant, atmosphere, agricultural management options, and others, which are interacting dynamically on various spatial and temporal scales. Specifically, the fully coupled soil-plant-atmosphere system has to be well understood in order to develop reasonable operational applications or recommendations for stakeholders. For these reasons, a comprehensive analysis of cause-effect relationships and principles that describe the influence of the state of the atmosphere, plants, and soil on different aspects of agricultural production, as well as the nature and importance of feedback between these elements of the system is necessary. Agrometeorological methods therefore use information and data from different key sciences such as soil physics and chemistry, hydrology, meteorology, crop and animal physiology and phenology, agronomy, and others. Observed information is often combined in more or less complex models, focused on various components of system parts such as mass balances (i.e. soil carbon, nutrients, and water), biomass production, crop growth and yield, and crop or pest phenology in order to detect sensitivities or potential responses of the soil-biosphere-atmosphere system. However, model applications still involve many uncertainties, which calls for further improvements of the description of system processes. A better quality of operational applications at various scales (monitoring, forecasting, warning, recommendations, etc.) is crucial for stakeholders. For example, new methods for spatial applications involve GIS and Remote Sensing for spatial data presentation and generation. Further, tailor-made products and information transfer are critical to allow effective management decisions in the short and long term. These should cover sustainability and enhancement strategies (including risk management, mitigation and adaptation) considering climate variability and change. Papers are invited addressing these problems in the context of agrometeorological applications in “atmosphere” as an actual and important contribution to the state of the art.\n\n"}
{"id": "21837971", "url": "https://en.wikipedia.org/wiki?curid=21837971", "title": "Alfred Biesiadecki", "text": "Alfred Biesiadecki\n\nAlfred Biesiadecki (13 March 1839 – 31 March 1889) was a Polish pathologist born in Dukla.\n\nHe studied medicine at the University of Vienna, earning his medical doctorate in 1862. In 1865 he became an assistant at the institute of pathological anatomy in Vienna under Karl Rokitansky. From 1868 to 1876 he was a professor of pathological anatomy at the Jagiellonian University in Kraków, afterwards moving to Lviv where he served as \"Protomedikus\", working as an organizer of health services.\n\nBiesiadecki was a pioneer of Polish histopathology, remembered for contributions made in research of skin diseases. His name is associated with \"Biesiadecki's fossa\", a peritoneal recess that is also known as the iliacosubfascial fossa. He published medical treatises in Polish and German.\n\n\n"}
{"id": "977752", "url": "https://en.wikipedia.org/wiki?curid=977752", "title": "Amniotic fluid", "text": "Amniotic fluid\n\nThe amniotic fluid is the protective liquid contained by the amniotic sac of a gravid amniote. This fluid serves as a cushion for the growing fetus, but also serves to facilitate the exchange of nutrients, water, and biochemical products between mother and fetus.\n\nFor humans, the amniotic fluid is commonly called water or waters (Latin liquor amnii).\n\nAmniotic fluid is present from the formation of the gestational sac. Amniotic fluid is in the amniotic sac. It is generated from maternal plasma, and passes through the fetal membranes by osmotic and hydrostatic forces. When fetal kidneys begin to function in about week 16, fetal urine also contributes to the fluid. In earlier times, it was believed that the amniotic fluid was composed entirely of fetal urine.\n\nThe fluid is absorbed through the fetal tissue and skin. After the 15th-25th week of pregnancy when the keratinization of an embryo's skin occurs, the fluid is primarily absorbed by the fetal gut.\n\nAt first, amniotic fluid is mainly water with electrolytes, but by about the 12-14th week the liquid also contains proteins, carbohydrates, lipids and phospholipids, and urea, all of which aid in the growth of the fetus.\n\nThe volume of amniotic fluid increases with the growth of fetus. From the 10th to the 20th week it increases from 25ml to 400ml approximately. Approximately in the 10th-11th week the breathing and swallowing of the fetus slightly decrease the amount of fluid, but neither urination nor swallowing contributes significantly to fluid quantity changes, until the 25th week, when keratinization of skin is complete. Then the relationship between fluid and fetal growth stops. It reaches a plateau of 800ml by the 28-week gestational age. The amount of fluid declines to roughly 400 ml at 42 weeks. There is about 1L of amniotic fluid at birth.\n\nThe forewaters are released when the amnion ruptures. This is commonly known as the time when a woman's \"water breaks\". When this occurs during labour at term, it is known as \"spontaneous rupture of membranes\". If the rupture precedes labour at term, however, it is referred to as \"premature rupture of membranes\". The majority of the hindwaters remain inside the womb until the baby is born. Artificial rupture of membrane (ARM), a manual rupture of the amniotic sac, can also be performed to release the fluid if the amnion has not spontaneously ruptured.\n\nSwallowed amniotic fluid creates urine and contributes to the formation of meconium. Amniotic fluid protects the developing baby by cushioning against blows to the mother's abdomen, allowing for easier fetal movement and promoting muscular/skeletal development. Amniotic fluid swallowed by the fetus helps in the formation of the gastrointestinal tract. Contrary to popular belief, amniotic fluid has not been conclusively shown to be inhaled and exhaled by the fetus. In fact, studies from the 1970s show that in a healthy fetus, there is no inward flow of amniotic fluid into the airway. Instead, lung development occurs as a result of the production of fetal lung fluid which expands the lungs. It also prevents the fetus from mechanical jerks and shocks.\n\nAmniotic fluid is removed from the mother by an amniocentesis procedure, where a long needle is inserted through the abdomen into the amniotic sac, using ultrasound guidance such that the fetus is not harmed. \nAmniocentesis is an abnormal procedure, and is only performed if there is a suspicion of health defects in the fetus, or if an early delivery of the fetus may be necessary, since there can be complications from the procedure. If warranted, fluid is collected between 16–42 weeks of fetal development, and 20-30ml of fluid are removed.\n\nAnalysis of amniotic fluid can reveal many aspects of the baby's genetic health as well as the age and viability of the fetus. This is because the fluid contains metabolic wastes and compounds used in assessing fetal age and lung maturity, but amniotic fluid also contains fetal cells, which can be examined for genetic defects.\n\nAmniotic fluid normally has a pH of 7.0 to 7.5. Because pH in the upper vagina is normally acidic (pH 3.8-4.5), a vaginal pH test showing a pH of more than 4.5 strengthens a suspicion of rupture of membranes in case of clear vaginal discharge in pregnancy. Other tests for detecting amniotic fluid mainly include nitrazine paper test and fern test. One main test that is performed on amniotic fluid is the L/S ratio test (lecithin/sphigomyelin). This test is used to determine fetal lung maturity. Both lecithin and sphingomyelin are lung surfactants that are present in increasing amounts in the maturing fetus, though past week 33, sphigomyelin levels remain relatively constant. Measuring a ratio of L/S of 2:1 or greater indicates that the fetus can be safely delivered, with functioning lungs.\n\nToo little amniotic fluid (oligohydramnios) can be a cause or an indicator of problems for the mother and baby. The majority of pregnancies proceed normally and the baby is born healthy, but this isn't always the case. Babies with too little amniotic fluid can develop contractures of the limbs, clubbing of the feet and hands, and also develop a life-threatening condition called hypoplastic lungs. If a baby is born with hypoplastic lungs, which are small underdeveloped lungs, this condition is potentially fatal and the baby can die shortly after birth due to inadequate oxygenation. Potter sequence refers to a constellation of findings related to insufficient amniotic fluid and includes shortened and malformed limbs with clubbed feet and the underdeveloped lungs that can lead to perinatal death.\n\nOn every prenatal visit, the obstetrician/gynaecologist or midwife should measure the patient's fundal height with a tape measure. It is important that the fundal height be measured and properly recorded to track proper fetal growth and the increasing development of amniotic fluid. The obstetrician/gynaecologist or midwife should also routinely ultrasound the patient—this procedure will also give an indication of proper fetal growth and amniotic fluid development. Oligohydramnios can be caused by infection, kidney dysfunction or malformation (since much of the late amniotic fluid volume is urine), procedures such as chorionic villus sampling (CVS), and preterm premature rupture of membranes (PPROM). Oligohydramnios can sometimes be treated with bed rest, oral and intravenous hydration, antibiotics, steroids, and amnioinfusion. It is also important to keep the baby warm and moist.\n\nThe opposite of oligohydramnios is polyhydramnios, an excess volume of amniotic fluid in the amniotic sac.\n\nA rare but very often fatal condition (fatal for both mother and child) connected with amniotic fluid is amniotic fluid embolism.\n\nRecent studies show that amniotic fluid contains a considerable quantity of stem cells. These amniotic stem cells are pluripotent and able to differentiate into various tissues, which may be useful for future human application. Some researchers have found that amniotic fluid is also a plentiful source of non-embryonic stem cells. These cells have demonstrated the ability to differentiate into a number of different cell-types, including brain, liver and bone.\n\nIt is possible to conserve the stem cells extracted from amniotic fluid in private stem cells banks. Some private companies offer this service for a fee.\n\n"}
{"id": "42157607", "url": "https://en.wikipedia.org/wiki?curid=42157607", "title": "British Society of Periodontology", "text": "British Society of Periodontology\n\nThe British Society of Periodontology is a society of periodontologists (i.e. dentists who specialize in treating gum diseases) in the United Kingdom. It was founded in 1949 and its aim is to \"promote the art and science of periodontology\". Its activities include advisory publications for professionals and the public, educational workshops, society meetings and awards for research.\n\n\n"}
{"id": "3083969", "url": "https://en.wikipedia.org/wiki?curid=3083969", "title": "Capital punishment in Denmark", "text": "Capital punishment in Denmark\n\nCapital punishment in Denmark ( - literally \"Death punishment\") was abolished in 1930 but restored from 1945 to 1950 in order to execute Nazi collaborators. Capital punishment for most instances of war crimes was legally ended in 1978 (and in all cases since 1 January 1994). The last execution was in June 1950.\n\nCurrently reinstitution of capital punishment is not supported by any political party in Parliament. According to an opinion poll from 2006, one fifth of Danes supported capital punishment for certain crimes. The number was unchanged since another poll in 1999.\n\nFor the most part, Denmark followed the style of other European nations, with government-employed executioners, called \"skarpretter\" (headsman) in Denmark. The headsman had the status of a Royal government employee.\n\nThe last public execution was in Lolland of Anders \"Sjællænder\" Nielsen, by decapitation in 1882. The spectacle generated calls for the abolishment of the death penalty, particularly since the headsman, Jens Seistrup, had to swing his axe several times in order to complete the job.\n\nThe last execution prior to 1946 was on 8 November 1892, in the courtyard of the State Prison of Horsens. Jens Nielsen, sentenced to a long prison term for arson, allegedly wished to commit suicide by provoking his execution and accordingly made three attempts to murder a guard over the years. His decapitation by Seistrup's axe followed the third attempt.\n\nThe last headsman in office was Carl Peter Hermann Christensen who held the position from 27 August 1906 until 1 April 1926, but never performed any executions.\n\nStarting during the first decennia of the 1800s, death penalties were increasingly commuted to life imprisonment by the Crown. After 1892, death sentences were handed down but not carried out. This also applied to the last death sentence prior to 1945 which was handed down in a civil court on 13 June 1928.\n\nOn 1 January 1933, Denmark abolished all capital punishment under the old penal code, when the new Danish Penal Code automatically came into effect, entirely replacing the older code from 10 February 1866. Under military law, however, capital punishment still remained an option.\n\nBetween 1945 and 1947, three special laws were enacted to bring capital punishment back into the penal code, to address crimes committed during the occupation of Denmark. These were ex post facto laws and were part of the purges (Danish: \"Retsopgøret\") attempting to meet public opinion demanding severe punishment for wartime offenders, in particular certain informants and those HIPO and Gestapo officers responsible for brutal murders or torture.\n\nAbout 13,500 people were sentenced as collaborators, denouncers or traitors under these laws. About 400 were killed, mostly in extralegal reprisals, with 76 formally sentenced to death and 46 of the capital sentences carried out. The 30 remaining were pardoned. The sentences were carried out by firing squads of 10 voluntary police officers, either in Undallslund Plantage (17), close to Viborg or on the military training grounds at Margreteholm, Christianshavn, Copenhagen (29). The latter execution area is today inside Christiania, on the Second Redan of the outer rampart, Enveloppen (in Christiania called \"Aircondition\", Dyssen area) where a concrete floor and drain can still be seen at coordinates . (\"See: Freetown Christiania#Barracks and ramparts\")\n\nThe last person to be executed in Denmark was Ib Birkedal Hansen, shot by firing squad on 20 July 1950.\n\nIn 1943, the clandestine Danish Freedom Council first issued their thoughts about Denmark's return to democracy after the war. Among their demands was prosecution of war criminals and of those responsible for the violation of Denmark's legal system and independence. They endorsed retroactive legislation but were then opposed to the death penalty.\n\nShortly before the German surrender, however, the Freedom Council worked with a clandestine committee of lawyers to elaborate a proposal for a war crimes Act that included the death penalty. The Prime Minister appointed another committee, consisting of civil servants and judges. These two proposals were merged in a subsequent bill. A major point of difference was whether the law would be retroactive to only 29 August 1943, when the Danish government resigned, or all the way back to 9 April 1940 when the occupation had begun. The resistance movement got its way and the latter was decided.\n\nThe first penal code appendix bill came before Parliament from 26 to 30 May 1945, just three weeks after the liberation on 5 May. 127 members of the Folketing voted for the law, 5 members of the Justice Party abstained because of opposition to the death penalty, and 19 were absent. On 31 May it was confirmed by the Landsting by 67 votes for, 1 against and 8 were absent. Among the opponents were J.K. Jensen of the Radical Liberal Party and Oluf Pedersen of the Justice Party. Pedersen proposed an amendment which would postpone any executions until a referendum had confirmed the new law. Subsequently, he received threats from former resistance fighters. The only politician who actually ventured to cast a 'no' vote was Ingeborg Hansen, speaker of the Landsting.\n\nK.K. Steincke of the Social Democrats, himself a lawyer, expressed the general viewpoint in this way:\n\n\"If anyone in 1939 had claimed that in six years from then I would be endorsing a bill about the death penalty, even with retroactive force, I would not have regarded him as sane. But since then, barbary and lawlessness have occurred, the normal state of law has been violated deeply, and I feel then more tied to a deeply violated public conscience than to normal conditions. We must deal with these criminals, not of a lust for revenge, but so that we soon may return to normal conditions.\"\n\nThe purge after World War II has been widely debated, partially because small offences were sentenced quicker and generally more severely than trials for greater offences which lasted longer, while moods were cooling down after the end of the war. Another point of critique was the retroactivity of the law. Contrarily, proponents in the 1945 debate argued that if the death penalty was not re-applied, war criminals would be subject to mob justice or lynchings. According to a 1945 opinion poll, about 90 percent of the population were in favour of a death penalty for certain war criminals.\n\nThe background been documented in depth by historian Ditlev Tamm.\n\nIn 1952, the post-war penal code provisions were amended in order to avoid again amending the law on a retroactive basis should Denmark again come under foreign occupation. The amendments reserved capital punishment for crimes committed with particular malice during wartime (murder, treason and denunciation, limited to offenders over the age of 21). This legal basis for civil executions was abolished in 1978 and capital punishment was abolished in military law at the same time. There were no capital sentences after 1950. Capital punishment was still mentioned in the preamble of the law text, however, a new amendment confirming the removal of the death penalty from all Danish law was approved in Parliament on 22 December 1993, effective from 1 January 1994. Subsequent polls have shown varying levels of support for reintroducing capital punishment, generally amounting to one fifth or one fourth of the population. No major political actors support the reintroduction of capital punishment.\n\n"}
{"id": "23658675", "url": "https://en.wikipedia.org/wiki?curid=23658675", "title": "Clinical significance", "text": "Clinical significance\n\nIn medicine and psychology, clinical significance is the practical importance of a treatment effect—whether it has a real genuine, palpable, noticeable effect on daily life.\n\nStatistical significance is used in hypothesis testing, whereby the null hypothesis (that there is no relationship between variables) is tested. A level of significance is selected (most commonly α = 0.05 or 0.01), which signifies the probability of incorrectly rejecting a true null hypothesis. If there is a significant difference between two groups at α = 0.05, it means that there is only a 5% probability of obtaining the observed results under the assumption that the difference is entirely due to chance (i.e., the null hypothesis is true); it gives no indication of the magnitude or clinical importance of the difference. When statistically significant results are achieved, they favor rejection of the null hypothesis, but they do not prove that the null hypothesis is false. Likewise, non-significant results do not prove that the null hypothesis is true; they also give no evidence of the truth or falsity of the hypothesis the researcher has generated. Statistical significance relates only to the compatibility between observed data and what would be expected under the assumption that the null hypothesis is true.\n\nIn broad usage, the \"practical clinical significance\" answers the question, \"how effective\" is the intervention or treatment, or how much change does the treatment causes. In terms of testing clinical treatments, practical significance optimally yields quantified information about the importance of a finding, using metrics such as effect size, number needed to treat (NNT), and preventive fraction. Practical significance may also convey semi-quantitative, comparative, or feasibility assessments of utility.\n\nEffect size is one type of practical significance. It quantifies the extent to which a sample diverges from expectations. Effect size can provide important information about the results of a study, and are recommended for inclusion in addition to statistical significance. Effect sizes have their own sources of bias, are subject to change based on population variability of the dependent variable, and tend to focus on group effects, not individual changes.\n\nAlthough clinical significance and practical significance are often used synonymously, a more technical restrictive usage denotes this as erroneous. This technical use within psychology and psychotherapy not only results from a carefully drawn precision and particularity of language, but it enables a shift in perspective from group effects to the specifics of change(s) within an individual.\n\nIn contrast, when used as a technical term within psychology and psychotherapy, clinical significance yields information on whether a treatment was effective enough to change a patient’s diagnostic label. In terms of clinical treatment studies, clinical significance answers the question \"Is a treatment effective enough to cause the patient to be normal [with respect to the diagnostic criteria in question]?\"\n\nFor example, a treatment might significantly change depressive symptoms (statistical significance), the change could be a large decrease in depressive symptoms (practical significance- effect size), and 40% of the patients no longer met the diagnostic criteria for depression (clinical significance). It is very possible to have a treatment that yields a significant difference and medium or large effect sizes, but does not move a patient from dysfunctional to functional.\n\nWithin psychology and psychotherapy, clinical significance was first proposed by Jacobson, Follette, and Revenstorf as a way to answer the question, is a therapy or treatment effective enough such that a client does not meet the criteria for a diagnosis? Jacobson and Truax later defined clinical significance as “the extent to which therapy moves someone outside the range of the dysfunctional population or within the range of the functional population.” They proposed two components of this index of change: the status of a patient or client after therapy has been completed, and “how much change has occurred during the course of therapy.” \n\nClinical significance is also a consideration when interpreting the results of the psychological assessment of an individual. Frequently, there will be a difference of scores or subscores that is statistically significant, unlikely to have occurred purely by chance. However, not all of those statistically significant differences are clinically significant, in that they do not either explain existing information about the client, or provide useful direction for intervention. Differences that are small in magnitude typically lack practical relevance and are unlikely to be clinically significant. Differences that are common in the population are also unlikely to be clinically significant, because they may simply reflect a level of normal human variation. Additionally, clinicians look for information in the assessment data and the client's history that corroborates the relevance of the statistical difference, to establish the connection between performance on the specific test and the individual's more general functioning.\n\nJust as there are many ways to calculate statistical significance and practical significance, there are a variety of ways to calculate clinical significance. Five common methods are the Jacobson-Truax method, the Gulliksen-Lord-Novick method, the Edwards-Nunnally method, the Hageman-Arrindell method, and hierarchical linear modeling.\n\nJacobson-Truax is common method of calculating clinical significance. It involves calculating a Reliability Change Index (RCI). The RCI equals the difference between a participant’s pre-test and post-test scores, divided by the standard error of the difference. Cutoff scores are established for placing participants into one of four categories: recovered, improved, unchanged, or deteriorated, depending on the directionality of the RCI and whether the cutoff score was met.\n\nThe Gulliksen-Lord-Novick method is similar to Jacobson-Truax, except that it takes into account regression to the mean. This is done by subtracting the pre-test and post-test scores from a population mean, and dividing by the standard deviation of the population.\n\nThe Edwards-Nunnally method of calculating clinical significance is a more stringent alternative to the Jacobson-Truax method. Reliability scores are used to bring the pre-test scores closer to the mean, and then a confidence interval is developed for this adjusted pre-test score. Confidence intervals are used when calculating the change from pre-test to post-test, so greater actual change in scores is necessary to show clinical significance, compared to the Jacobson-Truax method.\n\nThe Hageman-Arrindell calculation of clinical significance involves indices of group change and of individual change. The reliability of change indicates whether a patient has improved, stayed the same, or deteriorated. A second index, the clinical significance of change, indicates four categories similar to those used by Jacobson-Truax: deteriorated, not reliably changed, improved but not recovered, and recovered.\n\nHLM involves growth curve analysis instead of pre-test post-test comparisons, so three data points are needed from each patient, instead of only two data points (pre-test and post-test). A computer program, such as Hierarchical Linear and Nonlinear Modeling is used to calculate change estimates for each participant. HLM also allows for analysis of growth curve models of dyads and groups.\n\n"}
{"id": "23004762", "url": "https://en.wikipedia.org/wiki?curid=23004762", "title": "Dichoptic presentation", "text": "Dichoptic presentation\n\nDichoptic (from the Greek words δίχα \"dicha\", meaning \"in two,\" and ὀπτικός \"optikos\", \"relating to sight\") is viewing a separate and independent field by each eye. In dichoptic presentation, stimulus A is presented to the left eye and a different stimulus B is presented to the right eye.\n\nDichoptic perceptual training has been tested in order to stimulate the simultaneous use of both eyes. In recent years, efforts have been made to develop methods of perceptual learning in vision therapy for treating interocular suppression and improving binocular vision in patients with anisometropic or strabismic amblyopia.\n\nIn these methods, data has been presented within a virtual reality environment, and has also been presented using a computer screen or handheld device together with matched active or passive filter glasses for the user, which present a different image to each eye.\n\nIn order to balance the input of visual information from each eye to the brain, the data is presented in such a manner that the user needs to use both eyes to see the complete scene. Furthermore, the stimulus offered to the weaker eye may be stronger, for example with stronger contrast, than the stimulus for the weaker eye.\n\nSome of these methods involve the use of dichoptic computer games (such as the game Tetris). A dichoptic presentation of popular movies has also been proposed.\n\n\n"}
{"id": "20310419", "url": "https://en.wikipedia.org/wiki?curid=20310419", "title": "Disability Standard", "text": "Disability Standard\n\nThe Disability Standard is a benchmarking assessment run in the UK by Business Disability Forum.\n\nBest described as a management tool for employers, the Disability Standard acts as a statistical study providing us with a snapshot of UK businesses performance on disability in line with the Disability Discrimination Act.\n\nAll participating organisations receive a rank to demonstrate their commitment to disability confidence. Top scores achieve Platinum followed by Gold, Silver, Bronze and Participants ranks.\n\nThe UK benchmark on Disability - later to become the Disability Standard - was first devised and piloted in 2004. Built by the Employers' Forum on Disability on the foundation of the 'Diversity Change Model', designed by Dr. Gillian Shapiro as a method for measuring an organisations performance on diversity strands, the benchmark assessment enabled an organisation to assess their organisations performance on disability in relation to the Disability Discrimination Act (DDA).\n\nThe original research and development group who took part in this pilot project consisted of:\n\nAbbey, BT, BUPA, Barclays, Centrica, Cable & Wireless, HSBC, Royal Mail, PricewaterhouseCoopers and UnumProvident in the private sector and The Office of the Deputy Prime Minister, Metropolitan Police Service, Jobcentre Plus, Department for Education and Skills and the Department for Work and Pensions in the public sector.\n\nFollowing the success of the initial trail the Disability Standard was officially launched in 2005 with help from secretary of state at the time, Alan Johnson, under the title of Employers' Forum Disability Standard 2005 with 80 UK organisations taking part including the House of Commons.\n\nThe benchmark survey ran again in 2007 with 116 participants and additions such as an interactive website enabling organisations to browse case studies and submit online. In 2009 - during the recession - 106 organisations participated taking the total number of organisations to have taken part in the Disability Standard since 2005 to over 200.\nThe findings from the Disability Standard form the base of the Benchmark Report that is published by the Employers' Forum on Disability after each assessment. Disability Standard Benchmark Reports were published in 2005, 2007, and 2009\n\nOver 100 organisations took part in the 2009 Disability Standard. For the first time since the inauguration of the benchmark the report included a top ten list of employers:\n\n"}
{"id": "28236254", "url": "https://en.wikipedia.org/wiki?curid=28236254", "title": "Dry toilet", "text": "Dry toilet\n\nA dry toilet (or non-flush toilet, no flush toilet or toilet without a flush) is a toilet that operates without flush water, unlike a flush toilet. The dry toilet may have a raised pedestal on which the user can sit, or a squat pan over which the user squats in the case of a squat toilet. In both cases, the excreta (both urine and feces) falls through a drop hole. \n\nA dry toilet can be any of the following types of toilets: a composting toilet, urine-diverting dry toilet, arborloo, container-based toilet, bucket toilet, simple pit latrine (but not those that operate on a \"pour flush\" basis), incinerating toilets, or freezing toilets.\n\nThe urine and feces can either become mixed at the point of dropping or stay separated, which is called urine diversion.\n\nThere are several types of toilets which are referred to as \"dry toilets\". All of them work without flush water and without a connection to a sewer system or septic tank:\n\nOther types of dry toilets are under development at universities, for example since 2012 funded by the Bill and Melinda Gates Foundation. Such toilets are meant to operate off-the-grid without connections to water, sewer, or electrical lines.\nOne important source states that the term \"dry toilet\" should only refer to the \"user interface\" and not the subsequent storage and treatment steps. However, in the WASH sector, the term \"dry toilet\" is still used differently by different people. It often includes also the storage and treatment steps. For example, it is common that the term \"dry toilet\" is used to refer specifically to a urine-diverting dry toilet or a composting toilet. \n\nPeople also use the term to refer to a pit latrine without a water seal even though the pit of a pit latrine is not usually dry. The pit can become very wet because urine mixes with feces in the pit and drainage might be limited. Also, groundwater or surface water can also get into the pit in the event of heavy rains or flooding. Sometimes households even discard greywater (from showering) into the same pit.\n\nSome publications use the term \"dry sanitation\" to denote a system that includes dry toilets (in particular urine-diverting dry toilets) connected to a system to manage the excreta. However, this term is not in widespread use nowadays, and might rather be replaced with \"non sewer-based sanitation\" or \"non-sewered sanitation\" (see also fecal sludge management).\n\nThe term \"outhouse\" refers to a small structure, separate from a main building, which covers a pit toilet or a dry toilet. Although it strictly refers only to the structure above the toilet, it is often used to denote the entire toilet structure, i.e. including the hole in the ground in the case of a pit latrine.\n\nDry toilets (in particular simple pit latrines) are used in developing countries in situations in which flush toilets connected to septic tanks or sewer systems are not possible or not desired, for example due to costs. Sewerage infrastructure costs can be very high in instances of unfavorable terrain or sprawling settlement patterns.\n\nDry toilets (in particular composting toilets) are also used in rural areas of developed countries, e.g. many Scandinavian countries (Sweden, Finland, Norway) for summer houses and in national parks.\n\nDry toilets can be a suitable alternative to water flushed toilets when water for flushing is of short supply. Another reason for using dry toilets can be that the infrastructure to deal with the wastewater produced from flush toilets is too expensive to construct.\n\nDry toilets are used for three main reasons instead of flush toilets:\nDry toilets and excreta management without sewers can offer more flexibility in construction than flush toilet and sewer-based systems. It can be a suitable system to adapt to climate change scenarios in desert-like areas like Lima, Peru.\n\nDry toilets do not have a water seal, thus odors may be a problem. This is often the case for pit latrines, UDDTs or composting toilets if they are not designed well or not used properly.\n\nDry toilets that are connected to a pit (such as pit latrines) tend to make it very difficult to empty the pit in a safe manner when they are full (see fecal sludge management). On the other hand, dry toilets that are not connected to a pit (e.g. container-based toilets, UDDTs and composting toilets) usually have a safe method for emptying built into them as they are designed to be emptied on a regular and quite frequent basis (within days, weeks or months).\n\nThe use of dry toilets (in particular urine-diverting dry toilets) in urban settings of developed countries is very rare as they come with some significant social and technical challenges. Legal acceptability and support at the local policy level amongst the various government departments involved might be very low for dry toilets.\n\nThe history of dry toilets is essentially the same as the history of toilets in general (until the advent of flush toilets) as well as the history of ecological sanitation systems with regards to reuse of excreta in agriculture.\n\nIn Britain, use of dry toilets continued in some areas, often urban areas, through to the 1940s. It seems that these were often emptied directly onto their gardens, where the excreta was used as fertilizer. Sewer systems did not come to some rural areas in Britain until the 1950s or even after that.\n\nBrisbane, Australia was largely unsewered until the early 1970s, with many suburbs having a dry toilet (called dunny in Australia) behind each house.\n\n"}
{"id": "10624207", "url": "https://en.wikipedia.org/wiki?curid=10624207", "title": "Ego integrity", "text": "Ego integrity\n\nEgo integrity was the term given by Erik Erikson to the last of his eight stages of psychosocial development, and used by him to represent 'a post-narcissistic love of the human ego—as an experience which conveys some world order and spiritual sense, no matter how dearly paid for'.\n\nIntegrity of the ego can also be used with respect to the development of a reliable sense of self, a reliable sense of other, and an understanding of how those constructs interact to form a person's experience of reality; as well as to the way 'the synthetic function of the ego, though it is of such extraordinary importance, is subject...to a whole number of disturbances'.\n\nErikson wrote that 'for the fruit of these seven stages I know no better word than ego integrity...the ego's accrued assurance of its proclivity for order and meaning'. Erikson considered that 'if vigor of mind combines with the \"gift of responsible renunciation\", some old people can envisage human problems in their entirety...a living example of the \"closure\" of a style of life'. \n\nThe opposite of ego integrity was despair, as 'signified by fear of death: the one and only life cycle is not accepted as the ultimate of life. Despair expresses the feeling that the time is now too short...to try out alternative roads to integrity'.\n\n'Erikson's hypothesis that maturity involves working through a conflict between integrity and despair over past accomplishments' has received some empirical support: on one measure, 'the resolution of past life stages was more predictive of ego integrity than were other personality variables'.\n\nGail Sheehy termed the later stage of 'Second Adulthood...\"Age of Integrity\" (65-85+)'.\n\nThe ninth of Loevinger's stages of ego development was the ' \"Integrated Stage\"...and ego integrity versus despair are probably Erikson's version of the Integrated Stage'.\n\nIn his structural theory, Sigmund Freud described the ego as the mediator between the id and super-ego and the external world. The task of the ego is to find a balance between primitive drives, morals, and reality, while simultaneously satisfying the id and superego. Freudians saw the ego as forming from separate \"nuclei\": 'A final ego is formed by synthetic integration of these nuclei, and in certain states of ego regression a split of the ego into its original nuclei becomes observable'.\n\nThe main concern of the ego is with safety, ideally only allowing the id's desires to be expressed when the consequences are marginal. Ego defenses are often employed by the ego when id behaviour conflicts with reality and either society's morals, norms, and taboos, or an individual's internalization of these morals, norms, and taboos. Freud noted however that in the face of conflicts with superego or id, it was always 'possible for the ego to avoid a rupture by submitting to encroachments on its own unity and even perhaps by effecting a cleavage or division of itself'. In a late, unfinished paper he examined how sometimes 'the instinct is allowed to retain its satisfaction and proper respect is shown to reality...at the price of a rift in the ego which never heals but increases as time goes on...a splitting of the ego'. Lacan would develop this line of thought, and maintain indeed that 'it is in the disintegration of the imaginary unity constituted by the ego that the subject finds the signifying material of his symptoms'.\n\nFrom another standpoint, Object relations theory has explored 'the encounter with the \"other\" that threatens the ego's integrity', as when the object in question is lacking in 'its expected function as \"container\" of excitations'.\n\nThe word ego is taken directly from Latin where it is the nominative of the first person singular personal pronoun and is translated as \"I myself\" to express emphasis—it is a translation of Freud's German term \"Das Ich\", which in English would be \"the I\".\n\n'In Cicero's \"De Senectute\"...old age acquires a meaning identified with the achievement of total self-possession, ego-integrity, and wisdom...Erikson's own psychology, on its normative side, is finally only a restatement of Stoic ideals'.\n\nIn his late haiku, 'we see Issa the old man—hundreds of years, thousands of years old, the Old Man of Edward Lear. That is our fate too. We have to die, to become nothing, in order to know the meaning of something'.\n\n"}
{"id": "46556915", "url": "https://en.wikipedia.org/wiki?curid=46556915", "title": "Electropoise", "text": "Electropoise\n\nThe Electropoise was a fake medical instrument patented and sold in the United States of America by Hercules Sanche, who also invented and sold other later fake instruments later termed as \"electroquackery\" such as the \"Oxydonor\" to remedy a range of ailments.\n\nThe instruments were widely advertised in popular magazines and the company, Electrolibration, opened numerous offices far from their headquarters in Birmingham, Alabama including New York and London. The President of Electropoise, John N. Webb, admitting that he was not a man of science stated that his hands were full handling the business. It was sold for about twenty-five dollars. Other successors like Oxydonor were sold at ten to thirty dollars. Even during its heyday, a period when electricity was viewed with awe, some physicians called attention to the nature of the fraud but their efforts had little impact on the advertisement and sale. A letter to the editor of the \"Journal of the American Medical Association\" in 1897 by a physician named George N. Kreider states that \"One of the most glaring frauds of this decade has been an appliance known as 'Electropoise,' advertised in Harper's Monthly and other leading publications\" and promoted by a W.H.De Puy, editor of the New York Christian Advocate.\n\nN.C. Morse, a physician, tried investigating the instrument and wrote: \"I have had it sawed into sections and alas, like the goose that laid the golden egg of fable fame, there is nothing in the carcass!\" Another physician named Harding wrote in 1930 that the pricing of such devices helped in selling the remedy which may have had a placebo effect. The advertisement was clever in claiming that it enhanced the body's natural healing ability. The Electropoise, it claimed: \nElsewhere it claimed:\nSkeptics pointed out the fraudulent logic used in claims and endorsements of its curative effect stating that it was dangerous use of \"post hoc ergo propter hoc\" as the logic. \nSanche was careful in that he never claimed that his device would cure diseases like tuberculosis. Others like E.L. Moses, inventor of Oxypathor, would go to jail for 18 months in 1915 for making false curative claims.\n\nThe company was not without competition. A device called the Oxygenor was marketed by a rival and Sanche went to court. The court claimed that there was not enough evidence for the value of his invention and therefore that it could not be protected. Justice Shiras and the other judges declared that his theory of \"diaduction\" (a term he coined) was a mere pretense to allow him to obtain a patent. In Britain the Richardson Electro-Galvanic Belt of Ludgate Hill and the Magneto-Electric Battery Company were competing with Electropoise.\n\n\n\n"}
{"id": "6286814", "url": "https://en.wikipedia.org/wiki?curid=6286814", "title": "Food museum", "text": "Food museum\n\nA food museum tells the story of what sustains humankind. Such museums may be specifically focused on one plant, as is the Saffron Museum in Boynes, France. They may explore a food made from a plant, for example, The Bread Museum in Ulm, Germany; a product such as the National Mustard Museum in Wisconsin; the art of food displayed at California's Copia; or historic farms, for example, Iowa's Living History Farms.\n\nIn some cases, food museums focus on how and what the world eats. Agropolis in Montpellier, France does this, as does Nestle Foundation's Alimentarium, in Vevey, Switzerland. Japan's Ramen Museum is an innovative food museum in the form of a shopping arcade featuring different noodle restaurants and displays on ramen history.\n\nFood museums are a part of the emerging food heritage movement.\n\n\n"}
{"id": "437076", "url": "https://en.wikipedia.org/wiki?curid=437076", "title": "Gay Men's Health Crisis", "text": "Gay Men's Health Crisis\n\nThe GMHC (formerly Gay Men's Health Crisis) is a New York City–based non-profit, volunteer-supported and community-based AIDS service organization whose mission statement is \"end the AIDS epidemic and uplift the lives of all affected.\"\n\nThe organization was founded in January 1982 after reports began surfacing in San Francisco and New York City that a rare form of cancer called Kaposi's sarcoma was affecting young gay men. After the Centers for Disease Control declared the new disease an epidemic, Gay Men's Health Crisis was created when 80 men gathered in New York writer Larry Kramer's apartment to discuss the issue of \"gay cancer\" and to raise money for research. GMHC took its name from the fact that the earliest men who fell victim to AIDS in the early 1980s were gay.\n\nThe founders were Nathan Fain, Larry Kramer, Lawrence D. Mass, Paul Popham, Paul Rapoport and Edmund White. They organized the formal, tax-exempt entity. At the time it was the largest volunteer AIDS organization in the world. Paul Popham was chosen as the president.\n\nRodger McFarlane began a crisis counseling hotline that originated on his own home telephone, which ultimately became one of the organization's most effective tools for sharing information about AIDS. He was named as the director of GMHC in 1982, helping create a more formal structure for the nascent organization, which had no funding or offices when he took on the role. GMHC operated out of a couple of rooms for offices in a rooming house in Chelsea owned by Mel Cheren of West End Records.\n\nLarry Kramer wrote that by the time of McFarlane's death, \"the GMHC is essentially what he started: crisis counseling, legal aid, volunteers, the buddy system, social workers\" as part of an organization that serves more than 15,000 people affected by HIV and AIDS. In an interview with \"The New York Times\" after McFarlane's death in May 2009, Kramer described how \"single-handedly Rodger took this struggling ragtag group of really frightened and mostly young men, found us an office and set up all the programs.\"\n\nKramer resigned in 1983 to form the more militant ACT UP (the AIDS Coalition to Unleash Power) as a more political alternative. From that time on his public comments and posture toward GMHC were negative, if not hostile. Kramer's play \"The Normal Heart\" is a roman à clef of his involvement with the organization.\n\nOn April 30, 1983, the GMHC sponsored the first major fund-raising event for AIDS – a benefit performance of the Ringling Bros. and Barnum & Bailey Circus.\n\nBy 1984, the Centers for Disease Control had requested GMHC's assistance in planning public conferences on AIDS. That same year, the Human Immunodeficiency Virus was discovered by the French Drs Françoise Barré-Sinoussi and Luc Montagnier. Within two years, GMHC was assisting heterosexual men and women (See Dennis Levy), hemophiliacs, intravenous drug users, and children.\n\nGay Men's Health Crisis received extensive coverage in Randy Shilts's 1987 book \"And the Band Played On\". The book described the progress of the pandemic blaming the government, especially the Reagan administration and Secretary of Health Margaret Heckler, for failing to respond. It praised GMHC for its work. Shilts was a gay man who later died of AIDS.\n\nIn 1997 the organization moved into headquarters at the nine-story Tisch Building at 119 West 24 Street in the Chelsea neighborhood. The building underwent a $12.5 Million renovation. It is named for Preston Robert Tisch and Joan Tisch. The couple donated $3.5 million for the project and Joan is on the GMHC board of directors.\n\nIn the 1990s a fundraising event on the Atlantic Ocean beach at Fire Island Pines, New York evolved into a major Circuit Party and developed a reputation for being connected with unsafe sex and recreational drug use. GMHC pulled the plug after the 1998 fundraiser after one man died on Fire Island of an overdose of the drug gamma hydroxy butyrate (GHB) the evening before the party and 21 revelers were arrested for drug possession.\n\nGMHC has received multiple grants from the Carnegie Corporation, an organization that has supported more than 550 New York City arts and social service institutions since its inception in 2002, and which was made possible through a donation by New York City mayor Michael Bloomberg (along with 406 other arts and social service institutions).\n\nGay Men's Health Crisis (GMHC) has moved to a new and expanded home consisting of of redesigned and renovated space at 446 West 33rd Street in Manhattan. GMHC is expanding its wide range of services for over 100,000 New Yorkers affected by HIV/AIDS. These services include health and nutrition education, legal, housing and mental health support, vocational training and case management. With a new state-of-the-art kitchen and larger dining room, free hot meals will be served to more clients. The Keith Haring Food Pantry Program will increase its capacity to provide grocery bags and nutrition counseling to more people in need.\nThe new location has enabled GMHC to expand its services to meet the growing and complex needs of people affected by HIV/AIDS. In this 30th year of the epidemic, HIV continues to rise at alarming rates – locally and nationally – particularly among women, African Americans, Latinos and men who have sex with men.\nThe HIV prevention and testing programs are being expanded in the new GMHC Center for HIV Prevention at 224 West 29th Street in NYC which will include a new youth leadership-development program. The new Center for HIV Prevention opened May 31.\n\n"}
{"id": "8755385", "url": "https://en.wikipedia.org/wiki?curid=8755385", "title": "Growth attenuation", "text": "Growth attenuation\n\nGrowth attenuation is an elective medical treatment which involves administering estrogen to cause closure of the epiphyses of the bones (Epiphyseal plates), resulting in a reduced adult height. Since the 1960s this treatment has been performed primarily on children growing toward an adult height considered unacceptably excessive by their parents and doctors. The cultural consensus of what is considered an excessive height justifying treatment has differed in Europe and North America and has risen over the last 4 decades.\n\nMost of the children treated have been girls, with larger numbers treated in Europe than in North America. The height considered unacceptable by parents and doctors has become substantially taller over the last few decades. Very few boys have ever been treated for excessive tallness in North America, but this also has been done more often in Europe.\n\nMore recently, growth attenuation treatment has been in the news as part of the controversial Ashley Treatment administered to a developmentally disabled girl.\n"}
{"id": "38853402", "url": "https://en.wikipedia.org/wiki?curid=38853402", "title": "Health care finance in the United States", "text": "Health care finance in the United States\n\nHealth care finance in the United States discusses how Americans obtain and pay for their healthcare, and why U.S. healthcare costs are the highest in the world based on various measures, without better results.\n\nThe American system is a mix of public and private insurance. The government provides insurance coverage for approximately 53 million elderly via Medicare, 62 million lower-income persons via Medicaid, and 15 million military veterans via the Veteran's Administration. About 178 million employed by companies receive subsidized health insurance through their employer, while 52 million other persons directly purchase insurance either via the subsidized marketplace exchanges developed as part of the Affordable Care Act or directly from insurers. The private sector delivers healthcare services, with the exception of the Veteran's Administration, where doctors are employed by the government.\n\nThe Centers for Medicare and Medicaid (CMS) reported that U.S. health care costs rose 5.8% to reach $3.2 trillion in 2015, or $9,990 per person. As measured by CMS, the share of the U.S. economy devoted to health care spending was 17.8% GDP in 2015, up from 17.4% in 2014. Increases were driven by the coverage expansion that began in 2014 as a result of the Affordable Care Act (i.e., more persons demanding healthcare or more healthcare units consumed) as well as higher healthcare prices per unit.\n\nU.S. healthcare costs are considerably higher than other countries as a share of GDP, among other measures. According to the OECD, U.S. healthcare costs in 2015 were 16.9% GDP, over 5% GDP higher than the next most expensive OECD country. A gap of 5% GDP represents $1 trillion, about $3,000 per person relative to the next most expensive country. In other words, the U.S. would have to cut healthcare costs by roughly one-third to be competitive with the next most expensive country.\n\nReasons for higher costs than other countries including higher administrative costs, spending more for the same services (i.e., higher prices per unit), receiving more medical care (units) per capita than other countries, cost variation across hospital regions without different results, higher levels of per-capita income, and less active government intervention to reduce costs. Spending is highly concentrated among sicker patients. The Institute of Medicine reported in September 2012 that approximately $750B per year in U.S. health care costs are avoidable or wasted. This included: unnecessary services ($210 billion annually); inefficient delivery of care ($130 billion); excess administrative costs ($190 billion); inflated prices ($105 billion); prevention failures ($55 billion), and fraud ($75 billion).\n\nDespite this spending, the quality of health care overall is low by OECD measures. The Commonwealth Fund ranked the United States last in the quality of health care among similar countries.\n\nThe percentage of persons without health insurance (the \"uninsured\") fell from 13.3% in 2013 to 8.8% in 2016, due primarily to the Affordable Care Act. The number uninsured fell from 41.8 million in 2013 to 28.0 million in 2016, a decline of 13.8 million. The number of persons with insurance (public or private) rose from 271.6 million in 2013 to 292.3 million in 2016, an increase of 20.7 million. In 2016, approximately 68% were covered by private plans, while 37% were covered by government plans; these do not add to 100% because some persons have both.\n\nAmong those whose employer pays for health insurance, the employee may be required to contribute part of the cost of this insurance, while the employer usually chooses the insurance company and, for large groups, negotiates with the insurance company. The government subsidizes the employer-based insurance by excluding premiums paid by employers from the employees income. This subsidy tax expenditure reduced federal tax revenue by $248 billion in 2013, or 1.5% GDP.\n\nThe non-partisan Congressional Budget Office (CBO) reported in March 2017 that healthcare cost inflation and an aging population are primary drivers of increasing budget deficits over time, as outlays (spending) continue to rise faster than revenues relative to GDP. CBO forecast that spending on major healthcare programs (including Medicare and Medicaid) would rise from 5.5% GDP in 2017 to 9.2% GDP by 2047.\n\nThe Centers for Medicare and Medicaid (CMS) reported that U.S. health care costs rose to 17.8% GDP in 2015, up from 17.4% in 2014. Increases were driven by the coverage expansion that began in 2014 as a result of the Affordable Care Act (i.e., more persons demanding healthcare or more healthcare units consumed) as well as higher healthcare prices per unit.\n\nU.S. healthcare costs are considerably higher than other countries as a share of GDP, among other measures. According to the OECD, U.S. healthcare costs in 2015 were 16.9% GDP, over 5% GDP higher than the next most expensive OECD country. A gap of 5% GDP represents $1 trillion, about $3,000 per person relative to the next most expensive country. In other words, the U.S. would have to cut healthcare costs by roughly one-third ($1 trillion or $3,000 per person on average) to be competitive with the next most expensive country.\n\nThe Centers for Medicare and Medicaid (CMS) reported that U.S. health care costs rose 5.8% to reach $3.2 trillion in 2015, or $9,990 per person.\n\nThe Office of the Actuary (OACT) of the Centers for Medicare and Medicaid Services publishes data on total health care spending in the United States, including both historical levels and future projections. In 2007, the U.S. spent $2.26 trillion on health care, or $7,439 per person, up from $2.1 trillion, or $7,026 per capita, the previous year. Spending in 2006 represented 16% of GDP, an increase of 6.7% over 2004 spending. Growth in spending is projected to average 6.7% annually over the period 2007 through 2017.\n\nIn 2009, the United States federal, state and local governments, corporations and individuals, together spent $2.5 trillion, $8,047 per person, on health care. This amount represented 17.3% of the GDP, up from 16.2% in 2008. Health insurance costs are rising faster than wages or inflation, and medical causes were cited by about half of bankruptcy filers in the United States in 2001.\n\nThe Centers for Medicare and Medicaid Services reported in 2013 that the rate of increase in annual healthcare costs has fallen since 2002. However, costs relative to GDP and per capita continue to rise. Per capita cost increases have averaged 5.4% since 2000.\n\nAccording to Federal Reserve data, healthcare annual inflation rates have declined in recent decades:\n\nWhile this inflation rate has declined, it has generally remained above the rate of economic growth, resulting in a steady increase of health expenditures relative to GDP from 6% in 1970 to nearly 18% in 2015.\n\nU.S. healthcare costs in 2015 were 16.9% GDP according to the OECD, over 5% GDP higher than the next most expensive OECD country. With U.S. GDP of $19 trillion, healthcare costs were about $3.2 trillion, or about $10,000 per person in a country of 320 million people. A gap of 5% GDP represents $1 trillion, about $3,000 per person relative to the next most expensive country. In other words, the U.S. would have to cut healthcare costs by roughly one-third to be competitive with the next most expensive country.\n\nOne analysis of international spending levels in the year 2000 found that while the U.S. spends more on health care than other countries in the Organisation for Economic Co-operation and Development (OECD), the use of health care services in the U.S. is below the OECD median by most measures. The authors of the study concluded that the prices paid for health care services are much higher in the U.S.\n\nSpending is highly concentrated among a relatively few patients. The Kaiser Family Foundation reported that the concentration of health care spending in the U.S. in 2010 was as follows:\n\nOther studies have found similar results using AHRQ analysis. Relative to the overall population, those who remained in the top 10% of spenders between 2008 and 2009 were more likely to be in fair or poor health, elderly, female, non-Hispanic whites and those with public-only coverage. Those who remained in the bottom half of spenders were more likely to be in excellent health, children and young adults, men, Hispanics, and the uninsured. These patterns were stable through the 1970s and 1980s, and some data suggest that they may have been typical of the mid-to-early 20th century as well.\n\nAn earlier study by AHRQ the found significant persistence in the level of health care spending from year to year. Of the 1% of the population with the highest health care spending in 2002, 24.3% maintained their ranking in the top 1% in 2003. Of the 5% with the highest spending in 2002, 34% maintained that ranking in 2003. Individuals over age 45 were disproportionately represented among those who were in the top decile of spending for both years.\n\nSeniors spend, on average, far more on health care costs than either working-age adults or children. The pattern of spending by age was stable for most ages from 1987 through 2004, with the exception of spending for seniors age 85 and over. Spending for this group grew less rapidly than that of other groups over this period.\n\nThe 2008 edition of the Dartmouth Atlas of Health Care found that providing Medicare beneficiaries with severe chronic illnesses with more intense health care in the last two years of life—increased spending, more tests, more procedures and longer hospital stays—is not associated with better patient outcomes. There are significant geographic variations in the level of care provided to chronically ill patients, only 4% of which are explained by differences in the number of severely ill people in an area. Most of the differences are explained by differences in the amount of \"supply-sensitive\" care available in an area. Acute hospital care accounts for over half (55%) of the spending for Medicare beneficiaries in the last two years of life, and differences in the volume of services provided is more significant than differences in price. The researchers found no evidence of \"substitution\" of care, where increased use of hospital care would reduce outpatient spending (or vice versa).\n\nHealthcare spending in the U.S. was distributed as follows by type of service in 2014: Hospital care 32%; physician and clinical services 20%; prescription drugs 10%; and all other, including many categories individually making up less than 7% of spending. These first three categories accounted for 62% of spending.\n\nThis distribution is relatively stable; in 2008, 31% went to hospital care, 21% to physician/clinical services, 10% to pharmaceuticals, 4% to dental, 6% to nursing homes, 3% to home health care, 3% for other retail products, 3% for government public health activities, 7% to administrative costs, 7% to investment, and 6% to other professional services (physical therapists, optometrists, etc.).\n\nAccording to a report from the Agency for Healthcare Research and Quality (AHRQ), aggregate U.S. hospital costs in 2011 were $387.3 billion—a 63% increase since 1997 (inflation adjusted). Costs per stay increased 47% since 1997, averaging $10,000 in 2011.\n\nAn estimated 178 million persons under 65 obtain their insurance through their employer. Firms are often \"self-insured\", meaning they reimburse the insurance companies that pay the medical claims on behalf of their employees. Employers may use a stop-loss, meaning they pay the insurance company a premium to cover very expensive individual claims (e.g., the firm is self-insured up to a threshold for individual workers). Workers pay a share of their costs to their employers for coverage, basically a premium deducted from their paychecks. Workers also have deductibles and out-of-pocket costs. The structure of the insurance plan may also include a Health savings account or HSA, which enable workers to save money tax-free for health expenses.\n\nThe Kaiser Family Foundation reported that employer-based health insurance premiums for a family of four averaged $18,765 in 2017, up 3% from the prior year, although there was considerable variation around this average. For single coverage, the premium costs averaged $6,690, up 4% from the previous year. The typical worker contributed $5,714 on average towards their coverage, with the employer providing the remainder.\n\nDeductibles have been rising much faster than premiums in recent years. For example, deductibles rose 12% in 2016, four times faster than premiums. From 2011 to 2016, deductibles rose 63% for single coverage, versus 19% for single coverage premiums. During that time, worker earnings rose 11%. The average annual deductible is around $1,500. For employers with fewer than 200 employees, 65% of employees are now in \"high-deductible plans\" which averaged $2,000.\n\nOne consequence of employer-based coverage (as opposed to single-payer or government-funded via individual taxes) is that employers facing increasing healthcare costs offset the expense by either paying relatively less or hiring fewer workers. Since health insurance benefits paid by employers are not treated as income to employees, the government foregoes a sizable amount of tax revenue each year. This subsidy or tax expenditure was estimated by CBO at $281 billion in 2017. On March 1, 2010, billionaire investor Warren Buffett said that the high costs paid by U.S. companies for their employees' health care put them at a competitive disadvantage. He compared the roughly 17% of GDP spent by the U.S. on health care with the 9% of GDP spent by much of the rest of the world, noted that the U.S. has fewer doctors and nurses per person, and said, \"[t]hat kind of a cost, compared with the rest of the world, is like a tapeworm eating at our economic body.\"\n\nAn estimated 12 million persons obtained their insurance from insurance companies in 2016 via online marketplaces (federal or state) developed as part of the Affordable Care Act, also known as \"Obamacare.\" This insurance is federally subsidized through a premium tax credit, which varies based on the level of income of the individual. The credit is typically applied by the insurance company to lower the monthly premium payment. The post-subsidy premium cost is capped as a percentage of income, meaning as premiums rise the subsidies rise. Approximately 10 million persons on the exchanges are eligible for subsidies. An estimated 80% of persons obtaining coverage under the ACA can get it for less than $75 per month after subsidies, if they choose the lowest-cost \"bronze\" plan. The average cost for the \"second-lowest cost silver plan\" (the benchmark plan and one of the most popular) was $208/month after subsidy for a 40-year-old male non-smoker in 2017.\n\nPresident Trump's decision in November 2017 to end the cost sharing reductions subsidy, a second type of subsidy used to reduce deductibles and co-payments, was expected to increase premiums dramatically, thereby increasing the premium tax credits as well to maintain after-subsidy costs to participants at the same percentage of income. In other words, the after-subsidy cost would not rise for those with premium tax credit subsidies. Those obtaining their insurance via the exchanges without subsidies would pay up to 20 percentage points more for insurance. CBO also estimated a $200 billion increase in the budget deficit over a decade due to Trump's decision.\n\nThe CBO estimated that ending or not enforcing the individual mandate (which requires those without health insurance to pay a penalty) would increase the uninsured by 13 million by 2027, reducing the budget deficit by $338 billion over 10 years as subsidies fall. CBO also estimated that ending the mandate would encourage healthier people to drop out of the marketplaces, thus raising premiums by up to 10%.\n\nMedicaid is a joint federal and state program that helps with medical costs for about 74 million people (as of 2017) with limited income and resources. Medicaid also offers benefits not normally covered by Medicare, like nursing home care and personal care services. Medicaid is the largest source of funding for medical and health-related services for people with low income in the United States, providing free health insurance to low-income and disabled people. It is a means-tested program that is jointly funded by the state and federal governments and managed by the states, with each state currently having broad leeway to determine who is eligible for its implementation of the program. States are not required to participate in the program, although all have since 1982. Medicaid recipients must be U.S. citizens or legal permanent residents, and may include low-income adults, their children, and people with certain disabilities. Poverty alone does not necessarily qualify someone for Medicaid. The Federal Medical Assistance Percentage (FMAP), the percent of Medicaid program costs covered by the federal government, ranges from 50% for higher-income states to 75% for states with lower per-capita incomes.\n\nThe Affordable Care Act (\"Obamacare\") significantly expanded both eligibility for and federal funding of Medicaid starting in 2014, with an additional 11 million covered by 2016. Under the law as written, all U.S. citizens and legal residents with income up to 133% of the poverty line, including adults without dependent children, would qualify for coverage in any state that participated in the Medicaid program. However, the United States Supreme Court ruled in National Federation of Independent Business v. Sebelius that states do not have to agree to this expansion to continue to receive previously established levels of Medicaid funding, and 19 Republican-controlled states have chosen to continue with pre-ACA funding levels and eligibility standards. Expanding Medicaid in these 19 states would expand coverage for up to four million people.\n\nThe CBO reported in October 2017 that the federal government spent $375 billion on Medicaid in fiscal year 2017, an increase of $7 billion or 2% over 2016. The increase was primarily driven by more persons covered due to the ACA.\n\nMedicare covered 57 million people mainly aged 65 and over as of September 2016. Enrollees pay little in premiums but have deductibles for hospital stays. The program is funded partially by the FICA payroll tax and partially by the general fund (other tax revenues). The CBO reported in October 2017 that adjusted for timing differences, Medicare spending rose by $22 billion (4%) in fiscal year 2017 to $595 billion, reflecting growth in both the number of beneficiaries and in the average benefit payment. Medicare average spending per-enrollee was $10,986 in 2014 across the U.S., with states ranging from $8,238 in Montana to $12,614 in New Jersey.\n\nThe reasons for higher U.S. healthcare costs relative to other countries and over time are debated by experts.\n\nThere are many reasons why U.S. healthcare costs are higher than other OECD countries:\n\nIn December 2011, the outgoing Administrator of the Centers for Medicare & Medicaid Services, Dr. Donald Berwick, asserted that 20% to 30% of health care spending is waste. He listed five causes for the waste: (1) overtreatment of patients, (2) the failure to coordinate care, (3) the administrative complexity of the health care system, (4) burdensome rules and (5) fraud.\n\nThe Institute of Medicine reported in September 2012 that approximately $750B per year in U.S. health care costs are avoidable or wasted. This included: unnecessary services ($210 billion annually); inefficient delivery of care ($130 billion); excess administrative costs ($190 billion); inflated prices ($105 billion); prevention failures ($55 billion), and fraud ($75 billion).\n\nThe Congressional Budget Office analyzed the reasons for healthcare cost inflation over time, reporting in 2008 that: \"Although many factors contributed to the growth, most analysts have concluded that the bulk of the long-term rise resulted from the health care system's use of new medical services that were made possible by technological advances...\" In summarizing several studies, CBO reported the following drove the indicated share of the increase (shown as a range across three studies) from 1940 to 1990:\n\nSeveral studies have attempted to explain the reduction in the rate of annual increase following the Great Recession of 2007-2009. Reasons include, among others:\n\nIn September 2008 \"The Wall Street Journal\" reported that consumers were reducing their health care spending in response to the current economic slow-down. Both the number of prescriptions filled and the number of office visits dropped between 2007 and 2008. In one survey, 22% of consumers reported going to the doctor less often, and 11% reported buying fewer prescription drugs.\n\nThe Health and Human Services Department expects that the health share of GDP will continue its historical upward trend, reaching 19.6% of GDP by 2024.\n\nThe non-partisan Congressional Budget Office (CBO) reported in March 2017 that healthcare cost inflation and an aging population are primary drivers of increasing budget deficits over time, as outlays (spending) continue to rise faster than revenues relative to GDP. CBO forecast that spending on major healthcare programs (including Medicare and Medicaid) would rise from 5.5% GDP in 2017 to 9.2% GDP by 2047.\n\nThe Medicare Trustees provide an annual report of the program's finances. The forecasts from 2009 and 2015 differ materially, mainly due to changes in the projected rate of healthcare cost increases, which have moderated considerably. Rather than rising to nearly 12% GDP over the forecast period (through 2080) as forecast in 2009, the 2015 forecast has Medicare costs rising to 6% GDP, comparable to the Social Security program.\n\nThe increase in healthcare costs is one of the primary drivers of long-term budget deficits. The long-term budget situation has considerably improved in the 2015 forecast versus the 2009 forecast per the Trustees Report.\n\nDoctors and hospitals are generally funded by payments from patients and insurance plans in return for services rendered (fee-for-service or FFS). In the FFS payment model, each service provided is billed as an individual item, which creates an incentive to provide more services (e.g., more tests, more expensive procedures, and more medicines). This is in contrast to bundled payments, in which the amount the insurer will pay to the service providers is bundled per episode (e.g., for a heart attack patient, a total amount will be paid to the network providing the care for say 180 days). Bundling on a per patient basis (rather than per-episode) was referred to in the 1990s as a \"capitated payment\" but is now described as an accountable care organization. Bundling provides an incentive to lower costs, which requires offsetting measures and incentives for quality of care. Several best-practice healthcare systems, such as the Kaiser and Mayo health systems, use bundled payments.\n\nAmong those whose employer pays for health insurance, the employee may be required to contribute part of the cost of this insurance, while the employer usually chooses the insurance company and, for large groups, negotiates with the insurance company. In 2004, private insurance paid for 36% of personal health expenditures, private out-of-pocket 15%, federal government 34%, state and local governments 11%, and other private funds 4%. Due to \"a dishonest and inefficient system\" that sometimes inflates bills to ten times the actual cost, even insured patients can be billed more than the real cost of their care.\n\nInsurance for dental and vision care (except for visits to ophthalmologists, which are covered by regular health insurance) is usually sold separately. Prescription drugs are often handled differently from medical services, including by the government programs. Major federal laws regulating the insurance industry include COBRA and HIPAA.\n\nIndividuals with private or government insurance are limited to medical facilities which accept the particular type of medical insurance they carry. Visits to facilities outside the insurance program's \"network\" are usually either not covered or the patient must bear more of the cost. Hospitals negotiate with insurance programs to set reimbursement rates; some rates for government insurance programs are set by law. The sum paid to a doctor for a service rendered to an insured patient is generally less than that paid \"out of pocket\" by an uninsured patient. In return for this discount, the insurance company includes the doctor as part of their \"network\", which means more patients are eligible for lowest-cost treatment there. The negotiated rate may not cover the cost of the service, but providers (hospitals and doctors) can refuse to accept a given type of insurance, including Medicare and Medicaid. Low reimbursement rates have generated complaints from providers, and some patients with government insurance have difficulty finding nearby providers for certain types of medical services.\n\nCharity care for those who cannot pay is sometimes available, and is usually funded by non-profit foundations, religious orders, government subsidies, or services donated by the employees. Massachusetts and New Jersey have programs where the state will pay for health care when the patient cannot afford to do so. The City and County of San Francisco is also implementing a citywide health care program for all uninsured residents, limited to those whose incomes and net worth are below an eligibility threshold. Some cities and counties operate or provide subsidies to private facilities open to all regardless of the ability to pay. Means testing is applied, and some patients of limited means may be charged for the services they use.\n\nThe Emergency Medical Treatment and Active Labor Act requires virtually all hospitals to accept all patients, regardless of the ability to pay, for emergency room care. The act does not provide access to non-emergency room care for patients who cannot afford to pay for health care, nor does it provide the benefit of preventive care and the continuity of a primary care physician. Emergency health care is generally more expensive than an urgent care clinic or a doctor's office visit, especially if a condition has worsened due to putting off needed care. Emergency rooms are typically at, near, or over capacity. Long wait times have become a problem nationally, and in urban areas some ERs are put on \"diversion\" on a regular basis, meaning that ambulances are directed to bring patients elsewhere.\n\nMost Americans under age 65 (59.3%) receive their health insurance coverage through an employer (which includes both private as well as civilian public-sector employers) under group coverage, although this percentage is declining. Costs for employer-paid health insurance are rising rapidly: since 2001, premiums for family coverage have increased 78%, while wages have risen 19% and inflation has risen 17%, according to a 2007 study by the Kaiser Family Foundation. Workers with employer-sponsored insurance also contribute; in 2007, the average percentage of premium paid by covered workers is 16% for single coverage and 28% for family coverage. In addition to their premium contributions, most covered workers face additional payments when they use health care services, in the form of deductibles and copayments.\n\nJust less than 9% of the population purchases individual health care insurance. Insurance payments are a form of cost-sharing and risk management where each individual or their employer pays predictable monthly premiums. This cost-spreading mechanism often picks up much of the cost of health care, but individuals must often pay up-front a minimum part of the total cost (a \"deductible\"), or a small part of the cost of every procedure (a copayment). Private insurance accounts for 35% of total health spending in the United States, by far the largest share among OECD countries. Beside the United States, Canada and France are the two other OECD countries where private insurance represents more than 10% of total health spending.\n\nProvider networks can be used to reduce costs by negotiating favorable fees from providers, selecting cost effective providers, and creating financial incentives for providers to practice more efficiently. A survey issued in 2009 by America's Health Insurance Plans found that patients going to out-of-network providers are sometimes charged extremely high fees.\n\nDefying many analysts' expectations, PPOs have gained market share at the expense of HMOs over the past decade.\n\nJust as the more loosely managed PPOs have edged out HMOs, HMOs themselves have also evolved towards less tightly managed models. The first HMOs in the U.S., such as Kaiser Permanente in Oakland, California, and the Health Insurance Plan (HIP) in New York, were \"staff-model\" HMOs, which owned their own health care facilities and employed the doctors and other health care professionals who staffed them. The name health maintenance organization stems from the idea that the HMO would make it its job to maintain the enrollee's health, rather than merely to treat illnesses. In accordance with this mission, managed care organizations typically cover preventive health care. Within the tightly integrated staff-model HMO, the HMO can develop and disseminate guidelines on cost-effective care, while the enrollee's primary care doctor can act as patient advocate and care coordinator, helping the patient negotiate the complex health care system. Despite a substantial body of research demonstrating that many staff-model HMOs deliver high-quality and cost-effective care, they have steadily lost market share. They have been replaced by more loosely managed networks of providers with whom health plans have negotiated discounted fees. It is common today for a physician or hospital to have contracts with a dozen or more health plans, each with different referral networks, contracts with different diagnostic facilities, and different practice guidelines.\n\nGovernment programs directly cover 27.8% of the population (83 million), including the elderly, disabled, children, veterans, and some of the poor, and federal law mandates public access to emergency services regardless of ability to pay. Public spending accounts for between 45% and 56.1% of U.S. health care spending. Per-capita spending on health care by the U.S. government placed it among the top ten highest spenders among United Nations member countries in 2004.\n\nHowever, all government-funded healthcare programs exist only in the form of statutory law, and accordingly can be amended or revoked like any other statute. There is no constitutional right to healthcare. The U.S. Supreme Court explained in 1977 that \"the Constitution imposes no obligation on the States to pay ... any of the medical expenses of indigents.\"\n\nGovernment funded programs include:\n\nThe exemption of employer-sponsored health benefits from federal income and payroll taxes distorts the health care market. The U.S. government, unlike some other countries, does not treat employer funded health care benefits as a taxable benefit in kind to the employee. The value of the lost tax revenue from a benefits in kind tax is an estimated $150 billion a year. Some regard this as being disadvantageous to people who have to buy insurance in the individual market which must be paid from income received after tax.\n\nHealth insurance benefits are an attractive way for employers to increase the salary of employees as they are nontaxable. As a result, 65% of the non-elderly population and over 90% of the privately insured non-elderly population receives health insurance at the workplace. Additionally, most economists agree that this tax shelter increases individual demand for health insurance, leading some to claim that it is largely responsible for the rise in health care spending.\n\nIn addition the government allows full tax shelter at the highest marginal rate to investors in health savings accounts (HSAs). Some have argued that this tax incentive adds little value to national health care as a whole because the most wealthy in society tend also to be the most healthy. Also it has been argued, HSAs segregate the insurance pools into those for the wealthy and those for the less wealthy which thereby makes equivalent insurance cheaper for the rich and more expensive for the poor. However, one advantage of health insurance accounts is that funds can only be used towards certain HSA qualified expenses, including medicine, doctor's fees, and Medicare Parts A and B. Funds cannot be used towards expenses such as cosmetic surgery.\n\nThere are also various state and local programs for the poor. In 2007, Medicaid provided health care coverage for 39.6 million low-income Americans (although Medicaid covers approximately 40% of America's poor), and Medicare provided health care coverage for 41.4 million elderly and disabled Americans. Enrollment in Medicare is expected to reach 77 million by 2031, when the baby boom generation is fully enrolled.\n\nIt has been reported that the number of physicians accepting Medicaid has decreased in recent years due to relatively high administrative costs and low reimbursements. In 1997, the federal government also created the State Children's Health Insurance Program (SCHIP), a joint federal-state program to insure children in families that earn too much to qualify for Medicaid but cannot afford health insurance. SCHIP covered 6.6 million children in 2006, but the program is already facing funding shortfalls in many states. The government has also mandated access to emergency care regardless of insurance status and ability to pay through the Emergency Medical Treatment and Labor Act (EMTALA), passed in 1986, but EMTALA is an unfunded mandate.\n\nThe percentage of persons without health insurance (the \"uninsured\") fell from 13.3% in 2013 to 8.8% in 2016, due primarily to the Affordable Care Act. The number uninsured fell from 41.8 million in 2013 to 28.0 million in 2016, a decline of 13.8 million. The number of persons with insurance (public or private) rose from 271.6 million in 2013 to 292.3 million in 2016, an increase of 20.7 million. In 2016, approximately 68% were covered by private plans, while 37% were covered by government plans; these do not add to 100% because some persons have both.\n\nSome Americans do not qualify for government-provided health insurance, are not provided health insurance by an employer, and are unable to afford, cannot qualify for, or choose not to purchase, private health insurance. When charity or \"uncompensated\" care is not available, they sometimes simply go without needed medical treatment. This problem has become a source of considerable political controversy on a national level. The uninsured still receive emergency care and thus if they are unable to afford it, they impose costs on others who pay higher premiums and deductibles to cover these expenses indirectly. Estimates for 2008 reported that the uninsured would spend $30 billion for healthcare and receive $56 billion in uncompensated care, and that if everyone were covered by insurance then overall costs would increase by $123 billion. A 2003 Institute of Medicine (IOM) report estimated total cost of health care provided to the uninsured at $98.9 billion in 2001, including $26.4 billion in out-of-pocket spending by the uninsured, with $34.5 billion in \"free\" \"uncompensated\" care covered by government subsidies of $30.6 billion to hospitals and clinics and $5.1 billion in donated services by physicians.\n\nA 2003 study in \"Health Affairs\" estimated that uninsured people in the U.S. received approximately $35 billion in uncompensated care in 2001. The study noted that this amount per capita was half what the average insured person received. The study found that various levels of government finance most uncompensated care, spending about $30.6 billion on payments and programs to serve the uninsured and covering as much as 80–85% of uncompensated care costs through grants and other direct payments, tax appropriations, and Medicare and Medicaid payment add-ons. Most of this money comes from the federal government, followed by state and local tax appropriations for hospitals. Another study by the same authors in the same year estimated the additional annual cost of covering the uninsured (in 2001 dollars) at $34 billion (for public coverage) and $69 billion (for private coverage). These estimates represent an increase in total health care spending of 3–6% and would raise health care's share of GDP by less than one percentage point, the study concluded. Another study published in the same journal in 2004 estimated that the value of health forgone each year because of uninsurance was $65–$130 billion and concluded that this figure constituted \"a lower-bound estimate of economic losses resulting from the present level of uninsurance nationally.\"\n\nNumerous publicly funded health care programs help to provide for the elderly, disabled, military service families and veterans, children, and the poor, and federal law ensures public access to emergency services regardless of ability to pay; however, a system of universal health care has not been implemented nationwide. However, as the OECD has pointed out, the total U.S. public expenditure for this limited population would, in most other OECD countries, be enough for the government to provide primary health insurance for the entire population. Although the federal Medicare program and the federal-state Medicaid programs possess some monopsonistic purchasing power, the highly fragmented buy side of the U.S. health system is relatively weak by international standards, and in some areas, some suppliers such as large hospital groups have a virtual monopoly on the supply side. In most OECD countries, there is a high degree of public ownership and public finance. The resulting economy of scale in providing health care services appears to enable a much tighter grip on costs. The U.S., as a matter of oft-stated public policy, largely does not regulate prices of services from private providers, assuming the private sector to do it better.\n\nMassachusetts has adopted a universal health care system through the Massachusetts 2006 Health Reform Statute. It mandates that all residents who can afford to do so purchase health insurance, provides subsidized insurance plans so that nearly everyone can afford health insurance, and provides a \"Health Safety Net Fund\" to pay for necessary treatment for those who cannot find affordable health insurance or are not eligible.\n\nIn July 2009, Connecticut passed into law a plan called SustiNet, with the goal of achieving health care coverage of 98% of its residents by 2014.\n\nPrimary cost reduction opportunities correspond to the causes described above. These include:\n\nIncreased spending on disease prevention is often suggested as a way of reducing health care spending. Whether prevention saves or costs money depends on the intervention. Childhood vaccinations, or contraceptives save much more than they cost. Research suggests that in many cases prevention does not produce significant long-term cost savings. Some interventions may be cost-effective by providing health benefits, while others are not cost-effective. Preventive care is typically provided to many people who would never become ill, and for those who would have become ill is partially offset by the health care costs during additional years of life. On the other hand, research conducted by Novartis argues that the countries that have excelled in getting the highest value for healthcare spending are the ones who have invested more in prevention, early diagnosis and treatment. The trick is to avoid getting patients to hospital, which is where highest healthcare dollars are being consumed. Not all preventive measures have good ROI (EG. Global vaccination campaign for a rare infectious diseases). However, preventive measures such as diet, exercises and reduction of tobacco intake would have broad impact on many diseases and will offer good return of investment.\n\n"}
{"id": "49604", "url": "https://en.wikipedia.org/wiki?curid=49604", "title": "Hearing loss", "text": "Hearing loss\n\nHearing loss, also known as hearing impairment, is a partial or total inability to hear. A deaf person has little to no hearing. Hearing loss may occur in one or both ears. In children, hearing problems can affect the ability to learn spoken language and in adults it can create difficulties with social interaction and at work. In some people, particularly older people, hearing loss can result in loneliness. Hearing loss can be temporary or permanent.\nHearing loss may be caused by a number of factors, including: genetics, ageing, exposure to noise, some infections, birth complications, trauma to the ear, and certain medications or toxins. A common condition that results in hearing loss is chronic ear infections. Certain infections during pregnancy, such as syphilis and rubella, may also cause hearing loss in the child. Hearing loss is diagnosed when hearing testing finds that a person is unable to hear 25 decibels in at least one ear. Testing for poor hearing is recommended for all newborns. Hearing loss can be categorized as mild (25 to 40 dB), moderate (41 to 55 dB), moderate-severe (56 to 70 dB), severe (71 to 90 dB), or profound (greater than 90 dB). There are three main types of hearing loss: conductive hearing loss, sensorineural hearing loss, and mixed hearing loss.\nAbout half of hearing loss globally is preventable through public health measures. Such practices include immunization, proper care around pregnancy, avoiding loud noise, and avoiding certain medications. The World Health Organization recommends that young people limit the use of personal audio players to an hour a day in an effort to limit exposure to noise. Early identification and support are particularly important in children. For many hearing aids, sign language, cochlear implants and subtitles are useful. Lip reading is another useful skill some develop. Access to hearing aids, however, is limited in many areas of the world.\nAs of 2013 hearing loss affects about 1.1 billion people to some degree. It causes disability in 5% (360 to 538 million) and moderate to severe disability in 124 million people. Of those with moderate to severe disability 108 million live in low and middle income countries. Of those with hearing loss it began in 65 million during childhood. Those who use sign language and are members of Deaf culture see themselves as having a difference rather than an illness. Most members of Deaf culture oppose attempts to cure deafness and some within this community view cochlear implants with concern as they have the potential to eliminate their culture. The term hearing impairment is often viewed negatively as it emphasises what people cannot do.\n\nUse of the terms \"hearing impaired\", \"deaf-mute\", or \"deaf and dumb\" to describe deaf and hard of hearing people is discouraged by advocacy organizations as they are offensive to many deaf and hard of hearing people.\n\nHuman hearing extends in frequency from 20–20,000 Hz, and in amplitude from 0 dB to 130 dB or more. 0 dB does not represent absence of sound, but rather the softest sound an average unimpaired human ear can hear; some people can hear down to −5 or even −10 dB. 130 dB represents the threshold of pain. But the ear does not hear all frequencies equally well; hearing sensitivity peaks around 3000 Hz. There are many qualities of human hearing besides frequency range and intensity that can't easily be measured quantitatively. But for many practical purposes, normal hearing is defined by a frequency versus intensity graph, or audiogram, charting sensitivity thresholds of hearing at defined frequencies. Because of the cumulative impact of age and exposure to noise and other acoustic insults, 'typical' hearing may not be normal.\n\n\nHearing loss is sensory, but may have accompanying symptoms:\n\nThere may also be accompanying secondary symptoms:\n\nHearing loss has multiple causes, including ageing, genetics, perinatal problems and acquired causes like noise and disease. For some kinds of hearing loss the cause may be classified as of unknown cause.\n\nThere is a progressive loss of ability to hear high frequencies with aging known as presbycusis. For men, this can start as early as 25 and women at 30. Although genetically variable it is a normal concomitant of ageing and is distinct from hearing losses caused by noise exposure, toxins or disease agents. Common conditions that can increase the risk of hearing loss in elderly people are high blood pressure, diabetes or the use of certain medications harmful to the ear. While everyone loses hearing with age, the amount and type of hearing loss is variable.\n\nNoise exposure is the cause of approximately half of all cases of hearing loss, causing some degree of problems in 5% of the population globally.\nThe National Institute for Occupational Safety and Health (NIOSH) recognizes that the majority of hearing loss is not due to age, but due to noise exposure. By correcting for age in assessing hearing, one tends to overestimate the hearing loss due to noise for some and underestimate it for others.\n\nHearing loss due to noise may be temporary, called a 'temporary threshold shift', a reduced sensitivity to sound over a wide frequency range resulting from exposure to a brief but very loud noise like a gunshot, firecracker, jet engine, jackhammer, etc. or to exposure to loud sound over a few hours such as during a pop concert or nightclub session. Recovery of hearing is usually within 24 hours, but may take up to a week. Both constant exposure to loud sounds (85 dB(A) or above) and one-time exposure to extremely loud sounds (120 dB(A) or above) may cause permanent hearing loss.\n\nNoise-induced hearing loss (NIHL) typically manifests as elevated hearing thresholds (i.e. less sensitivity or muting) between 3000 and 6000  Hz, centred at 4000  Hz. As noise damage progresses, damage spreads to affect lower and higher frequencies. On an audiogram, the resulting configuration has a distinctive notch, called a 'noise' notch. As ageing and other effects contribute to higher frequency loss (6–8 kHz on an audiogram), this notch may be obscured and entirely disappear.\n\nVarious governmental, industry and standards organizations set noise standards.\n\nThe U.S. Environmental Protection Agency has identified the level of 70 dB(A) (40% louder to twice as loud as normal conversation; typical level of TV, radio, stereo; city street noise) for 24‑hour exposure as the level necessary to protect the public from hearing loss and other disruptive effects from noise, such as sleep disturbance, stress-related problems, learning detriment, etc. Noise levels are typically in the 65 to 75 dB (A) range for those living near airports of freeways and may result in hearing damage if sufficient time is spent outdoors.\n\nLouder sounds cause damage in a shorter period of time. Estimation of a \"safe\" duration of exposure is possible using an \"exchange rate\" of 3 dB. As 3 dB represents a doubling of the intensity of sound, duration of exposure must be cut in half to maintain the same energy dose. For workplace noise regulation, the \"safe\" daily exposure amount at 85 dB A, known as an exposure action value, is 8 hours, while the \"safe\" exposure at 91 dB(A) is only 2 hours. \nDifferent standards use exposure action values between 80dBA and 90dBA. Note that for some people, sound may be damaging at even lower levels than 85 dB A. Exposures to other ototoxins (such as pesticides, some medications including chemotherapy agents, solvents, etc.) can lead to greater susceptibility to noise damage, as well as causing its own damage. This is called a \"synergistic\" interaction. Since noise damage is cumulative over long periods of time, persons who are exposed to non-workplace noise, like recreational activities or environmental noise, may have compounding damage from all sources.\n\nSome national and international organizations and agencies use an exchange rate of 4 dB or 5 dB. While these exchange rates may indicate a wider zone of comfort or safety, they can significantly underestimate the damage caused by loud noise. For example, at 100 dB (nightclub music level), a 3 dB exchange rate would limit exposure to 15 minutes; the 5 dB exchange rate allows an hour.\n\nMany people are unaware of the presence of environmental sound at damaging levels, or of the level at which sound becomes harmful. Common sources of damaging noise levels include car stereos, children's toys, motor vehicles, crowds, lawn and maintenance equipment, power tools, gun use, musical instruments, and even hair dryers. Noise damage is cumulative; all sources of damage must be considered to assess risk. If one is exposed to loud sound (including music) at high levels or for extended durations (85 dB A or greater), then hearing loss will occur. Sound intensity (sound energy, or propensity to cause damage to the ears) increases dramatically with proximity according to an inverse square law: halving the distance to the sound quadruples the sound intensity.\n\nIn the USA, 12.5% of children aged 6–19 years have permanent hearing damage from excessive noise exposure. The World Health Organization estimates that half of those between 12 and 35 are at risk from using personal audio devices that are too loud.\n\nHearing loss due to noise has been described as primarily a condition of modern society. In preindustrial times, humans had far less exposure to loud sounds. Studies of primitive peoples indicate that much of what has been attributed to age-related hearing loss may be long term cumulative damage from all sources, especially noise. People living in preindustrial societies have considerably less hearing loss than similar populations living in modern society. Among primitive people who have migrated into modern society, hearing loss is proportional to the number of years spent in modern society. Military service in World War II, the Korean War, and the Vietnam War, has likely also caused hearing loss in large numbers of men from those generations, though proving that hearing loss was a direct result of military service is problematic without entry and exit audiograms.\n\nHearing loss in adolescents may be caused by loud noise from toys, music by headphones, and concerts or events. In 2017, the Centers for Disease Control and Prevention brought their researchers together with experts from the World Health Organization and academia to examine the risk of hearing loss from excessive noise exposure in and outside the workplace in different age groups, as well as actions being taken to reduce the burden of the condition. A summary report was published in 2018.\n\nHearing loss can be inherited. Around 75–80% of all these cases are inherited by recessive genes, 20–25% are inherited by dominant genes, 1–2% are inherited by X-linked patterns, and fewer than 1% are inherited by mitochondrial inheritance.\n\nWhen looking at the genetics of deafness, there are 2 different forms, syndromic and nonsyndromic. Syndromic deafness occurs when there are other signs or medical problems aside from deafness in an individual. This accounts for around 30% of deaf individuals who are deaf from a genetic standpoint. Nonsyndromic deafness occurs when there are no other signs or medical problems associated with an individual other than deafness. From a genetic standpoint, this accounts for the other 70% of cases, and represents the majority of hereditary hearing loss. Syndromic cases occur with diseases such as Usher syndrome, Stickler syndrome, Waardenburg syndrome, Alport's syndrome, and neurofibromatosis type 2. These are diseases that have deafness as one of the symptoms or as a common feature associated with it. Many of the genetic mutations giving rise to syndromic deafness have been identified. In nonsyndromic cases, where deafness is the only finding, it is more difficult to identify the genetic mutation although some have been discovered.\n\n\n\n\nSome medications may reversibly affect hearing. These medications are considered ototoxic. This includes loop diuretics such as furosemide and bumetanide, non-steroidal anti-inflammatory drugs (NSAIDs) both over-the-counter (aspirin, ibuprofen, naproxen) as well as prescription (celecoxib, diclofenac, etc.), paracetamol, quinine, and macrolide antibiotics. The link between NSAIDs and hearing loss tends to be greater in women, especially those who take ibuprofen six or more times a week. Others may cause permanent hearing loss. The most important group is the aminoglycosides (main member gentamicin) and platinum based chemotherapeutics such as cisplatin and carboplatin.\n\nOn October 18, 2007, the U.S. Food and Drug Administration (FDA) announced that a warning about possible sudden hearing loss would be added to drug labels of PDE5 inhibitors, which are used for erectile dysfunction.\n\nAudiologic monitoring for ototoxicity allows for the (1) early detection of changes to hearing status presumably attributed to a drug/treatment regime so that changes in the drug regimen may be considered, and (2) audiologic intervention when handicapping hearing impairment has occurred.\n\nCo-administration of anti-oxidants and ototoxic medications may limit the extent of the ototoxic damage\n\nIn addition to medications, hearing loss can also result from specific chemicals in the environment: metals, such as lead; solvents, such as toluene (found in crude oil, gasoline and automobile exhaust, for example); and asphyxiants. Combined with noise, these ototoxic chemicals have an additive effect on a person’s hearing loss.\n\nHearing loss due to chemicals starts in the high frequency range and is irreversible. It damages the cochlea with lesions and degrades central portions of the auditory system. For some ototoxic chemical exposures, particularly styrene, the risk of hearing loss can be higher than being exposed to noise alone. The effects is greatest when the combined exposure include impulse noise. \nA 2018 informational bulletin by the US Occupational Safety and Health Administration (OSHA) and the National Institute for Occupational Safety and Health (NIOSH) introduces the issue, provides examples of ototoxic chemicals, lists the industries and occupations at risk and provides prevention information.\n\nThere can be damage either to the ear itself or to the brain centers that process the aural information conveyed by the ears. People who sustain head injury are especially vulnerable to hearing loss or tinnitus, either temporary or permanent.\n\nSound waves reach the outer ear and are conducted down the ear canal to the eardrum, causing it to vibrate. The vibrations are transferred by the 3 tiny ear bones of the middle ear to the fluid in the inner ear. The fluid moves hair cells (stereocilia), and their movement generates nerve impulses which are then taken to the brain by the cochlear nerve. The auditory nerve takes the impulses to the brainstem, which sends the impulses to the midbrain. Finally, the signal goes to the auditory cortex of the temporal lobe to be interpreted as sound.\n\nHearing loss is most commonly caused by long-term exposure to loud noises, from recreation or from work, that damage the hair cells, which do not grow back on their own.\n\nOlder people may lose their hearing from long exposure to noise, changes in the inner ear, changes in the middle ear, or from changes along the nerves from the ear to the brain.\n\nIdentification of a hearing loss is usually conducted by a general practitioner medical doctor, otolaryngologist, certified and licensed audiologist, school or industrial audiometrist, or other audiometric technician. Diagnosis of the cause of a hearing loss is carried out by a specialist physician (audiovestibular physician) or otorhinolaryngologist.\n\nA case history (usually a written form, with questionnaire) can provide valuable information about the context of the hearing loss, and indicate what kind of diagnostic procedures to employ. Case history will include such items as:\n\n\nIn case of infection or inflammation, blood or other body fluids may be submitted for laboratory analysis.\n\nHearing loss is generally measured by playing generated or recorded sounds, and determining whether the person can hear them. Hearing sensitivity varies according to the frequency of sounds. To take this into account, hearing sensitivity can be measured for a range of frequencies and plotted on an audiogram.\n\nAnother method for quantifying hearing loss is a speech-in-noise test. As the name implies, a speech-in-noise test gives an indication of how well one can understand speech in a noisy environment. A person with a hearing loss will often be less able to understand speech, especially in noisy conditions. This is especially true for people who have a sensorineural loss – which is by far the most common type of hearing loss. As such, speech-in-noise tests can provide valuable information about a person's hearing ability, and can be used to detect the presence of a sensorineural hearing loss. A recently developed digit-triple speech-in-noise test may be a more efficient screening test.\n\nOtoacoustic emissions test is an objective hearing test that may be administered to toddlers and children too young to cooperate in a conventional hearing test. The test is also useful in older children and adults and is an important measure in diagnosing auditory neuropathy described above.\n\nAuditory brainstem response testing is an electrophysiological test used to test for hearing deficits caused by pathology within the ear, the cochlear nerve and also within the brainstem. This test can be used to identify delay in the conduction of neural impulses due to tumours or inflammation but can also be an objective test of hearing thresholds. Other electrophysiological tests, such as cortical evoked responses, can look at the hearing pathway up to the level of the auditory cortex.\n\nMRI and CT scans can be useful to identify the pathology of many causes of hearing loss. They are only needed in selected cases.\n\nHearing loss is categorized by type, severity, and configuration. Furthermore, a hearing loss may exist in only one ear (unilateral) or in both ears (bilateral). Hearing loss can be temporary or permanent, sudden or progressive.\n\nThe severity of a hearing loss is ranked according to ranges of nominal thresholds in which a sound must be so it can be detected by an individual. It is measured in decibels of hearing loss, or dB HL. The measurement of hearing loss in an individual is conducted over several frequencies, mostly 500 Hz, 1000 Hz, 2000 Hz and 4000 Hz. The hearing loss of the individual is the average of the hearing loss values over the different frequencies. Hearing loss can be ranked differently according to different organisations; and so, in different countries different systems are in use.\n\nHearing loss may be ranked as slight, mild, moderate, moderately severe, severe or profound as defined below:\n\nThe 'Audiometric Classifications of Hearing Impairment' according to the International Bureau Audiophonology (BIAP) in Belgium is as follows:\n\nHearing loss may affect one or both ears. If both ears are affected, then one ear may be more affected than the other. Thus it is possible, for example, to have normal hearing in one ear and none at all in the other, or to have mild hearing loss in one ear and moderate hearing loss in the other.\n\nFor certain legal purposes such as insurance claims, hearing loss is described in terms of percentages. Given that hearing loss can vary by frequency and that audiograms are plotted with a logarithmic scale, the idea of a percentage of hearing loss is somewhat arbitrary, but where decibels of loss are converted via a legally recognized formula, it is possible to calculate a standardized \"percentage of hearing loss\", which is suitable for legal purposes only.\n\nThere are four main types of hearing loss, conductive hearing loss, sensorineural hearing loss, central deafness and combinations of conductive and sensorineural hearing losses which is called mixed hearing loss. An additional problem which is increasingly recognised is auditory processing disorder which is not a hearing loss as such but a difficulty perceiving sound.\n\nConductive hearing loss is present when the sound is not reaching the inner ear, the cochlea. This can be due to external ear canal malformation, dysfunction of the eardrum or malfunction of the bones of the middle ear. The eardrum may show defects from small to total resulting in hearing loss of different degree. Scar tissue after ear infections may also make the eardrum dysfunction as well as when it is retracted and adherent to the medial part of the middle ear.\n\nDysfunction of the three small bones of the middle ear – malleus, incus, and stapes – may cause conductive hearing loss. The mobility of the ossicles may be impaired for different reasons including a boney disorder of the ossicles called otosclerosis and disruption of the ossicular chain due to trauma, infection or ankylosis may also cause hearing loss.\n\nSensorineural hearing loss is one caused by dysfunction of the inner ear, the cochlea or the nerve that transmits the impulses from the cochlea to the hearing centre in the brain. The most common reason for sensorineural hearing loss is damage to the hair cells in the cochlea. Depending on the definition it could be estimated that more than 50% of the population over the age of 70 has impaired hearing.\n\nDamage to the brain can lead to a central deafness. The peripheral ear and the auditory nerve may function well but the central connections are damaged by tumour, trauma or other disease and the patient is unable to process speech information.\n\nMixed hearing loss is a combination of conductive and sensorineural hearing loss. Chronic ear infection (a fairly common diagnosis) can cause a defective ear drum or middle-ear ossicle damages, or both. In addition to the conductive loss, a sensory component may be present.\n\n\nThis is not an actual hearing loss but gives rise to significant difficulties in hearing. One kind of auditory processing disorder is King-Kopetzky syndrome, which is characterized by an inability to process out background noise in noisy environments despite normal performance on traditional hearing tests. An auditory processing disorders is sometimes linked to language disorders in persons of all ages.\n\nThe shape of an audiogram shows the relative configuration of the hearing loss, such as a Carhart notch for otosclerosis, 'noise' notch for noise-induced damage, high frequency rolloff for presbycusis, or a flat audiogram for conductive hearing loss. In conjunction with speech audiometry, it may indicate central auditory processing disorder, or the presence of a schwannoma or other tumor.\nThere are four general configurations of hearing loss:\n\n1. Flat: thresholds essentially equal across test frequencies.\n\n2. Sloping: lower (better) thresholds in low-frequency regions and higher (poorer) thresholds in high-frequency regions.\n\n3. Rising: higher (poorer) thresholds in low-frequency regions and lower (better) thresholds in higher-frequency regions.\n\n4. Trough-shaped (\"cookie-bite\" or \"U\" shaped): greatest hearing loss in the mid-frequency range, with lower (better) thresholds in low- and high-frequency regions.\n\nPeople with unilateral hearing loss or single-sided deafness (SSD) have difficulty in:\n\n\nIn quiet conditions, speech discrimination is approximately the same for normal hearing and those with unilateral deafness; however, in noisy environments speech discrimination varies individually and ranges from mild to severe.\n\nOne reason for the hearing problems these patients often experience is due to the head shadow effect. Newborn children with no hearing on one side but one normal ear could still have problems. Speech development could be delayed and difficulties to concentrate in school are common. More children with unilateral hearing loss have to repeat classes than their peers. Taking part in social activities could be a problem. Early aiding is therefore of utmost importance.\n\nIt is estimated that half of cases of hearing loss are preventable. About 60% of hearing loss in children under the age of 15 can be avoided. A number of preventative strategies are effective including: immunization against rubella to prevent congenital rubella syndrome, immunization against \"H. influenza\" and \"S. pneumoniae\" to reduce cases of meningitis, and avoiding or protecting against excessive noise exposure. The World Health Organization also recommends immunization against measles, mumps, and meningitis, efforts to prevent premature birth, and avoidance of certain medication as prevention.\n\nNoise exposure is the most significant risk factor for noise-induced hearing loss that can be prevented. Different programs exist for specific populations such as school-age children, adolescents and workers. Education regarding noise exposure increases the use of hearing protectors. The use of antioxidants is being studied for the prevention of noise-induced hearing loss, particularly for scenarios in which noise exposure cannot be reduced, such as during military operations.\n\nNoise is widely recognized as an occupational hazard. In the United States, the National Institute for Occupational Safety and Health (NIOSH) and the Occupational Safety and Health Administration (OSHA) work together to provide standards and enforcement on workplace noise levels. The hierarchy of hazard controls demonstrates the different levels of controls to reduce or eliminate exposure to noise and prevent hearing loss, including engineering controls and personal protective equipment (PPE). Other programs and initiative have been created to prevent hearing loss in the workplace. For example, the Safe-in-Sound Award was created to recognize organizations that can demonstrate results of successful noise control and other interventions. Additionally, the Buy Quiet program was created to encourage employers to purchase quieter machinery and tools. By purchasing less noisy power tools like those found on the NIOSH Power Tools Database and limiting exposure to ototoxic chemicals, great strides can be made in preventing hearing loss.\n\nCompanies can also provide personal hearing protector devices tailored to both the worker and type of employment. Some hearing protectors universally block out all noise, and some allow for certain noises to be heard. Workers are more likely to wear hearing protector devices when they are properly fitted.\n\nOften interventions to prevent noise-induced hearing loss have many components. A 2017 Cochrane review found that stricter legislation might reduce noise levels. Providing workers with information on their noise exposure levels was not shown to decrease exposure to noise. Ear protection, if used correctly, can reduce noise to safer levels, but often, providing them is not sufficient to prevent hearing loss. Engineering noise out and other solutions such as proper maintenance of equipment can lead to noise reduction, but further field studies on resulting noise exposures following such interventions are needed. Other possible solutions include improved enforcement of existing legislation and better implementation of well-designed prevention programmes, which have not yet been proven conclusively to be effective. The conclusion of the Cochrane Review was that further research could modify what is now regarding the effectiveness of the evaluated interventions.\n\nThe United States Preventive Services Task Force recommends screening for all newborns.\n\nThe American Academy of Pediatrics advises that children should have their hearing tested several times throughout their schooling:\nWhile the American College of Physicians indicated that there is not enough evidence to determine the utility of screening in adults over 50 years old who do not have any symptoms, the American Language, Speech Pathology and Hearing Association recommends that adults should be screened at least every decade through age 50 and at 3-year intervals thereafter, to minimize the detrimental effects of the untreated condition on quality of life. For the same reason, the US Office of Disease Prevention and Health Promotion included as one of Healthy People 2020 objectives: to increase the proportion of persons who have had a hearing examination.\n\nTreatment depends on the specific cause if known as well as the extent, type and configuration of the hearing loss. Most hearing loss, that resulting from age and noise, is progressive and irreversible, and there are currently no approved or recommended treatments; management is by hearing aid. A few specific kinds of hearing loss are amenable to surgical treatment. In other cases, treatment is addressed to underlying pathologies, but any hearing loss incurred may be permanent.\n\nThere are a number of devices that can improve hearing in those who are deaf or hard of hearing or allow people with these conditions to manage better in their lives.\n\nHearing aids are devices that work to improve the hearing and speech comprehension of those with hearing loss. They work by magnifying the sound vibrations in the ear so that one can understand what is being said around them. Hearing aids have been shown to have a large beneficial effect in helping adults with mild to moderate hearing loss take part in everyday situations, and a smaller beneficial effect in improving physical, social, emotional and mental well-being in these people. Some people feel as if they cannot live without one because they say it is the only thing that keeps them engaged with the public. Conversely, there are many people who choose not to wear their hearing aids for a multitude of reasons. Up to 40% of adults with hearing aids for hearing loss fail to use them, or do not use them to their full effect. There are a number of reasons for this, stemming from factors such as: the aid amplifying background noises instead of the sounds they intended to hear; issues with comfort, care, or maintenance of the device; aesthetic factors; financial factors; and personal preference for quietness.\n\nThere is little evidence that interventions to encourage the regular use of hearing aids, (e.g. improving the information given to people about how to use hearing aids), increase daily hours of hearing aid use, and there is currently no agreed set of outcome measures for assessing this type of intervention.\n\nMany deaf and hard of hearing individuals use assistive devices in their daily lives:\n\nA wireless device has two main components: a transmitter and a receiver. The transmitter broadcasts the captured sound, and the receiver detects the broadcast audio and enables the incoming audio stream to be connected to accommodations such as hearing aids or captioning systems.\n\nThree types of wireless systems are commonly used: FM, audio induction loop, and InfraRed. Each system has advantages and benefits for particular uses. FM systems can be battery operated or plugged into an electrical outlet. FM system produce an analog audio signal, meaning they have extremely high fidelity. Many FM systems are very small in size, allowing them to be used in mobile situations. The audio induction loop permits the listener with hearing loss to be free of wearing a receiver provided that the listener has a hearing aid or cochlear implant processor with an accessory called a \"telecoil\". If the listener does not have a telecoil, then he or she must carry a receiver with an earpiece. As with FM systems, the infrared (IR) system also requires a receiver to be worn or carried by the listener. An advantage of IR wireless systems is that people in adjoining rooms cannot listen in on conversations, making it useful for situations where privacy and confidentiality are required. Another way to achieve confidentiality is to use a hardwired amplifier, which contains or is connected to a microphone and transmits no signal beyond the earpiece plugged directly into it.\n\nThere is no treatment, surgical or otherwise, for hearing loss due to the most common causes (age, noise, and genetic defects). For a few specific conditions, surgical intervention can provide a remedy:\n\nSurgical and implantable hearing aids are an alternative to conventional external hearing aids. \nIf the ear is dry and not infected, an air conduction aid could be tried; if the ear is draining, a direct bone condition hearing aid is often the best solution. If the conductive part of the hearing loss is more than 30–35 dB, an air conduction device could have problems overcoming this gap. A bone-anchored hearing aid could, in this situation, be a good option.\nThe active bone conduction hearing implant Bonebridge is also an option. This implant is invisible under the intact skin and therefore minimises the risk of skin irritations.\n\nCochlear implants improve outcomes in people with hearing loss in either one or both ears. They work by artificial stimulation of the cochlear nerve by providing an electric impulse substitution for the firing of hair cells. They are expensive, and require programming along with extensive training for effectiveness.\n\nCochlear implants as well as bone conduction implants can help with single sided deafness.\nMiddle ear implants or bone conduction implants can help with conductive hearing loss.\n\nPeople with cochlear implants are at a higher risk for bacterial meningitis. Thus, meningitis vaccination is recommended. People who have hearing loss, especially those who develop a hearing problem in childhood or old age, may need support and technical adaptations as part of the rehabilitation process. Recent research shows variations in efficacy but some studies show that if implanted at a very young age, some profoundly impaired children can acquire effective hearing and speech, particularly if supported by appropriate rehabilitation.\n\nFor a classroom setting, children with hearing loss often benefit from direct instruction and communication. One option for students is to attend a school for the Deaf, where they will have access to the language, communication, and education. Another option is to have the child attend a mainstream program, with special accommodation such as providing favorable seating for the child. Having the student sit as close to the teacher as possible improves the student's ability to hear the teacher's voice and to more easily read the teacher's lips. When lecturing, teachers can help the student by facing them and by limiting unnecessary noise in the classroom. In particular, the teacher can avoid talking when their back is turned to the classroom, such as while writing on a whiteboard.\n\nSome other approaches for classroom accommodations include pairing deaf or hard of hearing students with hearing students. This allows the deaf or hard of hearing student to ask the hearing student questions about concepts that they have not understood. The use of CART (Communication Access Real Time) systems, where an individual types a captioning of what the teacher is saying, is also beneficial. The student views this captioning on their computer. Automated captioning systems are also becoming a popular option. In an automated system, software, instead of a person, is used to generate the captioning. Unlike CART systems, automated systems generally do not require an Internet connection and thus they can be used anywhere and anytime. Another advantage of automated systems over CART is that they are much lower in cost. However, automated systems are generally designed to only transcribe what the teacher is saying and to not transcribe what other students say. An automated system works best for situations where just the teacher is speaking, whereas a CART system will be preferred for situations where there is a lot of classroom discussion.\n\nFor those students who are completely deaf, one of the most common interventions is having the child communicate with others through an interpreter using sign language.\n\nGlobally, hearing loss affects about 10% of the population to some degree. It caused moderate to severe disability in 124.2 million people as of 2004 (107.9 million of whom are in low and middle income countries). Of these 65 million acquired the condition during childhood. At birth ~3 per 1000 in developed countries and more than 6 per 1000 in developing countries have hearing problems.\n\nHearing loss increases with age. In those between 20 and 35 rates of hearing loss are 3% while in those 44 to 55 it is 11% and in those 65 to 85 it is 43%.\n\nA 2017 report by the World Health Organization estimated the costs of unaddressed hearing loss and the cost-effectiveness of interventions, for the health-care sector, for the education sector and as broad societal costs. Globally, the annual cost of unaddressed hearing loss was estimated to be in the range of $750–790 billion international dollars.\n\nData from the United States in 2011-2012 found that rates of hearing loss has declined among adults aged 20 to 69 years, when compared with the results from an earlier time period (1999-2004). It also found that adult hearing loss is associated with increasing age, sex, race/ethnicity, educational level, and noise exposure.\n\nNearly one in four adults had audiometric results suggesting noise-induced hearing loss. Almost one in four adults who reported excellent or good hearing had a similar pattern (5.5% on both sides and 18% on one side). Among people who reported exposure to loud noise at work, almost one third had such changes.\n\nAbbé Charles-Michel de l'Épée opened the first school for the deaf in Paris at the deaf school. The American Thomas Gallaudet witnessed a demonstration of deaf teaching skills from Épée's successor Abbé Sicard and two of the school's deaf faculty members, Laurent Clerc and Jean Massieu; accompanied by Clerc, he returned to the United States, where in 1817 they founded American School for the Deaf in Hartford, Connecticut. American Sign Language (ASL) started to evolve from primarily French Sign Language (LSF), and other outside influences.\n\n\"Post-lingual deafness\" is hearing loss that is sustained after the acquisition of language, which can occur due to disease, trauma, or as a side-effect of a medicine. Typically, hearing loss is gradual and often detected by family and friends of affected individuals long before the patients themselves will acknowledge the disability. Post-lingual deafness is far more common than pre-lingual deafness. Those who lose their hearing later in life, such as in late adolescence or adulthood, face their own challenges, living with the adaptations that allow them to live independently.\n\n\"Prelingual deafness\" is hearing loss that is sustained before the acquisition of language, which can occur due to a congenital condition or through hearing loss in early infancy. It is believed that prelingual deafness impairs an individual's ability to acquire a \"spoken\" language, but some deaf children can acquire spoken language through speech immersion along with support from sign language and hearing aids or cochlear implants. Non-signing parents of deaf babies usually go with oral approach without the support of sign language because prelingual hearing loss is acquired via either disease or trauma rather than genetically inherited, so families with deaf children nearly always lack previous experience with sign language. Unfortunately, this brings on the risk of language deprivation for the deaf baby because the deaf baby wouldn't have a language if the child is unable to acquire spoken language successfully. Deaf babies born into signing families rarely have delays in language development since they do meet language milestones, but in sign language in lieu of spoken language.\n\nThere has been considerable controversy within the culturally deaf community over cochlear implants. For the most part, there is little objection to those who lost their hearing later in life, or culturally deaf adults choosing to be fitted with a cochlear implant.\n\nMany in the deaf community strongly object to a deaf child being fitted with a cochlear implant (often on the advice of an audiologist); new parents may not have sufficient information on raising deaf children and placed in an oral-only program that emphasizes the ability to speak and listen over other forms of communication such as sign language or total communication. Many Deaf people view cochlear implants and other hearing devices as confusing to one's identity. A Deaf person will never be a hearing person and therefore would be trying to fit into a way of living that is not their own. Other concerns include loss of Deaf culture and identity and limitations on hearing restoration.\n\nJack Gannon, a professor at Gallaudet University, said this about Deaf culture: \"Deaf culture is a set of learned behaviors and perceptions that shape the values and norms of deaf people based on their shared or common experiences.\" Some doctors believe that being deaf makes a person more social. Bill Vicar, from ASL University, shared his experiences as a deaf person, \"[deaf people] tend to congregate around the kitchen table rather than the living room sofa… our good-byes take nearly forever, and our hellos often consist of serious hugs. When two of us meet for the first time we tend to exchange detailed biographies.\" Deaf culture is not about contemplating what deaf people cannot do and how to fix their problems, an approach known as the \"pathological view of the deaf.\" Instead deaf people celebrate what they can do. There is a strong sense of unity between deaf people as they share their experiences of suffering through a similar struggle. This celebration creates a unity between even deaf strangers. Bill Vicars expresses the power of this bond when stating, \"if given the chance to become hearing most [deaf people] would choose to remain deaf.\"\n\nThe United States-based National Association of the Deaf has a statement on its website regarding cochlear implants. The NAD asserts that the choice to implant is up to the individual (or the parents), yet strongly advocates a fully informed decision in all aspects of a cochlear implant. Much of the negative reaction to cochlear implants stems from the medical viewpoint that deafness is a condition that needs to be \"cured,\" while the Deaf community instead regards deafness a defining cultural characteristic.\n\nMany other assistive devices are more acceptable to the Deaf community, including but not limited to, hearing aids, closed captioning, email and the Internet, text telephones, and video relay services.\n\nSign languages convey meaning through manual communication and body language instead of acoustically conveyed sound patterns. This involves the simultaneous combination of hand shapes, orientation and movement of the hands, arms or body, and facial expressions to express a speaker's thoughts. \"Sign languages are based on the idea that vision is the most useful tool a deaf person has to communicate and receive information\".\n\nThose who are deaf (by either state or federal standards) have access to a free and appropriate public education. If a child does qualify as being deaf or hard of hearing and receives an individualized education plan, the IEP team must consider, \"the child's language and communication needs. The IEP must include opportunities for direct communication with peers and professionals. It must also include the student’s academic level, and finally must include the students full range of needs\"\n\nIn part, the Department of Education defines deafness as \"… a hearing impairment that is so severe that the child is impaired in processing linguistic information through hearing, with or without amplification ….\" Hearing impairment is defined as \"… an impairment in hearing, whether permanent or fluctuating, that adversely affects a child's educational performance but that is not included under the definition of deafness ….\"\n\nIn a residential school where all the children use the same communication system (whether it is a school using ASL, Total Communication or Oralism), students will be able to interact normally with other students, without having to worry about being criticized. An argument supporting inclusion, on the other hand, exposes the student to people who are not just like them, preparing them for adult life. Through interacting, children with hearing disabilities can expose themselves to other cultures which in the future may be beneficial for them when it comes to finding jobs and living on their own in a society where their disability may put them in the minority. These are some reasons why a person may or may not want to put their child in an inclusion classroom.\n\nThe communication limitations between people who are deaf and their hearing family members can often cause difficulties in family relationships, and affect the strength of relationships among individual family members. It was found that most people who are deaf have hearing parents, which means that the channel that the child and parents communicate through can be very different, often affecting their relationship in a negative way. If a parent communicates best verbally, and their child communicates best using sign language, this could result in ineffective communication between parents and children. Ineffective communication can potentially lead to fights caused by misunderstanding, less willingness to talk about life events and issues, and an overall weaker relationship. Even if individuals in the family made an effort to learn deaf communication techniques such as sign language, a deaf family member often will feel excluded from casual banter; such as the exchange of daily events and news at the dinner table. It is often difficult for people who are deaf to follow these conversations due to the fast-paced and overlapping nature of these exchanges. This can cause a deaf individual to become frustrated and take part in less family conversations. This can potentially result in weaker relationships between the hearing individual and their immediate family members. This communication barrier can have a particularly negative effect on relationships with extended family members as well. Communication between a deaf individual and their extended family members can be very difficult due to the gap in verbal and non-verbal communication. This can cause the individuals to feel frustrated and unwilling to put effort into communicating effectively. The lack of effort put into communicating can result in anger, miscommunication, and unwillingness to build a strong relationship.\n\nPeople who have hearing loss can often experience many difficulties as a result of communication barriers among them and other hearing individuals in the community. Some major areas that can be impacted by this are involvement in extracurricular activities and social relationships. For young people, extracurricular activities are vehicles for physical, emotional, social, and intellectual development. However, it is often the case that communication barriers between people who are deaf and their hearing peers and coaches/club advisors limit them from getting involved. These communication barriers make it difficult for someone with a hearing loss to understand directions, take advice, collaborate, and form bonding relationships with other team or club members. As a result, extracurricular activities such as sports teams, clubs, and volunteering are often not as enjoyable and beneficial for individuals who have hearing loss, and they may engage in them less often. A lack of community involvement through extracurricular activities may also limit the individual’s social network. In general, it can be difficult for someone who is deaf to develop and maintain friendships with their hearing peers due to the communication gap that they experience. They can often miss the jokes, informal banter, and \"messing around\" that is associated with the formation of many friendships among young people. Conversations between people who are deaf and their hearing peers can often be limited and short due to their differences in communication methods and lack of knowledge on how to overcome these differences. Deaf individuals can often experience rejection by hearing peers who are not willing to make an effort to find their way around communication difficulties. Patience and motivation to overcome such communication barriers is required by both the deaf or hard of hearing and hearing individuals in order to establish and maintain good friendships.\n\nMany people tend to forget about the difficulties that deaf children encounter, as they view the deaf child differently from a deaf adult. Deaf children grow up being unable to fully communicate with their parents, siblings and other family members. Examples include being unable to tell their family what they have learned, what they did, asking for help, or even simply being unable to interact in daily conversation. Deaf children have to learn sign language and to read lips at a young age, however they cannot communicate with others using it unless the others are educated in sign language as well. Children who are deaf or hard of hearing are faced with many complications while growing up, for example some children have to wear hearing aids and others require assistance from sign language (ASL) interpreters. The interpreters help them to communicate with other individuals until they develop the skills they need to efficiently communicate on their own. Although growing up for deaf children may entitle more difficulties than for other children, there are many support groups that allow deaf children to interact with other children. This is where they develop friendships. There are also classes for young children to learn sign language in an environment that has other children in their same situation and around their same age. These groups and classes can be very beneficial in providing the child with the proper knowledge and not to mention the societal interactions that they need in order to live a healthy, young, playful and carefree life that any child deserves.\n\nThere are three typical adjustment patterns adopted by adults with hearing loss. The first one is to remain withdrawn into your own self. This provides a sense of safety and familiarity which can be a comforting way to lead your life. The second is to act \"as if\" one does not even have hearing loss. A positive attitude will help people to live a life with no barriers and thus, engage in optimal interaction. The final and third pattern is for the person to accept their hearing loss as a part of them without undervaluing oneself. This means understanding that one is forced to live life with this disability, however it is not the only thing that constitutes life’s meaning. Furthermore, many feel as if their inability to hear others during conversation is their fault. It's important that these individuals learn how to become more assertive individuals who do not lack fear when it comes to asking someone to repeat something or to speak a little louder. Although there is much fatigue and frustration that is produced from one’s inability to hear, it is important to learn from personal experiences in order to improve on one’s communication skills. In essence, these patterns will help adults with hearing loss deal with the communication barriers that are present.\n\nIn most instances, people who are deaf find themselves working with hearing colleagues, where they can often be cut off from the communication going on around them. Interpreters can be provided for meetings and workshops, however are seldom provided for everyday work interactions. Communication of important information needed for jobs typically comes in the form of written or verbal summaries, which do not convey subtle meanings such as tone of voice, side conversations during group discussions, and body language. This can result in confusion and misunderstanding for the worker who is deaf, therefore making it harder to do their job effectively. Additionally, deaf workers can be unintentionally left out of professional networks, informal gatherings, and casual conversations among their collogues. Information about informal rules and organizational culture in the workplace is often communicated though these types of interactions, which puts the worker who is deaf at a professional and personal disadvantage. This could sever their job performance due to lack of access to information and therefore, reduce their opportunity to form relationships with their co-workers. Additionally, these communication barriers can all affect a deaf person’s career development. Since being able to effectively communicate with one's co-workers and other people relevant to one's job is essential to managerial positions, people with hearing loss can often be denied such opportunities.\n\nTo avoid these situations in the workplace, individuals can take full-time or part-time sign language courses. In this way, they can become better able to communicate with the deaf and hard of hearing. Such courses teach the American Sign Language (ASL) language as most North Americans use this particular language to communicate. It is a visual language made up of specific gestures (signs), hand shapes, and facial expressions that contain their own unique grammatical rules and sentence structures By completing sign language courses, it ensures that deaf individuals feel a part of the workplace and have the ability to communicate with their co-workers and employer in the manner as other hearing employees do.\n\nNot only can communication barriers between deaf and hearing people affect family relationships, work, and school, but they can also have a very significant effect on a deaf individual’s physical and mental health care. As a result of poor communication between the health care professional and the deaf or hard of hearing patient, many patients report that they are not properly informed about their disease and prognosis. \n\nThis lack of or poor communication could also lead to other issues such as misdiagnosis, poor assessments, mistreatment, and even possibly harm to patients. Poor communication in this setting is often the result of health care providers having the misconception that all people who are deaf or hard of hearing have the same type of hearing loss, and require the same type of communication methods. In reality, there are many different types and range of hearing loss, and in order to communicate effectively a health care provider needs to understand that each individual with hearing loss has unique needs. This affects how individuals have been educated to communicate, as some communication methods work better depending on an individual’s severity of hearing loss. For example, assuming every deaf or hard of hearing patient knows American Sign Language would be incorrect because there are different types of sign language, each varying in signs and meanings. A patient could have been educated to use cued speech which is entirely different from ASL. Therefore, in order to communicate effectively, a health care provider needs to understand that each individual has unique needs when communicating.\n\nAlthough there are specific laws and rules to govern communication between health care professionals and people who are deaf, they are not always followed due to the health care professional’s insufficient knowledge of communication techniques. This lack of knowledge can lead them to make assumptions about communicating with someone who is deaf, which can in turn cause them to use an unsuitable form of communication. \nActs in countries such as the Americans with Disabilities Act (ADA) state that all health care providers are required to provide reasonable communication accommodations when caring for patients who are deaf. These accommodations could include qualified sign language interpreters, CDIs, and technology such as Internet interpretation services. A qualified sign language interpreter will enhance communication between a deaf individual and a health care professional by interpreting not only a health professional’s verbal communication, but also their non-verbal such as expressions, perceptions, and body language. A Certified Deaf Interpreter (CDI) is a sign language interpreter who is also a member of the Deaf community. They accompany a sign language interpreter and are useful for communication with deaf individuals who also have language or cognitive deficits. A CDI will transform what the health care professional communicates into basic, simple language. This method takes much longer, however it can also be more effective than other techniques. Internet interpretation services are convenient and less costly, but can potentially pose significant risks. They involve the use of a sign language interpreter over a video device rather than directly in the room. This can often be an inaccurate form of communication because the interpreter may not be licensed, is often unfamiliar with the patient and their signs, and can lack knowledge of medical terminology.\n\nAside from utilizing interpreters, healthcare professionals can improve their communication with deaf or hard of hearing patients by educating themselves on common misconceptions and proper practices depending on the patient’s needs. For example, a common misconception is that exaggerating words and speaking loudly will help the patient understand more clearly. However, many individuals with hearing loss depend on lip-reading to identify words. Exaggerated pronunciation and a raised voice can distort the lips, making it even more difficult to understand. Another common mistake health care professionals make are the use of single words rather than full sentences. Although language should be kept simple and short, keeping context is important because certain homophonous words are difficult to distinguish by lip-reading. Health care professionals can further improve their own communication with their patients by eliminating any background noise and positioning themselves in a way where their face is clearly visible to the patient, and suitably lit. The healthcare professional should know how to use body language and facial expressions to properly communicate different feelings.\n\nA 2005 study achieved successful regrowth of cochlea cells in guinea pigs. However, the regrowth of cochlear hair cells does not imply the restoration of hearing sensitivity, as the sensory cells may or may not make connections with neurons that carry the signals from hair cells to the brain. A 2008 study has shown that gene therapy targeting Atoh1 can cause hair cell growth and attract neuronal processes in embryonic mice. Some hope that a similar treatment will one day ameliorate hearing loss in humans.\n\nRecent research, reported in 2012 achieved growth of cochlear nerve cells resulting in hearing improvements in gerbils, using stem cells. Also reported in 2013 was regrowth of hair cells in deaf adult mice using a drug intervention resulting in hearing improvement. The Hearing Health Foundation in the US has embarked on a project called the Hearing Restoration Project. Also Action on Hearing Loss in the UK is also aiming to restore hearing.\n\nResearchers reported in 2015 that genetically deaf mice which were treated with TMC1 gene therapy recovered some of their hearing. In 2017, additional studies were performed to treat Usher syndrome and here, a recombinant adeno-associated virus seemed to outperform the older vectors.\n\nBesides research studies seeking to improve hearing, such as the ones listed above, research studies on the deaf have also been carried out in order to understand more about audition. Pijil and Shwarz (2005) conducted their study on the deaf who lost their hearing later in life and, hence, used cochlear implants to hear. They discovered further evidence for rate coding of pitch, a system that codes for information for frequencies by the rate that neurons fire in the auditory system, especially for lower frequencies as they are coded by the frequencies that neurons fire from the basilar membrane in a synchronous manner. Their results showed that the subjects could identify different pitches that were proportional to the frequency stimulated by a single electrode. The lower frequencies were detected when the basilar membrane was stimulated, providing even further evidence for rate coding.\n\n"}
{"id": "23366307", "url": "https://en.wikipedia.org/wiki?curid=23366307", "title": "Hematological Cancer Research Investment and Education Act", "text": "Hematological Cancer Research Investment and Education Act\n\nThe Hematological Cancer Research Investment and Education Act of 2001 (P.L. 107-172) amends the Public Health Service Act to allocate funding and establish directed research and education programs targeted at forms of blood cancer, in particular leukemia, lymphoma, and multiple myeloma.\n\nThe bill was introduced as S. 1094 by Senator Kay Bailey Hutchison of Texas in June 2001. Hutchinson's brother has had multiple myeloma and she worked for several years to pass legislation of this type. The bill came out through the Senate Committee on Health, Education, Labor, and Pensions, and passed the United States Senate by unanimous consent in November 2001. It went through the House Committee on Energy and Commerce and passed the U.S. House of Representatives on a voice vote in April 2002. It was signed into law by President George W. Bush on May 14, 2002.\n\nThe research part of the bill, the Joe Moakley Research Excellence Program, requires the Director of the National Institutes of Health, through the National Cancer Institute, to expand and coordinate blood cancer research programs. It was named after former Massachusetts Congressman Joe Moakley, who died in May 2001 of myelodysplastic syndrome, a form of leukemia. The education part of the bill, the Geraldine Ferraro Cancer Education Program, requires the Secretary of Health and Human Services to establish an education program for patients of such blood cancers and the general public. It is named after former New York Congresswoman and 1984 Democratic vice-presidential nominee Geraldine Ferraro, who has been battling multiple myeloma since 1998. Ferraro did not publicly disclose her disease until June 2001, when she appeared in Congressional hearings to advocate for passage of the Act.\n\nAn example of the Moakley program funding is $12.75 million to The University of Texas M. D. Anderson Cancer Center. The Ferraro program was not funded in 2003, but received $5 million funding in 2004.\n"}
{"id": "37946285", "url": "https://en.wikipedia.org/wiki?curid=37946285", "title": "Hogewey", "text": "Hogewey\n\n', operated by nursing home ', is a gated model village in Weesp, Netherlands. It has been designed specifically as a pioneering care facility for elderly people with dementia. The benefit of using all-day reminiscence therapy at Hogewey, compared to traditional nursing homes, is that the residents with dementia are more active and require less medication. The setting has been compared to that depicted in the film \"The Truman Show\". Carers, doctors and nurses work around the clock to provide the 152 residents the necessary 24-hour care.\n\nThe Hogewey complex is set out like a village with a town square, supermarket, hairdressing salon, theatre, pub, café-restaurant—as well as the twenty-three houses themselves. Each house reflects a style that is common to, and familiar for, the six or seven people who live in that house. The seven settings provided are\n\nThe doctors, nurses and carers aim to make the experience as real as possible to the residents. Residents shop at the supermarket and assist with preparing and cooking as they would at home. The carers wear normal daytime clothing rather than clinical clothing and fit into a role that the dementia sufferers are likely to be comfortable with; in the working class households the carers are seen to be neighbours or carers, while in the aristocratic/upper class setting, the nurses act akin to servants. The living styles have different types of music playing, significantly varied interior design, food and methods of table setting. Residents in each house have their own large bedroom and meet with other residents to share the living room, kitchen and dining room. There are no locks on the doors, and residents are free to walk or cycle around the village, including choosing to visit the supermarket or cafe.\n\nThe village employs 250 staff.\n\nTo maintain the \"fake reality\" (hyperreality) that those living at Hogewey are comfortable with, the staff are instructed not to correct the residents as they talk about memories, background, and history. At the same time, the staff will not deceive the patients if directly asked, truthfully stating that the residents are in a place where they can receive required care for their condition. Because of the nature of Alzheimer's disease and dementia, the residents can remember the distant past but not the present, so even truthful answers given by the staff will be forgotten quickly.\n\nThe village came about in 1992, when Yvonne van Amerongen and another member of staff at a traditional nursing home had one of their parents die, being glad that their elderly parents had died quickly and had not had to endure hospital-like care. After a series of research and brainstorming sessions in 1993, they decided that people generally prefer to surround and interact with other like-minded people of similar backgrounds and experiences. The arrangement at Hogewey provides this by ensuring that residents with similar backgrounds continue to live closely together.\n\nThe Hogewey facility was designed by architects Molenaar&Bol&VanDillen, and opened in December 2009 on four acres of land. It consists of low two-story brick buildings, and amenities such as a supermarket and theatre. Construction of the new Hogewey facilities cost €19.3 million and was funded primarily by the Dutch government providing €17.8 million, plus €1.5 million in funding and sponsorship from local organisations.\n\nThe café–restaurant is to some extent open to the public and the theatre is hired by local businesses, both of which help with running costs. The cost per resident is similar to more traditional nursing homes at around €5,000 per month.\n\nAmerongen is Hogewey's director of innovation at Vivium—the government-owned company that runs Hogewey. Because of the amount of foreign interest and foreign visitors, Vivium recruited an information officer for Hogewey.\n\n"}
{"id": "14768059", "url": "https://en.wikipedia.org/wiki?curid=14768059", "title": "Indian states ranking by underweight people", "text": "Indian states ranking by underweight people\n\nThis is a list of the States of India ranked in order of percentage of people having a Body Mass Index lower than normal.\n\n"}
{"id": "29258529", "url": "https://en.wikipedia.org/wiki?curid=29258529", "title": "Institute for Quality and Efficiency in Health Care", "text": "Institute for Quality and Efficiency in Health Care\n\nThe Institute for Quality and Efficiency in Healthcare (IQWiG) () is a German agency responsible for assessing the quality and efficiency of medical treatments, including drugs, non-drug interventions (e.g. surgical procedures), diagnostic and screening methods, and treatment and disease management. IQWiG also supplies health information to patients and the general public.\n\nThe organization is independent of the pharmaceutical industry, contracted solely by the Federal Ministry of Health and the Joint Federal Committee.\n\nIQWiG was founded in 2004 under the directorship of Dr Peter Sawicki, who was in September 2010 replaced by Dr Jürgen Windeler. Its deputy director is Dr Stefan Lange.\n\nIQWiG is divided into the following departments, which publish reports:\n\n\nGeneral health information, written in plain language, is additionally produced by a Health Information department.\n\nThese departments are also supported by Administration and Communication departments.\n\nIQWiG hit international headlines in October 2010 with a report slamming Reboxetine as inefficient and harmful.\n\nSimilarly, in September 2010, another study rebuffed the use of Venlafaxine and Duloxetine as first-line treatment in major depression, but recommend them as a second line option.\n\nThe use of Memantine in Alzheimer's patients was deemed as insufficiently supported by scientific evidence. This led Merz to publish additional studies, and the Institute to change its mind.\n\nInsulin analogues were deemed as not superior to human insulin for the treatment of type I diabetes.\n\n\n"}
{"id": "1853112", "url": "https://en.wikipedia.org/wiki?curid=1853112", "title": "Jesse Pintado", "text": "Jesse Pintado\n\nJesus \"Jesse\" Ernesto Pintado Andrade (July 12, 1969 – August 27, 2006) was a lead guitar player born in Mexico who at an early age moved to the US. He started in the grindcore band Terrorizer where he recorded the album \"World Downfall\", the first album to feature Pete Sandoval who would later leave the band to join Morbid Angel. It was Jesse Pintado who coined the term \"grindcore\" for the first time (in 1983), to describe a musical mixture of \"noise and chaos\" which he was developing at that time.\n\nHe lived in Huntington Park, California (his home address was even on the booklet of the \"World Downfall\" CD for contacting), but moved to Birmingham, England after he joined Napalm Death, where he replaced guitarist Bill Steer immediately prior to the recording of their album \"Harmony Corruption\".\n\nIn 2004 he officially left Napalm Death; instead of replacing him, the band has since continued as a four-piece. Pintado later revived Terrorizer, recruiting Tony Norman of Monstrosity and Anthony Rezhawk of Resistant Culture; he and Pete Sandoval were the only original members.\n\nBesides Terrorizer and Napalm Death he also played in Lock Up and Brujeria (see discography below). Both bands also featured Napalm Death bass player Shane Embury.\n\nHis last residence was Ridderkerk in the Netherlands, and a few weeks after the release of Terrorizer's second album, he died in a hospital in the Netherlands due to liver failure after a diabetes-induced coma. His death also stemmed from excessive drinking.\n\n\nPintado was also credited for \"Order of the Leech\" and \"\", however, he did not actually appear on these records.\n\n\n\n\n\n\n\n\nPintado was credited as songwriting (assistant)\n\n\n\n\n\n\n"}
{"id": "40832168", "url": "https://en.wikipedia.org/wiki?curid=40832168", "title": "Jordan Valley Authority", "text": "Jordan Valley Authority\n\nThe Jordan Valley Authority (JVA) is a government agency in Jordan tasked with carrying out socioeconomic development of Jordan's side of the Jordan Valley. The agency was established in 1977. It was a successor to the Jordanian Valley Authority, the Jordan River Tributaries Regional Cooperation and several other government departments related to the Jordan Valley. The JVA became responsible for water supply to Jordan's urban areas and agriculture. The JVA however did also play a role in land distribution as in the year of its founding a law established that all developed agricultural land could only be sold to the JVA. \n\nThe JVA, under its president Omar Abdallah Dakhqan, was tasked in 1977 to seek a solution for North Jordan's water shortage. The affected area also included the capital, Amman. An earlier plan, to derive water from the King Talal Dam, was abandoned after there rose serious concerns for water quality safety. The JVA then proposed a plan for a dam on the Yarmouk River. The Maqarin dam as it was called was positioned however on the border between Syria and Jordan, and the river flow also effected Israel. Construction only began in 2004 and was finished in 2011. \n\n"}
{"id": "48743", "url": "https://en.wikipedia.org/wiki?curid=48743", "title": "List of phobias", "text": "List of phobias\n\nThe English suffixes -phobia, -phobic, -phobe (from Greek φόβος \"phobos\", \"fear\") occur in technical usage in psychiatry to construct words that describe irrational, abnormal, unwarranted, persistent, or disabling fear as a mental disorder (e.g. agoraphobia), in chemistry to describe chemical aversions (e.g. hydrophobic), in biology to describe organisms that dislike certain conditions (e.g. acidophobia), and in medicine to describe hypersensitivity to a stimulus, usually sensory (e.g. photophobia). In common usage, they also form words that describe dislike or hatred of a particular thing or subject (e.g. homophobia). The suffix is antonymic to -phil-.\n\nFor more information on the psychiatric side, including how psychiatry groups phobias such as agoraphobia, social phobia, or simple phobia, see phobia. The following lists include words ending in \"-phobia\", and include fears that have acquired names. In some cases, the naming of phobias has become a word game, of notable example being a 1998 humorous article published by \"BBC News\". In some cases, a word ending in \"-phobia\" may have an antonym with the suffix \"-phil-\", e.g. Germanophobe/Germanophile.\n\nA large number of \"-phobia\" lists circulate on the Internet, with words collected from indiscriminate sources, often copying each other. Also, a number of psychiatric websites exist that at the first glance cover a huge number of phobias, but in fact use a standard text to fit any phobia and reuse it for all unusual phobias by merely changing the name. Sometimes it leads to bizarre results, such as suggestions to cure \"prostitute phobia\". Such practice is known as content spamming and is used to attract search engines.\n\nAn article published in 1897 in \"American Journal of Psychology\" noted \"the absurd tendency to give Greek names to objects feared (which, as Arndt says, would give us such terms as klopsophobia – fear of thieves, triakaidekaphobia – fear of the number 13...\".\n\nSpecialists may prefer to avoid the suffix \"-phobia\" and use more descriptive terms such as personality disorders, anxiety disorders, and avoidant personality disorder. Terms should strictly have a Greek prefix although many are irregularly formed with Latin or even English prefixes. Many use inaccurate or imprecise prefixes, such as aerophobia (fear of air) for fear of flying.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiologists use a number of \"-phobia/-phobic\" terms to describe predispositions by plants and animals against certain conditions. For antonyms, see here\n\n\nThe suffix \"-phobia\" is used to coin terms that denote a particular anti-ethnic or anti-demographic sentiment, such as Americanophobia, Europhobia, Francophobia, Hispanophobia, and Indophobia. Often a synonym with the prefix \"anti-\" already exists (e.g. Polonophobia vs. anti-Polonism). Anti-religious sentiments are expressed in terms such as Christianophobia and Islamophobia.\n\nOther prejudices include:\n\n\n\n"}
{"id": "43307135", "url": "https://en.wikipedia.org/wiki?curid=43307135", "title": "MTNL Perfect Health Mela", "text": "MTNL Perfect Health Mela\n\nThe MTNL Perfect Health Mela is a five-day annual event centred around prevalent health issues and their prevention. This health fair is held between Dussehra and Diwali in the month of October every year, and features seminars, lectures, entertainment shows, health workshops, free on-ground checkups, pop–up stores, and competitions for schools, colleges and corporates. The event aims to create mass awareness on common healthcare issues in India. The brainchild of cardiologist Dr K K Aggarwal, the event is organised and managed by the Heart Care Foundation of India (HCFI). It is one of the most visited community health events, with a footfall of more than a lakh each year from all segments of society. Over the past two decades, the Mela has conducted many health awareness campaigns including hand hygiene, influenza management, heart health, dengue, and malaria.\n\nPerfect Health Mela was held for the first time in 1993. This was commemorated by a stamp issue.\n"}
{"id": "35631116", "url": "https://en.wikipedia.org/wiki?curid=35631116", "title": "Machik", "text": "Machik\n\nMachik is a U.S.-based non-profit, non-governmental organisation that incubates social innovation in Tibet. Their work focuses on five main areas: The Chungba Project, Women's Initiatives, Summer Enrichment Program, Social Entrepreneurship, Youth Leadership, and Governance.\n\nMachik was founded by Tibetan sisters Losang and Tashi Rabgey, two Ph.D.s who grew up predominantly in Canada. The group came about after the sisters and their parents built the Ruth Walter Chungba Primary School in the village of Chungba, located in the Kham region of Tibet. The school first opened in 2002, and is now considered an exemplary school in the region. The organisation has also gone on to develop the Chungba Middle School and create programs to offer educational opportunities for Tibetan women in the Amdo region and elsewhere.\n\nLosang and Tashi Rabgey were born in a Tibetan refugee settlement in India and later moved to Canada. Losang holds a Ph.D. from the University of London's School of Oriental and African Studies and was named an Emerging Explorer by the National Geographic; Tashi was the first Rhodes Scholar of Tibetan descent and holds a Ph.D. from Harvard University. She is now a Research Professor of International Affairs at the Elliott School of International Affairs at George Washington University. Both have published extensively on Tibet issues and continue to do fieldwork on the ground in the region.\n\nThe work of Machik has grown from their first school to include much broader initiatives. Their approach to making an impact has centered on community-based approaches and direct interventions. Projects have included bringing green energy and clean drinking water to a rural town, building a library in town with high literacy but low access to materials, creating a greenhouse, repairing roads and houses, and participating in health care initiatives.\n\nMachik is a partner of the University of Virginia Tibet Center, where Tashi Rabgey served as co-director and founded the Tibet Sustainable Governance Program. Through the program, Machik works with graduate students at the University to conduct research and participate in field work of the organisation.\nMachik has been able to achieve success in their programs without the political pitfalls that organisations have faced. Machik's staff speak both Mandarin Chinese and Tibetan, and their educational efforts in the U.S. emphasize an understanding of Tibet within the broader system of international affairs, including Chinese governance.\n"}
{"id": "58712505", "url": "https://en.wikipedia.org/wiki?curid=58712505", "title": "Male infertility crisis", "text": "Male infertility crisis\n\nThe male infertility crisis is a term used by the popular media to describe the rapid decrease in sperm quality, and consequential problems with male infertility, seen over the 40 year period starting in the late 1970s. Over that time period, the number of viable sperm in men in Australia, Europe, New Zealand and North America has roughly halved, falling at a rate of 1.4% per year. This reduction has not been seen in other parts of the world. A number of hypotheses have been put forward for the causes of the decline, including lifestyle factors and the presence of hormone-disrupting chemicals in the environment.\n\nThe phenomenon has been the subject of study for some years, starting with a 1992 paper by Skakkebaek et al. A meta-analysis by Levine et. al. published in 2017 confirmed Skakkebaek's earlier results, leading to considerable coverage in the mass media, characterizing the reduction in sperm quality as a \"male [in]fertility crisis\", with some comparing its possible long term effects with the fertility collapse in dystopian fictional works such as \"Children of Men\" and \"The Handmaid’s\" \"Tale\"\".\"\n\n"}
{"id": "33061470", "url": "https://en.wikipedia.org/wiki?curid=33061470", "title": "Methods of Information in Medicine", "text": "Methods of Information in Medicine\n\nMethods of Information in Medicine is a peer-reviewed scientific journal covering research in medical informatics. It is an official journal of the International Medical Informatics Association, the European Federation for Medical Informatics, and the German Association for Medical Informatics, Biometry and Epidemiology. It is the oldest and longest running journal in its field.\n\nThe journal is abstracted and indexed in:\nAccording to the \"Journal Citation Reports\", the journal had a 2014 impact factor of 2.2. In 2015 the journal did not receive an impact factor as it was delisted because of citation stacking. The 2016 impact factor subsequently fell to 1.8 and in the 2018 release of JCR 2017 fell further to 1.5 .\n\nThe editors of \"Methods of Information in Medicine\" and the other journal involved, \"Applied Clinical Informatics\" published a rebuttal in an editorial in both journals. The president and publication officer of the European Federation for Medical Informatics also commented on this matter.\nPhil Davis visualized the citation cartel pointing out that Lehmann and Haux (the editors of the two journals) produced a total of 4 papers in 2014 and 2015 that excessively cited each other's journal articles for the impact-factor relevant years.\n"}
{"id": "1515957", "url": "https://en.wikipedia.org/wiki?curid=1515957", "title": "NHS Care Records Service", "text": "NHS Care Records Service\n\nThe NHS Care Records Service is a service provided by NHS Connecting for Health for the National Health Service in England.\n\nThe project describes its objectives as follows:\n\n \n"}
{"id": "41138011", "url": "https://en.wikipedia.org/wiki?curid=41138011", "title": "Nae Danger", "text": "Nae Danger\n\nNae Danger is an energy drink from Scotland. It is claimed to be the first and only carbonated energy drink to have been created, branded and manufactured in Scotland, which is contested with IrnBru32 which was released in 2006 by A.G. Barr. Released in August 2012, the drink is a creation of Ross Gourlay, a Scottish entrepreneur from Bishopbriggs. Gourlay hoped to gain a slice of the growing caffeinated drinks market and also encourage recognisable Scottish brands within the UK. There are two main flavours: Blue Raspberry and Red Blueberry.\n\nThe can design includes the words \"Nae Danger\", a popular Strathclyde phrase equivalent to the Australian \"no worries\", spanning the length of the can. The can also states that it's \"an energy drink like nae other\" and \"it's only a quid\", endorsing the Scottish theme of the product. The company is a new addition to the energy drink market which is worth over £790 million in the UK having risen 17% between 2012 and 2013. The drink competes with other highly caffeinated energy products such as Relentless, Monster, Rockstar and Red Bull. Nae Danger has culminated an assortment of 6 drinks since its released in mid-2012.\n\nThe product name was initially brought up by Ross Gourlay while joking with friends. However, Gourlay had seen the potential in such a product name and believed he could compete with the market leaders, while entering the Scottish market through a quirky brand name. The saying \"Nae Danger\" is a commonly known south-western Scottish regional dialect for \"No Problem\" and this features on the can as well as other phrases such as \"Aye.. Ye Cannae Whack It\". The drink prides itself on being \"only a quid\", meaning that every can is £1 to purchase. Gourlay, who is now head of Nae Danger was a marketing director of a food and drink wholesaler Glencrest Ltd.\n\nNae Danger comes in 4 different flavours. The original flavours were Blue Raspberry and Red Blueberry and these came in 500ml cans. The company then went on to create an original energy drink which conforms to traditional energy drink flavour. There was also a \"Nae Sugar\" (No Sugar) version of Blue Raspberry released which had the same flavour as the standard Blue Raspberry. The company then went on to release two sports drinks, again with the flavour Blue Raspberry and a new addition of Orange.\n\nIn July 2013, they launched a SHY & DRS brand can of 'Original Energy' which featured a free download of their hit single \"Relapse\".\n\nAll Nae Danger drink products have bright, vibrant packaging with distinct flavours. These flavours are distinguishable via the can/bottle colours. The drink itself is coloured and the colour corresponds to the flavour of the can. Blue Raspberry is a light blue colour and Red Blueberry is a pink-red colour.\n\nThe ingredients of Nae Danger include: carbonated water, sugar, citric acid, flavouring; caffeine, taurine, colour E133. Most Nae Danger energy drink products contain a high caffeine content (32 mg/100ml), therefore the packaging contains text describing that the product is not suitable for children, pregnant women or those sensitive to caffeine.\n\nNae Danger mainly advertises through social media and promotional campaigns in Universities. As well as this, they backed up their marketing with radio advertisement and new media work campaigns on Facebook and Twitter. This is mostly where they interact with fans of the drink, as well as advertising their product and events. Nae Danger also sponsor talented Scots to market their product. The fields that they sponsor range from go-karting, skiing and skateboarding. They also sponsor Fiona Wallace who races in the Mini Super Cup which is a small racing competition held in Britain. This event is a feature race at British Touring Car Championship (BTCC) events. The company sponsored the 2013 National Skiathon organised by Disability Snowsport UK that aims to provide facilities for those with disabilities to be able to enjoy Snow sports.\n\nThe products are mainly aimed at the Scottish market, however the company branched out of Scotland with a release of the product in English retail outlets in Yorkshire, Manchester, Liverpool, Birmingham and even London through Nisa Local stores. Stores and outlets that stock Nae Danger in Scotland include Spar who began stocking the product in 2012 and Aldi who is the most recent addition to Nae Danger stockists. Aldi started stocking the product in its stores as of mid-October 2013, other stockists are lesser known convenience stores throughout Scotland such as Semichem and Keystore. The cans themselves are distributed by Glencrest Ltd, Gourlay's former company.\n\nNae Danger came to an agreement with Spar to stock their products in 2012. Ross Gourlay described the deal as a \"major coup\" for the brand as this was the first serious deal for his company. Spar own 12,500 stores worldwide mostly within Europe. A deal with Scottish Spar has meant that 15,000 cans were supplied to shops across the country. Nae Danger had already sold over half a million cans through independent retailers in only a few short months before they struck a deal with a major retailer.\n\n"}
{"id": "23536265", "url": "https://en.wikipedia.org/wiki?curid=23536265", "title": "Novo-kamenniy Bridge", "text": "Novo-kamenniy Bridge\n\nNovo-kamenniy Bridge is a crossing of the Obvodny Canal in Saint Petersburg, Russian Federation. The bridge forms part of Ligovsky Avenue and connects Bezimyanni Island with the Tsentralny and Frunzensky districts of the city.\n\nThe first bridge to be built was constructed at the end of the 18th century; construction was supervised by engineer Ivan Gerard. The ‘Gerard’ bridge replaced a wooden beam aqueduct built to carry the Ligovsky Canal—the Yamskoi Vodoprovodniy Aqueduct—though the Ligovsky Canal was filled-in in the 1890s; the ‘Gerard’ bridge occupied the alignment of the filled-in canal.\n\nAfter having problems with decay and levels of passenger traffic, the ‘Gerard’ bridge was replaced. Engineer Pierre-Dominique Bazaine supervised the construction of the second bridge on the current bridge's site, which took place between 1816 and 1821.\n\nA third bridge was constructed between 1846 and 1848 under the direction of engineer A. N. Erakov. The ‘Erakov’ bridge featured granite pools for drinking from, with the water being sourced from the Ligovsky Canal. The ‘Erakov’ bridge was repaired and reconstructed in 1862, 1872 and 1874—though changes were not made to the bridge's design or general appearance.\n\nWith intensive building in southern areas of (the then) Leningrad in the 1950s, Ligovsky Avenue and the roads around Obvodny Canal Quay became congested. The 14 metre-wide ‘Erakov’ bridge—built in 1848—was too narrow to cope with the volume of traffic using it and, thus, a decision was taken to substantially widen the bridge.\n\nThe current bridge was designed by architect L. A. Noskov and was constructed between 1968 and 1970, developed by engineers N. P. Agapov and A. D. Guttsajt. The ‘Guttsajt’ bridge was substantially wider than its predecessor—at just under 45 metres in width—and was opened to public use on November 7, 1970.\n"}
{"id": "2572243", "url": "https://en.wikipedia.org/wiki?curid=2572243", "title": "Peel Island", "text": "Peel Island\n\nPeel Island (Indigenous: Teerk Ro Ra) is a small heritage-listed island located in Moreton Bay, east of Brisbane, in South East Queensland, Australia. The island is within the local government area of Redland City.\n\nDuring the mid-19th century, Peel Island was used as a quarantine station for the colony of Brisbane. Sailing ships would anchor to the north of the island, and the passengers would disembark on Peel Island for a quarantine period before moving on to Dunwich on nearby North Stradbroke Island. The arriving sailing ships would be fumigated and scrubbed down with carbolic to sanitise them before they ventured on to Brisbane with the new arrivals. Remains of the old quarantine station are at the southwest corner of the island, where the old well can be found.\n\nPeel Island was used as an asylum for vagrants from Brisbane around the start of the 20th century, but the conditions were too harsh and the inmates were moved to Dunwich, on nearby Stradbroke Island. Peel Island was also used as a sisal farm. The inmates would harvest the sisal and manufacture rope which was sold to help fund the asylum. Remnants of the sisal plantations are still visible when walking around the western side of the island.\n\nBetween 1907 and 1959 the island was a leper colony. It is now enjoyed by many locals and visitors. The island is only accessible by watercraft. Dugongs, turtles, and dolphins frequent the waters around the island. There are often thousands of jellyfish following the surrounding currents, and sharks are known to inhabit these waters. Horseshoe Bay, with its sandy beach, is popular with boating visitors. It is a popular overnight anchorage for sailors, considered by many to be the best shelter from northerly winds in Moreton Bay. Sea kayakers also use the island for overnight stays. The island is known for its natural beauty, with bird and animal life largely undisturbed by pollution. Up to 74 bird species have been identified.\n\nIn 2007, the island was declared as Teerk Roo Ra National Park and Conservation Park. There are limited facilities in Peel Island; however, there is a toilet block. Tracks which were used when the island was a leper colony can now be used to walk across the island. A feature worth visiting is the old leper colony town, located on the northwest of the island. The leper colony's housing is currently being restored, possibly for school camps, but there is asbestos in some of the housing used for Indigenous Australians housed there. After the island was decommissioned as a leper colony, it was discovered that the strain of leprosy which infected its inhabitants was non-contagious.\n\nThe Harry Atkinson Artificial Reef has been constructed to the north of Peel Island.\n\nPeel Island is situated in the southern half of Moreton Bay on the east coast of Australia, approximately from Brisbane, Queensland, and from the town of Cleveland. The island lies between Cleveland Point and Dunwich on North Stradbroke Island, and is fringed with mudflats, seagrass, coral reefs and mangroves. The island covers an area of approximately , and extends for north to south and east to west. Horseshoe Bay, running in an unbroken arc along the southern side of the island, provides clean, sheltered waters for swimming.\n\nPeel Island operated as a lazaret from 1907–1959. The Peel Island lazaret is important to Queensland history because of its social and political significance in terms of state health policy, serving as a reminder of the conditions in which people lived and worked on the island.\n\nThe lazaret (lazaretto, leper colony or leprosaria) in Queensland was established to isolate those infected with leprosy. The influx of migrants to Queensland after free settlement brought leprosy, or Hansen’s disease, to Australia. Hansen’s disease has had a history of forced patient isolation from society, and Queensland’s Leprosy Act of 1892 was an example of legislation intended to isolate leprosy patients from the mainland.\nBefore Peel Island was used as a lazaret in 1907, it was used for a number of other purposes by colonial and Queensland governments, as well as being occupied by Australian Indigenous people. Before British colonial settlement in Australia, Indigenous people lived on Peel Island, with the land used as a feasting and ceremonial site. Archaeological studies show evidence of Indigenous occupancy through the presence of several midden sites. Into the 1800s, Peel Island, as well as North Stradbroke Island, was used as a quarantine station by the New South Wales colonial government which \"housed persons considered unsuitable for mainstream society\". Subsequently, the quarantine station developed into an inebriates' asylum, and then later a lazaret in 1907. There were already two established lazarets in Queensland: one on Friday Island and another on Dunwich, North Stradbroke Island. Both were closed due to varied criticism of conditions and treatment of patients. Subsequently, the Peel Island lazaret was established as a replacement. Peel Island was used for multiple purposes at any given time by the government, but was specifically chosen over North Stradbroke Island to permanently establish the lazaret.\n\nParticularly under earlier operations of the lazaret, the isolation of Peel Island more resembled incarceration than that of a medical institution for ill patients. In many instances, sufferers were removed from their families and communities without notice or an opportunity to say goodbye. Patients were often locked up or chained by police before they were taken to the lazaret. There have been several accounts of patients being trawled behind a charter ship, isolated on a dinghy en route to the island. Once at the facility, patients sought help from the outside community and the press in order to improve the dreadful conditions to which they were subjected. Because the lazaret was designed around the principle of isolation, each patient was housed in a separate hut, then grouped into three compounds according to gender, race and severity of illness. Each compound was surrounded by -tall wired fences which would be locked at night so as to prevent perceived \"illicit behaviour\" between the patients.\n\nIn a standard hut, each patient was supplied with a bed, chest of drawers, table and chair. In the lazaret's later years of operation, awnings were also added to the huts in order to protect the patients from the elements. Other lazaret buildings on the island included a kitchen, dining room, bathhouses, nurses’ cottages, attendants’ quarters and caretakers’ residences. For many years it was prohibited to remove the bodies of patients who had died on the island, making it necessary for them to be buried there.\n\nTo this day, the site has been preserved and remains a confronting reminder of the conditions of the lazaret.\n\nBoredom was a real issue for patients on Peel Island. Whilst staff could freely leave the island, patients were confined there – often for many years – without a release date. Patients, mostly men, would often go fishing or do some gardening to pass the days. Most patients had wireless radio sets, and in the later years of the lazaret, films were shown and dances were organised for both staff and patients. Many of these social events led to marriages over the years. Staff would often spend time at Horseshoe Bay, enjoying the beach and serenity away from the centre of the lazaret.\nDue to the isolation and oft-substandard living conditions, many patients and staff members enjoyed drinking. By the 1950s, the island's occupants had built a reputation among the wider mainland community for their alcohol consumption and intoxicated behaviour.\nAlthough the Queensland Government was unwavering in its policy of isolating Hansen disease sufferers on Peel Island, issues often arose due to lack of adequate funding. Problems such as poor food supplies, inadequate medical treatment and lack of maintenance only increased the sense of deprivation among patients, as well as staff. An Anglican Church of the Good Samaritan was built in the north-eastern corner of the lazaret in 1908, originally for primary use by Melanesian patients. In 1925, the island's first multi-purpose medical facility was built, and the first hospital building followed in 1937. It was not until 20 years after the opening of the lazaret on Peel Island that the first medical treatment building (a surgery) was erected, and electricity was not available on the island until 1948 – 17 years after it was available on the mainland.\n\nThere were dramatic disparities between the treatment of non-white patients (Aboriginals, Torres Strait Islanders, South Sea Islanders and Chinese) and white European patients. When leprosy re-emerged in the colonised world, it was viewed as an imperial disease associated with race. This was reflective of the social attitudes of the time.\n\nAfter much criticism of the conditions in former lazarets on both Friday Island (which held Indigenous Australians and South Seas Islanders) and Dunwich Benevolent Asylum (which held white Europeans), the opening of the new lazaret on Peel Island held both white and non-white leprosy patients for the first time in Queensland. This close proximity of inter-racial patients highlighted the inequality in patient care.\n\nThe lazaret was divided into compounds which separated white and non-white patients. The accommodation and facilities for non-white patients were far less-equipped than those provided for white patients. For the first three years, non-white patients were not provided with any cooking or washing facilities, and their huts were of a far lower standard than those provided to white patients. Non-white patients had to carry their own firewood and water, while white patients had theirs provided for them. At an inquiry into the complaints of patients in 1908, the caretaker of Peel Island highlighted various disparities in the distribution of rations. He stated \"half the amount of meat, butter and tobacco allocated to whites was given to coloureds. Unlike the whites, coloured patients were not allocated beer or tapioca.\" Many non-white patients lived in tents until their huts were constructed. In the early years of the lazaret, the huts in the non-white compound were made of corrugated iron, with corrugated iron roofs and walls. Windows were made by cutting the wall with tinsnips. At first, the floor was merely the existing dirt, which would turn to mud in the rain as there were cracks in the roofs. The floors were later covered in cement. Each hut also often housed two patients, although only built and designed for one. These living conditions were extremely harsh, leaving many non-white patients sick, and it is argued that this had a direct effect on their higher death rate on the island.\n\nAt the beginning of World War II, resources for the number of patients on the island became limited. As a result, in 1940 all 50 non-white patients detained on Peel Island were sent to Fantome Island. By 1945, 40 of the patients had died of tuberculosis leaving further speculation as to the treatment of the patients. Authorities recognised the segregation between the basic standard of housing and treatment provided to white versus non-white patients as early as 1912. However, it was not until much later in the operation of the lazaret that these conditions were revised and consequently improved.\n\nWhen the lazaret first opened in 1907 there were 71 patients – 26 transferred from North Stradbroke Island, 30 from Friday Island, and 15 arriving later from Cooktown, Cairns and Halifax. Over the 52 years that Peel Island was an operating lazaret, over 500 patients passed through its doors. Nearly 200 of these died, while others went into remission and eventually left the island. In some instances, the disease reoccurred, which meant patients had to return to the island, sometimes even for a third or fourth time.\nUnderstandably, patients on Peel Island did not agree with the isolation \"treatment\" policy, and spoke up against the idea. In 1926, 35 patients petitioned to the Premier of Queensland to repeal existing legislation. A section of the petition stated: \"There are patients who would astound you by their fine healthy appearance, still they are held in segregation by the cruel and unjust law in existence.\" Unfortunately, it would be another 33 years until the lazaret on Peel Island closed, and patients could return to their communities.\n\nFor many of the 52 years that Peel Island was an operating lazaret, it was inadequately staffed. Due to the social stigma associated with Hansen’s disease, and the perception that it was highly contagious, it was difficult to find willing nurses, doctors and maintenance staff to work on the island. It was not until 1946 that the island saw its first resident doctor, despite being an institution for the sick. Before this time, patients would receive a weekly visit by a qualified doctor who would provide basic medical care.\n\n\nOne of the first experimental treatments for Hansen’s disease was the short-lived drug nastin, which involved the injection of the culture of the bacillus of leprosy. This was followed by the common treatment of injecting patients with oil from the Chaulmoogra nut. Although this treatment was often painful, and there was doubt as to whether it had long-term benefits, it remained a main treatment on Peel Island and around the world for more than 30 years. During this time, many medical professionals believed that a good diet and a stress-free lifestyle was more likely to send the disease into remission. In January 1947, Peel Island patients were treated with the first of several sulfone derivative drugs, which were developed in the United States. These drugs proved the most successful in the long line of treatments for Hansen’s disease sufferers, and from then on, the disease became easy to treat.\n\nHansen’s disease was believed to be highly contagious, with mortality unavoidable. Despite an increase in public understanding of this inaccuracy, this stigma had an incredibly long-lasting impact on the perception of patients on Peel Island. The Queensland Health Department’s decision to allay public fears about the disease by isolating patients backfired, leading the public to believe the disease was worse than it actually was. Rosemary Opala described the island as \"folklore\" where \"the mystery, however gothic fiction|gothic, is so much more romantic and aesthetically satisfying.\" From the relocation of patients in 1959 to the Queensland Parks and Wildlife Service taking responsibility in 1992, Peel Island was left relatively untouched, as some of the original stigma remained.\nMuch criticism has been levelled at the treatment of the patients on the lazaret. Hansen’s disease not only affected the ill but also their families. As infected patients were sent into isolation, many families were left without a breadwinner; some were driven out of communities by fear and ignorance of the disease, and others found themselves unemployed as word spread about disease in the family. Furthermore, by extension, the carers of the island were viewed by many as a \"people apart\". Carers were viewed as \"do-gooders\", resented for their ability to come and go from the island at will.\n\nDue to the breakthrough in the treatment of Hansen’s disease in the 1940s, the need for isolating patients declined and, therefore, so did the purpose of the lazaret on Peel Island. In 1959, the lazaret officially closed, and the remaining ten patients were sent to the Princess Alexandra Hospital in Brisbane to finish their treatment. By this time, many of the original prejudices about Hansen’s disease had been overcome, and fear surrounding the disease had somewhat vanished. Today, several drugs are available that counteract symptoms of Hansen’s disease such as nerve damage, deformity, disability and further transmission. Researchers are also working on vaccines to prevent the disease, as well as early detection.\n\nThe isolation and limited access to Peel Island has meant that many of the original lazaret buildings still stand in original condition to this day. Fortunately, this means that visitors can gain a unique look into a rare 20th-century institution. However, access is restricted in an effort to preserve the historic remains. As a result, the Queensland Parks and Wildlife Service has managed the park since 1992, during which time they have restored a number of key structures, and have worked to make the island a safe place for future visitors.\n\nIn 1993, Peel Island was recognised for its outstanding cultural heritage, and was consequently placed on the Queensland Heritage Register and the former Register of the National Estate. In December 2007, Peel Island was declared as \"Teerk Roo Ra\" (Place of Many Shells) National Park and Conservation Park.\n\n\n\n"}
{"id": "26764043", "url": "https://en.wikipedia.org/wiki?curid=26764043", "title": "PsychAlive", "text": "PsychAlive\n\nPsychAlive is a nonprofit multimedia website offering psychological information to the general public through articles, blogs, videos, webinars, and workshops. Founded in 2004 by the Glendon Association, PsychAlive focuses on issues of self, intimacy and parenting, drawing on the contributions, research and theory of contemporary psychology experts.\n\nPsychAlive was created with the goal of providing an online avenue for people to take an active, introspective approach to their psychological well-being, covering topics such as depression, anger, stress, addiction, self-destructive thoughts and behaviors, parenting, intimacy and general self-help.\n\nThe site is divided into three sections: Alive to Self, Alive to Intimacy and Alive to Parenting. Each of these sections focuses on the emotional issues that can arise as people strive to lead their lives as successful individuals, partners and parents. The tools available on PsychAlive are intended to help people understand the emotional dynamics that operate within them and the limitations that restrict them in their daily lives.\n\nPsychAlive additionally provides free workshops, which are multimedia experiences involving step-by-step processes that help people to learn and assimilate the psychological information offered on the site. The workshops engage visitors in progressive reading and writing exercises that are modeled after traditional psychotherapy techniques and processes. The sections of the PsychAlive Workshop address some of the significant areas of life in which people suffer emotionally: within themselves, in their intimate relationships and with their children. PsychAlive offers additional support by providing a secure, private online journal and supplementary information such as text, guidance from experts, personal examples, video, podcasts, book recommendations, movies, and music that relate to one’s specific stage in the workshop process as well as a media library of video, audio, books and reference materials.\n\nThe website features sections for student and guest bloggers to share personal experiences, as well as free regular webinars, interviews and video clips presented by mental health professionals. A newer addition to the site is a separate page for continuing education or CE opportunities. Visitors to the site may post their own CE workshops as well as find links to those offered by PsychAlive.\n\n"}
{"id": "2497276", "url": "https://en.wikipedia.org/wiki?curid=2497276", "title": "Public health surveillance", "text": "Public health surveillance\n\nPublic health surveillance (also epidemiological surveillance, clinical surveillance or syndromic surveillance) is, according to the World Health Organization (WHO), \"the continuous, systematic collection, analysis and interpretation of health-related data needed for the planning, implementation, and evaluation of public health practice.\" Public health surveillance may be used to \"serve as an early warning system for impending public health emergencies; document the impact of an intervention, or track progress towards specified goals; and monitor and clarify the epidemiology of health problems, to allow priorities to be set and to inform public health policy and strategies.\"\n\nPublic Health surveillance systems can be passive or active. A passive surveillance system consists of the regular, ongoing reporting of diseases and conditions by all health facilities in a given territory. An active surveillance system is one where health facilities are visited and health care providers and medical records are reviewed in order to identify a specific disease or condition. Passive surveillance systems are less time-consuming and less expensive to run but risk under-reporting of some diseases. Active surveillance systems are most appropriate for epidemics or where a disease has been targeted for elimination.\n\nTechniques of public health surveillance have been used in particular to study infectious diseases. Many large institutions, such as the WHO and the CDC, have created databases and modern computer systems (public health informatics) that can track and monitor emerging outbreaks of illnesses such as influenza, SARS, HIV, and even bioterrorism, such as the 2001 anthrax attacks in the United States.\n\nMany regions and countries have their own cancer registry, one function of which is to monitor the incidence of cancers to determine the prevalence and possible causes of these illnesses.\n\nOther illnesses such as one-time events like stroke and chronic conditions such as diabetes, as well as social problems such as domestic violence, are increasingly being integrated into epidemiologic databases called disease registries that are being used in the cost-benefit analysis in determining governmental funding for research and prevention.\n\nSystems that can automate the process of identifying adverse drug events, are currently being used, and are being compared to traditional written reports of such events. These systems intersect with the field of medical informatics, and are rapidly becoming adopted by hospitals and endorsed by institutions that oversee healthcare providers (such as JCAHO in the United States). Issues in regard to healthcare improvement are evolving around the surveillance of medication errors within institutions.\n\nSyndromic surveillance is the analysis of medical data to detect or anticipate disease outbreaks. According to a CDC definition, \"the term 'syndromic surveillance' applies to surveillance using health-related data that precede diagnosis and signal a sufficient probability of a case or an outbreak to warrant further public health response. Though historically syndromic surveillance has been utilized to target investigation of potential cases, its utility for detecting outbreaks associated with bioterrorism is increasingly being explored by public health officials.\"\n\nThe first indications of disease outbreak or bioterrorist attack may not be the definitive diagnosis of a physician or a lab.\n\nUsing a normal influenza outbreak as an example, once the outbreak begins to affect the population, some people may call in sick for work/school, others may visit their drug store and purchase medicine over the counter, others will visit their doctor's office and other's may have symptoms severe enough that they call the emergency telephone number or go to an emergency department.\n\nSyndromic surveillance systems monitor data from school absenteeism logs, emergency call systems, hospitals' over-the-counter drug sale records, Internet searches, and other data sources to detect unusual patterns. When a spike in activity is seen in any of the monitored systems disease epidemiologists and public health professionals are alerted that there may be an issue.\n\nAn early awareness and response to a bioterrorist attack could save many lives and potentially stop or slow the spread of the outbreak. The most effective syndromic surveillance systems automatically monitor these systems in real-time, do not require individuals to enter separate information (secondary data entry), include advanced analytical tools, aggregate data from multiple systems, across geo-political boundaries and include an automated alerting process.\n\nA syndromic surveillance system based on search queries was first proposed by Gunther Eysenbach, who began work on such a system in 2004.\nInspired by these early, encouraging experiences, Google launched Google Flu Trends in 2008. More flu-related searches are taken to indicate higher flu activity. The results closely match CDC data, and lead it by 1–2 weeks. The results appeared in Nature. More recently, a series of more advanced linear and nonlinear approaches to influenza modelling from Google search queries have been proposed. Extending Google's work researchers from the Intelligent Systems Laboratory (University of Bristol, UK) created Flu Detector; an online tool which based on Information Retrieval and Statistical Analysis methods uses the content of Twitter to nowcast flu rates in the UK.\n\nInfluenzanet is a syndromic surveillance system based on voluntary reports of symptoms via the internet. Residents of the participant countries are invited to provide regular reports on the presence or absence of flu related symptoms. The system has been in place and running since 2003 in the Netherlands and Belgium. The success of this first initiative led to the implementation of Gripenet in Portugal in 2005 followed by Italy in 2008 and Brasil, Mexico, and the United Kingdom in 2009.\n\nSome conditions, especially chronic diseases such as diabetes mellitus, are supposed to be routinely managed with frequent laboratory measurements. Since many laboratory results, at least in Europe and the US, are automatically processed by computerized laboratory information systems, the results are relatively easy to inexpensively collate in special purpose databases or disease registries. Unlike most syndromic surveillance systems, in which each record is assumed to be independent of the others, laboratory data in chronic conditions can be theoretically linked together at the individual patient level. If patient identifiers can be matched, a chronological record of each patient's laboratory results can be analyzed as well as aggregated to the population level.\n\nLaboratory registries allow for the analysis of the incidence and prevalence of the target condition as well as trends in the level of control. For instance, an NIH-funded program called the Vermedx Diabetes Information System maintained a registry of laboratory values of diabetic adults in Vermont and northern New York State in the US with several years of laboratory results on thousands of patients. The data included measures of blood sugar control (glycosolated hemoglobin A1C), cholesterol, and kidney function (serum creatinine and urine protein), and were used to monitor the quality of care at the patient, practice, and population levels. Since the data contained each patient's name and address, the system was also used to communicate directly with patients when the laboratory data indicated the need for attention. Out of control test results generated a letter to the patient suggesting they take action with their medical provider. Tests that were overdue generated reminders to have testing performed. The system also generated reminders and alerts with guideline-based advice for the practice as well as a periodic roster of each provider's patients and a report card summarizing the health status of the population. Clinical and economic evaluations of the system, including a large randomized clinical trial, demonstrated improvements in adherence to practice guidelines and reductions in the need for emergency department and hospital services as well as total costs per patient. The system has been commercialized and distributed to physicians, insurers, employers and others responsible for the care of chronically ill patients. It is now being expanded to other conditions such as chronic kidney disease.\n\nA similar system, The New York City A1C Registry, is in used to monitor the estimated 600,000 diabetic patients in New York City, although unlike the Vermont Diabetes Information System, there are no provisions for patients to have their data excluded from the NYC database. The NYC Department of Health and Mental Hygiene has linked additional patient services to the registry such as health information and improved access to health care services. As of early 2012, the registry contains over 10 million test results on 3.6 million individuals. Although intended to improve health outcomes and reduce the incidence of the complications of diabetes, a formal evaluation has not yet been done.\n\nIn May 2008, the City Council of San Antonio, Texas approved the deployment of an A1C registry for Bexar County. Authorized by the Texas Legislature and the state Health Department, the San Antonio Metropolitan Health District implemented the registry which drew results from all the major clinical laboratories in San Antonio. The program was discontinued in 2010 for lack of funds.\n\nLaboratory surveillance differs from population-wide surveillance in that it can only monitor patients who are already receiving medical treatment and therefore having lab tests done. For this reason, it does not identify patients who have never been tested. Therefore, it is more suitable for quality management and care improvement than for epidemiological monitoring of an entire population or catchment area.\n\n"}
{"id": "36198777", "url": "https://en.wikipedia.org/wiki?curid=36198777", "title": "Strašnice Crematorium", "text": "Strašnice Crematorium\n\nStrašnice Crematorium (in ) is the largest crematory in Europe in terms of area. President Václav Havel was cremated here. The crematorium was involved in the disposal of those who had been executed by the Nazi and Communist regimes including writer Vladislav Vančura, general Josef Mašín, politician Milada Horáková and bishop Gorazd of Prague.\n\nThe crematorium was started in 1929 and it opened in 1932 in Prague-Vinohrady, Czechoslovakia (today the Czech Republic). In terms of area it is the largest crematorium in Europe. The main hall is sixteen metres high and covers an area of 450 square metres. The design brings in the hearses such that the journey into the building is down a gentle slope of three degrees. The ceremonial hall can accommodate 200 people but it has a large glass viewing area so that additional mourners can be accommodated outside. Music can either be recorded or the hall can also use its own organ.\n\nDuring both the Nazi and communist era the crematorium was involved in the disposal of bodies resulting from executions and judicial killings. 2,200 decapitated people were said to have been cremated by the \"night shift\" during the Second World War. The bodies included Vladislav Vančura, General Josef Mašín and the martyr Bishop Gorazd.\n\nAfter the war in 1948 the disposals started again with the remaining ashes destined to be added to compost. However some urns were kept although relatives were denied access to them. The crematorium was involved in the disposal of the body of Milada Horáková whose death was internationally condemned. The son of the crematorium's manager witnessed the disposal of bodies and this led to him joining the anti-communist resistance. He was sentenced to 25 years in jail in 1952.\n\nThe crematorium has been listed as a cultural monument of the Czech Republic since 1988.\n\nFormer President Václav Havel was cremated here before his ashes were placed in the family vault in Vinohrady Cemetery nearby.\n\nAs part of the European Heritage Days initiative this building was opened to the public in September 2012.\n"}
{"id": "39776872", "url": "https://en.wikipedia.org/wiki?curid=39776872", "title": "Sullivan's Index", "text": "Sullivan's Index\n\nSullivan's index also known as Disability Free Life Expectancy (DFLE) is a method to compute life expectancy free of disability. It is calculated by formula: <br>\nLife expectancy formula_1 duration of disability\n\n\n"}
{"id": "6771518", "url": "https://en.wikipedia.org/wiki?curid=6771518", "title": "TORCH report", "text": "TORCH report\n\nThe TORCH report (The Other Report on Chernobyl) was a health impacts report requested by the European Greens in 2006, for the twentieth anniversary of the Chernobyl disaster, in reply to the 2006 report of the Chernobyl Forum which was criticized by some advocacy organizations opposed to nuclear energy such as Greenpeace. \n\nIn 2006, German Green Member of the European Parliament Rebecca Harms, commissioned two British scientists to write an alternate report (TORCH, The Other Report on Chernobyl) as a response to the 2006 Chernobyl Forum report. The two British scientists that published the report were radiation biologist Ian Fairlie, who had published a number of papers dating back to at least 1992 and David Sumner. Both are members of the International Physicians for the Prevention of Nuclear War, an organization awarded the Nobel Peace Prize in 1985.\n\nIn 2016, an updated TORCH report was written. \n\nA summary of the 2006 TORCH report said, in part:\n\nThe 2006 TORCH Report stated that: \n\nThe 2016 report said that 5 million people still live in areas highly contaminated by radiation, in Belarus, Russia, and Ukraine. 400 million people still live in areas which are less contaminated.\n\nThe TORCH 2006 report \"estimated that more than half the iodine-131 from Chernobyl [which increases the risk of thyroid cancer] was deposited outside the former Soviet Union. Possible increases in thyroid cancer have been reported in the Czech Republic and the UK, but more research is needed to evaluate thyroid cancer incidences in Western Europe\". It predicted about 30,000 to 60,000 excess cancer deaths and warned that predictions of excess cancer deaths strongly depend on the risk factor used; and predicted excess cases of thyroid cancer range between 18,000 and 66,000 in Belarus alone depending on the risk projection model \n\nBy 2016, 6,000 cases of thyroid cancer had been diagnosed, with another 16,000 expected. More generally, 40,000 fatal cancer cases are expected across Europe.\n\nThe TORCH report also stated that \"two non-cancer effects, cataract induction and cardiovascular diseases, are well documented with clear evidence of a Chernobyl connection.\" \nQuoting the report, \"Nature\" wrote that: \"it is well known that radiation can damage genes and chromosomes\"; \"the relationship between genetic changes and the development of future disease is complex and the relevance of such damage to future risk is often unclear. On the other hand, a number of recent studies have examined genetic damage in those exposed to radiation from the Chernobyl accident. Studies in Belarus have suggested a twofold increase in the germline minisatellite mutation rate\".\n\nSome 116,000 people were evacuated from contaminated areas initially, and later 230,000 people were relocated and resettled.\n\n"}
{"id": "55677682", "url": "https://en.wikipedia.org/wiki?curid=55677682", "title": "The Mad Pooper", "text": "The Mad Pooper\n\nThe Mad Pooper is the nickname given to an unidentified woman in Colorado Springs, Colorado, United States, who repeatedly defecated in public while jogging during the summer months of 2017. While she has primarily targeted one family's property, she has not used it exclusively, leaving some of her excrement at other sites nearby. Photographs of her have been made public, but neither she nor anyone who knows her has come forward with further information that might identify her.\n\nPolice believe the woman's actions are intentional, since there are several public toilets within a block of the family's house that she could use. After the case received national media attention, a purported spokesman claimed in a YouTube video that her actions were related to recent medical issues and enjoyed First Amendment protection; however that video turned out to be a hoax. Procter & Gamble has offered her a free year's supply of its Charmin brand toilet paper if she turns herself in.\n\nAfter a burst of news media coverage in mid-September, police reported that there had been no further reports of the woman defecating in public, although she had not been identified. A few commentators speculated about her possible motives. One believed she might suffer from Crohn's disease; another, in \"Psychology Today\", proposed that she was an exhibitionist with poor impulse control who was taking revenge on the family that had caught her.\n\nCathy Budde says the defecations began in mid-July 2017, outside their house near the intersection of Briargate Boulevard and North Union Parkway in the northern end of Colorado Springs, Colorado. As she later told local TV station KKTV, her children came in one day and told her \"There's a lady taking a poop!\" \n\nBudde was not sure they were serious, and went outside to see for herself. There she saw the woman, squatting, with her jogging shorts around her ankles. When Budde confronted the woman about having defecated in public and allowing the Budde children to see her private parts in the process, the woman said \"Yeah ... sorry\" and left after pulling up her shorts. Budde assumed she would return quickly to clean up the feces, and never again jog in the area due to the embarrassment the incident caused.\n\nInstead, the Buddes found feces at the same spot on the sidewalk at least once a week. Paper napkins and wipes the jogger had used to clean herself were often nearby. On three occasions Budde saw the woman defecate. The Budde children began calling her \"The Mad Pooper of Pine Creek\" after a stream that flows through the neighborhood, sometimes used as its name.\n\n\"This is intentional\", Budde told \"The Washington Post\". There are, she explained, several public toilets nearby the woman could use. There is one across the street, portable toilets in a nearby public park, and some at a gas station a short distance away. All would be easy for the jogger to use if she regularly had the need while running. Neighbors also told Budde they had seen the woman defecating in their back yards as well as near a local Walgreens drugstore.\n\nBudde notified the Colorado Springs Police Department (CSPD), and made attempts on her own to end the jogger's defecations. She took pictures from the house, capturing a short brown-haired woman in a gray tank top and shorts. She put up a printed paper sign at the place near her house favored by the jogger, warning her that the police were aware of the situation, and asking her to stop. \n\nIn response, Budde said, the woman began changing the times she jogged by. She also ignored the note's pleas, running by it 15 times in one day before defecating near it again, according to Budde. The police have said she could face charges of indecent exposure and public defecation were they to identify and arrest her, charges that, in the event of a conviction, might require that she register as a sex offender.\n\nOne officer, Lt. Howard Black, said he had never seen anything similar in his 35-year career in Colorado Springs. Like Budde, he admitted to the \"Post\" that it was amusing, but allowed for the possibility that the jogger might not be in full control of her actions. \"If it's a mental health issue, she'll still be held accountable, but we would want to get her help.\"\n\nIn September, almost two months after the jogger's defecations had begun, Budde went to KKTV, which reported on the story. Shortly afterwards, the \"Post\" wrote its article, and the story went viral. Through its Twitter account, Procter & Gamble made an offer to the jogger: if she turned herself in to police, they would give her a year's supply of the company's Charmin toilet paper for free.\n\n\"Runner's World\" magazine ran a story with a headline asking the woman to stop. \"[A]s runners, we understand having a sudden emergency now and then. But when it becomes a habit, you need to change up your regimen.\" it wrote, giving readers the phone number for the CSPD to call if they believed they knew who the Mad Pooper was. The article further urged readers who might similarly be using public places to defecate on their runs to likewise stop. \"Pull up your pants, and take your business elsewhere. We do not know what else to say.\"\n\nSports news website \"Deadspin\" called the CSPD about the case so frequently the department asked them to stop in early October since they had nothing new to report. Since then, police report, the woman appears to have stopped. However, despite the increase in publicity around the case the woman has not been identified.\n\nIn response to the news coverage, a video was posted on YouTube by someone who claimed to be a family member of the jogger, who he called \"Shirley\". He said she was sorry for repeatedly fouling the Buddes' yard. However he also attributed her inability to control herself to a traumatic brain injury and complications from recent sex reassignment surgery.\n\nBeyond that, however, he said it was irrelevant as she was not breaking any law. Her actions, he argued, were protected under the First Amendment, much like breastfeeding. Jeremy Loew, a criminal defense attorney consulted by Colorado Springs television station KRDO-TV emphatically rejected that claim. If the video truly was posted by \"Shirley\"'s family, Loew said, it was \"actually the worst thing the[y] could do. ... Maybe they thought the videos would make the situation go away, but they won't.\" The spokesman might even find himself called as a witness, Loew added.\n\nA few days later, the man posted a second video in which he disclosed that the first one was a hoax. He was later revealed to be DISOP TV, another YouTube user known for producing videos of flatulent pets, who admitted he made up the first video as satire. Both that video and the original were soon deleted from the site.\n\nSome news outlets went beyond reporting on the case to speculate as to what the reasons for the jogger's actions might be. Allison Feller, a writer for \"Women's Health\", wondered if, like her, the Mad Pooper might be an avid runner who suffers from Crohn's disease, a bowel inflammation whose symptoms can sometimes include an inability to hold excrement long enough to reach a toilet. \"[M]y first thought upon seeing this headline\" she recalled, \"was, 'Oh my god, being the subject of this story is my greatest fear.'\"\n\nSometimes her disease left her in such pain as to be unable to run or even leave the house, Feller wrote. But when it did, she would experience symptoms very suddenly. \"[Crohn's] can strike when you least expect it. And when that happens, my intestines go from totally chill to totally explosive faster than you can even \"think\" about where the closest available restroom may be.\"\n\nFeller said she, too, took toilet paper and wipes with her every time she went for a run. \"I don't run with these things because I'm \"planning\" to use them. I run with them because I've learned to be prepared.\" She recounted many times when, due to public bathrooms being closed or too distant for her to reach she had had to seclude herself behind trees or under bridges in order to defecate. One time, she admitted, she had even had to use her pants.\n\n\"In an age where everyone is armed with a cell-phone camera, I am terrified of someone seeing me going to the bathroom in an unapproved place, then snapping a photo or video, and posting it for the world to see, LOL at, and comment on\", Feller wrote. She felt considerable shame over the way her disease inconvenienced her, and called the idea of experiencing public shaming on the Internet because of it \"paralyzing\". She warned readers who might not understand that feeling that they should not shame someone they see experiencing similar distress while running. \"[O]ne day, you, too, may eat a burrito that doesn't agree with your stomach, and you may find yourself in a non-bathroom bathroom situation.\n\nAt \"Psychology Today\", Dale Hartley, an associate professor of psychology at West Virginia University at Parkersburg, was dismissive of the idea that the Mad Pooper might suffer from an ailment that caused those problems, since she was so public in her defecations and had shown no sign of changing her routine to avoid having to do so. Nor did he think she had any psychosis, as she was \"too put-together to be out of touch with reality\" and kept to a regular schedule. \"We would not expect to see consistently organized, predictable behaviors in someone who has undergone a psychotic break,\" Hartley wrote.\n\nIt was more likely, Hartley presumed, that some lesser psychological issues led to her actions. \"Is she an exhibitionist? I think this is closer to the truth\", he wrote. But even if this was so, he believed, the jogger's real problem was impulse control disorder.\n\nHer choice of the same location for so many of her defecations, Hartley also speculated, may have been retaliation against the Buddes for having caught her in the act.\n\n"}
{"id": "40718778", "url": "https://en.wikipedia.org/wiki?curid=40718778", "title": "Theodora Mead Abel", "text": "Theodora Mead Abel\n\nTheodora Mead Abel (1899–1998) was an American clinical psychologist and educator, who used innovative ideas by combining sociology and psychology. She was a pioneer in cross-cultural psychology.\n\nTheodora was born in Newport, Rhode Island, on September 9, 1899 and raised in New York City. In 1917, she graduated from Miss Chapin's School, where she was president of the student government. Abel attended Vassar College and received her B.A. in 1921. In 1924, she received an M.A. from Columbia University, where one of her professors was Leta Stetter Hollingworth. She then attended the University of Paris and obtained her degree in psychology in 1923. Her final degree came from Columbia and was a Ph.D., in 1925.\n\nAfter receiving her education, Theodora spent time as an educator. She taught at the University of Illinois (1925-1926), Sarah Lawrence College (1929-1933), and the Manhattan Trade School for Girls. She then entered the civil world. She worked at the New York State Department of Mental Hygiene from 1940 until 1946, as their chief psychologist. In 1947 she took the position of director of psychology at New York City's Post-Graduate Center for Mental Health, a position she held for 24 years.\n\nIn 1971, after moving to New Mexico, she became chief of family therapy at the Child Guidance Center, in Albuquerque, where she also established a private practice. While in New Mexico, she conducted studies of Puebloan peoples.\n\nShe wrote many books including the following:\n\nThe last of these four books includes an introduction by Margaret Mead, whom Abel had met during graduate school at Columbia. They became friends after lining up alphabetically (both had the last name \"Mead\" but they were not related).\n\nAbel died in Forestburgh, New York, on December 2, 1998. Her husband, Theodore Abel, had died a decade earlier. They were survived by two daughters and a son, plus grandchildren and great-grandchildren.\n"}
{"id": "52346571", "url": "https://en.wikipedia.org/wiki?curid=52346571", "title": "Thunderstorm asthma", "text": "Thunderstorm asthma\n\nThunderstorm asthma is the triggering of an asthma attack by environmental conditions directly caused by a local thunderstorm. It has been proposed that during a thunderstorm, pollen grains can absorb moisture and then burst into much smaller fragments with these fragments being easily dispersed by wind. However, there is no experimental evidence confirming this theory. While larger pollen grains are usually filtered by hairs in the nose, the smaller pollen fragments are able to pass through and enter the lungs, triggering the asthma attack.\n\nThere have been events where thunderstorms have caused asthma attacks across cities such that emergency services and hospitals have been overwhelmed. The phenomenon was first recognised and studied after three recorded events in the 1980s; in Birmingham, England, in 1983 and in Melbourne, Australia in 1987 and 1989. Since then there have been further reports of widespread thunderstorm asthma in Wagga Wagga, Australia; London, England; Naples, Italy; Atlanta, United States; and Ahvaz, Iran. A further outbreak in Melbourne, in November 2016, that overwhelmed the ambulance system and some local hospitals, resulted in at least nine deaths. There was a similar incident in Kuwait in early December, 2016 with at least 5 deaths and many admissions to the ICU. \n\nMany of those affected during a thunderstorm asthma outbreak may have never experienced an asthma attack before.\n\nIt has been found 95% of those that were affected by thunderstorm asthma had a history of hayfever, and 96% of those people had tested positive to grass pollen allergies, particularly rye grass. A rye grass pollen grain can hold up to 700 tiny starch granules, measuring 0.6 to 2.5 μm, small enough to reach the lower airways in the lung.\n\n"}
{"id": "1906107", "url": "https://en.wikipedia.org/wiki?curid=1906107", "title": "Transtheoretical model", "text": "Transtheoretical model\n\nThe transtheoretical model of behavior change is an integrative theory of therapy that assesses an individual's readiness to act on a new healthier behavior, and provides strategies, or processes of change to guide the individual. The model is composed of constructs such as: stages of change, processes of change, levels of change, self-efficacy, and decisional balance.\n\nThe transtheoretical model is also known by the abbreviation \"TTM\" and sometimes by the term \"stages of change\", although this latter term is a synecdoche since the stages of change are only one part of the model along with processes of change, levels of change, etc. Several self-help books—\"Changing for Good\" (1994), \"Changeology\" (2012), and \"Changing to Thrive\" (2016)—and articles in the news media have discussed the model. It has been called \"arguably the dominant model of health behaviour change, having received unprecedented research attention, yet it has simultaneously attracted criticism\".\nJames O. Prochaska of the University of Rhode Island, and Carlo Di Clemente and colleagues developed the transtheoretical model beginning in 1977. It is based on analysis and use of different theories of psychotherapy, hence the name \"transtheoretical\".\n\nProchaska and colleagues refined the model on the basis of research that they published in peer-reviewed journals and books.\n\nThis construct refers to the temporal dimension of behavioural change. In the transtheoretical model, change is a \"process involving progress through a series of stages\":\n\nIn addition, the researchers conceptualized \"Relapse\" (recycling) which is not a stage in itself but rather the \"return from Action or Maintenance to an earlier stage\".\n\nThe quantitative definition of the stages of change (see below) is perhaps the most notorious feature of the model. However it is also one of the most critiqued, even in the field of smoking cessation, where it was originally formulated. It has been said that such quantitative definition (i.e. a person is in preparation if it intends to change within a month) does not reflect the nature of behaviour change, that it does not have better predictive power than simpler questions (i.e. \"do you have plans to change...\"), and that it has problems regarding its classification reliability.\n\nCommunication theorist and sociologist Everett Rogers suggested that the stages of change are analogues of the stages of the innovation adoption process in Rogers' theory of diffusion of innovations.\n\nStage 1: Precontemplation (not ready)\n\nPeople at this stage do not intend to start the healthy behavior in the near future (within 6 months), and may be unaware of the need to change. People here learn more about healthy behavior: they are encouraged to think about the pros of changing their behavior and to feel emotions about the effects of their negative behavior on others.\n\nPrecontemplators typically underestimate the pros of changing, overestimate the cons, and often are not aware of making such mistakes.\n\nOne of the most effective steps that others can help with at this stage is to encourage them to become more mindful of their decision making and more conscious of the multiple benefits of changing an unhealthy behavior.\n\nStage 2: Contemplation (getting ready)\n\nAt this stage, participants are intending to start the healthy behavior within the next 6 months. While they are usually now more aware of the pros of changing, their cons are about equal to their Pros. This ambivalence about changing can cause them to keep putting off taking action.\n\nPeople here learn about the kind of person they could be if they changed their behavior and learn more from people who behave in healthy ways.\n\nOthers can influence and help effectively at this stage by encouraging them to work at reducing the cons of changing their behavior.\n\nStage 3: Preparation (ready)\n\nPeople at this stage are ready to start taking action within the next 30 days. They take small steps that they believe can help them make the healthy behavior a part of their lives. For example, they tell their friends and family that they want to change their behavior.\n\nPeople in this stage should be encouraged to seek support from friends they trust, tell people about their plan to change the way they act, and think about how they would feel if they behaved in a healthier way. Their number one concern is: when they act, will they fail? They learn that the better prepared they are, the more likely they are to keep progressing.\n\nStage 4: Action (current action)\n\nPeople at this stage have changed their behavior within the last 6 months and need to work hard to keep moving ahead. These participants need to learn how to strengthen their commitments to change and to fight urges to slip back.\n\nPeople in this stage progress by being taught techniques for keeping up their commitments such as substituting activities related to the unhealthy behavior with positive ones, rewarding themselves for taking steps toward changing, and avoiding people and situations that tempt them to behave in unhealthy ways.\n\nStage 5: Maintenance (monitoring)\n\nPeople at this stage changed their behavior more than 6 months ago. It is important for people in this stage to be aware of situations that may tempt them to slip back into doing the unhealthy behavior—particularly stressful situations.\n\nIt is recommended that people in this stage seek support from and talk with people whom they trust, spend time with people who behave in healthy ways, and remember to engage in healthy activities to cope with stress instead of relying on unhealthy behavior.\n\nRelapse (recycling)\n\nRelapse in the TTM specifically applies to individuals who successfully quit smoking or using drugs or alcohol, only to resume these unhealthy behaviors. Individuals who attempt to quit highly addictive behaviors such as drug, alcohol, and tobacco use are at particularly high risk of a relapse. Achieving a long-term behavior change often requires ongoing support from family members, a health coach, a physician, or another motivational source. Supportive literature and other resources can also be helpful to avoid a relapse from happening.\n\nThe 10 processes of change are \"covert and overt activities that people use to progress through the stages\".\n\nTo progress through the early stages, people apply cognitive, affective, and evaluative processes. As people move toward Action and Maintenance, they rely more on commitments, conditioning, contingencies, environmental controls, and support.\n\nProchaska and colleagues state that their research related to the transtheoretical model shows that interventions to change behavior are more effective if they are \"stage-matched\", that is, \"matched to each individual's stage of change\".\n\nIn general, for people to progress they need:\n\nThe ten \"processes of change\" include:\n\nHealth researchers have extended Prochaska's and DiClemente's 10 original processes of change by an additional 21 processes. In the first edition of \"Planning Health Promotion Programs\", Bartholomew et al. (2006) summarised the processes that they identified in a number of studies; however, their extended list of processes was removed from later editions of the text. The additional processes of Bartholomew et al. were:\n\nWhile most of these processes are associated with health interventions such as smoking cessation and other addictive behaviour, some of them are also used in travel interventions. Depending on the target behaviour the effectiveness of the process should differ. Also some processes are recommended in a specific stage, while others can be used in one or more stages. Recently, these processes have been identified in travel interventions, broadening the scope of TTM in other research domains.\n\nThis core construct \"reflects the individual's relative weighing of the pros and cons of changing\". Decision making was conceptualized by Janis and Mann as a \"decisional balance sheet\" of comparative potential gains and losses. Decisional balance measures, the pros and the cons, have become critical constructs in the transtheoretical model. The pros and cons combine to form a decisional \"balance sheet\" of comparative potential gains and losses. The balance between the pros and cons varies depending on which stage of change the individual is in.\n\nSound decision making requires the consideration of the potential benefits (pros) and costs (cons) associated with a behavior's consequences. TTM research has found the following relationships between the pros, cons, and the stage of change across 48 behaviors and over 100 populations studied.\n\nThe evaluation of pros and cons is part of the formation of attitudes. Attitude is defined as a \"psychological tendency that is expressed by evaluating a particular entity with some degree of favour or disfavour\". This means that by evaluating pros and cons we form a positive or negative attitude about something or someone. During the change process individuals gradually shift from cons to pros, forming a more positive attitude towards the target behaviour. Attitudes are one of the core constructs explaining behaviour and behaviour change in various research domains. Other behaviour models, such as the theory of planned behavior (TPB) and the stage model of self-regulated change, also emphasise attitude as an important determinant of behaviour. The progression through the different stages of change is reflected in a gradual change in attitude before the individual acts. Most of the processes of change aim at evaluating and reevaluating as well as reinforcing specific elements of the current and target behaviour. The processes of change contribute to a great degree on attitude formation.\n\nDue to the synonymous use of decisional balance and attitude, travel behaviour researchers have begun to combine the TTM with the TPB. Forward uses the TPB variables to better differentiate the different stages. Especially all TPB variables (attitude, perceived behaviour control, descriptive and subjective norm) are positively show a gradually increasing relationship to stage of change for bike commuting. As expected, intention or willingness to perform the behaviour increases by stage. Similarly, Bamberg uses various behavior models, including the transtheoretical model, theory of planned behavior and norm-activation model, to build the stage model of self-regulated behavior change (SSBC). Bamberg claims that his model is a solution to criticism raised towards the TTM. Some researchers in travel, dietary, and environmental research have conducted empirical studies, showing that the SSBC might be a future path for TTM-based research.\n\nThis core construct is \"the situation-specific confidence people have that they can cope with high-risk situations without relapsing to their unhealthy or high risk-habit\". The construct is based on Bandura's self-efficacy theory and conceptualizes a person's perceived ability to perform on a task as a mediator of performance on future tasks. In his research Bandura already established that greater levels of perceived self-efficacy leads to greater changes in behavior. Similarly, Ajzen mentions the similarity between the concepts of self-efficacy and perceived behavioral control. This underlines the integrative nature of the transtheoretical model which combines various behavior theories. A change in the level of self-efficacy can predict a lasting change in behavior if there are adequate incentives and skills. The transtheoretical model employs an overall confidence score to assess an individual's self-efficacy. Situational temptations assess how tempted people are to engage in a problem behavior in a certain situation.\n\nThis core construct identifies the depth or complexity of presenting problems according to five levels of increasing complexity. Different therapeutic approaches are recommended for each level as well as for each stage of change. The levels are:\n\n\nThe outcomes of the TTM computerized tailored interventions administered to participants in pre-Action stages are outlined below.\n\nA national sample of pre-Action adults was provided a stress management intervention. At the 18-month follow-up, a significantly larger proportion of the treatment group (62%) was effectively managing their stress when compared to the control group. The intervention also produced statistically significant reductions in stress and depression and an increase in the use of stress management techniques when compared to the control group. Two additional clinical trials of TTM programs by Prochaska et al. and Jordan et al. also found significantly larger proportions of treatment groups effectively managing stress when compared to control groups.\n\nOver 1,000 members of a New England group practice who were prescribed antihypertensive medication participated in an adherence to antihypertensive medication intervention. The vast majority (73%) of the intervention group who were previously pre-Action were adhering to their prescribed medication regimen at the 12-month follow-up when compared to the control group.\n\nMembers of a large New England health plan and various employer groups who were prescribed a cholesterol lowering medication participated in an adherence to lipid-lowering drugs intervention. More than half of the intervention group (56%) who were previously pre-Action were adhering to their prescribed medication regimen at the 18-month follow-up. Additionally, only 15% of those in the intervention group who were already in Action or Maintenance relapsed into poor medication adherence compared to 45% of the controls. Further, participants who were at risk for physical activity and unhealthy diet were given only stage-based guidance. The treatment group doubled the control group in the percentage in Action or Maintenance at 18 months for physical activity (43%) and diet (25%).\n\nParticipants were 350 primary care patients experiencing at least mild depression but not involved in treatment or planning to seek treatment for depression in the next 30 days. Patients receiving the TTM intervention experienced significantly greater symptom reduction during the 9-month follow-up period. The intervention's largest effects were observed among patients with moderate or severe depression, and who were in the Precontemplation or Contemplation stage of change at baseline. For example, among patients in the Precontemplation or Contemplation stage, rates of reliable and clinically significant improvement in depression were 40% for treatment and 9% for control. Among patients with mild depression, or who were in the Action or Maintenance stage at baseline, the intervention helped prevent disease progression to Major Depression during the follow-up period.\n\nFifty-hundred-and-seventy-seven overweight or moderately obese adults (BMI 25-39.9) were recruited nationally, primarily from large employers. Those randomly assigned to the treatment group received a stage-matched multiple behavior change guide and a series of tailored, individualized interventions for three health behaviors that are crucial to effective weight management: healthy eating (i.e., reducing calorie and dietary fat intake), moderate exercise, and managing emotional distress without eating. Up to three tailored reports (one per behavior) were delivered based on assessments conducted at four time points: baseline, 3, 6, and 9 months. All participants were followed up at 6, 12, and 24 months. Multiple Imputation was used to estimate missing data. Generalized Labor Estimating Equations (GLEE) were then used to examine differences between the treatment and comparison groups. At 24 months, those who were in a pre-Action stage for healthy eating at baseline and received treatment were significantly more likely to have reached Protons or Maintenance than the comparison group (47.5% vs. 34.3%). The intervention also impacted a related, but untreated behavior: fruit and vegetable consumption. Over 48% of those in the treatment group in a pre-Action stage at baseline progressed to Action or Maintenance for eating at least 5 servings a day of fruit and vegetables as opposed to 39% of the comparison group. Individuals in the treatment group who were in a pre-Action stage for exercise at baseline were also significantly more likely to reach Action or Maintenance (44.9% vs. 38.1%). The treatment also had a significant effect on managing emotional distress without eating, with 49.7% of those in a pre-Action stage at baseline moving to Action or Maintenance versus 30.3% of the comparison group. The groups differed on weight lost at 24 months among those in a pre-action stage for healthy eating and exercise at baseline. Among those in a pre-Action stage for both healthy eating and exercise at baseline, 30% of those randomized to the treatment group lost 5% or more of their body weight vs.18.6% in the comparison group. Coaction of behavior change occurred and was much more pronounced in the treatment group with the treatment group losing significantly more than the comparison group. This study demonstrates the ability of TTM-based tailored feedback to improve healthy eating, exercise, managing emotional distress, and weight on a population basis. The treatment produced the highest population impact to date on multiple health risk behaviors.\n\nMultiple studies have found individualized interventions tailored on the 14 TTM variables for smoking cessation to effectively recruit and retain pre-Action participants and produce long-term abstinence rates within the range of 22% – 26%. These interventions have also consistently outperformed alternative interventions including best-in-class action-oriented self-help programs, non-interactive manual-based programs, and other common interventions. Furthermore, these interventions continued to move pre-Action participants to abstinence even after the program ended. For a summary of smoking cessation clinical outcomes, see Velicer, Redding, Sun, & Prochaska, 2007 and Jordan, Evers, Spira, King & Lid, 2013.\n\nIn the treatment of smoke control, TTM focuses on each stage to monitor and to achieve a progression to the next stage.\n\nIn each stage, a patient may have multiple sources that could influence their behavior. These may include: friends, books, and interactions with their healthcare providers. These factors could potentially influence how successful a patient may be in moving through the different stages. This stresses the importance to have continuous monitoring and efforts to maintain progress at each stage. TTM helps guide the treatment process at each stage, and may assist the healthcare provider in making an optimal therapeutic decision.\n\nThe use of TTM in travel behaviour interventions is rather novel. A number of cross-sectional studies investigated the individual constructs of TTM, e.g. stage of change, decisional balance and self-efficacy, with regards to transport mode choice. The cross-sectional studies identified both motivators and barriers at the different stages regarding biking, walking and public transport. The motivators identified were e.g. liking to bike/walk, avoiding congestion and improved fitness. Perceived barriers were e.g. personal fitness, time and the weather. This knowledge was used to design interventions that would address attitudes and misconceptions to encourage an increased use of bikes and walking. These interventions aim at changing people's travel behaviour towards more sustainable and more active transport modes. In health-related studies, TTM is used to help people walk or bike more instead of using the car. Most intervention studies aim to reduce car trips for commute to achieve the minimum recommended physical activity levels of 30 minutes per day. Other intervention studies using TTM aim to encourage sustainable behaviour. By reducing single occupied motor vehicle and replacing them with so called sustainable transport (public transport, car pooling, biking or walking), greenhouse gas emissions can be reduced considerably. A reduction in the number of cars on our roads solves other problems such as congestion, traffic noise and traffic accidents. By combining health and environment related purposes, the message becomes stronger. Additionally, by emphasising personal health, physical activity or even direct economic impact, people see a direct result from their changed behaviour, while saving the environment is a more general and effects are not directly noticeable.\n\nDifferent outcome measures were used to assess the effectiveness of the intervention. Health-centred intervention studies measured BMI, weight, waist circumference as well as general health. However, only one of three found a significant change in general health, while BMI and other measures had no effect. Measures that are associated with both health and sustainability were more common. Effects were reported as number of car trips, distance travelled, main mode share etc. Results varied due to greatly differing approaches. In general, car use could be reduced between 6% and 55%, while use of the alternative mode (walking, biking and/or public transport) increased between 11% and 150%. These results indicate a shift to action or maintenance stage, some researchers investigated attitude shifts such as the willingness to change. Attitudes towards using alternative modes improved with approximately 20% to 70%. Many of the intervention studies did not clearly differentiate between the five stages, but categorised participants in pre-action and action stage. This approach makes it difficult to assess the effects per stage. Also, interventions included different processes of change; in many cases these processes are not matched to the recommended stage. It highlights the need to develop a standardised approach for travel intervention design. Identifying and assessing which processes are most effective in the context of travel behaviour change should be a priority in the future in order to secure the role of TTM in travel behaviour research.\n\nThe TTM has been called \"arguably the dominant model of health behaviour change, having received unprecedented research attention, yet it has simultaneously attracted criticism\". Depending on the field of application (e.g. smoking cessation, substance abuse, condom use, diabetes treatment, obesity and travel) somewhat different criticisms have been raised.\n\nIn a systematic review, published in 2003, of 23 randomized controlled trials, the authors found that \"stage based interventions are no more effective than non-stage based interventions or no intervention in changing smoking behaviour. However, it was also mentioned that stage based interventions are often used and implemented inadequately in practice. Thus, criticism is directed towards the use rather the effectiveness of the model itself. Looking at interventions targeting smoking cessation in pregnancy found that stage-matched interventions were more effective than non-matched interventions. One reason for this was the greater intensity of stage-matched interventions. Also, the use of stage-based interventions for smoking cessation in mental illness proved to be effective. Further studies, e.g. a randomized controlled trial published in 2009, found no evidence that a TTM based smoking cessation intervention was more effective than a control intervention not tailored to stage of change. The study claims that those not wanting to change (i.e. precontemplators) tend to be responsive to neither stage nor non-stage based interventions. Since stage-based interventions tend to be more intensive they appear to be most effective at targeting contemplators and above rather than pre-contemplators. A 2010 systematic review of smoking cessation studies under the auspices of the Cochrane Collaboration found that \"stage-based self-help interventions (expert systems and/or tailored materials) and individual counselling were neither more nor less effective than their non-stage-based equivalents.\n\nMain criticism is raised regarding the \"arbitrary dividing lines\" that are drawn between the stages. West claimed that a more coherent and distinguishable definition for the stages is needed. Especially the fact that the stages are bound to a specific time interval is perceived to be misleading. Additionally, the effectiveness of stage-based interventions differs depending on the behavior. A continuous version of the model has been proposed, where each process is first increasingly used, and then decreases in importance, as smokers make progress along some latent dimension. This proposal suggests the use of processes without reference to stages of change.\n\nThe model \"assumes that individuals typically make coherent and stable plans\", when in fact they often do not.\n\nWithin research on prevention of pregnancy and sexually transmitted diseases a systematic review from 2003 comes to the conclusion that \"no strong conclusions\" can be drawn about the effectiveness of interventions based on the transtheoretical model. Again this conclusion is reached due to the inconsistency of use and implementation of the model. This study also confirms that the better stage-matched the intervention the more effect it has to encourage condom use.\n\nWithin the health research domain, a 2005 systematic review of 37 randomized controlled trials claims that \"there was limited evidence for the effectiveness of stage-based interventions as a basis for behavior change. Studies with which focused on increasing physical activity levels through active commute however showed that stage-matched interventions tended to have slightly more effect than non-stage matched interventions. Since many studies do not use all constructs of the TTM, additional research suggested that the effectiveness of interventions increases the better it is tailored on all core constructs of the TTM in addition to stage of change. In diabetes research the \"existing data are insufficient for drawing conclusions on the benefits of the transtheoretical model\" as related to dietary interventions. Again, studies with slightly different design, e.g. using different processes, proved to be effective in predicting the stage transition of intention to exercise in relation to treating patients with diabetes.\n\nTTM has generally found a greater popularity regarding research on physical activity, due to the increasing problems associated with unhealthy diets and sedentary living, e.g. obesity, cardiovascular problems. A 2011 Cochrane Systematic Review found that there is little evidence to suggest that using the Transtheoretical Model Stages of Change (TTM SOC) method is effective in helping obese and overweight people lose weight. Earlier in a 2009 paper, the TTM was considered to be useful in promoting physical activity. In this study, the algorithms and questionnaires that researchers used to assign people to stages of change lacked standardisation to be compared empirically, or validated.\n\nSimilar criticism regarding the standardisation as well as consistency in the use of TTM is also raised in a recent review on travel interventions. With regard to travel interventions only stages of change and sometimes decisional balance constructs are included. The processes used to build the intervention are rarely stage-matched and short cuts are taken by classifying participants in a pre-action stage, which summarises the precontemplation, contemplation and preparation stage, and an action/maintenance stage. More generally, TTM has been criticised within various domains due to the limitations in the research designs. For example, many studies supporting the model have been cross-sectional, but longitudinal study data would allow for stronger causal inferences. Another point of criticism is raised in a 2002 review, where the model's stages were characterized as \"not mutually exclusive\". Furthermore, there was \"scant evidence of sequential movement through discrete stages\". While research suggests that movement through the stages of change is not always linear, a study conducted in 1996 demonstrated that the probability of forward stage movement is greater than the probability of backward stage movement. Due to the variations in use, implementation and type of research designs, data confirming TTM are ambiguous. More care has to be taken in using a sufficient amount of constructs, trustworthy measures, and longitudinal data.\n\n\nThe following notes summarize major differences between the well-known 1983, 1992, and 1997 versions of the model. Other published versions may contain other differences. For example, Prochaska, Prochaska, and Levesque (2001) do not mention the Termination stage, Self-efficacy, or Temptation.\n\n"}
{"id": "5074109", "url": "https://en.wikipedia.org/wiki?curid=5074109", "title": "Venezuelan equine encephalitis virus", "text": "Venezuelan equine encephalitis virus\n\nVenezuelan equine encephalitis virus is a mosquito-borne viral pathogen that causes Venezuelan equine encephalitis or encephalomyelitis (VEE). VEE can affect all equine species, such as horses, donkeys, and zebras. After infection, equines may suddenly die or show progressive central nervous system disorders. Humans also can contract this disease. Healthy adults who become infected by the virus may experience flu-like symptoms, such as high fevers and headaches. People with weakened immune systems and the young and the elderly can become severely ill or die from this disease.\n\nThe virus that causes VEE is transmitted primarily by mosquitoes that bite an infected animal and then bite and feed on another animal or human. The speed with which the disease spreads depends on the subtype of the VEE virus and the density of mosquito populations. Enzootic subtypes of VEE are diseases endemic to certain areas. Generally these serotypes do not spread to other localities. Enzootic subtypes are associated with the rodent-mosquito transmission cycle. These forms of the virus can cause human illness but generally do not affect equine health.\n\nEpizootic subtypes, on the other hand, can spread rapidly through large populations. These forms of the virus are highly pathogenic to equines and can also affect human health. Equines, rather than rodents, are the primary animal species that carry and spread the disease. Infected equines develop an enormous quantity of virus in their circulatory system. When a blood-feeding insect feeds on such animals, it picks up this virus and transmits it to other animals or humans. Although other animals, such as cattle, swine, and dogs, can become infected, they generally do not show signs of the disease or contribute to its spread.\n\nThe virion is spherical and approximately 70 nm in diameter. It has a lipid membrane with glycoprotein surface proteins spread around the outside. Surrounding the nuclear material is a nucleocapsid that has an icosahedral symmetry of T = 4, and is approximately 40 nm in diameter.\n\nSerology testing performed on this virus has shown the presence of six different subtypes (classified I to VI). These have been given names, including Mucambo, Tonate, and Pixuna subtypes. There are seven different variants in subtype I, and three of these variants, A, B, and C are the epizootic strains.\n\nThe Mucambo virus (subtype III) appears to have evolved ~1807 AD (95% credible interval: 1559–1944). In Venezuela the Mucambo subtype was identified in 1975 by Jose Esparza and J. Sánchez using cultured mosquito cells.\n\nIn the Americas, there have been 21 reported outbreaks of Venezuelan Equine Encephalitis Virus. Outbreaks of Venezuelan equine encephalitis virus occurred in Central American and South American countries. This virus was isolated in 1938, and outbreaks have been reported in many different countries since then. Mexico, Colombia, Venezuela, and the United States are just some of the countries that have reported outbreaks. Outbreaks of VEE generally occur after periods of heavy precipitation that cause mosquito populations to thrive.\n\nBetween December 1992 and January 1993, the Venezuelan state of Trujillo experienced an outbreak of this virus. Overall, 28 cases of the disease were reported along with 12 deaths.\nJune 1993 saw a bigger outbreak, as 55 humans died as well as 66 equine deaths.\n\nA much larger outbreak in Venezuela and Colombia occurred in 1995. On May 23, 1995, equine encephalitis-like cases were reported in the northwest portion of the country. Eventually, the outbreak spread more towards the north as well as to the south. The outbreak caused about 11,390 febrile cases in humans as well as 16 deaths. About 500 equine cases were reported with 475 deaths. Washington, D.C., U.S.A: Pan American Health Organization, Pan American Sanitary Bureau, Regional Office of the World Health Organization, 2001. Print.</ref>\n\nAn outbreak of this disease occurred on Colombia in September 1995. This outbreak resulted in 14,156 human cases that were attributable to Venezuelan equine encephalitis virus with 26 human deaths. A possible explanation for the serious outbreaks was the particularly heavy rain that had fallen. This could have caused increased numbers of mosquitoes that could serve as vectors for the disease. A more likely explanation is that deforestation caused a change in mosquito species. \"Culex taenopius\" mosquitos, which prefer rodents, were replaced by \"Aedes taeniorhynchus\" mosquitos, which are more likely to bite humans and large equines.\n\nThough the majority of VEE outbreaks occur in Central and South America, the virus has potential to outbreak again in the United States. It has been shown the invasive mosquito species \"Aedes albopictus\" is a viable carrier of VEEV.\n\nThere is an inactivated vaccine containing the C-84 strain for VEEV that is used to immunize horses. Another vaccine, containing the TC-83 strain, is only used on humans in military and laboratory positions that risk contracting the virus. The human vaccine can result in side effects and does not fully immunize the patient. The TC-83 strain is generated by passing the virus 83 times through the heart cells of a guinea pig; C-84 is a derivative of TC-83.\n\nDuring the Cold War, both the United States biological weapons program and the Soviet biological weapons program researched and weaponized VEE.\n\nIn April 2009, the U.S. Army Medical Research Institute of Infectious Diseases at Fort Detrick reported that samples of Venezuelan equine encephalitis virus were discovered missing during an inventory of a group of samples left by a departed researcher. The report stated the samples were likely among those destroyed when a freezer malfunctioned.\n\n\n"}
{"id": "9156657", "url": "https://en.wikipedia.org/wiki?curid=9156657", "title": "Vocational Rehabilitation Act of 1973", "text": "Vocational Rehabilitation Act of 1973\n\nThe Vocational Rehabilitation Act of 1973 Title V, was put in place to correct the problem of discrimination against people with disabilities in the United States. Affirmative action programs were established in Title V, Sections 501, 502, 503, and 504. Individuals who qualify as having a disability have experienced discrimination both because of negative attitudes in regard to their ability to be an effective employee, as well as the physical barriers at work facilities. The Title V of the Vocational Rehabilitation Act requires private employers with federal contracts over $2,500 to take affirmative action to hire individuals with a mental or physical disability. While this means that employers must make reasonable accommodations for disabled employees, it does not mean they must hire unqualified individuals. There are additional sections of the Act that provide vocational counseling, training assistance and job placement for individuals with severe disabilities\n\nIn the context of the Vocational Rehabilitation Act, the term \"disabled individual\" means \"any person who (1) has a physical or mental impairment which substantially limits one or more of such person's major life activities, (2) has a record of such impairment, or (3) is regarded as having such an impairment.\" This definition is closely related to the definition provided by the Americans with Disabilities Act.\n\nIn the 1987 Supreme Court decision concerning the case Nassau County, Florida v Arline, it was ruled that employees that were infected with contagious diseases, such as tuberculosis, are considered disabled individuals and are therefore subject to the acts coverage. So long as the person is still qualified to do the job, employers are required to make reasonable accommodations to allow the disabled to continue working. Individuals with AIDS are also considered disabled and are covered under the act. Recent public interest in AIDS has presented organizational management with the challenge of how to address work-related concerns about AIDS.\n\nThe Vocational Rehabilitation Act does not require employers to hire or retain a disabled person if the individual has a contagious disease that poses a direct threat to the health and safety of others and the individual cannot be accommodated. Also employment is not required if the disability prevents the individual from being able to perform a required part of the job, or if the individual would be considered unqualified for the job regardless of their disease.\n\n"}
{"id": "8629677", "url": "https://en.wikipedia.org/wiki?curid=8629677", "title": "Water supply and sanitation in Uruguay", "text": "Water supply and sanitation in Uruguay\n\n\"This article was last comprehensively updated in September 2007. Please feel free to update it if need be.\"\n\nUruguay is the only country in Latin America that has achieved quasi-universal coverage of access to safe drinking water supply and adequate sanitation. Water service quality is considered good, with practically all localities in Uruguay receiving disinfected water on a continuous basis. 70% of wastewater collected by the national utility was treated. Given these achievements, the government's priority is to improve the efficiency of services and to expand access to sewerage, where appropriate, in areas where on-site sanitation is used.\n\nWater and sanitation coverage in Uruguay (2004)\n\n\"Source\": WHO/UNICEF Joint Monitoring Programme (JMP/2006). Data for water and sanitation based on the WHO World Health Survey (2003).\n\nPer capita water production is high at 411 liter/capita/day (90.4 Imperial gallons/c/d, 108.6 US gallons/c/d). Even after taking into account non-revenue water of 54%, at 183 liter/capita/day (40.3 Imperial gallons/c/d, 48.3 US gallons/c/d) it is still higher than in many European countries. However, water use is much lower than in neighboring Argentina, where metering is not widespread, while in Uruguay 96% of water connections were metered in 2004.\n\nLike many other developing countries Uruguay sought private sector participation in water supply and sanitation to improve efficiency and service quality. This was done through two concessions for secondary cities in the department of Maldonado, home to many tourist resorts and the town Punta del Este. The first concession was granted in 1993 to Aguas de la Costa, a Uruguayan firm which later became majority-owned by Aguas de Barcelona, itself a subsidiary of the French firm Suez. The second concession was granted in 2000 to URAGUA, a subsidiary of Aguas de Bilboa of Spain.\n\nTo complement the policy of private sector participation, the government created in 2002 the utility regulatory agency URSEA covering the power and water sector.\n\nNevertheless, the private concessions remained contentious. Following a vigorous campaign against them and allegations of overcharging and poor service quality, Parliament passed a constitutional amendment in October 2004 prohibiting any form of private sector participation in the water sector. As a result, the concession of URAGUA was withdrawn in the same year.\n\nIn 2005 the government passed law 17.930 with the objective of improving the effective participation of users and civil society in planning, management and control of activities in the sector. For that purpose the law established a water and sanitation directorate (DINASA) in the Ministry of Housing and Environment, as well as a Water and Sanitation Advisory Commission (COASAS).\n\nMeanwhile, the Uruguayan and foreign owners of Aguas de la Costa refused to yield to demands for nationalization. In 2006 the government bought the shares held by Aguas de Barcelona. From then on the enterprise operated as a mixed public-private enterprise with a majority public shareholding. In 2009 the company became 100% owned by the state.\n\nThe state-owned national utility, Administración de las Obras Sanitarias del Estado (OSE) provides water and sewer services to all of Uruguay with the exception of Montevideo, where the municipality provides sewerage and OSE provides water services only. OSE serves 330 localities with 2.8 million inhabitants with water services and 152 localities with 0.5 million inhabitants with sewer services. It had 4,362 employees in 2004. \n\nTo enhance sector performance, new institutions have been recently established, including the Regulatory Entity for Energy and Water (URSEA); the National Directorate of Water and Sanitation (DINASA) in the Ministry of Housing, Land Management and Environment, responsible for creating national sector policies on WSS; and the Advisory Commission on Water and Sanitation (COASAS). \n\nThe government of Uruguay intends to establish a comprehensive legal and regulatory framework for the water supply and sanitation sector through a new law. It also plans to develop a policy on appropriate sanitation standards and to further improve the efficiency of OSE by stimulating internal competition and reducing unaccounted for water. Concerning internal competition, OSE has introduced an internal benchmarking system comparing the utility’s performance across 21 cities based on 9 service quality indicators.\n\nBetween 1999 and 2005 OSE successfully increased labor productivity from 5.6 employees/1000 connections to 4.5, decreased operating costs from US$0.98/m³ (US$0.75/cu yd) to US$0.71/m³ (US$0.54/cu yd); and increased operating margins from 35% to 42%. However, OSE’s performance continues to present areas of inefficiency. Reducing unaccounted for water, which remains around 54%, will continue to require a concerted effort.\n\nWater and sewer tariffs charged by OSE differ depending on the category of users, with lower tariffs for residential users than for other users (commercial, industrial, official and public enterprises). Water and sewer tariffs in Uruguay have increased substantially since 1995. For example, a typical residential water bill (20 m³/month, 700 cu ft/month) for OSE consumers increased from the equivalent of 56 pesos/month (US$9.50) in 1995 to 207 pesos/month (US$18.20) in 2001, a 93% increase in US dollar terms in six years. Tariffs were further increased so that a typical residential bill reached 431 pesos/month in 2003. But due to the massive devaluation of 2001 the US dollar equivalent of the monthly bill decreased to US$15/month. In 2007 a monthly residential water bill was estimated to be at least 568 pesos, equivalent to more than US$22/month. \n\nGiven that the average monthly income of a household in the two lowest-income quintiles (a lower-middle income family) was about the equivalent of US$190 in 2003, water and sewer tariffs accounted for almost 8% of their income, an extraordinarily high percentage, which has probably further increased since then. Despite their very high levels, these tariffs are not subject to much public protest or complaints.\nThe national water and sewer company Administración de las Obras Sanitarias del Estado (OSE), created in 1952, does not receive direct subsidies from the government. OSE finances its investments from internal revenues and loans. However, OSE’s financial health had been in decline during the 1990s and early 2000 due to high levels of operational inefficiency, thereby threatening OSE’s ability to carry out future required investments. OSE’s operating performance and financial health have since begun improving.\n\nThe total expenditure of OSE between 1990 and 2005 was US$797 million, which is on annual average 0.24% of the Uruguayan GDP or US$15.3 per capita. The annual investment was highest at the end of the 1990s, reaching US$30.8 in 1996 and US$31.2 per capita in 1999. Since 2001, it fell back to only US$5.1 per capita in 2003. The average annual investment per capita between 1997 and 2003 was higher than in other Latin American countries like Argentina, Peru, Colombia and Mexico.\n\n\n"}
{"id": "23607513", "url": "https://en.wikipedia.org/wiki?curid=23607513", "title": "Wellhead protection area", "text": "Wellhead protection area\n\nA wellhead protection area is a surface and subsurface land area regulated to prevent contamination of a well or well-field supplying a public water system. This program, established under the Safe Drinking Water Act (42 U.S.C. 330f-300j), is implemented through state governments.\n\nIn 1986, the Environmental Protection Agency amended the Safe Drinking Water Act to include the Wellhead Protection Program. This was first enforced in 1990 in New England.\n\nPlans for protection of groundwater are developed by state governments according to location of wells, and potential threats from contaminants. The area designated can be determined by the well's ability to pump water as well as the quality of the source aquifer.\nWhile states are allowed to create their own programs, they must be submitted to and approved by the EPA before going into effect. The EPA requires state's proposals to include plans of action in case of contamination, regular testing of the source water, and management. They also are required to document data such as flow rate and direction, and groundwater levels.\n\nWellhead protection areas are important to designate because they can caution planners when building in those areas and keep control of potential drinking water contaminants. Planners will conduct research of what contaminants are most prominent and where they come from. These typically have a source of industrial or agricultural human activities. As part of the protection plan, states will have a backup source for drinking water in case of emergency such as well destruction or contamination.\n\nThe program is not funded federally, which can cause issues for some states.\n\n"}
{"id": "51214874", "url": "https://en.wikipedia.org/wiki?curid=51214874", "title": "World FUE Institute", "text": "World FUE Institute\n\nThe World FUE Institute (founded on February 22, 2016; Brussels, Belgium) is an educationally and scientifically driven non-profit organization of international medical professionals whose aim is to promote precision, artistry, competence, diligence and proficiency in the field of hair restoration using FUE (Follicular Unit Extraction) techniques and methodology.\nIn addition, the World FUE Institute supports education and scientific research in the area of FUE inspiring new ideas, evoking insights and encouraging innovation.\n\nThe World FUE Institute consists of an executive committee (president, vice-president, treasurer and secretary) and a governing board as well as general membership from around the world. Committees include Membership and Nominating, Finance, Scientific Research and Education, Workshop and Meeting, FUE Technique and Standards, and Communications, and Bylaws and Public Relations committees.\n\nThe World FUE Institute produces guidelines for the profession through scientific data that has been gathered from its lite membership. Guidelines aim to provide physicians with relevant evidence of perils and benefits in the application of certain techniques and the use of certain tools.\n\nThe World FUE Institute organizes annual educational meetings and workshops that are specifically aimed at physicians who are already performing FUE and wish to improve and update their techniques and use of cutting edge instrumentation, and those who are just entering the field.\n\nThe first World FUE Institute’s international workshop including live surgeries will be held in November 2016 in the Canary Islands (Spain).\n\nA Fellow of the World FUE Institute is a duly licensed medical physician according to the standards set forth in his/her own country who has completed at least three years of experience performing FUE (Follicular Unit Extraction) procedures and who has distinguished himself/herself in the professional application of FUE. Membership as a Fellow is considered a privilege and is therefore offered to potential candidates by invitation of the Governing Board. Fellow Members are entitled to the use of the logo and have access to exclusive material.\n\nOther membership categories include General Membership for those physicians who have not yet completed three years of FUE experience, Surgical Assistant Membership for surgical assistants to Fellow members and Adjunct Members for those holding a PhD and who are heavily involved in the research of hair restoration using FUE methodology.\n\nDr. Jose Lorenzo (Spain), Dr.Koray Erdogan (Turkey), Dr. Bijan Feriduni (Belgium), Dr. Emorane Lupanzula (Belgium), Dr. Alex Ginzburg (Israel) and Dr. Hussain Rahal (Canada)\nGoverning board members 2016–2018\n"}
{"id": "5246553", "url": "https://en.wikipedia.org/wiki?curid=5246553", "title": "Wrong Planet", "text": "Wrong Planet\n\nWrong Planet (sometimes referred to by its URL, wrongplanet.net) is an online community for individuals with autism spectrum disorders. The site was started in 2004 by Dan Grover and Alex Plank and includes a chatroom, a forum, and articles describing how to deal with daily issues. Wrong Planet has been referenced by the mainstream U.S. media. Wrong Planet comes up in the special education curriculum of many universities in the United States. A page is dedicated to Wrong Planet and its founder in \"Exceptional Learners: Introduction to Special Education\".\n\nIn 2006, Alex Plank was sued by the victims of a 19-year-old member of the site, William Freund, who shot two people and himself in Aliso Viejo, California after openly telling others on the site that he planned to do so.\n\nIn 2007, a man who was accused of murdering his dermatologist posted on the site while eluding the police. Wrong Planet was covered in a Dateline NBC report on the incident.\n\nIn 2008, Wrong Planet began getting involved in autism self-advocacy with the goal of furthering the rights of autistic individuals living in the United States. Alex Plank, representing the site, testified at the Health and Human Services's Interagency Autism Coordinating Committee.\n\nIn 2010, Wrong Planet created a television show about autism called \"Autism Talk TV\". Sponsors of this web series include Autism Speaks. The show is hosted by Alex Plank and Jack Robison, the son of author John Elder Robison. Neurodiversity advocates have accused Plank of betraying Wrong Planet's goal for autism acceptance by accepting money from Autism Speaks for this web series.\n\n"}
