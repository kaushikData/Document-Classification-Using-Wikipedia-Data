{"id": "25946734", "url": "https://en.wikipedia.org/wiki?curid=25946734", "title": "Abortion in Spain", "text": "Abortion in Spain\n\nAbortion in Spain is legal with some restrictions. Abortion during the first trimester is legal upon request. However, abortion during the second trimester is legal only for serious risk to the health of the woman or fetal defects.\n\nAbortion legislation in Spain has a varied history. During the 1930s, abortion law was liberalized in the area controlled by the Republicans, but this was short-lived, as the Franco regime with support of the Catholic Church, outlawed abortion again. The laws were relaxed in 1985, and were further liberalized in 2010. Abortion remains a controversial political issue in Spain, and regular attempts to restrict it occur but they have failed to get enough traction. \n\nIn recent years, abortion rates have been falling, as better access to emergency contraception has been introduced.\n\nVoluntary Interruption of Pregnancy Induced Abortion in Spain is regulated under Title II of the Organic Law 2/2010 of sexual and reproductive health and abortion. This law legalizes the practice of abortion during the first 14 weeks of pregnancy. The law came into force on 5 July 2010. The previous regulation - Organic Law 9/1985<ref name=\"Ley Orgánica 9/1985\">Ley Orgánica 9/1985</ref> - decriminalized abortion on several points. The conservative People's Party in June 2010 filed an action against several provisions of law to the Constitutional Court, which has not yet been pronounced. In the electoral program for the general election held on 20 November 2011, the People's Party included modifying the law on abortion.\n\nIn Spain, induced abortion has been a practice totally banned, illegal and punishable except for a period of the Second Republic and from the 9/1985 and the last 2/2010 law acts which, to varying degrees, have decriminalized abortion.\n\nFor authors such as Ibáñez and García Velasco, prohibition and criminalization of abortion did not prevent about 100,000 abortions from happening a year. From this point of view, punishment would not solve the problem, as hiding contributed decisively in the death of women who practiced abortion in conditions of illegality and legal and sanitary uncertainty. In 1976, according to the Supreme Court, between 200 and 400 women died from illegal abortions.\n\nMoreover, advocates of legally induced abortion argue that the problem of hiding mainly affects poor women with fewer economic resources, as women in a better social position would have resorted to the activity known as \"abortion tourism\", meaning they travel to other countries to get an abortion.\n\nConsidering the point of view presented as being against abortion, the arguments offered for banning it include that it is a criminal and murderous practice.\n\nIn any case, both the supporters of legalization, as well as its detractors, put the bulk of their argument in defense of life, firstly in the life of the mother, secondly in the life of the unborn child.\n\nIn both cases, virtually all the Spanish people interviewed, have been in favor of more social awareness on abortion and the need for government to regulate its intervention.\n\nOn 25 December 1936, in Catalonia, free abortion was legalized during the first 12 weeks of pregnancy with a decree signed by and published on 9 January 1937 ().\n\nIn the area loyal to the Republic during the Spanish Civil War, in 1937, the Minister for Health being the Catalan anarchist (CNT) Federica Montseny in the socialist Spanish Socialist Workers' Party (PSOE) government headed by Francisco Largo Caballero legalized the practice of abortion, but its effect was short-lived, as the Franco side repealed.\n\nIn the Organic Law 9/1985, adopted on 5 July 1985, induced abortion was legalized in three cases: serious risk to the physical or mental health of the pregnant woman (supposedly therapeutic), rape (supposedly criminology), and malformations or defects, physical or mental, in the fetus (supposedly eugenics).\n\nAccording to this law, the mother could terminate the pregnancy in public or private health centres in the first 12 weeks for reasons related to criminology, in the first 22 weeks in eugenics, and at any time during pregnancy for therapeutic reasons.\nIn the second and third cases, a medical report was required to certify compliance with the conditions laid down by law, in cases of rape, it was necessary to take the relevant prior police report. In these three cases, abortion was not punishable if undertaken by a doctor, or under their supervision, in a medical establishment approved for abortions, whether public or private, with the express consent of the woman. In other cases, the Penal Code provided various terms of imprisonment for both the mother and the doctors who performed abortions outside of the law.\n\nOn 3 March 2010, the Organic Law 2/2010 of sexual and reproductive health and abortion was promulgated. This law is to ensure fundamental rights in the field of sexual and reproductive health established by the World Health Organization (WHO), regulating the conditions of abortion and establishing the corresponding obligations of public authorities. The law came into force on 5 July 2010.\n\nIn Title II, Articles 13 and 14, the decriminalization of the practice of abortion during the first 14 weeks of pregnancy is specified. During this time, the woman can take a free and informed decision on the termination of her pregnancy. There will be no third party intervention in the decision.\n\nArticle 15 describes that the term of the possibility of abortion increases up to 22 weeks in cases of \"serious risks to life or health of the mother or fetus\". From the twenty-second week, pregnancy may be interrupted only on two assumptions: that \"fetal anomalies incompatible with life are detected\" or that \"an extremely serious and incurable disease is detected within the fetus at the time of diagnosis and is confirmed by a clinical committee\".\n\nArticle 13. Common requirements.\nThese are the requirements of the voluntary termination of pregnancy:\nThis information may be dispensed with when the minor reasonably claims that this will cause a serious conflict, manifested in certain danger of family violence, threats, coercion, abuse, or a situation of homelessness.\n\nArticle 14. Termination of pregnancy at the request of the woman.\n\nPregnancy can be terminated within the first fourteen weeks of gestation at the request of the pregnant woman, provided that these requirements have been followed:\n\na) the pregnant woman has been informed on the rights, benefits and public aid to mothers, on the terms set forth in paragraphs 2 and 4 of Article 17 of this Act\n\nb) the pregnant woman has completed a period of at least three days, from the moment the information was given to her mentioned in the previous paragraph to the realization of the intervention.\n\nIn 2009, a reform of the 1985 law that regulated abortion was processed based on three cases delimited by a new law that would permit, under any circumstances, intervention during the first 14 weeks of gestation, and until week 22 if there is serious risk to the life or health of the pregnant woman or risk of serious abnormalities to the fetus. In case of detection of fetal anomalies incompatible with life, there would be no time limit for abortion. The new law would also allow young people between 16 and 17 to have an abortion without requiring parental consent.\n\nThis reform, supported by the Spanish Socialist Party and endorsed by the Council of State, drew criticism from the conservative People's Party, the Catholic Church and anti-abortion groups.\n\nLaw 2/2010 of sexual and reproductive health and abortion was finally passed by 184 votes in favor, 158 against and one abstention. The law was supported by PSOE, the ruling party of Spain led by Jose Luis Rodriguez Zapatero and the Minister for Equality Bibiana Aido. The parties that supported the government were the Basque Nationalist Party (PNV), Republican Left of Catalonia (ERC), United Left (IU), Initiative for Catalonia Greens (ICV), Galician Nationalist Bloc (BNG), Nafarroa Bai, and two members of Convergence and Union (CiU).\n\nThe People's Party was the only party that opposed the adoption of the new law. It was also opposed by some members of other parties such as the Canarian Coalition, Navarrese People's Union (UPN), Union, Progress and Democracy (UPyD) and seven MPs from CiU. Outside parliament civil society organizations also expressed their rejection: representatives of the Spanish Episcopal Conference of the Catholic Church, Pro Life Associations and the Institute for Family Policies (IPF). In 2009, a survey on Spanish youth conducted by the Sociological Research Center or Centro de Investigaciones Sociológicas indicated that 55% of young people felt that it was only the woman who should decide the issue, one in four believed that society should place certain limits, while 15% objected to abortion in all cases.\n\nAlberto Ruiz-Gallardón, Minister of Justice for the conservative PP government led by Mariano Rajoy announced at his first appearance in parliament in January 2012, shortly after taking office, his intention to reform the Abortion Act passed by the Socialist government led by Rodriguez Zapatero, which is a law based on a time-limit model favoured by most European countries but was contested by the Catholic Church in Spain and the People's Party (especially on the issue of whether minors between 16 and 18 may abort regardless of their parents' consent), to return to the model of the 1982 Act, in which women had to argue the grounds for their decision to abort.\n\nOn 20 December 2013, the Government of Spain published its final draft law of abortion: Women undergoing abortion were to be considered \"victims\", and the practice would only be lawful in the case of rape or when there was a serious (but as yet undefined) health risk to the mother or the fetus. The likelihood of a child being born with disabilities would not be an acceptable justification for abortion.\n\nUnder the new law, women under 18 would require parental consent and parental accompaniment during relevant consultations. Those seeking abortion in Spain would need approval from two independent doctors who would not be permitted to participate in the actual procedure.\n\nThe Spanish Association of Accredited Abortion Clinics estimated that about 100,000 of the 118,000 abortions carried out in 2012 would be illegal under the new legislation. The revision was part of the 2011 election manifesto of the Party Popular, which, strongly influenced by the Roman Catholic church, was vigorously opposed by most opposition parties and women's groups, who saw it as an attack on women's rights.\n\nIn September 2014, PM Mariano Rajoy announced that the government would abandon the draft law due to lack of consensus, and that the only reform to the 2010 law that the government will seek is that 16 and 17-year-old women will require parental consent to have an abortion. As a result of this withdrawal, Minister of Justice Alberto Ruiz-Gallardón announced his resignation.\n\nIn 2009, the number of abortions was 112,000, about 4000 less than the previous year (115,812), the first time it had decreased since 1997. According to Trinidad Jimenez, then Minister for Health and Social Policy of Spain, the decline was due to over-the-counter sales in pharmacies for the so-called morning-after pill which was liberalized in late September 2009.\n\nIn Spain, the evolution of the number of abortions, according to statistics from the Ministry of Health, is as follows:\n\nInduced abortion or termination of unwanted pregnancy can be performed by two methods:\n\nMedical abortion - Using drugs or medications such as mifepristone and misoprostol.\n\nSurgical abortion - Clinic or hospital intervention : aspiration, dilation and curettage.\n\nIn Europe the use of medical abortion is generally broad, although its use varies according to countries, such as in Portugal: 67% of induced abortions, 49% in France, in England 40% and Wales, Scotland and Finland 70%. In Spain only 4% in Italy less than 4% since the beginning of the marketing of mifepristone in December 2009.\n\nAbortion was available in a restricted form from July 5, 1985. Under the previous law it was only allowed under the following conditions: to preserve the mental health of the mother (in which case two specialists have to approve); if the pregnancy was a byproduct of rape or incest reported to the police (the abortion must be performed in the first twelve weeks); if the fetus would suffer from deformities or mental handicaps upon birth (two specialists had to agree on the findings); or if the mother's physical health was in immediate danger (in which case an abortion could be performed without the consent of the woman's family physician or the woman herself).\n\nUnder the previous law, the threshold of \"endangering the mother's mental health\" was reported to be very low, making it a loophole for abortions on-demand. The abortion rate has more than doubled from 54,000 in 1998 to 112,000 in 2007.\n\nIn 2009, the Socialist government started to liberalize current abortion laws, sending a new law through the lower house of Parliament which would allow abortion on-demand for pregnancies through the fourteenth week. The government almost succeeded in lowering the age of consent for abortions to 16, but in the end the bill states that girls aged 16 and 17 must inform their parents (but do not need parental consent) for an abortion except if the girl comes from an abusive household and such news will cause more strife. An estimated one million protesters turned to the streets of Madrid in protest of the proposed abortion law changes. The law won final approval on February 24, 2010 and came into force on July 5, 2010.\n"}
{"id": "13564008", "url": "https://en.wikipedia.org/wiki?curid=13564008", "title": "Anthroponotic disease", "text": "Anthroponotic disease\n\nAn anthroponotic disease, or anthroponosis, is an infectious disease in which a disease causing agent carried by humans is transferred to other animals. It may cause the same disease or a different disease in other animals. Since humans do not generally inflict bite wounds on other animals, the method of transmissions is always a \"soft\" contact such as skin to skin transmission. An example is chytridiomycosis which can be spread by humans with the fungus on their skin handling frogs with bare hands.\n\nThe reverse situation, a disease transmitted from animals to humans, is known as zoonotic.\n\nIt can also be defined as a human-to-human infection with no animal vector.\n\nMany human diseases can be transmitted to other primates, due to their extensive biological similarities. As a result, centers that hold, treat, or involve close proximity to primates and some other kinds of animals (for example zoos, researchers, and animal hospitals), often take steps to ensure animals are not exposed to human diseases they can catch. In some cases animals are routinely immunized with the same vaccines given to humans.\n\n"}
{"id": "1558630", "url": "https://en.wikipedia.org/wiki?curid=1558630", "title": "Australian Beverages Council", "text": "Australian Beverages Council\n\nThe Australian Beverages Council, previously known as the Australian Soft Drinks Association (ASDA) is an industry group that represents the interests of Australian manufacturers, importers and distributors of non-alcoholic beverages. Their headquarters is in Waterloo, New South Wales, Australia.\n\n"}
{"id": "24395321", "url": "https://en.wikipedia.org/wiki?curid=24395321", "title": "Blue stain fungi", "text": "Blue stain fungi\n\nBlue stain fungi (also known as sap stain fungi) is a vague term including various fungi that cause dark staining in sapwood. The staining is most often blue, but could also be grey or black. Because the grouping is based solely on symptomatics, it is not monophyletic.\n\nDepending on the author, the group can include between 100–250 species of Ascomycetes and Deuteromycetes. They are usually divided into three different groups:\n\nThe major economic damage caused by the blue stain fungi is aesthetic because of the usually undesirable discoloration of wood. Some of the fungi have also been shown to have detrimental effects on the strength properties of infected wood.\n\n"}
{"id": "39527101", "url": "https://en.wikipedia.org/wiki?curid=39527101", "title": "Béla Bicsérdy", "text": "Béla Bicsérdy\n\nBéla Bicsérdy (Pest, March 20, 1872 - Billings, Montana, December 7, 1951) was a Hungarian pioneer in health culture, lifestyle reformer, alternative medicine advocate, lecturer, author of many books, athlete, supporter of rawism, fasting and holistic therapies.\n\nAccording to Bicsérdy, after doctors couldn't cure him from his illnesses, he became a raw foodist and took long fasts. Bicsérdy claimed that he cured himself from all his illnesses, and his hair and lost teeth grew back.\n\nIn the 1920s, Bicsérdy inspired a great many people in Transylvania and Hungary promoting a regime of a raw vegan diet (mostly fruits with some bread and raw milk), regular fasting, sunbathing, daily outdoor exercising and regular water-cure. He claimed that any person who followed his regime will be cured of illness and will live hundreds of years, just as known possible from ancient times (the Bible for example reported many long-lived people).\n\nBicsérdy delivered lectures throughout Transylvania where he had his greatest following. The method became known as \"bicsérdism\" and by 1925 he had 120 to 150 thousand followers in Transylvania alone. He wrote a popular book on the subject which was inspired by mazdaznan philosophy, and in which he compared his own beliefs with the Zoroastrian \"Zend Avesta\". In addition to the book there was a periodical that was first published in 1925, as \"On Behalf of Mankind\" and later under the title \"Bicsérdism\". After a few people died due to long fasting he received serious criticism. At the end of the 1920s, he withdrew from public life. By the mid 1940s, \"bicsérdism\" had largely been forgotten. \n\nAt the end of the World War II he moved to Germany, and then in 1951 to the USA, where he died in the same year, at the age of 79.\n\n"}
{"id": "56130440", "url": "https://en.wikipedia.org/wiki?curid=56130440", "title": "Cannabis in Brunei", "text": "Cannabis in Brunei\n\nCannabis in Brunei is illegal and trafficking can be punished with the death penalty. Crystal meth and herbal cannabis are the primary drugs in the nation. In 2004, Malaysian national Lam Ming Hwa was executed for possessing a 922-gram slab of cannabis.\n"}
{"id": "52356241", "url": "https://en.wikipedia.org/wiki?curid=52356241", "title": "Cannabis in Thailand", "text": "Cannabis in Thailand\n\nIn Thailand, cannabis, known by the name \"ganja\", is listed as a class-5 narcotic under the Narcotics Act, B.E. 2522 (1979). \n\nCannabis appears to have been introduced to Thailand by the Indians, with the similarity of the Thai term \"kancha\" to the Indian term \"ganja\" cited as evidence.\n\nPrior to 1979, the possession, sale, and use of cannabis was criminalised by the Cannabis Act, B.E. 2477 (1935).\n\nPossession, cultivation, and transport (import/export) of up to 10 kg cannabis may result in a maximum sentence of 5 years in prison and/or a fine. Possession, cultivation, and transport of more than 10 kg is punishable by 2–15 years in prison and/or a fine. For the majority of people arrested for simple possession of small quantities of cannabis (\"ganja\") a fine, rather than prison time, is imposed. Narcotics police in Thailand currently view methamphetamines (ice and \"ya ba\") as a more serious issue.\n\nSentence for crime:\n\nCannabis can be found openly sold in bars and restaurants in certain parts of the country. In tourist heavy areas cannabis is commonly found, businesses openly sell \"happy\" goods which have cannabis in them. Cannabis dealers do sometimes work with police who shakedown customers and demand a bribe. Many tourists do end up in jail despite the relaxed attitude.\n"}
{"id": "54073232", "url": "https://en.wikipedia.org/wiki?curid=54073232", "title": "Clinical pharmaceutical scientist", "text": "Clinical pharmaceutical scientist\n\nA clinical pharmaceutical scientist (or pharmacist-scientist) is a licensed, practicing pharmacist who also functions as an independent researcher in the pharmaceutical sciences. Clinical pharmaceutical scientists are a type of clinician scientist, analogous to physician-scientists.\n\nThe term clinical pharmaceutical scientist is distinct from the term pharmaceutical scientist, in that a clinical pharmaceutical scientist is a practicing clinical pharmacist involved in science relating to the discovery and/or development of pharmaceuticals, the development of new knowledge improving the use of pharmaceuticals in clinical practice, or any other subfield of pharmaceutical science (e.g. pharmacoeconomics, pharmacokinetics, outcomes research), while a pharmaceutical scientist need not also be a clinician (pharmacist). Thus, all clinical pharmaceutical scientists are pharmaceutical scientists, but not all pharmaceutical scientists are clinical pharmaceutical scientists. \n\nClinical pharmaceutical scientists are both practicing pharmacists and clinical pharmacologists.\n\nMost sources attribute the origination of the term \"clinical pharmaceutical scientist\" to the 1973–1975 Millis Study Commission on Pharmacy (often referred to simply as the \"Millis Commission\"). The Millis Commission was so-named after Dr. John (Jack) S. Millis, the commission's chairman. The Millis Commission was tasked with examining the place of pharmacists in the American healthcare system, the state of professional pharmacy education at the time, and the prevalence of prescription drug misuse. The Millis Commission recommended \"training skilled pharmacy practitioners in research to increase the number and variety of clinical pharmacists,\" who were \"equally skilled and trained in a science and in pharmacy practice\", but did not go as far as to explicitly define the term \"clinical pharmaceutical scientist\" or its educational requirements. Since the Millis Commission, multiple sources have conceptualized clinical pharmaceutical scientists. The 2003 edition of the \"Encyclopedia of Clinical Pharmacy\" defines the clinical pharmaceutical scientist as the following:A clinical pharmaceutical scientist is an independent investigator with education and training in pharmacotherapeutics who utilizes contemporary research approaches to generate new knowledge relevant to drug behavior in humans, to therapeutic interventions, and/or to patient outcomes.\n\nThe history of pharmacy contains numerous individuals that dabbled in both clinical pharmacy practice and pharmaceutical science. Many major scientific discoveries in pharmacology were by pharmacists, acting as clinical pharmaceutical scientists (although not necessarily recognized by that term). For example, Friedrich Sertürner was a German pharmacist who discovered and isolated the opiate morphine from opium in 1805.\n\n\"The American Pharmacy (1852–2002): A Collection of Historical Essays\" contains a comprehensive history of American pharmacy from the late modern period to the contemporary era. In his essay \"The Pharmaceutical Sciences in America, 1852–1902\", John Parascandola writes:\n\nUnlike Europe, the United States has never produced practicing pharmacist-scientists of the caliber of Carl Scheele or Joseph Pierre Pelletier, who made important discoveries in the laboratories associated with their pharmacies. It should be noted, however, that the sciences were becoming increasingly specialized by the second half of the 19th century, and the tradition of the apothecary-scientist was on the wane even in Europe by this time.\n\nA clinical pharmaceutical scientist must have education in both clinical pharmacy and research. Broadly, the clinical pharmaceutical scientist's clinical knowledge is centered around expertise pertaining to pharmacotherapy, with less emphasis on diagnostic skills and nonpharmacologic therapies (e.g. surgery). Clinical pharmaceutical scientists must possess expertise in the pharmaceutical sciences and in the management of drug therapy problems.\n\nTo achieve this expertise, clinical pharmaceutical scientists typically pursue education after their professional pharmacy degrees (in the United States, a PharmD is generally a prerequisite). This can involve a 2–3 year postdoctoral fellowship or pursuing graduate degrees (e.g. MS, PhD). Postdoctoral programs tend to emphasize a specific area of pharmaceutical science for the scientist to specialize in, such as pharmacoepidemiology or pharmacogenetics.\n\nPostdoctoral residency training is generally recommended, though is not a prerequisite, for most fellowship programs.\n\nSome programs offer combined PharmD-PhD training. While praised as convenient, time saving, and for growing clinical pharmaceutical science, there has also been concern that the intense requirements of graduate research may take precedence over the clinical curriculum and internship/clerkship opportunities of PharmD training. As one candidate wrote in \"Pharmacy Times\", the duration of combined PharmD-PhD programs can vary, usually lasting 7–8 years in total.\n\nAccording to Robert Powell, PharmD, of the Center for Drug Evaluation and Research (a division of the FDA), pharmacists may serve as principal investigators (PI's) on clinical trials for investigational new drug applications in the United States, provided that they meet the same requirements that every other PI must meet.\n\nThere are multiple postdoctoral pathways to entering a career as a clinical pharmaceutical scientist. The pathways tend to vary by the time required and the depth of training, with programs of longer duration providing more extensive training.\n\nThe shortest program is a graduate certificate in clinical pharmaceutical research, which tend to last one year. These programs provide graduates with the ability to conduct clinical research, though not necessarily design it as principal investigators. Due to the short duration of these programs, there is less time to develop independent research skills.\n\nMaster's degree programs in clinical pharmaceutical research typically last 2–3 years, placing more emphasis on designing clinical trials.\n\nFellowship training can also last 2–3 years, though the training is thought to place more emphasis on independence. Because fellowship training can occur in non-academic environments, fellowships can also prepare individuals to begin careers in the pharmaceutical industry or with federal regulators. Fellowships do not have a didactic curriculum, and can involve additional clinical responsibilities.\n\nThe longest program is a Doctor of Philosophy (PhD) in clinical pharmaceutical research, which can last 4–5 years. The goals of PhD programs are the same as fellowship training, though the added time provides an even greater depth in a research area of interest, in addition to supplemental didactic curriculum. PhD programs generally do not mandate any clinical responsibilities, though the University of Pittsburgh School of Pharmacy piloted the addition of a clinical component for their PhD clinical pharmaceutical scientist program.\n\nWithin the pharmaceutical industry, clinical pharmaceutical scientists can pursue careers as \"clinical research officers,\" also known as \"project managers.\" In these roles, clinical pharmaceutical scientists are intimately involved with designing, conducting, and consulting on clinical trials by pharmaceutical companies. Postdoctoral pharmacy fellowships with pharmaceutical industry sponsors are traditional gateways for pharmacists to enter the industry. Careers for clinical pharmaceutical scientists within the pharmaceutical industry often involve less clinical work, and are more business and/or research intensive. Clinical pharmaceutical scientists work in the areas of drug discovery, drug formulation optimization and pharmacokinetics, pharmacoeconomics, labeling and regulatory compliance, and post-marketing research.\n\nClinical pharmaceutical scientists working in academia must have expertise in what the American Pharmacists Association calls the \"three legs\" of academia: scholarship (research), teaching, and service (clinical practice and professional development activities). Clinical pharmaceutical scientists have expertise in both research and clinical pharmacy, which is conducive to a career that demands those skills. The Commission on the Future of Graduate Education in the Pharmaceutical Sciences, appointed in 1996 by the American Association of Colleges of Pharmacy (AACP), has recommended that more American pharmacy schools develop combined PharmD/PhD programs to train clinical pharmaceutical scientists. The AACP Educating Clinical Scientists Task Force has recommended that clinical practice be combined with teaching assistantship for first-year students in clinical pharmaceutical science programs. This would effectively prepare them for all three domains of academia identified by the American Pharmacists Association above.\n\nIn the United States, clinical pharmaceutical scientists can work as researchers for governmental agencies, such as the Food and Drug Administration and the National Institute of Health.\n\nClinical pharmaceutical scientists working in the community setting face unique challenges. The private sector is focused on individual profitability, rather than performing academic research and disseminating knowledge for the profession itself. This applies to both (non-academic affiliated) hospital and community/retail pharmacy practice, where time and funding constraints limit opportunities for individual research.\n\nThe research scope of clinical pharmaceutical scientists can be broken into the fields of preclinical, clinical, outcomes, and translational research, with translation research representing the middle ground between preclinical and clinical research. Subjects of interest to clinical pharmaceutical scientists in their respective field may involve the following:\n\n\n\n\nThe duties of clinical pharmaceutical scientists can be broadly divided into two categories: scientific and clinical duties:\n\n\n\n\n"}
{"id": "13584443", "url": "https://en.wikipedia.org/wiki?curid=13584443", "title": "Competency evaluation (law)", "text": "Competency evaluation (law)\n\nIn the United States criminal justice system, a competency evaluation is an assessment of the ability of a defendant to understand and rationally participate in a court process.\n\nCompetency was originally established by the Supreme Court of the United States as the evaluation of a defendant's competence to proceed to trial. In a subsequent ruling, the Court held that any prisoner facing the death penalty must be evaluated as competent to be executed, meaning that he must be capable of understanding why he has received the death penalty and the effect that the penalty will have. In further rulings, competence was also enlarged to include evaluation of the defendant's competence to plead guilty and competence to waive the right to counsel.\n\nThe American Bar Association's Criminal Justice Mental Health Standards stated in 1994 that the issue of a defendant's current mental incompetence is the single most important issue in the criminal mental health field, noting that an estimated 24,000 to 60,000 forensic evaluations of a criminal defendant's competency to stand trial were performed every year in the United States. A 1973 estimate put the number of competence evaluations at 25,000 to 36,000 each year. There are indications that the number of evaluations of criminal defendants is rising. One comparison of estimates between 1983 and 2004 suggest the annual number rose from 50,000 to 60,000 criminal competency evaluations respectively.\n\nThis standard is based on the Supreme Court decision \"Dusky v. United States\" in which the Court affirmed a defendant's right to have a competency evaluation before proceeding to trial. Competence to proceed was defined by the court as the defendant's ability to consult rationally with an attorney to aid in his own defense and to have a rational and factual understanding of the charges.\n\nIn this case Dusky presented a petition of writ of certiorari to the Supreme Court requesting that his conviction be reversed on the grounds that he was not competent to stand trial at the time of the proceeding. The court decided to grant the writ, based on a lack of recent evidence that the petitioner was competent at the time of the trial. The case was remanded to the district court for a new hearing to evaluate Dusky's competence to stand trial, and for a new trial if he was found competent.\n\nThe case set the current standard for adjudicative competency in the United States. Although the statutes addressing competency vary from state to state in the United States, the two elements outlined in the \"Dusky v. United States\" decision are held in common. The defendant must understand the charges and have the ability to aid his attorney in his own defense.\n\nCompetency to stand trial is generally determined via a pretrial evaluation of the defendant's overall mental status and mental state at the time of the examination. This evaluation aims to provide sufficient information to allow a judge to rule on the competency of the defendant should a motion to that effect be made by either the prosecutor or defense attorney. A judge may also directly rule a defendant incompetent to stand trial without receiving a motion to that effect from counsel.\n\nWho is deemed qualified to conduct a competency evaluation varies from state to state. Originally competency evaluations included a range of tests and assessments. Recently the tendency is to simplify the process by relying upon an interview and, optionally, a psychological evaluation. There are a number of standardized screening devices that also may be employed.\n\nA defendant who has been deemed incompetent to stand trial may be required to undergo mental health treatment, including court-ordered hospitalization and the administration of treatment against the defendant's wishes, in an effort to render the defendant competent to stand trial.\n\nAn additional evaluation is of the competence to be executed, that is the defendant must be capable of understanding why he is being executed and the effect execution will have.\n\nThe right to be competent to be executed resulted from the outcome of a United States Supreme Court case, \"Ford v. Wainwright\", in which a Florida inmate on death row took his case to the United States Supreme Court, declaring he was not competent to be executed. He had been evaluated as incompetent but the Governor of Florida made an executive branch decision and signed the death warrant anyway. The court ruled that a forensic professional must make that evaluation and, if the inmate is found incompetent, provide treatment to aid in the inmate gaining competency in order that the execution can take place.\n\nProviding treatment to an individual to enable that person to become competent to be executed places mental health professionals in an ethical dilemma. The National Medical Association takes the position that ethically it is a physician's duty to provide treatment, regardless of the patient's legal situation. Others feel that it unethical to treat a person in order to execute them. Most restorations of competency are accomplished through psychiatric medication.\n\nIt has been estimated that approximately 90 percent of all criminal cases in the United States are settled through guilty pleas, rather than a trial.\n\nIn \"Godinez v. Moran\", 1993, the Supreme Court held that the competency standard for pleading guilty or waiving the right to counsel is the same as the competency standard for proceeding to trial as established in \"Dusky v. United States\". A higher standard of competency is not required.\n\nIn \"United States v. Binion\" malingering or feigning illness during a competency evaluation was held to be obstruction of justice and led to an increased sentence.\n\nWhere a defendant does not raise the issue of mental competence before trial, the issue of competence may be deemed waived in the event of a conviction and appeal. For example, in \"United States v. Morin\" the United States Court of Appeals, Eighth Circuit upheld the defendant's conviction. The court rejected Mr. Morin’s argument, among others, that the district court violated his due process rights by refusing to allow him to waive competency at trial. The court held that since his competency to stand trial was never challenged, the issue of whether he was entitled to waive competency to stand trial was properly not considered.\n\nAlthough \"Dusky v. United States\" affirmed the right to a competency evaluation, the specifics of the evaluation remain ambiguous. Each evaluator must decide what is meant by \"sufficient present ability\" and \"has a rational as well as a factual understanding\" as set forth in the Dusky decision. One common principle is clear in forensic evaluations, however. Forensic evaluators cannot reach a finding independent of the facts of the case at hand.\n\nIn 1989, Kenneth Curtis of Stratford, Connecticut was initially found mentally incompetent to stand trial following the murder of his estranged girlfriend. But years later, as he had attended college and received good grades, this ruling was reversed, and he was ordered to stand trial.\n\nSome other notable cases include:\n\n\n"}
{"id": "39091628", "url": "https://en.wikipedia.org/wiki?curid=39091628", "title": "Convertible husbandry", "text": "Convertible husbandry\n\nWithin agriculture, convertible husbandry, also known as alternate husbandry, ley husbandry or up-and-down husbandry, was a process used during the 16th century through the 19th century by \"which a higher proportion of land was used to support increasing numbers of livestock in many parts of England.\" In the words of historian Eric Kerridge, convertible husbandry consisted of \"the floating of water-meadows, the substitution of up-and-down husbandry for permanent tillage and permanent grass or for shifting cultivation, the introduction of new fallow crops and selected grasses, marsh drainage, manuring, and stock breeding.\" Convertible husbandry is considered one of the most important changes of the British Agricultural Revolution.\n\nConvertible husbandry was a process that consisted of \"alternating arable and pasture on a given piece of land…[by doing so]… farmers almost eliminated the need for fallows between their grain crops and were able to control the quality of their pasture by sowing grass seeds.\" Alternate husbandry has also been praised as the \"best way to keep high fertility on both arable and pasture and to retain excellent soil texture and composition.\" This system utilized fertilizer in the form of animal manure. Fertilizer was used in greater quantities due to the increase in animal husbandry and resulted in benefiting crop yields when it was time for tillage. Convertible husbandry also came with the added benefit of allowing variations in the types of soils and the extent of leys used in rotation because it was a system in which multiple variables could be modified to suit the needs of the location/type of land/type of soil.\n\nBefore the 16th and 17th centuries, farmlands had mostly been founded on the idea of simple alternations of tilling and fallowing during different seasons over several years. However, as livestock became an increasing staple in the lives of farmers and society alike in the midlands, the \"rising population…density of settlements, lack of wastelands into which cultivation could expand…and 15th century enclosures for sheep\" all led to a need for an improved system of agriculture that allowed for increasing numbers of livestock and a greater increase in crop output compared to input. \"The introduction of up-and-down husbandry…helped solve the problems of the midland by providing a measured pasture and arable rotation which not only produced the same amount of grain on a much reduced area, but broke the agrarian cycle of diminished returns by allowing more sheep and cattle to be kept, animals whose dung maintained the fertility of the arable.\" Another factor that increased the popularity of convertible husbandry had to do with the fact that skill levels of workers were changing and the skills needed to manage a permanent grassland system were acquired too slowly to respond adequately to the growing demand of the population at the time.\n\nAlthough debatable, many agricultural historians also believe that the introduction of convertible husbandry was brought about with the introduction of the turnip. They argue that \"the lowly turnip made possible a change in crop rotation which did not require much capital, but which brought about a tremendous rise in agricultural productivity.\" They believe that this \"fodder\" crop pushed agriculture in a direction in which \"alternating\" husbandry was seen as more efficient than traditional permanent pasture farming and jump-started the improvement of crop rotation and agricultural output versus capital. Although the turnip was popularized by Lord Townshend during the mid-18th century, the use of turnips being grown as fodder was seen as early as the 16th century.\n"}
{"id": "24793227", "url": "https://en.wikipedia.org/wiki?curid=24793227", "title": "Country food", "text": "Country food\n\nCountry food, in Canada, refers to the traditional diets of Indigenous peoples (known in Canada as First Nations, Metis, and Inuit), especially in remote northern regions where Western food is an expensive import, and traditional foods are still relied upon.\nThe Government of the Northwest Territories estimated in 2015 that nearly half of N.W.T. residents in smaller communities relied on country food for 75% of their meat and fish intake, in larger communities the percentage was lower, with the lowest percentage relying on country foods (4%) being in Yellowknife, the capital and only \"large community\". The most common country foods in the NWT's area include mammals and birds (caribou, moose, ducks, geese, seals, hare, grouse, ptarmigan), fish (lake trout, char, inconnu (coney), whitefish, pike, burbot) and berries (blueberries, cranberries, blackberries, cloudberries).\n\nIn the eastern Canadian Arctic, Inuit consume a diet of foods that are fished, hunted, and gathered locally. This may include caribou, walrus, ringed seal, bearded seal, beluga whale, polar bear, berries, and fireweed.\n\nThe cultural value attached to certain game species, and certain parts, varies. For example, in the James Bay region, a 1982 study found that beluga whale meat was principally used as dog food, whereas the blubber, or muktuk was a \"valued delicacy\". Value also varies by age, with Inuit preferring younger ring seals, and often using the older ones for dog food.\n\nContaminants in country foods are a public health concern in Northern Canada; volunteers are tested to track the spread of industrial chemicals from emitters (usually in the South) into the northern food web via the air and water.\n\nIn 2017, the Government of the N.W.T. committed to using country foods in the soon-to-open Stanton Territorial Hospital, despite the challenges of obtaining, inspecting, and preparing sufficient quantities of wild game and plants.\n\nIn Southern Canada, wild foods (especially meats) are actually relatively rare in restaurants, due to wildlife conservation rules against selling hunted meat, as well as strict meat inspection rules. Therefore there is a cultural divide between rural and remote communities that rely on wild foods, and urban Canadians (the majority), who have little or no experience with them.\n\n"}
{"id": "4038299", "url": "https://en.wikipedia.org/wiki?curid=4038299", "title": "Dead arm syndrome", "text": "Dead arm syndrome\n\nDead arm syndrome starts with repetitive motion and forces on the posterior capsule of the shoulder. The posterior capsule is a band of fibrous tissue that interconnects with tendons of the rotator cuff of the shoulder. Four muscles and their tendons make up the rotator cuff. They cover the outside of the shoulder to hold, protect and move the joint.\n\nOveruse can lead to a buildup of tissue around the posterior capsule called hypertrophy. The next step is tightness of the posterior capsule called posterior capsular contracture. This type of problem reduces the amount the shoulder can rotate inwardly.\n\nOver time, with enough force, a tear may develop in the labrum. The labrum is a rim of cartilage around the shoulder socket to help hold the head of the humerus (upper arm) in the joint. This condition is called a superior labrum anterior posterior (SLAP) lesion. The final outcome in all these steps is the dead arm phenomenon.\n\nThe shoulder is unstable and dislocation may come next. Dead arm syndrome will not go away on its own with rest—it must be treated. If there is a SLAP lesion, then surgery is needed to repair the problem. If the injury is caught before a SLAP tear, then physical therapy with stretching and exercise can restore it.\n\nIt is common among baseball pitchers as they age, and it can also occur with quarterbacks in football and handball players also as they age.\n\n"}
{"id": "50059842", "url": "https://en.wikipedia.org/wiki?curid=50059842", "title": "Disability rights in New Zealand", "text": "Disability rights in New Zealand\n\nDisability rights are not specifically addressed by legislation in New Zealand. Instead, disability rights are addressed through human rights legislation. Human rights in New Zealand are protected by the New Zealand Bill of Rights Act 1990 (NZ BORA) and the Human Rights Act 1993. New Zealand also signed and ratified the United Nations Convention on the Rights of Persons with Disabilities (CRPD) in 2008.\n\nNew Zealand became a member of the United Nations on October 24, 1945 and was a signatory in the Universal Declaration of Human Rights. Article 1 of the Universal Declaration says that all human beings are born free and equal with dignity and rights. The Declaration became the standard in judging government conduct in relations to human rights. Although the Declaration started out as having only moral and political authority, unable to create legal obligations like the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR), its principles are now widely recognized. Today, the Declaration has attained a legal force as being part of the customary law of nations, including New Zealand. In addition, The Declaration has become the authority in interpreting human rights provisions of the United Nations Charter, binding all members.\n\nThe United Nations assumed the Universal Declaration would be enough in protecting everyone. However, the 650 million people in the world with disabilities, representing 10 percent of the world population, lack opportunities available to the mainstream population. People with disabilities face many physical and social obstacles that prevent them from exercising many of their rights. Therefore, a convention that deals with disability and the rights of people with disabilities was necessary.\n\nThe United Nations Convention on the Rights of Persons with Disabilities is the international standard for rights of the people with disabilities. New Zealand ratified the Convention in 2008 and therefore has a legal obligation to respect, promote, and fulfill the rights provided in the Convention. The Convention is aimed at protecting the dignity of people with disabilities and ensuring that they are treated fairly and equally under the law. It provides these individuals with a voice, visibility, and legitimacy as equal human beings in New Zealand and around the world. Although the Convention does not create new rights or entitlements, it expresses existing rights in a way that address the needs and situations of people with disabilities. New Zealand ratified the Optional Protocol to the CRPD on 4 October 2016.\n\nThe New Zealand Bill of Rights Act 1990 (NZ BORA) aimed to affirm, protect and promote human rights and fundamental freedoms in New Zealand. In addition, the NZ BORA affirms New Zealand’s commitment to the ICCPR and provides the right to be free from disability discrimination on the grounds stated in the Human Rights Act 1993. Despite propositions that the NZ BORA has obtained 'constitutional status', it is not supreme law, and it can still be overridden by Acts of the Parliament.\n\nSection 21(1)(h) of the Act makes discrimination based on disability, without lawful justification, unlawful. Section 21(1)(h) of the act defines disability as:\n\nDiscrimination happens when a person is treated unfairly or less-favourably than other people in the same or similar circumstances. Discrimination can also occur against the relatives and associates of the people with disabilities. Discrimination is only unlawful if it happens in one of the areas of activity set out in the Act, including employment, education, or government activity. The New Zealand Human Rights Commission provided a formula to help determine whether an activity or a practice amounts to unlawful discrimination. The following components must be present for the discrimination to be unlawful:\n\nLawful justifications of discrimination based on disability exist as well. In relation to employment, a person with a disability can be treated differently if he or she can not perform duties without the help of special services that are unreasonable to expect from the employer. In addition, if the environment where duties are performed poses an unreasonable risk to the person with disability or to those around them, including the risk of infecting others with an illness, then this is considered a lawful justification for discrimination as well. However, a justification will not be lawful if the duties can be performed with some adjustment of activities by the employer that does not involve unreasonable disruption.\n\nThe Human Rights Act also protects people from both direct and indirect discrimination. ‘Indirect discrimination’ describes the situation where an apparently neutral practice or condition has a disproportionate, negative impact on one of the groups against whom it is unlawful to discriminate, and the practice or condition cannot be justified objectively. People with disabilities are specifically susceptible to indirect discrimination. For example, indirect discrimination takes place against people who use wheelchairs if the only way to get to a store is by climbing the stairs.\n\nThe NZ BORA gives a person with disability equal right to vote by secret ballot. For this to take place, the individual must be over 18 years old and obtain a New Zealand citizenship or permanent residency. However, he or she can be denied this right in some contexts. For example, a person with disability is disqualified from voting if they have been detained for three or more years for a criminal offense. In addition, the right to a secret ballot can be compromised when the voter has visual impairment or has difficulty interpreting and reading the ballot. The individual would need assistance in placing their vote, therefore disclosing their preference to another person.\n\nThe Political Participation for Everyone Report found that New Zealand’s voting and political systems were not designed for everyone, specifically at disadvantage are those with disability. It found that people with disabilities have experienced hurdles in exercising their right to vote and right to participate in political and public life. These hurdles included inaccessible information and voting papers and a lack of physical facilities where people with disabilities can engage with politicians.\n\nIn September 20, 2014, New Zealand introduced voting by telephone. For this reason, The United Nations commended the country on its Concluding Observations for enabling people with disabilities to vote.\n\nThe Human Rights Commission claims that New Zealand is a fully inclusive society that recognises and values people with disability as equal participants. The needs of these individuals are considered integral to the social and economic order and not identified as “special”. A full inclusion requires a barrier-free physical and social environment. The Better Design and Buildings for Everyone: Disabled People's Rights and the Built Environment Report 2012 found that New Zealand’s built environment is rarely designed in considerations of all users. This particularly excludes people with disabilities, approximately 17 to 20 percent of New Zealand's population, from using and accessing facilities and services like buildings, parks and recreation facilities.\n\nThe Better Information for Everyone: Disabled People's Rights in the Information Age Report found that information is not accessible to a wide range of people with disabilities. Human rights depend on the accessibility of information, products and services. New Zealand information and communications were found to have often been designed for one kind of end user: an individual who is internet-literate, can see and read in English competently, and lacks learning disabilities.\n\nThe Commission is an independent Crown entity responsible for administering the Human Rights Act 1993 and monitoring the Convention on the Rights of Persons with Disabilities. An individual can make a complain to the Human Rights Commission if they feel that their human rights have been breached. A complaint can be made in sign language with interpreters available. In addition, the Commission offers free and confidential services. It can advise complainants on whether the complaint is covered by the Human Rights Act and if the Commission can help through mediation. If mediation does not work, advice can be given on possible legal options.\n\nThe Commission also developed guidelines for using disability rights language as a practical tool when referring to people with disability. However, the word-choice can reflect attitudes toward people with disability. Because of this, the Commission suggested to use people-centred language that recognises a person with disability is, first and foremost, a person. In their view, this is one of the many ways New Zealand can promote and respect the dignity of people with disability in accordance with the Disability Convention.\n\nThe International Day of Persons with Disabilities first started in 1992 and it is celebrated yearly on December 3. The United Nations promotes this day across the world to encourage a better understanding of disability issues. Each year has a different theme. For example, the theme for 2015 was “Inclusion matters: access and empowerment for people of all abilities”, which aimed at ensuring that people with disabilities are empowered to create and use opportunity. At this event, thousands of bright orange wristbands with \"Inclusion Matters\" written on them were sent out to organisations throughout New Zealand for the day of celebration and acknowledgement. The government of New Zealand believes that by wearing the wristband, \"you are calling for the inclusion of disabled people into all areas of life.\"\n\n"}
{"id": "10485922", "url": "https://en.wikipedia.org/wiki?curid=10485922", "title": "Duty of candour", "text": "Duty of candour\n\nIn UK public law, the duty of candour is the duty imposed on a public authority 'not to seek to win [a] litigation at all costs but\nto assist the court in reaching the correct result and thereby to improve standards in public administration'. Lord Donaldson MR in \"R v Lancashire County Council ex p. Huddleston\" stated that public servants should be willing 'to explain fully what has occurred and why'.\n\nThere is also a contractual duty of candour imposed on all NHS and non-NHS providers of services to NHS patients in the UK to 'provide to the service user and any other relevant person all necessary support and all relevant information' in the event that a 'reportable patient safety incident' occurs. A 'reportable patient safety incident' is one which could have or did result in moderate or severe harm or death.\n\nCampaigner Will Powell led a campaign for NHS managers and doctors to have a formal 'duty of candour' when dealing with complaints about negligent or poor standards of care in NHS hospitals.\n\nIn January 2014 David Behan, chief executive of the Care Quality Commission, threw his weight behind a wide definition for the statutory duty of candour which was recommended by the Francis Report. The Government originally intended the duty to be limited to cases of “severe harm” – when a patient had been killed or left permanently disabled, as a wider reporting requirement could inundate organisations with unnecessary bureaucracy. The CQC estimates there are about 11,000 incidents of severe harm per year, and up to 100,000 incidents of serious harm, although there may be significant under reporting of both. The charity Action Against Medical Accidents has been campaigning for a wide definition and Behan made it clear that he was supporting them.\n\n"}
{"id": "47964559", "url": "https://en.wikipedia.org/wiki?curid=47964559", "title": "Emotional approach coping", "text": "Emotional approach coping\n\nEmotional approach coping is a psychological construct that involves the use of emotional processing and emotional expression in response to a stressful situation. As opposed to emotional avoidance, in which emotions are experienced as a negative, undesired reaction to a stressful situation, emotional approach coping involves the conscious use of emotional expression and processing to better deal with a stressful situation. The construct was developed to explain an inconsistency in the stress and coping literature: emotion-focused coping was associated with largely maladaptive outcomes while emotional processing and expression was demonstrated to be beneficial.\n\nCoping is a conscious attempt to address and alleviate demands perceived as stressful. Research examining coping has suggested two broad categories of coping: emotion-focused and problem-focused coping. Emotion-focused coping involves attempts to regulate the negative emotional response to stress. Whereas problem-focused coping involves attempts to directly modify the stressor. Coping processes have also been defined instead on whether they involve approaching the stressful situation or avoiding it.\n\nThe experience of powerful emotions has been characterized by researchers as disruptive and dysfunctional, particularly for cognitive processes. Moreover, research also suggests links between emotion-focused coping and poor psychological outcomes. A review of over 100 studies found associations between emotion-focused coping and negative outcomes such as poor life satisfaction, greater depressive and anxious symptoms and neuroticism.\n\nHowever, there is some evidence in the empirical literature that emotional expression can be functional and adaptive. Experimental research on expressive writing, involving emotional disclosure, has been shown to have benefits for performance on cognitive tasks and for psychological outcomes, such as depressive symptoms. Emotion regulation has also illustrated the importance of emotional processing and expression for well-being. Therapeutic approaches have also demonstrated the important role of emotions in coping with difficult situations. Emotion-focused therapy is a clinical psychology approach that emphasizes the importance of acknowledging and tolerating negative emotions and enjoying positive emotions for healthy psychological adjustment.\n\nResearchers have attempted to disentangle the maladaptive and functional aspects of emotion-focused coping by examining the measurements of emotion-focused coping. Several studies have found that emotion-focused measurements of coping often aggregate approach and avoidance strategies. A second reason emotion-focused coping has been construed as maladaptive is that measures of emotion-focused coping are confounded with measures of distress. In an attempt to rectify these difficulties with the operationalization of emotion-focused coping, a new scale for assessing emotional approach coping was proposed.\n\nEmotional approach coping can be assessed using the emotional approach coping scales developed by Stanton, Kirk, Cameron, and Danoff-Burg in 2000. The scales involve two distinct subscales of items: emotional processing and emotional expression. Emotional processing and emotional expression scales are positively correlated but distinct. The emotional processing items reflect an attempt to understand, consider and examine emotions in response to a stressful event. For example, “I acknowledge my feelings” and “I take time to figure out what I’m really feeling.” Emotional expression items assess attempts to verbally and non-verbally communicate and share emotions. Sample items include: “I allow myself to express my feelings” and “I feel free to express my emotions.” The emotional approach coping scales have been tested and validated using situational (i.e., what do you do in response to a specific stressor) and dispositional (i.e., what do you do in general) instruction sets. The scales are uncorrelated with social desirability. In addition to English, the emotional approach coping scale has also been validated in Norwegian and Turkish.\n\nAmong heterosexual couples coping with infertility, emotional approach coping predicted decreased depressive symptoms for both members of the couple after an unsuccessful insemination attempt. Emotional approach coping may also confer benefits for partners. Having a male partner high in emotional approach coping was protective against depressive symptoms for female partners low in emotional approach coping.\n\nEmotional approach coping may confer some benefits to victims of sexual assault. Among sexual assault survivors, increases in emotional expression were associated with greater perceived control over the recovery process and feelings of control were associated with decreased distress after the assault.\n\nThere is mixed evidence for the utility of emotional approach coping in samples of women with breast cancer. In a longitudinal study of women with breast cancer, for women who perceived their social environments to be receptive, emotional expression predicted improved quality of life. Coping through emotional expression among women with breast cancer has also been found to predict an increase in post-traumatic growth. However, other studies have not found the same link between emotional expression and post-traumatic growth.\n\nCross-sectional studies illustrate the link between emotional approach coping and positive psychological adjustment, under certain conditions in student and community samples. In a cross-sectional study of undergraduate women, women who scored more highly on emotional approach coping reported more positive and less negative valenced repetitive thoughts. In a community sample of African-American adults, emotional approach coping has also been found to be negatively associated with anger, trait anxiety and depressive symptoms. In addition, women who reported higher dispositional emotional processing also reported fewer depressive and anxious symptoms and greater life satisfaction; while for men, higher dispositional emotional expression was linked to greater life satisfaction.\n\nThere is some evidence to suggest associations between emotional approach coping and psychological well-being. In a study of individuals who met DSM-IV criteria for anxiety disorder and healthy controls, levels of emotional approach coping were lower in those individuals who met the criteria than in controls. Another study examined veterans and found that higher levels emotional expression (but not emotional processing) were associated with lower depressive symptoms and decreased post-traumatic stress disorder, even when statistically controlling for age, gender, and race.\n\nCross-sectional research of cancer samples reveals some positive, negative and mixed links with emotional approach coping. Higher emotional processing and emotional expression in female cancer survivors was associated with higher positive emotions and lower negative emotions. In male cancer survivors, higher emotional processing has been linked to higher positive emotions and higher emotional expression has been linked with lower negative emotions and fewer intrusive thoughts. However, the links between emotional approach coping and psychological adjustment are not all positive some are negative or mixed. In one study of women who had received an abnormal result on an ovarian cancer screen, higher emotional processing was associated with higher intrusive thoughts and neither emotional processing nor emotional expression were associated with cancer-related post-traumatic growth.\n\nThere is from cross-sectional research that suggests the benefits of emotional processing for patients with diabetes. Among patients with type 2 diabetes, higher emotional processing was associated with greater diabetes-related knowledge, medication adherence and relevant self-care behaviors such as diet, physical activity and blood glucose monitoring. Similarly, in adolescent patients with Type 1 diabetes, emotional processing was revealed to be associated with better metabolic control.\n\nThe stressor and the individual's appraisal of the stressor may determine the effectiveness of emotional approach coping as a mechanism for managing stress. An appraisal of a stressful situation as uncontrollable may make emotional approach coping an advantageous coping mechanism. In fact, one study of undergraduates shows that when faced with a stressor individuals appraise as more uncontrollable, they are more likely to endorse using emotional approach coping to manage it.\n\nThere is some evidence to suggest that the utility of emotional approach coping varies by gender. In a longitudinal study, emotional approach coping was found to predict increased life satisfaction and decreased depressive symptoms over time in women; however, in men, emotional approach coping predicted poorer adjustment over time. Some samples have also found that women report using emotional processing and expression more than men. However, research of infertile couples found no differences in the utility of emotional approach coping for men and women.\n\nIndividual differences, such as skill at engaging active coping techniques and comfort with expressing emotions, may modify the tendency to successfully employ emotional approach coping. Individuals high in perceived emotional intelligence may also be more likely to use emotional approach coping skillfully. Holding unrealistic perceptions of control may make the use of coping through emotional approach less likely because expressing and processing emotions could lead to evaluations that result in acknowledgement of illusions of control. Personality attributes, such as hope, can also moderate the effectiveness of emotional approach coping. Women with breast cancer who were high in hope and reported coping with emotional expression, had fewer medical appointments for cancer-related complaints, enhanced physical health and decreased distress compared to women who did not cope using emotional expression.\n\nThe effects of emotional approach coping could be the result of identifying goals, understanding barriers to achieving those goals, and finding new pathways to achieve them. Emotional expression and processing could help individuals direct attention to identify the most important goals in their lives.\n\nThe effects of emotional approach coping could also be due to exposure to stressful stimuli when actively processing and expressing emotions. The repeated exposure to the stressor could result in physiological habituation. Repeated exposure to a stressor through emotional expression and processing could also lead to cognitive reappraisal of the stressor and related self-affirmations.\n\nThe process of labeling the emotions (i.e., putting them into words) may lessen the intensity of the emotional experience. Studies have shown the process of affective labeling leads to decreases in brain regions such as the amygdala and increases in activation of the prefrontal cortex, possibly indicating beneficial emotion regulation.\n\nThe use of emotional approach coping may signal to the social environment that an individual is in need of support. The responsiveness of the social environment will determine the adaptiveness of emotional approach coping. Emotional expression that is met with empathetic concern may lead to better adjustment than emotional expression met by rejection. Some evidence from the research suggests this could be a potential mechanism. For women with breast cancer who perceive their social environment to be highly receptive, coping through emotional expression predicts improved quality of life.\n"}
{"id": "1179168", "url": "https://en.wikipedia.org/wiki?curid=1179168", "title": "Eunice Kennedy Shriver National Institute of Child Health and Human Development", "text": "Eunice Kennedy Shriver National Institute of Child Health and Human Development\n\nThe \"Eunice Kennedy Shriver\" National Institute of Child Health and Human Development (NICHD) is one of the National Institutes of Health (NIH) in the United States Department of Health and Human Services. It supports and conducts research aimed at improving the health of children, adults, families, and communities, including:\n\nThe impetus for NICHD came from the Task Force on the Health and Well-Being of Children, convened in 1961 and led by Dr. Robert E. Cooke, a senior medical advisor to President John F. Kennedy. Eunice Kennedy Shriver also served on the task force, which reported that more research was needed on the physical, emotional, and intellectual growth of children.\n\nThe U.S. Congress established NICHD in 1962 as the first NIH institute to focus on the entire life process rather than on a specific disease or body system. NICHD became a funding source for research on birth defects and intellectual and developmental disabilities (IDDs), created a new pediatrics specialty, and established IDDs as a field of research. The institute also focused on the idea that adult health has its origins in early development and that behavior and social science were important aspects of human development.\n\nOn December 21, 2007, by act of Congress (Public Law 110-54), NICHD was renamed the \"Eunice Kennedy Shriver\" National Institute of Child Health and Human Development in honor of Mrs. Shriver's vision, dedication, and contributions to the founding of the institute.\n\nThe mission of NICHD is to ensure that every person is born healthy and wanted, that women suffer no harmful effects from reproductive processes, and that all children have the chance to achieve their full potential for healthy and productive lives, free from disease or disability, and to ensure the health, productivity, independence, and well-being of all people through optimal rehabilitation.\n\nAs of November 2016, the director of NICHD is Diana W. Bianchi.\n\nNICHD’s budget in 2015 was an estimated $1.3 billion, which supported research at institutions, universities, and organizations throughout the world, as well as research conducted by NICHD scientists on the NIH campus in Bethesda, MD, and at other facilities.\n\n\nNICHD has made numerous contributions to improving the health of children, adults, families, and communities. Selected research advances from 2015 include improving the health of infants born preterm, encouraging healthy behaviors, and optimizing rehabilitation.\n\nAdditional accomplishments can be found on the NICHD website.\n\n\n"}
{"id": "6646507", "url": "https://en.wikipedia.org/wiki?curid=6646507", "title": "European Prospective Investigation into Cancer and Nutrition", "text": "European Prospective Investigation into Cancer and Nutrition\n\nThe European Prospective Investigation into Cancer and Nutrition (EPIC) study is a Europe-wide prospective cohort study of the relationships between diet and cancer, as well as other chronic diseases, such as cardiovascular disease. With over half a million participants, it is the largest study of diet and disease to be undertaken.\n\nEPIC is coordinated by the International Agency for Research on Cancer (IARC), part of the World Health Organization, and funded by the \"Europe Against Cancer\" programme of the European Commission as well as multiple nation-specific grants and charities.\n\n521,457 healthy adults, mostly aged 35–70 years, were enrolled in 23 centres in ten European countries: Denmark (11%), France (14%), Germany (10%), Greece (5%), Italy (9%), The Netherlands (8%), Norway (7%), Spain (8%), Sweden (10%) and the United Kingdom (17%). One UK centre (Oxford) recruited 27,000 vegetarians and vegans; this subgroup forms the largest study of this dietary group. Recruitment to the study took place between 1993 and 1999, and follow up is planned for at least ten years, with repeat interview/questionnaires every three to five years. The main prospective data collected are standardised dietary questionnaires (self-administered or interview-based), seven-day food diaries, blood samples and anthropometric measurements, such as body mass index and waist-to-hip ratio. Additionally, the GenAir case-control study is studying the relationship of passive smoking and air pollution with cancers and respiratory diseases.\n\nUp to 2004, there were over 26,000 new cases of cancer recorded among participants, with the most common being cancers of the breast, colorectum, prostate and lung. Current analyses are focusing particularly on stomach, colorectal, breast, prostate and lung cancers. The different dietary patterns in the different countries should enable reliable associations to be made between particular diets and cancers. The analysis of stored blood samples should also allow dissection of genetic factors involved in cancers, as well as the effects of hormones and hormone-like factors.\n\nThe study and its analysis is ongoing, but key results of the study retrieved in 2008 are:\n\nSubsequent findings from 2012 and 2013 are:\n\nReview\n\nPrimary\n\n"}
{"id": "1002473", "url": "https://en.wikipedia.org/wiki?curid=1002473", "title": "Gastritis", "text": "Gastritis\n\nGastritis is inflammation of the lining of the stomach. It may occur as a short episode or may be of a long duration. There may be no symptoms but, when symptoms are present, the most common is upper abdominal pain. Other possible symptoms include nausea and vomiting, bloating, loss of appetite and heartburn. Complications may include bleeding, stomach ulcers, and stomach tumors. When due to autoimmune problems, low red blood cells due to not enough vitamin B12 may occur, a condition known as pernicious anemia.\nCommon causes include infection with \"Helicobacter pylori\" and use of nonsteroidal anti-inflammatory drugs (NSAIDs). Less common causes include alcohol, smoking, cocaine, severe illness, autoimmune problems, radiation therapy and Crohn's disease. Endoscopy, a type of X-ray known as an upper gastrointestinal series, blood tests, and stool tests may help with diagnosis. The symptoms of gastritis may be a presentation of a myocardial infarction. Other conditions with similar symptoms include inflammation of the pancreas, gallbladder problems, and peptic ulcer disease.\nPrevention is by avoiding things that cause the disease. Treatment includes medications such as antacids, H2 blockers, or proton pump inhibitors. During an acute attack drinking viscous lidocaine may help. If gastritis is due to NSAIDs these may be stopped. If \"H. pylori\" is present it may be treated with a combination of antibiotics such as amoxicillin and clarithromycin. For those with pernicious anemia, vitamin B12 supplements are recommended either by mouth or by injection. People are usually advised to avoid foods that bother them.\nGastritis is believed to affect about half of people worldwide. In 2013 there were approximately 90 million new cases of the condition. As people get older the disease becomes more common. It, along with a similar condition in the first part of the intestines known as duodenitis, resulted in 50,000 deaths in 2015. \"H. pylori\" was first discovered in 1981 by Barry Marshall and Robin Warren.\n\nMany people with gastritis experience no symptoms at all. However, upper central abdominal pain is the most common symptom; the pain may be dull, vague, burning, aching, gnawing, sore, or sharp. Pain is usually located in the upper central portion of the abdomen, but it may occur anywhere from the upper left portion of the abdomen around to the back.\n\nOther signs and symptoms may include the following:\n\nCommon causes include \"Helicobacter pylori\" and NSAIDs. Less common causes include alcohol, cocaine, severe illness and Crohn disease, among others.\n\n\"Helicobacter pylori\" colonizes the stomachs of more than half of the world's population, and the infection continues to play a key role in the pathogenesis of a number of gastroduodenal diseases. Colonization of the gastric mucosa with \"Helicobacter pylori\" results in the development of chronic gastritis in infected individuals, and in a subset of patients chronic gastritis progresses to complications (e.g., ulcer disease, stomach cancers, some distinct extragastric disorders). However, over 80 percent of individuals infected with the bacterium are asymptomatic and it has been postulated that it may play an important role in the natural stomach ecology.\n\nGastritis may also develop after major surgery or traumatic injury (\"Cushing ulcer\"), burns (\"Curling ulcer\"), or severe infections. Gastritis may also occur in those who have had weight loss surgery resulting in the banding or reconstruction of the digestive tract.\n\nEvidence does not support a role for specific foods including spicy foods and coffee in the development of peptic ulcers. People are usually advised to avoid foods that bother them.\n\nAcute erosive gastritis typically involves discrete foci of surface necrosis due to damage to mucosal defenses. NSAIDs inhibit cyclooxygenase-1, or COX-1, an enzyme responsible for the biosynthesis of eicosanoids in the stomach, which increases the possibility of peptic ulcers forming. Also, NSAIDs, such as aspirin, reduce a substance that protects the stomach called prostaglandin. These drugs used in a short period are not typically dangerous. However, regular use can lead to gastritis. Additionally, severe physiologic stress (\"stress ulcers\") from sepsis, hypoxia, trauma, or surgery, is also a common etiology for acute erosive gastritis. This form of gastritis can occur in more than 5% of hospitalized patients.\n\nAlso, note that alcohol consumption does not cause chronic gastritis. It does, however, erode the mucosal lining of the stomach; low doses of alcohol stimulate hydrochloric acid secretion. High doses of alcohol do not stimulate secretion of acid.\n\nChronic gastritis refers to a wide range of problems of the gastric tissues. The immune system makes proteins and antibodies that fight infections in the body to maintain a homeostatic condition. In some disorders the body targets the stomach as if it were a foreign protein or pathogen; it makes antibodies against, severely damages, and may even destroy the stomach or its lining. In some cases bile, normally used to aid digestion in the small intestine, will enter through the pyloric valve of the stomach if it has been removed during surgery or does not work properly, also leading to gastritis. Gastritis may also be caused by other medical conditions, including HIV/AIDS, Crohn's disease, certain connective tissue disorders, and liver or kidney failure. Since 1992, chronic gastritis lesions are classified according to the Sydney system.\n\nMucous gland metaplasia, the reversible replacement of differentiated cells, occurs in the setting of severe damage of the gastric glands, which then waste away (atrophic gastritis) and are progressively replaced by mucous glands. Gastric ulcers may develop; it is unclear if they are the causes or the consequences. Intestinal metaplasia typically begins in response to chronic mucosal injury in the antrum, and may extend to the body. Gastric mucosa cells change to resemble intestinal mucosa and may even assume absorptive characteristics. Intestinal metaplasia is classified histologically as complete or incomplete. With complete metaplasia, gastric mucosa is completely transformed into small-bowel mucosa, both histologically and functionally, with the ability to absorb nutrients and secrete peptides. In incomplete metaplasia, the epithelium assumes a histologic appearance closer to that of the large intestine and frequently exhibits dysplasia.\n\nOften, a diagnosis can be made based on the patient's description of their symptoms, but other methods which may be used to verify gastritis include:\n\nAntacids are a common treatment for mild to medium gastritis. When antacids do not provide enough relief, medications such as H blockers and proton-pump inhibitors that help reduce the amount of acid are often prescribed.\n\nCytoprotective agents are designed to help protect the tissues that line the stomach and small intestine. They include the medications sucralfate and misoprostol. If NSAIDs are being taken regularly, one of these medications to protect the stomach may also be taken. Another cytoprotective agent is bismuth subsalicylate .\n\nSeveral regimens are used to treat \"H. pylori\" infection. Most use a combination of two antibiotics and a proton pump inhibitor. Sometimes bismuth is added to the regimen.\n\nIn 1,000 A.D, Avicenna first gave the description of stomach cancer. In 1728, German physician Georg Ernst Stahl first coined the term \"gastritis\". Italian anatomical pathologist Giovanni Battista Morgagni further described the characteristics of gastric inflammation. He described the characteristics of erosive or ulcerative gastritis and erosive gastritis. Between 1808 and 1831, French physician François-Joseph-Victor Broussais gathered information from the autopsy of the dead French soldiers. He described chronic gastritis as \"Gastritide\" and erroneously believed that gastritis was the cause of ascites, typhoid fever, and meningitis. In 1854, Charles Handfield Jones and Wilson Fox described the microscopic changes of stomach inner lining in gastritis which existed in diffuse and segmental forms. In 1855, Baron Carl von Rokitansky first described hypetrophic gastritis. In 1859, British physician, William Brinton first described about acute, subacute, and chronic gastritis. In 1870, Samuel Fenwick noted that pernicious anemia causes glandular atrophy in gastritis. German surgeon, Georg Ernst Konjetzny noticed that gastric ulcer and gastric cancer are the result of gastric inflammation. Shields Warren and Willam A. Meissner described the intestinal metaplasia of the stomach as a feature of chronic gastritis.\n\n"}
{"id": "52921598", "url": "https://en.wikipedia.org/wiki?curid=52921598", "title": "Gastrophysics", "text": "Gastrophysics\n\nGastrophysics (gastronomical physics) is an emerging interdisciplinary science that employs principles from physics and chemistry to attain a fundamental understanding of the worlds of gastronomy and cooking. Gastrophysical topics of interest includes investigations of the raw materials of food, the effects of food preparation, and quantitative aspects of the physical basis for food quality, flavour, appreciation and absorption in the human body. \nGastrophysics is a scientific discipline that focuses on investigations of aspects of gastronomy and cooking that relates to phenomena, which can be described and explained in a frame of physics, physical chemistry, chemistry, and associated sciences.\n\nThe source of inspiration for gastrophysics is gastronomy and cooking. Gastrophysical studies has a gastronomic observation as its starting point, and aims at unravelling the scientific nature of the observations on many different length scales, including explaining physical and chemical aspects of the raw materials, of their transformations during the preparation of food, as well as of the sensory response while eating \n\nThe chemical and physical composition and properties of raw food materials are important for the transformations that occur in the food during preparation (heating, cooling, mixing, beating, fermenting, salting, drying, smoking, souring etc.). Flavour (taste and smell), mouthfeel, chemesthesis, astringency are all determinants for the sensory evaluation of food, and these characteristics are also related to the chemical properties and the physical texture of the food, and to how the food is transformed in the mouth. Gastrophysics deals with each of these components and aims at uncovering their mutual relations i.e. how the sensory input relates to the material composition and properties of food, and the absorption in the human body.\n\nGastrophysics is a scientifically inspired approach to gastronomy, but it is a science in its own right, and not a discipline to service chefs in creating new dishes. Gastrophysics focuses on gaining fundamental scientific insight to gastronomy and understanding universal phenomena, without removing any of the craft, creativity, and art characteristics of cooking.\n\nThe relation between gastrophysics and gastronomy, can be seen analogous to the relation between astrophysics and astronomy. Astronomers observe planets and stars, and describe where they are and how they move. Astrophysicists explain why the planets and stars are where they are, and how they got there. In the same way, gastrophysics aims at explaining the universal scientific nature of gastronomy.\n\nWhereas gastrophysics is a relatively new discipline within the physical sciences, the foundation for exploring the kind of soft matter that food is, is already well-established in other areas of modern physics. The methodology of gastrophysics greatly overlaps with e.g. (molecular) biophysics, soft matter physics, material physics, physical chemistry, analytical chemistry etc. This holds for both experimental, theoretical and phenomenological approaches. Gastrophysics leans on state-of-art technologies both experimentally and computationally.\nIt is unknown when the term gastrophysics was first coined, but it appears to have been independently proposed as a physics approach to gastronomy in the labs of the physicist Nicholas Kurti, Peter Barham, and Ole G. Mouritsen. \n"}
{"id": "16017459", "url": "https://en.wikipedia.org/wiki?curid=16017459", "title": "Genetic epidemiology", "text": "Genetic epidemiology\n\nGenetic epidemiology is the study of the role of genetic factors in determining health and disease in families and in populations, and the interplay of such genetic factors with environmental factors. Genetic epidemiology seeks to derive a statistical and quantitative analysis of how genetics work in large groups.\n\nThe use of the term \"Genetic epidemiology\" emerged in the mid 1980s as a new scientific field.\n\nIn formal language, genetic epidemiology was defined by Newton Morton, one of the pioneers of the field, as \"a science which deals with the etiology, distribution, and control of disease in groups of relatives and with inherited causes of disease in populations\". It is closely allied to both molecular epidemiology and statistical genetics, but these overlapping fields each have distinct emphases, societies and journals.\n\nOne definition of the field closely follows that of behavior genetics, defining genetic epidemiology as \"the scientific discipline that deals with the analysis of the familial distribution of traits, with a view to understanding any possible genetic basis\", and that \"seeks to understand both the genetic and environmental factors and how they interact to produce various diseases and traits in humans\". The adopts a similar definition, \"Genetic epidemiology is the study of the aetiology, distribution, and control of disease in groups of relatives and of inherited causes of disease in populations.\"\n\nAs early as the 4th century BC, Hippocrates suggested in his essay “On Airs, Waters, and Places” that factors such as behavior and environment may play a role in disease. Epidemiology entered a more systematic phase with the work of John Graunt, who in 1662 tried to quantify mortality in London using a statistical approach, tabulating various factors he thought played a role in mortality rates. John Snow is considered to be the father of epidemiology, and was the first to use statistics to discover and target the cause of disease, specifically of cholera outbreaks in 1854 in London. He investigated the cases of cholera and plotted them onto a map identifying the most likely cause of cholera, which was shown to be contaminated water wells.\n\nModern genetics began on the foundation of Gregor Mendel's work. Once this became widely known, it spurred a revolution in studies of hereditary throughout the animal kingdom; with studies showing genetic transmission and control over characteristics and traits. As gene variation was shown to affect disease, work began on quantifying factors affecting disease, accelerating in the 20th century. The period since the second world war saw the greatest advancement of the field, with scientists such as Newton Morton helping form the field of genetic epidemiology as it is known today, with the application of modern genetics to the statistical study of disease, as well as the establishment of large-scale epidemiological studies such as the Framingham Heart Study.\n\nIn the 1960s and 1970s, epidemiology played a part in strategies for the worldwide eradication of naturally occurring smallpox.\n\nTraditionally, the study of the role of genetics in disease progresses through the following study designs, each answering a slightly different question:\n\n\nThis traditional approach has proved highly successful in identifying monogenic disorders and locating the genes responsible.\n\nMore recently, the scope of genetic epidemiology has expanded to include common diseases for which many genes each make a smaller contribution (polygenic, multifactorial or multigenic disorders). This has developed rapidly in the first decade of the 21st century following completion of the Human Genome Project, as advances in genotyping technology and associated reductions in cost has made it feasible to conduct large-scale genome-wide association studies that genotype many thousands of single nucleotide polymorphisms in thousands of individuals. These have led to the discovery of many genetic polymorphisms that influence the risk of developing many common diseases.\n\nGenetic epidemiological research follows 3 discreet steps, as outlined by M.Tevfik Dorak:\nThese research methodologies can be assessed through either family or population studies.\n\n\n\n"}
{"id": "35004598", "url": "https://en.wikipedia.org/wiki?curid=35004598", "title": "Gerhard Fischer (diplomat)", "text": "Gerhard Fischer (diplomat)\n\nGerhard Fischer (20 September 1921 – 3 July 2006) was a German diplomat, ambassador and humanitarian who received the 1997 Gandhi Peace Prize in recognition of his work for leprosy and polio-afflicted patients in India.\n\nBorn in Oslo, Fischer grew up in China and studied medicine at Beijing Medical University, where he gained experience as a volunteer with leprosy patients. When he was forced to abandon the course by the Japanese occupation, he travelled to Germany intending to study medicine there, but instead was obliged to fight in World War II, then was held as a POW in France. After the war, he obtained a law degree and entered the German diplomatic service.\n\nWhile at the German consulate in Madras, he helped a German doctor to establish a leprosy treatment and rehabilitation centre at Chettipatty. He was also instrumental in setting up the Indian Institute of Technology in Madras, which was founded with financial and technical support from the West German government. He later became ambassador to Malaysia, Ireland, Netherlands, and finally Switzerland.\n\nHe resigned from the diplomatic service in order to devote himself full-time to the support of leprosy and polio patients in India. Fischer regarded rehabilitation as the most important aspect of his work, and emphasised the need for attitudes to leprosy to change. His work was recognised by the Indian government with the award of the Gandhi Peace Prize. Fischer used the money from the prize to set up a foundation for his humanitarian work.\n\nFischer was born in Oslo, Norway, the elder son of a Norwegian mother and a German father, Martin Fischer. When he was about 3 years old, his family moved to China, where his father was a sinologist. As a boy, Fischer had always wanted to be a doctor. He started to study medicine at Beijing Medical University, but was forced to abandon the course when the University was closed by the Japanese occupation. So he decided to travel—via Siberia—to Germany, intending to study medicine there, but was instead compelled to fight in \"another huge war\", a situation he regarded as a \"trap\". He spent five years fighting in the war, mainly on the Eastern Front, followed by two years nine months as a prisoner of war. Destitute after the war, studying medicine was out of the question. He worked as a truck driver and various other jobs, and after three years had saved enough money to study again. He entered a legal crammer, and in one year obtained a law degree, a course that normally requires four years' study.\n\nWhile studying medicine in Beijing, he volunteered to look after leprosy patients (an opportunity offered to all medical students there, in the days before Fleming and modern antibiotics). From then on, he wanted to care for leprosy patients.\n\nWith a law degree in hand, he decided to join the German foreign service, as that would take him overseas. In 1952–1953 he completed his training for the higher diplomatic and consular service in Speyer, Rhineland-Palatinate, passing the final exam in 1953. He began his career at the German legation in Addis Ababa, Ethiopia, followed by a posting to the consulate in Hong Kong from 1957 to 1960, when he took over the consulate in Madras. In 1963 he was promoted to Consul, and in 1964 returned to the Bonn headquarters of the Foreign Office (\"Auswärtiges Amt\"). He was appointed to the rank of Councillor (\"Vortragender Legationsrat\") in 1966 and from 1968 headed a political department there, becoming a First Councillor (\"Vortragender Legationsrat Erster Klasse\") shortly afterwards.\n\nHis first appointment as Ambassador came in 1970, when he represented West Germany in Kuala Lumpur, Malaysia. In March 1974 he was promoted to lead a large department in the Foreign Office dealing with Asia and Latin America. His second ambassadorial appointment came in July 1977, when he was sent to Dublin, followed by further appointments to The Hague, Netherlands (early 1980) and Bern, Switzerland (mid-1983). In December 1985, only six months before reaching his normal retirement age, he resigned from the diplomatic service in order to devote more time to leprosy paients in India.\n\nFischer later said that taking a European posting, after his service in Vietnam, Malaysia and India was a \"mistake\".\n\nAs well as volunteering with leprosy patients during his medical training in Beijing, Fischer had also looked after leprosy patients in his free time at Addis Ababa and Hong Kong.\n\nFischer's work with leprosy patients in India began in 1960, when he was German Consul-General in Madras (now Chennai), in Tamil Nadu.\nA German doctor, Elizabeth Vomstein, had asked for his help in obtaining a work permit so that she could work with a French nun she had heard of who was helping leprosy patients in Chettipatty, near Salem. Fischer successfully helped her get the work permit, then drove with her to Chettipatty. Fischer remained in post in Tamil Nadu for four years, during which time he regularly visited Vomstein's station in Chettipatty, which she slowly built up to what he regarded as a \"model station\".\n\nAfter resigning from the diplomatic service in 1985, Fischer regularly spent six months each year in India, and six months in the summer with his wife at their small farm near Chiemsee, a lake in Bavaria. His wife, Ann, ran the administration, fund-raising and publicity for Fischer's work. They had decided not to have \"an administration\", so Fischer and his wife did everything on their own. Fischer said that his wife \"deserves half this prize\" (the Gandhi Peace Prize).\n\nUntil 1991 he worked with Elizabeth Vomstein at her station in Chettipatty. He found her \"very, very difficult\", but said, \"If she wasn't that type, she wouldn't have lasted 38 years.\" By 1991, he had had enough, and started out on his own. He had already built his own leprosy station in the foothills of the Himalayas, and in the following years he established health centres, workshops and schools throughout India, building wells and latrines, and obtaining Jeeps, minibuses and other supplies. He was also involved with projects in Nepal and Vietnam.\n\nFischer regarded curing leprosy as the easy part, with modern medicines, good treatment and good food. Much more important to Fischer was rehabilitation, which he regarded as his primary aim. He recognised that patients with missing legs or fingers would have no chance of surviving outside his stations. So he trained them in skills to earn a living, such as making chappals, table cloths, mats, bed covers, beautiful carpets, and so on. He hoped to reach a position where his stations would no longer need donations, but could survive on their production.\n\nFischer's leprosy treatment, rehabilitation and vocational training centres also cared for polio victims. By 2005, new polio cases had been greatly reduced, thanks to vaccination and joint efforts between the Indian government and NGOs. As a result, Fischer's centres were able to take on additional responsibilities in other areas. But Fischer stressed the need for medical practitioners in the primary health centres to maintain a watch over polio and leprosy, and to be trained in recognising the symptoms.\n\nFischer insisted that his patients should never be called \"lepers\", but rather \"leprosy patients\". Although he stated that he believed in \"karma\", he condemned the consequent fatalism of, and inaction towards, leprosy patients in India, saying \"It is not their fault. It is the bloody bug that caught them.\" He felt that they were outcasts who needed attention. In his Gandhi peace prize acceptance speech at the Rashtrapati Bhavan, he said, \"Don't treat us like outcasts. Don't treat us like the forgotten. We are also here in this country although you don't care about us.\" Many visitors to his stations couldn't face the physically horrible nature of the disease, but Fischer said that it was far worse mentally to be an outcast from one's family and community. He said, \"If you work in a leprosy community, the mental anguish is supreme and it is no different for me.\"\n\nFischer would touch all his patients, saying \"Nalla thane irukku\" (\"You are all right\"). He never wore a mask or gloves, as he thought the \"human side\" of his work was very important, that there should be no barriers, and that there was \"no difference between us, two human beings\".\n\nFischer said, \"I need publicity. I want to beat the drum, Leprosy is curable. Come and watch. Forget about the stigma. Forget about treating them as outcasts. Here I am constantly shouting to make the community aware and this Prize gives me a chance to focus the attention on 'us' who are forgotten.\"\n\nFischer always emphasised the importance of early treatment. Patients were reluctant to admit that they might have leprosy, because of the fear of being thrown out of their family and community. Fischer said, \"If you get early treatment, nobody can see that you ever had leprosy. This message is very important.\"\n\nFischer received the Indian government's Gandhi Peace Prize for 1997 in New Delhi on 5 January 1998 from the President of India, K. R. Narayanan. The selection committee was unanimous in its decision to award the prize to Fischer. The prize carried an award of 10 million rupees, which Fischer used to set up a foundation for his humanitarian work. In his acceptance speech at the Rashtrapati Bhavan, the official residence of the President of India, Fischer said, \"It is not enough to have compassion, \"daya\", \"karuṇā\", and pity for the leprosy patients. We lepers, we don't care about compassion. We want action.\" Referring to the government slogan of \"eradicating leprosy by 2000\", he said that his speech made people such as Sonia Gandhi, who were sitting in the front of the audience, very uncomfortable.\n\nFischer died in Copenhagen on 3 July 2006, aged 84 years. On 8 July 2006, a silent procession was organised at Ayikudi as a mark of respect. N.S. Rao, a former colleague of Fischer at the German consulate in Madras, said that Fischer \"applied a missionary zeal to anything he put his mind to. He and his wife were a perfect two-people team and worked all over the country with tremendous energy and indefatigable enthusiasm. After his wife's death, he had been carrying on his social work relentlessly.\" Rao added that Fischer's daughter, Karen Fischer Koch, carried on her father's work after his death.\n"}
{"id": "33985855", "url": "https://en.wikipedia.org/wiki?curid=33985855", "title": "Graduate medical education", "text": "Graduate medical education\n\nGraduate Medical Education (GME) refers to any type of formal medical education, usually hospital-sponsored or hospital-based training, pursued after receipt of the M.D. or D.O. degree in the United States This education includes internship, residency, subspecialty and fellowship programs, and leads to state licensure and board certification.\n"}
{"id": "901771", "url": "https://en.wikipedia.org/wiki?curid=901771", "title": "HELLP syndrome", "text": "HELLP syndrome\n\nHELLP syndrome is a complication of pregnancy characterized by hemolysis, elevated liver enzymes, and a low platelet count. It usually begins during the last three months of pregnancy or shortly after childbirth. Symptoms may include feeling tired, retaining fluid, headache, nausea, upper right abdominal pain, blurry vision, nosebleeds, and seizures. Complications may include disseminated intravascular coagulation (DIC), placental abruption, and kidney failure.\nThe cause is unknown. Usually it occurs in association with preeclampsia or eclampsia. Other risk factors include previously having the syndrome, a mother older than 25 years, and being white. Diagnosis is generally based on blood tests finding signs of red blood cell break down (LDH greater than 600 U/L), an AST greater than 70 U/L, and platelets of less than 100x10/L. If not all the criteria are present the condition is incomplete.\nTreatment generally involves delivery of the baby as soon as possible. This is particularly true if the pregnancy is beyond 34 weeks of gestation. Medications may be used to decrease blood pressure and blood transfusions may be required. Corticosteroids may be used to speed development of the babies lungs, if it is early in pregnancy.\nHELLP syndrome occurs in about 0.7% of pregnancies and affects about 15% of women with eclampsia or severe preeclampsia. Death of the mother is uncommon. Outcomes in the baby are generally related to how premature they are at birth. The syndrome was first named in 1982.\n\nThe first signs of HELLP start appearing midway through the third trimester, though the signs can appear in earlier and later stages. Symptoms vary in severity and between individuals and are commonly mistaken with normal pregnancy symptoms, especially if they are not severe.\n\nHELLP syndrome patients suffer from general discomfort followed by severe epigastric pain or right upper abdominal quadrant pain, accompanied by nausea, vomiting, backache, anaemia, and hypertension. Some patients may also suffer from a headache and visual issues. These symptoms may also become more severe at night time. As the condition progresses and worsens, a spontaneous hematoma occurs following the rupture of the liver capsule, which occurs more frequently in the right lobe. The presence of any combinations of these symptoms, subcapsular liver hematoma in particular, warrants an immediate check-up due to the high morbidity and mortality rates of this condition.\n\nElevated body mass index (BMI) and metabolic disorders, as well as, antiphospholipid-antibody syndrome (APLS) significantly increase the risk of HELLP syndrome in all female patients. Females who have had or are related to a female with previous HELLP syndrome complications tend to be at a higher risk in all their subsequent pregnancies.\n\nThe risk of HELLP syndrome is not conclusively associated with a specific genetic variation, but it is likely that a combination of genetic variations, such as \"FAS\" gene, \"VEGF\" gene, \"glucocorticoid receptor\" gene and the \"tol-like receptor\" gene, increase the risk.\n\nThe pathophysiology is still unclear and an exact cause is yet to be found. However, it shares a common mechanism, which is endothelial cell injury, with other conditions, such as acute renal failure and thrombotic thrombocytopenic purpura. Increasing the understanding of HELLP syndrome’s pathophysiology will enhance diagnostic accuracy, especially in the early stages. This will lead to advancements in the prevention, management, and treatment of the condition, which will increase the likelihood of both maternal and fetal survival and recovery.\n\nAs a result of endothelial cell injury, a cascade of pathological reactions manifests and become increasingly severe and even fatal as signs and symptoms progress. Following endothelial injury, vasospasms and platelet activation occur alongside the decreased release of the endothelium-derived relaxing factor and increased the release of von Willebrand factor (vWF), leading to general activation of the coagulation cascade and inflammation. Placental components, such as inflammatory cytokines and syncytiotrophoblast particles interact with the maternal immune system and endothelial cells, further promoting coagulation and inflammation. These interactions also elevate leukocyte numbers and interleukin concentrations, as well as increase complement activity.\n\nvWF degradation in HELLP syndrome is inhibited due to decreased levels of degrading proteins, leading to an increased exposure of platelets to vWF. As a result, thrombotic microangiopathies develop and lead to thrombocytopenia.\n\nAs a result of the high number of angiopathies, the erythrocytes fragment as they pass through the blood vessels with damaged endothelium and large fibrin networks, leading to macroangiopathic haemolytic anaemia. As a consequence of hemolysis, lactic acid dehydrogenase (LDH) and hemoglobin are released, with the latter binding to serum bilirubin or haptoglobin.\n\nDuring the coagulation cascade, fibrin is deposited in the liver and leads to hepatic sinusoidal obstruction and vascular congestion, which increase intrahepatic pressure. Placenta-derived FasL (CD95L), which is toxic to human hepatocytes, leads to hepatocyte apoptosis and necrosis by inducing the expression of TNFα and results in the release of liver enzymes. Hepatic damages are worsened by the disrupted portal and total hepatic blood flow that result as a consequence of the microangiopathies. Collectively, widespread endothelial dysfunction and hepatocellular damage result in global hepatic dysfunction often leading to liver necrosis, haemorrhages, and capsular rupture.\n\nEarly and accurate diagnosis, which relies on laboratory tests and imaging exams, is essential for treatment and management and significantly reduces the morbidity rate. However, diagnosis of the syndrome is challenging, especially due to the variability in the signs and symptoms and the lack of consensus amongst healthcare professionals. Similarities to other conditions, as well as normal pregnancy features, commonly lead to misdiagnosed cases or more often, delayed diagnosis.\n\nThere is a general consensus regarding the main three diagnostic criteria of HELLP syndrome, which include hepatic dysfunction, thrombocytopenia and microangiopathic haemolytic anaemia in patients suspected to have preeclampsia. \n\nA number of other, but less conclusive, clinical diagnostic criteria are also used in diagnosis alongside the main clinical diagnostic criteria for HELLP syndrome. \n\nImaging tests, such as ultrasound, tomography or magnetic resonance imaging (MRI), are instrumental in the correct diagnosis of HELLP syndrome in patients with suspected liver dysfunction. Unurgent cases must undergo MRI, but laboratory tests, such as glucose determination, are more encouraged in mild cases of HELLP syndrome.\n\nA classification system, which was developed in Mississippi, measures the severity of the syndrome using the lowest observed platelet count in the patients alongside the appearance of the other two main clinical criteria. Class I is the most severe, with a relatively high risk of morbidity and mortality, compared to the other two classes.\n\nAnother classification system, introduced in Memphis, categorises HELLP syndrome based on its expression.\n\nThe only current recommended and most effective treatment is delivery of the baby, as the signs and symptoms diminish and gradually disappear following the delivery of the placenta. Prompt delivery is the only viable option in cases with multiorgan dysfunction or multiorgan failure, haemorrhage and considerable danger to the fetus. Certain medications are also used to target and alleviate specific symptoms.\n\nCorticosteroids are of unclear benefit, though there is tentative evidence that they can increase the mother's platelet count.\n\nWith treatment, maternal mortality is about 1 percent, although complications such as placental abruption, acute renal failure, subcapsular liver hematoma, permanent liver damage, and retinal detachment occur in about 25% of women. Perinatal mortality (stillbirths plus death in infancy) is between 73 and 119 per 1000 babies of woman with HELLP, while up to 40% are small for gestational age. In general, however, factors such as gestational age are more important than the severity of HELLP in determining the outcome in the baby.\nHELLP syndrome affects 10-20% of pre-eclampsia patients and is a complication in 0.5-0.9% of all pregnancies. Caucasian women over 25 years of age comprise most of the diagnosed HELLP syndrome cases. In 70% of cases before childbirth, the condition manifests in the third trimester, but 10% and 20% of the cases exhibit symptoms before and after the third trimester, respectively. Postpartum occurrences are also observed in 30% of all HELLP syndrome cases.\n\nHELLP syndrome was identified as a distinct clinical entity (as opposed to severe pre-eclampsia) by Dr. Louis Weinstein in 1982. In a 2005 article, Weinstein wrote that the unexplained postpartum death of a woman who had haemolysis, abnormal liver function, thrombocytopenia, and hypoglycemia motivated him to review the medical literature and to compile information on similar women. He noted that cases with features of HELLP had been reported as early as 1954.\n\n"}
{"id": "13320422", "url": "https://en.wikipedia.org/wiki?curid=13320422", "title": "Healthcare in Sweden", "text": "Healthcare in Sweden\n\nThe Swedish health care system is mainly government-funded and decentralized, although private health care also exists. The health care system in Sweden is financed primarily through taxes levied by county councils and municipalities.\n\nSweden's health care system is organized and managed on three levels: national, regional and local. At the national level, the Ministry of Health and Social Affairs establishes principles and guidelines for care and sets the political agenda for health and medical care. The ministry along with other government bodies supervises activities at the lower levels, allocates grants and periodically evaluates services to ensure correspondence to national goals.\n\nAt the regional level, responsibility for financing and providing health care is decentralized to the 21 county councils. A county council is a political body whose representatives are elected by the public every four years on the same day as the national general election. The executive board or hospital board of a county council exercises authority over hospital structure and management, and ensures efficient health care delivery. County councils also regulate prices and level of service offered by private providers. Private providers are required to enter into a contract with the county councils. Patients are not reimbursed for services from private providers who do not have an agreement with the county councils. According to the Swedish health and medical care policy, every county council must provide residents with good-quality health services and medical care and work toward promoting good health in the entire population.\n\nAt the local level, municipalities are responsible for maintaining the immediate environment of citizens such as water supply and social welfare services. Recently, post discharge care for the disabled and elderly, and long term care for psychiatric patients was decentralized to the local municipalities.\n\nCounty councils have considerable leeway in deciding how care should be planned and delivered. This explains the wide regional variations.\n\nIt is informally divided into 7 sections: \"Close-to-home care\" (primary care clinics, maternity care clinics, out-patient psychiatric clinics, etc.), emergency care, elective care, in-patient care, out-patient care, specialist care, and dental care.\n\nAll citizens are to be given on line access to their own electronic health records by 2020. Many different record systems are used which has caused problems for interoperability. A national patient portal, ‘1177.se’ is used by all systems, with both telephone and online access. At June 2017 about 41% of the population had set up their own account to use personal e-services using this system. A national Health Information Exchange platform provides a single point of connectivity to the many different systems. There is not yet a national regulatory framework for patients’ direct access to their health information.\n\nPrivate companies in 2015 provide about 20% of public hospital care and about 30% of public primary care, although in 2014 a survey by the SOM Institute found that 69% of Swedes were opposed to private companies profiting from providing public education, health, and social care, with only about 15% actively in favour.\n\nIn April 2015 Västernorrland County ordered its officials to find ways to limit the profits private companies can reap from running publicly funded health services.\n\nCosts for health and medical care amounted to approximately 9 percent of Sweden’s gross domestic product in 2005, a figure that remained fairly stable since the early 1980s. By 2015 the cost had risen to 11.9% of GDP -the highest in Europe. Seventy-one percent of health care is funded through local taxation, and county councils have the right to collect income tax. The state finances the bulk of health care costs, with the patient paying a small nominal fee for examination. The state pays for approximately 97% of medical costs.\n\nWhen a physician declares a patient to be ill for whatever reason (by signing a certificate of illness/unfitness), the patient is paid a percentage of their normal daily wage from the second day. For the first 14 days, the employer is required to pay this wage, and after that the state pays the wage until the patient is declared fit.\n\nPrescription drugs are not free but fees to the user are capped at 2,200 kr per annum. Once a patient's prescriptions reach this amount, the government covers any further expenses for the rest of the year. The funding system is automated. The country's pharmacies are connected over the Internet. Each prescription is sent to the pharmacy network, which stores information on a patient's medical history and on the prescriptions fulfilled previously for that patient. If the patient's pharmaceutical expenses have exceeded the annual limit, the patient receives the medication free of charge at the point of sale, upon producing identification.\n\nIn a sample of 13 developed countries Sweden was eleventh in its population weighted usage of medication in 14 classes in 2009 and twelfth in 2013. The drugs studied were selected on the basis that the conditions treated had high incidence, prevalence and/or mortality, caused significant long-term morbidity and incurred high levels of expenditure and significant developments in prevention or treatment had been made in the last 10 years. The study noted considerable difficulties in cross border comparison of medication use.\n\nA limit on health-care fees per year exists; 150-300 SEK for each visit to a doctor, regardless if they are a private doctor or work at a local health-care center or a hospital. When visiting a hospital, the entrance fee covers all specialist visits the doctor deems necessary, like x-ray, rheumatism specialist, heart surgery operations and so on. The same fee is levied for ambulance services. After 1100 SEK have been paid, health-care for the rest of the year will be provided free of charge.\n\nDental care is not included in the general health care system, but is partly subsidized by the government. Dental care is free for youths up to 19 years of age, while a general dental care insurance (introduced in 1974) covers all inhabitants from the age of 20 onwards.\n\nMental health care is an integrated part of the health care system and is subject to the same legislation and user fees as other health care services. If an individual has minor mental health issues, he is attended to by a GP in a primary health setting; if the patient has major mental health issues he is referred to specialized psychiatric care in hospitals.\n\nAccording to the Euro health consumer index the Swedish score for technically excellent healthcare services, which they rated 10th in Europe in 2015, is dragged down by access and waiting time problems, in spite of national efforts such as Vårdgaranti. It is claimed that there is a long tradition of steering patients away from their doctor unless they are really sick.\n\nUrgent cases are always prioritized and emergency cases are treated immediately. The national guarantee of care, Vårdgaranti, lays down standards for waiting times for scheduled care, aiming to keep waiting time below 7 days for a visit to a primary care physician, and no more than 90 days for a visit to a specialist.\n\n\n"}
{"id": "29371166", "url": "https://en.wikipedia.org/wiki?curid=29371166", "title": "Henry S. Tanner (doctor)", "text": "Henry S. Tanner (doctor)\n\nHenry Samuel Tanner (February 7, 1831 - December 28, 1918) was a physician who advocated fasting. He fasted for 40 days in Manhattan, New York City in 1880.\n\nHe was born on February 7, 1831 in England to Hannah and Samuel Tanner. He claimed to have completed a 42 day fast in 1879, but was unable to prove it. On June 28, 1880 he began a forty day fast in Manhattan. His first meal after completing the fast was milk, watermelon, and half a pound of beefsteak. On his 81st birthday, in 1911 he proposed an 80 day fast in Los Angeles, California. He died on December 28, 1918 in San Diego, California.\n\nOn June 28, 1880, Tanner began a forty days' fast at Clarendon Hall in Manhattan. After originally intending to go without food or water, he was persuaded to drink, before going without water from the second to the tenth day. Tanner lost almost 40 pounds by the conclusion of the experiment, and against the advice of his doctors began consuming meat, fruits, wine and milk immediately after.\n\nBecause no one believed his claim that he had fasted for 42 days, in January 1880, Tanner, a practitioner of hygienic medicine, announced that he would repeat his experiment to show that humans can survive without food and would agree to submit himself to be placed “under the care of any medical society” that would provide adequate housing. On June 30, Tanner began his attempt to duplicate his 40-day fast and after the 6th day, the New York Times began a series of articles chronicling his day –to-day progress, each dispatch becoming more ominous in its anticipation that his death by starvation was imminent. As the twelfth night approached, a Times headline announced that “The End [was] Predicted to be at Hand”. But rather than deteriorating, by the twentieth day, Tanner’s condition improved and he “looked and acted better than ever”.\n\nOn August 7, the Times reported that a crowd of over 2,000 would witness Tanner break his 40-day fast at midnight. The usual admission price of 25 cents was raised to half a dollar resulting in a box office take of over $2,000. The many doctors on hand still expected him to keel over though upon re-feeding and although he re-fed on milk (which today would be strongly discouraged) he suffered only minimal nausea and some vomiting. A few days, later the Times began reporting on Tanner’s recovery, gaining back some of his weight and that by September 10, the “fasting doctor” had launched a lecture tour touting “starvation” as a cure for disease.\n\nThe fast has been the subject of many books and studies taken since 1880. His daily log of weight and physical condition were released to the public for further evaluation. \n\nTanner's fame in the coming decades was enough for Mark Twain to mention in passing, \"I think that the Dr. Tanners and those others who go forty days without eating do it by resolutely keeping out the desire to eat, in the beginning, and that after a few hours the desire is discouraged and comes no more\" in \"Following the Equator: A Journey Around the World\" in 1897.\n\n"}
{"id": "13072988", "url": "https://en.wikipedia.org/wiki?curid=13072988", "title": "Hudson River State Hospital", "text": "Hudson River State Hospital\n\nThe Hudson River State Hospital, is a former New York state psychiatric hospital which operated from 1873 until its closure in the early 2000s. The campus is notable for its main building, known as a \"Kirkbride,\" which has been designated a National Historic Landmark due to its exemplary High Victorian Gothic architecture, the first use of that style for an American institutional building. It is located on US 9 on the Poughkeepsie-Hyde Park town line.\n\nFrederick Clarke Withers designed the hospital's buildings in 1867. Calvert Vaux and Frederick Law Olmsted designed the grounds. It was intended to be completed quickly, but went far over its original schedule and budget. The hospital opened on October 18, 1871 as the Hudson River State Hospital for the Insane and admitted its first 40 patients. Construction, however, was far from over and would continue for another 25 years. A century later, it was slowly closed down as psychiatric treatment had changed enough that large hospitals were no longer needed, and its services had been served by the nearby Hudson River Psychiatric Center until that facility's closure in January 2012.\n\nThe campus was closed and abandoned in 2003 and since then has fallen into a state of disrepair. Authorities struggle with the risk of arson and vandals after suspicion of an intentionally set fire. The male bedding ward, south of the main building, was critically damaged in a 2007 fire caused by lightning. The property was sold to an unnamed buyer in November 2013.\n\nThe Hospital includes a number of unique buildings:\n\nThe entire facility was built over the last three decades of the 19th century, at great cost. Once complete, it would be used as intended for much of the first half of the next century. As psychiatry moved away from inpatient treatments, it began to decline in use until its closure at century's end. Today, it is slowly deteriorating out of public view as it awaits reuse.\n\nNew York had opened what has since become Utica Psychiatric Center in 1843, the first state-run institution for the mentally ill. By the Civil War it was reaching its capacity, so in 1866 then Governor Reuben Fenton appointed a five-member state commission to look for a site for a second hospital in the Hudson Valley between New York and Albany, to serve New York City and the counties of Eastern New York. In January of the following year the members reported to the governor that they had temporarily secured a tract of land overlooking the Hudson River north of Poughkeepsie, formerly part of the estates of James Roosevelt and William A. Davis. It would cost nothing as the citizens of Dutchess County would be offering it to the state as a gift. Two months later, the state accepted.\n\nA nine-member Board of Managers was created and appointed to initiate and oversee construction of the actual building. They chose architect Frederick Clarke Withers to design a building according to the Kirkbride Plan, then a popular theory for the design of mental institutions. Withers planned a building 1,500 feet (457 m) in length and over 500,000 square feet (45,000 m²) in area, most of it two wings that would house patients. It was the first institutional building in the U.S. designed in the High Victorian Gothic style. Calvert Vaux and Frederick Law Olmsted, designers of New York's Central Park, laid out the surrounding landscape. Like Withers, they had been mentored by the influential Andrew Jackson Downing in nearby Newburgh.\n\nThe centerpiece of his design was the administration building, which branched off into two wings, composed of six parallel pavilions that flanked the central structure. The two wings, designed to hold 300 patients of either sex, were divided by a chapel placed between them in the yard behind the administration building so that patients could not see into the rooms of the opposite sex. The building and landscape plan were meant to aid in patients' recovery, by giving them adequate space and privacy and imbuing their healing with a sense of grandeur.\n\nConstruction began in 1868, with the cost estimated at $800,000. Cost-saving measures included the construction of a new dock on the Hudson so that building materials could be shipped more directly to the site, quarrying and cutting the foundation stones on site, mixing concrete from local materials and hiring local craftsmen instead of a general contractor. The board also deviated from the plan it had sent the state, in particular by building a shorter female wing when it came to believe that fewer patients of that sex would be admitted. As a result, it is one of the few Kirkbride hospitals to have been built with asymmetrical wings.\n\nDespite the efforts to save money, the board was slightly over the $100,000 it had expected to spend that year, according to its first annual report. The main building was completed and opened, with 40 patients admitted, in October 1871. As work continued on other structures planned for the complex, so did the cost overruns. In 1873, the year county residents had been promised the hospital would be finished, the \"New York Times\" ran an editorial harshly criticizing the board for not only having gone way over budget but for lavish extravagance and waste:\nSome efforts were made to stop the project, but the legislature continued to appropriate funds despite further revelations like these. Construction continued until 1895, when further money could not be found. Despite this expenditure of time and money, the hospital's original plan was still not complete, and never would be.\n\nA Refrigerating Plant was built 1948 and a Tubercular Hospital was built in 1951 to the designs of Samuel Juster of DePace & Juster. Buildings continued to be opened and reopened in the 20th century, and as late as 1952 the institution was treating as many as 6,000 patients.\n\nChanges in the treatment of mental illness, such as psychotherapy and psychotropic drugs, were making large-scale facilities relics and allowing more patients to lead more normal lives without being committed. A major fire destroyed a hospital wing in the 1960s and threatened to spread to the administration building, but was halted in a connecting hallway. The section was rebuilt, although some large roof beams still showed evidence of the earlier fire. Though 1971 saw the addition of the Snow Recreational center, by the late 1970s the hospital administration had decided to shut down the two main wings as few patients were residing in them and due to neglect some of the floors had collapsed. The hospital housed 1,780 patients by 1976. The state offices of Mental Health and Historic Preservation clashed over a plan to demolish the wings, even after the National Historic Landmark designation in 1989.\n\nIn the 1990s, more and more of the hospital site would be abandoned as its services were needed less and less. It was consolidated with another Dutchess County mental hospital, Harlem Valley Psychiatric Center, in 1994 and closed in 2003. The center moved operations into a much smaller building nearby, Ross Pavilion, located on a hilltop on the east side of Rt 9G.\n\nIn 2005, the state sold the property and subsequently, the Empire State Development Corporation sold including the Main Building to Hudson Heritage LLC, a subsidiary of the Chazen Companies, for $2.75 million. Hudson Heritage and Chazen had planned to thoroughly renovate the Main Building into a combination hotel/apartment complex as the centerpiece of a residential/commercial campus, Hudson Heritage Park.\nRedevelopment plans hit two setbacks in the mid- to late-2000s: in 2005, the Town of Poughkeepsie imposed a moratorium on new construction to cope with its growth. Hudson Heritage had been seeking to have a \"historic revitalization district\" created for the property that would help spur its growth.\n\nThen, on May 31, 2007, lightning struck the sprawling south wing, which held male housing, causing one of the most serious fires in Dutchess County's history. It is unclear whether that portion of the building can be effectively restored after such severe damage. The Administration building was again hit by fire on the morning on April 27, 2018.\n\nIn the meantime, the property has remained closed to the public, with signs posted at fenced-off entrances. Local firefighters have complained, after dealing with two fires in April 2010 that appeared to be deliberately set, that the property is not adequately secured against trespassing.\n\nAs of May 2012, the campus is owned by CPC Resources, a subsidiary of the New York City-based Community Preservation Corporation, a non-profit mortgage lender that finances multifamily developments. CPC has placed the 162-acre parcel on the market, citing that the poor economy has hindered timely development. Walmart has shown a strong interest in the property, but Poughkeepsie Town Supervisor Todd Tancredi noted that the Town Board cannot envision such a large piece of land used for a single store.\n\nAn unnamed buyer purchased the campus for an undisclosed sum in November 2013. The closing for the property occurred on November 8, 2013.\n\nAn intentional fire was set on the morning of April 27, 2018 at the Hudson River Psychiatric Center, according to Fairview Fire District Chief. The fire was located in the wing of the old Main Administration building, and firefighters were dispatched around 3:45 AM; more than a dozen agencies responded with assistance.\n\n\n"}
{"id": "31923406", "url": "https://en.wikipedia.org/wiki?curid=31923406", "title": "Hyperacuity (scientific term)", "text": "Hyperacuity (scientific term)\n\nThe sharpness of our senses is defined by the finest detail we can discriminate. Visual acuity is measured by the smallest letters that can be distinguished on a chart and is governed by the anatomical spacing of the mosaic of sensory elements on the retina. Yet spatial distinctions can be made on a finer scale still: misalignment of borders can be detected with a precision up to 10 times better than visual acuity, as already shown by Ewald Hering in 1899. This hyperacuity, transcending by far the size limits set by the retinal 'pixels', depends on sophisticated information processing in the brain.\n\nThe best example of the distinction between acuity and hyperacuity comes from vision, for example when observing stars on a night sky. The first stage is the optical imaging of the outside world on the retina. Light impinges on the mosaic of receptor sense cells, rods and cones, which covers the retinal surface without gaps or overlap, just like the detecting pixels in the film plane of digital cameras. Each receptor accepts all the light reaching it but acts as a unit, representing a single location in visual space. This compartmentalization sets a limit to the decision whether an image came from a single or a double star (resolution). For a percept of separately articulated stars to emerge, the images of the two must be wide enough apart to leave at least one intervening pixel relatively unstimulated between them. This defines the resolution limit and the basis of visual acuity.\n\nA quite different mechanism operates in hyperacuity, whose quintessential example and the one for which the word was initially coined, is vernier acuity: alignment of two edges or lines can be judged with a precision five or ten times better than acuity. In computer graphics the phrase “sub-pixel resolution” is sometimes used in discussions of anti-aliasing and geometrical superresolution. Though what is in fact involved is not \"resolution\" (is it one or two? – a \"qualitative\" distinction) but \"localization\" (exactly where? – a \"quantitative\" judgment) it captures the process. When an image spreads across several pixels, each with graded intensity response but only a single spatial value, the position of the image center can be located more exactly than the width of the pixel, much like the mean of a histogram can be calculated to a fraction of the bin width.\n\nIn the figure on the right, the retinal mosaic has superimposed on it, at top, the images of two stars at resolution limit when the intervening gap assures judgment that there are two stars and not a single elongated one. Shown below are the images of two separate short lines; the precision of the read-out of their location difference transcends the dimension of the mosaic elements.\n\nDetails of the neural apparatus for hyperacuity still await discovery. That the hyperacuity apparatus involves signals from a range of individual receptor cells, usually in more than one location of the stimulus space, has implications concerning performance in these tasks. Low contrast, close proximity of neighboring stimuli (crowding), and temporal asynchrony of pattern components are examples of factors that cause performance deficits. Of some conceptual interest are age changes and susceptibility to perceptual learning which can help in understanding underlying neural channeling.\n\nThe distinction between resolving power or acuity, literally sharpness, which depends on the spacing of the individual receptors through which the outside world is sampled, and the ability to identify individual locations in the sensory space is universal among modalities. There are many other examples where the organism’s performance substantially surpasses the spacing of the concerned receptor cell population. The normal human has only three kinds of color receptors in the retina, yet in color vision, by subtly weighing and comparing their relative output, one can detect thousand of hues. Braille reading involves hyperacuity among touch receptors in the fingertips. We can hear many more different tones than there are hair cells in the cochlea; pitch discrimination, without which a violin could not be played in tune, is a hyperacuity. Hyperacuity has been identified in many animal species, for example in the detection of prey by the electric fish, echolocation in the bat, and in the ability of rodents to localize objects based on mechanical deformations of their whiskers.\n\nIn clinical vision tests, hyperacuity has a special place because its processing is at the interfaces of the eye's optics, retinal functions, activation of the primary visual cortex and the perceptual apparatus. In particular, the determination of normal stereopsis is a hyperacuity task. Hyperacuity perimetry is used in clinical trials evaluating therapies for retinal degenerative changes.\n"}
{"id": "2116834", "url": "https://en.wikipedia.org/wiki?curid=2116834", "title": "Institute of Virus Preparations", "text": "Institute of Virus Preparations\n\nThe Institute of Virus Preparations was an agency of the former Soviet Union.\n\nIt was the equivalent to the Centers for Disease Control and the United States Army Medical Research Institute for Infectious Diseases (USAMRIID), founded by Kent Truslow. At one time, the Institute for Virus Preparations was one of the World Health Organization (WHO) collaborating centers which served along with the CDC as a central collection point for smallpox (variola major) samples during the eradication effort in the 1970s. The Institute remained one of the collaborating centers. Although the smallpox samples were transferred to the Vector Institute in 1998 officially, contradictory reports indicate this may have happened several years earlier. Additionally, reliable reports indicate that the smallpox samples were almost certainly used in a very advanced biological warfare program prior to this time.\n"}
{"id": "46997904", "url": "https://en.wikipedia.org/wiki?curid=46997904", "title": "Journal of Women &amp; Aging", "text": "Journal of Women &amp; Aging\n\nThe Journal of Women & Aging is a quarterly peer-reviewed healthcare journal focusing on health challenges facing women in their later years. The journal was established in 1989 and is published by Routledge. The editor-in-chief is Francine Conway (of Rutgers University).\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 0.846, ranking it 22nd out of 40 journals in the category \"Women's Studies\".\n\n"}
{"id": "47024473", "url": "https://en.wikipedia.org/wiki?curid=47024473", "title": "Kumdang-2", "text": "Kumdang-2\n\nKumdang-2 is an alleged cure for AIDS, Ebola, MERS and tuberculosis created in North Korea. According to the website Minjok Tongshin, a version of the drug was originally produced in 1996. The name means \"golden sugar\" in Korean.\n\nIt is manufactured by the Pugang Pharmaceutical Company. According to the Korean Central News Agency, the drug's ingredients include ginseng, small amounts of rare earth metals and trace amounts of gold and platinum. According to the KCNA, it can also cure cancer, morning sickness, and \"harm from the use of computers\". The drug was mentioned during deadly bird flu outbreaks in 2006 and 2013.\n"}
{"id": "1009651", "url": "https://en.wikipedia.org/wiki?curid=1009651", "title": "Male bonding", "text": "Male bonding\n\nIn ethology and social science, male bonding is the formation of close personal relationships, and patterns of friendship or cooperation between males.\n\nIn the context of human relationships, male bonding is used to describe friendship between men, or the way in which men befriend each other. The expression is sometimes used synonymously with the word camaraderie. The first widely noticed use of the term was in \"Men in Groups\" (1969; 2004) by anthropologist Lionel Tiger.\n\nMale bonding can take place in various locations such as gyms, locker rooms, sport fields or courts, fraternities, and barbershops. This can include playing musical instruments, video games, business ventures, creative endeavors, journeys, quests, sporting activities, fishing, hunting, camping, gambling, social drinking, smoking cigars, working with tools, or even just conversing.\n\n\n"}
{"id": "49127339", "url": "https://en.wikipedia.org/wiki?curid=49127339", "title": "Maxim Konchalovsky", "text": "Maxim Konchalovsky\n\nMaxim Petrovich Konchalovsky (; born 1 (13) October 1875, Odessa - November 29, 1942, Moscow) was a Russian and Soviet doctor, close clinician, founder of the school of internal medicine clinic.\n\nThe elder brother of the artist Pyotr Konchalovsky.\n\n"}
{"id": "33891488", "url": "https://en.wikipedia.org/wiki?curid=33891488", "title": "Mentor Foundation", "text": "Mentor Foundation\n\nMentor International (Mentor Foundation) is an international youth development NGO working in the field of drug use and substance abuse prevention. It was founded in 1994 by Queen Silvia of Sweden in collaboration with the World Health Organisation. Current trustees include Yvonne Thunell, current Chairman, Stefan Persson, Chairman of H&M and Bertil Hult, CEO and founder of EF Education First. Honorary board members include Queen Noor of Jordan, Henri, Grand Duke of Luxembourg and Talal bin Abdul-Aziz Al Saud and Turki bin Talal bin Abdul Aziz Al Saud of the Saudi Royal Family.\n"}
{"id": "25946752", "url": "https://en.wikipedia.org/wiki?curid=25946752", "title": "Mount Pleasant Mental Health Institute", "text": "Mount Pleasant Mental Health Institute\n\nThe Mount Pleasant Mental Health Institute was a psychiatric institution located in Mount Pleasant, Iowa, USA. Originally known as the Iowa Lunatic Asylum, it opened in 1861. It is located on the same campus as The Mount Pleasant Correctional Facility. There was also a labyrinth of underground tunnels which connect every building. Like most asylums of its time, it had a gruesome and dark history. Remnants of this are the graveyard, hydrotherapy tubs and lobotomy equipment. It was the first asylum in Iowa and was built under the Kirkbride Plan. It was closed after a massive fire destroyed most of the building and left the rest of the complex beyond repair.\n\nIt was constructed between 1855 and 1865 at a cost of $400,000. The first patient was admitted in February 1861. It is a Kirkbride building, and was the first asylum in Iowa. Over the years, the name was changed to the Mount Pleasant State Hospital because of the housing of drug addicts, geriatrics, alcoholics, etc. The Mount Pleasant Mental Health Institute is the oldest of the four Iowa Department of Human Services facilities serving persons affected by mental illness.\n\nIn 1936, a fire destroyed most of the administration section, leaving only a kitchen area at the back. In 1946, the facility reached its peak occupancy of 1,581 patients. Since 1945, new therapies and medications had helped to lower the facility's population, and the individual's average length of stay was reduced from years to a matter of weeks.\n\nThis allowed the facility to release many of its patients and eventually reassign the patients to Cherokee Mental Health Institute (Cherokee, Iowa]]), Clarinda Treatment Complex (Clarinda, Iowa) and Independence State Hospital (Independence, Iowa), which are still in use today. It has been known by many names, including the Mount Pleasant Insane Asylum, the Mount Pleasant Hospital for the Insane and the Mount Pleasant Mental Health Institute.\n\nIowa Governor Terry Branstad announced, much to the chagrin of citizens and legislators, that he would close Mount Pleasant and Clarinda MHIs in 2015. Despite the outcry in Iowa (actually more inpatient beds are urgently needed), and despite the questions about the legality of these shut-downs, Branstad appears immovable. There are plans to develop a \"crisis line\", which will treat chronic mental health disorders like schizophrenia.\n\n"}
{"id": "12040284", "url": "https://en.wikipedia.org/wiki?curid=12040284", "title": "Mucogingival junction", "text": "Mucogingival junction\n\nA mucogingival junction is an anatomical feature found on the intraoral mucosa. The mucosa of the cheeks and floor of the mouth are freely moveable and fragile, whereas the mucosa around the teeth and on the palate are firm and keratinized. Where the two tissue types meet is known as a mucogingival junction.\n\nThere are three mucogingival junctions: on the facial of the maxilla and on both the facial and lingual of the mandible. The palatal gingiva of the maxilla is continuous with the tissue of the palate, which is bound down to the palatal bones. Because the palate is devoid of freely moveable alveolar mucosa, there is no mucogingival junction.\n\nThe clinical importance of the mucogingival junction is in measuring the width of attached gingiva. Attached gingiva is important because it is bound very tightly to the underlying alveolar bone and provides protection to the mucosa during functional use of the structures of the oral cavity during function, such as chewing. Without attached gingiva, the freely moveable alveolar mucosa, being more fragile, would suffer injury during eating and cleansing activities, such as brushing of the teeth.\n\nThe width of attached tissue is critical, because the more there is available provides a greater sense of protection against the aforementioned insults to the tissue. Using the mucogingival junction as the boundary demarcating the apical border of the attached gingiva, a periodontal probe in inserted into the gingival sulcus to measure how much of the keratinized gingiva coronal to the mucogingival junction is in fact attached to the underlying bone. The depth of the gingival sulcus, determined by the depth to which the probe enters the sulcus, is not attached to the underlying bone, and is subtracted from the total height of the keratinized tissue.\n\nThus, if the entire height of the keratinized gingiva, from the free gingival margin to the mucogingival junction is 8 mm, and the probing depth on the tooth at that location is 2 mm, the effective width of attached gingiva is 6 mm.\n\nIf the probe enters the sulcus and can descend \"up to or beyond\" the mucogingival junction, that area is said to represent a \"mucogingival defect\".\n"}
{"id": "37770686", "url": "https://en.wikipedia.org/wiki?curid=37770686", "title": "NHS Professionals", "text": "NHS Professionals\n\nNHS Professionals is an organisation in the United Kingdom that supplies temporary staff to the National Health Service (NHS). It manages temporary staff banks on behalf of more than 55 NHS Trusts across England.\n\nThe concept of NHS Professionals, an NHS-owned solution to the growing issue of temporary workforce requirements, was first introduced in 2001 by the Secretary of State for Health and Social Care. By 2003 the NHS Professionals service was being provided via four main host trusts and there was significant variation between them in terms both of activity and performance. To combat this, NHS Professionals was established as a Special Health Authority (SpHA) on 1 April 2004, a nationally branded managed service for temporary staff in the NHS. NHS Professionals underwent further reorganisation to improve its service and in 2010 it was dissolved as a Special Health Authority and became NHS Professionals Ltd, a company wholly owned by the Secretary of State for Health in April 2010. The head office is in Watford, Hertfordshire, supported by a small finance team located in Tingley, Yorkshire as well as a client relationship team based both remotely and on-site at client trusts around the country. It announced its first ever surplus/profits in August 2010, and it continues to generate a surplus.\n\nIn April 2011, Stephen Dangerfield was announced as the chief executive of NHS Professionals. He has been with NHS Professionals since 2007, originally as Director of Operations and then as Chief Operating Officer. Robin Williams is chairman of NHS Professionals. He has experience in corporate finance, outsourcing, support services, manufacturing, contracting and construction, IT systems and managed services. He is a qualified Chartered Accountant and a graduate engineer.\n\nIn October 2014 Sky News said ministers were examining the future of NHS Professionals, saying the agency’s sale, at an estimated price of between £50 million and £100 million, was “a very real possibility”. \n\nIn April 2016, Nick Kirkbride replaced Stephen Dangerfield as chief executive officer of NHS Professionals.\n\nIn September 2017 plans to sell it were abandoned.\nNick Kirkbride was replaced by Colin McCready\n\nPrimarily it serves the needs of the NHS by offering NHS trusts temporary staffing solutions as a managed service. It also serves the temporary working needs of healthcare professionals who work in the NHS.\n\nThe company works in partnership with NHS trusts supporting their workforce strategy attempting improving their temporary workforce provision. As a managed bank service, NHS Professionals takes over the day-to-day management of the Trust bank to help control demand for additional hours, minimise agency expenditure and improve compliance of temporary staff.\n\nNHS Professionals gives staff the opportunity to work flexible hours at their convenience, across various trusts. It claims to be the NHS's largest temporary staff bank workforce with 70,000 on its books. Members of the NHS Professionals’ bank workforce are referred to as flexible workers. This refers to their flexible working patterns and contracts of engagement which contrasts with permanently employed trust staff.\n\nBank workers have access to the following benefits:\n\nNHS Professionals employs a wide variety of healthcare staff including: General and specialist nurses, care support workers (healthcare assistants), doctors, midwives, administrative and clerical, allied health professionals, healthcare scientists, support services among other healthcare professionals.\n\nAdministrative and clerical workers need to have relevant skills and experience to join the workforce. All doctors, nurses and healthcare assistants are required to have at least six months' experience within the last two years in the UK and at an NHS hospital.\n\n\n"}
{"id": "32825021", "url": "https://en.wikipedia.org/wiki?curid=32825021", "title": "Nahid Toubia", "text": "Nahid Toubia\n\nNahid Toubia (born 1951) is a Sudanese surgeon and women's health rights activist, specializing in research into female genital mutilation.\n\nToubia is the co-founder and director of RAINBO, the Research, Action and Information Network for Bodily Integrity of Women. She is an associate professor at Columbia University School of Public Health. She sits on scientific and advisory committees for the World Health Organization, UNICEF, and UNDP. She is also vice-chair of the advisory committee of the Women's Rights Watch Project of Human Rights Watch.\n\nFocusing on reproductive health and gender inequality in Africa and the Middle East, Toubia is the author or co-author of several books, including \"Women of the Arab World: The Coming Challenge\" (1988), \"Female Genital Mutilation: A Call for Global Action\" (1995), and \"Female Genital Mutilation: A Guide to Worldwide Laws and Policies\" (2000).\n\nToubia was born in Khartoum, Sudan, and attended medical school in Egypt. In 1981 she completed her surgical training in the United Kingdom, gaining an MPhil and a PhD in Public Health & Policy from the London School of Hygiene and Tropical Medicine. She became a fellow of the Royal College of Surgeons in 1981, and the first female surgeon in Sudan.\n\nIn 1985 she returned to Sudan, where she was head of paediatric surgery at Khartoum Teaching Hospital, and set up her own emergency clinic. As a result of the country's political instability she returned to the UK, and began her research into female genital mutilation (FGM). From 1990 she worked for four years at the Population Council in New York City.\n\nToubia is the founder and president of Research, Action and Information Network for the Bodily Integrity of Women (Rainbo), an international organisation which works to eliminate FGM through women’s self-empowerment and social change. The organisation has offices in New York City and London, and works in Uganda, South Africa, the Gambia, and Nigeria. Rainbo played a prominent role in changing the view of FGM from being a predominantly medical concern to a human rights issue.\n\nIn 2002 Toubia told the BBC World Service that the campaign against FGM was fundamentally about changing women's consciousness, and empowering them to change their social position. She said while most African governments, health professionals and NGOs had the issue on their agenda, the greatest challenge was at grassroots level. She said \"By allowing your genitals to be removed [it is perceived that] you are heightened to another level of pure motherhood - a motherhood not tainted by sexuality and that is why the woman gives it away to become the matron, respected by everyone. By taking on this practise, which is a woman's domain, it actually empowers them. It is much more difficult to convince the women to give it up, than to convince the men.\" \n\n"}
{"id": "4418506", "url": "https://en.wikipedia.org/wiki?curid=4418506", "title": "Orthotic Prosthetic Center", "text": "Orthotic Prosthetic Center\n\nThe Orthotic Prosthetic Center (OPC) is a family owned and operated prosthetics lab with offices in Fairfax, Virginia and Rockville, Maryland. The company is also featured on \"\", a reality TV show that debuted in January 2006 on the Discovery Health Channel. OPC was founded in 1980 by Joan Weintrob who was the first woman in the U.S. to earn an orthotics and prosthetics certification from the American Board for Certification.\n\n"}
{"id": "51508221", "url": "https://en.wikipedia.org/wiki?curid=51508221", "title": "Pauline Chaponnière-Chaix", "text": "Pauline Chaponnière-Chaix\n\nPauline Chaponnière-Chaix (Geneva, 1 November 1850 – Geneva, 6 December 1934) was a Swiss nurse, feminist and suffragette. She was one of four employees of the International Committee of the Red Cross after World War I, and served as president of the International Council of Women during the period of 1920-22.\n"}
{"id": "53683812", "url": "https://en.wikipedia.org/wiki?curid=53683812", "title": "Physiological functional capacity", "text": "Physiological functional capacity\n\nPhysiological functional capacity (PFC) is the ability to perform the physical tasks of daily life and the ease with which these tasks can be performed. PFC declines at some point with advancing age even in healthy adults, resulting in a reduced capacity to perform certain physical tasks. This can eventually result in increased incidence of functional disability, increased use of health care services, loss of independence, and reduced quality of life.\n\n"}
{"id": "11474489", "url": "https://en.wikipedia.org/wiki?curid=11474489", "title": "Rada (fiqh)", "text": "Rada (fiqh)\n\nRaḍāʿ or riḍāʿa (  , \"breastfeeding\") is a technical term from Sunni Islamic jurisprudence meaning \"the suckling which produces the legal impediment to marriage of foster-kinship\". The term derives from the infinitive noun of the Arabic word \"radiʿa\" or \"radaʿa\" (\"he sucked the breast of his mother\"). Often it is translated as \"fosterage\" or \"milk kinship\".\n\nThe concept of \"radāʿ\" derives from Islamic and pre-Islamic notions concerning the state of consanguinity created between wet nurse and unrelated nursling—that is, a woman and a baby other than her own—through the act of breastfeeding. \"Radāʿ\" also defines the links between various relations and family members of both wet nurse and baby, such that not only are the two forbidden in marriage to one another, but so are their relations in various combination (e.g. the nursling's biological brother with the milk-mother's biological daughter). Conversely, the milk-relationship allows usually forbidden familiarities between the two, particularly if the nursling is male and of adult stature, such as viewing the milk-mother unveiled or in private, exactly as if he were a relation.\n\n\"Radāʿ\" receives extensive treatment in the Islamic jurisprudence (\"fiqh\") of the classical jurists (\"faqih\"). A primary feature of such works is the delineation of which relationships are subject to prohibition once the milk relationship is established. The following are the sorts of questions directed to the founder of the Hanbali school of jurisprudence by his son:\n\nOther common topics included the following:\n\n\n\"ʿAdad al-radāʿ al-muharrim\", or minimal number of sucklings necessary to establish the milk-kinship, was the subject of extensive debate and ever more elaborate exegetical theorizing. For the adherents of older schools of law, such as the Malikis and Hanafis, one suckling was enough. Others, such as the Shāfi'īs, maintained that the minimum number was five or ten, and that in fact a Qur'ainic verse had once stipulated this exact number until its wording had been expurgated from the Qur'ānic text The following tradition (\"hadith\") treats both this topic as well as that of \"radāʿ al-kabīr\", or suckling of an adult:\n\nFor most jurists (Ibn Hazm being one prominent exception), the bar to marriage was effective only if the nursling was an infant. Yet even these allowed that a new relationship resulted between the two; Ibn Rushd, for example, ruled that the woman could now comport herself more freely in front of the nursed adult male, such as appearing before him unveiled. The famous traditionist Muhammad al-Bukhari was forced to resign his position of \"mufti\" and leave the city of Bukhara after ruling that two nurslings who suckled from the same farm animal became milk-siblings.\n\nAs per Quranic guidelines, non-mahram (not related) people are forbidden for each other as long as they are not legally married. Shi'ite Islam prohibits marriage to the consanguineous kin of a milk-parent. In Shi'ite societies, the wet nurse was always from a subordinate group, so that marriage to her kin would not have been likely.\n\nIn May 2007 Dr. Izzat Atiyya, lecturer at Cairo's Al-Azhar University, issued a \"fatwa\" that suggested that male and female colleagues could use breastfeeding to get around a religious ban on being alone together. The \"fatwa\" said that if a woman fed a male colleague \"directly from her breast\" at least five times they would establish a family bond and thus be allowed to be alone together at work. \"Breast feeding an adult puts an end to the problem of the private meeting, and does not ban marriage,\" he ruled. \"A woman at work can take off the veil or reveal her hair in front of someone whom she breastfed.\"\n\nThe \"fatwa\" sparked outrage and embarrassment, with critics deriding the author on Egyptian television. The university suspended the lecturer, who headed the university's \"hadith\" department. The \"fatwa\" was widely publicized by Arabic-language satellite television channels and was discussed in the Egyptian parliament. After being threatened with disciplinary action by the university, Atiyya issued a retraction, saying the \"fatwa\" was \"a bad interpretation of a particular case\" during the time of Muhammad and that it was based on the opinions of only a minority of scholars. Egypt's minister of religious affairs, Mahmoud Zaqzouq, has called for future \"fatwas\" to \"be compatible with logic and human nature\".\n\nIn 2010, a clerical adviser to the Royal court and Ministry of Justice issued a fatwa suggesting that women should provide breast milk to their employed drivers thereby making them relatives (a concept known as \"Rada\"). The driver could then be trusted to be alone with the woman. The fatwa was ridiculed by women campaigners.\n\n\n\n"}
{"id": "10754696", "url": "https://en.wikipedia.org/wiki?curid=10754696", "title": "Sam Boardman-Jacobs", "text": "Sam Boardman-Jacobs\n\nSam Boardman-Jacobs (born 1942) is a Wales-based playwright, director scenographer and recently choreographer, since receiving a master's degree from Trinity/ Laban. He now commutes between France and the UK.\n\nBoardman-Jacobs was Reader in Theatre & Media Drama at the University of Glamorgan. His research interests include Holocaust drama, Yiddish theatre, gay and lesbian theatre, Spanish playwright Federico García Lorca, and the Spanish Civil War. These interests are reflected in his plays.\n\nHe won acclaim for his work on Holocaust and Yiddish drama with the Manchester Youth Theatre and received a grant from the European Association of Jewish Culture in 2002 for his play \"Trying To Be\", an exploration of Jewish identity set in contemporary Britain. \nSam recently took an MA in Choreography at Laban, London, and now makes choreographic dance theatre with Found Reality Dance Theatre, Cardiff, of which he is artistic director.\n\n\"Play Federico For Me\" is the fictional story of Catalan actress Margarita Xirgu, who, during her exile after the Spanish Civil War, depends upon the ghost of Federico García Lorca, in her political-artistic battle with Eva Perón over the first performance of Lorca's \"The House of Bernarda Alba\". His translation and adaptation of Lorca's \"El público\" was produced by the Found Reality Theatre Company in 2005. His 2007 radio play, \"The Sixth Column Has Better Legs\", describes the experiences of four chorus girls in Madrid while the city is under siege.\n\n\"Passion for the Impossible\" tells the story of Violette Leduc and Jean Genet in wartime Paris and \"Red Hot and Blue\" is the story of singer Libby Holman, on the night before her suicide, as she looks back over a life that included a murder trial, an affair with Montgomery Clift and early Civil Rights campaigning during the Second World War.\n\nIn 2003 he taught for the Lemonia Disabled Writers' Residential Course, a project organised by Graeae Theatre Company, Writernet and Tŷ Newydd. The production of his 2004 play, \"Embracing Barbarians\", based on the political and sexual fantasies of dying Greek poet Constantine Cavafy, Sam attempted to make the piece accessible to both deaf and hearing performers and audiences, while casting a deaf performer in the role of a hearing character.\n\nHe has taught on several Writing Menoring and Dramaturgy courses at venues ranging from The Soho Theatre and The Actor's centre London to the Arvon Foundation and Ty Newydd in North Wales and on MA in Scriptwriting courses in Cardiff and Exeter. He now teaches Master classes in Scriptwriting and Dramaturgy in France and the UK.\n\nHe was also a scriptwriter for 12 years on BBC Radio 4's \"The Archers\" and one of the writers for Channel 4's \"Brookside\". He translates from Spanish to English.\n\n\n\nwith Found Reality Dance Theatre:\n\n"}
{"id": "4955228", "url": "https://en.wikipedia.org/wiki?curid=4955228", "title": "Spermatogenesis arrest", "text": "Spermatogenesis arrest\n\nSpermatogenesis arrest is known as the interruption of germinal cells of specific cellular type, which elicits an altered spermatozoa formation. Spermatogenic arrest is usually due to genetic factors resulting in irreversible azoospermia. However some cases may be consecutive to hormonal, thermic, or toxic factors and may be reversible either spontaneously or after a specific treatment. Spermatogenic arrest results in either oligospermia or azoospermia in men. It is quite a difficult condition to proactively diagnose as it tends to affect those who have normal testicular volumes; a diagnosis can be made however through a testicular biopsy.\n\nSpermatogenic arrest results in either oligospermia or azoospermia as mentioned above. Oligospermia is when extremely low concentrations of fertile sperm are found in semen or ejaculate, while azoospermia is when no fertile sperm are found in the semen or ejaculate. \n\nSpermatogenesis is controlled by androgens, namely testosterone and follicle-stimulating hormone (FSH), these are the most important androgens that control the process. FSH uses very specific G-coupled receptors that can be found only on Sertoli cells, this hormone is secreted by the pituitary gland, located in the brain. While testosterone, is produced within the testicles by Leydig cells. This hormone is the main androgenic steroid in the process of spermatogenesis and is regulated by a hormone known as luteinizing hormone. FSH plays a role in the spermatogenic capacity of the adult male as it controls the proliferation of Sertoli cells during either the perinatal or pubertal period, or both. However, testosterone has been found to be the most important hormone that is responsible for both the initiation and the maintenance of spermatogenesis . It is known that spermatogenesis is under the control of androgens, but germ cells (that will become gametes), do not express a functional androgen receptor, which are activated by the binding of androgenic hormones. It has been found through studies that spermatogenetic arrest tends to occur in the late spermatocyte/spermatid stage when the androgen receptor activation in Sertoli cells is interrupted or affected in some way. However, other studies have found that the condition can be due to either genetic factors or a variety of secondary factors. \nWhen using chemotherapy treatments, the possibility of azoospermia is dependent on the dose, duration, number and type of drugs used; the male’s fertility status before the treatment occurred is also taken into consideration. \nThe use of radiotherapy can cause a temporary bout of azoospermia, this however, is dependant solely on the nature of the dose that are delivered to the testes. Those who experience less than 100 rads will recover in 9-18 months, doses of 200-300 rads will recover in 30 months and doses of 400-600 rads will recover in less than or equal to five years. An irreversible sterility may occur however, for those experiencing a single dose field with 600-800 rads. \nStudies have shown that Vitamin A deficiencies in rats \n, as well as zinc deficiencies in human males may prevent the normal functioning of spermatogenesis. \nHeat may also be the cause of oligozoospermia which can lead to both partial and reversible spermatogenic arrest. \nAfter the occurrence of an infectious disease in humans, such as hypothermia and/or the presence of toxic or infectious factors spermatogenic arrest is likely to follow, however, the condition may be normalized once antibiotic and anti-inflammatory treatments have been put into effect. \n\nVarious treatments have been discovered in order to aid those with spermatogenesis arrest, one of these being through the use of arginine. A study done by Jungling and Bunge in 1976 had a small breakthrough in the field by orally distributing arginine, daily to a group of infertile men. Of the eighteen men in the test group only one experienced an increase in sperm count, while others saw no improvement; these men also experienced a decreased sperm motility. However, one of the patients in the group successfully impregnated his wife while taking part in the study. More recently, more successful treatments have been developed, such as through the use of gonadotropin treatment. A study conducted by Selman and El-Danasouri in 2006 proved that using long-term gonadotropin therapy on infertile men can improve sperm production quantitatively and increase sperm population in some patients and can in turn provide a successful in-vitro fertilization treatment. These results were found using men that had normal hormone levels but suffered from spermatogenic arrest. These men were treated using FSH treatments and had testicular biopsy’s performed on them before and after the treatment had been administered in order to track progress. \n\n"}
{"id": "11062680", "url": "https://en.wikipedia.org/wiki?curid=11062680", "title": "Stress-related disorders", "text": "Stress-related disorders\n\nStress-related disorders can include mental health disorders that are a result of an atypical response to both short and long-term anxiety due to physical, mental, or emotional stress. These disorders can include, but are not limited to obsessive-compulsive disorder and posttraumatic stress disorder.\n\nStress is a conscious or unconscious psychological feeling or physical condition resulting from physical or mental 'positive or negative pressure' that overwhelms adaptive capacities. It is a psychological process initiated by events that threaten, harm or challenge an organism or that exceed available coping resources and it is characterized by psychological responses that are directed towards adaptation. Stress is wear and tear on the body in response to stressful agents. Hans Selye called such agents stressors and said they could be physical, physiological, psychological or sociocultural.\nAnd stress is not an anxiety disorder and it is not a normative concept.\n\nA person typically is stressed when positive or negative (e.g., threatening) experiences temporarily strain or overwhelm adaptive capacities. Stress is highly individualized and depends on variables such as the novelty, rate, intensity, duration, or personal interpretation of the input, and genetic or experiential factors. Both acute and chronic stress can intensify morbidity from anxiety disorders. One person's fun may be another person's stressor. For an example, panic attacks are more frequent when the predisposed person is exposed to stressors.\n\nStress-reduction strategies can be helpful to many stressed/anxious people. However, many anxious persons cannot concentrate enough to use such strategies effectively for acute relief. (Most stress-reduction techniques have their greatest utility as elements of a prevention plan that attempts to raise one's threshold to anxiety-provoking experiences.)\n\nFive core concepts are used to reduce anxiety or stress.\n\n\nDefense mechanisms are behavior patterns primarily concerned with protecting ego. Presumably the process is unconscious and the aim is to fool oneself. It is intra psychic processes serving to provide relief from emotional conflict and anxiety. Conscious efforts are frequently made for the same reasons, but true defense mechanisms are unconscious.\n\nSome of the common defense mechanisms are:\ncompensation, conversion, denial, displacement, dissociation, idealization, identification, incorporation, introjection, projection, rationalization, reaction formation, regression, sublimation, substitution, symbolization and undoing.\n\nThe major function of these psychological defenses is to prevent the experiencing of painful emotions. There are several major problems with their use.\n\n\n\n\n\n\nAcute stress disorder occurs in individuals without any other apparent psychiatric disorder, in response to exceptional physical or psychological stress. While severe, such reactions usually subside within hours or days. The stress may be an overwhelming traumatic experience (e.g. accident, battle, physical assault, rape) or unusually sudden change in social circumstances of the individual, such as multiple bereavement.\nIndividual vulnerability and coping capacity play a role in the occurrence and severity of acute stress reactions, as evidenced by the fact that not all people exposed to exceptional stress develop symptoms. However, an acute stress disorder falls under the class of an anxiety disorder. \n\nSymptoms show considerable variation but usually include:\nAn initial state of \"DAZE\" with some constriction of the field of consciousness and narrowing of attention, inability to comprehend stimuli, disorientation. Followed either by further withdrawal from the surrounding situation to the extent of a dissociative stupor or by agitating and over activity.\n\nThe signs are: tachycardia (increased heart rate), sweating, hyperventilation (increased breathing).\nThe symptoms usually appear within minutes of the impact of the stressful stimulus and disappear within 2–3 days.\n\nThis arises after response to a stressful event or situation of an exceptionally threatening nature and likely to cause pervasive distress (great pain, anxiety, sorrow, acute physical or mental suffering, affliction, trouble) in almost anyone.\n\nThe causes of PTSD are: natural or human disasters, war, serious accident, witness of violent death of others, violent attack, being the victim of sexual abuse, rape, torture, terrorism or hostage taking. \n\nThe predisposing factors are: personality traits and previous history of psychiatric illness.\n\nFlashbacks are the repeated reliving of the trauma in the form of intrusive memories or dreams, intense distress at exposure to events that symbolize or resemble an aspect of the traumatic event, including anniversaries of the trauma, avoidance of activities and situations reminiscent of the trauma, emotional blunting or \"numbness\", a sense of detachment from other people, autonomic hyperarousal with hypervigilance, an enhanced startle reaction and insomnia, marked anxiety and depression and, occasionally, suicidal ideation.\n\nPsychiatric consultation: exploration of memories of the traumatic event, relief of associated symptoms and counseling.\n\nThe course is fluctuating but recovery can be expected in the majority of cases. Few people may show chronic course over many years and a transition to an enduring personality change\n\nStress ulceration is a single or multiple fundic mucosal ulcers that causes upper gastrointestinal bleeding, and develops during the severe physiologic stress of serious illness. It can also cause mucosal erosions and superficial hemorrhages in patients who are critically ill, or in those who are under extreme physiologic stress, causing blood loss that can require blood transfusion. \n\nOrdinary peptic ulcers are found commonly in the “gastric antrum and the duodenum” whereas stress ulcers are found commonly in “fundic mucosa and can be located anywhere within the stomach and proximal duodenum”.\n\n\n"}
{"id": "44605144", "url": "https://en.wikipedia.org/wiki?curid=44605144", "title": "Tapela and Another v Attorney General and Others", "text": "Tapela and Another v Attorney General and Others\n\nTapela and Another v Attorney General and Others is a legal case in Botswana concerning the right of two HIV-positive non-citizen prisoners to access antiretroviral (ARV) medication at state expense. On 22 August 2014, the Botswana High Court in Gaborone ordered the government of Botswana to provide ARV medication to the prisoners.\n\nThe case was brought by two Zimbabwean citizens who were imprisoned in Gaborone Central Prison and the Botswana Network on Ethics, Law and HIV/AIDS (BONELA). The two prisoners were both diagnosed as HIV-positive in prison. They were denied ARV treatment despite that their physical conditions met the treatment criteria under the government's Treatment Guidelines. The government denied them ARV treatment on the basis that they were not citizens. HIV-positive prisoners who were citizens were given access to ARV treatment at state expense in terms of the government's policy at the time.\n\nThe applicants brought a legal challenge against the government's refusal to provide the inmates with ARV treatment in the High Court in Gaborone. They argued that their denial of access to treatment, and the policy underlying the government's refusal to provide treatment, were unlawful and unconstitutional.\n\nThe state respondents initially failed to defend the application. Default judgment was therefore issued in the applicants' favour in March 2014. The default judgment was rescinded with the parties’ consent on 19 March 2014.\n\nThe merits of the case were argued in the Gaborone High Court on 10 June 2014. On 22 August 2014, Judge Bengame Sechele ruled in the applicants' favour. The Court held that the policy denying foreign prisoners ARV treatment was unlawful in terms of the common law, statutory law and the Constitution of Botswana. The Court held that denying the prisoners ARV treatment unjustifiably limited their rights to life, equality, and freedom from inhuman and degrading treatment. The Court ordered the Botswana government to enroll all foreign prisoners who meet the treatment criteria on ARV treatment.\n\nThe government respondents initially indicated an intention to comply with the Court's order but have since appealed the High Court decision to the Botswana Court of Appeal.\n\n"}
{"id": "51038293", "url": "https://en.wikipedia.org/wiki?curid=51038293", "title": "Thor Willy Ruud Hansen", "text": "Thor Willy Ruud Hansen\n\nThor Willy Ruud Hansen (born 21 December 1946 in Fredrikstad) is a Norwegian pediatrician and neonatologist. He is a Professor of Pediatrics at the University of Oslo and a former President of the Norwegian Society of Pediatricians (2009–2011). He is currently chairman of the clinical ethics committee at Oslo University Hospital. His research interests are neonatal medicine, including the neurotoxicology of neonatal jaundice, as well as clinical ethics.\n\nHansen earned his cand.med. (MD) at the University of Oslo in 1972 and his dr.med. (Med.Sc.D.) in 1988, and is a specialist in pediatrics. Following residencies at several Norwegian hospitals he worked in Quessua in Angola as the only doctor at small mission hospital 1977–1980. Since 1980 he has worked at Oslo University Hospital, Rikshospitalet (The National Hospital) in Norway, interrupted by two stays in the United States, including as a neonatologist and associate professor at the UPMC Children's Hospital of Pittsburgh 1994–1997. In 1998 he became head of neonatology at The National Hospital in Norway and in 2003 he became a full professor of neonatology at the University of Oslo.\n\n"}
{"id": "32922694", "url": "https://en.wikipedia.org/wiki?curid=32922694", "title": "Water crib", "text": "Water crib\n\nWater cribs are offshore structures that collect water from close to the bottom of a lake to supply a pumping station onshore. The name crib is derived from the function of the structure—to surround and protect the intake shaft. Cities supplied with drinking water collected by water cribs include Chicago, where two of the nine originally built cribs are in active use. Water cribs were also used as residences for caretakers who would live in the structure year round. Jobs included clearing debris and maintaining valves, gears, and instruments to keep the water flowing. These jobs have since been automated with periodic maintenance leaving no need for a full time caretaker.\n"}
{"id": "11706293", "url": "https://en.wikipedia.org/wiki?curid=11706293", "title": "Water supply and sanitation in India", "text": "Water supply and sanitation in India\n\nDrinking water supply and sanitation in India continue to be inadequate, despite longstanding efforts by the various levels of government and communities at improving coverage. The level of investment in water and sanitation, albeit low by international standards, has increased in size during the 2000s. For example, in 1980 rural sanitation coverage was estimated at 1% and reached 95% in 2018. Also, the share of Indians with access to improved sources of water has increased significantly from 72% in 1990 to 88% in 2008. At the same time, local government institutions in charge of operating and maintaining the infrastructure are seen as weak and lack the financial resources to carry out their functions. In addition, only two Indian cities have continuous water supply and according to an estimate from 2018 about 8% of Indians still lack access to improved sanitation facilities. A study by Water Aid estimated as many as 10 million Indians, or 5 percent of Indians living in urban areas, are live without adequate sanitation. India comes in first place globally for having the greatest number of urban-dwelling inhabitants living without sanitation. India tops the urban sanitation crisis, has the largest amount of urban dwellers without sanitation, and the most open defecators with over 5 million people.\n\nA number of innovative approaches to improve water supply and sanitation have been tested in India, in particular in the early 2000s. These include demand-driven approaches in rural water supply since 1999, community-led total sanitation, a public-private partnerships to improve the continuity of urban water supply in Karnataka, and the use of micro credits for water supply and sanitation in order to improve access to water and sanitation.\n\nIn 2015, 88% of the total population had access to at least basic water, or 96% in urban areas and 85% in rural areas. The term \"at least basic water\" is a new term since 2016, and is related to the previously used \"improved water source\". In India in 2015, 44% had access to \"at least basic sanitation\", or 65% in urban areas and 34% in rural areas. In 2015, there were still 150 million people without access to \"at least basic\" water and 708 million without access to \"at least basic\" sanitation.\n\nIn earlier years, in 2010, the UN estimated based on Indian statistics that 525 million people practice open defecation. In June 2012 Minister of Rural Development Jairam Ramesh stated India is the world's largest \"open air toilet\". He also remarked that Pakistan, Bangladesh and Afghanistan have better sanitation records.\n\nIn 2008, 88% of the population in India had access to an improved water source, but only 31% had access to improved sanitation. In rural areas, where 72% of India’s population lives, the respective shares are 84% for water and only 21% for sanitation. In urban areas, 96% had access to an improved water source and 54% to improved sanitation. Access has improved substantially since 1990 when it was estimated to stand at 72% for water and 18% for sanitation.\n\nAccording to Indian norms, access to improved water supply exists if at least 40 liters/capita/day of safe drinking water are provided within a distance of 1.6 km or 100 meter of elevation difference, to be relaxed as per field conditions. There should be at least one pump per 250 persons.\n\nIn urban areas, those that do not receive water from the piped network often have to purhchase expensive water of dubious quality from private water vendors. For example, in Delhi water trucks get water from illegal wells on the banks of the Yamuna River for 0.75 rupees per gallon (about USD 2.70/m).\n\nChallenges. As of 2010, only two cities in India — Trivandrum and Kota — get continuous water supply. In 2005 none of the 35 Indian cities with a population of more than one million distributed water for more than a few hours per day, despite generally sufficient infrastructure. Owing to inadequate pressure people struggle to collect water even when it is available. According to the World Bank, none have performance indicators that compare with average international standards. A 2007 study by the Asian Development Bank showed that in 20 cities the average duration of supply was only 4.3 hours per day. None of the 20 cities had continuous supply. The longest duration of supply was 12 hours per day in Chandigarh, and the lowest was 0.3 hours per day in Rajkot. According to the results of a Service Level Benchmarking (SLB) Program carried out by the Ministry of Urban Development (MoUD) in 2006 in 28 cities, the average duration of supply was 3.3 hours per day, with a range from one hour every three days to 18 hours per day. In Delhi residents receive water only a few hours per day because of inadequate management of the distribution system. This results in contaminated water and forces households to complement a deficient public water service at prohibitive 'coping' costs; the poor suffer most from this situation. For example, according to a 1996 survey households in Delhi spent an average of per year in time and money to cope with poor service levels. This is more than two times as much as the 2001 water bill of about 18 per year of a Delhi household that uses 20 cubic meters per month.\n\nAchievements. Jamshedpur, a city in Jharkhand with 573,000 inhabitants, provided 25% of its residents with continuous water supply in 2009. Navi Mumbai, a planned city with more than 1m inhabitants, has achieved continuous supply for about half its population as of January 2009. Badlapur, another city in the Mumbai Conurbation with a population of 140,000, has achieved continuous supply in 3 out of 10 operating zones, covering 30% of its population. Trivandrum, the capital of Kerala state with a population of 1,645,000 in 2011, is the largest Indian city that enjoys continuous water supply.\n\nMost Indians depend on on-site sanitation facilities which means mainly pit latrines in rural areas. In rural areas, the government has been promoting community-led sanitation approaches such as the Total Sanitation Campaign, with some success. In urban areas, a good practice example is the Slum Sanitation Program in Mumbai that has provided access to sanitation for a quarter million slum dwellers. Sewerage, where available, is often in a bad state. In Delhi the sewerage network has lacked maintenance over the years and overflow of raw sewage in open drains is common, due to blockage, settlements and inadequate pumping capacities. The capacity of the 17 existing wastewater treatment plants in Delhi is adequate to cater a daily production of waste water of less than 50% of the drinking water produced. Of the 892 million people in the world that defecate openly, some 150 million live in India, making it the country with the highest number of people who defecate in the open. This has serious public health implications.\n\nA specific Indian problem is also the (officially prohibited) \"manual scavenging\" which is connected to the officially banned caste system, and relates to unsafe and undignified emptying of toilets and pits, as well as handling of raw, untreated human excreta.\n\nAs of 2003, it was estimated that only 27% of India's wastewater was being treated, with the remainder flowing into rivers, canals, groundwater or the sea., For example, the sacred Ganges river is infested with diseases and in some places \"the Ganges becomes black and septic. Corpses, of semi-cremated adults or enshrouded babies, drift slowly by.\". In 2008, NewsWeek described Delhi's sacred Yamuna River as \"a putrid ribbon of black sludge\" where the concentration of fecal bacteria is 10,000 times the recommended safe maximum despite a 15-year program to address the problem. Cholera epidemics are not unknown.\n\nThe lack of adequate sanitation and safe water has significant negative health impacts including diarrhoea, referred to by travellers as the \"Delhi Belly\", and experienced by about 10 million visitors annually. While most visitors to India recover quickly and otherwise receive proper care. The dismal working conditions of sewer workers are another concern. A survey of the working conditions of sewage workers in Delhi showed that most of them suffer from chronic diseases, respiratory problems, skin disorders, allergies, headaches and eye infections.. Various other cities in India have a record of unsafe drinking water. Municipal water in Visakhapatnam is contaminated with too much chlorine and pharmaceuticals which cause headache, short term memory loss and loss of focus.\n\nDepleting ground water table and deteriorating ground water quality are threatening the sustainability of both urban and rural water supply in many parts of India. The supply of cities that depend on surface water is threatened by pollution, increasing water scarcity and conflicts among users. For example, Bangalore depends to a large extent on water pumped since 1974 from the Kaveri river, whose waters are disputed between the states of Karnataka and Tamil Nadu. As in other Indian cities, the response to water scarcity is to transfer more water over large distances at high costs. In the case of Bangalore, the Kaveri Stage IV project, Phase II, includes the supply of 500,000 cubic meter of water per day over a distance of 100 km, thus increasing the city's supply by two-thirds.\n\nIn some coastal areas seawater desalination is becoming an important source of drinking water supply. For example, the Chennai Metropolitan Water Supply and Sewerage Board has put into service a first large seawater desalination plant with a capacity of 100,000 m3 per day at Minjur in 2010. A contract for a second plant with the same capacity at Nemmeli was awarded in the same year.\n\nWater supply and sanitation is a State responsibility under the Indian Constitution. States may give the responsibility to the Panchayati Raj Institutions (PRI) in rural areas or municipalities in urban areas, called Urban Local Bodies (ULB). At present, states generally plan, design and execute water supply schemes (and often operate them) through their State Departments (of Public Health Engineering or Rural Development Engineering) or State Water Boards.\n\nHighly centralised decision-making and approvals at the state level, which are characteristic of the Indian civil service, affect the management of water supply and sanitation services. For example, according to the World Bank in the state of Punjab the process of approving designs is centralised with even minor technical approvals reaching the office of chief engineers. A majority of decisions are made in a very centralised manner at the headquarters. In 1993 the Indian constitution and relevant state legislations were amended in order to decentralise certain responsibilities, including water supply and sanitation, to municipalities. Since the assignment of responsibilities to municipalities is a state responsibility, different states have followed different approaches. According to a Planning Commission report of 2003 there is a trend to decentralise capital investment to engineering departments at the district level and operation and maintenance to district and gram panchayat levels.\n\nThe responsibility for water supply and sanitation at the central and state level is shared by various Ministries. At the central level three Ministries have responsibilities in the sector: The Ministry of Drinking Water and Sanitation (until 2011 the Department of Drinking Water Supply in the Ministry of Rural Development) is responsible for rural water supply and sanitation; the Ministry of Housing and Urban Poverty Alleviation and the Ministry of Urban Development share the responsibility for urban water supply and sanitation. Except for the National Capital Territory of Delhi and other Union Territories, the central Ministries only have an advisory capacity and a limited role in funding. Sector policy thus is a prerogative of state governments.\n\nNational Urban Sanitation Policy. In November 2008 the government of India launched a national urban sanitation policy with the goal of creating what it calls \"totally sanitized cities\" that are open-defecation free, safely collect and treat all their wastewater, eliminate manual scavenging and collect and dispose solid waste safely. As of 2010, 12 states were in the process of elaborating or had completed state sanitation strategies on the basis of the policy. 120 cities are in the process of preparing city sanitation plans. Furthermore, 436 cities rated themselves in terms of their achievements and processes concerning sanitation in an effort supported by the Ministry of Urban Development with the assistance of several donors. About 40% of the cities were in the \"red category\" (in need of immediate remedial action), more than 50% were in the \"black category\" (needing considerable improvement) and only a handful of cities were in the \"blue category\" (recovering). Not a single city was included in the \"green category\" (healthy and clean city). The rating serves as a baseline to measure improvements in the future and to prioritize actions. The government intends to award a prize called Nirmal Shahar Puraskar to the best sanitation performers.\n\nUrban areas. Institutional arrangements for water supply and sanitation in Indian cities vary greatly. Typically, a state-level agency is in charge of planning and investment, while the local government (Urban Local Bodies) is in charge of operation and maintenance. Some of the largest cities have created municipal water and sanitation utilities that are legally and financially separated from the local government. However, these utilities remain weak in terms of financial capacity. In spite of decentralisation, ULBs remain dependent on capital subsidies from state governments. Tariffs are also set by state governments, which often even subsidise operating costs. Furthermore, when no separate utility exists, there is no separation of accounts for different activities within a municipality. Some states and cities have non-typical institutional arrangements. For example, in Rajasthan the sector is more centralised and the state government is also in charge of operation and maintenance, while in Mumbai the sector is more decentralised and local government is also in charge of planning and investment. In 2012 the Delhi Jal Board contracted out operations and management in three zones of the city to private companies under performance-based contracts to reduce non-revenue water. The Vasant Vihar-Mehrauli zone is operated by SMPL Infrastructure of India, Malviya Nagar by Suez Environnement and the Nangloi zone by Veolia Environnement.\n\nPrivate sector participation. The private sector plays a limited, albeit recently increasing role in operating and maintaining urban water systems on behalf of ULBs. For example, the Jamshedpur Utilities & Services Company (Jusco), a subsidiary of Tata Steel, has a lease contract for Jamshedpur (Jharkhand), a management contract in Haldia (West Bengal), another contract in Mysore (Karnataka) and since 2007 a contract for the reduction of non-revenue water in parts of Bhopal (Madhya Pradhesh). The French water company Veolia won a management contract in three cities in Karnataka in 2005. In 2002 a consortium including Thames Water won a pilot contract covering 40,000 households to reduce non-revenue water in parts of Bangalore, funded by the Japan Bank for International Cooperation. The contract was scaled up in 2004. The Cypriot company Hydro-Comp, with two Indian companies, won a 10-year concession contract for the city of Latur City (Maharashtra) in 2007 and an operator-consultant contract in Madurai (Tamil Nadu). Furthermore, the private Indian infrastructure development company SPML is engaged in build-operate-transfer (BOT) projects, such as a bulk water supply project for Bhiwandi (Maharashtra).\n\nRural areas. There are about a 100,000 rural water supply systems in India. At least in some states, responsibility for service provision is in the process of being partially transferred from State Water Boards and district governments to Panchayati Raj Institutions (PRI) at the block or village level (there were about 604 districts and 256,000 villages in India in 2002, according to Subdivisions of India. Blocks are an intermediate level between districts and villages). Where this transfer has been initiated, it seems to be more advanced for single-village water schemes than for more complex multi-village water schemes. Despite their professed role Panchayati Raj Institutions, play only a limited role in provision of rural water supply and sanitation as of 2006. There has been limited success in implementing decentralisation, partly due to low priority by some state governments. Rural sanitation is typically provided by households themselves in the form of latrines.\n\nA number of innovative approaches to improve water supply and sanitation have been tested in India, in particular in the early 2000s. These include community-led total sanitation, demand-driven approaches in rural water supply, a public-private partnerships to improve the continuity of urban water supply in Karnataka, and the use of microcredits in water supply and sanitation to women in order to improve access to water.\n\nIn 1999 a demand-driven and people-centered sanitation program was initiated under the name Total Sanitation Campaign (TSC) which has some similarities with Community-led total sanitation (CLTS), but is not the same. It evolved from the limited achievements of the first structured programme for rural sanitation in India, the Central Rural Sanitation Programme, which had minimal community participation. The main goal of Total Sanitation Campaign is to eradicate the practice of open defecation by 2017. Community-led total sanitation is not focused on building infrastructure, but on preventing open defecation through self-awareness and shame. In Maharashtra where the program started more than 2000 Gram Panchayats have achieved \"open defecation free\" status. Villages that achieve this status receive monetary rewards and high publicity under a program called Nirmal Gram Puraskar.\n\nA new sanitation campaign was launched as Swachh Bharat Abhiyan (Clean India Mission) in October 2014.\n\nAs of 1 December 2017, Total Sanitation Coverage throughout India has risen to 73% up from 42% on October 2, 2014, the day Swachh Bharat Abhiyan was launched.\n\nMost rural water supply schemes in India use a centralised, supply-driven approach, i.e. a government institution designs a project and has it built with little community consultation and no capacity building for the community, often requiring no water fees to be paid for its subsequent operation. Since 2002 the Government of India has rolled out at the national level a program to change the way in which water and sanitation services are supported in rural areas. The program, called \"Swajaldhara\", decentralises service delivery responsibility to rural local governments and user groups. Under the new approach communities are being consulted and trained, and users agree up-front to pay a tariff that is set at a level sufficiently high to cover operation and maintenance costs. It also includes measures to promote sanitation and to improve hygiene behaviour. The national program follows a pilot program launched in 1999.\n\nAccording to a 2008 World Bank study in 10 Indian states, \"Swajaldhara\" results in lower capital costs, lower administrative costs and better service quality compared to the supply-driven approach. In particular, the study found that the average full cost of supply-driven schemes is per cubic meter, while it is only per cubic meter for demand-driven schemes. These costs include capital, operation and maintenance costs, administrative costs and coping costs incurred by users of malfunctioning systems. Coping costs include travelling long distances to obtain water, standing in long queues, storing water and repairing failed systems. Among the surveyed systems that were built using supply-driven approach system breakdowns were common, the quantity and quality of water supply were less than foreseen in designs, and 30% of households did not get daily supply in summer. The poor functioning of one system sometimes leads to the construction of another system, so that about 30% of households surveyed were served by several systems. As of 2008 only about 10% of rural water schemes built in India used a demand-driven approach. Since water users have to pay lower or no tariffs under the supply-driven approach, this discourages them to opt for a demand-driven approach, even if the likelihood of the systems operating on a sustainable basis is higher under a demand-driven approach.\n\nIn the cities of Hubli, Belgaum and Gulbarga in the state of Karnataka, the private operator Veolia increased water supply from once every 2–15 days for 1–2 hours, to 24 hours per day for 180,000 people (12% of the population of the 3 cities) within 2 years (2006–2008). This was achieved by carefully selecting and ring-fencing demonstration zones (one in each city), renovating the distribution network, installing meters, introducing a well-functioning commercial system, and effective grass-roots social intermediation by an NGO, all without increasing the amount of bulk water supplied. The project, known by its acronym as KUWASIP (Karnataka Urban Water Sector Improvement Project), was supported by a 39.5 million loan from the World Bank. It constitutes a milestone for India, where no large city so far has achieved continuous water supply. The project is expected to be scaled-up to cover the entire area of the three cities.\n\nIn Tiruchirapalli in Tamil Nadu, the NGO Gramalaya, established in 1987, and women self-help groups promote access to water supply and sanitation by the poor through microcredit. Among the benefits are that women can spend more time with their children, earn additional income, and sell surplus water to neighbours. This money contributes to her repayment of the WaterCredit loan. The initiative is supported by the US-based non-profit Water Partners International.\n\nThe Jamshedpur Utilities and Services Company (JUSCO) provides water and sanitation services in Jamshedpur, a major industrial center in East India that is home to Tata Steel. Until 2004 a division of Tata Steel provided water to the city’s residents. However, service quality was poor with intermittent supply, high water losses and no metering. To improve this situation and to establish good practices that could be replicated in other Indian cities, JUSCO was set up as a wholly owned subsidiary of Tata Steel in 2004.\n\nEfficiency and service quality improved substantially over the following years. The level on non-revenue water decreased from an estimated 36% in 2005 to 10% in 2009; one quarter of residents received continuous water supply (although the average supply remained at only 7 hours per day) in 2009; the share of metered connections increased from 2% in 2007 to 26% in 2009; the number of customers increased; and the company recovered its operating costs plus a portion of capital costs. Identifying and legalising illegal connections was an important element in the reduction of non-revenue water. The utility prides itself today of the good drinking water quality provided and encourages its customers to drink from the tap. The utility also operates a wastewater treatment plant that meets discharge standards. The private utility pays salaries that are higher than civil service salaries and conducts extensive training programs for its staff. It has also installed a modern system to track and resolve customer complaints. Furthermore, it conducts independent annual customer satisfaction surveys. JUSCO’s vision is to be the preferred provider of water supply and other urban services throughout India. Together with Ranhill Malaysia it won a 25-year concession contract for providing the water supply in Haldia City, West Bengal.\n\nThere are only limited data on the operating efficiency of utilities in India, and even fewer data on the efficiency of investments. Two indicators of operating efficiency are non-revenue water and labour productivity.\n\nNon-revenue water. According to the results of a Service Level Benchmarking (SLB) Program carried out by the Ministry of Urban Development (MoUD) in 2006 in 28 cities, the average level of non-revenue water (NRW) was 44 percent. Another study of 20 cities by the Jawaharlal Nehru National Urban Renewal Mission with the support of the Asian Development Bank showed an average level of non-revenue water (NRW) of 32%. However, 5 out of the 20 cities did not provide any data. For those that provided data there probably is a large margin of error, since only 25% of connections are metered, which makes it very difficult to estimate non-revenue water. Also, three utilities in the sample show NRW levels of less than 20%, two of which have practically no metering, which indicates that the numbers are not reliable and actual values are likely to be higher. In Delhi, which was not included in the ADB study, non-revenue water stood at 53% and there were about 20 employees per 1000 connections. Furthermore, only 70% of revenue billed was actually collected.\n\nLabour productivity. Concerning labour productivity, the 20 utilities in the sample had on average 7.4 employees per 1,000 connections, which is much higher than the estimated level for an efficient utility. A survey of a larger sample of Indian utilities showed an average ratio of 10.9 employees per 1,000 connections.\n\nWater and sewer tariffs in India are low in both urban and rural areas. In urban areas they were set at the equivalent of about 0.10 per cubic meter in 2007 and recovered about 60% of operating and maintenance costs, with large differences between cities. Some cities such as Kolkata do not bill residential users at all. In rural areas the level of cost recovery often is even lower than in urban areas and was estimated at only 20% in rural Punjab. Subsidies were estimated at 1.1 billion per year in the mid-1990s, accounting to 4% of all government subsidies in India. 70% of those benefiting from the subsidies are not poor.\n\nMetering. Water metering is the precondition for billing water users on the basis of volume consumed. Estimates of the share of customers metered vary depending on the study quoted. According to the results of a Service Level Benchmarking (SLB) Program carried out by the Ministry of Urban Development in 2006 in 28 cities, the share of metering was 50 percent. According to a 1999 survey of 300 cities about 62% of urban water customers in metropolitan areas and 50% in smaller cities are metered (average 55%). However, meters often do not work so that many \"metered\" customers are charged flat rates. Bangalore and Pune are among the few Indian cities that meter all their customers. Many other cities have no metering at all or meter only commercial customers. Users of standposts receive water free of charge. A 2007 study of 20 cities by the Jawaharlal Nehru National Urban Renewal Mission with the support of the Asian Development Bank (ADB) showed that only 25% of customers of these utilities were metered. Most other customers paid a flat tariff independent of consumption. Some utilities, such as the one serving Kolkata, actually do not bill residential users at all.\n\nTariff levels. According to the same ADB study the average tariff for all customers – including industrial, commercial and public customers – is per cubic meter. According to a 2007 global water tariff survey by the OECD the residential water tariff for a consumption of 15 m³ was equivalent to 0.15 per m3 in Bangalore, 0.12 per m3 in Calcutta, 0.11 per m3 in New Delhi and 0.09 per m3 in Mumbai. Only Bangalore had a sewer tariff of 0.02 per m3. The other three cities did not charge for sewerage, although the better-off tend to be the ones with access to sewers.\n\nTariff structure. The tariff for customers that are effectively metered is typically a uniform linear tariff, although some cities apply increasing-block tariffs.\n\nAffordability. Urban water tariffs were highly affordable according to data from the year 2000. A family of five living on the poverty line which uses 20 cubic meter of water per month would spend less than 1.2% of its budget on its water bill if it had a water meter. If it did not have a water meter and was charged a flat rate, it would pay 2.0% of its budget. This percentage lies below the often used affordability threshold of 5%. However, at that time the average metered tariff was estimated at only 0.03 per m3, or less than three times what it was estimated to be in 2007. Apparently no more up-to-date estimates on the share of the average water bill in the budget of the poor are available.\n\nCost recovery. According to a 2007 study of 20 cities the average rate of cost recovery for operating and maintenance costs of utilities in these cities was 60%. Seven of the 20 utilities generated a cash surplus to partially finance investments. Chennai generated the highest relative surplus. The lowest cost recovery ratio was found in Indore in Madhya Pradesh, which recovered less than 20% of its operating and maintenance costs. According to the results of a Service Level Benchmarking (SLB) Program carried out by the Ministry of Urban Development in 2006 in 28 cities, cost recovery was 67% on average.\n\nDelhi example. Between 2011 and 2013 Delhi Jal Board (DJB) has increased its revenues by 50 percent and reduced operating subsidies from the National Capital Territory of Delhi. It owes in debt, but hopes that the debt will be converted into a grant. The utility focuses on improving its customer database, meter reading through hand-held devices, billing and bill collection under a new manager, Debashree Mukherjee, who took the helm of the utility in 2012. As of 2004, in Delhi revenues were just sufficient to cover about 60% of operating costs of DJB; maintenance was, as a result, minimal. In the past, the Delhi utility has relied heavily on government financial support for recurrent and capital expenditures in the magnitude of per year and respectively. Accounts receivable represent more than 12 months of billing, part of it being non-recoverable. The average tariff was estimated at 0.074/m³ in 2001, compared to production costs of 0.085/m³, the latter probably being a very conservative estimate that does not take into account capital costs.\n\nChallenges faced in attempting to increase tariffs. Even if users are willing to pay more for better services, political interests often prevent tariffs from being increased even to a small extent. An example is the city of Jabalpur where the central government and the state government financed a water supply project from 2000–2004 to be operated by the Jabalpur Municipal Corporation, an entity that collected only less than half of its operational costs in revenues even before this major investment. Even so, the municipal corporation initially refused to increase tariffs. Only following pressure from the state government it reluctantly agreed to increase commercial tariffs, but not residential tariffs.\n\nCost recovery in rural areas is low and a majority of the rural water systems are defunct for lack of maintenance. Some state governments subsidise rural water systems, but funds are scarce and insufficient. In rural areas in Punjab, operation and maintenance cost recovery is only about 20%. On one hand, expenditures are high due to high salary levels, high power tariff and a high number of operating staff. On the other hand, revenue is paid only by the 10% of the households who have private connections. Those drawing water from public stand posts do not pay any water charges at all, although the official tariff for public stand post users is per month per household.\n\nThere are no accurate recent estimates of the level of subsidies for water and sanitation in India. It has been estimated that transfers to the water sector in India amounted to per year in the mid-1990s, accounting for 4% of all government subsidies in India. About 98% of this subsidy is said to come from State rather than Central budgets. This figure may only cover recurrent cost subsidies and not investment subsidies, which are even higher (see below). There is little targeting of subsidies. According to the World Bank, 70% of those benefiting from subsidies for public water supply are not poor, while 40% of the poor are excluded because they do not have access to public water services.\n\nInvestment in urban water supply and sanitation has increased during the first decade of the 21st century, not least thanks to increased central government grants made available under Jawaharlal Nehru National Urban Renewal Mission under the Congress government until 2014 and the Swachh Bharat Abhiyan (Clean India Mission) under the BJP government since 2014, alongside loans from the Housing and Urban Development Corporation.\n\nThe Eleventh Five-Year Plan (2007–2012) foresaw investments of for urban water supply and sanitation, including urban (stormwater) drainage and solid waste management.\n\nThe funding of government programmes for water supply and sanitation is shared by the central government, states and other contributors, with the share contributed by various stakeholders varying between programmes and over time. For example, as of 2016 the states pay 60% and the central government pays 40% for investments financed under the Clean India Mission and the National Rural Drinking Water Programme. Until 2015 the central government had funded 75% of the Clean India Mission.\n\nUnder the 11th Plan (2007–12) 55% of the investments were to be financed by the central government, 28% by state governments, 8% by \"institutional financing\" such as HUDCO, 8% by external agencies and 1.5% by the private sector. Local governments were not expected to contribute to the investments. The volume of investments was expected to double to reach 0.7% of GDP. Also, it implied a shift in financing from state governments to the central government. During the 9th Plan only 24% of investments were financed by the central government and 76% by state governments. Central government financing was heavily focused on water supply in rural areas.\n\nThe current system of financing water supply and sanitation is fragmented through a number of different national and state programs. This results in simultaneous implementation with different and conflicting rules in neighbouring areas. For example, in rural areas different programs undermine each other, adversely affecting demand driven approaches requiring cost sharing by users.\n\nState budgets the major source of financing for water supply and sanitation. State Financing Corporations (SFC) play an important role in making recommendations regarding the allocation of state tax revenues between states and municipalities, criteria for grants, and measures to improve the financial position of municipalities. According to the Planning Commission, SFCs are in some cases not sufficiently transparent and/or competent, have high transactions costs, and their recommendations are sometimes not being implemented. An important source of financing are loans from Housing and Urban Development Corporation Ltd (HUDCO), a Central government financial undertaking. HUDCO loans to municipal corporations need to be guaranteed by state governments. HUDCO also on-lends loans from foreign aid, including Japanese aid, to states. The Jawaharlal Nehru National Urban Renewal Mission (2005-2014) played an important role in financing urban water supply and sanitation through central government grants. However, its grants were limited to the 35 largest cities in the country and 28 other selected cities, so that most cities with less than 1 million inhabitants were not eligible to receive grants from this mission. It was replaced by the Atal Mission for Rejuvenation and Urban Transformation (AMRUT), the new government's flagship programme for urban development. In addition, in 2014 the new government announced its high-profile Swachh Bharat Abhiyan (Clean India Mission) that aims at eradicating open defecation by 2019, covering 4,041 cities and towns. The programme has received funding and technical support from the World Bank, corporations and state governments under the Sarva Shiksha Abhiyan and Rashtriya Madhyamik Shiksha Abhiyan schemes. Swachh Bharat Abhiyan is expected to cost over . An amount of was allocated for the mission in 2016 Union budget of India. In 2015 the government introduced a 0.5% service tax on air travel, telephony, eating out and banking to finance the Clean India Campaign. A budget tracking study revealed that the shift of policy focus from water to sanitation as part of the Clean India Campaign has resulted in a cut in government spending on rural water supply. A Parliamentary Standing Committee report found that the government would be unable to achieve its 2017 target of providing 50% rural households with piped water.\n\nIn 1996 Tamil Nadu introduced a public-private partnership, the Tamil Nadu Urban Development Fund (TNUDF), to channel both grants and loans to cities in the state. TNUDF has received funding from the World Bank, Japanese JICA and KfW from Germany. It also mobilizes funding from the capital market through a water and sanitation pooled fund, under which several municipalities joined together to issue a bond in the local market. TNUDF so far is the only functioning state-level fund that channels loans to ULBs in India. In 2012 the state of Orissa has created an Urban Development Fund modelled on the example of Tamil Nadu.\n\nIn absolute terms India receives almost twice as much development assistance for water, sanitation and water resources management as any other country, according to data from the Organisation for Economic Co-operation and Development. India accounts for 13 per cent of commitments in global water aid for 2006–07, receiving an annual average of about 830 million (€620 million), more than double the amount provided to China. India's biggest water and sanitation donor is Japan, which provided 635 million, followed by the World Bank with 130 million. The annual average for 2004–06, however, was about half as much at 448 million, of which Japan provided 293 million and the World Bank 87 million. The Asian Development Bank and Germany are other important external partners in water supply and sanitation.\n\nIn 2003 the Indian government decided it would only accept bilateral aid from five countries (the United Kingdom, the United States, Russia, Germany and Japan). A further 22 bilateral donors were asked to channel aid through nongovernmental organisations, United Nations agencies or multilateral institutions such as the European Union, the Asian Development Bank or the World Bank.\n\nIndia has increased its loans from the Asian Development Bank (ADB) since 2005 after the introduction of new financing modalities, such as the multitranche financing facility (MFF) which features a framework agreement with the national government under which financing is provided in flexible tranches for subprojects that meet established selection criteria. In 2008 four MFFs for urban development investment programs were under way in North Karnataka (862 million), Jammu and Kashmir (1,260 million), Rajasthan (450 million), and Uttarakhand (1,589 million). Included in these MFFs are major investments for the development of urban water supply and sanitation services.\n\nGermany supports access to water and sanitation in India through financial cooperation by KfW development bank and technical cooperation by GIZ. Since the early 1990s both institutions have supported watershed management in rural Maharashtra, using a participatory approach first piloted by the Social Center in Ahmednagar and that constituted a fundamental break with the previous top-down, technical approach to watershed management that had yielded little results. The involvement of women in decision-making is an essential part of the project. While the benefits are mostly in terms of increased agricultural production, the project also increases availability of water resources for rural water supply. In addition, GIZ actively supports the introduction of ecological sanitation concepts in India, including community toilets and decentralised wastewater systems for schools as well as small and medium enterprises. Many of these systems produce biogas from wastewater, provide fertiliser and irrigation water.\n\nIsrael is providing advanced water management technology and expertise to India.\n\nAs India's largest donor in the sector the Japan International Cooperation Agency (JICA) finances a multitude of projects with a focus on capital-intensive urban water supply and sanitation projects, often involving follow-up projects in the same locations.\n\nCurrent projects. Projects approved between 2006 and 2009 include the Guwahati Water Supply Project (Phases I and II) in Assam, the Kerala Water Supply Project (Phased II and III), the Hogenakkal Water Supply and Fluorosis Mitigation Project (Phases I and II) in Tamil Nadu, the Goa Water Supply and Sewerage Project, the Agra Water Supply Project, the Amritsar Sewerage Project in Punjab, the Orissa Integrated Sanitation Improvement Project, and the Bangalore Water Supply and Sewerage Project (Phase II).\n\nEvaluation of past projects. An ex-post evaluation of one large program, the Urban Water Supply and Sanitation Improvement Program, showed that \"some 60%–70% of the goals were achieved\" and that \"results were moderate\". The program was implemented by the Housing and Urban Development Corporation, Ltd. (HUDCO) from 1996 to 2003 in 26 cities. The evaluation says that \"state government plans were not based on sufficient demand research, including the research for residents' willingness to pay for services\", so that demand for connections was overestimated. Also fees (water tariffs) were rarely increased despite recommendations to increase them. The evaluation concludes that \"HUDCO was not able to make significant contributions to the effectiveness, sustainability, or overall quality of individual projects. One of the reasons that not much attention was given to this problem is probably that there was little risk of default on the loans thanks to state government guarantees.\"\n\nCurrent projects. The World Bank finances a number of projects in urban and rural areas that are fully or partly dedicated to water supply and sanitation. In urban areas the World Bank supported or supports among others the USD 1.55 bn National Ganga River Basin Project approved in 2011, the Andhra Pradesh Municipal Development Project (approved in 2009, 300 million loan), the Karnataka Municipal Reform Project (approved in 2006, 216 million loan), the Third Tamil Nadu Urban Development Project (approved in 2005, 300 million loan) and the Karnataka Urban Water Sector Improvement Project (approved in 2004, 39.5 million loan). In rural areas it supports the Andhra Pradesh Rural Water Supply and Sanitation (150 million loan, approved in 2009), the Second Karnataka Rural Water Supply and Sanitation Project (approved in 2001, 151.6 million loan), the Uttarakhand Rural Water Supply and Sanitation Project (approved in 2006, 120 million loan) and the Punjab Rural Water Supply and Sanitation Project (approved in 2006, 154 million loan).\n\nEvaluation of past projects. A study by the World Bank's independent evaluation department evaluated the impact of the World Bank-supported interventions in the provision of urban water supply and wastewater services in Mumbai between 1973 and 1990. It concluded that water supply and sewerage planning, construction and operations in Bombay posed daunting challenges to those who planned and implemented the investment program. At the outset, there was a huge backlog of unmet demand because of underinvestment. Population and economic growth accelerated in the following decades and the proportion of the poor increased as did the slums which they occupied. The intended impacts of the program have not been realised. Shortcomings include that \"water is not safe to drink; water service, especially to the poor, is difficult to access and is provided at inconvenient hours of the day; industrial water needs are not fully met; sanitary facilities are too few in number and often unusable; and urban drains, creeks and coastal waters are polluted with sanitary and industrial wastes.\"\n\n\n"}
{"id": "1831570", "url": "https://en.wikipedia.org/wiki?curid=1831570", "title": "World Health Assembly", "text": "World Health Assembly\n\nThe World Health Assembly (WHA) is the forum through which the World Health Organization (WHO) is governed by its 194 member states. It is the world's highest health policy setting body and is composed of health ministers from member states. \n\nThe members of the World Health Assembly generally meet every year in May in Geneva, the location of WHO Headquarters. The main tasks of the WHA are to decide major policy questions, as well as to approve the WHO work programme and budget and elect its Director General.\n\nThe original membership of the WHA, at the first assembly held in 1948, numbered 55 member states. The WHA has, currently, 194 member states.\n\nIn addition, seven agencies have observer status at the WHA - the Vatican, the Palestinian Authority, the Order of Malta, the International Committee of the Red Cross, the International Federation of Red Cross and Red Crescent Societies, the Inter-Parliamentary Union, and The Department of Health of the Republic of China, commonly known as Taiwan, was invited on 28 April 2009 to participate in the WHA 2009 as an observer for the first time since losing its China seat in United Nations to People's Republic of China in 1971. The invitation was extended to \"the Department of Health, Chinese Taipei.\"\n\nThe main international policy frameworks adopted through WHA resolutions include:\n\nIn addition, the WHA has endorsed through resolutions a number of WHO action plans dealing with different areas to improve health around the world, such as:\n\nThe WHA is also responsible for the endorsement of the WHO Family of International Classifications, a series of internationally standardized medical classifications, including the International Classification of Diseases (ICD) and the International Classification of Functioning, Disability and Health (ICF).\n\nIn her role as global patron of The White Ribbon Alliance for Safe Motherhood, and chair of the Maternal Mortality Campaign, Sarah Brown gave the keynote speech at the World Health Organization's 62nd WHA, alongside United Nations Secretary-General Ban Ki-moon, asking \"Where is the M in MCH?’ [maternal and child health]\" in an echo of Allan Rosenfield's landmark \"Lancet\" article of 1985 - and highlighting that the numbers of women dying in pregnancy and childbirth were still the same 14 years later.\n\nAmong other actions, the 65th Assembly endorsed the Rio Political Declaration to address the social determinants of health, intended to spearhead support for all countries to adopt inclusive ‘\"Health For All\"’ approaches to health promotion. It also endorsed the first World Immunization Week.\n\nIn her address to the 66th WHA in May 2013, WHO Director-General Margaret Chan traced a brief history of revisions to the International Health Regulations following the SARS outbreak in 2002-3, the \"first severe new disease of the 21st century.\" She observed that the two new diseases WHO is dealing with in 2013 are the novel coronavirus (MERS), from the same family as SARS, detected in 2012 in Saudi Arabia, and the first-ever human infections with the H7N9 avian influenza virus reported in China in 2013. She attributed the positive report by the \"World Health Statistics\" (May 2013) on dramatic improvement in health in the world's poorest countries from 1993-2013, to the emphasis placed on poverty alleviation by the Millennium Development Goals. She announced the emergence of global action plans for noncommunicable diseases, mental health, and the prevention of avoidable blindness and visual impairment calling for a life-course approach which includes \"equity through universal health coverage,\" preventive strategies and \"integrated service delivery.\"\n\nDr. Margaret Chan declared at the Assembly that Intellectual Property, or patents on strains of new virus, should not impede nations from protecting their citizens by limiting scientific investigations. Following the 2012 MERS outbreak in Saudi Arabia, Deputy Health Minister Ziad Memish raised concerns that scientists who applied for a patent would not allow the MERS-coronavirus to be used for investigations by other scientists and were therefore delaying the development of diagnostic tests. Ten of the 22 people who died and 22 of 44 cases reported were in Saudi Arabia. Saudi Arabia–based microbiologist Ali Mohamed Zaki reported the first known case, a 60-year-old Saudi man who got sick in June, 2012 on ProMed-mail, a public health on-line forum then published more details including the virus’s genetic makeup and closest relatives. The Erasmus Medical Center \"tested, sequenced and identified\" a sample provided by Ali Mohamed Zaki. Erasmus MC and Dr. Zaki strongly refuted all allegations concerning a presumed lack of willingness to cooperate in research into the new MERS coronavirus, making diagnostic tests and virus specimens freely available to all research institutions around the globe.\n\nThe 67th WHA took place in Geneva on 19–24 May 2014. Among the more than 20 resolutions adopted by the Assembly included ones concerning strengthening of national drug management systems to address antimicrobial resistance; implementation of the Minamata Convention to protect human health and the environment from effects of exposure to mercury and mercury compounds; and improving access to essential medicines worldwide. Also endorsed was a global monitoring framework for maternal, infant and child nutrition.\n\nFollowing the 67th WHA, the WHO's Director-General Dr Margaret Chan was criticized by the Association of Correspondents Accredited to the United Nations (ACANU) for not having spoken directly to the media during the course of the Assembly.\n\nThe 68th session of the World Health Assembly (WHA) took place in Geneva 18–26 May 2015. The Health Assembly is the supreme decision-making body of WHO. It is attended by delegations from all WHO Member States. Its main functions are to determine the policies of the Organization, supervise financial policies, and review and approve the proposed programme budget. Jagat Prakash Nadda assumed new presidency of WHA. India assumed the presidency after a gap of 19 years.\n\nDuring the assembly the WHA agreed to the Global Malaria Strategy and Programme Budget for 2016-2017, polio, International Health Regulations, strengthening surgical care, WHO's reform of its emergency and response programme, antimicrobial resistance, immunization gaps, malnutrition, air pollution, and epilepsy. Annual health awards were given by the Director-General of WHO and the President of WHA.\n\nThe 69th World Health Assembly took place 23-28 May 2016, and agreed to pursue the health-related Sustainable Development Goals (SDGs) through a comprehensive set of foundational steps, prioritizing universal health coverage, working with actors outside the health sector to address the social, economic and environmental root causes of antimicrobial resistance and other human health problems, to continue expanding efforts to address poor maternal and child health and infectious diseases in developing countries, and to focus upon equity within and between countries. Delegates decided to invite the WHO Framework Convention on Tobacco Control’s (WHO FCTC) Conference of the Parties (COP) to provide information on outcomes of this biennial event to future World Health Assembly meetings.\n\nThe 70th World Health Assembly took place 22-31 May 2017.\n\nFor the first time since 2009, Taiwan was completely excluded from the WHA, following political pressure from the People's Republic of China.\n\nThe 71st World Health Assembly took place 21-26 May 2018.\n\n\n"}
{"id": "52067264", "url": "https://en.wikipedia.org/wiki?curid=52067264", "title": "Zdenka Buben", "text": "Zdenka Buben\n\nZdenka Buben (1895 – 1988) was an American public health social worker that pioneered the development of many professional standards.\n\nBuben was born in Paris, France in 1895. Her mother did not like France, and thus her father suggested that they move. When Zdenka was two, Buben's father's friend arranged for Buben's father to transfer his work to New York, where the Buben family lived until 1908. In 1908, they moved to San Franscisco, where Buben finished high school. She went on to attend the University of California at Berkeley, where she originally majored in music. She switched her major to Hygiene, a department where people were trained as somewhat nurse and somewhat social work trainees. and went on to graduate and become a Health Visitor for the city of Alameda. Afterwards, her title changed, and she became a public health nurse in Los Angeles.\n\nBuben joined the Council of Social Agencies in its Health Division. She eventually helped work toward establishing a school of social work in Los Angeles, and also established a Bureau of Social Work in the Los Angeles County health department. At the time, many private medical doctors were against publicly funded medical care, and the Health Department also feuded with doctors about who should exactly receive care. Despite being the chief medical social worker in her department, Buben actually did not have a degree in social work until she decided to do so in the 1940s. She went on to attend the University of Chicago and obtained her Master's degree in Social Work. Afterward, she returned to Los Angeles and returned to her old position as the Director of Public Health Social Work, and maintained that position until her retirement in October 1961. Throughout her career, she advocated for the development of professional standards. Some of her lasting legacy of practices are the development of a sliding scale of fees for indigent persons who wished to receive medical care; the development of numerous social work training schools; and connecting many social agencies, the Health Department, and medical providers with one another. Buben also was a proponent of preventative care for diseases that are preventable, particularly tuberculosis and smallpox. She also lobbied for standards for social work and better licensure practices. In recognition of her significant contribution to social work, Buben was inducted to the California Social Work Hall of Distinction.\n"}
