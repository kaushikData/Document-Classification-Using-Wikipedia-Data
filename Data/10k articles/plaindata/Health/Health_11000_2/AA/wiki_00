{"id": "40080714", "url": "https://en.wikipedia.org/wiki?curid=40080714", "title": "ABC Nepal", "text": "ABC Nepal\n\nAgroforestry, Basic health, and Cooperative Nepal (ABC Nepal) is a nonprofit, non governmental organisation working in Nepal that focuses on women's rights and human trafficking in Nepal. Created in 1987, ABC Nepal was one of the very first Non Governmental Organisations established in Nepal. It was established before the multiparty democracy in Nepal in 1991 and officially registered then after. ABC Nepal is led by president Durga Ghimire.\n\nABC Nepal was the first Nepalese organisation to raise the issue of human trafficking on a national level, and did so by organizing conferences about the trafficking of underage girls and sex slavery in Nepal in 1991. It is one of the leading organisations working in this field. The combined effort of ABC Nepal along with other organisations has led to the creation and implementation of various laws regarding women rights and human trafficking.\n\nABC Nepal played a part in the rescue of 35 girls from the Apollo circus in New Delhi in 1996. They have also helped to rescue girls from brothels in India working as a sex slaves and later to rehabilitate them back into society.\n\nThey work to prevent trafficking in women and children by generating social awareness especially among rural people and conducting border monitoring and cross border programs. \n\nGirls from Nepal constitute the majority of commercial sex workers in India. Most of them are minors and most are infected with HIV (AIDS). They live in horrible conditions, such as red light zones where they are abused physically and sexually, bonded by the owner for years, tortured and imprisoned. On the other hands poor families living in rural parts of Nepal get easily attracted to the city, which offers good jobs and a better overall life. The illiterate girls are made to sell their body to re-earn the amount including the interest for which they were bought by the brothel and hence entrapped as bonded labour for years. Escape is virtually impossible under the direct surveillance, threat, and terror of owners. Corruption is prevalent in both the police and governments of India and Nepal, making both the trafficking easy and escape near impossible. The virgins, once worshiped as Goddess Kumari, return home back with HIV and the stigma attached to it. This makes them unacceptable to society, making reintegration and repatriation difficult and sometimes impossible.\n\nBesides sexual exploitation, girls have also been trafficked for various other reasons, including to have their kidneys sold, to be forced into marriage, or to work as domestic workers and cheap labourers in various parts of India.\n\nWith the changing public perspective of migration, raising level of awareness among women, and the shift of trafficking pattern in Nepal, ABC Nepal has now been focusing on trafficking of girls beyond India, especially in Saudi Arabia. Lots of Nepali youth are moving to Arabian countries, Malaysia, or elsewhere for newer and better employment opportunities, especially as housekeepers and factory workers. Many girls lured by dream of better world and job follow the path led by unauthorized dealers and traffickers and end up being sexually exploited and enslaved.\n\nABC Nepal has provided income-generating training to rural women and victims of trafficking to help them alleviate their poverty and improve their living standard. They have formed more than 300 women cooperative groups. ABC Nepal has also promoted self-reliance, self-confidence and leadership skill in women by economic empowerment, vocational training, and non formal education. The nonprofit focuses on reproductive health providing education to secondary school children's and operating health clinics and safe abortion campaigns. They have placed a special focus on HIV/AIDS prevention and awareness. The group enhances leadership in women and increases the participation of affected women at the local, regional, and national level. They also provide legal protection and can represent the victims in legal processes.\n\nABC Nepal operates rehabilitation homes in Kathmandu, Bhairahawa, and Biratnagar to provide shelter to the victims of trafficking and violence against women.\n\nABC Nepal is raising awareness for safe migration. For example, the group monitored the Tribhuwan International Airport, establishing help desk in major border transit of Bhairahawa. The current rehabilitation and reintegration programs are producing positive results.\n\nThe organization covers eastern, central, and western development regions.\n\n\n\n\n\nThe organization has published or contributed to several books, audio cassettes, and other publications.\n\n\n\n\n\n"}
{"id": "18301556", "url": "https://en.wikipedia.org/wiki?curid=18301556", "title": "AIDS Drug Assistance Programs", "text": "AIDS Drug Assistance Programs\n\nAIDS Drug Assistance Programs are a set of programs in all 50-states in the United States that provide Food and Drug Administration-approved HIV treatment drugs to low income patients in the U.S.\n\nThe programs are administered by each state with funds distributed by the United States government.\n\nIn June 2007 the program provided coverage for 102,000 or 30% of those infected with HIV in the United States. Drug expenditures were $100.1 million in 2007 and $8.8 million in money spent on helping with insurance payments. This represented 344,600 prescriptions.\n\nThe total program budget is $1.4 Billion with California receiving $288 Million, New York $241 Million, Texas $101 Million, and Florida $97 Million.\n\nThe program first began in 1987 with appropriations to help pay for AZT. The program was expanded in 1990 with the Ryan White Comprehensive AIDS Resources Emergency (CARE) Act (commonly referred to as the Ryan White Care Act. \n\nMost recipients are below 200% of the Federal Poverty Level (FPL) and 43 percent are below 100% the FPL. 63% are black or hispanic and 77% are male.\n\nIn 2010, some states, citing budgetary reasons began cutbacks to the ADAP Formulary or instituted waiting lists for medication. A controversial dialogue began in states like Florida as to how these cutbacks would affect lower income persons with HIV and whether the lack of funds should be blamed on the federal government or the state legislatures. \n\n"}
{"id": "12891806", "url": "https://en.wikipedia.org/wiki?curid=12891806", "title": "Abortion in Norway", "text": "Abortion in Norway\n\nThe legality of and public opinion toward abortion in Norway has changed dramatically in the last 100 years. Current Norwegian legislation and public health policy provides for abortion on demand in the first 12 weeks of gestation, by application up to the 18th week, and thereafter only under special circumstances until the fetus is viable, which is presumed at 21 weeks and 6 days.\n\nIn Christian V's legislation of 1687, abortion was punishable by death. By law of 1842, it was no longer a capital offense, but could be punished by up to six years of imprisonment and hard labor. In 1902, new legislation allowed for abortion in cases where the mother's life was in danger or the child would be stillborn.\n\nAn important milestone for the issue of abortion on demand came on January 15, 1915, when Katti Anker Møller gave a speech in Kristiania (now Oslo) calling for legalized abortion on demand. She said that \"the basis for all freedom is the governance over one's own body and everything that is in it. The opposite is the condition of a slave.\" (\"Grundlaget for al frihet er rådighet over egen krop og hvad i den er. Det motsatte er en slaves tilstand\")\n\nIn the period between 1920 and 1929, about 100 individuals were sentenced for illegal abortion. In one of Oslo's largest hospitals, 82 women died as a result of illegal abortions, and 3791 women were treated for injuries sustained under these procedures.\n\nIn 1934, the ministry of Justice named a committee to start work on new legislation on abortion, headed by Katti Anker Møller's daughter Tove Mohr. The following year, a campaign opposing the committee's work gathered 207,000 signatures. The government tabled the committee's work.\n\nThe political debate continued on the issue, though World War II put other priorities in the public discourse. During the German occupation, the maternal hygiene offices pioneered by Katti Anker Møller were shut down and all their materials put to the fire.\n\nWhen the maternal hygiene offices reopened in 1950, abortion counseling became one of their main services. An estimated 3,000 legal abortions and 7,000 - 10,000 illegal abortions were performed each year in the 1950s. In 1956, the prevalence of illegal abortions reached such levels that a council on penal law recommended stiffer penalties for illegal abortions.\n\nIn 1960, a new law allowed abortion by application approved by a commission of two physicians, and only on the basis of medical, eugenic, or criminal criteria; and with the consent of the husband if the applicant was married. This law went into effect in 1964.\n\nThe application was made by the woman's physician on her behalf, and she made her case alone before the commission. In 1964, 72% of the applications were approved. By 1974, 94% were approved, and the rate increased steadily through the 1970s. Still, practice varied considerably.\n\nIn 1969, the Norwegian Labour Party put abortion on demand on their platform, setting the stage for a mainstream debate on abortion within the broader framework of feminism. Proponents of abortion on demand improved their organizational strength, forming in 1974 Kvinneaksjonen for selvbestemt abort (\"The Women's campaign for abortion on demand\"). At the same time \"Folkeaksjonen mot selvbestemt abort\" (the Popular Movement Against Abortion on Demand) was formed and led by Anne Enger Lahnstein, submitting 610,000 signatures for their cause. The debate intensified, with feminists on one side of the issue, mainstream Christians on the other, and the medical community split.\n\nAbortion on demand was put to the vote in Stortinget but unexpectedly failed to pass by one vote when Socialist Left party representative Otto Hauglin voted against the bill. Although criteria for commission-approved abortions were somewhat relaxed, it became clear that abortion on demand would have to wait for the next session of parliament.\n\nThis period marked intensifying activism on both sides of the issue. The Christian newspaper Vårt Land became the platform for those opposed to the pending legislation, whereas the Socialist and feminist press advocated for it. The non-Socialist parties were unified in their opposition to abortion on demand, maintaining that the commission arrangement was appropriate.\n\nIn 1978, after much preparation and activism, by among others physicians Berthold Grünfeld and Axel Strøm of the Norwegian Labour Party and Socialist Left Party, respectively, passed with a one-vote majority the current law, which provides for abortion on demand in the first 12 weeks. Abortions after the end of the 12th week up to 18 weeks of pregnancy may be granted, by application, under special circumstances, such as the mother’s health or her social situation; if the baby is in great danger of severe medical complications; or if the woman has become pregnant while under-age, or after sexual abuse. After the 18th week the reasons for terminating a pregnancy must be extremely weighty. An abortion will not be granted after viability. Minor girls under 16 years of age need parental consent, although in some circumstances this may be overridden.\nSoon after the laws was passed, bishop Per Lønning resigned from his office in the Church of Norway in protest against the new legislation.\n\nAlthough some argued that easier access to abortion would cause abortion rates to increase, the number of abortions has remained stable since the early 1970s, especially when adjusted for demographic changes related to fertility.\n\nBetween 1999 and 2003, 1740 applications for abortions between the 12th and 16th week were considered by commissions in Norway. Of these, 1647, or 95.2% were approved.\n\nIn 2011, midwives at Rikshospitalet alarmed authorities that some aborted fetuses had heartbeats for up to one hour before they died. Data from the Norwegian Institute of Public Health revealed that between 2001 and 2011 17 fetuses had been aborted at 22 or 23 weeks gestation. In September 2013, a committee appointed by the Ministry of Health and Care Services recommended that abortion should not be allowed after 21 weeks and 6 days gestation. On 1 January 2015 the regulation on abortion was changed to say that a fetus is presumed viable at 21 weeks and 6 days, unless there is specific reasons to believe it is not.\n\n, the abortion rate was 16.2 abortions per 1000 women aged 15–44 years.\n\nEnglish\n\nNorwegian\n"}
{"id": "54753991", "url": "https://en.wikipedia.org/wiki?curid=54753991", "title": "Allergies in children", "text": "Allergies in children\n\nAllergies in children are those causes, pathophsiology, treatments, management, practices and control of allergies that develop in children. Up to 40 percent of children suffer from allergic rhinitis. And children are more likely to develop allergies if one or both parents have allergies. Allergies differ between adults and children. Part of the reason for this that the respiratory system in children is smaller. The bronchi and bronchioles are narrower so even a slight decrease in diameter of these airways can have serious consequences. In addition, children often 'outgrow' their allergies.\n\nThe incidence of childhood allergies has increased in the past 50 years.\n\nThe signs and symptoms of allergies in a child are:\n\nEach home contains possible allergens that can develop into allergies after exposure to:\n\nVitamin D deficiency at the time of birth and exposure to egg white, milk, peanut, walnut, soy, shrimp, cod fish, and wheat makes a child more susceptible to allergies. Soy-based infant formula is associated with allergies in infants.\n\nA child's allergy is an immune system reaction. The child is reacting to a specific substance, or allergen. The immune system of a child responds to the invading allergen by releasing histamine and other chemicals that typically trigger symptoms in the nose, lungs, throat, sinuses, ears, eyes, skin, or stomach lining. In some children, allergies can also trigger symptoms of asthma—a disease that causes wheezing or difficulty breathing. If a child has allergies and asthma, controlling the allergies is important because the lack of treatment may make the allergies worse. Compounds such as phthalates are associated with asthma in children. Asthma in children is associated with exposure to indoor allergens. in early childhood may prevent the development of asthma, but exposure at an older age may provoke bronchoconstriction. Use of antibiotics in early life has been linked to the development of asthma. Exposure to indoor volatile organic compounds may be a trigger for asthma; formaldehyde exposure, for example, has a positive association.\n\nTesting is available to help identify any environmental or food allergies. Caregivers and clinicians can assess the child for the development of an allergy by noting the presence of signs and symptoms and history of exposure.\n\nAvoiding allergens will help prevent symptoms. Allergies that a child has to the family pet can be controlled by removing the animal and finding it a new home. Exterminating cockroaches, mice and rats and a thorough cleaning can reduce symptoms of an allergy in children. Dust mites are attracted to moisture. They consume human skin that has come off and lodged in, furniture, rugs, mattresses, box springs, and pillows. The child's bedding can be covered with allergen-proof covers. Laundering of the child's clothing, bed linens and blankets will also reduce exposure.\n\nExposure to allergens outside the home can be controlled with the use of air conditioners. Washing the hair, taking a bath or shower before bedtime can be done to remove allergens that have been picked up from outside the home. If grass or grass pollen is an allergen it is sometimes beneficial to remain indoors while grass is being cut or mowed. Children with allergies to grass can avoid playing in the grass to prevent allergic symptoms. Staying out of piled leaves in the fall can help. Pets returning into the home after being outdoors may track in allergens.\n\nUp to 5% of infants that are fed cow's milk-based formula will develop an allergy to cow's milk. Over half of cases in children in the United States occur in areas with air quality below EPA standards.\n"}
{"id": "543434", "url": "https://en.wikipedia.org/wiki?curid=543434", "title": "American Academy of Pediatrics", "text": "American Academy of Pediatrics\n\nThe American Academy of Pediatrics (AAP) is an American professional association of pediatricians, headquartered in Itasca, Illinois. It maintains its Department of Federal Affairs office in Washington, D.C.\n\nThe academy was founded in 1930 by 35 pediatricians to address pediatric healthcare standards. It has 64,000 members in primary care and sub-specialist areas. Qualified pediatricians can become fellows (FAAP).\n\nThe academy runs continuing medical education (CME) programs for pediatricians and sub-specialists. The academy is divided into 14 departments and 26 divisions that assist with carrying out its mission.\n\nIt has the largest pediatric publishing program in the world, with more than 300 titles for consumers and over 500 titles for physicians and other health-care professionals. These publications include electronic products, professional references/textbooks, practice management publications, patient education materials and parenting books.\n\nThe \"AAP News\" is the academy's official newsmagazine, and \"Pediatrics\" is its flagship journal.\n\nThe academy has published hundreds of policy statements ranging from advocacy issues to practice recommendations. The academy's policy website contains all current academy policies and clinical reports.\n\nIn 2009, the national office and four of its State chapters provided training support to 49 pediatric practices to improve adherence to well-established asthma care guidelines. The percentage of patients at participating practices with well-controlled asthma (as defined by the National Heart, Lung, and Blood Institute) rose from 58 to 72 percent.\n\nThe AAP periodically issues guidance for child passenger safety, including policy recommendations for transitioning between rear-facing car seats, front-facing car seats, belt-positioning booster car seats, and vehicle safety belts. These recommendations are typically published in the peer-reviewed scientific journal Pediatrics, and tend to attract attention and controversy in popular press and social media.\n\nPreviously, the AAP recommended that children remain rear-facing until they are 2 years of age. In response to updated crash test, simulation, and field data, the AAP revised their guidance to exclude the age guideline entirely. Current AAP Child Passenger Safety recommendations (as of August 30, 2018) state that children should remain in a rear-facing car seat for as long as possible, until they meet the maximum height or weight dictated by the car seat manufacturer. The full recommendations state that:\n\n\nIn a 2012 position statement, the academy stated that a systematic evaluation of the medical literature shows that the \"preventive health benefits of elective circumcision of male newborns outweigh the risks of the procedure\" and that the health benefits \"are sufficient to justify access to this procedure for families choosing it and to warrant third-party payment for circumcision of male newborns,\" but \"are not great enough to recommend routine circumcision for all male newborns\". The academy takes the position that parents should make the final decision about circumcision, after appropriate information is gathered about the risks and benefits of the procedure. The 2012 statement is a shift in the academy's position from its 1999 statement in that the academy says the health benefits of the procedure outweigh the risks, and supports having the procedure covered by insurance.\n\nAfter the release of the position statement, a debate appeared in the journal \"Pediatrics\" and the \"Journal of Medical Ethics\". A group of 38 Northern European doctors, ethicists and lawyers co-authored a comment stating that they found the AAP's technical report and policy statement suffered from cultural bias, and reached recommendations and conclusions different from those of physicians in other parts of the world. An opinion by two authors stated that, in their view, the AAP's analysis was inaccurate, improper and incomplete. The AAP received further criticism from activist groups that oppose circumcision. The AAP responded to these criticisms in the \"Journal of Medical Ethics\", calling for respectful and evidence-based debate.\n\nIn April 2010, the academy revised its policy statement on female genital cutting, with one part of the new policy proving controversial. Although condemning female genital cutting overall, this statement suggested that current federal law banning the practice had the unintended consequence of driving families to perform the procedures in other countries, where these girls faced increased risk. As a possible compromise, this policy statement suggested that physicians have the option to perform a ceremonial \"nick\" on girls as a last resort to prevent them from being sent overseas for full circumcision. This particular position proved controversial to advocates for a full ban on female genital cutting under any circumstances and concern from other medical groups that even a \"nick\" would be condoning this widely rejected procedure. One month later, the academy retracted this policy statement.\n\nThe American Academy of Pediatrics says that although U.S. firearms-related deaths have dropped since the 1990s, guns were responsible for over 80 percent of teen homicides in 2009 and were the most common suicide method among teens. The AAP believes pediatricians should discuss guns and gun safety with parents before babies are born and at children's annual exams. It also advocates for, among other things, more background checks, an assault weapons ban, and federal research on gun violence.\n\nRecognizing that insufficient sleep in adolescents is an important public health issue that significantly affects the health and safety, as well as academic success, the American Academy of Pediatrics strongly supports efforts of school districts to optimize sleep in students and urges high schools and middle schools to aim for start times no earlier than 8:30 a.m. to allow students the opportunity to achieve optimal levels of sleep (8.5–9.5 hours) and to improve physical and mental health, safety, academic performance, and quality of life. Although the AAP acknowledges that numerous factors may impair the amount and/or quality of sleep in adolescents - among them, biological changes in sleep associated with puberty, lifestyle choices, and academic demands - it considers school start times before 8:30 a.m. (\"earlier school start times\") to be a key modifiable contributor to insufficient sleep, together with circadian rhythm disruption. It also recognizes that a substantial body of research has demonstrated that delaying the start of the school day is an effective countermeasure to chronic sleep loss and has a wide range of potential benefits to the physical and mental health, safety, and academic achievement of students - including reduced obesity risk, rates of depression, and drowsy driving crashes as well as improved academic performance and quality of life.\n\nThe AAP opposes legalization of marijuana, citing potential harms to children and adolescents. The Academy also opposes medical marijuana \"outside the regulatory process of the US Food and Drug Administration.\" In states that have already legalized marijuana, the Academy recommends that pediatricians and regulators treat it as they would tobacco. The Academy does support \"decriminalization\" of marijuana -- reductions in the penalties for its use and possession -- in combination with an increased commitment to substance-abuse treatment. The Academy also recommends changing marijuana from a DEA Schedule 1 to a DEA Schedule 2 to facilitate research into pharmaceutical uses.\n\nThe American Academy of Pediatrics AGCM posted guidelines in dealing with the ethical issues in pediatric genetic testing.\n\n"}
{"id": "3107620", "url": "https://en.wikipedia.org/wiki?curid=3107620", "title": "Autism Diagnostic Observation Schedule", "text": "Autism Diagnostic Observation Schedule\n\nThe Autism Diagnostic Observation Schedule (ADOS) is an instrument for diagnosing and assessing autism. The protocol consists of a series of structured and semi-structured tasks that involve social interaction between the examiner and the person under assessment. The examiner observes and identifies segments of the subject's behavior and assigns these to predetermined observational categories. Categorized observations are subsequently combined to produce quantitative scores for analysis. Research-determined cut-offs identify the potential diagnosis of classic autistic disorder or related autism spectrum disorders, allowing a standardized assessment of autistic symptoms. The Autism Diagnostic Interview-Revised (ADI-R), a companion instrument, is a structured interview conducted with the parents of the referred individual and covers the subject's full developmental history.\n\nThe Autism Diagnostic Observation Schedule was created by Catherine Lord, Ph.D., Michael Rutter, M.D., FRS, Pamela C. DiLavore, Ph.D., and Susan Risi, Ph.D. in 1989. It became commercially available in 2001 through WPS (Western Psychological Services).\n\nThe ADOS consists of a series of structured and semi-structured tasks and generally takes from 30 to 60 minutes to administer. During this time the examiner provides a series of opportunities for the subject to show social and communication behaviors relevant to the diagnosis of autism. \n\nEach subject is administered activities from just one of the four modules. The selection of an appropriate module is based on the developmental and language level of the referred individual. The only developmental level not served by the ADOS is that for adolescents and adults who are nonverbal. The ADOS should not be used for formal diagnosis with individuals who are blind, deaf, or otherwise seriously impaired by sensory or motor disorders, such as cerebral palsy or muscular dystrophy.\n\nModule 1 is used with children who use little or no phrase speech. Subjects that do use phrase speech but do not speak fluently are administered Module 2. Since these modules both require the subject to move around the room, the ability to walk is generally taken as a minimum developmental requirement to use of the instrument as a whole. Module 3 is for younger subjects who are verbally fluent and Module 4 is used with adolescents and adults who are verbally fluent. Some examples of Modules 1 or 2 include response to name, social smile, and free or bubble play. Modules 3 or 4 can include reciprocal play and communication, exhibition of empathy or comments on others' emotions.\n\nA revision, the Autism Diagnostic Observation Schedule, Second Edition (ADOS-2), was released by WPS in May 2012. It includes updated norms, improved algorithms for Modules 1 to 3 and a new Toddler Module that facilitates assessment in children ages 12 to 30 months.\n\nThere are several organizations that offer training in the ADOS-2.\n\nWPS offers an ADOS-2 Clinical Workshop for professionals unfamiliar with the ADOS-2. It provides attendees the opportunity to observe an instructor administering the ADOS-2 to a child with ASD. During the administration attendees practice scoring. The workshop focuses primarily on Modules 1 through 4, though attendees are given materials to study later in order to complete training in the Toddler Module.\n\nThe clinical workshop offered through WPS is a prerequisite to the more thorough research training offered by the ADOS-2 authors and their colleagues. Research training includes exercises to establish item coding accuracy to a specific criterion, and is designed to help individuals achieve the high cross-site interrater reliability that is required in published research. CADB also offers other training opportunities, such as one-day workshops focused solely on learning the Toddler Module (for researchers and clinicians who are already trained in Modules 1-4 of the ADOS or ADOS-2).\n\n"}
{"id": "40687401", "url": "https://en.wikipedia.org/wiki?curid=40687401", "title": "Cambridge Pulmonary Hypertension Outcome Review", "text": "Cambridge Pulmonary Hypertension Outcome Review\n\nThe Cambridge Pulmonary Hypertension Outcome Review (CAMPHOR) is a disease specific patient-reported outcome measure which assesses quality of life of patients with pulmonary hypertension (PH). It was the first pulmonary hypertension specific questionnaire for assessing patient reported symptoms, quality of life and functioning.\n\nThe CAMPHOR questionnaire was developed by Galen Research in 2006 to allow for cost-utility analyses for treatments of PH. The theoretical basis for the CAMPHOR is the needs-based model of quality of life, which states that quality of life is highest when an individual has the ability and capacity to satisfy their own needs.\n\nThe CAMPHOR is made up of 3 main dimensions which assess symptoms, functioning and quality of life (QoL). The symptom dimension is made up of 25 symptoms and is broken up into 3 subscales: energy, breathlessness and mood. The QoL scale has 25 items which focus on socialization, role, acceptance, self-esteem, independence, and security. The activity scale has 15 items. Response options include true and not true. Scores for QoL and symptoms range from 0-25, with higher scores indicating worse quality of life. Activity scores range from 0-30, with higher scores indicating more physical limitations.\n\nSince the development of the CAMPHOR, it has been translated and validated into fourteen different languages, including Australian and New Zealand English, German and Swedish.\n\nThe Cambridge Pulmonary Hypertension Outcome Review has been a useful tool in clinical trials as it allows researchers to assess whether new medication or therapy is effective. The CAMPHOR has been utilized in clinical trials which investigate the effects of treprostinil, as well as trials which investigate sildenafil.\n"}
{"id": "6334222", "url": "https://en.wikipedia.org/wiki?curid=6334222", "title": "Captain Scarlet (character)", "text": "Captain Scarlet (character)\n\nCaptain Scarlet is the fictional main character in Gerry Anderson's British Supermarionation science-fiction television series \"Captain Scarlet and the Mysterons\" and its computer-animated remake, \"Gerry Anderson's New Captain Scarlet\".\n\nWell-trusted by the commander-in-chief of Spectrum, Colonel White, Captain Scarlet is the primary agent of the organisation and is assigned the most dangerous and crucial missions.\nHe is a close friend of Captain Blue, who is his \"field partner\" and thus with whom he undertakes the majority of his missions, although he is on friendly terms with all other Spectrum agents. A close relationship with Destiny Angel is also hinted at several times in the series.\nCaptain Scarlet was killed in the first episode of the series, \"The Mysterons,\" in a car crash brought about by the Mysterons, which also resulted in the death of fellow officer Captain Brown. Both men were reconstructed by the aliens, who assigned their exact likenesses of both men to assassinate the World President. The Captain Brown likeness was turned into a walking bomb for this purpose. When this attempt in New York failed, the Captain Scarlet likeness kidnapped the President from Cloudbase and flew him to England, taking him to the top of the London Car-Vu, a large car park tower. Cornered while holding the President at gunpoint over the city below, the Captain Scarlet likeness was shot by Captain Blue and fell 800 feet to his apparent destruction. However, at the end of the episode it was revealed that Captain Scarlet was returning to life and had become almost incapable of dying permanently due to the powers of the Mysterons, although the fall had broken the Mysteron programming and returned him to his original personality. This extraordinary ability heals Scarlet of physical injuries within hours, making him virtually indestructible.\n\nCaptain Scarlet's Mysteronised body, like those of all Mysteron likenesses, is still vulnerable to electricity and impervious to X-rays. He also has a \"sixth sense\" when in the presence of a strong Mysteron influence – he becomes nauseated, sweats, and suffers a severe headache – but this sense sometimes does not indicate all Mysteron presences in an area. Though Captain Scarlet \"dies\" several times in the course of the series – usually quite violently – he always returns to life. In \"Attack on Cloudbase,\" Captain Scarlet is declared finally and permanently dead during the course of the battle for Cloudbase; however, this is later revealed to be Symphony Angel's hallucination as she is stranded in a desert, waiting for Spectrum to rescue her following a plane crash.\n\nCaptain Scarlet, as the main protagonist, is one of the most developed characters in the series. His real name is Paul Metcalfe. He has black hair and blue eyes, speaks with a Mid-Atlantic accent and is said to be from Winchester in Hampshire, England. He was born on 17 December 2036. He is not unfamiliar with gambling and drinking, even though he has apparently lost his vulnerability to drunkenness; in the episode \"Special Assignment\" plays, and loses heavily at, roulette. (This is under orders from Colonel White; Spectrum regulations prohibit gambling on grounds of its potentials both for inducing compulsions and for inflicting heavy debt that could lead to financial crimes.) In the episode \"Flight 104,\" Captain Scarlet expresses a preference for steak \"with all the trimmings.\"\n\nCaptain Scarlet is a competent pilot and can drive almost any vehicle; but in this last, he is only moderately skilled, as he tends to crash Spectrum Pursuit Vehicles (SPVs), the heavy-duty modified tanks that Spectrum employs (although this may be attributed to the fact that his indestructible nature 'permits' him to push the vehicles harder than others might as he knows he will survive whatever happens on the drive). He is also a qualified astronaut. He is a somewhat stereotypical hero in that he is dependable and always gets the job done although he is not always successful. He does, however, have a lighter side. He also has a rather dry wit and sarcastic sense of humour, often using this in dialogue with other Spectrum agents.\n\nHe can turn his hand to a variety of weapons from guns to electric cables. Captain Scarlet is not shown to have any love interests during the series although previous attractions are indicated at some points, and a popular speculation among the fan community of the series is that he has a soft spot for, if not a relationship with, compatriot Rhapsody Angel.\n\nCaptain Scarlet has a close friendship with Captain Blue, who acts as Captain Scarlet's \"field partner.\" Captain Blue cares about his friend and Captain Scarlet trusts him implicitly, although he is professional enough to use deadly force against him as necessary when Captain Scarlet was controlled by the Mysterons. In the episode \"Special Assignment,\" Captain Blue tries to stop Captain Scarlet's apparent spiral of self-destruction, showing the bond between them. (It was later revealed that this development was part of a plan to infiltrate a Mysteron attack, the plan being kept secret from Captain Blue so that his reaction to Captain Scarlet's dismissal would be natural.) In the episode \"Renegade Rocket,\" both men are prepared to stay in a missile base targeted by the Mysterons and die in a last-ditch attempt to stop its destruction.\n\nCaptain Scarlet is also friends with Lieutenant Green, as demonstrated when he accompanies Captain Scarlet and Captain Blue on certain missions. However, Captain Scarlet is friendly with all other Cloudbase personnel, and he has no particular enemies among those with whom he is closely associated.\n\nBorn in Winchester, England, UK, Scarlet's mother was Ann Brightman, a British astrophysicist, while his father, Tom Metcalfe, was an American pilot who later joined the International Space Agency. As a boy of ten, Paul watched his father take humankind's first steps on Mars and vowed to follow in his historic footsteps. He studied astrophysics at MIT before joining the US Air Force. But the outbreak of global terrorist wars, which saw the deaths of both his parents, changed his focus.\n\nHe transferred to US Special Forces and commanded an elite unit that saw action across the world. His second-in-command was Lieutenant Conrad Lefkon, who became his closest friend. With the end of the wars, a new security organisation was established: Spectrum. Paul was an obvious choice for the new organisation, and took on the codename \"Captain Scarlet\". His good friend Lefkon was also signed up as \"Captain Black\".\n\nScarlet later became Spectrum's special weapon in its \"war of nerves\" with the Mysterons; a weapon created by the Mysterons themselves when they killed Scarlet during a mission on Mars along with Captain Black. After they had discovered the Mysterons' city, Black fired a destructive shot against it, believing they were under attack. The two men watched in astonishment as the city rebuilt itself, and tried to flee to escape their fate, only to be killed afterwards.\n\nThe Mysterons rebuilt Scarlet as an invincible human replica to infiltrate Earth and lead their war against the planet. While under the control of the Mysterons, Scarlet tried to destroy Skybase, but was stopped in his efforts by Captain Blue. Falling down through a plasma stream, Scarlet was killed, but recovered later on in Sickbay, under the astonished eyes of Doctor Gold and other onlookers, and revealed himself to be fully free of the Mysterons' control. Metcalfe's human psyche had survived and regained control of his physical body. Instead of being killed by the power surge, Scarlet was only harmed temporarily, and was restored to life by the genetic mutation of the Mysteron \"retrometabolisation\" process which makes him virtually \"indestructible\".\n\nScarlet is a dedicated Spectrum officer. First and foremost, he is a soldier on the frontline of Earth's defences. However, he is also tormented by what he has become not fully human. Utterly fearless and dedicated to saving the human race, for all Scarlet's heroism there is one thing that makes him very uneasy – his growing feelings for Destiny Angel, the girlfriend of his late friend, Captain Black.\n\nRetrometabolism – so far, Captain Scarlet can survive, and heal from any injuries, even the more devastating ones, although the recovery is never instantaneous.\n\nIn a few episodes, it is stated that he can feel a Mysteron's presence, by feeling nauseated and unwell. In the pilot story (\"Instrument of Destruction\", Parts 1 and 2), it is also stated that Scarlet shared a kind of telepathic link with Captain Black – which is considered by Colonel White as an advantage in the war against the Mysterons that he cannot dismiss.\n\nDuring the episode \"Rat Trap\", the Mysterons were able to contact Scarlet and to talk to him, while nobody else could hear them. It is not clear if this phenomenon was because he was on the Mysterons' home planet, or if he would hear them, wherever he might be, if they chose to contact him again.\n\nIn \"Chiller\", it was revealed that, when particularly badly wounded, Scarlet's body has the capability to separate his emotional and physical selves so that the latter can heal quicker, leaving the former as a ghostlike apparition, detectable only by its coldness.\n\nCaptain Scarlet is referred to by The Kinks in the song \"Daylight\", on their album \"Preservation Act 1\".\n\nIn the BBC Past Doctor Adventures series – a series of novels based on \"Doctor Who\", featuring the first seven incarnations of the Doctor in original adventures – the novel \"The Indestructible Man\" by Simon Messingham draws heavily on Gerry Anderson's shows in general and \"Captain Scarlet\" in particular, with an organization known as 'PRISM' acting against the mysterious Myloki with the aid of Captain Grant Matthews, transformed into an indestructible state when he was killed and duplicated by the Myloki to such a degree that his original personality asserted itself. In keeping with the adult nature of this interpretation, Grant Matthews is presented as being tormented by his immortality, various sources noting that he is technically only a copy of Grant Matthews rather than being the original man in a new body, ideas that may have been suggested but never analysed regarding Captain Scarlet's nature in the original show.\n\nThe likeness of the original Captain Scarlet has variously been attributed to the actors Cary Grant and Roger Moore as well as to the character's voice actor, Francis Matthews. It is widely acknowledged that the voice was an imitation by Matthews of the English-American, Mid-Atlantic inflections of Grant.\n\nJim Sangster and Paul Condon, authors of \"Collins Telly Guide\" (2005), argue that Captain Scarlet's virtual indestructibility damages his credibility as a hero: \"He'll survive no matter what they throw at him, which should mean that there's zero tension in anything he takes on.\" Daniel O'Brien offers a similar assessment, identifying the character's nearly-unkillable nature as a \"possible miscalculation\" on the part of the series creators. To support his argument, he makes reference to a line of dialogue spoken by the character of Lieutenant Green in the episode \"Attack on Cloudbase,\" in which Lieutenant Green scoffs, \"Anyone can be brave if they're indestructible.\"\n\nScience-fiction writer John Peel considers Scarlet's power to be one of several major faults in the premise of \"Captain Scarlet.\" He also suggests that the character, due to his fundamental near invulnerability, served as a poor role model to the \"impressionable children\" who made up the series' target audience, explaining: \"Parents didn't like their children watching a show that appeared to be encouraging them to hero-worship someone who was indestructible.\" Peel cites \"Batman\" as an example of a TV series aimed at younger viewers who \"sometimes tried to copy their heroes, often with nasty or lethal results.\" He believes that the inducement to experiment with dangerous, imitable techniques is not lessened by the closing titles theme (with lyrics sung by pop group The Spectrum), which includes such lines as \"They crash him, and his body may burn\" and \"They smash him, but they know he'll return ... to live again.\" This led to the recording, by Colonel White's character-voice provider Donald Gray, of the disclaimer, \"Captain Scarlet is indestructible. You are not. Remember this. Do not try to imitate him\". This was played in the opening titles of several episodes of the original series.\n\nSangster and Condon explore the possibility of religious allegory embodied by the character, pointing out parallels between Captain Scarlet and Christ. Actor Cy Grant, who provided the voice of Lieutenant Green, made several observations concerning Christian symbolism. He interpreted the villainous Captain Black, Captain Scarlet's arch-enemy, to be a representation of the Devil; meanwhile, Captain Scarlet, as Christ, is descended from God in the form of Colonel White, Spectrum's commander-in-chief.\n\n"}
{"id": "55125324", "url": "https://en.wikipedia.org/wiki?curid=55125324", "title": "Child labour in Iraq", "text": "Child labour in Iraq\n\nChild labour in Iraq has risen due to poverty, violence and force displacement. Based on a 2016 report by the United Nations Children’s Fund (UNICEF) more than half a million children are working due to decline of family incomes, violence and displacement. The report indicates that the number of children working at the time of the report had increased more than 575,000 since 1990. \n\nBetween 2014,when ISIS took control of large areas in northern and western Iraq, and 2016 almost 10 per cent of Iraqi children or more than 1.5 million have been forced to flee their homes because of violence. The report said. \nAs a result 20 per cent of schools were shut down, and 3.5 million children were unable to go to school.\n\nAnother UNICEF report states that “As of mid-2016, 3.4 million Iraqis - almost 10% of the population - are now displaced, and millions more are in need of urgent humanitarian assistance. This massive internal displacement, as well as conflict-related economic decline, have put enormous strain on the host communities and strained social systems. Many parties to the conflict are engaged in gross human rights abuses and the number of grave violations against children has doubled over the past 12 months—girls who are captured face gender based violence and boys are recruited to fight or work on the front lines.” \nThe Iraqi observatory for Human Rights (IOHR) has documented many cases of child labour, particularly displaced children. The report emphasis the displaced children were forced to work to fulfill the basic needs of their families, but at the same time the employers had exploited the children by paying them very little, and their urgent need for work.\n\nArticle 6, chapter 3 of Iraqi Labour Law, states that the minimum age for employment is 15 years old. According to 1989 International Convention on the Rights of the Child, everyone under the age of 18 is considered a child who must have special protection and care.\n"}
{"id": "7697517", "url": "https://en.wikipedia.org/wiki?curid=7697517", "title": "College health", "text": "College health\n\nCollege Health is a field of medicine that exclusively deals with the medical care of college age students (from age 18 through 28 years). Many colleges and universities campuses offer some sort of student health service, but there is wide variability in the healthcare resources available from campus to campus, with models of student health ranging from first aid stations employing a single nurse to large multi-specialty clinics with hundreds of employees. The vast majority of college health services are set up as service units rather than academic departments. The educational aspect of college health is sometimes referred to Health Promotion in Higher Education.\n\nIn 1988, it was estimated that there were approximately 27.3 college health staff per 10,000 students, which if amortized to the 20.7 million students attending the more than 3,400 colleges and universities in the United States (in 2003) ), suggests that there are approximately 56,500 college health professionals in the United States. College health professionals include physicians, physician assistants, administrators, nurses, nurse practitioners, mental health professionals, health educators, athletic trainers, dietitians and nutritionists, and pharmacists. Some college health services extend to include massage therapists and other holistic health professionals.\nCollege health professionals are often members of a national body, such as the American College Health Association. Another national body among college health is the National Collegiate EMS Foundation (NCEMSF), which is dedicated to the promotion and support of emergency medical services on college and university campuses.\n\nMarijuana use is more prevalent among college students than the general population. A 2015 study, for example, found the daily use of marijuana was more common than cigarettes among college students. In 2016 39 % of college students reported using marijuana in the last year. Heavy marijuana use is linked to poor academic success and the non completion of college. Usage levels are on the rise due to the perception of risk and harm decreasing among the general population. Monitoring the Future is a research study at the University of Michigan that monitors substance abuse among college students. This research study has provided the conclusion that substance abuse is increasing among college students especially students in social organizations such as clubs and fraternities. \n\nMarijuana usage is reported to have the following short term negative effects: short term memory, anxiety, paranoia, panic, hallucinations, slower reaction time, increased heart rate, sexual problems ( In males), increased risk of stroke, and impaired coordination ( primarily in driving and sports). Long term effects of marijuana include: decline in IQ, poor school performance and higher chance of dropping out, impaired thinking and ability to learn and perform complex tasks, lower life satisfaction, addiction, potential development of opiate abuse, relationship problems, antisocial behavior, financial difficulties, and greater chances of being unemployed. \n\nAdderall is a stimulant drug used to treat attention deficit hyperactivity disorder but are becoming more and more recreational to college students. Over 2.5 million Americans are prescribed Adderall , and roughly 50% of college students that were prescribed this drug have been asked by their peers if they can buy some. This stimulant drug are being utilized by college students in order to increase studying focus. It is perceived by college students that the usage of these drugs is not a risk because the fact that it is prescribed to people but that is not the case. Adderall is linked to aggression, restlessness, increased blood pressure and heart rate, paranoia, psychosis, seizures, heart attack, and stroke. Students within Fraternities and Sororities are the most susceptible to the usage of stimulant drugs such as these. College students have been disillusioned with the idea that using Adderall will increase cognitive function and improve studying and school performance . Adderall is known to only improve the cognitive processing in those who have conditions such as ADHD, for individuals with no cognitive condition, the drug should have no effect and taking this drug might even result in negative effects .\n\nAlcohol or ethanol is a depressant used by people. It is found in beer, wine, and\nliquor. Alcohol is made when microorganisms metabolize the carbohydrates when there is\nno oxygen present. This process is called fermentation. Beer, wine, and liquor contain\ndifferent amounts of alcohol and thus affect the drinker differently. Liquor has the highest percentage of alcohol, while beer has the lowest.\n\nAccording to The National Council on Alcoholism and Drug dependence, studies have also shown that drinking alcohol moderately can be helpful to the coronary system. In general, for healthy people, one drink per day for women and no\nmore than two drinks per day for men would be considered the maximum amount of alcohol consumption to be considered moderate use. This shows that drinking can be beneficial in moderation. Binge drinking and alcoholism, however, have proven to be harmful.\n\nRoughly 60 % of college students ages 18-22 drank alcohol within the last month, with 2 out of 3 of them engaging in binge drinking activities at that time. One in four college students report academics effects from drinking including lower grades, missed classes, and performing poorly on exams. Death, Assault, and Sexual assault also occur more often when alcohol is involved. Campuses have put drinking awareness programs in place in order to combat student drinking. These programs include but are not limited to education and awareness programs, cognitive behavioral skill based approaches, motivation and feedback related approaches, and behavioral interventions by health professionals.\n\nEating disorders are relatively common among college students and can be caused due to changes in lifestyle and stress levels. Examples of common stressors with college students are relationships, classwork, and lack of sleep, which can cause students to exhibit eating disorders such as anorexia nervosa, binge eating, or bulimia nervosa.\n\nAnorexia nervosa is an eating disorder that is characterized by low appetite and a strong desire to lose weight, be thin, and fears of gaining weight. People suffering from anorexia nervosa frequently have an abnormally low body weight, which can cause muscle dysfunction and weak osmoregulation. It can also accompany emotional symptoms such as depression.\n\nBinge eating disorder is an eating disorder characterized by frequent and recurrent binge eating episodes with associated negative psychological and social problems, but without subsequent purging episodes (e.g. vomiting). People who have this disorder can experience an uncontrollable urge to consume large amounts of food regardless of whether or not they are hungry and can feel like they have no control while they are eating.\n\nBulimia Nervosa is an eating disorder characterized by recurring episodes of binge eating followed by compensatory behaviors such as self induced vomiting in order to compensate for all the excess of food and calories eaten. Other compensatory methods include misuse of laxatives, diuretics, or other medications to purge your body, and fasting and excessive exercise. People with this condition base their decisions mainly upon their body shape and weight. On average someone who is bulimic one goes through these recurring episodes at least once a week for three months.\n\nThe entrance of a new life means changing and adapting. Beginning freshmen enter a new phase that affects the way they eat. They are unaware of their nutrition and they only want something from what they see. High school is very different from what college is. Students transition from being in small classroom into big lecture rooms. The same goes for food, they transition from being served in cafeterias to buffet styles in college. Dining centers excite new students because this is a new experience, therefore they grab more than what they should. It’s something new and it’s what they’re paying for.\n\nAround campus are many network of vending machines fill with varieties of junk food. When students don’t have the time to eat they rely on vending machines as a part of their meal. Illinois: Champaign, 2004. Eating junk food lacks nutrition and proteins. Eventually those calories in the products will build up into fats.\n\nUniversities are not finding ways to reduce junk food around campus nor providing healthier meals in dining centers. Instead, university administrators give away specific amount of dining dollars to students who lives on campus. Although they would want to serve healthier food to their students, the price for them is just too expensive to spend.\n\nMany experiments are being tested to see how much weight can gain in one year. So far studies have shown that women with high stress levels are more likely to gain weight than non-stressed women. Women with stress are more likely to consume alcohol and having the tendencies to go out more in order to eliminate their emotion. They also eat low in fiber and consume more caffeine. Nevertheless, non-stress women have more vegetable in their body and they are cautious enough to stay away from high cholesterol food.\nAlthough students do gain weight in college, they can always burn it off by exercising and eating right. Overall meaning limiting how much they eat out and the junk food they pact into their body system. Students can prevent the ‘freshman 15’ if only they put effort and hard work for a healtheir body or image.\n\n\n\n"}
{"id": "47865555", "url": "https://en.wikipedia.org/wiki?curid=47865555", "title": "CorePower Yoga", "text": "CorePower Yoga\n\nCorePower Yoga is the largest privately held chain of yoga studios in the United States. The company is based in Denver, Colorado and headed by Eric Kufel who serves as the CEO. Each yoga studio teaches a CorePower yoga style developed by founder Trevor Tice that combines power yoga, Ashtanga yoga, Vinyasa yoga, and Bikram yoga.\n\nTrevor Tice (died December 2016) founded CorePower Yoga in 2002. Tice funded the first 20 studios with the proceeds from the sale of his previous company, Tech Partners International. The company later received an additional investment of over $100 million from Catterton Partners. ColoradoBiz included the company on its list of top companies in 2011.\n\nIn 2012, CorePower Yoga generated $45.2 million in revenue. The company was included on \"Chicago’s\" best of Chicago 2014 list. CorePower Yoga was voted “Best Yoga Class in Minnesota” by CBS Minnesota viewers in January 2015. \n\nIt was also included on Well+Good’s “Best Yoga Studios” list. By November 2016, the company had 160 studio locations throughout the United States.Eric Kufel succeeded Amy Shecter as CEO of CorePower Yoga in January 2016.\nCorePower Yoga offers a variety of heated and non-heated yoga class styles, including classes that utilize weights and other equipment to build strength. Studios provide occasional special classes and workshops, such as themed classes and arm balance and inversion workshops. Aside from its class offerings, CorePower Yoga also conducts Yoga Alliance certified 200-hour teacher training programs. Additional program offerings include yoga vacation retreats and two-week boot camps.\n"}
{"id": "11113685", "url": "https://en.wikipedia.org/wiki?curid=11113685", "title": "Cup-to-disc ratio", "text": "Cup-to-disc ratio\n\nThe cup-to-disc ratio (often notated CDR) is a measurement used in ophthalmology and optometry to assess the progression of glaucoma. The optic disc is the anatomical location of the eye's \"blind spot\", the area where the optic nerve and blood vessels enter the retina. The optic disc can be flat or it can have a certain amount of normal cupping. But glaucoma, which is in most cases associated with an increase in intraocular pressure, often produces additional pathological cupping of the optic disc. The pink rim of disc contains nerve fibers. The white cup is a pit with no nerve fibers. As glaucoma advances, the cup enlarges until it occupies most of the disc area.\n\nThe cup-to-disc ratio compares the diameter of the \"cup\" portion of the optic disc with the total diameter of the optic disc. A good analogy to better understand the cup-to-disc ratio is the ratio of a donut hole to a donut. The hole represents the cup and the surrounding area the disc. If the cup fills 1/10 of the disc, the ratio will be 0.1. If it fills 7/10 of the disc, the ratio is 0.7. The normal cup-to-disc ratio is 0.3. A large cup-to-disc ratio may imply glaucoma or other pathology. However, cupping by itself is not indicative of glaucoma. Rather, it is an increase in cupping as the patient ages that is an indicator for glaucoma. Deep but stable cupping can occur due to hereditary factors without glaucoma.\n"}
{"id": "20474901", "url": "https://en.wikipedia.org/wiki?curid=20474901", "title": "Customized employment", "text": "Customized employment\n\nCustomized employment (CE) is a way of personalizing the employment relationship between a candidate and an employer in order to meet the needs of both. It applies in particular to employees with disabilities. The individual employee's skills, interests and needs are identified in a process of \"discovery\", and job content and environment are tailored to these in a process of negotiation.\n\nCustomized employment aims to provide everyone with an equal opportunity to participate in community life. Community inclusion of individuals with disabilities requires support and advocacy from local businesses for concepts like customized employment. Molina, Leslie and Demchak, MaryAnn Demchak from \"Rural Special Education Quarterly\" says \"If people with intellectual disability are to become truly self-determined, they must be allowed to express choice throughout their lives, including employment. Expectations for competitive employment tend to be low for this population, if considered at all\" (Molina, Leslie, and MaryAnn Demchak. Rural Special Education Quarterly 35, no. 2: 24-32). Through the customized employment concept businesses can universally accept the practice that recognizes the power of community relationships with persons with disabilities who have been left out of the equation of community.\n\nKatherine Inge from Virginia Commonwealth University Rehabilitation Research and Training Center says \"The term, customized employment, is attributed to a speech that Secretary of Labor, Elaine Chao, made upon being conﬁrmed by the U.S. Senate in 2001. In that speech, Ms. Chao referred to customization as a trend in the labor market. Within 6 months of her speech, the Ofﬁce of Disability Employment Policy, a new ofﬁce within the U.S. Department of Labor, put forth in the Federal Register, a major initiative from the U.S. Department of Labor. They termed that initiative \"customized employment\" (Katherine Inge, Journal of Vocational Rehabilitation 28, no. 3: 133-134). The deﬁnition of customized employment that was published in the Federal Register is as follows: People with disabilities find customized employment to be successful way of gaining purposeful work with real wages. Todd Citron et al. says \"A person with a disability who needs supports often begins with a negative label and stands at risk of rejection, segregation, isolation and limited adult opportunities. While in our culture the notion of freedom is strongly tied to personal power, control and inﬂuence, many individuals with disabilities have been historically denied access to the opportunities for choice and decision-making necessary to experience becoming successful in what they wish to do with their lives\" (Citron et al., Journal Of Vocational Rehabilitation 28, no. 3: 169-179). Customized employment utilizes a process of \"discovery\" to uncover an individual's strengths, weaknesses, interests, task contributions, and conditions of employment to create meaningful work and a customized fit. Rather than looking at work opportunities driven by the market, customized employment practices create employment that fulfills both the applicant with disabilities and the employer's needs.\n\nDiscovery is a key component of customized employment. this is a process of identifying individual's skills and interests through interviews, observations, and conversations. Karen L. Heath et al., from Center for Human Development says \"The Discovery process is both open and formal; it is time-limited; and it is not concerned with predicting the future. Rather, it is focused on employment that matches who the individual is now: one potential match is self-employment. While self-employment is gaining credibility as a viable employment option for individuals with disabilities, self-employment is not for everyone\" (Heath, Karen et al., Journal of Vocational Rehabilitation 39, no. 1: 23-27). Even though self-employment for individuals with disabilities is less common, it allows individuals to receive assistance in creating of independently owned small business that are typically under five employees. However, there has been some debate regarding CE and its relationship to supported employment. Katherine J. Inge from Virginia University says \"Customized employment does not include group placements or sub-minimum wage positions that have unfortunately continued under supported employment services. However, since there are no formal regulations regarding customized employment implementation, the strategy faces the same pitfalls that have limited supported employment implementation. Hours worked, and wages earned will be issues as providers negotiate with employers to customize jobs for individuals with signiﬁcant disabilities\" (Inge, Journal of Vocational Rehabilitation 24, no. 3 (June 2006): 191-193). There is talk amongst some authors that individuals with disabilities are being placed in jobs that are driven by the local labor market rather than negotiated positions based on individual's preferences and choice.\n\nIndividuals, as well as companies, stand to benefit from customized employment. Customized employment concept provides the business with reliable and dependable employees, it reduces recruitment and hiring process, it matches job seekers with specific employment needs, increases employee retention, helps the business to attract broader customer base, enhance diversity, and increase tax benefits. Paulo dos Santos Rodrigues et al. from Brazilian Academy of Sciences say \"One U.S. study reports that employers express a high level of satisfaction when they \"customize\" job tasks for speciﬁc individual job candidates. Employers who hired individuals with signiﬁcant disabilities through a customized employment process, when interviewed about their experience, identiﬁed distinct advantageous results such as increased sales revenue, improved operations, and higher customer satisfaction\" (Dos Santos Rodrigues, Journal of Vocational Rehabilitation 38, no. 3: 185-194). Customized employment works in a way that it starts with the person and engages employers through based negotiation disclosing benefits that hiring a specific individual will have for both parties. Cary Griffin et al. from Griffin-Hammis Associates, LLC say \"Most supermarkets include a Union butcher shop, a produce department, an Information Technology department, Clerical, shipping and receiving, and management departments. All these operations employ people, therefore bagging groceries should be only one possibly out of a hundred options explored through creative instruction, job carving, and interest-based job negotiations\" (Griffin et al., Journal Of Vocational Rehabilitation 28, no. 3: 135-139). CE takes form in job carving, job sharing, and task reassignment which will be discussed below.\n\nTammy Jorgensen Smith et al. from Department of Rehabilitation and Mental Health Counseling, University of South Florida say \"An example of a person who may benefit from discovery is an individual with autism. Autism spectrum disorder (ASD) is a range of complex neurodevelopmental disorders, characterized by social impairments, communication difficulties, and restricted, repetitive, and stereotyped patterns of behavior (NINDS, 2014). Persons with autism may be resistant to change and may not be comfortable in offices or other unfamiliar settings. Sensitivity to stimuli may further this discomfort. Additionally, many people who have autism have issues with communication and some have limited or no speech. These characteristics may contribute to difficulties with traditional employment strategies, but do not indicate that the person does not have talents that can be translated into a work setting. Discovery is able to uncover these talents\" (Tammy Jorgensen Smith et al., Journal of Vocational Rehabilitation 42, no. 3: 201-208).\n\nSometimes customized jobs do not exist in complete job descriptions, but are created through initiatives like job carving, job creation, the development of a business-within-a-business, resource ownership, or a self-employment opportunity. Job carving happens when individuals analyze duties performed in given jobs and identify specific tasks within those existing positions that individuals with disabilities can accomplish. Jobs carved for individuals may be formed either by editing one existing job or by mixing tasks from multiple jobs to create new positions. In whichever way the process is completed, job carving is a means of focusing on individuals' abilities, skills, and talents they bring to potential employers.\n\nFor example: Erick is looking to work in the fields of journalism and advertising. He is a great storywriter and a salesman. He uses one finger to type on a computer keyboard, and he types 20 words per minute. Journalists for a local newspaper are expected to write three stories per week, he would only be able to complete one story a week. The newspaper happens to also have an advertising sales need. Erick cannot communicate effectively on the telephone, but he shows talent in courting the business of advertisers through electronic mail. The local newspaper decides to hire Erick. They use his abilities as both a storywriter and a salesman. This happens by giving Erick the tasks of researching, developing, and composing one feature story a week and successfully soliciting advertisers.\n\nJob creation happens when certain employers' needs are matched with the skills of job seekers. Developing new jobs can be through the process of job carving or by coming up with totally new job descriptions. In the latter case, individuals' unique assists are marketed to businesses. For example: Jane finds a local small business uses paper files to keep track of sales and inventory. The owner of the business has no employees and is solely running the daily operations of the establishment. Because their sales are increasing, keeping account of transactions and items on paper has proven extremely difficult. Through discussing her abilities and talents pertaining to computers and business management, Jane markets herself to the business owner. In turn, the owner decides to create a position for Jane. In this new position, Jane is responsible for developing, implementing, and maintaining a computerized system dedicated to recording sales and inventory.\n\nTim Riesen et al. from Department of Special Education and Rehabilitation say \"Targett, Young, Revell, Williams, and Wehman (2007) described how youth in transition used One Stop Career Centers to support customized career development. The authors explained how students used the resources from centers to obtain employment. These resources included career club curricula, mentoring programs, and internships. In addition, they described how students had access to CE resource staff who provided individualized representation and negotiation with employers\" (Riesen, Tim et al., Journal Of Vocational Rehabilitation 43, no. 3: 183-193). Individuals with disabilities realized early on that Customized Employment is real employment with real pay. CE encouraged students with disabilities to work and realize that employment is essential to successful adult transition. However, Christopher Rogers et al. from Institute on Community Integration, University of Minnesota say, \"Current school-to-career transition practices are not leading to sufficient levels of competitive employment and post-secondary education outcomes for youth and young adults with significant disabilities despite progressive mandates and policy improvements in federal and state secondary and post-secondary education, vocational rehabilitation, and workforce development services\" (Christopher Rogers et al., Journal Of Vocational Rehabilitation 28, no. 3: 191-207). When examined more closely, however, the studies showed that out of school youth had a similar rate of employment when compared to subgroup of students still attending high school. Pam Targett et al. say \"James is a pleasant young man with a learning disability who attended special education and general education classes. During his freshman year, James learned about the local One Stop's summer youth program and was encouraged to participate by school personnel. Later that year, he enrolled in the summer work program. This program provides eligible youth with 3 weeks of employment training followed by 149 hours of paid work experience. Throughout his remaining years in high school. James attended workshops and training at the One Stop on various work-related topics and independent living issues. He also continued to participate in the summer youth program. One summer he was advised to participate in vocational courses at the local technical center to further develop his work skills. When James graduated from high school with a general diploma, he had employment experience from the summer youth programs, and certificates for completing courses in cabinetmaking. building maintenance, and occupational safety and health from the technical center. Now he was ready to pursue full-time work\" (Christopher Rogers et al., Teaching Exceptional Children 40, no. 2: 6-11.)\n\nCE has proven to be a reliable employment option for individuals with disabilities; however, to implement customized employment solutions, service providers must expand capabilities that they may not have. Jennifer Harvey et al., from Deloitte Consulting, LLP says \"CE consists of four process components: Discovery; Job Search Planning; Job Development and Negotiation; and Post-Employment Support. Typically, the employment specialist leads the individual and the CE support team through the ﬁrst component, Discovery, to determine the individual's interests, skills, and preferences related to potential employment. That information is used to develop a plan, determine a list of potential employers, and conduct an analysis of beneﬁts during the Job Search Planning component. Once a potential employer is identiﬁed, the individual and the employer, in the third component, negotiate 1) a customized job, 2) the provision of supports, and 3) the terms of employment that will meet the needs of the individual and the employer. In the case of self-employment, the individual and the agency providing CE services, construct a customized self-employment situation, such as a small business, negotiate the provision of supports to help make the business a success, and tailor the business operations to meet the need of the individual\" (Harvey, Jennifer et al., Understanding the competencies needed to customize jobs: A competency model for customized employment.\" Journal of Vocational Rehabilitation 38, no. 2: 77-89.) Business that implement customized employment will be hiring individuals with skills outside of traditional employment and likely provide better service and broader opportunities to their clients. Michelle Ouimette, and Linda Rammler from \"Journal of Vocational Rehabilitation\" say \"Successful opportunities and innovations include the \"right kind\" of social enterprise, entrepreneurship through self-employment and micro-enterprises and other entrepreneurial models\" (Ouimette, Michelle, and Linda H. Rammler, Journal Of Vocational Rehabilitation 46, no. 3: 333-339). Leaders on all levels are treating employment as a priority since it has an impact in the long run on money and employment opportunities.\n\nDisclosing one's disability may be a concern for people with disabilities and well as organizations who assist them when looking for a job. Katherine J. Inge and Pam Targett from Virginia Commonwealth University say \"Access to an accommodation in the work place is often dependent on a person's disclosure of disability related needs. Individuals with visible or hidden disabilities, who know that they will need work-related accommodations including an individualized job description, should plan to disclose. If an accommodation is needed, the job seeker with his/her employment specialist must plan how and when to tell potential employers about the disability and be prepared to discuss support needs\" (Inge Katherine and Pam Targett, Journal of Vocational Rehabilitation 28, no. 2: 129-132). Disclosure is also important because an individual with disability may need to leave work several times a week for medical appointments and if the employer is unaware of the employee's disability they may have a different attitude towards employee's performance.\n\n\n\nThe following websites may provide more information on customized employment. \n"}
{"id": "8270573", "url": "https://en.wikipedia.org/wiki?curid=8270573", "title": "Diffuser (sewage)", "text": "Diffuser (sewage)\n\nAn air diffuser or membrane diffuser is an aeration device typically in the shape of a disc, tube or plate, which is used to transfer air and with that oxygen into sewage or industrial wastewater. Oxygen is required by microorganisms/bacteria residents in the water to break down the pollutants. Diffusers use either rubber membranes or ceramic elements typically and produce either fine or coarse bubbles.\n\nDiffusers are generally referred to as either:\n\n\nOther diffused aeration devices include: jet aerators, aspirators, and U tubes.\n\nTypical efficiency of a full floor coverage diffused aeration system in clean water is 2%/ft submergence or 6.6%/m submergence. When converted to mass transfer into process or dirty water, it is typically closer to about half of those figures. Manufacturers of fine bubble systems have supported claims that the type, number and size of \"pores\" have a great effect on efficiency of a diffused aeration system.\n\nDiffusers are typically connected to a piping system which is supplied with pressurized air by a blower. This system is commonly referred to as a diffused aeration system or aeration grid.\n\nThere are two main types of diffused aeration systems, retrievable and fixed grid, that are designed to serve different purposes. In the case of a plant with a single tank, a retrievable system is desirable, in order to avoid stopping operation of the plant when maintenance is required on the aeration system. Fixed systems, on the other hand, are typically less costly, and often more efficient because it is easier to make full use of the floor.\n\nAutomated software is available on the web to assist with drafting of aeration systems in CAD, as well as calculation software to help determine diffuser requirements for a given wastewater.\n\nA developments in recent years has been surface coatings of PTFE on EPDM membranes. This provides a buffer between the EPDM substrate and wastewater, hence reducing the likelihood of chemical attack and oxidation, and also providing better resistance to biological fouling and calcium scaling (manufacturers claim).\n\n"}
{"id": "33282575", "url": "https://en.wikipedia.org/wiki?curid=33282575", "title": "Doctor of Physiotherapy", "text": "Doctor of Physiotherapy\n\nThe Doctor of Physiotherapy is an extended masters degree under the Australian Qualifications Framework Level 9 classification. The Doctor of Physiotherapy program at this time is only offered in Australia. The degree should not be confused with the American Doctor of Physical therapy, which is a post baccalaureate degree.\n"}
{"id": "8810434", "url": "https://en.wikipedia.org/wiki?curid=8810434", "title": "Drug Abuse Warning Network", "text": "Drug Abuse Warning Network\n\nThe Drug Abuse Warning Network (DAWN) was a public health surveillance system in the United States that monitored drug-related visits to hospital emergency departments and drug-related deaths. DAWN was discontinued in 2011, but its creator, the Substance Abuse and Mental Health Services Administration (SAMHSA), continues to develop other sources of data on drug-related emergency visits.\n\nHospitals participating in DAWN are non-federal, short-stay general hospitals that feature a 24-hour emergency department. Patients are never interviewed. All data are collected through a retrospective review of patient medical records and decedent case files. DAWN collects detailed drug data, including illegal drugs of abuse, prescription and over-the-counter medications, dietary supplements, and non-pharmaceutical inhalants. Because the DAWN cases are defined broadly, DAWN captures many different types of drug-related cases. The whole point of this organization is to find out how many people abuse most drugs. They also seek short-stay hospitals, when a case is drug-related.\n\nIn 1974, DAWN was designed and developed by the scientific staff of the DEA's Office of Science and Technology. It was jointly funded with the National Institute of Drug Abuse (NIDA). DAWN then became a division of the United States Department of Justice before becoming part of NIDA in 1980.\nOn October 1, 1992, DAWN became part the Substance Abuse and Mental Health Services Administration (SAMHSA), an agency of the United States Department of Health and Human Services. SAMHSA has contracted with Westat, a private research corporation, to manage the New DAWN on the agency’s behalf.\n\nInformation collected by DAWN is widely cited by drug policy officials, who have sometimes confused drug-related episodes – emergency department visits induced by drugs – with drug mentions. The Wisconsin Department of Justice claimed, \"In Wisconsin, marijuana overdose visits in emergency rooms equal to heroin or morphine, twice as common as Valium.\" Common Sense for Drug Policy called this as a distortion, noting, \"The federal DAWN report itself notes that reports of marijuana do not mean people are going to the hospital for a marijuana overdose, it only means that people going to the hospital mention marijuana as a drug they use\". This criticism is also not correct. DAWN has recently clarified their use of the term \"drug mention\" in methodology because of this erroneous claim. The data is collected by a systematic and confidential review of patients' medical records. Thus, for example, a patient who broke an arm while high on marijuana would not be included in the data. A report released by DAWN in 2002 claims that marijuana overdose alone resulted in documented deaths in Atlanta and Boston, respectively. However, there is no known record or evidence to support the existence of a case of human fatality by result of marijuana overdose.\n\n"}
{"id": "2103123", "url": "https://en.wikipedia.org/wiki?curid=2103123", "title": "Edgar Schoen", "text": "Edgar Schoen\n\nEdgar J. Schoen (August 10, 1925 – August 23, 2016) was an Austrian and Hungarian physician who worked as a pediatric endocrinologist at Kaiser Permanente Medical Center in Oakland, California until 2003, and Clinical Professor in Pediatrics at the University of California, San Francisco until 2004. He held the position of Chair of the 1988 American Academy of Pediatrics Task Force on Circumcision.\n\nSchoen held positions at Children's Hospital of the East Bay in Oakland, CA, and the University of California Medical Center in San Francisco, CA and was Board-certified in Pediatrics and Pediatric Endocrinology. He practiced Pediatrics and Pediatric Endocrinology in Oakland, CA for 46 years. Schoen was Chief of Pediatrics at Kaiser Permanente in Oakland for 24 years.\n\nSchoen maintained Medicirc.org, an online resource in which he discussed what he perceived as the benefits of circumcision. It went offline at the end of 2012. Interviewed in the \"Eastbay Express\" (2000), he stated, \"Circumcision is one of the best health insurance policies you can give a son. A circumcised boy has a lifetime advantage over an uncircumcised one.\"\n\nSchoen has written about circumcision in the books \"Ed Schoen, MD on Circumcision\" () and \"Circumcision, Sex, God, and Science: Modern Health Benefits of an Ancient Ritual\" () as well as poetry on the topic in the \"American Journal of Diseases in Children\".\n\nIn a \"Boston Globe\" article, Schoen said, concerning the AAP's decision to not advocate circumcision, \"It's highly biased\". The 1989 report he oversaw stated that circumcision reduced the risks of urinary tract infections and sexually transmitted diseases.\n\n"}
{"id": "52299230", "url": "https://en.wikipedia.org/wiki?curid=52299230", "title": "Emil C. Gotschlich", "text": "Emil C. Gotschlich\n\nEmil Claus Gotschlich is a professor emeritus at the Rockefeller University. He is best known for his development of the first meningitis vaccine in 1970.\n\nGotschlich received his M.D. from the New York University School of Medicine in 1959. He interned at Bellevue Hospital in New York before joining The Rockefeller University's Laboratory of Bacteriology and Immunology in 1960. He was promoted to professor and senior physician at The Rockefeller University Hospital in 1978. From 1996 to 2005 he served as the hospital's vice president for medical sciences. He is a member of the National Academy of Sciences and its Institute of Medicine. He was the recipient of the 1978 Albert Lasker Clinical Medical Research Award. He was the recipient of the 2008 Dart/NYU Biotechnology Achievement Award. \n"}
{"id": "40331451", "url": "https://en.wikipedia.org/wiki?curid=40331451", "title": "Fatal Vision (goggles)", "text": "Fatal Vision (goggles)\n\nFatal Vision goggles are a line of training tools for simulating the effects of alcohol and drug intoxication without actually using these substances. Five of the models are intended to simulate five ranges of blood alcohol content: .06% or less, .07% to .10% or so, .12–.15% or so, .17–.20% or so, and .25% or so. A sixth model is said to provide \"extreme blurriness and double vision\", and thereby simulate intoxication with drugs other than alcohol.\n"}
{"id": "19893937", "url": "https://en.wikipedia.org/wiki?curid=19893937", "title": "Global Health Council", "text": "Global Health Council\n\nThe Global Health Council is a United States-based non-profit leading networking organization \"supporting and connecting advocates, implementers and stakeholders around global health priorities worldwide\". The Council is the world's largest membership alliance dedicated to advancing policies and programs that improve health around the world. The Council serves and represents thousands of public health professionals from over 150 countries across the globe. They work \"to improve health globally through increased investment, robust policies and the power of the collective voice.\": According to their website the Council \"convenes stakeholders around key global health priorities and actively engages key decision makers to influence health policy.\"\n\nAfter shutting its doors in 2012, GHC re-opened with a newly elected board of directors on January 1, 2013. In their new model, the Global Health Council works in three main areas: policy and advocacy, member engagement, and connections and coordination. Reflecting this focus, GHC offers an online platform that includes guest blogs, member spotlights, policy briefs, advocacy updates, and global health job postings. Additionally, the GHC has since participated in significant global health events at the national and international level including the World Health Assembly, Global Health Week on the Hill, and the Consortium of Universities for Global Health Consortium.\n\nThe Council coordinates and participates in a number of working groups, coalitions, and roundtables in the global health advocacy community, including the GHC Budget Roundtable and Global Health Security Roundtable. The Council maintains a global health Advocacy Hub and coordinates the biennial publication of the Global Health Briefing Book. The most recent publication, titled \"Global Health Works: Maximizing U.S. Investments for Healthier and Stronger Communities,\" was released in 2017 for the 115th U.S. Congress.\n\nThrough almost five decades of work, the Global Health Council has continuously served as the leading membership organization focused on global health advocacy.\n\nIn 1972, the ‘National Council for International Health’ was first established. In 1998, the organization became the Global Health Council to better represent its work in the 21st century. As the council evolved, its name had to evolve to correctly reflect the scope of the Council's work. The inclusion of global in its name reflected the Council's goal to include more international organizations and individuals in its membership and become the preeminent non-governmental source of information, practical experience, analysis and public advocacy for the most pressing global health issues.\n\nIn 1998, the Council began organizing the Global Health Action Network in pursuit of its advocacy building goals. The network was designed to establish groups of motivated citizens across the U.S. to educate local communities and their elected officials about the need for a more proactive approach to global health. With this network in place, the Council was able to implement nationwide advocacy campaigns dealing with vital global health issues. The Council continued to grow as the voice for global health by using its annual conference; its website to promote advocacy, education and information sharing; its media outreach through a diverse field of multi-disciplinary specialists; and its publications, including the Global Health Magazine \".\n\nAs part of the Council's work in advocacy and developing awareness of the AIDS crisis, the seventeen-year-old International AIDS Candlelight Memorial event came under their stewardship in 1999. The International AIDS Candlelight Memorial grew to include 1,500 communities in more than 100 countries. It is the world's largest and oldest grassroots HIV/AIDS event.\n\nIn April 2012, the Global Health Council announced it would be closing its doors for the upcoming months. The Board of Directors released the announcement acknowledging that, “although the Global Health Council will no longer play the same role, we will continue to fight for the goals that first inspired us to action.”\n\nWith no unified voice for global health in the advocacy sphere, 40 international organizations came together at the end of 2012 to pledge almost $300,000 over three years to establish a new Global Health Council. The new Council included a newly elected Board of Directors and leaner business model. In January 2013, the Global Health Council reopened its doors to continue their mission bringing global health concerns to the forefront of policy work. The Council added Global Impact to serve as the organization Secretariat, in order to allow the Council to focus on advocacy through convening, communications, and coalition-building. Dr. Christine Sow was selected as its new Executive Director, becoming the first woman to lead the organization.\n\nGHC has hundreds of individual and organizational members representing a number of sectors in the global health field. In 2014, GHC member organizations operated in more than 150 countries worldwide and had a combined annual revenue of over $40 billion. The Board of Directors is led by Dr. Jonathan Quick of Management Sciences for Health (MSH). Loyce Pace became President and Executive Director of the organization in December 2016.\n\n\nThe Council has administered a number of prominent awards. Some of the recipients have been barred from traveling to receive them which has brought attention to their work. In addition to the following honorific awards the organization has also conferred the \"Best Practices in Global Health Award\" and the \"Excellence in Media Award for Global Health\".\n\nThe Jonathan Mann Award for Health and Human Rights is named for former head of the World Health Organization (WHO)'s global AIDS program, Jonathan Mann, who resigned to protest the lack of response from the United Nations and WHO with regard to AIDS. In 2001, the recipient was Gao Yaojie, a retired Chinese gynecologist and one of China's foremost AIDS fighters who helped poor farmers in Henan Province that were infected with H.I.V. through selling their blood at for-profit and unsanitary collection stations. Yaojie was denied permission to attend an awards ceremony in Washington with Secretary General Kofi Annan of the United Nations as her host.\n\nIn 2008, the Mann award was to be given to Binayak Sen, a prominent Indian doctor responsible for drawing up one of the most successful community-based health-care models in India — based on the traditional mitanin, a health worker who advises the rural poor on preventive care — making health care available to many who had lacked access. He had been a vocal critic of the government's use of armed groups to push villagers out of mineral-rich forests to boost development and was jailed in April 2007 on sedition charges, including allegedly being linked to Maoist rebels and smuggling a letter for an accused Maoist prisoner he had visited. Sen denies the charges and his effort to get the award in person was bolstered by 22 Nobel laureates; he is out of jail on bail.\n\nNamed for and funded by Bill & Melinda Gates Foundation, the Microsoft Corp. founder and his wife, the Gates Award ($1 million) is administered by the Council. In 2004 the award went to Fazle Hasan Abed and his organization, Bangladesh Rural Advancement Committee (BRAC), one of the \"world's most successful development organizations, credited with improving the health and welfare of tens of millions of destitute people in Bangladesh\". Past winners include the Rotary Foundation, which has raised millions for an ongoing global campaign to stamp out polio.\n\nThe winner of the 2009 Gates Award was awarded to The London School of Hygiene & Tropical Medicine.\n\nFrom its inception through the 1990s, the Council was principally funded by grants (primarily from the U.S. Agency for International Development [USAID] and the Centers for Disease Control [CDC]).\" In 1998, Nils Daulaire, formerly of USAID, became president of The Global Health Council and felt that the council should be an independent voice. The council diversified its funding as a matter of principle, even though at the time its policy agenda was consistent with that of the then administration. By 2003, only 20 percent of the council’s funding came from the U.S. government.\"\n\nThe Council has received grants of varying sizes from a variety of foundations, including the Gates, Packard, Hewlett MacArthur, and Rockefeller.\n\nAs part of the President's Emergency Plan for AIDS Relief, US government support for AIDS prevention was contingent on opposing prostitution starting in 2003. The Council preferred to remain neutral so as not to alienate sex workers from their ant-HIV efforts so they sued in federal court with other non-profits. In 2013, the U.S. Supreme Court found that the requirement violated the First Amendment's prohibition against compelled speech in \"Agency for International Development v. Alliance for Open Society International, Inc.\"\n\n"}
{"id": "25879538", "url": "https://en.wikipedia.org/wiki?curid=25879538", "title": "Global Medical Device Nomenclature", "text": "Global Medical Device Nomenclature\n\nGlobal Medical Device Nomenclature (GMDN) is a system of internationally agreed generic descriptors used to identify all medical device products. This nomenclature is a naming system for products which include those used for the diagnosis, prevention, monitoring, treatment or alleviation of disease or injury in humans.\n\nThe main purpose of the GMDN is to provide health authorities / regulators, health care providers, conformity assessment bodies and others with a single generic naming system.\n\nMedical device experts from around the world (manufacturers, healthcare authorities and regulators) compiled the GMDN, based on the international standard ISO 15225. The work was mandated by the European Commission in order to provide the necessary tool to carry out the implementation of the Medical Devices Directive, including the European databank for medical devices (Eudamed).\n\nIn the draft of the proposed Medical Device Regulation (from the European Commission) it states “To facilitate the functioning of the European Databank on medical devices (Eudamed), a medical device nomenclature should be available free of charge to manufacturers and other natural or legal persons obliged to use that nomenclature under this Regulation. Furthermore this nomenclature should be provided, to the maximum possible extent free of charge, also to other stakeholders.”\n\nThe GMDN is the current nomenclature used by Members States in the National Competent Authority Reports as part of the current EUDAMED system to exchange information on device safety. The GMDN is not yet specified for the UDI database system proposed in new European Regulations intended to be used by manufacturers of medical devices who have their own UDIs (unique device identifiers) and traceability. \n\nThe GMDN meets the need to identify medical devices at the global level, as identified in the Global Harmonization Task Force (GHTF) that have since disbanded (2012) and replaced by the IMDRF\n\nGMDN is managed by the GMDN Agency, a non-profit organization and Registered Charity, which reports to its Board of Trustees, that represent medical device regulators and industry.\n\nThe GMDN term is in the form of a 5-digit numeric GMDN Code cross-referenced to a specific Term Name and Definition, with which all specific medical devices having substantially similar generic features, can be identified. The following is an example:\n\n\nThe GMDN term and other associated data is copyright protected and the GMDN is a Trademark.\n\nUniquely each GMDN term has a set of attributes, known as Collective Terms, which help to navigate the GMDN Database and aid the selection of a GMDN term by medical condition or product feature.\n\nThe GMDN is used by regulators, healthcare providers and others for activities such as medical device recalls, adverse event reporting and postmarket surveillance and monitoring, as well as inventory control and other healthcare management functions.\n\nThe GMDN Agency updates the GMDN utilizing member change requests, to add a new generic device term or to change an existing Term Name or Definition. The decisions are made by an international expert team, according to ISO 15225. The GMDN Agency releases updates to the GMDN on a daily basis, on their interactive website, the GMDN Database. Only Members have access to the GMDN Database and membership is priced according to organization type and size. The GMDN is available in English and other languages.\n\nThe GMDN is used in the European Databank on Medical Devices (Eudamed) which has been established by the European Commission to strengthen market surveillance and vigilance.\n\nThe GMDN has been identified as part of the 'minimum data set' for the proposed US FDA Unique Device Identification regulation for the registration of new Medical Devices intended for use in the United States. This follows the international consensus established by the International Medical Device Regulatory Forum (IMDRF).\n\nThe GMDN Agency has recently finalised the business principles that will form the basis of a long term cooperation with the IHTSDO. The Cooperation Agreement shall result in the use of the GMDN as the medical device component of SNOMED CT. This Agreement is consistent with the aims of both organisations to minimise duplication and to support harmonisation. The following objectives were agreed:\n\n\nThe Agreement will benefit patients across the world and all users of SNOMED CT and the GMDN in promoting comprehensive terminology based medical records, covering the needs of regulators, the medical device industry and healthcare professionals. The arrangement will enhance the application of care to individual patients for medical device, patient risk and safety use cases.\n\n\n"}
{"id": "723000", "url": "https://en.wikipedia.org/wiki?curid=723000", "title": "Gold standard (test)", "text": "Gold standard (test)\n\nIn medicine and statistics, gold standard test is usually diagnostic test or benchmark that is the best available under reasonable conditions. Other times, gold standard is the most accurate test possible without restrictions.\n\nBoth meanings are different because for example, in medicine, dealing with conditions that would require an autopsy to have a perfect diagnosis, the gold standard test would be the best one that keeps the patient alive instead of the autopsy.\n\n\"Gold standard\" can refer to the criteria by which scientific evidence is evaluated. For example, in resuscitation research, the \"gold standard\" test of a medication or procedure is whether or not it leads to an increase in the number of neurologically intact survivors that walk out of the hospital. Other types of medical research might regard a significant decrease in 30-day mortality as the gold standard.\n\nThe AMA Style Guide prefers the phrase \"Criterion Standard\" instead of \"gold standard\", and many medical journals now mandate this usage in their instructions for contributors. For instance, \"Archives of biological Medicine and Rehabilitation\" specifies this usage. When the criterion is a whole clinical testing procedure it is usually referred to as clinical case definition.\n\nA hypothetical ideal \"gold standard\" test has a sensitivity of 100% with respect to the presence of the disease (it identifies all individuals with a well defined disease process; it does not have any false-negative results) and a specificity of 100% (it does not falsely identify someone with a condition that does not have the condition; it does not have any false-positive results). In practice, there are sometimes no true \"gold standard\" tests.\n\nAs new diagnostic methods become available, the \"gold standard\" test may change over time. For instance, for the diagnosis of aortic dissection, the \"gold standard\" test used to be the aortogram, which had a sensitivity as low as 83% and a specificity as low as 87%. Since the advancements of magnetic resonance imaging, the magnetic resonance angiogram (MRA) has become the new \"gold standard\" test for aortic dissection, with a sensitivity of 95% and a specificity of 92%. Before widespread acceptance of any new test, the former test retains its status as the \"gold standard\".\n\nBecause tests can be incorrect (yielding a false-negative or a false-positive), results should be interpreted in the context of the history, physical findings, and other test results in the individual being tested. It is within this context that the sensitivity and specificity of the \"gold standard\" test is determined.\n\nWhen the gold standard is not a perfect one, its sensitivity and specificity must be calibrated against more accurate tests or against the definition of the condition. This calibration is especially important when a perfect test is available only by autopsy.\nIt is important to emphasize that a test has to meet some interobserver agreement, to avoid some bias induced by the study itself.\n\nCalibration errors can lead to misdiagnosis.\n\nSometimes \"gold standard test\" refers to the best performing test available. In these cases, there is no other criterion against which it can be compared and it is equivalent to a definition. When referring to this meaning, gold standard tests are normally not performed at all. This is because the gold standard test may be difficult to perform or may be impossible to perform on a living person (i.e. the test is performed as part of an autopsy or may take too long for the results of the test to be available to be clinically useful).\n\nOther times, \"gold standard\" does not refer to the best performing test available, but the best available under reasonable conditions. For example, in this sense, a MRI is the gold standard for brain tumour diagnosis, though it is not as good as a biopsy. In this case the sensitivity and specificity of the gold standard are not 100% and it is said to be an \"imperfect gold standard\" or \"alloyed gold standard\".\n\nThe term ground truth refers to the underlying absolute state of information; the gold standard strives to represent the ground truth as closely as possible. While the gold standard is a best effort to obtain the truth, ground truth is typically collected by direct observations. In machine learning and information retrieval, .\n\n"}
{"id": "546810", "url": "https://en.wikipedia.org/wiki?curid=546810", "title": "Health measures during the construction of the Panama Canal", "text": "Health measures during the construction of the Panama Canal\n\nOne of the greatest challenges facing the builders of the Panama Canal was dealing with the tropical diseases rife in the area. The health measures taken during the construction contributed greatly to the success of the canal's construction. These included general health care, the provision of an extensive health infrastructure, and a major program to eradicate disease-carrying mosquitoes from the area.\n\nBy the time the United States took control of the Panama Canal project on May 4, 1904, the Isthmus of Panama was notorious for tropical diseases. An estimated 12,000 workers had died during the construction of the Panama Railway and over 22,000 during the French effort to build a canal. Many of these deaths were due to disease, particularly yellow fever and malaria. At several times, construction on the Panama Railway had actually halted due to the lack of healthy workers.\n\nThe high rate of deaths among workers on the Panama Canal due to disease was the source of a great deal of controversy in the United States. Newspapers, such as \"The Independent\", frequently reported on the poor conditions workers in the Canal Zone experienced, including the rampant disease. Poultney Bigelow wrote an article in \"The Independent\" in 1906 critiquing the work on the Panama Canal, which was highly influential with the American public. Among other topics, Bigelow brought attention to the poor living conditions of the workers, including pools of standing water where mosquitoes could breed and spread disease from.\n\nIt was clear to organizers of the American effort that previous disease control efforts had been largely ineffective, as the causes of the two main diseases were unknown, but in 1897 it was proved by Britain's Ronald Ross in India that malaria was spread by mosquitoes.\n\nThe Canal Commission appointed Colonel William Crawford Gorgas in March 1904 as head of hospitals and sanitation. Under his leadership, many new departments of sanitation were founded, covering different aspects of the sanitation problem. Commissions were also formed to look after the basic welfare of laborers.\n\nThe sanitation work included clearing land and establishing quarantine facilities. The most ambitious part of the sanitation program, though, was undoubtedly the effort to eradicate the mosquitoes \"Aedes aegypti\" and \"Anopheles\", the carriers of yellow fever and malaria, respectively, from the canal zone. There was initially considerable resistance to this program, as the \"mosquito theory\" was still considered controversial and unproven. However, with the support of chief engineer John Frank Stevens, who took over the post on July 26, 1905, Gorgas was finally able to put his ideas into action.\n\nGorgas divided Panama into 11 districts, and Colón, Panama, into four. In each district, inspectors searched houses and buildings for mosquito larvae. If larvae were found, carpenters were dispatched to the building, and work was done to eliminate objects or places where stagnant water could collect.\n\nMosquitoes lay their eggs on the surface of standing water, and when the larvae hatch, they live just below the surface, breathing through a siphon in their tails. Therefore, by eliminating standing water where possible and by spreading oil on the surface of any remaining pools, the larvae could be destroyed.\n\nGorgas also had domestic water systems installed in urban areas around the Canal Zone. These systems eliminated the need for rainwater collection, which had been collected in barrels and was a place for mosquitoes to breed. The United States government also provided $20 million to give workers free medical care and burial services. Gorgas's sanitation department also provided about one ton of prophylactic quinine each year to people in the Canal Zone to combat malaria.\n\nGorgas organized a major program to drain and fill swamps and wetlands around the Canal Zone. Many miles of ditches were dug, and grass and brush were cut back over wide areas. Oiling was used in a variety of means: workers with spray tanks were sent to spray oil on standing pools, and smaller streams were tackled by placing a dripping oil can over the waterway, which created a film of oil over each still patch of water in the stream. About 700,000 gallons of oil and 124,000 gallons of larvicide were used on the project. Gorgas also took another step in his efforts to eradicate mosquitoes in Panama: fumigation. He fumigated the residences of Panamanians who had been confirmed to have contracted yellow fever. \"Pans of sulfur or pyrethrum were then placed in the rooms, the right quantity of powder was weighed out (two pounds per thousand cubic feet), and the pans were sprinkled with wood-alcohol and set alight\" (Cameron 132). When the effectiveness of this procedure was realized, fumigation was extended to all of Panama. Within a year of Stevens's appointment, every building in Panama had been fumigated, using up the entire US supply of sulfur and pyrethrum. In 1906, only one case of yellow fever was reported, and until the end of the Panama Canal's construction, there were zero.\n\nGorgas's final means of attack on disease was to quarantine individuals infected with yellow fever or malaria from the rest of the workforce. Those who were diagnosed with either disease were put into \"Portable Fever Cages\", easily transportable screened structures used to prevent mosquitoes from biting an infected person and carrying the disease to others. Gorgas also had the thousands of canal workers sleep in screened verandas, as the mosquitoes that spread malaria are nocturnal and would infect the most people at night.\n\nThe first two and a half years of the American canal effort were substantially dedicated to preparation, much of it making the area fit for large-scale human habitation. A significant part of this was the sanitation program put in place by Gorgas. Nearly $20 million was spent on health and sanitation during the ten years.\n\nIn the end, these efforts were a success: by 1906, yellow fever was virtually wiped out in the Canal Zone, and the number of deaths caused by the other top disease, malaria, was also reduced significantly. The hospitals maintained were by far the best to be found anywhere in the tropics; some 32,000 patients were treated per year.\n\nWhile disease reduction dramatically improved the health of white workers, black workers—the majority of the canal workforce—continued to die in large numbers, at ten times the rate of white workers in 1906. While medical care was provided to all, housing was not provided to black workers, many of whom had to live in tents and tenements outside the mosquito-controlled zone. In the end, 350 white workers had died compared to 4,500 black workers. While the loss was tragic, it was far less than during the French era.\n\nToday, the Panama canal area is regarded as free of yellow fever and malaria.\n\n"}
{"id": "10134014", "url": "https://en.wikipedia.org/wiki?curid=10134014", "title": "Host factor", "text": "Host factor\n\nHost factor is a medical term referring to the traits of an individual person or animal that affect susceptibility to disease, especially in comparison to other individuals. The term arose in the context of infectious disease research, in contrast to \"organism factors\", such as the virulence and infectivity of a microbe. Host factors that may vary in a population and affect disease susceptibility can be innate or acquired.\n\nSome examples:\nThe term is now used in oncology and many other medical contexts related to individual differences of disease vulnerability.\n\n"}
{"id": "9015626", "url": "https://en.wikipedia.org/wiki?curid=9015626", "title": "Ian Kennedy (legal scholar)", "text": "Ian Kennedy (legal scholar)\n\nSir Ian McColl Kennedy, QC (born 14 September 1941) is a British academic lawyer who has specialised in the law and ethics of health. He was appointed to chair the Independent Parliamentary Standards Authority in 2009.\n\n1952–1959: He attended King Edward VI College, Stourbridge, Worcestershire where he spent four years in the sixth form.\n\n1960–1963: He attended University College, London, graduated 1st class Hons. LLB.\n\n1963–1965: Fulbright Fellow at University of California, Berkeley (LLM).\n\n1965–1971: Sub-Dean, Tutor and Lecturer in Law at University College, London (LLD).\n\n1966–1967: Ford Foundation fellow of Yale University and Mexico University.\n\nIn the summer of 1969 Kennedy visited Cuba to study the administration of justice, supported by a Hayter Fellowship awarded by the Institute of Latin American Studies, University of London (by Prof. R.A. Humphreys). In December/January 1970 – 1971 he returned to Cuba to gain information for his paper titled “Cuba's Ley Contra La Vagancia - The Law on Loafing”.\n\nKennedy was Dean of the Law School at King's College London, from 1986 to 1996. Kennedy is Emeritus Professor of Health Law, Ethics and Policy at University College London. He is an honorary Bencher of the Inner Temple.\n\nKennedy was the BBC's Reith lecturer (on the subject of \"Unmasking Medicine\") in 1980 and hosted many editions of \"After Dark\" on Channel 4.\n\nKennedy has been a member of numerous committees and inquiries.\n\nFor nine years, he was a member of the General Medical Council. In 1978, he founded the Centre of Medical Laws and Ethics, of which he later became president. He also served as member of the Medicines Commission, and the Department of Health advisory group on AIDS. He is a member of the board of the UK Research Integrity Office.\n\nIn 1997, he took part in a UK Government inquiry that gave cautious approval to xenotransplantation (the use of animal-to-human transplants), and in 1998, was a member of the committee that recommended pet passports.\n\nKennedy was a member of the Nuffield Council on Bioethics 1991-2002 and Chair during 1998-2002.\n\nKennedy is a trustee of homeless health charity Pathway.\n\nHe chaired the public inquiry into children's heart surgery at the Bristol Royal Infirmary (1998–2001), which concluded that paediatric cardiac surgery services at Bristol were \"simply not up to the task\", because of shortages of key surgeons and nurses, and a lack of leadership, accountability, and teamwork. This resulted in his becoming chair of the Healthcare Commission, from its creation (in shadow form) in 2003, until it was merged with other regulatory bodies to form the Care Quality Commission in 2009. In October 2009, Kennedy became chair of the King's Fund’s inquiry into the quality of general practice in England, replacing Niall Dickson. He led an enquiry into the work of breast surgeon Ian Paterson in Birmingham in 2013.\n\nIn 2010, he was elected inaugural Vice-President of the \"College of Medicine,\" an organisation set up to bring together patients and clincians on an equal footing.\nSeveral commentators, writing in The Guardian and The British Medical Journal claim that this organisation is simply a re-branding of Prince Charles' alternative medicine lobbying group the \"Foundation for Integrated Health.\" This has been denied by the College of Medicine whose President is Graeme Catto, for seven years the President of the General Medical Council.\n\nKennedy was knighted in 2002 for services to medical law and bioethics. He is an Honorary Fellow of:\n\n\nHe was also awarded an Honorary DSc degree by the University of Glasgow in 2003.\n\n"}
{"id": "42239555", "url": "https://en.wikipedia.org/wiki?curid=42239555", "title": "Internalizing disorder", "text": "Internalizing disorder\n\nAn internalizing disorder (or internalising disorder) is one type of emotional and behavioral disorder, along with externalizing disorders, and low incidence disorders. One who has an internalizing disorder will keep their problems to themselves, or internalize the problems.\n\nBehaviors that are apparent in those with internalizing disorders include depression, withdrawal, anxiety, and loneliness. There are also behavioral characteristics involved with internalizing disorders. Some behavioral abnormalities include poor self-esteem, suicidal behaviors, decreased academic progress, and social withdrawal. Internalizing one's problems, like sadness, can cause the problems to grow into larger burdens such as social withdrawal, suicidal behaviors or thoughts, and other unexplained physical symptoms.\n\nThe internalizing disorders, with high levels of negative affectivity, include depressive disorders, anxiety disorders, obsessive-compulsive and related disorders, trauma and stressor-related disorders, and dissociative disorders. Others like bulimia, and anorexia also come under this category.\n\nSome treatments for internalizing disorders include antidepressants, electroconvulsive therapy, and psychotherapy.\n\n\n"}
{"id": "17118006", "url": "https://en.wikipedia.org/wiki?curid=17118006", "title": "Irrigation in Bolivia", "text": "Irrigation in Bolivia\n\nBolivia’s government considers irrigated agriculture as a major contributor to \"better quality of life, rural and national development.\" After a period of social unrest caused by the privatization of water supply in Cochabamba and La Paz, the government of Evo Morales is undertaking a major institutional reform in the water resources management and particularly in the irrigation sector, aimed at: (i) including indigenous and rural communities in decision making, (ii) integrating technical and traditional knowledge on water resources management and irrigation, (iii) granting and registering water rights, (iv) increasing efficiency of irrigation infrastructure, (v) enhancing water quality, and (v) promoting necessary investment and financial sustainability in the sector. Bolivia is the first country in Latin America with a ministry dedicated exclusively to integrated water resources management: the Water Ministry.\n\nBolivia is one of the poorest countries in Latin America. In 2006 the annual income per capita reached to 1,153 dollars and almost 40% of the population lived in extreme poverty. In addition, Bolivia is one of the most unequal countries in the continent with a Gini coefficient of about 0.6 and 10% of the population obtaining over 40% of the total income and indigenous and rural populations in particular suffering the effects of social and economic marginalization. Real per capita income has barely changed over the past fifty years, while increasing in 350% in Brazil, 200% in Chile and 75% in Argentina. Poverty in rural areas stands at 83 percent, compared to 54 percent urban areas, and there is an even greater gap in terms of unsatisfied basic needs (91 percent versus 39 percent). Despite recent improvements in living conditions nationwide, benefits have continued to accrue disproportionately to urban areas.\nDuring the 2000-2004 period agriculture contributed an average of 14% to GDP and employed 40% of the population. However, in the rural area agriculture employs up to 80% of the population. In 2001, the agricultural sector generated US$432 million and 30% of national exports. According to the Irrigation Vice-Ministry, the agricultural sector in the eastern part of Bolivia generated US$2,160 million exporting soya, sunflower and sugarcane products. The agricultural sector of the western part of Bolivia is mostly focused on subsistence agriculture and local markets.\n\nIn addition, frequent government changes over the last five years and social tensions have undermined progress in poverty reduction. The Government of President Morales—Bolivia’s first indigenous President—who came into power in January 2006, has prepared a Plan Nacional de Desarrollo: Bolivia Digna, Soberana, Productiva y Democrática para Vivir Bien (PND).\n\nIrrigation is a major component of the PND since it \"plays a fundamental role in increasing agricultural production and diversification, rural employment, and food security in Bolivia\" . Particularly, and according to the Water Ministry, irrigation contributes to rural development since it (i) decreases climatic risks providing water for ensure production; (ii) increases food security and supply to local and national markets; (iii) increases productivity generating export capacity and agricultural revenue; (iv) intensifies land use; (v) generates income and reduces migration; (vi) allows diversification of crops including high value cops; and (vii) generates productive investment.\n\nBolivia has approximately 226,500 irrigated hectares (ha) or about 11% of the total agricultural land 2,100,000 ha. There are about 5,000 irrigation systems in Bolivia, most of them located in the South and Southwestern areas (Valles and Altiplano). These irrigation systems consist of rudimentary web of canals supplied by rainfall with few regulatory schemes such as dams, which makes them very vulnerable to seasonality of rain. Overall efficiency of irrigation systems varies from 18 to 30% in traditional systems to 35-50% in improved systems.\n\nIrrigation systems by Department, size and area\n\n\"Source\": Ministerio del Agua\n\nIrrigation accounts for 94% of water withdrawals or about 2,000 million cubic meters annually. Bolivia can be divided into three areas, which correspond to the eastern area (a tropical and subtropical region), the western area (the arid, semi arid and sub-humid dry region), and the Titicaca basin.\nThe hydrographic system consists of three large basins: the Amazon Basin which measures approximately 724,000 km and covers 66% of Bolivia’s territory; the closed (endorheic) basin, which measures 145,081 km or 13% of the territory; and the Rio Plata Basin, which covers 229,500 km or 21% of the nation’s territory.\nThe Amazon basin has a high flow of water and it is prone to floods. The quantity and quality of hydrological information is very poor.\n\nThe main impacts of irrigated agriculture in Bolivia are soil erosion and pollution due to agricultural runoff. Nearly 41% of Bolivia’s national territory has lost its production capacity due to soil erosion. For example, in western regions of Oruro, Potosí and Tarija, close to 45,000 square kilometers have low soil productivity on account of erosion. The highland minifundios accelerate soil degradation processes. In the northern highlands, the production area of family agricultural production units is three to five hectares. Excess grazing and other agricultural activities have contributed to salinization and soil compression.\n\nAgricultural runoff is one of the main contributors to water pollution in Bolivia, together with domestic municipal wastewater and dumping by industries and mines. The greatest percentage of the pollution load is due to diffuse dumping from agricultural and fishing activities and runoffs of urban areas. There are no regulations or controls over major dumping from non-specific sources, despite its volume and toxicity.\n\nThe Spanish colonists, soon after their arrival in the central Andes in the 16th century, appropriated the best farmlands on the coasts and valleys and pushed the indigenous population to the more inhospitable highlands. The highlands had been grazing grounds for llamas and alpacas, but had not been used for agriculture because of their low productivity and high climatic risks. Under the new circumstances, the highlands became the center of Bolivia’s subsistence agriculture.\n\nTraditionally Bolivia has been dependent on the mining sector as a source of fiscal revenue and foreign exchange, and directed few resources towards the agricultural sector. Following the 1985 economic reform, the government of Paz Estenssoro aimed at moving towards a distortion-free market economy that would attract private investment to the agricultural sector. The World Bank considers that some of the reforms undertaken at that time were ill-informed, especially “the lack of constructive governmental intervention to provide needed public goods such as land titling, agricultural research and extension, and irrigation infrastructure.” As a result, the agricultural sector has lacked the underpinnings in both human and physical capital that facilitate development. The absence of new production and irrigation technologies left farmers with limited opportunities to raise their productivity and income discouraging investment.\nIn 1999 the total area equipped for irrigation added up to 128,240ha. The irrigated area has almost doubled since. More than 50% of the irrigated area is concentrated in the provinces of Cochabamba and La Paz in the center of the country.\n\nDuring the 1990s, water management was characterized by a sectoral approach with multitude of actors and legal approaches and overlapping of responsibilities. The Ministry of Rural Affairs and Indigenous People, the National Water Authority at the time, together with the Inter-Institutional Committee, the National Office for Irrigation and Drainage, and the National Secretary for Rural Development contributed to the mosaic of institutions in charge of water resources management for irrigation at the national level.\nIn 1998, the government approved a Resolution establishing the Water Intendancy as the authority for granting water user rights. In 1999, two major concessions for water and sanitation were granted to the private sector in La Paz and Cochabamba. The increase of water tariffs and the consequent limitation of access to water were followed with social upraising in 2000. After what is known as the \"Water War,\" water user associations, national and municipal government, NGOs and international research organizations engaged on intense negotiations aimed at redefining public water policies. This process is known by the Project for Water Rights (Proyecto Derechos de Agua – PDA). Irrigation organizations worked with PDA in creating a national irrigation strategy combining both traditional irrigation practices with technical and scientific knowledge. This participatory process informed the 2004 Irrigation Law No.2878 among others.\n\nEvo Morales administration is currently reforming the water institutional framework attributing competences to the newly created Environment and Water Resources Ministry as well as to municipalities and departments accordingly to the Decentralization Law No. 1654, and water users associations. The National Irrigation Plan has included as a challenge the still overlapping responsibilities of different institutions at the national and local level. (See institutional framework below)\n\nThe 2004 Irrigation Law No. 2878 aims at managing irrigation water resources through a decentralized institutional framework as well as securing water user rights through registration. The Irrigation Law 2878 also transfers operation and maintenance of irrigation infrastructure to local farmers and establish participatory mechanisms to promote investment on irrigation systems. The previous law dated from 1906 and was considered obsolete. through\n\nThe Irrigation Law specifically prohibits the transferring of water rights, hence the creation of water markets, and gives priority to collective users rights over individual users.\nUsers are granted water rights through registries or authorizations. Registries are granted to the indigenous and local families or communities and are aimed at securing water access for domestic or traditional agriculture use respectively. Authorizations are granted to other farmer organizations for agricultural or agro-forestry use for a maximum of 40 years.\n\nThe Irrigation Law recognizes the Water Ministry, previous Ministry for Agriculture and Campesino Issues, as the national water authority and created the National Irrigation Service (Servicio Nacional de Riego – SENARI) and the Local Irrigation Service (Servicio Departamental de Riego – SEDERI).\n\nThe Environment and Water Resources Ministry, created in 2009, is responsible for: (i) planning, implementing, monitoring, evaluating, and funding irrigation plans and policies in close collaboration with SENARI;(ii) managing national and international funds aimed at irrigation development;(iii) promote technical assistance, capacity building and applied research and development in irrigation; and (iv) promote participative decentralization in irrigation development at the departmental, prefectural, municipal, local and river basin level as established in Law 2878. The Ministry for Rural Development, Agriculture and Environment share the same responsibilities than the Water Ministry.\n\nThe Vice-Ministry for Irrigation aims at: (i) guaranteeing sustainable water use for irrigation through a comprehensive system for granting water rights and permits (ii) promoting national and local investments, and (iii) strengthening institutional capacity through technical and financial support.\n\nThe Ministry of Development Planning, together with the Water Ministry and the Ministry for Rural Development, Agriculture and Environment aims at: (i) planning and supervising water management at the river basin level, (ii) designing and implementation of environmental standards for irrigation works, (iii) monitoring water quality and mitigating water pollution.\n\nSENARI, under the Water Ministry, is responsible for planning and implementing water policies, granting water rights, conflict resolution, and coordinate with other water stakeholders and well as supervise SEDERI. SEDERIs, branches of SENARI at the level of departments, are responsible for proposing departmental irrigation strategies, supervising the Departmental Irrigation Service, promoting capacity building among water users, and updating the irrigation registry at the department level. There are currently seven SEDERIs in the departments of Chuquisaca, Cochabamba, La Paz, Oruro, Potosi, Santa Cruz and Tarija.\n\nThere are multiple irrigation and water users associations at the local, regional, river basing and departmental level, comprising associations, cooperatives, committees and communities more or less formal. The government through the new Water Law is aiming at promoting registration of informal associations in the process of receiving water user rights. Irrigation associations are organized at the national level through the National Association of Irrigators and Local Water System, and at the Departmental level through Departmental Irrigation Units (Unidades Departamentales de Riego - UDR) and Departmental Irrigators Associations (Asociaciones Departamentales de Regantes - ADR).\n\nEvo Morales’ administration is aiming at transforming the irrigation sector focusing on participatory decision-making and integrated water resources management at the river basin level with a strong emphasis on public investment.\n\nIn July 2007, Bolivia’s government approved a new National Irrigation Plan, called the \"new PRONAR\".\nto be implemented from 2007 to 2030. The preceding National Irrigation program, also called PRONAR, was approved in 1996 and was implemented until 2005. The new PRONAR consist of five major components: (i)support agricultural and forestry production, (ii) support water resources management, (iii) strengthen institutional framework capacities, (iv)increase investment on irrigation and drainage infrastructure, and (v)integral technical assistance, capacity building and research. PRONAR aims at building irrigation infrastructure on 275,000 ha, benefiting 200,000 farmers, with a total investment of US$1.2 billion by 2030.\n\nThere are a number of legal and commercial issues that will affect how these projects move forward and are structured. Whilst some of the legal issues are not confined to irrigation PPPs they can take on a new dimension and complexity when applied to irrigation: Land ownership; water extraction; public sector counterpart. These will be key issues in a PPP as the private provider will want to ensure a steady revenue stream.\n\nThere are also the usual legal considerations that need to be checked when developing PPPs in any sector, such as legal restrictions on the type of PPP arrangement that can be entered into, relevant procurement rules for entering into PPPs, existence of restrictions on foreign investment, taxation and potential for tax holidays and the ability to assign rights such as security and step in rights to lenders.\n\nFarmers contribute to operational and maintenance costs of irrigation infrastructure both in cash and kind. The Water Ministry estimates that water users contribute with cash to cover maintenance costs in 45% irrigation systems. Irrigation systems such as Guadalupe and Pampa Redonda in Santa Cruz and Chiara in Cochabamba, a total of 10% of all irrigation systems, received any payment for operation costs.\nFor example, in Cochabamba users pay approximately US$4.1 application fee and US$9.6 registration fee. Users pay O&M costs through a day of work or a US$2.7 fine per day of work.\n\nIn irrigation, there is trend of increasing investment in irrigation, from 132.7 UDS millions\nin 2001 invested in rural development (including irrigation) to 168.3 USD million in 2002.\nA large part of these investments were made through Municipal governments building\nsystems and transferring them to the communities, though in most of the cases there is no\nclarity about who actually finally owns those systems.\n\nRegarding particular irrigation projects, PRONAR implemented 158 projects from 1996 to 2005, in seven out of the nine departments of Bolivia, investing US$20 million in close collaboration with the Inter American Development Bank, irrigation associations, and municipal governments.\n\nIrrigation Investment by department and source\n\n\"Source\": Ministerio de Asuntos Campesinos y Agropecuarios (2005)\n\nAlthough specific impacts of Climate Change on irrigation in Bolivia are still unknown, phenomena such as a high intensity El Nino in the form of floods, droughts, frost and hail are generally expected to affect Bolivia. Natural disasters directly affect the country’s development, because it hurts its economic results, weaken its social well-being, cause capital losses, and damage the roads and energy and irrigation infrastructure. Such losses, in turn, influence economic indicators such as inflation and production, which in turn increase poverty\n\nFloods and landslides in the rainy season affect a wide range of infrastructure. Landslides in 1997 and 1998 in the communities of Cotahuma, Mokotor, and the Kunii area, in the department of La Paz, caused 24 deaths and destroyed 264 homes. An unprecedented hail storm in 2002 also in La Paz caused 70 deaths and damage was estimated at more than US$70 million. Droughts often recur, their area of incidence is quite large, and they are a major cause of migration from the countryside to the cities.\n\nIn addition, increase of temperatures in the Andes and the melting of glaciers may increase seasonal runoff in the short term and increase agricultural dependency on annual rainfall in the medium and long term. For example, Bolivia’s Chacaltaya glacier, situated 20 km NE of the city of La Paz, has lost 82% of its surface area since 1982 and may completely melt by 2013. (See Impacts of Glacier Retreat in the Andes:Documentary)\n\nThe World Bank is currently undertaken a US 78.1 million Second Participatory Rural Investment Project with the objective of piloting the consolidation of institutional arrangements between the national, prefecture and municipal governments and civil society for sustainable management of sub-national public investment in irrigated agriculture, forestry and fishing with an emphasis on territorial development. The World Bank is also supporting with US$12.5 million the implementation of the National Plan for Sustainable Rehabilitation and Reconstruction (PRRES), thought the Bolivia Emergency Recovery and Disaster Management Project aimed at strengthening the national system for risk management and rehabilitation, reconstruction, and small mitigation works. These works will be financed in specific areas determined to have been particularly affected by El Nino in the past.\n\nThe Inter American Development Bank is currently financing a US$270,000 \"Evaluation and Design of Irrigation Project\" to evaluating the irrigation systems in operation. The IDB together with the GTZ provided technical and financial assistance to Bolivia’s government in the implementation of a National Irrigation Plan, PRONAR that finalized in 2005. This evaluation is the groundwork for further collaboration among IDB and the Bolivia’s government.\n\nThe Water Ministry identified a number of lessons learned from the evaluation of PRONAR from 1996 to 2005. Some of the key aspects are:\n\n\n"}
{"id": "22175710", "url": "https://en.wikipedia.org/wiki?curid=22175710", "title": "James Walker Dawson", "text": "James Walker Dawson\n\nJames Walker Dawson (1870, India - 26 June 1927, Edinburgh) was a Scottish pathologist remembered for his work on multiple sclerosis including the description of the eponymous Dawson's fingers.\n\nJames Dawson started his medical training at Edinburgh University in 1888, but had to interrupt his studies due to tuberculosis. He spent 13 years overseas, mainly in India, the United States, Canada and New Zealand, and worked as a lumberjack and sheep farmer during that time. In 1903 he resumed his training, and graduated M.B, C.M. the next year. He started research into disorders of the nervous system at the Royal College of Physicians of Edinburgh under Dr. Alexander Bruce. In 1910, he was awarded the Syme Fellowship in surgery, and the following year he was awarded his M.D. with a gold medal for his thesis \"Studies on Inflammation\". He was unable to serve in World War I due to ill health, so he taught pathology at the University of Edinburgh.\n\nBruce died unexpectedly, and Dawson continued the research on his own. In 1916 he published a landmark paper on the histology of \"disseminated sclerosis\", describing the distribution and stages of lesions, reviewing theories on the aetiology and describing the inflammatory process seen in the disease. This work formed the basis for his D.Sc. thesis in the same year. In his book \"The Founders of Neurology\" which was published in 1953, W. Haymaker stated that little had been added to this work in the intervening 40 years.\n\nHe declined a number of appointments, again due to ill health, and continued working as a histologist in the laboratory at the Royal College of Physicians in Edinburgh. He produced publications including work on multiple neuromata of the central nervous system, generalised osteitis fibrosa, melanomata and syringomyelia. He became a Fellow of the Royal College of Physicians in 1924, and also published an address on \"The Spirit of Leisure and the Spirit of Work\" which was presented to Edinburgh medical students for many years. He was preparing three Morison Lectures for presentation to the Royal College of Physicians at the time of his death in 1927.\n\nJames Dawson was married to Edith Kate Dawson, who was herself a pathologist of international repute.\n"}
{"id": "33789483", "url": "https://en.wikipedia.org/wiki?curid=33789483", "title": "Jon Magnussen", "text": "Jon Magnussen\n\nJon Magnussen (born 12 August 1959) is a Norwegian Professor in health economics and Head of Department of Public Health and General Practice at Norwegian University of Science and Technology (NTNU) in Trondheim. Organization and financing of health care services and productivity and efficiency in the health care sector is some of his research topics.\n\nKittelsen S A C, Halsteinli V, Magnussen J: Productivity growth in Norwegian psychiatric outpatient clinics for children and youths. A panel data analysis of the period 1996-2001. J. Mental Health Policy Econ, 2005 8:183-91\n\n"}
{"id": "25281508", "url": "https://en.wikipedia.org/wiki?curid=25281508", "title": "Lake Doxa", "text": "Lake Doxa\n\nLake Doxa ( \"Limni Doxa\") is an artificial lake in western Corinthia, Greece. It is situated at an elevation of 900 m, in the municipal unit Feneos, near the village Archaia Feneos. Construction was completed in the late 1990s. It is fed and drained by the small river \"Doxa\" (\"Δόξα\"), which empties into the plain of Feneos. In the heart of the lake on a small peninsula features a small church of Agios Fanourios. The Saint George Monastery in Feneos was relocated to higher ground, north of the lake. The lake is surrounded by pine forests.\n\n"}
{"id": "29841360", "url": "https://en.wikipedia.org/wiki?curid=29841360", "title": "Lock hospital", "text": "Lock hospital\n\nA lock hospital was an establishment that specialized in treating sexually transmitted diseases. They operated in Britain and its colonies and territories from the 18th century to the 20th. The military had a close association with a number of the hospitals. By the mid-19th century most of the larger army bases in India were home to a lock hospital. There were more military than civil lock hospitals in India, due to the prevalence of venereal diseases amongst British troops. In 1858 the Admiralty paid to have one opened in Portsmouth and in 1863 another in Plymouth.\n\nThe earliest lock hospitals in India were established around 1797 at Berhampur, Kanpur, Danapur, and Fatehgarh. They were usually within bazaars, surrounded by a mud wall and manned by a doctor and a woman assistant. The local police were in charge of rounding-up women suspected of being diseased, who could return home only after obtaining a certificate of discharge.\n\nThe term \"lock hospital\" originates from their use as leprosariums, in which the patients were kept in restraints.\n\n\n"}
{"id": "5159156", "url": "https://en.wikipedia.org/wiki?curid=5159156", "title": "Medical Practitioners Disciplinary Tribunal", "text": "Medical Practitioners Disciplinary Tribunal\n\nThe Medical Practitioners Disciplinary Tribunal of New Zealand hears and determines disciplinary proceedings brought against medical practitioners under Part VIII of the Medical Practitioners Act of 1995.\n\nThe Tribunal comprises a Chair, a senior Deputy Chair, a Deputy Chair and a panel of not fewer than 12 members, all appointed by the Minister of Health. When the Tribunal sits to hear and determine any matter it sits with a Chair or Deputy Chair and four members, three of whom are medical practitioners, and one person who is not a medical practitioner.\n\nThe Medical Practitioners Disciplinary Tribunal is in the process of being superseded by the Health Practitioners Disciplinary Tribunal of New Zealand, established under the Health Practitioners Competence Assurance Act 2003 which came into effect on 18 September 2004.\n\n"}
{"id": "2674316", "url": "https://en.wikipedia.org/wiki?curid=2674316", "title": "Money-rich, time-poor", "text": "Money-rich, time-poor\n\nMoney-rich, time-poor is an expression which arose in Britain at the end of the 20th century to describe groups of people who, whilst having a high disposable income through well-paid employment, have relatively little leisure time as a result. Time poverty has also been coined as a noun for the phenomenon.\n\nMany people accept time poverty as a necessary condition of employment; others have sought to solve the problem through downshifting or through adoption of flexible working arrangements. The problem affects both salaried workers who work long hours even though they might be well compensated as well as hourly low-wage workers who work long hours to earn more money.\n\n\n\n"}
{"id": "21522783", "url": "https://en.wikipedia.org/wiki?curid=21522783", "title": "Musica Humana Research", "text": "Musica Humana Research\n\nMusica Humana Research is a collaboration between art, music and medical research \nThe idea behind this project was the creation of a specially composed and designed music for treatment of hospitalized patients and was initiated by Professor M.D. Lars Heslet Copenhagen University Hospital and the composer and oboist Niels Eje during the nineties. \nIn 1998 the Egmont Foundation granted economic support to complete the project.\nThe Egmont Foundation grant (1998–2003) was primarily aimed at developing a special composed music programme targeted at improving the sound environment in intensive care and recovery wards at a number of hospitals in Denmark. \n\nThis specially designed music was to be created by composer Niels Eje and according to the grant foundation, implemented and scientifically validated by a group of researchers affiliated to the project. \nThe goal was to actively use specially designed music to improve patient wellbeing and satisfaction during hospitalisation, reduce anxiety and stress, and also improve the patients ability to recover.\n\nSince 2001 Musica Humana Research has carried out many activities, including publishing and drawing attention to the research results and how to improve the sound environment for patients in hospitals. The activities includes articles in medical journals, presentations at conferences and symposiums all over the world.\nMusica Humana Research has since 2003 become an international network of independent researchers, achieving positive and documented results with clinical trials in Denmark, Sweden, Norway and United States.\n\n"}
{"id": "20499441", "url": "https://en.wikipedia.org/wiki?curid=20499441", "title": "N-Nitroso-N-methylurea", "text": "N-Nitroso-N-methylurea\n\n\"N\"-Nitroso-\"N\"-methylurea (NMU) is a highly reliable carcinogen, mutagen, and teratogen. NMU is an alkylating agent, and exhibits its toxicity by transferring its methyl group to nucleobases in nucleic acids, which can lead to AT:GC transition mutations.\n\nNMU is the traditional precursor in the synthesis of diazomethane. However, because it is unstable at temperatures beyond 20 °C and somewhat shock-sensitive, it has become obsolete for this purpose and replaced by other N-nitroso compounds: (\"N\"-methyl)nitrosamides and nitrosamines. Most chemical supply houses have stopped carrying it.\n\nAcute exposure to NMU in humans can result in skin and eye irritation, headache, nausea, and vomiting. NMU is \"reasonably anticipated to be a human carcinogen\" based on sufficient evidence of carcinogenicity in experimental animals (IARC 1972, 1978, 1987). Various cancers induced in animal models include: squamous cell carcinomas of the forestomach, sarcomas and gliomas of the brain, adenocarcinomas of the pancreas, mammary carcinomas, leukemia, and lymphomas. However, the actual potential for human exposure is quite limited, as the chemical is not produced or used in large quantities \n\nNMU is teratogenic and embryotoxic, resulting in craniofacial (cleft palate) and skeletal defects, fetal growth retardation, and increased fetal resorption. Exposure to NMU during pre-implantation, post-implantation, organogenesis, or by paternal exposure can result in these effects.\n"}
{"id": "52344551", "url": "https://en.wikipedia.org/wiki?curid=52344551", "title": "New York Genome Center", "text": "New York Genome Center\n\nThe New York Genome Center (NYGC) is a 501(c)(3) nonprofit biomedical research organization in New York, New York. A collaboration of academic, medical and industry leaders in New York and other partners throughout the country, the New York Genome Center focuses on translating genomic research into clinical solutions for serious diseases. NYGC faculty hold joint academic appointments at its member institutions and lead clinically focused genomic studies in a number of disease areas, including pediatric and adult cancer, asthma, autism, Alzheimer’s disease, ALS and other serious neurodegenerative diseases. NYGC scientists bring a multidisciplinary and in-depth approach to the field of genomics, conducting research in a wide range of areas, including single cell genomics, gene engineering, population and evolutionary genomics, technology and methods development, statistics, computational biology and bioengineering. In 2017, co-founder Tom Maniatis was appointed Scientific Director and Chief Executive Officer of the New York Genome Center.\n\nThe Center was legally founded in November 2011 as a collaboration among eleven academic institutions to advance genome research, based on leadership from Tom Maniatis and financial support of $2.5 million from each institution as well as a number of private philanthropists, including then Mayor Michael Bloomberg, James Simons and Russell Carson. One year after its founding, NYGC recruited Robert B. Darnell as President and Scientific Director, and formally opened in a multi-story building at 101 Avenue of the Americas. on September 19–20, 2013.\n\nThe 12 founding institutions (Albert Einstein College of Medicine joined the original 11 institutions on April 2013) were:\n\nThe Center's funding sources include US and New York state governments, a contract with 10 medical institutes for central management of clinical data, and charitable foundations. This includes funds pledged by the Simons Foundation and the Carson Family Charitable Trust of up to $100 million from 2016 to 2019.\n\nGovernment funding has included a $55 million grant from New York State to support genomic medicine and a $40 million grant for establishing a Center for Common Disease Genomics from the National Human Genome Research Institute, the aim of which is to describe a comprehensive list of genes underlying common diseases. Additionally, the Center and Weill Cornell Medicine received a National Cancer Institute grant to support a joint cancer genomics data center for the research and clinical interpretation of tumors, a part of the ongoing development of The Cancer Genome Atlas; the grant provides $490,000 per year for five years. The Center was also awarded a $13.5 million contract to conduct whole genome sequencing and analysis for the National Heart, Lung, and Blood Institute's TOPMed program.\n\nIn 2017, New York State committed $17 million in capital improvements for the New York Genome Center to house JLABS@NYC, a life sciences incubator set to open in summer 2018.\n\n\n"}
{"id": "46262", "url": "https://en.wikipedia.org/wiki?curid=46262", "title": "Nightmare", "text": "Nightmare\n\nA nightmare, also called a bad dream, is an unpleasant dream that can cause a strong emotional response from the mind, typically fear but also despair, anxiety and great sadness. However, psychological nomenclature differentiates between nightmares and bad dreams, specifically, people remain asleep during bad dreams whereas nightmares awaken individuals. Further, the process of psychological homeostasis employs bad dreams to protect an individual's Homeostatically Protected Mood (HPMood) from the impact of elevated anxiety levels. During sleep, nightmares indicate the failure of the homeostatic system employing bad dreams to extinguish anxiety accumulated throughout the day. The dream may contain situations of discomfort, psychological or physical terror or panic. After a nightmare, a person will often awaken in a state of distress and may be unable to return to sleep for a short period of time.\n\nNightmares can have physical causes such as sleeping in an uncomfortable position or having a fever, or psychological causes such as stress or anxiety. Eating before going to sleep, which triggers an increase in the body's metabolism and brain activity, is a potential stimulus for nightmares.\n\nRecurrent nightmares may require medical help, as they can interfere with sleeping patterns and cause insomnia.\n\nScientific research shows that nightmares may have many causes.\nIn a study focusing on children, researchers were able to conclude that nightmares directly correlate with the stress in children's lives. Children who experienced the death of a family member or a close friend or know someone with a chronic illness have more frequent nightmares than those who are only faced with stress from school or stress from social aspects of daily life.\nA study researching the causes of nightmares focuses on patients who have sleep apnea. The study was conducted to determine whether or not nightmares may be caused by sleep apnea, or being unable to breathe. In the nineteenth century, authors believed that nightmares were caused by not having enough oxygen, therefore it was believed that those with sleep apnea had more frequent nightmares than those without. The hypothesis, however, was proven wrong and the results actually showed that healthy people have more nightmares than the sleep apnea patients.\n\nIn Stephen LaBerge's book entitled \"Exploring the World of Lucid Dreaming (1990)\" he outlines a possible reason for how dreams are formulated and why nightmares occur with a high frequency. A dream starts with an individual thought or scene, in his example he uses the scene of walking down a dimly lit street. Since dreams are not predetermined, your brain responds to the situation by either thinking a good thought or a bad thought, and the dream framework follows from there. Since the prominence of bad thoughts in dreams is higher than good, the dream will proceed to be a nightmare.\n\nThere is a popular view, featured in the story \"A Christmas Carol\", that eating cheese before sleep can cause nightmares, but there is little scientific evidence for this.\n\nA study involving a large group of undergraduate students analyzes the effects of nightmares on the quality of sleep. The study showed that the participants experienced abnormal sleep architecture and that the results of having a nightmare during the night were very similar to those of people who have insomnia. This means that, like insomniacs, people who have nightmares do not get as much rest as those who do not have chronic nightmares. Therefore, they experience a lesser quality of sleep than others. This is thought to be caused by frequent nocturnal awakenings and fear of falling asleep.\n\nSigmund Freud and Carl Jung seemed to have shared a belief that people frequently distressed by nightmares could be re-experiencing some stressful event from the past. Both perspectives on dreams suggest that therapy can provide relief from the dilemma of the nightmare experience.\n\nHalliday (1987), grouped treatment techniques into four classes. Direct nightmare interventions that combine compatible techniques from one or more of these classes may enhance overall treatment effectiveness:\n\n\nReccurring post-traumatic stress disorder nightmares in which real traumas are re-experienced respond well to a technique called imagery rehearsal. First described in the 1996 book \"Trauma and Dreams\" by Harvard psychologist Deirdre Barrett, this contemporary dream interpretation involves dreamers coming up with alternative, mastery outcomes to the nightmares, mentally rehearsing those outcomes awake, and then reminding themselves at bedtime that they wish these alternate outcomes should the nightmares reoccur. Research has found that this technique not only reduces the occurrence of nightmares and insomnia, but also improves other daytime PTSD symptoms. According to Bret Moore and Barry Kraków, the most common variations of Imagery Rehearsal Therapy (IRT) \"relate to the number of sessions, duration of treatment, and the degree to which exposure therapy is included in the protocol\". Another kind of treatment not only helping the reduction of nightmares, sleep disturbance, and other PTSD symptoms is prazosin. There have been multiple studies conducted under placebo-controlled conditions. (Am J Psychiatry 2003; 160:371–373)\nA comprehensive model has been put forth by Krakow and Zadra (2006) that includes four group treatment sessions, ~2.25 to 2.5 hr in length. The first two sessions focus on how nightmares are closely connected to insomnia and how they become an independent symptom or disorder that warrants individually tailored and targeted intervention. The last two sessions focus on the imagery system and how IRT can reshape and eliminate nightmares through a relatively straightforward process akin to cognitive restructuring via the human imagery system. First, the patient is asked to select a nightmare, but for learning purposes the choice would not typically be one that causes a marked degree of distress. Second, and most commonly, guidance is not provided on how to change the disturbing content of the dream; the specific instruction developed by Joseph Neidhardt is \"change the nightmare anyway you wish\" (Neidhardt et al., 1992). In turn, this step creates a \"new\" or \"different\" dream, which may or may not be free of distressing elements. Our instructions, unequivocally, do not make a suggestion to the patient to make the dream less distressing or more positive or to do anything other than \"change the nightmare anyway you wish.\" Last, the patient is instructed to rehearse the \"new dream\" through imagery and to ignore the old nightmare.p. 234\n\nSome people may experience recurring nightmares due to posttraumatic stress disorder (PTSD), or they may have some other source of anxiety that influences their dreams at night. Whatever the cause, there are treatments available, some of them medical and some psychological. While most treatments are meant for people who have a true disorder, the techniques discussed above will work well for any person dealing with nightmares.\n\nFearfulness in waking life is correlated with nightmares. Studies of dreams have estimated that about 75% of the time, the emotions evoked by dreams are negative. However, it is worth noting that people are more likely to remember unpleasant dreams.\n\nOne definition of \"nightmare\" is a dream which causes one to wake up in the middle of the sleep cycle and experience a negative emotion, such as fear. This type of event occurs on average once per month. They are not common in children under five, but they are more common in young children (25% experiencing a nightmare at least once per week), most common in teenagers, and common in adults (dropping in frequency about one third from age 25 to 55).\n\nThe word \"nightmare\" is derived from the Old English \"mare\", a mythological demon or goblin who torments others with frightening dreams. Subsequently, the prefix \"night-\" was added to stress the dream aspect. The word \"nightmare\" is cognate with the older Dutch term \"nachtmerrie\" and German \"Nachtmahr\" (dated).\n\n\n\n"}
{"id": "39314166", "url": "https://en.wikipedia.org/wiki?curid=39314166", "title": "Park Seo-yang", "text": "Park Seo-yang\n\nPark Seo-yang (korean:박서양 朴瑞陽, September 30, 1885 – December 15, 1940) was a Korean early modern surgeon, doctor, chemist, and independence activist from a slave family. His real name was Bongchul (봉출) or Bongchuri (봉주리).\n\n\n"}
{"id": "20416105", "url": "https://en.wikipedia.org/wiki?curid=20416105", "title": "Pointing and calling", "text": "Pointing and calling\n\nPointing and calling is a method in occupational safety for avoiding mistakes by pointing at important indicators and calling out the status. It is common in Japan and railways of Taiwan, it is also sometimes referred to by its Japanese terms, \"shisa kanko\" (), \"shisa kakunin kanko\" () or \"yubisashi koshō\" (). Making large gestures and speaking out the status helps keeping focus and attention. The method was first used by train drivers and is now commonly used in Japanese industry. It is not common in other countries, though it is used in the New York City Subway system. It is recommended by the Japan Industrial Safety and Health Association (JISHA, ).\n\nPointing and calling requires co-action and co-reaction among the operator's brain, eyes, hands, mouth, and ears.\n\nThe method originated in Japan in the early 1900s, with train drivers calling out the status of signals. The pointing was added a few decades later.\n\nA 1994 study by the Railway Technical Research Institute showed that pointing and calling reduced mistakes by almost 85 percent when doing a simple task.\n\n\n"}
{"id": "25762746", "url": "https://en.wikipedia.org/wiki?curid=25762746", "title": "R v Woodrow", "text": "R v Woodrow\n\nRegina v. Woodrow, 15 M. & W. 404 (Exch. 1846) was a case decided by the English Court of Exchequer Chamber that first adopted a strict liability standard for the criminal offense of selling impure foods.\n\nThe defendant was charged with violating a statute that prohibited possession of adulterated tobacco. The court held the defendant criminally liable even though he had no knowledge or reason to suspect the adulteration. The court justified this adoption of strict liability as being in the interests of convenient prosecution.\n\nThis decision overruled \"Rex v. Dixon\", which had included a mens rea requirement.\n"}
{"id": "23140635", "url": "https://en.wikipedia.org/wiki?curid=23140635", "title": "Ride for Hope", "text": "Ride for Hope\n\nRide for Hope is a US coast to coast relay cycle ride which takes place over 10 days. Teams of riders cycle for 50 miles each day from Sausalito, CA to Annapolis, MD covering a total of 3080 miles.\n\nThe ride is organised to raise funds for the orphans in Africa from the AIDS and HIV pandemic who are supported by Christ's Hope International. The organiser and leader for the ride Tim Patton is a serious cyclist who started the event after having been to Africa and seeing the plight of the orphans there himself.\n\n'Ride for Hope' takes place in June 2009, 7th to the 17th and has so far raised $100,000 in support for the orphans.\n\nAs the teams cycle through Chicago on June 13, people are invited to join a 50 miles cycle to show support for the cause.\n\nMore information on this event and Christ's Hope International is available at Ride for Hope\n"}
{"id": "3527234", "url": "https://en.wikipedia.org/wiki?curid=3527234", "title": "Royal College of Pathologists of Australasia", "text": "Royal College of Pathologists of Australasia\n\nThe Royal College of Pathologists of Australasia is a medical organization that promotes the science and practice of pathology. It has members in Australia, New Zealand, Hong Kong, Singapore, Malaysia, and Saudi Arabia. \n\nThe object of the College is: \"To promote the study of the science and practice of Pathology in relation to medicine; to encourage research in pathology and ancillary sciences, to bring together pathologists for their common benefit and for scientific discussions and demonstrations; and to disseminate knowledge of the principles and practice of pathology in relation to medicine by such means as may be thought fit.\"\n\nThe College of Pathologists of Australia was incorporated on 10 April 1956. In 1970, the College was granted Royal assent, and became the Royal College of Pathologists of Australia. With the increasing number of Fellows in New Zealand, the College changed its name to the Royal College of Pathologists of Australasia in January 1980.\nSince 1986, the College has occupied Durham Hall, a heritage listed building in Sydney's Surry Hills and the adjacent 203-205 Albion Street, Surry Hills cottages.\n\nThe College conducts training and examinations in several sub-disciplines, including:\n\nThe College accredits laboratories for training, approves supervised training in accredicted laboratories, and conducts examinations leading to Fellowship of the College (FRCPA).\n\nSince its inception, the College has contributed to the continual development of knowledge and skills of it Fellows, and has established a formal Continuing Professional Development Program.\n\nThe College collaborated with the Commonwealth Government to establish the National Pathology Accreditation Advisory Council (NPAAC) in 1979. NPAAC advises the Commonwealth, State and Territory Health Ministers on matters relating to the accreditation of pathology laboratories, plays a key role in ensuring the quality of Australian pathology services and is responsible for the development and maintenance of standards and guidelines for pathology practices. \n\nWhile NPAAC provides the standards for laboratory practice, the actual accreditation process is carried out by NATA/RCPA, a joint initiative between the College and the National Association of Testing Authorities (NATA).\n"}
{"id": "21488622", "url": "https://en.wikipedia.org/wiki?curid=21488622", "title": "Self-administration", "text": "Self-administration\n\nSelf-administration is, in its medical sense, the process of a subject administering a pharmacological substance to themself. A clinical example of this is the subcutaneous \"self-injection\" of insulin by a diabetic patient.\n\nIn animal experimentation, self-administration is a form of operant conditioning where the reward is a drug. This drug can be administered remotely through an implanted intravenous line or an intracerebroventricular injection. Self-administration of putatively addictive drugs is considered one of the most valid experimental models to investigate drug-seeking and drug-taking behavior. The higher the frequency with which a test animal emits the operant behavior, the more rewarding (and addictive), the test substance is considered. Self-administration of addictive drugs has been studied using humans, non-human primates, mice, invertebrates such as ants, and, most commonly, rats.\n\nSelf-administration of heroin and cocaine is used to screen drugs for possible effects in reducing drug-taking behavior, especially reinstatement of drug seeking after extinction. Drugs with this effect may be useful for treating people with drug addiction by helping them establish abstinence or reducing their probability or relapsing to drug use after a period of abstinence.\n\nIn a prominent model of self-administration developed by George Koob, rats are allowed to self-administer cocaine for either 1 hour each day (short access) or 6 hours each day (long access). Those animals who are allowed to self-administer for 6 hours a day show behavior that is thought to resemble cocaine dependence, such as an escalation of the total dose taken during each session and an increase in the dose taken when cocaine is first made available.\n\nThe \"self-administration\" behavioral paradigm serves as an animal behavioral model of the human pathology of addiction. During the task, animal subjects are operant conditioned to perform one action, typically a lever press, in order to receive a drug. Reinforcement (through the use of the drug) occurs contingent upon the subject performing the desired behavior. Drug dosing in self-administration studies is response-dependent. This is an important element of creating a disease model of drug addiction in humans because response-independent drug administration is associated with increased toxicity and different neurobiological, neurochemical and behavioral effects. In summary, the effects of response-dependent drug dosing greatly differ from response-independent drug dosing and self-administration studies appropriately capture this distinction.\n\nAs far back as the mid-20th century, researchers have investigated animals’ drive to consume drugs of abuse in order to better understand human addictive processes. Spragg was one of the first researchers to create a model of chronic morphinism in a chimpanzee to explore the role of operant conditioning in relation to a drug dependency. When deprived of both food and morphine, chimpanzees would repeatedly attempt to seek out the drug of choice, even doing so much as to physically pull the experimenter into the room housing morphine and syringes. Weeks (1962) published an account of the first true use of the intravenous self-administration paradigm in a study aiming to model morphine addiction in unrestrained rats. For the first time, a drug of abuse served as an operant reinforcer and rats self-administered morphine to satiety in stereotyped response patterns.\n\nThe scientific community quickly adopted the self-administration paradigm as a behavioral means to examine addictive processes and adapted it to non-human primates. Thompson and Schuster (1964) studied the relative reinforcement properties of morphine in restrained rhesus monkeys using intravenous self-administration. Significant changes in response to other types of reinforcers (i.e., food, shock avoidance) were observed in drug-dependent subjects. In 1969, Deneau, Yanagita and Seevers provided macaque monkeys free access to a variety of drugs of abuse for investigating whether nonhuman primates would voluntarily initiate self-administration of these substances. Initiation and maintenance of self-administration produced dependence and toxicity in monkeys, thereby more closely approximating important aspects of drug addiction in humans and allowing for the first of modern self-administration studies.\n\nThe procedure of testing the efficacy of a pharmacological agent as a reinforcer would soon become a standard assay. Most frequently, studies were performed in nonhuman primates to identify abuse potential, as required by the drug development process. In 1983, Collins \"et al.\" published a landmark paper in which rats were exposed to a battery of 27 psychoactive substances. The team compared test drug self-administration rates with saline vehicle self-administration rates. If animals self-administered at a rate significantly greater than vehicle, the drug was considered an active reinforcer with abuse potential. With few exceptions, the abuse liability observed in rats paralleled that observed from previous research in monkeys. In light of these similarities between the different animal models, it was identified that the abuse potential of psychoactive substances could be investigated using rats instead of nonhuman primates.\n\nOperant conditioning represents the behavioral paradigm underlying self-administration studies. Although not always required, subjects may be first pre-trained to perform some action, such as a lever press or nosepoke to receive a food or water reward (under food- or water- restricted conditions, respectively). Following this initial training, the reinforcer is replaced by a test drug to be administered by one of the following methods: oral, inhalation, intracerebral, intravenous. Intravenous catheterization is used most commonly because it maximizes bioavailability and has rapid onset, although is inappropriate for drugs taken orally, such as alcohol. Humans suffering from addiction often resort to intravenous drug use for similar reasons, so this route of administration increases the face validity of the construct.\n\nUpon presentation of the drug to the subject, a number of experimental variables might be manipulated to test hypotheses:\n\nBoth humans and animals will adjust the rate and number of drug infusions to maintain stable rewarding blood levels of drugs, like cocaine. A dilute dose of cocaine will be administered intravenously at a faster rate than a concentrated dose of cocaine.\n\nContinuous reinforcement: A single operant response triggers the dispense of a single dose of reinforcer. A time-out period may follow each operant response that successfully yields a dose of reinforcer; during this period the lever used in training may be retracted preventing the animal from making further responses. Alternatively operant responses will fail to produce drug administration allowing previous injections to take effect. Moreover, time-outs also help prevent subjects from overdosing during self-administration experiments. \nFixed-ratio studies require a predefined number of operant responses to dispense one unit of reinforcer. Standard fixed ratio reinforcement schedules include FR5 and FR10, requiring 5 and 10 operant responses to dispense a unit of reinforcer, respectively.\nProgressive ratio reinforcement schedules utilize a multiplicative increase in the number of operant responses required to dispense a unit of reinforcer. For example, successive trials might require 5 operant responses per unit of reward, then 10 responses per unit of reward, then 15, and so on. The number of operant responses required per unit of reinforcer may be altered after each trial, each session, or any other time period as defined by the experimenter. Progressive ratio reinforcement schedules provide information about the extent that a pharmacological agent is reinforcing through the breakpoint. The breakpoint is the number of operant responses at which the subject ceases engaging in self-administration, defined by some period of time between operant responses (generally up to an hour).\nFixed interval (FI) schedules require that a set amount of time pass between drug infusions, regardless of the number of times that the desired response is performed. This “refractory” period can prevent the animal from overdosing on a drug.\nVariable interval (VI) schedules of reinforcement are identical to FI schedules, except that the amount of time between reinforced operant responses varies, making it more difficult for the animal to predict when the drug will be delivered.\n\nSecond-order reinforcement schedules build on basic reinforcement schedules by introducing a conditioned stimulus that has previously been paired with the reinforcer (such as the illumination of a light). Second-order schedules are built from two simpler schedules; completion of the first schedule results in the presentation of an abbreviated version conditioned stimulus, following completion of a fixed-interval, the drug is delivered, alongside the full-length conditioned stimulus. Second-order schedules result in a very high rate of operant responding at the presentation of the conditioned reinforcer becomes a reinforcing in its own right. Benefits of this schedule include the ability to investigate the motivation to seek the drug, without interference of the drug's own pharmacological effects, maintaining a high level of responding with relatively few drug infusions, reduced risk of self-administered overdose, and external validity to human populations where environmental context can provide a strong reinforcing effect for drug use.\n\nExtinction involves the discontinuation of a particular reinforcer in response to operant behavior, such as replacing a reinforcing drug infusion with a saline vehicle. When the reinforcing element of the operant paradigm is no longer present, a gradual reduction in operant responses results in eventual cessation or “extinction” of the operant behavior.\nReinstatement is the restoration of operant behavior to acquire a reinforcer, often triggered by external events/cues or exposure to the original reinforcer itself. Reinstatement can be broken into a few broad categories:\n\nDrug-induced reinstatement: exposure to a reinforcing drug after extinction of drug-seeking operant behavior can often reinstate drug-seeking, and can even occur when the new drug of exposure is different from the original reinforcer. This is thought to be strongly linked to drug sensitization \nCue-induced reinstatement: environmental cues associated with drug administration can trigger drug reinstatement by acting as conditioned stimuli, even during drug abstinence \n\n1. Environmental surroundings as well as drug-associated behavior or actions can function as environmental cues.\n\n2. Stress-induced reinstatement: in many cases, a stressor can reinstate drug-seeking in a drug-abstinent animal. This can include (but is not limited to) acute stressors such as foot-shock or social defeat stress. In many cases, it appears that social stress can potentiate drug reinstatement just as strongly as exposure to the drug itself \n\nAnimal self-administration experiments are typically performed in standard operant conditioning chambers adapted for the catheters used to deliver a drug intravenously. The catheter is secured to the animal by a harness or back plate and is tethered to a protective leash that extends upward through a hole in the top of a chamber, where it attaches to a rotating swivel on a mechanical arm that allows the subject to move around freely. The chamber houses two levers: one whose depression results in the delivery of a drug, the other whose depression does nothing. Activity on these levers can be used to measure drug administration (via activity at the drug-inducing lever) as well as changes in nonspecific behavior that reflect short- and long-term effects of the drug (via activity at the non-inducing lever). The sterile intravenous catheter used to deliver the drug into the bloodstream of the subject is typically composed of a flexible plastic, silastic tubing and nylon mesh placed subcutaneously. It is attached to a mechanical pump that can be calibrated to deliver a specific amount of drug upon depression of one of the levers in the chamber.\nOther chamber modifications are required if the drug is to be delivered orally or via inhalation, such as liquid containers or an aerosol distribution mechanism.\n\nSelf-administration studies have long been considered the “standard” in addiction research using both animal and human models.\nConducting self-administration studies in animal models provides a much greater level of experimental flexibility than in humans because investigating the effects of novel pharmacological drug treatments poses significantly fewer ethical and practical barriers. In 1999, Pilla and colleagues published in Nature a study documenting the efficacy of a partial D3-agonist (BP-897) in reducing environmental cue-induced cocaine craving and vulnerability to relapse. An interesting aspect of this study was the use of second-order reinforcement schedules to identify a dissociation in the effects of BP-897 in that the drug inhibits cue-induced cocaine seeking but has no primary reinforcement effect. This latter condition is important for any pharmacological agent to be used in the treatment of addiction—drugs used to treat addiction should be less reinforcing than the drug whose addiction they treat and optimally have no reinforcing effects.\n\nA recent study published in Nature showed an upregulation of microRNA-212 in the dorsal striatum of rats previously exposed to cocaine for extended periods. Animals infected with a viral vector overexpressing miR-212 in the dorsal striatum produced the same initial levels of cocaine intake; however, drug consumption progressively decreased as net cocaine exposure increased. The authors of the study noted that viral-infected animals exhibited decreased operant responding during the post-infusion time-out period and proposed that this demonstrated a reduction in compulsive drug-seeking behavior.(Hollander \"et al.\") miR-212 acts through Raf1 to enhance the CREB response; CREB-TORC is known to negatively regulate the reinforcing effects of cocaine. (Hollander \"et al.\") This study provides one example (miR-212, owing to its amplification of CREB) of a self-administration study that may provide potential therapeutic targets for the treatment of cocaine addiction.\nOne of the most important advances to emerge from self-administration studies comes from a behavioral model for addiction in animals. This model relies on observation of three separate phenomena to classify a rat as “addicted:”\n1) Persistence in drug-seeking: Depends on the attempts of rats to obtain drug during time-out or no-periods in the self-administration apparatus.\n2) Resistance to punishment: Measured by how much rats maintain rates of self-administration when cocaine infusion is paired with an electric shock.\n3) Motivation for the drug: Measured by the breakpoint in progressive ratio reinforcement. (Deroche-Gamonet et al.)\n\nThe researchers used an additional test to further support classification of a rat as “addicted” by measuring relapse rates during reinstatement paradigms. Human drug addicts reportedly relapse at a rate of >90% as measured from the initial diagnosis. Rats that responded at high rates after some form of cue-induced reinstatement could be considered likely to relapse.(Deroche-Gamonet et al.) This model provided an important advancement for the method of self-administration because it allows animal models to better approximate the physiological and behavioral aspects of drug addiction in humans.\n\nSelf-administration experiments can also be paired with methods such as in vitro electrophysiology or molecular biology to understand the effects of addiction on neural circuitry. Self-administration studies have allowed researchers to locate a staggering number of changes in brain signaling that occur in addiction. One example of such a study involved examining synaptic plasticity in rats undergoing the behavioral shift to addiction. Using the criteria for classifying rats as “addicts” or “non-addicts” as put forth by Deroche-Gamonet et al., it was found that addicted rats display a prolonged and persistent impairment in mGluR2/3-dependent Long-Term Depression. Despite exposure to the same self-administration paradigm, control rats recovered this form of synaptic plasticity. The authors of the study propose an important explanation for their results in that this specific loss of plasticity over an extended period is responsible for the progressive loss of controlled drug use.(Kasanetz et al.) This represents a potential molecular mechanism by which addicts might differ from non-addicts and undergo pathological learning processes during the development of addiction.\n\nMuch like animal studies, human experiments that pair self-administration studies with additional neuroscientific techniques provide unique insight into the disease of addiction. Human self-administration studies have gained momentum with the widespread use of fMRI technology to measure BOLD signals. Brain imaging coupled with human self-administration studies with the laboratory have led to the development of a three-stage model of human neurocircuitry of addiction: Binge/Intoxication, Preoccupation/Anticipation, and Withdrawal/Negative Effect. Koob, Lloyd, and Mason reviewed the laboratory models approximating each stage of the model of human addiction.(Koob et al.) The binge-intoxication phase traditionally has been modeled by drug or alcohol self-administration; the psychological effects of addiction might be modeled by the increased motivation for self-administration observed in drug-dependent animals. Self-administration studies capably model the somatic effects of addiction, but many of the most deleterious effects related to drug addiction can be considered psychological in nature. (Koob et al.) Models like the one published by Deroche-Gamonet and colleagues in 2004 better approximate the effects of addiction on physiology and psychology, but animal models are inherently limited in their ability to reproduce human behavior.\n\nThe use of the self-administration methodology to model human drug addiction provides powerful insight into the physiological and behavioral effects of the disease. While self-administration experiments in humans or animals each pose unique barriers to complete understanding of addiction, the scientific community continues to invest a great deal of effort in both avenues of research in the hopes of improving understanding and treatment of addiction.\n"}
{"id": "32827616", "url": "https://en.wikipedia.org/wiki?curid=32827616", "title": "Society of Obstetricians and Gynaecologists of Pakistan", "text": "Society of Obstetricians and Gynaecologists of Pakistan\n\nThe Society of Obstetricians and Gynaecologists of Pakistan (SOGP) is a professional medical association formed by practitioners of obstetrics and gynecology in Pakistan. A member of the International Federation of Gynecology and Obstetrics (FIGO), the body was established in 1957 and is among the oldest specialist professional bodies. When Prof. H.Dewattleville, the President of FIGO at that time, visited Karachi, he addressed staff and students at the Dow Medical College and members of the Pakistan Medical Council (now Pakistan Medical and Dental Council) and stressed the need for such an organisation in the country. The first constitution of the organisation was signed and published by the Journal of Pakistan Medical Association in June 1957.\n\nToday, it is the largest and most prominent organisation of obstetricians and gynaecologists in the country. SOGP is also a member of the South Asia Federation of Obstetrics and Gynaecology (SAFOG) and works in close collaboration with other national medical committees around the world. In 1997, Pakistan was also elected to the Executive Board of FIGO.\n\n"}
{"id": "4161298", "url": "https://en.wikipedia.org/wiki?curid=4161298", "title": "Sports periodization", "text": "Sports periodization\n\nPeriodization is the systematic planning of athletic or physical training. The aim is to reach the best possible performance in the most important competition of the year. It involves progressive cycling of various aspects of a training program during a specific period. Conditioning programs can use periodization to break up the training program into the off-season, preseason, inseason, and the postseason. Periodization divides the year round condition program into phases of training which focus on different goals.\n\nThe roots of periodization come from Hans Selye’s model, known as the General adaptation syndrome\nThe GAS describes three basic stages of response to stress: (a) the Alarm stage, involving the initial shock of the stimulus on the system, (b) the Resistance stage, involving the adaptation to the stimulus by the system, and (c) the Exhaustion stage, in that repairs are inadequate, and a decrease in system function results. The foundation of periodic training is keeping one's body in the resistance stage without ever going into the exhaustion stage. By adhering to cyclic training the body is given adequate time to recover from significant stress before additional training is undertaken. The response to a new stress is to first respond poorly and the response drops off. For example when the body is first exposed to sun a sunburn might develop. During the resistance stage adaptation improves the response to a higher level, called super compensation, than the previous equilibrium. The example would be that a suntan develops. The exhaustion stage is a continuation of the stimulus at too high a level and the increase gained from adaptation is now offset and all gains are lost. The example would be that wrinkles, spots, or even skin cancer develop. The goal in sports periodization is to reduce the stress at the point where the resistance stage ends so the body has time to recover. In this way the exhaustion stage does not reduce the gains achieved, the body can recover and remain above the original equilibrium point. The next cycle of increased stimulus now improves the response further and the equilibrium point continues to rise after each cycle.\n\nSelye (1957) labeled beneficial stresses as \"eustress\" and detrimental stresses as \"distress\". In athletics, when physical stress is at a healthy level (eustress), an athlete experiences muscular strength and growth, while excessive physical stress (distress) can lead to tissue damage, disease, and death. Periodization is most widely used in resistance program design to avoid over-training and to systematically alternate high loads of training with decreased loading phases to improve components of muscular fitness (e.g. strength, strength-speed, and strength-endurance). The Selye-cycles are similar to the \"micro cycles\" used at later times.\n\nRussian physiologist Leo Matveyev and Romanian sport scientist Tudor Bompa expanded and further organized the periodization model. Matveyev has been regarded as the father of modern periodization. He analysed the results of the Soviet athletes of the 1952 and 1956 summer Olympics and compared successful and not so successful athletes and their training schedules. From these training plans periodized schedules were developed for the 1960 Olympics. With the success of the Soviet athletes, Matveyev's plans were spread all over the Eastern Bloc in their annual coordination meetings. From there it also spread to Romania, where Tudor Bompa developed the system further. In the 1968 it was used for the first time in the GDR and in 1972 in West Germany. After the fall of the Soviet Union, periodization started to become modified. While Matveyev followed Pavlov and assumed that everybody should use the same periodization, individualised systems, using more and more biological data, were introduced.\n\nPeriodic training systems typically divide time up into three types of cycles: microcycle, mesocycle, and macrocycle. The microcycle is generally up to 7 days. The mesocycle may be anywhere from 2 weeks to a few months, but is typically a month. A macrocycle refers to the overall training period, usually representing a year or two. There are longer cycles as well for the Olympian, being 4 or 8 years, and the career plan which is usually only considered for Olympians and professional athletes.\n\nTraining should be organized and planned in advance of a competition or performance.\nIt should consider the athlete’s potential, his/her performance in tests or competition, and calendar of competition. It has to be simple, suggestive, and above all flexible as its content can be modified to meet the athletes rate of progress.\n\nA macrocycle is an annual plan that works towards peaking for the goal competition of the year. There are three phases in the macrocycle: preparation, competitive, and transition.\n\nThe entire preparation phase should be around 2/3 to 3/4 of the macrocycle. The preparation phase is further broken up into general and specific preparation of which general preparation takes over half. An example of general preparation would be building an aerobic base for an endurance athlete such as running on a treadmill and learning any rules or regulations that would be required such as proper swimming stroke as not to be disqualified. An example of specific preparation would be to work on the proper form to be more efficient and to work more on the final format of the sport, which is to move from the treadmill to the pavement.\n\nThe competitive phase can be several competitions, but they lead up to the main competition with specific tests. Testing might include any of the following: performance level, new shoes or gear, a new race tactic might be employed, pre-race meals, ways to reduce anxiety before a race, or the length needed for the taper. When the pre-competitions are of a higher priority there is a definite taper stage while lower priority might simply be integrated in as training. The competitive phase ends with the taper and the competition.\n\nThe transition phase is important for psychological reasons, a year of training means a vacation is in order. A typical weekend warrior might take three months while a professional athlete might take as little as two weeks.\n\nA mesocycle represents a phase of training with a duration of between 2 – 6 weeks or microcycles, but this can depend on the sporting discipline. A mesocycle can also be defined as a number of continuous weeks where the training program emphasize the same type of physical adaptations, for example muscle mass and anaerobic capacity. During the preparatory phase, a mesocycle commonly consists of 4 – 6 micro-cycles, while during the competitive phase it will usually consist of 2 – 4 micro-cycles depending on the competition’s calendar.\n\nThe goal of the plan is to fit the mesocycles into the overall plan timeline-wise to make each mesocycle end on one of the phases and then to determine the workload and type of work of each cycle based on where in the overall plan the given mesocycle falls. The goal in mind is to make sure the body peaks for the high priority competitions by improving each cycle along the way.\n\nA microcycle is typically a week because of the difficulty in developing a training plan that does not align itself with the weekly calendar. Each microcycle is planned based on where it is in the overall macrocycle.\n\nA micro-cycle is also defined as a number of training sessions, built around a given combination of acute program variables, which include progression as well as alternating effort (heavy vs. light days). The length of the micro-cycle should correspond to the number of workouts - empirically often 4-16 workouts - it takes for the athlete or fitness client to adapt to the training program. When the athlete or fitness client has adapted to the program and no longer makes progress, a change to one or more program variables should be made.\n\nThe annual plan is important in that it directs and guides performance training over a year. It is based on the concept of periodization and the principles of training. The objective of training is to reach a high level of performance (peak performance) and an athlete has to develop skills, biomotor abilities and psychological traits in a methodical manner.\n\nThis phase consists of the general preparation and specific preparation. Usually which can be subdivided into three different phases. One should always remember that this is a base creation phase with the objective to attain the previous training state, and the longest period of periodization must be devoted towards the preparatory period.\nThe performance depends on preparatory period, and is divided into three phases:\n\nPhase I) To regain previous training \n\nPhase II) Low training volume & High training load to develop the factors needed for performance.\nPhase III) Decrease in the intensity of load and increase in tactical training and aim at improving tactical under competition condition.\nThis phase may contain a few main competitions each containing a pre-competitive and a main competition. Within the main competition, an uploading phase and a special preparatory phase may be included. This also includes hypertropthy\n\nThis phase is used to facilitate psychological rest, relaxation and biological regeneration as well as to maintain an acceptable level of general physical preparation. This phase lasts 3–4 weeks (perhaps longer) but should not exceed five weeks under normal conditions and may be sports specific. It allows the body to fully regenerate so that it is prepared for the next discipline.\n\nA review published in the journal 'sports medicine' in 2017 has questioned the validity of the use of traditional models of periodization in sport. This is largely due to the over simplified assumptions put forward in the early development of periodization theory that are not always transferable to the psyco-biological effects of various training methods used in sport.\n\nFor many years, football training and its planning has been (and still is) characterised by fragmented thinking of which has perhaps been attributed to the success of such an approach in individual sports. The emphasis of planning and improvement was mainly in regards to the physical attributes of strength, speed and endurance. Furtheremore, whether it be the undulating model, the reverse linear model or the traditional periodization framework, one of the underpinning assumptions is that it would be best to segregate the programme into distinct training blocks in a sequential hierarchy i.e. a training block to build endurance before speed and strength before power. This is claimed to be based on the ‘science of periodization’. However, Kiely, argues that the proof on which this is all based on is flawed because the studies used to build this proof have only ‘compared training interventions with no training variation to those with degrees of variation’ but did not offer any insight ‘into how that variation is best scheduled and organised’ (Kiely, 2010, p. 4). So although it is commonly agreed that training variation is an important design feature which needs to be integrated into the training plan, there is ‘little or no supporting evidence’ which proves that the segregation of the program into distinct training blocks or that there should be sequential hierarchy of how this should be done is in fact advantageous.\n\nMany authors such as Garganta, Maia, & Marque, Oliviera, Castelo, and Gaiteiro have argued in regards to the indivisibility of the 4 components which make up a football performance...technical, tactical, physical and mental. This complexity makes football a multidimensional phenomenon which cannot be simply reduced to the sum of its parts. Is defending more important than attacking or vice versa? This question cannot be answered as football needs to be understood as a whole. As highlighted by Tamarit, football is a tactical game where players are constantly required to make decisions in response to specific situations. So football needs to be viewed as a tactical game which encompasses with it the physical, technical and mental aspects required for positive performances. This is the foundation upon which Victor Frade developed a training methodology known as tactical periodisation which emphasises the development of the tactical dimension. Therefore, any physical, technical or mental development must always have a tactical intention. In other words, although the ability to run for 90 minutes may seem vital to play the game, knowing when, where and how to run is much more important.\n"}
{"id": "2641050", "url": "https://en.wikipedia.org/wiki?curid=2641050", "title": "Taunton State Hospital", "text": "Taunton State Hospital\n\nTaunton State Hospital is a psychiatric hospital located on Hodges Avenue \nin Taunton, Massachusetts. Established in 1854, it was originally known as the State Lunatic Hospital at Taunton. It was the second state asylum in Massachusetts. Most of the original part of the facility was built in a unique and rare neo-classical style designed by architects Boyden & Ball. It is also a Kirkbride Plan hospital and is located on a large farm along the Mill River.\n\nThe complex was expanded at various times to include over forty buildings and structures. The main part of the hospital (known as \"the Kirkbride Building\") closed in 1975, and the buildings fell into disrepair. In 1994, the property was added to the National Register of Historic Places as a historic district. In 1999, the main dome of the administration building collapsed. In 2006, a large part of the historic complex was destroyed by fire. In 2009, the remaining parts were demolished. However, many of the newer buildings on the campus remain.\n\nIn 1851, the Massachusetts General Court appointed a commission to find a site for a new asylum to relieve the pressure of a rising patient population from its only facility in Worcester. The new at Taunton opened in April 1854. The large sprawling campus located on a hill offered fresh air and sunlight, following Kirkbride's concept for treating mental health patients. The complex was expanded in the early 1870s and again between 1887 and 1906. From the 1930s, juvenile facilities, crisis centers, sick wards, and group homes were added.\n\nOne of the building's most beautiful features was its breezeways, which were added in the 1890s to connect the end of the wards to the hospital's infirmary buildings. Its distinct cupolas, large dome, cast-iron capitals, and window bar gave this building its own very unusual personality.\n\nIn 1975, the main part of the hospital was closed and abandoned. In 1999, the large dome which towered over the hospital's administration building collapsed. Then on the night of March 19, 2006, a massive fire broke out in the center of the building, which included the administration and theater. Sections damaged by fire were then leveled, leaving only the decaying wings of the Kirkbride Building.\n\nIn May 2009, demolition of the remaining historic sections of the Kirkbride Building began. The facility had numerous architectural features that were salvaged and sold to individuals and companies throughout the United States, including architectural granite, bricks, timbers, iron gates, vintage plumbing and lighting fixtures, furniture, and slate roofing tiles. The project was completed in early 2010.\n\n\nIn the early 1990s, a $19 million capital improvement plan was implemented by the state to improve the still-operating portions of the campus. In early 2012, the state announced the closure of the remaining parts of the facility containing 169 beds. A plan to keep a portion of the facility open was vetoed by Governor Deval Patrick in July 2012.\n\nTaunton State Hospital remains open and houses 48 psychiatric beds, the Women's Recovery from Addiction Program, a residential program under the Department of Youth Services, and a substance abuse program administered by High Point Treatment Center. There is also a greenhouse on the campus that is staffed by patients and sells a variety of plants and seasonal produce to the public.\n\n\n\n"}
{"id": "4490456", "url": "https://en.wikipedia.org/wiki?curid=4490456", "title": "Telerehabilitation", "text": "Telerehabilitation\n\nTelerehabilitation (or e-rehabilitation) is the delivery of rehabilitation services over telecommunication networks and the internet. Most types of services fall into two categories: clinical assessment (the patient's functional abilities in his or her environment), and clinical therapy. Some fields of rehabilitation practice that have explored telerehabilitation are: neuropsychology, speech-language pathology, audiology, occupational therapy, and physical therapy. Telerehabilitation can deliver therapy to people who cannot travel to a clinic because the patient has a disability or because of travel time. Telerehabilitation also allows experts in rehabilitation to engage in a clinical consultation at a distance.\n\nMost telerehabilitation is highly visual. As of 2006 the most commonly used modalities are via webcams, videoconferencing, phone lines, videophones and webpages containing rich Internet applications. The visual nature of telerehabilitation technology limits the types of rehabilitation services that can be provided. It is most widely used for neuropsychological rehabilitation; fitting of rehabilitation equipment such as wheelchairs, braces or artificial limbs; and in speech-language pathology. Rich internet applications for neuropsychological rehabilitation (aka cognitive rehabilitation) of cognitive impairment (from many etiologies) was first introduced in 2001. This endeavor has recently (2006) expanded as a teletherapy application for cognitive skills enhancement programs for school children. Tele-audiology (hearing assessments) is a growing application. As of 2006, telerehabilitation in the practice of occupational therapy and physical therapy are very limited, perhaps because these two disciplines are more \"hands on\".\n\nTwo important areas of telerehabilitation research are (1) demonstrating equivalence of assessment and therapy to in-person assessment and therapy, and (2) building new data collection systems to digitize information that a therapist can use in practice. Ground-breaking research in telehaptics (the sense of touch) and virtual reality may broaden the scope of telerehabilitation practice, in the future.\n\nIn the United States, the National Institute on Disability and Rehabilitation Research's (NIDRR) supports research and the development of telerehabilitation. NIDRR's grantees include the \"Rehabilitation Engineering and Research Center\" (RERC) at the University of Pittsburgh, the Rehabilitation Institute of Chicago, the State University of New York at Buffalo, and the National Rehabilitation Hospital in Washington DC. Other federal funders of research are the Veterans Administration, the Health Services Research Administration in the US Department of Health and Human Services, and the Department of Defense. Outside the United States, excellent research is conducted in Australia and Europe.\n\nAs of 2006, only a few health insurers in the United States will reimburse for telerehabilitation services. If the research shows that tele-assessments and tele-therapy are equivalent to clinical encounters, it is more likely that insurers and Medicare will cover telerehabilitation services.\n\nIn 1999, D.M. Angaran published \"Telemedicine and Telepharmacy: Current Status and Future Implications\" in the American Journal of Health-System Pharmacy. He provided a comprehensive history of telecommunications, the internet and telemedicine since the 1950s. The Department of Defense (DoD) and the National Aeronautics and Space Administration (NASA) spearheaded the technology in the United States during the Vietnam War and the space program; both agencies continue to fund advances in telemedicine.\n\nThree early adopters of telemedicine were state penitentiary systems, rural health care systems, and the radiology profession. Telemedicine makes business sense for the states because they do not have to pay for security escorts to have a prisoner receive care outside the prison.\n\nRural telemedicine in the United States is heavily subsidized through federal agency grants for telecommunications operations. Most of this funding comes through the Health Services Research Administration and the Department of Commerce. Some state universities have obtained state funding to operate tele-clinics in rural areas. As of 2006, few (if any) of these programs are known to financially break-even, mostly because the Medicare program for people over age 65 (the largest payer) is very restrictive about paying for telehealth.\n\nIn contrast, the Veterans Administration is relatively active in using telemedicine for people with disabilities. There are several programs that provide annual physical exams or monitoring and consultation for veterans with spinal cord injuries. Similarly, some state Medicaid programs (for poor people and people with disabilities) have pilot programs using telecommunications to connect rural practitioners with subspecialty therapists. A few school districts in Oklahoma and Hawaii offer school-based rehabilitation therapy using therapy assistants who are directed by a remote therapist. The National Rehabilitation Hospital in Washington DC and Sister Kenny Rehabilitation Institute in Minneapolis provided assessment and evaluations to patients living in Guam and American Samoa. Cases included post-stroke, post-polio, autism, and wheel-chair fitting.\n\nAn argument can be made that \"telerehabilitation\" began in 1998 when NIDRR funded the first RERC on tele-rehabilitation. It was awarded to a consortium of biomedical engineering departments at the National Rehabilitation Hospital and The Catholic University of America, both located in Washington, DC; the Sister Kenny Rehabilitation Institute in Minnesota; and the East Carolina University in North Carolina. Some of this early research work, and its motivation, is reviewed in Winters (2002). The State of Science Conference held in 2002 convened most of military and civilian clinicians, engineers, and government officials interested in using telecommunications as a modality for rehabilitation assessment and therapy; a summary is provided in Rosen, Winters & Lauderdale (2002). The conference was attended by the incoming president of the American Telemedicine Association (ATA). This led to an invitation by ATA to the conference attendees to form a special interest group on telerehabilitation. NIDRR funded the second 5-year RERC on telerehabilitation in 2004, awarding it to the University of Pittsburgh. This RERC was renewed in 2010.\n\nIn 2001, O. Bracy, a neuropsychologist, introduced the first web based, rich internet application, for the telerehabilitation presentation of cognitive rehabilitation therapy. This system first provides the subscriber clinician with an economical means of treating their own patients over the internet. Secondly, the system then provides, directly to the patient, the therapy prescription set up and controlled by the member clinician. All applications and response data are transported via the internet in real time. The patient can login to do their therapy from home, the library or anywhere they have access to an internet computer. In 2006, this system formed the basis of a new system designed as a cognitive skills enhancement program for school children. Individual children or whole classrooms can participate in this program over the internet.\n\nIn 2006, M.J. McCue and S.E. Palsbo published an article in the Journal of Telemedicine and Telecare that explored how telemedicine can become a profitable business for hospitals. They argue that telerehabilitation should be expanded so that people with disabilities and people in pain (perhaps after hip-replacement surgery or people with arthritis) can get the rehabilitative therapy they need. It is unethical to limit paymente for telerehabilitation services only to patients in rural areas.\n\nResearch in telerehabilitation is in its infancy, with only a handful of equivalence trials. As of 2006, most peer-reviewed research in telemedicine are case reports of pilot programs or new equipment. Rehabilitation researchers need to conduct many more controlled experiments and present the evidence to clinicians (and payers) that telerehabilitation is clinically effective. The discipline of speech-language pathology is far head of occupational therapy and physical therapy in demonstrating equivalence over various types of telecommunications equipment.\n\n\n\nThe clinical services provided by speech-language pathology readily lend themselves to telerehabilitation applications due to the emphasis on auditory and visual communicative interaction between the client and the clinician. As a result, the number of telerehabilitation applications in speech-language pathology tend to outnumber those in other allied health professions. To date, applications have been developed to assess and/or treat acquired adult speech and language disorders, stuttering, voice disorders, speech disorders in children, and swallowing dysfunction. The technology involved in these applications has ranged from the simple telephone (Plain Old Telephone System – POTS) to the use of dedicated Internet-based videoconferencing systems.\n\nEarly applications to assess and treat acquired adult speech and language disorders involved the use of the telephone to treat patients with aphasia and motor speech disorders (Vaughan, 1976, Wertz, et al., 1987), a computer controlled video laserdisc over the telephone and a closed-circuit television system to assess speech and language disorders (Wertz et al., 1987), and a satellite-based videoconferencing system to assess patients in rural areas (Duffy, Werven & Aronson, 1997). More recent applications have involved the use of sophisticated Internet-based videoconferencing systems with dedicated software which enable the assessment of language disorders (Georgeadis, Brennan, Barker, & Baron, 2004, Brennan, Georgeadis, Baron & Barker, 2004) and the assessment and treatment of motor speech disorders (Hill, Theodoros, Russell, Cahill, Ward, Clark, 2006; Theodoros, Constantinescu, Russell, Ward, Wilson & Wootton, in press) following brain impairment and Parkinson's disease. Collectively, these studies have revealed positive treatment outcomes, while assessment and diagnoses have been found to be comparable to face-to-face evaluations.\n\nThe treatment of stuttering has been adapted to a telerehabilitation environment with notable success. Two Australian studies (Harrison, Wilson & Onslow, 1999; Wilson, Onslow & Lincoln, 2004) involving the distance delivery of the Lidcombe program to children who stutter have utilized the telephone in conjunction with offline video recordings to successfully treat several children. Overall, the parents and children responded positively to the program delivered at a distant. Using a high speed videoconferencing system link, Sicotte, Lehoux, Fortier-Blanc and Leblanc (2003) assessed and treated six children and adolescents with a positive reduction in the frequency of dysfluency that was maintained six months later. In addition, a videoconferencing platform has been used successfully to provide follow-up treatment to an adult who had previously received intensive therapy (Kully, 200).\n\nReports of telerehabilitation applications in paediatric speech and language disorders are sparse. A recent Australian pilot study has investigated the feasibility of an Internet-based assessment of speech disorder in six children (Waite, Cahill, Theodoros, Russell, Busuttin, in press). High levels of agreement between the online and face-to-face clinicians for single-word articulation, speech intelligibility, and oro-motor tasks were obtained suggesting that the Internet-based protocol had the potential to be a reliable method for assessing paediatric speech disorders.\n\nVoice therapy across a variety of types of voice disorders has been shown to be effectively delivered via a telerehabilitation application. Mashima et al. (2003) using PC based videoconferencing and speech analysis software compared 23 patients treated online with 28 persons treated face-to-face. The authors reported positive post treatment results with no significant difference in measures between the traditional and videoconferencing group, suggesting that the majority of traditional voice therapy techniques can be applied to distance treatment.\n\nAlthough obvious limitations exist, telerehabilitation applications for the assessment of swallowing function have also been used with success. Lalor, Brown and Cranfield (2000) were able to obtain an initial assessment of the nature and extent of swallowing dysfunction in an adult via a videoconferencing link although a more complete evaluation was restricted due to the inability to physically determine the degree of laryngeal movement. A more sophisticated telerehabilitation application for the assessment of swallowing was developed by Perlman and Witthawaskul (2002) who described the use of real-time videofluoroscopic examination via the Internet. This system enabled the capture and display of images in real-time with only a three to five second delay.\n\nThere continues to be a need for ongoing research to develop and validate the use of telerehabilitation applications in speech-language pathology in a greater number and variety of adult and paediatric communication and swallowing disorders.\n\n\n"}
{"id": "39904011", "url": "https://en.wikipedia.org/wiki?curid=39904011", "title": "Thomas Fresh", "text": "Thomas Fresh\n\nThomas Fresh (3 September 1803 – 1861) was a pioneer in British environmental health. In 1844, he became Liverpool's first public health officer.\n\nThomas Fresh was born on 3 September 1803 at the family farm 'Newbarns', in the village of Newbarns, in the Lake District parish of Dalton-in-Furness. The family also had interests in property and iron-ore mining and trading.\n\nFresh was appointed Liverpool's Inspector of Nuisances by the Borough's Health of the Town Committee on 4 September 1844, over two years before the celebrated appointments on 1 January 1847 of William Henry Duncan, as Britain's first Medical Officer of Health, James Newlands as the first Borough Engineer, and Fresh’s own re-appointment under the Liverpool ‘Sanatory’ Act of 1846.\n\nEven before his 1844 appointment Fresh was the officer responsible for environmental health interventions, working initially from the police department. He was also at various times concurrently Superintendent of Alms Houses, and Superintendent of Scavengers (i.e. manager of refuse collection and public cleansing). The latter was eventually subsumed into Newlands' department.\n\nThomas Fresh was the first inspector of nuisances appointed by a United Kingdom Health Committee. When he was re-appointed in January 1847 he became probably the first of the inspectors of nuisances (sanitary inspectors) appointed under a 'Sanatory Act' with statutorily defined powers and duties. They were the forefathers of today's environmental health practitioners. With no trained staff to call upon, and with no established systems or infrastructures, Fresh created a model sanitary department, and became nationally famous. He worked closely with William Duncan, the Medical Officer of Health, who had no field staff of his own.\n\nFresh died in Glasgow in 1861 and is buried in Liverpool’s St James’s Cemetery. He was succeeded by his wife Martha. They had no children.\n\nIn 1853 Thomas and Martha Fresh were living in a much extended and improved former agricultural cottage on his 'model farm' in the Formby district between Liverpool and Southport when, on behalf of local residents and the joint Lord of the Manor, he asked the directors of the Liverpool, Crosby and Southport Railway to construct a station at that point on the line. He donated his own land for the purpose. As a significant landowner in the area, he gave his name to the district of Freshfield and Freshfield railway station - built in 1854.\n\nFresh's house is today 95 Freshfield Road, Freshfield. A blue plaque has been affixed to the house by the Formby Civic Society, and an application is to be made to have the house 'listed' to celebrate its significance as the home of the founder of Freshfield and one of the Liverpool public health pioneers. A nearby public house is called 'The Freshfield'. Fresh's official duties included solving the problem of the disposal of Liverpool's \"night-soil\". He arranged for a part of it to be transported from Liverpool to Freshfield for use as fertiliser on hitherto unproductive nearby fields, helping to develop a local trade in potato and asparagus cultivation. Freshfield is now an affluent residential district in the Borough of Sefton.\n\nLiverpool John Moores University organises an annual 'Thomas Fresh' public health lecture in his name.\n\n"}
{"id": "10394247", "url": "https://en.wikipedia.org/wiki?curid=10394247", "title": "Veterinary pharmacist", "text": "Veterinary pharmacist\n\nA veterinary pharmacist is a specially trained pharmacist who dispenses veterinary drugs and supplies or products and advice to owners of companion animals and livestock.\n\nIn addition, they advise the regulatory bodies and are involved in the formulation of veterinary drugs.\n\n\n\nOthers\n"}
{"id": "12086946", "url": "https://en.wikipedia.org/wiki?curid=12086946", "title": "Water metering", "text": "Water metering\n\nWater metering is the process of measuring water use.\n\nIn many developed countries water meters are used to measure the volume of water used by residential and commercial buildings that are supplied with water by a public water supply system. Water meters can also be used at the water source, well, or throughout a water system to determine flow through a particular portion of the system. In most of the world water meters measure flow in cubic metres (m) or litres but in the USA and some other countries water meters are calibrated in cubic feet (ft.) or US gallons on a mechanical or electronic register. Some electronic meter registers can display rate-of-flow in addition to total usage.\n\nThere are several types of water meters in common use. The choice depends on the flow measurement method, the type of end user, the required flow rates, and accuracy requirements.\n\nIn North America, standards for manufacturing water meters are set by the American Water Works Association. Outside of North America, most countries use ISO standards\n\nThere are two common approaches to flow measurement, displacement and velocity, each making use of a variety of technologies. Common displacement designs include oscillating piston and nutating disc meters. Velocity-based designs include single- and multi-jet meters and turbine meters.\n\nThere are also non-mechanical designs, for example electromagnetic and ultrasonic meters, and meters designed for special uses. Most meters in a typical water distribution system are designed to measure cold potable water only. Specialty hot water meters are designed with materials that can withstand higher temperatures. Meters for reclaimed water have special lavender register covers to signify that the water should not be used for drinking.\n\nAdditionally, there are electromechanical meters, like prepaid water meters and automatic meter reading meters. The latter integrates an electronic measurement component and a LCD with a mechanical water meter. Mechanical water meters normally use a reed switch, hall or photoelectric coding register as the signal output. After processing by the microcontroller unit (MCU) in the electronic module, the data are transmitted to the LCD or output to an information management system.\n\nWater meters are generally owned, read and maintained by a public water provider such as a city, rural water association or private water company. In some cases an owner of a mobile home park, apartment complex or commercial building may be billed by a utility based on the reading of one meter, with the costs shared among the tenants based on some sort of key (size of flat, number of inhabitants or by separately tracking the water consumption of each unit in what is called submetering).\n\nThis type of water meter is most often used in residential and small commercial applications and homes. Displacement meters are commonly referred to as Positive Displacement, or \"PD\" meters. Two common types are oscillating piston meters and nutating disk meters. Either method relies on the water to physically displace the moving measuring element in direct proportion to the amount of water that passes through the meter. The piston or disk moves a magnet that drives the register.\n\nPD meters are generally very accurate at the low-to-moderate flow rates typical of residential and small commercial users, and commonly range in size from 5/8\" to 2\". Because displacement meters require that all water flows through the meter to \"push\" the measuring element, they generally are not practical in large commercial applications requiring high flow rates or low pressure loss. PD meters normally have a built-in strainer to protect the measuring element from rocks or other debris that could stop or break the measuring element. PD meters normally have bronze, brass or plastic bodies with internal measuring chambers made of molded plastics and stainless steel.\n\nA velocity-type meter measures the velocity of flow through a meter of a known internal capacity. The speed of the flow can then be converted into volume of flow to determine the usage. There are several types of meters that measure water flow velocity, including jet meters (single-jet and multi-jet), turbine meters, propeller meters and mag meters. Most velocity-based meters have an adjustment vane for calibrating the meter to the required accuracy.\n\nMulti-jet meters are very accurate in small sizes and are commonly used in ⅝\" to 2\" sizes for residential and small commercial users. Multi-jet meters use multiple ports surrounding an internal chamber to create multiple jets of water against an impeller, whose rotation speed depends on the velocity of water flow. Multi-jets are very accurate at low flow rates, but there are no large size meters since they do not have the straight-through flow path needed for the high flow rates used in large pipe diameters. Multi-jet meters generally have an internal strainer element that can protect the jet ports from getting clogged. Multi-jet meters normally have bronze alloy bodies or outer casings, with internal measuring parts made from modern thermoplastics and stainless steel.\n\nTurbine meters are less accurate than displacement and jet meters at low flow rates, but the measuring element does not occupy or severely restrict the entire path of flow. The flow direction is generally straight through the meter, allowing for higher flow rates and less pressure loss than displacement-type meters. They are the meter of choice for large commercial users, fire protection and as master meters for the water distribution system. Strainers are generally required to be installed in front of the meter to protect the measuring element from gravel or other debris that could enter the water distribution system. Turbine meters are generally available for 1-½\" to 12\" or higher pipe sizes. Turbine meter bodies are commonly made of bronze, cast iron or ductile iron. Internal turbine elements can be plastic or non-corrosive metal alloys. They are accurate in normal working conditions but are greatly affected by the flow profile and fluid conditions.\n\n\nA compound meter is used where high flow rates are necessary, but where at times there are also smaller rates of flow that need to be accurately measured. Compound meters have two measuring elements and a check valve to regulate flow between them. At high flow rates, water is normally diverted primarily or completely to the high flow element. The high flow element is typically a turbine meter. When flow rates drop to where the high flow element cannot measure accurately, a check valve closes to divert water to a smaller element that can measure the lower flow rates accurately. The low flow element is typically a multi-jet or PD meter. By adding the values registered by the high and low elements, the utility has a record of the total consumption of water flowing through the meter.\n\nMagnetic flow meters, commonly referred to as \"mag meters\", are technically a velocity-type water meter, except that they use electromagnetic properties to determine the water flow velocity, rather than the mechanical means used by jet and turbine meters. Mag meters use the physics principle of Faraday's law of induction for measurement, and require AC or DC electricity from a power line or battery to operate the electromagnets. Since mag meters have no mechanical measuring element, they normally have the advantage of being able to measure flow in either direction, and use electronics for measuring and totalizing the flow. Mag meters can also be useful for measuring raw (untreated/unfiltered) water and waste-water, since there is no mechanical measuring element to get clogged or damaged by debris flowing through the meter. Strainers are not required with mag meters, since there is no measuring element in the stream of flow that could be damaged. Since stray electrical energy flowing through the flow tube can cause inaccurate readings, most mag meters are installed with either grounding rings or grounding electrodes to divert stray electricity away from the electrodes used to measure the flow inside the flow tube.\n\nUltrasonic water meters use one or more ultrasonic transducer to send ultrasonic sound waves through the fluid to determine the velocity of the water. Since the cross-sectional area of the meter body is a fixed and known value, when the velocity of water is detected, the volume of water passing through the meter can be calculated with very high accuracy. Because water density changes with temperature, most ultrasonic water meters also measure the water temperature as a component of the volume calculation.\n\nThere are 2 primary ultrasonic measurement technologies used in water metering:\n\n\nUltrasonic meters may either be of flow-through or \"clamp-on\" design. Flow-through designs are those where the water passes directly through the meter, and are typically found in residential or commercial applications. Clamp-on designs are generally used for larger diameters where the sensors are mounted to the exterior of pipes, etc.\n\nUltrasonic water meters are typically very accurate, with residential meters capable of measuring down to 0.01 gallons or 0.001 cubic feet. In addition they have wide flow measurement ranges, require little maintenance and have long lifespans due to the lack of internal mechanical components to wear out. While relatively new to the American water utility market, ultrasonic meters have been used in commercial applications for many years and are becoming widely accepted due to their advantages over traditional mechanical designs.\n\nMeters can be prepaid or postpaid, depending on the payment method. Most mechanical type water meters are of the postpaid type, as are electromagnetic and ultrasonic meters. With prepaid water meters the user purchases and prepays for a given amount of water from a vending station. The amount of water credited is entered on media such as an IC or RF type card. The main difference is whether the card needs a contact with the processing part of the prepaid water meter. In some areas a prepaid water meter uses a keypad as the interface for inputting the water credit.\n\nThere are several types of registers on water meters. A standard register normally has a dial similar to a clock, with gradations around the perimeter to indicate the measuring unit and the amount of water used, if less than the lowest digit in a display similar to the odometer wheels in a car, their sum being the total volume used. Modern registers are normally driven by a magnetic coupling between a magnet in the measuring chamber attached to the measuring element and another attached to the bottom of the register. Gears in the register convert the motion of the measuring element to the proper usage increment for display on the sweep hand and the odometer-style wheels. Many registers also have a leak detector. This is a small visible disk or hand that is geared closer to the rotation speed of the drive magnet, so that very small flows that would be visually undetectable on the regular sweep hand can be seen.\n\nWith Automatic Meter Reading, manufacturers have developed pulse or encoder registers to produce electronic output for radio transmitters, reading storage devices, and data logging devices. Pulse meters send a digital or analog electronic pulse to a recording device. Encoder registers have an electronic means permitting an external device to interrogate the register to obtain either the position of the wheels or a stored electronic reading. Frequent transmissions of consumption data can be used to give smart meter functionality.\n\nThere are also some specialized types of registers such as meters with an LCD instead of mechanical wheels, and registers to output data or pulses to a variety of recording and controller devices. For industrial applications, output is often 4-20 mA analog for recording or controlling different flow rates in addition to totalization.\n\nDifferent size meters indicate different resolutions of the reading. One rotation of the sweep hand may be equivalent to 10 gallons or to 1,000 gallons (1 to 100 ft., 0.1 to 10 m). If one rotation of the hand represents 10 gallons, the meter has a 10-gallon sweep. Sometimes the last number(s) of the wheel display are non-rotating or printed on the dial face. The fixed zero number(s) are represented by the position of the rotating sweep hand. For example, if one rotation of the hand is 10 gallons, the sweep hand is on 7, and the wheel display shows 123456 plus a fixed zero, the actual total usage would be 1,234,567 gallons.\n\nIn the United States most utilities bill only to the nearest 100 or 1,000 gallons (10 to 100 ft., 1 to 10 m), and often only read the leftmost 4 or 5 numbers on the display wheels. Using the above example, they would read and bill 1,234, rounding to 1,234,000 gallons based on a 1,000 gallon billing resolution. The most common rounding for a particular size meter is often indicated by differently coloured number wheels, the ones ignored being black, and the ones used for billing being white.\n\nWater metering is common for residential and commercial drinking water supply in many countries, as well as for industrial self-supply with water. However, it is less common in irrigated agriculture, which is the major water user worldwide. Water metering is also uncommon for piped drinking water supply in rural areas and small towns, although there are examples of successful metering in rural areas in developing countries, such as in El Salvador.\n\nMetering of water supplied by utilities to residential, commercial and industrial users is common in most developed countries, except for the United Kingdom where only about 38% of users are metered. In some developing countries metering is very common, such as in Chile where it stands at 96%, while in others it still remains low, such as in Argentina.\n\nThe percentage of residential water metering in selected cities in developing countries is as follows:\n\n\nNearly two-thirds of OECD countries meter more than 90% of single-family houses. A few are also expanding their metering of apartments (e.g., France and Germany).\n\nThe benefits of metering are that:\n\n\nThe costs of metering include:\n\n\nWhile the cost of purchasing residential meters is low, the total life cycle costs of metering are high. For example, retrofitting flats in large buildings with meters for every flat can involve major and thus costly plumbing work.\n\nProblems associated with metering arise particularly in the case of intermittent supply, which is common in many developing countries. Sudden changes in pressure can damage meters to the extent that many meters in cities in developing countries are not functional. Also, some types of meters become less accurate as they age, and under-registering consumption leads to lower revenues if defective meters are not regularly replaced. Many types of meters also register air flows, which can lead to over-registration of consumption, especially in systems with intermittent supply, when water supply is re-established and the incoming water pushes air through the meters.\n\nThere is disagreement as to the effect of metering and water pricing on water consumption. The price elasticity of metered water demand varies greatly depending on local conditions. The effect of volumetric water pricing on consumption tends to be higher if the water bill represents a significant portion of household expenditures. There is evidence from the UK that there is an instant drop in consumption of some 10% when meters are installed, although in most instances consumption isn't directly measured prior to meter installation, so the benefits are uncertain. Whilst metered water users in the UK do use less than unmetered users, in most areas metering is not compulsory, so the metered customers are a self-selecting group. There is also concern that water metering could be socially regressive, as householders on low incomes are less able to invest in water efficiency measures and may experience water poverty (defined as when a household spends more than 3% of net income on water and sewage services). In Hamburg, Germany, domestic water consumption for metered flats (112 liter/capita/day) was 18% lower than for unmetered flats (137 liter/capita/day) in 1992.\n\n\n\n"}
