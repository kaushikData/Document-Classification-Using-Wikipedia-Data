{"id": "42368558", "url": "https://en.wikipedia.org/wiki?curid=42368558", "title": "Agios Pharmaceuticals", "text": "Agios Pharmaceuticals\n\nAgios Pharmaceuticals Inc. is a public American pharmaceutical company focused on developing small-molecule anti-cancer therapeutics targeting cancer cell metabolism via the growth factor pathway. Among the proteins under investigation by the company are IDH1 and IDH2. The company was founded in 2008 (or 2007) by Lewis Cantley, Tak Mak and Craig Thompson. Agios is a Delaware corporation headquartered in Cambridge, Massachusetts. The company tendered an initial public offering in July 2013.\n\nIn 2012, Agios was named among the defendants in a lawsuit against one of its founders, Craig Thompson, alleging that Thompson used research illegally taken from the Abramson Family Cancer Research Institute in research at Agios.\n\nIn May 2016, the company announced it would launch partnership with Celgene, developing metabolic immuno-oncology therapies and licensing AG-221 as well as AG-881 to Celgene, potentially garnering Agios $120 million in drug licensing payments.\n\nIn April 2017, the company raised $250 million in a new stock offering in anticipation of FDA approval for its first cancer drug.\n\nIn December 2017, the company filed a new drug application, or NDA, with the U.S. Food and Drug Administration, or FDA, for Ivosidenib for the treatment of adult patients with relapsed or refractory acute myeloid\nleukemia, or R/R AML with an IDH1 mutation.\n\n, Agios' CEO was David Schenkein.\n\nAgios was established as a private company and converted to a public company with its initial public offering in July 2013 and subsequent listing on NASDAQ.\n"}
{"id": "50966074", "url": "https://en.wikipedia.org/wiki?curid=50966074", "title": "Andrew Provence", "text": "Andrew Provence\n\nAndrew Clark Provence (born March 8, 1961) is a former American football defensive tackle who played five seasons with the Atlanta Falcons of the National Football League (NFL). He was drafted by the Falcons in the third round of the 1983 NFL Draft. He played college football at South Carolina. Provence was also a member of the Denver Broncos.\n\nProvence was a three-year starter at Benedictine Military School in Savannah, Georgia. He was named to the Savannah News-Press All-City team in 1977 and 1978. He also earned All-State honors in 1978. Provence was inducted into the Greater Savannah Athletic Hall of Fame in 1995.\n\nProvence played for the South Carolina Gamecocks of the University of South Carolina from 1979 to 1982. He recorded ten sacks his senior year in 1982, setting the school's single season sack record. He led the Gamecocks in tackles in 1981 and 1982. Provence was named an All-American by \"The Sporting News\" while garnering Associated Press Third Team All-American and Gannett News Service Second Team All-American accolades in 1982. He also played in the Senior Bowl after his senior year. He recorded 401 total tackles, 35.0 tackles for loss and 26.0 sacks during his college career. Provence was named to South Carolina’s Modern Era All-Time Team. He was inducted into the University of South Carolina Athletic Hall of Fame in 2010. He was named to the SEC Football Legends Class of 2010.\n\nProvence was selected by the Atlanta Falcons with the 75th pick in the 1983 NFL Draft. He played in 70 games, starting sixteen, for the team from 1983 to 1987 and accumulated five career sacks. He was named to the NFL All-Rookie Team by the Pro Football Writers of America.\n\nProvence was traded to the Denver Broncos in May 1988 for a tenth round pick in the 1989 NFL Draft. He was placed on injured reserve on September 1, 1988, after tearing the connective tissue on his left foot during practice on August 31, 1988. He re-signed with the Broncos in July 1989. Provence was placed on injured reserve in August 1989. He was on injured reserve when the Broncos lost to the San Francisco 49ers by a score of 55–10 in Super Bowl XXIV on January 28, 1990.\n\nProvence has worked as a mental health counselor since August 1990, months after retiring from the NFL. He also has a master’s degree in professional counseling from Liberty University and is an ordained minister. \n\n"}
{"id": "304566", "url": "https://en.wikipedia.org/wiki?curid=304566", "title": "Avolition", "text": "Avolition\n\nAvolition, as a symptom of various forms of psychopathology, is the decrease in the motivation to initiate and perform self-directed purposeful activities. Such activities that appear to be neglected usually include routine activities, including hobbies, going to work and/or school, and most notably, engaging in social activities. A person experiencing avolition may stay at home for long periods of time, rather than seeking out work or peer relations.\n\nPeople with avolition often want to complete certain tasks but lack the ability to initiate behaviours necessary to complete them. Avolition is most commonly seen as a symptom of some other disorder, but might be considered a primary clinical disturbance of itself (or as a coexisting second disorder) related to disorders of diminished motivation. In 2006, avolition was identified as a negative symptom of schizophrenia by the National Institute of Mental Health (NIMH), and has been observed in patients with bipolar disorder as well as resulting from trauma.\n\nAvolition is sometimes mistaken for other, similar symptoms also affecting motivation, such as aboulia, anhedonia and asociality, or strong general disinterest. For example, aboulia is also a restriction in motivation and initiation, but characterized by an inability to set goals or make decisions and considered a disorder of diminished motivation. In order to provide effective treatment, the underlying cause of avolition (if any) has to be identified and it is important to properly differentiate it from other symptoms, even though they might reflect similar aspects of mental illness.\n\nImplications from avolition often result in social deficits. Not being able to initiate and perform purposeful activities can have many implications for a person with avolition. By disrupting interactions with both familiar and unfamiliar people, it jeopardizes the patient's social relations. When part of a severe mental illness, avolition has been reported, in first person accounts, to lead to physical and mental inability to both initiate and maintain relationships, as well as work, eat, drink or even sleep.\n\nClinically, it may be difficult to engage an individual experiencing avolition in active participation of psychotherapy. Patients are also faced with the stresses of coping with and accepting a mental illness and the stigma that often accompanies such a diagnosis and its symptoms. Regarding schizophrenia, the American Psychiatric Association reported in 2013 that there currently are \"no treatments with proven efficacy for primary negative symptoms\" (such as avolition). Together with schizophrenia's chronic nature, such facts added to the outlook of never getting well, might further implicate feelings of hopelessness and similar in patients as well as their friends and family.\n\nAlthough medication is the first-line treatment for most psychiatric disorders, it does not always improve every aspect of a patient's life, and for the negative symptoms in schizophrenia, the responses to anti-psychotics are less favourable than for positive symptoms. As a result, psychotherapy might be an alternative for the treatment of these symptoms, even if medication has a good effect on other manifestations of the disorder.\n\nCognitive behavioural therapy (CBT), is the kind of psychotherapy that shows most promise in treating avolition (and other negative symptoms of schizophrenia), but more research is needed in the area. CBT focuses on understanding how thoughts and feelings influence behaviour, in order to help individuals develop methods and strategies to better handle the implications of their disorder. Some research suggests that CBT focusing on social skills and practice of interpersonal situations, like job interviews, seeing a doctor (to discuss medication, for example), or interacting with friends and co-workers, as well as seemingly simple things like riding a bus, might reduce negative symptoms of schizophrenia and be beneficial to patients with avolition.\n\nOther forms of psychotherapy might also complement the role of medication and help patients, their families, and friends to work through emotional and other challenges of living with a chronic psychological disorder, including avolition.\n\n"}
{"id": "24648206", "url": "https://en.wikipedia.org/wiki?curid=24648206", "title": "Bosworth fracture", "text": "Bosworth fracture\n\nThe Bosworth fracture is a rare fracture of the distal fibula with an associated fixed posterior dislocation of the proximal fibular fragment which becomes trapped behind the posterior tibial tubercle. The injury is caused by severe external rotation of the ankle. The ankle remains externally rotated after the injury, making interpretation of X-rays difficult which can lead to misdiagnosis and incorrect treatment. The injury is most commonly treated by open reduction internal fixation as closed reduction is made difficult by the entrapment of the fibula behind the tibia.\n\nThe entrapment of an intact fibula behind the tibia was described by Ashhurst and Bromer in 1922, who attributed the description of the mechanism of injury to Huguier's 1848 publication. The injury involving fibular fracture with posterior dislocation was described by David M. Bosworth in 1947.\n"}
{"id": "15639937", "url": "https://en.wikipedia.org/wiki?curid=15639937", "title": "Breast International Group", "text": "Breast International Group\n\nThe Breast International Group (BIG)-aisbl is a non-profit organisation for academic breast cancer research groups from around the world, with its headquarters at the Jules Bordet Institute in Brussels, Belgium.\n\nBIG facilitates breast cancer research at international level by stimulating cooperation between its members and other academic networks, and collaborating with, but working independently from, the pharmaceutical industry. Such large-scale cooperation is crucial to make significant advances in breast cancer research, reduce the wasteful duplication of effort, and optimally serve those affected by the disease.\n\nFounded by leading European opinion leaders in 1996, BIG now constitutes a network of 50 groups based in Europe, Canada, Latin America, Asia and Australasia. These research entities are tied to approximately 3000 specialised hospitals and research centres worldwide. More than 30 clinical trials are run or are under development under the BIG umbrella. BIG also works closely with the US National Cancer Institute (NCI) and the North American Breast Cancer Groups (NABCG), so that together they act as a strong integrating force in the breast cancer research arena.\n\nInternational collaboration makes it possible to conduct studies that would not be possible for a single research group or network to carry out on its own, especially as treatments become increasingly targeted. Combining efforts makes it possible to quickly enrol large numbers of patients, or to share data and knowledge and efficiently answer important scientific questions. Faster results mean faster direct benefits to patients.\n\nBIG to date has been mainly focused on large adjuvant trials looking at questions related to optimal chemo-, hormono- and biological therapies, or to special groups of patients, such as the young or the elderly. BIG trials incorporate a substantial translational research component and emphasise the collection and banking of biological specimens for future research. However, BIG also provides a discussion forum for prevention trials and studies in advanced breast cancer, and it launched a biomarkers and drug development programme centred on neo-adjuvant studies, NeoBIG.\n\nWhile nearing the end (in 2011) of its formal support by the European Commission under Framework Programme VI, the TRANSBIG consortium, managed by the BIG Headquarters, will leave a lasting legacy in the form of a biological materials bank (“biobank”). Extremely valuable for translational research, this will allow researchers from around the world to access to genomic data and biospecimens. Moreover, many of the TRANSBIG committees and structures are being adapted to expand their expertise across BIG, benefitting the association in the longer term.\n\n\n"}
{"id": "662195", "url": "https://en.wikipedia.org/wiki?curid=662195", "title": "Buck v. Bell", "text": "Buck v. Bell\n\nBuck v. Bell, 274 U.S. 200 (1927), is a decision of the United States Supreme Court, written by Justice Oliver Wendell Holmes, Jr., in which the Court ruled that a state statute permitting compulsory sterilization of the unfit, including the intellectually disabled, \"for the protection and health of the state\" did not violate the Due Process clause of the Fourteenth Amendment to the United States Constitution. The Supreme Court has never expressly overturned \"Buck v. Bell\".\n\nThe concept of \"eugenics\" had been put forward in 1883 by Francis Galton, who also coined the name. The trend first became popular in the United States, and found proponents in Europe by the start of the 20th century; 42 of the 58 research papers presented at the First International Congress of Eugenics held in London in 1912, were from American scientists. Indiana passed the first eugenic sterilization statute (1907), but it was legally flawed. To remedy this situation, Harry Laughlin of the Eugenics Record Office (ERO) at the Cold Spring Harbor Laboratory, designed a model eugenic law that was reviewed by legal experts. In 1924 the Commonwealth of Virginia adopted a statute authorizing the compulsory sterilization of the intellectually disabled for the purpose of eugenics. This 1924 statute was closely based on Laughlin's model.\nLooking to determine if the new law would pass a legal challenge, on September 10, 1924 Dr. Albert Sidney Priddy, superintendent of the Virginia State Colony for Epileptics and Feebleminded, filed a petition to his Board of Directors to sterilize Carrie Buck, an 18-year-old patient at his institution who he claimed had a mental age of 9. Priddy maintained that Buck represented a genetic threat to society. According to Priddy, Buck's 52-year-old mother possessed a mental age of 8 and had a record of prostitution and immorality. She had three children without good knowledge of their paternity. Carrie, one of these children, had been adopted and attended school for five years, reaching the level of sixth grade. However, according to Priddy, she had eventually proved to be \"incorrigible\" and eventually gave birth to an illegitimate child. Her adopted family had committed her to the State Colony as \"feeble-minded\", no longer feeling capable of caring for her. It was later discovered that Carrie's pregnancy was not caused by any \"immorality\" on her own part. In the summer of 1923, while her adoptive mother was away \"on account of some illness,\" her adoptive mother's nephew raped Carrie, and Carrie's later commitment has been seen as an attempt by the family to save their reputation.\n\nWhile the litigation was making its way through the court system, Priddy died and his successor, Dr. John Hendren Bell, took up the case. The Board of Directors issued an order for the sterilization of Buck, and her guardian appealed the case to the Circuit Court of Amherst County, which sustained the decision of the Board. The case then moved to the Supreme Court of Appeals of Virginia.\n\nThe appellate court sustained the sterilization law as compliant with both the state and federal constitutions, and it then went to the United States Supreme Court. Buck and her guardian contended that the due process clause guarantees all adults the right to procreate which was being violated. They also made the argument that the Equal Protection Clause in the 14th Amendment was being violated since not all similarly situated people were being treated the same. The sterilization law was only for the \"feeble-minded\" at certain state institutions and made no mention of other state institutions or those who were not in an institution.\n\nOn May 2, 1927, in an 8–1 decision, the Court accepted that she, her mother and her daughter were \"feeble-minded\" and \"promiscuous,\" and that it was in the state's interest to have her sterilized. The ruling legitimized Virginia's sterilization procedures until they were repealed in 1974.\n\nThe ruling was written by Oliver Wendell Holmes, Jr. In support of his argument that the interest of \"public welfare\" outweighed the interest of individuals in their bodily integrity, he argued:\n\nHolmes concluded his argument by declaring that \"Three generations of imbeciles are enough\". The sole dissenter in the court, Justice Pierce Butler, a devout Catholic, did not write a dissenting opinion.\n\nCarrie Buck was operated upon, receiving a compulsory salpingectomy (a form of tubal ligation). She was later paroled from the institution as a domestic worker to a family in Bland, Virginia. She was an avid reader until her death in 1983. Her daughter Vivian had been pronounced \"feeble minded\" after a cursory examination by ERO field worker Dr. Arthur Estabrook. According to his report, Vivian \"showed backwardness\", thus the \"three generations\" of the majority opinion. It is worth noting that the child did very well in school for the two years that she attended (she died of complications from measles in 1932), even being listed on her school's honor roll in April 1931.\n\nHistorian Paul A. Lombardo argued in 1985 that Buck was not \"feeble-minded\" at all, but that she had been put away to hide her rape, perpetrated by the nephew of her adoptive mother. He also asserted that Buck's lawyer, Irving Whitehead, poorly argued her case, failed to call important witnesses, and was remarked by commentators to often not know what side he was on. It is now thought that this was not because of incompetence, but deliberate. Whitehead had close connections to the counsel for the institution and to Priddy. Whitehead was a member of the governing board of the state institution in which Buck resided, had personally authorized Priddy's sterilization requests, and was a strong supporter of eugenic sterilization.\n\nThe effect of \"Buck v. Bell\" was to legitimize eugenic sterilization laws in the United States as a whole. While many states already had sterilization laws on their books, their use was erratic and effects practically non-existent in every state except for California. After \"Buck v. Bell\", dozens of states added new sterilization statutes, or updated their constitutionally non-functional ones already enacted, with statutes which more closely mirrored the Virginia statute upheld by the Court.\n\nThe Virginia statute which the ruling of \"Buck v. Bell\" supported was designed in part by the eugenicist Harry H. Laughlin, superintendent of Charles Benedict Davenport's Eugenics Record Office in Cold Spring Harbor, New York. Laughlin had, a few years previously, conducted a number of studies on the enforcement of sterilization legislation throughout the country and had concluded that the reason for their lack of use was primarily that the physicians who would order the sterilizations were afraid of prosecution by patients whom they operated upon. Laughlin saw the need to create a \"Model Law\" which could withstand a test of constitutional scrutiny, clearing the way for future sterilization operations. Adolf Hitler closely modelled his Law for the Prevention of Hereditarily Diseased Offspring on Laughlin's \"Model Law\". The Third Reich held Laughlin in such regard that they arranged for him to receive an honorary doctorate from Heidelberg University in 1936. At the Nuremberg trials after World War II, Nazi doctors explicitly cited Holmes's opinion in Buck v. Bell as part of their defense.\n\nSterilization rates under eugenic laws in the United States climbed from 1927 until \"Skinner v. Oklahoma\", 316 U.S. 535 (1942). While \"Skinner v. Oklahoma\" did not specifically overturn \"Buck v. Bell\", it created enough of a legal quandary to discourage many sterilizations. By 1963, sterilization laws were almost wholly out of use, though some remained officially on the books for many years. Language referring to eugenics was removed from Virginia's sterilization law, and the current law, passed in 1988 and amended in 2013, only authorizes the voluntary sterilization of those 18 and older, after the patient has given written consent and the doctor has informed the patient of the \"consequences\" as well as \"alternative methods of contraception\".\n\nThe story of Carrie Buck's sterilization and the court case was made into a television drama in 1994, \"\". It was also referred to in 1934's sensational film \"Tomorrow's Children\", and was covered in the October 2018 \"American Experience\" documentary \"The Eugenics Crusade\".\n\nAlthough this opinion and eugenics remain controversial, the decision in this case still stands. \"Buck v. Bell\" was cited as a precedent by the opinion of the court (part VIII) in \"Roe v. Wade\", but not in support of abortion rights. To the contrary, Justice Blackmun quoted it to justify that the constitutional right to abortion is not unlimited.\n\nIn the 1996 case of \"Fieger v. Thomas\", the United States Court of Appeals for the Sixth Circuit both recognized and criticized \"Buck v. Bell\" by writing \"as Justice Holmes pointed out in the only part of \"Buck v. Bell\" that remains unrepudiated, a claim of a violation of the Equal Protection Clause based upon selective enforcement 'is the usual last resort of constitutional arguments.'\" In 2001, the United States Court of Appeals for the Eighth Circuit cited \"Buck v. Bell\" to protect the constitutional rights of a woman coerced into sterilization without procedural due process. The court stated that error and abuse will result if the state does not follow the procedural requirements, established by \"Buck v. Bell\", for performing an involuntary sterilization.\n\n\n"}
{"id": "5667", "url": "https://en.wikipedia.org/wiki?curid=5667", "title": "Chlorine", "text": "Chlorine\n\nChlorine is a chemical element with symbol Cl and atomic number 17. The second-lightest of the halogens, it appears between fluorine and bromine in the periodic table and its properties are mostly intermediate between them. Chlorine is a yellow-green gas at room temperature. It is an extremely reactive element and a strong oxidising agent: among the elements, it has the highest electron affinity and the third-highest electronegativity, behind only oxygen and fluorine.\n\nThe most common compound of chlorine, sodium chloride (common salt), has been known since ancient times. Around 1630, chlorine gas was first synthesised in a chemical reaction, but not recognised as a fundamentally important substance. Carl Wilhelm Scheele wrote a description of chlorine gas in 1774, supposing it to be an oxide of a new element. In 1809, chemists suggested that the gas might be a pure element, and this was confirmed by Sir Humphry Davy in 1810, who named it from based on its colour.\n\nBecause of its great reactivity, all chlorine in the Earth's crust is in the form of ionic chloride compounds, which includes table salt. It is the second-most abundant halogen (after fluorine) and twenty-first most abundant chemical element in Earth's crust. These crustal deposits are nevertheless dwarfed by the huge reserves of chloride in seawater.\n\nElemental chlorine is commercially produced from brine by electrolysis. The high oxidising potential of elemental chlorine led to the development of commercial bleaches and disinfectants, and a reagent for many processes in the chemical industry. Chlorine is used in the manufacture of a wide range of consumer products, about two-thirds of them organic chemicals such as polyvinyl chloride, and many intermediates for the production of plastics and other end products which do not contain the element. As a common disinfectant, elemental chlorine and chlorine-generating compounds are used more directly in swimming pools to keep them clean and sanitary. Elemental chlorine at high concentrations is extremely dangerous and poisonous for all living organisms, and was used in World War I as the first gaseous chemical warfare agent.\n\nIn the form of chloride ions, chlorine is necessary to all known species of life. Other types of chlorine compounds are rare in living organisms, and artificially produced chlorinated organics range from inert to toxic. In the upper atmosphere, chlorine-containing organic molecules such as chlorofluorocarbons have been implicated in ozone depletion. Small quantities of elemental chlorine are generated by oxidation of chloride to hypochlorite in neutrophils as part of the immune response against bacteria.\n\nThe most common compound of chlorine, sodium chloride, has been known since ancient times; archaeologists have found evidence that rock salt was used as early as 3000 BC and brine as early as 6000 BC. Its importance in food was very well known in classical antiquity and was sometimes used as payment for services for Roman generals and military tribunes. Elemental chlorine was probably first isolated around 1200 with the discovery of \"aqua regia\" and its ability to dissolve gold, since chlorine gas is one of the products of this reaction: it was however not recognised as a new substance. Around 1630, chlorine was recognized as a gas by the Flemish chemist and physician Jan Baptist van Helmont.\n\nThe element was first studied in detail in 1774 by Swedish chemist Carl Wilhelm Scheele, and he is credited with the discovery. Scheele produced chlorine by reacting MnO (as the mineral pyrolusite) with HCl:\n\nScheele observed several of the properties of chlorine: the bleaching effect on litmus, the deadly effect on insects, the yellow-green color, and the smell similar to aqua regia. He called it \"dephlogisticated muriatic acid air\" since it is a gas (then called \"airs\") and it came from hydrochloric acid (then known as \"muriatic acid\"). He failed to establish chlorine as an element.\n\nCommon chemical theory at that time held that an acid is a compound that contains oxygen (remnants of this survive in the German and Dutch names of oxygen: \"sauerstoff\" or \"zuurstof\", both translating into English as \"acid substance\"), so a number of chemists, including Claude Berthollet, suggested that Scheele's \"dephlogisticated muriatic acid air\" must be a combination of oxygen and the yet undiscovered element, \"muriaticum\".\n\nIn 1809, Joseph Louis Gay-Lussac and Louis-Jacques Thénard tried to decompose \"dephlogisticated muriatic acid air\" by reacting it with charcoal to release the free element \"muriaticum\" (and carbon dioxide). They did not succeed and published a report in which they considered the possibility that \"dephlogisticated muriatic acid air\" is an element, but were not convinced.\n\nIn 1810, Sir Humphry Davy tried the same experiment again, and concluded that the substance was an element, and not a compound. He announced his results to the Royal Society on 15 November that year. At that time, he named this new element \"chlorine\", from the Greek word χλωρος (\"chlōros\"), meaning green-yellow. The name \"halogen\", meaning \"salt producer\", was originally used for chlorine in 1811 by Johann Salomo Christoph Schweigger. This term was later used as a generic term to describe all the elements in the chlorine family (fluorine, bromine, iodine), after a suggestion by Jöns Jakob Berzelius in 1826. In 1823, Michael Faraday liquefied chlorine for the first time, and demonstrated that what was then known as \"solid chlorine\" had a structure of chlorine hydrate (Cl·HO).\n\nChlorine gas was first used by French chemist Claude Berthollet to bleach textiles in 1785. Modern bleaches resulted from further work by Berthollet, who first produced sodium hypochlorite in 1789 in his laboratory in the town of Javel (now part of Paris, France), by passing chlorine gas through a solution of sodium carbonate. The resulting liquid, known as \"Eau de Javel\" (\"Javel water\"), was a weak solution of sodium hypochlorite. This process was not very efficient, and alternative production methods were sought. Scottish chemist and industrialist Charles Tennant first produced a solution of calcium hypochlorite (\"chlorinated lime\"), then solid calcium hypochlorite (bleaching powder). These compounds produced low levels of elemental chlorine and could be more efficiently transported than sodium hypochlorite, which remained as dilute solutions because when purified to eliminate water, it became a dangerously powerful and unstable oxidizer. Near the end of the nineteenth century, E. S. Smith patented a method of sodium hypochlorite production involving electrolysis of brine to produce sodium hydroxide and chlorine gas, which then mixed to form sodium hypochlorite. This is known as the chloralkali process, first introduced on an industrial scale in 1892, and now the source of most elemental chlorine and sodium hydroxide. In 1884 Chemischen Fabrik Griesheim of Germany developed another chloralkali process which entered commercial production in 1888.\n\nElemental chlorine solutions dissolved in chemically basic water (sodium and calcium hypochlorite) were first used as anti-putrefaction agents and disinfectants in the 1820s, in France, long before the establishment of the germ theory of disease. This practice was pioneered by Antoine-Germain Labarraque, who adapted Berthollet's \"Javel water\" bleach and other chlorine preparations (for a more complete history, see below). Elemental chlorine has since served a continuous function in topical antisepsis (wound irrigation solutions and the like) and public sanitation, particularly in swimming and drinking water.\n\nChlorine gas was first used as a weapon on April 22, 1915, at Ypres by the German Army. The effect on the allies was devastating because the existing gas masks were difficult to deploy and had not been broadly distributed.\n\nChlorine is the second halogen, being a nonmetal in group 17 of the periodic table. Its properties are thus similar to fluorine, bromine, and iodine, and are largely intermediate between those of the first two. Chlorine has the electron configuration [Ne]3s3p, with the seven electrons in the third and outermost shell acting as its valence electrons. Like all halogens, it is thus one electron short of a full octet, and is hence a strong oxidising agent, reacting with many elements in order to complete its outer shell. Corresponding to periodic trends, it is intermediate in electronegativity between fluorine and bromine (F: 3.98, Cl: 3.16, Br: 2.96, I: 2.66), and is less reactive than fluorine and more reactive than bromine. It is also a weaker oxidising agent than fluorine, but a stronger one than bromine. Conversely, the chloride ion is a weaker reducing agent than bromide, but a stronger one than fluoride. It is intermediate in atomic radius between fluorine and bromine, and this leads to many of its atomic properties similarly continuing the trend from iodine to bromine upward, such as first ionisation energy, electron affinity, enthalpy of dissociation of the X molecule (X = Cl, Br, I), ionic radius, and X–X bond length. (Fluorine is anomalous due to its small size.)\n\nAll four stable halogens experience intermolecular van der Waals forces of attraction, and their strength increases together with the number of electrons among all homonuclear diatomic halogen molecules. Thus, the melting and boiling points of chlorine are intermediate between those of fluorine and bromine: chlorine melts at −101.0 °C and boils at −34.0 °C. As a result of the increasing molecular weight of the halogens down the group, the density and heats of fusion and vaporisation of chlorine are again intermediate between those of bromine and fluorine, although all their heats of vaporisation are fairly low (leading to high volatility) thanks to their diatomic molecular structure. The halogens darken in colour as the group is descended: thus, while fluorine is a pale yellow gas, chlorine is distinctly yellow-green. This trend occurs because the wavelengths of visible light absorbed by the halogens increase down the group. Specifically, the colour of a halogen, such as chlorine, results from the electron transition between the highest occupied antibonding \"π\" molecular orbital and the lowest vacant antibonding \"σ\" molecular orbital. The colour fades at low temperatures, so that solid chlorine at −195 °C is almost colourless.\n\nLike solid bromine and iodine, solid chlorine crystallises in the orthorhombic crystal system, in a layered lattice of Cl molecules. The Cl–Cl distance is 198 pm (close to the gaseous Cl–Cl distance of 199 pm) and the Cl···Cl distance between molecules is 332 pm within a layer and 382 pm between layers (compare the van der Waals radius of chlorine, 180 pm). This structure means that chlorine is a very poor conductor of electricity, and indeed its conductivity is so low as to be practically unmeasurable.\n\nChlorine has two stable isotopes, Cl and Cl. These are its only two natural isotopes occurring in quantity, with Cl making up 76% of natural chlorine and Cl making up the remaining 24%. Both are synthesised in stars in the oxygen-burning and silicon-burning processes. Both have nuclear spin 3/2+ and thus may be used for nuclear magnetic resonance, although the spin magnitude being greater than 1/2 results in non-spherical nuclear charge distribution and thus resonance broadening as a result of a nonzero nuclear quadrupole moment and resultant quadrupolar relaxation. The other chlorine isotopes are all radioactive, with half-lives too short to occur in nature primordially. Of these, the most commonly used in the laboratory are Cl (\"t\" = 3.0×10 y) and Cl (\"t\" = 37.2 min), which may be produced from the neutron activation of natural chlorine.\n\nThe most stable chlorine radioisotope is Cl. The primary decay mode of isotopes lighter than Cl is electron capture to isotopes of sulfur; that of isotopes heavier than Cl is beta decay to isotopes of argon; and Cl may decay by either mode to stable S or Ar. Cl occurs in trace quantities in nature as a cosmogenic nuclide in a ratio of about (7–10) × 10 to 1 with stable chlorine isotopes: it is produced in the atmosphere by spallation of Ar by interactions with cosmic ray protons. In the top meter of the lithosphere, Cl is generated primarily by thermal neutron activation of Cl and spallation of K and Ca. In the subsurface environment, muon capture by Ca becomes more important as a way to generate Cl.\n\nChlorine is intermediate in reactivity between fluorine and bromine, and is one of the most reactive elements. Chlorine is a weaker oxidising agent than fluorine but a stronger one than bromine or iodine. This can be seen from the standard electrode potentials of the X/X couples (F, +2.866 V; Cl, +1.395 V; Br, +1.087 V; I, +0.615 V; At, approximately +0.3 V). However, this trend is not shown in the bond energies because fluorine is singular due to its small size, low polarisability, and lack of low-lying d-orbitals available for bonding (which chlorine has). As another difference, chlorine has a significant chemistry in positive oxidation states while fluorine does not. Chlorination often leads to higher oxidation states than bromination or iodination but lower oxidation states to fluorination. Chlorine tends to react with compounds including M–M, M–H, or M–C bonds to form M–Cl bonds.\n\nGiven that E°(O/HO) = +1.229 V, which is less than +1.395 V, it would be expected that chlorine should be able to oxidise water to oxygen and hydrochloric acid. However, the kinetics of this reaction are unfavorable, and there is also a bubble overpotential effect to consider, so that electrolysis of aqueous chloride solutions evolves chlorine gas and not oxygen gas, a fact that is very useful for the industrial production of chlorine.\n\nThe simplest chlorine compound is hydrogen chloride, HCl, a major chemical in industry as well as in the laboratory, both as a gas and dissolved in water as hydrochloric acid. It is often produced by burning hydrogen gas in chlorine gas, or as a byproduct of chlorinating hydrocarbons. Another approach is to treat sodium chloride with concentrated sulfuric acid to produce hydrochloric acid, also known as the \"salt-cake\" process:\nIn the laboratory, hydrogen chloride gas may be made by drying the acid with concentrated sulfuric acid. Deuterium chloride, DCl, may be produced by reacting benzoyl chloride with heavy water (DO).\n\nAt room temperature, hydrogen chloride is a colourless gas, like all the hydrogen halides apart from hydrogen fluoride, since hydrogen cannot form strong hydrogen bonds to the larger electronegative chlorine atom; however, weak hydrogen bonding is present in solid crystalline hydrogen chloride at low temperatures, similar to the hydrogen fluoride structure, before disorder begins to prevail as the temperature is raised. Hydrochloric acid is a strong acid (p\"K\" = −7) because the hydrogen bonds to chlorine are too weak to inhibit dissociation. The HCl/HO system has many hydrates HCl·\"n\"HO for \"n\" = 1, 2, 3, 4, and 6. Beyond a 1:1 mixture of HCl and HO, the system separates completely into two separate liquid phases. Hydrochloric acid forms an azeotrope with boiling point 108.58 °C at 20.22 g HCl per 100 g solution; thus hydrochloric acid cannot be concentrated beyond this point by distillation.\n\nUnlike hydrogen fluoride, anhydrous liquid hydrogen chloride is difficult to work with as a solvent, because its boiling point is low, it has a small liquid range, its dielectric constant is low and it does not dissociate appreciably into HCl and ions – the latter, in any case, are much less stable than the bifluoride ions () due to the very weak hydrogen bonding between hydrogen and chlorine, though its salts with very large and weakly polarising cations such as Cs and (R = Me, Et, Bu) may still be isolated. Anhydrous hydrogen chloride is a poor solvent, only able to dissolve small molecular compounds such as nitrosyl chloride and phenol, or salts with very low lattice energies such as tetraalkylammonium halides. It readily protonates electrophiles containing lone-pairs or π bonds. Solvolysis, ligand replacement reactions, and oxidations are well-characterised in hydrogen chloride solution:\n\nNearly all elements in the periodic table form binary chlorides. The exceptions are decidedly in the minority and stem in each case from one of three causes: extreme inertness and reluctance to participate in chemical reactions (the noble gases, with the exception of xenon in the highly unstable XeCl and XeCl); extreme nuclear instability hampering chemical investigation before decay and transmutation (many of the heaviest elements beyond bismuth); and having an electronegativity higher than chlorine's (oxygen and fluorine) so that the resultant binary compounds are formally not chlorides but rather oxides or fluorides of chlorine.\n\nChlorination of metals with Cl usually leads to a higher oxidation state than bromination with Br when multiple oxidation states are available, such as in MoCl and MoBr. Chlorides can be made by reaction of an element or its oxide, hydroxide, or carbonate with hydrochloric acid, and then dehydrated by mildly high temperatures combined with either low pressure or anhydrous hydrogen chloride gas. These methods work best when the chloride product is stable to hydrolysis; otherwise, the possibilities include high-temperature oxidative chlorination of the element with chlorine or hydrogen chloride, high-temperature chlorination of a metal oxide or other halide by chlorine, a volatile metal chloride, carbon tetrachloride, or an organic chloride. For instance, zirconium dioxide reacts with chlorine at standard conditions to produce zirconium tetrachloride, and uranium trioxide reacts with hexachloropropene when heated under reflux to give uranium tetrachloride. The second example also involves a reduction in oxidation state, which can also be achieved by reducing a higher chloride using hydrogen or a metal as a reducing agent. This may also be achieved by thermal decomposition or disproportionation as follows:\n\nMost of the chlorides of the pre-transition metals (groups 1, 2, and 3, along with the lanthanides and actinides in the +2 and +3 oxidation states) are mostly ionic, while nonmetals tend to form covalent molecular chlorides, as do metals in high oxidation states from +3 and above. Silver chloride is very insoluble in water and is thus often used as a qualitative test for chlorine.\n\nAlthough dichlorine is a strong oxidising agent with a high first ionisation energy, it may be oxidised under extreme conditions to form the cation. This is very unstable and has only been characterised by its electronic band spectrum when produced in a low-pressure discharge tube. The yellow cation is more stable and may be produced as follows:\nThis reaction is conducted in the oxidising solvent arsenic pentafluoride. The trichloride anion, , has also been characterised; it is analogous to triiodide.\n\nThe three fluorides of chlorine form a subset of the interhalogen compounds, all of which are diamagnetic. Some cationic and anionic derivatives are known, such as , , , and ClF. Some pseudohalides of chlorine are also known, such as cyanogen chloride (ClCN, linear), chlorine cyanate (ClNCO), chlorine thiocyanate (ClSCN, unlike its oxygen counterpart), and chlorine azide (ClN).\n\nChlorine monofluoride (ClF) is extremely thermally stable, and is sold commercially in 500-gram steel lecture bottles. It is a colourless gas that melts at −155.6 °C and boils at −100.1 °C. It may be produced by the direction of its elements at 225 °C, though it must then be separated and purified from chlorine trifluoride and its reactants. Its properties are mostly intermediate between those of chlorine and fluorine. It will react with many metals and nonmetals from room temperature and above, fluorinating them and liberating chlorine. It will also act as a chlorofluorinating agent, adding chlorine and fluorine across a multiple bond or by oxidation: for example, it will attack carbon monoxide to form carbonyl chlorofluoride, COFCl. It will react analogously with hexafluoroacetone, (CF)CO, with a potassium fluoride catalyst to produce heptafluoroisopropyl hypochlorite, (CF)CFOCl; with nitriles RCN to produce RCFNCl; and with the sulfur oxides SO and SO to produce ClOSOF and ClSOF respectively. It will also react exothermically and violently with compounds containing –OH and –NH groups, such as water:\n\nChlorine trifluoride (ClF) is a volatile colourless molecular liquid which melts at −76.3 °C and boils at 11.8 °C. It may be formed by directly fluorinating gaseous chlorine or chlorine monofluoride at 200–300 °C. It is one of the most reactive known chemical compounds, reacting with many substances which in ordinary circumstances would be considered chemically inert, such as asbestos, concrete, and sand. It explodes on contact with water and most organic substances. The list of elements it sets on fire is diverse, containing hydrogen, potassium, phosphorus, arsenic, antimony, sulfur, selenium, tellurium, bromine, iodine, and powdered molybdenum, tungsten, rhodium, iridium, and iron. An impermeable fluoride layer is formed by sodium, magnesium, aluminium, zinc, tin, and silver, which may be removed by heating. When heated, even such noble metals as palladium, platinum, and gold are attacked and even the noble gases xenon and radon do not escape fluorination. Nickel containers are usually used due to that metal's great resistance to attack by chlorine trifluoride, stemming from the formation of an unreactive nickel fluoride layer. Its reaction with hydrazine to form hydrogen fluoride, nitrogen, and chlorine gases was used in experimental rocket motors, but has problems largely stemming from its extreme hypergolicity resulting in ignition without any measurable delay. For these reasons, it was used in bomb attacks during the Second World War by the Nazis. Today, it is mostly used in nuclear fuel processing, to oxidise uranium to uranium hexafluoride for its enriching and to separate it from plutonium. It can act as a fluoride ion donor or acceptor (Lewis base or acid), although it does not dissociate appreciably into and ions.\n\nChlorine pentafluoride (ClF) is made on a large scale by direct fluorination of chlorine with excess fluorine gas at 350 °C and 250 atm, and on a small scale by reacting metal chlorides with fluorine gas at 100–300 °C. It melts at −103 °C and boils at −13.1 °C. It is a very strong fluorinating agent, although it is still not as effective as chlorine trifluoride. Only a few specific stoichiometric reactions have been characterised. Arsenic pentafluoride and antimony pentafluoride form ionic adducts of the form [ClF][MF] (M = As, Sb) and water reacts vigorously as follows:\n\nThe product, chloryl fluoride, is one of the five known chlorine oxide fluorides. These range from the thermally unstable FClO to the chemically unreactive perchloryl fluoride (FClO), the other three being FClO, FClO, and FClO. All five behave similarly to the chlorine fluorides, both structurally and chemically, and may act as Lewis acids or bases by gaining or losing fluoride ions respectively or as very strong oxidising and fluorinating agents.\n\nThe chlorine oxides are well-studied in spite of their instability (all of them are endothermic compounds). They are important because they are produced when chlorofluorocarbons undergo photolysis in the upper atmosphere and cause the destruction of the ozone layer. None of them can be made from directly reacting the elements.\n\nDichlorine monoxide (ClO) is a brownish-yellow gas (red-brown when solid or liquid) which may be obtained by reacting chlorine gas with yellow mercury(II) oxide. It is very soluble in water, in which it is in equilibrium with hypochlorous acid (HOCl), which it is the anhydride of. It is thus an effective bleach and is mostly used to make hypochlorites. It explodes on heating or sparking or in the presence of ammonia gas.\n\nChlorine dioxide (ClO) was the first chlorine oxide to be discovered in 1811 by Humphry Davy. It is a yellow paramagnetic gas (deep-red as a solid or liquid), as expected from its having an odd number of electrons: it is stable towards dimerisation due to the delocalisation of the unpaired electron. It explodes above −40 °C as a liquid and under pressure as a gas and therefore must be made at low concentrations for wood-pulp bleaching and water treatment. It is usually prepared by reducing a chlorate as follows:\nIts production is thus intimately linked to the redox reactions of the chlorine oxoacids. It is a strong oxidising agent, reacting with sulfur, phosphorus, phosphorus halides, and potassium borohydride. It dissolves exothermically in water to form dark-green solutions that very slowly decompose in the dark. Crystalline clathrate hydrates ClO·\"n\"HO (\"n\" ≈ 6–10) separate out at low temperatures. However, in the presence of light, these solutions rapidly photodecompose to form a mixture of chloric and hydrochloric acids. Photolysis of individual ClO molecules result in the radicals ClO and ClOO, while at room temperature mostly chlorine, oxygen, and some ClO and ClO are produced. ClO is also produced when photolysing the solid at −78 °C: it is a dark brown solid that explodes below 0 °C. The ClO radical leads to the depletion of atmospheric ozone and is thus environmentally important as follows:\n\nChlorine perchlorate (ClOClO) is a pale yellow liquid that is less stable than ClO and decomposes at room temperature to form chlorine, oxygen, and dichlorine hexoxide (ClO). Chlorine perchlorate may also be considered a chlorine derivative of perchloric acid (HOClO), similar to the thermally unstable chlorine derivatives of other oxoacids: examples include chlorine nitrate (ClONO, vigorously reactive and explosive), and chlorine fluorosulfate (ClOSOF, more stable but still moisture-sensitive and highly reactive). Dichlorine hexoxide is a dark-red liquid that freezes to form a solid which turns yellow at −180 °C: it is usually made by reaction of chlorine dioxide with oxygen. Despite attempts to rationalise it as the dimer of ClO, it reacts more as though it were chloryl perchlorate, [ClO][ClO], which has been confirmed to be the correct structure of the solid. It hydrolyses in water to give a mixture of chloric and perchloric acids: the analogous reaction with anhydrous hydrogen fluoride does not proceed to completion.\n\nDichlorine heptoxide (ClO) is the anhydride of perchloric acid (HClO) and can readily be obtained from it by dehydrating it with phosphoric acid at −10 °C and then distilling the product at −35 °C and 1 mmHg. It is a shock-sensitive, colourless oily liquid. It is the least reactive of the chlorine oxides, being the only one to not set organic materials on fire at room temperature. It may be dissolved in water to regenerate perchloric acid or in aqueous alkalis to regenerate perchlorates. However, it thermally decomposes explosively by breaking one of the central Cl–O bonds, producing the radicals ClO and ClO which immediately decompose to the elements through intermediate oxides.\n\nChlorine forms four oxoacids: hypochlorous acid (HOCl), chlorous acid (HOClO), chloric acid (HOClO), and perchloric acid (HOClO). As can be seen from the redox potentials given in the adjacent table, chlorine is much more stable towards disproportionation in acidic solutions than in alkaline solutions:\n\nThe hypochlorite ions also disproportionate further to produce chloride and chlorate (3 ClO 2 Cl + ) but this reaction is quite slow at temperatures below 70 °C in spite of the very favourable equilibrium constant of 10. The chlorate ions may themselves disproportionate to form chloride and perchlorate (4 Cl + 3 ) but this is still very slow even at 100 °C despite the very favourable equilibrium constant of 10. The rates of reaction for the chlorine oxyanions increases as the oxidation state of chlorine decreases. The strengths of the chlorine oxyacids increase very quickly as the oxidation state of chlorine increases due to the increasing delocalisation of charge over more and more oxygen atoms in their conjugate bases.\n\nMost of the chlorine oxoacids may be produced by exploiting these disproportionation reactions. Hypochlorous acid (HOCl) is highly reactive and quite unstable; its salts are mostly used for their bleaching and sterilising abilities. They are very strong oxidising agents, transferring an oxygen atom to most inorganic species. Chlorous acid (HOClO) is even more unstable and cannot be isolated or concentrated without decomposition: it is known from the decomposition of aqueous chlorine dioxide. However, sodium chlorite is a stable salt and is useful for bleaching and stripping textiles, as an oxidising agent, and as a source of chlorine dioxide. Chloric acid (HOClO) is a strong acid that is quite stable in cold water up to 30% concentration, but on warming gives chlorine and chlorine dioxide. Evaporation under reduced pressure allows it to be concentrated further to about 40%, but then it decomposes to perchloric acid, chlorine, oxygen, water, and chlorine dioxide. Its most important salt is sodium chlorate, mostly used to make chlorine dioxide to bleach paper pulp. The decomposition of chlorate to chloride and oxygen is a common way to produce oxygen in the laboratory on a small scale. Chloride and chlorate may comproportionate to form chlorine as follows:\n\nPerchlorates and perchloric acid (HOClO) are the most stable oxo-compounds of chlorine, in keeping with the fact that chlorine compounds are most stable when the chlorine atom is in its lowest (−1) or highest (+7) possible oxidation states. Perchloric acid and aqueous perchlorates are vigorous and sometimes violent oxidising agents when heated, in stark contrast to their mostly inactive nature at room temperature due to the high activation energies for these reactions for kinetic reasons. Perchlorates are made by electrolytically oxidising sodium chlorate, and perchloric acid is made by reacting anhydrous sodium perchlorate or barium perchlorate with concentrated hydrochloric acid, filtering away the chloride precipitated and distilling the filtrate to concentrate it. Anhydrous perchloric acid is a colourless mobile liquid that is sensitive to shock that explodes on contact with most organic compounds, sets hydrogen iodide and thionyl chloride on fire and even oxidises silver and gold. Although it is a weak ligand, weaker than water, a few compounds involving coordinated are known.\n\nLike the other carbon–halogen bonds, the C–Cl bond is a common functional group that forms part of core organic chemistry. Formally, compounds with this functional group may be considered organic derivatives of the chloride anion. Due to the difference of electronegativity between chlorine (3.16) and carbon (2.55), the carbon in a C–Cl bond is electron-deficient and thus electrophilic. Chlorination modifies the physical properties of hydrocarbons in several ways: chlorocarbons are typically denser than water due to the higher atomic weight of chlorine versus hydrogen, and aliphatic organochlorides are alkylating agents because chloride is a leaving group.\n\nAlkanes and aryl alkanes may be chlorinated under free radical conditions, with UV light. However, the extent of chlorination is difficult to control: the reaction is not regioselective and often results in a mixture of various isomers with different degrees of chlorination, though this may be permissible if the products are easily separated. Aryl chlorides may be prepared by the Friedel-Crafts halogenation, using chlorine and a Lewis acid catalyst. The haloform reaction, using chlorine and sodium hydroxide, is also able to generate alkyl halides from methyl ketones, and related compounds. Chlorine adds to the multiple bonds on alkenes and alkynes as well, giving di- or tetra-chloro compounds. However, due to the expense and reactivity of chlorine, organochlorine compounds are more commonly produced by using hydrogen chloride, or with chlorinating agents such as phosphorus pentachloride (PCl) or thionyl chloride (SOCl). The last is very convenient in the laboratory because all side products are gaseous and do not have to be distilled out.\n\nMany organochlorine compounds have been isolated from natural sources ranging from bacteria to humans. Chlorinated organic compounds are found in nearly every class of biomolecules including alkaloids, terpenes, amino acids, flavonoids, steroids, and fatty acids. Organochlorides, including dioxins, are produced in the high temperature environment of forest fires, and dioxins have been found in the preserved ashes of lightning-ignited fires that predate synthetic dioxins. In addition, a variety of simple chlorinated hydrocarbons including dichloromethane, chloroform, and carbon tetrachloride have been isolated from marine algae. A majority of the chloromethane in the environment is produced naturally by biological decomposition, forest fires, and volcanoes.\n\nSome types of organochlorides, though not all, have significant toxicity to plants or animals, including humans. Dioxins, produced when organic matter is burned in the presence of chlorine, and some insecticides, such as DDT, are persistent organic pollutants which pose dangers when they are released into the environment. For example, DDT, which was widely used to control insects in the mid 20th century, also accumulates in food chains, and causes reproductive problems (e.g., eggshell thinning) in certain bird species. Due to the ready homolytic fission of the C–Cl bond to create chlorine radicals in the upper atmosphere, chlorofluorocarbons have been phased out due to the harm they do to the ozone layer.\n\nChlorine is too reactive to occur as the free element in nature but is very abundant in the form of its chloride salts. It is the twentieth most abundant element in Earth's crust and makes up 126 parts per million of it, through the large deposits of chloride minerals, especially sodium chloride, that have been evaporated from water bodies. All of these pale in comparison to the reserves of chloride ions in seawater: smaller amounts at higher concentrations occur in some inland seas and underground brine wells, such as the Great Salt Lake in Utah and the Dead Sea in Israel.\n\nSmall batches of chlorine gas are prepared in the laboratory by combining hydrochloric acid and manganese dioxide, but the need rarely arises due to its ready availability. In industry, elemental chlorine is usually produced by the electrolysis of sodium chloride dissolved in water. This method, the chloralkali process industrialized in 1892, now provides most industrial chlorine gas. Along with chlorine, the method yields hydrogen gas and sodium hydroxide, which is the most valuable product. The process proceeds according to the following chemical equation:\n\nThe electrolysis of chloride solutions all proceed according to the following equations:\n\nIn diaphragm cell electrolysis, an asbestos (or polymer-fiber) diaphragm separates a cathode and an anode, preventing the chlorine forming at the anode from re-mixing with the sodium hydroxide and the hydrogen formed at the cathode. The salt solution (brine) is continuously fed to the anode compartment and flows through the diaphragm to the cathode compartment, where the caustic alkali is produced and the brine is partially depleted. Diaphragm methods produce dilute and slightly impure alkali, but they are not burdened with the problem of mercury disposal and they are more energy efficient.\n\nMembrane cell electrolysis employs permeable membrane as an ion exchanger. Saturated sodium (or potassium) chloride solution is passed through the anode compartment, leaving at a lower concentration. This method also produces very pure sodium (or potassium) hydroxide but has the disadvantage of requiring very pure brine at high concentrations.\n\nIn the Deacon process, hydrogen chloride recovered from the production of organochlorine compounds is recovered as chlorine. The process relies on oxidation using oxygen:\n\nThe reaction requires a catalyst. As introduced by Deacon, early catalysts were based on copper. Commercial processes, such as the Mitsui MT-Chlorine Process, have switched to chromium and ruthenium-based catalysts. The chlorine produced is available in cylinders from sizes ranging from 450 g to 70 kg, as well as drums (865 kg), tank wagons (15 tonnes on roads; 27–90 tonnes by rail), and barges (600–1200 tonnes).\n\nSodium chloride is by a huge margin the most common chlorine compound, and it is the main source of chlorine and hydrochloric acid for the enormous chlorine-chemicals industry today. About 15000 chlorine-containing compounds are commercially traded, including such diverse compounds as chlorinated methanes and ethanes, vinyl chloride and its polymer polyvinyl chloride (PVC), aluminium trichloride for catalysis, the chlorides of magnesium, titanium, zirconium, and hafnium which are the precursors for producing the pure elements, and so on.\n\nQuantitatively, of all elemental chlorine produced, about 63% is used in the manufacture of organic compounds, and 18% in the manufacture of inorganic chlorine compounds. About 15,000 chlorine compounds are used commercially. The remaining 19% of chlorine produced is used for bleaches and disinfection products. The most significant of organic compounds in terms of production volume are 1,2-dichloroethane and vinyl chloride, intermediates in the production of PVC. Other particularly important organochlorines are methyl chloride, methylene chloride, chloroform, vinylidene chloride, trichloroethylene, perchloroethylene, allyl chloride, epichlorohydrin, chlorobenzene, dichlorobenzenes, and trichlorobenzenes. The major inorganic compounds include HCl, ClO, HOCl, NaClO, chlorinated isocyanurates, AlCl, SiCl, SnCl, PCl, PCl, POCl, AsCl, SbCl, SbCl, BiCl, SCl, SCl, SOCI, ClF, ICl, ICl, TiCl, TiCl, MoCl, FeCl, ZnCl, and so on.\n\nIn France (as elsewhere), animal intestines were processed to make musical instrument strings, Goldbeater's skin and other products. This was done in \"gut factories\" (\"boyauderies\"), and it was an odiferous and unhealthy process. In or about 1820, the Société d'encouragement pour l'industrie nationale offered a prize for the discovery of a method, chemical or mechanical, for separating the peritoneal membrane of animal intestines without putrefaction. The prize was won by Antoine-Germain Labarraque, a 44-year-old French chemist and pharmacist who had discovered that Berthollet's chlorinated bleaching solutions (\"Eau de Javel\") not only destroyed the smell of putrefaction of animal tissue decomposition, but also actually retarded the decomposition.\n\nLabarraque's research resulted in the use of chlorides and hypochlorites of lime (calcium hypochlorite) and of sodium (sodium hypochlorite) in the \"boyauderies.\" The same chemicals were found to be useful in the routine disinfection and deodorization of latrines, sewers, markets, abattoirs, anatomical theatres, and morgues. They were successful in hospitals, lazarets, prisons, infirmaries (both on land and at sea), magnaneries, stables, cattle-sheds, etc.; and they were beneficial during exhumations, embalming, outbreaks of epidemic disease, fever, and blackleg in cattle.\n\nLabarraque's chlorinated lime and soda solutions have been advocated since 1828 to prevent infection (called \"contagious infection\", presumed to be transmitted by \"miasmas\"), and to treat putrefaction of existing wounds, including septic wounds. In his 1828 work, Labarraque recommended that doctors breathe chlorine, wash their hands in chlorinated lime, and even sprinkle chlorinated lime about the patients' beds in cases of \"contagious infection\". In 1828, the contagion of infections was well known, even though the agency of the microbe was not discovered until more than half a century later.\n\nDuring the Paris cholera outbreak of 1832, large quantities of so-called \"chloride of lime\" were used to disinfect the capital. This was not simply modern calcium chloride, but chlorine gas dissolved in lime-water (dilute calcium hydroxide) to form calcium hypochlorite (chlorinated lime). Labarraque's discovery helped to remove the terrible stench of decay from hospitals and dissecting rooms, and by doing so, effectively deodorised the Latin Quarter of Paris. These \"putrid miasmas\" were thought by many to cause the spread of \"contagion\" and \"infection\" – both words used before the germ theory of infection. Chloride of lime was used for destroying odors and \"putrid matter\". One source claims chloride of lime was used by Dr. John Snow to disinfect water from the cholera-contaminated well that was feeding the Broad Street pump in 1854 London, though three other reputable sources that describe that famous cholera epidemic do not mention the incident. One reference makes it clear that chloride of lime was used to disinfect the offal and filth in the streets surrounding the Broad Street pump—a common practice in mid-nineteenth century England.\n\nPerhaps the most famous application of Labarraque's chlorine and chemical base solutions was in 1847, when Ignaz Semmelweis used chlorine-water (chlorine dissolved in pure water, which was cheaper than chlorinated lime solutions) to disinfect the hands of Austrian doctors, which Semmelweis noticed still carried the stench of decomposition from the dissection rooms to the patient examination rooms. Long before the germ theory of disease, Semmelweis theorized that \"cadaveric particles\" were transmitting decay from fresh medical cadavers to living patients, and he used the well-known \"Labarraque's solutions\" as the only known method to remove the smell of decay and tissue decomposition (which he found that soap did not). The solutions proved to be far more effective antiseptics than soap (Semmelweis was also aware of their greater efficacy, but not the reason), and this resulted in Semmelweis's celebrated success in stopping the transmission of childbed fever (\"puerperal fever\") in the maternity wards of Vienna General Hospital in Austria in 1847.\n\nMuch later, during World War I in 1916, a standardized and diluted modification of Labarraque's solution containing hypochlorite (0.5%) and boric acid as an acidic stabilizer, was developed by Henry Drysdale Dakin (who gave full credit to Labarraque's prior work in this area). Called Dakin's solution, the method of wound irrigation with chlorinated solutions allowed antiseptic treatment of a wide variety of open wounds, long before the modern antibiotic era. A modified version of this solution continues to be employed in wound irrigation in modern times, where it remains effective against bacteria that are resistant to multiple antibiotics (see Century Pharmaceuticals).\n\nThe first continuous application of chlorination to drinking U.S. water was installed in Jersey City, New Jersey in 1908. By 1918, the US Department of Treasury called for all drinking water to be disinfected with chlorine. Chlorine is presently an important chemical for water purification (such as in water treatment plants), in disinfectants, and in bleach. Even small water supplies are now routinely chlorinated.\n\nChlorine is usually used (in the form of hypochlorous acid) to kill bacteria and other microbes in drinking water supplies and public swimming pools. In most private swimming pools, chlorine itself is not used, but rather sodium hypochlorite, formed from chlorine and sodium hydroxide, or solid tablets of chlorinated isocyanurates. The drawback of using chlorine in swimming pools is that the chlorine reacts with the proteins in human hair and skin. The distinctive 'chlorine aroma' associated with swimming pools is not the result of chlorine itself, but of chloramine, a chemical compound produced by the reaction of free dissolved chlorine with amines in organic substances. As a disinfectant in water, chlorine is more than three times as effective against \"Escherichia coli\" as bromine, and more than six times as effective as iodine.\n\nIt is often impractical to store and use poisonous chlorine gas for water treatment, so alternative methods of adding chlorine are used. These include hypochlorite solutions, which gradually release chlorine into the water, and compounds like sodium dichloro-s-triazinetrione (dihydrate or anhydrous), sometimes referred to as \"dichlor\", and trichloro-s-triazinetrione, sometimes referred to as \"trichlor\". These compounds are stable while solid and may be used in powdered, granular, or tablet form. When added in small amounts to pool water or industrial water systems, the chlorine atoms hydrolyze from the rest of the molecule forming hypochlorous acid (HOCl), which acts as a general biocide, killing germs, micro-organisms, algae, and so on.\n\nChlorine gas, also known as bertholite, was first used as a weapon in World War I by Germany on April 22, 1915 in the Second Battle of Ypres. As described by the soldiers, it had the distinctive smell of a mixture of pepper and pineapple. It also tasted metallic and stung the back of the throat and chest. Chlorine reacts with water in the mucosa of the lungs to form hydrochloric acid, destructive to living tissue and potentially lethal. Human respiratory systems can be protected from chlorine gas by gas masks with activated charcoal or other filters, which makes chlorine gas much less lethal than other chemical weapons. It was pioneered by a German scientist later to be a Nobel laureate, Fritz Haber of the Kaiser Wilhelm Institute in Berlin, in collaboration with the German chemical conglomerate IG Farben, which developed methods for discharging chlorine gas against an entrenched enemy. After its first use, both sides in the conflict used chlorine as a chemical weapon, but it was soon replaced by the more deadly phosgene and mustard gas.\n\nChlorine gas was also used during the Iraq War in Anbar Province in 2007, with insurgents packing truck bombs with mortar shells and chlorine tanks. The attacks killed two people from the explosives and sickened more than 350. Most of the deaths were caused by the force of the explosions rather than the effects of chlorine since the toxic gas is readily dispersed and diluted in the atmosphere by the blast. In some bombings, over a hundred civilians were hospitalized due to breathing difficulties. The Iraqi authorities tightened security for elemental chlorine, which is essential for providing safe drinking water to the population.\n\nOn 24 October 2014, it was reported that the Islamic State of Iraq and the Levant had used chlorine gas in the town of Duluiyah, Iraq. Laboratory analysis of clothing and soil samples confirmed the use of chlorine gas against Kurdish Peshmerga Forces in a vehicle-borne improvised explosive device attack on 23 January 2015 at the Highway 47 Kiske Junction near Mosul.\n\nThe Syrian government allegedly uses chlorine as a chemical weapon, often dropping it in barrel bombs, but sometimes also in rockets.\n\nThe chloride anion is an essential nutrient for metabolism. Chlorine is needed for the production of hydrochloric acid in the stomach and in cellular pump functions. The main dietary source is table salt, or sodium chloride. Overly low or high concentrations of chloride in the blood are examples of electrolyte disturbances. Hypochloremia (having too little chloride) rarely occurs in the absence of other abnormalities. It is sometimes associated with hypoventilation. It can be associated with chronic respiratory acidosis. Hyperchloremia (having too much chloride) usually does not produce symptoms. When symptoms do occur, they tend to resemble those of hypernatremia (having too much sodium). Reduction in blood chloride leads to cerebral dehydration; symptoms are most often caused by rapid rehydration which results in cerebral edema. Hyperchloremia can affect oxygen transport.\n\nChlorine is a toxic gas that attacks the respiratory system, eyes, and skin. Because it is denser than air, it tends to accumulate at the bottom of poorly ventilated spaces. Chlorine gas is a strong oxidizer, which may react with flammable materials.\n\nChlorine is detectable with measuring devices in concentrations as low as 0.2 parts per million (ppm), and by smell at 3 ppm. Coughing and vomiting may occur at 30 ppm and lung damage at 60 ppm. About 1000 ppm can be fatal after a few deep breaths of the gas. The IDLH (immediately dangerous to life and health) concentration is 10 ppm. Breathing lower concentrations can aggravate the respiratory system and exposure to the gas can irritate the eyes. The toxicity of chlorine comes from its oxidizing power. When chlorine is inhaled at concentrations greater than 30 ppm, it reacts with water and cellular fluid, producing hydrochloric acid (HCl) and hypochlorous acid (HClO).\n\nWhen used at specified levels for water disinfection, the reaction of chlorine with water is not a major concern for human health. Other materials present in the water may generate disinfection by-products that are associated with negative effects on human health.\n\nIn the United States, the Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for elemental chlorine at 1 ppm, or 3 mg/m. The National Institute for Occupational Safety and Health has designated a recommended exposure limit of 0.5 ppm over 15 minutes.\n\nIn the home, accidents occur when hypochlorite bleach solutions come into contact with certain acidic drain-cleaners to produce chlorine gas. Hypochlorite bleach (a popular laundry additive) combined with ammonia (another popular laundry additive) produces chloramines, another toxic group of chemicals.\n\nChlorine is widely used for purifying water, especially potable water supplies and water used in swimming pools. Several catastrophic collapses of swimming pool ceilings have occurred from chlorine-induced stress corrosion cracking of stainless steel suspension rods. Some polymers are also sensitive to attack, including acetal resin and polybutene. Both materials were used in hot and cold water domestic plumbing, and stress corrosion cracking caused widespread failures in the US in the 1980s and 1990s. The adjacent picture shows a fractured acetal joint in a water supply system. The cracks started at injection molding defects in the joint and slowly grew until the part failed. The fracture surface shows iron and calcium salts that were deposited in the leaking joint from the water supply before failure.\n\nThe element iron can combine with chlorine at high temperatures in a strong exothermic reaction, creating a \"chlorine-iron fire\". Chlorine-iron fires are a risk in chemical process plants, where much of the pipework that carries chlorine gas is made of steel.\n\n\n"}
{"id": "1620725", "url": "https://en.wikipedia.org/wiki?curid=1620725", "title": "Dental degree", "text": "Dental degree\n\nThere are a number of professional degrees in dentistry offered by dental schools in various countries around the world.\n\nDegrees acknowledged worldwide across North America, Europe, Africa and Asia are:\n\nThere are a number of post-graduate degrees in dentistry as well.\nThere are the number of degrees in dentistry recognised In Australia and New Zealand \n- Bachelor of Dental Surgery\n- Bachelor of Dental Science \n- Doctor of Dental Medicine \n- Doctor of Dental Surgery\n- Doctor of Dental Science \n- Master of Dentistry ( only Griffith University)\n\nIn some universities, especially in the U.S., some post-graduate programs award Certificate only.\n\nIn Commonwealth countries, the Royal Colleges of dentistry (or Faculty of Dentistry of the College) awards post-nominals upon completion of a series of examinations.\nIn the U.S., most dental specialists attain Board Certification (Diplomate Status) by completing a series of written and oral examinations with the appropriate Boards. e.g. Diplomate, American Board of Periodontics.\n\nEach fully qualifies the holder to practice dentistry in at least the jurisdiction in which the degree was presented, assuming local and federal government licensure requirements are met.\n\nIn addition to general dentistry, there are about 9 recognized dental specialties in the US, Canada, India and Australia. To become a specialist requires one to train in a residency or advanced graduate training program. Once residency is completed, the doctor is granted a certificate of specialty training. Many specialty programs have optional or required advanced degrees such as a master's degree: (MS, MSc, MDS, MSD, MDSc, MMSc, MPhil, or MDent), doctoral degree: (DClinDent, DChDent, DMSc, PhD), or medical degree: (MD/MBBS specific to maxillofacial surgery and sometimes oral medicine).\n\n\nThe following are not currently recognized dental specialties in the US:\n\nDentists who have completed accredited specialty training programs in these fields are designated registrable (U.S. \"Board Eligible\") and warrant exclusive titles such as orthodontist, oral and maxillofacial surgeon, endodontist, pedodontist, periodontist, or prosthodontist upon satisfying certain local (U.S. \"Board Certified\"), (Australia/NZ: \"FRACDS\"), or (Canada: \"FRCD(C)\") registry requirements.\n\nAustralia has nine dental schools:\n\n(*) indicates new university dental programs that have opened up to aim at increasing the number of rural dental students entering and to return to rural practice. Traditional \"sandstone\" universities have been Sydney, Melbourne, Queensland, Adelaide and Western Australia.\n\nSydney (as of 2001), Melbourne (as of 2010) and Western Australia (as of 2013) have switched to 4-year graduate program that require a previous bachelor's degree for admission.\n\nPost-graduate training is available in all dental specialties. Degrees awarded used to be Master of Dental Surgery/Science (MDS/MDSc), but lately have changed to Doctorate in Clinical Dentistry (DClinDent).\n\nNew Zealand has only one dental school:\n\nThe Faculty of Dentistry awards Bachelor of Dental Surgery (BDS) and Master of Community Dentistry (MComDent) for public health & community dentistry, and Doctorate in Clinical Dentistry (DClinDent) for the other dental specialties.\n\nThe body responsible for registering dental practitioners is the Dental Council of New Zealand (DCNZ).\n\nBoth Australia and New Zealand recognize the educational and professional qualifications and grant professional licenses via reciprocity identical to the United States and Canada.\n\nThe United Kingdom General Dental Council had been recognizing the Australian and New Zealand dental qualification as registrable degree until 2000. Graduates who have applied for dental license registration in the United Kingdom now have to sit the Overseas Registration Exam (ORE), a three-part examination.\n\nAustralia and Canada have a reciprocal accreditation agreement which allows graduates of Canadian or Australian dental schools to register in either country. However, this only applies to the graduates of 2011 class and does not apply to the previous years graduates.\n\nRoyal Australasian College of Dental Surgeons (RACDS) is a post-graduate body that focuses on post-graduate training of general practitioners and specialist dentists. Additional post-graduate qualifications can be obtained through the College after the candidate has completed the Primary Examination (basic science examination in Anatomy, Histology, Physiology, Biochemistry, Pathology and Microbiology) and the Final Examination (clinical subjects in dentistry). After the successful completion of the examinations and meeting the College requirements, the candidate is awarded the title of Fellow of Royal Australasian College of Dental Surgeons (FRACDS). For the dental specialists, the exam pathway is similar (Primary Examinations) and then clinical/oral examinations just prior to completing the specialist training leads to the award of the title Member of Royal Australasian College of Dental Surgeons in Special Field Stream (MRACDS(SFS)). For the busy GP dentists, MRACDS in general stream is also available.\n\nThere are ten approved dental schools in Canada:\n\nSeveral Universities in Canada offer the DDS degree, including the University of Toronto, the University of Western Ontario, the University of Alberta, and Dalhousie University, while the remaining Canadian dental schools offer the Doctor of Dental Medicine degree to their graduates.\n\nAdditional qualifications can be obtained through the Royal College of Dentists of Canada (RCDC), which administers examinations for qualified dental specialists as part of the dentistry profession in Canada. The current examinations are known as the \"National Dental Specialty Examination\" (NDSE). Successful completion may lead to \"Fellowship\" in the College (FRCD(C)) and may be used for provincial registration purposes.\n\nCanada has a reciprocal accreditation agreement with Australia, Ireland, and the United States which recognizes the dental training of graduates of Canadian dental schools. Obtaining licensure to work in any of the 3 other countries will often require additional steps, such as successfully completing national board exams and fulfilling requirements of local governing bodies.\n\nIn Finland, education in dentistry is through a 5.5-year \"Licenciate of Dental Medicine\" (DMD or DDS) course, which is offered after high school graduation. Application is by a national combined dental and medical school entry examination. As of 2011, dentistry is provided by Faculties of Medicine in 4 universities:\n\n\n1st phase of training begins with a unified 2-year pre-clinical training for dentists and physicians. Problem-based learning (PBL) is employed depending on university. 3rd year autumn consists of clinico-theoretical phase in pathology, genetics, radiology and public health and is partially combined with physicians' 2nd phase. 3rd phase clinical training lasts for the remaining 3 years and includes periods of being on call at University Central Hospital Trauma Centre, Clinic of Oral and Maxillofacial Diseases and at the Children's clinic. Candidates who successfully complete the 4th year of training qualify for a paid summer rotation in a Community health center of their choice. Annual intake of dentists into Faculties of Medicine is a national total 160 students.\n\nPh.D. research is strongly encouraged alongside post graduate training. Post graduate training is available in all 4 universities and lasts an additional 3–6 years.Starting in 2014, the University of Helsinki introduced a new doctoral training system. In this new system all doctoral candidates belong to a doctoral programme within a doctoral school. FINDOS Helsinki - Doctoral Programme in Oral Sciences - is a programme in the Doctoral School in Health Sciences.\nThere are 11 post graduate programs:\n\nClinical dentistry:\nDiagnostic dentistry:\n\nOther:\n\nIn India, training in dentistry is through a 5 -year BDS (Bachelor of Dental Surgery) course, which includes 4 years of study followed by one year of internship. As of 2010, there were a total of 291 colleges (39 run by the government and 252 in the private sector) offering dental education. This amounts to an annual intake of 23,690 graduates.\n\nPost graduate training is for three years in the concerned speciality. Master of Dental Surgery (MDS) is offered in the following subjects -\n\nDental education in India is regulated by the Dental Council of India.\n\nIn Israel there are two dental schools, The Hebrew University-Hadassah School of Dental Medicine in Jerusalem, founded by the Alpha Omega Fraternity and The Tel Aviv University School of Dental Medicine in Tel Aviv. The two schools have 6-year program and provide Doctor of Dental Medicine (DMD) degrees. In last decades, the students eligible to Bachelor of Medical Sciences (BMedSc) degree after the first three years of training.\n\nIn the United States, at least three years of undergraduate education are required in order to be admitted to a dental school; however, most dental schools require at least a bachelor's degree. There is no mandatory course of study as an undergraduate other than completing the requisite \"pre-dental\" courses, which generally includes one year of general biology, chemistry, organic chemistry, physics, English, and higher level mathematics such as statistics and calculus. Some dental schools have requirements that go beyond the basic requirements such as psychology, sociology, biochemistry, anatomy, physiology etc. The majority of pre-dental students major in a science but this is not required as some students elect to major in a non-science related field.\n\nIn addition to core prerequisites, the Dental Admission Test, a multiple choice standardized exam, is also required for potential dental students. The DAT is usually taken during the spring semester of one's junior year. The vast majority of dental schools require an interview before admissions can be granted. The interview is designed to evaluate the motivation, character, and personality of the applicant. It is often a crucial step in the admissions process.\n\nFor the 2009-2010 application cycle, 11,632 applicants applied for admission to dental schools in the United States. Just 4,067 were eventually accepted. The average dental school applicant entering the school year in 2009 had an overall GPA of 3.54 and a science GPA of 3.46. Additionally, their mean DAT Academic Average (AA) was 19.00 while their DAT Perceptual Ability Test (PAT) score was 19.40.\n\nDental school is four academic years in duration and is similar in format to medical school: two years of basic medical and dental sciences, followed by two years of clinical training (with continued didactic coursework). Before graduating, every dental student must successfully complete the National Board Dental Examination Part I and II (commonly referred to as NBDE I & II). The NBDE Part I is usually taken at the end of the second year after the majority of the didactic courses have been completed. The NBDE Part I covers Gross Anatomy, biochemistry, physiology, microbiology, pathology, and dental anatomy and occlusion. The NBDE Part II is usually taken during winter of the last year of dental school and consists of operative dentistry, pharmacology, endodontics, periodontics, oral surgery, pain control, prosthodontics, orthodontics, pedodontics, oral pathology, and radiology. NBDE Part I scores are Pass/Fail since 2012.\n\nAfter graduating, the vast majority of new dentists go directly into practice while a small, yet increasing, percentage of dentists apply to a residency program. Some residency programs train dentists in advanced general dentistry such as General Practice Residencies and Advanced Education in General Dentistry Residencies, commonly referred to as GPR and AEGD. Most GPR and AEGD programs are one year in duration but several are two years long or provide an optional second year. GPR programs are usually affiliated with a hospital and thus require the dentist to treat a wide variety of patients including trauma, critically ill, and medically compromised patients. Additionally, GPR programs require residents to rotate through various departments within the hospital, such as anesthesia, internal medicine, and emergency medicine, to name a few. AEGD programs are usually in a dental school setting where the focus is treating complex cases in a comprehensive manner.\n\nIn the United States the Doctor of Dental Surgery and Doctor of Dental Medicine are terminal professional doctorates which qualify a professional for licensure. The DDS and DMD degrees are considered equivalent. The American Dental Association specifies:\nHarvard University was the first dental school to award the DMD degree. Harvard only grants degrees in Latin, and the Latin translation of Doctor of Dental Surgery, \"Chirurgiae Dentium Doctoris\", did not share the \"DDS\" initials of the English term. \"The degree 'Scientiae Dentium Doctoris', which would leave the initials of DDS unchanged, was then considered, but was rejected on the ground that dentistry was not a science.\" (The word order in Latin is not fixed, only the inflections; \"Scientiae Dentium Doctoris\" = \"Doctoris Dentium Scientiae\".) A Latin scholar was consulted. It was finally decided that \"Medicinae Doctoris\" be modified with \"Dentariae\". This is how the DMD, or \"Doctor Medicinae Dentariae\" degree, was started. (The genitive inflection \"-is\" on \"Doctoris\" instead of the nominative \"Doctor\" simply reflects that the syntax on the diploma was \"the degree of Doctor of Dental Medicine\"; they are both correct.) The assertion that \"dentistry was not a science\" reflected the view that dental surgery was an art informed by science, not a science \"per se\"—notwithstanding that the scientific component of dentistry is today recognized in the Doctor of Dental Science (DDSc) degree.\n\nOther dental schools made the switch to this notation, and in 1989, 23 of the 66 North American dental schools awarded the DMD. There is no meaningful difference between the DMD and DDS degrees, and all dentists must meet the same national and regional certification standards in order to practice.\n\nSome other prominent dental schools which award the DMD degree are the Medical University of South Carolina, Augusta University (formerly Medical College of Georgia), University of Connecticut, University of Alabama at Birmingham, University of Louisville, University of Puerto Rico, Rutgers University, Tufts University, Oregon Health and Sciences University, University of Pennsylvania, University of Illinois at Chicago, Boston University, Temple University, Western University of Health Sciences, University of Pittsburgh, and University of Nevada, Las Vegas.\n\nThe United States Department of Education and the National Science Foundation do not include the DDS and DMD among the degrees that are equivalent to research doctorates.\n\nTo practice, a dentist must pass a licensing examination administered by an individual state or more commonly a region. There are a handful of states that maintain independent dental licensing examinations while the majority accept a regional board examination. The Northeast Regional Board (NERB), Western Regional Board (WREB), Central Regional Dental Testing Service (CRDTS), and Southern Regional Testing Agency (SRTA), Council of Interstate Testing Agencies (CITA) are the five regional testing agencies that administer licensing examinations. Once the examination is passed, the dentist may then apply to individual states that accept the regional board test passed. Each state requires prospective practitioners to pass an ethics/jurisprudence examination as well before a license is granted. To maintain one's dental license the doctor must complete Continuing Dental Education (CDE) courses periodically (usually annually). This promotes the continued exploration of knowledge. The amount of CE required varies from state to state but is generally 10-25 CE hours a year.\n\nThe completion of a dental degree can be followed by either an entrance into private practice, further postgraduate study and training, or research and academics.\n\nChina has many universities teaching dental degree both in undergraduate and postgraduate level. Though the Chinese system of education is not open and clear to outside world, universities have adapted the programes of American and European degrees. Under-graduate degree is Bachelor of Stomatology (口腔医学本科） and post graduate degree is Master of Stomatology (口腔医学硕士). There is variation in degree such as BDS, DDS, DMD for bachelor level. Recently, China has new name for its master degree as Master of Stomatological medicine. There are other branches of dentistry remaining the same as American universities.\n\nTraining in South Africa generally comprises a 5-year degree, with 1 year compulsory medical service / internship. The country has 5 universities with dental faculties:\n\nUntil 2003, Stellenbosch University offered the B.Ch.D degree. In 2004 the dental faculties of The University of the Western Cape and Stellenbosch University merged and moved to The University of the Western Cape, which is currently the largest dental school in Africa.\n\nSpecialisation is through one of the Universities as a Master of Dentistry, or through the \"College of Dentistry\" within the Colleges of Medicine of South Africa, with certifications offered in oral medicine and periodontics, orthodontics, and prosthodontics. Research degrees are the M.Sc.(Dent) / MDS and Ph.D.(Dent).\n\nMany universities award BDS (Bachelor of Dental Surgery) degrees, including The University of Sheffield, The University of Bristol, Barts and the London School of Medicine and Dentistry, The University of Birmingham, The University of Liverpool, The University of Manchester, The University of Glasgow, The University of Dundee, The University of Aberdeen, King's College London, the University of Cardiff, Newcastle University, Queen's University Belfast, The University of Central Lancashire and Peninsula College of Medicine and Dentistry.\n\nIn the Republic of Ireland, the University College Cork awards BDS degree and Trinity College Dublin awards the BDentSc degree.\n\nThe University of Leeds awards BChD and MChD (Bachelor/Master of Dental Surgery) degrees.\n\nThe Royal Colleges of Surgeons in England, Edinburgh, Glasgow, and Ireland award LDS (Licence/Licentiate in Dental Surgery) degrees.\n\nThe graduation in Dentistry is named here as Bachelor of Dental Surgery (BDS). At present there are three universities that have medical faculty that offer dental degree: The University of Dhaka, The University of Chittagong and the University of Rajshahi. These public universities have dental colleges and hospitals that may be publicly or privately funded, that offer education for the degree. The list of dentals school providing graduate education in Dentistry includes the following colleges:\n\nColleges affiliated to University of Dhaka:\n\nColleges affiliated to University of Chittagong: \n\nColleges affiliated to University of Rajshahi: \n\nAt present post graduation in specialized dentistry exist in main four clinical specialities:\n\nThe FCPS degree which require four years of residency training is awarded by Bangladesh College of Physician and Surgeons. It considered the most prestigious specialized Degree in Dentistry at present. There are also MS degree only in Oral and Maxillofacial Surgery from University of Dhaka. Very recently MS in dentistry was also started in Bangabandhu Sheikh Mujib Medical University (BSMMU), the first batch of which graduated in 2016.\n\n\n"}
{"id": "51391626", "url": "https://en.wikipedia.org/wiki?curid=51391626", "title": "Discretionary food", "text": "Discretionary food\n\nDiscretionary food is a term for foods and drinks not necessary to provide the nutrients the human body's needs, but that may add variety to a person's diet.\n\nAustralia's National Health and Medical Research Council describes discretionary foods as \"“foods and drinks not necessary to provide the nutrients the body needs, but that may add variety. However, many of these are high in saturated fats, sugars, salt and/or alcohol, and are therefore described as energy dense. They can be included sometimes in small amounts by those who are physically active, but are not a necessary part of the diet.”\"\n"}
{"id": "1702858", "url": "https://en.wikipedia.org/wiki?curid=1702858", "title": "Entoptic phenomenon", "text": "Entoptic phenomenon\n\nEntoptic phenomena (from Greek ἐντός \"within\" and ὀπτικός \"visual\") are visual effects whose source is within the eye itself. (Occasionally, these are called entopic phenomena, which is probably a typographical mistake.)\n\nIn Helmholtz's words; \"Under suitable conditions light falling on the eye may render visible certain objects within the eye itself. These perceptions are called \"entoptical\".\"\n\nEntoptic images have a physical basis in the image cast upon the retina. Hence, they are different from optical illusions, which are caused by the visual system and characterized by a visual percept that (loosely said) appears to differ from reality. Because entoptic images are caused by phenomena within the observer's own eye, they share one feature with optical illusions and hallucinations: the observer cannot share a direct and specific view of the phenomenon with others.\n\nHelmholtz commented on entoptic phenomena which could be seen easily by some observers, but could not be seen at all by others. This variance is not surprising because the specific aspects of the eye that produce these images are unique to each individual. Because of the variation between individuals, and the inability for two observers to share a nearly identical stimulus, these phenomena are unlike most visual sensations. They are also unlike most optical illusions which are produced by viewing a common stimulus. Yet, there is enough commonality between the main entoptic phenomena that their physical origin is now well understood.\n\nSome examples of entoptical effects include:\n\nA phenomenon that could be entoptical if the eyelashes are considered to be part of the eye is seeing light diffracted through the eyelashes. The phenomenon appears as one or more light disks crossed by dark blurry lines (the shadows of the lashes), each having fringes of spectral colour. The disk shape is given by the circular aperture of the pupil.\n\n"}
{"id": "55798286", "url": "https://en.wikipedia.org/wiki?curid=55798286", "title": "Evelina Betancourt-Anthony", "text": "Evelina Betancourt-Anthony\n\nEvelina Betancourt-Anthony (born 1950) is an Aruban-born Bonairean health administrator and politician. From 1 December 2015 she has served as the served as the Deputy Lieutenant Governor of the island and in the absence of the Island Governor fills the duties of that office.\n\nEvelina Anthony was born in 1950, in Aruba, where her father was working in the oil industry. Both of her parents were originally from Bonaire, as were her grandparents. When her father was laid off from the refinery, the entire family returned to Bonaire, where they still had a house. After her primary schooling, she attended Meer Uitgebreid Lager Onderwijs (MULO) for two years in Bonaire and in 1968 moved to the Netherlands to finished her secondary education. She attended Schoevers, in Amsterdam studying management and organization, but didn't have the money to pay for further schooling. Instead, she entered nursing where she could train, room and board were provided, and she received a small monthly stipend. She began her training at the hospital in Amsterdam-Noord, but stopped before she finished her training to take full time employment at the Argentinean State Shipping Company in Amsterdam to allow her fiancé to finish his education. \n\nThe couple married and upon her husband's graduation, Anthony moved with him to West Friesland. She had two children and did volunteer work. When her mother-in-law became ill, they returned to Bonaire. While her husband worked in government service as the Head of Finance, Anthony returned to school, taking law courses at the University of the Netherlands Antilles in Curaçao. After two years, she was offered a position in the administrative department at a health clinic in Nort Saliña. After two years, she took a government post in 1990 as a health project coordinator for the entire Netherlands Antilles, which required a lot of travel to various islands. In 1996, Anthony also became involved with the Association for Nursing and Care for the Elderly () being elected as chair of the Association a few months after her first meeting with the board. She and her first husband divorced, and Anthony remarried a Cuban name Betancourt. \n\nIn 1999, Anthony decided to enter politics and when her party won the elections, she was appointed as Deputy over Health Care. It was a time of change in the island, as before the police had operated the ambulance service and most major care required that patients be sent via boat to Curaçao. A supervisory board was established to take over the ambulance service, home health care, and provide training for health workers. In 2000, Anthony was appointed as a Minister in the government at The Hague and returned to the Netherlands for two years. She returned to Bonaire and resumed the chair of the Association, which had been renamed as the Mariadal Foundation (. Under the new system, the chair was a rotating position on the Board, and Anthony served at the facility until 2014.\n\nOn 1 December 2015, Anthony was appointed as the temporary acting Island Governor for Bonaire and the Kingdom of the Netherlands representative for Bonaire, Saba and Sint Eustatius for a period of one year, serving in his stead when Lieutenant Governor, Edison Rijna is away from the island. After that timeframe, her title was changed to Deputy Lieutenant Governor.\n\n"}
{"id": "21746736", "url": "https://en.wikipedia.org/wiki?curid=21746736", "title": "Familial cirrhosis", "text": "Familial cirrhosis\n\nFamilial cirrhosis is a form of liver disease that is inherited and the liver scarring is not caused by any obvious disease process. This type of cirrhosis is a keratin disease. Damage progresses until function becomes impaired.\n\nCurrent cirrhosis treatment is aimed at managing complications as well as chronic poor health related to liver damage. Treatments include abstinence from alcohol, nutritional supplement, identification of any identifiable disease process, management of portal hypertension, and liver transplantation.\n\nIt is associated with KRT8 and KRT18.\n"}
{"id": "10646", "url": "https://en.wikipedia.org/wiki?curid=10646", "title": "Food", "text": "Food\n\nFood is any substance consumed to provide nutritional support for an organism. It is usually of plant or animal origin, and contains essential nutrients, such as carbohydrates, fats, proteins, vitamins, or minerals. The substance is ingested by an organism and assimilated by the organism's cells to provide energy, maintain life, or stimulate growth.\n\nHistorically, humans secured food through two methods: hunting and gathering and agriculture. Today, the majority of the food energy required by the ever increasing population of the world is supplied by the food industry.\n\nFood safety and food security are monitored by agencies like the International Association for Food Protection, World Resources Institute, World Food Programme, Food and Agriculture Organization, and International Food Information Council. They address issues such as sustainability, biological diversity, climate change, nutritional economics, population growth, water supply, and access to food.\n\nThe right to food is a human right derived from the International Covenant on Economic, Social and Cultural Rights (ICESCR), recognizing the \"right to an adequate standard of living, including adequate food\", as well as the \"fundamental right to be free from hunger\".\nMost food has its origin in plants. Some food is obtained directly from plants; but even animals that are used as food sources are raised by feeding them food derived from plants. Cereal grain is a staple food that provides more food energy worldwide than any other type of crop. Corn (maize), wheat, and rice – in all of their varieties – account for 87% of all grain production worldwide. Most of the grain that is produced worldwide is fed to livestock.\n\nSome foods not from animal or plant sources include various edible fungi, especially mushrooms. Fungi and ambient bacteria are used in the preparation of fermented and pickled foods like leavened bread, alcoholic drinks, cheese, pickles, kombucha, and yogurt. Another example is blue-green algae such as Spirulina. Inorganic substances such as salt, baking soda and cream of tartar are used to preserve or chemically alter an ingredient.\n\nMany plants and plant parts are eaten as food and around 2,000 plant species are cultivated for food. Many of these plant species have several distinct cultivars.\n\nSeeds of plants are a good source of food for animals, including humans, because they contain the nutrients necessary for the plant's initial growth, including many healthful fats, such as omega fats. In fact, the majority of food consumed by human beings are seed-based foods. Edible seeds include cereals (corn, wheat, rice, et cetera), legumes (beans, peas, lentils, et cetera), and nuts. Oilseeds are often pressed to produce rich oils - sunflower, flaxseed, rapeseed (including canola oil), sesame, et cetera.\n\nSeeds are typically high in unsaturated fats and, in moderation, are considered a health food. However, not all seeds are edible. Large seeds, such as those from a lemon, pose a choking hazard, while seeds from cherries and apples contain cyanide which could be poisonous only if consumed in large volumes.\n\nFruits are the ripened ovaries of plants, including the seeds within. Many plants and animals have coevolved such that the fruits of the former are an attractive food source to the latter, because animals that eat the fruits may excrete the seeds some distance away. Fruits, therefore, make up a significant part of the diets of most cultures. Some botanical fruits, such as tomatoes, pumpkins, and eggplants, are eaten as vegetables. (For more information, see list of fruits.)\n\nVegetables are a second type of plant matter that is commonly eaten as food. These include root vegetables (potatoes and carrots), bulbs (onion family), leaf vegetables (spinach and lettuce), (bamboo shoots and asparagus), and (globe artichokes and broccoli and other vegetables such as cabbage or cauliflower).\n\nAnimals are used as food either directly or indirectly by the products they produce. Meat is an example of a direct product taken from an animal, which comes from muscle systems or from organs.\n\nFood products produced by animals include milk produced by mammary glands, which in many cultures is drunk or processed into dairy products (cheese, butter, etc.). In addition, birds and other animals lay eggs, which are often eaten, and bees produce honey, a reduced nectar from flowers, which is a popular sweetener in many cultures. Some cultures consume blood, sometimes in the form of blood sausage, as a thickener for sauces, or in a cured, salted form for times of food scarcity, and others use blood in stews such as jugged hare.\n\nSome cultures and people do not consume meat or animal food products for cultural, dietary, health, ethical, or ideological reasons. Vegetarians choose to forgo food from animal sources to varying degrees. Vegans do not consume any foods that are or contain ingredients from an animal source.\n\nAdulteration is a legal term meaning that a food product fails to meet the legal standards. One form of adulteration is an addition of another substance to a food item in order to increase the quantity of the food item in raw form or prepared form, which may result in the loss of actual quality of food item. These substances may be either available food items or non-food items. Among meat and meat products some of the items used to adulterate are water or ice, carcasses, or carcasses of animals other than the animal meant to be consumed.\n\nCamping food includes ingredients used to prepare food suitable for backcountry camping and backpacking. The foods differ substantially from the ingredients found in a typical home kitchen. The primary differences relate to campers' and backpackers' special needs for foods that have appropriate cooking time, perishability, weight, and nutritional content.\n\nTo address these needs, camping food is often made up of either freeze-dried, precooked or dehydrated ingredients. Many campers use a combination of these foods.\n\nFreeze-drying requires the use of heavy machinery and is not something that most campers are able to do on their own. Freeze-dried ingredients are often considered superior to dehydrated ingredients however, because they rehydrate at camp faster and retain more flavor than their dehydrated counterparts. Freeze-dried ingredients take so little time to rehydrate that they can often be eaten without cooking them first and have a texture similar to a crunchy chip.\n\nDehydration can reduce the weight of the food by sixty to ninety percent by removing water through evaporation. Some foods dehydrate well, such as onions, peppers, and tomatoes. Dehydration often produces a more compact, albeit slightly heavier, end result than freeze-drying.\n\nSurplus precooked military Meals, Meals, Ready-to-Eat (MREs) are sometimes used by campers. These meals contain precooked foods in retort pouches. A retort pouch is a plastic and metal foil laminate pouch that is used as an alternative to traditional industrial canning methods.\n\nDiet food (or \"dietetic food\") refers to any food or beverage whose recipe is altered to reduce fat, carbohydrates, abhor/adhore sugar in order to make it part of a weight loss program or diet. Such foods are usually intended to assist in weight loss or a change in body type, although bodybuilding supplements are designed to aid in gaining weight or muscle.\n\nThe process of making a diet version of a food usually requires finding an acceptable low-food-energy substitute for some high-food-energy ingredient. This can be as simple as replacing some or all of the food's sugar with a sugar substitute as is common with diet soft drinks such as Coca-Cola (for example Diet Coke). In some snacks, the food may be baked instead of fried thus reducing the food energy. In other cases, low-fat ingredients may be used as replacements.\n\nIn whole grain foods, the higher fiber content effectively displaces some of the starch component of the flour. Since certain fibers have no food energy, this results in a modest energy reduction. Another technique relies on the intentional addition of other reduced-food-energy ingredients, such as resistant starch or dietary fiber, to replace part of the flour and achieve a more significant energy reduction.\n\nFinger food is food meant to be eaten directly using the hands, in contrast to food eaten with a knife and fork, spoon, chopsticks, or other utensils. In some cultures, food is almost always eaten with the hands; for example, Ethiopian cuisine is eaten by rolling various dishes up in \"injera\" bread. Foods considered street foods are frequently, though not exclusively, finger foods.\n\nIn the western world, finger foods are often either appetizers (hors d'œuvres) or entree/main course items. Examples of these are miniature meat pies, sausage rolls, sausages on sticks, cheese and olives on sticks, chicken drumsticks or wings, spring rolls, miniature quiches, samosas, sandwiches, Merenda or other such based foods, such as pitas or items in buns, bhajjis, potato wedges, vol au vents, several other such small items and risotto balls (arancini). Other well-known foods that are generally eaten with the hands include hamburgers, pizza, Chips, hot dogs, fruit and bread.\n\nIn East Asia, foods like pancakes or flatbreads (\"bing\" 饼) and street foods such as chuan (串, also pronounced \"chuan\") are often eaten with the hands.\n\nFresh food is food which has not been preserved and has not spoiled yet. For vegetables and fruits, this means that they have been recently harvested and treated properly postharvest; for meat, it has recently been slaughtered and butchered; for fish, it has been recently caught or harvested and kept cold.\n\nDairy products are fresh and will spoil quickly. Thus, fresh cheese is cheese which has not been dried or salted for aging. Soured cream may be considered \"fresh\" (crème fraîche). \n\nFresh food has not been dried, smoked, salted, frozen, canned, pickled, or otherwise preserved.\n\nFreezing food preserves it from the time it is prepared to the time it is eaten. Since early times, farmers, fishermen, and trappers have preserved grains and produce in unheated buildings during the winter season. Freezing food slows down decomposition by turning residual moisture into ice, inhibiting the growth of most bacterial species. In the food commodity industry, there are two processes: mechanical and cryogenic (or flash freezing). The freezing kinetics is important to preserve the food quality and texture. Quicker freezing generates smaller ice crystals and maintains cellular structure. Cryogenic freezing is the quickest freezing technology available due to the ultra low liquid nitrogen temperature .\n\nPreserving food in domestic kitchens during modern times is achieved using household freezers. Accepted advice to householders was to freeze food on the day of purchase. An initiative by a supermarket group in 2012 (backed by the UK's Waste & Resources Action Programme) promotes the freezing of food \"as soon as possible up to the product's 'use by' date\". The Food Standards Agency was reported as supporting the change, providing the food had been stored correctly up to that time.\n\nA functional food is a food given an additional function (often one related to health-promotion or disease prevention) by adding new ingredients or more of existing ingredients. The term may also apply to traits purposely bred into existing edible plants, such as purple or gold potatoes having enriched anthocyanin or carotenoid contents, respectively. Functional foods may be \"designed to have physiological benefits and/or reduce the risk of chronic disease beyond basic nutritional functions, and may be similar in appearance to conventional food and consumed as part of a regular diet\".\n\nThe term was first used in Japan in the 1980s where there is a government approval process for functional foods called Foods for Specified Health Use (FOSHU).\n\nHealth food is food marketed to provide human health effects beyond a normal healthy diet required for human nutrition. Foods marketed as health foods may be part of one or more categories, such as natural foods, organic foods, whole foods, vegetarian foods or dietary supplements. These products may be sold in health food stores or in the health food or organic sections of grocery stores.\n\nA healthy diet is a diet that helps to maintain or improve overall health. A healthy diet provides the body with essential nutrition: fluid, macronutrients, micronutrients, and adequate calories.\n\nFor people who are healthy, a healthy diet is not complicated and contains mostly fruits, vegetables, and whole grains, and includes little to no processed food and sweetened beverages. The requirements for a healthy diet can be met from a variety of plant-based and animal-based foods, although a non-animal source of vitamin B12 is needed for those following a vegan diet. Various nutrition guides are published by medical and governmental institutions to educate individuals on what they should be eating to be healthy. Nutrition facts labels are also mandatory in some countries to allow consumers to choose between foods based on the components relevant to health.\n\nA healthy lifestyle includes getting exercise every day along with eating a healthy diet. A healthy lifestyle may lower disease risks, such as obesity, heart disease, type 2 diabetes, hypertension and cancer.\n\nThere are specialized healthy diets, called medical nutrition therapy, for people with various diseases or conditions. There are also prescientific ideas about such specialized diets, as in dietary therapy in traditional Chinese medicine.\n\nThe World Health Organization (WHO) makes the following 5 recommendations with respect to both populations and individuals:\n\nKosher foods are those that conform to the Jewish dietary regulations of \"kashrut\" (dietary law), primarily derived from Leviticus and Deuteronomy. Food that may be consumed according to \"halakha\" (law) is termed \"kosher\" () in English, from the Ashkenazi pronunciation of the Hebrew term \"kashér\" (), meaning \"fit\" (in this context, fit for consumption). Food that is not in accordance with law is called \"treif\" (; , derived from \"trāfáh\") meaning \"torn.\"\n\nLive food is living food for carnivorous or omnivorous animals kept in captivity; in other words, small animals such as insects or mice fed to larger carnivorous or omnivorous species kept in either in a zoo or as pet. \n\nLive food is commonly used as feed for a variety of species of exotic pets and zoo animals, ranging from alligators to various snakes, frogs and lizards, but also including other, non-reptile, non-amphibian carnivores and omnivores (for instance, skunks, which are omnivorous mammals, can be technically be fed a limited amount of live food, though this is not known to be a common practice). Common live food ranges from crickets (used as an inexpensive form of feed for carnivorous and omnivorous reptiles such as bearded dragons and commonly available in pet stores for this reason), waxworms, mealworms and to a lesser extent cockroaches and locusts, to small birds and mammals such as mice or chickens. \n\nMedical foods are foods that are specially formulated and intended for the dietary management of a disease that has distinctive nutritional needs that cannot be met by normal diet alone. In the United States they were defined in the Food and Drug Administration's 1988 Orphan Drug Act Amendments and are subject to the general food and safety labeling requirements of the Federal Food, Drug, and Cosmetic Act. In Europe the European Food Safety Authority established definitions for \"foods for special medical purposes\" (FSMPs) in 2015.\n\nMedical foods, called \"food for special medical purposes\" in Europe, are distinct from the broader category of foods for special dietary use, from traditional foods that bear a health claim, and from dietary supplements. In order to be considered a medical food the product must, at a minimum:\n\nMedical foods can be classified into the following categories:\n\nNatural foods and \"all natural foods\" are widely used terms in food labeling and marketing with a variety of definitions, most of which are vague. The term is often assumed to imply foods that are not processed and whose ingredients are all natural products (in the chemist's sense of that term), thus conveying an appeal to nature. But the lack of standards in most jurisdictions means that the term assures nothing. In some countries, the term \"natural\" is defined and enforced. In others, such as the United States, it is not enforced.\n\n“Natural foods” are often assumed to be foods that are not processed, or do not contain any food additives, or do not contain particular additives such as hormones, antibiotics, sweeteners, food colors, or flavorings that were not originally in the food. In fact, many people (63%) when surveyed showed a preference for products labeled \"natural\" compared to the unmarked counterparts, based on the common belief (86% of polled consumers) that the term \"natural\" indicated that the food does not contain any artificial ingredients. The terms are variously used and misused on labels and in advertisements.\n\nThe international Food and Agriculture Organization’s \"Codex Alimentarius\" does not recognize the term “natural” but does have a standard for organic foods.\nA negative-calorie food is food that supposedly requires more food energy to be digested than the food provides. Its thermic effect or specific dynamic action—the caloric \"cost\" of digesting the food—would be greater than its food energy content. Despite its recurring popularity in dieting guides, there is no scientific evidence supporting the idea that any food is calorically negative. While some chilled beverages are calorically negative, the effect is minimal and drinking large amounts of water can be dangerous.\n\nOrganic food is food produced by methods that comply with the standards of organic farming. Standards vary worldwide, but organic farming in general features practices that strive to cycle resources, promote ecological balance, and conserve biodiversity. Organizations regulating organic products may restrict the use of certain pesticides and fertilizers in farming. In general, organic foods are also usually not processed using irradiation, industrial solvents or synthetic food additives.\n\nCurrently, the European Union, the United States, Canada, Mexico, Japan, and many other countries require producers to obtain special certification in order to market food as organic within their borders. In the context of these regulations, organic food is produced in a way that complies with organic standards set by regional organizations, national governments and international organizations. Although the produce of kitchen gardens may be organic, selling food with an organic label is regulated by governmental food safety authorities, such as the US Department of Agriculture (USDA) or European Commission (EC).\n\nFertilizing and the use of pesticides in conventional farming has caused, and is causing, enormous damage worldwide to local ecosystems, biodiversity, groundwater and drinking water supplies, and sometimes farmer health and fertility. These environmental, economic and health issues are intended to be minimized or avoided in organic farming. From a consumers perspective, there is not sufficient evidence in scientific and medical literature to support claims that organic food is safer or healthier to eat than conventionally grown food. While there may be some differences in the nutrient and antinutrient contents of organically- and conventionally-produced food, the variable nature of food production and handling makes it difficult to generalize results. Claims that organic food tastes better are generally not supported by tests.\n\nPeasant foods are dishes specific to a particular culture, made from accessible and inexpensive ingredients, and usually prepared and seasoned to make them more palatable. They often form a significant part of the diets of people who live in poverty, or have a lower income compared to the average for their society or country.\n\nPeasant foods have been described as being the diet of peasants, that is, tenant or poorer farmers and their farm workers, and by extension, of other cash-poor people. They may use ingredients, such as offal and less-tender cuts of meat, which are not as marketable as a cash crop. Characteristic recipes often consist of hearty one-dish meals, in which chunks of meat and various vegetables are eaten in a savory broth, with bread or other staple food. Sausages are also amenable to varied readily available ingredients, and they themselves tend to contain offal and grains.\n\nPeasant foods often involve skilled preparation by knowledgeable cooks using inventiveness and skills passed down from earlier generations. Such dishes are often prized as ethnic foods by other cultures and by descendants of the native culture who still desire these traditional dishes.\n\nPrison food is the term for meals served to prisoners while incarcerated in correctional institutions. While some prisons prepare their own food, many use staff from on-site catering companies. Many prisons today support the requirements of specific religions, as well as vegetarianism. It is said that prison food of many developed countries is adequate to maintain health and dieting.\n\n\"Seasonal\" here refers to the times of year when the harvest or the flavour of a given type food is at its peak. This is usually the time when the item is harvested, with some exceptions; an example being sweet potatoes which are best eaten quite a while after harvest. It also appeals to people who prefer a low carbon diet that reduces the greenhouse gas emissions resulting from food consumption (Food miles).\n\nShelf-stable food (sometimes ]]ambient food]]) is food of a type that can be safely stored at room temperature in a sealed container. This includes foods that would normally be stored refrigerated but which have been processed so that they can be safely stored at room or ambient temperature for a usefully long shelf life. \n\nVarious food preservation and packaging techniques are used to extend a food's shelf life. Decreasing the amount of available water in a product, increasing its acidity, or irradiating or otherwise sterilizing the food and then sealing it in an air-tight container are all ways of depriving bacteria of suitable conditions in which to thrive. All of these approaches can all extend a food's shelf life without unacceptably changing its taste or texture.\n\nFor some foods alternative ingredients can be used. Common oils and fats become rancid relatively quickly if not refrigerated; replacing them with hydrogenated oils delays the onset of rancidity, increasing shelf life. This is a common approach in industrial food production, but recent concerns about health hazards associated with trans fats have led to their strict control in several jurisdictions. Even where trans fats are not prohibited, in many places there are new labeling laws (or rules), which require information to be printed on packages, or to be published elsewhere, about the amount of trans fat contained in certain products.\n\nSpace food is a type of food product created and processed for consumption by astronauts in outer space. The food has specific requirements of providing balanced nutrition for individuals working in space, while being easy and safe to store, prepare and consume in the machinery-filled weightless environments of manned spacecraft. \n\nIn recent years, space food has been used by various nations engaging on space programs as a way to share and show off their cultural identity and facilitate intercultural communication. Although astronauts consume a wide variety of foods and beverages in space, the initial idea from The Man in Space Committee of the Space Science Board in 1963 was to supply astronauts with a formula diet that would supply all the needed vitamins and nutrients.\n\nTraditional foods are foods and dishes that are passed through generations or which have been consumed many generations. Traditional foods and dishes are traditional in nature, and may have a historic precedent in a national dish, regional cuisine or local cuisine. Traditional foods and beverages may be produced as homemade, by restaurants and small manufacturers, and by large food processing plant facilities.\n\nSome traditional foods have geographical indications and traditional specialities in the European Union designations per European Union schemes of geographical indications and traditional specialties: Protected designation of origin (PDO), Protected geographical indication (PGI) and Traditional specialities guaranteed (TSG). These standards serve to promote and protect names of quality agricultural products and foodstuffs.\n\nThis article also includes information about traditional beverages.\n\nWhole foods are plant foods that are unprocessed and unrefined, or processed and refined as little as possible, before being consumed. Examples of whole foods include whole grains, tubers, legumes, fruits, vegetables.\n\nThere is some confusion over the usage of the term surrounding the inclusion of certain foods, in particular animal foods. The modern usage of the term whole foods diet is now widely synonymous with \"whole foods plant-based diet\" with animal products, oil and salt no longer constituting whole foods.\n\nThe earliest use of the term in the post-industrial age appears to be in 1946 in The Farmer, a quarterly magazine published and edited from his farm by F. Newman Turner, a writer and pioneering organic farmer. The magazine sponsored the establishment of the Producer Consumer Whole Food Society Ltd, with Newman Turner as president and Derek Randal as vice-president. Whole food was defined as \"mature produce of field, orchard, or garden without subtraction, addition, or alteration grown from seed without chemical dressing, in fertile soil manured solely with animal and vegetable wastes, and composts therefrom, and ground, raw rock and without chemical manures, sprays, or insecticides,\" having intent to connect suppliers and the growing public demand for such food. Such diets are rich in whole and unrefined foods, like whole grains, dark green and yellow/orange-fleshed vegetables and fruits, legumes, nuts and seeds.\n\nMost food has always been obtained through agriculture. With increasing concern over both the methods and products of modern industrial agriculture, there has been a growing trend toward sustainable agricultural practices. This approach, partly fueled by consumer demand, encourages biodiversity, local self-reliance and organic farming methods. Major influences on food production include international organizations (e.g. the World Trade Organization and Common Agricultural Policy), national government policy (or law), and war.\n\nIn popular culture, the mass production of food, specifically meats such as chicken and beef, has come under fire from various documentaries, most recently Food, Inc, documenting the mass slaughter and poor treatment of animals, often for easier revenues from large corporations. Along with a current trend towards environmentalism, people in Western culture have had an increasing trend towards the use of herbal supplements, foods for a specific group of people (such as dieters, women, or athletes), functional foods (fortified foods, such as omega-3 eggs), and a more ethnically diverse diet.\n\nSeveral organisations have begun calling for a new kind of agriculture in which agroecosystems provide food but also support vital ecosystem services so that soil fertility and biodiversity are maintained rather than compromised. According to the International Water Management Institute and UNEP, well-managed agroecosystems not only provide food, fiber and animal products, they also provide services such as flood mitigation, groundwater recharge, erosion control and habitats for plants, birds, fish and other animals.\n\nAnimals, specifically humans, have five different types of tastes: sweet, sour, salty, bitter, and umami. As animals have evolved, the tastes that provide the most energy (sugar and fats) are the most pleasant to eat while others, such as bitter, are not enjoyable. Water, while important for survival, has no taste. Fats, on the other hand, especially saturated fats, are thicker and rich and are thus considered more enjoyable to eat.\n\nGenerally regarded as the most pleasant taste, sweetness is almost always caused by a type of simple sugar such as glucose or fructose, or disaccharides such as sucrose, a molecule combining glucose and fructose. Complex carbohydrates are long chains and thus do not have the sweet taste. Artificial sweeteners such as sucralose are used to mimic the sugar molecule, creating the sensation of sweet, without the calories. Other types of sugar include raw sugar, which is known for its amber color, as it is unprocessed. As sugar is vital for energy and survival, the taste of sugar is pleasant.\n\nThe stevia plant contains a compound known as steviol which, when extracted, has 300 times the sweetness of sugar while having minimal impact on blood sugar.\n\nSourness is caused by the taste of acids, such as vinegar in alcoholic beverages. Sour foods include citrus, specifically lemons, limes, and to a lesser degree oranges. Sour is evolutionarily significant as it is a sign for a food that may have gone rancid due to bacteria. Many foods, however, are slightly acidic, and help stimulate the taste buds and enhance flavor.\n\nSaltiness is the taste of alkali metal ions such as sodium and potassium. It is found in almost every food in low to moderate proportions to enhance flavor, although to eat pure salt is regarded as highly unpleasant. There are many different types of salt, with each having a different degree of saltiness, including sea salt, fleur de sel, kosher salt, mined salt, and grey salt. Other than enhancing flavor, its significance is that the body needs and maintains a delicate electrolyte balance, which is the kidney's function. Salt may be iodized, meaning iodine has been added to it, a necessary nutrient that promotes thyroid function. Some canned foods, notably soups or packaged broths, tend to be high in salt as a means of preserving the food longer. Historically salt has long been used as a meat preservative as salt promotes water excretion. Similarly, dried foods also promote food safety.\n\nBitterness is a sensation often considered unpleasant characterized by having a sharp, pungent taste. Unsweetened dark chocolate, caffeine, lemon rind, and some types of fruit are known to be bitter.\n\nUmami, the Japanese word for delicious, is the least known in Western popular culture but has a long tradition in Asian cuisine. Umami is the taste of glutamates, especially monosodium glutamate (MSG). It is characterized as savory, meaty, and rich in flavor. Salmon and mushrooms are foods high in umami.\n\nMany scholars claim that the rhetorical function of food is to represent the culture of a country, and that it can be used as a form of communication. According to Goode, Curtis and Theophano, food \"is the last aspect of an ethnic culture to be lost\".\n\nMany cultures have a recognizable cuisine, a specific set of cooking traditions using various spices or a combination of flavors unique to that culture, which evolves over time. Other differences include preferences (hot or cold, spicy, etc.) and practices, the study of which is known as gastronomy. Many cultures have diversified their foods by means of preparation, cooking methods, and manufacturing. This also includes a complex food trade which helps the cultures to economically survive by way of food, not just by consumption.\n\nSome popular types of ethnic foods include Italian, French, Japanese, Chinese, American, Cajun, Thai, African, Indian and Nepalese. Various cultures throughout the world study the dietary analysis of food habits. While evolutionarily speaking, as opposed to culturally, humans are omnivores, religion and social constructs such as morality, activism, or environmentalism will often affect which foods they will consume. Food is eaten and typically enjoyed through the sense of taste, the perception of flavor from eating and drinking. Certain tastes are more enjoyable than others, for evolutionary purposes.\n\nAesthetically pleasing and eye-appealing food presentations can encourage people to consume foods. A common saying is that people \"eat with their eyes\". Food presented in a clean and appetizing way will encourage a good flavor, even if unsatisfactory.\n\nTexture plays a crucial role in the enjoyment of eating foods. Contrasts in textures, such as something crunchy in an otherwise smooth dish, may increase the appeal of eating it. Common examples include adding granola to yogurt, adding croutons to a salad or soup, and toasting bread to enhance its crunchiness for a smooth topping, such as jam or butter.\n\nAnother universal phenomenon regarding food is the appeal of contrast in taste and presentation. For example, such opposite flavors as sweetness and saltiness tend to go well together, as in kettle corn and nuts.\n\nWhile many foods can be eaten raw, many also undergo some form of preparation for reasons of safety, palatability, texture, or flavor. At the simplest level this may involve washing, cutting, trimming, or adding other foods or ingredients, such as spices. It may also involve mixing, heating or cooling, pressure cooking, fermentation, or combination with other food. In a home, most food preparation takes place in a kitchen. Some preparation is done to enhance the taste or aesthetic appeal; other preparation may help to preserve the food; others may be involved in cultural identity. A meal is made up of food which is prepared to be eaten at a specific time and place.\nThe preparation of animal-based food usually involves slaughter, evisceration, hanging, portioning, and rendering. In developed countries, this is usually done outside the home in slaughterhouses, which are used to process animals en masse for meat production. Many countries regulate their slaughterhouses by law. For example, the United States has established the Humane Slaughter Act of 1958, which requires that an animal be stunned before killing. This act, like those in many countries, exempts slaughter in accordance to religious law, such as kosher, shechita, and dhabīḥah halal. Strict interpretations of kashrut require the animal to be fully aware when its carotid artery is cut.\n\nOn the local level, a butcher may commonly break down larger animal meat into smaller manageable cuts, and pre-wrap them for commercial sale or wrap them to order in butcher paper. In addition, fish and seafood may be fabricated into smaller cuts by a fish monger. However, fish butchery may be done on board a fishing vessel and quick-frozen for preservation of quality.\n\nThe term \"cooking\" encompasses a vast range of methods, tools, and combinations of ingredients to improve the flavor or digestibility of food. Cooking technique, known as culinary art, generally requires the selection, measurement, and combining of ingredients in an ordered procedure in an effort to achieve the desired result. Constraints on success include the variability of ingredients, ambient conditions, tools, and the skill of the individual cook. The diversity of cooking worldwide is a reflection of the myriad nutritional, aesthetic, agricultural, economic, cultural, and religious considerations that affect it.\n\nCooking requires applying heat to a food which usually, though not always, chemically changes the molecules, thus changing its flavor, texture, appearance, and nutritional properties. Cooking certain proteins, such as egg whites, meats, and fish, denatures the protein, causing it to firm. There is archaeological evidence of roasted foodstuffs at \"Homo erectus\" campsites dating from 420,000 years ago. Boiling as a means of cooking requires a container, and has been practiced at least since the 10th millennium BC with the introduction of pottery.\n\nThere are many different types of equipment used for cooking.\n\nOvens are mostly hollow devices that get very hot (up to ) and are used for baking or roasting and offer a dry-heat cooking method. Different cuisines will use different types of ovens. For example, Indian culture uses a tandoor oven, which is a cylindrical clay oven which operates at a single high temperature. Western kitchens use variable temperature convection ovens, conventional ovens, toaster ovens, or non-radiant heat ovens like the microwave oven. Classic Italian cuisine includes the use of a brick oven containing burning wood. Ovens may be wood-fired, coal-fired, gas, electric, or oil-fired.\n\nVarious types of cook-tops are used as well. They carry the same variations of fuel types as the ovens mentioned above. Cook-tops are used to heat vessels placed on top of the heat source, such as a sauté pan, sauce pot, frying pan, or pressure cooker. These pieces of equipment can use either a moist or dry cooking method and include methods such as steaming, simmering, boiling, and poaching for moist methods, while the dry methods include sautéing, pan frying, and deep-frying.\n\nIn addition, many cultures use grills for cooking. A grill operates with a radiant heat source from below, usually covered with a metal grid and sometimes a cover. An open pit barbecue in the American south is one example along with the American style outdoor grill fueled by wood, liquid propane, or charcoal along with soaked wood chips for smoking. A Mexican style of barbecue is called barbacoa, which involves the cooking of meats such as whole sheep over an open fire. In Argentina, an asado (Spanish for \"grilled\") is prepared on a grill held over an open pit or fire made upon the ground, on which a whole animal or smaller cuts are grilled.\n\nCertain cultures highlight animal and vegetable foods in a raw state. Salads consisting of raw vegetables or fruits are common in many cuisines. Sashimi in Japanese cuisine consists of raw sliced fish or other meat, and sushi often incorporates raw fish or seafood. Steak tartare and salmon tartare are dishes made from diced or ground raw beef or salmon, mixed with various ingredients and served with baguettes, brioche, or frites. In Italy, carpaccio is a dish of very thinly sliced raw beef, drizzled with a vinaigrette made with olive oil. The health food movement known as raw foodism promotes a mostly vegan diet of raw fruits, vegetables, and grains prepared in various ways, including juicing, food dehydration, sprouting, and other methods of preparation that do not heat the food above . An example of a raw meat dish is ceviche, a Latin American dish made with raw meat that is \"cooked\" from the highly acidic citric juice from lemons and limes along with other aromatics such as garlic.\n\nRestaurants employ chefs to prepare the food, and waiters to serve customers at the table. The term restaurant comes from an old term for a restorative meat broth; this broth (or bouillon) was served in elegant outlets in Paris from the mid 18th century. These refined \"restaurants\" were a marked change from the usual basic eateries such as inns and taverns, and some had developed from early Parisian cafés, such as Café Procope, by first serving bouillon, then adding other cooked food to their menus. \n\nCommercial eateries existed during the Roman period, with evidence of 150 \"thermopolia\", a form of fast food restaurant, found in Pompeii, and urban sales of prepared foods may have existed in China during the Song dynasty.\n\nIn 2005, the population of the United States spent $496 billion on out-of-home dining. Expenditures by type of out-of-home dining were as follows: 40% in full-service restaurants, 37.2% in limited service restaurants (fast food), 6.6% in schools or colleges, 5.4% in bars and vending machines, 4.7% in hotels and motels, 4.0% in recreational places, and 2.2% in others, which includes military bases.\n\nPackaged foods are manufactured outside the home for purchase. This can be as simple as a butcher preparing meat, or as complex as a modern international food industry. Early food processing techniques were limited by available food preservation, packaging, and transportation. This mainly involved salting, curing, curdling, drying, pickling, fermenting, and smoking. Food manufacturing arose during the industrial revolution in the 19th century. This development took advantage of new mass markets and emerging technology, such as milling, preservation, packaging and labeling, and transportation. It brought the advantages of pre-prepared time-saving food to the bulk of ordinary people who did not employ domestic servants.\n\nAt the start of the 21st century, a two-tier structure has arisen, with a few international food processing giants controlling a wide range of well-known food brands. There also exists a wide array of small local or national food processing companies. Advanced technologies have also come to change food manufacture. Computer-based control systems, sophisticated processing and packaging methods, and logistics and distribution advances can enhance product quality, improve food safety, and reduce costs.\n\nThe World Bank reported that the European Union was the top food importer in 2005, followed at a distance by the USA and Japan. Britain's need for food was especially well illustrated in World War II. Despite the implementation of food rationing, Britain remained dependent on food imports and the result was a long term engagement in the Battle of the Atlantic.\n\nFood is traded and marketed on a global basis. The variety and availability of food is no longer restricted by the diversity of locally grown food or the limitations of the local growing season. Between 1961 and 1999, there was a 400% increase in worldwide food exports. Some countries are now economically dependent on food exports, which in some cases account for over 80% of all exports.\n\nIn 1994, over 100 countries became signatories to the Uruguay Round of the General Agreement on Tariffs and Trade in a dramatic increase in trade liberalization. This included an agreement to reduce subsidies paid to farmers, underpinned by the WTO enforcement of agricultural subsidy, tariffs, import quotas, and settlement of trade disputes that cannot be bilaterally resolved. Where trade barriers are raised on the disputed grounds of public health and safety, the WTO refer the dispute to the Codex Alimentarius Commission, which was founded in 1962 by the United Nations Food and Agriculture Organization and the World Health Organization. Trade liberalization has greatly affected world food trade.\n\nFood marketing brings together the producer and the consumer. The marketing of even a single food product can be a complicated process involving many producers and companies. For example, fifty-six companies are involved in making one can of chicken noodle soup. These businesses include not only chicken and vegetable processors but also the companies that transport the ingredients and those who print labels and manufacture cans. The food marketing system is the largest direct and indirect non-government employer in the United States.\n\nIn the pre-modern era, the sale of surplus food took place once a week when farmers took their wares on market day into the local village marketplace. Here food was sold to grocers for sale in their local shops for purchase by local consumers. With the onset of industrialization and the development of the food processing industry, a wider range of food could be sold and distributed in distant locations. Typically early grocery shops would be counter-based shops, in which purchasers told the shop-keeper what they wanted, so that the shop-keeper could get it for them.\n\nIn the 20th century, supermarkets were born. Supermarkets brought with them a self service approach to shopping using shopping carts, and were able to offer quality food at lower cost through economies of scale and reduced staffing costs. In the latter part of the 20th century, this has been further revolutionized by the development of vast warehouse-sized, out-of-town supermarkets, selling a wide range of food from around the world.\n\nUnlike food processors, food retailing is a two-tier market in which a small number of very large companies control a large proportion of supermarkets. The supermarket giants wield great purchasing power over farmers and processors, and strong influence over consumers. Nevertheless, less than 10% of consumer spending on food goes to farmers, with larger percentages going to advertising, transportation, and intermediate corporations.\n\nIt is rare for price spikes to hit all major foods in most countries at once, but food prices suffered all-time peaks in 2008 and 2011, posting a 15% and 12% deflated increase year-over-year, representing prices higher than any data collected.\n\nIn December 2007, 37 countries faced food crises, and 20 had imposed some sort of food-price controls. In China, the price of pork jumped 58% in 2007. In the 1980s and 1990s, farm subsidies and support programs allowed major grain exporting countries to hold large surpluses, which could be tapped during food shortages to keep prices down. However, new trade policies had made agricultural production much more responsive to market demands, putting global food reserves at their lowest since 1983.\n\nRising food prices in those years have been linked with social unrest around the world, including rioting in Bangladesh and Mexico, and the Arab Spring.\nFood prices worldwide increased in 2008.\nOne cause of rising food prices is wealthier Asian consumers are westernizing their diets, and farmers and nations of the third world are struggling to keep up the pace. The past five years have seen rapid growth in the contribution of Asian nations to the global fluid and powdered milk manufacturing industry, which in 2008 accounted for more than 30% of production, while China alone accounts for more than 10% of both production and consumption in the global fruit and vegetable processing and preserving industry.\n\nIn 2013 Overseas Development Institute researchers showed that rice has more than doubled in price since 2000, rising by 120% in real terms. This was as a result of shifts in trade policy and restocking by major producers. More fundamental drivers of increased prices are the higher costs of fertiliser, diesel and labour. Parts of Asia see rural wages rise with potential large benefits for the 1.3 billion (2008 estimate) of Asia's poor in reducing the poverty they face. However, this negatively impacts more vulnerable groups who don't share in the economic boom, especially in Asian and African coastal cities. The researchers said the threat means social-protection policies are needed to guard against price shocks. The research proposed that in the longer run, the rises present opportunities to export for Western African farmers with high potential for rice production to replace imports with domestic production.\n\nMost recently, global food prices have been more stable and relatively low, after a sizable increase in late 2017, they are back under 75% of the nominal value seen during the all-time high in the 2011 food crisis.\n\nInstitutions such as hedge funds, pension funds and investment banks like Barclays Capital, Goldman Sachs and Morgan Stanley have been instrumental in pushing up prices in the last five years, with investment in food commodities rising from $65bn to $126bn (£41bn to £79bn) between 2007 and 2012, contributing to 30-year highs. This has caused price fluctuations which are not strongly related to the actual supply of food, according to the United Nations. Financial institutions now make up 61% of all investment in wheat futures. According to Olivier De Schutter, the UN special rapporteur on food, there was a rush by institutions to enter the food market following George W Bush's Commodities Futures Modernization Act of 2000. De Schutter told the Independent in March 2012: \"What we are seeing now is that these financial markets have developed massively with the arrival of these new financial investors, who are purely interested in the short-term monetary gain and are not really interested in the physical thing – they never actually buy the ton of wheat or maize; they only buy a promise to buy or to sell. The result of this financialisation of the commodities market is that the prices of the products respond increasingly to a purely speculative logic. This explains why in very short periods of time we see prices spiking or bubbles exploding, because prices are less and less determined by the real match between supply and demand.\" In 2011, 450 economists from around the world called on the G20 to regulate the commodities market more.\n\nSome experts have said that speculation has merely aggravated other factors, such as climate change, competition with bio-fuels and overall rising demand. However, some such as Jayati Ghosh, professor of economics at Jawaharlal Nehru University in New Delhi, have pointed out that prices have increased irrespective of supply and demand issues: Ghosh points to world wheat prices, which doubled in the period from June to December 2010, despite there being no fall in global supply.\n\nFood deprivation leads to malnutrition and ultimately starvation. This is often connected with famine, which involves the absence of food in entire communities. This can have a devastating and widespread effect on human health and mortality. Rationing is sometimes used to distribute food in times of shortage, most notably during times of war.\n\nStarvation is a significant international problem. Approximately 815 million people are undernourished, and over 16,000 children die per day from hunger-related causes. Food deprivation is regarded as a deficit need in Maslow's hierarchy of needs and is measured using famine scales.\n\nFood aid can benefit people suffering from a shortage of food. It can be used to improve peoples' lives in the short term, so that a society can increase its standard of living to the point that food aid is no longer required. Conversely, badly managed food aid can create problems by disrupting local markets, depressing crop prices, and discouraging food production. Sometimes a cycle of food aid dependence can develop. Its provision, or threatened withdrawal, is sometimes used as a political tool to influence the policies of the destination country, a strategy known as food politics. Sometimes, food aid provisions will require certain types of food be purchased from certain sellers, and food aid can be misused to enhance the markets of donor countries. International efforts to distribute food to the neediest countries are often coordinated by the World Food Programme.\n\nFoodborne illness, commonly called \"food poisoning\", is caused by bacteria, toxins, viruses, parasites, and prions. Roughly 7 million people die of food poisoning each year, with about 10 times as many suffering from a non-fatal version. The two most common factors leading to cases of bacterial foodborne illness are cross-contamination of ready-to-eat food from other uncooked foods and improper temperature control. Less commonly, acute adverse reactions can also occur if chemical contamination of food occurs, for example from improper storage, or use of non-food grade soaps and disinfectants. Food can also be adulterated by a very wide range of articles (known as \"foreign bodies\") during farming, manufacture, cooking, packaging, distribution, or sale. These foreign bodies can include pests or their droppings, hairs, cigarette butts, wood chips, and all manner of other contaminants. It is possible for certain types of food to become contaminated if stored or presented in an unsafe container, such as a ceramic pot with lead-based glaze.\n\nFood poisoning has been recognized as a disease since as early as Hippocrates. The sale of rancid, contaminated, or adulterated food was commonplace until the introduction of hygiene, refrigeration, and vermin controls in the 19th century. Discovery of techniques for killing bacteria using heat, and other microbiological studies by scientists such as Louis Pasteur, contributed to the modern sanitation standards that are ubiquitous in developed nations today. This was further underpinned by the work of Justus von Liebig, which led to the development of modern food storage and food preservation methods. In more recent years, a greater understanding of the causes of food-borne illnesses has led to the development of more systematic approaches such as the Hazard Analysis and Critical Control Points (HACCP), which can identify and eliminate many risks.\n\nRecommended measures for ensuring food safety include maintaining a clean preparation area with foods of different types kept separate, ensuring an adequate cooking temperature, and refrigerating foods promptly after cooking.\n\nFoods that spoil easily, such as meats, dairy, and seafood, must be prepared a certain way to avoid contaminating the people for whom they are prepared. As such, the rule of thumb is that cold foods (such as dairy products) should be kept cold and hot foods (such as soup) should be kept hot until storage. Cold meats, such as chicken, that are to be cooked should not be placed at room temperature for thawing, at the risk of dangerous bacterial growth, such as \"Salmonella\" or \"E. coli\".\n\nSome people have allergies or sensitivities to foods which are not problematic to most people. This occurs when a person's immune system mistakes a certain food protein for a harmful foreign agent and attacks it. About 2% of adults and 8% of children have a food allergy. The amount of the food substance required to provoke a reaction in a particularly susceptible individual can be quite small. In some instances, traces of food in the air, too minute to be perceived through smell, have been known to provoke lethal reactions in extremely sensitive individuals. Common food allergens are gluten, corn, shellfish (mollusks), peanuts, and soy. Allergens frequently produce symptoms such as diarrhea, rashes, bloating, vomiting, and regurgitation. The digestive complaints usually develop within half an hour of ingesting the allergen.\n\nRarely, food allergies can lead to a medical emergency, such as anaphylactic shock, hypotension (low blood pressure), and loss of consciousness. An allergen associated with this type of reaction is peanut, although latex products can induce similar reactions. Initial treatment is with epinephrine (adrenaline), often carried by known patients in the form of an Epi-pen or Twinject.\n\nHuman diet was estimated to cause perhaps around 35% of cancers in a human epidemiological analysis by Richard Doll and Richard Peto in 1981. These cancer may be caused by carcinogens that are present in food naturally or as contaminants. Food contaminated with fungal growth may contain mycotoxins such as aflatoxins which may be found in contaminated corn and peanuts. Other carcinogens identified in food include heterocyclic amines generated in meat when cooked at high temperature, polyaromatic hydrocarbons in charred meat and smoked fish, and nitrosamines generated from nitrites used as food preservatives in cured meat such as bacon.\n\nAnticarcinogens that may help prevent cancer can also be found in many food especially fruit and vegetables. Antioxidants are important groups of compounds that may help remove potentially harmful chemicals. It is however often difficult to identify the specific components in diet that serve to increase or decrease cancer risk since many food, such as beef steak and broccoli, contain low concentrations of both carcinogens and anticarcinogens.\nThere are many international certifications in cooking field, such as Monde Selection、A.A.Certification、iTQi. They use the high quality evaluation methods to make the food become more safe.\n\nMany cultures hold some food preferences and some food taboos. Dietary choices can also define cultures and play a role in religion. For example, only kosher foods are permitted by Judaism, halal foods by Islam, and in Hinduism beef is restricted. In addition, the dietary choices of different countries or regions have different characteristics. This is highly related to a culture's cuisine.\n\nDietary habits play a significant role in the health and mortality of all humans. Imbalances between the consumed fuels and expended energy results in either starvation or excessive reserves of adipose tissue, known as body fat. Poor intake of various vitamins and minerals can lead to diseases that can have far-reaching effects on health. For instance, 30% of the world's population either has, or is at risk for developing, iodine deficiency. It is estimated that at least 3 million children are blind due to vitamin A deficiency. Vitamin C deficiency results in scurvy. Calcium, Vitamin D, and phosphorus are inter-related; the consumption of each may affect the absorption of the others. Kwashiorkor and marasmus are childhood disorders caused by lack of dietary protein.\n\nMany individuals limit what foods they eat for reasons of morality, or other habit. For instance, vegetarians choose to forgo food from animal sources to varying degrees. Others choose a healthier diet, avoiding sugars or animal fats and increasing consumption of dietary fiber and antioxidants. Obesity, a serious problem in the western world, leads to higher chances of developing heart disease, diabetes, cancer and many other diseases. More recently, dietary habits have been influenced by the concerns that some people have about possible impacts on health or the environment from genetically modified food. Further concerns about the impact of industrial farming (grains) on animal welfare, human health, and the environment are also having an effect on contemporary human dietary habits. This has led to the emergence of a movement with a preference for organic and local food.\n\nBetween the extremes of optimal health and death from starvation or malnutrition, there is an array of disease states that can be caused or alleviated by changes in diet. Deficiencies, excesses, and imbalances in diet can produce negative impacts on health, which may lead to various health problems such as scurvy, obesity, or osteoporosis, diabetes, cardiovascular diseases as well as psychological and behavioral problems. The science of nutrition attempts to understand how and why specific dietary aspects influence health.\n\nNutrients in food are grouped into several categories. Macronutrients are fat, protein, and carbohydrates. Micronutrients are the minerals and vitamins. Additionally, food contains water and dietary fiber.\n\nAs previously discussed, the body is designed by natural selection to enjoy sweet and fattening foods for evolutionary diets, ideal for hunters and gatherers. Thus, sweet and fattening foods in nature are typically rare and are very pleasurable to eat. In modern times, with advanced technology, enjoyable foods are easily available to consumers. Unfortunately, this promotes obesity in adults and children alike.\n\nSome countries list a legal definition of food, often referring them with the word \"foodstuff\". These countries list food as any item that is to be processed, partially processed, or unprocessed for consumption. The listing of items included as food include any substance intended to be, or reasonably expected to be, ingested by humans. In addition to these foodstuffs, drink, chewing gum, water, or other items processed into said food items are part of the legal definition of food. Items not included in the legal definition of food include animal feed, live animals (unless being prepared for sale in a market), plants prior to harvesting, medicinal products, cosmetics, tobacco and tobacco products, narcotic or psychotropic substances, and residues and contaminants.\n\nSources:\n\n\n"}
{"id": "6494264", "url": "https://en.wikipedia.org/wiki?curid=6494264", "title": "Foundation for Revitalisation of Local Health Traditions", "text": "Foundation for Revitalisation of Local Health Traditions\n\nFoundation for Revitalisation of Local Health Traditions (FRLHT) is a registered Public Trust and Charitable Society, which started its activities in 1993 under the guidance of Sam Pitroda and Dr. Anant Darshan Shankar. The Indian Ministry of Science & Technology recognizes FRLHT as a scientific and research organization. The Ministry of Environment and Forests has designated FRLHT as a National Center of Excellence for medicinal plants and traditional knowledge.\n\nThe foundation plans to \"revitalize Indian medical heritage” through creative applications of traditional health sciences for enhancing the quality of health care in rural and urban India and globally. The stated mission of the foundation is to demonstrate the contemporary relevance of Indian Medical Heritage by designing and implementing innovative programs on a size and scale that will have societal impact.\n\nThe aim of the foundation is to make full use of India's rich and diverse medical knowledge.\n\nThe organization received the Norman Borlaug Award in 1998, Equator Initiative Prize of the United Nations in 2002 and the Columbia University Award from Rosenthal Centre of Columbia University and College of Physicians and Surgeons, New York, in 2003. Dr. Anant Darshan Shankar, the Director of the institution was honored by the Government of India in 2011 with the award of Padma Shri.\n\n"}
{"id": "3380814", "url": "https://en.wikipedia.org/wiki?curid=3380814", "title": "Gonadotropin-releasing hormone agonist", "text": "Gonadotropin-releasing hormone agonist\n\nA gonadotropin-releasing hormone agonist (GnRH agonist) is a type of medication which affects gonadotropins and sex hormones. They are used for a variety of indications including in fertility medicine and to lower sex hormone levels in the treatment of hormone-sensitive cancers such as prostate cancer and breast cancer, certain gynecological disorders like heavy periods and endometriosis, high testosterone levels in women, early puberty in children, as a part of transgender hormone therapy, and to delay puberty in transgender youth among other uses. GnRH agonists are given by injections into fat, as implants placed into fat, and as nasal sprays.\nSide effects of GnRH agonists are related to sex hormone deficiency and include symptoms of low testosterone levels and low estrogen levels such as hot flashes, sexual dysfunction, vaginal atrophy, osteoporosis, infertility, and diminished sex-specific physical characteristics. They are agonists of the GnRH receptor and work by increasing or decreasing the release of gonadotropins and the production of sex hormones by the gonads. When used to suppress gonadotropin release, GnRH agonists can lower sex hormone levels by 95% in both sexes.\nGnRH was discovered in 1971 and GnRH analogues were introduced for medical use in the 1980s. The most well-known and widely used GnRH analogue is leuprorelin (brand name Lupron). GnRH analogues are available as generic medications. Despite this however, they continue to be very expensive.\n\nGnRH agonists are useful in:\n\n\nWomen of reproductive age who undergo cytotoxic chemotherapy have been pretreated with GnRH agonists to reduce the risk of oocyte loss during such therapy and preserve ovarian function. Further studies are necessary to prove that this approach is useful.\n\nGnRH agonists that have been marketed and are available for medical use include buserelin, gonadorelin, goserelin, histrelin, leuprorelin, nafarelin, and triptorelin. GnRH agonists that are used mostly or exclusively in veterinary medicine include deslorelin and fertirelin. GnRH agonists can be administered by injection, by implant, or intranasally as a nasal spray. Injectables have been formulated for daily, monthly, and quarterly use, and implants are available that can last from one month to a year. With the exception of gonadorelin, which is used as a progonadotropin, all approved GnRH agonists are used as antigonadotropins.\n\nThe clinically used desensitizing GnRH agonists are available in the following pharmaceutical formulations:\n\n\nGnRH agonists are pregnancy category X drugs.\n\nSide effects of the GnRH agonists are signs and symptoms of hypoestrogenism, including hot flashes, headaches, and osteoporosis. In patients under long-term therapy, small amounts of estrogens could be given back (“add-back regimen”) to combat such side effects and to prevent bone wastage. Generally, long-term patients, both male and female, tend to undergo annual DEXA scans to appraise bone density.\n\nThere is also a report that GnRH agonists used in the treatment of advanced prostate cancer may increase the risk of heart problems by 30%.\n\nGnRH agonists acts as agonists of the GnRH receptor, the biological target of gonadotropin-releasing hormone (GnRH). These drugs can be both peptides and small-molecules. They are modeled after the hypothalamic neurohormone GnRH, which interacts with the GnRH receptor to elicit its biologic response, the release of the pituitary hormones follicle-stimulating hormone (FSH) and luteinizing hormone (LH). However, after the initial \"flare\" response, continued stimulation with GnRH agonists desensitizes the pituitary gland (by causing GnRH receptor downregulation) to GnRH. Pituitary desensitization reduces the secretion of LH and FSH and thus induces a state of hypogonadotropic hypogonadal anovulation, sometimes referred to as “pseudomenopause” or “medical oophorectomy.” GnRH agonists are able to completely shutdown gonadal testosterone production and thereby suppress circulating testosterone levels by 95% or into the castrate/female range in men.\n\nAgonists do not quickly dissociate from the GnRH receptor. As a result, initially there is an increase in FSH and LH secretion (so-called \"flare effect\"). Levels of LH may increase by up to 10-fold, while levels of testosterone generally increase to 140 to 200% of baseline values. However, after continuous administration, a profound hypogonadal effect (i.e. decrease in FSH and LH) is achieved through receptor downregulation by internalization of receptors. Generally this induced and reversible hypogonadism is the therapeutic goal. During the flare, peak levels of testosterone occur after 2 to 4 days, baseline testosterone levels are returned to by 7 to 8 days, and castrate levels of testosterone are achieved by 2 to 4 weeks. Following cessation of exogenous GnRH agonist it takes 5 to 8 days before normal gonadotropin secretion is completely restored.\n\nVarious medications can be used to prevent the testosterone flare and/or its effects at the initiation of GnRH agonist therapy. These include antigonadotropins such as progestogens like cyproterone acetate and chlormadinone acetate and estrogens like diethylstilbestrol, fosfestrol (diethylstilbestrol diphosphate), and estramustine phosphate; antiandrogens such as nonsteroidal antiandrogens like flutamide, nilutamide, and bicalutamide; and androgen synthesis inhibitors such as ketoconazole and abiraterone acetate.\n\nGnRH agonists are synthetically modeled after the natural GnRH decapeptide with specific modifications, usually double and single substitutions and typically in position 6 (amino acid substitution), 9 (alkylation) and 10 (deletion). These substitutions inhibit rapid degradation. Agonists with two substitutions include: leuprorelin, buserelin, histrelin, goserelin, and deslorelin. The agents nafarelin and triptorelin are agonists with single substitutions at position 6.\n\nGnRH analogues are also used in veterinary medicine. Uses include:\n\n\n\n"}
{"id": "3014519", "url": "https://en.wikipedia.org/wiki?curid=3014519", "title": "Goodell's sign", "text": "Goodell's sign\n\nIn medicine, Goodell sign is an indication of pregnancy. It is a significant softening of the vaginal portion of the cervix from increased vascularization. This vascularization is a result of hypertrophy and engorgement of the vessels below the growing uterus. This sign occurs at approximately four weeks' gestation. \n\nThe sign is named after William Goodell.\n\n\n"}
{"id": "51635965", "url": "https://en.wikipedia.org/wiki?curid=51635965", "title": "Haplarithmisis", "text": "Haplarithmisis\n\nHaplarithmisis (Greek for haplotype numbering) is a conceptual process in Genetics that enables simultaneous haplotyping and copy-number profiling of DNA samples derived from cells. Haplarithmisis also reveals parental, segregation, and mechanistic origins of genomic anomalies. The resulting profiles of haplarithmisis are called parental haplarithms (i.e. paternal haplarithm and maternal haplarithm).\n\nHaplarithmisis enabled a new form of preimplantation genetic diagnosis, such that not only segmental and full chromosome anomalies could be detected but also it allowed tracing back of these anomalies to meiosis and mitosis.\n\nIn its first application in basic genome research, haplarithmisis led to discovery of parental genome segregation, a phenomenon that causes the segregation of entire parental genomes in distinct blastomere lineages causing cleavage-stage chimerism and mixoploidy. \n"}
{"id": "50115585", "url": "https://en.wikipedia.org/wiki?curid=50115585", "title": "Health&amp;help", "text": "Health&amp;help\n\nHealth&Help is a non-profit humanitarian aid organization with a clinic in rural Guatemala and plans to expand to other areas in South and Central America. The project was founded in 2015 by Viktoria Valikova, MD, an infectious disease specialist from Ufa, Russia, who has been working in Central America since 2014, and Karina Basharova, the CEO of the project. Construction for the first clinic began in 2016 and its doors opened in early 2017. Health&Help staff includes volunteer physicians and other medical professionals, as well as locally hired staff in an effort to create jobs and empower local community members.\n\nThe first clinic was built in the village of Chuinajtayub (municipio de Momostenango, Totonicapan, Guatemala). Health&Help clinic is the only health center in this area; the nearest ambulatory clinic is in Momostenango, and hospitals of Quetzaltenango and Huehuetenango are several hours away and not accessible due to the time, distance and cost of travel. Health&Help clinic provides health services to a population 20,000 of people, including the surrounding villages. Construction began in 2016 and the clinic’s doors opened on February 24, 2017. This site maintains multiple permanent staff positions and is open year-round. Health&Help has plans to expand in the near future to other areas in Central and South America using a similar service model.\n\nIn 2014 Victoria Valikova, MD, the founder of the project, graduated from the Institute of Tropical Medicine Antwerp, Belgium, majoring in \"Tropical Medicine and Health Organization in countries with limited resources\". In connection with the specifics of the profession, a large part of her training was devoted to the problems of developing nations, non-governmental organizations and the development of medical infrastructure in those countries. Out of the list of places where doctors and nurses were needed most, Valikova chose Guatemala due to the overwhelming poverty and lack of health services. There are no clinics or ambulances in rural areas and they are far from big cities. Additionally there is no municipal transport to get people to the cities to seek medical treatment. Traditional medicine in Guatemala is generally at the most basic level; typically only healers and a local midwives.\n\nAfter returning to Russia, Valikova commenced work on the project. In Ufa, Russia, she met Karina Basharova, who became a CEO and the driving force of the project. Together they developed the idea to build a clinic, found volunteers through social networks, and raised money on the crowdfunding platforms Boomstarter and Generosity. Soon they found the architects Mikhail and Elizaveta Shishin, who previously built a school in Nepal, and who developed the Health&Help clinic's layout.\n\nIn June 2016 the construction process was started, and over 35 volunteers from all over the world came to help. Local people participated in the construction of the clinic as well. During the construction the doctors were seeing the patients in a small room at the local school, and the patient care was provided consistently despite the lack of space and shortage of resources. The construction was finished on February 24, 2017, and the clinic opened its doors for patients immediately. Today Health&Help is opened 24/7 for emergencies, and 8 hours a day/6 days a week as an ambulatory center for non-emergencies. The staff of the clinic sees from 40 to 70 patients a day, providing preventive care, delivering babies, doing ambulatory surgery, distributing medications and contraception, and educating local people about diabetes, hypertension management, child malnutrition, infectious diseases, and many other topics.\n\nThe medical staff of the clinic and the volunteers, who help the project remotely, work salary free. Physician and non-physician volunteers come from all over the world. The assignments last between a few weeks to eighteen months, and provide volunteers with a breadth of experiences, including direct hands on medical help, medical education, helping with the operations of the clinic, and immersion into local culture. Specialists and non-specialist volunteers who are able to commit to a long stay are invited free of charge, and provided housing, transportation, and three hot meals a day. There is a modest fee for short term visitors and students to put a limit on the number of temporary volunteers. The goal of the project is to create and maintain an ongoing relationship between the staff and the local people, and foster development of jobs occupied by locals.\n\n\n"}
{"id": "59094450", "url": "https://en.wikipedia.org/wiki?curid=59094450", "title": "Health in Dominica", "text": "Health in Dominica\n\nLife expectancy in Dominica was 74.2 years for men and 80.3 years for women in 2017. Infant mortality in 2013 was 8.0 per 1,000 live births.\n\nHealth services in Dominica are financed largely by general taxes. Government spending on health was 4.2% of GDP in 2011, equivalent to US$418 per capita. Primary care services are provided at no cost at 7 health centres and 44 clinics around the country. Princess Margaret Hospital is the main hospital. There is a smaller hospital at Portsmouth and cottage hospitals at Marigot and Grand Bay. There is some commercial outpatient care provided by private practitioners. Tertiary care is mostly provided outside the country. People under the age of 17 years old, pregnant women, the indigent, and those suffering from communicable diseases are exempt from medical care charges.\n\nThere are six ambulances operated by the fire department.\n\nThe government is part of the Eastern Caribbean Drug Service, a pooled procurement scheme for importing pharmaceuticals and medical supplies.\n\nAll Saints University School of Medicine is a commercial organisation based in Roseau.\n"}
{"id": "21282020", "url": "https://en.wikipedia.org/wiki?curid=21282020", "title": "Hearing", "text": "Hearing\n\nHearing, or auditory perception, is the ability to perceive sounds by detecting vibrations, changes in the pressure of the surrounding medium through time, through an organ such as the ear. The academic field concerned with hearing is auditory science.\n\nSound may be heard through solid, liquid, or gaseous matter. It is one of the traditional five senses; partial or total inability to hear is called hearing loss.\n\nIn humans and other vertebrates, hearing is performed primarily by the auditory system: mechanical waves, known as vibrations are detected by the ear and transduced into nerve impulses that are perceived by the brain (primarily in the temporal lobe). Like touch, audition requires sensitivity to the movement of molecules in the world outside the organism. Both hearing and touch are types of mechanosensation.\n\nThere are three main components of the human ear: the outer ear, the middle ear, and the inner ear.\n\nThe outer ear includes the pinna, the visible part of the ear, as well as the ear canal which terminates at the eardrum, also called the tympanic membrane. The pinna serves to focus sound waves through the ear canal toward the eardrum. Because of the asymmetrical character of the outer ear of most mammals, sound is filtered differently on its way into the ear depending on what vertical location it is coming from. This gives these animals the ability to localize sound vertically. The eardrum is an airtight membrane, and when sound waves arrive there, they cause it to vibrate following the waveform of the sound.\n\nThe middle ear consists of a small air-filled chamber that is located medial to the eardrum. Within this chamber are the three smallest bones in the body, known collectively as the ossicles which include the malleus, incus, and stapes (also known as the hammer, anvil, and stirrup, respectively). They aid in the transmission of the vibrations from the eardrum into the inner ear, the cochlea. The purpose of the middle ear ossicles is to overcome the impedance mismatch between air waves and cochlear waves, by providing impedance matching.\n\nAlso located in the middle ear are the stapedius muscle and tensor tympani muscle, which protect the hearing mechanism through a stiffening reflex. The stapes transmits sound waves to the inner ear through the oval window, a flexible membrane separating the air-filled middle ear from the fluid-filled inner ear. The round window, another flexible membrane, allows for the smooth displacement of the inner ear fluid caused by the entering sound waves.\n\nThe inner ear consists of the cochlea, which is a spiral-shaped, fluid-filled tube. It is divided lengthwise by the organ of Corti, which is the main organ of mechanical to neural transduction. Inside the organ of Corti is the basilar membrane, a structure that vibrates when waves from the middle ear propagate through the cochlear fluid – endolymph. The basilar membrane is tonotopic, so that each frequency has a characteristic place of resonance along it. Characteristic frequencies are high at the basal entrance to the cochlea, and low at the apex. Basilar membrane motion causes depolarization of the hair cells, specialized auditory receptors located within the organ of Corti. While the hair cells do not produce action potentials themselves, they release neurotransmitter at synapses with the fibers of the auditory nerve, which does produce action potentials. In this way, the patterns of oscillations on the basilar membrane are converted to spatiotemporal patterns of firings which transmit information about the sound to the brainstem.\n\nThe sound information from the cochlea travels via the auditory nerve to the cochlear nucleus in the brainstem. From there, the signals are projected to the inferior colliculus in the midbrain tectum. The inferior colliculus integrates auditory input with limited input from other parts of the brain and is involved in subconscious reflexes such as the auditory startle response.\n\nThe inferior colliculus in turn projects to the medial geniculate nucleus, a part of the thalamus where sound information is relayed to the primary auditory cortex in the temporal lobe. Sound is believed to first become consciously experienced at the primary auditory cortex. Around the primary auditory cortex lies Wernickes area, a cortical area involved in interpreting sounds that is necessary to understand spoken words.\n\nDisturbances (such as stroke or trauma) at any of these levels can cause hearing problems, especially if the disturbance is bilateral. In some instances it can also lead to auditory hallucinations or more complex difficulties in perceiving sound.\n\nHearing can be measured by behavioral tests using an audiometer. Electrophysiological tests of hearing can provide accurate measurements of hearing thresholds even in unconscious subjects. Such tests include auditory brainstem evoked potentials (ABR), otoacoustic emissions (OAE) and electrocochleography (ECochG). Technical advances in these tests have allowed hearing screening for infants to become widespread.\n\nThe hearing structures of many species have defense mechanisms against injury. For example, the muscles of the middle ear (e.g. the stapedius muscle) in many mammals contract reflexively in reaction to loud sounds which may otherwise injure the hearing ability of the organism.\n\nThere are several different types of hearing loss: Conductive hearing loss, sensorineural hearing loss and mixed types.\n\nThere are defined degrees of hearing loss:\n\n\n\nHearing protection is the use of devices designed to prevent Noise-Induced Hearing Loss (NIHL), a type of post-lingual hearing impairment. The various means used to prevent hearing loss generally focus on reducing the levels of noise to which people are exposed. One way this is done is through environmental modifications such as acoustic quieting, which may be achieved with as basic a measure as lining a room with curtains, or as complex a measure as employing an anechoic chamber, which absorbs nearly all sound. Another means is the use of devices such as earplugs, which are inserted into the ear canal to block noise, or earmuffs, objects designed to cover a person's ears entirely.\n\n\nHearing threshold and the ability to localize sound sources are reduced underwater in humans but not in aquatic animals, including whales, seals, and fishes which have ears adapted to process water borne sound. \nSome research suggests underwater hearing in humans may occur through bone conduction but with poor localization. This is related to differences of the speed of sound in water vs air and the blocking of normal air conducted sound paths.\n\nNot all sounds are normally audible to all animals. Each species has a range of normal hearing for both amplitude and frequency. Many animals use sound to communicate with each other, and hearing in these species is particularly important for survival and reproduction. In species that use sound as a primary means of communication, hearing is typically most acute for the range of pitches produced in calls and speech.\n\nFrequencies capable of being heard by humans are called audio or sonic. The range is typically considered to be between 20 Hz and 20,000 Hz. Frequencies higher than audio are referred to as ultrasonic, while frequencies below audio are referred to as infrasonic. Some bats use ultrasound for echolocation while in flight. Dogs are able to hear ultrasound, which is the principle of 'silent' dog whistles. Snakes sense infrasound through their jaws, and baleen whales, giraffes, dolphins and elephants use it for communication. Some fish have the ability to hear more sensitively due to a well-developed, bony connection between the ear and their swim bladder. The \"aid to the deaf\" of fishes appears in some species such as carp and herring.\n\nVertebrates aren't the only group of animals that have hearing. Some insects have hearing organs as well (e.g. the long-horned grasshopper, lubber grasshopper and the cicada); they use sound as a form of communication.\n\nSomething widely spread among insects is body hair, that can be made to swing by sonar waves. Due to the resonance phenomenon certain hairs swing stronger when exposed to a specific sonar-frequency. This specificity depends on the stiffness and the length of the hairs. That is why certain caterpillar species have evolved hair that would resonate with the sound of buzzing wasps, thus warning them of the presence of natural enemies. Moreover, mosquitoes have hair on their antennae that resonate with the flying sound of homogeneous females, enabling the males the ability to detect potential sexual partners.\n\nSome insects possess a tympanal organ. These are \"eardrums\", that cover air filled chambers on the legs. Similar to the hearing process with vertebrates, the eardrums react to sonar waves. Receptors that are placed on the inside translate the oscillation into electric signals and send them to the brain. Several groups of flying insects that are preyed upon by echolocating bats can perceive the ultrasound emissions this way and reflexively practice ultrasound avoidance.\n\nThe basilar membrane of the inner ear spreads out different frequencies: high frequencies produce a large vibration at the end near the middle ear (the \"base\"), and low frequencies a large vibration at the distant end (the \"apex\"). Thus the ear performs a sort of frequency analysis, roughly similar to a Fourier transform. However, the nerve pulses delivered to the brain contain both rate-versus-place and fine temporal structure information, so the similarity is not strong.\n\n\n\n\n\n\n"}
{"id": "46654330", "url": "https://en.wikipedia.org/wiki?curid=46654330", "title": "Helse (magazine)", "text": "Helse (magazine)\n\nHelse is a family and lifestyle magazine with a special reference to medical topics. The magazine is published in Copenhagen, Denmark. Its name refers to a traditional Danish word with the meaning of health.\n\n\"Helse\" was established by Toft Nielsen, an economist, in 1955. The magazine, based in Copenhagen, features articles on advances in health-care services. Helse-Active Living was the owner of the magazine until 2008. The Danish Medical Association supported the magazine, which was financed by the Federation of Sick-benefits Associations until 2012. It was distributed free of charge until that date and has been published six to eight times per year since then. The Pharmacists Association and local authorities began to finance the magazine in 2012.\n\nAt the beginning of the 1960s \"Helse\" sold nearly 310,000 copies.\n\nList of magazines in Denmark\n\n"}
{"id": "13071797", "url": "https://en.wikipedia.org/wiki?curid=13071797", "title": "Hospital network", "text": "Hospital network\n\nA hospital network is a network or group of hospitals that work together to coordinate and deliver a broad spectrum of services to their community. A hospital system or health care system is 2 or more hospitals owned, sponsored, or contract managed by a central organization.\n\nThe Catholic Church established a hospital system in Medieval Europe that vastly improves from the merely reciprocal hospitality of the Greeks and family-based obligations of the Romans. These hospitals were established to cater to \"particular social groups marginalized by poverty, sickness, and age,\" according to historian of hospitals, Guenter Risse.\n\nTo avoid financial losses due to shrinking reimbursements and rising costs as well as improving quality of care and avoid duplication of services, hospitals may consolidate certain services at one hospital. However, patients may need to travel farther if those services are no longer offered at their local hospital.\n"}
{"id": "25043987", "url": "https://en.wikipedia.org/wiki?curid=25043987", "title": "Integrated care", "text": "Integrated care\n\nIntegrated care, also known as integrated health, coordinated care, comprehensive care, seamless care, or transmural care, is a worldwide trend in health care reforms and new organizational arrangements focusing on more coordinated and integrated forms of care provision. Integrated care may be seen as a response to the fragmented delivery of health and social services being an acknowledged problem in many health systems.\n\nIntegrated care covers a complex and comprehensive field, and there are many different approaches to and definitions of the concept. WHO gives the following definition: \"Integrated care is a concept bringing together inputs, delivery, management and organization of services related to diagnosis, treatment, care, rehabilitation and health promotion. Integration is a means to improve services in relation to access, quality, user satisfaction and efficiency.\"\n\nThe integrated care literature distinguishes between different ways and degrees of working together and three central terms in this respect are autonomy, co-ordination, and integration. While autonomy refers to the one end of a continuum with least co-operation, integration (the combination of parts into a working whole by overlapping services) refers to the end with most co-operation and co-ordination (the relation of parts) to a point in between.\n\nDistinction is also made between horizontal integration (linking similar levels of care like multiprofessional teams) and vertical integration (linking different levels of care like primary, secondary, and tertiary care).\n\nContinuity of care is closely related to integrated care and emphasizes the patient's perspective through the system of health and social services, providing valuable lessons for the integration of systems. Continuity of care is often subdivided into three components: continuity of information (by shared records), continuity across the secondary-primary care interface (discharge planning from specialist to generalist care), and provider continuity (seeing the same professional each time, with value added if there is a therapeutic, trusting relationship).\n\nIntegrated care seems particularly important to service provision to the elderly, as elderly patients often become chronically ill and subject to co-morbidities and so have a special need of continuous care.\n\nCollaborative care is a related healthcare philosophy and movement that has many names, models, and definitions that often includes the provision of mental-health, behavioral-health and substance-use services in primary care. Common derivatives of the name collaborative care include integrated care, primary care behavioral health, integrated primary care, and shared care. \n\nThe Agency for Healthcare Research and Quality (AHRQ) published an overview of many different models as well as research that supports them. These are the key features of collaborative care models:\n\n\nThere are various national associations committed to collaborative care such as the Collaborative Family Healthcare Association.\n\nThe proper integrating of care does not mean the merging of roles. It remains uneconomical to make a physician serve as a nurse. Besides, the opposite approach is strictly prohibited by accreditation and certification schemes. The mix of staff for the various roles is maintained to enable a profitable integration in caring.\n\n\n"}
{"id": "41403638", "url": "https://en.wikipedia.org/wiki?curid=41403638", "title": "International League Against Epilepsy", "text": "International League Against Epilepsy\n\nThe International League Against Epilepsy was started in 1909. Its goal is to improve the lives of people with epilepsy through research.\n\nThey run the medical journal Epilepsia, Epilepsia Open, and Epileptic Disorders.\n"}
{"id": "905394", "url": "https://en.wikipedia.org/wiki?curid=905394", "title": "Jerzy Kukuczka", "text": "Jerzy Kukuczka\n\nJerzy Kukuczka (24 March 1948 in Katowice, Poland – 24 October 1989 Lhotse, Nepal) was a Polish alpine and high-altitude climber. Born in Katowice, his family origin is Goral. On 18 September 1987, he became the second man (after Reinhold Messner), to climb all fourteen eight-thousanders in the world, it took him less than 8 years. He is the only person in the world who has climbed two eight-thousanders in one winter. Altogether, he ascended four eight-thousanders in winter, including three as first ascents. Along with Tadeusz Piotrowski, Kukuczka established a new route on K2 in alpine style (the so-called \"Polish Line\"), which no one has repeated.\n\nKukuczka is widely considered among the climbing community to be one of the best high-altitude climbers in history. He ascended all fourteen eight-thousanders in just seven years, 11 months and 14 days - He held the world record for shortest time span to summit the eight-thousanders for nearly 27 years until May 2014 when Kim Chang-ho beat his mark by one month and eight days. Unlike many prominent high-altitude climbers of his time, the routes Kukuczka chose on the Himalayan giants were usually original, many of them first ascents and often done in the grip of winter wind and cold. During his career, Kukuczka established ten new routes (still unbeaten record) and climbed four summits in winter. He was one of an elite group of Polish Himalayan mountaineers who specialized in winter ascents (called \"Ice Warriors\"). \n\nIn an era in Poland where even the most basic foods were scarce, Kukuczka was able successfully to mount and equip numerous ventures to the far-flung reaches of the world. Usually pressed for cash and equipment, he painted factory chimneys to earn precious złotys to finance his mountaineering dreams.\n\nHe climbed all summits, except for Mount Everest, without the use of supplemental oxygen.\n\nKukuczka died attempting to climb the unclimbed South Face of Lhotse in Nepal on 24 October 1989. He was leading a pitch at an altitude of about 8,200 meters on a 6 mm secondhand rope he had picked up in a market in Kathmandu (according to Ryszard Pawłowski, Kukuczka's climbing partner on the tragic day, the main single rope used by the team was too jammed to be used and the climbers decided to use transport rope instead). When he lost his footing and fell, the cord was either cut or snapped from the fall, plunging Kukuczka ~2000 meters to his death. Kukuczka's body was never found, but the official version was that he was buried in an icy crevasse near the place of fall. Such a step was dictated by the need to find the body to pay compensation to the deceased's family. \n\n\n\n"}
{"id": "12652314", "url": "https://en.wikipedia.org/wiki?curid=12652314", "title": "John Belisario", "text": "John Belisario\n\nJohn Belisario (1820-1900) was an Australian dental surgeon who was a pioneer in the use of anaesthesia in dentistry.\n\nBorn in England of Spanish ancestry to a slaveholding family, Belisario was a frail child who was sent to his uncle's plantation in the West Indies to improve his health. He returned to England and served an apprenticeship at St Thomas's Hospital in London but, having developed a preference for warmer climes, he decided to move to Sydney, Australia in 1841.\n\nIn June 1847 Belisario used ether to anaesthetise a patient; he is believed to be the first person in Australia to do so. An account of Belisario's use of ether was reported in The Sydney Morning Herald on 16 June 1847. In an advertisement in the same year, Belisario announced that he was able, with the use of \"ethereal inhalation, to perform the most difficult operations in dental surgery, with perfect freedom from pain\". Being a pioneer in the use of anaesthesia in dentistry brought him substantial fame. In 1854 he was awarded an honorary doctorate from the Baltimore College of Dental Surgery. He was a corresponding member of the Academy of Natural Sciences, Philadelphia, the Odontological Society of London, and the American Academy of Dental Surgery.\n\nBelisario met Mary Longfield of Cork on the voyage to Australia in 1841. They were married on 2 January 1843 at St Peter's Church of England, Campbelltown. Mary died, aged 24, on 31 May 1849. He then married Isabella Helen, daughter of Dr Ramsay of Dobroyd. Belisario died on 17 June 1900. He was survived by Isabella, three children of his first marriage, and two of the second marriage.\n\nBelisario family. \"Belisario family miscellaneous papers, 1819, 1860-1900, being mainly the correspondence of Dr and Mrs John Belisario\" State Library of New South Wales. Retrieved 7 June 2016\".\"\n"}
{"id": "13294825", "url": "https://en.wikipedia.org/wiki?curid=13294825", "title": "John G. Rangos School of Health Sciences", "text": "John G. Rangos School of Health Sciences\n\nThe John G. Rangos School of Health Sciences (RSHS) is one of the ten constituent colleges that comprise Duquesne University in Pittsburgh, Pennsylvania.\n\nOn January 29, 1990, Dr. John E. Murray, Jr., then President of Duquesne University, announced the creation of a School of Health Sciences in order to \"graduate professionals who will provide assistance to people in maintaining their physical well-being. It will make them more self-sufficient physically and provide great hope for their futures. It meets an overwhelming societal need, and it enlarges opportunities for students at Duquesne. In serving the citizens of Western Pennsylvania, and our students, this initiative is precisely in accordance with the purposes of Duquesne University.\" On March 18, 1991, Mr. John G. Rangos, Sr. made a major gift to Duquesne University in support of the school, and so it was announced that the school would be named after him.\n\nThe Rangos School houses such programs as Athletic Training (B.S.), Health Management Systems (B.S., M.H.M.S.), Occupational Therapy (M.S.), Physical Therapy (D.P.T.), Physician Assistant (M.P.A.), Speech-Language Pathology (M.S.), and the Ph.D. in Rehabilitation Science. In addition, the school offers a variety of bachelor's degree programs, entry-level master's degree programs, master's degree programs, a doctor of physical therapy program, joint and second degree opportunities, and inter-school majors and minors.\n\nThe dean of the school is Gregory H. Frazer, Ph.D.\n\n"}
{"id": "37959983", "url": "https://en.wikipedia.org/wiki?curid=37959983", "title": "Korte's law", "text": "Korte's law\n\nIn psychophysics, Korte's law, also known more completely as Korte's third law of apparent motion, is an observation relating the phenomenon of apparent motion to the distance and duration between two successively presented stimuli. It was originally proposed in 1915 by Adolf Korte. A modern formulation of the law is that the greater the length of a path between two successively presented stimuli, the greater the stimulus onset asynchrony (SOA) must be for an observer to perceive the two stimuli as a single mobile object. Typically, the relationship between distance and minimal SOA is linear.\n\nArguably, Korte's law is counterintuitive. One might expect that successive stimuli are \"less\" likely to be perceived as a single object as both distance and SOA increase, and therefore, a negative relationship should be observed instead. In fact, such a negative relationship can be observed as well as Korte's law. Which relationship holds depends on speed.\n"}
{"id": "3551105", "url": "https://en.wikipedia.org/wiki?curid=3551105", "title": "List of cancer mortality rates in the United States", "text": "List of cancer mortality rates in the United States\n\nDifferent types of cancer can vary wildly in their prognosis. While the stage of cancer at diagnosis is most relevant to the survival of an individual patient, the type of cancer suggests an overall survival rate of the population.\n\nThe figures below are an overall reflection of mortality rates throughout the U.S. population. For example, those diagnosed with breast or prostate cancer have a much better outcome than those diagnosed with lung or stomach cancer. In most statistical records, cancers are grouped by location, although some cancers of the same location can have extremely variable survival rates depending on the type of cancer. For example, stage 1 pancreatic adenocarcinoma has a 5-year survival rate of 12%, while stage 1 pancreatic neuroendocrine tumors have a 5-year survival rate of 61%.\nBetween 2007 and 2013, the percentage of cancer patients alive within five years after cancer diagnosis are displayed in the table below. These figures represent all deaths, whether due to the cancer itself, or death from another cause in a person with cancer.\n\nNote: \"This is not a complete list of cancer mortality rates as published by the NCI. These figures are at least five years old and do not reflect recent advances in medicine that have improved the detection and treatments of cancer and their outcomes. Again, these are average death rates that should not be assumed to apply to individuals, whose prognoses will vary depending on age, sex, race, general health, swiftness of detection, type of treatment, progression of disease, and complicating factors.\"\n\n\n"}
{"id": "2529380", "url": "https://en.wikipedia.org/wiki?curid=2529380", "title": "List of nursing specialties", "text": "List of nursing specialties\n\nIn the modern world, there are a number of nursing specialities.\n\nProfessional organizations or certifying boards issue voluntary certification in many of these specialties.\n\n"}
{"id": "51301214", "url": "https://en.wikipedia.org/wiki?curid=51301214", "title": "MDsave", "text": "MDsave\n\nMDsave is an online platform. Founded in 2011 in Brentwood, Tennessee officially launched in 2013, the company connects uninsured patients, health savings account holders, and high deductible health insurance patients with medical providers who offer pre-negotiated savings on medical services.\n\nMDsave was founded in 2011 in Brentwood, Tennessee by Paul Ketchel, the current CEO, to meet growing demand for accessible pricing, communication and upfront costs. Paul Ketchel was formerly chief operating officer of Diagnostics Network Alliance and a director for American Capitol Group.\n\nThe company officially launched in 2013. They had raised more than $14 million from investors and earned $5 million in 2015, with investors including MTS Health Partners. By 2015, it was in place in 24 states.\n\nIn August 2016, MDSave secured a $5 million investment from Cambia Health Solutions. In 2016, the company was doing around 2,000 transactions per month.\n\nIn June 2018, MDsave began working with St. Cloud Regional Medical Center.\n\nMDsave offers medical providers a way to package their services into a single bundled procedure online, and offers consumers paying out of pocket a way to comparison shop different providers. It is the world’s first transactional healthcare marketplace.\n\nA patient searches for a treatment or procedure in a location, and MDsave returns a list of providers in that area with price, location and doctor’s ratings. A patient purchases the bundled procedure, which includes all associated fees and services, using PayPal, a credit card or a financing plan online. The patient receives a voucher to present to the doctor’s office at their first appointment. The providers get paid within six days, versus the average 60–90 days through insurance.\n\nMDsave uses a patented technology that cross-references Medicare billing codes, procedural codes, and diagnosis codes to create an upfront price for the bundled services involved in a single treatment or procedure.\n\nMDsave Plus is geared toward employers as a supplement to employee health benefits. MDsave negotiates prices with providers and then passes the savings along to employees, who can save up to 60% on their medical costs.\n\nMDsave works with hospital groups and ambulatory surgical centers. In 2016 it had a presence in 24 states and 120 markets, and was growing at a rate of 20% month over month. Providers in 2015 included Dignity Health, Catholic Health Initiatives, Community Health Systems, Tenet Healthcare and others.\n\nBoard members include former Senate Majority Leader Dr. Bill Frist.\n"}
{"id": "27606148", "url": "https://en.wikipedia.org/wiki?curid=27606148", "title": "Menzies Institute for Medical Research", "text": "Menzies Institute for Medical Research\n\nThe Menzies Institute for Medical Research is an Australian medical research institute of the University of Tasmania based in Hobart, Tasmania. Formerly known as the Menzies Centre for Population Health Research, the institute was established in 1988 and conducts innovative, world-class medical research to improve human health and well-being.\n\nIn the late 1980s, the Menzies Foundation supported the establishment of an epidemiology research centre at the University of Tasmania in Hobart, to be named the Menzies Centre for Population Health Research. The Foundation provided annual funding to the Institute and was successful in obtaining matching funds from the Tasmanian Government. The Menzies Centre for Population Health Research was formed in 1988 and became the Menzies Research Institute in 2004.\n\nFrom modest beginnings, the Menzies Research Institute quickly gained a reputation for its ground-breaking work into the link between babies’ sleeping position and sudden infant death syndrome (SIDS). Menzies developed into an established centre for population health research, with a global reputation in epidemiology. Some notable successes included highlighting the importance of vitamin D in the development of bones in children and adults; showing evidence of the link between early life sun exposure and susceptibility to multiple sclerosis; discovering the link between babies' sleeping position and sudden infant death syndrome (SIDS); and discovering platelets found in the blood kill the malaria parasite during the early stages of a malarial infection.\n\nThe Menzies Institute for Medical Research aspires to contribute significantly to human health and wellbeing, with particular emphasis upon research that takes advantage of Tasmania's unique population resource and other competitive advantages. Its research efforts focus on preventing a range of diseases including cancer, multiple sclerosis, cardiovascular disease, diabetes, chronic lung disease, osteoporosis, epilepsy and dementia.\n\nThe Institute's five major research themes are public health and primary care, brain diseases and injury, heart and blood vessels, bone and muscle health, and cancer, genetics and immunology.\n\nThe Menzies Research Institute Tasmania has two research centres; the Australian Cancer Research Foundation Tasmanian Inherited Cancer Centre and the Wicking Dementia and Research Education Centre.\n\n"}
{"id": "51789317", "url": "https://en.wikipedia.org/wiki?curid=51789317", "title": "Neurological Society of India", "text": "Neurological Society of India\n\nThe Neurological Society of India (NSI) is the apex body representing neuroscientists of the country. It was founded in 1951 by Jacob Chandy, Balasubramaniam Ramamurthi, S. T. Narasimhan, and Baldev Singh, who together have been credited to be pioneers in development of epilepsy surgery in India. The society appointed Jacob Chandy as its first President. The society publishes the bi-monthly journal \"Neurology India\".\n\nNeurosurgeon Jacob Chandy and neurologist Baldev Singh were associated with Christian Medical College & Hospital, Vellore and physicianelectrophysiologist S. T. Narasimhan and neurosurgeon Balasubramaniam Ramamurthi were from Madras Medical College & Hospital. They together founded the society in 1951 and had its first meeting in March 1952 at Hyderabad and published its own journal dedicated to the neurosciences titled \"Indian JournalNeurology\". There being another journal by the same name, the society decided to rename their journal to \"Neurology India\" in 1953.\n\nSince its inception, the society held its annual meetings with the Association of Physicians of India but starting next year, it established its own annual conference and created its sub-sections of Neurology, Neurosurgery, Neuropathology and Neuroradiology in the Society. On 25 October 1969, the society registered itself with the Charity Commissioner, Public Trusts Registration Office, Greater Bombay Region (No. F-1819 (B)).\n\nIn 1972, the society formed a sub-committee for standardization of Postgraduate Education. In 1977, the society became the first medical professional society in the country to conduct the continuing medical education by providing \"updates on selected topics and encouraging interdisciplinary interaction among trainees in the various branches of the Neurological Sciences\". In 1979, NSI founded a new association, the Satellite Conference of Neuro-nurses of India, as its branch held with the main objective to \"set high standards of neuro-nursing in the country\". The conferences of the association are conducted annually along with the annual conference of NSI. In 1977, the society formed four Working Groups to deal with Medical Education and Training, Research priorities, Manpower requirements and development of Services, Community Programme, and Instrumentation along with two new sub-sections, Neurophysiology and Electroencephalography. In 1979, the Association of Neurological Nurses as another sub-section was formed and named as \"Society of Indian Neurosciences Nurses (SINN)\". The society formed a sub-section of Neurophysiological Technologists in 1988 and named as \"Association of Neurophysiological Technologists of India (ANTI)\".\n\nThe society instituted the \"Dr. Jacob Chandy Oration\" in 1969 and the \"Dr. B. Ramamurthi Oration\" in 1974, both to be held every two years along with the \"Dr. R.G. Ginde Oration\" in 1988, the \"Dr. Baldev Singh Oration\" in 1994, and the \"Dr. Tandon Oration\" in 1998 which are to be held every three years. Only two orations are organized during the annual conference each year and the orators are selected by the incumbent and immediate two past Presidents of the society. The society has five subspeciality societies associated with it which includes the Indian Society for Paediatric Neurosurgery, the Indian Society for Stereotactic and Functional Neurosurgery, the Skull Base Surgery Society, the Neurotraumatology Society of India, and the Cerebrovascular Society of India.\n\nThe Neurological Society of India is affiliated to the World Federation of Neurosurgical Societies, the World Federation of Neurology, the International Society of Neuropathology, the International Federation of Societies for Electroencephalography and Clinical Neurophysiology and there are two associations affiliated to the NSI, the Association of Neurological Nurses and the Association of Neurophysiological Technologists. The society has over 2000 members and has been involved in organising seminars and conferences to initiate interaction among practicing neurologists. In 2014, the society launched an awareness campaign called \"Heads.. We Win\" to encourage wearing of helmets, the lack of which causes major head injuries.\n\nIn 2008, the society instituted a Lifetime Achievement Award and following 21 recipients have been awarded; S. N. Bhagwati, H. M. Dastur, Prakash Narain Tandon, and Noshir Wadia in 2008, Sanatan Rath and Krishnamoorthy Srinivas in 2009, A. K. Banerji, V. S. Dave, and J. S. Chopra in 2010, R. Marthanda Varma and G. N. N. Reddy in 2011, Anupam Das Gupta and B. S. Das in 2012, B. J. Damany, V. K. Kak, and G. M. Taori in 2013 and P. S. Ramani, G. Arjun Das, S. Kalyanraman, T. S. Kanaka, and Ashru K. Banerjee in 2014 The award is conferred only on the life members of the society above the age of 75 years with the significant contribution to the development of Neurosciences in India and is presented during the inaugural function of the annual conference. In 2014, the General body of the society revised and passed its constitution and bye-laws to award only three recipients per year.\n\nVarious individuals who have been associated with the society as honorary members include John A Simpson, John Walton, Baron Walton of Detchant, Theodore Rasmussen, Lindsay Simon, Majid Samii, Tetsuo Kanno, Balasubramaniam Ramamurthi, Jacob Chandy, and T. K. Ghosh.\n\n"}
{"id": "11741505", "url": "https://en.wikipedia.org/wiki?curid=11741505", "title": "Paul Sarvela", "text": "Paul Sarvela\n\nPaul Douglas Sarvela (August 7, 1959 – November 9, 2014) was Professor of health education and Dean of the College of Applied Sciences and Arts at Southern Illinois University Carbondale (SIUC). The college is unique in being the only comprehensive, technically oriented college that is part of a major research university. Following the departure of Rita Cheng to become president of Northern Arizona University, Sarvela was named as interim chancellor July 8, 2014, and was confirmed July 24, 2014. Born in Gloucester, Massachusetts, he died on November 9, 2014 in Carbondale, Illinois while undergoing treatment for cancer.\n\nHe is author or coauthor of more than 60 publications in peer-reviewed journals, as well as several textbooks, monographs, and technical reports. With more than 130 conference papers to his credit, his work has been presented nationwide in the United States, as well as in parts of Europe, where he has been a visiting professor at the University of Cologne (Germany) and lectured in Finland.\n\nSarvela earned a bachelor's degree in psychology in 1981, a master's degree in educational psychology in 1983 and a doctorate in health education in 1984, all from the University of Michigan. He began his professional career as a program evaluator for Ford Aerospace and Communications Corporation (1984–86), primarily doing classified work for the National Security Agency.\n\nIn 1986 he joined the faculty of SIUC’s highly regarded Department of Health Education, where he was widely regarded as the replacement for recently departed Robert S. Gold. Many saw him as being potentially a “young Bob Gold”. He was promoted to the rank of associate professor in 1989 and to full professor in 1992.\n\nCollaborating with Elena Sliepcevich and David Duncan, his publications from the early 1980s largely focused on the study of risk taking and risk reduction related to alcohol and other drug use in youth. Many of the studies during this early stage of his career were conducted in rural settings, thus providing a catalyst for his later work in the SIU-C Center for Rural Health and Social Service Development. Throughout his career many of his publications have been concerned with instrument development, needs assessment, program evaluation, and the creation of responsive interventions and curricula. His current research blends the rural setting; the multiple health-related problems of that setting; and the challenging issues of measurement and evaluation.\n\nDuring the 1996-1997 academic year, Dr Sarvela was an American Council on Education (ACE) Fellow at the University of Wisconsin–Madison. This experience afforded him an opportunity to enhance his skills in educational leadership, strategic planning, and systems operations. Aspects of this experience became two chapters (\"Needs Assessment and Strategic Planning\"; \"Assessing Program Costs and Effects\") in the book Health Education Evaluation and Measurement - A Practitioner's Perspective, 2nd edition, which he coauthored with Dr Robert J McDermott in 1999. This fellowship also contributed to his movement into administration first as Director of the Center for Rural Health and Social Service Development. He became chairman of the Department of Health Care Professions in 1999. In 2002, he was appointed to his present position as Dean of the College of Applied Sciences and Arts.\n\n"}
{"id": "5464750", "url": "https://en.wikipedia.org/wiki?curid=5464750", "title": "Personal injury lawyer", "text": "Personal injury lawyer\n\nA personal injury lawyer is a lawyer who provides legal services to those who claim to have been injured, physically or psychologically, as a result of the negligence of another person, company, government agency or any entity. Personal injury lawyers tend to practice primarily in the area of law known as tort law. Examples of common personal injury claims include injuries from slip and fall accidents, traffic collisions, defective products, workplace injuries and professional malpractice.\n\nThe term \"trial lawyers\" is sometimes used to refer to personal injury lawyers, even though many other types of lawyers, including defense lawyers and criminal prosecutors also appear in trials and even though most personal injury claims are settled without going to trial.\n\nA personal injury lawyer must qualify to practice law in the jurisdiction in which the lawyer practices. In many states, they must also pass a written ethics examination.\n\nLawyers may take continuing legal education (CLE) classes in order to learn about developments in the law or to learn about new practice areas. In states that require lawyers to attend CLE, personal injury lawyers may take CLE courses relevant to personal injury law, but are not required to do so.\n\nSome bar associations and attorney organizations offer certifications, including certification of lawyers in the field of personal injury. Certification is not required to practice personal injury law, but may help a lawyer demonstrate knowledge in the field to potential clients. Within the U.S., not all state bars offer certification for personal injury law. Some states, such as New Jersey, allow lawyers to become Certified Trial Attorneys, a credential that is available to both plaintiff and defense attorneys. Some states, such as Arizona, restrict the use of the words \"specialist\" or \"specialize\" to lawyers who have obtained a certification from the State Bar Board of Legal Specialization in a specific field of law, with one such certification being in the area of personal injury law.\n\nLawyers may concentrate their practice to specific areas of law, including personal injury law. Some lawyers may further specialize to a specific area of personal injury, such as medical malpractice law. By limiting the range of cases they handle, personal injury lawyers are able to acquire specialized knowledge and experience.\n\nBefore accepting a new case, a personal injury lawyer will normally interview a prospective client and evaluate the client's case to determine the basic facts and potential legal claims that might be made, identify possible defendants, and evaluate the strength of the case. A lawyer may decline to accept a case if the lawyer believes that the legal claims will not succeed in court, if the cost of litigation is expected to exceed the amount that can reasonably be recovered from the defendants as compensation for the client's injury.\n\nLawyer fees may be charged in a number of ways, including contingency fees, hourly rates, and flat fees. In many countries, personal injury lawyers work primarily on a contingency fee basis, sometimes called an \"if-come\" fee, through which the lawyer receives a percentage of a client's recovery as a fee, but does not recover a fee if the claim is not successful.\n\nIn some jurisdictions, or by virtue of the retainer agreement between an attorney and client, the amount of the legal fee may vary depending upon whether a case settles before a lawsuit is filed, after a lawsuit is filed but before trial, or if the case goes to trial. For example, a retainer agreement might provide that a lawyer will receive a 33 and 1/3% contingency fee if a case settles before a lawsuit is filed, and a 40% contingency fee if the case settles after the lawsuit is filed.\n\nDue to the high cost of litigation, personal injury lawyers are rarely retained to work based on an hourly fee. However, defense attorneys who are hired to contest personal injury claims are often paid on an hourly basis.\n\nAn attorney should provide diligent representation to clients, and the ultimate professional responsibility of a personal injury lawyer is to help plaintiffs obtain just compensation for their losses. As with all lawyers, the attorney-client relationship is governed by rules of ethics.\n\nIn the United States, lawyers are regulated by codes of conduct established by state bar associations, which have the power to take disciplinary action against lawyers who violate professional or ethical regulations. States normally require all contingency agreements between lawyers and their clients to be in writing, and may limit the amount that may be charged as a contingency fee to a specific maximum percentage of the recovery.\n\nAlthough membership is not required for personal injury practice, many personal injury lawyers join professional associations. For example:\n\n\nCritics of personal injury lawyers claim that litigation increases the cost of products and services, and the cost of doing business.\n\nFor example, critics of medical malpractice lawyers argue that lawsuits increase the cost of healthcare, and that lawsuits may inspire doctors to leave medical practice or create doctor shortages. These concerns have not been well substantiated. A publication by the Robert Wood Johnson Foundation found little evidence that traditional tort reforms affect medical liability costs or defensive medicine. A study conducted on a bi-partisan basis in Texas has found that tort reform, once enacted had no impact on reducing the cost of medical care, tending to throw doubt on claims made by tort reform advocates.\n\n\n"}
{"id": "826478", "url": "https://en.wikipedia.org/wiki?curid=826478", "title": "Population viability analysis", "text": "Population viability analysis\n\nPopulation viability analysis (PVA) is a species-specific method of risk assessment frequently used in conservation biology.\nIt is traditionally defined as the process that determines the probability that a population will go extinct within a given number of years.\nMore recently, PVA has been described as a marriage of ecology and statistics that brings together species characteristics and environmental variability to forecast population health and extinction risk. Each PVA is individually developed for a target population or species, and consequently, each PVA is unique. The larger goal in mind when conducting a PVA is to ensure that the population of a species is self-sustaining over the long term.\n\nPopulation viability analysis (PVA) is used to estimate the likelihood of a population’s extinction and indicate the urgency of recovery efforts, and identify key life stages or processes that should be the focus of recovery efforts. PVA is also used to compare proposed management options and assess existing recovery efforts. PVA is frequently used in endangered species management to develop a plan of action, rank the pros and cons of different management scenarios, and assess the potential impacts of habitat loss.\n\nIn the 1970s, Yellowstone National Park was the centre of a heated debate over different proposals to manage the park’s problem grizzly bears (\"Ursus arctos\"). In 1978, Mark Shaffer proposed a model for the grizzlies that incorporated random variability, and calculated extinction probabilities and minimum viable population size. The first PVA is credited to Shaffer.\n\nPVA gained popularity in the United States as federal agencies and ecologists required methods to evaluate the risk of extinction and possible outcomes of management decisions, particularly in accordance with the Endangered Species Act of 1973, and the National Forest Management Act of 1976.\n\nIn 1986, Gilpin and Soulé broadened the PVA definition to include the interactive forces that affect the viability of a population, including genetics.\nThe use of PVA increased dramatically in the late 1980s and early 1990s following advances in personal computers and software packages.\n\nA PVA for the endangered Fender's blue butterfly (\"Icaricia icarioides\") was recently performed with a goal of providing additional information to the United States Fish and Wildlife Service, which was developing a recovery plan for the species. The PVA concluded that the species was more at risk of extinction than previously thought and identified key sites where recovery efforts should be focused. The PVA also indicated that because the butterfly populations fluctuate widely from year to year, to prevent the populations from going extinct the minimum annual population growth rate must be kept much higher than at levels typically considered acceptable for other species.\n\nFollowing a recent outbreak of canine distemper virus, a PVA was performed for the critically endangered island fox (\"Urocyon littoralis\") of Santa Catalina Island, California. The Santa Catalina island fox population is uniquely composed of two subpopulations that are separated by an isthmus, with the eastern subpopulation at greater risk of extinction than the western subpopulation. PVA was conducted with the goals of 1) evaluating the island fox’s extinction risk, 2) estimating the island fox’s sensitivity to catastrophic events, and 3) evaluating recent recovery efforts which include release of captive-bred foxes and transport of wild juvenile foxes from the west to the east side. Results of the PVA concluded that the island fox is still at significant risk of extinction, and is highly susceptible to catastrophes that occur more than once every 20 years. Furthermore, extinction risks and future population sizes on both sides of the island were significantly dependent on the number of foxes released and transported each year.\n\nPVAs in combination with sensitivity analysis can also be used to identify which vital rates has the relative greatest effect on population growth and other measures of population viability. For example, a study by Manlik \"et al.\" (2016) forecast the viability of two bottlenose dolphin populations in Western Australia and identified reproduction as having the greatest influence on the forecast of these populations. One of the two populations was forecast to be stable, whereas the other population was forecast to decline, if it isolated from other populations and low reproductive rates persist. The difference in viability between the two studies was primarily due to differences in reproduction and not survival. The study also showed that temporal variation in reproduction had a greater effect on population growth than temporal variation in survival.\n\nDebates exist and remain unresolved over the appropriate uses of PVA in conservation biology and PVA’s ability to accurately assess extinction risks.\n\nA large quantity of field data is desirable for PVA; some conservatively estimate that for a precise extinction probability assessment extending \"T\" years into the future, five-to-ten times \"T\" years of data are needed. Datasets of such magnitude are typically unavailable for rare species; it has been estimated that suitable data for PVA is available for only 2% of threatened bird species. PVA for threatened and endangered species is particularly a problem as the predictive power of PVA plummets dramatically with minimal datasets. Ellner et al. (2002) argued that PVA has little value in such circumstances and is best replaced by other methods. Others argue that PVA remains the best tool available for estimations of extinction risk, especially with the use of sensitivity model runs.\n\nEven with an adequate dataset, it is possible that a PVA can still have large errors in extinction rate predictions. It is impossible to incorporate all future possibilities into a PVA: habitats may change, catastrophes may occur, new diseases may be introduced. PVA utility can be enhanced by multiple model runs with varying sets of assumptions including the forecast future date. Some prefer to use PVA always in a relative analysis of benefits of alternative management schemes, such as comparing proposed resource management plans.\n\nAccuracy of PVAs has been tested in a few retrospective studies. For example, a study comparing PVA model forecasts with the actual fate of 21 well-studied taxa, showed that growth rate projections are accurate, if input variables are based on sound data, but highlighted the importance of understanding density-dependence (Brook \"et al.\" 2000). Also, McCarthey \"et al.\" (2003) showed that PVA predictions are relatively accurate, when they are based on long-term data. Still, the usefulness of PVA lies more in its capacity to identify and assess potential threats, than in making long-term, categorical predictions (Akçakaya & Sjögren-Gulve 2000).\n\nImprovements to PVA likely to occur in the near future include: 1) creating a fixed definition of PVA and scientific standards of quality by which all PVA are judged and 2) incorporating recent genetic advances into PVA.\n\n\n"}
{"id": "5555430", "url": "https://en.wikipedia.org/wiki?curid=5555430", "title": "Poulaphouca", "text": "Poulaphouca\n\nPoulaphouca, officially Pollaphuca (), is the name of a waterfall and bridge on the River Liffey between County Wicklow and County Kildare. It is primarily known for its hydroelectric generating station and artificial lake, known as Poulaphouca Reservoir, Poulaphouca Lake, or Blessington Lake. The once famous Poulaphouca Waterfall has little water running over it any longer because of the hydroelectric project.\n\n<nowiki>A waterfall immediately west of the bridge, renowned as a beauty spot from at least the 18th century, was lost with the construction of the Poulaphouca Reservoir. The waterfall, marked as '</nowiki>\"<nowiki>Poolapooka - a remarkable cataract'</nowiki>\" on Noble & Keenan's map of 1752, is depicted and described in the \"Post-Chaise Companion\" of 1786, when Ballymore parish was still within Dublin: \n\nBoth Pollaphuca and a second bridge crossing a dry gorge 150m to the southwest, were designed by Alexander Nimmo in Gothic style and built between 1822 and 1827 for a total cost of £4,704. Poulaphouca Bridge replaced Horsepass Bridge to the northeast, now under Poulaphouca Reservoir. Wright's \"Guide to the County of Wicklow\" describes the bridge and waterfall in 1827:\n\nA four and a half mile extension of the Dublin and Blessington Steam Tramway terminating at Poulaphouca, built by the Blessington and Poulaphouca Steam Tramway, opened on May 1, 1895 and closed in 1932. The ticket office survives as a private residence immediately northeast of the bridge on the N81 road.\n\n\n\n"}
{"id": "32465168", "url": "https://en.wikipedia.org/wiki?curid=32465168", "title": "Prevention Project Dunkelfeld", "text": "Prevention Project Dunkelfeld\n\nThe Prevention Project Dunkelfeld (PPD) is an effort founded in Germany to provide clinical and support services to individuals who are sexually attracted to children (pedophiles and hebephiles) and want help controlling their sexual urges, but are otherwise unknown to the legal authorities. The term \"dunkelfeld\" is German for \"dark field.\" The project began in Berlin in June 2005 with a large media campaign to contact pedophiles and hebephiles who wanted help from clinicians to manage their paraphilia. The campaign pledged medically confidential treatment free-of-charge. It was initially funded by the Volkswagen Foundation, and has been financially supported by the German government since 2008. The project's slogan is \"You are not guilty because of your sexual desire, but you are responsible for your sexual behavior. There is help! Don’t become an offender!\" \n\n1,134 men had responded by 2010. 499 had a completed diagnosis, and 255 had been offered a place in therapy. More than half had previously attempted to find therapy without success. The therapy offered has three main components. Patients are encouraged to accept their sexual inclinations, integrate it into their self-concept, and involve relatives or partners in the therapeutic process. Cognitive behaviour therapy is used to improve coping skills, stress management, and sexual attitudes. Drugs that reduce general sex drive, such as serotonin reuptake inhibitors and anti-androgens, may also be offered. The PPD and the individuals who join the organization have been the subject of several research studies.\n\n\n"}
{"id": "19817658", "url": "https://en.wikipedia.org/wiki?curid=19817658", "title": "Remote data capture", "text": "Remote data capture\n\nRemote data capture is the process of automatic collection of scientific data. It is widely used in clinic trials, where it is referred to as electronic data capture. In physical sciences, automatic observation hardware in the field can be linked to an observer in a laboratory through a cellphone or other communication link, for example in hydrology. RDC systems influenced the design of later electronic data capture (EDC) systems.\n"}
{"id": "42335757", "url": "https://en.wikipedia.org/wiki?curid=42335757", "title": "Saint Petersburg Children's Hospice", "text": "Saint Petersburg Children's Hospice\n\nThe Medical institution \"Children's Hospice\" (Russian: \"Медицинское учреждение «Детский хоспис»\") - is a non-profit institution of pediatric palliative care for minors under 18 years. The first children's hospice in Russia.\n\nThe St. Petersburg Children's Hospice has two facilities: one in the city park “Kurakina Dacha” near the St. Petersburg River Station and another one in the village of Ol'gino. The Children's Hospice provides physical, psychological, emotional, social and spiritual care. The basic idea of the hospice care is to enable children with severe and incurable diseases and all members of the child's family to live a full life.\n\nThe St. Petersburg Children's Hospice started its work in 2003 under the guidance of Archpriest Alexander Tkachenko. In 2010, the Children's Hospice became the first governmental institution providing pediatric palliative care in Russia. The first inpatient facility was opened in the “Kurakina Dacha” building – the former “Nikolayevsky” orphanage. The second facility for children of the Leningrad Oblast (and other regions of Russia) was opened in the village of Ol'gino situated in the Resort District of St. Petersburg.\n\nIn its early years, the Children’s Hospice was a home-based service consisting of doctors, social workers, nurses and psychologists. The aim was to provide hospice outpatient care to terminally ill children and their families.\n\nThe St. Petersburg Children's Hospice has become a model for the establishment of such institutions in the city of Moscow, the Moscow Oblast and other regions of Russia\n\nThe appearance of the inpatient facility in St. Petersburg was preceded by 7 years of work by hospice employees. On June 1, 2010, on its seventh birthday, the St. Petersburg Children's Hospice opened the doors of the inpatient facility \"Magic House\" for seriously ill children.\nWhen visiting the Children's Hospice on November 20, 2010, Kirill, the Patriarch of the Russian Orthodox Church, said:\n\n“Today, on such a personal day for myself, I am happy to visit the hospice, and believe me that perhaps the most beautiful gifts that I have been presented today are faces of doctors, attendants, and parents testifying to their courage and faith, and of course, the children who are going through this suffering for reasons unknown to our human mind”.\n\nOn November 21, 2011, Federal Law No 323 \"On the Fundamentals of Health Protection in the Russian Federation\" was passed, in which Article 36 \"Palliative medical care\" was added at the initiative of the St. Petersburg Children’s Hospice. The Children's Hospice staff participated in the development of methodical guidelines for professionals and parents.\n\nIn 2012-2013, under the auspices of the hospice, several palliative medical care services were opened in Leningrad and Moscow regions. In 2015, it is planned to complete the construction of a second inpatient facility of the St. Petersburg State Autonomous Healthcare Institution (SAHI) “Children’s Hospice”.\n\nIn 2011, the Children's Hospice founder, Archpriest Alexander Tkachenko, received St Andrew the First-Called Foundation Award \"For Faith and Loyalty\". In March 2013, Archpriest Alexander Tkachenko and the Children’s Hospice executive director Pavel Krupnik were awarded the Certificates of Honor by the Federation Council, the upper house of the Federal Assembly of Russia. On July 31, 2014, in the Kremlin, Russian President Vladimir Putin awarded Archpriest Alexander Tkachenko the governmental mark of distinction \"For Beneficence\".\n\nThe St. Petersburg Children's Hospice is structured as follows:\n\nThe St. Petersburg Children’s Hospice has run the home-based service since its very inception. The physicians, nurses, psychologists and social workers of the service regularly visit families in the care of the hospice and provide them with counseling, medical and psychological assistance. The service oversees about 300 children living in St. Petersburg and the Leningrad region.\n\nThe station is designed for round-the-clock care for 20 patients and, in addition, for 10 patients from the daycare station. The length of stay depends on individual needs of a child and his family. The purpose of the care station is to stabilize and possibly improve the child’s state of health, as well as to provide respite care for the family. The care station provides the children with all necessary medical and psychological rehabilitation opportunities. The interior of the entire inpatient facility is different from public children’s hospitals. There are no wards with white walls. Instead, there are rooms decorated with drawings for children living together with their mothers. All the furnishings inside the hospice facility are designed to improve the child's emotional state.\n\nThe Intensive Care Unit consists of wards with modern equipment and is designed for round-the-clock observation of children who have reached a terminal state.\n\nAt the daycare station the children enjoy the same range of services as in the round-the-clock station, but they do not stay overnight.\n\nThe St. Petersburg Children's Hospice Director is Russian Orthodox Church archpriest Alexander Tkachenko. He is also considered the main inspirer of establishing children's palliative care as a separate branch of the Russian healthcare system.\n\nAccording to data from 2013, the St. Petersburg city budget allocates to the St. Petersburg State Autonomous Healthcare Institution “Children’s Hospice” about 50 million rubles a year. The non-profit Medical Institution “Children’s Hospice” finances itself solely by donations.\n\n"}
{"id": "8416361", "url": "https://en.wikipedia.org/wiki?curid=8416361", "title": "Sanitary engineering", "text": "Sanitary engineering\n\nSanitary engineering, also known as public health engineering, is the application of engineering methods to improve sanitation of human communities, primarily by providing the removal and disposal of human waste, and in addition to the supply of safe potable water. Traditionally a branch of civil engineering, in the mid-19th century, the discipline concentrated on the reduction of disease, then thought to be caused by miasma. This was accomplished mainly by the collection and segregation of sewerage flow in London specifically, and Great Britain generally. These and later regulatory improvements were reported in the United States as early as 1865. \n\nIt is not concerned with environmental factors that do not have an immediate and clearly understood effect on public health. Areas outside the purview of sanitary engineering include traffic management, concerns about noise pollution or light pollution, aesthetic concerns such as landscaping, and environmental conservation as it pertains to plants and animals.\n\nSkills within this field are usually employed for the primary goal of disease prevention within human beings by assuring a supply of healthy drinking water, treatment of waste water, removing garbage from inhabited areas, and so on.\n\nCompared to (for example) electrical engineering or mechanical engineering which are concerned primarily with closed systems, sanitary engineering is a very interdisciplinary field which may involve such elements as hydraulics, constructive modelling, information technology, project design, microbiology, pathology and the many divisions within environmental science and environmental technology. In some cases, considerations that fall within the field of social sciences and urban planning must be factored in as well.\n\nAlthough sanitary engineering may be most associated with the design of sewers, sewage treatment and waste water treatment facilities, recycling centers, public landfills and other things which are constructed, the term applies equally to (for example) a plan of action to reverse the effects of water pollution or soil contamination in a specific area.\n\nIrrigation systems were invented five to seven thousand years ago as a means of supplying water to agriculture-based societies. Aqueducts and irrigation systems were among the first forms of wastewater engineering. As population centers became more dense, they were used to remove sewage from settlements. The Romans were among the first to demonstrate the effectiveness of the aqueduct. The Dark Ages marked a period where progress in water management came to a halt.\n\nAs populations grew, the management of human waste became a growing concern and a public health threat. By the 1850s in London more than 400,000 tons of sewage were flushed into the Thames River each day - around 150 million tons a year. Diseases such as smallpox, diphtheria, measles, scarlet fever, typhus, cholera and typhoid were spread via the contaminated water supply. During the 19th century major cities started building sewage systems to remove human waste out of cities and into rivers.\n\nDuring the 1900s the activated sludge process was invented. The activated sludge process is a form of water purification that uses bacteria to consume human feces. Chlorine is used to kill off the bacteria.\n\nOver the centuries much has changed in the field of wastewater engineering. Advancements in microbiology, chemistry, and engineering have drastically changed the field. Today wastewater engineers work on the collection of clean water for drinking, chemically treating it, and using UV light to kill off micro-organisms. They also treat Water Pollution in wastewater (blackwater and greywater) so that this water may be made safe for use without endangering the population and environment around it.\n\nGenerally, sanitary engineers work for municipalities and are highly trained professionals with a diverse range of engineering skills. Some are involved with a specific area of concern such as waste collection or the maintenance of waste water facilities and storm water drainage systems within a district.\n\nOthers cover a broader scope of activities that might include the two just listed as well as such maintenance of the public water supply, collection of residential yard waste program, disposal of hazardous waste, recycling strategies and even community programs where individuals or businesses \"adopt\" an area and either maintain it themselves or donate funds for doing so.\n\nWastewater engineering is not usually its own degree course but a specialization from degrees such as civil engineering, environmental engineering, Bio-chemical engineering or chemical engineering. Wastewater engineering deals with the transportation and cleaning of blackwater, greywater, and irrigation water. Wastewater treatment and water reclamation are areas of concern in this field.\n\nWastewater engineers map out topographical and geographical features of Earth to determine the best means of collection, design pipe and pumped collection systems, and design treatment processes for collected wastewater.\n\nWastewater engineers use a variety of skills and must have knowledge of mechanical and environmental engineering. They are required to perform tasks and demonstrate knowledge in design, mathematics, English, construction, physics, chemistry, biology, management, and personnel. Wastewater engineers must have skills in complex problem solving, critical thinking, mathematics, active listening, judgement, reading comprehension, speaking, writing, science, and system analysis. Typical work activities include problem solving, communication with management and staff, gathering information, analyzing data, evaluating standards and complying with them, and communicating with others in the field.\n\nWastewater engineers perform these activities by combining their knowledge and skills to perform tasks. These tasks are to understand computer-aided design programs, and to conduct studies for the construction of facilities, water supply systems and collection systems. They may design systems for wastewater collection machinery, as well as system components. They may perform water flow analysis, then select designs and equipment based on government and industry standards.\n\nWastewater engineers work for private companies, state and local governments, and special districts. As of 2012 there were 53,000 wastewater engineers employed in the United States.\n\nToday the wastewater managers confront new challenges and the need for new technology as water levels decrease due to increasingly frequent and extended droughts. Technologies such as sonar mapping are being used in wells to determine the volume of water that they can hold. For example, the U.S. Geological Survey (USGS) and the State of New York worked together to map underground aquifers since the 1980s. Today they have thorough maps of these aquifers to assist in water management.\n\nWastewater treatment contributes to global warming in many ways. One of the factors that contributes to global warming is wastewater treatment facilities and their emissions of greenhouse gases. Some of those gases are carbon dioxide, methane, and nitrous oxide. These gases occur because of the decomposition of organic material from the anaerobic bacteria. These bacteria clean the leftover waste. Even if the anaerobic bacteria decomposition produces these gases, the percentage of greenhouse gases that other equipment produce is still greater than the contribution of the anaerobic bacteria. Also, the power usage from those machinery is very high. That is why many facilities are undergoing renovation to use higher levels of anaerobic bacteria compared to other types of equipment.\n\nThe effects of global warming on waste water treatment facilities vary from location to location. In the Northeast there has been increased rainfall causing treatment plants to be too small relative to the amount coming in. In the West there is a lack of rainfall and water sources are being depleted. This means the facilities will need to be updated to meet modern and future environmental changes such as the increased use of recycled and reclaimed water Climate change also affects piping and increases the rate of corrosion, adding to facility cost.\n\nDesalination plants may be required in the future for those regions hardest hit by water scarcity. Desalination is a process of cleaning water by means of evaporation. Water is evaporated and it passes through membranes. The water is then cooled and condenses allowing it to flow either back into the main water line or out to sea. Desalination is not completely \"green\" since the plants require energy to run and due to the fact that the brine waste must managed to avoid ecological damage to the ocean.\n\nFormal education for wastewater engineers begins in high school with students taking classes such as chemistry, biology, physics, and higher mathematics including calculus. After high school most jobs require certification from a state agency. Those wanting to advance in the industry should pursue a civil engineering, mechanical engineering, environmental engineering, or a facilities engineering degree. Gaining experience through internships and working while in college is a common pathway toward advancement.\n\nEducation about waste treatment requires course work in systems design, machinery design principles, water chemistry, and similar coursework. Other classes may include Chemistry of Plant Processes, and various plant operations courses.\n\nInitial employment in wastewater engineering can be obtained by those with and without advanced formal education. The California State Water Resources Control Board (SWRCB), for example, shows how individuals can advance through a progression of certifications as Waste Water Treatment Operators. The Board uses a five level classification system to classify\nwater treatment facilities into categories I-V according to the population served\nand the complexity of the treatment system.\n\nThe Operator Certification requirements for water treatment operators and waste water treatment operators are described in detail by State law. To meet certification requirements, operators must submit an application to SWRCB, have the necessary work experience, meet the educational requirements, and pass an examination based on the knowledge, skill,and abilities described in the regulations. Operators are required to renew their certificates every three years. To be elibible for renewal, certified operators must complete a specified number of continuing education hours after the previous issuance of a certificate.\n\nWastewater engineers may advance in their careers through additional education and experience. With additional knowledge and experience one can become the manager of an entire plant. The accreditation body certifying the education for the degree and license is the Accreditation Board for Engineering and Technology (ABET). Over time some companies may require the wastewater engineer to continue their education to keep up with any changes in technology.\n\nObtaining one’s master's degree is encouraged since many companies list it as a preference in selection.\n\nIn this field 76 percent of those employed have a bachelor's degree, 17 percent have a master's degree and three percent have a post-doctoral degree as of 2013. The average annual salary is approximately $83,360.\n"}
{"id": "1668486", "url": "https://en.wikipedia.org/wiki?curid=1668486", "title": "Scottish Youth Hostels Association", "text": "Scottish Youth Hostels Association\n\nHostelling Scotland (SYHA; Gaelic: \"Comann Osdailean Òigridh na h-Alba\"), founded in 1931, is part of Hostelling International and provides youth hostel accommodation in Scotland. As of 2013, around 60% of its guests come from outwith Scotland.\n\nAs of 2016, the hostel guide and website lists over 60 hostels, 28 of which are independently owned affiliate hostels such as those of the Gatliff Hebridean Hostel Trust and various local communities and authorities. Hostels vary from modern purpose-built premises to historic buildings and country cottages, sited in major towns and cities and in rural locations, including remote islands.\n\nAccommodation is generally dormitory-style but increasingly this is being subdivided into smaller units. For example, the most modern hostel, Edinburgh Central, has many single and twin-bedded rooms with ensuite facilities. All have a lounge/sitting room, shared bathrooms and self-catering kitchens. Many hostels provide meals at request.\n\nHostelling Scotland is a self-funding charitable organisation, and as a not-for-profit business invests all surplus back into the organisation, both to develop the network and to improve older hostels. Today it faces competition from the more numerous independent hostels, and from rural hotels which provide bunkhouse accommodation.\n\nIt has been claimed that it has left its roots as a working class movement to \"provide accommodation to people of limited means\" behind, and become too expensive. The SYHA's defenders, including Allan Wilson MSP, point out that hostellers today require higher levels of comfort than when the hostelling movement began.\n\nIn 1938, there were more than 60 hostels and membership was approaching 20,000. At their highest point, the SHYA had 99 hostels, by 1995 this had reduced to 85.\n\nAs of 2018, The Scottish Youth Hostels Association rebranded as Hostelling Scotland, dropping the SYHA from their name. The new logo reflects the internationally recognised symbol for youth hostelling and the house, with its characteristic gable-end chimney, gives the pictorial a subtle hint of Scotland whilst the colour mirrors that of the Saltire.\n\n\n"}
{"id": "125704", "url": "https://en.wikipedia.org/wiki?curid=125704", "title": "Sisters of Mercy", "text": "Sisters of Mercy\n\nThe Religious Sisters of Mercy (R.S.M.) are members of a religious institute of Catholic women founded in 1831 in Dublin, Ireland by Catherine McAuley (1778–1841). In 2018 the institute has about 6200 sisters worldwide, organized into a number of independent congregations. They also started many education and health care facilities around the globe.\n\nThe Congregation of the Sisters of Mercy began when Catherine McAuley used an inheritance to build a large house on Baggot Street, Dublin, as a school for poor girls and a shelter for homeless servant girls and women. She was assisted in the works of the house by local women. As the number of lay co-workers at Baggot Street increased, so did severe lay and clerical criticism of the House: Why did these women look like a religious order, yet not abide by the normal regulations of religious orders? Who was this \"upstart\" Miss McAuley? Why was the \"unlearned sex\" doing the work of the clergy?\n\nBy 1830 Catherine and her co-workers realized that the stability of the works of mercy they performed, including visiting the sick poor in their homes and in hospitals, and their continued appeal to co-workers, called for revision of their lay community. So, on 8 September, Catherine McAuley, Anna Maria Doyle, and Elizabeth Harley entered the Presentation Convent in Dublin to begin formal preparation for founding the Sisters of Mercy.\n\nOn 12 December 1831, Catherine McAuley, Mary Ann Doyle, and Mary Elizabeth Harley professed their religious vows as the first Sisters of Mercy, thereby founding the congregation. The rule and constitutions of the congregation were not completed until 1834, nor approved until 1835, yet they contained in substance only that which had been observed from the year 1827.\n\nThe rapid expansion of the Sisters of Mercy in the six years 1835-1841 flowed from Catherine McAuley's ever generous response to human need.\nShe founded nine additional autonomous Convents of Mercy in Tullamore (1836), Charleville (1836), Carlow (1837), Cork (1837), Limerick (1838), Bermondsey, London (1839), Galway (1840), Birr (1840), and Birmingham (1841), and branch houses of the Dublin community in Kingstown (1835) and Booterstown (1838).\n\nCatherine McAuley died on 11 November 1841.\n\nIn May 1842, at the request of Bishop Fleming, a small colony of Sisters of Mercy crossed the Atlantic to found the congregation at St. John's, Newfoundland. In December 1843 Sr Frances Warde led the first group of Sisters to The United States, beginning in Pittsburgh. The sisters arrived in Perth, Australia in 1846, and in 1850, a band from Carlow arrived in New Zealand. Sisters from Limerick opened a house in Glasgow in 1849, and in 1868 the English community established a house in Guernsey.\n\nIn 1992 the leaders of the various congregations created the Mercy International Association to foster collaboration and cooperation. The purpose of the association is to provide support and foster collaboration, organisation and inspiration for the ministries of the Sisters of Mercy and their associates.\n\nThe sisters were the first nurses to respond to the British Government request for nurses in the Crimea in 1853. They ran several hospitals during the war and provided nurses who were not under the control of Florence Nightingale. However their involvement was overshadowed by hers for political reasons.\n\nSisters of Mercy is an international community of Roman Catholic women religious vowed to serve people who suffer from poverty, sickness and lack of education with a special concern for women and children. Members take vows of poverty, chastity, and obedience, the evangelical counsels commonly vowed in religious life, and, in addition, vows of service. They continue to participate in the life of the surrounding community. In keeping with their mission of serving the poor and needy, many sisters engage in teaching, medical care, and community programs. The organization is active in lobbying and politics.\n\nThe Sisters of Mercy are constituted as religious and charitable organizations in a number of countries. Mercy International Association is a registered charity in the Republic of Ireland.\n\nOn 20 May 2009, the institute was condemned in an Irish government report known as the Ryan Report, the work of\nthe Commission to Inquire into Child Abuse. The Sisters of Mercy were named as the chief among the institutes under whose care girls \"endured frequent assaults and humiliation designed to make them feel worthless ... personal and family denigration was widespread\".\n\nIn 2011, a monument was erected in Ennis at the site of the former industrial school 'in appreciation' of the Sisters of Mercy.\n\n\n\n\n\nIn 1849 Bishop Pompallier visited St Leo's Convent in Carlow, Ireland, seeking sisters to emigrate; eight left from St Leo's, led by Mother Mary Cecilia. They travelled to New Zealand, learning Māori along the way, establishing the Sisters of Mercy in Auckland as the first female religious community in New Zealand in 1850.\nMylnhurst, Sheffield \n\n\n\n\n\n\n"}
{"id": "17488630", "url": "https://en.wikipedia.org/wiki?curid=17488630", "title": "Specified risk material", "text": "Specified risk material\n\nSpecified risk material (SRM) is the general term designated for tissues of ruminant animals that cannot be inspected and passed for human food because scientists have determined that BSE-causing prions concentrate there. The term was referred to in the United Kingdom's \"Specified Risk Material Order 1997\" (S.I. 1997/2964), in the United States Department of Agriculture's, and in the Canadian Food Inspection Agency's regulatory response to the first confirmed U.S. BSE case in December 2003. \n\nThese can include brains, eyes, spinal cord, and other organs; the exact definition varies by jurisdiction. Under the new regulations (69 FR 1862, January 12, 2004), SRMs are: the brain, skull, eyes, trigeminal ganglia, spinal cord, vertebral column (with some exclusions), dorsal root ganglia (DRG) of cattle 30 months of age and older, and the tonsils and distal ileum of the small intestine of all cattle.\n\nThe BSE infective agent has been found to concentrate in specific tissues of BSE-infected cattle and these tissues are all part of the central nervous system, as BSE has not been shown to infect muscle.\n\nIn both the United States (U.S.) and Canada, considered as controlled risk countries, SRMs are defined as: skull, brain, trigeminal ganglia (nerves attached to brain and close to the skull exterior), eyes, spinal cord, distal ileum (a part of the small intestine), and the dorsal root ganglia (nerves attached to the spinal cord and close to the vertebral column) of cattle aged 30 months or older. On January 12, 2004, the Food Safety and Inspection Service of the USDA published new rules banning such materials from the human food supply.\n\nIn countries classified as undetermined risk, the OIE code recommends SRM removal as follows: tonsils and intestines in cattle at all ages; brains, eyes, spinal cord, skull and vertebral column from animals over twelve months of age.\n\nIn the European Union (E.U.), SRMs are excluded by law from the human and animal food chain.\n\nThe World Organization for Animal Health (OIE) has established recommendations and guidelines for SRM removal based on the level of risk. In the U.S., tonsils are removed from cattle of all ages. SRMs must be removed at slaughter and disposed as inedible material. The dorsal root ganglia must be removed during the deboning process and in animals older than 30 months, the vertebral column (excluding the vertebrae of the tail, the transverse processes of the lumbar and thoracic vertebrae, and the wings of the sacrum) is removed to be certain the dorsal root ganglia is extracted in its entirety.\n\n"}
{"id": "1053949", "url": "https://en.wikipedia.org/wiki?curid=1053949", "title": "Spinal cord injury", "text": "Spinal cord injury\n\nA spinal cord injury (SCI) is damage to the spinal cord that causes temporary or permanent changes in its function. Symptoms may include loss of muscle function, sensation, or autonomic function in the parts of the body served by the spinal cord below the level of the injury. Injury can occur at any level of the spinal cord and can be \"complete injury\", with a total loss of sensation and muscle function, or \"incomplete\", meaning some nervous signals are able to travel past the injured area of the cord. Depending on the location and severity of damage, the symptoms vary, from numbness to paralysis to incontinence. Long term outcomes also ranges widely, from full recovery to permanent tetraplegia (also called quadriplegia) or paraplegia. Complications can include muscle atrophy, pressure sores, infections, and breathing problems.\nIn the majority of cases the damage results from physical trauma such as car accidents, gunshots, falls, or sports injuries, but it can also result from nontraumatic causes such as infection, insufficient blood flow, and tumors. Just over half of injuries affect the cervical spine, while 15% occur in each of the thoracic spine, border between the thoracic and lumbar spine, and lumbar spine. Diagnosis is typically based on symptoms and medical imaging.\nEfforts to prevent SCI include individual measures such as using safety equipment, societal measures such as safety regulations in sports and traffic, and improvements to equipment. Treatment starts with restricting further motion of the spine and maintaining adequate blood pressure. Corticosteroids have not been found to be useful. Other interventions vary depending on the location and extent of the injury, from bed rest to surgery. In many cases, spinal cord injuries require long-term physical and occupational therapy, especially if it interferes with activities of daily living.\nIn the United States, about 12,000 people a year survive with a spinal cord injury. The most commonly affected group are young adult males. SCI has seen great improvements in its care since the middle of the 20th century. Research into potential treatments includes stem cell implantation, engineered materials for tissue support, epidural spinal stimulation, and wearable robotic exoskeletons.\n\nSpinal cord injury can be traumatic or nontraumatic, and can be classified into three types based on cause: mechanical forces, toxic, and ischemic (from lack of blood flow). The damage can also be divided into primary and secondary injury: the cell death that occurs immediately in the original injury, and biochemical cascades that are initiated by the original insult and cause further tissue damage. These secondary injury pathways include the ischemic cascade, inflammation, swelling, cell suicide, and neurotransmitter imbalances. They can take place for minutes or weeks following the injury.\n\nAt each level of the spinal column, spinal nerves branch off from either side of the spinal cord and exit between a pair of vertebrae, to innervate a specific part of the body. The area of skin innervated by a specific spinal nerve is called a dermatome, and the group of muscles innervated by a single spinal nerve is called a myotome. The part of the spinal cord that was damaged corresponds to the spinal nerves at that level and below. Injuries can be cervical 1–8 (C1–C8), thoracic 1–12 (T1–T12), lumbar 1–5 (L1–L5), or sacral (S1–S5). A person's level of injury is defined as the lowest level of full sensation and function. Paraplegia occurs when the legs are affected by the spinal cord damage (in thoracic, lumbar, or sacral injuries), and tetraplegia occurs when all four limbs are affected (cervical damage).\n\nSCI is also classified by the degree of impairment. The \"International Standards for Neurological Classification of Spinal Cord Injury\" (ISNCSCI), published by the American Spinal Injury Association (ASIA), is widely used to document sensory and motor impairments following SCI. It is based on neurological responses, touch and pinprick sensations tested in each dermatome, and strength of the muscles that control key motions on both sides of the body. Muscle strength is scored on a scale of 0–5 according to the table on the right, and sensation is graded on a scale of 0–2: 0 is no sensation, 1 is altered or decreased sensation, and 2 is full sensation. Each side of the body is graded independently.\n\nIn a \"complete\" spinal injury, all functions below the injured area are lost, whether or not the spinal cord is severed. An \"incomplete\" spinal cord injury involves preservation of motor or sensory function below the level of injury in the spinal cord. To be classed as incomplete, there must be some preservation of sensation or motion in the areas innervated by S4 to S5, e.g. voluntary external anal sphincter contraction. The nerves in this area are connected to the very lowest region of the spinal cord, and retaining sensation and function in these parts of the body indicates that the spinal cord is only partially damaged. Incomplete injury by definition includes a phenomenon known as sacral sparing: some degree of sensation is preserved in the sacral dermatomes, even though sensation may be more impaired in other, higher dermatomes below the level of the lesion. Sacral sparing has been attributed to the fact that the sacral spinal pathways are not as likely as the other spinal pathways to become compressed after injury due to the lamination of fibers within the spinal cord.\n\nSpinal cord injury without radiographic abnormality exists when SCI is present but there is no evidence of spinal column injury on radiographs. Spinal column injury is trauma that causes fracture of the bone or instability of the ligaments in the spine; this can coexist with or cause injury to the spinal cord, but each injury can occur without the other. Abnormalities might show up on magnetic resonance imaging (MRI), but the term was coined before MRI was in common use.\n\nCentral cord syndrome, almost always resulting from damage to the cervical spinal cord, is characterized by weakness in the arms with relative sparing of the legs, and spared sensation in regions served by the sacral segments. There is loss of sensation of pain, temperature, light touch, and pressure below the level of injury. The spinal tracts that serve the arms are more affected due to their central location in the spinal cord, while the corticospinal fibers destined for the legs are spared due to their more external location. The most common of the incomplete SCI syndromes, central cord syndrome usually results from neck hyperextension in older people with spinal stenosis. In younger people, it most commonly results from neck flexion. The most common causes are falls and vehicle accidents; however other possible causes include spinal stenosis and impingement on the spinal cord by a tumor or vertebral disk.\n\nAnterior cord syndrome, due to damage to the front portion of the spinal cord or reduction in the blood supply from the anterior spinal artery, can be caused by fractures or dislocations of vertebrae or herniated disks. Below the level of injury, motor function, pain sensation, and temperature sensation are lost, while sense of touch and proprioception (sense of position in space) remain intact. These differences are due to the relative locations of the spinal tracts responsible for each type of function.\n\nBrown-Séquard syndrome occurs when the spinal cord is injured on one side much more than the other. It is rare for the spinal cord to be truly hemisected (severed on one side), but partial lesions due to penetrating wounds (such as gunshot or knife wounds) or fractured vertebrae or tumors are common. On the ipsilateral side of the injury (same side), the body loses motor function, proprioception, and senses of vibration and touch. On the contralateral (opposite side) of the injury, there is a loss of pain and temperature sensations.\n\nPosterior cord syndrome, in which just the dorsal columns of the spinal cord are affected, is usually seen in cases of chronic myelopathy but can also occur with infarction of the posterior spinal artery. This rare syndrome causes the loss of proprioception and sense of vibration below the level of injury while motor function and sensation of pain, temperature, and touch remain intact. Usually posterior cord injuries result from insults like disease or vitamin deficiency rather than trauma. Tabes dorsalis, due to injury to the posterior part of the spinal cord caused by syphilis, results in loss of touch and proprioceptive sensation.\n\nConus medullaris syndrome is an injury to the end of the spinal cord, located at about the T12–L2 vertebrae in adults. This region contains the S4–S5 spinal segments, responsible for bowel, bladder, and some sexual functions, so these can be disrupted in this type of injury. In addition, sensation and the Achilles reflex can be disrupted. Causes include tumors, physical trauma, and ischemia.\n\nCauda equina syndrome (CES) results from a lesion below the level at which the spinal cord splits into the cauda equina, at levels L2–S5 below the conus medullaris. Thus it is not a true spinal cord syndrome since it is nerve roots that are damaged and not the cord itself; however it is common for several of these nerves to be damaged at the same time due to their proximity. CES can occur by itself or alongside conus medullaris syndrome. It can cause low back pain, weakness or paralysis in the lower limbs, loss of sensation, bowel and bladder dysfunction, and loss of reflexes. Unlike in conus medullaris syndrome, symptoms often occur on only one side of the body. The cause is often compression, e.g. by a ruptured intervertebral disk or tumor. Since the nerves damaged in CES are actually peripheral nerves because they have already branched off from the spinal cord, the injury has better prognosis for recovery of function: the peripheral nervous system has a greater capacity for healing than the central nervous system.\n\nSigns (observed by a clinician) and symptoms (experienced by a patient) vary depending on where the spine is injured and the extent of the injury. \n\nA section of skin innervated through a specific part of the spine is called a dermatome, and injury to that part of the spine can cause pain, numbness, or a loss of sensation in the related areas. Paraesthesia, a tingling or burning sensation in affected areas of the skin, is another symptom. A person with a lowered level of consciousness may show a response to a painful stimulus above a certain point but not below it. \n\nA group of muscles innervated through a specific part of the spine is called a myotome, and injury to that part of the spinal cord can cause problems with movements that involve those muscles. The muscles may contract uncontrollably (spasticity), become weak, or be completely paralysed. Spinal shock, loss of neural activity including reflexes below the level of injury, occurs shortly after the injury and usually goes away within a day.\n\nThe specific parts of the body affected by loss of function are determined by the level of injury.\n\nThe effects of injuries at or above the lumbar or sacral regions of the spinal cord (lower back and pelvis) include decreased control of the legs and hips, genitourinary system, and anus. People injured below level L2 may still have use of their hip flexor and knee extensor muscles. Bowel and bladder function are regulated by the sacral region. It is common to experience sexual dysfunction after injury, as well as dysfunction of the bowel and bladder, including fecal and urinary incontinence. It is also possible for the bladder to fail to empty, leading to a potentially harmful buildup of urine. One sign of spinal cord injury that emergency providers may find is priapism, an erection of the penis.\n\nIn addition to the problems found in lower-level injuries, thoracic (chest height) spinal lesions can affect the muscles in the trunk. Injuries at the level of T1 to T8 result in inability to control the abdominal muscles. Trunk stability may be affected; even more so in higher level injuries. The lower the level of injury, the less extensive its effects. Injuries from T9 to T12 result in partial loss of trunk and abdominal muscle control. Thoracic spinal injuries result in paraplegia, but function of the hands, arms, and neck are not affected.\nOne condition that occurs typically in lesions above the T6 level is autonomic dysreflexia (AD), in which the blood pressure increases to dangerous levels, high enough to cause potentially deadly stroke. It results from an overreaction of the system to a stimulus such as pain below the level of injury, because inhibitory signals from the brain cannot pass the lesion to dampen the excitatory sympathetic nervous system response. Signs and symptoms of AD include anxiety, headache, nausea, ringing in the ears, blurred vision, flushed skin, and nasal congestion. It can occur shortly after the injury or not until years later.\n\nOther autonomic functions may also be disrupted. For example, problems with body temperature regulation mostly occur in injuries at T8 and above.\n\nAnother serious complication that can result from lesions above T6 is neurogenic shock, which results from an interruption in output from the sympathetic nervous system responsible for maintaining muscle tone in the blood vessels. Without the sympathetic input, the vessels relax and dilate. Neurogenic shock presents with dangerously low blood pressure, low heart rate, and blood pooling in the limbs—which results in insufficient blood flow to the spinal cord and potentially further damage to it.\n\nSpinal cord injuries at the cervical (neck) level result in full or partial tetraplegia (also called quadriplegia). Depending on the specific location and severity of trauma, limited function may be retained.\nAdditional signs and symptoms of cervical injuries include low heart rate, low blood pressure, problems regulating body temperature, and breathing dysfunction. If the injury is high enough in the neck to impair the muscles involved in breathing, the person may not be able to breathe without the help of an endotracheal tube and mechanical ventilator.\n\nComplications of spinal cord injuries include pulmonary edema, respiratory failure, neurogenic shock, and paralysis below the injury site.\nIn the long term, the loss of muscle function can have additional effects from disuse, including atrophy of the muscle. Immobility can lead to pressure sores, particularly in bony areas, requiring precautions such as extra cushioning and turning in bed every two hours (in the acute setting) to relieve pressure. In the long term, people in wheelchairs must shift periodically to relieve pressure. Another complication is pain, including nociceptive pain (indication of potential or actual tissue damage) and neuropathic pain, when nerves affected by damage convey erroneous pain signals in the absence of noxious stimuli. Spasticity, the uncontrollable tensing of muscles below the level of injury, occurs in 65–78% of chronic SCI. It results from lack of input from the brain that quells muscle responses to stretch reflexes. It can be treated with drugs and physical therapy. Spasticity increases the risk of contractures (shortening of muscles, tendons, or ligaments that result from lack of use of a limb); this problem can be prevented by moving the limb through its full range of motion multiple times a day. Another problem lack of mobility can cause is loss of bone density and changes in bone structure. Loss of bone density (bone demineralization), thought to be due to lack of input from weakened or paralysed muscles, can increase the risk of fractures. Conversely, a poorly understood phenomenon is the overgrowth of bone tissue in soft tissue areas, called heterotopic ossification. It occurs below the level of injury, possibly as a result of inflammation, and happens to a clinically significant extent in 27% of people.\n\nPeople with SCI are at especially high risk for respiratory and cardiovascular problems, so hospital staff must be watchful to avoid them. Respiratory problems (especially pneumonia) are the leading cause of death in people with SCI, followed by infections, usually of pressure sores, urinary tract infections and respiratory infections. Pneumonia can be accompanied by shortness of breath, fever, and anxiety.\nAnother potentially deadly threat to respiration is deep venous thrombosis (DVT), in which blood forms a clot in immobile limbs; the clot can break off and form a pulmonary embolism, lodging in the lung and cutting off blood supply to it. DVT is an especially high risk in SCI, particularly within 10 days of injury, occurring in over 13% in the acute care setting. Preventative measures include anticoagulants, pressure hose, and moving the patient's limbs. The usual signs and symptoms of DVT and pulmonary embolism may be masked in SCI cases due to effects such as alterations in pain perception and nervous system functioning.\nUrinary tract infection (UTI) is another risk that may not display the usual symptoms (pain, urgency and frequency); it may instead be associated with worsened spasticity. The risk of UTI, likely the most common complication in the long term, is heightened by use of indwelling urinary catheters. Catheterization may be necessary because SCI interferes with the bladder's ability to empty when it gets too full, which could trigger autonomic dysreflexia or damage the bladder permanently. The use of intermittent catheterization to empty the bladder at regular intervals throughout the day has decreased the mortality due to kidney failure from UTI in the first world, but it is still a serious problem in developing countries.\nAn estimated 24–45% of people with SCI suffer disorders of depression, and the suicide rate is as much as six times that of the rest of the population. The risk of suicide is worst in the first five years after injury. In young people with SCI, suicide is the leading cause of death. Depression is associated with an increased risk of other complications such as UTI and pressure ulcers that occur more when self-care is neglected.\n\nSpinal cord injuries are most often caused by physical trauma. Forces involved can be hyperflexion (forward movement of the head); hyperextension (backward movement); lateral stress (sideways movement); rotation (twisting of the head); compression (force along the axis of the spine downward from the head or upward from the pelvis); or distraction (pulling apart of the vertebrae). Traumatic SCI can result in contusion, compression, or stretch injury. It is a major risk of many types of vertebral fracture. Pre-existing asymptomatic congenital anomalies can cause major neurological deficits, such as hemiparesis, to result from otherwise minor trauma.\n\nIn the US, Motor vehicle accidents are the most common cause of SCIs; second are falls, then violence such as gunshot wounds, then sports injuries. In some countries falls are more common, even surpassing vehicle crashes as the leading cause of SCI. The rates of violence-related SCI depend heavily on place and time. Of all sports-related SCIs, shallow water dives are the most common cause; winter sports and water sports have been increasing as causes while association football and trampoline injuries have been declining. Hanging can cause injury to the cervical spine, as may occur in attempted suicide. Military conflicts are another cause, and when they occur they are associated with increased rates of SCI. Another potential cause of SCI is iatrogenic injury, caused by an improperly done medical procedure such as an injection into the spinal column.\n\nSCI can also be of a nontraumatic origin. Nontraumatic lesions cause anywhere from 30 to 80% of all SCI; the percentage varies by locale, influenced by efforts to prevent trauma. Developed countries have higher percentages of SCI due to degenerative conditions and tumors than developing countries. In developed countries, the most common cause of nontraumatic SCI is degenerative diseases, followed by tumors; in many developing countries the leading cause is infection such as HIV and tuberculosis. SCI may occur in intervertebral disc disease, and spinal cord vascular disease. Spontaneous bleeding can occur within or outside of the protective membranes that line the cord, and intervertebral disks can herniate. Damage can result from dysfunction of the blood vessels, as in arteriovenous malformation, or when a blood clot becomes lodged in a blood vessel and cuts off blood supply to the cord. When systemic blood pressure drops, blood flow to the spinal cord may be reduced, potentially causing a loss of sensation and voluntary movement in the areas supplied by the affected level of the spinal cord. Congenital conditions and tumors that compress the cord can also cause SCI, as can vertebral spondylosis and ischemia. Multiple sclerosis is a disease that can damage the spinal cord, as can infectious or inflammatory conditions such as tuberculosis, herpes zoster or herpes simplex, meningitis, myelitis, and syphilis.\n\nVehicle-related SCI is prevented with measures including societal and individual efforts to reduce driving under the influence of drugs or alcohol, distracted driving, and drowsy driving. Other efforts include increasing road safety (such as marking hazards and adding lighting) and vehicle safety, both to prevent accidents (such as routine maintenance and antilock brakes) and to mitigate the damage of crashes (such as head restraints, air bags, seat belts, and child safety seats). Falls can be prevented by making changes to the environment, such as nonslip materials and grab bars in bathtubs and showers, railings for stairs, child and safety gates for windows. Gun-related injuries can be prevented with conflict resolution training, gun safety education campaigns, and changes to the technology of guns (such as trigger locks) to improve their safety. Sports injuries can be prevented with changes to sports rules and equipment to increase safety, and education campaigns to reduce risky practices such as diving into water of unknown depth or head-first tackling in association football.\n\nA person's presentation in context of trauma or non-traumatic background determines suspicion for a spinal cord injury. The features are namely paralysis, sensory loss, or both at any level. Other symptoms may include incontinence.\n\nA radiographic evaluation using an X-ray, CT scan, or MRI can determine if there is damage to the spinal column and where it is located. X-rays are commonly available and can detect instability or misalignment of the spinal column, but do not give very detailed images and can miss injuries to the spinal cord or displacement of ligaments or disks that do not have accompanying spinal column damage. Thus when X-ray findings are normal but SCI is still suspected due to pain or SCI symptoms, CT or MRI scans are used. CT gives greater detail than X-rays, but exposes the patient to more radiation, and it still does not give images of the spinal cord or ligaments; MRI shows body structures in the greatest detail. Thus it is the standard for anyone who has neurological deficits found in SCI or is thought to have an unstable spinal column injury.\n\nNeurological evaluations to help determine the degree of impairment are performed initially and repeatedly in the early stages of treatment; this determines the rate of improvement or deterioration and informs treatment and prognosis. The ASIA Impairment Scale outlined above is used to determine the level and severity of injury.\n\nThe first stage in the management of a suspected spinal cord injury is geared toward basic life support and preventing further injury: maintaining airway, breathing, and circulation and restricting further motion of the spine.\nIn the emergency setting, most people who has been subjected to forces strong enough to cause SCI are treated as though they have instability in the spinal column and have spinal motion restricted to prevent damage to the spinal cord. Injuries or fractures in the head, neck, or pelvis as well as penetrating trauma near the spine and falls from heights are assumed to be associated with an unstable spinal column until it is ruled out in the hospital. High-speed vehicle crashes, sports injuries involving the head or neck, and diving injuries are other mechanisms that indicate a high SCI risk. Since head and spinal trauma frequently coexist, anyone who is unconscious or has a lowered level of consciousness as a result of a head injury is spinal motion restricted.\n\nA rigid cervical collar is applied to the neck, and the head is held with blocks on either side and the person is strapped to a backboard. Extrication devices are used to move people without excessively moving the spine if they are still inside a vehicle or other confined space. The use of a cervical collar has been shown to increase mortality in people with penetrating trauma and is thus not routinely recommended in this group.\n\nModern trauma care includes a step called clearing the cervical spine, ruling out spinal cord injury if the patient is fully conscious and not under the influence of drugs or alcohol, displays no neurological deficits, has no pain in the middle of the neck and no other painful injuries that could distract from neck pain. If these are all absent, no spinal motion restriction is necessary.\nIf an unstable spinal column injury is moved, damage may occur to the spinal cord. Between 3 and 25% of SCIs occur not at the time of the initial trauma but later during treatment or transport. While some of this is due to the nature of the injury itself, particularly in the case of multiple or massive trauma, some of it reflects the failure to adequately restrict motion of the spine. SCI can impair the body's ability to keep warm, so warming blankets may be needed.\n\nInitial care in the hospital, as in the prehospital setting, aims to ensure adequate airway, breathing, cardiovascular function, and spinal motion restriction. Imaging of the spine to determine the presence of a SCI may need to wait if emergency surgery is needed to stabilize other life-threatening injuries. Acute SCI merits treatment in an intensive care unit, especially injuries to the cervical spinal cord. People with SCI need repeated neurological assessments and treatment by neurosurgeons. People should be removed from the spine board as rapidly as possible to prevent complications from its use.\nIf the systolic blood pressure falls below 90 mmHg within days of the injury, blood supply to the spinal cord may be reduced, resulting in further damage. Thus it is important to maintain the blood pressure which may be done using intravenous fluids and vasopressors. Vasopressors used include phenylephrine, dopamine, or norepinephrine. Mean arterial blood pressure is measured and kept at 85 to 90 mmHg for seven days after injury. The treatment for shock from blood loss is different from that for neurogenic shock, and could harm people with the latter type, so it is necessary to determine why someone is in shock. However it is also possible for both causes to exist at the same time. Another important aspect of care is prevention of insufficient oxygen in the bloodstream, which could deprive the spinal cord of oxygen. People with cervical or high thoracic injuries may experience a dangerously slowed heart rate; treatment to speed it may include atropine.\nThe corticosteroid medication methylprednisolone has been studied for use in SCI with the hope of limiting swelling and secondary injury. As there does not appear to be long term benefits and the medication is associated with risks such as gastrointestinal bleeding and infection its use is not recommended as of 2018. Its use in traumatic brain injury is also not recommended.\nSurgery may be necessary, e.g. to relieve excess pressure on the cord, to stabilize the spine, or to put vertebrae back in their proper place. In cases involving instability or compression, failing to operate can lead to worsening of the condition. Surgery is also necessary when something is pressing on the cord, such as bone fragments, blood, material from ligaments or intervertebral discs, or a lodged object from a penetrating injury. Although the ideal timing of surgery is still debated, studies have found that earlier surgical intervention (within 24 hours of injury) is associated with better outcomes. Sometimes a patient has too many other injuries to be a surgical candidate this early. Surgery is controversial because it has potential complications (such as infection), so in cases where it is not clearly needed (e.g. the cord is being compressed), doctors must decide whether to perform surgery based on aspects of the patient's condition and their own beliefs about its risks and benefits.\n\nIn cases where a more conservative approach is chosen, bed rest, cervical collars, motion restriction devices, and optionally traction are used. Surgeons may opt to put traction on the spine to remove pressure from the spinal cord by putting dislocated vertebrae back into alignment, but herniation of intervertebral disks may prevent this technique from relieving pressure. \"Gardner-Wells tongs\" are one tool used to exert spinal traction to reduce a fracture or dislocation and to reduce motion to the affected areas.\n\nSCI patients often require extended treatment in specialized spinal unit or an intensive care unit. The rehabilitation process typically begins in the acute care setting. Usually the inpatient phase lasts 8–12 weeks and then the outpatient rehabilitation phase lasts 3–12 months after that, followed by yearly medical and functional evaluation. Physical therapists, occupational therapists, recreational therapists, nurses, social workers, psychologists and other health care professionals work as a team under the coordination of a physiatrist to decide on goals with the patient and develop a plan of discharge that is appropriate for the person’s condition.\nIn the acute phase physical therapists focus on the patient’s respiratory status, prevention of indirect complications (such as pressure ulcers), maintaining range of motion, and keeping available musculature active.\nFor people whose injuries are high enough to interfere with breathing, there is great emphasis on airway clearance during this stage of recovery. Weakness of respiratory muscles impairs the ability to cough effectively, allowing secretions to accumulate within the lungs. As SCI patients suffer from reduced total lung capacity and tidal volume, physical therapists teach them accessory breathing techniques (e.g. apical breathing, glossopharyngeal breathing) that typically are not taught to healthy individuals. Physical therapy treatment for airway clearance may include manual percussions and vibrations, postural drainage, respiratory muscle training, and assisted cough techniques. Patients are taught to increase their intra-abdominal pressure by leaning forward to induce cough and clear mild secretions. The quad cough technique is done lying on the back with the therapist applying pressure on the abdomen in the rhythm of the cough to maximize expiratory flow and mobilize secretions. Manual abdominal compression is another technique used to increase expiratory flow which later improves coughing. Other techniques used to manage respiratory dysfunction include respiratory muscle pacing, use of a constricting abdominal binder, ventilator-assisted speech, and mechanical ventilation.\n\nThe amount of functional recovery and independence achieved in terms of activities of daily living, recreational activities, and employment is affected by the level and severity of injury. The Functional Independence Measure (FIM) is an assessment tool that aims to evaluate the function of patients throughout the rehabilitation process following a spinal cord injury or other serious illness or injury. It can track a patient's progress and degree of independence during rehabilitation. People with SCI may need to use specialized devices and to make modifications to their environment in order to handle activities of daily living and to function independently. Weak joints can be stabilized with devices such as ankle-foot orthoses (AFOs) and knee-AFOs, but walking may still require a lot of effort. Increasing activity will increase chances of recovery.\n\nSpinal cord injuries generally result in at least some incurable impairment even with the best possible treatment. The best predictor of prognosis is the level and completeness of injury, as measured by the ASIA impairment scale. The neurological score at the initial evaluation done 72 hours after injury is the best predictor of how much function will return. Most people with ASIA scores of A (complete injuries) do not have functional motor recovery, but improvement can occur. Most patients with incomplete injuries recover at least some function. Chances of recovering the ability to walk improve with each AIS grade found at the initial examination; e.g. an ASIA D score confers a better chance of walking than a score of C. The symptoms of incomplete injuries can vary and it is difficult to make an accurate prediction of the outcome. A person with a mild, incomplete injury at the T5 vertebra will have a much better chance of using his or her legs than a person with a severe, complete injury at exactly the same place. Of the incomplete SCI syndromes, Brown-Séquard and central cord syndromes have the best prognosis for recovery and anterior cord syndrome has the worst. \n\nPeople with nontraumatic causes of SCI have been found to be less likely to suffer complete injuries and some complications such as pressure sores and deep vein thrombosis, and to have shorter hospital stays. Their scores on functional tests were better than those of people with traumatic SCI upon hospital admission, but when they were tested upon discharge, those with traumatic SCI had improved such that both groups' results were the same. In addition to the completeness and level of the injury, age and concurrent health problems affect the extent to which a person with SCI will be able to live independently and to walk. However, in general people with injuries to L3 or below will likely be able to walk functionally, T10 and below to walk around the house with bracing, and C7 and below to live independently.\nOne important predictor of motor recovery in an area is presence of sensation there, particularly pain perception. Most motor recovery occurs in the first year post-injury, but modest improvements can continue for years; sensory recovery is more limited. Recovery is typically quickest during the first six months. Spinal shock, in which reflexes are suppressed, occurs immediately after the injury and resolves largely within three months but continues resolving gradually for another 15.\nSexual dysfunction after spinal injury is common. Problems that can occur include erectile dysfunction, loss of ability to ejaculate, insufficient lubrication of the vagina, and reduced sensation and impaired ability to orgasm. Despite this, many people learn ways to adapt their sexual practices so they can lead satisfying sex lives.\nAlthough life expectancy has improved with better care options, it is still not as good as the uninjured population. The higher the level of injury, and the more complete the injury, the greater the reduction in life expectancy. Mortality is very elevated within a year of injury.\n\nWorldwide, the number of new cases since 1995 of SCI ranges from 10.4 to 83 people per million per year. This wide range of numbers is probably partly due to differences among regions in whether and how injuries are reported. In North America, about 39 people per every million incur SCI traumatically each year, and in Western Europe the incidence is 16 per million. In the United States, the incidence of spinal cord injury has been estimated to be about 40 cases per 1 million people per year or around 12,000 cases per year. In China, the incidence is approximately 60,000 per year.\n\nThe estimated number of people living with SCI in the world ranges from 236 to 4187 per million. Estimates vary widely due to differences in how data are collected and what techniques are used to extrapolate the figures. Little information is available from Asia, and even less from Africa and South America. In Western Europe the estimated prevalence is 300 per million people and in North America it is 853 per million. It is estimated at 440 per million in Iran, 526 per million in Iceland, and 681 per million in Australia. In the United States there are between 225,000 and 296,000 individuals living with spinal cord injuries, and different studies have estimated prevalences from 525 to 906 per million.\nSCI is present in about 2% of all cases of blunt force trauma. Anyone who has undergone force sufficient to cause a thoracic spinal injury is at high risk for other injuries also. In 44% of SCI cases, other serious injuries are sustained at the same time; 14% of SCI patients also suffer head trauma or facial trauma. Other commonly associated injuries include chest trauma, abdominal trauma, pelvic fractures, and long bone fractures.\nMales account for four out of five traumatic spinal cord injuries. Most of these injuries occur in men under 30 years of age. The average age at the time of injury has slowly increased from about 29 years in the 1970s to 41. Rates of injury are at their lowest in children, at their highest in the late teens to early twenties, then get progressively lower in older age groups; however rates may rise in the elderly. In Sweden between 50 and 70% of all cases of SCI occur in people under 30, and 25% occur in those over 50. While SCI rates are highest among people age 15–20, fewer than 3% of SCIs occur in people under 15. Neonatal SCI occurs in one in 60,000 births, e.g. from breech births or injuries by forceps. The difference in rates between the sexes diminishes in injuries at age 3 and younger; the same number of girls are injured as boys, or possibly more. Another cause of pediatric injury is child abuse such as shaken baby syndrome. For children, the most common cause of SCI (56%) is vehicle crashes. High numbers of adolescent injuries are attributable in a large part to traffic accidents and sports injuries. For people over 65, falls are the most common cause of traumatic SCI. The elderly and people with severe arthritis are at high risk for SCI because of defects in the spinal column. In nontraumatic SCI, the gender difference is smaller, the average age of occurrence is greater, and incomplete lesions are more common.\n\nSCI has been known to be devastating for millennia; the ancient Egyptian Edwin Smith Papyrus from 2500 BC, the first known description of the injury, says it is \"not to be treated\". Hindu texts dating back to 1800 BC also mention SCI and describe traction techniques to straighten the spine. The Greek physician Hippocrates, born in the fifth century BC, described SCI in his Hippocratic Corpus and invented traction devices to straighten dislocated vertebrae. But it was not until Aulus Cornelius Celsus, born 30 BC, noted that a cervical injury resulted in rapid death that the spinal cord itself was implicated in the condition. In the second century AD the Greek physician Galen experimented on monkeys and reported that a horizontal cut through the spinal cord caused them to lose all sensation and motion below the level of the cut. The seventh-century Greek physician Paul of Aegina described surgical techniques for treatment of broken vertebrae by removing bone fragments, as well as surgery to relieve pressure on the spine. Little medical progress was made during the Middle Ages in Europe; it was not until the Renaissance that the spine and nerves were accurately depicted in human anatomy drawings by Leonardo da Vinci and Andreas Vesalius.\n\nIn 1762 a surgeon named Andre Louis removed a bullet from the lumbar spine of a patient, who regained motion in the legs. In 1829 the surgeon Gilpin Smith performed a successful laminectomy that improved the patient's sensation. However, the idea that SCI was untreatable remained predominant until the early 20th century. In 1934, the mortality rate in the first two years after injury was over 80%, mostly due to infections of the urinary tract and pressure sores. It was not until the latter half of the century that breakthroughs in imaging, surgery, medical care, and rehabilitation medicine contributed to a substantial improvement in SCI care. The relative incidence of incomplete compared to complete injuries has improved since the mid-20th century, due mainly to the emphasis on faster and better initial care and stabilization of spinal cord injury patients. The creation of emergency medical services to professionally transport people to the hospital is given partial credit for an improvement in outcomes since the 1970s. Improvements in care have been accompanied by increased life expectancy of people with SCI; survival times have improved by about 2000% since 1940. IN 2015/2016 23% of people in nine spinal injury centres in England had their discharge delayed because of disputes about who should pay for the equipment they needed.\n\nScientists are investigating various avenues for treatment of spinal cord injury. Therapeutic research is focused on two main areas: neuroprotection and neuroregeneration. The former seeks to prevent the harm that occurs from secondary injury in the minutes to weeks following the insult, and the latter aims to reconnect the broken circuits in the spinal cord to allow function to return. Neuroprotective drugs target secondary injury effects including inflammation, damage by free radicals, excitotoxicity (neuronal damage by excessive glutamate signaling), and apoptosis (cell suicide). Several potentially neuroprotective agents that target pathways like these are under investigation in human clinical trials.\nStem cell transplantation is an important avenue for SCI research: the goal is to replace lost spinal cord cells, allow reconnection in broken neural circuits by regrowing axons, and to create an environment in the tissues that is favorable to growth. A key avenue of SCI research is research on stem cells, which can differentiate into other types of cells—including those lost after SCI. Types of cells being researched for use in SCI include embryonic stem cells, neural stem cells, mesenchymal stem cells, olfactory ensheathing cells, Schwann cells, activated macrophages, and induced pluripotent stem cells. Hundreds of stem cell studies have been done in humans, with promising but inconclusive results. An ongoing Phase 2 trial in 2016 presented data showing that after 90 days, 2 out of 4 subjects had already improved two motor levels and had thus already achieved its endpoint of 2/5 patients improving two levels within 6–12 months. Six-month data is expected in January 2017.\n\nAnother type of approach is tissue engineering, using biomaterials to help scaffold and rebuild damaged tissues. Biomaterials being investigated include natural substances such as collagen or agarose and synthetic ones like polymers and nitrocellulose. They fall into two categories: hydrogels and nanofibers. These materials can also be used as a vehicle for delivering gene therapy to tissues.\n\nOne avenue being explored to allow paralyzed people to walk and to aid in rehabilitation of those with some walking ability is the use of wearable powered robotic exoskeletons. The devices, which have motorized joints, are put on over the legs and supply a source of power to move and walk. Several such devices are already available for sale, but investigation is still underway as to how they can be made more useful.\n\nPreliminary studies of epidural spinal cord stimulators for motor complete injuries have demonstrated some improvement.\n\n"}
{"id": "11257311", "url": "https://en.wikipedia.org/wiki?curid=11257311", "title": "Stateville Penitentiary Malaria Study", "text": "Stateville Penitentiary Malaria Study\n\nThe Stateville Penitentiary malaria study was a controlled study of the effects of malaria on the prisoners of Stateville Penitentiary near Joliet, Illinois in the 1940s. The study was conducted by the Department of Medicine at the University of Chicago in conjunction with the United States Army and the State Department. The study is notable for its impacts on the Nuremberg Medical Trial and subsequent medical experimentation on prisoners.\n\nThe circumstances of World War II resulted in an urgent need for the development of new malaria treatments. First, U.S. soldiers were deployed to areas of the Pacific with extremely high rates of malaria infection. The U.S. Army estimated millions of man-hours lost due to malaria throughout the war; thus it was critical important to mitigate the effects of the disease in the interest of the military. Second, the conventional treatment for malaria, quinine, was largely unavailable throughout the war. Japanese control of the Philippines and Indonesia cut off the supply of quinine to the United States, adding to the need for alternate treatments. The new risks of malaria posed by World War II called for experimentation at an unprecedented scale, with a particular focus on human subjects.\n\nThe prison offered an environment conducive to controlled scientific experimentation on human subjects. The malaria experiments at Stateville Penitentiary are noteworthy for their utilization of this prison environment across all aspects of the experiments. A prison population allowed for researchers to limit extraneous variables across the subjects. Participants of the studies were exclusively white men, of similar age and health, which was the primary demographic of Stateville Penitentiary. The population of prisoners was also inherently homogenous in terms of behavior, due to the restrictions imposed by the maximum-security prison. Follow-up evaluations were possible for virtually all subjects of the study, since all had long-term sentences. Offers of parole reevaluation, as well as financial incentives (typically $25–$100 for an experimental trial), yielded an exceptionally high availability of subjects willing to participate.\n\nIn 1944, the U.S. Committee on Medical Research, formed a contract with the University of Chicago to test novel malaria treatments at the Stateville Penitentiary. Alf Alving, a nephrologist from the University of Chicago, directed the research, and oversaw the formation of a clinical research division of the prison hospital. Alving worked with Ray Dern and Ernest Beutler, two physicians also from the University of Chicago.\n\nThe Malaria Research Project was primarily conducted on a floor of the prison hospital in the Stateville Penitentiary. The study aimed to understand the effect of various antimalarial drugs on relapses of malaria, primarily from the 8-aminoquinoline group of compounds. The study marked the first human test of the antimalarial drug primaquine. For the experiment, doctors from the University of Chicago bred Anopheles quadrimaculatus mosquitoes. The mosquitoes were infected with a plasmodium vivax malaria strain that was isolated from a military patient.\n\nThe study considered the Chesson strain of \"P. vivax\", sourced from a military patient infected in the Pacific. \"P. vivax\", the predominant form of malaria in the Pacific, is associated with milder symptoms and unlike \"P. falciparum\", it typically is not deadly. This strain was known for its resistance to the standard treatment of quinine, with frequent occurrences of relapse. Subjects at the prison maintained the strain via blood inoculations. For the both control and test subjects, a constant number of mosquito bites from infected insects were administered; then, researchers dissected the mosquitoes to determine the intensity of the resultant infection.\n\nThe researchers tested patient responses to various potential treatments. The majority of these treatments had not yet been evaluated on humans, and their toxicity and potency was thus unknown. Most were 8-aminoquinoline compounds, analogs of pamaquine, an existing alternative to quinine that was unfavorable due to its higher toxicity. The researchers tested a wide range of doses, including some exceptionally high doses of treatments known to be toxic. The purpose of this was to establish a maximum margin of safety, and to observe the manifestation of side effects. Thus, adverse side effects were intentionally caused to subjects, to demonstrate the possible worst-case reaction to extremely high-potency treatments. In one case, a subject died several days after he was injected with a high dosage of SN-8233, a potential treatment considered in the study.\n\nThe prison of the environment of the study created a unique and complex social dynamic, with prisoners involved in many aspects of the study, not only as subjects. A well-known participant of the study was Nathan Leopold, who (together with Richard Loeb, who was killed after being sentenced) kidnapped and murdered a teenager, while they were students at the University of Chicago. Serving his sentence at Stateville Penitentiary, Leopold took interest in the malaria studies, first enrolling as a subject. Throughout the experiment he assumed many other roles, recruiting subjects, observing experiments, serving as an X-ray technician, and dissecting mosquitoes. As a technician, he was assigned roles critical to the success of the research, and the flow of knowledge, communication and resources critically depended on him at times. These accounts of his participation largely come from his autobiography \"Life Plus 99 Years\", the factual accuracy of which is verified through reputable accounts of the study. While Leopold’s range of roles was exceptional, numerous other prisoners assumed similar responsibilities in the experiments.\n\nThe experiments were widely publicized, though in a controlled manner. In 1944, \"Life Magazine\" documented the experiments with a photo series. Accounts of prisoner subjects were included, though these were allegedly scripted.\n\nWhile a series of research publications came out of the Stateville Penitentiary experiments, the results had minimal long-term impact on malaria treatment methods. The main legacy of the study is instead the ethical contention raised by prisoner experimentation, manifesting in the trials of Nazi Germany for its experiments on human subjects.\n\nIn the 1946 Nuremberg trials in Germany, the International Military Tribunal prosecuted leaders of former Nazi Germany for war crimes and events of the Holocaust, in particular, experimentation on human subjects. The Stateville malaria experiments were used as a critical point of defense for the Nazis, who argued similarities between their prisoner experimentation and the United States’ at Stateville Penitentiary. Andrew Ivy, a physician from Chicago, testified as an expert witness in the trials. He was asked to differentiate Nazi malaria experiments at the Dachau concentration camp and the Stateville Penitentiary malaria experiments. There were key distinctions, such as a higher rate of subject fatalities and lack of voluntary consent in the Nazi experiments. However, the procedures, motives and premise of the studies were arguably similar. The U.S. supported Ivy’s claims of fundamental differences, and publicized them as a justification for continuing the Stateville experiments. The international Nuremberg Code of human experimentation ethics, which resulted from the trials, contained clauses directly violated by the Stateville experiments. The U.S. never formally ratified the code, however, calling into question the ethics of prison experimentation and the Stateville Penitentiary malaria experiments in particular.\n\nPublic opposition to medical experimentation on prisoners was scant during the war. The Green Report was published in the \"Journal of the American Medical Association\" and opened the door for legal, ethical experimentation on prisoners in the United States. Until later in the Century, the medical community in the United States largely regarded the Nuremberg Code to be applicable to war criminals and not to the practices of U.S. researchers.\n\n\n"}
{"id": "45299420", "url": "https://en.wikipedia.org/wiki?curid=45299420", "title": "Sulabh International Museum of Toilets", "text": "Sulabh International Museum of Toilets\n\nThe Sulabh International Museum of Toilets in Delhi is run by the Sulabh International, dedicated to the global history of sanitation and toilets. According to \"Time\" magazine, the museum is one of the weirdest museums among the \"10 museums around the world that are anything but mundane\". It was established in 1992 by Dr. Bindeshwar Pathak, a social activist, founder of Sulabh Sanitation and Social Reform Movement, recipient of national and international awards including the Stockholm Water Prize in 2009. His objective in establishing this museum was to highlight the need to address the problems of the sanitation sector in the country, considering the efforts made in various parts of the world in this field since the third millennium BC.\n\nThe museum, established in 1992, has exhibits from 50 countries, arranged sequentially in three sections of \"Ancient, Medieval and Modern\", according to the period of the sanitation artifacts collected from 3000 BC till the end of the 20th century.\n\nThe museum's exhibits bring out the development of the toilet related technology of the entire gamut of human history, social habits, etiquettes specific to existing sanitary situation and the legal framework in different periods. The items on display not only include privies, chamber pots, decorated Victorian toilet seats, toilet furniture, bidets and water closets in vogue since from 1145 AD to-date. Display boards have poetry related to toilet and its use.\n\nSome of the interesting and amusing objects and information charts on display are: a reproduction of a commode in the form of treasure chest of the British medieval period; a reproduction of the supposed toilet of King Louis XIV which is reported to have been used by the king to defecate while holding court; a toilet camouflaged in the form of a bookcase; information on the technology transfer from Russia to NASA to convert urine into potable water, a deal of $19 million; display boards with comics, jokes and cartoons related to humour on toilets; toilet pots made of gold and silver used by the Roman emperors; information about flush pot designed in 1596 by Sir John Harington during Queen Elizabeth I's regime; the sewerage system that existed during the Harappan Civilization; and historical information from the Lothal archeological site on the development of toilets during the Indus Valley Civilization.\n"}
{"id": "21851125", "url": "https://en.wikipedia.org/wiki?curid=21851125", "title": "Tapestries of Hope", "text": "Tapestries of Hope\n\nTapestries of Hope is a feature-length documentary that exposes the virgin cleansing myth that if a man rapes a virgin he will be cured of HIV/AIDS. The film focuses on the work human rights activist Betty Makoni has done to protect and re-empower girls who have been victimized through sexual abuse. \"Tapestries of Hope\" aims to bring awareness to the widespread abuse of women and girls as well as the efforts of the Girl Child Network and its founder, Betty Makoni. \n\nThe film is directed by Michealene Cristini Risley, written by Susan Black and Michealene Cristini Risley, and produced by Michealene Cristini Risley, Susan Black, Christopher Bankston, Anand Chandrasekaran, and Ray Arthur Wang. \"Tapestries of Hope\" was theatrically released September 28, 2010 in over 100 theaters across the U.S.\n\nDirector Michealene Cristini Risley traveled to Zimbabwe to explore the rape and AIDS crisis in the country. She had previously befriended Betty Makoni, a born and raised Zimbabwean, and got to know about Makoni's organization, The Girl Child Network, which aims to empower the girls in Zimbabwe to stand up for their rights and also to provide a network as protection for these girls.\n\nThe stories were told by the girls of Zimbabwe and Makoni was the main cast for this film. Throughout the film, it was shown how Makoni has helped the girls in finding their voice and speaks out on the atrocities they have experienced. Risley and her crew were arrested and incarcerated shortly after shooting 22 hours of footage in Zimbabwe. The film was also seized by the Zimbabwean Intelligence Office at one time. However, the team managed to retrieve the footage and left Zimbabwe shortly after.\n\n\n\n\n"}
{"id": "37581508", "url": "https://en.wikipedia.org/wiki?curid=37581508", "title": "WHO Regional Office for the Eastern Mediterranean", "text": "WHO Regional Office for the Eastern Mediterranean\n\nThe WHO Regional Office for the Eastern Mediterranean is the regional office of the World Health Organization that serves 22 countries and territories in the Middle East, the North Africa, the Horn of Africa and Central Asia. It is one of the WHO's six regional offices around the world.\n\nAll the regional divisions of WHO were created between 1949 and 1952. They are based on article 44 of WHO's constitution, which allows the WHO to \"establish a [single] regional organization to meet the special needs of [each defined] area\". Many decisions are made at regional level, including importance discussions over WHO's budget, and in deciding the members of the next assembly, which are designated by the regions.\n\nThe WHO Regional Office for the Eastern Mediterranean aims to work with local governments, specialized agencies, partners and other stakeholders in the field of public health to develop health policies and strengthen national health systems.\n\nIt serves the WHO Eastern Mediterranean Region, which includes 21 member states in the Middle East, North Africa, the Horn of Africa and Central Asia, as well as the occupied Palestinian territory (West Bank and Gaza Strip). The office covers an area of nearly 583 million people. The countries and territories in the WHO Regional Office for the Eastern Mediterranean are:\nThe WHO Regional Office for the Eastern Mediterranean was originally based in Alexandria, Egypt. It was later moved to its new location in Nasr City, Cairo.\n\nThe official languages of WHO in the Eastern Mediterranean Region are Arabic, English and French. However, other national languages such as Persian, Urdu, Dari, Pashto and Somali are also used in communicating health messages and delivering health programs.\n\n\n\n"}
{"id": "36543224", "url": "https://en.wikipedia.org/wiki?curid=36543224", "title": "Zynx Health", "text": "Zynx Health\n\nZynx Health Incorporated is an American corporation specializing in providing evidence-based clinical decision support system solutions made available at the point of patient care through electronic health record (EHR) systems. Based in Los Angeles, the company serves over 1,900 hospitals and outpatient practices globally, though most are located in the US. As of 2012, Zynx Health products and services impact over 50% of hospital discharges in the US. Because of the company’s impact on care delivery proportional to its size, \"Healthcare Informatics\" named it one of the “Most Interesting Vendors” in 2011 and included it on its HCI 100 list. \"Modern Healthcare\" named Zynx Health one of the “Best Places to Work in Healthcare” in 2011. In 2012 the company’s Chief Technology Officer, Mark Long, was awarded CIO of the Year by the \"Los Angeles Business Journal\". The company is generally perceived to be the first mover in evidence-based clinical decision support solutions.\n\nZynx Health was formed in 1996 by thought leaders in evidence-based healthcare and healthcare quality improvement. As of 2004 it is a subsidiary of Hearst Corporation.\n\nIn 1996 a group of clinicians at Cedars-Sinai Medical Center founded Zynx Health as a wholly owned subsidiary of the hospital. An earlier pilot project had demonstrated the value of evidence-based guidelines in hospitals, suggesting a clear opportunity to improve patient care. The company’s first significant advancements were in 2002 and 2003, when healthcare providers realized the merit in outsourcing clinical decision support.\n\nOn May 1, 2002, Zynx Health was acquired by Cerner. Less than two years later the company was acquired by Hearst Corporation. In 2004, Zynx Health moved its offices from its original Beverly Hills, CA, location to its current location in Los Angeles, CA.\n\nOn March 30, 2017, Zynx Health partnered with Healthwise, a health education, technology, and services company.\n\nMost Zynx Health products are delivered through software as a service and are customized by the end user using patented online interfaces. Currently Zynx Health produces five products related to clinical decision support. ZynxOrder is a system used by hospitals for developing and maintaining order sets based on clinical evidence, making use of rules, reminders and other tools to assist with physician decision making. ZynxCare is a care plan development system designed for hospital nursing staff and interdisciplinary teams, helping clinicians create evidence-based plans of care. ZynxAmbulatory is an evidence-based order set development system designed for outpatient physicians, also making use of clinical rules and reminders. ZynxEvidence is an online database of clinical evidence drawn from medical and interdisciplinary literature, peer-reviewed research, and national guidelines and performance measures, and is the content foundation on which other Zynx Health products are based. Access to the database is provided to client hospitals and outpatient centers as a reference resource.\n\nTo develop and maintain these products and services, Zynx Health physicians, nurses and other clinicians conduct unbiased research and analysis of peer-reviewed studies and best practice guidelines, distilling clinical evidence into recommendations via web-based tools that integrate into a hospital’s EHR or computerized physician order entry (CPOE) system.\n\n"}
