{"id": "49513734", "url": "https://en.wikipedia.org/wiki?curid=49513734", "title": "1917 Potato riots", "text": "1917 Potato riots\n\nThe Potato riots in June-July 1917 was a popular uprising in the Dutch capital city Amsterdam that was caused by the food shortage in the Netherlands during World War I.\n\nIn the beginning of the 20th century food was more within the reach of the workers. The First World War changed this. The Netherlands was a neutral country, but experienced discomfort and hard circumstances. Imports and exports of goods stagnated. Bread and other food was rationed and soup kitchens sprang up. A bread ration was established in January 1917. On 28 June 1917, there was a shortage of potatoes. It became known in the neighborhoods of Amsterdam that there was a ship with potatoes in the Prinsengracht, but these were for the army. In order to feed their families, the working women of the Eastern Islands and the Czar Peter Neighborhood plundered the ship. According to the Councillor Josephus Jitta, there was no overall shortage of food as the workers had an extra supply of rice provided.\n\nIn the first week of July of that year, the unrest grew and the workers themselves also saw action. Warehouses and shops were looted. The police were powerless and the army acted. The revolt culminated in a battle on 5 July 1917, in which soldiers opened fire on a crowd that had gathered at the Haarlemmerplein. The revolt was beaten. There were nine dead and 114 people wounded.\n\nThe food situation deteriorated further in the last year of the war 1918. Many people lived on the brink of starvation and unemployment rose. The Spanish flu epidemic hit and killed thousands of people, weakened as they were by starvation. The armistice on 11 November 1918, came just in time for the Netherlands. For the workers another winter of hunger and cold was prevented, as well as an impending revolution.\n\n"}
{"id": "54970152", "url": "https://en.wikipedia.org/wiki?curid=54970152", "title": "Alexander Mair (physician)", "text": "Alexander Mair (physician)\n\nProf Alexander Mair OBE FRSE DPH (1912–1995) was a Scottish public health expert.\n\nHe was born in Portknockie in northern Scotland on 9 September 1912, the son of local herring fisherman, Alexander Mair (1881–1941) and his wife, Jessie Slater. His father worked on the Zulu class trawler \"Evangeline\". He was educated locally and left school at the then normal UK age of 14. He worked in the fishing industry (the mainstay of the local community) but was an on-shore clerk overseeing catches in Buckie rather than being a fisherman.\n\nNot until the onset of the Second World War was he inspired to study Medicine, joining Aberdeen University as a mature student in 1939 and graduating MB ChB in 1942. He then joined the Royal Army Medical Corps and was stationed first in Normandy in the aftermath of the D-Day landings and later in Brussels, rising to the rank of Captain.\n\nAfter the war he returned to academia gaining a Diploma in Public Health at Aberdeen then beginning lecturing in Public Health 1948. In 1952 he became Senior Lecturer in Public Health at St Andrews University and was given his Professorship in 1954.\n\nHe was elected a Fellow of the Royal Society of Edinburgh in 1980. His proposers were John McQueen Johnston, Anthony Elliot Ritchie, John Cameron, Lord Cameron, R. M. S. Smellie.\n\nHe retired in 1982 and died of a cardiac arrest in Broughty Ferry on 6 August 1995. He is buried with his parents at Portknockie.\n\n\nHe was married to Nancy Waddington and had three children.\n"}
{"id": "10580036", "url": "https://en.wikipedia.org/wiki?curid=10580036", "title": "American Sexual Health Association", "text": "American Sexual Health Association\n\nThe American Sexual Health Association (ASHA) is an American non-profit organization established in 1914, that cites a mission to improve the health of individuals, families, and communities, with an emphasis on sexual health, as well as a focus on preventing sexually transmitted infections and their harmful consequences. ASHA uses tools such as education, communication, advocacy and policy analysis activities with the intent to heighten public, patient, provider, policymaker and media awareness of STI prevention, screening, diagnosis and treatment strategies.\nASHA was born out of the early 20th century social hygiene movement. At the beginning of the twentieth century venereal disease (VD), or what we now call sexually transmitted diseases or infections (STDs/STIs), was a prevalent concern for social health organizations. Sexuality was not an acceptable topic for polite conversation in Victorian society, and VD largely remained behind a veil of shame. At the same time, however, reports of rising incidence rates for syphilis and gonorrhoea gave cause for serious concern. For example, by one perhaps inflated but widely accepted estimate in 1901, as many as 80 percent of all men in New York City had a gonorrhoea infection at one time or another. Early efforts to eradicate \"social diseases\" focused on prostitution on one hand, as both a threat to the family as a source of infection, and the threat to public health on the other, with the focus on addressing the problem through both medical and educational means.\n\nIn 1913, at a conference in Buffalo, New York, several organizations dedicated to fighting prostitution and venereal disease joined together to form the American Social Hygiene Association (ASHA). The association was established to stop the venereal disease epidemic through public education on STIs and working to break down the social stigma attached to VD. In 1914, ASHA established its national headquarters in New York City. Founders and supporters included Charles Eliot, president of Harvard University; Jane Addams of Chicago's Hull House; Edward Keyes, Jr., M.D.; Dr. Thomas N. Hepburn, leader of the Connecticut social hygiene movement; Grace Dodge, philanthropist; John D. Rockefeller, Jr., initial financial contributor; and Dr. William Freeman Snow, Stanford University professor and secretary of the California State Board of Health.\nASHA's early worked focused on education and awareness efforts within the armed forces. ASHA worked with the US War Department during World War I when VD occurrences surged among soldiers. Their efforts included educating soldiers about venereal diseases and their transmission and attempting to eliminate prostitution, which was believed to be the primary vehicle for VD transmission among the armed forces. ASHA was successful in shutting down many of the prostitution rings that traditionally surrounded military bases. Due to its contribution to the war effort, ASHA gained national attention and succeeded in creating public awareness of VD.\n\nDuring the 1920s, ASHA served as a central coordinator for the local or regional committees, doctors, public health officials, and social welfare agencies that were combating sexually transmitted infections. In addition, ASHA published the Journal of Social Hygiene and the Social Hygiene Bulletin, conducted studies on the prevalence of syphilis, undertook surveys on VD, published synopsis of laws concerning prostitution, and supported legislation that required a premarital exam for syphilis. The program also promoted character and sex education as a means of preventing the spread of STIs. The ASHA educational program emphasized preparation for a wholesome family life, avoiding VD, and physical as well as moral fitness. \n\nASHA had developed into a mature organization by the 1930s with an effective network of supporting local organizations. ASHA also was involved in cooperative projects with a variety of organizations. In a single year, ASHA collaborated with the Federal Council of Churches and the National Congress of Parents and Teachers to promote sex education programs and materials; provided leadership for efforts by the White House Conference on Child Health and Protection to consider the social hygiene elements of child health; conducted institutes for public health nurses under the auspices of the National Organization of Public Health Nursing; and provided data to the U.S. Bureau of Indian Affairs in preparation for Indian Health Programs. \n\nDuring World War II, ASHA fulfilled a role reminiscent of its work during World War I, serving on the VD Coordinating Committee for the US military and working against prostitution. ASHA's efforts contributed to a 50% drop in VD infection rates in the military during the first years of the war. In 1944, the military began using penicillin as a cure for syphilis and by the late 1950s it was believed that syphilis was no longer a serious health threat. As a result, the Journal of Social Hygiene discontinued publication.\n\nThe release of the Kinsey reports on sexual behavior in 1948 and 1954 created national controversy. ASHA played a prominent role in the debate, organizing a national conference to bring together leading authorities in the fields of psychology, statistics, education, medicine, law, religion, anthropology and sociology to exchange views as to the significance of Kinsey's new information. The goal was to look at the information as scientific data instead of pornography. As Walter Clark, ASHA president from 1937 to 1951, commented, \"The truth never harms . . . And it seems reasonable to hope that when today's older generation, conditioned against frankness in sex matters, passes away and today's youth takes over tomorrow's world, the truth about sex shall indeed make them free—free of the diseases, the exploitations, the ignorance and superstitions which for ages have burdened and blighted society.\"\n\nAlso in the 1950s, ASHA began to distribute a comprehensive annual questionnaire to health officers across the country. The responses, tabulated by ASHA and analyzed by the Centers for Disease Control and Prevention, were published annually as Today's VD Control Problem, continuing under ASHA auspices until 1975. Today's VD Control Problem provided the statistical basis for testimony before Congress, as ASHA returned year after year to urge adequate federal appropriations for STD control.\n\nAs ASHA began to recognize that the STD issue connected with other issues, it reflected its broader approach in a name change, moving from American Social Hygiene Association to American Social Health Association in 1960. Among the issues of concern identified by ASHA was the link between STDs and drug use. In 1961 ASHA launched a new program that was the first to focus on the prevention of narcotic addiction and the treatment and rehabilitation of drug addicts. The agency evaluated existing programs, issued position papers and informational booklets, maintained a clearinghouse, and sponsored four community-based pilot projects. \n\nBy the 1970s, though, additional novel problems led to soaring STD rates, including the sexual revolution, more international travel, gay liberation, birth control for women, and increasing drug use. Scientists were also recognizing more sexually transmitted pathogens, and viral STDs were making an appearance as well, from herpes simplex virus (HSV) to human papilloma virus (HPV). ASHA's public outreach efforts in this era included the creation of the Herpes Resource Center in 1979, the first program in the U.S. for people living with a viral STD. ASHA also established the National VD Hotline, where trained volunteers offers scientifically accurate information and support. ASHA also launched its first modern public awareness campaign, VD is For Everybody. Working with the National Advertising Council, ASHA called attention to the alarming increase in the number of STDs by means of radio, television, and print public service announcements.\n\nDuring the 1980s, ASHA continued to educate the public about sexually transmitted infections, primarily by means of telephone information and referral hot-lines, such as the National STD Hotline and the National AIDS Hotline. ASHA established the latter hotline in 1986, at that time the largest health-related hotline in the world. By the 1990s, the hotline was answering more than 1.5 million calls per year. The association also continued to advocate for public policies to combat STDS and increased funding for research. The identification of the AIDS virus added a new area of concern to the association's fight against sexually transmitted infections.\n\nASHA continues its mission to improve the health of improving the health of individuals, families and communities with a focus on preventing sexually transmitted diseases and their harmful consequences, and has broadened its efforts into the field of sexual health. Its current programs include:\n\nSTI Resource Center, that provides information, materials and referrals to the public who have questions or concerns about sexually transmitted infections. Through its telephone hotline and online message boards, the Center answers questions on such topics as transmission, risk reduction, prevention, testing, and treatment and partner communication.\n\nHerpes Resource Center, founded in 1979, continues to provide information and support through its multiple web pages, message board and publications. The center also offers The Helper, a quarterly journal that discusses the latest in herpes information, research, treatment, testing and patient-advocacy.\n\nHPV and Cervical Cancer Prevention Resource Center, established in 1991, offers information and referrals about the Human Papillomavirus to patients, health care providers, and policy makers. The HPV Resource Center focuses on issues including HPV vaccines, partner communication and cervical cancer screening. The Center also offers a bimonthly electronic journal, HPV News, that covers the latest in HPV research, treatment and testing options, and policy issues. \n\nASHA's Research division conducts numerous research projects among local, regional, and national constituents focusing on a variety of populations and an assortment of health topics including sexually transmitted infections. ASHA's Washington staff works to educate members of Congress and other important voices in health policy about the urgency of research and frontline programs in the STI field. \n\nAdvocacy: ASHA has maintained a policy office in Washington, DC and has worked in partnership with other organizations in the area of sexual and reproductive health to advocate for proper attention and funding to STD research and programs.\n\nNational Cervical Cancer Coalition: NCCC became an ASHA program in the fall of 2011. Founded as a grass-roots organization in 1996, NCCC supports cervical cancer patients/survivors, families, and caregivers. NCCC has chapters across the U.S.\n\n\n\n"}
{"id": "45467117", "url": "https://en.wikipedia.org/wiki?curid=45467117", "title": "Amoreira Aqueduct", "text": "Amoreira Aqueduct\n\nThe \"Amoreira Aqueduct\" () is a 16th-century aqueduct (begun in 1537) that spans the Portuguese municipality of Elvas, bringing water into the fortified seat.\n\nBy around 1498, the only fountain and source of potable water since the Moorish occupation had been the \"Poço de Alcalá\", alongside the \"Porta do Bispo\". As the well had already begun drying-up, and owing to the difficulty of gathering water from the wells surrounding the town, King D. Manuel authorized a tax (the \"Real de Água\") to fix the \"Poço de Alcalá\". Yet, the attempts did not meet with success, and an idea developed to collect water from Amoreira, some away. Consequently, in 1529 the first works to build the aqueduct were begun. In 1537, then King John III intervened by sending architect Francisco de Arruda to Elvas, to make changes to the primitive plan. Owing to the lack of funds for such a project, the King then authorized the diversion of monies from fines and sales associated with the sale of municipal lands for the project, including some loans from the children's orphanages in Elves and Estremoz.\n\nThese early problems with the aqueduct's construction, were portents of future problems. Sometime during 1533, the work done by Lourenço Domingues (the receiver for the public works in Elvas, Campo Maior and Olivença) must have been unsatisfactory, for he was discharged on 9 September. Following various diversions of water, the post of \"Visitador do Cano\" (\"Seer of Pipe\") was established in 1542. A year later, Diogo Mendes was responsible for the maintenance of the aqueduct, receiving 6000 reais, paid by the town. But, by 1547, the construction was suspended due to a lack of funds, but were re-initiated in 1571. But, on 20 March 1558, Queen D. Catherine recommended that the town work on the aqueduct. King D. Sebastian later authorized the application of a levy on the residents of Elvas, considered equivalent to each's income. Later that year, on 23 August, Sebastian personally inspected the aqueduct and finding the works incomplete, he ordered the Senate (on 29 May 1573) to correct the remaining sections, along with Afonso Álvares. The King's successor would have similar problems with this project; in 1579, Cardinal King Henrique solicited the bishops of Elvas to participate monetarily in the aqueduct's construction. \nWith the Castilian invasion in 1580, priority was given to the city's fortifications and it was decided, if necessary, to drop the project. One of the conditions made by the Portuguese, during the surrender negotiations of the city, was that the Amoreira Aqueduct be complete. The city looked into new springs in the foothills, sometime around 1602, in order to improve the water pressure/volume. The aqueduct project was restarted in 1606, with the imposition of a new \"Real de Água\" tax. But, following various stops on the project, on 5 March 1610, the municipal council solicited the King to re-continue the aqueduct's construction, while proposing alterations in the project in order to provide water to the higher settlements of the region. King Phillip II authorized the continuation of project on 11 July 1610, and sent Diogo Marques Lucas to study the plan in order to improve its efficiency on 26 July. Lucas determined the need to elevate the aqueduct an additional 25 palms in order to better improve water delivery to the \"Largo da Misericórdia\".\n\nThe first waters reached into the city gates in 1620, to a provisionary fountain at the Church of Madalena. On 23 June 1622, the aqueduct and Fonte da Misericórdia was inaugurated, but, owing to structural problems with the \"completed\" project, interventions were needed. In 1625, it was decided to construct hollowed buttresses, filled in with dirt and loose stone. Ancillary projects began to develop following the completion of the main project, including diversion of water down the \"Rua de São Lourenço\", a lengthy project owing to the rock substrata. These projects resulted in the extension of the taxes on 1 August 1627, for another two years, that included the construction of city's fountains. By 1628, the aqueduct was successfully fed the Fonte da Misericórdia, Chafariz da Madalena, Chafariz de São Lourenço, Chafariz de São Domingos, Chafariz de São Vicente and the Chafariz da Alameda, and construction of fountains of Biquinha, Cavaleiros and São José were underway.\n\nIn 1641, King D. John IV authorized the demolition of the aqueduct in order to construct a new fortification. Martinho Afonso de Melo, Count of São Lourenço, proposed the construction of a cistern, in order that most of the aqueduct could be maintained functioning. It was not until 1650 that the huge cistern was built beneath Elvas, under the direction of Nicholas de Langres, allowing for the storage of water capable of providing enough for the entire population for four months, when rationed judiciously. Two arches of the aqueduct were destroyed following a tempest in 1646, and two years earlier Castilian forces had already destroyed or damaged segments of the vital resource. Therefore, it came as no surprise that in May 1648 a discussion began on whether destroying the aqueduct was necessary, in order to improve the city's defences. In 1652, the Count of Soure ordered the construction of subterranean pipe to feed water the city, thereby maintaining the city's defences (but also resulting the partial loss of the aqueduct). Three years later, Governor André de Albuquerque Ribafria also requested that that aqueduct be demolished, and that the segments that corresponded with the citadel be converted to subterranean passages.\n\nDuring the 1663 Castilian invasions by John of Austria, the aqueduct suffered some damage. In order to remedy its vulnerability, two redoubts were constructed in Outeiro dos Pobres and Outeiro de São Francisco, reinforced by a zone that crossed the two by casements of canon emplacements.\n\nA new plan for the aqueduct was drafted in 1683 by Francisco Álvares Ribeiro, that included system. On 30 March 1689, Manuel Moniz was nominated to the position of master-builder for the aqueduct, on the death of Francisco Ferreira (who exercised this post), receiving a stipend of 12$000 reis annually. In 1698, there was a rupture that linked the hospital.\n\nBy the beginning of the 18th century water pressure was already much reduced, due to the accumulation of calcium (a problem that persisted until the 19th century). A sedimentation arch was constructed at the Borracha spring to alleviate some of these problems. João Fernandes Cordeiro was named master-builder, and on 20 June 1702, Francisco Martins was named \"master of pipe\" for the Amoreira Aqueduct, following the death of Manuel Moniz, receiving 12$000 reis annually. In 1708, protests resulted in damage to the structure. King John V suggested substituting the piping to a fountain system (or inverted siphon system). But, the municipal authority could only decide on reconstructing the arches, since the system was impractical in case of ruptures, requiring opening-up the entire network in case of localizing the problem. Alternately, the structure was reinforced with piers and in 1715, the system was expanded with waters from the \"Poço do Concão\", later nine assorted springs, including one from the Serra do Bispo region. On 5 June 1727, João Fernandes Cordeiro was confirmed as the new aqueduct master-builder. The system was also expanded in 1733 with the waters from the nine springs situated in the Herdade de Trinta-Alferes, reinforced in 1739 with the exploration of the \"Poço do Gorgulhão\" in 1743. On 17 March 1745, José Ramalho Rogado took over the position of aqueduct master-builder, responsible for expanding the water supply with the piping to the \"Poço Seco\" spring. In January 1796, a earthquake destroyed two arches in the Outeiro dos Pobres, which was quickly repaired. In 1825, the discovery of new springs in the Herdade de Trinta-Alferes, resulted in the exploration by Lieutenant Francisco de Paulo de Sousa Pegado. This man later directed new repairs on the aqueduct between 1825 and 1827. Between 26–27 February 1846, a seasonal storm destroyed a buttress.\n\nThe first municipal azulejos were erected in 1864, and were later supported with alternates dating to 1873. In 1870, the aldermen requested that the Cortes provide funding to repair and clean the structure, as well as construct a supplementary canal to facilitate future cleaning. During these motions, it was indicated that calcium buildup in the pipes should be removed every ten years, to remedy these problems.\n\nDuring the 19th century, the exploration of the Herdade dos Vales de Santarém springs was initiated.\n\nBetween 1872 and 1890, the aqueduct underwent important public works to conserve and restore the aqueduct, that included the construction of a secondary pipe and construction of 462 supplementary arches. By 1902, the springs were considered impotable, except those at Ruy de Mello and São Lourenço, and continued to feed the aqueduct. In the 1980s, the aqueduct continued to feed the city of Elves, by way of the cistern, using waters raised from the Trinta-Alferes and Algaravenha springs. Repair and cleaning of the piping occurred in 1995.\n\nThe aqueduct crosses the rural-urban divide, covering the plains across the valley of São Francisco and Rossio, encountering the springs at Fonte da Amoreira (in the Serra do Bispo) and terminating at the Fonte da Misericórdia. In front of the Elvas portion of the aqueduct is the Municipal Gardens, while to the south is the link to the Convent of São Francisco, protected in the southwest by a ravelin.\n\nThe aqueduct has a length of from its spring in the Serra do Bispo until the Chafariz do Jardim, and another to the town fountain. Its track includes subterranean segments, to a maximum altitude of above sea level, while include arcade segments. From Outeiro de São Francisco is the second canal, which redirects waters to Outeiro dos Pobres, identifiable in the arcade at Rossio. In total there are 833 arches, with at most four registers at any one time (diminishing size at higher altitudes), supported by rectangular pillars and strengthened by semi-circular and pyramidal buttresses. The structure includes the municipal coat-of-arms constructed of marble or azulejo. A reservoir is located in Horta de Trinta-Alferes, a rounded arch in the São Gonçalo downtown (Herdade da Serra do Bispo), as well as an arch and tank at the spring of Borracha.\n\n\n\n"}
{"id": "39762687", "url": "https://en.wikipedia.org/wiki?curid=39762687", "title": "Beating Bowel Cancer", "text": "Beating Bowel Cancer\n\nBeating Bowel Cancer is the support and campaigning charity for everyone affected by bowel cancer in the UK. The charity provides support and information for bowel cancer patients and their families through the UK's only nurse-led specialist helpline for bowel cancer, their online forum and booklets and fact sheets. The charity also works tirelessly to raise public awareness of bowel cancer and campaign to ensure Governments and health services provide the highest quality care and treatments.\n\nThe charity's mission statement is as follows:\n\n\"We are dedicated to saving lives by working in partnership with individuals, local communities, clinical communities and government to improve public awareness of bowel cancer and to increase the rate of early diagnosis. We help patients access the treatment they need and provide emotional and practical support to improve the lives of everyone affected by bowel cancer.\"\n\nBeating Bowel Cancer is the secretariat for the National Colorectal Cancer Nurses Network (NCCNN).\n\nThe charity organises a Patient Day for patient and family once per year.\n\nThe Chief Executive Officer is Mark Flannagan.\n\nA key part of the charity's fundraising strategy is Decembeard, which took place in 2011, 2012, 2013, 2014, 2015 and 2016. This was reported on by Third Sector as having 'netted an impressive return'.\n\nThe charity is involved each year with Bowel Cancer Awareness Month, which takes place in April.\n\nChairman\nSir Christopher Pitchers\n\nBoard Members\n\n\nThe charity's Charity Number is 1063614 (England and Wales) and SC043340 (Scotland).\n\n"}
{"id": "3516857", "url": "https://en.wikipedia.org/wiki?curid=3516857", "title": "Blood donation in England", "text": "Blood donation in England\n\nIn England, blood and other tissues are collected by NHS Blood and Transplant (NHSBT), which also processes and supplies blood products to hospitals in the country through the Bio Products Laboratory. NHSBT Blood Donation was previously known as the National Blood Service until it merged with UK Transplant in 2005 to form a NHS special health authority. Other official blood services in the United Kingdom include the Northern Ireland Blood Transfusion Service, the Scottish National Blood Transfusion Service and the Welsh Blood Service.\n\nThe service depends entirely on voluntary donations from the public. Originally, Blood was collected from various donor clinics located over the country. In 1994, the first mobile session was held in Elstree, hosted by the Joely Bear Appeal. Currently, Blood donation sessions are set up throughout the country and take place in many diverse venues. From village halls, to mobile collection units (known as Bloodmobiles), and sessions set up companies and organisations so people can donate at work. Donors are generally required to be fit and healthy, weigh and be aged between 17 and 60. However, regular (healthy) donors are permitted to donate past the age of 60 as long as they remain healthy. Donors are encouraged to give blood up to three times a year (once every 16 weeks) for women and four times a year (every 12 weeks) for men.\n\nPrior to each donation, the donor's iron level is checked to make sure they are not anaemic. The donor will be required to fill in a questionnaire to provide consent and declare that the donation will be safe (for example, stating that the donor does not have a heart condition), and it is safe to give the donor's blood to someone else.\n\n\n\n\n\n\n\n\n\n\n\n\nNote: This 3 month deferral period only applies to England, Wales and Scotland - not Northern Ireland. Northern Ireland still has a 12 month ban in place for these groups.\n\nOnce the preliminary checks are complete, the donor lies on a bed and a sterile hypodermic needle connected to a bag is inserted into a vein in their inner elbow. The donation usually lasts between five and 10 minutes, during which of whole blood is given.\n\nFollowing the donation (and subsequent dressing of the wound), donors are invited to refreshments. This period serves two vital purposes: to replace certain lost fluids, and to allow staff to monitor the donor's wellbeing.\n\nDonations can also be taken by machines called cell separators, usually in larger blood donation centres located in city centres. These machines use a process called apheresis to collect either blood plasma only, or plasma and platelets, the other blood cells being returned to the patient. Platelets are the tiny fragments of cells in the blood which help it to clot and so stop bleeding, and are used in the treatment of cancer and leukaemia. A constant supply is vital because platelets only last five days once collected. People who give plasma and/or platelets can donate every two weeks, and each donation usually gives two or three adult doses. One adult dose of platelets would otherwise require four whole blood donations.\n\nBlood is made in the bone marrow. It is composed of red blood cells, platelets, plasma and white blood cells, collectively referred to as blood components. Donations given at regular blood donation sessions are referred to as \"Whole Blood\". \nPlatelets are very small cells. They work with the clotting factors in plasma to form a mesh \"plug\" to stop or prevent bleeding. Plasma is the fluid part of the blood. It contains protein, salts and clotting factors. White cells fight harmful bacteria and help prevent infection. Red cells carry oxygen from the lungs to the tissues.\n\nMost platelet donations are given to patients who are unable to make enough platelets in their bone marrow. For example, patients with leukaemia or other cancers may have too few platelets as the result of their disease or treatment. Also after major surgery or extensive injury, patients may need platelet transfusions to replace those lost through bleeding. Platelets are often life-saving and special in that they can help up to 3 adults or even 12 children. As platelets can only be stored for a few days, regular and frequent donors are in great demand and that is why platelet donors are asked to attend at least 8 - 10 times per year.\n\nThe service had come under criticism for a long implemented policy of banning men who have sex with men (MSM) from ever being blood donors. University students in both England and Scotland protested against the ban,\nand University of Birmingham's Guild of Students banned the National Blood Service from setting up a recruitment stall during Freshers' week.\n\nThe policy was changed in 2011 to prevent MSMs from donating for 12 months after having anal or oral sex with another man (with or without a condom). This also applied to women who have had sex with a MSMs. In November 2017 the referral period on MSMs was reduced to 3 months for both MSMs and women who have sex with them.\n\nA vs The National Blood Authority (Queens Bench Division). This trial concerned the claims of 114 Claimants, for recovery of damages arising out of their infection with Hepatitis C, from blood and blood products through blood transfusions from 1 March 1988. All the claimants received blood transfusions or blood products usually in the course of undergoing surgery, whether consequent upon having suffered an accident or otherwise, immediately after childbirth.\n\n\n"}
{"id": "35159726", "url": "https://en.wikipedia.org/wiki?curid=35159726", "title": "Brent Robbins", "text": "Brent Robbins\n\nBrent Dean Robbins is Associate Professor of psychology at Point Park University in Pittsburgh, Pennsylvania. His areas of research include grief, humor, self-consciousness, spirituality/religion, death anxiety, and the medicalization of the body. He is Editor-in-Chief and founder of \"Janus Head: Journal of Interdisciplinary Studies in Literature, Continental Philosophy, Phenomenological Psychology, and the Arts\", and is a Board member for a number of journals, including \"International Journal of Transpersonal Studies\", the \"International Journal of Existential Psychology and Psychotherapy\", \"PsyCRITIQUES\", and \"Terrorism Research\". Robbins is a co-editor of \"The Legacy of R.D. Laing\", published by Trivium Press. Robbins is a recipient of the Harmi Carari Early Career Award, from the Society for Humanistic Psychology. He holds a doctorate in clinical psychology from Duquesne University.\n\nIn 2011, Robbins co-authored an open letter from the Society for Humanistic Psychology regarding the \"Diagnostic and Statistical Manual of Mental Disorders\"s fifth edition, the DSM-5. The letter has been endorsed by thirteen other American Psychological Association divisions, and has been signed as a petition by over 15,000 people. In a recent \"San Francisco Chronicle\" article about the debate over the DSM-5, Robbins noted that, under the new guidelines, certain responses to grief could be labeled as pathological disorders, instead of being recognized as being normal human experiences.\n\nBrent was born into a nominally Catholic family, became an atheist when in college, and then he reverted to the Catholic faith after experienced something in a retreat.\n\n"}
{"id": "32898182", "url": "https://en.wikipedia.org/wiki?curid=32898182", "title": "Bristol heart scandal", "text": "Bristol heart scandal\n\nThe Bristol heart scandal occurred in England during the 1990s. At the Bristol Royal Infirmary, babies died at high rates after cardiac surgery. An inquiry found \"staff shortages, a lack of leadership, [a] ... unit ... 'simply not up to the task' ... 'an old boy's culture' among doctors, a lax approach to safety, secrecy about doctors' performance and a lack of monitoring by management\". The scandal resulted in cardiac surgeons leading efforts to publish more data on the performance of doctors and hospitals.\n\nAn investigation chaired by Professor Ian Kennedy QC was set up in 1998. It reported in 2001. It concluded that paediatric cardiac surgery services at Bristol were \"simply not up to the task\", because of shortages of key surgeons and nurses, and a lack of leadership, accountability, and teamwork. In fact the unit, which had “not been up to the task” in 5 years (1991-1995) had left 34 children under one year of age dead, who would have survived in other NHS units (Ref ). Overall 170 children died in the Bristol unit between 1986 – 1995 who would have survived in other NHS hospitals as estimated by Laurence Vick, the lawyer most closely involved in the Bristol Scandal (Ref https://www.enablelaw.com/news/published-articles/loneliness-nhs-whistleblower/). Sadly the same expert estimates that 25-30 children suffered permanent brain damage after cardiac surgery by the Bristol surgeons over the same 10 year time span (Ref https://www.lexology.com/library/detail.aspx?g=9d6ebc53-0371-48cf-b2bd-0324f800f8da). \n\nThe NHS Plan 2000 published a year earlier, included the establishment of the Commission for Health Improvement, which was intended to tackle such problems.\n\nBy 2010, the mortality rate within 30 days of a child's heart operation had fallen from 4.3% in 2000 to 2.6%. Plans to reduce the number of centres performing children's heart surgery have been opposed. A report to NHS England in July 2015 proposed a “three tier” model for all hospitals providing congenital heart disease care. It suggested that they would work within “regional, multi-centre networks, bringing together foetal, children’s and adult services” and noted that since 2001 there “have been subsequent reviews each making a series of recommendations, but no coordinated programme of change, and concerns have remained”.\n\n\n"}
{"id": "5150941", "url": "https://en.wikipedia.org/wiki?curid=5150941", "title": "Condom machine", "text": "Condom machine\n\nA condom machine is a vending machine for the sale of condoms. Condom machines are often placed in public toilets, subway stations, railway stations and airports as a public health measure to promote safe sex. Many pharmacies also keep one outside, for after-hours access. Rare examples exist that dispense female condoms.\n\nCondom vending machines were introduced in 1928 by Julius Fromm's company.\n\nIn the novel \"Porterhouse Blue\", conflict over an attempt to introduce such a device to a Cambridge college, is one of the factors that leads to murder.\n\nAccording to the United States Food and Drug Administration, when using condoms from a machine, one should check the expiration date, that the condoms are latex and labelled for disease prevention, and that the machine is not exposed to direct sunlight or other source of extreme temperatures.\n\n"}
{"id": "52958268", "url": "https://en.wikipedia.org/wiki?curid=52958268", "title": "David Dunn (character)", "text": "David Dunn (character)\n\nDavid Dunn is a fictional character and the main protagonist in the \"Unbreakable\" film series, portrayed by American actor Bruce Willis. Dunn is a former college football prodigy and presently a security guard who discovers he has superhuman abilities. He is the protagonist in \"Unbreakable\", a minor cameo character in \"Split\", and will return in \"Glass\" as the main protagonist.\n\nDavid Dunn was a football player until an auto car accident occurred. Sometime after the accident, he went off to marry Audrey who together have a son named Joseph.\n\nIn the present, David is a security guard looking for a new job. On his way home from an interview in New York, he gets involved in another accident, this time in a train explosion, which he became the only unharmed sole survivor. He is stalked by comic book store and art gallery owner Elijah Price who contacts him to explain his unique abilities but David challenges him with the theory by using a childhood incident when he almost drowned with Elijah further stating that water is his weakness. He later believes that he is a superhero which Joseph convinces him to be later on. Later that day, David begins to train using a 350 pound bench press. David uses one his abilities; one where he can see visions of crimes when people bump into him, at a train station. He finds a janitor who David follows home where he is holding children hostage. David rescues the family and is claimed a \"Superhero\" in the papers the following day which he shows to Joseph as a secret. David attends an exhibition at Elijah's art gallery where he meets Elijah's mother. Elijah calls David to the back of his room where he forces his hand on Elijah which visions on him show Elijah conducting the train explosion. David calls police to arrest Elijah, who is taken to an institution for the criminally insane. \n\nIn 2016, David now has a new job. He visits a coffee shop where a woman asks about a man which David realises she was talking about Elijah, who had taken on his childhood nickname \"Mr. Glass\". \n\nWhen M. Night Shyamalan conceived the idea for \"Unbreakable\", the outline had a comic book's traditional three-part structure (the superhero's \"birth\", his struggles against general evil-doers, and the hero's ultimate battle against the \"archenemy\"). However, he found the origin story most interesting, and chose to write \"Unbreakable\" as one. Willis became attached while shooting \"The Sixth Sense\", also directed by Shyamalan.\n\n\"Unbreakable\" was first released in November 2000. In the film, Dunn is a security guard who discovers he has supernatural abilities, with superhuman levels of strength, stamina, and invulnerability, as well as an extrasensory ability to see the crimes people have committed by touching them. Throughout the film, he is encouraged by Elijah Price, also known as \"Mr. Glass\" (Samuel L. Jackson) to become a superhero. Price, whose bones break easily because of a disease, develops a theory that if there is an extreme fragility there must be an \"unbreakable\" human in existence. As various people bump into him, he senses the crimes they perpetrated, such as theft and rape, and finds one he can act on: a sadistic janitor who invaded a family home, killed the parents and is holding the children captive. David follows the janitor to the victims' house and frees the children, but the janitor ambushes him and pushes him off a balcony into a swimming pool. David nearly drowns (since he cannot swim), but the children rescue him. He then attacks the janitor from behind and strangles him to death while once more remaining uninjured. At the end of the film, through his abilities, Dunn senses that Elijah orchestrated the train crash he had been in previously and countless other disasters that have resulted in massive losses of life, in an attempt to find David. His justification for this seemingly is claiming the purpose of his life and condition was to be David's archenemy. Dunn turns Price in to authorities, and Price is arrested for murder and terrorism and later committed to an institution for the criminally insane.\n\nIn a final reveal in the 2016 film \"Split\", Dunn appears in a cameo role, revealing that the film takes place in the same universe as \"Unbreakable\".\n\nWhile sitting in a diner, Dunn and several other patrons listen to the media coverage of the crimes committed by Kevin Wendell Crumb, a man who has been nicknamed \"The Horde\". The patron sitting next to Dunn notes the resemblance between \"The Horde\" and a wheelchair-bound criminal who was arrested 15 years prior. When she struggles to remember the man's name, Dunn tells her it was \"Mr. Glass\".\n\nShyamalan expressed hope for a third installment following \"Split\", saying, \"I hope [a third \"Unbreakable\" film happens]. The answer is yes. I'm just such a wimp sometimes. I don't know what's going to happen when I go off in my room, a week after this film opens, to write the script. But I'm going to start writing. [I have] a really robust outline, which is pretty intricate. But now the standards for my outlines are higher. I need to know I've won already. I'm almost there but I'm not quite there.\" In April 2017, Shyamalan announced that the film will be titled \"Glass\", with Bruce Willis, Samuel L. Jackson, Anya Taylor-Joy, and James McAvoy reprising their respective roles in the series.\n\nWillis received critical praise for his performance as Dunn in \"Unbreakable\". Critic Roger Ebert believed that Willis' \"subtle acting\" was positively different from the actor's usual work in \"brainless action movies\". Quentin Tarantino, who directed Willis in \"Pulp Fiction\", has also praised his performance in \"Unbreakable\", saying he considers it his best work.\n\n"}
{"id": "40674659", "url": "https://en.wikipedia.org/wiki?curid=40674659", "title": "Dietary exposure assessments in the United States", "text": "Dietary exposure assessments in the United States\n\nDietary exposure assessments involve the evaluation of dietary consumption and chemical residue data while taking into consideration additional factors that may affect a specified population of interest or sensitive population. The process of conducting a dietary exposure assessment involves the determination of the chemical residues on a particular food or foods and the calculation of the dietary exposure to these chemicals based on consumption data for the specified food or foods. In the most simplified form, a dietary exposure assessment can be summarized with the following calculation:\n\nThe purpose of calculating dietary exposure to a given chemical or contaminant is so the estimated dietary exposure can be compared to a relevant health standard such as the acceptable daily intake (ADI), the acute reference dose (ARfD) or reference dose (RfD), or a level known to cause adverse effects in animal or human health studies. From this comparison, one can begin to assess the risk of adverse effects from a chemical or contaminant due to dietary exposure.\n\nQuantifying dietary exposure most often involves the use of models. Dietary exposure models vary in complexity depending on the tier, or level of detail of the assessment in which they are utilized. Lower tier models may be deterministic and use single point estimates for inputs whereas higher tier models may be more stochastic, utilizing probabilistic inputs. Simple dietary exposure models calculate dietary intake of the chemical of concern as the product of the mass of the food item consumed during the specified time period and the average concentration of the contaminant in the item. This basic approach is also used in more complex models; however, the contributions of various food items and types are summed as inputs. Exposure model outputs can be expressed as a single point estimates or as a probability distribution. In general, the more robust and extensive the input data, the more accurate the estimate of exposure.\n\nModeling dietary exposure is important to support dietary risk assessments. Validated dietary exposure models can be used in total ingestion studies which focus on the level of exposure to a contaminant from all food sources and they can also be used in aggregate exposure models which may include exposure to a chemical or contaminant through additional means of exposure such as inhalation and dermal or skin contact exposure. In addition, dietary exposure models are useful for identifying at risk populations and potential exposure “hot-spots”. For government agencies, modeling dietary exposure is especially useful in estimating possible exposure to a chemical during a pre-market review. An example of such review would the evaluation of a new pesticide by the Environmental Protection Agency (EPA) to determine if it is safe for use or to determine regulations to control such use.\n\nIn order to assess dietary exposure, it is necessary to understand the dietary consumption of the population being assessed. One must understand the diet of the population of interest, including the quantity and types of foods consumed, in order to determine the exposure to a chemical via the diet. Consumption data for a population of interest can be generated through various types of consumption surveys or it may be available in databases maintained by various government agencies and world organizations. These consumption surveys and databases are discussed in the following sections.\n\nFood consumption surveys are conducted by government agencies and world organization in order to maintain their respective databases. These consumption surveys are also conducted independently to generate data for specific assessment needs. Different types of surveys are used to generate consumption data. It is important to understand the strengths and weaknesses associated with the different types of surveys in order to choose the appropriate survey and interpret existing survey data properly.\n\nFood supply surveys, also known as food balance sheets or food disappearance data are conducted countrywide by almost every country. These surveys provide data on food availability and disappearance rather than actual food consumption. This information does allow for the indirect estimation of the foods consumed by the country’s population but does so as an overall average consumption and does not account for consumption variation within the population. Food supply surveys do not take into account food waste or processing but they do allow for assessments at the international level using comparable data. Food supply data is generated from the following equation:\n\nIn the United States food supply data is generated by the USDA Economic Research Service. The FAO and WHO collect food supply data for countries all over the world and in Europe food supply surveys are conducted by the Organization for Economic Cooperation and Development and the Statistical Office of the European Communities.\n\nHousehold inventories account for what food is available in the household. Surveys may include: what food enters the household and where they were purchased, grown or obtained; what foods were used; how they were used; and who used them. Quantities of food are often inventoried as well.\n\nHousehold use survey methods typically include food accounts, inventories, and list recalls which are used to account for all food used in the household during a survey period. Limitations of household use surveys are that they do not include food consumed outside of the household, food waste is often not accounted for and consumption by different subpopulations within the household cannot be distinguished.\n\nIndividual consumption studies provide data at the individual consumer level. These surveys can be retrospective, prospective, or a combination of both. The most commonly used surveys are retrospective food recall studies or prospective food record studies. Individual consumption surveys obtain diaries from a population-based probability sample of respondents who report detailed information of their food consumption by type.\n\nFood consumption databases are available through various government agencies and international organizations. Utilization of consumption databases is useful when it is not possible or not desired to generate new consumption data for an assessment. Choosing the appropriate database is important in order to accurately assess the consumption of the target population being evaluated.\n\nThe USDA Food Availability Data System is one of the primary databases tracking consumption in the United States. The data in this database reflects the amount of food available for human consumption in the United States and is the only source of time series data on U.S. food availability in the country. This database takes regularly consumed foods and aggregates them into approximately 800 core foods with similar raw agricultural commodities. Converting these regularly consumed foods into raw agricultural commodities allows for easier cross referencing with chemical residue databases.\n\nThe World Health Organization implements the Global Environment Monitoring System – Food Contamination Monitoring and Assessment Program, or GEMS/FOOD which informs governments, institutions and the public on levels and trends of contaminants in food, their contribution to total human health exposure and the significance with regard to public health. Data includes per capita food consumption with diets based on the per capita data compiled by the Food and Agriculture Organization which provides statistics on a country’s annual food production, imports and exports found in the FAOSTAT database. This database also includes individual food consumption data based on national surveys and a database of the level of chemicals in raw food commodities as well as in food as consumed by final consumer.\n\nIn order to quantify the level of dietary exposure to a chemical residue, the level of the residue of interest within a particular food of interest or the whole food supply must be estimated. It is important to understand that each source of chemical residue data has a unique bias or limitation such as geographical limitation, limited number of commodities, analytical method limitations or sample size limitations.\n\nThere is an extensive range of the types of residues evaluated in dietary exposure assessment. These include naturally occurring chemicals, intentionally applied chemicals, and inadvertently applied chemicals. Some examples of chemicals include pesticides, herbicides, insecticides, fungicides or other chemicals used for agricultural purposes. These chemical residues may be found as a result of intentional application or from inadvertent application when wind or water carries them away from the source of application. Other residues may be antibiotics, heavy metals such as lead, or naturally occurring toxins such as mycotoxins, phycotoxins and phytotoxins.\n\nAlthough chemical residue data can be generated independently for specific research or assessment needs, primary sources for chemical residue data are various databases maintained by government agencies. These databases typically contain data for residues found on raw agricultural commodities which is collected during routine monitoring and enforcement.\n\nThe FDA Total Diet Study (also referred to as the market basket study) determines levels of various contaminants such as pesticide residues, industrial chemicals, toxic and nutrient elements and other nutrients in foods. The foods analyzed are purchased at the retail level and prepared as they would be consumed prior to analysis so that the analytical results are indicative of realistic estimates of dietary intake. The complete overview of FDA Total Diet Study and data from 1991 to present can be found be found on the FDA’s website which is linked above.\n\nA database containing pesticide residue data at the raw agricultural commodity level is the USDA Pesticide Data Program USDA Pesticide Data Program(PDP). The USDA initiated the PDP in 1991 to test commodities in the U.S food supply for pesticide residues. Since the passage of the Food Quality Protection Act (FQPA), one of the focuses of the PDP has been testing foods that are most likely consumed by infants and children. The monitoring program is run by the USDA Agricultural Marketing Service (AMS) which partners with state agencies to collect and analyze pesticide residue data. The EPA uses this data to enhance its programs for food safety and evaluate dietary exposure to pesticides.\n\nIn addition to dietary consumption data and chemical residue data, dietary exposure assessments often need to consider other real world factors that may affect the level of exposure or a corresponding adverse effect for a particular population. A couple of the most common examples of supplemental information are the evaluation of any sensitive populations that fall within the scope of the assessment and the evaluation of the point at which residue data is collected compared to how foods are consumed. These two factors are discussed in the following sections.\n\nExposure assessments must also take into consideration sensitive subgroups of the population such as infants, children, nursing women and the elderly who may be more susceptible to adverse health effects of exposure. A sensitive population could also be one in which the exposure level is much greater than the average for reasons such as geography, culture, or traditional diets. When characterizing exposure, it is necessary to evaluate it in the context of these more susceptible populations as well as the general public.\n\nResidue data is often collected on raw agricultural commodities at the time of harvest which is not necessarily representative or the residue levels present at the time of consumption. The processing and preparation of commodities is subject to increase or decrease the level of residues at the time of consumption. For example, dehydration of a product may increase the concentration of antibiotic residue whereas washing of produce may reduce the concentration of a pesticide residue. Since chemical residue data is often only available at the raw agricultural commodity level and consumption data is often available for foods as consumed, it is necessary to convert between foods as consumed and raw agricultural commodities. This can be completed with custom models or with national databases.\n\nThe USDA Food Intakes Converted to Retail Commodities Database converts foods consumed in national dietary surveys to retail-level commodities. The databases were developed by USDA Agricultural Research Service and Economic Research Service from six dietary surveys conducted between 1994 and 2008. Foods from these surveys are converted into 65 retail-level commodities grouped into eight major categories.\n\nDietary exposure assessments are often completed using models to simulate exposure. Once dietary consumption data, chemical residue data and any additional supplemental information has been compiled, one can utilize the data to model the dietary exposure of the population of interest to the chemical of interest. Most commonly, computer models are used to combine dietary intake and residue data. These models can be designed specifically for an intended application or there are standard models available that can be used for dietary exposure assessments. Two existing models are detailed in the following sections.\n\nDEPM is an EPA dietary exposure model. The DEPM database system contains numerous national, government-sponsored food intake surveys and chemical residue data from monitoring programs. In this model, consumption is expressed in terms 11 food groups containing 800 core foods which were selected to allow matching between food consumption and chemical residue data. These 800 core food items were established from mean values of consumption of over 6,700 foods commonly identified in food surveys. This model utilizes per capita food consumption rates for these 800 core foods for the population in the USA and also for roughly 24 subpopulations defined by different demographic factors such as gender, age, and ethnicity. A critical component of the DEPM model is the recipes which convert food as consumed to the raw agricultural commodities for which the chemical residue data is collected. The residue database contains average values of residues for more than 350 pesticides and environmental contaminants.\n\nThe model is run by selecting a food consumption database, a chemical residue database and a population. Based on these selections the model output is the average daily intake of a chemical for an individual with the average diet for the selected population. A diet can be specified based on selecting core foods then that diet can be matched with residue data and used to estimate exposure.\n\nDEEM is a computer based model developed by Novigen Sciences, Inc. that estimates dietary intake of chemical residues and provides output including dietary exposure estimates for different time ranges for populations or for individuals. This model converts food consumed to raw agricultural commodities using the USDA-EPA Food Commodity Intake Database recipes. DEEM utilizes Monte Carlo analysis to provide probabilistic assessments of dietary pesticide exposure. The EPA Office of Pesticide Programs uses DEEM for exposure and risk assessments.\nThe DEEM can be used for cumulative exposure analysis when multiple chemicals on multiple foods must be evaluated for a total exposure assessment. Also, DEEM software can be used with Calendex which is a cumulative aggregate exposure assessment software used to combine dietary and non-dietary or residential exposures.\n\n"}
{"id": "24494046", "url": "https://en.wikipedia.org/wiki?curid=24494046", "title": "Dirt", "text": "Dirt\n\nDirt is unclean matter, especially when in contact with a person's clothes, skin or possessions. In such case they are said to become dirty. Common types of dirt include:\n\nA season of artworks and exhibits on the theme of dirt was sponsored by the Wellcome Trust in 2011. The centrepiece was an exhibition at the Wellcome Collection showing pictures and histories of notable dirt such as the great dust heaps at Euston and King's Cross in the 19th century and the Fresh Kills landfill which was once the world's largest landfill.\n\nWhen things are dirty they are usually cleaned with solutions like hard surface cleaner and other chemicals solutions; much domestic activity is for this purpose — washing, sweeping and so forth.\n\nIn a commercial setting, a dirty appearance gives a bad impression. An example of such a place is a restaurant. The dirt in such cases may be classified as temporary, permanent, and deliberate. Temporary dirt is streaks and detritus that may be removed by ordinary daily cleaning. Permanent dirt is ingrained stains or physical damage to an object, which require major renovation to remove. Deliberate dirt is that which results from design decisions such as decor in dirty orange or grunge styling.\n\nAs cities developed, arrangements were made for the disposal of trash through the use of waste management services. In the United Kingdom, the Public Health Act 1875 required households to place their refuse into a container which could be moved so that it could be carted away. This was the first legal creation of the dustbin.\n\nModern society is now thought to be more hygienic. Lack of contact with microorganisms in dirt when growing up is hypothesised to be the cause of the epidemic of allergies such as asthma. The human immune system requires activation and exercise in order to function properly and exposure to dirt may achieve this. For example, the presence of staphylococcus bacteria on the surface of the skin regulates the inflammation which results from injury.\n\nEven when no visible dirt is present, contamination by microorganisms, especially pathogens, can still cause an object or location to be considered dirty. For example, computer keyboards are especially dirty as they contain on average 70 times more microbes than a lavatory seat.\n\nPeople and animals may eat dirt. This is thought to be caused by mineral deficiency and so the condition is commonly seen in pregnant women.\n\nPeople may become obsessed by dirt and engage in fantasies and compulsive behaviour about it, such as making and consuming mud pies and pastries. The source of such thinking may be genetic, as the emotion of disgust is common and the location for this activity in the brain has been proposed.\n\n\n\n"}
{"id": "675275", "url": "https://en.wikipedia.org/wiki?curid=675275", "title": "Distraction", "text": "Distraction\n\nDistraction is the process of diverting the attention of an individual or group from a desired area of focus and thereby blocking or diminishing the reception of desired information. Distraction is caused by: the lack of ability to pay attention; lack of interest in the object of attention; or the great intensity, novelty or attractiveness of something other than the object of attention. Distractions come from both external sources, and internal sources. External distractions include factors such as visual triggers, social interactions, music, text messages, and phone calls. There are also internal distractions such as hunger, fatigue, illness, worrying, and daydreaming. Both external and internal distractions contribute to the interference of focus.\n\nDistracted driving is a dangerous threat to road safety across the world. While drunk driving rates have been on the decline since 1983, distracted driving has been increasing in recent years. Many feel this incline is due to the widespread prevalence of cell phones. While distracted driving can be attributed to anything that diverts attention away from the road, it is often the cell phone that receives the blame for distracted driving incidents. Most of the recent Studies have shown that cell phone usage while driving has striking similarities to the effects of drinking while driving; Cell phones tend to take the driver's attention away from the road and onto itself. With drunk driving, drivers often experience the \"looking but not seeing\" phenomena. While their eyes do indeed view objects on the road, their brains do not comprehend the meaning behind the image. All levels of distraction while driving are dangerous, and potential drivers are cautioned to keep awareness of their surroundings.\n\nMany psychological studies show that switching between tasks, use of technology, and overstimulation has increased levels of distraction in the school setting. At school, distraction is often viewed as a source of poor performance and misbehavior. Distraction makes focusing on singular, assigned tasks more difficult. Digital components of learning are an emerging component to classroom distraction. Parents, teachers, students, and scholars all have opinions about how technology either benefits or harms a students’ focus in an academic setting. Research studies show that neuron circuits indicate a decrease in ability to be attentive to goal relative stimulus with the addition of distracting stimuli interference. School-aged students, with developing brains, are more apt to conflicting stimuli while trying to focus. Large classroom sizes, technology use in and outside the classroom, and less natural stimuli have been seen as contributing factors to deflating test scores and classroom participation.\n\nMultitasking could also be considered as distraction in situations requiring full attention on a single object (e.g., sports, academic tests, performance). The issue of distraction in the workplace is studied in interruption science. According to Gloria Mark, a leader in interruption science, the average knowledge worker switches tasks every three minutes, and, once distracted, a worker takes nearly a half-hour to resume the original task.\n\nIn works of fiction, distraction is often used as a source of comedy, whether the amusement comes from the gullibility of those distracted or the strangeness of whatever is utilized to create the distraction. Examples of comedic distraction, also called comic relief, can oftentimes be found in Shakespearean plays. In \"Hamlet\", Shakespeare includes a scene in which two gravediggers joke around about Ophelia's death. While her death is by no means meant to be funny, a small break from the sadness helped to appease the groundlings in Shakespeare's time, as well as allow the rest of the audience to take a break from the constant \"doom and gloom\" of his tragedies.\n\nRabbi Allen Lew in his book, \"This is Real and You are Completely Unprepared\", writes, \"The thoughts that carry our attention away [during prayer or meditation] are never insignificant thoughts and they never arise at random. We lose our focus precisely because these thoughts need our attention and we refuse to give it to them. This is why they keep sneaking up on our attention and stealing it away. This is how it is that we come to know ourselves as we settle deeply into the act of prayer [or meditation]\". According to philosopher Damon Young, distraction is chiefly an inability to identify, attend to or attain what is valuable, even when we are hard-working or content.\n\nDistraction was a key battle strategy in tales from the Trojan War. According to the legend, the Greeks seemed to have retreated by pretending to sail away. In their stead, they left a large wooden horse, which the Trojans then chose to bring back within their walls in order to celebrate their supposed victory. The Greeks used the Trojans' pride as a distraction, as they actually hid men within the Trojan Horse in order to let the rest of the army in during the cover of night. The Greeks then entered and destroyed the city of Troy, effectively ending the 10-year standoff that was the Trojan War.\n\nDistraction is useful in the management of pain and anxiety. Dentists, for example may intentionally hum an annoying tune or engage in small talk just to create a diversion from the dental surgery process. Topical ointments containing capsaicin, provide a superficial burning sensation that can momentarily distract a patient's attention away from the more serious pain of arthritis or muscle strain. A similar effect is made by oil of cloves, which produces a burning sensation on the gums, and distracts from toothache.\n\nDistraction is often used as a coping mechanism for short-term emotion regulation. When presented with an unpleasant reality, humans often choose to occupy their attention with some other reality in order to remain in a positive mental state. This is referred to as ‘procrastination’ when the unpleasant reality is in the form of work. The natural human inclination to distract oneself was put to the test when the Department of Psychology at Humboldt-Universität zu Berlin (Humboldt University of Berlin) held an experiment to study distraction. The goal of the experiment was to examine whether the effects of distraction on where subjects held their attention during repeated picture processing is changed by regular emotional functions. Furthermore, they hypothesized that while distraction assists in short-term emotional regulation, it is actually harmful in the long term. In order to do so, the experimenters had subjects view 15 unpleasant pictures (Set A) and “attend” to them (meaning the subjects were asked to pay full attention to the pictures). Next, the subjects were shown 15 unpleasant pictures (Set B) and were asked to distract themselves from the pictures (meaning they were to think about anything other than the picture on the screen; their example was to think about “the way to the supermarket”). Finally, the subjects were shown 15 neutral pictures (Set C) and were asked to attend to them. After 10 minutes of rest, the subjects entered the “re-exposure phase”, which repeated the experiment- this time requiring the subjects to pay attention to all of the sets, including Set B. This experiment was performed on 3 separate blocks of participants. To examine the state of the subjects’ brain, the subject was to wear “Ag/AgCl-electrodes from 61 head sites using an EasyCap electrode system with an equidistant electrode montage. Additional external electrodes were placed below the left (IO1) and right eye (IO2), below T1 (ground), on the nasion, and on the neck.” The subjects were also asked to rate the unpleasantness of the picture on the screen on a scale of 1-9. To test whether distraction in the first phase resulted in increased responsiveness during the re-exposure phase, experimenters “compared mean unpleasantness ratings between unpleasant pictures that were previously presented in the attend (previous attention) versus distract (previous distraction) condition using a paired t-test”. The end results of the experiment were as such:\nEssentially, when exposed to an unpleasant image, the subject feels initial discomfort. However, after being exposed to it once with their full attention, the subject feels much less discomfort the second time they are exposed. When the subject distracts themselves from the initial unpleasant image, the subject feels more discomfort the second time when they are required to attend to the image. The experimenters’ conclusion is thus: “the obtained results suggest that distraction inhibits elaborate processing of the stimulus' meaning and adapting to it.”\n\nCon artists and shoplifters sometimes create a distraction to facilitate their crimes. Armed robbers may create a distraction after their robbery, such as pulling a fire alarm, to create confusion and aid in their getaway. In a more serious case of crime, Norwegian Anders Behring Breivik exploded a car bomb in Oslo city. It was a reportedly distraction that directed police resources to Oslo city, allowing him to carry out his shooting spree unopposed on Utøya island.\n\nMagicians use distraction techniques to draw the audience's attention away from whichever hand is engaged in sleight of hand. Magicians can accomplish this by encouraging the audience to look elsewhere or by having an assistant do or say something to draw the audience's attention away.\nSleight of hand is often used in close-up magic, performed with the audience close to the magician, usually within three or four meters, possibly in physical contact. It often makes use of everyday items as props, such as cards and coins. The guiding principle of sleight-of-hand, articulated by legendary close-up magician Dai Vernon, is \"be natural\". A well-performed sleight looks like an ordinary, natural and completely innocent gesture, change in hand-position or body posture.\n\nIt is commonly believed that sleight of hand works because “the hand is quicker than the eye” but this is usually not the case. In addition to manual dexterity, sleight of hand depends on the use of psychology, timing, misdirection, and natural choreography in accomplishing a magical effect. Misdirection is perhaps the most important component of the art of sleight of hand. The magician choreographs his actions so that all spectators are likely to look where he or she wants them to. More importantly, they do not look where the performer does not wish them to look. Two types of misdirection are timing and movement. Timing is simple: by allowing a small amount of time to pass after an action, events are skewed in the viewer's mind. Movement is a little more complicated. A phrase often used is \"A larger action covers a smaller action\". Care must be taken however to not make the larger action so big that it becomes suspicious.\n\nPropagandizing techniques of distraction are used in media manipulation. The idea is to encourage the public to focus on a topic or idea that the compliance professional feels is supportive of their cause. By focusing attention, a particular ideology can be made to seem the only reasonable choice. Oftentimes, media competition is the driving force for media bias, manipulation, and distraction. If a media company can find an audience with a united ideology, it then develops a loyal consumer base, as its consumers will be happy with the way media is presented. A so-called \"conservative\" media outlet would not hire a \"liberal\" reporter, as they would run the risk of alienating its viewership. Dubious\n\nDistraction is also important in studies of media multitasking, or the simultaneous use of multiple media at once. This behavior has emerged as increasingly common, especially among younger media users. Studies show that while humans are predisposed to the desire to multitask, most people struggle to have legitimate productivity while multitasking. Instead of giving a task full attention, the split attention that multitasking necessitates can cause one task to be a distraction to another. On the other hand, some studies show that multitasking has the potential for a high-risk high-reward situation, leading to the idea that success can arise from multitasking if one is good at the activity.\n\n"}
{"id": "3625081", "url": "https://en.wikipedia.org/wiki?curid=3625081", "title": "Essential medicines", "text": "Essential medicines\n\nEssential medicines, as defined by the World Health Organization (WHO), are the medicines that \"satisfy the priority health care needs of the population\". These are the medications to which people should have access at all times in sufficient amounts. The prices should be at generally affordable levels.\n\nThe WHO has published a model list of essential medicines. Each country is encouraged to prepare their own lists taking into consideration local priorities. Over 150 countries have published an official essential medicines list. The essential medicines list enables health authorities, especially in developing countries, to optimize pharmaceutical resources. The WHO List contains a core list and a complementary list.\n\nThe core list presents a list of minimum medicine needs for a basic health care system, listing the most efficacious, safe and cost-effective medicines for priority conditions. Priority conditions are selected on the basis of current and estimated future public health relevance, and potential for safe and cost-effective treatment.\n\nThe complementary list presents essential medicines for priority diseases, for which specialized diagnostic or monitoring facilities are needed. In case of doubt medicines may also be listed as complementary on the basis of higher costs or less attractive cost-effectiveness in a variety of settings. The list is important because it forms the basis of national drugs policy in more than 155 countries, both in the developed and developing world. Many governments refer to WHO recommendations when making decisions on health spending.\n\nThe definition of essential medicines has changed over time.\n\nThe original 1977 WHO definition was that they were medicines \"of utmost importance, basic, indispensable, and necessary for the healthcare needs of the population\". The concept was mentioned in one of the ten points of the 1978 Alma Ata Declaration on primary health care.\n\nIn 2002 definition was changed to:\n\nEssential medicines are those that satisfy the priority health care needs of the population.\n\nAnd this remains the current definition as of 2017.\n\nItems are chosen as essential medicines based on how common the disease is that they treat, evidence of benefit and the degree of side effects, and the cost compared to other options.\n\nCost effectiveness is the subject of debate between producers (pharmaceutical companies) and purchasers of drugs (national health services). It is estimated that access to essential medicines could save 10 million people a year.\n\nThe WHO Model List of Essential Medicines has been updated every two years since 1977. The current version, the 20th, was published in March 2017.\n\nThe first edition of the \"WHO Model List of Essential Medicines for Children\", was published in 2007 while the 5th edition was published in 2015. It was created to make sure that the needs of children were systematically considered such as availability of proper formulations. The first edition contained 450 formulations of 200 different medications.\n\nThe number of medications has nearly doubled, from the original 208 in 1977 to more than 340. The range has increased over the years and now includes an antimigraine drug, antidotes, and antineoplastic drugs. The third list for children from 2011 contains 269 medications.\n\nAccess to essential medicines are part of the Sustainable Development Goals, specifically goal 3.8.\n\nA number of organizations, which are global in scope, use the list to determine which medications they will supply.\n\n"}
{"id": "57565979", "url": "https://en.wikipedia.org/wiki?curid=57565979", "title": "Flour extraction", "text": "Flour extraction\n\nFlour extraction is the common process of refining Whole Grain Flour first milled from grain or grist by running it through sifting devices.\n\nFor centuries, much of the flour milled for human consumption has been run through some kind of “bolting”, sifting or “extraction” process. This flour is extracted from whole grains for one of two reasons; firstly, to decrease the tendency for rancidity. The milling systems with a lower extraction percentage discard most of the rancidity prone nutritional minerals and oils associated with the bran and germ elements of the wheat kernel. Baking functionality is the other issue, with increased loaf volume accomplished by simply removing just the larger flour particles. Like the lower extraction white flour, higher extraction flour still creates a smoother dough more inclined to hold the gas created during fermentation. However, higher extraction flour also retains the sensory flavors and nutrition associated with the smaller bran and germ elements that are also extracted along with the endosperm.\n\n“White flour”, extracted from whole grains by Roller mills that eliminates the rancidity prone bran and germ elements of the wheat kernel was introduced in the late 19th century. By first hydrating the outer wheat kernel bran and germ elements to keep them intact, this new system then employed steel rollers instead of circulating stones to repetitively fracture the remaining starchy endosperm element into fine particles. The extracted endosperm flour came to be known as “white flour” as this element of the wheat kernel is white. This system ingeniously accomplished the extraction of most of the starchy endosperm while separating out virtually all of the bran and germ elements, extracting about 72% of the whole grain kernel. Roller milling eventually came (and continues) to dominate the world’s flour production. Well over 90% of U.S. flour production in 2017 was roller milled white enriched flour. \n\nOnce roller milling made white flour affordable for almost everyone, public health issues arose. As scientists learned more about the crucial health contributions of the bran and the germ, artificial enrichment of white flour was introduced that restores a small part of the nutrition lost by eliminating all the bran and germ elements.\n\nThere are generally two benefits from extraction:\n\nThe general availability of refrigeration and even flour itself, has diminished the significance of the rancidity/shelf life/keeping quality issue of whole grain flour. Higher extraction rates just focus on the elimination of the larger flour particles to increase loaf volume while retaining the majority of the nutritional bran and germ elements along with the endosperm. This is accomplished by direct extraction of the fine whole grain flour output of impact or attrition mills. Millers have been able to match the finer particle size distribution of low extraction roller milled white flour (72% extraction) that eliminated all the bran and germ elements with a higher +88% extraction that retains most of them.\n"}
{"id": "935302", "url": "https://en.wikipedia.org/wiki?curid=935302", "title": "Fusarium", "text": "Fusarium\n\nFusarium is a large genus of filamentous fungi, part of a group often referred to as hyphomycetes, widely distributed in soil and associated with plants. Most species are harmless saprobes, and are relatively abundant members of the soil microbial community. Some species produce mycotoxins in cereal crops that can affect human and animal health if they enter the food chain. The main toxins produced by these \"Fusarium\" species are fumonisins and trichothecenes. Despite most species apparently being harmless, some \"Fusarium\" species and subspecific groups are among the most important fungal pathogens of plants and animals. \n\nThe name of \"Fusarium\" comes from Latin \"fusus\", meaning a spindle.\n\nThe taxonomy of the genus is complex. A number of different schemes have been used, and up to 1,000 species have been identified at times, with approaches varying between wide and narrow concepts of speciation ('lumpers' and 'splitters').\n\nPhylogenetic studies indicate seven major clades within the genus.\n\nVarious schemes have subdivided the genus into subgenera and sections. There is a poor correlation between sections and phylogenetic clades.\n\nSections previously described include;\n\nSelected species include;\n\nThe name of \"Fusarium\" comes from Latin \"fusus\", meaning a spindle.\n\nThe genus includes a number of economically important plant pathogenic species.\n\n\"Fusarium graminearum\" commonly infects barley if there is rain late in the season. It is of economic impact to the malting and brewing industries, as well as feed barley. \"Fusarium\" contamination in barley can result in head blight, and in extreme contaminations, the barley can appear pink. The genome of this wheat and maize pathogen has been sequenced. \"F. graminearum\" can also cause root rot and seedling blight. The total losses in the US of barley and wheat crops between 1991 and 1996 have been estimated at $3 billion.\n\n\"Fusarium oxysporum f.sp. cubense\" is a fungal plant pathogen that causes Panama disease of banana (\"Musa\" spp.), also known as fusarium wilt of banana. Panama disease affects a wide range of banana cultivars, which are propagated asexually from offshoots and therefore have very little genetic diversity. Panama disease is one of the most destructive plant diseases of modern times, and caused the commercial disappearance of the once dominant Gros Michel cultivar. A more recent strain also affects the Cavendish cultivars used as a substitute for Gros Michel. It is considered inevitable that this susceptibility will spread globally and commercially wipe out the Cavendish cultivar, for which there are currently no acceptable replacements.\n\n\"Fusarium oxysporum\" f. sp. \"narcissi\" causes rotting of the bulbs (basal rot) and yellowing of the leaves of daffodils (Narcissi).\n\nSome species may cause a range of opportunistic infections in humans. In humans with normal immune systems, fusarial infections may occur in the nails (onychomycosis) and in the cornea (keratomycosis or mycotic keratitis). In humans whose immune systems are weakened in a particular way, (neutropenia, i.e., very low neutrophils count), aggressive fusarial infections penetrating the entire body and bloodstream (disseminated infections) may be caused by members of the \"Fusarium solani\" complex, \"Fusarium oxysporum\", \"Fusarium verticillioides\", \"Fusarium proliferatum\" and, rarely, other fusarial species.\n\n\"Fusarium venenatum\" is produced industrially for use as a human food by Marlow Foods, Ltd., and is marketed under the name Quorn in Europe and North America.\n\nSome consumers of fusarium products have shown food allergies similar in nature to peanut and other food allergies. People with known sensitivities to molds should exercise caution when consuming such products.\n\nMass casualties occurred in the Soviet Union in the 1930s and 1940s when \"Fusarium\"-contaminated wheat flour was baked into bread, causing alimentary toxic aleukia with a 60% mortality rate. Symptoms began with abdominal pain, diarrhea, vomiting, and prostration, and within days, fever, chills, myalgias and bone marrow depression with granulocytopenia and secondary sepsis occurred. Further symptoms included pharyngeal or laryngeal ulceration and diffuse bleeding into the skin (petechiae and ecchymoses), melena, bloody diarrhea, hematuria, hematemesis, epistaxis, vaginal bleeding, pancytopenia and gastrointestinal ulceration. \"Fusarium sporotrichoides\" contamination was found in affected grain in 1932, spurring research for medical purposes and for use in biological warfare. The active ingredient was found to be trichothecene T-2 mycotoxin, and it was produced in quantity and weaponized prior to the passage of the Biological Weapons Convention in 1972. The Soviets were accused of using the agent, dubbed \"yellow rain\", to cause 6,300 deaths in Laos, Kampuchea, and Afghanistan between 1975 and 1981. The \"biological warfare agent\" was later purported to be merely bee feces, but the issue remains disputed.\n\nFollowing an outbreak of \"Fusarium oxysporum\" that affected coca plantations in Peru, and other crops planted in the area, the United States has proposed the use of the agent as a mycoherbicide in drug eradication. In 2000, a proposal was passed to use the agent as part of Plan Colombia. In response to concerns use of the fungus could be perceived as biological warfare, the Clinton Administration \"waived\" this use of \"Fusarium\". A subsequent law passed in 2006 has mandated the testing of mycoherbicide agents - either \"Fusarium oxysporum\" or \"Crivellia papaveracea\" - in field trials in U.S. territory. Use of \"Fusarium oxysporum\" for these tests has raised concerns because resistant coca from the previous outbreak has been widely cultivated, and the fungus has been implicated in the birth of 31 anencephalic children in the Rio Grande region of Texas in 1991, the loss of palm trees in Los Angeles, and eye infections from contact lens solutions. The alternative \"Crivellia papaveracea\" is less well known; despite decades of study in the Soviet biowarfare lab in Tashkent, Uzbekistan, the relevant mycotoxins reportedly have not yet been isolated, named, or studied.\n\nFusarium has posed a threat to the ancient cave paintings in Lascaux since 1955, when the caves were first opened to visitors. The caves subsequently closed and the threat subsided, but the installation of an air conditioning system in 2000 caused another outbreak of the fungus which is yet to be resolved.\n\n\n"}
{"id": "2016889", "url": "https://en.wikipedia.org/wiki?curid=2016889", "title": "Genital Autonomy America", "text": "Genital Autonomy America\n\nGenital Autonomy America, formerly NOCIRC (the National Organization of Circumcision Information Resource Centers), is an educational nonprofit organization headquartered in California and with centers in other US states and countries. In 2016 the organization changed its name from NOCIRC to Genital Autonomy America.\nIts goal is to secure \"the birthright of male, female, and intersex children and babies to keep their sex organs intact.\"\n\nThe organization was founded in 1985 by registered nurse Marilyn Milos, who remains the director today. The organization was the first national clearinghouse in the US for information about circumcision. In its first decade, NOCIRC grew into an international network and now has over 110 centers worldwide, including at least 17 outside of the US. It is a member of the International Coalition for Genital Integrity (ICGI).\n\nNOCIRC opposes the non-therapeutic circumcision of children. The organization suggests that circumcision be performed only when it is absolutely medically necessary and after alternative non-invasive therapies have failed, or when an adult gives consent, after he has been fully informed about the risks and harm of the procedure. NOCIRC also provides material on their view of circumcision and care of the foreskin. NOCIRC states that their position is based on the understanding that there is not a national or international health association in the world that recommends routine infant circumcision, yet declare that they hold the medical community accountable for \"misconstruing the scientific database available on human circumcision in the world today\". In a 1996 press release, NOCIRC stated that \"We expect this professional organization [the American Academy of Pediatrics] that is concerned with the health of infants and children to reveal the harm of genital cutting and to protect every child's birthright to keep his or her sex organs intact.\" The American Academy of Pediatrics said in 1999 that the perceived benefits of circumcision are not sufficient to recommend routine neonatal circumcision, but that it should be the decision of informed parents.\n\nNOCIRC publishes a number of brochures for parents, providing their recommendations about infant circumcisions, penile care and several related topics. NOCIRC also distributes an annual newsletter to its members. Most of the organization's material is directly available on their website.\n\nSince 1989 NOCIRC has sponsored and organized international symposia on circumcision, human rights, and genital integrity. The twelfth international symposium took place in September and October 2012 in Finland. The following list details the history of NOCIRC's symposia:\n\n\n"}
{"id": "14752443", "url": "https://en.wikipedia.org/wiki?curid=14752443", "title": "Glisodin", "text": "Glisodin\n\nGlisodin is the registered trademark of a nutritional supplement based on two constituents:\n\nAs oxygen metabolizes in the body, potentially harmful reactive oxygen species (ROS) are created. The human body implements an antioxidant defense system to protect against ROS. In the event that these defenses are overpowered by the ROS, cell damage results (which is a major cause of aging in the body).\n\nSuperoxide dismutase (SOD) helps to slow the creation of ROS, ultimately playing a key role in the defense against cell damage. However, due to a very fragile molecular structure, it is particularly prone to damage from stomach acids and digestive enzymes when taken orally. Gliadin, which is well known as a carrier protein for controlled drug release, helps to protect SOD.\n\nA wide range of clinical research has been undertaken to study glisodin's antioxidant capacities. This has extended into a wide range of applications, including protection from ultraviolet radiation, athletic performance, cardiovascular health, ischemia and reperfusion injury.\n\nA group of researchers in France and Germany led by Dr. Claus Muth concluded that glisodin is helpful in protecting against DNA damage caused by hyperbaric oxidation. A 2005 study at Rutgers University also concluded that glisodin is helpful in enhancing athletic performance while minimizing fatigue.\n\n\n"}
{"id": "7932917", "url": "https://en.wikipedia.org/wiki?curid=7932917", "title": "Goose Guandong virus", "text": "Goose Guandong virus\n\nThe Goose Guandong virus refers to the strain A/Goose/Guangdong/1/96 (Gs/Gd)-like H5N1 HPAI viruses. It is a strain of the Influenzavirus A subtype H5N1 virus that was first detected in a goose in Guangdong in 1996. It is an HPAI (High Pathogenic Avian Influenza) virus, meaning that it can kill a very high percentage of chickens in a flock in mere days. It is believed to be the immediate precursor of the current dominant strain of HPAI A(H5N1) that evolved from 1999 to 2002 creating the Z genotype (also called \"Asian lineage HPAI A(H5N1)\") that is spreading globally and is epizootic (an epidemic in nonhumans) and panzootic (affecting animals of many species, especially over a wide area), killing tens of millions of birds and spurring the culling of hundreds of millions of others to stem its spread.\n\nThe conversion to the Z genotype probably occurred by reassortment with a teal (duck) virus H6N1 during a mixed influenza infection:\n\n\n"}
{"id": "19027904", "url": "https://en.wikipedia.org/wiki?curid=19027904", "title": "HIV/AIDS in Rwanda", "text": "HIV/AIDS in Rwanda\n\nRwanda faces a generalized epidemic, with an HIV prevalence rate of 3.1 percent among adults ages 15 to 49. The prevalence rate has remained relatively stable, with an overall decline since the late 1990s, partly due to improved HIV surveillance methodology. In general, HIV prevalence is higher in urban areas than in rural areas, and women are at higher risk of HIV infection than men. Young women ages 15 to 24 are twice as likely to be infected with HIV as young men in the same age group. Populations at higher risk of HIV infection include people in prostitution and men attending clinics for sexually transmitted infections.\n\nRwanda is among the world's least developed countries, ranking 166 of 187 in the United Nations Development Program's 2011 Human Development Index. Some 60 percent of the population lives in poverty. During the three months of genocide in 1994, mass rape, sexual torture and psychological trauma were common. Massive population flows following the Rwandan genocide of 1994 have resulted in an increase in the urban population. The shortage of human resources throughout the health sector is a significant constraint. Of Rwandans killed or displaced during the genocide, a disproportionate number were highly skilled and educated members of society, including doctors, nurses and other health workers. Many health centers lack essential physical facilities, equipment and supplies. Electricity supply is erratic throughout Rwanda, affecting hospitals, health centers and laboratories. Blood safety, data management and drug storage are all impacted by the erratic electricity supply. While stigma continues to be a problem for people living with HIV/AIDS, the situation is slowly improving due to good information sharing at all levels about HIV/AIDS. The President Paul Kagame has made many efforts in improving the situation, through different awareness raising initiatives.\n\nAcquired immune deficiency syndrome (AIDS) is a disease comprising associated conditions caused by a human immunodeficiency virus (HIV) infection. Despite myriad research studies, unresolved questions about origins and epidemic emergence of HIV/AIDS remain. At the beginning of the global AIDS epidemic in the early 1980s, HIV/AIDS was considered a disease exclusive to homosexual men and intravenous drug users, but in Africa, new HIV/AIDS cases were observed across numerous subpopulations. Proposed reasons for the emergence of HIV in Africa in the 20th century include, but are not limited to, rapid population growth, change in population structure, and clinical interventions that provided the opportunity for rapid human-to-human transmission.\n\nThe prevalence of HIV/AIDS is a major public health concern in Rwanda as HIV/AIDS-related mortality has substantial negative social and economic consequences for residents and the government. The first case of HIV infection in Rwanda was reported in 1983. The estimated incidence rate for HIV in Rwanda is 0.11%; this is a stable rate.\n\nAccording to the 2014–2015 Rwanda Demographic and Health Survey (RDHS), \"In Rwanda, much of the information on national HIV prevalence is derived from the antenatal care (ANC) sentinel surveillance system. Although surveillance data do not provide estimates of HIV prevalence for the general population, they do provide results specific to women attending antenatal clinics. The inclusion of HIV testing in the 2005, 2010, and 2014–15 RDHS surveys offer[ed] the opportunity to better understand the magnitude and patterns of infection in the general population of reproductive age, including men age 15–59 who are not tested as part of antenatal sentinel surveillance. The 2014–15 RDHS is the third RDHS survey to anonymously link HIV testing results with key behavioral and sociodemographic characteristics of both male and female respondents, the first being the 2005 RDHS. These surveys provide national, population-based trend data on HIV prevalence among women age 15–49 and men age 15–59. In addition, for the first time, the 2014–15 RDHS included HIV testing of children age 0–14.\"\n\nIn Rwanda, HIV prevalence has been stable since 2005 and remains at 3% among adults age 15–49 (4% among women and 2% among men). The prevalence of HIV is higher in urban areas (6%) than rural areas (2%); HIV prevalence is 6% in the capital city of Kigali and 2–3% in each of the other provinces.\n\nThe HIV prevalence increases with age. Less than 1% of children (ages 0–14) are living with HIV. Across age groups, the highest HIV prevalence is observed among women age 40–44 (8%) and men age 45–49 (9%).\n\nRegardless of sex, HIV prevalence is closely related to marital status. Fifteen percent of widows and 8% of those divorced or separated reported being HIV positive, as compared with only 3% of those who were married at the time of the survey.\n\nBy wealth, HIV prevalence is highest among both young women and young men in the highest wealth quintile. However, the relationship between HIV prevalence and household wealth quintile is not linear.\n\nAmong youth in Rwanda, HIV prevalence by educational attainment. Five percent of young women with no education are HIV-positive whereas 2% of young women with a primary education and 1% with a secondary education or higher are HIV-positive. Among young men, HIV prevalence is higher among those with any education than among those with none.\n\nRwanda's HIV/AIDS surveillance efforts began in 1984 with the establishment of a national AIDS case reporting system in hospitals and health centers. The country's early response to its HIV/AIDS epidemic was relatively rapid and sustained. In 1985, the Rwandan Ministry of Health and the Red Cross established one of the first and most effective blood donor screening programs in Africa. In 1986, Rwanda was the first country in the world to conduct and report on a nationally representative HIV/AIDS seroprevalence survey. In 1987, the National AIDS Program was established in collaboration with the World Health Organization (WHO).\n\nRwanda's civil war began in 1990. Between April and July 1994, genocide claimed the lives of an estimated 800,000 Rwandese, displaced nearly four million people, and had a devastating impact on national health infrastructure. In addition to severe limitations being placed on the ability of the Rwandan government to prevent and treat HIV/AIDS during the genocide, the International Criminal Tribunal for Rwanda noted the use of 'genocidal rape' as a weapon of war during this time, with between 250,000 and 500,000 women and girls being subjected to rape. Deliberate infection with HIV was observed as a pattern of warfare. The exact effect of these actions on HIV/AIDS prevalence is, however, not known.\n\nAfter the civil war and the genocide, Rwanda's health system had been devastated, with life expectancy at just 30 years and four out of five children dying before their first birthday. Between 1990 and 2002, the country recorded steadily increasing numbers of new AIDS cases with between 1,000 and 4,000 new case reports per year. This was followed more recently by dramatic increases in cases in 2003 and 2004 (over 6,000 and 12,000 cases reported, respectively). Overall, HIV prevalence is thought to have stabilized since its peak in the early 1990s, partly also due to the effects of the genocide.\n\nRwanda adopted a community-based healthcare model (called Mutuelles de Santé, the insurance requires community members to pay a premium based on their income and a 10% upfront charge for each visit) to counter shortages in skilled professionals and prioritized national ownership of healthcare as a central aspect of its health reconstruction efforts after the war.\n\n\"Vision 2020\", the national development plan which aims to make Rwanda a lower-middle income-country by 2020, includes health as a central aspect of development and guides the national allocation of resources. Rwanda favors a multi-sectoral approach to health care, and, while funding for the health system is heavily dependent on donor aid, State ownership and control over policy is strong.\n\nThere has been growing availability of HIV testing, care, and treatment services in Rwanda since 2000. Anti-retroviral therapy (ART) was first introduced in 1999. The 2005–2009 Health Sector Strategic Plan names as one of its goals the curbing and reversal of the spread of HIV infection by 2015. It also includes as goals increasing demand for HIV prophylaxis and treatment through the development of public education campaigns and 'gender-specific' implementation. In 2006, it was reported that voluntary testing and counseling was available at 226 sites. Large-scale Prevention of Mother to Child Transmission (PMTCT) and health promotion initiatives were also reported in at least 208 sites.\n\nIn 2009, Rwanda published its first National Strategic Plan on HIV and AIDS, outlining the overarching goals for the country's multi-sector response. \"It is based on the most up-to-date understanding of the epidemic and the strengths and weaknesses of the systems and mechanisms that are used to respond.\" The Plan calls for \"universal access to HIV and AIDS services\". In addition, it counts the reduction of infections, reduced morbidity and mortality and equal opportunities for persons living with HIV/AIDS as goals. The plan also targeted behavior change and risk reduction as important outcomes. In 2012, it was estimated that 80% of people in need of ART were able to access it through the community-based healthcare system. The National Strategic Plan (2009–2012) was succeeded by the Second National Strategic Plan on HIV and AIDS (2013–2018).\n\nHIV/AIDS is prioritized in several policy instruments, including Vision 2020 and the Economic Development and Poverty Reduction Strategy 2013–2018 (EDPRS II), which includes a framework for multi-sectoral responses to HIV/AIDS. Each sector within EDPRS II has specific HIV mainstreaming strategies and targets, including education, health, labor, military, transport, gender, young people, agriculture, finance and social welfare. The third Health Sector Strategic Plan (HSSP-III) is a framework document for the development and shaping of health policy in Rwanda. Among its goals are the reduction of HIV infections, reduction of HIV-related morbidity and mortality, strengthened management of HIV/AIDS and equal opportunities for people living with HIV.\n\nThe National Strategic Plan is a reference document for all sectors, institutions and partners involved in the fight against HIV and AIDS. The National Strategic Plan goals include:\n\nThe National Strategic Plan addresses issues of key populations and vulnerable groups. These include men who have sex with men, sex workers, mobile populations, persons in uniform, young people, women and girls and people with disabilities. Key settings such as prisons, schools and workplaces are also taken into account. Cross cutting issues related to human rights protection, stigma and discrimination, gender inequality, poverty and involvement of people living with HIV also feature in the National Strategic Plan. The National Strategic Plan outlines strategies such as creating public awareness of stigma and discrimination and addressing the legal barriers that prevent key populations from accessing and utilizing services. Due to the lack of a mid-term progress report, it is unclear if this objective has been met.\n\nPrevention of sexual transmission of HIV and sexually transmitted infections, prevention of mother to child transmission of HIV, counselling and testing and prevention of HIV in health care settings are stated priorities of the National Strategic Plan. In addition, the key drivers of HIV in Rwanda have been identified through the Mode of Transmission model. The strategies for prevention have since been revised and updated to be more consistent with new developments and technology. For example, male circumcision using Prepex is currently being rolled out in the Rwanda. According to the National Strategic Plan, priority interventions relating to HIV treatment include increasing access and enrolment on ART, providing treatment for TB/HIV co-infection and community and home-based palliative care. There has been a lot of progress in the area of treatment including more people being able to access ART, the adoption of the test and treat strategy for discordant couples, sex workers, and the adoption of the new WHO treatment guidelines and the availability of treatment for prisoners. However, challenges still remain with regard to pre-ART care, treatment for opportunistic infections and improving adherence.\n\nWith regard to orphans and vulnerable children (OVC), the development of OVC standards of care has recently taken place. The National Strategic Plan focuses on strategic needs of OVC such as protecting their human rights and ensuring access to adequate food, shelter, education and health services, and protection from abuse. Major challenges include the continuous increase in OVC, poor data collection and lack of a national OVC database.\n\nThe National Strategic Plan includes indicators and targets, making it possible to track progress and follow up on commitments made. It will be evaluated both at midterm and at the end of the cycle. Thus far, there have not been any mid-term reports published.\n\nThe Constitution of Rwanda and the regulation regarding labor in Rwanda (N° 13/2009 of 27/05/2009) prohibits discrimination within certain contexts. These are general laws with no specific reference to HIV and AIDS. With regard to the laws to reduce violence against women, the Law on Prevention and Punishment of Gender-Based Violence was enacted in 2008. It outlaws gender-based violence which is defined broadly to include physical, sexual, economic and psychological violence. Read with the Penal Code, the Act criminalizes willful HIV transmission. This is due to the fact that the Act defines sexual abuse to include \"the engagement of another person in sexual contact, whether married or not, which includes sexual conduct that abuses, humiliates or degrades the other person or otherwise violates another person's sexual integrity, or sexual contact by a person aware of being infected with HIV or any other sexually transmitted infection with another person without that other person being given prior information of the infection.\"\n\nRwanda uses a number of HIV/AIDs prevention strategies. These include social and educational programs, condom distribution, volunteer medical male circumcision, and prevention of mother to child transmission.\n\nHIV testing and counseling (HTC) services are provided free of charge in all public health facilities and accredited private clinics in Rwanda. Outreach HCT campaigns are regularly carried out to deliver services to areas with less access to the health system. These campaigns are conducted in partnership with community-based organizations, the private sector, non-governmental organizations (NGOs) and faith-based organizations.\n\nIn 2013, Rwanda introduced testing using \"finger prick\" blood collection in all health facilities. The number of health facilities offering voluntary counseling and testing has increased from 15 in 2001 to 493 in 2013.\n\nSpecial attention is given to the prevention of HIV among vulnerable groups. Of these vulnerable groups, female sex workers were identified as key in preventing the further spread of HIV. In 2010, the prevalence of HIV among sex workers was 51%. To address female sex workers, national guidelines for HIV prevention in this vulnerable population were developed and disseminated as a part of the HIV National Strategic Plan 2013–2018. ROADS II, a USAID project, has been key in facilitating trainings, mentorships and peer groups to improve knowledge of HIV and prevention strategies, and condom distribution through peer education.\n\nRwanda has made gains in the distribution of condoms through social marketing and rapid sales outlets. The private sector has also been active in the uptake of condom use. From 2009 to 2013, 5 million more condoms have been distributed.\n\nIn 2014, the prevalence of male circumcision was 30% between the ages of 15-49. Voluntary medical male circumcision was added to the 2013–2018 National Strategic Plan of HIV. Surgical kit for voluntary medical male circumcision were provided to all facilities and two healthcare workers were trained per facility. These services are now regularly provided.\n\nPMTCT services have been scaled up throughout the country with 97% of all facilities provide PMCTC services by 2013. The national elimination of mother to child transmission strategy of 2011–2015 integrated PMTCT services into the regular healthcare system. Community health workers are also active in seeking out women who have missed follow-up appointments.\n\nServices for PMTCT provided during antenatal care visits include pre and post-test HIV counseling, blood draws for CD4, an appointment for CD4 results, partner testing, hemoglobin testing, WHO HIV clinical classification and enrolment in care, the initiation of ART, counseling on infant feeding and counseling on family planning.\n\nOption B+, an alternative PMTCT method, starts pregnant women on ARTs and continues treatment through pregnancy and life regardless of CD4 count. This treatment has been initiated in Rwanda and is being scaled to every health facility.\n\nRwanda has been updated its management and protocols in accordance with 2013 WHO recommendations. New efforts include treatment as prevention for female sex workers and men who have sex with men and test and treat protocols for tuberculosis-HIV co-infection, hepatitis B virus-HIV co-infection, and hepatitis C virus-HIV co-infection.\n\nOver the course of the last ten years, treatment of HIV patients with ARTs has been considerably scaled up. In 2002, only four facilities delivered ARTs compared to 465 in 2013. Now 91% of cases receive care and treatment.\n\nContinued, rapid decline in donor funding from external resources may affect targets set by the government \"to reduce by 75% new HIV infections, reduce by 50% AIDS related deaths, and reach zero stigma and discriminations for people infected and affected by HIV, the government of Rwanda is putting in all efforts to takeover, however the reduction trend from external resources\".[xxxvi]\n\nWidespread stigma and discrimination toward those living with HIV and AIDS can adversely impact willingness to be tested for HIV and compliance with ART.\n\nKey populations – female workers, youth, and men with higher educational attainment – play an important role in the dynamic of HIV in Rwanda.\n\nHealth expenditures on HIV are tracked through the Health Resource Tracking tool. All health sector actors, including Government institutions and Development Partners, are required to report annually their HIV expenditures for the previous fiscal year, as well as their budgets for the current fiscal year. Rwanda's Global AIDS Response Progress Report 2014 provides recent national HIV financing information for fiscal year 2011–2012 and fiscal year 2012–2013.\n\nRwanda's key HIV development and funding partners include the Global Fund to Fight AIDS, Tuberculous and Malaria and PEPFAR. Additional development partners (including international foundations and NGOs, bilateral agencies, and the United Nations (UN)) provide financial and technical support and aid in Rwanda's process of HIV policy and program development.\n\nDevelopment partners work with the Rwandan Government to set and achieve targets outlined in the National Strategic Plan 2013–2018. Development partners conduct joint planning and coordination with the Government and submit annual reports and budgets to ensure the Government can monitor and maximize development partners' resources. Rwanda has a common monitoring and evaluation system managed by Rwanda Biomedical Center-HIV Division where development partners can utilize reporting tools. National and international partners are encouraged to work together to maximize time spent supporting beneficiaries and minimize the reporting burden.\n\nPublic and external funding sources for HIV/AIDS in Rwanda in fiscal year 2011–2012 totaled USD 234.6 million. Of the total funding, USD 17.7 million (7.6% of total HIV/AIDS spending) came from public funding, and USD 216.8 million (92.4% of total HIV/AIDS spending) came from external funding. This total funding excludes out-of-pocket and private sector contribution.\n\nAmong external HIV/AIDS funding sources in Rwanda, the Global Fund made up 48.8% of HIV/AIDS funding, followed by the U.S. government (39.3%), international NGOs (2.8%), and UN agencies (1.8%). Additional bilateral organizations played a smaller role, with the Government of Luxembourg and Swiss Development Cooperation combined contributing 0.2% of total HIV/AIDS funding. The only other multilateral agency contributing to the total funding included the World Bank, representing 0.01% of total HIV/AIDS funding.\n\nPublic and external funding sources for HIV/AIDS in Rwanda in fiscal year 2012–2013 totaled USD 243.6 million, a four percent increase from the previous fiscal year spending. Of the total funding, USD 20.0 million (8.2% of total HIV/AIDS spending) came from public funding, and USD 223.6 million (91.8% of total HIV/AIDS spending) came from external funding. This total funding excludes out-of-pocket and private sector contribution.\n\nAmong external HIV/AIDS funding sources in Rwanda, Global Fund for AIDS, Tuberculous and Malaria made up 54.7% of HIV/AIDS funding, followed by the U.S. government (34.6%), international NGOs (1.0%), and UN Agencies (1.0%). Additional bilateral organizations contributed less than 0.5% of total HIV/AIDS funding.\n\n"}
{"id": "46497724", "url": "https://en.wikipedia.org/wiki?curid=46497724", "title": "Healing Simurgh", "text": "Healing Simurgh\n\nThe Healing Simurgh (Simorgh, Seemorgh) (Persian: سیمرغ درمانگر) is the symbol of medicine in Iran. The name is inspired by Simurgh the Persian mythical flying creature, to which healing capacities have been attributed in Ferdowsi's Shahnameh.\n\nThe concept and the logo design of the Healing Simurgh was approved by the supreme board of the Islamic Republic of Iran's Medical Council in November 2013 as the emblem of the Physicians Guild of Iran, replacing the Caduceus.\n\nIn summer of 2013, the emblem was introduced by Mahmoud Fazel, MD, the vice president of Iran's Medical Council. It was later developed by a group of renowned scholars including Jalal al-Din Kazzazi, Persian literary scholar, Mehdi Mohaghegh, author and director of the Society for the Appreciation of Cultural works and Dignitaries, Alireza Zali, President of Iran's Medical Council, Professor Nasrin Moazzemi, Biologist and member of UNESCO's Scientific Board of the International Basic Sciences Programme, and Ali Javadi Pouya, sustainable development activist and the CEO of Iran's Sustainable Development Strategy Group. The board formed a permanent association to continue with the subsequent activities required for the overhaul of Iran's national emblem for medicine, and for safeguarding this intangible cultural heritage.\n\nThe previous sign (bowl and snakes) was considered to be primarily focused on physical health. However, in the works of the prominent Iranian scholar and one of the precursors of medicine in the world, Avicenna, there is a more comprehensive understanding of health. He claims that health (\"sihha)\" is the \"disposition of human body with respect to its temperament and structure such that all of [its] actions [or functions] proceed from it in a sound and unimpaired way.\" Thus the board argued that the Healing Simurgh can better represent the Iranian historical concept of health.\n\nThe official logo of the Healing Simurgh was designed by Touraj Saberivand, and Iranian graphic designer, and was also published as a posting stamp by the Islamic Republic of Iran Post Company.\n"}
{"id": "15985944", "url": "https://en.wikipedia.org/wiki?curid=15985944", "title": "Health in Japan", "text": "Health in Japan\n\nThe level of health in Japan is due to a number of factors including cultural habits, isolation, and a universal health care system. John Creighton Campbell, a professor at the University of Michigan and Tokyo University, told the New York Times in 2009 that Japanese people are the healthiest group on the planet. Japanese visit a doctor nearly 14 times a year, more than four times as often as Americans. Life expectancy in 2013 was 83.3 years - among the highest on the planet. \n\nA new measure of expected human capital calculated for 195 countries from 1990 to 2016 and defined for each birth cohort as the expected years lived from age 20 to 64 years and adjusted for educational attainment, learning or education quality, and functional health status was published by the Lancet in September 2018. Japan had the highest level of expected human capital among the 20 largest countries: 24.1 health, education, and learning-adjusted expected years lived between age 20 and 64 years. \n\nObesity in Japan in 2014 was about 3.3%, about 10% of that in USA, presumably because of the Japanese diet. It has the lowest rate of heart disease in the OECD, and the lowest level of dementia in the developed world.\n\nJapan's suicide rate is high compared to the USA; the \"Yomiuri Shimbun\" reported in June 2008 that more than 30,000 people had killed themselves every year for the past decade. A study published in 2006 suspects that health problems were a factor in almost 50 percent of Japan's suicides in 2006. However the \"Yomiuri\"s 2007 figures show 274 school children were among those who took their own lives. Bullying is often a factor in such cases. In 2011, suicide remained over 30,000 for the 14th year running.\n\nOne of the biggest public health issues is smoking in Japan, which according to Tadao Kakizoe (honorary president of the National Cancer Center) kills more than 100,000 people per year and is responsible for one in ten deaths.\n\nA team led by Professor Osaki of Tottori University estimated the social cost of excessive drinking in Japan to be 4.15 trillion yen a year.\n\nIn Japan, services are provided either through regional/national public hospitals or through private hospitals/clinics, and patients have universal access to any facility, though hospitals tend to charge higher for those without a referral. However, space can be an issue in some regions. More than 14,000 emergency patients were rejected at least three times by Japanese hospitals before getting treatment in 2007, according to the latest government survey. In the worst case, a woman in her 70s with a breathing problem was rejected 49 times in Tokyo. Public health insurance covers most citizens/residents and pays 70% or more cost for each care and each prescribed drug. Patients are responsible for the remainder (upper limits apply). The monthly insurance premium is 0–50,000 JPY per household (scaled to annual income). Supplementary private health insurance is available only to cover the co-payments or non-covered costs, and usually makes a fixed payment per days in a hospital or per surgery performed, rather than per actual expenditure. In 2005, Japan spent 8.2% of GDP on health care or US$2,908 per capita. Of that, approximately 83% was government expenditure.\n\nTraditional Chinese medicine was introduced to Japan with other elements of Chinese culture during the 5th to 9th century. Since around 1900, Chinese-style herbalists have been required to be licensed medical doctors. Training was professionalized and, except for East Asian healers, was based on a biomedical model of the disease. However, the practice of biomedicine was influenced as well by Japanese social organization and cultural expectations concerning education, the organization of the workplace, and social relations of status and dependency, decision-making styles, and ideas about the human body, causes of illness, gender, individualism, and privacy. Anthropologist Emiko Ohnuki-Tierney notes that \"daily hygienic behavior and its underlying concepts, which are perceived and expressed in terms of biomedical germ theory, in fact, are directly tied to the basic Japanese symbolic structure.\"\n\nWestern medicine was introduced to Japan with the Rangaku studies during the Edo period. A number of books on pharmacology and anatomy were translated from Dutch and Latin to Japanese. During the Meiji period (late 19th century), the Japanese health care system was modeled after the model of Western biomedicine. At that time, western doctors came to Japan to create medical faculties at the newly built Japanese universities, and students also went abroad. Innovations like vaccines were introduced to Japan, improving average life expectancy. From the Meiji period through the end of World War II, German was a mandatory foreign language for Japanese students of medicine. Patient charts in Japanese teaching hospitals were even written in German.\n\nBut even today, a person who becomes ill in Japan has a number of alternative options. One may visit a priest, or send a family member in his or her place. There are numerous folk remedies, including hot springs baths (onsen) and chemical and herbal over-the-counter medications. A person may seek the assistance of traditional healers, such as herbalists, masseurs, and acupuncturists.\n\nAlthough the number of AIDS cases remained small by international standards, public health officials were concerned in the late 1980s about the worldwide epidemic of acquired immune deficiency syndrome (AIDS). The first confirmed case of AIDS in Japan was reported in 1985. By 1991 there were 553 reported cases, and by April 1992 the number had risen to 2,077. While frightened by the deadliness of the disease yet sympathetic to the plight of hemophiliac AIDS patients, most Japanese are unconcerned with contracting AIDS themselves. Various levels of government responded to the introduction of AIDS awareness into the heterosexual population by establishing government committees, mandating AIDS education, and advising testing for the general public without targeting special groups. A fund, underwritten by pharmaceutical companies that distributed imported blood products, was established in 1988 to provide financial compensation for AIDS patients.\n\n\n"}
{"id": "6142110", "url": "https://en.wikipedia.org/wiki?curid=6142110", "title": "Health professional", "text": "Health professional\n\nA health professional, health practitioner or healthcare provider (sometimes simply \"provider\") is an individual who provides preventive, curative, promotional or rehabilitative health care services in a systematic way to people, families or communities.\n\nA health professional may operate within all branches of health care, including medicine, surgery, dentistry, midwifery, pharmacy, psychology, nursing or allied health professions. A health professional may also be a public/community health expert working for the common good of the society.\n\nHealthcare practitioners include\n\nand a wide variety of other human resources trained to provide some type of health care service.\n\nThey often work in hospitals, healthcare centres and other service delivery points, but also in academic training, research and administration. Some provide care and treatment services for patients in private homes. Many countries have a large number of community health workers who work outside formal healthcare institutions. Managers of healthcare services, health information technicians, and other assistive personnel and support workers are also considered a vital part of health care teams.\n\nHealthcare practitioners are commonly grouped into health professions.\nWithin each field of expertise, practitioners are often classified according to skill level and skill specialization. “Health professionals” are highly skilled workers, in professions that usually require extensive knowledge including university-level study leading to the award of a first degree or higher qualification. This category includes physicians, physician assistants, dentists, midwives, radiographers, registered nurses, pharmacists, physiotherapists, optometrists, operating department practitioners and others. Allied health professionals, also referred to as \"health associate professionals\" in the International Standard Classification of Occupations, support implementation of health care, treatment and referral plans usually established by medical, nursing, and other health professionals, and usually require formal qualifications to practice their profession. In addition, unlicensed assistive personnel assist with providing health care services as permitted.\n\nAnother way to categorize healthcare practitioners is according to the sub-field in which they practice, such as mental health care, pregnancy and childbirth care, surgical care, rehabilitation care, or public health.\n\nA mental health practitioner is a health worker who offers services for the purpose of improving the mental health of individuals or treating mental illness. These include psychiatrists, clinical psychologists, clinical social workers, psychiatric-mental health nurse practitioners, marriage and family therapists, mental health counselors, as well as other health professionals and allied health professions. These health care providers often deal with the same illnesses, disorders, conditions, and issues; however their scope of practice often differs. The most significant difference across categories of mental health practitioners is education and training.\n\nA maternal and newborn health practitioner is a health worker who deals with the care of women and their children before, during and after pregnancy and childbirth. Such health practitioners include obstetricians, midwives, obstetrical nurses and many others. One of the main differences between these professions is in the training and authority to provide surgical services and other life-saving interventions. In some developing countries, traditional birth attendants, or traditional midwives, are the primary source of pregnancy and childbirth care for many women and families, although they are not certified or licensed.\n\nA geriatric care practitioner plans and coordinates the care of the elderly and/or disabled to promote their health, improve their quality of life, and maintain their independence for as long as possible. They include geriatricians, adult-gerontology nurse practitioners, clinical nurse specialists, geriatric clinical pharmacists, geriatric nurses, geriatric care managers, geriatric aides, Nursing aides, Caregivers and others who focus on the health and psychological care needs of older adults.\n\nA surgical practitioner is a healthcare professional who specializes in the planning and delivery of a patient's perioperative care, including during the anaesthetic, surgical and recovery stages. They may include general and specialist surgeons, surgeon's assistant, assistant surgeon, surgical assistant, anesthesiologists, anesthesiologist assistant, nurse anesthetists, surgical nurses, clinical officers, operating department practitioners, anaesthetic technicians, perioperative nursing, surgical technologists, and others.\n\nA rehabilitation care practitioner is a health worker who provides care and treatment which aims to enhance and restore functional ability and quality of life to those with physical impairments or disabilities. These include physiatrists, rehabilitation nurses, clinical nurse specialists, nurse practitioners, physiotherapists, orthotists, prosthetists, occupational therapists, recreational therapists, audiologists, speech and language pathologists, respiratory therapists, rehabilitation counsellors, physical rehabilitation therapists, athletic trainers, physiotherapy technicians, orthotic technicians, prosthetic technicians, personal care assistants, and others.\n\nCare and treatment for the eye and the adnexa may be delivered by ophthalmologists specializing in surgical/medical care, or optometrists specializing in refractive management and medical/therapeutic care.\n\nMedical diagnosis providers are health workers responsible for the process of determining which disease or condition explains a person's symptoms and signs. It is most often referred to as diagnosis with the medical context being implicit. This usually involves a team of healthcare providers in various diagnostic units. These include radiographers, radiologists, medical laboratory scientists, pathologists, and related professionals.\n\nA dental care practitioner is a health worker who provides care and treatment to promote and restore oral health. These include dentists and dental surgeons, dental assistants, dental auxiliaries, dental hygienists, dental nurses, dental technicians, dental therapists, and related professionals.\n\nCare and treatment for the foot, ankle, and lower leg may be delivered by podiatrists, pedorthists, foot health practitioners, podiatric medical assistants, podiatric nurse and others.\n\nA public health practitioner focuses on improving health among individuals, families and communities through the prevention and treatment of diseases and injuries, surveillance of cases, and promotion of healthy behaviors. This category includes community and preventive medicine specialists, public health nurses, clinical nurse specialists, dietitians, environmental health officers, paramedics, epidemiologists, health inspectors, and others.\n\nIn many societies, practitioners of alternative medicine have contact with a significant number of people, either as integrated within or remaining outside the formal health care system. These include practitioners in acupuncture, Ayurveda, herbalism, homeopathy, naturopathy, Reiki, Shamballa Reiki energy healing, Siddha medicine, traditional Chinese medicine, traditional Korean medicine, Unani, and Yoga. In some countries such as Canada, chiropractors and osteopaths (not to be confused with doctors of osteopathic medicine in the United States) are considered alternative medicine practitioners.\n\nMany jurisdictions report shortfalls in the number of trained health human resources to meet population health needs and/or service delivery targets, especially in medically underserved areas. For example, in the United States, the 2010 federal budget invested $330 million to increase the number of doctors, nurses, and dentists practicing in areas of the country experiencing shortages of trained health professionals. The Budget expands loan repayment programs for physicians, nurses, and dentists who agree to practice in medically underserved areas. This funding will enhance the capacity of nursing schools to increase the number of nurses. It will also allow states to increase access to oral health care through dental workforce development grants. The Budget’s new resources will sustain the expansion of the health care workforce funded in the Recovery Act. There were 15.7 million health care professionals in the US as of 2011.\n\nIn Canada, the 2011 federal budget announced a Canada Student Loan forgiveness program to encourage and support new family physicians, nurse practitioners and nurses to practice in underserved rural or remote communities of the country, including communities that provide health services to First Nations and Inuit populations.\n\nIn Uganda, the Ministry of Health reports that as many as 50% of staffing positions for health workers in rural and underserved areas remain vacant. As of early 2011, the Ministry was conducting research and costing analyses to determine the most appropriate attraction and retention packages for medical officers, nursing officers, pharmacists, and laboratory technicians in the country’s rural areas.\n\nAt the international level, the World Health Organization estimates a shortage of almost 4.3 million doctors, midwives, nurses, and support workers worldwide to meet target coverage levels of essential primary health care interventions. The shortage is reported most severe in 57 of the poorest countries, especially in sub-Saharan Africa.\n\nOccupational stress and occupational burnout are highly prevalent among health professionals. Some studies suggest that workplace stress is pervasive in the health care industry because of inadequate staffing levels, long work hours, exposure to infectious diseases and hazardous substances leading to illness or death, and in some countries threat of malpractice litigation. Other stressors include the emotional labor of caring for ill people and high patient loads. The consequences of this stress can include substance abuse, suicide, major depressive disorder, and anxiety, all of which occur at higher rates in health professionals than the general working population. Elevated levels of stress are also linked to high rates of burnout, absenteeism and diagnostic errors, and to reduced rates of patient satisfaction. In Canada, a national report (\"Canada's Health Care Providers\") also indicated higher rates of absenteeism due to illness or disability among health care workers compared to the rest of the working population, although those working in health care reported similar levels of good health and fewer reports of being injured at work. There is some evidence that cognitive-behavioral therapy, relaxation training and therapy (including meditation and massage), and modifying schedules can reduce stress and burnout among multiple sectors of health care providers. Research is ongoing in this area, especially with regards to physicians, whose occupational stress and burnout is less researched compared to other health professions.\n\nExposure to respiratory infectious diseases like tuberculosis (caused by \"Mycobacterium tuberculosis\") and influenza can be reduced with the use of respirators; this exposure is a significant occupational hazard for health care professionals. Exposure to dangerous chemicals, including chemotherapy drugs, is another potential occupational risk. These drugs can cause cancer and other health conditions. Healthcare workers are also at risk for diseases that are contracted through extended contact with a patient, including scabies. Health professionals are also at risk for contracting blood-borne diseases like hepatitis B, hepatitis C, and HIV/AIDS through needlestick injuries or through contact with bodily fluids. This risk can be mitigated with vaccination when there is a vaccine available, like with hepatitis B. In epidemic situations, such as the 2014-2016 West African Ebola virus epidemic or the 2003 SARS outbreak, healthcare workers are at even greater risk, and were disproportionately affected in both the Ebola and SARS outbreaks. In general, appropriate personal protective equipment (PPE) is the first-line mode of protection for healthcare workers from infectious diseases. For it to be effective against highly contagious diseases, personal protective equipment must be watertight and prevent the skin and mucous membranes from contacting infectious material. Different levels of personal protective equipment created to unique standards are used in situations where the risk of infection is different. Practices such as triple gloving and multiple respirators do not provide a higher level of protection and present a burden to the worker, who is additionally at increased risk of exposure when removing the PPE. Compliance with appropriate personal protective equipment rules may be difficult in certain situations, such as tropical environment or low-resource settings. A 2016 Cochrane systematic review found low quality evidence that using more breathable fabric in PPE, double gloving, and active training reduce the risk of contamination.\n\nFemale health care workers may face specific types of workplace-related health conditions and stress. According to the World Health Organization, women predominate in the formal health workforce in many countries, and are prone to musculoskeletal injury (caused by physically demanding job tasks such as lifting and moving patients) and burnout. Female health workers are exposed to hazardous drugs and chemicals in the workplace which may cause adverse reproductive outcomes such as spontaneous abortion and congenital malformations. In some contexts, female health workers are also subject to gender-based violence including from coworkers and patients.\n\nHealthcare workers are at higher risk of on-the-job injury due to violence. Drunk, confused, and hostile patients and visitors are a continual threat to providers attempting to treat patients. Frequently, assault and violence in a healthcare setting goes unreported and is wrongly assumed to be part of the job. Violent incidents typically occur during one-on-one care; being alone with patients increases healthcare workers' risk of assault. In the United States, healthcare workers suffer ⅔ of nonfatal workplace violence incidents. Psychiatric units represent the highest proportion of violent incidents, at 40%; they are followed by geriatric units (20%) and the emergency department (10%). Workplace violence can also cause psychological trauma.\n\nThe Occupational Health Safety Network is a system developed by the National Institute for Occupational Safety and Health (NIOSH) to address health and safety risks among health care providers. Hospitals and other healthcare facilities can upload the occupational injury data they already collect to the secure database for analysis and benchmarking with other de-identified facilities from throughout the U.S. NIOSH works with OHSN participants in identifying and implementing timely and targeted interventions. OHSN modules currently focus on three high risk and preventable events that can lead to injuries or musculoskeletal disorders among healthcare providers: musculoskeletal injuries from patient handling activities; slips, trips, and falls; and workplace violence. Slips, trips, and falls are the second-most common cause of worker's compensation claims in the US, and cause 21% of work absences due to injury. These injuries most commonly result in strains and sprains; women, those older than 45, and those who have been working less than a year in a healthcare setting are at the highest risk.\n\nHealth care professionals are also likely to experience sleep deprivation due to their jobs. Many health care professionals are on a shift work schedule, and therefore experience misalignment of their work schedule and their circadian rhythm. In 2007, 32% of healthcare workers were found to get fewer than 6 hours of sleep a night. Sleep deprivation also predisposes healthcare professionals to make mistakes that may potentially endanger a patient.\n\nPracticing without a license that is valid and current is typically illegal. In most jurisdictions, the provision of health care services is regulated by the government. Individuals found to be providing medical, nursing or other professional services without the appropriate certification or license may face sanctions and criminal charges leading to a prison term. The number of professions subject to regulation, requisites for individuals to receive professional licensure, and nature of sanctions that can be imposed for failure to comply vary across jurisdictions.\n\nIn the United States, under Michigan state laws, an individual is guilty of felony if identified as practicing in the health profession without a valid personal license or registration. Health professionals can also be imprisoned if found guilty of practicing beyond the limits allowed by their licences and registration. The state laws define the scope of practice for medicine, nursing, and a number of allied health professions. In Florida, practicing medicine without the appropriate license is a crime classified as a third degree felony, which may give imprisonment up to five years. Practicing a health care profession without a license which results in serious bodily injury classifies as a second degree felony, providing up to 15 years' imprisonment.\n\nIn the United Kingdom, healthcare professionals are regulated by the state; the UK Health and Care Professions Council (HCPC) protects the 'title' of each profession it regulates. For example, it is illegal for someone to call himself an Occupational Therapist or Radiographer if they are not on the register held by the HCPC.\n\n"}
{"id": "21260216", "url": "https://en.wikipedia.org/wiki?curid=21260216", "title": "Healthcare in Tanzania", "text": "Healthcare in Tanzania\n\nTanzania has a hierarchical health system which is in tandem with the political administrative hierarchy. At the bottom there are the dispensaries found in every village where the village leaders have a direct influence in its running. The health centres are found at ward level and the health centre in charge is answerable to the ward leaders. At the district there is a district hospital and at regional level a regional referral hospital. The tertiary level is usually the zone hospitals and at national level there is the national hospital. There are also some specialized hospitals which do not fit directly into this hierarchy and therefore are directly linked to the ministry of health. \n\nThe government has several key plans and policies guiding healthcare provision and development. The Health Sector Strategic Plan III (2009–15) is guided by the Vision 2015 and guides planning for health facilities. The Big Results Now (BRN) was copied from the Malaysian Model of Development and placed health as a key national result area and mainly was for priority setting, focused planning and efficient resource use. There are many other policies aiming at improving the health system and health acre provision in Tanzania.\n\nThe leading causes of mortality in Tanzania include: HIV 17%, lower respiratory infections 11%, malaria 7%, diarrheal diseases 6%, tuberculosis 5%, cancer 5%, ischemic heart disease 3%, stroke 3%, STDs 3% and sepsis 2% and this shows the double burden of disease the country has to bear. \n\nHealth care financing is among the key component of a functional health system. Financing involves three aspects, namely revenue collection, risk pooling, and purchasing. In recent years, there has been a growing demand for access to high-quality and affordable care for all, thus the government is committed to respond with a process of developing health financing strategy is underway since early 2013. An inter-ministerial steering committee has been developed, composed of key ministries and department to ensure that the proposed reforms meet the needs of the population. Improving the prepayment mechanisms are the main agenda in the development of the strategy, which is assumed to be a potential facilitator in the progress towards UHC.\n\nThe Arusha Declaration in 1967 was initiated by the president Julius Nyerere, outlining the principles of Ujamaa (Nyerere vision of social and economic policies) to develop the national economy. It marked the start of a series of health sector reforms with the intention of increasing universal access to social services to the poor and those living in marginalized rural areas. Followed by the Government banning private-for-profit medical practice in 1977 and took on the task of providing health services free of charge.\nHowever, by the early 1990s, the strain of providing free health care for all became evident in the face of rising health care costs and a struggling economy. Early 1990s the government adopted health sector reforms that changed the financing system from free services to mixed financing mechanisms including cost sharing policies. Cost sharing in the form of user fees was introduced in four phases: Phase I from July 1993 to June 1994 to referral and some services in regional hospital; Phase II from July 1994 to December 1994 to regional hospital; Phase III from January 1995 onwards to district hospital and Phase IV introduced to health centre and Dispensary after completion of introduction to all district hospital. Exemption and waiver were integral part of the cost sharing policy introduced in 1994.\n\nCurrent data shows in Tanzania there has been an increase in budget allocation for health over the years: Total Health Expendinture (THE) increased from US$734 million in 2002/2003 to US$1.75 billion in 2009/2010 as indicated in the National Health Accounts 2010 report. However donors have been the main financier of health, despite the decrease in their share of health expenditure from 44 percent in 2005/2006 to 40 percent in 2009/2010. (Table 1). Overall, the government allocation for health spending has remained almost constant at about 7 percent since 2002/2003, far away from reaching the Abuja declaration target of 15% of total government expenditure. The increase in donor funding is attributed to the commencement of financing for HIV and AIDS by the Global Fund in 2001 and the commencement of health financing through Sector wide Approach (SWAp) in early 2000.\n\nTable 1: Financing sources as a % of Total Health Expenditure\n\nOn the other hand, there has been a commitment to expand the insurance coverage in the country, however the insurance schemes are highly fragmented. There are four health insurance schemes which are publicly owned, namely National Health Insurance Fund (NHIF), Social Health Insurance Benefit (SHIB) established as a benefit under the National Social Security Fund (NSSF) and the Community Health Fund (CHF) and Tiba Kwa Kadi (TIKA). Recent statistics shows that there were about 7 private firms as indicated in the Tanzania Insurance Regulatory authority (TIRA) which were providing health insurance per se, while a few of other general insurance firms combine health insurance benefit under life insurance.\n\nThe NHIF was established by the Act of Parliament No. 8 of 1999 and began its operations in June 2001. The scheme was initially intended to cover public servants but recently there have been provisions which allow private membership. The public formal sector employees pay a mandatory contribution of 3% of their monthly salary and the government as an employer matches the same. This scheme covers the principal member, spouse and up to four below 18 years legal dependants. There has been a steady increase in coverage from 2.0% of the total population in 2001/2002 to 7.1% in 2011.\n\nSocial Health Insurance Benefit (SHIB) is part of the National Social Security Benefits introduced in 2007. All members of NSSF have access to medical care through SHIB after undergoing registration process with only one facility of their choice. The scheme accredits both public and private providers.The benefit is part of their 20% contribution to the NSSF.\n\nCommunity Health Fund is the scheme that targets the largest population in the rural informal sector and membership is voluntary. There is a counterpart called TIKA which mainly targets the informal sector individuals in urban areas. The CHF and TIKA are both regulated under the CHF act 2001 and managed at district level. At the district level, council health service boards (CHSB) and health facilities governing committees (HFGC) are responsible to oversee the operation of CHF and sensitization. In 2009 the National management role of CHF was given to the NHIF.\n\nStrategis was one of the first registered (2002) private health insurance firms in Tanzania. Members of Strategies insurance are corporate employees and become members through their company.\n\nAAR is another private health insurance in Tanzania. The firm started as a health-maintenance organisation (HMO) in 1999 but in 2007 it was re-registered as a private health insurance company.\n\nJubilee Insurance, Resolution Health and Metropolitan Insurance are other examples of private health insurance firms with more less similar features as strategies and AAR.\n\nHealth insurance coverage is still low in Tanzania. As of June 2013 NHIF was estimated to be covering about 6.6% of the population while CHF covers about 7.3% of the population based on 2012 Census. Beneficiaries of NHIF includes the contributing members, spouse and up to four dependants. The CHF beneficiaries include head of household, spouse and all children below 18 years. Other prepayment schemes cover less than 1% of the population. CHF mainly focuses its coverage in rural population while private health insurance schemes target urban population. Low insurance coverage leads to overreliance on direct payment at the point of use of health care, which is among the fundamental problem that restrain the move towards universal health coverage in many developing countries. Direct payment can lead to high level of inequity, and in most cases denying the poorest access to needed health care.\n\n\n"}
{"id": "37858149", "url": "https://en.wikipedia.org/wiki?curid=37858149", "title": "History of hospitals", "text": "History of hospitals\n\nThe history of hospitals has stretched over 2500 years.\n\nIn ancient cultures, religion and medicine were linked. The earliest documented institutions aiming to provide cures were ancient Egyptian temples. \n\nIn ancient Greece, temples dedicated to the healer-god Asclepius, known as \"Asclepieia\" (, sing. \"Asclepieion\", ), functioned as centres of medical advice, prognosis, and healing. Asclepeia provided carefully controlled spaces conducive to healing and fulfilled several of the requirements of institutions created for healing. Under his Roman name Æsculapius, he was provided with a temple (291 B.C.) on an island in the Tiber in Rome, where similar rites were performed.\n\nAt these shrines, patients would enter a dream-like state of induced sleep known as \"enkoimesis\" () not unlike anesthesia, in which they either received guidance from the deity in a dream or were cured by surgery. Asclepeia provided carefully controlled spaces conducive to healing and fulfilled several of the requirements of institutions created for healing. In the Asclepieion of Epidaurus, three large marble boards dated to 350 BC preserve the names, case histories, complaints, and cures of about 70 patients who came to the temple with a problem and shed it there. Some of the surgical cures listed, such as the opening of an abdominal abscess or the removal of traumatic foreign material, are realistic enough to have taken place, but with the patient in a state of enkoimesis induced with the help of soporific substances such as opium. The worship of Asclepius was adopted by the Romans. Under his Roman name Æsculapius, he was provided with a temple (291 BC) on an island in the Tiber in Rome, where similar rites were performed.\n\nInstitutions created specifically to care for the ill also appeared early in India. Fa Xian, a Chinese Buddhist monk who travelled across India ca. 400 AD, recorded in his travelogue that \n\nThe earliest surviving encyclopaedia of medicine in Sanskrit is the Carakasamhita (Compendium of Caraka). This text, which describes the building of a hospital is dated by the medical historian Dominik Wujastyk to the period between 100 BCE and 150 CE. The description by Fa Xian is one of the earliest accounts of a civic hospital system anywhere in the world and this evidence, coupled with Caraka’s description of how a clinic should be built and equipped, suggests that India may have been the first part of the world to have evolved an organized cosmopolitan system of institutionally-based medical provision.\n\nKing Ashoka is wrongly said by many secondary sources to have founded at hospitals in ca. 230 BCE\n\nAccording to the Mahavamsa, the ancient chronicle of Sinhalese royalty, written in the sixth century CE, King Pandukabhaya of Sri Lanka (reigned 437 BCE to 367 BCE) had lying-in-homes and hospitals (Sivikasotthi-Sala) built in various parts of the country. This is the earliest documentary evidence we have of institutions specifically dedicated to the care of the sick anywhere in the world. Mihintale Hospital is the oldest in the world. Ruins of ancient hospitals in Sri Lanka are still in existence in Mihintale, Anuradhapura, and Medirigiriya.\n\nThe Romans constructed buildings called \"valetudinaria\" for the care of sick slaves, gladiators, and soldiers around 100 BCE, and many were identified by later archeology. While their existence is considered proven, there is some doubt as to whether they were as widespread as was once thought, as many were identified only according to the layout of building remains, and not by means of surviving records or finds of medical tools.\n\nThe declaration of Christianity as an accepted religion in the Roman Empire drove an expansion of the provision of care. Following First Council of Nicaea in 325 CE construction of a hospital in every cathedral town was begun. Among the earliest were those built by the physician Saint Sampson in Constantinople and by Basil, bishop of Caesarea in modern-day Turkey. Called the \"Basilias\", the latter resembled a city and included housing for doctors and nurses and separate buildings for various classes of patients. There was a separate section for lepers. Some hospitals maintained libraries and training programs, and doctors compiled their medical and pharmacological studies in manuscripts. Thus in-patient medical care in the sense of what we today consider a hospital, was an invention driven by Christian mercy and Byzantine innovation. Byzantine hospital staff included the Chief Physician (archiatroi), professional nurses (hypourgoi) and the orderlies (hyperetai). By the twelfth century, Constantinople had two well-organized hospitals, staffed by doctors who were both male and female. Facilities included systematic treatment procedures and specialized wards for various diseases.\n\nA hospital and medical training centre also existed at Gundeshapur. The city of Gundeshapur was founded in 271 CE by the Sassanid king Shapur I. It was one of the major cities in Khuzestan province of the Persian empire, in Iran. A large percentage of the population were Syriacs, most of whom were Christians. Under the rule of Khusraw I, refuge was granted to Greek Nestorian Christian philosophers including the scholars of the Persian School of Edessa (Urfa) (also called the Academy of Athens), a Christian theological and medical university. These scholars made their way to Gundeshapur in 529 following the closing of the academy by Emperor Justinian. They were engaged in medical sciences and initiated the first translation projects of medical texts. The arrival of these medical practitioners from Edessa marks the beginning of the hospital and medical centre at Gundeshapur. It included a medical school and hospital (bimaristan), a pharmacology laboratory, a translation house, a library and an observatory. Indian doctors also contributed to the school at Gundeshapur, most notably the medical researcher Mankah. Later after Islamic invasion, the writings of Mankah and of the Indian doctor Sustura were translated into Arabic at Baghdad's House of Wisdom.\n\nMedieval hospitals in Europe followed a similar pattern to the Byzantine. They were religious communities, with care provided by monks and nuns. (An old French term for hospital is \"hôtel-Dieu\", \"hostel of God.\") Some were attached to monasteries; others were independent and had their own endowments, usually of property, which provided income for their support. Some hospitals were multi-functional while others were founded for specific purposes such as leper hospitals, or as refuges for the poor, or for pilgrims: not all cared for the sick. \n\nAround 529 A.D. St. Benedict of Nursia (480-543 A.D.), later a Christian saint, the founder of western monasticism and the Order of St. Benedict, today the patron saint of Europe, established the first monastery in Europe (Monte Cassino) on a hilltop between Rome and Naples, that became a model for the Western monasticism and one of the major cultural centers of Europe throughout the Middle Ages. St. Benedict wrote the Rule of Saint Benedict which mandated the moral obligations to care for the sick.\n\nThe first Spanish hospital, founded by the Catholic Visigoth bishop Masona in 580 CE at Mérida, was a \"xenodochium\" designed as an inn for travellers (mostly pilgrims to the shrine of Eulalia of Mérida) as well as a hospital for citizens and local farmers. The hospital's endowment consisted of farms to feed its patients and guests. From the account given by Paul the Deacon we learn that this hospital was supplied with physicians and nurses, whose mission included the care the sick wherever they were found, \"slave or free, Christian or Jew.\" \nIn 650, the \"Hôtel-Dieu\" was found in Paris, it is considered as the oldest worldwide hospital still operating today. It was a multipurpose institution which catered for the sick and poor, offering shelter, food and medical care.\n\nDuring the late 8th and early 9th centuries, Emperor Charlemagne decreed that those hospitals that had been well conducted before his time and had fallen into decay should be restored in accordance with the needs of the time. He further ordered that a hospital should be attached to each cathedral and monastery.\n\nDuring the 10th century, the monasteries became a dominant factor in hospital work. The famous Benedictine Abbey of Cluny, founded in 910, set the example which was widely imitated throughout France and Germany. Besides its infirmary for the religious, each monastery had a hospital in which externs were cared for. These were in charge of the \"eleemosynarius\", whose duties, carefully prescribed by the rule, included every sort of service that the visitor or patient could require.\n\nAs the eleemosynarius was obliged to seek out the sick and needy in the neighborhood, each monastery became a center for the relief of suffering. Among the monasteries notable in this respect were those of the Benedictines at Corbie in Picardy, Hirschau, Braunweiler, Deutz, Ilsenburg, Liesborn, Pram, and Fulda; those of the Cistercians at Arnsberg, Baumgarten, Eberbach, Himmenrode, Herrnalb, Volkenrode, and Walkenried.\nNo less efficient was the work done by the diocesan clergy in accordance with the disciplinary enactments of the councils of Aachen (817, 836), which prescribed that a hospital should be maintained in connection with each collegiate church. The canons were obliged to contribute towards the support of the hospital, and one of their number had charge of the inmates. As these hospitals were located in cities, more numerous demands were made upon them than upon those attached to the monasteries. In this movement the bishop naturally took the lead, hence the hospitals founded by Heribert (d. 1021) in Cologne, Godard (d. 1038) in Hildesheim, Conrad (d. 975) in Constance, and Ulrich (d. 973) in Augsburg. But similar provision was made by the other churches; thus at Trier the hospitals of Saint Maximin, Saint Matthew, Saint Simeon, and Saint James took their names from the churches to which they were attached. During the period 1207–1577 no less than one hundred and fifty-five hospitals were founded in Germany.\n\nThe Ospedale Maggiore, traditionally named Ca' Granda (i.e. Big House), in Milan, northern Italy, was constructed to house one of the first community hospitals, the largest such undertaking of the fifteenth century. Commissioned by Francesco Sforza in 1456 and designed by Antonio Filarete it is among the first examples of Renaissance architecture in Lombardy.\n\nThe Normans brought their hospital system along when they conquered England in 1066. By merging with traditional land-tenure and customs, the new charitable houses became popular and were distinct from both English monasteries and French hospitals. They dispensed alms and some medicine, and were generously endowed by the nobility and gentry who counted on them for spiritual rewards after death.\n\nThe Hospitaller Order of St. John of Jerusalem, founded in 1099 (The Knights of Malta) itself has - as its raison d’être - the founding of a hospital for pilgrims to the Holy Land. In Europe, Spanish hospitals are particularly noteworthy examples of Christian virtue as expressed through care for the sick, and were usually attached to a monastery in a ward-chapel configuration, most often erected in the shape of a cross. This style reached a high point during the hospital building campaign of Portuguese St. John of God in the sixteenth-century, the founder of the Hospitaller Order of the Brothers of John of God.\n\nSoon many monasteries were founded throughout Europe, and everywhere there were hospitals like in Monte Cassino. By the 11th century, some monasteries were training their own physicians. Ideally, such physicians would uphold the Christianized ideal of the healer who offered mercy and charity towards all patients and soldiers, whatever their status and prognosis might be. In the 6th–12th centuries the Benedictines established lots of monk communities of this type. And later, in the 12th–13th centuries the Benedictines order built a network of independent hospitals, initially to provide general care to the sick and wounded and then for treatment of syphilis and isolation of patients with communicable disease. The hospital movement spread through Europe in the subsequent centuries, with a 225-bed hospital being built at York in 1287 and even larger facilities established at Florence, Paris, Milan, Siena, and other medieval big European cities.\n\nIn the North during the late Saxon period, monasteries, nunneries, and hospitals functioned mainly as a site of charity to the poor. After the Norman Conquest of 1066, hospitals are found to be autonomous, freestanding institutions. They dispensed alms and some medicine, and were generously endowed by the nobility and gentry who counted on them for spiritual rewards after death. In time, hospitals became popular charitable houses that were distinct from both English monasteries and French hospitals.\n\nThe primary function of medieval hospitals was to worship to God. Most hospitals contained one chapel, at least one clergyman, and inmates that were expected to help with prayer. Worship was often a higher priority than care and was a large part of hospital life until and long after the Reformation. Worship in medieval hospitals served as a way of alleviating ailments of the sick and insuring their salvation when relief from sickness could not be achieved.\n\nThe secondary function of medieval hospitals was charity to the poor, sick, and travellers. Charity provided by hospitals surfaced in different ways, including long-term maintenance of the infirm, medium-term care of the sick, short-term hospitality to travellers, and regular distribution of alms to the poor. Though these were general acts of charity among medieval hospitals, the degree of charity was variable. For example, some institutions that perceived themselves mainly as a religious house or place of hospitality turned away the sick or dying in fear that difficult healthcare will distract from worship. Others, however, such as St. James of Northallerton, St. Giles of Norwich, and St. Leonard of York, contained specific ordinances stating they must cater to the sick and that \"all who entered with ill health should be allowed to stay until they recovered or died\".\n\nThe tertiary function of medieval hospitals was to support education and learning. Originally, hospitals educated chaplains and priestly brothers in literacy and history; however, by the 13th century, some hospitals became involved in the education of impoverished boys and young adults. Soon after, hospitals began to provide food and shelter for scholars within the hospital in return for helping with chapel worship.\n\nThree well-documented medieval European hospitals are St. Giles in Norwich, St. Anthony's in London, and St. Leonards in York. St. Giles, along with St. Anthony's and St. Leonards, were open ward hospitals that cared for the poor and sick in three of medieval England's largest cities. The study of these three hospitals can provide insight into the diet, medical care, cleanliness and daily life in a medieval hospital of Europe.\n\nDiscrepancies exist among sources regarding the founding of St. Giles of Norwich, or the \"Great Hospital\" as it is known today. Some sources maintain that it was founded in 1246. Other sources state that it was founded in 1249. Though the date may be debatable, it seems agreed upon that The Great Hospital was founded by Walter Suffield, a Bishop known to be very liberal to the poor especially in the city of Norwich. St Giles provided thirty beds and maintained within its ten-acre precinct, many meadows courtyards, ponds, and fruit trees until the late fifteenth century. The hospital cultivated many productive gardens abundant in apples, leeks, garlic, onions and honey. The gardens were so productive that surplus goods were sold on the open market. St Giles of Norwich owned six manors and advowson of eleven churches.\n\nSt Giles was unique in that food was provided for children who were getting free education elsewhere. It is also noted that St Giles arranged for seven poor scholars to receive board at the hospital during their term at Norwich School. Accommodations of early medieval hospitals were frequently communal. For example, in St Giles, the master and brothers ate in the common hall while sisters ate by themselves. St Giles hospital was a complex building that housed a community of clergy with cloister and residential accommodation, a hospital and a parish church. St Giles was also wealthy enough to maintain its own kitchen and staff. This allowed poor men to receive a dish of meat, fish, eggs or cheese in addition to the customary daily ration of bread and drink.\n\nSt Anthony's was erected in the thirteenth century (some time before 1254), in the heart of London on Threadneedle Street, atop the less than ideal site of a Jewish synagogue. The chapel of St. Anthony's was built in 1310 without permission of the bishop of London. To prevents its degradation, the hospital petitioned for a chapel on the bishops terms. Unlike St. Giles, there was insufficient land at St. Anthony's, London, for recreation or food production. As a result, herbs or 'erbys' and vegetables had to be bought on a daily basis for consumption by the entire community. Accounts of foreign expenses at St. Anthony's also show the purchase of various spices, often with intrinsic medicinal qualities that could alter the level of heat and moisture within the body. Some of the spices bought include, saffron, cloves, ginger, cinnamon, lavender, pepper and mustard. The amount spent on herbs, produce, and spices were far surpassed by the amount spent on fish and meat.\n\nAccording to quarterly expenditure reports, fifty-eight percent of the quarterly budget was spent on meat, thirty-four percent on fish, three percent on pottage, two percent on dairy, one percent herbs and one percent on eggs. The unusually detailed records of diet and expenditures at St. Anthony's have revealed that the diet of the clerical establishment ('the hall') and the diets of the almsmen, patients and children ('the hospital') were quite different and class-based. During a typical week, \"the entire community shared dishes of pottage, veal, mutton and eggs; the hall alone consumed pork, ribs of roast beef, duck, fresh salmon and eels; and the hospital was supplied with mutton, plaice and haddock.\" It is clear that the hall, or more wealthy, enjoyed extravagant meals of meat and fish, while the hospital, the patients and the poor, were fed simpler and cheaper food.\n\nIn addition to its reputation of spending lavishly on food, St. Anthony's was famous for its grammar school, choir and pigs, which roamed freely among the streets identified by bells. Pigs on sale in London, which were considered by officials to be unfit for food were handed over to St. Anthony's. The pigs were fed through charity or by scavenging and later, when their condition improved, they were then taken by the hospital for use as food for the poor or sick.\n\nAs mentioned, medieval hospitals became concerned in education and in the feeding and housing of students as early as the thirteenth century. In 1441, John Carpenter, the master of St. Anthony London, was able to finance a grammar school whose teachings were without fees to any student. This was the first source of free education in London and remained one of London's leading schools for one hundred years following its founding.\n\nIn 1449, St. Anthony's received a handsome legacy for the support of a clerk to train scholars in both polyphony and plainsong. St. Anthony's became so famous for its choir that in 1469, the royal minstrels set up a fraternity at the hospital so they may also study music.\n\nSt Leonard's was one on England's largest and richest hospitals with a primary purpose of caring for the poor, the sick, the old and infirm. It maintained 200 beds and in its prosperous days, \"maintained up to eighteen clergy, 16 sister and female servants, 30 choristers, 10 private boarders (corrodarians) and between 144 and 240 poor sick people.\" Additionally, during Easter of 1370, records show accommodation of 224 sick and poor in the infirmary and 23 children in the orphanage.\n\nThe records of St Leonard provide the best details of daily hospital worship and patient life. In 1249, for example, matins and lauds were said in the morning hours of darkness, followed by mass of the Virgin Mary held by members of the clergy. The \"lesser hours and mass of the day were said at mid-morning, vespers in the afternoon and compline in the early evening after supper.\" Sisters at St. Leonard's were instructed to feed the poor and the sick, wash them, and lead them around the grounds. Although the food given to the sick were the simple, and often quite cheap, daily provisions, sisters were allowed to distribute special food if they were very ill.\n\nAt St Leonard's, charity and care for the sick was not only given to inmates of the hospitals, but also to the poor in other neighboring institutions as well as inmates of local leper houses. Additionally, one or two of the chaplains at St Leonard's were instructed to \"minister spiritually to the poor by speaking consoling words, hearing confessions, and administering the sacraments\".\n\nThe first Muslim hospital was an asylum to contain leprosy, built in the early eighth century, where patients were confined but, like the blind, were given a stipend to support their families. The earliest general hospital was built in 805 in Baghdad by Harun Al-Rashid. By the tenth century, Baghdad had five more hospitals, while Damascus had six hospitals by the 15th century and Córdoba alone had 50 major hospitals, many exclusively for the military. Many of the prominent early Islamic hospitals were founded with assistance by Christians such as Jibrael ibn Bukhtishu from Gundeshapur. \"Bimaristan\" is a compound of “bimar” (sick or ill) and “stan” (place). In the medieval Islamic world, the word \"bimaristan\" referred to a hospital establishment where the ill were welcomed, cared for and treated by qualified staff.\n\nThe United States National Library of Medicine credits the hospital as being a product of medieval Islamic civilization. Compared to contemporaneous Christian institutions, which were poor and sick relief facilities offered by some monasteries, the Islamic hospital was a more elaborate institution with a wider range of functions. In Islam, there was a moral imperative to treat the ill regardless of financial status. Islamic hospitals tended to be large, urban structures, and were largely secular institutions, many open to all, whether male or female, civilian or military, child or adult, rich or poor, Muslim or non-Muslim. The Islamic hospital served several purposes, as a center of medical treatment, a home for patients recovering from illness or accidents, an insane asylum, and a retirement home with basic maintenance needs for the aged and infirm.\n\nThe typical hospital was divided into departments such as systemic diseases, surgery and orthopedics with larger hospitals having more diverse specialties. \"Systemic diseases\" was the rough equivalent of today's internal medicine and was further divided into sections such as fever, infections and digestive issues. Every department had an officer-in-charge, a presiding officer and a supervising specialist. The hospitals also had lecture theaters and libraries. Hospitals staff included sanitary inspectors, who regulated cleanliness, and accountants and other administrative staff. The hospital in Baghdad employed twenty-five staff physicians. The hospitals were typically run by a three-man board comprising a non-medical administrator, the chief pharmacist, called the shaykh saydalani, who was equal in rank to the chief physician, who served as mutwalli (dean). Medical facilities traditionally closed each night, but by the 10th century laws were passed to keep hospitals open 24 hours a day.\n\nFor less serious cases, physicians staffed outpatient clinics. Cities also had first aid centers staffed by physicians for emergencies that were often located in busy public places, such as big gatherings for Friday prayers to take care of casualties. The region also had mobile units staffed by doctors and pharmacists who were supposed to meet the need of remote communities. Baghdad was also known to have a separate hospital for convicts since the early 10th century after the vizier ‘Ali ibn Isa ibn Jarah ibn Thabit wrote to Baghdad’s chief medical officer that “prisons must have their own doctors who should examine them every day”. The first hospital built in Egypt, in Cairo's Southwestern quarter, was the first documented facility to care for mental illnesses while the first Islamic psychiatric hospital opened in Baghdad in 705.\n\nMedical students would accompany physicians and participate in patient care. Hospitals in this era were the first to require medical diplomas to license doctors. The licensing test was administered by the region's government appointed chief medical officer. The test had two steps; the first was to write a treatise, on the subject the candidate wished to obtain a certificate, of original research or commentary of existing texts, which they were encouraged to scrutinize for errors. The second step was to answer questions in an interview with the chief medical officer. Physicians worked fixed hours and medical staff salaries were fixed by law. For regulating the quality of care and arbitrating cases, it is related that if a patient dies, their family presents the doctor's prescriptions to the chief physician who would judge if the death was natural or if it was by negligence, in which case the family would be entitled to compensation from the doctor. The hospitals had male and female quarters while some hospitals only saw men and other hospitals, staffed by women physicians, only saw women. While women physicians practiced medicine, many largely focused on obstetrics.\n\nHospitals were forbidden by law to turn away patients who were unable to pay. Eventually, charitable foundations called waqfs were formed to support hospitals, as well as schools. Part of the state budget also went towards maintaining hospitals. While the services of the hospital were free for all citizens and patients were sometimes given a small stipend to support recovery upon discharge, individual physicians occasionally charged fees. In a notable endowment, a 13th-century governor of Egypt Al Mansur Qalawun ordained a foundation for the Qalawun hospital that would contain a mosque and a chapel, separate wards for different diseases, a library for doctors and a pharmacy and the hospital is used today for ophthalmology. The Qalawun hospital was based in a former Fatimid palace which had accommodation for 8,000 people - \"it served 4,000 patients daily.\" The waqf stated, \"...The hospital shall keep all patients, men and women, until they are completely recovered. All costs are to be borne by the hospital whether the people come from afar or near, whether they are residents or foreigners, strong or weak, low or high, rich or poor, employed or unemployed, blind or sighted, physically or mentally ill, learned or illiterate. There are no conditions of consideration and payment, none is objected to or even indirectly hinted at for non-payment.\"\n\nThe first, most well known physicians in the Medieval Islamic world were polymaths Ibn Sina, (Greek: Avicenna) and Al Rhazi (Greek: Rhazes) during the 10th and 11th centuries.\n\nIn Europe the medieval concept of Christian care evolved during the sixteenth and seventeenth centuries into a secular one. Theology was the problem. The Protestant reformers rejected the Catholic belief that rich men could gain God's grace through good works – and escape purgatory – by providing endowments to charitable institutions, and that the patients themselves could gain grace through their suffering.\n\nAfter the dissolution of the monasteries in 1540 by King Henry VIII the church abruptly ceased to be the supporter of hospitals, and only by direct petition from the citizens of London, were the hospitals St Bartholomew's, St Thomas's and St Mary of Bethlehem's (Bedlam) endowed directly by the crown; this was the first instance of secular support being provided for medical institutions. It was at St. Bartholomew that William Harvey conducted his research on the circulatory system in the 17th century, Percivall Pott and John Abernethy developed important principles of modern surgery in the 18th century, and Mrs. Bedford Fenwich worked to advance the nursing profession in the late 19th century.\n\nThere were 28 asylums in Sweden at the start of the Reformation. Gustav Vasa removed them from church control and expelled the monks and nuns, but allowed the asylums to keep their properties and to continue their activities under the auspices of local government.\n\nIn much of Europe town governments operated small Holy Spirit hospitals, which had been founded in the 13th and 14th centuries. They distributed free food and clothing to the poor, provided for homeless women and children, and gave some medical and nursing care. Many were raided and closed during the Thirty Years War (1618–48), which ravaged the towns and villages of Germany and neighboring areas for three decades.\n\nMeanwhile, in Catholic lands such as France, rich families continued to fund convents and monasteries that provided free health services to the poor. French practices were influenced by a charitable imperative which considered care of the poor and the sick to be a necessary part of Catholic practice. The nursing nuns had little faith in the power of physicians and their medicines alone to cure the sick; more important was providing psychological and physical comfort, nourishment, rest, cleanliness and especially prayer.\n\nIn Protestant areas the emphasis was on scientific rather than religious aspects of patient care, and this helped develop a view of nursing as a profession rather than a vocation. There was little hospital development by the main Protestant churches after 1530. Some smaller groups such as the Moravians and the Pietists at Halle gave a role for hospitals, especially in missionary work.\n\nIn the 18th century, under the influence of the Age of Enlightenment, the modern hospital began to appear, serving only medical needs and staffed with trained physicians and surgeons. The nurses were untrained workers. The goal was to use modern methods to cure patients. They provided more narrow medical services, and were founded by the secular authorities. A clearer distinction emerged between medicine and poor relief. Within the hospitals, acute cases were increasingly treated alone, and separate departments were set up for different categories of patient.\nThe voluntary hospital movement began in the early 18th century, with hospitals being founded in London by the 1710s and 20s, including Westminster Hospital (1719) promoted by the private bank C. Hoare & Co and Guy's Hospital (1724) funded from the bequest of the wealthy merchant, Thomas Guy. Other hospitals sprang up in London and other British cities over the century, many paid for by private subscriptions. St. Bartholomew's in London was rebuilt in 1730, and the London Hospital opened in 1752.\n\nThese hospitals represented a turning point in the function of the institution; they began to evolve from being basic places of care for the sick to becoming centres of medical innovation and discovery and the principle place for the education and training of prospective practitioners. Some of the era's greatest surgeons and doctors worked and passed on their knowledge at the hospitals. They also changed from being mere homes of refuge to being complex institutions for the provision of medicine and care for sick. The Charité was founded in Berlin in 1710 by King Frederick I of Prussia as a response to an outbreak of plague.\n\nThe concept of voluntary hospitals also spread to Colonial America; the Bellevue Hospital opened in 1736, Pennsylvania Hospital in 1752, New York Hospital in 1771, and Massachusetts General Hospital in 1811. When the Vienna General Hospital opened in 1784 (instantly becoming the world's largest hospital), physicians acquired a new facility that gradually developed into one of the most important research centres.\n\nAnother Enlightenment era charitable innovation was the dispensary; these would issue the poor with medicines free of charge. The London Dispensary opened its doors in 1696 as the first such clinic in the British Empire. The idea was slow to catch on until the 1770s, when many such organizations began to appear, including the Public Dispensary of Edinburgh (1776), the Metropolitan Dispensary and Charitable Fund (1779) and the Finsbury Dispensary (1780). Dispensaries were also opened in New York 1771, Philadelphia 1786, and Boston 1796.\n\nAcross Europe medical schools still relied primarily on lectures and readings. In the final year, students would have limited clinical experience by following the professor through the wards. Laboratory work was uncommon, and dissections were rarely done because of legal restrictions on cadavers. Most schools were small, and only Edinburgh, Scotland, with 11,000 alumni, produced large numbers of graduates.\n\nThe first hospital founded in the Americas was the Hospital San Nicolás de Bari [Calle Hostos] in Santo Domingo, Distrito Nacional Dominican Republic. Fray Nicolás de Ovando, Spanish governor and colonial administrator from 1502–1509, authorized its construction on December 29, 1503. This hospital apparently incorporated a church. The first phase of its construction was completed in 1519, and it was rebuilt in 1552. Abandoned in the mid-18th century, the hospital now lies in ruins near the Cathedral in Santo Domingo.\n\nConquistador Hernán Cortés founded the two earliest hospitals in North America: the Immaculate Conception Hospital and the Saint Lazarus Hospital. The oldest was the Immaculate Conception, now the Hospital de Jesús Nazareno in Mexico City, founded in 1524 to care for the poor.\n\nEnglish physician Thomas Percival (1740–1804) wrote a comprehensive system of medical conduct, \"Medical Ethics; or, a Code of Institutes and Precepts, Adapted to the Professional Conduct of Physicians and Surgeons\" (1803) that set the standard for many textbooks.\nIn the mid 19th century, hospitals and the medical profession became more professionalized, with a reorganization of hospital management along more bureaucratic and administrative lines. The Apothecaries Act 1815 made it compulsory for medical students to practice for at least half a year at a hospital as part of their training. An example of this professionalization was the Charing Cross Hospital, set up in 1818 as the 'West London Infirmary and Dispensary' from funds provided by Dr. Benjamin Golding. By 1821 it was treating nearly 10,000 patients a year, and it was relocated to larger quarters near Charing Cross in the heart of London. Its Charing Cross Hospital Medical School opened in 1822. It expanded several times and 1866 added a professional nursing staff.\n\nFlorence Nightingale pioneered the modern profession of nursing during the Crimean War when she set an example of compassion, commitment to patient care and diligent and thoughtful hospital administration. The first official nurses’ training programme, the Nightingale School for Nurses, was opened in 1860, with the mission of training nurses to work in hospitals, to work with the poor and to teach.\n\nNightingale was instrumental in reforming the nature of the hospital, by improving sanitation standards and changing the image of the hospital from a place the sick would go to die, to an institution devoted to recuperation and healing. She also emphasized the importance of statistical measurement for determining the success rate of a given intervention and pushed for administrative reform at hospitals.\n\nDuring the middle of the 19th century, the Second Viennese Medical School emerged with the contributions of physicians such as Carl Freiherr von Rokitansky, Josef Škoda, Ferdinand Ritter von Hebra, and Ignaz Philipp Semmelweis. Basic medical science expanded and specialization advanced. Furthermore, the first dermatology, eye, as well as ear, nose, and throat clinics in the world were founded in Vienna.\n\nBy the late 19th century, the modern hospital was beginning to take shape with a proliferation of a variety of public and private hospital systems. By the 1870s, hospitals had more than tripled their original average intake of 3,000 patients. In continental Europe the new hospitals generally were built and run from public funds. Nursing was professionalized in France by the turn of the 20th century. At that time, the country's 1,500 hospitals were operated by 15,000 nuns representing over 200 religious orders. Government policy after 1900 was to secularize public institutions, and diminish the role the Catholic Church. This political goal came in conflict with the need to maintain better quality of medical care in antiquated facilities. New government-operated nursing schools turned out nonreligous nurses who were slated for supervisory roles. During the First World War, an outpouring of patriotic volunteers brought large numbers of untrained middle-class women into the military hospitals. They left when the war ended but the long-term effect was to heighten the prestige of nursing. In 1922 the government issued a national diploma for nursing.\n\nIn the U.S., the number of hospitals reached 4400 in 1910, when they provided 420,000 beds. These were operated by city, state and federal agencies, by churches, by stand-alone non-profits, and by for-profit enterprises. All the major denominations built hospitals; the 541 Catholic ones (in 1915) were staffed primarily by unpaid nuns. The others sometimes had a small cadre of deaconesses as staff. Non-profit hospitals were supplemented by large public hospitals in major cities and research hospitals often affiliated with a medical school. The largest public hospital system in America is the New York City Health and Hospitals Corporation, which includes Bellevue Hospital, the oldest U.S. hospital, affiliated with New York University Medical School.\n\nThe National Health Service, the principal provider of health care in Britain, was founded in 1948, and took control of nearly all the hospitals.\n\nAt the turn of the 19th century, Paris medicine played a significant role in shaping clinical medicine. New emphasis on physically examining the body led to methods such as percussion, inspection, palpation, auscultation, and autopsy. The situation in Paris was particularly unique due to the fact that there was a very large concentration of medical professionals in a very small setting allowing for a large flow of ideas and the spread of innovation. One of the innovations to come out of the Paris hospital setting was Laennec's stethoscope. Weiner states that the widespread acceptance of the stethoscope would likely not have happened in any other setting, and the setting allowed for Laennec to pass on this technology to the eager medical community that had gathered there. This invention also brought even more attention to the Paris scene.\n\nBefore the start of the 19th century, there were many problems existing within the French medical system. These problems were outlined by many seeking to reform hospitals including a contemporary surgeon Jacques Tenon in his book \"Memoirs on Paris Hospitals\". Some of the problems Tenon drew attention to were the lack of space, the inability to separate patients based on the type of illness (including those that were contagious), and general sanitation problems. Additionally, the secular revolution led to the nationalization of hospitals previously owned by the Catholic Church and led to a call for a hospital reform which actually pushed for the deinstitutionalization of medicine. This contributed to the state of disarray Paris hospitals soon fell into which ultimately called for the establishment of a new hospital system outlined in the law of 1794. The law of 1794 played a significant part in revolutionizing Paris Medicine because it aimed to address some of the problems facing Paris Hospitals of the time.\n\nFirst, the law of 1794 created three new schools in Paris, Montpellier, and Strasbour due to the lack of medical professionals available to treat a growing French army. It also gave physicians and surgeons equal status in the hospital environment, whereas previously physicians were regarded as intellectually superior. This led to the integration of surgery into traditional medical education contributing to a new breed of doctors that focused on pathology, anatomy and diagnosis. The new focus on anatomy was further facilitated by this law because it ensured that medical students had enough bodies to dissect. Additionally, pathological education was furthered by the increased use of autopsies to determine a patient's cause of death. Lastly, the law of 1794 allocated funds for full-time salaried teachers in hospitals, as well as creating scholarships for medical students. Overall, the law of 1794 contributed to the shift of medical teaching away from theory and towards practice and experience, all within a hospital setting. Hospitals became a center for learning and development of medical techniques, which was a departure from the previous notion of a hospital as an area that accepted people who needed help of any kind, ill or not. This shift was consistent with much of the philosophy of the time, particularly the ideas of John Locke who preached that observation using ones senses was the best way to analyze and understand a phenomenon. Foucalt, however, criticized this shift in his book \"The Birth of the Clinic,\" stating that this shift took attention away from the patient and objectified patients, ultimately resulting in a loss of the patient's narrative. He argued that from this point forward, in the eyes of doctors, patients lost their humanity and became more like objects for inspection and examination.\n\nThe next advancement in Paris medicine came with the creation of an examination system, that after 1803, was required for the licensing of all medical professions creating a uniform and centralized system of licensing. This law also created another class of health professionals, mostly for those living outside of cities, who did not have to go through the licensing pricess but instead went through a simpler and shorter training process.\n\nAnother area influenced by Paris hospitals was the development of specialties. The structure of a Paris hospital allowed physicians more freedom to pursue interests as well as providing the necessary resources. An example of a physician who used this flexibility to conduct research is Phillipe Pinel who conducted a four-year study on the hospitalization and treatment of mentally-ill women within the Salpêtriére hospital. This was the first study ever done of this magnitude by a physician, and the Pinel was the first to realize that patients dealing with similar illnesses could be group together, compared, and classified.\n\nThe Protestant churches reentered the health field in the 19th century, especially with the establishment of orders of women, called deaconesses who dedicated themselves to nursing services. This movement began in Germany in 1836 when Theodor Fliedner and his wife opened the first deaconess motherhouse in Kaiserswerth on the Rhine. It became a model, and within half a century there were over 5,000 deaconesses in Europe. The Church of England named its first deaconess in 1862. The North London Deaconess Institution trained deaconesses for other dioceses and some served overseas.\n\nWilliam Passavant in 1849 brought the first four deaconesses to Pittsburgh, in the United States, after visiting Kaiserswerth. They worked at the Pittsburgh Infirmary (now Passavant Hospital).\n\nThe American Methodists made medical services a priority from the 1850s. They began opening charitable institutions such as orphanages and old people's homes. In the 1880s, Methodists began opening hospitals in the United States, which served people of all religious beliefs. By 1895, 13 hospitals were in operation in major cities.\n\nIn Quebec, Catholics operated hospitals continuously from the 1640s; they attracted nuns from the provincial elite. Jeanne Mance (1606–73) founded Montreal's city's first hospital, the Hôtel-Dieu de Montréal, in 1645. In 1657 she recruited three sisters of the Religious Hospitallers of St. Joseph, and continued to direct operations of the hospital. The project, begun by the niece of Cardinal de Richelieu was granted a royal charter by King Louis XIII and staffed by a colonial physician, Robert Giffard de Moncel. The General Hospital in Quebec City opened in 1692. They often handled malaria, dysentery, and respiratory sickness.\n\nIn the 1840s–1880s era, Catholics in Philadelphia founded two hospitals, for the Irish and German Catholics. They depended on revenues from the paying sick, and became important health and welfare institutions in the Catholic community. By 1900 the Catholics had set up hospitals in most major cities. In New York the Dominicans, Franciscans, Sisters of Charity, and other orders set up hospitals to care primarily for their own ethnic group. By the 1920s they were serving everyone in the neighborhood. In smaller cities too the Catholics set up hospitals, such as St. Patrick Hospital in Missoula, Montana. The Sisters of Providence opened it in 1873. It was in part funded by the county contract to care for the poor, and also operated a day school and a boarding school. The nuns provided nursing care especially for infectious diseases and traumatic injuries among the young, heavily male clientele. They also proselytized the patients to attract converts and restore lapsed Catholics back into the Church. They built a larger hospital in 1890. Catholic hospitals were largely staffed by Catholic orders of nuns, and nursing students, until the population of nuns dropped sharply after the 1960s. The Catholic Hospital Association formed in 1915.\n\n\n\n\n\n"}
{"id": "10451871", "url": "https://en.wikipedia.org/wiki?curid=10451871", "title": "Hosteen Klah", "text": "Hosteen Klah\n\nHosteen Klah (, 1867– February 27, 1937) was a Navajo artist and medicine man. He documented aspects of Navajo religion and related ceremonial practices. He was also a master weaver.\n\nHosteen Klah, also spelled Hastiin Klah, was born in 1867 at Bear Mountain, near Fort Wingate, New Mexico. His name essentially means Sir Left Handed in Navajo; in his youth, he was called Ahway Eskay (). Hoksay Nolyae was his father and Ahson Tsosie, of the Clan, was his mother. Klah was born after the Navajos’ return to their homelands from forced government internment at Bosque Redondo. Klah avoided attending government school; rather, he received training in the traditionally male realm of ceremonial practices (chanting and sandpainting) from his uncle. While most individuals master only one or two complete chants, Klah mastered at least eight.\n\nHosteen Klah is most commonly believed to be intersex. Klah was important to the development of Navajo weaving. Among the Navajo, weavers are typically women, and chanters () are normally male. Hosteen Klah was both a weaver and a chanter. This was possible because of his particular gender status. Klah was a \"\" (meaning \"one who is transformed\" or \"one who changes\"). A could be born male, female, or intersex.\n\nIdentified as a in adolescence, Klah began his training in the traditionally female craft of weaving with his mother and sister in the 1880s. Klah wove his first complete weaving at the 1892–1893 World's Columbian Exposition in Chicago, where he was probably part of a sandpainting demonstration.\n\nIn 1916, Klah wove imagery from the dance into a rug. He incorporated more representations of Navajo religion into his weaving, including sandpainting imagery by 1919. This practice was regarded as sacrilegious by many Navajo traditionalists and is regarded as such by some Navajo people today.\n\nKlah taught his two nieces both his weaving techniques and designs.\n\nIn 1921, Hosteen Klah was introduced to Mary Cabot Wheelwright, a Boston heiress. The two became friends and collaborated in founding the Wheelwright Museum of the American Indian in Santa Fe, New Mexico. Fearing for the future of Navajo religion after witnessing decades of assimilationist assaults on traditional culture by missionaries and the US government, Klah wanted to document Navajo religion and make it available for future generations. The museum was initially called the Navajo House of Prayer and House of Navajo Religion, but then renamed the Museum of Navajo Ceremonial Art, and ultimately renamed in 1977, when the museum repatriated sensitive cultural patrimony back to the Navajo Nation.\n\nHosteen Klah died on February 27, 1937 from pneumonia, and he is buried on the grounds of the Wheelwright Museum.\n\n\n"}
{"id": "40200219", "url": "https://en.wikipedia.org/wiki?curid=40200219", "title": "International Federation for Emergency Medicine", "text": "International Federation for Emergency Medicine\n\nThe International Federation for Emergency Medicine (IFEM) is an organisation promoting international emergency medicine around the world. It is a consortium of over 60 national emergency medicine organisations. IFEM organises the International Conference on Emergency Medicine (\"ICEM\").\n\nInternational emergency medicine has been defined as \"the area of emergency medicine concerned with the development of emergency medicine in other countries.\" In that definition, \"other countries\" refers to nations that do not have a mature emergency care system (exemplified by board certified emergency physicians and academic emergency medicine, among other things). Included in those nations are some that are otherwise quite developed but lack a complete emergency medical system, such as Armenia, China, Israel, Nicaragua, and the Philippines. Work in international emergency medicine can be broken down into two main categories: 1) the promotion of emergency medicine as a recognized and established specialty in other countries, and 2) The provision of humanitarian assistance.\n\nHowever, William Burdick, Mark Hauswald, and Kenneth Iserson have criticized the above definition for being an oxymoron, given the international nature of medicine and the number of physicians working internationally. From their point of view, international emergency medicine is not solely about development of emergency medical systems but is instead better described as the training required for, as well as the reality of, practicing the specialty abroad from one's native country.\n\nThe precursor to the International Foundation for Emergency Medicine (IFEM) was the International Conference on Emergency Medicine (ICEM), which was first held in 1986 in London. The idea of having such a conference had been first suggested by William Rutherford and Gautam Bodiwala at the Scientific Assembly of the American College of Emergency Physicians (ACEP) in 1984. The only four organizations present at this first conference were ACEP, the British Association for Emergency Medicine (BAEM), the Canadian Association of Emergency Physicians (CAEP), and the Australasian College for Emergency Medicine (ACEM). In 1988, at the second ICEM in Brisbane, Australia, the idea of creating an international emergency medicine organization was first proposed. The following year, the four organizations agreed to form IFEM, although their presidents did not sign the charter until 1991. Its initial purpose was to assist them in running ICEM, which was important because it allowed \"the founding nations to share experiences, network, develop collaborations and assist trainees and specialists to rotate between countries and learn new approaches to old problems.\"\n\nIn 1998, IFEM opened up membership to other national emergency medicine organizations. Since then, membership has grown rapidly, with 15 additional national emergency medicine organizations joining by 2003, bringing the total number of members to 19. By 2010, IFEM had more than 40 members. As of 2013, IFEM represents over 60 national emergency medicine organizations.\n\nThe stated mission of IFEM is \"to promote at an international level interchange, understanding and cooperation among physicians practicing emergency medicine.\"\n\nIt also has nine principles and sixteen aims, all of which be summarized by IFEM's general aim \"to be recognized as the international voice for quality emergency medical care.\"\n\nAdditionally, it supports the following principles: \n\nIFEM has three levels of membership: full members, affiliate members, and ex officio members.\n\nA full member is defined as \"the leading national emergency medicine organization for physicians in a country in which emergency medicine is officially recognized as a medical specialty and where there also exists at least one recognized training program in emergency medicine.\" They may appoint one representative to the IFEM Board; this representative has one vote. Full members can apply to host ICEM. As of 2013, there are 34 full members.\n\nAn affiliate member is defined as \"any national emergency medicine organization for physicians practicing emergency medicine in a country where the specialty of emergency medicine is not yet officially recognized or residency equivalent training programs in emergency medicine do not yet exist.\" They may also be any additional national emergency medicine organization for physicians practicing emergency medicine from a country already represented by a full or founding member. Affiliate members may appoint one non-voting member to the IFEM Board. Like full members, they can apply to host ICEM.\" As of 2013, there are 14 affiliate members.\n\nAn \"ex officio\" member is defined as a multinational emergency medicine organisation. They may appoint one non-voting member to the IFEM Board. As of 2013, there are 5 \"ex officio\" members.\n\nAn organization from a country with a per capita GNI below a certain threshold will have its annual subscription waived.\n\nIFEM was incorporated as a public company limited by guarantee in Australia in 2010. As of 2013, it has three components to its governing structure: a board, an executive, and an assembly.\n\nUntil 2011, the IFEM Board was composed of a representative from each member country, with full members having one voting representative and affiliate and \"ex officio\" members having one non-voting representative. By that point, IFEM has over 50 full, affiliate, and \"ex officio\" members, resulting in a board whose size was unwieldy. It was then agreed that a new board of 12 members would be established in 2012. Six of these members consist of office-bearers from the IFEM Executive, with the other six being representatives from six geographic regions: Africa, Australasia, Europe, North America, and South America.\n\nThe IFEM Executive was established in 2006 to improve responsiveness. It consists of six office-bearers: a president, vice-president, president-elect, secretary, treasurer, and member-at-large. As of 2013, the president is Peter Cameron, the vice-president is James Ducharme, the president-elect is C. James Holliman, the secretary is Robert Schafermeyer, the treasurer is Andrew Singer, and the member-at-large is Hiu-fai Ho.\n\nTo replace the former Board, which previously represented all members, the IFEM Assembly was created. It is composed of one voting representative from each founding member and one voting representative from each full member. Representatives from affiliate members may attend and participate in Assembly meeting but do not have voting rights. The Assembly’s primary purpose is to ratify or reject proposals brought forward by either the board or the executive. Thus, it has the final decision-making power for major issues related to the IFEM.\n\nThe IFEM Board can establish committees to perform focused work on behalf of IFEM. Committees established by the board include a finance committee, a core curriculum and education committee, nominations committee, specialty implementation committee, governance committee, clinical practice committee, and research committee. Membership on a committee is open to anyone who is part of an IFEM member society or college.\n\nIFEM has four special interest groups: pediatric emergency medicine, triage, ultrasound, and disaster medicine. Each group reports to the clinical practice committee. Unlike committees, \"other interested individuals\" who are not part of an IFEM member society or college may join special interest groups.\n\nIFEM offers two main awards: Order of the IFEM and IFEM Humanitarian Award. The Order of the IFEM (FIFEM) was established in 1999 \"to recognise those individuals who have contributed significantly to the development of emergency medicine in their country and to the development of the International Federation for Emergency Medicine.\" More than 70 Fellows of the IFEM have been inducted since 2000, when it was first awarded at the 8th ICEM; new fellows are inducted biennially at the ICEM. The IFEM Humanitarian Award was established in 2000 and is given to either an individual physician or an organization whose work \"has a major humanitarian or public health benefit.\" It has been awarded 11 times since it was first inaugurated in 2004 at the 10th ICEM, twice to organizations and nine times to individuals.\n\nOne of the goals of IFEM is to equip nations to develop emergency medical systems, and one key component of doing so is to identify the aspects of training that are essential for health care providers. Although countries have very different needs and resources, a standard curriculum is still useful for identifying core issues. Thus, IFEM developed a model curriculum in 2009. This initiative seeks to provide a minimum basic standard that can be tailored to the specific needs of the various nations implementing training in emergency medicine. It is targeted towards all medical students in order to produce a minimum competency in emergency care for all physicians, regardless of their specialty.\n\n\"Emergency Medicine Journal\" calls ICEM a major international emergency medicine conference, while Kumar Alagappan and C. James Holliman refer to IFEM as \"probably the most active, broad-based, international organization dealing with international EM [emergency medicine] development issues.\" The IFEM has played a part in helping to increase global acceptance of the specialty of emergency medicine.\n\nAdditionally, IFEM serves as the \"umbrella group\" for all of the national and regional societies representing international emergency medicine. It has also been described as \"the peak EM [emergency medicine] in the world\".\n"}
{"id": "47618351", "url": "https://en.wikipedia.org/wiki?curid=47618351", "title": "Japan Confederation of A- and H-Bomb Sufferers Organizations", "text": "Japan Confederation of A- and H-Bomb Sufferers Organizations\n\nThe is a group formed by \"hibakusha\" in 1956 with the goals of pressuring the Japanese government to improve support of the victims and lobbying governments for the abolition of nuclear weapons.\n\n\n\n"}
{"id": "58189583", "url": "https://en.wikipedia.org/wiki?curid=58189583", "title": "Jónína Kristín Berg", "text": "Jónína Kristín Berg\n\nJónína Kristín Berg (born 3 September 1962) is an Icelandic art teacher, aromatheratist and neopagan leader. She is the regional \"gothi\" of the Western Region for Ásatrúarfélagið since 1996 and acted as interim \"allsherjargoði\" in 2002-2003.\n\nShe was born in Akranesi and grew up in Giljahlíð in Flókadal, Borgarfjörður. She was educated at the Reykjavík School of Visual Arts, from 1985 at the Icelandic Academy of Fine Arts and Crafts where she completed her degree in 1989, then studied at the school's graphic department until 1990, after which she studied at the Reykjavík University where she completed her teaching degree. She has also studied massage and essential oil therapy at Lífsskólann – Aromatherapyskóla Íslands from which she graduated in 2002. She has worked with individual training for disabled people in Kópavogur, but since 2004 lives in Borganesi where she works as a primary school art teacher. She has also participated in group exhibitions with her own art. She performs aromatherapy and has served on the Board of Icelandic Healers.\n\nShe followed Ásatrúarfélagið from its inception as she was a neighbour of the co-founder Sveinbjörn Beinteinsson. She became active in the organization in 1985. In 1993 she was elected as a board member. Since 1994 she is responsible for annual blóts in Snæfellsnes and Borgarfjörður and since 1996 she is the regional \"gothi\" of the Western Region.\n\nIn 2002 she became the interim \"allsherjargoði\" after a conflict resulted in Jörmundur Ingi Hansen's removal from the office. She was succeeded by Hilmar Örn Hilmarsson in 2003 after a regular election could be held.\n\nShe has legal rights to perform marriages.\n"}
{"id": "10091083", "url": "https://en.wikipedia.org/wiki?curid=10091083", "title": "Kalasa-Banduri Nala project", "text": "Kalasa-Banduri Nala project\n\nThe Kalasa-Banduri Nala is a project undertaken by the Government of Karnataka to improve drinking water supply to the Districts of Belagavi, Dharwad and Gadag. It involves building across Kalasa and Banduri, two tributaries of the Mahadayi river to divert 7.56 TMC of water to the Malaprabha river, which supplies the drinking water needs of the said 3 districts, i.e., Dharwad, Belagavi and Gadag.\nThis project had been on paper for decades and the Karnataka government decided to implement it during S M Krishna's regime. Clearance for the project was received from the center in 2002. The project however, soon ran into trouble when the then Bharatiya Janata Party (BJP) government of Goa headed by Manohar Parrikar raised objections to the project claiming that the project would harm Goa's flora and fauna. Following this, the then National Democratic Alliance (NDA) government which was in power at the center put on hold its approval and funding of the project.\n\nKarnataka and Goa have since then been at loggerheads over the project. The project which had been put in abeyance for four years was revived after H. D. Kumaraswamy became the Chief Minister. In September 2006, the coalition government in Karnataka headed by Chief Minister H. D. Kumaraswamy decided to go ahead with the construction work. It was inaugurated on September 22 at Kanakumbi in Khanapur Taluk of Belgaum District. The Goa government, however, soon moved the courts and brought a stay on the construction work. The matter is now before the Mahadayi Water Disputes Tribunal.\n\nProtest to Implement the Project<br>\nCommunist Party of India(Marxist) organized a rally in Belagavi on 16th Sept, 2015 to implement this long time pending project. CPI(M) State Secretary GV Srirama Reddy along with his colleague Swamy, Varalaxmi and district secretary G.M. Jaine Khan lead this rally. GV Srirama Reddy said \"The present imbroglio on the projects to the long pursued politics between the Congress and the BJP in Karnataka and Goa.\" Srirama Reddy was critical of Prime Minister Narendra Modi for throwing the ball in the courts of Congress instead of taking a lead to resolve the dispute: “It is unbecoming of a Prime Minister to take a political stand based on his political differences with Congress on an issue concerning people.”.\nA statewide bandh was organized by pro-Kannada organizations on Sept 26, 2015 demanding the implementation of the project.\nThe agitation in the malaprabha catchment area under various organisations has now reached 265 days.\n\nA report in \"The Hindu\"<br>\nBhoomi Pooja for Kalasa-Banduri\n"}
{"id": "34255164", "url": "https://en.wikipedia.org/wiki?curid=34255164", "title": "Mental Health (Care and Treatment) Act (Singapore)", "text": "Mental Health (Care and Treatment) Act (Singapore)\n\nThe Mental Health (Care and Treatment) Act (Singapore) was passed in 2008 to regulate the involuntary detention of a person in a psychiatric institution for the treatment of a mental disorder, or in the interest of the health and safety of the person or the persons around him.\n\nForm 1 in the first schedule of the act empowers a designated medical practitioner at a psychiatric institution to admit a person suffering from a mental disorder into the psychiatric institution, and detain the patient for up to 72 hours.\n\nA patient that has been admitted under Form 1, may be detained for a further period of 1 month after the expiration of the 72-hour period under the provision of Form 2 in the first schedule of the act, if the following requirements are fulfilled:\n\nForm 3 allows for a patient detained under form 2 to be further detained for up to 6 months if:\n\nForm 4 of the Act allows for visitors to apply to a Magistrate for Form 5.\nThe visitors are appointed by the Minister of Health. At least 12 visitors are appointed at any time, and at least 6 have to be medical practitioners. 2 or more visitors have to visit the designated psychiatric institution at least once every 3 months.\n\nForm 5 in the First Schedule of the act is an order made by a Magistrate, which allows for the patient to be detained for a further 12 months.\n"}
{"id": "53617225", "url": "https://en.wikipedia.org/wiki?curid=53617225", "title": "Muhammad Akram (blind cricketer)", "text": "Muhammad Akram (blind cricketer)\n\nMuhammad Ahmed Akram is a Pakistani blind cricketer who has represented his country in the inaugural edition of the 2012 Blind T20 World Cup where his team ended up as runnersup. During that world cup, he set the world record for the highest ever individual score by any blind cricketer in a T20I as well as in Blind T20 World Cup history(264)\n"}
{"id": "2019238", "url": "https://en.wikipedia.org/wiki?curid=2019238", "title": "NHS Digital", "text": "NHS Digital\n\nNHS Digital is the trading name of the Health and Social Care Information Centre, which is the national provider of information, data and IT systems for commissioners, analysts and clinicians in health and social care in England, particularly those involved with the National Health Service (England). \n\nIt provides digital services for the NHS and social care, including the management of large health informatics programmes. They deliver national systems through in-house teams, and by contracting private suppliers. These services include managing patient data including the Spine, which allows the secure sharing of information between different parts of the NHS, and forms the basis of the Electronic Prescription Service, Summary Care Record. and Electronic Referral Service.\n\nNHS Digital is also the national collator of information about health and social care, and publishes over 260 statistical publications each year, including Official Statistics and National Statistics. It also runs the NHS.UK (formerly NHS Choices) website, which is the national website for the NHS in England.\n\nThe organisation is an executive non-departmental public body of the Department of Health, and was re-branded as NHS Digital on 1 August 2016.\n\nNHS Digital has taken on the roles of a number of predecessor bodies including the NHS Information Centre, NHS Connecting for Health, and parts of NHS Direct. The organisation produces more than 260 official and national statistical publications. This includes national comparative data for secondary uses, developed from the long-running Hospital Episode Statistics which can help local decision makers to improve the quality and efficiency of frontline care.\n\nThe organisation was created as a special health authority on 1 April 2005 by a merger of parts of the Department of Health, parts of the NHS Information Authority, and the Prescribing Support Unit.\n\nFollowing the Health and Social Care Act 2012, the HSCIC changed from a special health authority to an executive non-departmental public body (ENDPB) on 1 April 2013. Effective at this time, HSCIC took over parts of the troubled NHS National Programme for IT (NPfIT) from the agency NHS Connecting for Health (CfH) which ceased to exist. It also runs the Health Survey for England (HSE).\n\nOn 20 April 2016, it was announced that HSCIC would be rebranding, changing its name to NHS Digital in July 2016.\n\nNHS Digital runs the Spine service for the NHS, which is a central, secure system for patient data in England. This enables a number of services for patients, including:\n\nAs the HSCIC, the organisation ran the care.data programme, which was cancelled in 2016.\n\nNHS Digital collects the national 'Hospital Episode Statistics', which is a record of every 'episode' of admitted patient care (counted by completing care with a consultant, meaning that more than one episode can be associated with a single stay in hospital) delivered by the NHS in England, including those done under contract by private providers. This involves the tracking of around 16 million 'episodes' of care every year. This information is used for a range of statistical analysis, as well as for determining payments to providers.\n\nA troubleshooting operation was established in 2018 to help NHS trusts when major IT deployments go wrong. Eight trusts needed emergency assistance in 2018 after a deployment led to severe service disruptions. Funding of £2 million a year for the service has been allocated and expansion is expected.\n\nNHS Digital compiles national data about the NHS and social care, with over 260 publications every year. In addition, they provide data analysis, and access to data and clinical indicators.\n\nNHS.UK (formerly NHS Choices) is the public website for the NHS Services in England, and is run by a team at NHS Digital; mandated by DOH with input from PHE.\n\n"}
{"id": "52794627", "url": "https://en.wikipedia.org/wiki?curid=52794627", "title": "Newport News asbestos litigation", "text": "Newport News asbestos litigation\n\nIn Newport News, Virginia, asbestos litigation is driven by the presence of Newport News Shipbuilding and other defense contractors. Asbestos was widely used in the shipbuilding industry.\n\nAbout 500 people have died due to asbestos exposure at the Newport News shipyard.\n\nIn March 2011, a jury in Newport News awarded about $25 million to a former shipyard worker named Rubert Minton. Minton had worked on 17 different Exxon tankers over the course of his career. Decades later he began to suffer from mesothelioma. He filed suit against Exxon in 2009. Minton's lead attorney was Robert Hatten. Hatten said the award would be reduced to about $17.5 million because the $12.5 million in punitive damages awarded by the jury exceeded the $5 million that had been demanded by Minton.\n\nSuits against ship owners were a novel legal development. Most of the asbestos suits in Newport News had previously been brought against parts makers. Shipyards have immunity from asbestos suits under worker's compensation laws. Exxon said the shipyard was solely responsible for the safety of its workers and that there was no proof on its ships. Minton lawyers said Exxon new about health risks from asbestos in the 1930s and created rules to protect its own workers but did nothing to warn shipyard workers.\n\nGibbs was an active-duty member of the United States Navy when he was exposed to asbestos after being ordered to take part in pre-commission tests of a nuclear submarine. He later developed mesothelioma and died in 2009. Gibbs sued Newport News in 2008. His wife continued that suit and sued for wrongful death.\n\nThe state circuit court dismissed her case. The court ruled that worker's compensation was her exclusive remedy. The Virginia state supreme court overturned the circuit court's decision in 2012. It found that because the Navy was not liable under worker's compensation Gibbs could file suit against Newport News.\n\nThe American Tort Reform Association (ATRA) has named Newport News a \"Judicial Hellhole\" for allegedly applying the law in biased manner in favor of plaintiffs. Plaintiffs in Newport News have roughly an 87% success rate. Robert Hatten, a leading asbestos attorney in Newport News of the firm Patten, Wornom, Hatten & Diamondstein, said the ATRA was engaging in \"propaganda.\"\n"}
{"id": "11270361", "url": "https://en.wikipedia.org/wiki?curid=11270361", "title": "Nurse's cap", "text": "Nurse's cap\n\nA nurse's cap or nursing cap is part of the female nurse's uniform, introduced early in the history of the profession. The cap's original purpose was to keep the nurse's hair neatly in place and present a modest appearance. Male nurses do not wear caps.\n\nIn some schools, a \"capping ceremony\" presents new nursing students their caps before beginning their clinical (hospital) training.\n\nThe nurse's cap originated from a group of women in the early Christian era, called \"deaconesses.\" Deaconesses are now recognized as religious order nuns. These women were distinguished from other women during this time by white coverings worn on their heads. This particular head covering was worn to show that this group of women worked in the service of caring for the sick. Originally, this head covering was more of a veil, but it later evolved into a white cap during the Victorian era. It was during this era that proper women were required to keep their heads covered. The cap worn was hood-shaped with a ruffle around the face and tied under the chin, similar to cleaning ladies of that day. Long hair was fashionable during the Victorian era, so the cap kept the nurse's hair up and out of her face, as well as keeping it from becoming soiled.\n\nThe nurse's cap was derived from the nun's habit and developed over time into two types:\nThe nursing cap was originally used by Florence Nightingale in the 1800s.\n\nDifferent styles of caps were used to depict the seniority of the nurse, the frillier and longer the more senior the nurse.\n\nThe nursing cap is a nearly universally recognized symbol of nursing. It allows patients to quickly identify a nurse in the hospital from other members of the health team.\n\nSome claim the cap is a potential carrier of bacteria and other disease-causing pathogens that could then be transmitted from patient to patient. However, such incidents can be prevented when infection control procedures are followed.\n\nAround 1874, the Bellevue Hospital School of Nursing in New York City adopted a special nursing cap as a way to identify nurses who had graduated from Bellevue. The Bellevue cap covered the entire head except the ears, and can be compared to a current ski hat, although it was made out of white linen and had fringe around the bottom. As the number of nursing schools increased, so did the need for unique caps. Each nursing school decided to design their own style of nurse’s cap. Some became very elaborate and some were even different shapes. Because each school had their own cap, it became very easy to determine which school the nurse had graduated from.\nIt was common for a black stripe (usually a black velvet ribbon) on the cap to signify a Registered Nurse. In some regions, two thinner stripes were used to signify the award of a Bachelors of Science in Nursing (BSN). The caps needed to be washed regularly and the black stripe(s) needed to be easy to remove and reattach. Water soluble lubricants such as KY jelly became solid when dried and were plentiful in hospitals. Nurses often used a thin layer of these lubricants applied to the back of the ribbon to attach stripes to their caps.\n\nIn a global perspective, the nurses' cap continues to be widely used. However, the use of the nurses' cap had begun to slowly decline in Western Europe and Northern America by the mid 1970s. The use of nurses' caps in the medical facilities of the United States all but disappeared by the late 1980s with the near universal adoption of \"scrubs.\" \n\nIn areas where healthcare facilities no longer required their nurses to wear nurse’s caps, nursing schools eliminated the cap as a mandatory part of the students’ uniform. In addition, with the growth of technology in the health-care setting, some felt that the nurse’s caps was an obstacle for nurses wearing them, while others disagreed. Also, with the rapid growth of the number of men in nursing, some felt a need for a unisex uniform, while others saw no difficulty with gender specific uniforms as is the case in many uniformed professions. However, nurses' caps can still be found in many developing and developed nations. Japan and South Korea are examples of developed countries with near universal use of the nurses' cap. It is also common for students of nursing to have their graduation portraits taken while wearing nurses' caps.\n\nIn countries where the nursing cap is no longer required as a part of a nurse’s uniform, it still holds the same significance that it did during the time of Florence Nightingale. The nursing cap symbolizes the goal of the nurse, which is to provide “service to those in need.” Furthermore, the cap is a sign of the industry’s ageless values of dedication, honesty, wisdom, and faith.\n\n\n"}
{"id": "15409459", "url": "https://en.wikipedia.org/wiki?curid=15409459", "title": "Oklahoma Office of the Chief Medical Examiner", "text": "Oklahoma Office of the Chief Medical Examiner\n\nThe Office of the Chief Medical Examiner (OCME) is the agency of the government of Oklahoma responsible for investigating sudden, unexpected, violent or suspicious deaths. In this capacity, OCME provides support services to State law enforcement agencies, prosecutors, and public health officials. \n\nThe Office of the Chief Medical Examiner was created by the Oklahoma Legislature in 1961 but did not receive funding to become operational until 1967. The Chief Medical Examiner must be a licensed physician, trained and certified in forensic pathology.\n\nThe Board of Medicolegal Investigations is responsible for governing the operations of OCME and for setting policy which the Chief Medical Examiner executes. It is the responsibility of the Board to appoint and dismiss the Chief Medical Examiner. All members of the Board receive no compensation for their services.\n\n\nThe Board appoints a Chief Medical Examiner, who is the chief executive of the OCME.\n\nUnder Oklahoma statute, deaths of the types listed below must be reported to the Office of the Chief Medical Examiner. The Office has the authority to further investigate for the benefit of the public health, as funding, staffing, facilities, and other regulations allow. \n\nThe Office of the Chief Medical Examiner was authorized a budget of $6.2 million for State fiscal year 2012. The Oklahoma Legislature authorized the agency to employ 77 full time equivalent positions for that period.\n\n\n"}
{"id": "12878882", "url": "https://en.wikipedia.org/wiki?curid=12878882", "title": "Omegaven", "text": "Omegaven\n\nOmegaven is a fatty acid emulsion produced by Fresenius Kabi. It is used for total parenteral nutrition (feeding directly into a venous catheter), e.g. in short bowel syndrome. It is rich in omega-3 fatty acids.\n\nIt has gained popularity in children in preference to the more commonly used Intralipid after case reports that it reduced the risk of liver damage.\n\nA recent study indicated that the use of Omegaven may be an appropriate intervention strategy for newborns with a very low birth weight, gastrochisis, and jejunal atresia.\n\nIt is currently undergoing a clinical trial at National Taiwan University Hospital.\n\nAlthough the use of Omegaven in children in the United States is experimental, the use of it in adults in Europe is less controversial. In European studies, Omegaven has been associated with a reduction in psoriasis, when contrasted to administration of omega-6 fatty acid Lipoven. Omegaven has also been associated with reduced mortality and antibiotic use during hospital stays.\n\n"}
{"id": "22279932", "url": "https://en.wikipedia.org/wiki?curid=22279932", "title": "Orin Levine", "text": "Orin Levine\n\nOrin Levine is a recognized expert in the fields of international public health, child survival, and pneumonia. He is currently the Director of Vaccine Delivery at the Bill & Melinda Gates Foundation in Seattle, USA. In the past he was the Executive Director of the International Vaccine Access Center (IVAC), the Co-Chair of the Pneumococcal Awareness Council of Experts (PACE), and is a Professor at The Johns Hopkins Bloomberg School of Public Health in the Department of International Health. He is also an adjunct assistant professor of Epidemiology at The Rollins School of Public Health at Emory University in Atlanta. Additionally, he is currently president of the American Society of Tropical Medicine and Hygiene (ASTMH) Council on Global Health. He resides in Washington, DC.\n\nOrin Levine was born in Richmond, Virginia. He graduated with a bachelor's degree from Gettysburg College in Gettysburg, Pennsylvania. He continued his studies at The Johns Hopkins Bloomberg School of Public Health in Baltimore, Maryland, where he received a PhD in epidemiology.\n\nAfter receiving his PhD, Levine spent 5 years working for The Centers for Disease Control and Prevention (CDC) in Atlanta. There, he served first as an Epidemic Intelligence Service officer, and then as a staff epidemiologist in the Respiratory Diseases Branch. He then spent 3 years working at the National Institutes of Health in Bethesda, Maryland. In 2003, he joined the Department of International Health at the Johns Hopkins Bloomberg School of Public Health, and started (with Katherine O'Brien) the PneumoADIP, a small, focused organization dedicated to accelerating access to pneumococcal vaccines for the world's poorest children. In the past 6 years, Levine and the PneumoADIP team have successfully competed for and been awarded over 100 million dollars in research grants from the GAVI Alliance and The Bill & Melinda Gates Foundation. He's now at the Gates Foundation in Seattle.\n\nAs the executive director of the International Vaccine Access Center (IVAC), Levine leads nearly 30 professionals with experience in epidemiology, economics, policy analysis, financing, and advocacy. The mission of IVAC is to accelerate global access to vaccines through the development and implementation of evidence-based policies. As a consultant who works both nationally and internationally, Levine's knowledge of epidemiology places him in high demand. Levine's professional memberships align with his special interests. Currently, he is a steering committee member of Johns Hopkins Vaccine Initiative (JHVI), at the Johns Hopkins Bloomberg School of Public Health, and President of the American Society of Tropical Medicine & Hygiene's Committee on Global Health. He is also a steering committee member of the Decade of Vaccines Collaboration, and the co-chair of the Decade of Vaccines Collaboration's Global Access Working Group.\n\nLevine has authored over 100 peer-reviewed publications and book chapters. He also writes a regular blog for the Huffington Post on global health and vaccine-related issues, and appears regularly on radio, television, and in print as an expert authority on these topics.\n\nLevine's research has predominantly been focused on the two most common causes of fatal pneumonia, S. pneumoniae and H. influenzae type b. Pneumonia kills over 2 million children each year and >98% of these deaths occur in developing countries. Pneumococcal and Hib pneumonia are preventable by vaccination, but these vaccines have not reached the children who needed them the most: those in developing countries. Levine's efforts, and those of PneumoADIP, are aimed at accelerating access to these life-saving vaccines so that children everywhere can benefit from them.\n\nTwo of his current research projects are: Pneumonia Etiology Research for Child Health (PERCH), which is funded by a 43 million dollar grant from the Bill and Melinda Gates Foundation and the Accelerated Vaccine Introduction (AVI) Project, which is funded by the GAVI Alliance. The purpose of PERCH is to achieve a greater understanding of the causes of pneumonia around the world. Better pneumonia surveillance is critical to the global fight against this disease. The purpose of AVI is to accelerate access to life-saving pneumococcal and rotavirus vaccines for children in the world's poorest countries. Levine is also involved in dengue vaccines, acting as the lead at Johns Hopkins in the institution's involvement in the Dengue Vaccine Initiative, a consortium created to accelerate development and subsequent use of dengue vaccines.\n\nPneumonia is the leading infectious disease cause of death among children worldwide. Existing knowledge on the etiologies of childhood pneumonia is largely based on studies conducted in the 1980s and 1990s. With expanded use of new pneumonia vaccines and changes in host and environmental factors, a new evidence base that harnesses novel diagnostic technologies is needed.\n\nPERCH is a multi-center, case-control study of the etiology of severe and very severe pneumonia in children aged 1–59 months. With seven sites in Africa and Asia and a projected sample size of over 12,000 patients and controls, PERCH is the largest study of the etiology of pneumonia among hospitalized pediatric pneumonia patients in developing countries that has ever been undertaken. Over two years, PERCH will enroll approximately 6,500 hospitalized cases with WHO-defined severe and very severe pneumonia. Approximately the same number of community-based controls will be enrolled to assess the community prevalence of pathogens, as well as known and unstudied risk factors for pneumonia. Seven research sites were selected to be representative of areas where most of the severe pneumonia cases in children are expected to occur in 2015. There are five sites in Africa (Gambia, Mali, Kenya, South Africa, and Zambia) and two in Asia (Bangladesh and Thailand). The study will be carried out using standardized clinical and laboratory methods and techniques, in combination with innovative diagnostic and specimen collection methods. The data will be analyzed using advanced statistical methods and their interpretation will be considered prior to the release the specific microbiologic results.\n\nMore information on the study protocol and the PERCH project is available at http://www.jhsph.edu/ivac/perch.html.\n\nThe mission of the Hib Initiative is to expedite and execute evidence-based decisions regarding the use of Hib vaccination in order to prevent childhood meningitis and pneumonia. The Hib Initiative strategic plan provides a roadmap to fulfill this mission, and focuses on three strategic areas to support evidence-based decisions at the country level: coordination, communication and research.\n\nIn conjunction with the Medical Research Council (MRC) in Gambia, the team will coordinate and perform a range of surveillance and research activities designed to determine the burden of pneumococcal infections among and demonstrate the impact of pneumococcal conjugate vaccines on children and adults in the Upper River Division in The Gambia. The surveillance and post-vaccination evaluation being conducted by MRC Gambia is a multi-year project.\n\nIn conjunction with the University of Oxford, this team will coordinate and perform a range of activities designed to estimate the total reduction in disease burden attributable to programmatic use of pneumococcal conjugate vaccine and to evaluate the efficacy of the vaccine following introduction into the routine childhood immunization schedule. The pneumococcal surveillance and post-vaccine introduction evaluation being conducted is a multi-year project.\n\nThis program was conceived to develop an evidence-based, rigorous, and transparent process for collecting, analyzing and interpreting the evidence for pneumococcal serotype replacement and placing it in this larger context. The approach is to develop hypothesis-driven analyses and to evaluate them on the basis of comparable data from different settings in order to formulate a set of “best-practice principles” for pneumococcal disease surveillance in developing countries planning to introduce PCV. In this way, new data on pneumococcal vaccine impact may be collected and reported in ways that are useful in understanding the true effects of PCV. The process of determining the proportionate role of serotype replacement in post-pneumococcal conjugate vaccine nonvaccine-serotypes invasive pneumococcal disease changes will provide a framework for countries to evaluate their own data and make decisions related to their PCV immunization program.\n\nThe project aims to evaluate the utility of the Binax NOW® S. pneumoniae immuno-chromatographic test, for diagnosing pneumococcal pneumonia and sepsis in low- and middle-income countries. Detection of pneumococcal pneumonia and sepsis is limited by the poor sensitivity of existing diagnostic methods, hindering accurate measurement of disease burden where the Binax NOW® test may enable the identification of additional pneumococcal cases among patients with pneumonia and sepsis.\n\nThe project aims to identify the key supply- and demand-side bottlenecks to routine immunization coverage in Nigeria and determine drivers of low coverage and inequalities. Specific aims include to describe the landscape and status of previous and current routine immunization strengthening programs in Nigeria and to identify context-specific opportunities and strategies for improving immunization service access and delivery, utilization, uptake and demand.\n\nIn addition to his academic publishing, Levine appears frequently as an expert in print, on the radio, and on television. He has authored and/or co-authored Op-Eds that have appeared in publications around the world. His television appearances include two BBC World documentaries on pneumococcal disease and prevention by vaccination, and news interviews on BBC World, BBC News, Al-Jazeera (The Pulse), South African Broadcasting Corporation, and Arirang (South Korea). His work on pneumonia research and prevention has been profiled in the New York Times (see New York Times Article: A Campaign to Get a Disease Some Respect). On August 4, 2009, Levine was featured in an episode of the \"Kill or Cure\" series on BBC World. This episode, called \"Saving Lives\" (http://www.rockhopper.tv/programmes/262/), focused on efforts to increase access to pneumococcal vaccines through an initiative called the Advance Market Commitment (http://www.vaccineamc.org). Click here to access all the episodes of Kill or Cure on the Rockhopper.tv website (http://www.rockhopper.tv/programmes/18/). Most recently Levine was featured in the Johns Hopkins Bloomberg School of Public Health magazine for his work on advancing vaccine access.\n\n\n\n"}
{"id": "339761", "url": "https://en.wikipedia.org/wiki?curid=339761", "title": "Ovarian cyst", "text": "Ovarian cyst\n\nAn ovarian cyst is a fluid-filled sac within the ovary. Often they cause no symptoms. Occasionally they may produce bloating, lower abdominal pain, or lower back pain. The majority of cysts are harmless. If the cyst either breaks open or causes twisting of the ovary, it may cause severe pain. This may result in vomiting or feeling faint.\nMost ovarian cysts are related to ovulation, being either follicular cysts or corpus luteum cysts. Other types include cysts due to endometriosis, dermoid cysts, and cystadenomas. Many small cysts occur in both ovaries in polycystic ovarian syndrome. Pelvic inflammatory disease may also result in cysts. Rarely, cysts may be a form of ovarian cancer. Diagnosis is undertaken by pelvic examination with an ultrasound or other testing used to gather further details.\nOften, cysts are simply observed over time. If they cause pain, medications such as paracetamol (acetaminophen) or ibuprofen may be used. Hormonal birth control may be used to prevent further cysts in those who are frequently affected. However, evidence does not support birth control as a treatment of current cysts. If they do not go away after several months, get larger, look unusual, or cause pain, they may be removed by surgery.\nMost women of reproductive age develop small cysts each month. Large cysts that cause problems occur in about 8% of women before menopause. Ovarian cysts are present in about 16% of women after menopause and if present are more likely to be cancer.\nSome or all of the following symptoms may be present, though it is possible not to experience any symptoms:\nOther symptoms may depend on the cause of the cysts:\n\nThe effect of cysts not related to PCOS on fertility is unclear.\n\nA ruptured ovarian cyst is usually self-limiting, and only requires keeping an eye on the situation and pain medications. The main symptom is abdominal pain, which may last a few days to several weeks, but they can also be asymptomatic. Rupture of large ovarian cysts can cause bleeding inside the abdominal cavity and in some cases shock.\n\nOvarian cysts increase the risk for ovarian torsion; cysts which are larger than 4 cm are associated with approximately 17% risk. The torsion can cause obstruction of blood flow and lead to infarction.\n\nOvarian cysts are usually diagnosed by ultrasound, CT scan, or MRI, and correlated with clinical presentation and endocrinologic tests as appropriate.\n\nFollow-up imaging in women of reproductive age for incidentally discovered simple cysts on ultrasound is not needed until 5 cm, as these are usually normal ovarian follicles. Simple cysts 5 to 7 cm in premenopausal females should be followed yearly. Simple cysts larger than 7 cm require further imaging with MRI or surgical assessment. Because they are large, they cannot be reliably assessed by ultrasound alone because it may be difficult to see the soft tissue nodularity or thickened septation at their posterior wall due to limited penetrance of the ultrasound beam. For the corpus luteum, a dominant ovulating follicle that typically appears as a cyst with circumferentially thickened walls and crenulated inner margins, follow up is not needed if the cyst is less than 3 cm in diameter. In postmenopausal patients, any simple cyst greater than 1 cm but less than 7 cm needs yearly follow-up, while those greater than 7 cm need MRI or surgical evaluation, similar to reproductive age females.\n\nFor incidentally discovered dermoids, diagnosed on ultrasound by their pathognomonic echogenic fat, either surgical removal or yearly follow up is indicated, regardless of patient age. For peritoneal inclusion cysts, which have a crumpled tissue-paper appearance and tend to follow the contour of adjacent organs, follow up is based on clinical history. Hydrosalpinx, or fallopian tube dilation, can be mistaken for an ovarian cyst due to its anechoic appearance. Follow-up for this is also based on clinical presentation.\n\nFor multiloculate cysts with thin septation less than 3 mm, surgical evaluation is recommended. The presence of multiloculation suggests a neoplasm, although the thin septation implies that the neoplasm is benign. For any thickened septation, nodularity, or vascular flow on color doppler assessment, surgical removal should be considered due to concern for malignancy.\n\nThere are several systems to assess risk of an ovarian cyst of being an ovarian cancer, including the RMI (risk of malignancy index), LR2 and SR (simple rules). Sensitivities and specificities of these systems are given in tables below:\n\nOvarian cysts may be classified according to whether they are a variant of the normal menstrual cycle, referred to as a functional or follicular cyst.\n\nOvarian cysts are considered large when they are over 5 cm and giant when they are over 15 cm. In children, ovarian cysts reaching above the level of the umbilicus are considered giant.\n\nFunctional cysts form as a normal part of the menstrual cycle. There are several types of cysts:\n\nNon-functional cysts may include the following:\n\nIn juvenile hypothyroidism multicystic ovaries are present in about 75% of cases, while large ovarian cysts and elevated ovarian tumor marks are one of the symptoms of the Van Wyk and Grumbach syndrome.\n\nThe CA-125 marker in children and adolescents can be frequently elevated even in absence of malignancy and conservative management should be considered.\n\nPolycystic ovarian syndrome involves the development of multiple small cysts in both ovaries due to an elevated ratio of leutenizing hormone to follicle stimulating hormone, typically more than 25 cysts in each ovary, or an ovarian volume of greater than 10 mL.\n\nLarger bilateral cysts can develop as a result of fertility treatment due to elevated levels of HCG, as can be seen with the use of clomifene for follicular induction, in extreme cases resulting in a condition known as ovarian hyperstimulation syndrome. Certain malignancies can mimic the effects of clomifene on the ovaries, also due to increased HCG, in particular gestational trophoblastic disease. Ovarian hyperstimulation occurs more often with invasive moles and choriocarcinoma than complete molar pregnancies.\n\nA widely recognised method of estimating the risk of malignant ovarian cancer based on initial workup is the \"risk of malignancy index\" (RMI). It is recommended that women with an RMI score over 200 should be referred to a centre with experience in ovarian cancer surgery.\n\nThe RMI is calculated as follows:\n\nThere are two methods to determine the ultrasound score and menopausal score, with the resultant RMI being called RMI 1 and RMI 2, respectively, depending on what method is used:\n\nAn RMI 2 of over 200 has been estimated to have a sensitivity of 74 to 80%, a specificity of 89 to 92% and a positive predictive value of around 80% of ovarian cancer. RMI 2 is regarded as more sensitive than RMI 1.\n\nCysts associated with hypothyroidism or other endocrine problems are managed by treating the underlying condition.\n\nAbout 95% of ovarian cysts are benign, not cancerous.\n\nFunctional cysts and hemorrhagic ovarian cysts usually resolve spontaneously. However, the bigger an ovarian cyst is, the less likely it is to disappear on its own. Treatment may be required if cysts persist over several months, grow, or cause increasing pain.\n\nCysts that persist beyond two or three menstrual cycles, or occur in post-menopausal women, may indicate more serious disease and should be investigated through ultrasonography and laparoscopy, especially in cases where family members have had ovarian cancer. Such cysts may require surgical biopsy. Additionally, a blood test may be taken before surgery to check for elevated CA-125, a tumour marker, which is often found in increased levels in ovarian cancer, although it can also be elevated by other conditions resulting in a large number of false positives.\n\nPain associated with ovarian cysts may be treated in several ways:\n\nAlthough most cases of ovarian cysts involve monitoring, some cases require surgery. This may involve removing the cyst, or one or both ovaries. Technique is typically laparoscopic, unless the cyst is particularly large, or if pre-operative imaging suggests malignancy or complex anatomy. In certain situations, the cyst is entirely removed, while with cysts with low recurrence risk, younger patients, or which are in anatomically eloquent areas of the pelvis, they can be drained. Features that may indicate the need for surgery include:\n\nMost women of reproductive age develop small cysts each month, and large cysts that cause problems occur in about 8% of women before menopause. Ovarian cysts are present in about 16% of women after menopause and if present are more likely to be cancer.\n\nBenign ovarian cysts are common in asymptomatic premenarchal girls and found in approximately 68% of ovaries of girls 2–12 years old and in 84% of ovaries of girls 0–2 years old. Most of them are smaller than 9 mm while about 10-20% are larger macrocysts. While the smaller cysts mostly disappear within 6 months the larger ones appear to be more persistent.\n"}
{"id": "32458568", "url": "https://en.wikipedia.org/wiki?curid=32458568", "title": "Overall nutritional quality index", "text": "Overall nutritional quality index\n\nThe overall nutritional quality index is a nutritional rating system developed at the Yale-Griffin Prevention Research Center. It assigns foods a score between 1 and 100 to reflect the overall nutrition provided relative to the calories consumed. The system has been marketed commercially as NuVal, and some consumer foods in the United States are marked with ONQI values as \"NuVal\".\n\nThe ONQI for a food is the ratio of a \"numerator\" value representing beneficial nutrients such as iron, dietary fibre and vitamins, and a \"denominator\" value representing detrimental nutrients such as cholesterol and saturated fat.\n\nThe following foods have the maximum ONQI of 100: broccoli, blueberries, okra, oranges and green beans. Some of the lowest ONQIs are for white bread (9), hot dog (5), apple pie (2) and Ice pop (1).\n\nHere is a selection of food rankings from Yale University's Overall Nutritional Quality Index (scores out of 100)\n"}
{"id": "5057565", "url": "https://en.wikipedia.org/wiki?curid=5057565", "title": "PGY", "text": "PGY\n\nPGY, short for postgraduate year, refers to a North American numerical scheme denoting the progress of postgraduate dental, medicine, podiatry or pharmacy residents in their residency programs. It is used to stratify responsibility in most training programs and to determine salary. The grade of the resident is denoted with a numeral after the PGY designation, such as PGY-3 for a third-year resident.\n\nThe length of residency depends mostly on the field a graduate chooses to take. Medical specialties such as family medicine and internal medicine often requires three years, whereas surgery usually requires a minimum of five, and neurological surgery is the longest at seven years. Subspecialization (vascular or orthopedic spine surgery as a branch of surgery, for example) in any field will add time to postgraduate training.\n\nFor more information on specific medical residency programs, see the American Medical Association's Fellowship and Residency Electronic Interactive Database.\n\nDental residencies for general practice, known as GPRs, are generally one year, with a possibility of a second year at some facilities. Dental specialties, such as orthodontics, require 2–4 years, while oral and maxillofacial surgery requires 4–6 years. Some specialty programs require that applicants have completed at least a one-year GPR residency, while other programs require applicants to have some private practice experience as a general dentist. Regardless of requirements, completing a GPR residency will make an applicant more competitive for any specialty program. \n\nMedical physics residencies range between two and four years, with at least two years fulfilling the necessary clinical experience. Completion of a CAMPEP-accredited residency allows one to sit for board examinations administered through the American Board of Radiology. PGY-3, and/or if also available -4, generally consist of scholarly research years, akin to a postdoctoral research position. Residencies options are either radiation oncology physics or medical imaging.\n\nPharmacy residencies are usually one year, but a PGY-2 can be completed, often as an option, for pharmacy specialties such as critical care, cardiology, oncology, etc.\n\nIn some teaching institutions, trainees are required to indicate level of training on all signatures (John Doe, M.D., PGY-1 or R-1; or John Doe, D.O., PGY-1 or R-1).\n\nResidencies are also offered for those in the physician assistant profession in a variety of specialties such as surgery and emergency medicine.\n\nInternships and residencies are also offered for those who have completed graduate education in psychology, known as interns and post-doc fellows.\n\n"}
{"id": "33719065", "url": "https://en.wikipedia.org/wiki?curid=33719065", "title": "Papan Reservoir", "text": "Papan Reservoir\n\nPapan Reservoir (), is a reservoir of the Ak-Buura River, located in Nookat District of Naryn Province of Kyrgyzstan. It is used for irrigation purposes and also water supply of Osh.\n"}
{"id": "4660212", "url": "https://en.wikipedia.org/wiki?curid=4660212", "title": "Peter Knight (anti-abortion activist)", "text": "Peter Knight (anti-abortion activist)\n\nPeter James Knight (born 1 January 1954) is an Australian anti-abortion activist who shot dead a security guard in a Melbourne abortion clinic. Following his arrest and a criminal trial, Knight is serving a life sentence with a minimum non-parole period of 23 years.\n\nKnight was one of six children born into a Roman Catholic family in , New South Wales. He led a hermit's life in the years leading up to the incident in a bush camp in the Killanbutta State Forest near , 'off the grid' without a telephone or electricity. He did, however, frequently attend anti-abortion rallies in Sydney and Melbourne. Knight was also opposed to smoking, smokers, tobacco companies and the taking of oaths.\n\nOn 16 July 2001 he walked into the East Melbourne Fertility Clinic, a private abortion provider, carrying a rifle and other weapons including of kerosene, three lighters, torches, 30 gags, and a handwritten note that read \"We regret to advise that as a result of a fatal accident involving some members of staff, we have been forced to cancel all appointments today\".\n\nKnight later stated that he intended to massacre everyone in the clinic, and attack all Melbourne abortion clinics. He developed home made mouth gags and door jambs to restrain all patients and staff inside a clinic while he doused them with the kerosene. He shot 44-year-old Stephen Gordon Rogers, a security guard, in the chest, killing him. Staff and clients overpowered him soon after. He intended to massacre the 15 staff and 26 patients at the clinic by burning them alive.\n\nFor many weeks after his arrest Knight refused to answer questions or cooperate with police investigations. Due to his hermitic life, Victorian Police were unable to confirm his identity until three months after his arrest, even though his photographs were published in major newspapers. According to psychiatrist Don Sendipathy, Knight interpreted the Bible in his own unique way and believed in his own brand of Christianity.\n\nAfter choosing to not obtain legal representation, Knight was found guilty by a jury and, on 19 November 2002, he was sentenced to life in prison, with a minimum non-parole period of 23 years. Knight will be aged in his 70s before being eligible for parole. On 14 May 2003 Knight lodged an appeal against his conviction; however his appeal was dismissed as it was not lodged within fourteen days of his sentence, as required under the law. He is currently serving time in HM Prison Barwon, near Geelong. \n\nThis incident is the only killing by an anti-abortion activist in Australia's history.\n\n\n"}
{"id": "7466703", "url": "https://en.wikipedia.org/wiki?curid=7466703", "title": "Project SCUM", "text": "Project SCUM\n\nProject SCUM was a plan proposed in 1995 by R. J. Reynolds Tobacco Company (RJR) to sell cigarettes to members of the \"alternative lifestyle\" areas of San Francisco, in particular the large number of gay people in the Castro and homeless people in the Tenderloin. The acronym \"SCUM\" stood for \"subculture urban marketing.\" Perhaps recognizing the offensive nature of its label, the marketing plan was later renamed Project Sourdough.\n\nAn anti-smoking campaign called Truth targeted R. J. Reynolds for Project SCUM, arguing that it not only showed the usual exploitative tobacco marketing techniques but added to them an explicit contempt or even hatred for the people it was trying to market its products to. \"SF Weekly\" reported:\n\n\"This is a hate crime, plain and simple,\" says Kathleen DeBold, who directs the Washington, D.C.-based Mautner Project for Lesbians With Cancer. \"What else do you call it when a group thinks of gays and lesbians as 'scum,' and then targets us with something that kills?\"\n\nSan Francisco Supervisor Chris Daly, who represents the Tenderloin District, is equally upset. \"It's racist, it's classist, it's oppressive. And it is really disheartening to hear. But I can't say that I am surprised. Low-income communities and people of color have always been derided and taken advantage of. Obviously, the tobacco companies feel like they can make money off other people's misery.\"\n\nProject SCUM documents came to light after a court order forced R. J. Reynolds to hand them over during the State of California's litigation against tobacco companies. R. J. Reynolds's marketing in the 1990s of its Camel and Winston cigarette brands drew the attention of attorneys representing California cities and counties. Project SCUM highlighted how tobacco companies in the 1990s were targeting young adults to be lifetime smokers.\n\nRevelations about Project SCUM were among the mountains of evidence ensuring that anti-tobacco litigation would continue. In 1998, a resolution of the litigation came about in the Master Settlement Agreement between more than 40 state attorneys general and the tobacco industry.\n\n\n"}
{"id": "20295296", "url": "https://en.wikipedia.org/wiki?curid=20295296", "title": "Pulpotomy", "text": "Pulpotomy\n\nA pulpotomy is the removal of a portion of the pulp, including the diseased aspect, with the intent of maintaining the vitality of the remaining pulpal tissue by means of a therapeutic dressing. A healthy tooth has a space inside it called the \"pulp space\" which is filled with soft tissues - nerves, blood vessels, and pink connective tissue. When a carious process develops in a tooth, the bacteria associated with it can cause pulpal inflammation, which is often what causes toothache.\n\nDamage to the pulp of permanent teeth usually requires a root canal treatment or endodontic therapy. The pulp of primary or deciduous teeth, which only have to survive until an adult teeth come in, and because they have a better blood supply, can sometimes be saved.\n\nPrimary/deciduous (baby) teeth in children have relatively large pulp spaces. Caries do not have to develop significantly before they reach the pulp chamber.\n\nWhen the soft tissue in the pulp chamber is \"infected\" (has bacteria in it) or \"affected\" (is inflamed), it can be removed by a dentist or dental therapist under local anaesthetic. If the soft tissue in the canals is still healthy enough, a special medicated filling can be put into the chamber in an attempt to keep the remaining pulp (in the canals) alive. The process of removing the pulp from the chamber is the actual \"pulpotomy\", though the word is often used for the entire process including placement of the medication. There are many medicaments that can be used to fill the pulp chamber including zinc-oxide eugenol as well as mineral trioxide aggregate. \n\nThere are two types of pulpotomy techniques depending the extent of caries in a tooth and the symptoms it presents. A vital pulpotomy or a non-vital pulpotomy can be carried out. However, recent research shows that non-vital pulpotomies are rarely indicated due to their low success rates and it is therefore sometimes better to extract the tooth. \n\nAfterwards the tooth is restored with a regular filling, either composite or amalgam, or a stainless steel crown. Due to the process of a pulpotomy causing the tooth to become slightly brittle, a stainless steel crown is normally indicated as the preferred choice of definitive restoration.\n\nA pulpotomy can be done to both permanent and primary teeth.\n\nIn some cases, radicular pulp (pulp within the root of a tooth) may remain healthy despite carious exposure of the pulp chamber. In cases where root formation is incomplete (as during adolescent years), a partial pulpotomy may keep radicular pulp vital long enough to allow the roots to develop fully.\n\nA partial pulpotomy for traumatic exposures is also called a Cvek Pulpotomy. When a baby tooth or young permanent tooth is traumatised - say, hitting your teeth on the handlebars of a bike - it can be broken in such a way that the pulp is exposed. Again, a partial pulpotomy may help it to finish developing and be saved.\n\nAdult pulpotomies are not as popular with dentists today as they were 100 years ago. Historically, traditional endodontia has been reported to be a more reliable treatment than pulpotomy techniques, based on the rate of saved teeth and the longevity of their preservation.However, new research is changing this view. The pulpotomy, as a therapeutic dental treatment, has a long history. It has been used for thousands of years.\n\nIn primary teeth medicaments such as formocresol, mineral trioxide aggregate, zinc oxide eugenol and calcium hydroxide can be used in pulpotomy. Formocresol use has been questioned due to toxicity concerns.\n\nFerric sulphate, sodium hypochlorite or a local anaesthetic solution containing a vasoconstrictor agent can be used to arrest any bleeding from the pulp prior to the placement of medicament.\n\nCalcium enriched mixtures have been used in permanent molar teeth with irreversible pulpitis showing positive outcomes.\n\nAlso known as non-chemical devitalization. Its mechanism of action is the cauterization of the pulp tissue. It carburizes heat-denaturated pulp and bacterial contamination.\n\nThis technique overcomes histological effects of electro-surgery. It creates a superficial zone of coagulation or necrosis that remains compatible with the underlying tissue and isolates the pulp from vigorous effects of the sub-base.\n\n\n\n"}
{"id": "25323675", "url": "https://en.wikipedia.org/wiki?curid=25323675", "title": "Robert M. Blizzard", "text": "Robert M. Blizzard\n\nRobert M. Blizzard was a pediatric endocrinologist and a founding member of the Lawson Wilkins Pediatric Endocrine Society.\n"}
{"id": "142415", "url": "https://en.wikipedia.org/wiki?curid=142415", "title": "Sizergh Castle and Garden", "text": "Sizergh Castle and Garden\n\nSizergh Castle and Garden is a stately home and garden at Helsington in the English county of Cumbria, about south of Kendal. The castle, a grade I listed building, is in the care of the National Trust along with its garden and estate. It is the home of the Hornyold-Strickland family. \n\nIn 2016 the Sizergh estate was included in the newly extended Lake District National Park.\n\nThe earliest part of the building is a tower of fourteenth or fifteenth century date.\n\nThere are oak-panelled interiors, including the Inlaid Chamber, where the panelling is inlaid with floral and geometric patterns in pale\npoplar and dark bog-oak. The contents of the Inlaid Chamber were sold to the Victoria and Albert Museum (V&A) in the 1890s and it was displayed as a reconstructed period room.\nThe return of the panelling to its original location at Sizergh was advocated by among others Mark Girouard. The panelling returned in 1999 under a long-term loan. In 2017 it was reported that transfer of ownership to the National Trust had been made formal.\n\nThe bargeboards probably date from the seventeenth century.\n\nThe Castle contains a variety of paintings, including the following:\n\n\nThe Deincourt family owned this land from the 1170s. On the marriage of Elizabeth Deincourt to Sir William de Stirkeland in 1239, the estate passed into the hands of what became the Strickland family, who owned it until it was gifted to the National Trust in 1950 by Gerald Strickland, 1st Baron Strickland's grandson Lt. Cdr. Thomas Hornyold-Strickland, 7th Count della Catena.\n\nCatherine Parr, the sixth wife of Henry VIII and a relative of the Stricklands, is thought to have lived here after her first husband died in 1533. Catherine's second husband, Lord Latymer, was kin to the dowager Lady Strickland.\n\nIt was extended in Elizabethan times. Sir Thomas Strickland went into exile with James II.\n\nAround 1770, the great hall was again expanded in the Georgian style.\n\nThe Castle was featured in the ITV documentary \"Inside the National Trust\".\n\nThe garden has a lake and a kitchen garden as well as an award-winning rock garden. The rock garden, which was constructed in the 1920s, is the largest limestone rock garden belonging to the National Trust. It includes part of the National Collection of hardy ferns.\n\nIn 1336 a grant from Edward III allowed Sir Walter Strickland to enclose the land around Sizergh as his exclusive park.\n\nThe estate covers .\n\nThere are various types of habitat on the estate. For example, in 2014 it was reported that 35 ha of wetland habitat was being created in the Lyth Valley on the western edge of the estate. The project received funding from Natural England as part of a higher level stewardship scheme. It is hoped to attract bittern and other wildlife.\n\nSizergh has received support from the Morecambe Bay Nature Improvement Area which received three years of government grant funding (2012–15). Projects continue under the auspices of the Morecambe Bay Partnership, a registered charity.\n\nThe Sizergh estate is a good place to see birds. For example, hawfinches are attracted to the area because of its hornbeam trees, and these birds sometimes come close to the main car park.\n\nFritillary butterflies live on the estate.\n\n\n"}
{"id": "5744399", "url": "https://en.wikipedia.org/wiki?curid=5744399", "title": "Smoking ban in England", "text": "Smoking ban in England\n\nA smoking ban in England, making it illegal to smoke in all enclosed work places in England, came into force on 1 July 2007 as a consequence of the Health Act 2006. Similar bans had already been introduced by the rest of the United Kingdom: in Scotland on 26 March 2006, Wales on 2 April 2007 and Northern Ireland on 30 April 2007.\n\nBefore the ban many businesses voluntarily introduced bans on smoking mainly as a result of public feedback. The pub chain Wetherspoons was the first major chain to introduce a complete ban on indoor smoking, doing so in May 2006.\n\nTobacco advertising had been banned in England gradually starting with a ban on Television Advertising of cigarettes in 1965 to a complete ban on all Tobacco Advertising in 2005.\n\nOn 16 November 2004 a Public Health white paper proposed a smoking ban in almost all public places in England and Wales. Smoking restrictions would be phased in, with a ban on smoking in NHS and government buildings by 2006, in enclosed public places by 2007, and pubs, bars and restaurants (except pubs not serving food) by the end of 2008.\n\nOn 26 October 2005, after external challenge and debates within the Cabinet, the government announced that it would continue with its plans. All workplaces, including restaurants and pubs selling food, would have to comply by summer 2007 However, there was widespread criticism from all sides of the argument on this, with a number of MPs threatening to try to overturn the bill. Many representatives of the licensed trade told the Government that only a total ban would work, and over 90 MPs signed a motion demanding this, with over 100 signing a petition for a free vote on the issue. It was reported on 24 November that Chief Medical Officer Liam Donaldson nearly quit over the partial ban, but decided to stay to champion a total ban. On the same day, the government released the results of the public consultation, after Cancer Research UK demanded them under the Freedom of Information Act, which revealed that nearly 9 out of 10 respondents wanted a total ban.\n\nOn 11 January 2006, the government further announced that it would give MPs a free vote on an amendment to the Health Bill, submitted by the Health select committee, to instigate a comprehensive smoke-free workplace regulations. Health Secretary Patricia Hewitt voted in favour of the amendment and, in so doing, voted against her own Department's then publicly stated policy (i.e. the proposed partial regulations). All other parties had offered free votes on the issue which was debated on 14 February, with three options: the present compromise, a total ban, or an exemption for members' clubs only.\n\nOn 14 February 2006, the House of Commons first voted on the amendment to the original compromise plan, to extend the ban to all enclosed public places except private members' clubs. The amendment was carried with a large majority. MPs then voted on a further amendment to ban smoking in all enclosed public places including private members' clubs. Again this amendment gained significant support and was carried with a large majority. This therefore replaced the earlier successful amendment which would have allowed smoking only in private members' clubs. The legislation was passed by the House of Lords, allowing a total smoking ban in enclosed public places to come into force in England.\n\nPolitical opposition did not entirely disappear at this point, the House of Lords Economic Affairs Committee accused the Government of overreacting to the threat posed by passive smoking and said that the smoking ban was symptomatic of MPs' failure to understand risk on 7 June 2006.\n\nThe ban came into force on 1 July 2007, as announced on 30 November 2006 by former Secretary of State for Health Patricia Hewitt, who called it \"a huge step forward for public health\".\n\nOn 30 June 2010, the recently formed Coalition Government announced that it would not be reviewing the ban. An attempt in October 2010 by Conservative MP David Nuttall to amend the law to exempt private members' clubs and pubs from the smoking ban was defeated in the House of Commons on its first reading.\n\nWhile the ban affects almost all indoor workplaces, some exemptions were provided:\n\nAn exemption was also theoretically possible within the\nPalace of Westminster, as for other Royal Palaces, although members of the House of Commons and the House of Lords agreed to observe the spirit of the ban and restrict any smoking within the grounds of Parliament to four designated outside areas.\n\nSmoking is permitted in a private residence, although not in areas used as a shared work-space. In flats with communal entrances or shared corridors, these must be smoke-free.\n\nUniversity halls of residence presented some dilemmas in practice as regards defining what is public and private. Several universities have imposed a blanket ban on smoking including halls of residence.\n\nAs part of the implementation of the smoking ban, Transport for London announced that smoking would be no longer permitted on taxis, private hire vehicles and all London Buses premises, including all bus stop shelters.\n\nThe Association of Train Operating Companies and Network Rail introduced an extended ban on smoking covering all railway property including all National Rail station platforms whether enclosed or not. The ban has since been extended to cover the use of Electronic cigarettes. Smoking on board trains was banned in 2005 when both GNER and First Caledonian Sleeper withdrew smoking accommodation from their services.\n\nThe Tyne and Wear Metro was the first public transport system to ban in its entirety which has been enforced since the system first opened in 1980.\n\nThe ban is enforced by Environmental Health Officers in England, who issue warnings and offer advice before resorting to punitive measures and have had to issue a low frequency of fines since the law came into force. However, there were some objectors who generated higher-profile legal cases, for instance Hugh Howitt, also known as Hamish Howitt, the landlord of the Happy Scots Bar in Blackpool who was the first landlord to be prosecuted for permitting smoking in a smoke-free place under his control. On 2 August 2007, Howitt appeared before Blackpool Magistrates' Court and pleaded not guilty to 12 counts of failing to stop people smoking in his pub. On 2 December 2008, Howitt effectively had his premises licence revoked, after an appeal by Blackpool Council was upheld; he was not allowed to appeal, and Howitt had to close the Delboys Bar following the decision.\n\nThere have been some incidents of violence perpetrated by people refusing to obey the ban, in one of which a former heavyweight boxer, James Oyebola, was shot in the head after he asked a customer at a nightclub to stop smoking and later died of his injuries. However, the view of enforcement authorities is that the smoke-free workplace regulations are simple to understand, popular, and as a result largely 'self-policing'. For a short while, bars in the UK that offered shisha (the smoking of flavoured tobacco through a pipe) were still allowed to provide their services inside the establishment, however the ban covered this area in late 2007 leading to a rapid decline in shisha bars.\n\nA legal blunder by Stoke-on-Trent City Council meant that, while the smoking ban still applied in the city, the council were unable to issue fines to people caught flouting the law until 16 July 2007. The blunder caused the city to briefly be dubbed Smoke-on-Trent.\n\nA group calling themselves \"Freedom To Choose\", launched a campaign for a judicial review of the smoke-free workplace regulations claiming a breach of the Human Rights Act 1998, as it does not respect the right to privacy of people who wish to smoke in public. Supporters of the regulations put forward counter-arguments positing that the rights of smokers to indulge in their habit cease as soon as it negatively affects other people in the vicinity. In 2010 pub landlord Nick Hogan was jailed for an offence related to the smoking ban. In 2012 it was reported that \"Five years after the introduction of the smoking ban in England, almost seven out of 10 licensees want the legislation amended to allow for separate smoking rooms in pubs\".\n\nA 2017 YouGov survey indicated growing support for the smoking ban, up to 83% from the 2007 figure of 78%. This was attributed to more smokers supporting the legislation. \n\n\n\n"}
{"id": "9528195", "url": "https://en.wikipedia.org/wiki?curid=9528195", "title": "Time-trade-off", "text": "Time-trade-off\n\nIn health economics, Time-Trade-Off (TTO) is a technique used to measure the quality of life that a person or group is experiencing. An individual will be presented with a set of directions such as:\n\nImagine that you are told that you have 10 years left to live. In connection with this you are also told that you can choose to live these 10 years in your current health state or that you can choose to give up some life years to live for a shorter period in full health. Indicate with a cross on the line the number of years in full health that you think is of equal value to 10 years in your current health state.\n\nThe answer given by the individual shows how many years in the current health state they would be willing to 'trade off', in order to regain full health. This answer can be used to calculate the individual's quality of life in that health state. \n\nFor example, an individual with severe asthma could be offered 10 years in their current condition, or a shorter length of time in full health. If this individual is willing to trade off two of the ten offered years in order to regain full health, this suggests that eight years in full health has the same value as ten years with severe asthma. In this case, the individual would have valued living with severe asthma at 0.8, relative to full health (defined as 1.0).\n\nTTO scores can be influenced by socio-demographic characteristics and attitudinal variables of respondents, such as age, gender, marital status, having children, health status, education level, socio-economic status, ethnicity, and religious beliefs. Also, they can be influenced by the effects of ill health on consumptive activities and non-health-related utility.\n\nTime-trade-off results are often used to calculate quality-adjusted life years (QALYs), allowing healthcare decisionmakers to combine mortality and morbidity into a single interval scale. \n\nOther tools that are used to determine quality of life are the visual analogue scale (VAS), the standard gamble method, the EQ-5D, the Health Utilities Index (HUI), etc.\n"}
{"id": "12022414", "url": "https://en.wikipedia.org/wiki?curid=12022414", "title": "William Nicol (surgeon)", "text": "William Nicol (surgeon)\n\nWilliam Nicol DL (15 September 1790 – 8 August 1879) was a Scottish surgeon, founder of the firm Wm. Nicol & Co, and Conservative Party Member of Parliament for Dover.\n\nHe was born 15 September 1790 at Fawsyde, in the parish of Kinneff, in the Mearns. He was educated in Aberdeen.\n\nIn 1810, Nicol went to Bombay on board the ship Carmarthen as surgeon. Nicol settled in Bombay in 1820 and founded the firm of Wm. Nicol & Co. in Bombay, in 1820. He left Bombay in 1828. Nicol retired from Wm. Nicol & Co. in 1839, and went to England.\n\nNicol was Member of Parliament for Dover from 1859 to 1865.\n\nNicol was later a J.P. and Deputy Lieutenant for Kincardineshire.\n\nNicol died in London on 8 August 1879, and is buried in Brompton Cemetery, London.\n\nHe married his cousin Margaret Dyce Nicol (1799–1860) in Bombay on 4 January 1822. They had no issue.\n"}
