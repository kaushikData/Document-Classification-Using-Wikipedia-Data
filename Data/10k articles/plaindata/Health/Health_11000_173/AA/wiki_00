{"id": "47619364", "url": "https://en.wikipedia.org/wiki?curid=47619364", "title": "Abortion Support Network", "text": "Abortion Support Network\n\nThe Abortion Support Network is a UK based charity which provides financial assistance, accommodation and consultation to women from the Republic of Ireland and Northern Ireland who are seeking an abortion in the UK.\n\nThe charity was founded in 2009 by Mara Clarke. and continues the work of the (now defunct) London based 1980's Irish Women's Abortion Support Group.\n\nIn 2015 they received widespread attention in the UK due to the popular parenting site Mumsnet using their annual charity fundraising appeal to raise money for the ASN.\n\nIn 2017 as part of a coalition, ASN made a submission to the Citizens' Assembly. That same year, ASN fund-raised and provided over £73,000 (€84,000) worth of grants for all associated expenses of obtaining an abortion, including travel. The team of volunteers fielded 1,009 phone calls (685 from Ireland) providing free advice.\n\n"}
{"id": "51094877", "url": "https://en.wikipedia.org/wiki?curid=51094877", "title": "Alavida Health Inc.", "text": "Alavida Health Inc.\n\nAlavida Health Inc. is a company headquartered in British Columbia, Canada offering treatment of alcohol use disorder that combines drug therapy and counselling. Alavida was founded by CEO, Elliot Stone; Medical Director, Diane Rothon and Clinical Program Director, Terri-Lynn MacKay.\n\nAlavida Health Inc. was founded in 2016, located in British Columbia, Canada. It was founded by Chief executive officer, Elliot Stone; Medical Director, Diane Rothon a former Chief Coroner of British Columbia with over 30 years experience in addiction medicine.\n\nThe Alavida Method is based on a Finnish treatment model involving a combination of drug therapy and counselling that has been used in that country for more than 20 years. Alavida Health Ltd. has licensed this treatment program and adapted it to fit the Canadian market, currently focusing on British Columbia. The Alavida program can be used in conjunction with other therapy and programs such as SMART Recovery, or as a preparatory program for abstinence.\n\nPharmacological extinction (commonly referred to as The Sinclair Method) is used to address the physiological elements of addiction. Naltrexone, the medication used in the treatment, is taken before a person drinks alcohol. The FDA approved naltrexone as an alcohol abuse treatment in 1994.\n\n"}
{"id": "14483595", "url": "https://en.wikipedia.org/wiki?curid=14483595", "title": "Bangladesh Rehabilitation Centre for Trauma Victims", "text": "Bangladesh Rehabilitation Centre for Trauma Victims\n\nBangladesh Rehabilitation Centre for Trauma Victims (BRCT; ) is a Bangladeshi NGO, working in the area of rehabilitation of trauma victims. It was established in 1992. BRCT began its journey by providing medical treatment, legal support, and rehabilitation to the victims on 25 February 1992 with support from foreign medical experts.\n\nAkram H. Chowdhury, a human rights activist and 9th Parliament Member (MP) is the founding General Secretary of BRCT. He is also Secretary General of Bangladesh Institute of Human Rights (BIHR) in Dhaka, and a Council Member of Executive Committee, of International Rehabilitation Centre for Torture Victims (IRCT), Copenhagen, Denmark. \n\nThere are two major area of concentration or activities of BRCT – the curative and the preventive ones. BRCT has been operating a very up-dated treatment at Dhaka exclusively for the torture survivors to improve their physical and psychological structure for restoration in the society under the supervision of allied experts. Besides Medicare, physiotherapy and psychotherapy it provides prompt counselling, legal aids, lobbying home visit provide them physical rehabilitation and financial rehabilitation for the torture survivors.\n\nThe legal department of BRCT was developed in 1994. The legal department of the BRCT is concerned about both curative and preventive measures. Curative activities include arranging bail, providing legal advice, taking cases to court and lobbying legislative changes. Preventative activities include raising awareness to the public, seminars, symposiums and training community health workers (CHW) through use DDCAT. Taking cases trial can be considered curative as well as preventative as these acts could be deterrents for police to commit such crimes.\n\nThe monthly regular publications of BRCT is Manobadhikar Aunushandhani (Human Rights Fact-finder) and \"Article 14\" is a quarterly published English newsletter. BRCT observes the International Day in Support of Torture Victims on 26 June and International Human Rights Day on 10 December regularly and makes awareness among the people of Bangladesh. It organises various rallies, seminars, postering, stickers & leaflets, bills, etc. to create public awareness in Bangladesh.\n\n\n\nAs already stated, the ultimate goal of BRCT is to provide victims of torture with a combination of treatment and rehabilitation to allow full integration back into their homes and communities. Early on, BRCT recognised that completed rehabilitation could not occur with a little medication and a bandage. Rehabilitation involves a multidisciplinary approach and at BRCT. This approach has come to be known as the integrated rehabilitation approach (IRA). The IRA consists of services at BRCT including counselling, medical care, physiotherapy, psychotherapy, and legal assistance. The IRA also involves group and family therapy, follow-ups including home visits.\n\nOnce the victims, especially of rural areas, return to their communities, they will often form or join a victim's association (VA). The VA was initiated by the BRCT as a way for victims to continue rehabilitation at home by actively participating in a program that supports one another, future victims and as a means to prevent future torture occurrences from happening by spreading awareness.\n\nVA's are based on three pillars of strength: Mental (মনবল), Human Power (জনবল) and Financial Solvency (অর্থবল), and have the impact of shifting feelings of loneliness to unity. They are self-help groups that consist of approximately 10 – 30 members and they hold monthly meeting to discuss their won social and economic problems.\n\nBRCT started a unique program called Task Force against Torture (TFT), in the preventive part, formed at different district level consisting of local doctors, lawyers, journalists, teachers and social workers, who will take initiative for the protection against torture as well as human rights violations. The members of the TFT will facilitate treatment for the victims of torture locally, will arrange seminars, worships and discussion meeting for general awareness about torture and human rights violations and will take necessary initiative to stop any incidence of torture.\n\nThe Door-to-Door Campaign Against Torture (DDCAT) is a village-level anti-torture and human rights empowerment campaign initiated by BRCT in April 2000. Its mandate is to remove the veil of silence that has kept torture victims from asserting their rights as human beings and as citizens of their own country, and to amplify those voices muffled by poverty, illiteracy and fear through a national movement against torture. The DDCAT began as a pilot program stemming from the UN human rights Decade (1995–2004) and two stickers were produced in the first three years of the program in an attempt to spread the anti-torture message into every home in the country. Today the DDCAT program is focused mainly in the 10 districts of the division of Khulna as this is an area prone to violence and it is also where most other BRCT programs are now initiated.\n\nBRCT under curative activities sincerely addresses almost all complications with available resource. BRCT Treatment Center is concerned with the treatment of the torture victims and provides services through a process termed as Integrated Rehabilitation Approach (IRA). It is a multidisciplinary approach, which includes physical, psychological as well as economic rehabilitation measures. From the very outset BRCT is operating a Dhaka-based \"treatment centre\" as outdoor services provided to the victims. BRCT considers a person as torture survivors when victimised only perpetrated by the members of the law enforcing agencies and security forces of Bangladesh. Most of the torture survivors visited the centre with acute physical complication to get the psychological, economical and legal supports.\n\nBRCT formally opened the Zero Pain Movement on 30 March 2006 at BRCT. It is a revolutionary social movement to bring basic physiotherapeutic principles and prevention awareness to the masses. With the Zero Pain Movement, BRCT in conjunction with IPRR (Institute of Physiotherapy Rehabilitation and Research) will use physiotherapy as a key tool to reduce the pain and sufferings of the general public, especially those who are involved in daily manual labour where the pain becomes chronic.\n\nBRCT is running a Documentation Centre to having information and documents on human rights topics focusing on torture. The main objective of BRCT documentation centre is to organise a human rights library and to provide information on human rights to the human rights activist and promoting awareness among the people of Bangladesh about fundamental rights.\n\nThe documentation centre is regularly maintaining communication with local and international organisation to collect documents.\n\nThe most expending and escalating activities of the organisation is BRCT documentation unit. It has created scope for the users to enjoy new collections of information with Internet facilities. A good number of readers are regularly collecting materials from the library of BRCT. BRCT is also providing photocopy service to the users on request.\nBesides the regular work of Documentation centre, which includes readers' service, feature services, collecting library materials etc.\n\n"}
{"id": "241649", "url": "https://en.wikipedia.org/wiki?curid=241649", "title": "Cochlear implant", "text": "Cochlear implant\n\nA cochlear implant (CI) is a surgically implanted neuroprosthetic device that provides a sense of sound to a person with severe to profound sensorineural hearing loss. Cochlear implants bypass the normal acoustic hearing process, instead replacing it with electric hearing. Namely, the sound sensation comes from the sound that is converted to electric signals which directly stimulate the auditory nerve. The brain adapts to the new mode of hearing, and eventually can interpret the electric signals as sound and speech.\n\nThe implant has two main components. The outside component is generally worn behind the ear, but could also be attached to clothing, for example, in young children. This component, the sound processor, contains microphones, electronics that include DSP chips, battery, and a coil which transmits a signal to the implant across the skin. The inside component, the actual implant, has a coil to receive signals, electronics, and an array of electrodes which is placed into the cochlea, which stimulate the cochlear nerve.\n\nThe surgical procedure is performed under general anesthesia. Surgical risks are minimal but can include tinnitus and dizziness.\n\nFrom the early days of implants in the 1970s and the 1980s, speech perception via an implant has steadily increased. Many users of modern implants gain reasonable to good hearing and speech perception skills post-implantation, especially when combined with lipreading. One of the challenges that remain with these implants is that hearing and speech understanding skills after implantation show a wide range of variation across individual implant users. Factors such as duration and cause of hearing loss, how the implant is situated in the cochlea, the overall health of the cochlear nerve, but also individual capabilities of re-learning are considered to contribute to this variation, yet no certain predictive factors are known. In children with severe to profound hearing loss, implants have shown to positively contribute to spoken language development.\n\nDespite providing the ability for hearing and oral speech communication to children and adults with severe to profound hearing loss, there is also controversy around the devices. Much of the strongest objection to cochlear implants has come from the Deaf community. For some in the Deaf community, cochlear implants are an affront to their culture, which as some view it, is a minority threatened by the hearing majority.\n\nAndré Djourno and Charles Eyriès invented the original cochlear implant in 1957. This original design distributed stimulation using a single channel. Two years later they went their separate ways due to personal and professional differences.\n\nThe first cochlear implant was invented by William House, in 1961. In 1964, Blair Simmons and Robert J. White implanted a six-channel electrode in a patient's cochlea at Stanford University.\n\nThe modern multichannel cochlear implant was independently developed and commercialized by Graeme Clark from Australia and Ingeborg Hochmair and her future husband, Erwin Hochmair, with the Hochmairs' first implanted in a person in December 1977 and Clark's in August 1978.\n\nCochlear implants bypass most of the peripheral auditory system which receives sound and converts that sound into movements of hair cells in the cochlea; the inside-portion of these hair cells release potassium ions in response to the movement of the hairs, and the potassium in turn stimulates other cells to release the neurotransmitter, glutamate, which makes the cochlear nerve send signals to the brain, which creates the experience of sound. Instead, the devices pick up sound and digitize it, convert that digitized sound into electrical signals, and transmit those signals to electrodes embedded in the cochlea. The electrodes electrically stimulate the cochlear nerve, causing it to send signals to the brain.\n\nThere are several systems available, but generally they have the following components:\n\n\n\nThe surgical procedure most often used to implant the device is called mastoidectomy with facial recess approach (MFRA). If a person's individual anatomy prevents MFRA, other approaches, such as through the suprameatal triangle are used. A systematic literature review published in 2016 found that studies comparing the two approaches were generally small, not randomized, and retrospective so were not useful for making generalizations; it is not known which approach is safer or more effective.\n\nThe procedure is usually done under general anesthesia. Risks of the procedures include mastoiditis, otitis media (acute or with effusion), shifting of the implanted device requiring a second procedure, damage to the facial nerve, damage to the chorda tympani, and wound infections.\n\nThe rate of complications is about 12% for minor complications and 3% for major complications; major complications include infections, facial paralysis, and device failure. To avoid the risk of bacterial meningitis, which while low is about thirty times as high compared to people who don't undergo CI procedures, the FDA recommends vaccination prior to the procedure. The rate of transient facial nerve palsy is estimated to be approximately 1%. Device failure requiring reimplantation is estimated to occur in 2.5-6% of the time. Up to one-third of people experience disequilibrium, vertigo, or vestibular weakness lasting more than 1 week after the procedure; in people under 70 these symptoms generally resolve over weeks to months, but in people over 70 the problems tend to persist.\n\nCochlear implants are only approved for people who are deaf in both ears; a cochlear implant had been used experimentally in some people who had acquired deafness in one ear after they had learned how to speak, and none who were deaf in one ear from birth; clinical studies had been too small to draw generalizations from.\n\nA 2011 AHRQ review of the evidence of the effectiveness of CI in people with bilateral hearing loss - the device's primary use - found low to moderate quality data that showed speech perception in noisy conditions was much better for people who had implants in both ears done at the same time compared to people who had only one. The data also showed that no conclusions could be drawn about changes in speech perception in quiet conditions and health-related quality-of-life. There was only one good study comparing implanting implants in both ears at the same time to implanting them sequentially; this study found that in the sequential approach, the second implantation made no change, or made things worse.\n\nA 2012 review found that the ability to communicate in spoken language was better the earlier the implantation was done. This review also found that, overall, the efficacy of cochlear implants is highly variable, and that it was not possible to accurately predict which children will and will not acquire spoken language successfully.\n\nA 2015 review examined whether CI implantation to treat people with bilateral hearing loss had any effect on tinnitus. This review found the quality of evidence to be poor and the results variable: overall total tinnitus suppression rates varied from 8% to 45% of people who received CI; decrease of tinnitus was seen in 25% to 72%, of people; for 0% to 36% of the people there was no change; increase of tinnitus occurred in between 0% to 25% of patients; and, in between 0 and 10% of cases, people who did not have tinnitus before the procedure, got it.\n\nA 2015 literature review on the use of CI for people with auditory neuropathy spectrum disorder found that, as of that date, description and diagnosis of the condition was too heterogeneous to make clear claims about whether CI is a safe and effective way to manage it.\n\nA 2016 systematic review of CI for people with unilateral hearing loss (UHL) found that of the studies conducted and published, none were randomized, only one evaluated a control group, and no study was blinded. After eliminating multiple uses of the same subjects, the authors found that 137 people with UHL had received a CI. While acknowledging the weakness of the data, the authors found that CI in people with UHL improves sound localization compared with other treatments in people who lost hearing after they learned to speak; in the one study that examined this, CI did improve sound localization in people with UHL who lost hearing before learning to speak. It appeared to improve speech perception and to reduce tinnitus.\n\n, approximately 188,000 individuals had been fitted with cochlear implants. , the same publication cited approximately 324,000 cochlear implant devices having been surgically implanted. In the U.S., roughly 58,000 devices were implanted in adults and 38,000 in children. , the Ear Foundation in the United Kingdom, estimates the number of cochlear implant recipients to be around 600,000.\n\nIn the United States, the overall cost of getting cochlear implants was about $100,000 . Some or all of this may be covered by health insurance. In the United Kingdom, the NHS covers cochlear implants in full, as does Medicare in Australia, and the Department of Health in Ireland, Seguridad Social in Spain and Israel, and the Ministry of Health or ACC (depending on the cause of deafness) in New Zealand. According to the US National Institute on Deafness and Other Communication Disorders, the estimated total cost is $60,000 per person implanted.\n\nA study by Johns Hopkins University determined that for a three-year-old child who receives them, cochlear implants can save $30,000 to $50,000 in special-education costs for elementary and secondary schools as the child is more likely to be mainstreamed in school and thus use fewer support services than similarly deaf children.\n\n, the three cochlear implant devices approved for use in the US were manufactured by Cochlear Limited (Australia), Advanced Bionics (a division of Sonova) and MED-EL (Austria). In Europe, Africa, Asia, South America, and Canada, an additional device manufactured by Neurelec (France, a division of William Demant) was available. A device made by Nurotron (China) was also available in some parts of the world. Each manufacturer has adapted some of the successful innovations of the other companies to its own devices. There is no consensus that any one of these implants is superior to the others. Users of all devices report a wide range of performance after implantation.\n\nMuch of the strongest objection to cochlear implants has come from within the Deaf community, some of whom are pre-lingually Deaf people whose first language is a sign language. For some in the Deaf community, cochlear implants are an affront to their culture, which, as they view it, is a minority threatened by the hearing majority. This is an old problem for the Deaf community, going back as far as the 18th century with the argument of manualism vs. oralism. This is consistent with medicalisation and the standardisation of the \"normal\" body in the 19th century, when differences between normal and abnormal began to be debated. It is important to consider the sociocultural context, particularly in regards to the Deaf community, which considers itself to possess its own unique language and culture. This accounts for the cochlear implant being seen as an affront to their culture, as many do not believe that deafness is something that needs to be cured. However, it has also been argued that this does not necessarily have to be the case: the cochlear implant can act as a tool deaf people can use to access the \"hearing world\" without losing their Deaf identity.\n\nCochlear implants for congenitally deaf children are considered to be most effective when implanted at a young age, during the critical period in which the brain is still learning to interpret sound. Hence they are implanted before the recipients can decide for themselves, on the assumption that deafness is a disability. Deaf culture critics argue that the cochlear implant and the subsequent therapy often become the focus of the child's identity at the expense of a possible future deaf identity and ease of communication in sign language, and claim that measuring the child's success only by their mastery of hearing and speech will lead to a poor self-image as \"disabled\" (because the implants do not produce normal hearing) rather than having the healthy self-concept of a proudly deaf person.\n\nChildren with cochlear implants are more likely to be educated orally, in the standard fashion, and without access to sign language and are often isolated from other deaf children and from sign language. Cochlear implants have been one of the technological and social factors implicated in the decline of sign languages in the developed world. Some of the more extreme responses from deaf activists have labeled the widespread implantation of children as \"cultural genocide\".\n\nAs the trend for cochlear implants in children grows, Deaf-community advocates have tried to counter the \"either or\" formulation of oralism vs manualism with a \"both and\" approach; some schools are now successfully integrating cochlear implants with sign language in their educational programs.\n\n\n"}
{"id": "53745171", "url": "https://en.wikipedia.org/wiki?curid=53745171", "title": "Congeneric reliability", "text": "Congeneric reliability\n\nIn statistical models applied to psychometrics, congeneric reliability formula_1 (\"rho C\") is the reliability of a unidimensional congeneric measurement model. It can be interpreted as an indicator of how reliably the items of such a measurement model reflect the same underlying variable. Synonymous terms are \"composite reliability\", \"unidimensional omega\" and \"Raju (1977) coefficient\").\n\nTau-equivalent reliability (formula_2), which has traditionally been called \"Cronbach's formula_3\", assumes that all factor loadings are equal (i.e. formula_4). In reality, this is rarely the case and, thus, it systematically underestimates the reliability. In contrast, congeneric reliability (formula_1) explicitly acknowledges the existence of different factor loadings. According to Bagozzi & Yi (1988), formula_1 should have a value of at least around 0.6. Often, higher values are desirable. However, such values should not be misunderstood as strict cutoff boundaries between \"good\" and \"bad\". Moreover, formula_7 values close to 1 might indicate that items are too similar. Another property of a \"good\" measurement model besides reliability is construct validity.\n\nCongeneric reliability was first introduced by Jöreskog (1971), when he used the term \"reliability\" but implicitly referred to congeneric measurement models. Werts et al. (1978) also used the more general term \"reliability\" but for the first time also used the term \"composite reliability\" to distinguish it from \"single-item reliability\". As no other term was available, the term \"composite reliability\" was subsequently in use but has also been criticized since. More recently, Cho (2016) suggested to use the term \"congeneric reliability\" instead.\n\nThere are different ways to calculate congeneric reliability. These ways are equivalent, i.e. they will lead to the same result. Traditionally, formula_7 is calculated as follows:\n\nHere, formula_10 is the number of items of the measurement model, formula_11 the factor loading of item formula_12 and formula_13 the observed variance of the error formula_14. An alternative way to calculate congeneric reliability suggested by Cho (2016) is realized as follows, where formula_15 is the variance of the test result:\n\nThe advantage of this alternative formula is that it is embedded in Cho's (2016) system of formulas, which makes it easier to be compared to other coefficients, e.g. tau-equivalent reliability (= \"Cronbach's formula_3\"). This system is also the reason why the term \"composite reliability\" should be dismissed for the benefit of the term \"congeneric reliability\".\n\nThe following illustrative example to calculate formula_1 using the aforementioned formulas is based on Cho (2016). These are the raw data of the covariance matrix:\n\nThese are the raw data of the factor loadings and errors:\n\nNow the traditional formula from above can be used to calculate formula_7, whereby a hat above a variable indicates that the calculation is based on sample data:\n\nAccordingly with the alternative formula of Cho (2016):\n\nA related coefficient is average variance extracted.\n\n"}
{"id": "10588046", "url": "https://en.wikipedia.org/wiki?curid=10588046", "title": "Diet and cancer", "text": "Diet and cancer\n\nDietary factors are recognized as having a significant effect on the risk of cancers, with different dietary elements both increasing and reducing risk. Diet and obesity may be related to up to 30-35% of cancer deaths, while physical inactivity appears to be related to 7% risk of cancer occurrence. One review in 2011 suggested that total caloric intake influences cancer incidence and possibly progression.\n\nWhile many dietary recommendations have been proposed to reduce the risk of cancer, few have significant supporting scientific evidence. Obesity and drinking alcohol are confirmed causes of cancer. Lowering the drinking of beverages sweetened with sugar is recommended as a measure to address obesity. A diet low in fruits and vegetables and high in red meat has been implicated but not confirmed, and the effect may be small for well-nourished people who maintain a healthy weight. \n\nSome specific foods are linked to specific cancers. Studies have linked eating red or processed meat to an increased risk of breast cancer, colon cancer, prostate cancer, and pancreatic cancer, which may be partially explained by the presence of carcinogens in foods cooked at high temperatures. Aflatoxin B1, a frequent food contaminate, causes liver cancer, but drinking coffee is associated with a reduced risk. Betel nut chewing causes oral cancer. Pickled vegetables are directly linked to increased risks of several cancers. The differences in dietary practices may partly explain differences in cancer incidence in different countries. For example, stomach cancer is more common in Japan due to its high-salt diet and colon cancer is more common in the United States. Immigrant communities tend to develop the risk of their new country, often within one generation, suggesting a substantial link between diet and cancer.\n\nDietary recommendations for cancer prevention typically include weight management and eating \"mainly vegetables, fruit, whole grains and fish, and a reduced intake of red meat, animal fat, and refined sugar.\"\n\nA number of diets and diet-based regimes are claimed to be useful against cancer. Popular types of \"anti-cancer\" diet include the Breuss diet, Gerson therapy, the Budwig protocol and the macrobiotic diet. None of these diets has been found to be effective, and some of them have been found to be harmful.\n\nNutritional epidemiologists use multivariate statistics, such as principal components analysis and factor analysis, to measure how patterns of dietary behavior influence the risk of developing cancer. (The most well-studied dietary pattern is the mediterranean diet.) Based on their dietary pattern score, epidemiologists categorize people into quantiles. To estimate the influence of dietary behavior on risk of cancer, they measure the association between quantiles and the distribution of cancer prevalence (in case-control studies) and cancer incidence (in longitudinal studies). They usually include other variables in their statistical model to account for the other differences between people with and without cancer (confounders). For breast cancer, there is a replicated trend for women with a more \"prudent or healthy\" diet, i.e. higher in fruits and vegetables, to have a lower risk of cancer. A \"drinker dietary pattern\" is also associated with higher breast cancer risk, while the association is inconsistent between a more westernized diet and elevated risk of breast cancer. Pickled foods are linked with cancer.\n\nAlcohol is associated with an increased risk of a number of cancers. 3.6% of all cancer cases and 3.5% of cancer deaths worldwide are attributable to drinking of alcohol. Breast cancer in women is linked with alcohol intake. Alcohol also increases the risk of cancers of the mouth, esophagus, pharynx and larynx, colorectal cancer, liver cancer, stomach and ovaries. The International Agency for Research on Cancer (Centre International de Recherche sur le Cancer) of the World Health Organization has classified alcohol as a Group 1 carcinogen. Its evaluation states, \"There is sufficient evidence for the carcinogenicity of alcoholic beverages in humans. …Alcoholic beverages are carcinogenic to humans (Group 1).\"\n\nOn October 26, 2015, the International Agency for Research on Cancer of the World Health Organization reported that eating processed meat (e.g., bacon, ham, hot dogs, sausages) or red meat was linked to some cancers.\n\nThe evidence on the effect of dietary fiber on the risk of colon cancer is mixed with some types of evidence showing a benefit and others not. While eating fruit and vegetables has a benefit, it has less benefit on reducing cancer than once thought.\n\nA 2014 study found fruit but not vegetables protected against upper gastrointestinal tract cancer. While fruit, vegetable and fiber protected against colorectal cancer and fiber protected against liver cancer.\n\nFlavonoids (specifically flavonoids such as the catechins) are \"the most common group of polyphenolic compounds in the human diet and are found ubiquitously in plants.\" While some studies have suggested flavonoids may have a role in cancer prevention, others have been inconclusive or suggested they may be harmful.\n\nAccording to Cancer Research UK, \"there is currently no evidence that any type of mushroom or mushroom extract can prevent or cure cancer\", although research into some species continues.\n\nAccording to the American Cancer Society, although laboratory research has shown the possibility of some connection between soybeans and cancer, as yet there is no conclusive evidence about the anti-cancer effect of soy on human beings.\n\nLaboratory experiments have found that turmeric might have an anti-cancer effect. Although trials are ongoing, large doses would need to be taken for any effect. It is not known what, in any, positive effect turmeric has for human beings with cancer.\n\nAlthough green tea has been promoted for its anti-cancer effect, research into it has produced mixed results; it is not known if it helps people prevent or treat cancer. A review of all published studies by the US Food and Drug Administration in 2011 concluded it is very unlikely that green tea prevents any kind of cancer in humans.\n\nResveratrol has shown anti-cancer activity in laboratory experiments, but , there is no evidence of an effect on cancer in humans.\n\nVitamin D supplements have been widely marketed on the internet and elsewhere for their claimed anti-cancer properties. There is however insufficient evidence to recommend that vitamin D be prescribed for people with cancer, although there is some evidence that hypovitaminosis D may be associated with a worse outcome for some cancers. A 2014 systematic review by the Cochrane Collaboration found, \"no firm evidence that vitamin D supplementation decreases or increases cancer occurrence in predominantly elderly community-dwelling women.\"\n\nAlthough numerous cellular mechanisms are involved in food intake, many investigations over the past decades have pointed out defects in the methionine metabolic pathway as cause of carcinogenesis. For instance, deficiencies of the main dietary sources of methyl donors, methionine and choline, lead to the formation of liver cancer in rodents. Methionine is an essential amino acid that must be provided by dietary intake of proteins or methyl donors (choline and betaine found in beef, eggs and some vegetables). Assimilated methionine is transformed in S-adenosyl methionine (SAM) which is a key metabolite for polyamine synthesis, e.g. spermidine, and cysteine formation (see the figure on the right). Methionine breakdown products are also recycled back into methionine by homocysteine remethylation and methylthioadenosine (MTA) conversion (see the figure on the right). Vitamins B, B, folic acid and choline are essential cofactors for these reactions. SAM is the substrate for methylation reactions catalyzed by DNA, RNA and protein methyltransferases. \n\nThe products of these reactions are methylated DNA, RNA or proteins and S-adenosylhomocysteine (SAH). SAH has a negative feedback on its own production as an inhibitor of methyltransferase enzymes. Therefore, SAM:SAH ratio directly regulates cellular methylation, whereas levels of vitamins B, B, folic acid and choline regulates indirectly the methylation state via the methionine metabolism cycle. A near ubiquitous feature of cancer is a maladaption of the methionine metabolic pathway in response to genetic or environmental conditions resulting in depletion of SAM and/or SAM-dependent methylation. Whether it is deficiency in enzymes such as methylthioadenosine phosphorylase, methionine-dependency of cancer cells, high levels of polyamine synthesis in cancer, or induction of cancer through a diet deprived of extrinsic methyl donors or enhanced in methylation inhibitors, tumor formation is strongly correlated with a decrease in levels of SAM in mice, rats and humans.\n\nAccording to a 2012 review, the effect of methionine restriction on cancer has yet to be studied directly in humans and \"there is still insufficient knowledge to give reliable nutritional advice\".\n\nMultiple oncogenic signaling pathways have been involved in the processes of cancer cell invasion and metastasis. Among these signaling pathways, Wnt and Hedgehog signaling pathways are involved in the embryonic development, in the biology of cancer stem cells (CSCs) and in the acquisition of epithelial to mesenchymal transition (EMT).\n\n"}
{"id": "38501477", "url": "https://en.wikipedia.org/wiki?curid=38501477", "title": "Donabedian model", "text": "Donabedian model\n\nThe Donabedian model is a conceptual model that provides a framework for examining health services and evaluating quality of health care. According to the model, information about quality of care can be drawn from three categories: “structure,” “process,” and “outcomes.\" Structure describes the context in which care is delivered, including hospital buildings, staff, financing, and equipment. Process denotes the transactions between patients and providers throughout the delivery of healthcare. Finally, outcomes refer to the effects of healthcare on the health status of patients and populations. Avedis Donabedian, a physician and health services researcher at the University of Michigan, developed the original model in 1966. While there are other quality of care frameworks, including the World Health Organization (WHO)-Recommended Quality of Care Framework and the Bamako Initiative, the Donabedian Model continues to be the dominant paradigm for assessing the quality of health care.\n\nThe model is most often represented by a chain of three boxes containing structure, process, and outcome connected by unidirectional arrows in that order. These boxes represent three types of information that may be collected in order to draw inferences about quality of care in a given system \"Donabedian(2003). An introduction to quality assurance in health care. (1st ed., Vol. 1). New York, NY: Oxford University Press.</ref>\n\nStructure includes all of the factors that affect the context in which care is delivered. This includes the physical facility, equipment, and human resources, as well as organizational characteristics such as staff training and payment methods. These factors control how providers and patients in a healthcare system act and are measures of the average quality of care within a facility or system. Structure is often easy to observe and measure and it may be the upstream cause of problems identified in process.\n\nProcess is the sum of all actions that make up healthcare. These commonly include diagnosis, treatment, preventive care, and patient education but may be expanded to include actions taken by the patients or their families. Processes can be further classified as technical processes, how care is delivered, or interpersonal processes, which all encompass the manner in which care is delivered. According to Donabedian, the measurement of process is nearly equivalent to the measurement of quality of care because process contains all acts of healthcare delivery. Information about process can be obtained from medical records, interviews with patients and practitioners, or direct observations of healthcare visits.\n\nOutcome contains all the effects of healthcare on patients or populations, including changes to health status, behavior, or knowledge as well as patient satisfaction and health-related quality of life. Outcomes are sometimes seen as the most important indicators of quality because improving patient health status is the primary goal of healthcare. However, accurately measuring outcomes that can be attributed exclusively to healthcare is very difficult. Drawing connections between process and outcomes often requires large sample populations, adjustments by case mix, and long-term follow ups as outcomes may take considerable time to become observable.\n\nAlthough it is widely recognized and applied in many health care related fields, the Donabedian Model was developed to assess quality of care in clinical practice. The model does not have an implicit definition of quality care so that it can be applied to problems of broad or narrow scope. Donabedian notes that each of the three domains has advantages and disadvantages that necessitate researchers to draw connections between them in order to create a chain of causation that is conceptually useful for understanding systems as well as designing experiments and interventions.\n\nDonabedian developed his quality of care framework to be flexible enough for application in diverse healthcare settings and among various levels within a delivery system.\n\nAt its most basic level, the framework can be used to modify structures and processes within a healthcare delivery unit, such as a small group practice or ambulatory care center, to improve patient flow or information exchange. For instance, health administrators in a small physician practice may be interested in improving their treatment coordination process through enhanced communication of lab results from laboratorian to provider in an effort to streamline patient care. The process for information exchange, in this case the transfer of lab results to the attending physician, depends on the structure for receiving and interpreting results. The structure could involve an electronic health record (EHR) that a laboratorian fills out with lab results for use by the physician to complete a diagnosis. To improve this process, a healthcare administrator may look at the structure and decide to purchase an information technology (IT) solution of pop-up alerts for actionable lab results to incorporate into the EHR. The process could be modified through a change in standard protocol of determining how and when an alert is released and who is responsible for each step in the process. The outcomes to evaluate the efficacy of this quality improvement (QI) solution might include patient satisfaction, timeliness of diagnosis, or clinical outcomes.\n\nIn addition to examining quality within a healthcare delivery unit, the Donabedian model is applicable to the structure and process for treating certain diseases and conditions with the aim to improve the quality of chronic disease management. For example, systemic lupus erythematosus (SLE) is a condition with significant morbidity and mortality and substantial disparities in outcomes among rheumatic diseases. The propensity for SLE care to be fragmented and poorly coordinated, as well as evidence that healthcare system factors associated with improved SLE outcomes are modifiable, points to an opportunity for process improvement through changes in preventive care, monitoring, and effective self-care. A researcher may develop evidence within these areas to analyze the relationship between structure and process to outcomes in SLE care for the purposes of finding solutions to improve outcomes. An analysis of SLE care structure may reveal an association between access to care and financing to quality outcomes. An analysis of process may look at hospital and physician specialty in SLE care and how it relates to SLE mortality in hospitals, or the effect on outcomes by including additional QI indicators to the diagnosis and treatment of SLE. To assess these changes in structure and process, evidence garnered from changes in mortality, disease damage, and health-related quality of life would be used to validate structure-process changes.\n\nDonabedian’s model can also be applied to a large health system to measure overall quality and align improvement work across a hospital, group practice or the large integrated health system to improve quality and outcomes for a population. In 2007, the US Institute for Healthcare Improvement proposed “whole system measures” that address structure, process, and outcomes of care. These indicators supply health care leaders with data to evaluate the organization’s performance in order to design strategic QI planning. The indicators are limited to 13 non-disease specific measures that provide system-level indications of quality, applicable to both inpatient and outpatient settings and across the continuum of care. In addition to informing the QI plan, these measures can be used to evaluate the quality of the system’s care over time, how it performs relative to stated strategic planning goals, and how it performs compared to similar organizations.\n\nWhile the Donabedian model continues to serve as a touchstone framework in health services research, potential limitations have been suggested by other researchers, and, in some cases, adaptations of the model have been proposed. The sequential progression from structure to process to outcome has been described by some as too linear of a framework, and consequently has a limited utility for recognizing how the three domains influence and interact with each other. The model has also been criticized for failing to incorporate antecedent characteristics (e.g. patient characteristics, environmental factors) which are important precursors to evaluating quality care. Coyle and Battles suggest that these factors are vital to fully understanding the true effectiveness of new strategies or modifications within the care process. According to Coyle and Battles, patient factors include genetics, socio-demographics, health habits, beliefs and attitudes, and preferences. Environmental factors include the patients' cultural, social, political, personal, and physical characteristics, as well as factors related to the health profession itself.\n\nAvedis Donabedian first described the three elements of the Donabedian Model in his 1966 article, “Evaluating the Quality of Medical Care.” As a preface to his analysis of methodologies used in health services research, Donabedian identified the three dimensions that can be utilized to assess quality of care (structure, process, and outcome) that would later become the core divisions of the Donabedian Model. “Evaluating the Quality of Medical Care” became one of the most frequently cited public health-related articles of the 20th century, and the Donabedian Model gained widespread acceptance.\n\nIn 1980, Donabedian published The Definition of Quality and Approaches to its Assessment, vol. 1: Explorations in Quality Assessment and Monitoring, which provided a more in-depth description of the structure—process– outcome paradigm. In his book, Donabedian once again defines structure, process, and outcome, and clarifies that these categories should not be mistaken for attributes of quality, but rather they are the classifications for the types of information that can be obtained in order to infer whether the quality of care is poor, fair, or good. Furthermore, he states that in order to make inferences about quality, there needs to be an established relationship between the three categories and that this relationship between categories is a probability rather than a certainty.\n"}
{"id": "42022888", "url": "https://en.wikipedia.org/wiki?curid=42022888", "title": "EConsult", "text": "EConsult\n\neConsult is a medical app developed by the Hurley Group. \n\nIn June 2018 the practice announced that it planned to use the eConsult online system for most consultations. This enables patients to submit their symptoms to a GP electronically, and offers around the clock NHS self-help information, signposting to services, and a symptom checker. 12 GPs are using eConsult and it is planned to recruit 12 more, so patients will only be seen in person where necessary. 340 other practices are also using eConsult, a majority of the 479 GP practices are expected to have implemented an online consultation system by April 2018.\n\n"}
{"id": "4151398", "url": "https://en.wikipedia.org/wiki?curid=4151398", "title": "EcoHealth", "text": "EcoHealth\n\nEcoHealth is an emerging field of study researching how changes in the earth’s ecosystems affect human health. It has many prospects. EcoHealth examines changes in the biological, physical, social and economic environments and relates these changes to human health. Examples of these changes and their effects abound. Common examples include increases in asthma rates due to air pollution, PCB contamination of game fish in the Great Lakes of the United States, and habitat fragmentation leading to increasing rates of Lyme disease.\n\nRecently virulent new infectious diseases such as SARS, Ebola virus, Nipah virus, bird flu and hantavirus have all been found to result from ecosystem change created by humans. These diseases have high death rates and very few effective therapies.\n\nEcoHealth is bringing together physicians, veterinarians, ecologists, economists, social scientists, planners and others to study and understand how ecosystem changes affect human health. EcoHealth strives to provide innovative, practical solutions to reduce or reverse the negative health effects of ecosystem change.\n\nEcosystem approaches to health, or ecohealth, emerged as a defined field of inquiry and application in the 1990s, primarily through the global research supported by the International Development Research Centre in Ottawa (IDRC), Canada (Lebel, 2003). However, this was a resurrection of an approach to health and ecology that can be traced back, in Western societies, to Hippocrates, and to much earlier eras in Eastern societies. The approach was prominent among many scientists in the 18th and 19th centuries, but fell into disfavour in the twentieth century, when technical professionalism and expertise were assumed to be sufficient to deal with health and disease. In this relatively brief era, evaluation of the negative human health impacts of environmental change (both the natural and built environment) was allotted to the fields of medicine and environmental health. One medicine, as championed by scholars and practitioners such as Calvin Schwabe, was largely considered a marginal activity.\n\nIntegrated approaches to health and ecology re-emerged in the 1990s, and included one health, conservation medicine, ecological resilience, ecological integrity, health communities, and a variety of other approaches. These new movements were able to draw on a tradition that stretches from Hippocrates, to Rudolf Virchow and Louis Pasteur, who did not recognize the boundaries between human and animal medicine, and environmental and social change; to William Osler, who was a member of both the McGill medical faculty and the Montreal Veterinary College; Calvin Schwabe, whose 1984 book, Veterinary Medicine and Human Health, is a classic in the field; and James Steele, who founded the first veterinary public health unit in the United States.\n\nEcohealth approaches as currently practiced are participatory, systems-based approaches to understanding and promoting health and wellbeing in the context of social and ecological interactions. What differentiates these approaches from earlier integrative attempts is a firm grounding in complexity theories and post-normal science (Waltner-Toews, 2004; Waltner-Toews \"et al.\", 2008). While a variety of organizations promote integrative approaches such as One Health, the primary funder and promoter of ecohealth in particular, world-wide, is the International Development Research Centre in Ottawa (http://www.idrc.ca/ecohealth/).\n\nAfter a decade of international conferences in North America and Australia under the more contentious umbrella of \"ecosystem health\", the first \"ecosystem approach to human health\" (ecohealth) forum was held in Montreal in 2003, followed by conferences and forums in Wisconsin, U.S., and Mérida, Mexico, all with major support from IDRC. Since then the International Association for Ecology and Health, and the journal Ecohealth have established the field as a legitimate scholarly and development activity (www.ecohealth.net).\n\nEcoHealth studies differ from traditional, single discipline studies. A traditional epidemiological study may show increasing rates of malaria in a region, but not address how or why the rate is increasing. An environmental health study may recommend the spraying of a pesticide in certain amounts in certain areas to reduce spread. An economic analysis may calculate the cost and effectiveness per dollar spent on such a program.\nAn EcoHealth study uses a different approach. It brings the multiple specialist disciplines together with members of the affected community before the study begins. Through pre-study meetings the group shares knowledge and adopts a common language. These pre-study meetings often lead to creative and novel approaches and can lead to a more \"socially robust\" solution. EcoHealth practitioners term this synergy transdisciplinarity, and differentiate it from multidiscipline studies.\nEcoHealth studies also value participation of all involved groups, including decision makers and believe issues of equity (between gender, socioeconomic classes, age and even species) are important to fully understand the problem to be studied.\nJean Lebel (2003) phrased \"transdisciplinarity, participation and equity the three pillars of EcoHealth\" (Lebel, 2003). \nThe IDRC now speaks of six principles, instead of three pillars, namely transdisciplinarity, participation, gender and social equity, system-thinking, sustainability and research-to-action (Charron, 2011).\n\nA short example of a transdisciplinary study in the field of EcoHealth appears below. This is excerpted from HEALTH: An Ecosystem Approach, by Jean Lebel. (IDRC 2003, .)\n\n“In Mexico in the 1940s and 1950s, close to 24 000 of the 2.4 million people who caught malaria every year died as a result. Massive use of the powerful insecticide DDT was the linchpin of the government's effort to eradicate the disease. Over time, some progress was made against malaria, but the war was far from won. The use of DDT also posed its own threats to the health of the ecosystem. Moreover, as required by the North American Free Trade Agreement, Mexico had to completely eliminate the use of DDT by 2002.\nTo meet this challenge, an EcoHealth research project was set up to pool the knowledge of a team of specialists in epidemiology, computer science, entomology, and social sciences, from both government and academia.\nThis team has accumulated volumes of information about the prevalence of malaria in 2 000 villages. Data from powerful geographical information systems enabled them to conclude that mosquitoes do not travel very much. \"If you have a place to lay your eggs and feed yourself, why go elsewhere?\" explains Mario Henry Rodriguez, Director of Research on Infectious Diseases at the National Institute for Public Health (NIPH). In addition, as confirmed by Juan Eugenio Hernández, NIPH's Director of Informatics, it is now believed that \"human beings are the vectors of malaria,\" which explains why more cases of malaria are found in villages located alongside roads.”\nWith community help, the team studied the population's living conditions, including behavioural differences between men and women. It was found that while women are more likely to be bitten by mosquitoes early in the morning when they go to fetch water, the men are likely to be bitten in the coffee plantations at night.\nSeveral preventive actions have been taken. The scientists have proposed a new insecticide that, unlike DDT, does not persist in the environment. They have also developed a more effective pump that can spray 40 homes a day instead of 8, and uses less insecticide. A new malaria testing kit now detects the presence or absence of parasites in a patient's blood in only a few minutes, unlike laboratory tests that take three to four weeks to confirm a diagnosis. Previously, the need to wait for test results forced the authorities to treat everyone who showed vague symptoms of the illness, such as a high fever or headaches. Now, volunteers administer these tests to the people in close to 60 villages. \"We have given communities the means to take care of themselves,\" says Mario Rodriguez.\nThe fight against malaria in Mexico is now no longer solely the responsibility of government employees. Women also play a role by removing, every two weeks, the algae that harbour mosquito larvae in bodies of water. As a result, the number of cases of malaria in the state of Oaxaca has dropped from 15 000 in 1998 to only 400 today — and all without using any DDT. \"Our experience has taught us that we need to bolster the social science research component if we want to extend this program to other parts of the country, while maintaining it in Oaxaca. The challenge is to draw the lessons that will lead to application of the program on a much wider scale,\" says Dr Rodriguez.”\n\nThis study reveals both the nature of the complex interactions of the problem and the extent to which a successful solution must cross research disciplines. The solution involved creative thinking on the part of many individuals, and produced a win-win situation for researchers, business and most importantly, for the community. \nAlthough many of the dramatic effects of ecosystem change and much of the research is focused in developing countries, the ecosystem of the built environment in urban areas of the developed world is also a major determinant of human health. Obesity, diabetes, asthma, and heart disease are all directly related to how humans interact with the local urban ecosystem in which they live. Urban design and planning determine car use, food choices available, air pollution levels and the safety and walkability of the neighborhoods in which people live.\nOther examples of the EcoHealth approach can be found in Linking Social and Ecological Systems: Management Practice and Social Mechanisms for Building Resilience, edited by Fikrit Berkes and Carl Folke (1998, Cambridge University Press, ) and Panarchy: Understanding Transformations in Human and Natural Systems, edited by Lance H. Gunderson and C. S. Holling (2002, Island Press, ).\n\nCourses in ecosystem approaches to health have been developed and delivered in several North American and Australian universities. One innovative Canadian course involved a collaboration of all of Canada's veterinary colleges, and focused on integrative field cases in all parts of the country; another Canadian course was a joint effort of the medical school at the University of Western Ontario and the Ontario Veterinary College at the University of Guelph. In the years after 2003, communities of practice for ecosystem approaches to health in Canada, Latin America and the Caribbean, and Africa (largely supported by the IDRC) created intensive graduate level courses that integrated systems approaches with participatory approaches, and used actual case studies so that students gained applied clinical experience in \"transdisciplinary\" problem solving (see www.copeh-canada.org for one example). CoPEH-Canada organizes a yearly professional development and graduate level intensive training course in ecosystem approaches to health and have created a teaching manual with much of their material which is free on their site in English, French and Spanish (www.copeh-canada.org).\n\nJohns Hopkins Bloomberg School of Public Health and the University of Wisconsin–Madison created a website to promote education in this area EcoHealth (Environmental Change and Our Health). The website examines the changes that are transforming Earth and what they can mean for our health.\n\nThis website is geared to middle-school students and their teachers, and delivers scientific information in a kid-friendly, engaging, and visually vibrant manner. Since its initial partnership with the Journey to Planet Earth television mini-series, hosted and narrated by actor Matt Damon and aired on PBS in Spring 2003 and 2004, the website has become an educational complement to this TV series, as well as a dynamic stand-alone tool for students and teachers. Since then, the site's appeal has grown far broader, reaching high-school students, and anyone interested in environmental and health issues—or simply wanting a reliable and fun resource for being able to sort the science from the sound bites. EcoHealth 101 provides the in-depth analysis and context behind today's headline news.\n\n\"The idea for the site grew from the positive feedback following public lectures I've given on global environmental health\", says Dr. Jonathan Patz, associate professor at the University of Wisconsin & adjunct associate professor at Johns Hopkins Bloomberg School of Public Health. \"'Students should really learn about how their health is tied so closely to the global environment,' was the comment I often heard.\"\n\n\"The site is very exciting because it deals with serious and thought-provoking topics\", adds Dr. Patz. \"Nonetheless, it shows middle-school students how to have fun with the visual elements and discover what potential solutions exist for alleviating the negative effects of climate change and other changes to our planet.\"\n\nIn a rapidly changing world, our school curriculum must expand to include accurate science on global warming, environmental degradation, and its negative effects on human, wildlife and ecosystem health. The mission of EcoHealth 101 is to educate and inform the next generation of leaders who may have the answers to today's problems. Chapters from the website include Global Warming, Stratospheric Ozone Depletion, The Balance of Nature, Modern Agriculture and Drinking Water, and Globalization and Disease Without Borders.\n\nThe site was reviewed for accuracy and fairness by science, health, and environmental experts in a wide range of specialties.\n\nVisit http://ecohealth101.org.\n\n\n\n\n\n"}
{"id": "28541070", "url": "https://en.wikipedia.org/wiki?curid=28541070", "title": "Edwina Rogers", "text": "Edwina Rogers\n\nEdwina Rogers ( ; born May 27, 1964) is an American lobbyist and former White House staff member. She is the founder and CEO of the Global Healthspan Policy Institute, the founding Executive Director and current President of the Secular Policy Institute, the CEO of the Center for Prison Reform, and a partner at the law firm of Johnson, Rogers and Clifton.\n\nRogers grew up in rural Alabama. She attended the University of Alabama on scholarship, where she received a Bachelor of Science degree in corporate finance, and later attended The Catholic University of America, from which she received her JD degree.\n\nAfter graduation from law school, Rogers worked on international trade for President George H. W. Bush at the Department of Commerce from 1989 to 1991. She practiced law in the Washington office of Balch and Bingham from 1991 until 1994, then served as General Counsel of the National Republican Senatorial Committee during the Republican take-over of the Senate in 1994. She worked for Senator Trent Lott while he was the Senate Majority Leader in 1999.\n\nShe was an Economic Advisor for President George W. Bush at the White House during 2001 and 2002 at the National Economic Council, focusing on health and social security policy.\n\nRogers handled health policy for Senator Jeff Sessions in 2003 and 2004 before serving as Vice President of the Health Policy for the ERISA (Employee Retirement Income Security Act of 1974) Industry Committee (ERIC) in Washington, DC from May 2004 until January 2009. ERIC advocates the employee benefits and compensation interests of America’s major employers.\n\nShe worked with Senator Paul Coverdell to establish the Fair Government Foundation, a non-profit, non-partisan organization established to research and educate the public on First Amendment rights, campaign finance and political action committees, lobbying, government ethics and election law fairness issues.\n\nRogers was a Fellow at Harvard’s John F. Kennedy School of Government in 1996.\n\nShe served as the founding Executive Director of the Patient-Centered Primary Care Collaborative from 2006-2011, a Washington DC trade association, responsible for the national Patient Centered Medical Home movement and implementing the model around the US. She was a supporter of The Patient Protection and Affordable Care Act (PPACA), commonly called the Affordable Care Act (ACA) or, colloquially, Obamacare.\n\nIn 2012, Rogers signed on as Executive Director of the Secular Coalition for America. The Secular Coalition represents 11 member organizations and their members on Capitol Hill. Rogers’ selection came on the heels of the March 2012 Reason Rally, a Secular Coalition for America sponsored event that drew tens of thousands of atheists, agnostics, humanists and other non-theistic Americans to Washington, D.C.\n\nThe Secular Coalition for America emphasizes separation of church and state, and champions issues with which the Republican Party typically comes into conflict. The people on its staff and in its member organizations typically support gay marriage, think contraception should be more broadly available, and work against school vouchers that use public funds to support \"religious indoctrination.\" As a former Republican staffer, Rogers faced a great deal of scrutiny from the non-religious community when she joined the SCA.\n\nIn answer to her skeptical critics, Rogers describes herself as \"nontheist\" and as a libertarian-leaning economic conservative who is also \"laissez-faire on social issues\". According to Rogers, being a professional lobbyist and political staffer meant going along with certain causes even when she didn't believe in them. In 2007, when Rogers was vice president of health policy for the ERISA Industry Committee, she testified in the House against a bill mandating more generous mental-health coverage, even though she personally favored the legislation. And she handled health-policy issues for pro-life senator Jeff Sessions despite being pro-choice herself. Rogers states she has donated to Planned Parenthood over the past 25 years.\n\nRogers' work for the Secular Coalition included meeting with White House officials and members of Congress, and coordinating events.\n\nOn June 6, 2014, the Secular Coalition for America announced that Rogers had moved on from her role.\n\nRogers is the founder and currently serves as CEO of the Center for Prison Reform for Non-Violent Offenders, a lobbying group that promotes reform of the U.S. prison system's handling of non-violent offenders. She also worked on reports for government on sentence stacking and prison life expectancy. She has said that she views the move towards private prisons as a mistake, as it creates an incentive for the prison companies to lobby to keep the prison population high.\n\nFollowing her work at the Secular Coalition for America, Rogers founded and served as Executive Director of the Secular Policy Institute. She currently serves as President.\n\nIn 2016, Rogers founded the Global Healthspan Policy Institute, where she currently serves as CEO.\n\nRogers has been a regular contributor of conservative newspaper columns, health and policy journals. She has appeared as a commentator for Fox News and MSNBC, and wrote a conservative column for \"The Georgetowner\" newspaper in Washington.\n\nIn 2008 Rogers appeared on the pilot for an NBC television show called \"PowerHouse\", for which she demonstrated a method of wrapping small presents with uncut sheets of U.S. dollar bills. She has said she started doing this as a \"cheap\" and \"unique\" way of wrapping inexpensive gifts while still complying with ethics rules governing gift-giving in Washington.\n\nIn 2010 Rogers made a guest appearance on \"The Real Housewives of D.C.\"\n\nRogers has worked as a lobbyist in the Middle East on behalf of the government of South Korea, and has served on the board of directors for Semco Energy, Inc. (NYSE: SEN), a natural gas distribution company. She currently serves on the advisory boards for eHealth First, The Evolution Institute, and the Lifeboat Foundation, and serves as an expert for Geopolitical Intelligence Services. Rogers is also rumored to be the Vice Chairman (US) for Le Cercle.\n\nIn 1989, she married Ed Rogers, a protégé of political strategist Lee Atwater who served in two presidential administrations and later founded a lobbying firm with former Mississippi governor Haley Barbour. The couple lived in an 18,000-square-foot mansion in McLean, Virginia, and had two children: Hal Rogers (1999) and Sabra Rogers (2002). The couple divorced in 2012 . In 2016 Rogers married Gregory Neimeyer, Ph.D. an American psychologist affiliated with the American Psychological Association in Washington, DC, and with the University of Florida. The Rogers-Neimeyer couple reside in Bethesda, Maryland.\n\n"}
{"id": "50337", "url": "https://en.wikipedia.org/wiki?curid=50337", "title": "Fructose", "text": "Fructose\n\nFructose, or fruit sugar, is a simple ketonic monosaccharide found in many plants, where it is often bonded to glucose to form the disaccharide sucrose. It is one of the three dietary monosaccharides, along with glucose and galactose, that are absorbed directly into blood during digestion. Fructose was discovered by French chemist Augustin-Pierre Dubrunfaut in 1847. The name \"fructose\" was coined in 1857 by the English chemist William Allen Miller. Pure, dry fructose is a sweet, white, odorless, crystalline solid, and is the most water-soluble of all the sugars.\nFructose is found in honey, tree and vine fruits, flowers, berries, and most root vegetables.\n\nCommercially, fructose is derived from sugar cane, sugar beets, and maize. Crystalline fructose is the monosaccharide, dried, ground, and of high purity. High-fructose corn syrup is a mixture of glucose and fructose as monosaccharides. Sucrose is a compound with one molecule of glucose covalently linked to one molecule of fructose. All forms of fructose, including fruits and juices, are commonly added to foods and drinks for palatability and taste enhancement, and for browning of some foods, such as baked goods. About 240,000 tonnes of crystalline fructose are produced annually.\n\nExcessive consumption of fructose may contribute to insulin resistance, obesity, elevated LDL cholesterol and triglycerides, leading to metabolic syndrome, type 2 diabetes and cardiovascular disease. The European Food Safety Authority stated that fructose is preferable over sucrose and glucose in sugar-sweetened foods and beverages because of its lower effect on postprandial blood sugar levels, and also noted that \"high intakes of fructose may lead to metabolic complications such as dyslipidaemia, insulin resistance, and increased visceral adiposity\". Further, the UK’s Scientific Advisory Committee on Nutrition in 2015 disputed the claims of fructose causing metabolic disorders, stating that \"there is insufficient evidence to demonstrate that fructose intake leads to adverse health outcomes independent of any effects related to its presence as a component of total and free sugars.\"\n\nThe word \"fructose\" was coined in 1857 from the Latin for \"fructus\" (fruit) and the generic chemical suffix for sugars, \"-ose\". It is also called fruit sugar and levulose.\n\nFructose is a 6-carbon polyhydroxyketone. Crystalline fructose adopts a cyclic six-membered structure owing to the stability of its hemiketal and internal hydrogen-bonding. This form is formally called -fructopyranose. In solution, fructose exists as an equilibrium mixture of 70% fructopyranose and about 22% fructofuranose, as well as small amounts of three other forms, including the acyclic structure.\n\nFructose may be anaerobically fermented by yeast or bacteria. Yeast enzymes convert sugar (glucose, or fructose) to ethanol and carbon dioxide. The carbon dioxide released during fermentation will remain dissolved in water, where it will reach equilibrium with carbonic acid, unless the fermentation chamber is left open to the air. The dissolved carbon dioxide and carbonic acid produce the carbonation in bottled fermented beverages.\n\nFructose undergoes the Maillard reaction, non-enzymatic browning, with amino acids. Because fructose exists to a greater extent in the open-chain form than does glucose, the initial stages of the Maillard reaction occur more rapidly than with glucose. Therefore, fructose has potential to contribute to changes in food palatability, as well as other nutritional effects, such as excessive browning, volume and tenderness reduction during cake preparation, and formation of mutagenic compounds.\n\nFructose readily dehydrates to give hydroxymethylfurfural (\"HMF\").\nThis process, in the future, may become part of a low-cost, carbon-neutral system to produce replacements for petrol and diesel from plants.\n\nThe primary reason that fructose is used commercially in foods and beverages, besides its low cost, is its high relative sweetness. It is the sweetest of all naturally occurring carbohydrates. The relative sweetness of fructose has been reported in the range of 1.2–1.8 times that of sucrose. However, it is the 6-membered ring form of fructose that is sweeter; the 5-membered ring form tastes about the same as usual table sugar. Warming fructose leads to formation of the 5-membered ring form. Therefore, the relative sweetness decreases with increasing temperature. However it has been observed that the absolute sweetness of fructose is identical at 5 °C as 50 °C and thus the relative sweetness to sucrose is not due to anomeric distribution but a decrease in the absolute sweetness of sucrose at lower temperatures.\n\nThe sweetness of fructose is perceived earlier than that of sucrose or glucose, and the taste sensation reaches a peak (higher than that of sucrose) and diminishes more quickly than that of sucrose. Fructose can also enhance other flavors in the system.\n\nFructose exhibits a sweetness synergy effect when used in combination with other sweeteners. The relative sweetness of fructose blended with sucrose, aspartame, or saccharin is perceived to be greater than the sweetness calculated from individual components.\n\nFructose has higher solubility than other sugars as well as other sugar alcohols. Fructose is, therefore, difficult to crystallize from an aqueous solution. Sugar mixes containing fructose, such as candies, are softer than those containing other sugars because of the greater solubility of fructose.\n\nFructose is quicker to absorb moisture and slower to release it to the environment than sucrose, glucose, or other nutritive sweeteners. Fructose is an excellent humectant and retains moisture for a long period of time even at low relative humidity (RH). Therefore, fructose can contribute a more palatable texture, and longer shelf life to the food products in which it is used.\n\nFructose has a greater effect on freezing point depression than disaccharides or oligosaccharides, which may protect the integrity of cell walls of fruit by reducing ice crystal formation. However, this characteristic may be undesirable in soft-serve or hard-frozen dairy desserts.\n\nFructose increases starch viscosity more rapidly and achieves a higher final viscosity than sucrose because fructose lowers the temperature required during gelatinizing of starch, causing a greater final viscosity.\n\nAlthough some artificial sweeteners are not suitable for home-baking, many traditional recipes use fructose.\n\nNatural sources of fructose include fruits, vegetables (including sugar cane), and honey. Fructose is often further concentrated from these sources. The highest dietary sources of fructose, besides pure crystalline fructose, are foods containing table sugar (sucrose), high-fructose corn syrup, agave nectar, honey, molasses, maple syrup, fruit and fruit juices, as these have the highest percentages of fructose (including fructose in sucrose) per serving compared to other common foods and ingredients. Fructose exists in foods either as a free monosaccharide or bound to glucose as sucrose, a disaccharide. Fructose, glucose, and sucrose may all be present in a food; however, different foods will have varying levels of each of these three sugars.\n\nThe sugar contents of common fruits and vegetables are presented in Table 1. In general, in foods that contain free fructose, the ratio of fructose to glucose is approximately 1:1; that is, foods with fructose usually contain about an equal amount of free glucose. A value that is above 1 indicates a higher proportion of fructose to glucose, and below 1 a lower proportion. Some fruits have larger proportions of fructose to glucose compared to others. For example, apples and pears contain more than twice as much free fructose as glucose, while for apricots the proportion is less than half as much fructose as glucose.\n\nApple and pear juices are of particular interest to pediatricians because the high concentrations of free fructose in these juices can cause diarrhea in children. The cells (enterocytes) that line children's small intestines have less affinity for fructose absorption than for glucose and sucrose. Unabsorbed fructose creates higher osmolarity in the small intestine, which draws water into the gastrointestinal tract, resulting in osmotic diarrhea. This phenomenon is discussed in greater detail in the Health Effects section.\n\nTable 1 also shows the amount of sucrose found in common fruits and vegetables. Sugarcane and sugar beet have a high concentration of sucrose, and are used for commercial preparation of pure sucrose. Extracted cane or beet juice is clarified, removing impurities; and concentrated by removing excess water. The end-product is 99.9%-pure sucrose. Sucrose-containing sugars include common table white granulated sugar and powdered sugar, as well as brown sugar.\n\nAll data with a unit of g (gram) are based on 100 g of a food item.\nThe fructose/glucose ratio is calculated by dividing the sum of free fructose plus half sucrose by the sum of free glucose plus half sucrose.\n\nFructose is also found in the manufactured sweetener, high-fructose corn syrup (HFCS), which is produced by treating corn syrup with enzymes, converting glucose into fructose. The common designations for fructose content, HFCS-42 and HFCS-55, indicate the percentage of fructose present in HFCS. HFCS-55 is commonly used as a sweetener for soft drinks, whereas HFCS-42 is used to sweeten processed foods, breakfast cereals, bakery foods, and some soft drinks.\n\nData obtained from Kretchmer, N. & Hollenbeck, CB (1991). Sugars and Sweeteners, Boca Raton, FL: CRC Press, Inc. for HFCS, and USDA for fruits and vegetables and the other refined sugars.\n\nCane and beet sugars have been used as the major sweetener in food manufacturing for centuries. However, with the development of HFCS, a significant shift occurred in the type of sweetener consumption in certain countries, particularly the United States. Contrary to the popular belief, however, with the increase of HFCS consumption, the total fructose intake relative to the total glucose intake has not dramatically changed. Granulated sugar is 99.9%-pure sucrose, which means that it has equal ratio of fructose to glucose. The most commonly used forms of HFCS, HFCS-42, and HFCS-55, have a roughly equal ratio of fructose to glucose, with minor differences. HFCS has simply replaced sucrose as a sweetener. Therefore, despite the changes in the sweetener consumption, the ratio of glucose to fructose intake has remained relatively constant.\n\nProviding 368 kcal per 100 grams of dry powder (table), fructose has 95% the caloric value of sucrose by weight. Fructose powder is 100% carbohydrates and supplies no nutrients in significant content (table).\n\nFructose exists in foods either as a monosaccharide (free fructose) or as a unit of a disaccharide (sucrose). Free fructose is absorbed directly by the intestine. When fructose is consumed in the form of sucrose, it is digested (broken down) and then absorbed as free fructose. As sucrose comes into contact with the membrane of the small intestine, the enzyme sucrase catalyzes the cleavage of sucrose to yield one glucose unit and one fructose unit, which are then each absorbed. After absorption, it enters the hepatic portal vein and is directed toward the liver.\n\nThe mechanism of fructose absorption in the small intestine is not completely understood. Some evidence suggests active transport, because fructose uptake has been shown to occur against a concentration gradient. However, the majority of research supports the claim that fructose absorption occurs on the mucosal membrane via facilitated transport involving GLUT5 transport proteins. Since the concentration of fructose is higher in the lumen, fructose is able to flow down a concentration gradient into the enterocytes, assisted by transport proteins. Fructose may be transported out of the enterocyte across the basolateral membrane by either GLUT2 or GLUT5, although the GLUT2 transporter has a greater capacity for transporting fructose, and, therefore, the majority of fructose is transported out of the enterocyte through GLUT2.\n\nThe absorption capacity for fructose in monosaccharide form ranges from less than 5 g to 50 g (per individual serving) and adapts with changes in dietary fructose intake. Studies show the greatest absorption rate occurs when glucose and fructose are administered in equal quantities. When fructose is ingested as part of the disaccharide sucrose, absorption capacity is much higher because fructose exists in a 1:1 ratio with glucose. It appears that the GLUT5 transfer rate may be saturated at low levels, and absorption is increased through joint absorption with glucose. One proposed mechanism for this phenomenon is a glucose-dependent cotransport of fructose.\nIn addition, fructose transfer activity increases with dietary fructose intake. The presence of fructose in the lumen causes increased mRNA transcription of GLUT5, leading to increased transport proteins. High-fructose diets (>2.4 g/kg body wt) increase transport proteins within three days of intake.\n\nSeveral studies have measured the intestinal absorption of fructose using the hydrogen breath test. These studies indicate that fructose is not completely absorbed in the small intestine. When fructose is not absorbed in the small intestine, it is transported into the large intestine, where it is fermented by the colonic flora. Hydrogen is produced during the fermentation process and dissolves into the blood of the portal vein. This hydrogen is transported to the lungs, where it is exchanged across the lungs and is measurable by the hydrogen breath test. The colonic flora also produces carbon dioxide, short-chain fatty acids, organic acids, and trace gases in the presence of unabsorbed fructose. The presence of gases and organic acids in the large intestine causes gastrointestinal symptoms such as bloating, diarrhea, flatulence, and gastrointestial pain Exercise immediately after consumption can exacerbate these symptoms by decreasing transit time in the small intestine, resulting in a greater amount of fructose emptied into the large intestine.\n\nAll three dietary monosaccharides are transported into the liver by the GLUT2 transporter. Fructose and galactose are phosphorylated in the liver by fructokinase (K= 0.5 mM) and galactokinase (K = 0.8 mM), respectively. By contrast, glucose tends to pass through the liver (K of hepatic glucokinase = 10 mM) and can be metabolised anywhere in the body. Uptake of fructose by the liver is not regulated by insulin. However, insulin is capable of increasing the abundance and functional activity of GLUT5 in skeletal muscle cells.\n\nThe initial catabolism of fructose is sometimes referred to as fructolysis, in analogy with glycolysis, the catabolism of glucose. In fructolysis, the enzyme fructokinase initially produces fructose 1-phosphate, which is split by aldolase B to produce the trioses dihydroxyacetone phosphate (DHAP) and glyceraldehyde . Unlike glycolysis, in fructolysis the triose glyceraldehyde lacks a phosphate group. A third enzyme, triokinase, is therefore required to phosphorylate glyceraldehyde, producing glyceraldehyde 3-phosphate. The resulting trioses are identical to those obtained in glycolysis and can enter the gluconeogenic pathway for glucose or glycogen synthesis, or be further catabolized through the lower glycolytic pathway to pyruvate.\n\nThe first step in the metabolism of fructose is the phosphorylation of fructose to fructose 1-phosphate by fructokinase, thus trapping fructose for metabolism in the liver. Fructose 1-phosphate then undergoes hydrolysis by aldolase B to form DHAP and glyceraldehydes; DHAP can either be isomerized to glyceraldehyde 3-phosphate by triosephosphate isomerase or undergo reduction to glycerol 3-phosphate by glycerol 3-phosphate dehydrogenase. The glyceraldehyde produced may also be converted to glyceraldehyde 3-phosphate by glyceraldehyde kinase or further converted to glycerol 3-phosphate by glycerol 3-phosphate dehydrogenase. The metabolism of fructose at this point yields intermediates in the gluconeogenic pathway leading to glycogen synthesis as well as fatty acid and triglyceride synthesis.\n\nThe resultant glyceraldehyde formed by aldolase B then undergoes phosphorylation to glyceraldehyde 3-phosphate. Increased concentrations of DHAP and glyceraldehyde 3-phosphate in the liver drive the gluconeogenic pathway toward glucose and subsequent glycogen synthesis. It appears that fructose is a better substrate for glycogen synthesis than glucose and that glycogen replenishment takes precedence over triglyceride formation. Once liver glycogen is replenished, the intermediates of fructose metabolism are primarily directed toward triglyceride synthesis.\n\nCarbons from dietary fructose are found in both the free fatty acid and glycerol moieties of plasma triglycerides. High fructose consumption can lead to excess pyruvate production, causing a buildup of Krebs cycle intermediates. Accumulated citrate can be transported from the mitochondria into the cytosol of hepatocytes, converted to acetyl CoA by citrate lyase and directed toward fatty acid synthesis. In addition, DHAP can be converted to glycerol 3-phosphate, providing the glycerol backbone for the triglyceride molecule. Triglycerides are incorporated into very-low-density lipoproteins (VLDL), which are released from the liver destined toward peripheral tissues for storage in both fat and muscle cells.\n\nIn a meta-analysis of clinical trials with controlled feeding — where test subjects were fed a fixed amount of energy rather than being allowed to choose the amount they ate — fructose was not an independent factor for weight gain; however, fructose consumption was associated with weight gain when the fructose provided excess calories.\n\nAn expert panel of the European Food Safety Authority concluded that fructose is preferred in food and beverage manufacturing to replace sucrose and glucose due to the lower effect of fructose on blood glucose levels following a meal. However, as a common sweetening agent for foods and beverages, fructose has been associated with increased risk of obesity, diabetes, and cardiovascular disorders that are part of metabolic syndrome. Clinical research has provided no or only limited direct evidence that fructose itself is associated with elevated LDL cholesterol and triglycerides leading to metabolic syndrome, but rather indicates that excessive consumption of sugar-sweetened foods and beverages, and the concurrent increase in calorie intake, underlies metabolic syndrome. Similarly, increased consumption of sweetened foods and beverages raises risk of cardiovascular disease, including hypertension, but there is no direct cause and effect relationship in humans showing that fructose is the causative factor.\n\nFructose is often recommended for diabetics because it does not trigger the production of insulin by pancreatic β cells, probably because β cells have low levels of GLUT5, For a 50 gram reference amount, fructose has a glycemic index of 23, compared with 100 for glucose and 60 for sucrose. Fructose is also 73% sweeter than sucrose at room temperature, allowing diabetics to use less of it per serving. Fructose consumed before a meal may reduce the glycemic response of the meal. Fructose-sweetened food and beverage products cause less of a rise in blood glucose levels than do those manufactured with sucrose or glucose.\n\n"}
{"id": "2058099", "url": "https://en.wikipedia.org/wiki?curid=2058099", "title": "Grape seed extract", "text": "Grape seed extract\n\nGrape seed extract is an industrial derivative of whole grape seeds. The extract contains proanthocyanidins. Grape seed extract quality is measured by the content of procyanidins which are formed from proanthocyanidins. Generally, grape seed extract quality contains 95% procyanidins, but potency varies among products.. Eating foods or beverages high in procyanidin results in the sensation of the mouth puckering and dehydration, otherwise known as astringency, as felt after certain alcoholic drinks.\n\nThe properties of grape seed extract depends on the extraction process used to obtain it and how the grapes were grown. The classic method incorporates extraction with organic solvents such as acetone, acetonitrile, ethyl acetate, and methanol. Other methods using hot water have been used, but are not as effective in maximizing in extract production in both quantity and efficiency. High performance liquid chromatography seems to be the most effective analysis along with proton NMR spectroscopy with principal component analysis to ensure accurate composition.\n\nA meta-analysis of 16 randomized controlled trials concluded that grape seed extract, in a dose of under 800 milligrams per day over at least 8 weeks, significantly lowered systolic and diastolic blood pressure, although the amounts were small (3–6 mmHg) and occurred only in obese people under age 50 with existing metabolic syndrome and hypertension. An earlier meta-analysis reported lower systolic blood pressure and heart rate, with no effect on blood lipids or C-reactive protein levels.\n\nThe US National Center for Complementary and Integrative Health reported that oral administration of grape seed extract (dose and frequency unreported) was well tolerated in people over 14 weeks. Side effects may include itchy scalp, dizziness, headache, high blood pressure, and nausea. In alternative medicine, grape seed extract is sold in dietary supplement form and claimed to have numerous health benefits, none of which is supported by sufficient medical evidence.\n\n"}
{"id": "50741092", "url": "https://en.wikipedia.org/wiki?curid=50741092", "title": "Health Intervention and Technology Assessment Program", "text": "Health Intervention and Technology Assessment Program\n\nThe Health Intervention and Technology Assessment Program (HITAP) is a semi-autonomous research unit under Thailand’s Ministry of Public Health. It was established in 2007 as a non-profit organization in order to take responsibility for appraising a wide range of health technologies and programs, including pharmaceuticals, medical devices, interventions, individual and community health promotion, and disease prevention as well as social health policy to inform policy decisions in Thailand. \nHITAP assumes an advisory role to health governmental authorities by providing rigorous scientific evidence through professional assessment of health data in support of public decision-making. These assessments cover a range of topics including system design, selection of technologies for assessment, and the actual assessment of those selected and agreed upon by relevant government agencies.\nIn this effort, HITAP publishes research and studies in the following areas: methodological development, (HTA and cost) databases and guidelines; knowledge transfer and exchange (KTE) and capacity development; technology assessments on drugs, medical devices, medical procedures, disease prevention and health promotion measures; benefit packages of care – mixing screening and treatments; and other public health policies, e.g. evaluation of Thailand’s government compulsory license policy.\n\nHTA is well known as a useful tool to assist health policy decision-making. As a result, there are attempts to establish HTA in Thailand in various sectors. An early attempt from governmental side was the production of a background paper on HTA for the National Public Health Assembly, later known as National Health Assembly, commissioned by Thailand’s Health System Research Institute (HSRI) to a highly respected clinical epidemiologist, Professor Dr. Chitr Sitthi-Amorn, in 1988. Following the initiation, there were attempts to create HTA agencies either inside or outside the Ministry of Public Health with the aim to use HTA to inform health policy-makings. These include at least two currently discontinued projects, namely, a collaborative project between Thailand and Sweden called Technology Assessment and Social Security in Thailand (TASSIT) and a Thailand and Australia collaboration called Setting Priorities Using Information on Cost-Effectiveness (SPICE). Another attempt was made to create Institute of Medical Research and Technology assessment (IMRTA) in the Department of Medical Services in the Ministry. The IMRTA, unlike the other two examples, continues to generate HTA evidence and build HTA capacity to these days.\n\nDr.Yot Teerawattanon and Dr.Sripen Tantivess, the two co-founders of HITAP, were research fellows under the mentorship of Dr.Viroj Tangcharoensathien at the International Health Policy Program (IHPP), which was then led by Dr.Suwit Wibulpolpresert, before they got scholarship to pursue their PhD in the UK. After their return in 2006, with consultation from the two mentors, the idea to initiate HITAP materialized. With a seed grant from the Thai Health Promotion Foundation (ThaiHealth) for $1,000,000 to be spent in the period of 3 years, HITAP started out with less than 10 staff with Dr.Yot as the Program Leader and Dr.Sripen as a senior researcher. To maintain the link with policy decision-making bodies without impairing its independency and flexibility, HITAP opted for two statuses: a program under IHPP in the Bureau of Policy and Strategy in the Ministry of Public Health and the HITAP Foundation which is the managing body for HITAP. While the first status keep HITAP in close contact with policy-makers, the latter allow HITAP to work to respond to the broader need in and outside to country. No government budget was allocated for HITAP nor, to avoid conflict of interest, do HITAP receive funding support from industries.\n\nWith the aim to link its work to policy decision-making, the first batch of HITAP’s research topics were derived from the list of prioritized topic compiled by policy-makers. HITAP sent out letters calling for topic nominations from policy-making bodies such as departments in the Ministry of Public Health and healthcare payers, review about the nominated topics and prioritize them according to the criteria constructed based on a literature review before presenting them to the policy-makers in a topic selection workshop for their comments and final says on the prioritized topics. With this approach, all of HITAP’s first batch HTA researches was used to inform policy decision-making, mostly for the development of the Thai pharmaceutical reimbursement called the National List of Essential Medicine (NLEM) and the Benefit Package (BP) under the Universal Coverage Scheme (UCS). This build a path to formal channel of integrating HTA to the development of both NLEM and BP later.\n\nIn operationalizing HITAP’s goals and in fulfilling its advisory role in the decision making process HITAP follows the five key strategies of having an 1) HTA Fundamental System; 2) strengthening Human Capacity; 3) HTA Research; 4) Knowledge Management; and 5) creating an HTA Network.\n\nHaving an HTA fundamental system entails basic research and development for HTA or the creation of infrastructure to support health intervention and technology assessment by developing standards that are on par with the international level whilst taking into account resource constraints in the Thai context. Under this strategy, methodological guidelines, a database of HTA studies in Thailand, tools and quality of life measures for cost-utility analysis, and a social value based threshold ceiling were developed.\nTo strengthen Human Capacity is to work at the individual, institutional and system levels. To date, HITAP along with its partners have sponsored the completion of graduate studies at both the masters (17 staff) and doctoral level (8 staff) with the aim of strengthening staff technical competency for research. In addition, for stakeholders interested in HTA, training courses are organized annually for physicians, pharmacists, public health administrators, and others.\nHITAP’s core business is HTA Research. The strategy therefore addresses the growing demand for studies, particularly on cost-effectiveness and budget impact appraisals in Thailand. HITAP’s current body of research includes 124 published reports and publications on pharmaceuticals (26), medical devices (8), medical procedures (4), health interventions (26), health promotion and disease prevention interventions (38), health policy (8), and others (14).\nThrough applying a knowledge management strategy and by disseminating research results to all relevant stakeholders, translation and integration of research findings into policy becomes effective. HITAP actively engages stakeholders through a variety of channels – meetings, seminars, press releass, blogs and social media including, Facebook, Twitter and online campaigns.\nIts fifth strategy is creating an HTA network. HITAP’s collaborations operate at the regional, national and international levels including academic institutes, health professional consortia and associations, and health providers. Notably, HITAP is in partnership with more than 20 HTA institutes in Asia and additionally involved as a core partner in the international Decision Support Initiative (IDSI) of NICE International, UK funded by the Bill and Melinda Gates Foundation.\n\nThe National Health Security Office (NHSO), which institutes and manages the largest health plan in Thailand (Universal Coverage Scheme [UC]), initiated a collaborative research and development project with two independent research institutes – the Health Intervention and Technology Assessment Program and the International Health Policy Program – in 2009. The aim of the project was to develop an optimal strategy for the development of the UC benefit package, that is, to determine which interventions should be candidate for public reimbursement. The project is named \"research for development of health benefit package under the universal health care coverage scheme\", or known as UCBP (see www.ucbp.net). The project incorporates multiple-criteria decision analysis (MCDA) and a deliberative process and multi stakeholders’ involvement to guide national-level priority setting in health care coverage decision. The review documented the experience of seven health technology assessment organizations in Canada, England and Wales, the United States, the Netherlands, Germany, Sweden and Spain, which all use an explicit process of priority setting. Its findings concluded that all these organizations consider multiple criteria, involved multiple stakeholders, and distinguish, in one way or another, four basic steps in their priority setting Process. These steps were then also applied in the Thai setting and included. The results of the review were adapted to the Thai setting, resulting in 4 steps of explicit priority setting including: 1) nomination of interventions for assessments, 2) selection of interventions for assessment, 3) technology assessment of interventions, and 4) appraisal of interventions. Since the beginning of the research project up to 119 topics have been proposed for inclusion into the benefit package, with 53 topics selected for further research or HTA analysis.\n\nOne of the benefit packages revised through UCBP is the development of a health screening package under the universal health coverage in 2010. Currently, the three public insurance schemes in Thailand offer different health screening packages. The study was designed as a response to requests from stakeholders including decision-makers and representatives from the general public, to develop an evidenced-based health screening package for the population that could ensure equitable access to essential health screening under the three schemes. The results led to advice against elements of current clinical practice, such as annual chest X-rays and particular blood test (e.g. kidney function test), and indicated that the introduction of certain new population-based health screening programs, such as for chronic hepatitis B, would provide substantial health and economic benefits to the Thais. The final results were presented to a wide group of stakeholders, including decision-makers at the Ministry of Public Health and the public health insurance schemes, to verify and validate the findings and policy recommendations. The package has been endorsed by the Thai UHC Benefit Package Committee for implementation in fiscal year 2016.\n\nIncreasingly, awareness and realization that the evidence required for optimal coverage decisions involved analyses in cost-effectiveness as a fifth criterion. In 2009, the Health Economics Working Group was established under the subcommittee. The working group was composed of health economists, representatives from the subcommittee, academics and representatives from the three health insurance schemes and the working group secretariat. In this process, HITAP in collaboration with the Food and Drug Administration was involved as the secretariat of the working group whose function was to generate procedures and assure quality of the evidence provided. The HEWG then prioritized the requests based on burden of disease, the risk to life and financial burden on households posed by the condition and social consideration and commissions the cost-effectiveness research from non-profit agencies (like HITAP).\n\nIn late 2013 in response to the increasing requests for involvement in international projects, HITAP created the HITAP International Unit in order to collaborate with international partners and networks working to improve health intervention and technology assessment (HITA) for Universal Health Care (UHC) and priority-setting capacity in low- and middle-income countries (LMICs). The HIU draws upon the experiences of HITAP experts to match the growing demand for evidence-informed policy at the global level. Under the HIU, dedicated professionals work and collaborate to provide the means with which to build institutions dedicated towards establishing HTA and priority setting at the local, national and global levels through research, capacity building activities and knowledge products. With the vision of Building Health Technology Assessment Capacity for a Better Society engages in projects, activities and partnerships working towards the same efforts. The HIU has previously worked with the National Center for Pharmaceutical Access and Management (NCPAM), Philippines, Health Technology Assessment Committee (HTAC), Indonesia, Health Systems and Policy Institute (HSPI), Vietnam, the International Decision Support Initiative (IDSI), UK and the National Institute for Health and Care Excellence International (NI), UK.\n\nIn the past HITAP has been instrumental in pushing HTA forward in international policy by becoming part of the delegation representing Thailand as sponsors and writers of several resolutions in the World Health Assembly (WHA) and the South-East Asia Regional Office (SEARO) of the World Health Organization (WHO), including:\n\nHITAP has also worked to establish regional collaboration amongst HTA units in Asia. Along with the National Evidence-based Health Care Collaborating Agency, South Korea (NECA) and the Center for Drug Evaluation, Taiwan (CDE), HITAP founded the HTAsiaLink Network in 2010. The network is a collaborative platform for knowledge sharing and best practices of HTA in the Asia-Pacific region. One of its many activities is the production of a biannual HTAsialink Newsletter and an Annual HTAsiaLink Conference held in different member countries in Asia. Currently the Network has 30 member organizations from 16 Countries.\nHITAP has previously organized the Framework Convention on Tobacco Control: Reflecting on a Decade of Achievement and Strengthening a Multi-Sectoral Approach to Implementation as well as co-organized the Prince Mahidol Award Conference (PMAC) 2016: Priority Setting for Universal Health Care in 2015.\n\n"}
{"id": "20723187", "url": "https://en.wikipedia.org/wiki?curid=20723187", "title": "Health Sponsorship Council", "text": "Health Sponsorship Council\n\nThe Health Sponsorship Council (HSC) was a New Zealand Crown entity that used health promotion to promote health and encourage healthy lifestyles.\n\nThe long-term focus of the HSC was on reducing the social, financial and health sector costs of smoking, skin cancer, problem gambling, and obesity.\n\nHSC communicated with the public using marketing tools such as sponsorship, advertising, partnerships and school-based activities.\n\nHSC’s health promotion activities and campaigns included \"Slip, Slop, Slap and Wrap\"; \"Smoking Not Our Future\"; \"Auahi Kore\"; \"Face the Facts\"; \"Smokefree Schools\"; \"Choice Not Chance\"; \"Breakfast-eaters have it better\"; and \"Feeding our Families\".\n\nHSC was established in 1990 following enactment of the Smoke-free Environments Act 1990, and dissolved following the enactment of the New Zealand Public Health and Disability Amendment Act 2012.\n\nOn 1 July 2012 the functions of HSC were taken over by a new Crown entity known as the Health Promotion Agency (HPA). HPA's work includes all of the HSC's functions as well as those of the Alcohol Advisory Council (ALAC) and some health promotion functions from the Ministry of Health.\n\nHSC undertook and commissioned research to both inform and evaluate its health promotion activities and, in particular, the campaigns. Research findings were published on the HSC website and included insights into health and lifestyles, sun exposure, and attitudes to smoking.\n\n\n\n"}
{"id": "2773710", "url": "https://en.wikipedia.org/wiki?curid=2773710", "title": "Health in India", "text": "Health in India\n\nThe Constitution of India makes healthcare in India the responsibility of the state governments, rather than the central federal government. It makes every state responsible for \"raising the level of nutrition and the standard of living of its people and the improvement of public health as among its primary duties\". \n\nThe National Health Policy was endorsed by the Parliament of India in 1983 and updated in 2002 and then in 2017. The recent four main updates in 2017 mentions the need to focus on the growing burden of the non-communicable diseases, on the emergence of the robust healthcare industry,on growing incidences of catastrophic expenditure due to health care costs and on rising economic growth enabling enhanced fiscal capacity.\n\nIndia's population, as per census 2011 stood at 1.21 billion (0.62 billion males and 0.588 females). There are great inequalities in health between states. The infant mortality in Kerala is 6 per thousand live births, but in Uttar Pradesh it is 64. According to World Bank, the total expenditure on health care as a proportion of GDP in 2015 was 3.89%. Out of 3.89%, the governmental health expenditure as a proportion of GDP is just 1% and the out-of-pocket expenditure as a proportion of the current health expenditure was 65.06% in 2015.\n\nThe life expectancy at birth has increased from 49.7 years in 1970-1975 to 67.9 years in 2010-2014. For the same period, the life expectancy for females is 69.6 years and 66.4 years for males. In 2018, the life expectancy at birth is said to be 69.1 years. \n\nThe infant mortality rate has declined from 74 per 1,000 live births in 1994 to 37 per 1,000 live births in 2015. However, the differentials of rural (41) and urban (25) as of 2015 are still high. In 2016, the infant mortality rate was estimated to be 34.6 per 1,000 live births.\n\nThe under five mortality rate for the country was 113 per 1,000 live births in 1994 whereas in 2018 it reduced to 41.1 per 1,000 live births.\n\nThe maternal mortality ratio has declined from 212 per 100 000 live births in 2007-2009 to 167 per 100 000 live births in 2011-2013. However, the differentials for state Kerala (61) and Assam (300) as of 2011-2013 are still high. In 2013, the maternal mortality ratio was estimated to be 190 per 100 000 live births.\n\nThe total fertility rate for the country was 2.3 in rural areas whereas it has been 1.8 in urban areas during 2015.\n\nThe most common cause of disability adjusted life years lost for Indian citizens as of 2016 for all ages and sexes was ischemic heart disease (accounting for 8.66% of total DALYs ), 2nd chronic obstructive pulmonary disease (accounting for 4.81% of total DALYs), 3rd diarrhea (accounting for 4.64% of total DALYs) and 4th lower respiratory infections (accounting for 4.35% of total DALYs). \n\nAs per the figures about the child mortality rate which is quite a big hurdle for the government, the 2nd most common cause of DALYs lost for children under 5 years of age was diseases like diarrhea, lower respiratory tract infections and other communicable diseases (accounting for 22,598.71 DALYs per 100 000 population) as of 2016 which can be preventable.\n\nMalnutrition refers to deficiencies, excesses or imbalances in a person’s intake of energy and/or nutrients. The term malnutrition covers 2 broad groups of conditions. One is undernutrition - which includes stunting (low height for age), wasting (low weight for height), underweight (low weight for age) and micronutrient deficiencies or insufficiencies (a lack of important vitamins and minerals). The other is overweight - overweight, obesity and diet-related noncommunicable diseases (such as heart disease, stroke, diabetes and cancer).\n\nAccording to a 2005 report, 60% of India’s children below the age of three were malnourished, which was greater than the statistics of sub-Saharan African of 28%. World Bank data indicates that India has one of the world’s highest demographics of children suffering from malnutrition – said to be double that of sub-Saharan Africa with dire consequences. India’s Global Hunger Index India ranking of 67, the 80 nations with the worst hunger situation places it even below North Korea or Sudan. 44% of children under the age of 5 are underweight, while 72% of infants have anemia. It is considered that one in every three malnourished children in the world lives in India. \n\nStates where malnutrition is prominent: \n\n\n\n<nowiki>*</nowiki> : <Median -2SD of WHO Child Growth Standards\n\nA well-nourished child is one whose weight and height measurements compare very well within the standard normal distribution of heights and weights of healthy children of same age and sex. A child without sufficient nutrients in its daily intake is not only exposed to physical and motor growth delays, but also to heightened risk of mortality, reduced immune defenses and decreased cognitive and learning capacities. Malnutrition limits the productivity of all those who are its victims, and thus serves to perpetuate poverty. As with serious malnutrition, growth delays also hinder a child’s intellectual development. Sick children with chronic malnutrition, especially when accompanied by anemia, often suffer from a lower learning capacity during the crucial first years of attending school.\n\n<nowiki>*</nowiki> : <Median -2SD of WHO Child Growth Standards\n\nDue to their lower social status, girls are far more at risk of malnutrition than boys of their age. Partly as a result of this cultural bias, up to one third of all adult women in India are underweight. Inadequate care of these women already underdeveloped, especially during pregnancy, leads them in turn to deliver underweight babies who are vulnerable to further malnutrition and disease.\n\nDiseases such as dengue fever, hepatitis, tuberculosis, malaria and pneumonia continue to plague India due to increased resistance to drugs.\n\nIn 2012, India was polio-free for the first time in its history. This was achieved because of the Pulse Polio programme started in 1995–96 by the government.\n\nIndia has witnessed huge progress in the health status of its population after its independence. During this period the transition have been seen in economic development, nutritional status, fertility and mortality rates and consequently, the disease profile has changed considerably. Although great efforts have been done to control the communicable diseases, but they still contribute significantly to disease burden of the country. Decline in DALYs and deaths from communicable diseases have been accompanied by a gradual shift to, and accelerated rise in the prevalence of chronic non-communicable diseases (NCDs) such as cardiovascular disease (CVD), diabetes, chronic obstructive pulmonary disease (COPD), cancers, mental health disorders and injuries.. Indians are at particularly high risk for atherosclerosis and coronary artery disease. This may be attributed to a genetic predisposition to metabolic syndrome and adverse changes in coronary artery vasodilation. NGOs such as the Indian Heart Association was created to raise awareness.. \n\nAs there are more than 122 million households that have no toilets, and 33% lack access to latrines, over 50% of the population (638 million) defecate in the open.(2008 estimate) This is relatively higher than Bangladesh and Brazil (7%) and China (4%). Although 211 million people gained access to improved sanitation from 1990–2008, only 31% use the facilities provided. Only 11% of Indian rural families dispose of stools safely whereas 80% of the population leave their stools in the open or throw them in the garbage. Open air defecation leads to the spread of disease and malnutrition through parasitic and bacterial infections.\n\nSeveral million more suffer from multiple episodes of diarrhea and still others fall ill on account of Hepatitis A, enteric fever, intestinal worms and eye and skin infections caused by poor hygiene and unsafe drinking water.\n\nAccess to protected sources of drinking water has improved from 68% of the population in 1990 to 88% in 2008. However, only 26% of the slum population has access to safe drinking water, and 25% of the total population has drinking water on their premises. This problem is exacerbated by falling levels of groundwater caused mainly by increasing extraction for irrigation. Insufficient maintenance of the environment around water sources, groundwater pollution, excessive arsenic and fluoride in drinking water pose a major threat to India's health.\nDespite health improvements over the last thirty years, lives continue to be lost to early childhood diseases, inadequate newborn care and childbirth-related causes. More than two million children die every year from preventable infections.\n\nApproximately 1.72 million children die each year before turning one. The under five mortality and infant mortality rates have been declining, from 202 and 190 deaths per thousand live births respectively in 1970 to 64 and 50 deaths per thousand live births respectively in 2009 and to 41.1 (in 2018) and 34.6 (in 2016) deaths per thousand live births respectively. However, this decline is slowing. Reduced funding for immunization leaves only 43.5% of the young fully immunized. A study conducted by the Future Health Systems Consortium in Murshidabad, West Bengal indicates that barriers to immunization coverage are adverse geographic location, absent or inadequately trained health workers and low perceived need for immunization. Infrastructure like hospitals, roads, water and sanitation are lacking in rural areas. Shortages of healthcare providers, poor intrapartum and newborn care, diarrheal diseases and acute respiratory infections also contribute to the high infant mortality rate.\n\nMaternal deaths are similarly high. The reasons for this high mortality are that few women have access to skilled birth attendants and fewer still to quality emergency obstetric care. In addition, only 15 per cent of mothers receive complete antenatal care and only 58 per cent receive iron or folate tablets or syrup.\nWomen's health in India involves numerous issues. Some of them include the following: \n\nRural India contains over 68% of India's total population, and half of all residents of rural areas live below the poverty line, struggling for better and easy access to health care and services. Health issues confronted by rural people are many and diverse – from severe malaria to uncontrolled diabetes, from a badly infected wound to cancer. Postpartum maternal illness is a serious problem in resource-poor settings and contributes to maternal mortality, particularly in rural India. A study conducted in 2009 found that 43.9% of mothers reported they experienced postpartum illnesses six weeks after delivery. Furthermore, because of limited government resources, much of the health care provided comes from non profits such as The MINDS Foundation.\n\nThe Twelfth Five Year plan covering 2012-2017 was formulated based on the recommendation of a High Level Experts Group (HLEG) and other stakeholder consultations. The long term objective of this strategy is to establish a system of Universal Health Coverage (UHC) in the country. Key points include:\n\nThe High Level Expert Group report recommends an increase in public expenditure on health from 1.58 per cent of GDP currently to 2.1 per cent of GDP by the end of the 12th five-year plan. However, even this is far lower than the global median of 5 per cent. The lack of extensive and adequately funded public health services pushes large numbers of people to incur heavy out of pocket expenditures on services purchased from the private sector. Out of pocket expenditures arise even in public sector hospitals, since lack of medicines means that patients have to buy them. This results in a very high financial burden on families in case of severe illness. Though, the 12th plan document express concern over high out-of-pocket (OOP) expenditure, it does not give any target or time frame for reducing this expense . OOP can be reduced only by increasing public expenditure on health and by setting up widespread public health service providers. But the planning commission is planning to do this by regulating private health care providers. It takes solace from the HLEG report which admits that, \"the transformation of India’s health system to become an effective platform for UHC is an evolutionary process that will span several years\".\n\nInstead of developing a better public health system with enhanced health budget, 12th five-year plan document plans to hand over health care system to private institutions. The 12th plan document causes concern over Rashtriya Swasthya Bhima Yojana being used as a medium to hand over public funds to the private sector through an insurance route. This has also incentivised unnecessary treatment which in due course will increase costs and premiums. There have been complaints about high transaction cost for this scheme due to insurance intermediaries. RSBY does not take into consideration state specific variation in disease profiles and health needs. Even though these things are acknowledged in the report, no alternative remedy is given. There is no reference to nutrition as key component of health and for universal Public Distribution System (PDS) in the plan document or HLEG recommendation. In the section of National Rural Health Mission (NRHM) in the document, the commitment to provide 30- to 50-bed Community Health Centres (CHC) per 100 000 population is missing from the main text. It was easy for the government to recruit poor women as ASHA (Accredited Social Health Activist) workers but it has failed to bring doctors, nurses and specialist in this area. The ASHA workers who are coming from a poor background are given incentive based on performance. These people lose many days job undertaking their task as ASHA worker which is not incentivised properly. Even the 12th plan doesn't give any solace. To summarize, successive administrative and political reforms have conveniently bypassed training citizens and local bodies to actively participate in healthcare. In a situation where people are not enabled to identify poor quality, speak up and debate. There is dire need for the health system to fill that role on behalf of the people and can be easily done by decentralization of healthcare governance.\n\nA recent study pointed out that access to advanced medical facilities under a single roof was the main reason for the choice of private hospitals in both rural and urban areas. The second major reason for private healthcare preference was proximity of the facility in the rural area and approachability and friendly conduct of doctors and staff in the urban centers.\n\nPreventive and Promotive Healthcare\n\n\nProgrammes for Communicable Diseases\n\n\nProgrammes for Non-communicable Diseases\n\n\nNational Nutritional Programmes\n\n\nProgrammes Related to System Strengthening / Welfare\n\n\nMiscellaneous\n\n\n\n"}
{"id": "15297282", "url": "https://en.wikipedia.org/wiki?curid=15297282", "title": "Healthcare in the Netherlands", "text": "Healthcare in the Netherlands\n\nHealthcare in the Netherlands can be divided in several ways: firstly in three different echelons; secondly in somatic versus mental healthcare; and thirdly in \"cure\" (short term) versus \"care\" (long term). Home doctors (\"huisartsen\", comparable to general practitioners) form the largest part of the first echelon. Being referred by a first echelon professional is frequently required for access to treatment by the second and third echelons, or at least to qualify for insurance coverage for that treatment. The Dutch health care system is quite effective in comparison to other western countries but is not the most cost-effective. Costs are said to be high because of over-use of in-patient care, institutionalised psychiatric care and elderly care.\n\nThe Netherlands has a network of 160 acute primary care centres, open 24 hours a day, 7 days a week, making an open clinic within easy reach for most people. Acute primary care is offered by a combination of 121 general practice health centers, that are open outside office hours, and a total of 94 medical emergency units with surgery facilities, of which 90 are at hospital locations, open 24/7. In 71 cases general practice services and emergency rooms are found in one hospital location, bringing the total number of locations where acute care is offered to 160. Analysis by the Netherlands National Institute for Public Health and the Environment showed that 99.8 percent of the people can be transported to an emergency unit / casualty ward, or a hospital offering emergency obstetrics within 45 minutes in 2015.\nFor acute medical questions outside ones home doctor's office hours, a general doctors health practice can be called by phone, and advice will be given by the doctor and their assistant. If the issue seems to be urgent, the caller will be advised to come to the practice, and if necessary referred to an emergency room for more serious treatment. For severe medical emergencies, the Netherlands uses 112 to call an ambulance.\n\nThe vast majority of GPs and all pharmacies and hospitals use Electronic health records. In hospitals, computerized order management and medical imaging systems (PACS) are widely accepted.\nWhereas healthcare institutions continue to upgrade their EHR's functionalities, the national infrastructure is still far from being generally accepted.\n\nIn 2012 the national EHR restarted under the joined ownership of GPs, pharmacies and hospitals. A major change is that, as of January 2013, patients have to give their explicit permission that their data may be exchanged over the national infrastructure.\nThe national EHR is a virtual EHR and is a reference server which \"knows\" in which local EHR what kind of patient record is stored.\nEDIFACT still is the most common way to exchange patient information electronically between hospitals and GP's.\n\nSee also Electronic health record.\n\nA programme of mammography screening for breast cancer was started in 1989 for women aged 50-69, and was extended to women aged 70-75 in 1997. A study of effectiveness and overdiagnosis of the programme was published in the British Medical Journal in 2017. It had little impact on death rates. The incidence of stage 2-4 breast cancers in women aged 50 or more was 168 per 100,000 in 1989 and 166 per 100,000 in 2012. About half of the cancers detected were over-diagnosed. \n\nMost hospitals in the Netherlands are privately run, non-profit foundations, whereas most healthcare insurers are for-profit companies. There are some 90 hospital organisations in the Netherlands, with some of them running multiple actual physical hospitals, usually as a result of mergers of previously independent hospitals.\n\nIn general, there are three types of hospitals in the Netherlands: university hospitals, general hospitals, and a category in between that call themselves \"top-clinical\" teaching hospitals. There are eight academic hospitals, or \"university medical centers\", each of which is directly connected with the medicine faculty of a major Dutch university. These are the largest hospitals in the country, and they have the largest number and greatest variety of specialists and researchers working in them. They are able to provide the most complex and specialised treatment.\n\nBetween 26 and 28 hospital organizations are members of the STZ (\"Samenwerkende Topklinische opleidingsZiekenhuizen\"), the collaborative association of \"top-clinical\" teaching hospitals. Although not directly tied to one particular university, these are large hospitals that house the full range of medical specialists (hence \"top-clinical\"), and that can offer both standard and complex care. The top-clinical teaching hospitals collaborate with university hospitals to aid in the education of nurses and medicine students, as well as to offer certain more specialised treatments. Interns frequently accompany doctors during procedures. Aside from training a lot of medical professionals, each top-clinical hospital specializes in one or two specific disciplines, and conducts its own research to stay ahead in its particular field of expertise. The research done is particularly patient-centric, and focused on improving the practical application and achieving the best results for patients.\n\nThe remaining general hospitals provide high standard healthcare for less specialised problems. They will, if necessary, refer patients to more specialised facilities.\n\nMost insurance packages allow patients to choose where they want to be treated. To help patients choose, the Dutch government has set up websites where information is gathered (Zorginzicht) and disclosed (KiesBeter) about provider performance. Patients dissatisfied with their healthcare insurance can choose another insurance package at the end of each year (with few exceptions).\n\nIn 2015 the Netherlands maintained its number one position at the top of the annual Euro health consumer index, which compares healthcare systems in Europe, scoring 916 of a maximum 1,000 points. The Netherlands is the only country that has been in the top three ranking in every Euro health consumer index published since 2005. On 48 indicators such as patient rights and information, accessibility, prevention and outcomes, the Netherlands secured its top position among 37 European countries for the fifth year in a row. \nThe Netherlands was also ranked first in a study comparing the health care systems of the United States, Australia, Canada, Germany and New Zealand.\n\nEver since a major reform of the health care system in 2006, the Dutch system received more points in the Index each year. According to the Health Consumer Powerhouse, the Netherlands has 'a chaos system', meaning patients have a great degree of freedom from where to buy their health insurance, to where they get their healthcare service. But the difference between the Netherlands and other countries is that the chaos is managed. Healthcare decisions are being made in a dialogue between the patients and healthcare professionals.\n\nA comparison of consumer experiences over time yielded mixed results in 2009, and a 2010 review indicated it was too early to tell whether the reform has led to gains in efficiency and quality.\n\nHowever, in November 2007 the leading peer-reviewed journal of health policy thought and research published the results of a survey of adults' health care experiences in the Netherlands, Germany and five English-speaking countries. The survey Toward Higher-Performance Health Systems revealed that the Dutch public stood out for its positive views. Of the Dutch adults surveyed, 59 percent said that they were very confident of receiving high quality and safe health care, compared to only 35 percent of the American adults surveyed.\n\nBased on public statistics, patient polls, and independent research the Netherlands arguably has the best health care system of 32 European countries. In 2009, Health Consumer Powerhouse research director, Dr. Arne Bjornberg, commented: \n\"As the Netherlands [is] expanding [its] lead among the best performing countries, the [Euro Health Consumer] Index indicates that the Dutch might have found a successful approach. It combines competition for funding and provision within a regulated framework. There are information tools to support active choice among consumers. The Netherlands [has] started working on patient empowerment early, which now clearly pays off in many areas. And politicians and bureaucrats are comparatively far removed from operative decisions on delivery of Dutch healthcare services!\"\n\nWaiting lists in the Netherlands increased since the 1980s due to global budgets imposed on the hospital sector although waits remained low compared to many countries. Several changes contributed to waiting times reduction. One was the replacement in 2001 of fixed hospital budgets with introduction of (probably capped) activity-based payment for hospitals, as well as the removal of a government limit on number of hospital specialists eligible for payment by link Social Health Insurance funds covering 2/3 of the population, which had lengthened waits. These two measures had limited hospital care supply. Mean waits for all inpatient cases fell from 8.6 weeks to 5.5 in 2003, and from 6.4 to 5.1 weeks for outpatient cases.\n\nIn 2005, as part of health care reforms, a per-case payment system for hospital care was introduced. Over time, the percent of cases where hospitals and insurers could negotiate the volume and price of each type of case increased. Health insurers also monitored waiting times (which hospitals must publish), and assisted patients with finding the shortest waits (sometimes abroad). Specialists's fixed lump-sum payments were replaced with a payment per patient case, which increased their activity greatly. Mean waits for most surgery were 5 weeks or less by 2011 (Siciliani, Borowitz and Moran, 2013, pp. 184, 187, 189-195).\n\nIn 2010, 70% of Dutch respondents to the Commonwealth Fund 2010 Health Policy Survey in 11 Countries said they waited less than 4 weeks to see a specialist. A moderate 16% said they waited 2 months or more. A moderate 59% waited less than 1 month for elective surgery. Only 5% waited 4 months or more, similar to American respondents.\n\nHealth insurance in the Netherlands is mandatory. Healthcare in the Netherlands is covered by two statutory forms of insurance:\nWhile Dutch residents are automatically insured by the government for Wlz, everyone has to take out their own basic healthcare insurance (basisverzekering), except those under 18 who are automatically covered under their parents' premium. If you don’t take out insurance, you risk a fine.\nInsurers have to offer a universal package for everyone over the age of 18 years, regardless of age or state of health – in most cases it’s illegal to refuse an application or impose special conditions, but not always.\nIn contrast to many other European systems, the Dutch government is responsible for the accessibility and quality of the healthcare system in the Netherlands, but not in charge of its management.\n\nHealthcare in the Netherlands is financed by a dual system that came into effect in January 2006. Long-term treatments, especially those that involve semi-permanent hospitalization, and also disability costs such as wheelchairs, are covered by a state-controlled mandatory insurance. This is laid down in the Wet langdurige zorg (\"General Law on Longterm Healthcare\") which first came into effect in 1968 under the name of Algemene Wet Bijzondere Ziektekosten (AWBZ). In 2009 this insurance covered 27% of all health care expenses.\n\nFor all regular (short-term) medical treatment, there is a system of obligatory health insurance, with private health insurance companies. These insurance companies are obliged to provide a package with a defined set of insured treatments. This insurance covers 41% of all health care expenses.\n\nOther sources of health care payment are taxes (14%), out of pocket payments (9%), additional optional health insurance packages (4%) and a range of other sources (4%). Affordability is guaranteed through a system of income-related allowances and individual and employer-paid income-related premiums.\n\nA key feature of the Dutch system is that premiums may not be related to health status or age. Risk variances between private health insurance companies due to the different risks presented by individual policy holders are compensated through risk equalization and a common risk pool. Funding for all short-term health care is 50% from employers, 45% from the insured person and 5% by the government. Children under 18 are covered for free. Those on low incomes receive compensation to help them pay their insurance. Premiums paid by the insured are, on average, €111  per month for basic health care ('basisverzekering') (about US$133 in Apr. 2018) with variation of about 5% between the various competing insurers, and a mandatory personal sum ('eigen risico') of €385 (US$401) (in 2016, 2017 and 2018).\n\nFrom 1941 to 2006, there were separate public and private systems of short-term health insurance. The public insurance system was implemented by non-profit \"health funds,\" and financed by premiums taken directly out of the wages (together with income taxes). Everyone earning less than a certain threshold qualified for the public insurance system. However, anyone with income over that threshold was obliged to have private insurance instead.\n\nA new system of health care insurance based on risk equalization through a risk equalization pool was introduced in 2006. A compulsory insurance package is available to all citizens at affordable cost without the need for the insured to be assessed for risk by the insurance company. Indeed, health insurers are now willing to take on high risk individuals because they receive compensation for the higher risks.\n\nA 2008 article in the journal \"Health Affairs\" suggested that the Dutch health system, which combines mandatory universal coverage with competing private health plans, could serve as a model for reform in the US. However,\nan assessment of the 2006 Dutch health insurance reforms published in Duke University's \"Journal of Health Politics, Policy and Law\" in 2008 raised concerns. The analysis found that market-based competition in healthcare may not have the advantages over more publicly based single payer models that were originally envisioned for the reforms:\n\nThe first lesson for the United States is that the new (post-2006) Dutch health insurance model may not control costs. To date, consumer premiums are increasing, and insurance companies report large losses on the basic policies. Second, regulated competition is unlikely to make voters/citizens happy; public satisfaction is not high, and perceived quality is down. Third, consumers may not behave as economic models predict, remaining responsive to price incentives. If regulated competition with individual mandates performs poorly in auspicious circumstances such as the Netherlands, how will this model fare in the United States, where access, quality, and cost challenges are even greater? Might the assumptions of economic theory not apply in the health sector?\n\nThe Netherlands has a dual-level system. All primary and curative care (i.e. the family doctor service and hospitals and clinics) is financed from private mandatory insurance. Long term care for the elderly, the dying, the long term mentally ill etc. is covered by social insurance funded from earmarked taxation under the provisions of the Algemene Wet Bijzondere Ziektekosten, which came into effect in 1968.\n\nPrivate insurance companies must offer a core universal insurance package for the universal primary curative care, which includes the cost of all prescription medicines. They must do this at a fixed price for all. The same premium is paid whether young or old, healthy or sick. It is illegal in The Netherlands for insurers to refuse an application for health insurance or to impose special conditions (e.g. exclusions, deductibles, co-payments, or refuse to fund doctor-ordered treatments). The system is 50% financed from payroll taxes paid by employers to a fund controlled by the Health regulator. The government contributes an additional 5% to the regulator's fund. The remaining 45% is collected as premiums paid by the insured directly to the insurance company. Some employers negotiate bulk deals with health insurers and some even pay the employees' premiums as an employment benefit. All insurance companies receive additional funding from the regulator's fund.\n\nThe regulator has sight of the claims made by policyholders and therefore can redistribute the funds it holds on the basis of relative claims made by policy holders. Thus insurers with high payouts receive more from the regulator than those with low payouts. Thus insurance companies have no incentive to deter high cost individuals from taking insurance and are compensated if they have to pay out more than a threshold. This threshold is set above the expected costs. Insurance companies compete with each other on price for the 45% direct premium part of the funding and should try to negotiate deals with hospitals to keep costs low and quality high. The competition regulator is charged with checking for abuse of dominant market positions and the creation of cartels that act against the consumer interests. An insurance regulator ensures that all basic policies have identical coverage rules so that no person is medically disadvantaged by his or her choice of insurer.\n\nInsurance companies can offer additional services at extra cost over and above the universal system laid down by the regulator, e.g. for dental care. The standard monthly premium for health care paid by individual adults is about €100 per month. People on low incomes can get assistance from the government if they cannot afford these payments. Children under 18 are insured by the system at no additional cost to them or their families, because the insurance company receives the cost of this from the regulator's fund.\n\nDutch consumers and expats working in the Netherlands who are obliged to be mandatorily insured by Dutch law have the opportunity to switch insurance companies each year. The health insurance companies have to publish the premium for the coming year before 12 November. This is a week earlier than in 2016. The former Dutch minister of health Mrs. Edith Schippers gave an indication that the premiums could rise up to € 5,- per month. When insured for health care customers can switch to a different insurer until 31 December. Or when the person insured cancels his/her insurance before the 31st they are allowed to choose until 31 January. Any health insurance costs will in that case be covered by the current health insurance agency until the switch is finalized.\n\nSpecific minority groups in Dutch society, most notably certain branches of orthodox Calvinism and Evangelical Christian groups, refuse to have insurance for religious reasons. To take care of these religious principled objections, the Dutch system provides a special opt-out clause. The amount of money for health care that would be paid by an employer in payroll taxes is in those cases not used for redistribution by the government, but instead, after request to the tax authorities, credited to a private health care savings account. The individual can draw from this account for paying medical bills, however if the account is depleted, one has to find the money elsewhere. If the person dies and the account still contains a sum, that sum is included in the inheritance.\n\nIf a person with a private health savings account changes his or her mind and wants to get insurance, the tax authorities release the remaining sum in the health account into the common risk pool.\n\nThe set of rules around the opt-out clauses have been designed in such way that people who do not want to be insured can opt out but not engage in a free ride on the system. However, ultimately health care providers are obliged to provide acute health care irrespective of insurance or financial status.\n\n"}
{"id": "12344162", "url": "https://en.wikipedia.org/wiki?curid=12344162", "title": "International Trachoma Initiative", "text": "International Trachoma Initiative\n\nInternational Trachoma Initiative (ITI) is a US-based non-profit organization committed to the elimination of blinding trachoma, the most common cause of preventable blindness. Founded in 1998 by the Edna McConnell Clark Foundation and Pfizer Inc, ITI has operated in 14 developing countries in Africa and Southeast Asia. ITI builds on growing international momentum to support the World Health Organization’s goal of eliminating blinding trachoma as a public health concern by 2020. Working in countries where the WHO has documented widespread disease, ITI catalyzes partnerships among international agencies and governmental and non-governmental organizations. ITI supports national governments in their efforts to implement sustainable trachoma control programs using the WHO-recommended SAFE strategy, which includes:\n\n\nSince launching its first programs in Morocco and Tanzania in 1999, ITI has steadily expanded trachoma control efforts to an additional 12 countries in Africa and Asia. In 2008, ITI will take on two additional country programs. This rapid expansion has been made possible by Pfizer’s donation of the antibiotic Zithromax, the dedication of national governments, and the commitment of ITI’s partners to support the F and E components of SAFE implementation. To date over 53 million antibiotic treatments have been administered, over 276,000 individuals have received sight-saving surgery, and millions of people in endemic countries have benefited from health education and improved access to water and sanitation.\n\nTrachoma is a highly infectious eye disease caused by the bacterium \"Chlamydia trachomatis\", which can be spread through contact with an infected person. Untreated, repeated trachoma infections result in a painful form of permanent blindness when the eyelids turn inward, causing the eyelashes to scratch the cornea. Although children are the most susceptible to infection, the effects are often not felt until adulthood. Women, traditionally the caretakers of the home, are three times more likely than men to be blinded by the disease. Without intervention, trachoma keeps families shackled within a cycle of poverty, as the disease and its long-term effects are passed from one generation to the next.\n\n"}
{"id": "899236", "url": "https://en.wikipedia.org/wiki?curid=899236", "title": "Irukandji syndrome", "text": "Irukandji syndrome\n\nIrukandji syndrome is a condition that results from venomization by certain box jellyfish. Unless immediate medical action is taken, those affected can, in rare instances, go into cardiac arrest and die. The most common jellyfish involved is the \"Carukia barnesi\", a species of Irukandji jellyfish.\n\nThe syndrome was given its name in 1952 by Hugo Flecker, after the Aboriginal Irukandji people who live in Palm Cove, north of Cairns, Queensland, Australia, where stings are common.\n\nMost stings occur during the summer wet season in October–May in North Queensland, with different seasonal patterns elsewhere. Because jellyfish are very small, the venom is only injected through the tips of the nematocysts (the cnidocysts) rather than the entire lengths, as a result the sting may barely be noticed at first. It has been described as feeling like little more than a mosquito bite. The symptoms, however, gradually become apparent and then more and more intense in the subsequent 5 to 120 minutes (30 minutes on average). Irukandji syndrome includes an array of systemic symptoms, including severe headache, backache, muscle pains, chest and abdominal pain, nausea and vomiting, sweating, anxiety, hypertension, tachycardia and pulmonary edema. One unusual symptom associated with the syndrome is a feeling of \"impending doom\". Patients have been reported as being so certain they are going to die, they beg their doctors to kill them to get it over with. Symptoms generally abate in four to 30 hours, but may take up to two weeks to resolve completely.\n\nWhen properly treated, a single sting is almost always not fatal; however, two people in Australia are believed to have died from Irukandji stings, which has greatly increased public awareness of Irukandji syndrome. It is unknown how many other deaths from Irukandji syndrome have been wrongly attributed to other causes.\n\nThe exact mechanism of action of the venom is unknown, but catecholamine excess may be an underlying mechanism in severe cases. Animal studies appear to confirm a relationship between envenoming and an increase in circulating noradrenaline and adrenaline.\n\nSimilar to other box jellyfish stings, first aid consists of flushing the area with vinegar to neutralize the tentacle stinging apparatus. As no antivenom is available, treatment is largely supportive, with analgesia being the mainstay of management. Nitroglycerin, a common drug used for cardiac conditions, is utilised by medical personnel to minimise the risk of pulmonary edema and to reduce hypertension. Antihistamines may be of benefit for pain relief, but most cases require intravenous opioid analgesia. Fentanyl or morphine are usually chosen. Pethidine (meperidine, brand name Demerol in the US) should be avoided, as large doses are often required for pain relief and in this situation significant adverse effects from the pethidine metabolite norpethidine may occur.\n\nMagnesium sulfate has been proposed as a treatment for Irukandji syndrome after being apparently successfully used in one case. Early evidence suggested a benefit; however, according to a later report, a series of three patients failed to show any improvement with magnesium; the author reiterated the experimental status of this treatment. Some preliminary laboratory experiments using the venom extracted from \"Malo maxima\" (the 'Broome Irukandji') on rat cardiovascular tissue \"in vitro\" has suggested that magnesium does in fact block many of the actions of this venom.\n\nReports of Irukandji syndrome have come from Australia, Hawaii, Florida, the French Antilles, Bonaire, the Caribbean, Timor Leste and Papua New Guinea. Cubozoan species other than \"Carukia barnesi\" are presumed to be responsible for envenomations outside Australia.\n\nIn 1964 Jack Barnes confirmed the cause of the syndrome was a sting from a small box jellyfish: the Irukandji jellyfish, which can fire venom-filled stingers out of its body and into passing victims. To prove that the jellyfish was the cause of the syndrome, he captured one and deliberately stung himself while his son Nick and a local lifeguard then observed the resulting symptoms (before rushing him to the ICU). Other cubozoans possibly can cause Irukandji syndrome; those positively identified include\" Carukia barnesi\", \"Alatina\" cf. \"mordens\", \"Alatina alata\", \"Malo maximus\", \"Malo kingi\", \"Carybdea xaymacana\", \"Keesingia gigas\", an as-yet unnamed \"fire jelly\", and another unnamed species.\n\nA 2005 Discovery Channel program, \"Killer Jellyfish\", portrayed the severity of the pain from an Irukandji jellyfish sting when two Australian researchers (Jamie Seymour and Teresa Carrette) were stung. Another program aired on the Discovery Channel, \"Stings, Fangs and Spines\", featured a 20-minute spot on Irukandji syndrome. In the segment, a young Australian woman was stung and developed a severe case.\n\nA 2007 fictional \"Sea Patrol\" episode (S1, E4), involves the Irukandji Jellyfish. Two of HMAS Hammersley's crew, Jaffa and Cheffo, are stung by the jellyfish when recovering the seaboat. Charge is also stung on the eye but it is not life-threatening. Jaffa eventually dies from the sting. Cheffo and Charge both recover from the sting. \n\nOn the television program \"Super Animal\", a woman compared her experience with Irukandji syndrome to the pain from childbirth.\n\nSteve Backshall reports with accounts from victims of Irukandji stings on his ITV wildlife series \"Fierce\" in 2016. \n\n\n"}
{"id": "1948619", "url": "https://en.wikipedia.org/wiki?curid=1948619", "title": "Jomanda", "text": "Jomanda\n\nJohanna Wilhelmina Petronella Damman, known under her nickname Jomanda (born May 5, 1948), is a controversial Dutch healing medium who refers to herself as the \"Lady of the light.\"\n\nBorn Johanna Wilhelmina Petronella Damman in Deventer, Jomanda is a Dutch spiritualist who described herself as being \"a healing medium.\" She claims to have psychic powers of clairvoyance, empathy and prescience, aided by her late father and other powers from the \"world divine\". She is known to be a devoted fan of paranormal author Jozef Rulof.\n\nIn 1991, she gained national fame after making an appearance on a popular talk show. In the next decade, she held public gatherings in Tiel where she announced that she would be performing healings, which drew large groups of people. During these sessions she would 'infuse' bottles of tap water, which she claimed would make them gain healing properties. She also appeared on radio shows, where people were asked to put a bottle of water in front of their radio in order to have it remotely 'infused'.\n\nIn 2001 Jomanda gave medical and spiritual advice to the popular Dutch actress Sylvia Millecam, who later died of cancer. In 2004 the Dutch Health Inspectorate filed suit against Jomanda claiming that she and three alternative therapists misled Millecam by claiming that the actress was merely suffering from an inflammation. In October 2006, the investigation was dropped, but in April 2008 an Amsterdam court ruled that Jomanda and two doctors who practise alternative therapies should be prosecuted for their roles in Millecam's death.\nThe trial started at 30 October 2008. On 12 May 2009 Ronald ter Heegde, Jomanda's former assistant, confessed on television in the Dutch newsmagazine \"Nova\" that Jomanda had used other persons to gather information about Sylvia Millecam. She then told Millecam she had gathered this information as a medium. On 12 June 2009 the court found Jomanda not guilty as a party to Millecam's death. The court criticised her actions, but stated that she could not be held fully responsible for Millecam's fate.\n\nThe Dutch Federation Against Quacks (Nederlandse Vereniging tegen de Kwakzalverij) listed Jomanda as #15 on their top 20 list of quacks.\n\nIn October 2004 Jomanda drew publicity again by stating, in accordance with the ideas of Jozef Rulof, 'Cremation hurts'.\n\n"}
{"id": "15116071", "url": "https://en.wikipedia.org/wiki?curid=15116071", "title": "KMEHR", "text": "KMEHR\n\nKMEHR or Kind Messages for Electronic Healthcare Record is a Belgian medical data standard introduced in 2002, designed to enable the exchange of structured clinical information. It is funded by the Belgian federal Ministry of public health and assessed in collaboration with Belgian industry.\n\nThe initiative lead to the specification of about 20 specific XML messages (the Kind Messages for Electronic Healthcare Records - Belgian implementation standard or KMEHR-bis).\n\nThe KMEHR standard consists of an XML (eXtensible Markup Language) message format defined by the KMEHR XML Schema, a set of reference tables and a set of recognized medical transactions compliant with this grammar.\n\nA KMEHR XML message is composed of two components: a header and at least one folder. The header of the message describes the sender, the recipient(s) and a few technical statuses.\n\nThe folder itself gathers the information about a patient, where each folder identifies the subject of care (patient) and contains at least one medical transaction.\n\nThe medical transaction item gathers the information reported by one healthcare professional at a given instance. Its attributes are type, author, date and time.\n\nSummarized Electronic Health Record (SumEHR) is a KMEHR message, used for the exchange of medical information. It summarizes the minimal set of data that a physician needs in order to understand the medical status of the patient in a few minutes and to ensure the continuity of care. The SumEHR standard was introduced by the Belgian government in 2005 and an EMD software package used by a physician (GP) should be capable of exporting a SumEHR message (KMEHR message level 4) for any given patient.\n\nThe KMEHR-bis standard comprises a set of dictionaries which define the transaction types, heading types, item types, severity levels and administration routes.\n\nThe KMEHR-bis standard supports links to either internal or external objects, e.g. an image or another KMEHR-message.\n\nThe KMEHR-Bis specification is extended with web services (based on SOAP), which define request and response elements to offer standard web services.\n\n\n"}
{"id": "50761050", "url": "https://en.wikipedia.org/wiki?curid=50761050", "title": "Kit Parker", "text": "Kit Parker\n\nKevin Kit Parker is a lieutenant colonel in the United States Army Reserve and the Tarr Family Professor of Bioengineering and Applied Physics at Harvard University. His research includes cardiac cell biology and tissue engineering, traumatic brain injury, and biological applications of micro- and nanotechnologies. Additional work in his laboratory has included fashion design, marine biology, and the application of counterinsurgency methods to countering transnational organized crime.\n\nParker attended Boston University's College of Engineering and graduated in 1989. He earned a Master of Science degree in 1993 and a doctoral degree in applied physics in 1998 from Vanderbilt University.\n\nParker is a paratrooper who has served in the United States Army since 1992. After the September 11 attacks, he served two tours of duty in Afghanistan.\n\nIn addition to his combat tours, Parker conducted two missions into Afghanistan as part of the Gray Team in 2011.\n\nInitially, at Harvard the focus of his research was heart muscle cells. He turned to traumatic brain injury in 2005 after realizing that an Army friend of his, who had received injuries in an IED blast in Iraq in 2005, was suffering from an undiagnosed medical condition rather than a psychological problem. \n\nOther research of Parker's includes designing camouflage using skin cells of cuttlefish and the use of a cotton candy machine to make dressings for wounds.\n\nParker served on the Defense Science Research Council for nearly a decade, the Defense Science Board Task Force on Autonomy, and has consulted to other US government agencies as well as the medical device and pharma industry.\n\nIn 2011, Parker headed Harvard's committee for reintroducing ROTC at the university.\n\nIn July 2016, it was announced that The Disease Biophysics Group at Harvard, led by Kit Parker, created a tissue-engineered soft-robotic ray that swims using wave-like fin motions, and turns according to externally applied light cues.\n\n"}
{"id": "21567707", "url": "https://en.wikipedia.org/wiki?curid=21567707", "title": "Lead contamination in Washington, D.C. drinking water", "text": "Lead contamination in Washington, D.C. drinking water\n\nLead contamination in Washington, D.C. drinking water, first discovered in 2001, left thousands of children with lifelong health risks, and led to a re-evaluation of the use of chloramine in public drinking-water systems. Professor Marc Edwards, an expert in plumbing corrosion, discovered lead levels at least 83 times higher than the accepted safe limit while performing research into premature pipe corrosion for the District of Columbia Water and Sewer Authority (WASA). He found that the decision to change from chlorine to chloramine as a treatment chemical had caused the spike in lead levels.\n\nAfter the \"Washington Post\" ran a series of front-page articles about Edwards's findings, resulting in widespread public concern, the United States House of Representatives conducted an investigation. The House found that the U.S. Centers for Disease Control and Prevention (CDC) had made \"scientifically indefensible\" claims in a report that had indicated there was no risk from the high lead levels. The \"Post\" investigation uncovered evidence of widespread misreporting of lead levels at water agencies across the United States, leading to regulatory crackdowns and changes in Environmental Protection Agency policies.\n\nThe problem was addressed in 2004 by adding additional treatments to the water, preventing the chloramine from dissolving lead in the water mains, solder joints, and plumbing fixtures.\n\nIn 2010, the CDC reported that 15,000 homes in the Washington, D.C. area might still have water supplies with dangerous levels of lead.\n\nIn 2001, more than half the water samples taken from 53 DC-area homes under the procedures required by the EPA's Lead and Copper Rule showed levels of lead exceeding the national standard of 15 parts per billion (ppb). Lead disrupts the physical and mental development of fetuses, babies, and young children, and can cause kidney problems and high blood pressure in adults. The rule does not assume that there is a \"safe\" level of exposure, but notes that 15 ppb is an \"action level\" where utilities must take action. The rule was created in 1991, after research showed that drinking water could account for one-fifth of all lead intake. Lead is not normally present in drinking water; it is released from the inside surface of lead service lines (pipes that run from the main to the house), joints connected with lead-based solder, and lead fixtures inside the house. Based on these findings, WASA was required to notify the public and implement plans to replace lead service lines in key areas of the municipal water system.\n\nThe first media attention came in late 2002, when the \"Washington City Paper\" ran an article about a resident of American University Park whose water tested six to 18 times the EPA Lead and Copper Rule's action level. WASA found that homes in its service area with lead service lines averaged five times the EPA limit for lead during a year-long period. The results were unexpected; the EPA scientist overseeing DC's water suggested that drought conditions might have raised the alkalinity levels of the Potomac River, leading to a change in the pH of the water. As a result, WASA was required to start replacing seven percent of the district's lead service lines each year until the levels dropped below 0.015 milligrams per liter. At the time, about 23,000 WASA customers had lead service lines.\n\nIn March 2003, Marc Edwards, a professor of civil engineering and expert on corrosion in drinking-water systems, was conducting research into an unexpected increase in pinhole leaks in copper water pipes in the DC area. WASA funded Edwards's research. Suspecting the leaks were caused by a change in the water chemistry, Edwards used a field meter to test for lead in one home's water. The meter could read values up to 140 ppb. His initial reading pegged the meter, so he diluted the sample to ten percent of its original strength. Even so, diluted, the sample still pegged the meter, indicating the water contained at least 1,250 ppb of lead. \"Some of it would literally have to be classified as a hazardous waste,\" Edwards said.\n\nAfter Seema Bhat, a water quality manager at WASA, told her superiors at the agency and at the EPA about the lead levels and warned that federal guidelines required aggressive action, she was fired by the agency. A federal investigator found that she had been improperly terminated.\n\nThe lead levels required WASA to conduct a wider survey of their water quality. By the fall of 2003, it had tested more than 6,000 homes in the District, finding that two-thirds tested had more than 15 ppb of lead in their water. The survey showed that over 4,000 homes served by WASA had lead levels exceeding the acceptable level. More than a third of the homes surveyed—2,287 out of 6,118—had levels exceeding 50 ppb. The water tested over 300 ppb in 157 homes. Despite this result, WASA did not notify its customers of the risk until November 2003.\n\nAlthough regulations required WASA to include the specific warning \"Some homes in this community have elevated lead levels in their drinking water. Lead can pose a significant risk to your health.\" in the water bills of each affected customer, WASA's notice omitted key parts of the phrase such as \"in their drinking water\" and \"significant\". Although Federal law required WASA to hold public meetings to discuss the problem and actions people could take to protect themselves, they advertised the meetings as being \"to discuss and solicit public comments on WASA's Safe Drinking Water Act projects\", omitting mention of lead. The EPA was required to review the notice before it was sent out; one D.C. Council member, commenting on the EPA's approval of the faulty notice, asked \"Where were you, EPA?\" The EPA later found that WASA's notice violated federal law because of the omissions.\n\nIn January 2004, the \"Washington Post\" reported that the mayor and several D.C. Council members had not been informed of the elevated lead levels. The \"Washington City Paper\" said that Carol Schwartz, the councilmember who chaired the Committee on Public Works and the Environment, was not informed of the lead issue until that newspaper contacted her during the last week of January 2004. Early communications from WASA limited the health advisory to pregnant women and small children in homes with lead service leads, but later tests showed the high lead levels were also present in homes with copper service leads.\n\nMeanwhile, on January 2, 2004, WASA called Edwards and threatened to cut off his access to monitoring data needed for his research, and divert funding from him to other researchers, unless he stopped working with the homeowners whose water showed high lead levels. Soon after, the EPA discontinued its own contract with Edwards.\n\nThe issue became front-page news when the \"Washington Post\" ran an article titled \"Water in D.C. Exceeds EPA Lead Limit\" on January 31, 2004 across six columns of page A1. Reporter David Nakamura was contacted by one of the homeowners whose water was tested by WASA during its survey after he received the test results. Nakamura—who had no prior experience with clean water issues—initially thought it was a minor issue, but agreed to help the homeowner get a response from WASA. When WASA would not give him a straight answer, Nakamura pressed them for full data on the tests. Nakamura says that, even though he was \"stunned\" at the facts reported in that first story, the \"Post\" \"had no idea about the size and scope of what was to follow.\"\n\nThe article quoted Erik Olson, an analyst for the Natural Resources Defense Council, as saying \"This is a really big deal... If schools go over 20 parts per billion, they immediately take the water out of production.\" WASA recommended that residents let the tap run for 30 seconds to one minute before using it to reduce the risk.\n\nThis first \"Post\" article was the first public mention of the theory that the lead levels were tied to chloramine. The traditional use of chlorine had been stopped four years earlier, out of concerns that it could produce harmful chemicals in the pipes. Long-term exposure to the byproducts from chlorine treatment has been linked with cancer. The paper quoted officials as saying that it was possible chloramine was more corrosive to lead pipes. (In fact, the Army Corps of Engineers, which is responsible for the Washington Aqueduct that supplies water to WASA, rejected a recommendation to add phosphates to the water to prevent lead leaching in the mid-1990s.) The change to chloramine was made after the EPA issued regulations concerning disinfection byproducts formed when chlorine reacts with organic matter in drinking water; the EPA considered these byproducts to be a potential health threat.\n\nAfter the first article, the \"Post\" formed a core team of reporters to investigate the issue. Nakamura was joined by reporters Carol Leonnig, Jo Becker, Avram Goldstein, and D'Vera Cohn; editor Marcia Slacum Greene provided daily oversight. Nakamura was the lead reporter for the breaking news coming from WASA and City Hall, and wrote profiles of key players. Leonnig reported on the federal and EPA response to the contamination. Cohn investigated the Washington Aqueduct angle and worked with water quality and environmental experts. Goldstein covered the DC Department of Health. Becker looked at water quality nationwide.\n\nAfter the first article appeared, residents inundated WASA's water hotline with calls and overwhelmed water testing laboratories with requests to have their tap water tested for lead contamination. District elected officials immediately called for an emergency public meeting, and established an inter-agency task force with the EPA to investigate and manage the problem. However, messages to the public at the time were often confusing and contradictory: while WASA was suggesting running taps for 90 seconds to flush out any lead, the EPA was demanding that recommendation be changed to ten minutes.\n\nThe \"Post\" article lead WASA to hand out over 30,000 free water filters, hire health experts, and offer free blood tests to residents. Some water fountains were turned off due to lead levels. In 2004, the D.C. Council held 11 oversight hearings on the issue; the US Congress held four. American University claimed that its water was safe to drink, because the larger water mains feeding commercial sites like the college were not made of lead.\n\nAfter Nakamura's first few stories for the \"Post\" on the issue, he was contacted by Seema Bhat's attorney. Bhat, who was then fighting her dismissal from WASA, shared thousands of internal documents with Nakamura. The documents, Nakamura said, \"provided a fairly clear picture\" that WASA had been trying to find a way to avoid the cost of replacing pipes and adding additional chemicals to the water. He said Bhat's documents were critical to the \"Post\"'s investigation. He recalls that, while reading the documents, the team found WASA memos indicating that they tried to find \"clean\" houses to test to reduce the apparent average lead level from the testing, but the more they tested, the more \"dirty\" houses they found.\n\nBy April 2004, there were reports of some DC-area homes reaching lead levels of 6,000 ppb to 48,000 ppb.\n\nIn June 2004, the EPA cited WASA for a \"serious breach\" of the law, including withholding six test results showing high lead levels in 2001, dropping half of the homes that had previously tested high for lead levels in subsequent testing, and avoiding homes known to be at high risk for contamination. In July, a WASA-commissioned report supported the \"Post\"'s claims that the agency had known about the high lead levels for years, but had failed to notify regulators or the public.\nIn August 2004, the Army Corps of Engineers started adding orthophosphate to the water in hopes of preventing lead leeching. By November, WASA's board of directors had committed to a plan that would replace all of the agency's 23,000 lead pipes by 2010, at a cost of $300 million—starting with 2,800 lead pipes to be replaced in 2005. WASA estimated that the repairs would cost residents $6 to $7 a month. The agency was not legally responsible for the portion of the lead service lines within a homeowner's property lines; however, they offered to perform the work at a set rate, and arranged a low-interest loan program with Wachovia Bank to help homeowners afford the cost.\n\nBy January 31, 2005, the \"Post\" had run over 200 stories on the issue, amassing thousands of pages of correspondence through the Freedom of Information Act. Gloria Borland, a District resident, testified before Congress: \"If the \"[Washington] Post\" had not exposed this scandal, our children today would still be drinking lead contaminated water\".\n\n\"Post\" reporters Nakamura, Leonnig, Cohn, Becker, Craig Timberg, Monte Reel, and Sarah Cohen won the 2005 Selden Ring Award for Investigative Reporting for the articles. Michael Parks, director of USC Annenberg's School of Journalism and Pulitzer Prize-winning former editor of the \"Los Angeles Times\", said \"The \"Washington Post\"s work was a very important piece of journalism—important to every man, woman and child living in the District of Columbia, drinking its water and thinking it was pure. And it was important to the residents of other cities whose water is contaminated by lead and other toxic substances\". The award's cash prize of $35,000 is believed to be the largest in journalism. According to Nakamura, some at the \"Post\" were surprised to win the award, because of the atypical nature of the \"Post\"'s investigation. Most winners conduct a long-term investigation and then publish long articles over a few days with the results; the \"Post\" covered the investigation as a series of beat stories. Nakamura said he had never heard of the Selden Ring Award until the day his editor told him that the team had won it.\n\nOn March 30, 2004, an \"MMWR dispatch\", \"Blood Lead Levels in Residents of Homes with Elevated Lead in Tap Water --- District of Columbia, 2004\", was made available on the MMWR web site. It was then published by CDC as \"MMWR Weekly, April 2, 2004 / 53(12);268-270\". The report \"summarizes the results of the preliminary investigations, which indicated that the elevated water lead levels might have contributed to a small increase in blood lead levels (BLLs)\". The report describes the background, and the various kinds of blood tests it employed, and explicitly states: \"All blood tests were used in this analysis.\" There is no mention at all of any test results not being available, not even in the \"caveat\" section, where other potential sources of error are discussed.\n\nThe report concludes that the high amounts of lead in the drinking water may have led to a slight rise of the blood levels; however, not to the levels of official concern. It also claims that the average levels were sinking. However, the report notes that there is no known \"risk free\" level, and therefore recommends that efforts should be made to eliminate lead in children's blood entirely.\n\nThe report later was strongly criticized, both by Marc Edwards and by the United States House Committee on Science, Space and Technology; see .\n\nAt an oversight hearing before the House Committee on Government Reform in early March 2004, Marc Edwards testified that his studies showed the change from chlorine to chloramine was the cause of the elevated lead levels. He stated that the chloramine-treated water was leaching lead not only from the old lead lines, but also from brass fixtures in homes. Brass is made with lead; even brass classified as \"lead free\" under the Safe Drinking Water Act can contain up to eight percent lead. Edwards said that this could be the cause of the high lead levels in areas where WASA did not use lead lines. He also cautioned that replacing lead service lines with copper could make the problem worse; newly installed copper lines could react with the chloramine in a way that would increase corrosion of the remaining lead lines in the system. The chief of the Washington Aqueduct disagreed, saying that tests taken after the chloramine treatment commenced didn't show any additional corrosiveness. He believed corrosion inhibitors, like orthophosphate, could be added to the water to prevent lead leaching. In the spring of 2004, a temporary switch from chloramine back to chlorine for annual pipe flushing caused a 25 to 30 percent drop in lead levels, lending credence to the chloramine theory. In one home that was tested by WASA before and after the switch, lead levels dropped tenfold.\n\nOn March 23, 2004, Anthony A. Williams (the mayor of the District of Columbia) and Carol Schwartz (chair of the DC Council's Committee on Public Works and the Environment) wrote a letter to President George W. Bush, asking the federal government to reimburse WASA $24,093,700 and the District of Columbia $1,730,401 to cover expenses from the lead contamination. The letter justified the request for full reimbursement by saying \"the regulatory decisions of EPA appear to have generated these costs\" and that it would be \"inherently unfair\" for taxpayers and WASA customers to pay those costs. In 2005, President Bush proposed cutting the EPA's budget by almost a half-billion dollars, mostly from clean-water programs. He wanted to decrease spending on replacing aging water facilities by 83 percent.\n\nOn March 30, 2004, the Centers for Disease Control and Prevention (CDC) published a Morbidity and Mortality Weekly Report (MMWR) that found the lead \"might have contributed a small increase in blood lead levels.\" The report claimed that no children with dangerously high blood lead levels were found, even in the homes with the worst lead levels. Officials in other cities, such as New York City and Seattle, cited the report as justification for a less-than-aggressive response to high lead levels in their own water. Water testing in the first six months of 2004 showed 90 percent of homes having 63 ppb of lead or less in the water.\n\nIn October 2004, Edwards co-authored an article in the \"Journal of the American Water Works Association\" that linked chloramine use with greatly increased lead leaching.\n\nA report commissioned by the D.C. Council released on December 8, 2004 faulted the federal government's regulation of the city's water supply as a factor in the lead contamination. The report noted that no one agency was responsible for the water quality: The Army Corps of Engineers treated the water, WASA delivered it, and the EPA performed the quality checks. It urged the Council to assume authority for the entire system. In the last six months of 2004, 90 percent of homes tested had lead levels of 59 ppb or lower.\n\nThe city's interim inspector general, Austin A. Andersen, released a report on WASA's performance in January 2005. Andersen called for WASA to enter a formal agreement with the city's Health Department to ensure that future communications about water health issues were timely and worded appropriately. WASA rejected the need for such an agreement. That same month, the Architect of the Capitol issued a warning to Capitol Hill employees that they should not use water from bathroom or kitchen faucets for drinking or cooking. Some water fountains on the campus were turned off, although employees were told it was okay to continue using the others. One home on Capitol Hill was found to have 308 ppb of lead in 2003.\n\nOn January 21, 2005, the EPA ordered WASA to notify more than 400 homeowners that the agency had mistakenly told them their house's water lead levels were safe, and to replace an additional 500 service lines to comply with federal law. The EPA said that WASA had flushed the water line for five minutes before running the lead tests for those homes, resulting in artificially low readings; such a lengthy flush is not permitted by EPA standards. The EPA did not believe that the flushes were done intentionally by WASA to reduce lead levels. The testing had been done as part of an effort to avoid the expense of replacing lead lines in areas where it found low lead levels.\n\nIn 1999, an EPA survey estimated that the United States's drinking-water systems were in need of $150 billion worth of repairs over the following 20 years. However, in April 2004, an EPA spokesperson told the \"Washington Times\" that \"high lead levels are not a pervasive problem\". At a Congressional hearing that month, the EPA testified that it had no current information on lead levels in 78 percent of the nation's water systems, and that as many as 20 states had not provided any data.\n\nOn October 5, 2004, the \"Washington Post\" ran a front-page article reporting that cities across the United States were illegally manipulating lead testing results, such as discarding high readings or avoiding homes likely to have high readings. A former EPA official described it as \"widespread fraud and manipulation\" on the part of water utilities. That July, however, an EPA administrator told Congress that \"we have not identified a systemic problem\". Using data from the EPA, the \"Post\" identified 274 utilities that had reported unsafe lead levels between 2000 and 2004. Some utilities defended their testing practices as being approved by state regulators; others argued that the lead was actually leaching from customers' fixtures, not their plumbing.\n\nAmong the cities that the \"Post\" faulted through the EPA data were:\n\nThe \"Post\" article led the United States Attorney for the Southern District of New York to open an investigation into the city's drinking-water lead levels. Regulators in Michigan and Oregon also investigated utilities singled out by the \"Post\" in those states. Senators James M. Jeffords and Paul S. Sarbanes called for the EPA to impose tougher standards; Jeffords and Senator Hillary Clinton called for an investigation of the EPA following the \"Post\"'s findings.\n\nSeattle had already suffered widespread lead contamination in its public school system in 2003. One parent, a scientist who had initiated the investigation there, said \"we continue to suffer from an epidemic of lead complacency\" nationwide.\n\nThe EPA said that between 2003 and 2005, only four large water systems had unsafe lead levels: Washington, DC; St. Paul, Minnesota; Port St. Lucie, Florida; and Ridgewood, New Jersey.\n\nBy January 2005, a year after the high lead levels were publicized by the \"Post\", advocates were calling for the firing of local and federal officials involved in the issue, saying that they had done too little to fix the problem. Eric Olsen of the Natural Resources Defense Council said that officials \"have fallen down on the job\" because thousands of residents still had unsafe water. The group Lead Emergency Action for the District called for an overhaul of WASA's management, an independent study of needed improvements to the water system, stronger laws, and action from the EPA. \"We want fines and a criminal review,\" said Olsen.\n\nIn March 2005, the EPA proposed changes strengthening the Lead and Copper Rule. The changes require utilities to give test results directly to homeowners, and to notify state and federal regulators before changing water-treatment methods. Critics, such as Clean Water Action, called the changes \"revisions at the margins\". Rick Maas, co-director of the Environmental Quality Institute at the University of North Carolina, said the revisions would \"solve maybe 5 to 10 percent of the problem\" with the Rule. Trade organizations, such as the American Water Works Association, found the changes \"basically reasonable\".\n\nThat month, WASA said that recent tests showed encouraging declines in lead levels, which it attributed to the orthophosphate treatments started in August 2004. Out of 51 homes tested, only four had readings above 15 ppb; ten of those homes were above the 15-ppb standard the previous year, but fell below it in this test. WASA also noted that the Army Corps of Engineers would continue using chloramine throughout the year to keep the water chemistry stable. By May 10, 2005, the company was announcing that its tests were \"below the '90th percentile lead action level.'\"\n\nAs of January 2006, WASA said that about 29 percent of its customers had chosen to replace the lead service lines within the customer's property. In a January press release, WASA said that the average lead level in its most recent tests was 7 ppb. The authority also claimed that a voluntary blood-lead-level screening it funded showed no identifiable public health impact from the elevated lead levels.\n\nThe EPA formally reduced its oversight of WASA in May 2006, after testing showed the lead levels had remained below 15 ppb for a full year. However, the agency continued to require WASA to submit 100 samples every six months for at least another year; the normal requirement is 50 samples a year.\n\nEven after the lead levels abated, the \"Post\" continued to note other problems with the District's water supply. In 2007, it noted that WASA's periodic \"chlorine surge\" resulted in high levels of haloacetic acids, a chlorine disinfection byproduct believed to be unsafe. Utilities use annual or biannual chlorine surges to remove pathogens such as cryptosporidium that are not adequately killed by chloramine. Independent testing by the Environmental Working Group showed 40 percent of samples exceeded federal safety limits for chlorine byproducts, but officials from the Washington Aqueduct and WASA said the levels were probably temporary and that the water was safe to drink.\n\nIn January 2006, the Government Accountability Officereleased a study criticizing the EPA's efforts monitoring lead levels in drinking water across the United States.Although the study found that lead in drinking water had generally declined since the 1990s, it noted that data collection problems \"may be undermining the intended level of public health protection\".It should be noted that there have been studies that show that there is no safe level for lead consumption. \n\nIn 2007, the journal \"Environmental Health Perspectives\" published a paper about the public health response to the lead contamination, written by a team of academic investigators headed by Tee Guidotti of George Washington University. Guidotti and his team had been hired as consultants by WASA in 2004; he had told the WASA board that water only accounts for seven percent of the typical two-year-old child's lead exposure. The paper analyzed data from the 2004 screening program, identifying sources of confounding or bias. Four indicators were examined, and none showed evidence that blood lead levels had been affected by the elevation of lead in drinking water. The investigators concluded that the evidence did not clearly demonstrate a correlation between lead in the District's drinking water and blood lead levels. However, they cautioned that a population study was not suitable for establishing such relationships. They called for reduction in exposure from water as well as from other sources. The paper stated \"There appears to have been no identifiable public health impact from the elevation of lead in drinking water in Washington DC in 2003 and 2004.\"\n\nThe Guidotti paper was called into question by members of the DC Council in February 2009, after the \"Washington Post\" ran an article about a more recent study by Marc Edwards that found a correlation between water and blood lead levels in area children. The National Institutes of Health, publisher of the journal in which the paper appeared, were unaware that George Washington University's contract with WASA gave the water authority final approval over anything Guidotti wrote concerning the authority. Guidotti was also supposed to remove the sentence about public health impact from the paper before publication, because experts reviewing the paper had rejected that finding; he failed to do so. Guidotti and WASA both denied that the agency influenced the paper; Guidotti said he didn't view the contract as giving WASA preapproval over the paper, and therefore didn't disclose it to the journal. In an August 2006 e-mail obtained by the \"Post\" from Edwards, Guidotti agreed to replace the sentence with \"Measures to protect residents from exposure to lead in drinking water may have prevented more frequent elevations in blood lead\" before its publication, but that did not happen. The journal refuses to accept research that is under a sponsor's control; it conducted a review to determine if the paper should be retracted—the journal's first such review in its 30-year history. The paper had been cited as evidence that the lead contamination had not harmed District residents.\n\nIn an e-mail to the \"Washington City Paper\", Guidotti refuted any claims that WASA influenced the team's findings. He clarified that WASA's contract was with the university, not Guidotti personally. He said \"The data in our 2007 study are valid, the analysis was accurately reported, and we stand by the conclusions.\"\n\nThe review panel convened by \"Environmental Health Perspectives\" released its findings in June 2009, finding that the controversial sentence was included due to \"inattention to detail\" and found no evidence that Guidotti was trying to mislead readers. The panel recommended that Guidotti submit an apology and correction; he did so. The panel said it found evidence that neither Guidotti nor WASA intended for the utility to exercise approval over the research conclusions.\n\nThe March 1, 2009 issue of \"Environmental Science and Technology\" included a paper by Marc Edwards, Simoni Triantafyllidou, and Dana Best that established a link between the elevated lead levels in the drinking water and potentially harmful blood lead levels in area children. The \"Washington Post\" announced the results of that study on January 27, 2009. The report found that 42,000 children who were in the womb or under age 2 during the contamination are at risk of health and behavioral problems. In some areas, the number of children with enough lead to cause irreversible IQ loss and developmental delays more than doubled between 2000 and 2003. These findings contradicted previous statements by WASA that there were no health impacts, as well as the 2004 CDC MMWR report. David Bellinger, a Harvard University neurologist, told the \"Post\", \"If these data were available previously, I would be surprised that anyone would be assuring the public there was no problem.\"\n\nThe CDC refused to provide Edwards with the data necessary to perform the study. He convinced the Children's National Medical Center to share the data with him in 2008.\n\nThe paper won the Editor's Choice Award for Best Science Paper of 2009 from the editors of \"Environmental Science and Technology\".\n\nEdwards's study raised new questions about the March 2004 CDC Morbidity and Mortality Weekly Report that downplayed the health impact of the lead contamination. \"Salon\" noted that the CDC had found a link between lead pipes and high childhood blood lead levels in the district in 2007, but did not publicize the study. The principal author of the 2004 study—Mary Jean Brown, who co-authored the 2007 study—acknowledged that thousands of blood tests were missing from the 2004 study, but defended the paper's conclusion that any harm was slight. \"There is no indication that DC residents have blood lead levels above the CDC levels of concern\", she wrote.\n\nHowever, Edwards's results came from analysis of the same data used for the 2004 CDC report. When he wrote to the CDC's associate director of science, questioning the report's conclusions and methodology, he received a belated reply: \"We have examined CDC's role in the study and have found no evidence of misconduct.\"\n\nThe United States House of Representatives' Science and Technology Committee opened a congressional investigation into the 2004 CDC report. Investigators found that although the CDC and city health department reported dangerous lead levels in 193 children in 2003, the actual number was 486 according to records taken directly from the testing laboratories. Representative Brad Miller of North Carolina called the CDC report's data \"wildly incomplete\". The underreported results came from the city's health department, which had said the missing tests were omitted because some labs did not report low lead levels to the city. The health department data was also the basis for Guidotti's paper.\n\nThe investigation found that the CDC knowingly used flawed data in drafting the report, leading to \"scientifically indefensible\" claims in the 2004 paper. It also cited the CDC for failing to publicize later research showing that the harm was more serious than the 2004 report suggested. After the investigation's findings were released, the CDC initially stood by the report's findings that no significant harm was caused by the lead, but acknowledged that the claim that no children with lead poisoning had been found was \"misleading\". Edwards called for the CDC to retract the paper and for Brown's resignation. \"If you were a child living in D.C. at that time, a single glass of tap water could have elevated your blood lead to unsafe levels,\" said Edwards.\n\nThe report strongly criticized Brown for failing to check the original lab results. It also noted that one section of the 2004 CDC report said not one of the people living in a home with water lead levels 20 times higher than the action level had elevated blood lead, but it failed to mention that most of those people were drinking bottled or filtered water, not tap water, before their blood was tested—a fact Brown and her co-authors were well aware of. The investigation found that Brown gave her fellow authors just three hours to review her work before it was submitted for publication.\n\nA year after the \"Washington Post\" published the results of the investigation, the CDC published a \"Notice to Readers\" in the May 21, 2010 issue of the \"Morbidity and Mortality Weekly Report\" admitting that the 2004 report was misleading. It said the original report \"did not reflect findings of concern from the separate longitudinal study that showed that children living in homes serviced by a lead water pipe were more than twice as likely as other DC children to have had a blood lead level ≥10 µg/dL.\" A further Notice to Readers published in June 2010 clarified that the results in the 2004 report \"should not be used to make conclusions about the contribution of water lead to blood levels in DC, to predict what might occur in other situations where lead levels in drinking water are high, or to determine safe levels of lead in drinking water.\"\n\nThe \"Post\" described the CDC's Notice to Readers as \"a full vindication\" for Edwards, who spent tens of thousands of dollars out of his own pocket to fund his research, and who was the target of attempts to besmirch his professional reputation by the CDC and EPA. Tom Sinks, the deputy director of the CDC's national center for environmental health, told the Post \"Looking backward six years, it's clear that this report could have been written a little better\".\n\nIn 2010, the Union of Concerned Scientists (UAS) claimed that the CDC Advisory Committee on Childhood Lead Poisoning had been expected to lower the action level for lead in drinking water below 10 micrograms per deciliter in the summer of 2002. According to the UAS, Tommy Thompson, the Secretary of Health and Human Services, took the unprecedented step of rejecting a number of nominees to the committee selected by staff scientists. Instead, the group claims, Thompson appointed at least two appointees with financial ties to the lead industry. A 2010 \"Washington Post\" editorial cited the group's statement as a reason why the District's lead contamination \"was practically inevitable\" due to politicization of the CDC.\n\nEleanor Holmes Norton, a Delegate to Congress representing the District, accused the CDC of a coverup. Jim Graham, a member of the DC Council, said \"To now learn that the Centers for Disease Control not only got it wrong but may have intentionally misled District residents and our water agency is the ultimate betrayal of the public trust\".\nA 2010 study by Brown at the CDC essentially confirmed Edwards's findings. It also found that the 15,000 \"partial pipe replacements\" performed by WASA—where lead service lines were replaced up to a homeowner's property line, but no further—may not have effectively reduced lead levels, and may have made the problem worse. WASA spent $97 million to replace 17,000 pipes, including the 15,000 partial replacements. A \"Post\" article about the report lead to jammed phone lines at WASA and increased demand for bottled-water delivery. The study indicated that there is no safe level of lead in drinking water, and that children in homes with even a partial lead service line are at a much higher risk of lead poisoning than those with no lead in the line. WASA said the finding \"is not news\" to them; General Manager George Hawkins told WAMU that the utility had been acting as if the CDC's findings were fact for some time before the report was released. Hawkins said that WAMU was not seeing elevated levels of lead in homes with partial pipe replacements.\n\nOn February 17, 2009, John Parkhurst, the father of twin boys living on Capitol Hill, through his attorneys Stefanie Roemer and David Sanford of Sanford, Wittels & Heisler, filed a class action lawsuit against WASA in the Superior Court of the District of Columbia, seeking $200 million in compensatory damages plus punitive damages. Parkhurst, a 50-year-old single father and psychologist, prepared food and formula for the boys using tap water from the time they were 8 months old until 2002, when they were two years old. The complaint was later amended to add additional five children, on behalf of a proposed class consisting of all children who, at any time from 2000 to 2004, while six years of age or younger, consumed water supplied by WASA that passed through a line containing lead (whether directly or prenatally through their birth mothers), and who had blood-lead levels of 10μg/dl or higher. The suit claims that WASA \"undertook Herculean efforts to shield itself from liability and to otherwise deny responsibility\". Contemporary news reports indicated that WASA had not seen the lawsuit at the time the story broke, but included comments from WASA spokespeople that more studies would be needed before lead in drinking water could be linked to health and behavioral issues. The agency said that such claims would need to be substantiated on a case-by-case basis. WASA's motion to dismiss the complaint was denied in October 2009. In 2012 the named plaintiffs on the case sought to make the case a class action, but on April 8, 2013, judge Anthony C. Epstein denied their motion for class certification stating that the \"class of tap water drinkers under 7 years old with elevated blood lead levels was too broad\", and that it was not clear the class had or even possibly would suffer any injury.\n\nIn 2016, in congressional testimony, Marc Edwards estimated that the Washington, D.C. crisis was \"20 to 30 times worse\" than the Flint water crisis.\n\nThe Washington Post noted that the problem of lead in Washington, DC's drinking water is decades old. Within this new outlet, there are several articles that use incriminating and extreme headlines to grab the readers attention about the DC Water system. Many media outlets have compared the Flint Water Crisis to the decades old water issue in Washington, DC because both problems pertain to lead in drinking water.\n\nIn their 2010 water quality report, DC Water stated that there was lead found in some water samples taken during monitoring. The possible causes linked to the lead contamination was said to be \"corrosion of household plumbing systems; erosion of natural deposits.\" The report did not state where the lead was found or who it impacted. A Washington Post article, discussed that there was still work to be done to DC's drinking water systems. \n\nIn 2017, the DC Water Quality Report, Washington DC was found to be in compliance with the EPA's standards for lead within drinking water. However, there is no safe level of lead for children to consume. According to the EPA, lead at any level can be harmful. The  EPA has a maximum contamination goal of zero; however, they have not stated how or when they are going to enforce the regulation if it is amended.\n\nIn 2018, DC Water's Drinking Water Quality Report states that there is infrastructure in Washington, DC that contains lead that may impact the drinking water in certain areas. Tips for how to prevent contamination are provided on the report. All the lead pipes in Washington, DC have not been replaced and those that remain may cause problems for those consuming the water. DC Water created an interactive map to show its residents which pipe lines are made out of lead or other dangerous metals. The information was gathered from historical data and inspections.\n\n\n"}
{"id": "306364", "url": "https://en.wikipedia.org/wiki?curid=306364", "title": "Liquid nitrogen", "text": "Liquid nitrogen\n\nLiquid nitrogen is nitrogen in a liquid state at an extremely low temperature. It is a colorless clear liquid with a density of 0.807 g/ml at its boiling point () and a dielectric constant of 1.43. Nitrogen was first liquefied at the Jagiellonian University on 15 April 1883 by Polish physicists, Zygmunt Wróblewski and Karol Olszewski. It is produced industrially by fractional distillation of liquid air. Liquid nitrogen is often referred to by the abbreviation, LN or \"LIN\" or \"LN\" and has the UN number 1977. Liquid nitrogen is a diatomic liquid, which means that the diatomic character of the covalent N bonding in N gas is retained after liquefaction.\n\nLiquid nitrogen is a cryogenic fluid that can cause rapid freezing on contact with living tissue. When appropriately insulated from ambient heat, liquid nitrogen can be stored and transported, for example in vacuum flasks. The temperature is held constant at 77 K by slow boiling of the liquid, resulting in the evolution of nitrogen gas. Depending on the size and design, the holding time of vacuum flasks ranges from a few hours to a few weeks. The development of pressurised super-insulated vacuum vessels has enabled liquefied nitrogen to be stored and transported over longer time periods with losses reduced to 2% per day or less.\n\nThe temperature of liquid nitrogen can readily be reduced to its freezing point by placing it in a vacuum chamber pumped by a vacuum pump. Liquid nitrogen's efficiency as a coolant is limited by the fact that it boils immediately on contact with a warmer object, enveloping the object in insulating nitrogen gas. This effect, known as the Leidenfrost effect, applies to any liquid in contact with an object significantly hotter than its boiling point. Faster cooling may be obtained by plunging an object into a slush of liquid and solid nitrogen rather than liquid nitrogen alone.\n\nLiquid nitrogen is a compact and readily transported source of dry nitrogen gas, as it does not require pressurization. Further, its ability to maintain temperatures far below the freezing point of water makes it extremely useful in a wide range of applications, primarily as an open-cycle refrigerant, including:\n\nThe culinary use of liquid nitrogen is mentioned in an 1890 recipe book titled \"Fancy Ices\" authored by Mrs. Agnes Marshall, but has been employed in more recent times by restaurants in the preparation of frozen desserts, such as ice cream, which can be created within moments at the table because of the speed at which it cools food. The rapidity of chilling also leads to the formation of smaller ice crystals, which provides the dessert with a smoother texture. The technique is employed by chef Heston Blumenthal who has used it at his restaurant, The Fat Duck to create frozen dishes such as egg and bacon ice cream. Liquid nitrogen has also become popular in the preparation of cocktails because it can be used to quickly chill glasses or freeze ingredients. It is also added to drinks to create a smoky effect, which occurs as tiny droplets of the liquid nitrogen come into contact with the surrounding air, condensing the vapour that is naturally present.\n\nBecause the liquid-to-gas expansion ratio of nitrogen is 1:694 at , a tremendous amount of force can be generated if liquid nitrogen is rapidly vaporized in an enclosed space. In an incident on January 12, 2006 at Texas A&M University, the pressure-relief devices of a tank of liquid nitrogen were malfunctioning and later sealed. As a result of the subsequent pressure buildup, the tank failed catastrophically. The force of the explosion was sufficient to propel the tank through the ceiling immediately above it, shatter a reinforced concrete beam immediately below it, and blow the walls of the laboratory 0.1–0.2 m off their foundations.\n\nBecause of its extremely low temperature, careless handling of liquid nitrogen and any objects cooled by it may result in cold burns. In that case, special gloves should be used while handling. However, a small splash or even pouring down skin will not burn immediately because of the Leidenfrost effect, the evaporating gas thermally insulates to some extent, like touching a hot element very briefly with a wet finger. If the liquid nitrogen manages to pool anywhere, it will burn severely.\n\nAs liquid nitrogen evaporates it reduces the oxygen concentration in the air and can act as an asphyxiant, especially in confined spaces. Nitrogen is odorless, colorless, and tasteless and may produce asphyxia without any sensation or prior warning.\n\nOxygen sensors are sometimes used as a safety precaution when working with liquid nitrogen to alert workers of gas spills into a confined space.\n\nVessels containing liquid nitrogen can condense oxygen from air. The liquid in such a vessel becomes increasingly enriched in oxygen (boiling point ) as the nitrogen evaporates, and can cause violent oxidation of organic material.\n\nIngestion of liquid nitrogen can cause severe internal damage, due to freezing of the tissues which come in contact with it and to the volume of gaseous nitrogen evolved as the liquid is warmed by body heat. In 1997, a physics student demonstrating the Leidenfrost effect by holding liquid nitrogen in his mouth accidentally swallowed the substance, resulting in near-fatal injuries. This was apparently the first case in medical literature of liquid nitrogen ingestion. In 2012, a young woman in England had her stomach removed after ingesting a cocktail made with liquid nitrogen.\n\nLiquid nitrogen is produced commercially from the cryogenic distillation of liquified air or from the liquefication of pure nitrogen derived from air using pressure swing adsorption. An air compressor is used to compress filtered air to high pressure; the high-pressure gas is cooled back to ambient temperature, and allowed to expand to a low pressure. The expanding air cools greatly (the Joule–Thomson effect), and oxygen, nitrogen, and argon are separated by further stages of expansion and distillation. Small-scale production of liquid nitrogen is easily achieved using this principle. Liquid nitrogen may be produced for direct sale, or as a byproduct of manufacture of liquid oxygen used for industrial processes such as steelmaking. Liquid-air plants producing on the order of tons per day of product started to be built in the 1930s but became very common after the Second World War; a large modern plant may produce 3000 tons/day of liquid air products.\n\n"}
{"id": "52683089", "url": "https://en.wikipedia.org/wiki?curid=52683089", "title": "List of CAHIIM-accredited HIM Programs in the United States", "text": "List of CAHIIM-accredited HIM Programs in the United States\n\nThe Commission on Accreditation for Health Informatics and Information Management Education (CAHIIM) has given full accreditation to the following list of Health information management (HIM) programs in the United States.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "31712157", "url": "https://en.wikipedia.org/wiki?curid=31712157", "title": "M. Tech Clinical engineering", "text": "M. Tech Clinical engineering\n\nMaster of Technology in Clinical engineering is a graduate program in Clinical engineering offered jointly by three institutes in India. This is the only graduate program currently in India that trains engineers in the field of clinical engineering. It is one of the two programs jointly offered by the three institutes, the other being a PhD in Biomedical devices & technology.\n\nOver the past five decades, healthcare delivery has increasingly become technology driven - be it development of new drugs, vaccines or medical devices. Innovative solutions to healthcare can be developed with a cross-disciplinary approach with systematic training, in which, students with a background in engineering are trained in biological sciences and clinical practices.\n\nThe aim is to address the country's need of human resource development in Clinical engineering as well as in Biomedical Device Development, and thereby contribute to the overall development of healthcare delivery in the country.\n\nThis Program was started in the year 2008 with the assistance of department of science & technology, Government of India. The three institutes in this joint program are premier institutes in India in their respective fields.\n\n\n\nThe orientation program is for about 3–4 weeks period at SCTIMST, Trivandrum. This orientation introduces the new entrants about the entire program and clinical engineering in generally. They are taken to various departments in hospital wing and also at the BMT wing. introductory classes are taken related to clinical sciences and medical device development by practicing clinicians and scientists who work in development of medica devices.\n\nThe courses of 1st semester are held at the IIT Madras.\n\nThe course work for this semester is held at Christian medical college, Vellore.\n\nThe courses of third semester are at SCTIMST, Trivandrum\n\n\n\nVisits to various rural hospitals are undertaken for period of about 1 week to learn and understand about the ground realities in rural healthcare system. Some of the locations to which the students have visited so far includes:\n\nThese full-time internships are taken up by all students in this program for a period of about 4 weeks in the biomedical / clinical engineering departments of various corporate hospital across the major cities of India. So far the following are the hospitals where students have gone for internship:\n\nAll the students are required to independently identify the problems in clinical settings during their clinical attachment and then try to solve them using an innovative approach and finally submit 25 solutions which are novel, practical and implementable. one or more these ideas might on a later stage be implemented as a project.\n\nThis gives the students an opportunity to horn their skills of innovative thinking. Hundreds of such ideas have been generated so far.\n\nThe intention of clinical attachment for engineers is quite similar to that why a clinical student is put into it i.e. to learn how to work in clinical settings and understand the various challenges faced there. This gives a good working experience to the students before the graduate itself.\n\nAt CMC Vellore, the clinical attachment is 2 days per week for an entire semester leading to more than 100 hours of total clinical rotation across about 10 clinical departments.\n\nB.E./B.Tech./Recognized appropriate four-year course; on the basis of GATE (Graduate Aptitude Test in Engineering) subject (AE, BT, CE, CH, CS, EC, EE, IN, IT, ME, MN, MT, PI, TF, XE) and score.\n\nThe admissions are carried out by IIT Madras at the IIT Madras campus, Chennai. Only the candidates with a valid GATE score are eligible to apply for this program. Applications can be sent in when the notification is released (generally in the period of march every year). The list of shortlisted is declared by mid-April and these candidates are generally called further rounds of selection in the month of may. These selections include a screening test which is on basic mathematics and physics. This is followed by personal interview of those candidates who have cleared the screening test. Those who finally clear this interview process are offered admission into the M.Tech program.\n\nIntake currently being offered under this program is around 16 students every year.\n\n"}
{"id": "53026638", "url": "https://en.wikipedia.org/wiki?curid=53026638", "title": "Maternal Mortality and Morbidity Task Force", "text": "Maternal Mortality and Morbidity Task Force\n\nThe Maternal Mortality and Morbidity Task Force was started by the Department of State in 2013 to help reduce maternal death in Texas. The task force and DSHS must submit a joint report on the findings of the task force and recommendations to the governor, lieutenant governor, speaker of the House of Representatives, and appropriate committees of the Texas Legislature by September 1 of each even-numbered year, beginning September 1, 2016. The maternal mortality ratio (MMR) for the state of Texas was concluded to be the highest in the developed world in 2016, with the maternal mortality rate (MMRate) of the state surging beyond the poor MMRate of 48 states of the US (excluding California and Texas) at 23.8% to a remarkably high 35.8%.\nIn the United States the maternal mortality ratio rose during the years 2002-2015. Although improvements in health care facilitated a dramatic decline in maternal mortality worldwide during the 20th century, women still die from complications of pregnancy, though there are significant differences in the top causes per region and income class. Since 1990 the World Health Organization measures both the maternal mortality ratio and the maternal mortality rate. In 1994 the period of measurement for a \"maternal-related death\" after childbirth was extended from six weeks to a year after the \"birth event\". The United States is one of the few developed countries for which both of the WHO measurements have gone up and not down. Nationally as well as in Texas, black women have a maternal mortality rate more than twice as high as White women and this disparity gap has increased since 2007, but the causes of this increase were unclear in 2013 and therefore the task force was formed to investigate. The estimated maternal mortality rate (per 100,000 live births) for 48 states and Washington, DC (excluding California and Texas) increased by 26.6%, from 18.8 in 2000 to 23.8 in 2014. California showed a declining trend, whereas Texas had a sudden increase in 2011-2012. The Texas rate had nearly doubled. Although reproductive health and maternal death is significantly different for black women, this is not part of the study conducted by the Boston Black Women's Health Study and the taskforce is the only known body studying this aspect. Since convening in 2013 the task force has produced two reports, in July 2014 and in September 2016.\nIn the latest report it shows a significantly higher increase in maternal mortality for black women. This may be related to a higher incidence of pre-pregnancy obesity, which has both been shown to be higher in black women and to be correlated to a first-time cesarean section. It may also be related to discrimination in health care for black women, which can take the form of reduced access to services and information.\n\n"}
{"id": "36431738", "url": "https://en.wikipedia.org/wiki?curid=36431738", "title": "Medicines and Related Substances Control Amendment Act, 1997", "text": "Medicines and Related Substances Control Amendment Act, 1997\n\nIn an effort to combat the growing HIV/AIDS epidemic, the Government of South Africa enacted the Medicines and Related Substances Control Amendment Act in 1997. The Act aimed to both reduce the cost of drugs and increase their availability.\n\nIn February 1998, the South African Pharmaceutical Manufactures Association and forty Multinational Corporations (MNC) brought a suit against the government of South Africa for its passage of the Medicines and Related Substances Control Amendment Act No. 90 of 1997, arguing that it violated the Agreement on Trade-Related Aspects of Intellectual Property Rights. The intent of the Act was to reduce drug prices by allowing generic substitution of off-patent drugs, the parallel importation of on-patent drugs as well as price transparency. In agreement with the suit, the United States (US) and European Communities (EC) threatened economic sanctions. However, HIV/AIDS activists successfully contested this, demonstrators alleging that then United States presidential-candidate Al Gore was killing babies in Africa—and forced the US and EC to back off the South African government. As a result of immense international pressure, including from NGOs such as Oxfam, the pharmaceutical companies dropped their case in April 2001.\n"}
{"id": "55139946", "url": "https://en.wikipedia.org/wiki?curid=55139946", "title": "Medigadda Barrage", "text": "Medigadda Barrage\n\nMedigadda Barrage is the starting point of the proposed Kaleshwaram Project which envisages construction of three barrages between Yellampally & Medigadda. Currently it is under construction and its objective is to utilize Godavari water for drinking and irrigation. The Barrage/Project site is located at Medigadda Village, Mahadevpur Mandal, Jayashankar Bhupalpally district in Telangana State, India.\nProposed Barrage Details:\nMedigadda Barrage foundation was laid by First Chief Minister of Telangana, K.Chandrashekar Rao on 02 May 2016.\n\nThe project started by Telangana govt as part of the Kaleshwaram Lift Irrigation Schema to irrigate the of new land and stabilize the of existing irrigated land.\n\n"}
{"id": "19870248", "url": "https://en.wikipedia.org/wiki?curid=19870248", "title": "Mexican paradox", "text": "Mexican paradox\n\nThe Mexican paradox is the observation that the Mexican people exhibit a surprisingly low incidence of low birth weight (LBW), contrary to what would be expected from their socioeconomic status (SES). This appears as an outlier in graphs correlating SES with low-birth-weight rates.\n\nIt has been proposed that resistance to changes in diet is responsible for the positive birth weight association for Mexican-American mothers.\n\nNevertheless, the medical causes of lower rates of low birth weights among birthing Mexican mothers has been called into question.\n\nThe results of the study showed that the mean birth weight of Mexican-American babies was 3.34 kg (7.37 lbs), while that of non-Hispanic White babies was 3.39 kg (7.48 lbs.). This finding re-emphasized the independence of mean birth weight and LBW. This however did not refute the discrepancies in LBW for Mexicans.\n\nThe study also showed that the overall preterm birth rate was higher among Mexican Americans (10.6%) than non-Hispanic Whites (9.3%).\n\nThe overall hypothesis of the authors was that this finding reflected an error in recorded gestational age, described in a strongly bimodal birth-weight distribution at young gestational ages for Mexican-Americans.\n\n\n\n"}
{"id": "53452227", "url": "https://en.wikipedia.org/wiki?curid=53452227", "title": "Milo tin", "text": "Milo tin\n\nMilo tin is a Malaysian pejorative used to describe unsafe or cheaply made vehicles. It is comparable to the slang term, 'deathtrap'.\n\nThe term 'Milo tin' originated in the 1950s as a result of shoddy workmanship and cost-cutting measures, in which damaged vehicles were often repaired with recycled Milo tins as opposed to genuine parts. The workshops would repaint the tin panels, painting over the word 'Milo'. When the repaired vehicles became involved in subsequent accidents, the paint surface would scratch off and the word 'Milo' became visible again. Thus, the term 'Milo tin' was created.\n\nIn the 1960s, the 'Milo tin' term gained further popularity as a means of criticism towards the light and flimsy construction of early Japanese cars. The Malaysian market had historically been a stronghold for Western car companies, and Japanese cars were initially perceived as inferior and cheaply made. However, the Japanese cars continued to improve and gained a reputation for quality, reliability, high fuel efficiency and value for money. By the 1970s, the Malaysian market was dominated by Japanese cars, and the usage of the 'Milo tin' term against Japanese cars gradually faded.\n\nIn the 1980s and 1990s, 'Milo tin' regained popularity as a discriminatory term towards Proton and Perodua cars. The early Proton and Perodua vehicles lacked modern safety features such as airbags and ABS. The thin construction also drew criticism, and the door closing action produced an unpleasant tinny sound. Newer models have remedied the old problems, and Proton scored its first 5-star ANCAP safety rating in 2013. However, the negative stigma persists and the term 'Milo tin' is still widely used on Proton and Perodua cars today.\n\n"}
{"id": "31494523", "url": "https://en.wikipedia.org/wiki?curid=31494523", "title": "National Association of Statutory Health Insurance Physicians", "text": "National Association of Statutory Health Insurance Physicians\n\nThe National Association of Statutory Health Insurance Physicians (NASHIP) - in German \"Kassenärztliche Bundesvereinigung (KBV)\", based in Berlin, is the co-ordinating body of the about 150,000 office-based physicians' and psychotherapists' self-administration in Germany. It co-ordinates the activities of the 17 State Association of Statutory Health Insurance Physicians. NASHIP is co-owner of the medical journal Deutsches Ärzteblatt and of the German Agency for Quality in Medicine, a member of the Guidelines International Network - together with the German Medical Association. Andreas Köhler MD is chairman of the board.\n\nNASHIP being a body under public law is part of the medical self-government in Germany. The organisation advocates positions of the office-based physicians and physiotherapists in legislative processes, keeps the federal registry of physicians, and concludes contracts with the national confederations of the health insurance funds and other parties of the health care sector. Together with the health insurance funds it devises and revises the office-based doctors’ fee schedule, the so-called Uniform Assessment Standard. As a member of the Federal Joint Committee it is also one of the organizations that determine the benefits catalogue.\n"}
{"id": "29472059", "url": "https://en.wikipedia.org/wiki?curid=29472059", "title": "National Obesity Observatory", "text": "National Obesity Observatory\n\nThe National Obesity Observatory (NOO) was a publicly funded body that is part of a network of Public Health Observatories across Britain and Ireland. It published data, information, and intelligence related to obesity, overweight, and their underlying causes. NOO is now part of [Public Health England] who now carry out their work. \n\nNOO worked closely with a wide range of organisations to assist policy makers and practitioners who were involved in understanding and tackling obesity at population level. It did this through analysing and interpreting research and data to produce reports and briefings. To support these activities it also produced analytical and data visualisation tools. These are used, for example, to map obesity and associated determinants at national, regional and local levels.\n\nMembers of NOO were instrumental in establishing the National Child Measurement Programme (NCMP). Analysis and interpretation of the NCMP dataset is a core element of NOO's work.\n\nNOO is a member of the Association of Public Health Observatories.\n\nThe Standard Evaluation Framework (SEF) for weight management interventions was published by the NOO in 2009. It comprises a guidance document in three parts: an introductory guide to evaluation; a table listing essential and desirable data to be collected; and a detailed description of each data item in the table.\n\nThe SEF is intended to help people to achieve and maintain a healthy weight. The aim of the SEF is to increase the number and quality of evaluations and this in turn will support the generation of evidence-based practice.\n\nThe SEF was developed following extensive consultation with a wide range of interested parties, including practitioners, academic experts, and representatives from public health observatories, regional public health groups, primary care trusts and other relevant organisations.\n\nThe SEF lists the minimum data necessary for a meaningful evaluation; alongside these it also lists a further set of desirable data that would enhance the evaluation. Supporting material explains the reasons behind these classifications and provides guidance on how to collect data. The SEF also provides basic guidance on how to conduct an evaluation for people with limited experience in this area.\nThe SEF can be used with a range of interventions including those conducted with individuals on a one-to-one basis or in groups, and in clinical and community settings. It is not intended for use with surgical and medicinal approaches to weight management, or in broader programmes that include changes to the built environment.\nThe SEF is very much a work in progress, and will itself be subject to an evaluation.\n\n"}
{"id": "33615951", "url": "https://en.wikipedia.org/wiki?curid=33615951", "title": "Navajo medicine", "text": "Navajo medicine\n\nNavajo medicine today has remained preserved for millennia as many Navajo people have relied on traditional medicinal practices as their primary source of healing. However, modern day residents within the Navajo Nation have incorporated contemporary medicine into their society with the establishment of Western hospitals and clinics on the reservation over the last century.\n\nIn addition, medicine and healing are deeply tied with religious and spiritual beliefs, taking on a form of shamanism. These cultural ideologies deem overall health to be ingrained in supernatural forces that relate to universal balance and harmony. The spiritual significance has allowed the Navajo healing practices and Western medical procedure to coexist as the former is set apart as a way of age-long tradition.\n\nIllness is described as the manifested mental or physical consequence brought on by a disruption of patient harmony. Some causes of this disruption include taboo transgression, excessive behavior, improper animal contact, improper ceremony conduction, or contact with malignant entities including spirits, skin-walkers and witches. Breaking taboos is believed to be acting against the principles devised by the Holy People that withhold personal harmony with the environment.\nThere are some cases in which illness is merely the result of accident. Personal injury or illness can be the error from lack of judgment or unintentional contact with harmful creatures of nature.\nIllness can also be brought on by malevolent practitioners of negative medicine. This belief in hóchxǫ́, translated as \"chaos\" or \"sickness\", is the opposite of hózhǫ́ and helps to explain why people, who are intended to be in harmony, perform actions counter to their ideals, thus reinforcing the need for healing practices as means of balance and restoration. Those who practice witchcraft include shape shifters who intend to use spiritual power and ceremony to acquire wealth, seduce lovers, harm enemies and rivals.\nIll health is also believed to be brought upon by chindi (ghost) who can bring about a kind of ghost sickness that leads others to death.\n\nreference\naziz baloch\n\nNavajo Hatałii are traditional medicine men who are called upon to perform healing ceremonies. Each medicine man begins training as an apprentice to an older practicing singer. During apprenticeship, the apprentice assembles medicine bundles (\"jish\") required to perform ceremonies and assist the teacher until deemed ready for independent practice. Throughout his lifetime, a medicine man can only learn a few chants as each requires a great deal of time and effort to learn and perfect. Songs are orally passed down in traditional Navajo from generation to generation. Unlike other American Indian medical practitioners that rely on visions and personal powers, a healer acts as a facilitator that transfers power from the Holy People to the patient to restore balance and harmony. Healing practice is performed within a ceremonial hogan. It is common for medicine men to receive payment for their healing services. In the past, healing was exchanged for sheep. In modern times however, monetary payment has become a widely accepted form of compensation. It should be noted that women can also play the role of healer in medicinal practice.\n\nHand tremblers act as medical diagnosticians and are sometimes called upon in order to verify an illness by drawing on divine power within themselves as received from the Gila monster. Typical services can be provided in the form of songs, prayers, and herb usage. During a diagnosis a hand trembler traces symbols in the dirt while holding a \"trembling arm\" over the patient. Movement of the arm signifies a new drawn symbol or a possible identification to the cause of illness. Once a solution has been found, the patient can be referred to a herbalist or singer needed to perform a healing ceremony.\n\nA number of healing ceremonies are performed according to a given patient situation. Some chants and rites for curing purposes include:\n\nSee Navajo ethnobotany for a list of plants and how they were used.\n\nNavajo Indians utilize approximately 450 species for medicinal purposes, the most plant species of any native tribe. Herbs for healing ceremonies are collected by a medicine man accompanied by an apprentice. Patients can also collect these plants for treatment of minor illnesses. Once all necessary wild plants are collected, an herbal tea is made for the patient, accompanied by a short prayer. In some ceremonies, the herbal mixture causes patient vomiting to ensure bodily cleanliness. Purging can also require the patient to immerse themselves in a yucca root sud bath. Any distribution of medicinal herbs to a patient is accompanied by spiritual chanting.\nThe Navajo people recognize the need for botanical conservation when gathering desired healing herbs. When a medicinal plant is taken, the neighboring plants of the same species receive a prayer in respect. Despite this fact, the collection of medicinal herbs has been more difficult in recent years as the result of migrating plant spores.\nPopular plants included in Navajo herbal medicine include Sagebrush (\"Artemisia\" spp.), Wild Buckwheats (\"Eriogonum\" spp.), Puccoon (\"Lithospermum multiflorum\"), Cedar Bark (\"Cedrus deodara\"), Sage (\"Salvia\" spp.), Indian Paintbrush (\"Castilleja\" spp.), Juniper Ash (\"Juniperus\" spp.), and Larkspur (\"Delphinium\" spp.).\n\nSand painting is the transfer of strength and beauty to the patient through various drawings made by a medicine man in the surrounding sand during a ceremony. Elaborate figures are drawn in the sand using colorful crushed minerals and plants. Many sand paintings contain depictions of spiritual yeii to whom a medicine man will ask to come into the painting in order for patient healing to occur. After each ceremony, the sacred sand painting is destroyed.\n\nAs prompted by the Meriam Report in 1928, federal commitment to Indian health care under the New Deal increased as the Bureau of Indian Affairs (BIA) Medical Division expanded, making medical care more accessible, affordable, and tolerated by the Navajo populace.\n\nIncreased demand of BIA medical care by Native Indians conflicted with post World War II conservatives who resented government funded and privileged health care. Growing interest in Indian termination policy in addition to unaided medical attention called for a transition of medical affluence by both native and non-native parties.\n\nUnder the Kennedy and Johnson administrations, funding was provided for the United States Public Health Service to gain a \"Division of Indian Health\" which would help provide a stronger federal commitment to health care. This division would later be renamed the division of Indian Health Service. Despite its initial successes, the Indian Health Service on the Navajo Nation faced challenges of being underfunded and understaffed. In addition, language barriers and cross-cultural tensions continued to complicate the hospital and clinic experience.\n\nExpanding Western medical influence and diminishing medicine men in the second half of the 20th century helped to initiate activism for traditional medical preservation as well as Indian representation in Western medical institutions.\n\nWith the coming of the 1970s spawned new opportunities for Navajo medical self-determination. The Indian Health Care Improvement Act 1976 aided local Navajo communities in autonomously administering their own medical facilities and prompted natives to gain more bureaucratic positions in the Indian Health Service. The gained presence of native people in medical institutions also helped ease many who regarded non-Navajo medical providers with mistrust.\n\nCommunity medical care that relied less on government involvement also took root in Rough Rock and Ganado, both towns that administered their own health care services. Navajo Nation Health Foundations was run in Ganado solely by Navajo people. In expressing identity in the medical community, the Navajo Nation took advantage of the National Health Planning and Resources Development Act to create the Navajo Health Systems Agency in 1975, being the only American Indian group to do so during that time.\n"}
{"id": "93446", "url": "https://en.wikipedia.org/wiki?curid=93446", "title": "Nintinugga", "text": "Nintinugga\n\nNintinugga was a Babylonian goddess of healing, the consort of Ninurta. She is identical with the goddess of Akkadian mythology, known as Bau (cuneiform: 𒀭𒁀𒌑 ba-u), Baba though it would seem that the two were originally independent. Later as Gula and in medical incantations, Bēlet or Balāti, also as the Azugallatu the \"great healer\",same as her son Damu. Other names borne by this goddess are Nin-Karrak, Nin Ezen, Ga-tum-dug and Nm-din-dug. Her epithets are \"great healer of the land\" and \"great healer of the black-headed ones\", a \"herb grower\", \"the lady who makes the broken up whole again\", and \"creates life in the land\", making her a vegetation/fertility goddess endowed with regenerative power. She was the daughter of An and a wife of Ninurta. She had seven daughters, including Hegir-Nuna (Gangir). \n\nThe name Bau is more common in the oldest period and gives way to Gula after the First Babylonian Dynasty. Since it is probable that Ninib has absorbed the cults of minor sun-deities, the two names may represent consorts of different gods. However this may be, the qualities of both are alike, and the two occur as synonymous designations of Ninib's female consort.\n\nShe was known as a patron deity of Lagash, where Gudea built her a temple.\n\nAfter the Great Flood, she helped \"breathe life\" back into mankind. The designation well emphasizes the chief trait of Bau-Gula which is that of healer. She is often spoken of as \"the great physician,\" and accordingly plays a specially prominent role in incantations and incantation rituals intended to relieve those suffering from disease.\n\nShe is, however, also invoked to curse those who trample upon the rights of rulers or those who do wrong with poisonous potions. As in the case of Ninib, the cult of Bau-Gula is prominent in Shirgulla and in Nippur. While generally in close association with her consort, she is also invoked alone, giving her more dominance than most of the goddesses of Babylonia and Assyria.\n\nIn the Neo-Babylonian period, she also had an oneiric quality. She had sometimes violent nature as the \"queen whose 'tempest', like a raging storm, makes heaven tremble, makes earth quake\". ). She was a source for blasphemous remarks where Gula and her dogs are mentioned in formulae of a curse.\n\nShe appears in a prominent position on the designs accompanying the Kudurrus boundary-stone monuments of Babylonia, being represented by a portrait, when other gods and goddesses are merely pictured by their shrines, by sacred animals or by weapons. In neo-Babylonian days her cult continues to occupy a prominent position, and Nebuchadrezzar II speaks of no less than three chapels or shrines within the sacred precincts of E-Zida in the city of Borsippa, besides a temple in her honour at Babylon.\n\n"}
{"id": "53727266", "url": "https://en.wikipedia.org/wiki?curid=53727266", "title": "Nonresidential water use in the U.S.", "text": "Nonresidential water use in the U.S.\n\nNonresidential water use refers to all uses (and users) of publicly-supplied (municipal) water other than residential use. The nonresidential users include industrial (I), commercial (C), and institutional (I) sub-sectors, which are often jointly designated as ICI or CII sector. \nIn the United States, a USGS nationwide compilation of public supply withdrawals and deliveries indicates that in 2010 the total daily volume of nonresidential use was approximately 12,000 million gallons per day (mgd) and accounted for about 29 percent of public supply withdrawals (or 45 gallons per capita per day when divided by the estimated 268 million people who relied on public-supply water). This estimate is obtained by subtracting from total public supply freshwater withdrawals (41,700 mgd) the reported domestic (residential) use (23,800 mgd) and allowing for 14 percent of unaccounted for water (because of leaks, hydrant flushing, tower maintenance, and other system losses, also called non-revenue water). The estimates of average water loss in public supply systems in the U.S. range from 12 to 16 percent. The share of CII use varies across water utilities; in a sample of 31 utilities in Texas it ranged from less than 4 percent of metered use in Lamar County WSD to 79 percent in Borger MWS, with average share of 35 percent.\nIn order to achieve improvements in the efficiency of CII water use, it is necessary to obtain information about the existing conservation potential in different types of establishments and about specific end uses of water in each CII category. Studies of water use in the CII sector were conducted by the American Water Works Association, Pacific Institute, Colorado WaterWise Council, Conserve Florida Water Clearinghouse (CFWC), and Water Research Foundation (WRF).\n\nNearly a hundred specific end uses within seven major groupings (i.e., washing and sanitation, domestic-type uses, landscape irrigation, outdoor and indoor water features, cooling and heating, food service, and process water) were compiled in the WRF study. Typically, the total CII use in an urban area or a region is broken down by categories of commercial establishments (and types of institutions or industrial plants) based on the kinds of goods and services provided, or their function. The category's water use is then separated into several end uses (or purposes). \nThe following list of 14 major CII categories of nonresidential buildings or establishments (with examples of possible subcategories) are commonly found in an urban water service area of a public water supplier (or water utility):\n\nThese CII categories and subcategories account for a high percentage of the total CII use. Some of these building/establishment types are identified at some levels of aggregation of the North American Industry Classification System (NAICS) and former Standard Industrial Classification (SIC) codes. Also, the Energy Information Administration (EIA) identified 85 types of commercial buildings and facilities which were grouped into 16 general categories. Many of these overlap with the list of 14 groupings listed above.\n\nWater conservation planners (at water utilities) and CII customers use several metrics of water use (and their associated benchmarks) in order to assess their success in achieving water use efficiency goals. A “metric” (or “performance indicator”) is a unit of measure (based on a formula) that can be used to calculate the rate of water use during a given period of time (and at a given level of data aggregation). A “benchmark” is a particular (numerical) value of a metric that denotes a specific level of performance, such as a water-use efficiency target.\nWhen quantifying water use in the CII sector, the total volume of consumption (over any given time interval) at any given CII facility is “normalized” by the scale of water-using activity, which corresponds to the rate at which water is used (also referred to as water use intensity or WUI). Two scaling factors (or \"proxy\" measures of scale), the number of employees and the square footage of buildings (or facilities) are commonly used in the CII sector because the employment and floor space data are generally available. Examples of the corresponding proxy metrics are: water use per employee in manufacturing plants, or water use per square foot in office buildings. However, more appropriate are \"functional\" metrics such as water use per occupied room in hotels, or water use per meal served in restaurants, since they are linked more directly to water use. \nNational surveys of commercial buildings by the U.S. DOE Energy Information Administration and EPA Energy Star provided proxy metrics data on the mean and median values of water use per square foot of buildings floor space. The median values are useful when placing water use in a CII establishment among its peers because the sample distributions typically have a pronounced right tail skew. Mean (or average) values are useful to planners who need to estimate total category use based on results from a sample of establishments.\n\nWater utilities often adopt water efficiency programs that are directed specifically to CII customers. The programs often target the largest water users as well as specific categories of CII customers including government and municipal buildings, large landscape areas, schools and colleges, office buildings, restaurants and hotels. Water use information in these and other readily recognizable functional classes of CII users from several studies of the CII sector are briefly characterized below.\n\nIn the U.S. cities, the share of the total CII use of water in office buildings ranges from 12 percent (in Tampa, Florida) to 30 percent (in New York City). The three largest uses of water in office buildings include restrooms, heating and cooling, and landscape watering. \nThe reported proxy metrics of water use intensity (WUI) for office buildings range from 25 gallons/1000 square feet/day (g/ksf/d) to 129 g/ksf/d with reported weighted average use from eight utilities from Florida and Texas of 88 g/ksf/d. The reported median values range from 34 g/ksf/d to 62 g/ksf/d. \nEstimates of average daily use per employee range from 9 gallons/employee/day (ged) to 18 ged with the median value of 13 ged in the EPA’s Energy Star Portfolio Manager data. \nNationwide, there are approximately 1,012 thousand office buildings with a combined floor space of 1,952 million square feet. Assuming the WUI of 88 g/ksf/d as representing average office usage in the U.S., the total country-wide use would be 1,400 mgd or close to 12 percent of CII use. Significant quantities of water can be saved in older office buildings by replacing bathroom fixtures, cooling tower efficiency retrofits, and adopting efficient landscape irrigation measures. Estimates of potential water conservation savings range from 19 percent in Southwest Florida to 30 percent or more in California.\n\nRetail (also classified under mercantile or commerce) includes retail stores, vehicle dealerships, strip shopping centers, and enclosed malls. Retail outlets account for significant percentage of CII use: e.g., 4 percent in Oakland (East Bay MUD), 5 percent in Austin, Texas, and about 15 percent in Tampa, Florida, 17 percent in Phoenix, Arizona, and 20 percent in New York City. The major end uses in retail stores are space cooling, restrooms and landscape irrigation. The reported proxy metrics on water use intensity in retail outlets include an estimate of 122 gallons/1000 square feet/day (g/ksf/d) in Phoenix, Arizona, 40 g/ksf/d in Santa Fe, New Mexico, and 98 g/ksf/d for one-story stores and 115 g/ksf/d for community shopping centers based on data from eight utilities in Florida and Texas. According to the EIA survey there are 438 thousand retail stores (other than malls) and 164 thousand enclosed and strip malls in the U.S. Their estimated total floor space is, respectively, 5,439 and 5,890 million square feet. Assuming the WUI of 98 g/ksf/d for retail stores and 115 g/ksf/d for shopping centers, the combined use of retail outlets in the U.S. would be 1,210 mgd or 10 percent of total CII use.\n\nRestaurants (and fast food places) account for about 3 percent of total CII use in Austin, Texas and 5 percent in Oakland, California (EBMUD) and up to 8 percent in the State of Florida. The largest uses of water in restaurants result from kitchen activities such as washing dishes, making ice, and preparing food. A significant amount of water is also used for restrooms.\nThe proxy metrics data show WUIs in restaurants ranging from 474 to 578 g/ksf/d in selected Colorado utilities, an average of 589 g/ksf/d in Austin, Texas, and 670 g/ksf/d in Florida. Somewhat wider range of 356 to 907 g/ksf/d was reported in an earlier study. \nData on functional metrics suggests that restaurants would use around 6 to 9 gallons of water per meal served, 20 to 31 gallons per seat per day, and 86 to 122 gallons per employee per day. Other studies reported 17 gallons/seat/day in Santa Fe, New Mexico and 29 gallons/seat per day in Colorado utilities. Given the total floor space of 380 thousand food service buildings of 1,819 million square feet and assuming average use of 521 g/ksf/d, the total U.S.-wide use in restaurants would be 950 mgd or close to 8 percent of CII use. Restaurants can save significant amounts of water by maximizing the efficiency of pre-rinse spray valves, food disposal systems and upgrading dishwashers, ice machines, and steam cookers to Energy Star® qualified models that use less water.\n\nThe shares of total CII use in schools range from 3.5 percent in Phoenix, Arizona to 6.5 percent in Tampa, Florida and up to 8 percent in Oakland, California. Most schools use water for restrooms, cooling and heating, irrigation of outdoor playing fields and lawns, locker rooms, laboratories, and cafeteria kitchens. \nAn expected intensity of use (WUI) in schools would be between 22 and 44 g/ksf/d for indoor use and from 110 to 255 g/ksf/d of total use. More recent studies show observed school use of 33 to 52 g/ksf/d in Colorado and 76 g/ksf/d in eight utilities from Florida and Texas. The EPA Energy Star’s Portfolio Manager shows median use of 27 g/ksf/d. Data on functional metrics show schools' use between 3 and 15 gallons per school day per student for indoor use. and between 4.7 and 23.6 gallons/student/day in Santa Fe, New Mexico. There are 389,000 school buildings in the U.S. with the total floor space of 12,239 million square feet. This building count includes preschools and day care, elementary and middle schools, high schools, and colleges and universities. Given the total floor space of these educational facilities and assuming average use (WUI) of 68 g/ksf/d, as representing average use in the U.S., the total use in schools would be about 830 mgd or close to 7 percent of the total CII use. Because most water is used in restrooms and locker rooms, installing WaterSense labeled showerheads, toilets, bathroom faucets, and flushing urinals and also periodically checking automatic sensors on these fixtures would help ensure they are operating properly and thus eliminate unnecessary water use.\n\nThe share of total CII water use by hotels and motels in the U.S. cities ranges from 3 percent in Oakland, California and 4 percent in Austin, Texas to 13 percent in the State of Florida. Average daily use in hotels depends on the type and size of the hotel and the presence of the major end uses of water including cooling towers, on-site laundry, dining and kitchen, swimming pool, and landscape irrigation. For example, in a sample of 706 hotels in New York City, average daily water use intensity in 2011 ranged from 60 to 456 gallons per 1000 square feet (g/ksf/d), with the median use of 215 g/ksf/d. In other areas the median use per 1000 square feet were reported at: 257 gallons in Florida, and 219 gallons in Austin, Texas. The EPA Energy Star’s Portfolio Manager that tracks water use at CII facilities found median use in hotels of 102 gallons/room/day. Given the existing inventory of about 5 million hotel rooms in the country with the combined floor space of 3,319 million square feet, and assuming average use of 209 g/ksf/d, the estimated total water use would be 694 mgd or about 6 percent of the CII use. A popular way of saving water in hotels is by encouraging guests to reuse towels and bed linens in order to cut down on the amount of water used in hotel laundry and also by upgrading guest rooms with EPA WaterSense labeled faucets, shower heads, and toilets.\n\nWater use by hospitals and other health care facilities accounts for about 3 percent of CII use in Phoenix, Arizona, and about 11 percent in New York City. The EPA Energy Star’s Portfolio Manager, (designed to track water use of CII facilities) found hospitals to be the third most intense use category, at around 150 gallons per 1000 square feet per day (g/ksf/d), following senior care facilities and hotels, with some facilities reporting WUIs of greater than 410 g/ksf/d. Other studies reported average use of 159 g/ksf/d in Austin, Texas, and a median of 140 g/ksf/d in EPA's Portfolio Managers data set. Data from medical offices show usage rates of 156 g/ksf/d in Florida. and 64 g/ksf/d in Santa Fe, New Mexico. With respect to functional metrics, the Portfolio Manager data show the median hospital use of 315 gallons of water per hospital bed per day. Given the total floor space of 2,374 million square feet in 10 thousand inpatient care (hospital) buildings and assuming the average WUI of 186 g/ksf/d, the total use in hospitals would be 442 mgd or close to 3.7 percent of CII use. In addition, there are 147 thousand buildings with outpatient care clinics and medical offices with the total floor space of 1,780 million square feet. Assuming average use of 132 g/ksf/d, the total use in outpatient care would be 235 mgd or close to 2.0 percent of CII use. Hospitals can save water by employing water-efficient practices through operational improvements and upgraded equipment, especially cooling towers and other cooling and heating equipment.\n\nNursing homes and assisted living communities account for 3.2 percent of CII use in the State of Florida and 5.4 percent in the urban area served by Tampa Bay Water. The reported estimates of WUI are 232 g/ksf/d in eight utilities from Florida and Texas and a range from 170 to 277 g/ksf/d in Colorado.\nThere are approximately 15,600 nursing homes (with 1,663,300 beds) and 30,200 residential care communities (with 1,000,000 beds) in the U.S. Based on the total floor space of 1,275 million square feet and an assumed average use of 232 g/ksf/d, the total water use by elderly care buildings in the U.S. would be 296 mgd or 2.5 percent of CII use.\n\nThere are 113 thousand commercial car washes in the U.S. and about 8 million cars are washed each day. Average use of fresh water to wash a car is about 38 gallons per vehicle (gpv) in automatic bay and conveyor types and about 15 gallons in self-service bays. Significantly less fresh water is needed in washes with reclaim water systems. Total estimated use of water in commercial car washes is about 2 percent of CII use.\n\nWater used in food stores and supermarkets represents about 1 percent or less of total CII use. Supermarkets use considerable quantities of water to cool the condensers units for the refrigeration systems, such as display coolers and freezers, storage coolers and freezers, butcher shops, delis, and bakeries. In addition, water is used in the cleaning and preparation of the fresh produce, meats, and fish before the products are put onto the shelves. Data on the intensity of water use include the estimate of 113 g/ksf/d in Santa Fe and a range from 161 to 295 g/ksf/d in other Southwestern U.S. locations. EPA's Portfolio Manager reported median use of 66 g/ksf/d. Other metrics include estimated 2.6 to 4.5 gallons per transaction. In the U.S., there are 177 thousand buildings involved in food sales. These include convenience stores, grocery stores and supermarkets with the total floor space of 1,252 million square feet. Given the total floor space and assuming average use intensity (WUI) of 65 g/ksf/d, as representing average use in the U.S., the total use in food sales would be 81 mgd or 0.7 percent of CII use. \nUse of public supply water by manufacturing category varies greatly across U.S. cities depending on the presence (or absence) of water-intensive manufacturing plants and the availability of local sources of water supply. The available data indicate shares of 9 percent in Tampa, Florida and New York City; 15 percent in Austin, Texas, 21 percent in Oakland, California (East Bay MUD) and 13 percent in North Carolina. In the U.S. there are 170,169 manufacturing establishments with a combined floor space of 11,100 million square feet. Most large manufacturing plants have their own source of water supply and purchase some amounts of drinking quality water from public systems. In 2010, the self-supplied industrial use (excluding thermoelectric plants and mining) was 15,900 mgd. The amount purchased from public systems was reported to be 4,750 mgd in 1995 (or about 40 percent of total CII use). Based on the total floor space of 11,100 million square feet in manufacturing buildings in the U.S. and assuming average use of 215 g/ksf/d, the total use of publicly-supplied water by the CII manufacturing category in the U.S. would be about 2,400 mgd or about 20 percent of the total CII use.\n\n"}
{"id": "38407784", "url": "https://en.wikipedia.org/wiki?curid=38407784", "title": "Onyebuchi Chukwu", "text": "Onyebuchi Chukwu\n\nOnyebuchi Chukwu is a Nigerian politician. He was the former Minister of Health.\n\nBorn on 22 April 1962 in Yaba, Lagos, Nigeria. Chukwu trained as a medical doctor at the College of Medicine of the University of Lagos in 1986. Subsequently, he obtained post-graduate qualification in orthopaedic surgery from the West African Postgraduate of Medical College. \n\nHe is a Fellow of the West African College of Surgeons and a Fellow of the International College of Surgeons. He is an International Affiliate and a member of the American Academy of Orthopaedic Surgeons and a member of the Societe Internationale de Chirurgie Orthopedique et de Traumatologie (SICOT).\n\nHe was the Chief Medical Director/Chief Executive Officer of the Ebonyi State University Teaching Hospital, Abakaliki (2003–2008). In 2007, he was appointed Professor of Orthopaedic Surgery at the Ebonyi State University Abakaliki Nigeria and visiting Professor of Surgery at the University of Nigeria, Enugu Campus in 2010.\n\nC. O. Onyebuchi Chukwu was the former Nigerian Minister of Health, who resigned in October 2014, to run as a candidate for the Governorship of Ebonyi state, Nigeria. He was the minister that led the hugely successful fight against the Ebola Virus Disease outbreak in Nigeria.He is a fellow of both the West African and the International College of Surgeons and a member of the Board of Partnership for Maternal Newborn and Child Health. Currently, he is the Chairman of the Country Coordinating Mechanism (Nigeria) of the Global Fund to fight AIDS, Tuberculosis and Malaria (GAFTM) and the West and Central African Constituency representative on the Board of the GAFTM; as well as the Chairman of the Bureau of Ministers of Health of the African Union (CAMH6).\n\nOnyebuchi Chukwu was first appointed Minister of Health of the Federal Republic of Nigeria by Goodluck Ebele Jonathan, President of the Federal Republic of Nigeria, in April 2010 and re-appointed in June 2011. As Minister of Health of Nigeria, he has championed the Transformation Agenda of the President in the health sector with considerable success. He ensured the approval of the National Strategic Health Development Plan (NSHDP) by the Federal Executive Council and the signing of the International Health Partnership (IHP+) compact securing partners’ commitment to the implementation of the plan.\n\nDuring his term, Guinea Worm Disease was eradicated in Nigeria for which the WHO in December 2013 certificated Nigeria as a Guinea Worm Free country.\n"}
{"id": "1417614", "url": "https://en.wikipedia.org/wiki?curid=1417614", "title": "Ovarian hyperstimulation syndrome", "text": "Ovarian hyperstimulation syndrome\n\nOvarian hyperstimulation syndrome (OHSS) is a medical condition that can occur in some women who take fertility medication to stimulate egg growth, and in other women in very rare cases. Most cases are mild, but rarely the condition is severe and can lead to serious illness or death.\n\nOHSS is divided into the categories mild, moderate, severe, and critical.\nIn mild forms of OHSS the ovaries are enlarged (5–12 cm) and there may be additional accumulation of ascites with mild abdominal distension, abdominal pain, nausea, and diarrhea. In severe forms of OHSS there may be hemoconcentration, thrombosis, distension, oliguria (decreased urine production), pleural effusion, and respiratory distress. Early OHSS develops before pregnancy testing and late OHSS is seen in early pregnancy.\n\nCriteria for severe OHSS include enlarged ovary, ascites, hematocrit > 45%, WBC > 15,000, oliguria, creatinine 1.0-1.5 mg/dl, creatinine clearance > 50 ml/min, liver dysfunction, and anasarca. Critical OHSS includes enlarged ovary, tense ascites with hydrothorax and pericardial effusion, hematocrit > 55%, WBC > 25,000, oligoanuria, creatinine > 1.6 mg/dl, creatinine clearance < 50 ml/min, renal failure, thromboembolic phenomena, and ARDS.\n\nOvarian hyperstimulation syndrome is particularly associated with injection of a hormone called human chorionic gonadotropin (hCG) which is used for inducing final oocyte maturation and/or triggering oocyte release. The risk is further increased by multiple doses of hCG after ovulation and if the procedure results in pregnancy.\n\nUsing a GnRH agonist instead of hCG for inducing final oocyte maturation and/or release results in an elimination of the risk of ovarian hyperstimulation syndrome, but a slight decrease of the delivery rate of approximately 6%.\n\nSymptoms are set into 3 categories: mild, moderate, and severe. Mild symptoms include abdominal bloating and feeling of fullness, nausea, diarrhea, and slight weight gain. Moderate symptoms include excessive weight gain (weight gain of greater than 2 pounds per day), increased abdominal girth, vomiting, diarrhea, darker urine, decreased urine output, excessive thirst, and skin and/or hair feeling dry (in addition to mild symptoms). Severe symptoms are fullness/bloating above the waist, shortness of breath, pleural effusion, urination significantly darker or has ceased, calf and chest pains, marked abdominal bloating or distention, and lower abdominal pains (in addition to mild and moderate symptoms).\n\nOHSS may be complicated by ovarian torsion, ovarian rupture, thrombophlebitis and renal insufficiency. Symptoms generally resolve in 1 to 2 weeks, but will be more severe and persist longer if pregnancy occurs. This is due to human chorionic gonadotropin (hCG) from the pregnancy acting on the corpus luteum in the ovaries in sustaining the pregnancy before the placenta has fully developed. Typically, even in severe OHSS with a developing pregnancy, the duration does not exceed the first trimester.\n\nOHSS has been characterized by the presence of multiple luteinized cysts within the ovaries leading to ovarian enlargement and secondary complications, but that definition includes almost all women undergoing ovarian stimulation. The central feature of clinically significant OHSS is the development of vascular hyperpermeability and the resulting shift of fluids into the third space.\n\nAs hCG causes the ovary to undergo extensive luteinization, large amounts of estrogens, progesterone, and local cytokines are released. It is almost certain that vascular endothelial growth factor (VEGF) is a key substance that induces vascular hyperpermeability, making local capillaries \"leaky\", leading to a shift of fluids from the intravascular system to the abdominal and pleural cavity. Supraphysiologic production of VEGF from many follicles under the prolonged effect of hCG appears to be the specific key process underlying OHSS. Thus, while the woman accumulates fluid in the third space, primarily in the form of ascites, she actually becomes hypovolemic and is at risk for respiratory, circulatory (such as arterial thromboembolism since blood is now thicker), and renal problems. Women who are pregnant sustain the ovarian luteinization process through the production of hCG.\n\nAvoiding OHSS typically requires interrupting the pathological sequence, such as avoiding the use of hCG. One alternative is to use a GnRH agonist instead of hCG. While this has been repeatedly shown to \"virtually eliminate\" OHSS risk, there is some controversy regarding the effect on pregnancy rates if a fresh non-donor embryo transfer is attempted, almost certainly due to a luteal phase defect. There is no dispute that the GnRH agonist trigger is effective for oocyte donors and for embryo banking (cryopreservation) cycles.\n\nSporadic OHSS is very rare, and may have a genetic component. Clomifene citrate therapy can occasionally lead to OHSS, but the vast majority of cases develop after use of gonadotropin therapy (with administration of FSH), such as Pergonal, and administration of hCG to induce final oocyte maturation and/or trigger oocyte release, often in conjunction with IVF. The frequency varies and depends on a woman's risk factors, management, and methods of surveillance. About 5% of treated women may encounter moderate to severe OHSS. Risk factors include young age, the development of many ovarian follicles under stimulation, extreme elevated serum estradiol concentrations, the use of hCG for final oocyte maturation and/or release, the continued use of hCG for luteal support, and the occurrence of a pregnancy (resulting in hCG production).\n\nMortality is low, but several fatal cases have been reported.\n\nPhysicians can reduce the risk of OHSS by monitoring of FSH therapy to use this medication judiciously, and by withholding hCG medication.\n\nCabergoline confers a significant reduction in the risk of OHSS in high risk women according to a Cochrane review of randomized studies, but the included trials did not report the live birth rates or multiple pregnancy rates. Cabergoline, as well as other dopamine agonists, might reduce the severity of OHSS by interfering with the VEGF system. A systematic review and meta-analysis concluded that prophylactic treatment with cabergoline reduces the incidence, but not the severity of OHSS, without compromising pregnancy outcomes.\n\nThe risk of OHSS is smaller when using GnRH antagonist protocol instead of GnRH agonist protocol for suppression of ovulation during ovarian hyperstimulation. The underlying mechanism is that, with the GnRH antagonist protocol, initial follicular recruitment and selection is undertaken by endogenous endocrine factors prior to starting the exogenous hyperstimulation, resulting in a smaller number of growing follicles when compared with the standard long GnRH agonist protocol.\n\nA Cochrane review found administration of hydroxyethyl starch decreases the incidence of severe OHSS. There was insufficient evidence to support routine cryopreservation and insufficient evidence for the relative merits of intravenous albumin versus cryopreservation. Also, \"coasting\", which is ovarian hyperstimulation without induction of final maturation, does not significantly decrease the risk of OHSS.\n\nTreatment of OHSS depends on the severity of the hyperstimulation.\nMild OHSS can be treated conservatively with monitoring of abdominal girth, weight, and discomfort on an outpatient basis until either conception or menstruation occurs. Conception can cause mild OHSS to worsen in severity.\n\nModerate OHSS is treated with bed rest, fluids, and close monitoring of labs such as electrolytes and blood counts. Ultrasound may be used to monitor the size of ovarian follicles. Depending on the situation, a physician may closely monitor a women's fluid intake and output on an outpatient basis, looking for increased discrepancy in fluid balance (over 1 liter discrepancy is cause for concern). Resolution of the syndrome is measured by decreasing size of the follicular cysts on 2 consecutive ultrasounds.\n\nAspiration of accumulated fluid (ascites) from the abdominal/pleural cavity may be necessary, as well as opioids for the pain. If the OHSS develops within an IVF protocol, it can be prudent to postpone transfer of the pre-embryos since establishment of pregnancy can lengthen the recovery time or contribute to a more severe course. Over time, if carefully monitored, the condition will naturally reverse to normal – so treatment is typically supportive, although a woman may need to be treated or hospitalized for pain, paracentesis, and/or intravenous hydration.\n\n"}
{"id": "15912430", "url": "https://en.wikipedia.org/wiki?curid=15912430", "title": "Prince Henry Hospital, Sydney", "text": "Prince Henry Hospital, Sydney\n\nThe Prince Henry Hospital site, formerly known as the Prince Henry Hospital, Sydney, is a heritage-listed former teaching hospital and infectious diseases hospital and now UNSW teaching hospital and spinal rehabilitation unit located at 1430 Anzac Parade, Little Bay, City of Randwick, New South Wales, Australia. It was designed by NSW NSW Colonial Architect and NSW Government Architect and built from 1881 by NSW Public Works Department. It is also known as Prince Henry Hospital and The Coast Hospital. The property is owned by Landcom, an agency of the Government of New South Wales. It was added to the New South Wales State Heritage Register on 2 May 2003.\n\nThe greater Sydney region has been inhabited by Aboriginal people for at least 20,000 years with dated sheltered occupation sites occurring in the Blue Mountains and its foothills. Aboriginal occupation of coastal NSW has also been dated to extend back to at least 20,000 years before present at Burrill Lake on the South Coast and 17,000 years before present at Bass Point. At the time of these periods of occupation, both sites would have been located within hinterland areas situated some distance away from the coast. In the case of Burrill Lake, the sea would have been up to 16km further east than at present and the site would have been located within an inland environment drained by rivers, creeks and streams.\n\nThere are no other Pleistocene sites recorded on the Sydney Coast. There are however two sites that have been dated to the early Holocene around 7,000 to 8,000 years ago. These are located at the current Prince of Wales Hospital site (a hearth dated to 7,800 years ago) and a rock shelter site at Curracurrang. It is likely that many coastal Aboriginal sites of a similar age within the Sydney region have been submerged and/or destroyed by sea-level changes which have occurred in eastern Australia during the last 20,000 years. In general terms, the majority of sites recorded within the Sydney Basin investigated to the present are dated to within the last 2,500 years that in most cases demonstrate Aboriginal exploitation of marine resources at the current sea levels.\n\nAvailable evidence indicates that Aboriginal occupation of the Sydney region was initially sporadic, and that population numbers were fairly low during the earliest periods. From around 5,000 years ago an increasing and continued use of many sites investigated through archaeology appears to have ensued. Evidence for the Aboriginal use and occupation of the Sydney region from this period is therefore far more \"archaeologically visible\" than for previous periods.\n\nIn the South Sydney region at least three archaeological sites have produced dates of Aboriginal occupation that range from between 3,000 and 5,000 years ago. From about 3,000 years ago to Contact the number of occupied sites appears to have increased dramatically. This may reflect a \"real\" increase in the number of sites (and hence people) in the region, or may reflect preservation factors (particularly associated with shell midden deposits) where older sites have been destroyed by thousands of years of erosion, and accelerated by development in the post-Contact period.\n\nOver the 20,000 years of Aboriginal occupation in the region, and in particular the last 5,000 to 8,000 years, numerous changes in excavated stone tool assemblages have been observed. Various temporal markers have been established by archaeologist in an attempt to distinguish what are considered to be the more significant changes in tool types and tool kit composition. The assumption being that changes in one (or more) components of the excavated material culture may reflect changes in other aspects of past Aboriginal social, economic and technological practices.\n\nThese arguments are based upon changes in stone tool assemblages and observable changes in the use of certain types of stone used in Aboriginal tool manufacture. Excavation of a number of rock-shelter occupation sites in particular indicates that the earlier phases of occupation are largely characterised by the presence of large cores and scraper tools. This appears to be followed by the addition of a variety of smaller backed implements (known variously as backed blades, geometric microliths or Bondi Points) to the toolkit previously dominated by larger tools at around 5,000 years ago. By around 1,500 years ago the smaller backed forms appear (on available evidence) to have gone out of use and excavated site assemblages are characterised by quartz bi-polar artefacts and more opportunistic or undifferentiated small tools. It is reasonable to assume that the many artefacts made by Aboriginal people from shell, bone or wood as observed at Contact were also used in the past but these materials have not survived in the archaeological record.\n\nResearch indicates that coastal sites in the Sydney region have been largely ignored by archaeologists until relatively recently. Prior to work completed over the last two decades, the majority of Aboriginal archaeological sites investigated were located south of Sydney and the Georges River. Previous focus of investigations was frequently on the stone artefacts made by Aboriginal people in the past, the sequence of changes in their form and composition, and upon comparisons between coastal and inland sites that sought to understand how people used the landscape as a means to characterise Aboriginal life on the eastern coast of NSW prior to Contact. More recent archaeological studies have focused upon the way Aboriginal people adapted to the coastal environment and the immediate hinterland, and how other aspects of the archaeological record (such as food, art, site complexity and composition, and site distribution data) can contribute to our understanding of prehistoric Aboriginal life.\n\nOne of the earliest land grants in this area was made in 1824 to Captain Francis Marsh, who received 12 acres bounded by the present Botany and High Streets, Alison and Belmore Roads. In 1839 William Newcombe acquired the land north-west of the present town hall in Avoca Street.\n\nRandwick takes its name from the town of Randwick, Gloucestershire, England. The name was suggested by Simeon Pearce (1821-86) and his brother James. Simeon was born in the English Randwick and the brothers were responsible for the early development of both Randwick and its neighbour, Coogee. Simeon had come to the colony in 1841as a 21 year old surveyor. He built his Blenheim House on the 4 acres he bought from Marsh, and called his property \"Randwick\". The brothers bought and sold land profitably in the area and elsewhere. Simeon campaigned for construction of a road from the city to Coogee (achieved in 1853) and promoted the incorporation of the suburb. Pearce sought construction of a church modelled on the church of St. John in his birthplace. In 1857 the first St Jude's stood on the site of the present post office, at the corner of the present Alison Road and Avoca Street.\n\nRandwick was slow to progress. The village was isolated from Sydney by swamps and sandhills, and although a horse-bus was operated by a man named Grice from the late 1850s, the journey was more a test of nerves than a pleasure jaunt. Wind blew sand over the track, and the bus sometimes became bogged, so that passengers had to get out and push it free. From its early days Randwick had a divided society. The wealthy lived elegantly in large houses built when Pearce promoted Randwick and Coogee as a fashionable area. But the market gardens, orchards and piggeries that continued alongside the large estates were the lot of the working class. Even on the later estates that became racing empires, many jockeys and stablehands lived in huts or even under canvas. An even poorer group were the immigrants who existed on the periphery of Randwick in a place called Irishtown, in the area now known as The Spot, around the junction of St.Paul's Street and Perouse Road. Here families lived in makeshift houses, taking on the most menial tasks in their struggle to survive.\n\nIn 1858 when the NSW Government passed the Municipalities Act, enabling formation of municipal districts empowered to collect rates and borrow money to improve their suburb, Randwick was the first suburb to apply for the status of a municipality. It was approved in February 1859, and its first Council was elected in March 1859.\n\nRandwick had been the venue for sporting events, as well as duels and illegal sports, from the early days in the colony's history. Its first racecourse, the Sandy Racecourse or Old Sand Track, had been a hazardous track over hills and gullies since 1860. When a move was made in 1863 by John Tait, to establish Randwick Racecourse, Simeon Pearce was furious, especially when he heard that Tait also intended to move into Byron Lodge. Tait's venture prospered, however and he became the first person in Australia to organise racing as a commercial sport. The racecourse made a big difference to the progress of Randwick. The horse-bus gave way to trams that linked the suburb to Sydney and civilisation. Randwick soon became a prosperous and lively place, and it still retains a busy residential, professional and commercial life.\n\nToday, some of the houses have been replaced by home units. Many European migrants have made their homes in the areaa, along with students and workers at the nearby University of NSW and the Prince of Wales Hospital.\n\nThe Prince Henry Hospital and former Coast Hospital at Little Bay represent an important phase in the provision of public health in New South Wales and Australia. Established by the Board of Health in 1881, in response to an outbreak of smallpox, the hospital was the first government-controlled public hospital in the post-convict era. The Board of Health, forerunner to the Department of Health, was created initially to deal with the smallpox outbreak of 1881. The Board of Health and New South Wales government's involvement in the early administration at the hospital empowered both organisations in their dealings with other New South Wales private hospitals in the late nineteenth and early twentieth century. It also laid the foundations for the administrative policies in regard to hospitals that became standard within the system.\n\nThe location of the Coast Hospital was a reflection of the prevailing beliefs with regard to the treatment of infectious disease and in health care generally. Fear of infectious diseases in the nineteenth century meant that those diagnosed or suspected of having infection were geographically isolated and removed from the general population. At the same time, fresh ocean air was considered highly beneficial in the treatment of disease. The Coast Hospital was built with both these ideals in mind. Not only was the original hospital well removed from the populated areas in Sydney, but within the grounds of the institution, the patients were duly separated depending on their ailment. The main section was located on the southern headland of Little Bay where maximum exposure to the elements was assured.\n\nThe isolated nature of the Coast Hospital also led to the establishment of the first complete ambulance service in New South Wales and a forerunner of permanent ambulance services throughout the entire country.\n\nThe Coast Hospital cemetery was the second burial place for the hospital, between 1897 and 1952. It was not within the grounds itself, but away to the south in an isolated position to minimise the spread of disease. The cemetery has ongoing significance for the Aboriginal community as the Dharawal Resting Place, where ancestral remains of the La Perouse Aboriginal people, returned from both Australian and international museums, can be returned to country and buried. The first reburial took place in June 2002. The cemetery is now within Botany Bay National Park and pressed by golf courses.\n\nAs the isolation of the hospital was gradually reduced, through the encroachment of Sydney's suburbs and improvement in transport facilities, the demand for the services of the hospital grew. The first years of the twentieth century reflected this change as a major building program was initiated at the hospital.\n\nThe proposal for the construction of up to 20 new wards between 1914 and 1920 reflected a growing community belief that the provision of public health was a universal right to those in the community - a view held by the then New South Wales government and the Minister for Health, Fred Flowers. The new wards built on the slope to the west, away from the original coast section, were named the Flowers Hospital after the minister.\n\nThe overall redevelopment, wards, theatres and auxiliary rooms meant that by 1929 the hospital was the largest in NSW. In 1934 the hospital was renamed the Prince Henry Hospital in honour of the recent visit by Prince Henry, Duke of Gloucester. The establishment of the hospital originally as an infectious disease hospital allowed it to develop an expertise in the diagnosis and treatment of infectious diseases that stayed with the facility throughout its operation. Its almost continual use as an infectious disease hospital since its opening provides valuable evidence of the community's attitude, and an official attitude, to the treatment of infectious diseases.\n\nNew techniques for the diagnosis and treatment of infection and research into disease were a specialist function of the hospital as a unit. The expertise of the staff who were stationed at the hospital in diagnosis and treatment made them highly valued at other institutions around the country and gained the hospital a worldwide reputation. The training of nurses at the hospital had been standard practice since 1894, while from 1937 all nurses were required to spend two months training in the Nurses Preliminary Training School before entering the wards.\n\nDuring the same period (1936) the hospital was chosen by the New South Wales Postgraduate Committee as an official postgraduate teaching hospital. A postgraduate medical school was opened in 1938, although it only operated until 1943, and was finally abandoned due to wartime restrictions on staff and services. In 1960, the hospital became the first teaching hospital for the newly created University of NSW, continuing a proud history of educating medical staff.\n\nThe hospital's expertise and specialisation extended beyond the infectious disease wards and the training facilities. Specialist services were offered to treat soldiers during the early years of the Second World War, including the first of the American troops to land in Sydney (before the American military established their own hospitals). In 1946 a special police ward was created to treat those members of the police force who needed treatment; while on the other side of the law, one ward in the Delaney House was converted and secured to treat prisoners from Long Bay Gaol. (The link between the hospital and Long Bay Gaol also included the excellent bread baked and delivered daily to Prince Henry Hospital).\n\nPart of the hospital's reputation has come from its association with prominent medical professionals and administrators who have worked there over the years. Some have been remembered in the naming of buildings on site after them, including Matron E McNevin, Matron CM Dickson, FW Marks, Bob Heffron, and JE Delaney. Both Matron McNevin and Matron Dickson were honoured through the naming of the two main nurses' homes after them.\n\nMatron Clarice Dickson had served at the Coast Hospital since 1909 when she joined the nursing staff. She went to France during World War I to serve for the Red Cross and was awarded a medal for courageous dedication to duty under fire. She returned to the Coast Hospital in 1920 as Sub-Matron, but transferred to Newington State Hospital in 1926 for six months before returning to the Coast Hospital as Sub-Matron in 1927. She became Matron of Prince Henry Hospital in 1936 and retired in 1937. The new nurses' home was named after her on her retirement.\n\nMatron Dickson was followed by Ethel McNevin as Matron of Prince Henry Hospital. Matron McNevin had arrived at the Coast Hospital in 1915 as a trainee, and served at the hospital until 1926 when she resigned to become Matron of Coonamble District Hospital. In 1928 she was appointed Matron of the Perth Hospital in Western Australia. She returned to Prince Henry Hospital in 1937 as Matron, a position in which she served until her retirement in September 1955. During her time as Matron, McNevin introduced the Nurses Preliminary Training School in which new nurses would spend two months learning the basics of the profession before transferring to the wards. The school became an integral part of the nursing experience at Prince Henry Hospital. Following her retirement, Matron McNevin returned to Prince Henry Hospital as the librarian in the Medical Library and lived in a small flat in the Matron Dickson Nurses Home. She died at the hospital in July 1960.\n\nBoth Heffron and Marks had served on the Board of Directors for the hospital as directors. Bob Heffron, MLA, was appointed to the Board in 1942 and was Chairman of the Board between September 1950 and November 1959. He served as the local member for Botany from 1930 and as NSW Premier between 1959 and 1964. The new Ward Block A was named after him in 1961 in recognition of his seventeen years of service on the Hospital Board. FW Marks was Chairman of the Board between 1936 until his death in 1942. The contribution of the Marks family was recognised by the naming of the new infectious diseases ward the FW Marks Pavilion. Other members of staff left lasting impressions on the hospital through their devotion to the patients and staff while they were in residence.\n\nDr CJM \"Cec\" Walters, who served as the Medical Superintendent of Prince Henry Hospital from October 1936 until December 1959, is fondly remembered by many ex-staff for his loyalty to the hospital and devotion to duty. Dr Walters started his career as a veterinary surgeon in 1913, before enlisting in 1914 and serving in mobile veterinary hospitals in France, including in command positions. On his return to Australia he was appointed in charge of the Veterinary Clinic at Sydney University. In 1923 he graduated from the School of Medicine and came to work at the Coast Hospital in 1924 where he remained, except for a brief period as a Macquarie Street specialist, until 1959. Throughout this time he continued to practice as a veterinary surgeon, working from time to time on the thoroughbreds in Vic Field's stable at Randwick Racecourse.\n\nJohn E. Delaney became Chief Executive Officer in 1973, succeeding J. R. Clancy, a brother of the former Catholic Archbishop of Sydney, Cardinal Clancy. Delaney is remembered as a fine administrator who fought for a dual carriageway along Anzac Parade to the Prince Henry site.\n\nThe work of Dr Neville Stanley is remembered in association with the Pathology Building, which was named after benefactors Hugh and Catherine McIlrath. His virus research team achieved national prominence in Australia, in relation to research into viral meningitis and the polio virus.\n\nThe closure of the Prince Henry Hospital was announced in September 1988. The facilities were to be slowly relocated to the Prince of Wales Hospital, forming a \"super hospital\" on the proceeds of the sale of Prince Henry Hospital. However, prior to the official announcement, since the early 1970s, the services offered by Prince Henry had been slowly withdrawn. From 1984 the future of the hospital was being continually reassessed. A lack of funds for capital works and the uncertainty over its future resulted in many of the buildings becoming run-down across the site. Following the official announcement of its closure, services continued to be downgraded, wards closed and staff relocated. There are few working precincts remaining on the Prince Henry site as of May 2002.\n\nThe Prince Henry Hospital grew to provide a range of medical services on the site. The hospital became a major teaching hospital, operating at its peak in the mid 1980s. Since that time, as a result of state government policy, which focused on the consolidation of health services at other hospitals, the Prince Henry Hospital has been progressively closed. In 1999 the Minister for Health announced the transfer of the remaining hospital services to the Prince of Wales Hospital at Randwick. The redevelopment of the Prince Henry site was also announced, to provide private housing, aged care housing and selected medical services. This redevelopment was to include the restoration of heritage buildings on the site.\n\nA two-staged approach to the preparation of a Conservation Management Plan was agreed with the then NSW Heritage Office and Randwick City Council to guide the Prince Henry site Masterplan. A Stage 2 Conservation Management Plan, dated May 2002 (amended February 2003) including Archaeological Management Plan, dated August 2002 was endorsed by the Heritage Council on 27 June 2003.\n\nThe Prince Henry Masterplan was approved by the Heritage Council Approvals Committee in December 2001. In September 2002, the Heritage Council recommended to the Minister that the site be listed on the State Heritage Register. The site was listed on 2 May 2003.\n\nThe revised Masterplan was approved by the Heritage Council in May 2003. Heritage Council general terms of approval were issued for a Stage 1 infrastructure IDA in 2003. Heritage Council general terms of approval were issued in March 2004 for DA7 for amalgamation of parts of Anzac Parade with the site and community title subdivision of site to create 27 allotments. An integrated development application 1103/2003, DA7 for the land management regime for Prince Henry Hospital site was established as a \"Community Scheme\" for the entire site. Site Masterplan amendments were approved on 6 October 2005.\n\nThe Prince Henry site contains a variety of buildings in an open landscape setting, as well as archaeological features and artefacts that provide evidence of its continuous use as a hospital for over 120 years.\n\nNatural landscape elements such as the Little Bay Geological Site, areas of sandstone outcropping and indigenous vegetation have been overlaid by numerous cultural landscape elements such as cultural plantings (several species of Phoenix palms, banksias and Norfolk Island pines) and retaining walls and rock cuttings. There are significant views from the site towards Little Bay and the coastal headlands as well as major visual axes along Pine Avenue and between the Flowers Wards.\n\nThe existing buildings and structures, relate to the four key phases of development at the Prince Henry site and include elements that represent each of the major building types. These include hospital wards and operating theatres, specialist and research facilities, administration buildings, nurses and doctors' quarters, maintenance and services as well as laundry, kitchen and education facilities.\n\nBelow is a list of built and landscape elements of particular significance at the Prince Henry Site, Little Bay (see also attached plans):\n\nThis includes a collection of significant built and landscape elements relating to the development of the Coast Hospital and Prince Henry Hospital and their settings. Centred on Pine Avenue, it includes historic roads, cultural plantings, rock cuttings and outcrops, kerbs, retaining walls, spatial relationships between buildings and groups, and views within and beyond the precinct;\n\nLittle Bay Beach, adjoining Headlands of Little Bay and Coastline, including coastal views and scenery;\n\nFormer Male Lazaret site, including sandstone wall, sandstone drain/culvert and ornamental palm;\n\nA number of movable items relating to the cultural history of the Hospital, particularly the history of medical treatment, technological development and nursing care on the site from 1881, have been collected within the PHHTNA Museum (Prince Henry Hospital Nursing and Medical Museum) and have been identified in the Museum Plan.\nOther movable items, ranging from medical equipment to garage doors are located throughout the Prince Henry site. They are identified in the Conservation Management Plan.\n\nBurials: 1897-1952; Dharawal Resting Place: 2002-present\nNestled in the cliffs of Little Bay, the cemetery is poignant reminder of the devastating effects of epidemics in Sydney. The Coast Hospital was established in 1881 during the small pox epidemic. This cemetery was the second burial place for the hospital, between 1897 and 1952. It was not within the hospital grounds itself, but away to the south in an isolated position to minimise the spread of disease. There is still a visual link between cemetery and hospital across the cliffs. Scattered monuments remain amongst mown grass. Some graves are marked by kerbing, one by a small timber picket fence. The majority of graves are unmarked: it is estimated there are over 2000 burials here. Only 78 graves are still visible. A row of grave markers to nurses and staff is on the right as you enter. There are two simple Gothic arch headstones to Chinamen, Ton Dong (d.1902) and Ah Wong (d.1902), both of the plague. The most unusual are two matching grave markers to the Rouse family, Enid Pearl (d.1907) and her mother Alice (d.1917). A semicircular barrel-top sarcophagus covered in tiles defines each grave plot: a sandstone headstone the inscription. Each is enclosed in matching cast-iron fences. Executed by monumental mason James Cunningham, Sydney, this style of funerary monument is rarely seen in NSW. The cemetery has ongoing significance for the Aboriginal community as the Dharawal Resting Place, where ancestral remains of the La Perouse Aboriginal people, returned from both Australian and international museums, can be returned to country and buried. The first reburial took place in June 2002. The cemetery is now within Botany Bay National Park and pressed by golf courses.\n\nAs at 14 July 2003, Evidence of Aboriginal occupation prior to the establishment of the Coast Hospital in 1881 includes a diverse collection of prehistoric Aboriginal sites, such as open and sheltered middens, open campsites, rock engravings, axe-grinding grooves and pathways, a possible fish trap and ochre source. The area also retains the potential to contain previously unidentified Aboriginal artefacts and significant sites (see attached plans).\n\nIdentified Aboriginal Archaeological sites located within the existing boundaries of the Prince Henry site are:\n\n\nThe following types of Aboriginal archaeological sites may potentially remain undetected within the Prince Henry site:\n\n\nThe Prince Henry site has known archaeological potential as the first post-convict era hospital in NSW.\n\nPrince Henry also contains archaeological evidence of former activities associated with the use of the site for hospital services over the last 120 years. This archaeological evidence is primarily associated with the original Coast Hospital, located on the south headland of Little Bay and the Male Lazaret to the north of Little Bay. It also provides some evidence of the later Prince Henry Hospital, which developed nearer to Anzac Parade to the north and south of Pine Avenue (see attached plans).\n\nIdentified Historical Archaeological Items located within the existing boundaries of the Prince Henry site are:\n\nA. Rock-Cut Steps\nB. Retaining Wall\nC. Canalised Water Courses (Canals)\nD. Rock Shelf, Rock Cutting and Graffiti\nE. Canalised Watercourse\nF. Resident Medical Officers Quarters Site\nG. North Rock Anchor Site\nH. Footings/Kerbing\nI. Rock Cutting 'South Drain'\nJ. Remnant Garden Beds\nK. Cemetery Road\nL. Sandstone Platform\nM. A small number of Movable Items (in addition to those identified in the Conservation management Plan), include cut sandstone blocks, the 1937 Entrance Gates (also identified as a movable item in the CMP) and concrete plinths.\n\nOther items are located within Historical Archaeological Zones as identified in the attached plans including retaining walls, sandstone drains, sandstone kerbing, remnant timber split rail fencing, defence related items and rock-cut features. Although features associated with the two cemeteries including the former Cemetery Road, gravestones, timber post-and-rail fencing and sandstone blockwork are beyond the study area, they are also associated with the Prince Henry site. Historical archaeological evidence, including sandstone drains and road alignments, of the former Working Patients Dormitories, also continues to exist to the south of the Prince Henry site.\n\nThe Prince Henry site, Little Bay has undergone numerous and continual modification and change since its inception. Four key phases of development reflect the site's transition from the Coast Hospital, built for the isolation and treatment of infectious diseases, to the Prince Henry Hospital, which later became a major general and teaching hospital.\n\nThis phase represents the first use of the site in 1881, for temporary accommodation as a response to an outbreak of smallpox. One hundred and seventy-five hectares of land at Little Bay were reserved for quarantine purposes, ultimately leading to the establishment of an isolation hospital and sanatorium. The layout and design of the Coast Hospital reflected hospital design practices of the time including separate locations for infectious patients and the sanatorium. Apart from archaeological evidence of the Coast Hospital and the Male Lazaret, only a small number of features survive from this period, including the Dam, the former Coast Hospital Steam Laundry, Pine Cottage, Pine Avenue (historic road alignment, sandstone kerbing, retaining wall and pine trees), the Artisans' Cottages and buildings from the Infectious Diseases Division (Ward 16, the Kitchen and Boiler House) which have been incorporated into the Institute of Tropical Medicine. A number of other historic road alignments remain including, the loop roads to the Infectious Diseases Division and Nurses (14) Quarters (Sewing Cottage) and to the Matron Dickson Nurses Home and the Bush Wards, the road associated with the Artisans' Cottages, the Coast Hospital Road and the Cemetery Road as does the Second (North) Cemetery.\n\nThis phase represents a period of growth that began after the then Board of Health unveiled plans for considerable expansion of the Coast Hospital, which was to include the construction of up to 20 new wards, each of which would contain 50 beds. This area was to be known as the \"Flowers Hospital\". Although only six of the wards were completed, due to a change of government in 1917, they contributed to the hospital becoming the largest in New South Wales by 1929. Apart from the archaeological evidence of the Working Patient's Dormitories, the original fabric of the former Nurses Dining Hall/Lecture Hall and the Bush Wards, the six Flowers Wards buildings, within an open setting bounded by historic road alignments, are the most significant elements that survive from this period.\n\nThis phase represents the shift of facilities from the former Coast Hospital area to the Flowers Wards area (known as 'The Hill'). It began after November 1934 when it was announced that the Coast Hospital was to be renamed the \"Prince Henry Hospital\" in honour of Prince Henry, the Duke of Gloucester (who had recently visited Sydney, but not the hospital). Plans were also announced for a large works program to increase the capacity of the hospital to 1,000 beds. Along with the passing of the Prince Henry Hospital Act 1936, which attempted to establish Prince Henry as a postgraduate teaching hospital, the emphasis of this period, apart from increasing capacity was on rectifying the inefficient layout of facilities on the site. Many of the distinctive brick buildings, including Heffron House, the Delaney Building, Matron Dickson Nurses Home and the McIlrath Pathology Building survive from this period.\n\nThis phase represents the establishment and consolidation of the role of Prince Henry Hospital as a general and major teaching hospital. It followed legislation passed in 1959 to reform Prince Henry Hospital as a postgraduate hospital associated with the University of New South Wales (UNSW) and the University of Sydney. This formalised Prince Henry Hospital's role as a teaching hospital with closer connections to medical research undertaken by both universities. The immediate effect was that a seventeen-hectare portion of land to the north was transferred to the University of NSW as well as two of the 1917 military wards for animal research. From 1960, extensive renovations throughout Prince Henry Hospital were undertaken to accommodate its new association with the UNSW Medical School. In 1964 major new works were undertaken. Major buildings from this phase such as the Rehabilitation Medicine Centre, The Psychiatric Block, Operating Theatres and the Diagnostic Radiology Building all survive as does the Interdenominational Australian Nurses War Memorial Chapel.\n\nOccasionally a site presents such complex challenges - politically as well as architecturally - that only the most thoughtful and nuanced solution is acceptable. The old Prince Henry Hospital is such a site - and Landcom's masterplan is such a solution. From the time of its mooted closure, Prince Henry became the subject of intense and justifiable community concern. Thanks to Landcom's unique public benefit mandate, a plan has been developed which delivers innovative solutions to achieving density while maintaining amenity, beauty and social cohesion on a site both large and significant, yet also highly constrained. The masterplan crafts a new residential and community precinct that seamlessly balances old and new, open space and built form, private and public uses, creating a rare showcase of sustainable coastal urban renewal. The cultural and community benefits are immense: 80 per cent of the site retained in public hands; improved access to Little Bay Beach; facilities for seven community groups, as well as a 1500m2 community centre and a new Rescue Helicopter Service facility off site. Heritage issues have also been well addressed, with the site listed on the State Heritage Register, 19 heritage buildings and landscape items conserved, and the historic Flowers Ward restored and adapted as a Nursing Museum to honour the site's long history of healing and care. The masterplan also provides major environmental benefits including: comprehensive site remediation; 9.2 hectares of parks and protected bushland; minimising urban runoff; repairing creek lines and riparian zones, and reusing rainwater for irrigation. Buildings will be required to have 4.5 NatHERS ratings and around 90 per cent of demolition material has been recycled. In short, Prince Henry delivers an impressive social and environmental dividend with few precedents in the field of urban renewal in NSW. It is particularly gratifying that all of this was achieved by a government agency, Landcom, leading a large team of urban designers, architects, planners and landscape architects in delivering a plan that has community benefit and public amenity at its heart. I therefore take this occasion to affirm the enduring relevance of public authorities such as Landcom and the Government Architect's Office in bringing \"urban decency\" to our cities, towns and suburbs, a role as significant today as when Francis Greenway made his first sketches 200 years ago. In that spirit, I take great pleasure in conferring the 2008 Premier's Award on Landcom for the Prince Henry masterplan, a project of vision and integrity that will bring lasting credit to everyone associated with it.\nMorris Iemma MP, Premier of New South Wales\n\nAs at 14 July 2003, The Prince Henry site was the most important site for the treatment of infectious diseases in New South Wales from its inception in the 1880s, when, as the Coast Hospital, it became the first public hospital in New South Wales in the post-convict era. The Hospital played a prominent role in treating and overcoming infectious diseases and later as a general hospital and teaching hospital for the University of NSW, until its closure was announced in 1988. Its isolation led to the establishment of the first ambulance service in New South Wales from within its grounds.\n\nThe location of the Hospital by the sea, the design and siting of buildings in a spacious open setting, their relationship with each other and the layout of the site itself, created an aesthetically distinctive complex with Pine Avenue as its central axis. The buildings and landscape provide evidence of the prevailing attitude to health care during a number of important phases of development. The Flowers Wards and the remains of the early infectious disease hospital, including Ward 16, the former Nurses (14) Quarters, the former Nurses Dining Hall/Nurses Lecture Hall, the Bush Wards and the site of the Male Lazaret, demonstrate the isolation required for the treatment of infectious diseases and early attitudes to public health, which saw health benefits in being by the sea. The architectural character of these early buildings contrasts with later buildings built after 1934, after the Hospital changed its name to Prince Henry and a new phase of expansion began. The larger scaled Heffron and Delaney Medical Ward Buildings, the Matron Dickson Nurses Home, and the McIlrath Pathology Building provide evidence of changing practices in medical care and staff accommodation, as well as contributing visually to the ambience of the place. A range of ancillary buildings, such as the former Water Reservoir, the Memorial Clock Tower, Water Tower, and 'Hill Theatres'add visual as well as technological interest.\n\nA coastal landscape of high scenic and scientific value is enhanced by the beach, headlands and pockets of indigenous vegetation. A geological exposure area has research and educational value relating to the development of the present coastline and to the climate and vegetation of the area twenty million years ago. A number of cultural landscape features including the Norfolk Island Pine trees along Pine Avenue, plantings of palms, New Zealand Christmas trees and banksias, rock cuttings, retaining walls, early road alignments and sandstone kerbs, provide evidence of human intervention in this coastal landscape. The North Cemetery, although separated from the present hospital site, is an important component of the cultural landscape.\n\nThe history of the Prince Henry site is interwoven with Aboriginal people and wider communities, many of whom were patients or worked on the site and still visit it. The site is valued by Aboriginal people for its historical associations and Aboriginal occupation prior to European occupation, as well as its associations with Aboriginal people treated for infectious diseases.\nThe Prince Henry site is also important to many of the thousands of nurses, doctors and administrators who value their training and achievements at the hospital, which gained them a high reputation throughout New South Wales and Australia. Many former nurses have remained actively associated with the site, and have created a museum to conserve its history and artefacts. They come to the site to enjoy its ambience and continue to use the Interdenominational Australian Nurses War Memorial Chapel, built in memory of service nurses, many of whom died at sea.\nMuch more about the history of the Prince Henry site is yet to be learnt from the rich array of known and potential Aboriginal and historical archaeological sites, from further research and archival recording, and from the oral histories of those who worked or trained there.\n\nThe Prince Henry site contains both identified archaeological features and areas of known archaeological potential. These elements are part of the total physical record of the first post-convict era hospital in New South Wales. The physical evidence at the site documents, and therefore provides opportunities to investigate, evolving medical practice associated with the treatment of infectious disease. In a wider context the site reflects changes and development in state health policy for more than 100 years. The research value of the site's historical archaeological resource is only moderate, however, because of the physical impact of ongoing development. Although the extant archaeological resource is therefore not intact, and there are extensive documentary sources available, the place has potential to yield information about site use and occupation. The spectrum of archaeological features across the site also provides a rare opportunity to use archaeology as an investigative tool on a wide scale. The historical archaeological resource at the Prince Henry site also contributes to the total ensemble providing an indication of former activities or features. They are therefore part of the site's wider social and historic value and have educational and interpretive potential.\n\nPrince Henry Site was listed on the New South Wales State Heritage Register on 2 May 2003 having satisfied the following criteria.\n\nThe place is important in demonstrating the course, or pattern, of cultural or natural history in New South Wales.\n\nThe Prince Henry site was in almost continual use for over 120 years as an infectious disease and general hospital.\nThe distance of the hospital from Sydney reflected contemporary community fear of virulent disease, such as smallpox, cholera, influenza, leprosy and plague.\nThe Prince Henry site contains physical evidence of major public works associated with State health policy.\nPrince Henry Hospital played a major role as a teaching hospital from the 1960s and as a centre of excellence for a number of medical procedures and technologies.\nThe site contains geological deposits attesting to physiographic, climatic and botanical conditions at a very early phase of the development of current coastal geography of eastern Australia.\nPrince Henry Hospital was the first public hospital in the post-convict era.\nThe Prince Henry site was important in the Colonial Government's response to public health concerns, in particular in regard to infectious disease;\nIn 1960, Prince Henry Hospital was proclaimed the first teaching hospital of the newly formed University of NSW (UNSW).\nPrince Henry Hospital operated as a public hospital from its inception in the 1880s until its progressive closure in the 1990s.\n\nThe distance of the Hospital from Sydney reflected contemporary community fear of virulent disease, such as smallpox, cholera, influenza, leprosy and plague.\n\nThe Prince Henry site contains physical evidence of major public works associated with State health policy.\n\nPrince Henry Hospital played a major role as a teaching hospital from the 1960s and as a centre of excellence for a number of medical procedures and technologies.\n\nPrince Henry Hospital was the first public hospital in the post-convict era.\nThe Prince Henry site was important in the colonial Government's response to public health concerns, in particular in regard to infectious disease.\n\nIn 1960, Prince Henry Hospital was proclaimed the first teaching hospital of the new University of NSW (UNSW).\n\nPrince Henry Hospital operated as a public hospital since its inception in the 1880s until its closure in 1988.\n\nThe place has a strong or special association with a person, or group of persons, of importance of cultural or natural history of New South Wales's history.\n\nThe Prince Henry site is associated with the establishment of the first permanent ambulance service in NSW, operating from within the hospital from the 1880s.\nThe Prince Henry site is associated with a number of prominent medical personnel, including Dr J Ashburton-Thompson, Dr CJM Waters and Dr N Stanley.\nThe Prince Henry site has strong associations with the training of medical and nursing staff, many of whom remained in the hospital at Little Bay following the completion of their training. Some, such as Matron Dickson and Matron McNevin, were remembered through the naming of nurses' accommodation after them.\nThe Prince Henry site was encouraged to develop in the early twentieth century by the then Minister for Health, Mr Fred Flowers.\nThe Prince Henry site has associations with a number of prominent administrators, public officials and benefactors, such as RJ Heffron and JE Delaney, FW Marks and H and C McIlrath, which is reflected in the naming of buildings after them.\nThe Prince Henry site has a minor association with internationally renowned Australian test cricketer Charlie McCartney who is associated with the Prince Henry site, having acted as hospital amenities officer from 1948, establishing a cricket oval and tutoring staff in various sports.\n\nThe place is important in demonstrating aesthetic characteristics and/or a high degree of creative or technical achievement in New South Wales.\n\nThe Prince Henry site contains a rich assembly of medical, nursing and administrative buildings that reflect changing attitudes to medical care and public health administration over a period of over 120 years.\nThe Prince Henry site, its location and its open spatial setting by the sea demonstrates the isolation required for the early treatment of infectious diseases in New South Wales and the health benefits that the seaside setting was thought to offer.\nThe Prince Henry site is a visually distinctive cultural landscape of buildings, open space and seascape.\nA number of buildings, including the Flowers Wards, Matron Dickson Nurses Home, Heffron House and the Delaney Building are individually aesthetically distinctive, and contribute to the aesthetic values of the site as a whole.\nNatural and man-made features, including Pine Avenue, unfolding vistas across the landscape to the headland the sea, the coastal landscape, including Little Bay, contribute to the high visual values and landmark qualities of the Prince Henry site.\nA number of built elements and landscape features, such as the pine trees along Pine Avenue, the Memorial Clock Tower, the Water Tower, the War Memorial Chapel, the Flowers Wards, the Heffron and Delaney Buildings, rock cuttings, rock outcrops and regenerated bushland are landmark features in their own right and heighten the landmark qualities of the site.\nThe design of early buildings, their configuration and relationship to each other, historic roads and the layout of the site itself creates an aesthetically distinctive complex that provides built evidence of a number of important phases of the site's development as a major public and teaching hospital.\nThe architectural character of the buildings, which are associated with particular types of medical and nursing activities, reflect the changing tastes and technologies towards these practices. This is evident in the contrasts between early wards such as Ward 16, the Flowers Wards and later wards such as the Heffron and Delaney Buildings, the old and new Pathology Department buildings and differences between the various nursing and residential accommodation on the site, from the early timber cottages to the Matron Dickson and Matron McNevin Nurses Homes.\n\nThe place has a strong or special association with a particular community or cultural group in New South Wales for social, cultural or spiritual reasons.\n\nThe Prince Henry site is important to Aboriginal people as a place with which they have spiritual connections and where physical links to the land can be demonstrated. Aboriginal people from all over New South Wales were patients at the hospital and worked there, many for long periods.\nThe Prince Henry site is of profound importance to former nurses and nursing administrators. Thousands of nurses trained on the site. They gained skills that were highly valued and many had distinguished careers, not only at Prince Henry, but in hospitals throughout Australia and overseas.\nThe Prince Henry site is valued by former medical administrators for its prominence in the treatment of infectious diseases in New South Wales, its role as a teaching hospital, a major hospital and for the work of many prominent medical administrators and specialists in advancing medical procedures and technologies.\nThe Prince Henry site is interwoven with the local community, including the La Perouse Aboriginal community. Local people have strong associations with the place as staff, patients and visitors.\nThe Prince Henry site is important to former nurses and nursing as a place where they return to meet and reflect, visit its chapel and museum, and enjoy its ambience.\nThe Prince Henry site is important to many senior members of the community who support the centre at the site in order to keep the Prince Henry site alive as well as enhancing their lifestyle at the former hospital as it is today.\nThe Prince Henry site is valued by former nurses, staff and the community as a place where many historic themes and phases in the development of the hospital can be appreciated.\nThe Prince Henry site is valued by community and cultural groups that feel regret about the loss of the hospital and are concerned that its spatial qualities and ambience should not be lost for themselves or future generations.\nit is a place that is held in high esteem by a number of identifiable groups for its cultural values;\nif Prince Henry was damaged or destroyed, it would cause the community and cultural groups a sense of loss; and\nit contributes to the sense of identity of the community and a number of cultural groups.\n\nThe place has potential to yield information that will contribute to an understanding of the cultural or natural history of New South Wales.\n\nThe geological gully-fill deposits and their relationships, especially within the Critical Exposure Area, have the potential to provide further detailed knowledge of near-coastal climate and vegetation of the middle Miocene Period. The unique exposures need to be conserved for further study, in order to test and refine developing geological concepts concerning continental and indeed global earth processes of the period.\nThe site, through further analysis of documentary and physical evidence, including oral sources and archaeological investigation, has high potential to yield further substantial information about Aboriginal occupation from the prehistoric period to the present day.\nThe site has potential to contribute to research into the development of an important sector of the health community during the nineteenth century, particularly medical practices associated with the isolation and treatment of infectious diseases.\nThe site's association with Aboriginal prehistory, the interaction of Aboriginal people with public health practices, its role in the treatment of infectious disease and as a major teaching and public hospital establishes it as a benchmark or reference type that distinguishes it from hospitals elsewhere.\n\nThe place possesses uncommon, rare or endangered aspects of the cultural or natural history of New South Wales.\n\nPrince Henry Hospital's role in the treatment of infectious diseases is unparalleled in New South Wales. While a detailed comparative study has not been undertaken, it is known from available information that no other hospital in NSW was set up solely for the purpose of treating infectious diseases. Where such treatment occurred at other hospitals or institutions, including the Quarantine Station, the facilities were not as extensive or comprehensive as at the Prince Henry site. It provides rare evidence of medical and nursing practices that are now defunct, and its isolation led to the formation of the NSW Ambulance service operating from within its grounds.\nThe geological gully-fill deposits of the Miocene period are unique, especially the peaty shale deposits of estuarine origin. Even more significant is the assemblage of fossil pollens and other microflora in the shale. The clear evidence of lateritic mantling is also unique on the eastern Australian coastline.\nThe Prince Henry site provides evidence of Aboriginal prehistory, the treatment of infectious diseases among Aboriginals and in the community generally that is rare in New South Wales and important to community groups.\nIt would require further research and analysis to detail the extent to which the Prince Henry site demonstrates other aspects of the rarity criteria. However, the assessment undertaken to date is clearly sufficient to establish the Prince Henry site as having rarity value at State and Local levels.\n\nThe place is important in demonstrating the principal characteristics of a class of cultural or natural places/environments in New South Wales.\n\nThe Prince Henry site, through the nature and degree of its historic, aesthetic, social, significance, technical/ research potential and rarity, provides ample evidence to represent the following key State themes:\nscience;\ngovernment and administration;\nhealth;\neducation;\ndeath; and\npersons.\nIt satisfies all of the following inclusion guidelines, at State and Local levels:\nis a fine example of its type;\nhas the potential characteristics of an important class or group of items;\nhas attributes typical of a particular way of life, philosophy, custom, significant process, design, technique or activity;\nis a significant variation to a class of items;\nis part of a group, which collectively illustrates a representative type;\nis outstanding because of its setting, condition or size; and\nis outstanding because of its integrity of the esteem in which it is held.\nSome of the geological elements have some importance under this criterion, namely the shale, the gully-fill sand and the lateritic soil horizon. In each case the element is a significant variation to a class of items rather than being typical of the relevant class.\n\n"}
{"id": "52933470", "url": "https://en.wikipedia.org/wiki?curid=52933470", "title": "Ras Al-Khair Power and Desalination Plant", "text": "Ras Al-Khair Power and Desalination Plant\n\nThe Ras Al-Khair Power and Desalination Plant is a power and desalination plant located in Ras Al-Khair on the eastern coast of Saudi Arabia. It is operated by the Saline Water Conversion Corporation of Saudi Arabia. The plant began operating in April 2014 and, , is the world's largest hybrid water desalination plant. The project includes a power plant capable of producing 2400 MW of electricity. In 2015, it won the Global Water Awards \"Desalination Plant of the Year\" award.\n\nConstruction of the plant began in 2011. Though originally slated to begin producing freshwater by the end of 2013, the first freshwater produced from the plant was in early 2015. Full commercial operation of the plant was achieved in March 2016.\n\nThe primary contractors for construction of the plant were the Doosan Group and Saudi Archirodon. Doosan was awarded the prime contract in September, 2010. Additional contractors for the plant included Fluid Equipment Development Company for energy recovery devices, Siemens for generators, turbines, and related equipment, and Hyosung for various motors.\n\nThe plant has created 3,500 direct and indirect jobs.\n\nWater intake for the plant comes from the Persian Gulf, on the shore of which the plant sits. The plant includes five high-efficiency gas turbines operated in combined cycle mode and in single mode. It uses a hybrid system of eight multi-stage flashing units and 17 reverse osmosis units. Freshwater output from the plant is pumped via pipelines to Riyadh and Hafr Al-Batin. Electricity and freshwater output from the plant also supply a nearby aluminium oxide refinery. Of the 2400 MW electricity produced by the plant, 200 MW are used by the plant itself. Effluent from the plant is processed locally by a dedicated wastewater treatment plant and discharged into the Persian Gulf.\n\n"}
{"id": "55198067", "url": "https://en.wikipedia.org/wiki?curid=55198067", "title": "Relapsing–remitting", "text": "Relapsing–remitting\n\nRelapsing–remitting is a medical term referring to a presentation of disease symptoms that become worse over time (relapsing), followed by periods of less severe symptoms that do not completely cease (remitting). The term is sometimes applied to groupings of presentations of particular diseases that display this pattern, such as:\n\n"}
{"id": "6631880", "url": "https://en.wikipedia.org/wiki?curid=6631880", "title": "Royal Australasian College of Surgeons", "text": "Royal Australasian College of Surgeons\n\nThe Royal Australasian College of Surgeons (RACS) is the leading advocate for surgical standards, professionalism and surgical education in Australia and New Zealand. The head office of the College is in Melbourne, Australia.\n\nThe College is a not-for-profit organisation that represents more than 7000 surgeons and 1300 surgical trainees and International Medical Graduates. RACS also supports healthcare and surgical education in the Asia-Pacific region and is a substantial funder of surgical research. \n\nRACS was formed in 1927 as the Australian College of Surgeons, and its prime purpose, as outlined by the Medical Journal, was to \"enable the public to distinguish between surgeons and men who undertake operations\". Its major roles are in training surgeons, continuing education, and setting standards for surgical practice. The members of the College fall into the following categories: Fellows (who possess the fellowship of the College - FRACS), Trainees (doctors training to be surgeons) and IMGs (International Medical Graduates). The College is independent from government and universities and is funded through fees paid by Trainees and Fellows.\n\nThe College trains in nine surgical specialty areas:\n\nSome surgical specialties receive their training from separate colleges, these include: ophthalmic surgeons who are examined by the Royal Australian and New Zealand College of Ophthalmologists (RANZCO), oral and maxillofacial surgeons who are examined by the Royal Australasian College of Dental Surgeons (RACDS), and obstetric and gynecological surgeons who are examined by the Royal Australian and New Zealand College of Obstetricians and Gynaecologists (RANZCOG).\n\nThe major activities of the College are surgical training and examination, setting standards for surgical practice, continuing professional development and government and media relations. The Surgical Education and Training (SET) program has improved the efficiency of surgical education and training through early selection into specialty training and streamlining training.\n\n\n"}
{"id": "1563008", "url": "https://en.wikipedia.org/wiki?curid=1563008", "title": "Sebastian Kneipp", "text": "Sebastian Kneipp\n\nSebastian Kneipp (17 May 1821, Stephansried, Germany – 17 June 1897, in Bad Wörishofen) was a Bavarian priest and one of the forefathers of the naturopathic medicine movement. He is most commonly associated with the \"Kneipp Cure\" form of hydrotherapy, the application of water through various methods, temperatures and pressures which he claimed to have therapeutic or healing effects, thus building several hospitals in Bad Wörishofen.\n\nAlthough most commonly associated with one area of Nature Cure, Kneipp was the proponent of an entire system of healing which rested on 5 main tenets:\n\n\nKneipp was born in 1821 in Bavaria. His father was a weaver, and Kneipp trained as a weaver until he was 23 when he began training for the priesthood. Matthias Merkle, a priest in Grönenback began instructing him, but Kneipp fell ill with tuberculosis in 1847. Kneipp was so ill that he was visited by a physician around 100 times in each of his last two years of study. While Kneipp was ill, he began reading many books and found his illness described in a book about water cures. In 1850, Kneipp met a student in the Georgianum seminary in Munich that was also ill and shared water cures with him. Both Kneipp and his friend at the Georgianum recovered from their illnesses and with his renewed health Kneipp was able to complete his studies. He was ordained as a Catholic priest in 1852.\n\nIn the 19th century, there was a popular revival in the application of hydrotherapy, instigated around 1829 by Vincent Priessnitz, a peasant farmer in Gräfenberg, then part of the Austrian Empire. This revival was continued by Kneipp, \"an able and enthusiastic follower\" of Priessnitz, \"whose work he took up where Priessnitz left it\", after he came across a treatise on the cold water cure. At Worishofen, while serving as the confessor to the monastery, he began offering treatments of hydrotherapy, botanical treatments, exercise and diet to the people who lived in the village. Some of his suggested treatments included \"ice cold baths and walking barefoot in the snow\" and other \"harsh\" methodologies. In 1893, M. E. Bottey described Kneipp's water cures as \"dangerous in most cases\".\nWorishofen became known as a place with a reputation for spiritual healing. In addition to \"peasants\", Kneipp's clients also included Archduke Franz Ferdinand of Austria and his father, Archduke Karl Ludwig as well as Pope Leo XIII. Others took Kneipp's processes back to their home countries to found alternative therapy spas and colleges.\n\nKneipp began developing his healing methods in 1849 after contracting tuberculosis and experimenting with the water treatments developed by Sigmund Hahn. After being ordained in 1852, he continued to experiment with water treatments in his parish. Kneipp began working with the cures developed by Vincenz Priessnitz but developed a more complicated and gentle method. His gentle cures contrast the earlier water cures that he referred to as horse cures for their strenuous nature. Kneipp's treatment of patients also contrasted that of hospital medicine because it was personalized and took into account the patient's individual strengths and weaknesses.\n\nKneipp's approach comes from his theory that all diseases originate in the circulatory system. This theory is similar to humoral theory. Like those that believed in humoral theory, Kneipp asserted that breathing miasmatic or excessively hot air would lead to disease. While it may deal with one humor instead of four, his theory still asserts that an imbalance in the blood whether it be circulation or foreign matter is the root of disease. Under Kneipp's depiction of disease, water cures work by affecting the blood. They dissolve foreign matter, cleanse the blood of this matter, aid in circulation, and strengthen the body as a whole.\n\nIn addition to specific cures, Kneipp had prescriptions with regard to food drink and clothing. He believed that food should be dry and simple and should not be spicy. He also believed that people should drink primarily water but also allowed consumption of alcohol in moderation. As for clothing, Kneipp preferred self-spun clothing made of linen or hemp over wool.\n\nKneipp's approach to medicine was not independent of his Catholic faith. His focus on water and herbs stems from the idea that remedies are naturally provided by God. His emphasis on plain food, drink, and clothing comes from the theory that humans should live in accord with nature. He used scripture as well as references to Roman practice to support the reasoning behind his cure and admitted that his treatments did not fall in line with current scientific understanding. The fact that his treatments were not based in scientific theory did not bother Kneipp because they were seen as able to succeed where scientific medicine could not.\n\nSebastian Kneipp had a particular dedication to helping the poor and those that physicians can't help. His suffering early in life caused Kneipp to develop a deep sympathy for those less fortunate than him. He turned down many patients that could feasibly recover on their own but claims to have never refused to treat a patient that is poor or untreatable by other methods.\n\nKneipp's book \"My Water Cure\" was published in 1886 with many subsequent editions, and translated into many languages. He also wrote \"Thus Shalt Thou Live\", \"My Will\", and \"The Care of Children In Sickness and In Health\".\n\nKneipp expanded the definition of health to include a more holistic view which included mental, social, and spiritual aspects. Toward the end of his life and after his death, various organizations were created to teach his methods. In 1891, he founded Kneipp Bund, an organization that promotes water healing to this day. In America, Kneipp Societies were founded, which, under the influence of Benedict Lust, changed their name to Naturopatic Society of America. Today there are 600 organizations that are a part of Kneipp Worldwide and there are approximately 1000 members of the International Society of Kneipp Physicians. After his death, his treatments became part of mainstream medicine in Germany.\n\nArchduke Josef dedicated his medical atlas to Kneipp.\nKneipp's likeness was featured on a stamp. His recipe for whole wheat bread, called Kneippbrød, is the most commonly eaten bread in Norway.\n\n\n"}
{"id": "1130302", "url": "https://en.wikipedia.org/wiki?curid=1130302", "title": "Sleep inertia", "text": "Sleep inertia\n\nSleep inertia is a physiological state of impaired cognitive and sensory-motor performance that is present immediately after awakening. It persists during the transition of sleep to wakefulness, where an individual will experience feelings of drowsiness, disorientation and a decline in motor dexterity. Impairment from sleep inertia may take several hours to dissipate. In the majority of cases, morning sleep inertia is experienced for 15 to 30 minutes after waking.\n\nSleep inertia is of concern when decision-making abilities, safety-critical tasks and the ability to operate efficiently are important soon after awakening. In these situations, it poses an occupational hazard due to the cognitive and motor deficits that may be present.\n\nThese symptoms are expressed with the greatest intensity immediately after waking, and dissipate following a period of extended wakefulness. The duration of symptoms varies on a conditional basis, with primary expression during the first 15–60 minutes after waking and potentially extending for several hours. Tasks that require more complex cognitive operations will feature greater deficits as compared to a simple motor task; the accuracy of sensory and motor functioning is more impaired by sleep inertia as compared to sheer speed. In order to measure the cognitive and motor deficiencies associated with sleep inertia, a battery of tests may be utilized including psychomotor vigilance task, descending subtraction task, auditory reaction time task, and the finger tapping task. \n\n\nThere has been a great deal of research into potential methods to relieve the effects of sleep inertia. The demand for remedies is driven by the occupational hazards of sleep inertia for employees who work extended shifts such as medical professionals, emergency responders, or military personnel. The motor functioning and cognitive ability of these professionals who must immediately respond to a call can pose a safety hazard in the workplace. Below are some of the various methods that have been suggested to combat sleep inertia.\n\nWhen sleep deprived, re-entering sleep may provide a viable route to reduce mental and physical fatigue but this can also induce sleep inertia. In order to limit sleep inertia, one should avoid waking from the deeper stages of slow-wave sleep. The onset of slow-wave sleep occurs approximately 30 minutes after falling asleep, therefore a nap should be limited to under 30 minutes to prevent waking during slow-wave sleep and enhancing sleep inertia. Furthermore, self-awakening from a short nap was shown to relieve disorientation of sleep inertia as opposed to a forced awakening but these results may warrant more research into the nature of arousal after sleep periods.\n\nCaffeine is a xanthine derivative that can cross the blood-brain barrier. The caffeine present in coffee or tea exerts its stimulating action by blocking adenosine receptors in the brain. By antagonizing the adenosine receptors caffeine limits the effects of adenosine buildup in the brain and increases alertness and attentiveness. Previous research has shown that coupled with a short nap, consuming caffeine prior to the nap can alleviate the effects of sleep inertia. Nonetheless, individual degree of consumption and tolerance to caffeine may be responsible for variation in its efficacy to reduce sleep inertia symptoms.\n\nThe natural light provided by the morning sunrise may contribute to a reduction in sleep inertia effects. Research simulating increase of light at dawn was shown to potentiate the cortisol awakening response (CAR). The CAR is a spike in blood cortisol levels following awakening, and is associated with the return to an alert cognitive state.\n\nSome other stimuli that could potentially minimize the effects of sleep inertia are sound and temperature. There is moderate evidence that the presence of mild sounds and a sharp decrease in the temperature of the extremities may independently reverse sleep inertia symptoms. Noise is thought to increase attentiveness upon awakening. A drop in temperature of the extremities may prevent heat loss, facilitating the return of core body temperature to homeostatic daytime levels.\n\n\n"}
{"id": "55985926", "url": "https://en.wikipedia.org/wiki?curid=55985926", "title": "Slow medicine", "text": "Slow medicine\n\nSlow medicine is a movement calling for change in medical practice which took inspiration from the wider slow food movement. Practitioners of slow medicine have published several different definitions, but the common emphasis is on the word \"slow,\" meaning to allow the medical practitioner to have sufficient time with the patient. Like for the slow food movement, slow medicine is a call to balance over-emphasis on fast processes which reduce quality. \n\nThe first mention of slow medicine in print took place in the first decade of the twenty-first century, about fifteen years after the start of the slow food movement in Italy. In the year 2002 an article was published in an Italian medical journal which used the words \"slow medicine\" to mean an approach to medicine which would allow practitioners sufficient time to evaluate the patient and his or her wider social context, to reduce anxiety, to evaluate new methods and technologies, to prevent premature release from the hospital and also to provide adequate emotional support. Later, in English-language publications, several physicians independently started using the term slow medicine.\n\nA slow medicine society was formed in Italy in the year 2011, and the first Italian national conference on slow medicine took place in Turin, Italy in November, 2011. Since that time, slow medicine societies have been formed in other countries.\n\nDifferent medical practitioners emphasize different aspects of medicine when using the term \"slow medicine.\" For some, slow medicine means taking time and not rushing when evaluating a patient. For others, slow medicine is a careful evaluation of medical evidence and a desire not to \"overdiagnose\" or \"overtreat.\" The original Slow Medicine society in Italy points to three key words of being \"measured,\" \"respectful\" and \"equitable,\" which focuses on the social and political aspects of medicine. One early practitioner of slow medicine sees the patient in the metaphor of a plant which needs to be nourished and for impediments to be removed in order to allow healing to occur.\n\n\n"}
{"id": "47677275", "url": "https://en.wikipedia.org/wiki?curid=47677275", "title": "Steroid use in Australia", "text": "Steroid use in Australia\n\nAnabolic/androgenic steroids are drugs that are obtained from the male hormone, testosterone. Anabolic steroids are used for muscle-building and strength gain for cosmetic reasons as well as for performance-enhancement in athletics and bodybuilding. Anabolic steroids work in many ways by increasing protein synthesis in the muscles and by eliminating the catabolic process (the process of breaking down skeletal muscle for energy). It is common for teens and adults to use steroids as they stimulate and encourage muscle growth much more rapidly than natural body building.\n\nIn Australia, many people are encouraged to use steroids due to the body image expectations created by society. In secondary schools, 3.2% of boys and 1.2% of girls are using steroids. Many Australian bodybuilders visit Bangkok and Pattaya in Thailand because the pharmacies there sell some steroid brands ten times cheaper than they available on the Australian black market. Australians were also purchasing their steroids in other countries to avoid a possible criminal record at home. Australian Crime Commission statistics have shown that there was a 106% increase in the last financial year of \"performance and image-enhancing-drugs\", showing 5,561 border detections. \n\nIn the first 3 months of 2008, 300 AAS seizures were reported by the Australian Customs and Border Protection Service.\n\n"}
{"id": "2424904", "url": "https://en.wikipedia.org/wiki?curid=2424904", "title": "WHO Disease Staging System for HIV Infection and Disease in Adults and Adolescents", "text": "WHO Disease Staging System for HIV Infection and Disease in Adults and Adolescents\n\nWHO Disease Staging System for HIV Infection and Disease in Adults and Adolescents was first produced in 1990 by the World Health Organization and updated in September 2005. It is an approach for use in resource limited settings and is widely used in Africa and Asia and has been a useful research tool in studies of progression to symptomatic HIV disease.\n\nFollowing infection with HIV, the rate of clinical disease progression varies enormously between individuals. Many factors such as host susceptibility and immune function, health care and co-infections, as well as factors relating to the viral strain may affect the rate of clinical disease progression.\n\n\n\n\nConditions where a presumptive diagnosis can be made on the basis of clinical signs or simple investigations\n\nConditions where confirmatory diagnostic testing is necessary\n\nConditions where a presumptive diagnosis can be made on the basis of clinical signs or simple investigations\n\nConditions where confirmatory diagnostic testing is necessary\n\nPerformance scale: 1: asymptomatic, normal activity.\n\n\nAnd/or performance scale 2: symptomatic, normal activity.\n\n\nAnd/or performance scale 3: bedridden < 50% of the day during last month.\n\n\"The declaration of AIDS\"\n\nAnd/or performance scale 4: bedridden > 50% of the day during last month.\n\n(*) HIV wasting syndrome: weight loss of > 10% of body weight, plus either unexplained chronic diarrhoea (> 1 month) or chronic weakness and unexplained prolonged fever (> 1 month).\n\n(**) HIV encephalopathy: clinical findings of disabling cognitive and/or motor dysfunction interfering with activities of daily living, progressing over weeks to months, in the absence of a concurrent illness or condition other than HIV infection which could explain the findings.\n"}
{"id": "54014325", "url": "https://en.wikipedia.org/wiki?curid=54014325", "title": "Warren State Hospital", "text": "Warren State Hospital\n\nWarren State Hospital is a public psychiatric hospital in Warren, Pennsylvania, established in 1880. The original hospital was designed by John McArthur, Jr. and constructed under the Kirkbride Plan. Its population peaked at 2,562 residents in 1947. As of 2017, the hospital is still active.\n\nFamous residents included Nictzin Dyalhis and Joe Root. Some famous staff included Penny Colman and Philipp Schwartz.\n\n"}
{"id": "11844406", "url": "https://en.wikipedia.org/wiki?curid=11844406", "title": "Water supply and sanitation in Ethiopia", "text": "Water supply and sanitation in Ethiopia\n\nAccess to water supply and sanitation in Ethiopia is amongst the lowest in Sub-Saharan Africa and the entire world. While access has increased substantially with funding from foreign aid, much still remains to be done to achieve the Millennium Development Goal of halving the share of people without access to water and sanitation by 2015, to improve sustainability and to improve service quality.\n\nSome factors inhibiting the achievement of these goals are the limited capacity of water bureaus in the country's nine regions,two city administrations and water desks in the 550 districts of Ethiopia (\"woreda\"s); insufficient cost recovery for proper operation and maintenance; and different policies and procedures used by various donors, notwithstanding the Paris Declaration on Aid Effectiveness.\n\nIn 2001 the government adopted a water and sanitation strategy that called for more decentralized decision-making; promoting the involvement of all stakeholders, including the private sector; increasing levels of cost recovery; as well as integrating water supply, sanitation and hygiene promotion activities. Implementation of the policy apparently is uneven.\n\nIn 2005 the government announced highly ambitious targets to increase coverage in its Plan for Accelerated Sustained Development and to End Poverty (PASDEP) for 2010. The investment needed to achieve the goal is about US$300 million per year, compared to actual investments of US$39 million in 2001–2002. In 2010 the government presented the equally ambitious Growth and Transformation Plan (GTP) 2011–2015, which aims at increasing drinking water coverage, based on the government's definition, from 68.5% to 98.5%. While donors have committed substantial funds to the sector, effectively spending the money and to ensure the proper operation and maintenance of infrastructure built with these funds remain a challenge.\n\nEthiopia has 12 river basins with an annual runoff volume of 122 billion m of water and an estimated 2.6 - 6.5 billion m of ground water potential. This corresponds to an average of 1,575 m of physically available water per person per year, a relatively large volume. However, due to large spatial and temporal variations in rainfall and lack of storage, water is often not available where and when needed. Only about 3% of water resources are used, of which only about 11% (0.3% of the total) is used for domestic water supply.\n\nThe capital Addis Ababa's main source of drinking water is the Gafsara dam built during the Italian occupation and rehabilitated in 2009. Wells and another dam complement the supply.\n\nThe city of Dire Dawa is supplied exclusively from groundwater that is highly polluted. The situation is most dramatic in Harar where \"a steady decrease of the level of Lake Alemaya has resulted in the complete closure of the treatment plant\". Due to supply shortfall, water vendors sell untreated water at extremely high prices. The lake dries up because of local climate change, changes in land use in its basin and increased irrigation of khat, a mild drug that is being grown for local consumption and export. A pipeline is expected to bring water over a distance of 75 km from a well field near Dire Dawa to Harar.\n\nThe great majority of the rural community water supply relies on groundwater through shallow wells, deep wells and springs. People who have no access to improved supply usually obtain water from rivers, unprotected springs and hand-dug wells. Well, rivers and springs can be contaminated and can cause waterborne diseases. Rainwater harvesting is also common.\n\nThe number of people lacking access to \"improved\" water in 2015 was 42 million. Regarding sanitation, progress has been slower and there were still 71 million people without access to \"improved\" sanitation, in 2015. According to data from the Joint Monitoring Programme for Water Supply and Sanitation of WHO and UNICEF, which are in turn based on data from various national surveys including the 2005 Ethiopia Demographic and Health Survey (DHS), access to an improved water source and improved sanitation was estimated as follows in 2008:\n\n\nAccording to figures used by the Ministry of Finance and Economic Development for planning purposes, however, access was much higher. In 2010, access to drinking water was estimated at 68.5%: 91.5% in urban areas (within 0.5 km) and 65.8% in rural areas (within 1.5 km). The higher figure for rural areas may be because the distance to an improved water source used in this definition is higher than the distance used by the Demographic and Health Survey.\n\nIn 1990 access to improved water supply had been estimated at only 17%, and access to improved sanitation had been estimated at only 4%. There thus has been a significant increase in access for water supply and sanitation, which spans both urban and rural areas. More than 138,000 improved community water points were constructed and rehabilitated from 2008 to 2010.\n\nIn communities that lack access to an improved water source, women bear the brunt of the burden of collecting water. For example, according to an article by Tina Rosenberg for National Geographic, in the mountain-top village Foro in the Konso special woreda of southwestern Ethiopia women make three to five round trips per day to fetch dirty water from the Koiro river. Each roundtrip lasts two to three hours and water is carried in \"50-pound jerrycans\".\n\nDrinking water quality. Drinking water quality in Ethiopia varies. The most comprehensive picture of drinking water quality are the results of a national statistically representative survey of piped water supply, boreholes, protected dug wells and protected springs carried out by the WHO and UNICEF in 2004-2005. It shows that 72% of samples complied with the values for coliform bacteria in the Ethiopian drinking water standard ES 261:2001 and the WHO guidelines for drinking water. In the case of piped water supply by utilities compliance was highest at 88%. Open wells and unprotected springs were not included in the survey. Besides bacterial contamination, natural contamination with fluoride is an issue in the Rift Valley. The results of the survey confirm the results of routine monitoring undertaken in the laboratories of the Regional Water Bureaus and the Regional Health Bureaus. The latter results are archived at the Ethiopian Health and Nutrition Research Institute. Interaction and exchange of information between regional health bureaus and regional water bureaus is poor.\n\nOther aspects of service quality. In 2010, 20 percent of rural water systems were malfunctioning, down from 25% in 2007. About 35 percent of the estimated 30,000 hand pumps in Ethiopia, serving an estimated 2 million people, were non-functioning in the mid-2000s. In piped water systems rationing and service interruptions are frequent. There are no wastewater treatment plants in Ethiopia, so all wastewater collected in sewers is discharged without any treatment to the environment.\n\nFor more details see: Regions of Ethiopia\n\nIn order to understand responsibilities in the sector it is necessary to provide a brief overview of local government in Ethiopia. Ethiopia is a federal state consisting of the following subdivisions:\n\n\nIn addition to the nine regions there are two “chartered cities”, (Addis Ababa and Dire Dawa), where the lower-level administrative units mentioned above do not exist. There is wide disparity in development and institutional capacity between regions and also within regions. The Amhara, Oromia, Tigray regions as well as the small Harari region are relatively developed. About 70% of Ethiopians live in these four regions. The Southern Nations, Nationalities, and Peoples' Region, where about 20% of the population lives, is very heterogeneous. In the more pastoralist and remote “emerging” regions Somali, Afar, Gambela and Benishangul-Gumuz Regions, where about 10% of the population lives, capacity tends to be lowest.\n\nThere are strong national water supply and sanitation policies and key agencies have clear roles and strategies. National policies are set by the Ministry of Water and Energy (MWE), formerly the Ministry of Water Resources (MWR), for water supply, and by the Ministry of Health for sanitation.\n\nIn 2006 the government adopted a Universal Access Plan (UAP) to achieve 98% access for rural water supply and 100% access for urban water supply and sanitation by 2012. Its cost was estimated at US$2.5bn. During the first phase until 2012 the focus is on affordable and appropriate technologies, with the following service standards:\n\nIn October 2006 a Memorandum of Understanding was signed by the Ministry of Water Resources, the Ministry of Health and the Ministry of the Environment to clearly define the roles and responsibilities of each Ministry in the implementation of the Universal Access Plan. Regional Water Bureaus and Woreda Water Desks are in charge of investment planning, monitoring and technical assistance to service providers. Their capacity to fulfill these tasks is sometimes limited.\n\nFormally the MWE's mandate covers only water resources management and it has no legal mandate concerning drinking water supply. Nevertheless, it is de facto the entity in charge of setting policies for water supply and to channel donor funds in the sector to local government entities. As of 2009, what was then the Ministry of Water Resources had 737 employees in eight departments and 10 \"services\". One of the eight departments is the Water Supply and Sewerage Department. As of 2011 Alamayehu Tegenu is the Minister of Water and Energy.\n\nIn 2001 the government adopted a National Water Strategy prepared by the MWR. The overall strategy includes a water resources strategy, a hydropower development strategy, a water supply and sanitation strategy, and an irrigation strategy.\n\nConcerning water supply and sanitation, the strategy aims at:\n\n\nThe strategy document does not include a diagnostic of the current situation. The water and sanitation part of the strategy alone includes 44 recommendations concerning technical, institutional, capacity building, social, economic and environmental issues. There is no priorization between the recommendations and the strategy does not establish mechanisms to monitor the implementation of the strategy.\n\nThe Ministry of Health is in charge of policies related to sanitation and hygiene promotion. It has adopted a Sanitation and Hygiene Promotion Strategy. De facto sewers in urban areas are under the responsibility of the MWR, while the promotion of on-site sanitation is the responsibility of the Ministry of Health. The Sanitation and Hygiene Promotion Strategy has re-focused government resources on the promotion of pro-poor, low-cost practices.\n\nIn the capital, the Addis Ababa Water and Sewer Authority provides water and sewer services. In other cities and towns, Town Water Boards are responsible for service provision. They are expected to contract out service provision to private operators. In rural areas community water and sanitation committees operate water systems and promote sanitation. Not all the local committees are registered, which is a prerequisite to open a bank account to hold funds collected from users. Woreda Water Desks (local government entities) are supposed to support local committees. However, according to a research report funded by DFID, the community management service delivery approach that dominates in rural areas in Ethiopia \"has bypassed local government authorities and reduced their ownership; however these actors are still needed if rural services are to be sustainable and scalable.\"\n\nRegional water resources development bureaus play an important role in planning investments at the regional level and in capacity building.\n\nThe Ethiopian Social Rehabilitation and Development Fund (ERSDF) – a Social Fund established in 1996 – is also an important actor, especially in rural areas. It has financed almost 2,000 rural water projects serving about 2.5 million people. However, the government has decided to phase out the ERSDF and to re-deploy its staff to other institutions.\n\nUntil 1995 the national government had been responsible for centrally planning and implementing water and sanitation projects. Under the 1995 constitution Ethiopia became a federal state, which implied the decentralization of many functions to lower levels of government. This process has now been under way for more than a decade, but decentralization has been hampered by the limited capacity of local government to carry out its new responsibilities.\n\nAlso in 1995, a Ministry of Water Resources was created, taking over many of the responsibilities of the water resources department of the former Ministry of Public Works.\n\nIn 1999 the government adopted a National Water Resources Management Policy, which was followed by the establishment of a Water Resources Development Fund (2002) and a Water Sector Development Program. The latter includes a water supply and sewerage development program (nota bene the focus on sewerage and thus the absence of on-site sanitation from the program).\n\nThe government’s Plan for Accelerated Sustained Development and to End Poverty (PADEP), covering the period 2005-2010, aimed at increasing access to an improved water source to 84% and access to improved sanitation to 80% by 2010. These ambitious targets go well beyond the water and sanitation targets of the Millennium Development Goals, which aim at halving the share of people without access by 2015. According to one set of government figures, which is used by the Ministry of Finance and Economic Development for planning purposes, access to drinking water reached 68.5% in 2010. According to another set of government figures, based on national survey data and used by the WHO and UNICEF to monitor the Millennium Development Goals, in 2008 access to an improved water source was only 38% and to improved sanitation 12%.\n\nIn 2010 the government presented the equally ambitious Growth and Transformation Plan (GTP) 2011-2015 that aims at increasing drinking water coverage, based on the government's definition, from 68.5% to 98.5%.\n\nOn average cost recovery is too low to recover operating costs, not to speak of providing adequate maintenance of facilities. Recurrent expenditures – estimated at US$29 million in 2001–02 – were financed primarily through user charges (64%), as well as by subsidies from the regional governments (31%) and the federal government (5%). Despite this overall bleak picture, a few service providers recover all operating costs and generate a modest cash surplus.\n\nThe National Water Resources Management Policy aims at full cost recovery for urban systems and recovery of operation and maintenance costs for rural systems. It is not clear if progress has been made to achieve this ambitious objective since the policy was adopted.\n\nActual investment. There are no recent reliable estimates of actual investment levels in the sector, and available estimates vary greatly. A detailed estimate of investment and financial flows in the Ethiopian water sector was carried out by the World Bank's Water and Sanitation Program (WSP) for the financial year 2001-02. It estimated total sector investments at US$39 million or less than half a dollar per capita, being one of the lowest recorded sector investment levels in the world. Water and sanitation have declined as a share of total poor-focused expenditure from\n7.4% in 2005/06 to 3.4% in 2009/10. In addition, according to research funded by DFID, \"actual expenditure may fall well short of these levels. It seems that a very significant proportion of resources are not utilised and are subject to bottlenecks.\"\n\nProjected investment. The government estimates that annual investments in the 2006-2015 period will reach about US$100 million per year, or about two and a half times their level in 2001-2002. This projection is based on funding commitments made by donors and the government. It thus does not take into account bottlenecks in implementation due to limited capacities or other potential pitfalls. The government estimates that the actual investment needs are about three times as much or US$297 million per year for the period 2006-2015.\n\nSources. According to the WSP estimate quoted, in 2001-2002 only 9% of sector investments were funded by the federal budget, 55% through the regional budget, 33% off-budget by NGOs, 2% by the ERSDF and 1% by other sources. This estimate does not include community in-kind contributions, which are high for rural water supply and sanitation. A high but unknown share of the federal budget and probably also of the Woreda budget devoted to the sector is funded by donors. Concerning projected investments for 2006–2015, it is estimated that 12% (US$12 million) will be funded by the government with its own resources, 15% (US$16 million) by communities and 73% (US$75 million) by donors. It is not clear if this estimate includes off-budget support by NGOs. Because of the different categories used, a comparison between the historical and projected sources of financing is not possible.\n\nProcesses. According to a research report funded by DFID, the levels of funding to services delivered by regional and local government are difficult to predict. A large share of donor funding is channeled outside the government budget, although as of 2009 there was a trend for donors and government to \"ensure the alignment of financing mechanisms\". Where financing is provided through the budget, conditions and reporting procedures are \"excessively onerous\". Budget transparency and accountability are limited, although efforts are being made to create an open budget system at all levels of government. According to the report the government, particularly at lower tiers, \"does not recognise citizens’ rights to information about budget allocations\". The Universal Access Plan uses national-level unit costs to allocated government funds to local governments. The unit cost approach fails to take regional differences into account. There is no funding for operation and maintenance or the costs of local governments to support communities in operation and maintenance. This increases the risk that newly built systems will be failing. For the country's 550 Woredas an important source of financing are block grants from the central government which they can use autonomously within broad criteria set by the Water Resources Development Fund (WRDF). The WRDF itself provides loans and is administered by a Board that is responsible to the MWE and is funded through budgetary allocations and donor funds.\n\nDonors finance numerous projects in water supply and sanitation in Ethiopia – some through the Federal Government and some directly to regions, towns and communities. The donors have established a technical working group (TWG) on water as part of a core donor group called the Development Assistance Group. A Multi-Stakeholder Forum is also supported through the European Union Water Initiative. Despite improved coordination, donors still use different implementation arrangements. As a result, according to the World Bank, transaction cost are high.\n\nImportant donors in the sector are the African Development Bank, CIDA, China, the British DFID, the EU, FINIDA, AFD from France, Germany (through GTZ and KfW), JICA, the Netherlands, UNDP, UNICEF and the World Bank. There are also about 500 local and foreign NGOs, many of which are active in water supply and sanitation.\n\nThe African Development Bank provided a US$64 million grant for rural water supply and sanitation approved in 2005.\n\nIn November 2011 it was announced that China would provide a US$100 million loan for water supply in Addis Ababa.\n\nThe British NGO WaterAid is engaged in Ethiopia since 1983. It works closely with established local NGOs. In Oromia Region, water projects tend to be spring-fed gravity schemes, some of which are very large, providing water for tens of thousands of people. In Southern Nations, Nationalities, and People's Region schemes have included deep boreholes as water is sometimes only found below 200 metres. For example, in the village of Orbesho residents - mainly women - built themselves an access road to allow drilling equipment to be brought in, dug trenches for pipes and collected stones for structures. In Amhara and Tigray the main technologies have been hand-dug wells and spring development. In Benishangul-Gumuz rope pumps are also used. In sanitation, WaterAid supports the construction and use of latrines. Hygiene education has increasingly focused on the close links between proper handwashing at critical times, like before eating and after going to the toilet, and improved health. In all cases WaterAid works closely with communities from the start. Particular attention is now being paid to engaging women. Since 1998 WaterAid has also been engaged in the slum areas of Addis Ababa. Projects include establishing communal water points linked to the city's piped systems, as well as shower and latrine blocks.\n\nIn May 2012 the World Bank provided approved a USD 150 million soft loan for an urban water and sanitation project as additional financing to a USD 100 million soft loan approved in 2007. The project covers Addis Ababa, Gondar, Hawassa, Jimma, Mekelle and Diredawa. On the sanitation side, it will promote household latrines, hygiene and sanitation practices and constructing communal sanitation facilities. Water production is expected to increase from 50 to 75 liter per capita per day in Addis Ababa and from 30 to 50 liter in the five secondary cities. The water distribution network will be extended to serve 400,000 more people (40,000 connections) in Addis Ababa and 500,000 more people (50,000 connections) in the five secondary cities. The project will also promote awareness of water conservation among customers. The project will also reduce water losses: In Addis Ababa out of the 50 liters per capita per day produced, only 35 reached consumers due to water losses. In the remaining cities only 20 of the 30 liters produced reached consumers.\n\nIn March 2010, the World Bank approved additional financing of USD 80 million for a USD 100 million rural water supply and sanitation project approved in 2004. According to the World Bank, until 2010 the original project had financed the construction of 1288 hand dug wells, 835 protected springs, 576 shallow wells, 99 deep wells, 75 rural piped systems and 35 rainwater harvesting, as well as conducting hygiene and sanitation promotion. In rural areas alone, according to the World Bank the project facilitated access to clean water and improved sanitation facilities to about 1.4 million people. In urban areas, the project provided \"immediate service improvement\" in 87 towns which benefited about 143,000 people.\n\nIt is important for water to be drinkable and usable for daily use. Without any clean water there would more chances at water related diseases. The World Bank also stated that in Ethiopia from 2011-2015 every 64 in 1000 infants pass away before the age of 1. So in Ethiopia there is a 6.4% chance that one's infant would not be able to survive more than one year. Also take in account that these 6.4% infants are mostly born in the rural areas of Ethiopia.\n\nIn the rural areas of Ethiopia, woman and children walk up to six hours to collect water. They collect water from unprotected ponds which the share with animals. Others collect from wells. Both of these methods can be easily contaminated when rainwater washes waste into the pond or well. Under half of the population in Ethiopia has access to an improved water supply. Also only 21% of the population has access to adequate sanitation services. In the last 20 years, Ethiopia has suffered a lot because of droughts, shortages and famines. When there are droughts water-related diseases recur consistently. Also when there is too much water because of the rain it becomes as a breeding place for mosquitoes. Image of contaminated water in Ethiopia.\n\nThere are a lot of ways that humans have collaborated to find a way to make everyone have adequate hygiene knowledge or access to water. Here are a few examples for how we can approach issues relating to inadequate access to water or hygiene.\n\nWater Supply:\n\n\nA solar powered pump that uses locally sourced equipment can pump around 30,000 liters of clean, safe water per day.\n\n\nSome of the world's poorest people are those who live in poverty and inhuman places. Installing effective sanitation or hygienic toilets would be useful and would reduce a lot of diseases spreading around.\n\n\nSimple irrigation techniques can and will help families move from being malnourished to being self-sufficient.\n\nSanitation:\n\n\nPeople living in poverty areas or region mostly have poor hygiene behavior. This is not because they are like that but because they don't have the resources to do so. They lack of sanitation mostly due to rapid urbanization.\n\n\n\n"}
{"id": "1579924", "url": "https://en.wikipedia.org/wiki?curid=1579924", "title": "Wheelchair ramp", "text": "Wheelchair ramp\n\nA wheelchair ramp is an inclined plane installed in addition to or instead of stairs. Ramps permit wheelchair users, as well as people pushing strollers, carts, or other wheeled objects, to more easily access a building.\n\nA wheelchair ramp can be permanent, semi-permanent or portable. Permanent ramps are designed to be bolted or otherwise attached in place. Semi-permanent ramps rest on top of the ground or concrete pad and are commonly used for the short term. Permanent and semi-permanent ramps are usually of aluminum, concrete or wood. Portable ramps are usually aluminum and typically fold for ease of transport. Portable ramps are primarily intended for home and building use but can also be used with vans to load an unoccupied mobility device or to load an occupied mobility device when both the device and the passenger are easy to handle.\n\nRamps must be carefully designed in order to be useful. In many places, laws dictate a ramp's minimum width and maximum slope. \n\nIn general, reduced incline rises are easier for wheelchair users to traverse and are safer in icy climates. However, they consume more space and require traveling a greater distance to go up. Hence, in some cases it is preferable to include an elevator or other type of wheelchair lift.\n\nIn many countries, wheelchair ramps and other features to facilitate universal access are required by building code when constructing new facilities which are open to the public. Internationally, the United Nations Convention on the Rights of Persons with Disabilities mandates nations take action to \"enable persons with disabilities to live independently and participate fully in all aspects of life.\" Among other requirements, it compels countries to institute \"minimum standards and guidelines...\" for accessibility. \n\nIn the U.S.A, the Americans with Disabilities Act (ADA) requires a slope of no more than 1:12 for wheelchairs and scooters for business and public use, which works out to of ramp for each of rise. For example, a rise requires a minimum of in length of ramp. Additionally, ADA limits the longest single span of ramp, prior to a rest or turn platform, to .\n\nRamps can be as long as needed, but no single run of ramp can exceed . Residential Applications usually are not required to meet ADA standards (ADA is a commercial code).\n\nThe UK's guidelines as recommended by the Disability Discrimination Act 1995 and Equality Act 2010 are a maximum of 1:12 for ramps (with exceptions for existing buildings)\n\"Ramps should be as shallow as possible.\nThe maximum permissible gradient is 1:12 [...], with the occasional exception in the case of short, steeper ramps when refitting existing buildings.\"\n\nRamps can have a maximum going of , beyond which there has to be a landing before continuing as a ramp. The maximum permissible gradient for non domestic dwellings, 1:12, applies to ramps with a going no greater than . This equates to a maximum Rise of . The gradient of the longest permissible ramp going of must not be steeper than 1:20. This equates to a maximum Rise of . In between these two limits of ramp goings the allowable steepest gradient varies in a graduated way. This is shown in the Building Regulations 2004 Part M on a graph from which the reader is required to interpolate the allowable gradient. Alternatively, there is a simple calculation method which gives a very accurate result. The formulae for these are <reference ODPM>:\nGoing = (Rise X 10 000) / (1000 - Rise) Note. The calculated Going is the minimum allowable for the given Rise.\nRise = (Going x 1000) / (Going + 10 000) Note. The calculated Rise is the maximum allowable for the given Going.\n\nIn Hong Kong, wheelchair ramp may not exceed a 1:12 slope for wheelchairs except in some situations under the Barrier Free Access (BFA) terms.\n\nIn South Africa 1:12 max unless difference in level is less than 400mm in which case 1:10 max. [SANS 10400-S SS2(a)].\n\nIn Australia, the Australian Standards Council requires a wheelchair ramp to have a maximum incline of 1 in 14. This means that for every travelled horizontally, the ramp rises . The wheelchair ramp must also have a minimum width of .\n\nWheelchair accessible vehicles may also include a ramp to facilitate entry and exit. These may be built-in or portable designs. Most major automotive companies offer rebates for portable ramps and mobility access equipment for new vehicles.\n\n\n"}
