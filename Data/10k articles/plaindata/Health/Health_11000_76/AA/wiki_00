{"id": "20527491", "url": "https://en.wikipedia.org/wiki?curid=20527491", "title": "2008 Zimbabwean cholera outbreak", "text": "2008 Zimbabwean cholera outbreak\n\nThe 2008 Zimbabwean cholera outbreak was an epidemic of cholera affecting much of Zimbabwe from August 2008 until June 2009. The outbreak began in Chitungwiza in Mashonaland East Province in August 2008, then spread throughout the country so that by December 2008, cases were being reported in all 10 provinces. In December 2008, The Zimbabwean government declared the outbreak a national emergency and requested international aid. The outbreak peaked in January 2009 with 8,500 cases reported per week. Cholera cases from this outbreak were also reported in neighboring countries South Africa, Malawi, Botswana, Mozambique, and Zambia. With the help of international agencies, the outbreak was controlled, and by July 2009, after no cases had been reported for several weeks, the Zimbabwe Ministry of Health and Child Welfare declared the outbreak over. In total, 98,596 cases of cholera and 4,369 deaths were reported, making this the largest outbreak of cholera ever recorded in Zimbabwe. The large scale and severity of the outbreak has been attributed to poor sanitation, limited access to healthcare, and insufficient healthcare infrastructure throughout Zimbabwe.\n\nThe 2008 cholera outbreak was caused by widespread infection with the bacterium \"Vibrio cholerae\" which is spread through water contaminated with the feces of infected individuals. Cholera had been seen in Zimbabwe in the decade leading up to the 2008 outbreak. However, the severity of the 2008 has been attributed to a combination of societal factors including poor access to health care and poor health care infrastructure, high HIV prevalence, political instability, food shortages, high levels of displaced people, and lack of access to safe water. In 2008, Zimbabwe was suffering from an economic crisis and hyperinflation which led to shortages of food and other basic goods, disruption of public services, and a large number of refugees moving within the country and to neighboring countries.\nOne of the major contributing factors to the outbreak was the breakdown of the municipal water supply, sanitation, and waste collection programs throughout the country, but especially in urban areas. With this, the onset of the rainy season led to cholera-contaminated faeces being washed into water sources, in particular public drains, as well as providing readily available but contaminated water. Due to a shortage of purification chemicals, such as chlorine, the capital city of Harare stopped receiving piped water on 1 December 2008. By that date, many suburbs had not had any water supply for much longer. On 4 December 2008, the Zimbabwe deputy minister for water and infrastructural development stated that there were only sufficient treatment chemicals in stock nationally for 12 weeks supply. The collapse of these systems was blamed on the then-current economic crisis; many households cannot afford fuel to boil water. According to Médecins Sans Frontières, the spread of cholera from urban to rural areas from December 2008 onwards was due to infected city-dwellers visiting their families' rural homes for Christmas and the burial of infected city-dwellers in rural areas.\n\nThe 2008 cholera epidemic in Zimbabwe had an unusually high fatality rate; Oxfam attributed the high mortality to a population \"seriously weakened by hunger, HIV and AIDS\". A major contributing factor to the severity of the outbreak was the collapse of Zimbabwe's public health system, declared a national emergency on 4 December 2008. By the end of November 2008, three of Zimbabwe's four major hospitals had shut down, along with the Zimbabwe Medical School, and the fourth major hospital had two wards and no operating theatres working. Zimbabwean hospitals still open by December 2008 lacked medicines and staff. Due to hyperinflation, hospitals were not able to buy basic drugs and medicines, and the resources of even internationally funded emergency clinics were stretched. The ongoing political and economic crisis contributed to the emigration of doctors and people with medical knowledge. Some victims were travelling to Botswana and other neighbouring countries for treatment.\n\nThe 2008 outbreak began in Chitungwiza on 20 August 2008. In September, cases spread to the urban areas of Makonde and Chinhoyi. By the end of October, cases had spread to 3 provinces: Mashonaland West, Mashonaland East, and Harare city. In the first two weeks of November, the epidemic rapidly spread across Zimbabwe, appearing in a total of 9 provinces and 54 districts. The disease spread to reach all of Zimbabwe's ten provinces. The attack rate was highest in Beitbridge, Chegutu, Mudzi and Zvimba Districts (above 1,000 cases per 100,000 people or 1.0%).\n\nThe number of cases reported by the United Nations Office for the Coordination of Humanitarian Affairs escalated from 30 on 1 September 2008 to 15,572 by 10 December. According to the Red Cross, around 46% of reported deaths occur en route to clinics and hospitals. The head of the British Department for International Development in Harare said that \"there are probably twice as many people with cholera as turn up for treatment\".\n\nThe case fatality rate for the outbreak was higher than expected for such outbreaks, although it began declining by January 2009. Official estimates of fatalities have run from 484 to 800, since the outbreak in August 2008, with an upper estimate of 3,000 from an anonymous senior official in the Ministry of Health and Child Welfare. Fatality rates varied from 2.5% in Harare to 18% in Chitungwiza. In Harare, the crisis reached the extent that the city council offered free graves to cholera victims. By 7 December, Oxfam estimated 60,000 cases by the end of January 2009 and a 10% fatality rate, with UNICEF giving a similar estimate. On 4 December 2008, the Zimbabwe government declared the outbreak to be a national emergency.\n\nAssistance after the 2008 outbreak was made available by numerous international agencies, and funding for water, sanitation and hygiene programmes, epidemic response and the provision of essential drugs came from several governments and trans-governmental organisations:\n\nBy 7 December 2008, UNICEF had secured international donor funding to provide sufficient water treatment chemicals for three weeks water supply for Harare and had arranged a shipment of chemical sufficient for four months supply. UNICEF distributed 360,000 litres of water per day in Harare, as well as handing out soap and buckets. Notwithstanding the contributions received, UNICEF indicated on 9 December 2008 that US$17,500,000 was needed to respond properly to the outbreak. As of 15 December, following agreement with the Zimbabwe government, the World Health Organization was procuring medical supplies to roll out a response plan to run health centres.\n\nThe 2008 cholera outbreak spread to districts in Botswana, Mozambique, South Africa and Zambia bordering Zimbabwe.\n\nCholera spread to the Zimbabwean migrant worker community in Limpopo and Mpumalanga provinces of South Africa and cholera bacteria were detected in the Limpopo River on 3 December 2008. By 12 December 2008, 11 deaths and 859 infections had been recorded in South Africa, rising to 2,100 cases and 15 deaths by 14 January 2009, and to 12,000 cases and 59 deaths by 10 March.\n\nThe South African government set up medical facilities and drinking water supplies at the Beitbridge border post and deployed the National Outbreak Response Team and additional medical personnel to Musina. Anthony Turton, a political scientist and Unit Fellow with the Council for Scientific and Industrial Research (CSIR) in South Africa, who had earlier warned of the risk of cholera in South Africa and wrote a report that recommended that the South African government increase its spending on water treatments lest a cholera outbreak occur in the country, was suspended for having made \"inappropriate statements to the media\". On 10 December 2008, the Limpopo Provincial Government declared Vhembe District Municipality, which borders Zimbabwe at Beitbridge, Matabeleland South province, a disaster area. On a 28 January 2009 visit to Musina with high-ranking government and ruling party officials, Health Minister Barbara Hogan said\nThe spread of cholera to Zimbabwe's other neighbouring countries was initially slower than in South Africa, with one death recorded in Kafue District in Zambia and none in Botswana or Namibia by 9 December 2008. In 2009, cases increased, with 4,354 cases and 55 deaths reported by 10 February 2009 in Zambia and 1,596 cases and 14 deaths in Katanga, the southernmost province of the DR Congo. In Mozambique, cholera spread to 10 out of 11 provinces, with a total of 9,533 cases by 1 Jan to 1 Mar 2009 and 119 deaths by 17 March. Four health workers also died in a mob attack, blamed on \"misinformation and misunderstanding in efforts to combat cholera\", and 12 of the prisoners from the incident died in jail. In Malawi 104 deaths were recorded since January, making it the worst outbreak since 2001–02 where 960 people died. Kenya, Somalia, Tanzania, DRC, and Ghana have had unrelated cholera outbreaks with between 10–100 deaths in 2009 as of February.\n\nAfter the 2008 epidemic was declared a national emergency, the Ministry of Health and Child Welfare (MOHCW) collaborated with several other departments, governments, and non-governmental organizations to create a Cholera Command and Control Centre. This Centre works to prevent cholera outbreaks in Zimbabwe by addressing broader societal factors that could contribute to cholera outbreaks, such as water sanitation and poor hygiene habits.\n\nBecause of its well-organised health care system and effective water sanitation facilities, Rita R. Colwell of the James Baker Institute says Zimbabwe was historically one of the African countries least affected by cholera. A news commentary in \"The Lancet\" said that, under President Robert Mugabe, the country's health programs were negatively impacted, resulting in diminished health care for those infected with cholera. According to a draft paper from the WHO's World Conference on Social Determinants in Health, there were fewer health workers in the villages than in urban areas, which hindered early detection and isolation of cholera cases.\n\nA news commentator writing for \"The Lancet\", Andrew Meldrum, said that President Mugabe’s Youth Militia threatened health professionals that provided medical treatment to political opponents. He said that, combined with decreasing education standards, low pay, and a shortage of medical supplies like latex gloves, this led doctors to leave Zimbabwe at an alarming rate. According to Douglas Gwatidzo, the chairman of the Zimbabwe Doctors for Human Rights group, Zimbabwe had only one doctor assigned to a group of 12,000 citizens. Doctors in Zimbabwe fill only 25% of the medical posts available, and even fewer specialist positions are taken. According to Meldrum, this poses serious challenges to health care for diseases like HIV/AIDS and cholera. Similarly, the effects of cholera are exacerbated without proper nutrition, and Zimbabwe has faced food shortages for the last several years.\n\nCholera and malnutrition keep children out of school – a serious social consequence of the outbreak. Rachel Pound, the director of Save the Children in Zimbabwe, said that attending school may be dangerous in Zimbabwe, instead of providing a ladder for self-improvement. She noted that \"Sanitation is now so bad in schools that they may become a breeding ground for infection\", rather than a place of valuable education.\n\nAccording to Meldrum, Zimbabwe's high inflation left the country with a lack of financial resources, resulting in a shortage of ambulances and pharmaceutical drugs. According to Eric Pruyt of the Delft University of Technology in The Netherlands, this was exacerbated by a shortage of international aid, as Zimbabwe's government didn't acknowledge the epidemic and accept aid until the disease was widespread. It was not contained or prevented from spreading. Until 2008, the government insisted that there was no cholera in Zimbabwe, and Pruyt says the U.N. did not provide the country with safe drinking water until after the crisis started. Meldrum says that, during Zimbabwe's continuing HIV/AIDS dilemma, some major international donors did not give much money because they believed it would help President Mugabe stay in power, which they did not want.\n\nAs the outbreak and health crisis grew worse, American and British leaders cited the crisis as further proof that it was, in their view, \"well past time for (President) Robert Mugabe to leave\" and that Zimbabwe had become a failed state. Marian Tupy of the Cato Institute said that the crisis began in 2005 when the government took over water treatment facilities but without sufficient funding to maintain purification processes. The transfer of water treatment from local government to the Zimbabwe National Water Authority was criticized by Innocent Nhapi of the National University of Rwanda on the basis of capacity and funding of the authority. The lack of funding for water treatment chemicals, maintenance and staff salaries was cited by Colwell of the Baker Institute as a major cause of the epidemic. According to Colwell, before funds were diverted from the plants to other uses, there were only sixty-five cases and four deaths from cholera in Zimbabwe.\n\nAccording to an editorial by Daniel J Ncayiyana in the \"South African Medical Journal\", President Mugabe blamed the U.S. and the U.K. for the cholera outbreak, saying that they sent the disease so that they have a reason to credibly remove him from the presidency. One Zimbabwean citizen was shown with a sign that blamed UK Prime Minister Gordan Brown for the disease; the sign expressed the horrors of \"Brown’s cholera\".\n\nAccording to a news report in \"Al Jazeera\", the Zimbabwe government and state media blamed the outbreak on European and American sanctions and a Reuters report said it accused Britain of plotting an invasion under the cover of the outbreak. Information minister Sikhanyiso Ndlovu blamed the cholera deaths on Western sanctions, saying \"the cholera issue has been used to drive a wedge among us\". On 12 December, Ndlovu repeated his accusation, and claimed that the cholera outbreak was actually a \"serious biological-chemical weapon\" attack by the United Kingdom, which Ndlovu asserted was trying to commit genocide. Said Ndlovu:\n\nIn the meantime, a senior ZANU-PF official argued that the government and party leadership was more focussed on the forthcoming ZANU-PF conference than on the current crisis. On 11 December 2008, President Robert Mugabe made a speech screened on national television in which he said:\n\nReports from the WHO contradicted Mugabe's view and indicated a growing death toll. According to the WHO, as of 8 December nearly 800 people had died of cholera and more than 16,000 cases were being treated. Later that same day, Zimbabwean visas were denied to six French aid workers, including three crisis management specialists, two epidemiologists and a water treatment expert. Britain's Africa minister, Mark Malloch-Brown, dismissed Mugabe's claim that the Zimbabwe cholera crisis is over, commenting as follows:\n\nThe French foreign ministry and USAID also contradicted Mugabe's statements and called on him to allow aid to reach the people in need.\n\n\n"}
{"id": "9627157", "url": "https://en.wikipedia.org/wiki?curid=9627157", "title": "Abortion in El Salvador", "text": "Abortion in El Salvador\n\nAbortion in El Salvador is illegal. The law formerly permitted an abortion to be performed under some limited circumstances, but, in 1998, all exceptions were removed when a new abortion law went into effect.\n\nEl Salvador's 1956 Penal Code contained no explicit exception to its prohibition of abortion, although, under accepted principles of criminal law, one could be justified if necessary to preserve the life of the pregnant woman. In response to the fact that the practice of illegal abortion was common, and was a major contributor to the rate of maternal mortality, the Salvadoran government chose to expand the cases in which abortion was permitted.\n\nUnder the new Penal Code of 1973, an abortion could be legally allowed under three major conditions: if the pregnant woman's life was endangered and abortion was the only means to preserve it, if her pregnancy had resulted from rape or statutory rape, or if a serious congenital disorder was detected in the fetus. An abortion caused on part of the woman's negligence was exempted from prosecution, and the government also provided reduced penalties for a woman of good standing if she had consented to an illegal abortion, or self-induced one, in the interest of protecting her reputation.\n\nProposals to eliminate the exceptions to the general prohibition against abortion started to come before the country's Legislative Assembly in 1992. One bill would have resulted in the investigation of medical clinics suspected of providing abortion; as a result of a 1993 study, overseen by a politician affiliated with the Christian Democratic Party, several health care workers were arrested. Another proposal in 1993, which was supported by the Archbishop of San Salvador and the Say Yes to Life Foundation (a pro-life group), would have made December 28, a traditional Roman Catholic feast day known as the Day of the Innocents, the \"Day of the Unborn\".\n\nIn 1997, the Nationalist Republican Alliance (ARENA) submitted a draft bill, designed to amend the Penal Code to withdraw all grounds under which abortion was then permitted. On April 25, 1997, the Legislative Assembly voted 61 out of 84 to approve this modification to the Code.\n\nOn April 20, 1998, the new Penal Code was enacted, removing the exceptions that had been instituted in 1973, including the provision for the pregnant woman's life. Under this Code, a person who performs an abortion with the woman's consent, or a woman who self-induces or consents to someone else inducing her abortion, can be imprisoned for two to eight years. A person who performs an abortion to which the woman has not consented can be sentenced to four to ten years in jail; if the person is a physician, pharmacist, or other health care worker, he or she is instead subject to between six and 12 years.\n\nEl Salvador also amended its Constitution in January 1999 to recognize human life from the moment of conception.\n\nThe organizations IPAS, MADRE, and Women's Link Worldwide submitted a report to the United Nations Human Rights Committee in which they contended that the El Salvadoran law against abortion violates several treaties that El Salvador has ratified: the International Covenant on Civil and Political Rights (ICCPR); the International Covenant on Economic, Social and Cultural Rights (ICESCR); the International Convention on the Elimination of All Forms of Racial Discrimination (CERD); the Convention on the Rights of the Child (CRC); and the Convention on the Elimination of All Forms of Discrimination against Women (CEDAW).\n\nThey point out that \"The Constitution of El Salvador protects every person’s right to life, liberty, security of person, and social justice. Furthermore it establishes that all persons are equal before the law and there can be no restrictions based on race, gender or religion. El Salvador’s criminal anti-abortion legislation violates all of these constitutional and human rights established in the previously mentions international conventions.\"\n\nUnsafe abortion is a serious public health problem and the second direct cause of maternal mortality in El Salvador. In 1994, the third-most-prevalent cause of mortality among adolescent girls was pregnancy and postpartum complications. Some of the girls arrested for trying to have abortions are as young as ten. It's a drain on resources in a country where health care expenditures in 1997 were $24 per person per year.\n\nThe criminalization of abortion has extremely serious consequences for women’s lives and health: abortions performed under dangerous conditions; high mortality and morbidity rates; and a lack of reliable studies that could help health services provide better care to their clients, including women who have had abortions in unsafe conditions. This situation is further exacerbated by the persecution of women by the Salvadoran justice and health systems. In some cases, therapeutic abortions are performed by physicians in medical settings, and justified as other procedures, e.g. a laparotomy (abdominal incision) rather than an abortion.\n\nMany low-income women who have a miscarriage or a stillbirth are often prosecuted. Oftentimes they are reported by medical personnel to the police and subsequently arrested in the hospital. They are wrongly accused of abortion or homicide and sentenced up to 40 years in prison. Currently, there are 17 women in prison for pregnancy related complications that have not had due process while being prosecuted.\n\nDoctors in El Salvador report that women seeking abortions use a wide variety of methods: clothes hangers, metal rods, high doses of contraceptives, fertilizers, gastritis remedies, soapy water, and caustic fluids (such as battery acid). The most common methods are pills, such as Cytotec and potassium permanganate (inserted vaginally); catheters to inject soapy water or caustic liquids; rods of any type of material to penetrate the uterus; injections of unknown solutions; or a combination of abortion methods, such as pills, a catheter, and an injection or pills and a metal rod.\n\nUsing pills, catheters, injections and rods can kill a woman or injure her permanently. In addition to having only dangerous methods at their disposal, the women being tried for abortion were forced to self-induce abortions in their homes, in unsanitary conditions or in clandestine clinics that could not guarantee adequate procedures. If complications arise due to the conditions in which the abortion was practiced, they are then at risk of being reported by hospital staff who treat the complications. All of this highlights the risk to life, health, security of person and liberty that terminating an unwanted pregnancy represents for young, low-income women in El Salvador.\n\nA report in 2001 revealed that, after the new Penal Code went into effect in 1998, 69 cases of illegal abortions had been prosecuted. In 23 of those cases, the women involved had been turned over to the authorities by health care workers when they arrived at the hospital seeking treatment after an unsafe abortion. Most abortions had been self-induced, through the use of clothes hangers, or by the ingestion of harmful amounts of hormonal contraception pills, antacids, or misoprostol pills.\n\nIn an article published in the April 9, 2006, edition of the \"New York Times Magazine\", writer Jack Hitt explored the effect of 1998 Penal Code. The article was later discredited when it was revealed that a woman mentioned as having been sentenced to 30 years in prison for an abortion, Carmen Climaco, had been jailed for homicide of a full-term infant. In fact, Karina del Carmen Herrera Climaco had given birth at home and then began to bleed heavily. (The pregnancy was almost certainly unintentional, as it occurred after a tubal ligation.) Her mother called police to take her to hospital. While she was in hospital, police searched her home and found a lifeless infant. Medical examination failed to determine if the infant was born alive or dead, nor a cause of death. Nevertheless, a sentence of aggravated homicide was passed, separating her from her three children. Almost 8 years later, thanks to the efforts of a group of activists and national and international attorneys, Karina’s sentence was reviewed. It was annulled and she was released. She has not been compensated for her eight years of incarceration.\n\nAnother 30-year sentence was passed out for an apparent miscarriage, in August 2008, by the Tribunal of San Francisco Gotera in the department of Morazan. María Edis Hernández Méndez de Castro, 30, was a single mother with 4 children when she found out she was pregnant. Maria told her family that she was pregnant, even though she didn't know how far along the pregnancy was. During the pregnancy, Maria felt a pain and went to the bathroom in her home at which time she suffered labor complications and passed out. She regained consciousness in the National Hospital of San Francisco. The doctor that treated Maria reported her to the police on suspicion of having an abortion. She was convicted of aggravated homicide and sentenced to thirty years in prison. Two more cases of women sentenced to lengthy prison terms for what might have been abortions or miscarriages are detailed in the same report.\n\nThe lack of contraceptive information and the ban on abortion put women's lives at risk, particularly when they are young. One-third of the women giving birth are 19 or younger, and a handful are 10 – 14 years old.\n\nIn 2013, a case arose of a pregnant 22-year-old woman, identified as \"Beatriz,\" who, due to lupus aggravated by loss of kidney function, was told by doctors that she would likely die if she gave birth and that the child, due to its anencephaly, would likely have only a few hours of life. Her doctors requested permission from the government to perform an abortion because the fetus was nonviable and Beatriz was likely to die. When the case came before the Supreme Court, the court denied Beatriz's request.\n\nShe was given permission to have an early caesarean section, the same procedure that would have been used had the court ruled in her favor. Legally, this was not an abortion because the fetus was incubated and given fluids. The child died five hours after the procedure. Beatriz was able to recover and leave the hospital. The case was identified by reproductive rights activists as evidence of a lack of progress towards the goals of the 1994 International Conference on Population and Development, which affirmed reproductive rights as human rights and emphasized the importance of reducing maternal mortality.\n\n\n"}
{"id": "58255313", "url": "https://en.wikipedia.org/wiki?curid=58255313", "title": "Alexander Gordon (physician)", "text": "Alexander Gordon (physician)\n\nAlexander Gordon (1752-1799) was a renowned Edwardian obstetrician best known for being the first person to describe the infectious nature of childbirth fever in his paper of 1795 entitled \"Treatise on the Epidemic Puerperal Fever of Aberdeen\". His remarkable paper gave insights into the contagious nature of puerperal fever more than half a century before Ignaz Semmelweis and before the science of bacteria was known about. \nGordon was born in 1752 in Strachan, Scotland. He studied medicine at the University of Aberdeen's Marischal College.\n"}
{"id": "25071634", "url": "https://en.wikipedia.org/wiki?curid=25071634", "title": "America's Health Rankings", "text": "America's Health Rankings\n\nAmerica's Health Rankings started in 1990 and is the longest-running annual assessment of the nation’s health on a state-by-state basis. It is founded on the World Health Organization holistic definition of health, which says health is a state of complete physical, mental, and social well-being and not merely the absence of disease or infirmity. America’s Health Rankings is a partnership of the United Health Foundation, and the American Public Health Association. \nAmerica's Health Rankings releases two yearly reports, one on the health of the general population in the 50 US states plus a senior report on the population aged 65 and older in each state. Both reports include some health metrics that are stratified by race/ethnicity, gender, age, education, place of residence, and economic status. State rankings are based on a methodology approved by a Scientific Advisory Committee. This methodology balances the contributions of health determinants—including 1) Behaviors; 2) Community and Environment; 3) Policy pertaining to our health care system, government, and numerous prevention programs; and 4) Clinical Care received—and Outcomes such as diabetes and deaths from cardiovascular disease.\n\nThe ultimate purpose of the two reports is to improve the health of the US population by providing information that stimulates individuals, elected officials, health care professionals, public health professionals, employers, educators, and communities to act and create change. The publication of the rankings stimulates conversations concerning health in each state and across the nation. The fundamental conviction of America’s Health Rankings is that each person in his or her capacity as an employee, employer, educator, student, voter, community volunteer, health care professional, public health professional, or elected official can contribute to the advancement of the health of his or her state.\n\nAmerica’s Health Rankings was first published in 1990 and ranked the 50 US states using 16 health measures. The 2014 25th anniversary edition of the report used 27 Core Measures and 22 Supplemental Measures to evaluate the health of each state and the nation. America’s Health Rankings Senior Report started in 2013 and used 34 Core Measures as well as five Supplemental Measures.\n\nIn 2002 United Health Foundation and the American Public Health Association commissioned the University of North Carolina at Chapel Hill School of Public Health to do an ongoing review of America’s Health Rankings. The Scientific Advisory Committee, currently led by Anna Schenck, PhD, MSPH, formed as a result and was charged with recommending improvements that maintain the value of the comparative, longitudinal information. Improvements also reflect the evolving role and science of public health as well as include adding and refining health measures as they become available and acceptable. \nThe Scientific Advisory Committee includes representatives from local health departments, the American Public Health Association, current and former state health officers, and experts from many academic disciplines.\n\nFour primary considerations drive the design of America’s Health Rankings and the selection of each measure:\n\nAmericas Health Rankings includes in its model of health four groups of Determinants:\n\nThese four groups of measures influence health Outcomes of a state’s population, and improving these Determinants will improve Outcomes. Most measures are a combination of activities in all four groups. For example, the prevalence of smoking is one of the Behaviors strongly influenced by the Community and Environment, by public Policy including taxation and restrictions on smoking in public places, and by Clinical Care received to treat the chemical and behavioral addictions associated with tobacco.\n\nAmerica's Health Rankings employs a unique methodology developed and annually reviewed by the Scientific Advisory Committee, a panel of leading public health scholars. This methodology weighs and balances the contributions of factors such as smoking, obesity, binge drinking, high school graduation rates, children in poverty, access to care, and incidence of preventable disease to evaluate the health of a state's population. The report is based on data from the U.S. Departments of Health and Human Services, Commerce, Education and Labor; U.S. Environmental Protection Agency; the American Medical Association; the Dartmouth Atlas of Health Care Project; Centers for Disease Control and Prevention; the Administration on Aging; the National Center for Health Statistics; the Centers for Medicare and Medicaid Services; National Federation to End Senior Hunger; the National Institute for Occupational Safety and Health; the Kaiser Family Foundation; Brown University; the American Geriatrics Society; the Commonwealth Fund; and the Trust for America's Health.\n\nThe overall score for each state is calculated by adding the z scores of each measure multiplied by its percentage of total overall ranking (weight) and the effect (positively or negatively correlated) it has on health. Weights for individual metrics vary from 7.5% to 2.5%. The ranking is the ordering of each state according to value.\n\nThe measures used in the 2014 America's Health Rankings are in two subgroups: Core Measures and Supplemental Measures.\n\nCore Measures consist of health Determinants (risk factors), and health Outcomes. Determinants are actions that affect the population’s future health, while Outcomes represent what has already occurred through death, disease, or missed days due to illness. There are four groups of Determinants: Behaviors, Community and Environment, Policy, and Clinical Care. The measures in these four groups influence a state’s health Outcomes, and improving the Determinants over time will improve Outcomes.\n\nSupplemental Measures provide additional perspective on the health in a state. Supplemental Measures do not factor into a state’s overall score and ranking, but they are useful in forming a fuller understanding of the health of a state.\n\nBehaviors\nCommunity and Environment\nPolicy\nClinical Care\nOutcomes\n\nBehaviors\nChronic Disease\nClinical Care\nEconomic Environment\nOutcomes\n\nThe measures that comprise 2015 America's Health Rankings Senior Report are in two subgroups: Core Measures and Supplemental Measures.\n\nCore Measures consist of health Determinants (risk factors), and health Outcomes. Determinants are actions that affect the population’s future health, while Outcomes represent what has already occurred through death, disease, or missed days due to illness. There are four groups of Determinants: Behaviors, Community and Environment, Policy, and Clinical Care. The measures in these four groups influence a state’s health Outcomes, and improving the Determinants over time will improve Outcomes.\n\nSupplemental Measures provide additional perspective on the health in a state. Supplemental Measures do not factor into a state’s overall score and ranking, but they are useful in forming a fuller understanding of the health of a state.\n\nBehaviors\nCommunity and Environment\nPolicy\nClinical Care\n\nOutcomes\n\n\n"}
{"id": "219640", "url": "https://en.wikipedia.org/wiki?curid=219640", "title": "Animal husbandry", "text": "Animal husbandry\n\nAnimal husbandry is the branch of agriculture concerned with animals that are raised for meat, fibre, milk, eggs, or other products. It includes day-to-day care, selective breeding and the raising of livestock.\n\nHusbandry has a long history, starting with the Neolithic revolution when animals were first domesticated, from around 13,000 BC onwards, antedating farming of the first crops. By the time of early civilisations such as ancient Egypt, cattle, sheep, goats and pigs were being raised on farms.\n\nMajor changes took place in the Columbian Exchange when Old World livestock were brought to the New World, and then in the British Agricultural Revolution of the 18th century, when livestock breeds like the Dishley Longhorn cattle and Lincoln Longwool sheep were rapidly improved by agriculturalists such as Robert Bakewell to yield more meat, milk, and wool.\n\nA wide range of other species such as horse, water buffalo, llama, rabbit and guinea pig are used as livestock in some parts of the world. Insect farming, as well as aquaculture of fish, molluscs, and crustaceans, is widespread.\n\nModern animal husbandry relies on production systems adapted to the type of land available. Subsistence farming is being superseded by intensive animal farming in the more developed parts of the world, where for example beef cattle are kept in high density feedlots, and thousands of chickens may be raised in broiler houses or batteries. On poorer soil such as in uplands, animals are often kept more extensively, and may be allowed to roam widely, foraging for themselves.\n\nMost livestock are herbivores, except for pigs and chickens which are omnivores. Ruminants like cattle and sheep are adapted to feed on grass; they can forage outdoors, or may be fed entirely or in part on rations richer in energy and protein, such as pelleted cereals. Pigs and poultry cannot digest the cellulose in forage, and require cereals and other high-energy foods.\n\nThe domestication of livestock was driven by the need to have food on hand when hunting was unproductive. The desirable characteristics of a domestic animal are that it should be useful to man, should be able to thrive in his company, should breed freely and be easy to tend.\n\nDomestication was not a single event, but a process repeated at various periods in different places. Sheep and goats were the animals that accompanied the nomads in the Middle East, while cattle and pigs were associated with more settled communities.\n\nThe first wild animal to be domesticated was the dog. Half-wild dogs, perhaps starting with young individuals, may have been tolerated as scavengers and killers of vermin, and being naturally pack hunters, were predisposed to become part of the human pack and join in the hunt. Prey animals, sheep, goats, pigs and cattle, were progressively domesticated early in the history of agriculture.\n\nPigs were domesticated in Mesopotamia around 13,000 BC, and sheep followed, some time between 11,000 and 9,000 BC. Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan around 8,500 BC.\n\nA cow was a great advantage to a villager as she produced more milk than her calf needed, and her strength could be put to use as a working animal, pulling a plough to increase production of crops, and drawing a sledge, and later a cart, to bring the produce home from the field. Draught animals were first used about 4,000 BC in the Middle East, increasing agricultural production immeasurably. In southern Asia, the elephant was domesticated by 6,000 BC.\n\nFossilised chicken bones dated to 5040 BC have been found in northeastern China, far from where their wild ancestors lived in the jungles of tropical Asia, but archaeologists believe that the original purpose of domestication was for the sport of cockfighting.\n\nMeanwhile, in South America, the llama and the alpaca had been domesticated, probably before 3,000 BC, as beasts of burden and for their wool. Neither was strong enough to pull a plough which limited the development of agriculture in the New World.\n\nHorses occur naturally on the steppes of Central Asia, and their domestication, around 3,000 BC in the Black Sea and Caspian Sea region, was originally as a source of meat; use as pack animals and for riding followed. Around the same time, the wild ass was being tamed in Egypt. Camels were domesticated soon after this, with the Bactrian camel in Mongolia and the Arabian camel becoming beasts of burden. By 1000 BC, caravans of Arabian camels were linking India with Mesopotamia and the Mediterranean.\n\nIn ancient Egypt, cattle were the most important livestock, and sheep, goats, and pigs were also kept; poultry including ducks, geese, and pigeons were captured in nets and bred on farms, where they were force-fed with dough to fatten them.\n\nThe Nile provided a plentiful source of fish. Honey bees were domesticated from at least the Old Kingdom, providing both honey and wax.\n\nIn ancient Rome, all the livestock known in ancient Egypt were available. In addition, rabbits were domesticated for food by the first century BC. To help flush them out from their underground burrows, the polecat was domesticated as the ferret, its use described by Pliny the Elder.\n\nIn northern Europe, agriculture including animal husbandry went into decline when the Roman empire collapsed. Some aspects such as the herding of animals continued throughout the period. By the 11th century, the economy had recovered and the countryside was again productive.\n\nThe \"Domesday Book\" recorded every parcel of land and every animal in Britain: \"there was not one single hide, nor a yard of land, nay, moreover ... not even an ox, nor a cow, nor a swine was there left, that was not set down in [the king's] writ.\" For example, the royal manor of Earley in Berkshire, one of thousands of villages recorded in the book, had in 1086 \"2 fisheries worth [paying tax of] 7s and 6d [each year] and 20 acres of meadow [for livestock]. Woodland for [feeding] 70 pigs.\"\n\nExploration and colonisation of North and South America resulted in the introduction into Europe of such crops as maize, potatoes, sweet potatoes and manioc, while the principal Old World livestock – cattle, horses, sheep and goats – were introduced into the New World for the first time along with wheat, barley, rice and turnips.\n\nSelective breeding for desired traits was established as a scientific practice by Robert Bakewell during the British Agricultural Revolution in the 18th century. One of his most important breeding programs was with sheep. Using native stock, he was able to quickly select for large, yet fine-boned sheep, with long, lustrous wool. The Lincoln Longwool was improved by Bakewell and in turn the Lincoln was used to develop the subsequent breed, named the New (or Dishley) Leicester. It was hornless and had a square, meaty body with straight top lines. These sheep were exported widely and have contributed to numerous modern breeds. Under his influence, English farmers began to breed cattle for use primarily as beef. Long-horned heifers were crossed with the Westmoreland bull to create the Dishley Longhorn.\n\nThe semi-natural, unfertilised pastures formed by traditional agricultural methods in Europe were managed by grazing and mowing. As the ecological impact of this land management strategy is similar to the impact of such natural disturbances as a wildfire, this agricultural system shares many beneficial characteristics with a natural habitat, including the promotion of biodiversity. This strategy is declining in Europe today due to the intensification of agriculture. The mechanized and chemical methods used are causing biodiversity to decline.\n\nTraditionally, animal husbandry was part of the subsistence farmer's way of life, producing not only the food needed by the family but also the fuel, fertiliser, clothing, transport and draught power. Killing the animal for food was a secondary consideration, and wherever possible its products, such as wool, eggs, milk and blood (by the Maasai) were harvested while the animal was still alive.\nIn the traditional system of transhumance, people and livestock moved seasonally between fixed summer and winter pastures; in montane regions the summer pasture was up in the mountains, the winter pasture in the valleys.\n\nAnimals can be kept extensively or intensively. Extensive systems involve animals roaming at will, or under the supervision of a herdsman, often for their protection from predators. Ranching in the Western United States involves large herds of cattle grazing widely over public and private lands.\n\nSimilar cattle stations are found in South America, Australia and other places with large areas of land and low rainfall. Similar ranching systems have been used for sheep, deer, ostrich, emu, llama and alpaca.\n\nIn the uplands of the United Kingdom, sheep are turned out on the fells in spring and graze the abundant mountain grasses untended, being brought to lower altitudes late in the year, with supplementary feeding being provided in winter. In rural locations, pigs and poultry can obtain much of their nutrition from scavenging, and in African communities, hens may live for months without being fed, and still produce one or two eggs a week.\n\nAt the other extreme, in the more developed parts of the world, animals are often intensively managed; dairy cows may be kept in zero-grazing conditions with all their forage brought to them; beef cattle may be kept in high density feedlots; pigs may be housed in climate-controlled buildings and never go outdoors; poultry may be reared in barns and kept in cages as laying birds under lighting-controlled conditions. In between these two extremes are semi-intensive, often family-run farms where livestock graze outside for much of the year, silage or hay is made to cover the times of year when the grass stops growing, and fertiliser, feed, and other inputs are brought onto the farm from outside.\n\nAnimals used as livestock are predominantly herbivorous, the main exception being the pig which is an omnivore. The herbivores can be divided into \"concentrate selectors\" which selectively feed on seeds, fruits and highly nutritious young foliage, \"grazers\" which mainly feed on grass, and \"intermediate feeders\" which choose their diet from the whole range of available plant material. Cattle, sheep, goats, deer and antelopes are ruminants; they digest food in two steps, chewing and swallowing in the normal way, and then regurgitating the semidigested cud to chew it again and thus extract the maximum possible food value.\nThe dietary needs of these animals is mostly met by eating grass. Grasses grow from the base of the leaf-blade, enabling it to thrive even when heavily grazed or cut.\n\nIn many climates grass growth is seasonal, for example in the temperate summer or tropical rainy season, so some areas of the crop are set aside to be cut and preserved, either as hay (dried grass), or as silage (fermented grass). Other forage crops are also grown and many of these, as well as crop residues, can be ensiled to fill the gap in the nutritional needs of livestock in the lean season.\n\nExtensively reared animals may subsist entirely on forage, but more intensively kept livestock will require energy and protein-rich foods in addition. Energy is mainly derived from cereals and cereal by-products, fats and oils and sugar-rich foods, while protein may come from fish or meat meal, milk products, legumes and other plant foods, often the by-products of vegetable oil extraction.\nPigs and poultry are non-ruminants and unable to digest the cellulose in grass and other forages, so they are fed entirely on cereals and other high-energy foodstuffs. The ingredients for the animals' rations can be grown on the farm or can be bought, in the form of pelleted or cubed, compound foodstuffs specially formulated for the different classes of livestock, their growth stages and their specific nutritional requirements. Vitamins and minerals are added to balance the diet. Farmed fish are usually fed pelleted food.\n\nThe breeding of farm animals seldom occurs spontaneously but is managed by farmers with a view to encouraging certain traits that are seen as desirable. These include hardiness, prolificness, mothering abilities, fast growth rates, low feed consumption per unit of growth, better body proportions, higher yields, better fibre qualities and other characteristics. Undesirable traits such as health defects, aggressiveness or lack of docility are selected against.\n\nSelective breeding has been responsible for some large increases in productivity. In 2007, a typical broiler chicken at eight weeks old was 4.8 times as heavy as a bird of similar age in 1957. In the thirty years to 2007, the average milk yield of a dairy cow in the United States nearly doubled.\n\nTechniques such as artificial insemination and embryo transfer are frequently used today, not only as methods to guarantee that females breed regularly but also to help improve herd genetics. This may be done by transplanting embryos from high-quality females into lower-quality surrogate mothers – freeing up the higher-quality mother to be reimpregnated. This practice vastly increases the number of offspring which may be produced by a small selection of the best quality parent animals.\n\nOn one hand, this improves the ability of the animals to convert feed to meat, milk, or fiber more efficiently, and improve the quality of the final product. On the other, it decreases genetic diversity, increasing the severity of certain disease outbreaks among other risks.\n\nGood husbandry, proper feeding, and hygiene are the main contributors to animal health on the farm, bringing economic benefits through maximised production. When, despite these precautions, animals still become sick, they are treated with veterinary medicines, by the farmer and the veterinarian. In the European Union, when farmers treat their own animals, they are required to follow the guidelines for treatment and to record the treatments given.\nAnimals are susceptible to a number of diseases and conditions that may affect their health. Some, like classical swine fever and scrapie are specific to one type of stock, while others, like foot-and-mouth disease affect all cloven-hoofed animals.\n\nWhere the condition is serious, governments impose regulations on import and export, on the movement of stock, quarantine restrictions and the reporting of suspected cases. Vaccines are available against certain diseases, and antibiotics are widely used where appropriate. At one time, antibiotics were routinely added to certain compound foodstuffs to promote growth, but this practice is now frowned on in many countries because of the risk that it may lead to antibiotic resistance.\n\nAnimals living under intensive conditions are particularly prone to internal and external parasites; increasing numbers of sea lice are affecting farmed salmon in Scotland. Reducing the parasite burdens of livestock results in increased productivity and profitability.\n\nGovernments are particularly concerned with zoonoses, diseases that humans may acquire from animals. Wild animal populations may harbour diseases that can affect domestic animals which may acquire them as a result of insufficient biosecurity. An outbreak of Nipah virus in Malaysia in 1999 was traced back to pigs becoming ill after contact with fruit-eating flying foxes, their faeces and urine. The pigs in turn passed the infection to humans. Avian flu H5N1 is present in wild bird populations and can be carried large distances by migrating birds. This virus is easily transmissible to domestic poultry, and to humans living in close proximity with them. Other infectious diseases affecting wild animals, farm animals and humans include rabies, leptospirosis, brucellosis, tuberculosis and trichinosis.\n\nThere is no single universally agreed definition of which species are livestock. Widely agreed types of livestock include cattle for beef and dairy, sheep, goats, pigs, and poultry. Various other species are sometimes considered livestock, such as horses, while poultry birds are sometimes excluded. In some parts of the world, livestock includes species such as buffalo, and the South American camelids, the alpaca and llama. Some authorities use much broader definitions to include fish in aquaculture, micro-livestock such as rabbits and rodents like guinea pigs, as well as insects from honey bees to crickets raised for human consumption.\n\nAnimals are raised for a wide variety of products, principally meat, wool, milk, and eggs, but also including tallow, isinglass and rennet. Animals are also kept for more specialised purposes, such as to produce vaccines and antiserum (containing antibodies) for medical use. Where fodder or other crops are grown alongside animals, manure can serve as a fertiliser, returning minerals and organic matter to the soil in a semi-closed organic system.\n\nAlthough all mammals produce milk to nourish their young, the cow is predominantly used throughout the world to produce milk and milk products for human consumption. Other animals used to a lesser extent for this purpose include sheep, goats, camels, buffaloes, yaks, reindeer, horses and donkeys.\n\nAll these animals have been domesticated over the centuries, being bred for such desirable characteristics as fecundity, productivity, docility and the ability to thrive under the prevailing conditions. Whereas in the past, cattle had multiple functions, modern dairy cow breeding has resulted in specialised Holstein Friesian-type animals that produce large quantities of milk economically. Artificial insemination is widely available to allow farmers to select for the particular traits that suit their circumstances.\n\nWhereas in the past, cows were kept in small herds on family farms, grazing pastures and being fed hay in winter, nowadays there is a trend towards larger herds, more intensive systems, the feeding of silage and \"zero grazing\", a system where grass is cut and brought to the cow, which is housed year-round.\n\nIn many communities, milk production is only part of the purpose of keeping an animal which may also be used as a beast of burden or to draw a plough, or for the production of fibre, meat and leather, with the dung being used for fuel or for the improvement of soil fertility. Sheep and goats may be favoured for dairy production in climates and conditions that do not suit dairy cows.\n\nMeat, mainly from farmed animals, is a major source of dietary protein around the world, averaging about 8% of man's energy intake. The actual types eaten depend on local preferences, availability, cost and other factors, with cattle, sheep, pigs and goats being the main species involved. Cattle generally produce a single offspring annually which takes more than a year to mature; sheep and goats often have twins and these are ready for slaughter in less than a year; pigs are more prolific, producing more than one litter of up to about 11 piglets each year. Horses, donkeys, deer, buffalo, llamas, alpacas, guanacos and vicunas are farmed for meat in various regions. Some desirable traits of animals raised for meat include fecundity, hardiness, fast growth rate, ease of management and high food conversion efficiency. About half of the world's meat is produced from animals grazing on open ranges or on enclosed pastures, the other half being produced intensively in various factory-farming systems; these are mostly cows, pigs or poultry, and often reared indoors, typically at high densities.\n\nPoultry, kept for their eggs and for their meat, include chickens, turkeys, geese and ducks. The great majority of laying birds used for egg production are chickens. Methods for keeping layers range from free-range systems, where the birds can roam as they will but are housed at night for their own protection, through semi-intensive systems where they are housed in barns and have perches, litter and some freedom of movement, to intensive systems where they are kept in cages. The battery cages are arranged in long rows in multiple tiers, with external feeders, drinkers, and egg collection facilities. This is the most labour saving and economical method of egg production but has been criticised on animal welfare grounds as the birds are unable to exhibit their normal behaviours.\n\nIn the developed world, the majority of the poultry reared for meat is raised indoors in big sheds, with automated equipment under environmentally controlled conditions. Chickens raised in this way are known as broilers, and genetic improvements have meant that they can be grown to slaughter weight within six or seven weeks of hatching. Newly hatched chicks are restricted to a small area and given supplementary heating. Litter on the floor absorbs the droppings and the area occupied is expanded as they grow. Feed and water is supplied automatically and the lighting is controlled. The birds may be harvested on several occasions or the whole shed may be cleared at one time.\n\nA similar rearing system is usually used for turkeys, which are less hardy than chickens, but they take longer to grow and are often moved on to separate fattening units to finish. Ducks are particularly popular in Asia and Australia and can be killed at seven weeks under commercial conditions.\n\nAquaculture has been defined as \"the farming of aquatic organisms including fish, molluscs, crustaceans and aquatic plants and implies some form of intervention in the rearing process to enhance production, such as regular stocking, feeding, protection from predators, etc. Farming also implies individual or corporate ownership of the stock being cultivated.\" In practice it can take place in the sea or in freshwater, and be extensive or intensive. Whole bays, lakes or ponds may be devoted to aquaculture, or the farmed animal may be retained in cages (fish), artificial reefs, racks or strings (shellfish). Fish and prawns can be cultivated in rice paddies, either arriving naturally or being introduced, and both crops can be harvested together.\n\nFish hatcheries provide larval and juvenile fish, crustaceans and shellfish, for use in aquaculture systems. When large enough these are transferred to growing-on tanks and sold to fish farms to reach harvest size. Some species that are commonly raised in hatcheries include shrimps, prawns, salmon, tilapia, oysters and scallops. Similar facilities can be used to raise species with conservation needs to be released into the wild, or game fish for restocking waterways. Important aspects of husbandry at these early stages include selection of breeding stock, control of water quality and nutrition. In the wild, there is a massive amount of mortality at the nursery stage; farmers seek to minimise this while at the same time maximising growth rates.\n\nBees have been kept in hives since at least the First Dynasty of Egypt, five thousand years ago, and man had been harvesting honey from the wild long before that. Fixed comb hives are used in many parts of the world and are made from any locally available material. In more advanced economies, where modern strains of domestic bee have been selected for docility and productiveness, various designs of hive are used which enable the combs to be removed for processing and extraction of honey. Quite apart from the honey and wax they produce, honey bees are important pollinators of crops and wild plants, and in many places hives are transported around the countryside to assist in pollination.\n\nSericulture, the rearing of silkworms, was first adopted by the Chinese during the Shang dynasty. The only species farmed commercially is the domesticated silkmoth. When it spins its cocoon, each larva produces an exceedingly long, slender thread of silk. The larvae feed on mulberry leaves and in Europe, only one generation is normally raised each year as this is a deciduous tree. In China, Korea and Japan however, two generations are normal, and in the tropics, multiple generations are expected. Most production of silk occurs in the Far East, with a synthetic diet being used to rear the silkworms in Japan.\n\nInsects form part of the human diet in many cultures. In Thailand, crickets are farmed for this purpose in the north of the country, and palm weevil larvae in the south. The crickets are kept in pens, boxes or drawers and fed on commercial pelleted poultry food, while the palm weevil larvae live on cabbage palm and sago palm trees, which limits their production to areas where these trees grow. Another delicacy of this region is the bamboo caterpillar, and the best rearing and harvesting techniques in semi-natural habitats are being studied.\n\nAnimal husbandry has a significant impact on the world environment. It is responsible for somewhere between 20 and 33% of the fresh water usage in the world, and livestock, and the production of feed for them, occupy about a third of the earth's ice-free land. Livestock production is a contributing factor in species extinction, desertification, and habitat destruction. Animal agriculture contributes to species extinction in various ways. Habitat is destroyed by clearing forests and converting land to grow feed crops and for animal grazing, while predators and herbivores are frequently targeted and hunted because of a perceived threat to livestock profits; for example, animal husbandry is responsible for up to 91% of the deforestation in the Amazon region. In addition, livestock produce greenhouse gases. Cows produce some 570 million cubic metres of methane per day, that accounts for from 35 to 40% of the overall methane emissions of the planet. Livestock is responsible for 65% of all human-related emissions of the powerful and long-lived greenhouse gas nitrous oxide. As a result, ways of mitigating animal husbandry's environmental impact are being studied. Strategies include using biogas from manure.\n\nSince the 18th century, people have become increasingly concerned about the welfare of farm animals. Possible measures of welfare include longevity, behavior, physiology, reproduction, freedom from disease, and freedom from immunosuppression. Standards and laws for animal welfare have been created worldwide, broadly in line with the most widely held position in the western world, a form of utilitarianism: that it is morally acceptable for humans to use non-human animals, provided that no unnecessary suffering is caused, and that the benefits to humans outweigh the costs to the livestock. An opposing view is that animals have rights, should not be regarded as property, and should never be used by humans. Live export of animals has risen to meet increased global demand for livestock such as in the Middle East. Animal rights activists have objected to long-distance transport of animals; one result was the banning of live exports from New Zealand in 2003.\n\nSince the 18th century, the farmer John Bull has represented English national identity, first in John Arbuthnot's political satires, and soon afterwards in cartoons by James Gillray and others including John Tenniel. He likes food, beer, dogs, horses, and country sports; he is practical and down to earth, and anti-intellectual.\n\nFarm animals are widespread in books and songs for children; the reality of animal husbandry is often distorted, softened, or idealized, giving children an almost entirely fictitious account of farm life. The books often depict a rural idyll of happy animals free to roam in attractive countryside, which is completely at odds with the realities of the impersonal, mechanized activities involved in modern intensive farming.\n\nPigs, for example, appear in several of Beatrix Potter's \"little books\", as Piglet in A. A. Milne's Winnie the Pooh stories, and somewhat more darkly (with a hint of animals going to slaughter) as Babe in Dick King-Smith's \"The Sheep-Pig\", and as Wilbur in \"Charlotte's Web\". Pigs tend to be \"bearers of cheerfulness, good humour and innocence\". Many of these books are completely anthropomorphic, dressing farm animals in clothes and having them walk on two legs, live in houses, and perform human activities. The children's song \"Old MacDonald Had a Farm\" describes a farmer named MacDonald and the various animals he keeps, celebrating the noises they each make.\nMany urban children experience animal husbandry for the first time at a petting farm; in Britain, some five million people a year visit a farm of some kind. This presents some risk of infection, especially if children handle animals and then fail to wash their hands; a strain of \"E. coli\" infected 93 people who had visited an interactive farm in an outbreak in 2009. Historic farms such as those in the United States offer farmstays and \"a carefully curated version of farming to those willing to pay for it\", sometimes giving visitors a romanticised image of a pastoral ideal from an unspecified time in the pre-industrial past.\n\n\n\n"}
{"id": "9736652", "url": "https://en.wikipedia.org/wiki?curid=9736652", "title": "Auditory masking", "text": "Auditory masking\n\nAuditory masking occurs when the perception of one sound is affected by the presence of another sound.\n\nAuditory masking in the frequency domain is known as simultaneous masking, frequency masking or spectral masking. Auditory masking in the time domain is known as temporal masking or non-simultaneous masking.\n\nThe \"unmasked threshold\" is the quietest level of the signal which can be perceived without a masking signal present. The \"masked threshold\" is the quietest level of the signal perceived when combined with a specific masking noise. The amount of masking is the difference between the masked and unmasked thresholds.\nGelfand provides a basic example. Let us say that for a given individual, the sound of a cat scratching a post in an otherwise quiet environment is first audible at a level of 10 dB SPL. However, in the presence of a masking noise (for example, a vacuum cleaner that is running simultaneously) that same individual cannot detect the sound of the cat scratching unless the level of the scratching sound is at least 26 dB SPL. We would say that the unmasked threshold for that individual for the target sound (i.e., the cat scratching) is 10 dB SPL, while the masked threshold is 26 dB SPL. The amount of masking is simply the difference between these two thresholds: 16 dB.\n\nThe amount of masking will vary depending on the characteristics of both the target signal and the masker, and will also be specific to an individual listener. While the person in the example above was able to detect the cat scratching at 26 dB SPL, another person may not be able to hear the cat scratching while the vacuum was on until the sound level of the cat scratching was increased to 30 dB SPL (thereby making the amount of masking for the second listener 20 dB).\n\nSimultaneous masking occurs when a sound is made inaudible by a noise or unwanted sound of the same duration as the original sound. For example, a powerful spike at 1 kHz will tend to mask out a lower-level tone at 1.1 kHz. Also, two sine tones at 440 and 450 Hz can be perceived clearly when separated. They cannot be perceived clearly when presented simultaneously.\n\nIf two sounds of two different frequencies are played at the same time, two separate sounds can often be heard rather than a combination tone. The ability to hear frequencies separately is known as \"frequency resolution\" or \"frequency selectivity\". When signals are perceived as a combination tone, they are said to reside in the same \"critical bandwidth\". This effect is thought to occur due to filtering within the cochlea, the hearing organ in the inner ear. A complex sound is split into different frequency components and these components cause a peak in the pattern of vibration at a specific place on the cilia inside the basilar membrane within the cochlea. These components are then coded independently on the auditory nerve which transmits sound information to the brain. This individual coding only occurs if the frequency components are different enough in frequency, otherwise they are in the same critical band and are coded at the same place and are perceived as one sound instead of two.\n\nThe filters that distinguish one sound from another are called auditory filters, listening channels or critical bandwidths. Frequency resolution occurs on the basilar membrane due to the listener choosing a filter which is centered over the frequency they expect to hear, the signal frequency. A sharply tuned filter has good frequency resolution as it allows the center frequencies through but not other frequencies (Pickles 1982). Damage to the cochlea and the outer hair cells in the cochlea can impair the ability to tell sounds apart (Moore 1986). This explains why someone with a hearing loss due to cochlea damage would have more difficulty than a normal hearing person in distinguishing between different consonants in speech.\n\nMasking illustrates the limits of frequency selectivity. If a signal is masked by a masker with a different frequency to the signal, then the auditory system was unable to distinguish between the two frequencies. By experimenting with conditions where one sound can mask a previously heard signal, the frequency selectivity of the auditory system can be tested.\n\nHow effective the masker is at raising the threshold of the signal depends on the frequency of the signal and the frequency of the masker. The graphs in Figure B are a series of masking patterns, also known as masking audiograms. Each graph shows the amount of masking produced at each masker frequency shown at the top corner, 250, 500, 1000 and 2000 Hz. For example, in the first graph the masker is presented at a frequency of 250 Hz at the same time as the signal. The amount the masker increases the threshold of the signal is plotted and this is repeated for different signal frequencies, shown on the X axis. The frequency of the masker is kept constant. The masking effect is shown in each graph at various masker sound levels.\n\nFigure B shows along the Y axis the amount of masking. The greatest masking is when the masker and the signal are the same frequency and this decreases as the signal frequency moves further away from the masker frequency. This phenomenon is called on-frequency masking and occurs because the masker and signal are within the same auditory filter (Figure C). This means that the listener cannot distinguish between them and they are perceived as one sound with the quieter sound masked by the louder one (Figure D).\n\nThe amount the masker raises the threshold of the signal is much less in off-frequency masking, but it does have some masking effect because some of the masker overlaps into the auditory filter of the signal (Figure E)\n\nOff-frequency masking\nrequires the level of the masker to be greater in order to have a masking effect; this is shown in Figure F. This is because only a certain amount of the masker overlaps into the auditory filter of the signal and more masker is needed to cover the signal.\n\nThe masking pattern changes depending on the frequency of the masker and the intensity (Figure B). For low levels on the 1000 Hz graph, such as the 20-40 dB range, the curve is relatively parallel. As the masker intensity increases the curves separate, especially for signals at a frequency higher than the masker. This shows that there is a spread of the masking effect upward in frequency as the intensity of the masker is increased. The curve is much shallower in the high frequencies than in the low frequencies. This flattening is called upward spread of masking and is why an interfering sound masks high frequency signals much better than low frequency signals.\n\nFigure B also shows that as the masker frequency increases, the masking patterns become increasingly compressed. This demonstrates that high frequency maskers are only effective over a narrow range of frequencies, close to the masker frequency. Low frequency maskers on the other hand are effective over a wide frequency range.\n\nHarvey Fletcher carried out an experiment to discover how much of a band of noise contributes to the masking of a tone. In the experiment, a fixed tone signal had various bandwidths of noise centered on it. The masked threshold was recorded for each bandwidth. His research showed that there is a critical bandwidth of noise which causes the maximum masking effect and energy outside that band does not affect the masking. This can be explained by the auditory system having an auditory filter which is centered over the frequency of the tone. The bandwidth of the masker that is within this auditory filter effectively masks the tone but the masker outside of the filter has no effect (Figure G).\n\nThis is used in MP3 files to reduce the size of audio files. Parts of the signals which are outside the critical bandwidth are represented with reduced precision. The parts of the signals which are perceived by the listener are reproduced with higher fidelity.\n\nVarying intensity levels can also have an effect on masking. The lower end of the filter becomes flatter with increasing decibel level, whereas the higher end becomes slightly steeper. Changes in slope of the high frequency side of the filter with intensity are less consistent than they are at low frequencies. At the medium frequencies (1–4 kHz) the slope increases as intensity increases, but at the low frequencies there is no clear inclination with level and the filters at high center frequencies show a small decrease in slope with increasing level. The sharpness of the filter depends on the input level and not the output level to the filter. The lower side of the auditory filter also broadens with increasing level. These observations are illustrated in Figure H.\n\nTemporal masking or non-simultaneous masking occurs when a sudden stimulus sound makes inaudible other sounds which are present immediately preceding or following the stimulus. Masking which obscures a sound immediately preceding the masker is called \"backward masking\" or \"pre-masking\" and masking which obscures a sound immediately following the masker is called \"forward masking\" or \"post-masking\". Temporal masking's effectiveness attenuates exponentially from the onset and offset of the masker, with the onset attenuation lasting approximately 20 ms and the offset attenuation lasting approximately 100 ms.\n\nSimilar to simultaneous masking, temporal masking reveals the frequency analysis performed by the auditory system; forward masking thresholds for complex harmonic tones (e.g., a sawtooth probe with a fundamental frequency of 500 Hz) exhibit threshold peaks (i.e., high masking levels) for frequency bands centered on the first several harmonics. In fact, auditory bandwidths measured from forward masking thresholds are narrower and more accurate than those measured using simultaneous masking.\n\nTemporal masking should not be confused with the ear's acoustic reflex, an involuntary response in the middle ear that is activated to protect the ear's delicate structures from loud sounds.\n\nIpsilateral (\"same side\") masking is not the only condition where masking takes place. Another situation where masking occurs is called contralateral (\"other side\") simultaneous masking. In this case, the instance where the signal might be audible in one ear but is deliberately taken away by applying a masker to the other ear.\n\nThe last situation where masking occurs is called central masking. This refers to the case where a masker causes a threshold elevation. This can be in the absence of, or in addition to, another effect and is due to interactions within the central nervous system between the separate neural inputs obtained from the masker and the signal.\n\nExperiments have been carried out to see the different masking effects when using a masker which is either in the form of a narrow band noise or a sinusoidal tone.\n\nWhen a sinusoidal signal and a sinusoidal masker (tone) are presented simultaneously the envelope of the combined stimulus fluctuates in a regular pattern described as beats. The fluctuations occur at a rate defined by the difference between the frequencies of the two sounds. If the frequency difference is small then the sound is perceived as a periodic change in the loudness of a single tone. If the beats are fast then this can be described as a sensation of roughness. When there is a large frequency separation, the two components are heard as separate tones without roughness or beats. Beats can be a cue to the presence of a signal even when the signal itself is not audible. The influence of beats can be reduced by using a narrowband noise rather than a sinusoidal tone for either signal or masker.\n\nThere are many different mechanisms of masking, one being suppression. This is when there is a reduction of a response to a signal due to the presence of another. This happens because the original neural activity caused by the first signal is reduced by the neural activity of the other sound.\n\nCombination tones are products of a signal and a masker. This happens when the two sounds interact causing new sound, which can be more audible than the original signal. This is caused by the non linear distortion that happens in the ear. For example, the combination tone of two maskers can be a better masker than the two original maskers alone.\n\nThe sounds interact in many ways depending on the difference in frequency between the two sounds. The most important two are cubic difference tones and quadratic difference tones.\n\nCubic difference tones are calculated by the sum\n\n2F1 – F2\n\nThese are audible most of the time and especially when the level of the original tone is low. Hence they have a greater effect on psychoacoustic tuning curves than quadratic difference tones.\n\nQuadratic difference tones are the result of\n\nF2 – F1\n\nThis happens at relatively high levels hence have a lesser effect on psychoacoustic tuning curves.\n\nCombination tones can interact with primary tones resulting in secondary combination tones due to being like their original primary tones in nature, stimulus like. An example of this is\n\n3F1 – 2F2\n\nSecondary combination tones are again similar to the combination tones of the primary tone.\n\nOff frequency listening is when a listener chooses a filter just lower than the signal frequency to improve their auditory performance. This “off frequency” filter reduces the level of the masker more than the signal at the output level of the filter, which means they can hear the signal more clearly hence causing an improvement of auditory performance.\nAuditory masking is used in tinnitus maskers to suppress annoying ringing, hissing, or buzzing or tinnitus often associated with hearing loss. It is also used in various kinds of audiometry, including pure tone audiometry, and the standard hearing test to test each ear unilaterally and to test speech recognition in the presence of partially masking noise.\n\nAuditory masking is exploited to perform data compression for sound signals (MP3).\n\n\n"}
{"id": "10632548", "url": "https://en.wikipedia.org/wiki?curid=10632548", "title": "Beef hormone controversy", "text": "Beef hormone controversy\n\nThe Beef Hormone Dispute is one of the most intractable agricultural controversies since the establishment of the World Trade Organization (WTO).\n\nIt has sometimes been called the \"beef war\" in the media, similarly to the UK-EU Beef war over the \"mad cow disease\" issue, creating some confusion, since these two wars overlapped in time.\n\nIn 1989, the European Union banned the importation of meat that contained artificial beef growth hormones approved for use and administered in the United States. Originally, the ban covered six such hormones but was amended in 2003 to permanently ban one hormone —estradiol-17β — while provisionally banning the use of the five others. WTO rules permit such bans, but only where a signatory presents valid scientific evidence that the ban is a health and safety measure. Canada and the United States opposed this ban, taking the EU to the WTO Dispute Settlement Body. In 1997, the WTO Dispute Settlement Body ruled against the EU.\n\nThe hormones banned by the EU in cattle farming were estradiol, progesterone, testosterone, zeranol, melengestrol acetate and trenbolone acetate. Of these, the first three are synthetic versions of endogenous hormones that are naturally produced in humans and animals, and also occur in a wide range of foods, whereas the last two are synthetic and not naturally occurring, which mimic the behaviour of endogenous hormones. Zeranol (alpha-zearalanol) is produced semi-synthetically, but it also occurs naturally in some foods. It is one of several derivatives of zearalenone produced by certain Fusarium. Although its occurrence in animal products can be partly due to its ingestion in such feeds, alpha-zearalanol can also be produced endogenously in ruminants that have ingested zearalenone and some zearalenone derivatives in such feeds. The EU did not impose an absolute ban. Under veterinary supervision, cattle farmers were permitted to administer the synthetic versions of natural hormones for cost-reduction and possibly therapeutic purposes, such as synchronising the oestrus cycles of dairy cows. All six hormones were licensed for use in the US and in Canada.\n\nUnder the Agreement on the Application of Sanitary and Phytosanitary Measures, signatories have the right to impose restrictions on health and safety grounds subject to scientific analysis. The heart of the Beef Hormone Dispute was the fact that all risk analysis is statistical in nature, and thus unable to determine with certainty the absence of health risks, and consequent disagreement between the US and Canada beef producers on the one hand, who believed that a broad scientific consensus existed that beef produced with the use of hormones was safe, and the EU on the other, which asserted that it was not safe.\n\nThe use of these hormones in cattle farming had been studied scientifically in North America for 50 years prior to the ban, and there had been widespread long-term use in over 20 countries. Canada and the United States asserted that this provided empirical evidence both of long-term safety and of scientific consensus.\n\nThe EU ban was not, as it was portrayed to rural constituencies in the US and Canada, protectionism. The EU had already had other measures that effectively restricted the import of North American beef. In the main, the North American product that the new ban affected, which existing barriers did not, was edible offal.\n\nIt was not producers asking for protectionist measures who were pressuring the EU, but consumers, expressing concerns over the safety of hormone use. There were a series of widely publicized \"hormone scandals\" in Italy in the late 1970s and early 1980s. The first, in 1977, was signs of the premature onset of puberty in northern Italian schoolchildren, where investigators had cast suspicion in the direction of school lunches that had used meat farmed with the (illegal) use of growth hormones. No concrete evidence linking premature puberty to growth hormones was found, in part because no samples of the suspect meals were available for analysis. But public anger arose at the use of such meat production techniques, to be further fanned by the discovery in 1980 of the (again illegal) presence of diethylstilbestrol (DES), another synthetic hormone, in veal-based baby foods.\n\nThe scientific evidence for health risks associated with the use of growth hormones in meat production was, at best, scant. However, consumer lobbyist groups were far more able to successfully influence the European Parliament to enact regulations in the 1980s than producer lobbyist groups were, and had far more influence over public perceptions. This is in contrast with the US at the time, where there was little interest from consumer organizations in the subject prior to the 1980s, and regulations were driven by a well-organized coalition of export-oriented industry and farming interests, who were only opposed by traditional farming groups.\n\nUntil 1980, the use of growth hormones, both endogenous and exogenous, was completely prohibited in (as noted above) Italy, Denmark, the Netherlands, and Greece. Germany, the largest beef producer in the EU at the time, prohibited just the use of exogenous growth hormones. The five other member countries, including the second and third largest beef producers, France and the United Kingdom, permitted their use. (The use of growth hormones was particularly common in the U.K., where beef production was heavily industrialized.) This had resulted in several disputes amongst member countries, with the countries that had no prohibitions arguing that the restrictions by the others acted as non-tariff trade barriers. But in response to the public outcry in 1980, in combination with the contemporary discovery that DES was a teratogen, the EU began to issue regulations, beginning with a directive prohibiting the use of stilbenes and thyrostatics issued by the European Community Council of Agriculture Ministers in 1980, and the commissioning of a scientific study into the use of estradiol, testosterone, progesterone, trenbolone, and zeranol in 1981.\n\nThe European Consumers' Organisation (BEUC) lobbied for a total ban upon growth hormones, opposed, with only partial success, by the pharmaceutical industry, which was not well organized at the time. (It was not until 1987, at the instigation of US firms, that the European Federation of Animal Health, FEDESA, was formed to represent at EU level the companies that, amongst other things, manufactured growth hormones.) Neither European farmers nor the meat processing industry took any stance on the matter. With the help of the BEUC consumer boycotts of veal products, sparked in Italy by reports about DES in Italian magazines and in France and Germany by similar reports, spread from those three countries across the whole of the EU, causing companies such as Hipp and Alete to withdraw their lines of veal products, and veal prices to drop significantly in France, Belgium, West Germany, Ireland, and the Netherlands. Because of the fixed purchases guaranteed by the EU's Common Agricultural Policy, there was a loss of ECU 10 million to the EU's budget.\n\nThe imposition of a general ban was encouraged by the European Parliament, with a 1981 resolution passing by a majority of 177:1 in favour of a general ban. MEPs, having been directly elected for the first time in 1979, were taking the opportunity to flex their political muscles, and were in part using the public attention on the issue to strengthen the Parliament's rôle. The Council of Ministers was divided along lines that directly matched each country's domestic stance on growth hormone regulation, with France, Ireland, the U.K., Belgium, Luxembourg, and Germany all opposing a general ban. The European Commission, leery of a veto by the Council and tightly linked to both pharmaceutical and (via Directorate VI) agricultural interests, presented factual arguments and emphasized the problem of trade barriers.\n\nThe WTO Appellate Body affirmed the WTO Panel conclusion in a report adopted by the WTO Dispute Settlement Body on 13 February 1998. Section 208 of this report says:\n[W]e find that the European Communities did not actually proceed to an assessment, within the meaning of Articles 5.1 and 5.2, of the risks arising from the failure of observance of good veterinary practice combined with problems of control of the use of hormones for growth promotion purposes. The absence of such risk assessment, when considered in conjunction with the conclusion actually reached by most, if not all, of the scientific studies relating to the other aspects of risk noted earlier, leads us to the conclusion that no risk assessment that reasonably supports or warrants the import prohibition embodied in the EC Directives was furnished to the Panel. We affirm, therefore, the ultimate conclusions of the Panel that the EC import prohibition is not based on a risk assessment within the meaning of Articles 5.1 and 5.2 of the SPS Agreement and is, therefore, inconsistent with the requirements of Article 5.1. \nOn 12 July 1999, an arbitrator appointed by the WTO Dispute Settlement Body authorized the US to impose retaliatory tariffs of US$ 116.8 million per year on the EU.\n\nIn 2002 the EU Scientific Committee on Veterinary Measures relating to Public Health (SCVPH) claimed that the use of beef growth hormones posed a potential health risk, and in 2003 the EU enacted Directive 2003/74/EC to amend its ban, but the US and Canada rejected that the EU had met WTO standards for scientific risk assessment.\n\nThe EC made the scientific claim that the hormones used in treating cattle remain in the tissue, specifically the hormone, 17-beta estradiol. However, despite this evidence the EC declared there was no clear link to health risks in humans for the other five provisionally banned hormones. The EC has also found high amounts of hormones in areas where there are dense cattle lots. This increase in hormones in the water has affected waterways and nearby wild fish. Contamination of North American waterways by hormones would not, however, have any direct impact on European consumers or their health.\n\nIn November 2004, the EU requested WTO consultations, claiming that the United States should remove its retaliatory measures since the EU has removed the measures found to be WTO-inconsistent in the original case. In 2005, the EU initiated new WTO dispute settlement proceedings against the United States and Canada, and a March 2008 panel report cited fault with all three parties (EU, United States, and Canada) on various substantive and procedural aspects of the dispute. In October 2008, the WTO Appellate Body issued a mixed ruling that allows for continued imposition of trade sanctions on the EU by the United States and Canada, but also allows the EU to continue its ban on imports of hormone-treated beef.\n\nIn November 2008, the EU filed a new WTO challenge following the announcement by the USTR that it was seeking comment on possible modification of the list of EU products subject to increased tariffs under the dispute, and in January 2009 the USTR announced changes to the list of EU products subject to increased tariffs. In September 2009, the United States and the European Commission signed a memorandum of understanding, which established a new EU duty-free import quota for grain-fed, high quality beef (HQB) as part of a compromise solution. However, in December 2016, the US took steps to reinstate retaliatory tariffs on the list of EU products under the dispute given continued concerns about US beef access to the EU market.\n\nThe EU often applies the precautionary principle very stringently in regards to food safety. The precautionary principle means that in a case of scientific uncertainty, the government may take appropriate measures proportionate to the potential risk (EC Regulation 178/2002). In 1996, the EU banned imported beef from the US and continued to do so after the 2003 Mad Cow scare. A more sophisticated risk assessment found there to be insufficient risk to ban certain hormones, but continued to ban others. Labeling of meat was another option, however warnings were also insufficient because of the criteria specified in the SPS (Sanitary and Phyto-Sanitary agreement). This agreement allows members to use scientifically based measures to protect public health. Most specifically the Equivalence provision in Article 4 which states the following: \"an importing country must accept an SPS measure which differs from its own as equivalent if the exporting country’s measure provides the same level of health or environmental protection.\" Therefore, although the E.U. is a strong proponent of labels and banning meat that contains growth hormones, requiring the US to do the same would have violated this agreement.\n\nOne of the effects of the Beef Hormone Dispute in the US was to awaken the public's interest in the issue. This interest was not wholly unsympathetic to the EU. In 1989, for example, the Consumer Federation of America and the Center for Science in the Public Interest both pressed for an adoption of a ban within the US similar to that within the EU. US consumers appear to be less concerned with the use of synthetic chemicals in food production. Because of current policy, in which all beef is allowed whether produced with hormones or genetically modified, US consumers now have to rely on their own judgment when buying goods. However, in a study done in 2002, 85% of respondents wanted mandatory labeling on beef produced with growth hormones. The public in general is motivated to purchase organic or natural meats for several reasons. Organic meats and poultry is the fastest growing agricultural sector, from 2002–2003 there was a growth of 77.8%, accounting for $23 billion in the entire organic food market.\n\n\n\n"}
{"id": "46294636", "url": "https://en.wikipedia.org/wiki?curid=46294636", "title": "Bliss point (food)", "text": "Bliss point (food)\n\nIn the formulation of food products, the bliss point is the amount of an ingredient such as salt, sugar, or fat which optimizes tastiness. Pioneering work on the bliss point was carried out by American market researcher and psychophysicist Howard Moskowitz, known for his successful work in product creation and optimization for foods ranging from spaghetti sauce to soft drinks. Moskowitz describes the bliss point as \"that sensory profile where you like food the most.\n\nThe bliss point for salt, sugar, or fat is a range within which perception is that there is neither too much nor too little, but the \"just right\" amount of saltiness, sweetness, or richness. The human body has evolved to favor foods delivering these tastes: the brain responds with a \"reward\" in the form of a jolt of endorphins, remembers what we did to get that reward, and makes us want to do it again, an effect run by dopamine, the neurotransmitter. Combinations of sugar, fat, and salt act synergistically, and are more rewarding than any one alone. In food product optimization, the goal is to include two or three of these nutrients at their bliss point.\n\n"}
{"id": "41121277", "url": "https://en.wikipedia.org/wiki?curid=41121277", "title": "Bone health", "text": "Bone health\n\nThe human skeletal system is a complex organ in constant equilibrium with the rest of the body. In addition to support and structure of the body, bone is the major reservoir for many minerals and compounds essential for maintaining a healthy pH balance. The deterioration of the body with age renders the elderly particularly susceptible to and affected by poor bone health. Illnesses like osteoporosis, characterized by weakening of the bone’s structural matrix, increases the risk of hip-fractures and other life-changing secondary symptoms. In 2010, over 258,000 people aged 65 and older were admitted to the hospital for hip fractures. Incidence of hip fractures is expected to rise by 12% in America, with a projected 289,000 admissions in the year 2030. Other sources estimate up to 1.5 million Americans will have an osteoporotic-related fracture each year. The cost of treating these people is also enormous, in 1991 Medicare spent an estimated $2.9 billion for treatment and out-patient care of hip fractures, this number can only be expected to rise.\n\nWhen more sulfur containing amino acids, methionine and cystine, are consumed than the body can use for growth and repair, they are broken down yielding sulfate, or sulfuric acid among other products. Animal foods such as meat, dairy, and eggs are high in protein and “dietary animal protein intake is highly correlated with renal net acid excretion”. Research dating back to the early 1900s has shown correlations between high protein diets and increased acid excretion. One measure of the acidic or basic effects foods have in the body is Potential Renal Acid Load (PRAL). Cheeses with protein content of 15 g protein/100g or higher have a high PRAL value of 23.6 mEq/100 g edible portion. Meats, fish, other cheeses and flour or noodles all have a PRAL around 8.0 mEq/100 g edible portion, where fruits and vegetables actually have a negative PRAL. \nIn healthy adults, bone is undergoing constant repair and renewal. New bone is deposited by osteoblast cells and resorbed or destroyed by osteoclast cells. This addition and subtraction of bone usually yields no net change in the overall mass of the skeleton, but the turnover process can be significantly affected by pH.\n\nBone Mineral Density (BMD) is a measure commonly used to quantify bone health. A lower BMD value indicates an increased risk of a osteoporosis or a fracture. There is a large range of factors influencing BMD. Protein consumption has shown to be beneficial for bone density by providing amino acid substrates necessary for bone matrix formation. It is also thought that blood concentration of the bone formation stimulant, Insulin-like Growth Factor-I (IGF-I), is increased from high protein consumption and parathyroid hormone (PTH), a bone resorption stimulant, is decreased. Although protein has shown to be beneficial for increasing bone mass, or bone mineral density, there is no significant association between protein intake and fracture incidence. In other words, a low BMD can be predictive of osteoporosis and increased fracture risk, but a higher BMD does not necessarily mean better bone health. High BMD is also correlated with other health issues. For example, a higher BMD has also been associated with increased risk of breast cancer.\n\nMost metabolic processes have a specific and narrow range of pH where operation is possible, multiple regulatory systems are in place to maintain homeostasis. Fluctuations away from optimal operating pH can slow or impair reactions and possibly cause damage to cellular structures or proteins. To maintain homeostasis the body may excrete excess acid or base through the urine, via gas exchange in the lungs, or buffer it in the blood. The bicarbonate buffering system of blood plasma effectively holds a steady pH and helps to hold extracellular pH around 7.35. The kidneys are responsible for the majority of acid-base regulation but can excrete urine no lower than a pH of 5. This means that a 330mL can of cola, for example, usually ranging in pH from 2.8 - 3.2, would need to be diluted 100 fold before being excreted. Instead of producing 33L of urine from one can of cola, the body relies on buffer to neutralize the acid. Systemic acidosis can be the result of multiple factors, not just diet. Anaerobic exercise, diabetes, AIDS, aging, menopause, inflammation, infections, tumours, and other wounds and fractures all contribute to acidosis. Blood has an average pH of 7.40 but interstitial fluid can vary. Interstitial pH of the skin, for example, is ~7.1. There is no data available for bone.\n\nHomocysteine, a non-protein amino acid and analogue to the protein amino acid cystine, has been shown to have negative effects on bone health. Higher homocysteine concentrations are likely a result of folate, vitamin B B deficiencies. In addition, it was found that homocysteine concentration was significantly affected by physical activity. The stimulation of the skeleton through physical activity promotes positive bone remodelling and decreases levels of homocysteine, independently from nutritional intake. Four methods have been proposed regarding the interaction of homocysteine and bone; increase in osteoclast activity, decrease in osteoblast activity, decrease in bone blood flow, and direct action of homocysteine on bone matrix. Homocysteine inhibits lysyl oxidase which is responsible for post-translational modifications of collagen, a key component to bone structure\n\nOsteoclasts are located on the surface of bones and form resorption pits by excreting H+ to the bone surface removing hydroxyapatite, multiple bone minerals, and organic components: collagen and dentin. The purpose of bone resorption is to release calcium to the blood stream for various life processes. These resorption pits are visible under electron microscopy and distinctive trails are formed from prolonged resorption. Osteoclasts have shown to be “absolutely dependent on extracellular acidification”. A drop in pH of <0.1 units can cause a 100% increase in osteoclast cell activity, this effect persists with prolonged acidosis with no desensitization, “amplifying the effects of modest pH differences”. Osteoclast cells show little or no activity at pH 7.4 and are most active at pH 6.8 but can be further stimulated by other factors such as parathyroid hormone.\n\nOsteoblast are responsible for the mineralization and construction of bone matrix. Like osteoclast cells, osteoblast cell activity is directly related to extracellular pH mirroring of osteoclast activity. At pH 7.4, where osteoclasts are inactive, osteoblast are at peak activity. Likewise, at pH 6.9 osteoblast activity is non-existent. The hormone estrogen is also important for osteoblast regulation. In postmenopausal women estrogen levels are decreased which has negative effects on bone remodelling. Homocysteine further exacerbates this problem by reducing estrogen receptor α mRNA transcription. Thus reducing any beneficial effect that estrogen plays on bone remodelling.\n\nAcidosis inhibits bone osteoblast matrix mineralization with reciprocal effect on osteoclast activation. The combined responses of these cells to acidosis maximizes the availability of hydroxyl ions in solution that can be used to buffer protons. The utilization of bone to buffer even a small percentage of daily acid production can lead to significant loss of bone mass in the course of a decade. Additionally, as the body ages there is a steady decline in renal function. Metabolic acidosis can become more severe as kidney function weakens, and the body will depend more heavily on bone and blood to maintain acid-base homeostasis.\n\nThere is no one food or nutrient capable of providing adequate bone health on its own. Instead, a balanced diet sufficient in fruits and vegetables for their vitamins, minerals, and alkalinizing substrates is thought to be most beneficial. High protein diets supply larger amounts of amino acids that could be degraded to acidic compounds. Protein consumption above the Recommended Dietary Allowance is also known to be beneficial to calcium utilization. Overall it is understood that high-protein diets have a net benefit for bone health because changes in IGF-I and PTH concentrations outweigh the negative effects of metabolic acid production. The source of protein, plant or animal, does not matter in terms of acid produced from amino acid metabolism. Any differences in Methionine and Cysteine content is not significant to affect the overall potential renal acid load (PRAL) of the food. In addition to their acid precursor protein content, plants also contain significant amounts of base precursors. Potassium bicarbonate, a basic salt, is produced via the metabolism of other organic potassium salts: citrate, malate, and gluconate, which are substantial in plants. The discrepancy observed in PRAL is accounted for by differences in base precursor content.\n\n\n"}
{"id": "8222701", "url": "https://en.wikipedia.org/wiki?curid=8222701", "title": "Capital punishment in Australia", "text": "Capital punishment in Australia\n\nCapital punishment in Australia was a form of punishment in Australia that has been abolished in all jurisdictions. Queensland abolished the death penalty in 1922. Tasmania did the same in 1968, the federal government abolished the death penalty in 1973, with application also in the Australian Capital Territory and the Northern Territory. Victoria did so in 1975, South Australia in 1976, and Western Australia in 1984. New South Wales abolished the death penalty for murder in 1955, and for all crimes in 1985. In 2010, the federal government passed legislation prohibiting the re-establishment of capital punishment by any state or territory. Neither the Commonwealth nor any of the states will extradite or deport a prisoner to another jurisdiction if they will face the death penalty, and police co-operation with other countries which have the death penalty has been questioned.\n\nThe last execution in Australia took place in 1967, when Ronald Ryan was hanged in Victoria. Between Ryan's execution in 1967 and 1984, several more people were sentenced to death, but had their sentences commuted to life imprisonment. The last death sentence was given in August 1984, when Brenda Hodge was sentenced to death in Western Australia (and subsequently had her sentence commuted to life imprisonment). \n\nDeath sentences were carried out under Aboriginal customary law, either directly or through sorcery. In some cases the condemned could be denied mortuary rites. The first executions carried out under European law in Australia took place in Western Australia in 1629, when Dutch authorities hanged the mutineers of the \"Batavia\".\n\nCapital punishment had been part of the legal system of Australia since British settlement. During the 19th century, crimes that could carry a death sentence included burglary, sheep stealing, forgery, sexual assaults, murder and manslaughter, and there is one reported case of someone being executed for \"being illegally at large\". During the 19th century, these crimes saw about 80 people hanged each year throughout Australia.\n\nBefore and after federation, each state made its own criminal laws and punishments. For a full list see: \n\nIn 1973 the Death Penalty Abolition Act 1973 of the Commonwealth abolished the death penalty for federal offences. It provided in Section 3 that the Act applied to any offence against a law of the Commonwealth, the Territories or under an Imperial Act, and in s. 4 that \"[a] person is not liable to the punishment of death for any offence\".\n\nNo executions were carried out under the bridge of the federal government, and the passage of the Death Penalty Abolition Act 1973 saw the death penalty replaced with life imprisonment as their maximum punishment. Since the Commonwealth effects of utilising this Act, no more individuals have been exposed to the death penalty. On 11 March 2010, Federal Parliament passed laws that prevent the death penalty from being reintroduced by any state or territory in Australia.\n\nNeither the Commonwealth nor any of the states will extradite or deport a prisoner to another jurisdiction if they will face the death penalty. \n\nA recent case involving this was the case of American Gabe Watson, who was convicted of the manslaughter of his wife in North Queensland, and faced capital murder charges in his home state of Alabama. His deportation was delayed until the government received assurances that he would not be executed if found guilty.\n\nThe last execution in New South Wales was carried out on 24 August 1939, when John Trevor Kelly was hanged at Sydney's Long Bay Correctional Centre for the murder of Marjorie Constance Sommarlad. New South Wales abolished the death penalty for murder in 1955, but retained it as a potential penalty for treason, piracy, and arson in naval dockyards until 1985. New South Wales was the last Australian state to formally abolish the death penalty for all crimes.\n\nVictoria’s first executions occurred in 1842 when two Aboriginal men, Tunnerminnerwait and Maulboyheenner, were hanged outside the site of the Melbourne Gaol for the killing of two whalers in the Westernport district. Ronald Ryan was the last man executed at Pentridge Prison and in Australia. He was hanged on 3 February 1967 after being convicted of the shooting dead a prison officer during an escape from Pentridge Prison, Coburg, Victoria in 1965.\nRyan was the last of 186 executions.\n\nVictoria was also the state of the last woman executed in Australia: Jean Lee was hanged in 1951. She was accused of being an accomplice in the murder of 73-year-old William ('Pop') Kent. She, along with her accomplices, were executed on 19 February 1951. Victoria would not carry out another execution until that of Ronald Ryan.\n\nNot all those executed were murderers: for instance, Albert McNamara was hanged for arson causing death in 1902, and David Bennett was hanged in 1932 after being convicted of raping a four-year-old girl. Bennett was the last man to be hanged in Australia for an offence other than murder. \n\nThis number includes triple murderer Edward Leonski, executed by the U.S. Army in 1942.\n\nThe beam used to execute condemned prisoners was removed from Old Melbourne Gaol and installed in D Division at Pentridge Prison by the condemned child rapist David Bennett, who was a carpenter by trade. It was used for all 10 Pentridge hangings (including a double hanging). After Victoria abolished capital punishment in 1975, the beam was removed and put into storage, and was reinstalled at the Old Melbourne Gaol in August 2000.\n\nA total of 94 people were hanged in the Moreton Bay/Queensland region from 1830 until 1913. The last person hanged was Ernest Austin on 22 September 1913 for the rape and murder of 11-year-old Ivy Mitchell. \n\nThe only woman to be hanged was Ellen Thompson on 13 June 1887; she was hanged alongside her lover, John Harrison, for murdering her husband William.\n\nIn 1922, Queensland became the first part of the British Commonwealth to abolish the death penalty.\n\nIn Western Australia, the first legal executions were under Dutch VOC law on 2 October 1629 on Long Island, Houtman Abrolhos (near Geraldton), when Jeronimus Corneliszoon and six others were hanged as party to the murders of 125 men, women and children.\n\nFollowing British colonization, between 1833 and 1855 executions by firing squad and hanging were performed at a variety of places, often at the site of the offence. Even with the construction of the new Perth Gaol in 1855 as the main execution site in the state, executions were also carried out at various country locations until 1900. In 1886 the Fremantle Prison was handed over to the colonial government as the colony's major prison; from 1889 43 men (and one woman, Martha Rendell) were hanged there.\n\nThe first execution under British law was that of Midgegooroo, who on 22 May 1833 was executed by firing squad while bound to the door of the original Perth Gaol. John Gavin, a Parkhurst apprentice, was the first British settler to be executed in Western Australia. In 1844 he was hanged for murder at the Fremantle Round House, at the age of fifteen. Bridget Larkin was the first woman to be executed in Western Australia, for the murder of John Hurford, in 1855.\n\nThe last execution was that of Eric Edgar Cooke on 26 October 1964 at Fremantle Prison. Cooke had been convicted on one count of murder, but evidence and his confessions suggested he had committed many more. The last sentence of death in Western Australia was passed in 1984, but the female killer (Brenda Hodge) in question had her sentence commuted to imprisonment for life, as was customary by this stage.\n\nCapital punishment was formally removed from the statutes of the state with the passage of the Acts Amendment (Abolition of Capital Punishment) Act 1984.\n\nThe Adelaide Gaol was the site of forty-four hangings, from Joseph Stagg on 18 November 1840 to Glen Sabre Valance, murderer and rapist, on 24 November 1964. Three executions also occurred at Mount Gambier Gaol.\n\nTwo Ngarrindjeri men were controversially executed by hanging along the Coorong on 22 August 1840, after a drumhead court-martial conducted by Police Commissioner O'Halloran on the orders of Governor George Gawler. The men were found to be guilty of murdering the twenty-five survivors of the shipwreck \"Maria\" a few months before.\n\nElizabeth Woolcock, the only woman ever to have been executed under South Australian law, was hanged on 30 December 1873. Her body was not released to the family and was buried between the inner and outer walls of the prison, identified by a number and the date of the execution.\n\nIn 1976, the \"Criminal Law Consolidation Act\" was modified so that the death sentence was changed to life imprisonment.\n\nIn the early days of colonial rule Tasmania, then known as Van Diemen's Land, was the site of penal transports. Mary McLauchlan was convicted in 1830 for infanticide; she was sentenced to both death and dissection. She was the first woman to be hanged in Tasmania.\n\nThe last execution was on February 14 1946, when serial rapist and killer Frederick Thompson was hanged for the murder of eight-year-old Evelyn Maughan. The death penalty was abolished in 1968.\n\nNo executions were carried out in the Australian Capital Territory, where federal legislation abolished capital punishment in 1973.\n\nThere were nine executions between 1893 and 1952. Seven of them took place at Fannie Bay Gaol, the other two at regional locations close to where the crime took place.\n\nThe last execution in the Northern Territory was a double hanging at Fannie Bay Gaol on 8 August 1952. The death penalty was abolished in 1973.\n\nIn Australia capital punishment was banned on a state-by-state basis through the 20th century. Despite the ban, polls have indicated varying support for the reintroduction of the practice. A 2005 Bulletin poll showed that most Australians supported capital punishment. The Australian National University's 2007 Electoral Survey found that 44 per cent of people thought the death penalty should be reintroduced, while only 38 per cent disagreed. In the recent case of the Bali bombers, then prime minister John Howard stated that Australians expected their execution by Indonesia.\n\nOn occasion the issue of capital punishment is published in the media or is subject to media and public support and scrutiny. Most occasions where capital punishment is brought up in the media, it is regarding current cases of intense media coverage regarding murder, rape and in extreme circumstances such as terrorism. On various occasions, the media and public express support for capital punishment for the most heinous of crimes including mass murder such as in the cases of the Milat Backpacker Murders and the Bryant Port Arthur massacre, in which a total of 42 people were killed stirring strong emotions as to whether or not to reintroduce the death penalty. A similar surge in public support for reintroduction occurred when Lindy Chamberlain was convicted of murdering her baby Azaria in 1982, although it was subsequently shown that she was not guilty. This miscarriage of justice has been used as a powerful example of the risks of the death penalty. However, no person of significant stature or influence has advocated the death penalty for quite some time since the last execution in 1967. The death penalty was completely abolished in Australia with the \"Crimes Legislation Amendment (Torture Prohibition and Death Penalty Abolition) Bill 2009\" passing the Australian Senate without amendments in March 2010.\n\nIn 2009 a public opinion survey was conducted by Roy Morgan Research where responders were given the question: \"In your opinion, should the penalty for murder be death or imprisonment?\" The surveyors conducted the poll for people from 14 and onwards in age with around 687 people completing the survey for publication. The results of the poll are as follows:\n\nIn 2014 another public opinion survey was conducted by Roy Morgan Research where responders were given the question: “If a person is convicted of a terrorist act in Australia which kills someone should the penalty be death?” The surveyors conducted the poll with a cross-section of 1,307 Australians. The poll showed a small majority of Australians (52.5%) favoured the death penalty for deadly terrorist acts in Australia while 47.5% didn’t. This was a significant increase from 2009 when only 23% of Australians supported the death penalty being imposed for convicted murderers.\n\n\n\n"}
{"id": "30608323", "url": "https://en.wikipedia.org/wiki?curid=30608323", "title": "Cardiac fibroma", "text": "Cardiac fibroma\n\nCardiac fibroma, also known as cardiac fibromatosis, is a rare benign tumor of the heart that occurs primarily in infants and children. Benign tumors are typically a solitary, firm grey-white, non-encapsulated tumor that is composed of fibrous and dense connective tissue. It is most commonly located in the interventricular septum or left ventricular wall. Symptoms depend on the size of the tumor, its location relative to the conduction system, and whether it obstructs blood flow. Two-thirds of children with this tumor are asymptomatic, showing no signs and symptoms. Symptomatic cardiac fibromas may be treated by surgical resection. It is associated with Gorlin syndrome. Benign cardiac tumors are rare, 75% are histologically benign. Cardiac fibromas only occur 4-6%, which is less common compared to myxomas (75%) and rhabdomymoas (5-10%).\n\nThe diagnosis of these tumors require physical checkups, imaging studies on the heart, and specialized tests to evaluate the heart. Cardiac fibroma is considered a congenital tumor where an ultrasound prenatal scan may help detect during fetal stage. Surgery is the best treatment for an individual with cardiac fibroma. During this surgery, the tumor is completely removed by the surgeon. The overall prognosis is very good with a surgical removal. There have been 200 cases of cardiac fibroma recorded in the medical literature. Risk factors are still unidentified, but 1 in 30 individuals with Gorlin syndrome are known to be present with cardiac fibroma.\nCardiac fibroma is a slow-growing tumor that can cause heart electrical transmission defects and arrhythmias. Some features may be seen in the ventricle wall separating the right and left lower chambers or the ventricle muscle. This tumor is rarely seen in atrial locations. Cardiac fibromas are mostly single and well-circumscribed and the average size of the tumor is circular and is 5 cm. Sometimes signs and symptoms are difficult to find in 35% of individuals. Situations like this, the tumor is incidentally diagnosed during a health checkup for other medical conditions. An individual may have abnormal heart sounds, such as a heart murmur.\n\nIn 65% of individuals, signs and symptoms are more obvious due to the large size of the tumor. Also, there is blood flow obstruction, especially into or out of the valves. The valves function becomes affected, which leads to heart failure. An individual might experience bluish skin (cyanosis), severe arrhythmias, dizziness, fainting, and other obstructive symptoms may be present.\nThe cause of development for cardiac fibroma is still unknown or unexplained. Some of these cases are observed to be linked to Gorlin syndrome; a complex genetic disorder causing the formation of tumors in various parts of the body. Research is currently being undertaken to identify relevant casual factors.\n\nThe mechanism behind cardiac fibromas are still unclear. Fibromas have a homogeneous mass of fibroblasts mixed with an abundance of collagen and elastic fibers. These masses represent mesenchymal growth, but lacks other mesenchymal elements, such as blood vessels, cartilage bone, and muscle. They often entrap cardiomyocytes, which are muscle cells that make up the cardiac muscle. Cells are decreasing during this time, while collagen contents are increasing. The growth of cardiac fibromas are slow and produces detrimental physical effects. This is done by infiltrating and replacing the myocardium and protruding into the cavity of the heart. These tumors usually occur within the anterior wall of the left ventricle or the interventricular septum and rarely involves the right ventricle. Large fibromas bulge into the cavity of the chamber, interfering with the functions of heart valves and blood flow through the heart. When the left ventricular is obstructed, the outflow tract may give rise to a failed diagnosis of congenital subaortic stenosis. Moreover, it can cause severe congestive heart failure. Cardiac fibromas may present lymphocyte and monocyte aggregates, as well as areas of calcification, which are shown on a chest x-ray or CT scan. The dimensions of these masses and the location cause clinical symptoms. Fibromas cause ventricular arrhythmias and conduction disturbances that become lethal causing a sudden death.\n\nThe following tests and exams are taken to diagnose Cardiac fibroma:\n\nCardiac fibroma is commonly treated through surgical excision procedures. The removal of cardiac tumors require an open heart surgery. During the surgery, the surgeon removes the tumor and tissues around it to reduce the risk of the tumor returning. A heart-lung machine is used to take over the work of the heart and lungs because surgery is complicated and requires a still heart. The recovery is usually between 4–5 days in the hospital and 6 weeks in total. An echocardiogram is taken every year to make sure the tumor has not returned or formed any new growth.\n\nIf surgery is too difficult, a heart transplantation is a second option. Continuous observations and checkups are recommended to monitor the condition. In cases of arrhythmias, anti-arrhythmic medication is given before surgical treatments are considered. There has been excellent outcomes for individuals who undergo surgery to remove the tumor. If the tumor is completely resected, individuals will have a disease-free survival. If the tumor is incomplete it will continue to grow and recurrence of symptoms occur.\n\nLiterature survey on epidemiology and pathology of cardiac fibroma:\n\nDuring this study, researchers searched through the literature databases on cardiac fibroma to find factors that predict poor outcomes that lead to death. Researchers found that patients who did not survive were significantly younger than those who did survive. These results suggest that younger individuals diagnosed with cardiac fibroma are associated with a poorer outcome. They found no significant difference between the maximum diameter of the tumor between age groups. Even though younger individuals have smaller hearts, the high ratio of tumor-to-heart sizes may generate low cardiac output, which leads to a poor outcome. Literature revealed that 18 of 178 patients with cardiac fibroma were diagnosed during prenatal and neonatal periods, resulting in the tumor having a certain size regardless of the child's age. These findings suggest that cardiac fibromas may be a congenital disorder.\n\nSuccessful Surgical Excision of a Large Cardiac Fibroma in an Asymptomatic Child:\n\nA 3-year-old girl, who was asymptomatic, underwent a successful surgical excision of a large cardiac fibroma. She had frequent coughs, which led to a chest radiograph. A cardiac mass was found on the echocardiography and later was confirmed by magnetic resonance imaging (MRI). After 24 hours of being monitored, it showed sinus rhythms of normal variability. The mass dimensions were 38 X 28 mm in the apical area of the left ventricle. A surgical procedure was recommended due to the risk of ventricular arrhythmias and sudden cardiac death. The surgery was a success and they were able to remove the entire tumor without any complications. Follow-up evaluations at six-months and a year showed the patient was in good health and no signs of tumor recurrence. Asymptomatic patients with cardiac fibroma becomes controversial because these tumors have the tendency to grow. Situations like this, a surgical removal will be the top recommendation for patients.\n\nPrimary cardiac tumors in children: a center's experience:\n\nThe Department of Cardiac Surgery Children's Hospital in China conducted a study to analyze different characteristics and outcomes of pediatric patients who have primary cardiac tumors treated in their center. They had sixteen patients with primary cardiac tumors between the ages of 1–13 years. All patients were diagnosed by echocardiography, MRI, and computed tomography (CT). As a result, they were able to successfully remove the mass from 15 patients with cardiopulmonary bypass, whereas partial resection was done in one patient. Unfortunately, one patient died during surgery due to low cardiac output syndrome at 5 days after operation. The pathological examination of the cardiac masses showed that rhabdomyoma is the most frequent tumor in children, followed by myxoma, fibromas, etc. Morbidity of rhabdomyomas and fibromas were reported higher in infancy, while myxomas are more frequent in older children.\n"}
{"id": "50462916", "url": "https://en.wikipedia.org/wiki?curid=50462916", "title": "Childbirth in Zambia", "text": "Childbirth in Zambia\n\nThe maternal mortality rate is 224 deaths per 100,000 births, which is the 23rd highest in the world. The mean age of mothers at birth is 19.3 years old, and the fertility rate is 5.72 children born per woman, which is the 7th highest in the world. The contraceptive rate is only 40.8%, and the birth rate is the 4th highest in the world at 42.13 births/1,000 population. Infectious disease is a key contributor to the poor health of the nation, and the risk is very high for diseases such as protozoal and bacterial diarrhea, hepatitis A, typhoid fever, malaria, dengue fever, schistosomiasis, and rabies. The adult prevalence rate of HIV/AIDS is 12.37%, which is the 7th highest in the world.\n\nDue to the poor health outcomes for mothers, and the challenges to access of care there is a significant impact on the culture of birth. Diversity in language and ethnicity plays a role in the diversity of care throughout Zambia. Establishing safety for the mother and baby through national health improvement, particularly concerning infectious disease and access to clean water, is a priority for Zambia.\n\nWhile the woman is pregnant, extra precaution were taken as it is believed the women and her baby are in a physically and spiritually weaker state; this means they are more susceptible to sickness and evil forces. Preventative measures include changes in the women’s nutrition. Certain foods are considered harmful to the fetus and should be avoided by the mother. For example, eating eggs while pregnant will cause the child to be born without hair. Eating fish could cause an infant abnormality.\n\nPostpartum care isn’t widely accessible or accessed, but research has shown that access to antenatal care in preparation for the birth is associated with better postpartum care and contraceptive use. Use of postpartum birth control and family planning leads to better maternal health outcomes, including a reduction in the number of pregnancies, reduced transmission of STIs, and overall decreased risk for maternal mortality. \n\nAccess to clean drinking water is a major concern in Zambia. 85.6% of the population in urban areas and 51.3% of the population in rural areas as access to improved water sources, and 14.4% of the urban population and 48.7% of the rural population has access to unimproved water sources. Access to sanitation facility access is unimproved in 44.4% of the urban population and 64.3% in the rural population.\n\nWomen are not allowed to have sexual intercourse after the eighth month of pregnancy, this is believed to prevent the baby to have “white stuff” (vernix) on their skin when they are born. Furthermore, extramarital relations are believed to bring harm to the baby.\n\nThere are certain food women are allowed to and are not allowed to eat during pregnancy and labor, as they could have a lasting effect of the child’s health. For example, women are not allowed to eat eggs.\n\nLabor usually takes place in the home of the laboring women, should complications arise they are transferred to a maternity birthing center. However, due to geographic constraints sometime transportation is not an option, these geographic barriers contribute to the poor maternal health outcomes.\nLabor is meant to be a private matter between the women and a few support people, the woman will often isolate herself from those not acting as support people during her laboring. Males are also not allowed to be a part of the laboring process.\n\nLaboring mothers often behave in ways that conform to societal expectations in terms how a laboring mother should act. For example, it is the belief that mothers should not should not be vocal during the delivery process as it could lead to complications for the baby. \nWomen are encouraged by their mbusa to push whenever they experience pain. Mbusas will also utilize, pieces of clothing called a chitenge to assist women into different positions, in an attempt to alleviate some of their pain.\nThree to four women are allowed to be present during the labor and delivery process. These women are usually relatives or close friends such as: daughters, sisters, nieces, neighbors, and co-wives. Should a women tell more than a few close relatives and friends she is in labor it is believed that she will become bewitched and complications will occur during the labor process. Laboring and mothers giving birth preferred to have someone close by acting as a support person for them, however when naming the person they want to be there, a husband or male support person is usually not named. \nMbusas operate under the assumption that less interference during the laboring process is better. However, at times they will employ a variety of strategies to accelerate the labor process. These include: providing fundal pressure, using a cooking stick to provide pressure in their mouth, and physically moving the mother. The gag reflex is used to help speed up either the labor process or the delivery of the placenta.\n\nOnly 5.4% of birth in Zambia are attended by a trained traditional birth attendant. Most laboring women will have a social support person present during the antepartum period, however if a women gives birth in a hospital, the support person will, most likely, not be present during the actual laboring process. These women are called “mbusas”. Mbusas and those providing support during the laboring process have not received any formal education or training surround care of the laboring women. In most cases they do not have the skills needed to handle obstetrical emergencies. In those cases, they choose to accompany the laboring mothers to maternity units rather than delivering in the home. The major roles of mbusas during the laboring process is to provide emotional support to the mother. \nWhen mothers are brought into maternity centers, often do not have any direction making power. Many of the mbusas and those to bring the mothers to the maternity center are often only allowed in the room during visiting hours, however many women will choose to wait outside the unit. Should a mother be able to labor at home most of the power still belongs to them throughout the whole process. Furthermore, women who give birth in a medical center are subjected to more of the “coercive obstetric procedures” these include birthing positions that are convenient for the health care provider but not the laboring women.\n53% of births occur in the home. The majority of the rest of the birth occur in maternity centers and hospitals. There seems to be a divide between those who gave birth in health care centers as opposed to those who gave at home. This is evidence by the fact that those who gave birth at home, do not receive as much postpartum as those who give birth in a healthcare setting. The women who choose to have a birth at home, states that they feel unwelcome at postpartum care clinics.\nTypically in labors occurring in the home mothers are free to choose a birthing position that is most comfortable to them: some of these positions include, sitting, kneeling, and squatting. Mbusa will help with positioning through the use of a chitenge, a fabric Zambian women use to tie around the waist. Those laboring in health care centers may not have as much freedom in positioning. \nSimilar to the pain experienced during labor women giving birth are encouraged to push through the pain. Typically, no pharmacological pain relief is given to mothers birthing at home and mbusas will resort to using position changes as a way to relive some of the mother’s pain.\n\nOnce the placenta is delivered it is buried in a specific place near the house or village. It is done this way as to protect the mother and her baby from witchcraft. The placenta also plays a major role in the preservation of the women’s fertility. One mbusa states: “The placenta is buried, with the side where the baby lies facing the sky, and leaving the end of the cord showing, so that when the cord drops from the baby it should be connected with the cord buried with the placenta to protect the fertility.”.\n\nSome women cite the feelings of embarrassment dictating their personal behaviors. In a qualitative study one woman stated: “I would feel shy to have somebody next to me because we make a lot of noise and we are not covered.”. Furthermore, with the presence of a support person women feel that their behaviors must conform strictly with societal norms, as the support person could tell others if she did not.\n\nShould a complication arise women birthing at home do not have the resources of skills to be treated at home, women are transferred to the maternity center if they are able to get there logistically. Societal norms at times can view complications or an unhealthy mother and/or baby as the fault of the mother due to her behaviors during the pregnancy or labor.\n\nObstetrical technology is not used in home births, this is one of the reason, home births with complications are transferred to a healthcare setting if possible. The use of technology within hospitals is also limited. For example, continuous fetal monitoring does not occur in most maternity centers. In hospitals the rate of episiotomies is not documented however it is estimated to be around 28%.\n\nOnce babies are born they are placed on a delivery mat until the mother delivers the placenta. The umbilical cord is then cut with a razor blade or sugar cane peel. Afterwards both the mother and the baby are bathed. Postpartum visits usually take place within the first 6 weeks after the birth of the child.\n\nWomen who gave birth at home will then recover at home at well. There was no information about the average length of stay after a birth for women who birth in maternity centers.\n\nPregnancy due dates and the start of labor are not announced as many are fearful this could evoke evil witchcraft.\n\nThere are no significant differences noticed between women who have given birth before and those who have not, the labor process seems to be approached in the same way.\n\nOne third of women who are married also use contraception. Furthermore, on average women begin to use contraception 8 month after their childbirth. After a child is born and while they are breastfeeding sexual activity is not allowed, as to protect the child.\n\nThere is a large divide between those women who live in urban areas and those who live in rural areas in terms of postpartum care. Reports show that the rate of postpartum visits (up to 6 weeks after the birth of the baby) is 84% in Lukasa, the capital of Zambia. As opposed to a rate of 42% of women from rural areas went to postpartum care.\n\nCare for the newborn is focused mainly on superstitious issues surrounding the umbilical cord. Women dress the umbilical cord with different mixtures such as ash, seashell with oil, or breastmilk and believe that the cord dropping on the male genitals will lead to infertility. It is also believed that a mother should not cook until the umbilical cord has fallen off, or the mother will get a mysterious disease. A baby born covered in vernix is considered to be dirty. Babies are traditionally left either unwrapped, or wrapped in piece of cloth immediately after birth. Skin-to-skin contact isn’t always employed, unless the birth attendants have been trained to do so. These traditions may lead to hypothermia and delayed breast feeding. In hospitals and maternity centers specifically, the mothers were not encouraged to have skin to skin contact immediately following the birth of their child, nor were they encouraged to breastfeed.\n\nDue to the high maternal mortality rate of 224 deaths per 100,000 live births, many newborns are left without a mother and may therefore be orphaned and sent to an orphanage. Contributing to the high maternal mortality is access to care for mothers in need. Zambia has only 0.17 physicians and 2 hospital beds per 1,000. Poor maternal outcomes lead to poor newborn outcomes as well, particularly regarding the loss of a primary caregiver. The national rate of infant mortality is the 17th highest in the world at 64.72 per 1,000 live births.\n\nAccess to clean water and sanitations is a considerable concern for the health and nutrition of the mother. If mom doesn’t have proper nutrition and is breastfeeding, chances are the child will be malnourished as well. In Zambia, 14.9% of children under 5 are underweight.\n\nIn Zambia, 8 out of 10 mothers breastfeeds exclusively for 6 months. The exclusive rate for breastfeeding of children under 6 months is 61%, and early initiation (within the first hour) occurs in 57% of moms. The prevalence of adequate feeding practices in children aged 6–23 months is only 37%. Due to lack of access to clean water, breastfeeding is highly encouraged for Zambian mothers as formula requires a clean and safe water source that many don’t have access to. Movement for baby friendly hospitals, ones that accommodate babies in the rooms with moms, promote breastfeeding, and provide lactation consultant services. Protection from HIV is another reason breastfeeding is encouraged, as exclusive breastfeeding builds the baby’s immunity. Prior to the early 2000s, it was believed that HIV+ mothers should not breastfeed to avoid spreading HIV to their baby. When this was proven false and the National AIDS Council published a recommendation for HIV mothers to breastfeed their babies, breastfeeding rates began to rise again around 2007. An advocacy campaign was launched by the Zambian ministry of health in 2009 to promote improvement of exclusive breastfeeding rate. They have created a video is used to promote breastfeeding. However, as noted earlier, the access to media is limited in Zambia, so alternate methods of dissemination are required. The following graphic represents the increase in coverage of exclusive breastfeeding across Zambia over 5 year increments.\n\nZambia is one of 22 African countries with the highest burden of undernutrition in children under five, and Micronutrient deficiencies are common in Zambian children, as vitamin A deficiencies are present in 54% of children under 5, and anemia in 53%. A child’s nutritional status is influenced by access to food, care, and health, and poor feeding practices combined with illnesses such as intestinal parasites, diarrhea, pneumonia, malaria, and HIV/AIDS cause this high burden of undernutrition. Improving the rate of exclusive breastfeeding in mothers will help to improve the nutritional issues present in children and infants.\n\nCircumcision is generally perceived and suggested to be beneficial for reduction of STI and HIV transmission. Many religious and cultural groups have circumcision as a standard practice, but the medical circumcision movement has expanded as the understanding of the benefits has.\n"}
{"id": "33176650", "url": "https://en.wikipedia.org/wiki?curid=33176650", "title": "College of Medicine (UK)", "text": "College of Medicine (UK)\n\nThe College of Medicine (CoM) (formerly the College of Integrated Health (2009–10)) is a United Kingdom based organisation founded in 2010 for healthcare professionals and those interested in promoting alternative medicine within the National Health Service. The College originated from the collapse of Charles, Prince of Wales' controversial \"Foundation for Integrated Health\", which closed after allegations of money-laundering and fraud.\n\nIts officers and associates include current and former senior NHS staff. Its current Vice Presidents are Sir Muir Gray, currently Chief Knowledge Officer of the NHS, Duncan Selbie, who is also Chief Executive of Public Health England and Harry Brünjes, founder of the Premier Medical Group. Its first President was Graeme Catto, former President of the General Medical Council.\n\nIts current Chairman is Michael Dixon.\n\nOne director of the College, Michael Dixon, is a former director of the \"Foundation for Integrated Health\". A former director of the College, George Lewith, was a council member of the foundation and his research unit at the University of Southampton played an important role in the development of the foundation.\n\nThe \"Foundation for Integrated Health\" promoted alternative medicine (using the term \"integrative medicine\") and closed down in 2010 after an accounting fraud.\n\nWhen the College of Medicine was launched, several commentators writing in the \"Guardian\" and the \"BMJ\", expressed the opinion that the new organisation was simply a re-branding of the Prince's \"Foundation\", some describing it as \"Hamlet without the Prince\".\n\nAlternative medicine critic and pharmacologist David Colquhoun has argued that the College is extremely well-funded and seemed from the beginning to be very confident of the Prince's support, explicitly describing its mission as \"to take forward the vision of HRH the Prince of Wales\" in an early presentation.\n\nThe College responded to this initial criticism by stating that it aims were to \"promote a more politically and professionally transparent, patient centred, and sustainable approach to healthcare, using whatever social or therapeutic approaches are safe, effective, and empowering for patients\".\n\n"}
{"id": "22212218", "url": "https://en.wikipedia.org/wiki?curid=22212218", "title": "Community health worker", "text": "Community health worker\n\nCommunity health worker (CHW) are members of a community who are chosen by community members or organizations to provide basic health and medical care to their community capable of providing preventive, promotional and rehabilitation care to these communities. Other names for this type of health care provider include village health worker, community health aide, community health promoter, and lay health advisor.\n\nCommunity health workers contribute to community development and can help communities improve access to basic health services. They are most effective when they are properly trained to provide information and services to the community. Community health workers are the most promising form of delivering health services to resource-constrained areas. They are seen as secondary health services in most low-income countries are available as a service to the community.\n\nIn many developing countries, especially in Sub-Saharan Africa, there are critical shortages of highly educated health professionals. Current medical and nursing schools cannot train enough workers to keep up with increasing demand for health care services, internal and external emigration of health workers, deaths from AIDS and other diseases, low workforce productivity, and population growth. Community health workers are given a limited amount of training, supplies and support to provide essential primary health care services to the population. Programs involving CHWs in China, Brazil, Iran and Bangladesh have demonstrated that utilizing such workers can help improve health outcomes for large populations in under-served regions. \"Task shifting\" of primary care functions from professional health workers to community health workers is considered to be a means to make more efficient use of the human resources currently available and improving the health of millions at reasonable cost.\n\nIt is unclear where the usage of community health workers began, although China and Bangladesh have been cited as possible origins. Melinda Gates, co-founder of the Bill & Melinda Gates Foundation, said the nongovernmental organization BRAC in Bangladesh \"pioneered the community health worker model.\" Catherine Lovell writes that BRAC's decision to train locally recruited paramedics was \"based on the Chinese barefoot doctor model then becoming known worldwide.\"\n\nScientific medicine has evolved slowly over the last few millennia and very rapidly over the last 150 years or so. As the evidence mounted of its effectiveness, belief and trust in the traditional ways waned. The rise of university-based medical schools, the increased numbers of trained physicians, the professional organizations they created, and the income and attendant political power they generated resulted in license regulations. Such regulations were effective in improving the quality of medical care but also resulted in a reduced supply of clinical care providers. This further increased the fees doctors could charge and encouraged them to concentrate in larger towns and cities where the population was denser, hospitals were more available, and professional and social relationships more convenient.\n\nIn the 1940s Chairman Mao Tse Tung in China faced these problems. His anger at the \"urban elite\" medical profession over the maldistribution of medical services resulted in the creation of \"Barefoot doctors\". Hundreds of thousands of rural peasants, chosen by their colleagues, were given rudimentary training and assigned medical and sanitation duties in addition to the collective labor they owed the commune. By 1977 there were over 1.7 million barefoot doctors. As professionally trained doctors and nurses became more available, the program was abolished in 1981 with the end of agricultural communes. Many Barefoot Doctors passed an examination and went to medical school. Many became health aides and some were relieved of duty.\n\nBrazil undertook a medical plan named the Family Health Program in the 1990s that made use of large numbers of community health agents. Between 1990 and 2002 the infant mortality rate dropped from about 50 per 1000 live births to 29.2. During that period the Family Health Program increased its coverage of the population from 0 to 36%. The largest impact appeared to be a reduction of deaths from diarrhea. Though the program utilized teams of physicians, nurses and CHWs, it could not have covered the population it did without the CHW. Additionally, there is evidence in Brazil that the shorter period of training does not reduce the quality of care. In one study workers with a shorter length of training complied with child treatment guidelines 84% of the time whereas those with longer training had 58% compliance.\n\nIran utilizes large numbers of para-professionals called \"behvarz\". These workers are from the community and are based in 14,000 \"health houses\" nationwide. They visit the homes of the underserved providing vaccinations and monitoring child growth. Between 1984 and 2000 Iran was able to cut its infant mortality in half and raise immunization rates from 20 to 95%. The family planning program in Iran is considered highly successful. Fertility has dropped from 5.6 lifetime children per woman in 1985 to 2 in 2000. Though there are many elements to the program (including classes for those who marry and the ending of tax incentives for large families), behvarz are extensively involved in providing birth control advice and methods. The proportion of rural women on contraceptives in 2000 was 67%. The program resulted in profound improvement in maternal mortality going from 140 per 100,000 in 1985 to 37 in 1996.\n\nThe Government of Liberia launched the National Community Health Assistant Program in 2016 to accelerate progress towards universal health coverage for the most vulnerable populations, especially those in remote communities. Liberia's program seeks to transform an existing cadre of unpaid and poorly coordinated CHWs into a more effective workforce by enhancing recruitment, supervision and compensation. The health ministry has organized a coalition of funding and implementation partners to support this new program, which aims to train, supervise, equip and pay 4000 Community Health Assistants, supported by 400 clinical supervisors, to extend primary care services to 1.2 million people living in remote rural communities.\n\nThe World Health Organization estimates there are over 1.3 million community health workers worldwide. In addition to the large-scale implementation by countries such as China, Brazil, and Iran, many countries have implemented CHW programs in small-scale levels for a variety of health issues.\n\nIn India, community health workers have been utilized to increase mental health service utilization and decrease stigma associated with mental illness. In this program respected female members of the community were chosen to participate. All of the women were married, came from a good social standing, displayed a keen interest in the program, and were encouraged by their family to participate. The women chosen were then trained in identification and referral of patients with mental illnesses, the common myths and misconceptions prevalent in the area and in conducting community surveys. The training lasted 3 days and included lectures, role plays and observation of patient interviews at the psychiatry outpatient department at St. John's Medical College Hospital. A population of 12,886 were surveyed using a brief questionnaire. Out of this population, 574 were suspected patients. Out of this 242 suspected patients visited the clinic after follow up from the community health worker. Also in India, The MINDS Foundation has developed a grassroots program targeted at providing mental health services to rural citizens. They leave the responsibility in the hands of local rural citizens who are trained as Community Mental Healthcare Workers (CMHWs).\n\nIn Tanzania, village health workers were part of a community-based safe-motherhood approach. The VHWs assisted pregnant women with birth planning, which included timely identification of danger signs, preparation, and accumulation of two or more essential supplies such as soap, razors, gloves for clean delivery, and mobilizing household resources, people and money to manage a possible emergency. Approximately one year after the CBRHP's major interventions ceased in these communities, most of the VHWs continued to do health promotion by visiting pregnant women, teaching them about birth planning and danger signs, and assisting them in obtaining both prenatal and obstetric services. Local VHW associations are forming with support from local political leaders, the Ministry of Health, and the non-governmental organization CARE to sustain the work of the VHWs. The community development officers, some of whom were also the master trainers, are involved in spearheading the formation of VHW organizations.\n\nIn Mali, community health workers with the Mali Health Organizing Project in Bamako have helped reduce child mortality (under 5 years old) in their community to less than 1%, compared to a national average of 19%.\n\nThe use of CHWs is not limited to developing countries. In New York, CHWs have been deployed across the state to provide care to patients with chronic illnesses like diabetes that require sustained, comprehensive care. They work in both rural communities where access to primary care is sparse, and in urban communities where they are better able to bridge communication gaps that may arise between patients and doctors. They are seen to play an important role in assisting patients with navigating a complex, uncoordinated health care system.\n\nA randomized controlled intervention on the U.S.-Mexico border, used \"promotoras\" or \"female promoters\" to increase the number of women utilizing routine preventive examinations. The control group received a postcard reminding women to get preventive screening. The free comprehensive clinical exam included a Pap test, a clinical breast exam, human papillomavirus (HPV) testing, blood draw for total cholesterol and blood glucose, and a blood pressure measurement. The other group received the same postcard and a follow-up visit from a promotora. The group that was followed up by a promotora saw a 35% increase in visits to get the free screening.\n\nA program in Karnataka, India took a slightly different approach now referred to as the \"link worker\" model. The Samastha project developed a network in which trained workers, village health committees, government facilities, people living with HIV (PLHIV) networks, and participating NGOs collaborated to improve recruitment and retention of PLHIV while strengthening and supporting their adherence to treatment. Link workers were PLHIV who were selected by Samastha from a small number of HIV-positive candidates proposed by their community; they received an allowance for their work. The link workers' key tasks revolved around prevention, stigma reduction, and support for PLHIV that included adherence support to both treatment and care. Ultimately, the link workers' coordinating role became a hallmark of Samastha's interventions in high prevalence rural areas. Link workers formed the essential connection between PLHIV, government and community structures, and HIV care and treatment services, commonly accompanying persons from their catchment area to these services.\n\nCommunity health workers have also been utilized to assist in research. Martin et al. found that the Latin-American population in the United States frequently does not benefit from health programs due to language barriers, distrust of the government, and unique health beliefs and practices, and specifically that providing effective asthma care to the Latino population is an enormous challenge. In addition, they found that Latinos are also often excluded from research due to a lack of validated research instruments in Spanish, unsuccessful study recruitment, and a limited number of Latino researchers. Thus, Martin and colleagues decided to use community health workers to recruit participants. To gauge the effectiveness of their recruitment strategy to other more traditional recruitment models they looked at two studies. Both these studies offered significant monetary incentives for participation while the CHW study offered nothing for the initial participation. Martin et al. found that individuals who chose not to participate in the study went on to receive other services in the areas of diabetes and cancer prevention, which was not the case for the other studies.\n\nIn the Philippines, community health workers are known as barangay health volunteers. With decentralization of healthcare through the 1991 Local Government Code, the responsibility of delivery of primary health services were transferred from the central government to locally elected provincial, city, and municipal governments. They provide preventative health services and treatments, especially for the poor.\n\nCost and access to medical care remain problems of worldwide scope. They are particularly severe in the developing world and it is estimated one million more health care workers are needed in Africa to meet the health-related Millennium Development Goals. Doctors are few and concentrated in cities. In Uganda, some 70% of medical doctors and 40% of nurses and midwives are based in urban areas, serving only 12% of the population. Medical training is long and expensive. It is estimated that to meet health workforce needs using the American or European model, Africa would need to build 300 medical schools with a total training cost of over $33 billion and it would take over 20 years just to catch up. In many countries salaries of doctors and nurses are less than that of engineers and teachers. Bright young medical professionals often leave practice for more lucrative opportunities. Emigration of trained personnel to countries with higher salaries is high. In Zambia of the 600 doctors trained since independence it is estimated only 50 practice in their home country. In some countries, AIDS is killing experienced nurses and doctors amounting to 30-50% of the number trained yearly. Though many countries have increased their spending on health care and foreign money has been injected, much of it has been on specific disease-oriented programs. Health systems remain extremely weak, especially in rural areas. The World Health Assembly in 2006 called for, \"A health workforce which is matched in number, knowledge and skill sets to the needs of the population and which contributes to the achievement of health outcomes by utilizing a range of innovative methods\".\n\nCommunity health workers are thought to be part of the answer. They can be trained to do specialized tasks such as provide sexually transmitted disease counseling, directly observed therapy for tuberculosis control, or act as trained birth attendants. Others work on specific programs performing limited medical evaluations and treatment. Others have a far broader primary care function. With training, monitoring, supervision, and support such workers have been shown to be able to achieve outcomes far better than baseline and in some studies, better than physicians.\n\nImportant attributes of community health workers are to be a member of and chosen by the community they serve. This means they are easily accepted by their fellows and have natural cultural awareness. This is crucial because many communities are disengaged from the formal health system. In Sub-Saharan Africa, 53% of the poorest households do not seek care outside the home. Barriers include clinic fees, distance, community beliefs and the perception of the skills and attitudes of medical clinic workers. Community health workers are unable to emigrate because they do not have internationally recognized qualifications. Finally, the variation in incentives between areas of the country tends to be low. All these factors combined with strong community ties, tend to result in retention at the community level.\n\nMuch remains to be learned about the recruitment, training, functions, incentives, retention and professional development of community health workers. Learning developed in one country may not be applicable to another due to cultural differences. Health worker adaptability to local requirements and needs is key to improving medical outcomes. That being said, it has been estimated that six million children’s lives a year could be saved if 23 evidence-based interventions were provided systematically the children living in the 42 countries responsible for 90% of childhood mortality. Over 50% of this benefit could be obtained with an integrated, high-coverage, family-community care based system. Community health workers may be an integral and crucial component of the health human resources team needed to achieve such goals.\n\n"}
{"id": "56285", "url": "https://en.wikipedia.org/wiki?curid=56285", "title": "Disability", "text": "Disability\n\nA disability is an impairment that may be cognitive, developmental, intellectual, mental, physical, sensory, or some combination of these. It substantially affects a person's life activities and may be present from birth or occur during a person's lifetime.\nDisability is a contested concept, with different meanings in different communities. It may be used to refer to physical or mental attributes that some institutions, particularly medicine, view as needing to be fixed (the medical model). It may refer to limitations imposed on people by the constraints of an ableist society (the social model). Or the term may serve to refer to the identity of disabled people. Physiological functional capacity (PFC) is a related term that describes an individual's performance level. It gauges one's ability to perform the physical tasks of daily life and the ease with which these tasks are performed. PFC declines with advancing age to result in frailty, cognitive disorders or physical disorders, all of which may lead to labeling individuals as disabled.\n\nThe discussion over disability's definition arose out of disability activism in the United States and the United Kingdom in the 1970s, which challenged how the medical concept of disability dominated perception and discourse about disabilities. Debates about proper terminology and their implied politics continue in disability communities and the academic field of disability studies. In some countries, the law requires that disabilities are documented by a healthcare provider in order to assess qualifications for disability benefits.\n\nContemporary understandings of disability derive from concepts that arose during the West's scientific Enlightenment; prior to the Enlightenment, physical differences were viewed through a different lens.\n\nDuring the Middle Ages, madness and other conditions were thought to be caused by demons. They were also thought to be part of the natural order, especially during and in the fallout of the Plague, which wrought impairments throughout the general population. In the early modern period there was a shift to seeking biological causes for physical and mental differences, as well as heightened interest in demarcating categories: for example, Ambroise Pare, in the sixteenth century, wrote of \"monsters\", \"prodigies\", and \"the maimed\". The European Enlightenment's emphases on knowledge derived from reason and on the value of natural science to human progress helped spawn the birth of institutions and associated knowledge systems that observed and categorized human beings; among these, the ones significant to the development of today's concepts of disability were asylums, clinics, and, prisons.\n\nContemporary concepts of disability are rooted in eighteenth- and nineteenth-century developments. Foremost among these was the development of clinical medical discourse, which made the human body visible as a thing to be manipulated, studied, and transformed. These worked in tandem with scientific discourses that sought to classify and categorize and, in so doing, became methods of normalization.\n\nThe concept of the \"norm\" developed in this time period, and is signaled in the work of the Belgian statistician, sociologist, mathematician, and astronomer Adolphe Quetelet, who wrote in the 1830s of l'homme moyen – the average man. Quetelet postulated that one could take the sum of all people's attributes in a given population (such as their height or weight) and find their average, and that this figure should serve as a norm toward which all should aspire.\n\nThis idea of a statistical norm threads through the rapid take up of statistics gathering by Britain, United States, and the Western European states during this time period, and it is tied to the rise of eugenics. Disability, as well as other concepts including: abnormal, non-normal, and normalcy came from this. The circulation of these concepts is evident in the popularity of the freak show, where showmen profited from exhibiting people who deviated from those norms.\n\nWith the rise of eugenics in the latter part of the nineteenth century, such deviations were viewed as dangerous to the health of entire populations. With disability viewed as part of a person's biological make-up and thus their genetic inheritance, scientists turned their attention to notions of weeding such \"deviations\" out of the gene pool. Various metrics for assessing a person's genetic fitness, which were then used to deport, sterilize, or institutionalize those deemed unfit. At the end of the Second World War, with the example of Nazi eugenics, eugenics faded from public discourse, and increasingly disability cohered into a set of attributes that medicine could attend to – whether through augmentation, rehabilitation, or treatment. In both contemporary and modern history, disability was often viewed as a by-product of incest between first-degree relatives or second-degree relatives.\n\nIn the early 1970s, disability activists began to challenge how society treated disabled people and the medical approach to disability. Due to this work, physical barriers to access were identified. These conditions functionally disabled them, and what is now known as the social model of disability emerged. Coined by Mike Oliver in 1983, this phrase distinguishes between the medical model of disability – under which an impairment needs to be fixed – and the social model of disability – under which the society that limits a person needs to be fixed.\n\nDifferent terms have been used for disabled people in different times and places. \"Disability\" or \"impairment\" are commonly used, as are more specific terms, such as \"blind\" (to describe having no vision at all) or \"visually impaired\" (to describe having limited vision).\n\n\"Handicap\" has been disparaged as a result of false folk etymology that says it is a reference to begging. It is actually derived from an old game, Hand-i'-cap, in which two players trade possessions and a third, neutral person judges the difference of value between the possessions. The concept of a neutral person evening up the odds was extended to handicap racing in the mid-18th century. In handicap racing, horses carry different weights based on the umpire's estimation of what would make them run equally. The use of the term to describe a person with a disability—by extension from handicap racing, a person carrying a heavier burden than normal—appeared in the early 20th century.\n\nPeople-first language is one way to talk about disability that some people prefer. Using people-first language is said to put the person before the disability, so those individuals who prefer people-first language, prefer to be called, \"a person with a disability\". Some people prefer person-first phrasing, while others prefer identity-first phrasing.\n\nFor people-first guidelines, check out, \"Cerebral Palsy: A Guide for Care\" at the University of Delaware:\"The American Psychological Association style guide states that, when identifying a person with a disability, the person's name or pronoun should come first, and descriptions of the disability should be used so that the disability is identified, but is not modifying the person. Acceptable examples included \"a woman with Down syndrome\" or \"a man who has schizophrenia\". It also states that a person's adaptive equipment should be described functionally as something that assists a person, not as something that limits a person, for example, \"a woman who uses a wheelchair\" rather than \"a woman in/confined to a wheelchair\".A similar kind of \"people-first\" terminology is also used in the UK, but more often in the form \"people with impairments\" (such as \"people with visual impairments\"). However, in the UK, the term \"disabled people\" is generally preferred to \"people with disabilities\". It is argued under the social model that while someone's impairment (for example, having a spinal cord injury) is an individual property, \"disability\" is something created by external societal factors such as a lack of accessibility. This distinction between the individual property of impairment and the social property of disability is central to the social model. The term \"disabled people\" as a political construction is also widely used by international organisations of disabled people, such as Disabled Peoples' International (DPI).\n\nThe use of “people-first” terminology has given rise to the use of the acronym PWD to refer to person(s) (or people) with disabilities (or disability). However other individuals and groups prefer identity-first language to emphasize how a disability can impact peoples identities. Which style of language used varies between different countries, groups and individuals.\n\nTo a certain degree, physical impairments and changing mental states are almost ubiquitously experienced by people as they age. Aging populations are often stigmatized for having a high prevalence of disability. Kathleen Woodward, writing in \"Key Words for Disability Studies\", explains the phenomenon as follows:\nAs stated above, studies have illustrated a correlation between disabilities and poverty. Notably, jobs offered to disabled people are scarce. \"For global demographic data on unemployment rates for the disabled, see Disability and poverty.\" However, there are current programs in place that aid intellectually disabled (ID) people to acquire skills they need in the workforce. Such programs include sheltered workshops and adult day care programs. Sheltered programs consist of daytime activities such as, gardening, manufacturing, and assembling. These activities facilitate routine-oriented tasks that in turn allow intellectually disabled people to gain experience before entering the workforce. Similarly, adult day care programs also include day time activities. However, these activities are based in an educational environment where intellectually disabled are able to engage in educational, physical, and communication based tasks. This educational based environment helps facilitate communication, memory, and general living skills. In addition, adult day care programs arrange opportunities for their students to engage in community activities. Such opportunities are arranged by scheduling field trips to public places (e.g. Disneyland, Zoo, and Movie Theater). Despite, both programs providing essential skills for intellectually disabled prior to entering the workforce researchers have found that intellectually disabled people prefer to be involved with community-integrated employment. Community-integrated employment are job opportunities offered to intellectually disabled people at minimum wage or a higher rate depending on the position. Community-integrated employment comes in a variety of occupations ranging from customer service, clerical, janitorial, hospitality and manufacturing positions. Within their daily tasks community-integrated employees work alongside employees who do not have disabilities, but who are able to assist them with training. All three options allow intellectually disabled people to develop and exercise social skills that are vital to everyday life. However, it is not guaranteed that community-integrated employees receive the same treatment as employees that do not have ID. According to Lindstrom, Hirano, McCarthy, and Alverson, community-integrated employees are less likely to receive raises. In addition, studies conducted in 2013 illustrated only 26% of employees with ID retained full-time status.\n\nFurthermore, many with disabilities, intellectual and (or) psychical, finding a stable workforce poses many challenges. According to a study conducted by JARID (Journal of Applied Research and Intellectual Disability, indicates that although finding a job may be difficult for an intellectually disabled individual, stabilizing a job is even harder. This is largely due to two main factors: production skills and effective social skills. This idea is supported by Chadsey-Rusch, who claims that securing employment for the intellectually disabled, requires adequate production skills and effective social skills. However, other underlying factors for job loss include, structural factors and the integration between worker and workplace. As stated by Kilsby, limited structural factors can effect a multitude of factors in a job. Factors such as a restricted number of hours an intellectually disabled person is allowed to work. This in return, according to Fabian, Wistow, and Schneider leads to a lack of opportunity to develop relationships with coworkers and a chance to better integrate within the workplace. Nevertheless, those who are unable to stabilize a job often are left discouraged. According to the same study conducted by JARED, many who had participated, found that they had made smaller incomes when compared to their co-workers, had an excess of time throughout their days, because they did not have work. They,also, had feelings of hopelessness and failure. According to the NOD ( National Organization On Disability), not only do the (ID) face constant discouragement, but many live below the poverty line, because they are unable to find or stabilize employment and (or) because of employee restricting factors placed on ID workers. This then causes the (ID) the incapacity to provide for themselves basic necessities one needs. Items such as, food, medical care, transportation, and housing.\n\nThere is a global correlation between disability and poverty, produced by a variety of factors. Disability and poverty may form a vicious circle, in which physical barriers and stigma of disability make it more difficult to get income, which in turn diminishes access to health care and other necessities for a healthy life. The World report on disability indicates that half of all disabled people cannot afford health care, compared to a third of abled people. In countries without public services for adults with disabilities, their families may be impoverished.\n\nThere is limited research knowledge, but many anecdotal reports, on what happens when disasters impact disabled people. Individuals with disabilities are greatly affected by disasters. Those with physical disabilities can be at risk when evacuating if assistance is not available. Individuals with cognitive impairments may struggle with understanding instructions that must be followed in the event a disaster occurs. All of these factors can increase the degree of variation of risk in disaster situations with disabled individuals.\n\nResearch studies have consistently found discrimination against individuals with disabilities during all phases of a disaster cycle. The most common limitation is that people cannot physically access buildings or transportation, as well as access disaster-related services. The exclusion of these individuals is caused in part by the lack of disability-related training provided to emergency planners and disaster relief personnel.\n\nThe International Classification of Functioning, Disability and Health (ICF), produced by the World Health Organization, distinguishes between body functions (physiological or psychological, such as vision) and body structures (anatomical parts, such as the eye and related structures). Impairment in bodily structure or function is defined as involving an anomaly, defect, loss or other significant deviation from certain generally accepted population standards, which may fluctuate over time. Activity is defined as the execution of a task or action. The ICF lists 9 broad domains of functioning which can be affected:\nIn concert with disability scholars, the introduction to the ICF states that a variety of conceptual models have been proposed to understand and explain disability and functioning, which it seeks to integrate. These models include the following:\n\nThe medical model views disability as a problem of the person, directly caused by disease, trauma, or other health conditions which therefore requires sustained medical care in the form of individual treatment by professionals. In the medical model, management of the disability is aimed at a \"cure\", or the individual's adjustment and behavioral change that would lead to an \"almost-cure\" or effective cure. In the medical model, medical care is viewed as the main issue, and at the political level, the principal response is that of modifying or reforming healthcare policy.\n\nThe social model of disability sees \"disability\" as a socially created problem and a matter of the full integration of individuals into society. In this model, disability is not an attribute of an individual, but rather a complex collection of conditions, created by the social environment. The management of the problem requires social action and it is the collective responsibility of society to create a society in which limitations for disabled people are minimal. Disability is both cultural and ideological in creation. According to the social model, equal access for someone with an impairment/disability is a human rights concern. The social model of disability has come under criticism. While recognizing the importance played by the social model in stressing the responsibility of society, scholars, including Tom Shakespeare, point out the limits of the model, and urge the need for a new model that will overcome the \"medical vs. social\" dichotomy. The limitations of this model mean that often the vital services and information persons with disabilities face are simply not available, often due to limited economic returns in supporting them.\n\nSome say medical humanities is a fruitful field where the gap between the medical and the social model of disability might be bridged.\n\nThe social construction of disability is the idea that disability is constructed by social expectations and institutions rather than biological differences. Highlighting the ways society and institutions construct disability is one of the main focuses of this idea. In the same way that race and gender are not biologically fixed, neither is disability.\n\nAround the early 1970s, sociologists, notably Eliot Friedson, began to argue that labeling theory and social deviance could be applied to disability studies. This led to the creation of the social construction of disability theory. The social construction of disability is the idea that disability is constructed as the social response to a deviance from the norm. The medical industry is the creator of the ill and disabled social role. Medical professionals and institutions, who wield expertise over health, have the ability to define health and physical and mental norms. When an individual has a feature that creates an impairment, restriction, or limitation from reaching the social definition of health, the individual is labeled as disabled. Under this idea, disability is not defined by the physical features of the body but by a deviance from the social convention of health.\n\nSocial construction of disability would argue that the medical model of disability's view that a disability is an impairment, restriction, or limitation is wrong. Instead what is seen as a disability is just a difference in the individual from what is considered \"normal\" in society.\n\n\nIn contexts where their differences are visible, persons with disabilities often face stigma. People frequently react to disabled presence with fear, pity, patronization, intrusive gazes, revulsion, or disregard. These reactions can, and often do, exclude persons with disabilities from accessing social spaces along with the benefits and resources these spaces provide. Disabled writer/researcher Jenny Morris describes how stigma functions to marginalize persons with disabilities:\n\nAdditionally, facing stigma can cause harm to psycho-emotional well-being of the person being stigmatized. One of the ways in which the psycho-emotional health of persons with disabilities is adversely affected is through the internalization of the oppression they experience, which can lead to feeling that they are weak, crazy, worthless, or any number of other negative attributes that may be associated with their conditions. Internalization of oppression damages the self-esteem of the person affected and shapes their behaviors in ways that are compliant with nondisabled dominance. Ableist ideas are frequently internalized when disabled people are pressured by the people and institutions around them to hide and downplay their disabled difference, or, \"pass\". According to writer Simi Linton, the act of passing takes a deep emotional toll by causing disabled individuals to experience loss of community, anxiety and self-doubt. The media play a significant role in creating and reinforcing stigma associated with disability. Media portrayals of disability usually cast disabled presence as necessarily marginal within society at large. These portrayals simultaneously reflect and influence the popular perception of disabled difference.\n\nThere are distinct tactics that the media frequently employ in representing disabled presence. These common ways of framing disability are heavily criticized for being dehumanizing and failing to place importance on the perspectives of persons with disabilities.\n\nInspiration porn refers to portrayals of persons with disabilities in which they are presented as being inspiring simply because the person has a disability. These portrayals are criticized because they are created with the intent of making able-bodied viewers feel better about themselves in comparison to the individual portrayed. Rather than recognizing the humanity of persons with disabilities, inspiration porn turns them into objects of inspiration for a nondisabled audience.\n\nThe supercrip trope refers to instances when media reports on or portray a disabled person who has made a noteworthy achievement; but center on their disability rather than what they actually did. They are portrayed as awe-inspiring for being exceptional compared to others with the same or similar conditions. This trope is widely used in reporting on disabled athletes as well as in portrayals of autistic savants.\n\nMany disabled people denounce these representations as reducing people to their condition rather than viewing them as full people. Furthermore, supercrip portrayals are criticized for creating the unrealistic expectation that disability should be accompanied by some type of special talent, genius, or insight.\n\nCharacters in fiction that bear physical or mental markers of difference from perceived societal norms are frequently positioned as villains within a text. Lindsey Row-Heyveld notes, for instance, “that villainous pirates are scraggly, wizened, and inevitably kitted out with a peg leg, eye patch, or hook hand whereas heroic pirates look like Johnny Depp's Jack Sparrow.” Disabled people's visible differences from the abled majority are meant to evoke fear in audiences that can perpetuate the mindset of disabled people being a threat to individual or public interests and well-being.\n\nSome disabled people have attempted to resist marginalisation through the use of the social model in opposition to the medical model; with the aim of shifting criticism away from their bodies and impairments and towards the social institutions that oppress them relative to their abled peers. Disability activism that demands many grievances be addressed, such as lack of accessibility, poor representation in media, general disrespect, and lack of recognition, originates from a social model framework.\n\nEmbracing disability as a positive identity by becoming involved in disabled communities and participating in disabled culture can be an effective way to combat internalised prejudice; and can challenge dominant narratives about disability.\n\nThe experiences that disabled people have navigating social institutions vary greatly as a function of what other social categories they may belong to. The categories that intersect with disability to create unique experiences of ableism include, but aren't limited to, race and gender. The United Nations Convention on the Rights of Persons with Disabilities differentiates two kinds of disability intersection, race disability intersection and gender disability intersection.\n\nDisabled people who are also racial minorities generally have less access to support and are more vulnerable to violent discrimination. For example, in the United States people of color who are mentally ill are more frequently victims of police brutality than their white counterparts. Camille A. Nelson, writing for the \"Berkeley Journal of Criminal Law\", notes that for “people who are negatively racialized, that is people who are perceived as being non-white, and for whom mental illness is either known or assumed, interaction with police is precarious and potentially dangerous.”\n\nThe marginalization of disabled people can leave persons with disabilities unable to actualize what society expects of gendered existence. This lack of recognition for their gender identity can leave persons with disabilities with feelings of inadequacy. Thomas J. Gerschick of Illinois State University describes why this denial of gendered identity occurs:\n\nTo the extent that women and men with disabilities are gendered, the interactions of these two identities lead to different experiences. Disabled women face a sort of “double stigmatization” in which their membership to both of these marginalized categories simultaneously exacerbates the negative stereotypes associated with each as they are ascribed to them. According to The UN Woman Watch, \"Persistence of certain cultural, legal and institutional barriers makes women and girls with disabilities the victims of two-fold discrimination: as women and as persons with disabilities.\" As Rosemarie Garland-Thomson puts it, “Women with disabilities, even more intensely than women in general, have been cast in the collective cultural imagination as inferior, lacking, excessive, incapable, unfit, and useless.”\n\nAssistive Technology is a generic term for devices and modifications (for a person or within a society) that help overcome or remove a disability. The first recorded example of the use of a prosthesis dates to at least 1800 BC. The wheelchair dates from the 17th century. The curb cut is a related structural innovation. Other examples are standing frames, text telephones, accessible keyboards, large print, Braille, & speech recognition software. Disabled people often develop personal or community adaptations, such as strategies to suppress tics in public (for example in Tourette's syndrome), or sign language in deaf communities.\n\nAs the personal computer has become more ubiquitous, various organizations have formed to develop software and hardware to make computers more accessible for disabled people. Some software and hardware, such as Voice Finger, Freedom Scientific's \"JAWS\", the Free and Open Source alternative \"Orca\" etc. have been specifically designed for disabled people while other software and hardware, such as Nuance's Dragon NaturallySpeaking, were not developed specifically for disabled people, but can be used to increase accessibility. The LOMAK keyboard was designed in New Zealand specifically for persons with disabilities.\nThe World Wide Web consortium recognised a need for International Standards for Web Accessibility for persons with disabilities and created the Web Accessibility Initiative (WAI). As at Dec 2012 the standard is WCAG 2.0 (WCAG = Web Content Accessibility Guidelines).\n\nThe Paralympic Games (meaning \"alongside the Olympics\") are held after the (Summer and Winter) Olympics. The Paralympic Games include athletes with a wide range of physical disabilities. In member countries, organizations exist to organize competition in the Paralympic sports on levels ranging from recreational to elite (for example, Disabled Sports USA and BlazeSports America in the United States).\n\nThe Paralympics developed from a rehabilitation programme for British war veterans with spinal injuries. In 1948, Sir Ludwig Guttman, a neurologist working with World War II veterans with spinal injuries at Stoke Mandeville Hospital in Aylesbury in the UK, began using sport as part of the rehabilitation programmes of his patients.\n\nIn 2006, the Extremity Games were formed for physically disabled people, specifically limb loss or limb difference, to be able to compete in extreme sports.\n\nThe disability rights movement aims to secure equal opportunities and equal rights for disabled people. The specific goals and demands of the movement are accessibility and safety in transportation, architecture, and the physical environment; equal opportunities in independent living, employment, education, and housing; and freedom from abuse, neglect, and violations of patients' rights. Effective civil rights legislation is sought to secure these opportunities and rights.\n\nThe early disability rights movement was dominated by the medical model of disability, where emphasis was placed on curing or treating disabled people so that they would adhere to the social norm, but starting in the 1960s, rights groups began shifting to the social model of disability, where disability is interpreted as an issue of discrimination, thereby paving the way for rights groups to achieve equality through legal means.\n\nOn December 13, 2006, the United Nations formally agreed on the Convention on the Rights of Persons with Disabilities, the first human rights treaty of the 21st century, to protect and enhance the rights and opportunities of the world's estimated 650 million disabled people. , 99 of the 147 signatories had ratified the Convention. Countries that sign the convention are required to adopt national laws, and remove old ones, so that persons with disabilities will, for example, have equal rights to education, employment, and cultural life; to the right to own and inherit property; to not be discriminated against in marriage, etc.; and to not be unwilling subjects in medical experiments. UN officials, including the High Commissioner for Human Rights, have characterized the bill as representing a paradigm shift in attitudes toward a more rights-based view of disability in line with the social model.\n\nIn 1976, the United Nations began planning for its International Year of Disabled Persons (1981), later renamed the International Year of Disabled Persons. The UN Decade of Disabled Persons (1983–1993) featured a World Programme of Action Concerning Disabled Persons. In 1979, Frank Bowe was the only person with a disability representing any country in the planning of IYDP-1981. Today, many countries have named representatives who are themselves individuals with disabilities. The decade was closed in an address before the General Assembly by Robert Davila. Both Bowe and Davila are deaf. In 1984, UNESCO accepted sign language for use in education of deaf children and youth.\n\nIn the United States, the Department of Labor's 2014 rules for federal contractors, defined as companies that make more than $50,000/year from the federal government, required them to have as a goal that 7% of their workforce must be disabled people. In schools, the ADA says that all classrooms must be wheelchair accessible. The U.S. Architectural and Transportation Barriers Compliance Board, commonly known as the Access Board, created the Rehabilitation Act of 1973 to help offer guidelines for transportation and accessibility for the physically disabled.\n\nAbout 12.6% of the U.S. population are individuals who suffer from a mental or physical disability. Many are unemployed because of prejudiced assumptions that a person with disabilities is unable to complete tasks that are commonly required in the workforce. This became a major Human rights issue because of the discrimination that this group faced when trying to apply for jobs in the U.S. Many advocacy groups protested against such discrimination, asking the federal government to implement laws and policies that would help individuals with disabilities.\n\nThe Rehabilitation Act of 1973 was enacted with the purpose of protecting individuals with disabilities from prejudicial treatment by government funded programs, employers, and agencies. The Rehabilitation Act of 1973 has not only helped protect U.S. citizens from being discriminated against but it has also created confidence amongst individuals to feel more comfortable with their disability. There are many sections within The Rehabilitation Act of 1973, that contains detailed information about what is covered in this policy.\n\n\nThe federal government enacted The Americans with Disabilities Act of 1990, which was created to allow equal opportunity for jobs, access to private and government funded facilities, and transportation for disabled people. This act was created with the purpose to ensure that employers would not discriminate against any individual despite their disability. In 1990, data was gathered to show the percentage of disabled people who worked in the U.S. Out of the 13% who filled out the survey, only 53% percent of individuals with disabilities worked while 90% of this group population did not, the government wanted to change this, they wanted Americans with disabilities to have the same opportunities as those who did not have a disability. The ADA not only required corporations to hire disabled people but that they also accommodate them and their needs.\n\n\nIn the UK, the Department for Work and Pension is a government department responsible for promoting disability awareness and among its aims is to increase the understanding of disability and removal of barriers for disabled people in the workplace. According to a news report, a people survey conducted in the UK shows a 23% increase in reported discrimination and harassment in the workplace at The Department for Work and Pension. The survey shows the number of reports for discrimination due to disability was in majority compared to discrimination due to gender, ethnicity or age. DWP received criticism for the survey results. As a department responsible for tackling discrimination at work, the DWP results may indicate room for improvement from within. A DWP spokesperson said the survey results do not necessarily indicate an increase in the number of reports, but rather reflecting the outcomes of efforts to encourage people to come forward.\n\nPolitical rights, social inclusion and citizenship have come to the fore in developed and some developing countries. The debate has moved beyond a concern about the perceived cost of maintaining dependent disabled people to finding effective ways to ensure that disabled people can participate in and contribute to society in all spheres of life.\n\nIn developing nations, where the vast bulk of the estimated 650 million disabled people reside, a great deal of work is needed to address concerns ranging from accessibility and education to self-empowerment, self-supporting employment, and beyond.\n\nIn the past few years, disability rights activists have focused on obtaining full citizenship for the disabled.\n\nThere are obstacles in some countries in getting full employment; public perception of disabled people may vary.\n\nDisability abuse happens when a person is abused physically, financially, verbally or mentally due to the person having a disability. As many disabilities are not visible (for example, asthma, learning disabilities) some abusers cannot rationalize the non-physical disability with a need for understanding, support, and so on.\n\nAs the prevalence of disability and the cost of supporting disability increases with medical advancement and longevity in general, this aspect of society becomes of greater political importance. How political parties treat their disabled constituents may become a measure of a political party's understanding of disability, particularly in the social model of disability.\n\nDisability benefit, or disability pension, is a major kind of disability insurance that is provided by government agencies to people who are temporarily or permanently unable to work due to a disability. In the U.S., disability benefit is provided in the category of Supplemental Security Income. In Canada, it is within the Canada Pension Plan. In other countries, disability benefit may be provided under social security systems.\n\nCosts of disability pensions are steadily growing in Western countries, mainly in Europe and the United States. It was reported that, in the UK, expenditure on disability pensions accounted for 0.9% of gross domestic product (GDP) in 1980; two decades later it had reached 2.6% of GDP. Several studies have reported a link between increased absence from work due to sickness and elevated risk of future disability pension.\n\nA study by researchers in Denmark suggests that information on self-reported days of absence due to sickness can be used to effectively identify future potential groups for disability pension. These studies may provide useful information for policy makers, case managing authorities, employers, and physicians.\n\nPrivate, for-profit disability insurance plays a role in providing incomes to disabled people, but the nationalized programs are the safety net that catch most claimants.\n\nEstimates of worldwide and country-wide numbers of individuals with disabilities are problematic. The varying approaches taken to defining disability notwithstanding, demographers agree that the world population of individuals with disabilities is very large. For example, in 2012, the World Health Organization estimated a world population of 6.5 billion people. Of those, nearly 650 million people, or 10%, were estimated to be moderately or severely disabled. In 2018 the International Labour Organization estimated that about a billion people, one seventh of the world population, had disabilities, 80% of them in developing countries, and 80% of working age. Excluding disabled people from the workforce was reckoned to cost up to 7% of gross domestic product.\n\nAfter years of war in Afghanistan, there are more than 1 million disabled people. Afghanistan has one of the highest incidences of disabled people in the world. An estimated 80,000 Afghans are missing limbs, usually from landmine explosions.\n\nIn Australia, 18.5% of the population reported having a disability in a 2009 survey.\n\nAccording to the U.S. Census Bureau, , there were some 56.7 million disabled people, or 19% (by comparison, African Americans are the largest racial minority in the U.S., but only constitute 12.6% of the U.S. population).\n\nDisabled individuals make up one of the most inclusive minority groups in the United States. According to the 2014 Disability status report of the Cornell University Yang Tan Institute the prevalence rate of individuals with disabilities in the United States was 12.6% in that year. ambulatory disability had the highest prevalence (7.1%) in the United States. By contrast, visual disability had the lowest prevalence (2.3%). Additionally, 3.6% of people in the United States were reported to have had an auditory disability in the same year.\n\n5.8% of individuals ages 16–20 reported having any disability, physical and/ or cognitive. Adults 21 to 64 had a prevalence of 10.8% with over half of these (5.5%) being ambulatory disabilities. Ambulatory disability prevalence raised to 15.8% in adults 65–74 years of age. Adults 75 years and older comprised the highest prevalence with any disability at 50.3%.\n\nFemale individuals across all ages reported a total 0.4% higher prevalence rate than males who reported 12.4%.\n\nIn the U.S. 17.9% of Native American peoples reported having a disability while 4.5% reporting were of Asian descent, these were the two opposing poles of the prevalence rate within race .\n\nAlthough there are acts that have been imposed in order to prevent the discrimination of individuals with disabilities in the workplace, there is still an employment gap that can be seen between those with and without disabilities. In regards to employment, the institute's status report accounts that 34.6% of people with any disability reported being employed. By comparison; 77.6% of individuals, who did not report having a disability, reported having a full-time job in 2014.\n\nFor those employed full-time, individuals with disabilities on average earned $5,100 less than employees without a disability who were also employed full-time. Those affected the most by these differences were intellectually disabled people.\n\nNearly 8 million European men were permanently disabled in World War I. About 150,000 Vietnam veterans came home wounded, and at least 21,000 were permanently disabled. As of 2008, there were 2.9 million disabled veterans in the United States, an increase of 25 percent over 2001.\n\nThere is widespread agreement among experts in the field, that disability is more common in developing than in developed nations. The connection between disability and poverty is thought to be part of a \"vicious cycle\" in which these constructs are mutually reinforcing.\n\n\n\n\n"}
{"id": "27914146", "url": "https://en.wikipedia.org/wiki?curid=27914146", "title": "Drugs.com", "text": "Drugs.com\n\nDrugs.com is an online pharmaceutical encyclopedia which provides drug information for consumers and healthcare professionals, primarily in the USA.\n\nThe Drugs.com website is owned and operated by the Drugsite Trust. The Drugsite Trust is a privately held trust administered by two New Zealand pharmacists, Karen Ann and Phillip James Thornton.\n\nThe site contains a library of reference information which includes content from Cerner Multum, Micromedex from Truven Health Analytics, Wolters Kluwer Health, U.S. Food and Drug Administration (FDA), A.D.A.M., Stedmans, AHFS, Harvard Health Publications, Mayoclinic, North American Compendiums, and Healthday.\n\nDrugs.com is certified by the TRUSTe online privacy certification program and the HONcode of Health on the Net Foundation.\n\nThe domain Drugs.com was originally registered by Bonnie Neubeck in 1994. In 1999 at the height of the dotcom boom, Eric MacIver purchased an option to buy the domain from Neubeck. In August 1999, MacIver sold the domain at auction for US$823,666 to Venture Frogs, a startup incubator run by Tony Hsieh and Alfred Lin, best known for their involvement in LinkExchange and later Zappos.com. Venture Frogs sold the drugs.com domain name to a private investor in June 2001, allowing Hsieh & Lin to focus on Zappos.com.\n\nThe Drugs.com website was officially launched in September 2001. \n\nIn March 2008, Drugs.com announced the release of Mednotes—an online personal medication record application which connected to Google Health (On June 24, 2011 Google announced it was retiring Google Health in January 1, 2012).\n\nIn May 2010, U.S. FDA announced a collaboration with Drugs.com to distribute consumer health updates on the Drugs.com website and mobile platform.\n\nIn February 2016, comScore stated that Drugs.com was the sixth most popular health network receiving approximately 23 million visitors for the month, while Searchmetrics listed Drugs.com in the top 100 US websites for search visibility.\n\nIn April 2017, The Harris Poll listed Drugs.com as the Health Information Website Brand of the Year.\n\n"}
{"id": "33536067", "url": "https://en.wikipedia.org/wiki?curid=33536067", "title": "Femur fibula ulna syndrome", "text": "Femur fibula ulna syndrome\n\nFemur-fibula-ulna syndrome (FFU syndrome) or femur-fibula-ulna complex is a very rare syndrome characterized by abnormalities of the femur (thigh bone), fibula (calf bone) and the ulna (forearm bone). There have been suggestions that FFU complex may be the same as proximal femoral focal deficiency (PFFD) although authors are currently in disagreement over whether or not the disorders are in fact separate. The breadth of the abnormality and number of limbs involved is considered sporadic although upper limbs are more affected than lower limbs and right side malformation is more prevalent than the left. The condition was first noted by Lenz and Feldman in 1977.\n\n"}
{"id": "2192496", "url": "https://en.wikipedia.org/wiki?curid=2192496", "title": "Fungiculture", "text": "Fungiculture\n\nFungiculture is the process of producing food, medicine, and other products by the cultivation of mushrooms and other fungi. A \"mushroom farm\" is in the business of growing fungi.\n\nThe word is also commonly used to refer to the practice of cultivating fungi by leafcutter ants, termites, ambrosia beetles, and marsh periwinkles.\n\nMushrooms are not plants, and require different conditions for optimal growth. Plants develop through photosynthesis, a process that converts atmospheric carbon dioxide into carbohydrates, especially cellulose. While sunlight provides an energy source for plants, mushrooms derive all of their energy and growth materials from their growth medium, through biochemical decomposition processes. This does not mean that light is an irrelevant requirement, since some fungi use light as a signal for fruiting. However, all the materials for growth must already be present in the growth medium. Mushrooms grow well at relative humidity levels of around 95–100%, and substrate moisture levels of 50 to 75%.\n\nInstead of seeds, mushrooms reproduce asexually through spores. Spores can be contaminated with airborne microorganisms, which will interfere with mushroom growth and prevent a healthy crop.\n\nMycelium, or actively growing mushroom culture, is placed on a substrate—usually sterilized grains such as rye or millet—and induced to grow into those grains. This is called inoculation. Inoculated grains are referred to as spawn. Spores are another inoculation option, but are less developed than established mycelium. Since they are also contaminated easily, they are only manipulated in laboratory conditions with a laminar flow cabinet.\n\nAll mushroom growing techniques require the correct combination of humidity, temperature, substrate (growth medium) and inoculum (spawn or starter culture). Wild harvests, outdoor log inoculation and indoor trays all provide these elements.\n\nMushrooms can be grown on logs placed outdoors in stacks or piles, as has been done for hundreds of years. Sterilization is not performed in this method. Since production may be unpredictable and seasonal, less than 5% of commercially sold mushrooms are produced this way. Here, tree logs are inoculated with spawn, then allowed to grow as they would in wild conditions. Fruiting, or pinning, is triggered by seasonal changes, or by briefly soaking the logs in cool water. Shiitake and oyster mushrooms have traditionally been produced using the outdoor log technique, although controlled techniques such as indoor tray growing or artificial logs made of compressed substrate have been substituted.\n\nShiitake mushrooms grown under a forested canopy are considered non-timber forest products In the Northeast shiitake mushrooms can be cultivated on a variety of hardwood logs including oak, American beech, sugar maple and hophornbeam. Softwood should not be used to cultivate shiitake mushrooms. The resin of softwoods will oftentimes inhibit the growth of the shiitake mushroom making it impractical as a growing substrate.\n\nIn order to produce shiitake mushrooms, 1 metre (3 foot) hardwood logs with a diameter ranging between are inoculated with the mycelium of the shiitake fungus. Inoculation is completed by drilling holes in hardwood logs, filling the holes with cultured shiitake mycelium or inoculum, and then sealing the filled holes with hot wax. After inoculation, the logs are placed under the closed canopy of a coniferous stand and are left to incubate for 12 to 15 months. Once incubation is complete, the logs are soaked in water for 24 hours. 7 to 10 days after soaking, shiitake mushrooms will begin to fruit and can be harvested once fully ripe.\n\nIndoor growing provides the ability to tightly regulate light, temperature and humidity while excluding contaminants and pests. This allows consistent production, regulated by spawning cycles. This is typically accomplished in windowless, purpose-built buildings, for large scale commercial production.\n\nIndoor tray growing is the most common commercial technique, followed by containerized growing. The tray technique provides the advantages of scalability and easier harvesting. Unlike wild harvests, indoor techniques provide tight control over growing substrate composition and growing conditions. Indoor harvests are much more predictable.\n\nAccording to Daniel Royse and Robert Beelman, \"[Indoor] Mushroom farming consists of six steps, and although the divisions are somewhat arbitrary, these steps identify what is needed to form a production system. The six steps are phase I composting, phase II fertilizing, spawning, casing, pinning, and cropping.\"\n\nComplete sterilization is not always required or performed during composting. In some cases, a pasteurization step is not included to allow some beneficial microorganisms to remain in the growth substrate.\n\nSpecific time spans and temperatures required during stages 3–6 will vary respective to species and variety. Substrate composition and the geometry of growth substrate will also affect the ideal times and temperatures.\n\nPinning is the trickiest part for a mushroom grower, since a combination of carbon dioxide (CO) concentration, temperature, light, and humidity triggers mushrooms towards fruiting. Up until the point when rhizomorphs or mushroom \"pins\" appear, the mycelium is an amorphous mass spread throughout the growth substrate, unrecognizable as a mushroom.\n\nCarbon dioxide concentration becomes elevated during the vegetative growth phase, when mycelium is sealed in a gas-resistant plastic barrier or bag which traps gases produced by the growing mycelium. To induce pinning, this barrier is opened or ruptured. CO concentration then decreases from about 0.08% to 0.04%, the ambient atmospheric level.\n\nMushroom production converts the raw natural ingredients into mushroom tissue, most notably the carbohydrate chitin.\n\nAn ideal substrate will contain enough nitrogen and carbohydrate for rapid mushroom growth. Common bulk substrates include several of the following ingredients:\n\n\nMushrooms metabolize complex carbohydrates in their substrate into glucose, which is then transported through the mycelium as needed for growth and energy. While it is used as a main energy source, its concentration in the growth medium should not exceed 2%. For ideal fruiting, closer to 1% is ideal.\n\nOne of the most sustainable ways of mushroom cultivation is using coffee grounds as a substrate. This process was pioneered by Prof. Chang Shuting in the early 1990s while he worked at the Chinese University in Hong Kong. Coffee ground are sterile, and rich in fibers. It is more environmentally-friendly; as an estimated millions of kilos of coffee waste disposed in landfill every day, which could be diverted into sustainable food production. The spent substrate, after harvesting mushrooms, is enriched in essential amino-acids, and therefore an ideal feed for animals.\n\nParasitic insects, bacteria and other fungi all pose risks to indoor production. The sciarid fly or phorid fly may lay eggs in the growth medium, which hatch into maggots and damage developing mushrooms during all growth stages. Bacterial blotch caused by \"Pseudomonas\" bacteria or patches of \"Trichoderma\" green mold also pose a risk during the fruiting stage. Pesticides and sanitizing agents are available to use against these infestations. Biological controls for insect sciarid and phorid flies have also been proposed.\n\nA recent epidemic of Trichoderma green mold has significantly affected mushroom production: \"From 1994–96, crop losses in Pennsylvania ranged from 30 to 100%\".\n\n\n\n\nPennsylvania is the top-producing mushroom state in the United States, and celebrates September as \"Mushroom Month\".\n\nThe borough of Kennett Square is a historical and present leader in mushroom production. It currently leads production\nof Agaricus-type mushrooms, followed by California, Florida and Michigan.\n\nOther mushroom-producing states:\n\n\nVancouver, British Columbia, also has a significant number of producers — about 60 as of 1998 — mostly located in the lower Fraser Valley.\n\nThe oyster mushroom cultivation lately is taking off in Europe. Many entrepreneurs nowadays find it as a quite profitable business, a start-up with a small investment and good profit. Italy with 785,000 tonnes and Netherlands with 307,000 tonnes are between the top ten mushroom producing countries in the world. The world’s biggest producer of mushroom spawn is also situated in France.\n\nAccording to a research carried out on Production and Marketing of Mushrooms: Global and National Scenario Poland, Netherlands, Belgium, Lithuania are the major exporting mushrooms countries in Europe and countries like UK, Germany, France, Russia are considered to be the major importing countries.\n\nOyster mushroom cultivation is a sustainable business where different natural resources can be used as a substrate. The number of people becoming interested in this field is rapidly increasing. The possibility of creating a viable business in urban environments by using coffee grounds is appealing for many entrepreneurs.\n\nSince mushroom cultivation is not a subject available at school, most urban farmers learned it by doing. The time to master mushroom cultivation is time consuming and costly in missed revenue. For this reason there are numerous companies in Europe specialized in mushroom cultivation that are offering training for entrepreneurs and organizing events to build community and share knowledge. They also show the potential positive impact of this business on the environment.\n\nCourses about mushroom cultivation can be attended in many countries around Europe. There is education available for growing mushrooms on coffee grounds, more advanced training for larger scale farming, spawn production and lab work and growing facilities.\n\nEvents are organised with different intervals. The Mushroom Learning Network gathers once a year in Europe. The International Society for Mushroom Science gathers once every five year somewhere in the world.\n"}
{"id": "55086850", "url": "https://en.wikipedia.org/wiki?curid=55086850", "title": "Geoffrey D. Lehmann", "text": "Geoffrey D. Lehmann\n\nGeoffrey D. Lehmann (12 January 1904 – 15 April 1994) was a religious and medical missionary founded and built the Herbertpur Christian Hospital along with his wife Monica Lehmann, in the Herbertpur village, located in the Doon Valley in India in 1936. The hospital is now operated as the EHA Herbertpur Christian Hospital and includes the nursing school.\n\nThe aim of his mission was to spread the Christian faith among a largely Hindu and Muslim population in Herbertpur and surrounding areas as well as to provide medical care and treatment at extremely cheap or no cost to the villagers.\n\nLehmann was born in 1904 in London and attended prep school as a child. He later trained as an engineer from Oxford University, after which he pursued medicine from the same university. Lehmann specialized in ophthalmology and tropical medicine, but carried out a wide range of surgical and consultation treatments in the Herbertpur Hospital.\n\nHe also served as the Chairman of Board of Governors at Wynberg Allen School.\nLehmann was born on January 12, 1904, near the Alexandra Palace in London, England. His family resided on a farm near Sherwood in Nottinghamshire, where they raised chickens, ducks, turkeys, cows and more animals. As a child, Lehmann’s parents tried to raise him with wholesome experiences, as he enjoyed many different past-times such as: watching the Queen’s servants on the royal grounds, enjoying nature, and watching the family’s chauffeur turn on their steam engines.\n\nGrowing up, Lehmann was a member of England’s Children’s Special Service Mission, a youth evangelism group also referred to as CSSM. Lehmann took interest in the missionaries who frequented his childhood home. However, after a bad encounter with a Scandinavian worker, Lehmann decided that he no longer want to be a missionary. As a young boy, Lehmann believed in the Christian doctrine, but he did not like that parents tend to over-shelter their children from the world.\n\nLehmann attended the Plymouth Brethren Primary School, a prep school which was operated by strict adherent of the Plymouth Brethren. Lehmann was sufficiently exposed to Christian teachings as part of his primary education.\n\nHe trained as an engineer in the Oxford University and graduated with a Bachelor of Science degree in civil and mechanical engineering.\n\nHe then trained in medicine from Liverpool University Medical School and travelled to the US to intern at the Manhattan Eye, Ear and Throat Hospital to study with Dr. Chester Mayo.\n\nLehmann practiced corneal grafting at Boston Hospital before returning to Liverpool to study tropical medicine.\n\nAs the age of twelve, Lehmann met Monica Allen while on a holiday. They had become good friends, but Lehmann initially had no thought of marrying Monica, as she wanted to one day return to India as a missionary.\n\nWhen finishing up his doctorate education in engineering, Lehmann felt a sudden need of a spiritual refresher. He decided to attend the Keswick convention, where the speakers discussed the global need for medical missionaries. At the time, Lehmann felt like God was speaking to him and that this was his calling. When Lehmann came to this realization, he realized that he and Monica must have been destined to be with each other, as Monica dreamed of being a missionary from a young age.\n\nLehmann and Monica had three daughters and one son. Their first child, Priscilla Ruth, was born in Kachhwa. Petronella Anna was born in March 1937, Donald in May 1940, and Susana Joan in December 1946. At a young age, both Priscilla and Petrinilla were sent to London as there were no school in Herbertpur to educate them.\n\nOn their arrival in India, Dr. Lehmann and his wife first worked in a hospital in Kachhwa. Lehmann worked in Kacchwa for 18 months where his daily routine included operations in the mornings and afternoon with routine check-ups in between and language study in the evening. In Kachhwa, the Lehmanns distributed Christian literature for a small fee.\n\nThe Lehmanns established the Herbertpur Hospital in November 1936, in Herbertpur village, 25 miles away from Dehradun. The hospital became popular amongst locals in a short period of time and was visited by over 2000 people daily shortly after its opening. The hospital was mainly visited by low-income people from backward classes but catered to people from all socio-economic classes including Brahmins, farmers and merchants.\n\nBy 1937, the Lehmanns built a larger building for the hospital and treated nearly 3000 patients a day along with their staff.\n\nDuring the second world war, the hospital was closed for nearly a year due to limitation of staff and because Dr. Lehmann was himself asked to serve in the war front.\n\nThe hospital reopened in 1954, with two operating theaters, special eye wards, male and female general wards, midwifery and tuberculosis block and private rooms. Dr. Lehmann carried out treatments as complex as cataract removal surgery, appendectomy, and blood infection treatments at the Herbertpur Hospital.\n\nIn addition to medical services, the hospital also exposed to patients to Christianity during their stay in the hospital. Gospel service was carried out daily in the hospital yard by the hospital’s evangelist, Mr. Mall. Lehmann also often included himself in the service for he felt that his involvement would mean a lot to the patients and would help in strengthening their beliefs.\n\nDr. Lehmann along with his wife also established a number of schools in the region. These also served as establishments for congregation. Dr. Lehmann also worked with the Evangelical Alliance Mission and served as the chairman on board of governors for the Wynberg Allen School. This school provided education to children form displaced Tibetan families living in the Himalayan valley.\n\nThroughout his time in India, Lehmann faced opposition for trying to convert Indians to Christianity. In general, there was a lack of enthusiasm about Christianity—even from local Christians—as Indians did not like talking about the crucifixion. From time to time, the hospital and the school were able to influence locals to convert, however, when the villagers would return to their home, some inevitable turned away from Christianity due to the local opposition and resistance.\n\nMultiple parties pressured missionaries in India, such as the Arya Samahajhists, one of the more violent group of protesters. At one point, even the District Magistrate told Lehmann that they would close down his hospital if he continued to preach about Christianity, as he was considered to be taking advantage of those who needed medical help. Despite the opposition, Lehmann continued to preach and distributed Christian literature and films to connect with the people. Most medical evangelists gave full attention to their medical work and left missionary work for the local evangelists. However, Lehmann believed that paying attention to people's souls was just as important as healing them physically, and took a liking to the title \"Missionary Medic.\"\n\nThe Herbertpur Hospital became a member of the Emmanuel Hospital Association (EHA) in 1973 and continues to serve patients across the Doon valley to this day.\n\nThe Lehmanns established the Herbertpur Trust to encourage national evangelism programs.\n\nDr. Geoffrey Lehmann’s life and work were the subject of \"The Himalayan Heartbeat\", a book written by Ken Anderson, which was published in 1965.\n"}
{"id": "51040760", "url": "https://en.wikipedia.org/wiki?curid=51040760", "title": "George Ritchie Gilruth", "text": "George Ritchie Gilruth\n\nDr George Ritchie Gilruth FRSE LRCP LRCSE (1842-1921) was a Scottish surgeon and author.\n\nHe was born on 24 October 1842 the son of John Gilruth a writing master in Leith just north of Edinburgh and living at 28 Constitution Street. His mother was Eliza Ritchie. He is presumed to have qualified as a doctor at Edinburgh University. He qualified as a surgeon in 1865.\n\nIn 1877 he was living at 9 Union Street in Edinburgh and is noted as a qualified surgeon. He was a Demonstrator in Anatomy at the Royal College of Surgeons of Edinburgh. In 1880 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were William Ferguson, Andrew Douglas Maclagan, Patrick Heron Watson and Thomas Alexander Goldie Balfour.\n\nIn June 1888 he is noted as an Acting Surgeon in the Edinburgh City Artillery Volunteer Corps. He was then living at 48 Nothumberland Street.\n\nIn 1911 he is noted as living at 53 Northumberland Street in Edinburgh's Second New Town.\n\nHe later worked as Resident Surgeon at Consett Infirmary in the north of England.\n\nHe died at Allanton near Bridge of Allan, Stirlingshire on 15 August 1921.\n"}
{"id": "35194264", "url": "https://en.wikipedia.org/wiki?curid=35194264", "title": "Health Consumer Powerhouse", "text": "Health Consumer Powerhouse\n\nHealth Consumer Powerhouse is a Swedish health policy think tank which specialises in comparing healthcare systems throughout Europe. It produces the Euro health consumer index and other indexes comparing healthcare.\n\nIt was created in 2004 by Johan Hjertqvist, a Swedish entrepreneur, author and former local politician. It has offices in Stockholm, Sweden. Dr Arne A Björnberg is the President of the company. he was formerly Chief Executive of the Swedish National Pharmacy Corporation.\n\nThe Euro Health Consumer Index is a comparison of European health care systems based on waiting times, results, and generosity. The information is presented as a graphic index. Part of the motivation of the HCP in 2004 was to stimulate the European Union to take action on transparency and quality measures. The 2014 ranking included 37 countries measured by 48 indicators.\nThe Euro Health Consumer Index measurements started in 2005. During 2013 the European Commission assessed available systems for healthcare comparison including WHO, OECD, European Observatory and so forth and concluded that this is the most reliable. \n\nScoring is partly based on the responses from patient organisations to a questionnaire, particularly when their responses indicate a radically different situation from that officially reported. 976 responses were used in the 2015 exercise.\n\n"}
{"id": "19923366", "url": "https://en.wikipedia.org/wiki?curid=19923366", "title": "Heilpraktiker", "text": "Heilpraktiker\n\nHeilpraktiker (\"healing practitioner\") is a naturopathic profession in Germany. It is recognized as an alternative and complementary health care profession by German law. A heilpraktiker does not need to have any formal education or training but must do an exam at the health authorities. This exam used to be somewhat basic until the 1980s, at which time it was made to become much more demanding. A candidate needs to have good knowledge of medical sciences, such as anatomy, physiology and pathology and psychiatry; a good knowledge of law regulations is also needed. Healing practitioners often specialize in a complementary and alternative field of healthcare that could be anything from homeopathy, phytotherapy, Chinese medicine, Ayurvedic medicine, to reflexology or acupuncture. A healing practitioner is a person who is allowed to practice as a non-medical practitioner using any unconventional therapy. About 35000 heilpraktiker are accredited in Germany.\n"}
{"id": "58593780", "url": "https://en.wikipedia.org/wiki?curid=58593780", "title": "Herdis von Magnus", "text": "Herdis von Magnus\n\nHerdis von Magnus (23 September 1912 – 15 March 1992) was a renowned Danish polio expert who once worked with Jonas Salk, following which she directed the first Danish polio vaccination programme with her husband Preben von Magnus. She also worked on encephalitis.\n"}
{"id": "34231074", "url": "https://en.wikipedia.org/wiki?curid=34231074", "title": "Hereditary diffuse leukoencephalopathy with spheroids", "text": "Hereditary diffuse leukoencephalopathy with spheroids\n\nHereditary diffuse leukoencephalopathy with spheroids (HDLS) is a rare adult onset autosomal dominant disorder characterized by cerebral white matter degeneration with demyelination and axonal spheroids leading to progressive cognitive and motor dysfunction. Spheroids are axonal swellings with discontinuous or absence of myelin sheaths. It is believed that the disease arises from primary microglial dysfunction that leads to secondary disruption of axonal integrity, neuroaxonal damage, and focal axonal spheroids leading to demyelination. Spheroids in HDLS resemble to some extent those produced by shear stress in a closed head injury with damage to axons, causing them to swell due to blockage of axoplasmic transport. In addition to trauma, axonal spheroids can be found in aged brain, stroke, and in other degenerative diseases. In HDLS, it is uncertain whether demyelination occurs prior to the axonal spheroids or what triggers neurodegeneration after apparently normal brain and white matter development, although genetic deficits suggest that demyelination and axonal pathology may be secondary to microglial dysfunction. The clinical syndrome in patients with HDLS is not specific and it can be mistaken for Alzheimer's disease, frontotemporal dementia, atypical Parkinsonism, multiple sclerosis, or corticobasal degeneration.\n\nWith symptoms of personality changes, behavioral changes, dementia, depression, and epilepsy, HDLS has been commonly misdiagnosed for a number of other diseases. Dementia or frontotemporal behavioral changes, for example, have commonly steered some clinicians to mistakenly consider diagnoses such as Alzheimer’s disease, frontotemporal dementia or atypical Parkinsonism. The presence of white matter changes has led to misdiagnosis of multiple sclerosis. HDLS commonly manifests with neuropsychiatric symptoms, progressing to dementia, and after a few years shows motor dysfunction. Eventually patients become wheelchair-bound or bedridden.\n\nWhite matter degeneration is associated with and makes differential diagnoses out of other adult onset leukodystrophies such as metachromatic leukodystrophy (MLD), Krabbe disease (globoid cell leukodystrophy), and X-linked adrenoleukodystrophy (X-ADL).\n\nMany neuropsychiatric symptoms have been identified in clinical studies of HDLS patients. These include severe depression and anxiety that have been identified in about 70% of HDLS families, verging on suicidal tendencies and substance abuse such as alcoholism. Additionally, patients may exhibit disorientation, confusion, agitation, irritability, aggressiveness, an altered mental state, the loss of the ability to execute learned movements (apraxia), or the inability to speak (mutism).\n\nPersons with HDLS can suffer from tremors, decreased body movement, unsteadiness (Parkinsonism, muscles on one side of the body in constant contraction (spastic hemiparesis), impairment in motor and sensory function in the lower extremities (paraparesis), paralysis resulting in partial or total loss of all extremities and torso (tetraparesis), and the lack of voluntary coordination of muscle movements (ataxia).\n\nRelated disorders in the same disease spectrum as HDLS include Nasu-Hakola disease (polycystic lipomembranous osteodysplasia with sclerosing leukoencephalopathy), and a type of leukodystrophy with pigment-filled macrophages called pigmentary orthochromatic leukodystrophy (POLD). In addition to white matter disease, Nasu-Hakola causes bone cysts. It is caused by mutations in the genes involved in the same colony stimulating factor (CSF) signaling pathway cascade as identified in HDLS.\n\nNasu-Hakola disease appears to be caused by mutations in the TYRO protein tyrosine kinase-binding protein (TYROBP - also known as DAP12) or the triggering receptor expressed on myeloid cells 2 (TREM2) protein. While different gene mutations occur within the pathway for Nasu-Hakola and HDLS, both are characterized by white matter degeneration with axonal spheroids. Current researchers in the field believe that more in depth analysis and comparison of the two genetic abnormalities in these disorders could lead to a better understanding of the disease mechanisms in these rare disorders. POLD exhibits noninflammatory demyelination of axons with initial symptoms of euphoria, apathy, headache, and executive dysfunction. While HDLS is autosomal dominant, some families with POLD have features that suggest autosomal recessive inheritance. Nevertheless, POLD has recently been shown to have the same genetic basis as HDLS.\nThe cause of HDLS in most families is mutation in the colony stimulating factor 1 receptor (CSF1R), a growth factor for microglia and monocyte/macrophages, suggesting that microglial dysfunction may be primary in HDLS.\n\nThe mutations are concentrated in tyrosine kinase domain (TKD) of the protein. Mutations were mainly found in exons 12-22 of the intracellular TKD, including 10 missense mutations that have a single nucleotide deletion and a single codon deletion that consists of a triplet of nucleotides that have been removed causing a whole amino acid to not be coded. Additionally, three splice site mutations were identified that caused an in-frame deletion of an exon, an expressed nucleotide sequence, leading to the removal of more than 40 amino acids in the TKD.\n\nThis determination has based upon genetic studies of 14 HDLS families confirming mutations in this gene. The CSF1 receptor protein primarily functions in regulation, survival, proliferation, and differentiation of microglial cells. The mechanism of microglial dysfunction due to mutations in CSF1R to the myelin loss and axonal spheroid formation remains unknown. Further research is needed to better understand disease pathogenesis.\n\nIn HDLS, there is enlargement of the lateral ventricles and marked thinning or weakening of cerebral white matter. The loss of white matter is caused by myelin loss. These changes are associated with diffuse gliosis, moderate loss of axons and many axonal spheroids.\n\nActivated or ameboid microglia and macrophages that contain myelin debris, lipid droplets and brown autofluorescent pigment granules are found in the areas with demyelination and axonal spheroids. In severely degenerated areas there are many large, reactive astrocytes filled with glial fibrils.\n\nIn autopsy cases, it has been shown that white matter abnormalities are relatively confined to the cerebrum while avoiding the cerebellum and many of the major fiber tracts of the nervous system. The exception is the corticospinal tracts(pyramidal tracts) in the brainstem and sometimes spinal cord.\n\nThe brain pathology of HDLS resembles that of Nasu-Hakola disease (polycystic lipomembranous osteodysplasia with sclerosing leukoencephalopathy).\n\nResearch as of 2012 includes investigations of microglial function. This work would further clarify whether the disease is primarily a defect in microglia function. For such a study, microglial cells from HDLS kindred can be cultured from autopsy brain and analyzed in comparison to normal microglial cells on the basis of differences in mutation occurrences and growth factor expression.\n\nTo gain a better understanding of the disease, researchers have retrospectively reviewed medical records of probands and others who were assessed through clinical examinations or questionnaires. Blood samples are collected from the families of the probands for genetic testing. These family members are assessed using their standard medical history, on their progression of Parkinson's like symptoms (Unified Parkinson's Disease Rating Scale), and on their progression of cognitive impairment such as dementia (Folstein Test).\n\nStandard MRI scans have been performed on 1.5 Tesla scanners with 5 mm thickness and 5 mm spacing to screen for white matter lesions in identified families. If signal intensities of the MRI scans are higher in white matter regions than in grey matter regions, the patient is considered to be at risk for HDLS, although a number of other disorders can also produce white matter changes and the findings are not diagnostic without genetic testing or pathologic confirmation.\n\nTissue sections from brain biopsies or autopsy brains are commonly embedded in paraffin from which sections are cut an mounted on glass slides for histologic studies. Special stains for myelin and axonal pathology show the abnormal changes that are characteristic of HDLS are identified in white matter of the neocortex, basal ganglia, thalamus, midbrain, pons and spinal cord. In addition to routine histologic methods (H&E staining), samples are evaluated with immunohistochemistry for ubiquitin, amyloid precursor protein, and neurofilament to characterize axonal changes and myelin basic protein for myelin pathology. Immunohistochemical stains for microglia (CD68 or HLA-DR) and astrocytes (GFAP) are also helpful techniques to characterize white matter pathology. With a similar pathology to POLD, HDLS is commonly grouped as adult-onset leukoencephalopathy with axonal spheroids and pigmented glia (ALSP) so as to give these individually under-recognized conditions heightened attention.\nHDLS falls under the category of brain white matter diseases called leukoencephalopathies that are characterized by some degree of white matter dysfunction. HDLS has white matter lesions with abnormalities in myelin sheath around axons, where the causative influences are being continually explored based upon recent genetic findings. Studies by Sundal and colleagues from Sweden showed that a risk allele in Caucasians may be causative because cases identified have thus far been among large Caucasian families.\n\nAn average clinical profile from published studies shows that the median onset age for HDLS patients is 44.3 years with a mean disease duration of 5.8 years and mean age of death at 53.2 years. As of 2012, there have been around 15 cases identified with at least 11 sporadic cases of HDLS. HDLS cases have been located in Germany, Norway, Sweden, and the United States, showing an international distribution focusing between Northern Europe and the United States.\n\nThrough the study of numerous kindred, it was found that the disease did not occur among just males or females, but rather was evenly distributed indicative of an autosomal rather than a sex-linked genetic disorder. It was also observed that the HDLS cases did not skip generations as it would occur with a recessive inheritance, and as such has been labeled autosomal dominant.\n\nThis disease was first described in 1984 by Axelsson \"et al.\" in a large Swedish pedigree. It is a disorder better known to neuropathologists than clinicians. A neuropathologist with an interest in HDLS, Dr. Dennis W. Dickson, has identified a number of cases from neuropathology study of brains submitted for investigation of familial adult-onset dementia and movement disorders in New York and later in Florida. Recognition of the importance of this disorder as a cause of adult onset dementia and movement disorders was further heightened in 1997 at the Mayo Clinic when Dr. Zbigniew K. Wszolek identified a family with HDLS that was initially thought to be due to another disease process (FTDP-17), but only an autopsy of one and then other family members revealed it to be HDLS. Wszolek established an international consortium in 2005 to identify other families and to collect DNA or brain samples from family members for neuropathologic confirmation and genetic research at the Mayo Clinic in Florida.\n\n"}
{"id": "49216352", "url": "https://en.wikipedia.org/wiki?curid=49216352", "title": "Home and Hospital Education", "text": "Home and Hospital Education\n\nHome and Hospital Education (HHE) refers to all the activities related to the wide word of the education for children or youngsters with medical needs, being it in hospital (with hospital sections or hospital schools) or at home (as home tuition).\n\nThe term HHE has been firstly introduced in the writing of the proposal for the EU funded project is an international 3-year project funded by the European Union, and managed by the Fondazione Politecnico di Milano aiming to research, develop and disseminate effective pedagogical practices and appropriate use of ICT within the Hospital School Sector.\n\nNow the HHE acronym is commonly used in research and on a practical level in Europe and over.\n\n\nThe LeHo Project (Learning at Home and in the Hospital)\n\nICT for Home and Hospital Education – based on best practices of the LeHo (Learning at Home and in the Hospital) project \n"}
{"id": "24973826", "url": "https://en.wikipedia.org/wiki?curid=24973826", "title": "Human genetic resistance to malaria", "text": "Human genetic resistance to malaria\n\nHuman genetic resistance to malaria refers to inherited changes in the DNA of humans which increase resistance to malaria and result in increased survival of individuals with those genetic changes. The existence of these genotypes is likely due to evolutionary pressure exerted by parasites of the genus \"Plasmodium\" which cause malaria. Since malaria infects red blood cells, these genetic changes are most commonly alterations to molecules essential for red blood cell function (and therefore parasite survival), such as hemoglobin or other cellular proteins or enzymes of red blood cells. These alterations generally protect red blood cells from invasion by \"Plasmodium\" parasites or replication of parasites within the red blood cell.\n\nThese inherited changes to hemoglobin or other characteristic proteins, which are critical and rather invariant features of mammalian biochemistry, usually cause some kind of inherited disease. Therefore, they are commonly referred to by the names of the blood disorders associated with them, including sickle-cell disease, thalassemia, glucose-6-phosphate dehydrogenase deficiency, and others. These blood disorders cause increased morbidity and mortality in areas of the world where malaria is less prevalent.\n\nMicroscopic parasites, like viruses, protozoans that cause malaria, and others, cannot replicate on their own and rely on a host to continue their life cycles. They replicate by invading the hosts' cells and usurping the cellular machinery to replicate themselves. Eventually, unchecked replication causes the cells to burst, killing the cells and releasing the infectious organisms into the bloodstream where they can infect other cells. As cells die and toxic products of invasive organism replication accumulate, disease symptoms appear. Because this process involves specific proteins produced by the infectious organism as well as the host cell, even a very small change in a critical protein may render infection difficult or impossible. Such changes might arise by a process of mutation in the gene that codes for the protein. If the change is in the gamete, that is, the sperm or egg that join to form a zygote that grows into a human being, the protective mutation will be inherited. Since lethal diseases kill many persons who lack protective mutations, in time, many persons in regions where lethal diseases are endemic come to inherit protective mutations.\n\nWhen the \"P. falciparum\" parasite infects a host cell, it alters the characteristics of the red blood cell membrane, making it \"stickier\" to other cells. Clusters of parasitized red blood cells can exceed the size of the capillary circulation, adhere to the endothelium, and block circulation. When these blockages form in the blood vessels surrounding the brain, they cause cerebral hypoxia, resulting in neurological symptoms known as cerebral malaria. This condition is characterized by confusion, disorientation, and often terminal coma. It accounts for 80% of malaria deaths. Therefore, mutations that protect against malaria infection and lethality pose a significant advantage.\n\nMalaria has placed the strongest known selective pressure on the human genome since the origin of agriculture within the past 10,000 years. \"Plasmodium falciparum\" was probably not able to gain a foothold among African populations until larger sedentary communities emerged in association with the evolution of domestic agriculture in Africa (the agricultural revolution). Several inherited variants in red blood cells have become common in parts of the world where malaria is frequent as a result of selection exerted by this parasite. This selection was historically important as the first documented example of disease as an agent of natural selection in humans. It was also the first example of genetically controlled innate immunity that operates early in the course of infections, preceding adaptive immunity which exerts effects after several days. In malaria, as in other diseases, innate immunity leads into, and stimulates, adaptive immunity.\n\nMutations may have detrimental as well as beneficial effects, and any single mutation may have both. Infectiousness of malaria depends on specific proteins present in the cell walls and elsewhere in red blood cells. Protective mutations alter these proteins in ways that make them inaccessible to malaria organisms. However, these changes also alter the functioning and form of red blood cells that may have visible effects, either overtly, or by microscopic examination of red blood cells. These changes may impair the function of red blood cells in various ways that have a detrimental effect on the health or longevity of the individual. However, if the net effect of protection against malaria outweighs the other detrimental effects, the protective mutation will tend to be retained and propagated from generation to generation.\n\nThese alterations which protect against malarial infections but impair red blood cells are generally considered blood disorders, since they tend to have overt and detrimental effects. Their protective function has only in recent times, been discovered and acknowledged. Some of these disorders are known by fanciful and cryptic names like sickle-cell anemia, thalassaemia, glucose-6-phosphate dehydrogenase deficiency, ovalocytosis, elliptocytosis and loss of the Gerbich antigen and the Duffy antigen. These names refer to various proteins, enzymes, and the shape or function of red blood cells.\n\nThe potent effect of genetically controlled innate resistance is reflected in the probability of survival of young children in areas where malaria is endemic. It is necessary to study innate immunity in the susceptible age group (younger than four years), because in older children and adults, the effects of innate immunity are overshadowed by those of adaptive immunity. It is also necessary to study populations in which random use of antimalarial drugs does not occur. Some early contributions on innate resistance to infections of vertebrates, including humans, are summarized in Table 1.\nIt is remarkable that two of the pioneering studies were on malaria. The classical studies on the Toll receptor in \"Drosophila\" fruit fly were rapidly extended to Toll-like receptors in mammals and then to other pattern recognition receptors, which play important roles in innate immunity. However, the early contributions on malaria remain as classical examples of innate resistance, which have stood the test of time.\n\nThe mechanisms by which erythrocytes containing abnormal hemoglobins, or are G6PD deficient, are partially protected against \"P. falciparum\" infections are not fully understood, although there has been no shortage of suggestions. During the peripheral blood stage of replication malaria parasites have a high rate of oxygen consumption and ingest large amounts of hemoglobin. It is likely that HbS in endocytic vesicles is deoxygenated, polymerizes and is poorly digested. In red cells containing abnormal hemoglobins, or which are G6PD deficient, oxygen radicals are produced, and malaria parasites induce additional oxidative stress. This can result in changes in red cell membranes, including translocation of phosphatidylserine to their surface, followed by macrophage recognition and ingestion. The authors suggest that this mechanism is likely to occur earlier in abnormal than in normal red cells, thereby restricting multiplication in the former. In addition, binding of parasitized sickle cells to endothelial cells is significantly decreased because of an altered display of \"P. falciparum\" erythrocyte membrane protein-1 (PfMP-1). This protein is the parasite’s main cytoadherence ligand and virulence factor on the cell surface. During the late stages of parasite replication red cells are adherent to venous endothelium, and inhibiting this attachment could suppress replication.\n\nSickle hemoglobin induces the expression of heme oxygenase-1 in hematopoietic cells. Carbon monoxide, a byproduct of heme catabolism by heme oxygenase-1(HO-1), prevents an accumulation of circulating free heme after \"Plasmodium\" infection, suppressing the pathogenesis of experimental cerebral malaria. Other mechanisms, such as enhanced tolerance to disease mediated by HO-1 and reduced parasitic growth due to translocation of host micro-RNA into the parasite, have been described.\n\nThe first line of defense against malaria is mainly exerted by abnormal hemoglobins and glucose-6-phosphate dehydrogenase deficiency. The three major types of inherited genetic resistance – sickle cell disease, thalassemias, and G6PD deficiency – were present in the Mediterranean world by the time of the Roman Empire.\n\nMalaria does not occur in the cooler, drier climates of the highlands in the tropical and subtropical regions of the world.\nTens of thousands of individuals have been studied, and high frequencies of abnormal hemoglobins have not been found in any population that was malaria free. The frequencies of abnormal hemoglobins in different populations vary greatly, but some are undoubtedly polymorphic, having frequencies higher than expected by recurrent mutation. There is no longer doubt that malarial selection played a major role in the distribution of all these polymorphisms. All of these are in malarious areas,\n\n\nThe thalassemias have a high incidence in a broad band extending from the Mediterranean basin and parts of Africa, throughout the Middle East, the Indian subcontinent, Southeast Asia, Melanesia, and into the Pacific Islands.\n\nSickle-cell disease was the genetic disorder to be linked to a mutation of a specific protein. Pauling introduced his fundamentally important concept of sickle cell anemia as a genetically transmitted molecular disease.\n\nThe molecular basis of sickle cell anemia was finally elucidated in 1959, when Ingram perfected the techniques of tryptic peptide fingerprinting. In the mid-1950s, one of the newest and most reliable ways of separating peptides and amino acids was by means of the enzyme trypsin, which split polypeptide chains by specifically degrading the chemical bonds formed by the carboxyl groups of two amino acids, lysine and arginine. Small differences in hemoglobin A and S will result in small changes in one or more of these peptides . To try to detect these small differences, Ingram combined paper electrophoresis and the paper chromotography methods. By this combination he created a two-dimensional method that enabled him to comparatively \"fingerprint\" the hemoglobin S and A fragments he obtained from the tryspin digest. The fingerprints revealed approximately 30 peptide spots, there was one peptide spot clearly visible in the digest of haemoglobin S which was not obvious in the haemoglobin A fingerprint. The HbS gene defect is a mutation of a single nucleotide (A to T) of the β-globin gene replacing the amino acid glutamic acid with the less polar amino acid valine at the sixth position of the β chain.\n\nHbS has a lower negative charge at physiological pH than does normal adult hemoglobin. The consequences of the simple replacement of a charged amino acid with a hydrophobic, neutral amino acid are far ranging, Recent studies in West Africa suggest that the greatest impact of Hb S seems to be to protect against either death or severe disease—that is, profound anemia or cerebral malaria—while having less effect on infection per se. Children who are heterozygous for the sickle cell gene have only one-tenth the risk of death from falciparum as do those who are homozygous for the normal hemoglobin gene. Binding of parasitized sickle erythrocytes to endothelial cells and blood monocytes is significantly reduced due to an altered display of \"Plasmodium falciparum\" erythrocyte membrane protein 1 (PfEMP-1), the parasite’s major cytoadherence ligand and virulence factor on the erythrocyte surface.\n\nProtection also derives from the instability of sickle hemoglobin, which clusters the predominant integral red cell membrane protein (called band 3) and triggers accelerated removal by phagocytic cells. Natural antibodies recognize these clusters on senescent erythrocytes. Protection by HbAS involves the enhancement of not only innate but also of acquired immunity to the parasite. Prematurely denatured sickle hemoglobin results in an upregulation of natural antibodies which control erythrocyte adhesion in both malaria and sickle cell disease. Targeting the stimuli that lead to endothelial activation will constitute a promising therapeutic strategy to inhibit sickle red cell adhesion and vaso-occlusion.\n\nThis has led to the hypothesis that while homozygotes for the sickle cell gene suffer from disease, heterozygotes might be protected against malaria. Malaria remains a selective factor for the sickle cell trait.\n\nIt has long been known that a kind of anemia, termed thalassemia, has a high frequency in some Mediterranean populations, including Greeks and southern Italians. The name is derived from the Greek words for sea (\"thalassa\"), meaning the Mediterranean sea, and blood (\"haima\"). Vernon Ingram deserves the credit for explaining the genetic basis of different forms of thalassemia as an imbalance in the synthesis of the two polypeptide chains of hemoglobin.\n\nIn the common Mediterranean variant, mutations decrease production of the β-chain (β-thalassemia). In α-thalassemia, which is relatively frequent in Africa and several other countries, production of the α-chain of hemoglobin is impaired, and there is relative over-production of the β-chain. Individuals homozygous for β-thalassemia have severe anemia and are unlikely to survive and reproduce, so selection against the gene is strong. Those homozygous for α-thalassemia also suffer from anemia and there is some degree of selection against the gene.\n\nThe lower Himalayan foothills and Inner Terai or Doon Valleys of Nepal and India are highly malarial due to a warm climate and marshes sustained during the dry season by groundwater percolating down from the higher hills. Malarial forests were intentionally maintained by the rulers of Nepal as a defensive measure. Humans attempting to live in this zone suffered much higher mortality than at higher elevations or below on the drier Gangetic Plain. However, the Tharu people had lived in this zone long enough to evolve resistance via multiple genes. Medical studies among the Tharu and non-Tharu population of the Terai yielded the evidence that the prevalence of cases of residual malaria is nearly seven times lower among Tharus. The basis for resistance has been established to be homozygosity of α-Thalassemia gene within the local population. Endogamy along caste and ethnic lines appear to have prevented these genes from being more widespread in neighboring populations.\n\nThere is evidence that the persons with α-thalassemia, HbC and HbE have some degree of protection against the parasite.\nHemoglobin C (HbC) is an abnormal hemoglobin with substitution of a lysine residue for glutamic acid residue of the β-globin chain, at exactly the same ß-6 position as the HbS mutation. The \"C\" designation for HbC is from the name of the city where it was discovered—Christchurch, New Zealand. People who have this disease, particularly children, may have episodes of abdominal and joint pain, an enlarged spleen, and mild jaundice, but they do not have severe crises, as occur in sickle cell disease. Haemoglobin C is common in malarious areas of West Africa, especially in Burkina Faso. In a large case–control study performed in Burkina Faso on 4,348 Mossi subjects, that HbC was associated with a 29% reduction in risk of clinical malaria in HbAC heterozygotes and of 93% in HbCC homozygotes. HbC represents a ‘slow but gratis’ genetic adaptation to malaria through a transient polymorphism, compared to the polycentric ‘quick but costly’ adaptation through balanced polymorphism of HbS.\nHbC modifies the quantity and distribution of the variant antigen \"P. falciparum\" erythrocyte membrane protein 1 (PfEMP1) on the infected red blood cell surface and the modified display of malaria surface proteins reduces parasite adhesiveness (thereby avoiding clearance by the spleen) and can reduce the risk of severe disease.\n\nHemoglobin E is due to a single point mutation in the gene for the beta chain with a glutamate-to-lysine substitution at position 26. It is one of the most prevalent hemoglobinopathies with 30 million people affected. Hemoglobin E is very common in parts of Southeast Asia. HbE erythrocytes have an unidentified membrane abnormality that renders the majority of the RBC population relatively resistant to invasion by \"P falciparum\".\n\nOther genetic mutations besides hemoglobin abnormalities that confer resistance to \"Plasmodia\" infection involve alterations of the cellular surface antigenic proteins, cell membrane structural proteins, or enzymes involved in glycolysis.\n\nGlucose-6-phosphate dehydrogenase (G6PD) is an important enzyme in red cells, metabolizing glucose through the pentose phosphate pathway, an anabolic alternative to catabolic oxidation (glycolysis), while maintaining a reducing environment. G6PD is present in all human cells but is particularly important to red blood cells. Since mature red blood cells lack nuclei and cytoplasmic RNA, they cannot synthesize new enzyme molecules to replace genetically abnormal or ageing ones. All proteins, including enzymes, have to last for the entire lifetime of the red blood cell, which is normally 120 days.\n\nIn 1956 Alving and colleagues showed that in some African Americans the antimalarial drug primaquine induces hemolytic anemia, and that those individuals have an inherited deficiency of G6PD in erythrocytes. G6PD deficiency is sex-linked, and common in Mediterranean, African and other populations. In Mediterranean countries such individuals can develop a hemolytic diathesis (favism) after consuming fava beans. G6PD deficient persons are also sensitive to several drugs in addition to primaquine.\n\nG6PD deficiency is the second most common enzyme deficiency in humans (after ALDH2 deficiency), estimated to affect some 400 million people. There are many mutations at this locus, two of which attain frequencies of 20% or greater in African and Mediterranean populations; these are termed the A- and Med mutations. Mutant varieties of G6PD can be more unstable than the naturally occurring enzyme, so that their activity declines more rapidly as red cells age.\n\nThis question has been studied in isolated populations where antimalarial drugs were not used in Tanzania, East Africa and in the Republic of the Gambia, West Africa, following children during the period when they are most susceptible to \"falciparum\" malaria. In both cases parasite counts were significantly lower in G6PD-deficient persons than in those with normal red cell enzymes. The association has also been studied in individuals, which is possible because the enzyme deficiency is sex-linked and female heterozygotes are mosaics due to lyonization, where random inactivation of an X-chromosome in certain cells creates a population of G6PD deficient red blood cells coexisting with normal red blood cells. Malaria parasites were significantly more often observed in normal red cells than in enzyme-deficient cells. An evolutionary genetic analysis of malarial selection of G6PD deficiency genes has been published by Tishkoff and Verelli. The enzyme deficiency is common in many countries that are, or were formerly, malarious, but not elsewhere.\n\nPyruvate kinase (PK) deficiency, also called erythrocyte pyruvate kinase deficiency, is an inherited metabolic disorder of the enzyme pyruvate kinase. In this condition, a lack of pyruvate kinase slows down the process of glycolysis. This effect is especially devastating in cells that lack mitochondria, because these cells must use anaerobic glycolysis as their sole source of energy because the TCA cycle is not available. One example is red blood cells, which in a state of pyruvate kinase deficiency rapidly become deficient in ATP and can undergo hemolysis. Therefore, pyruvate kinase deficiency can cause hemolytic anemia.\n\nThere is a significant correlation between severity of PK deficiency and extent of protection against malaria.\n\nElliptocytosis a blood disorder in which an abnormally large number of the patient's erythrocytes are elliptical. There is much genetic variability amongst those affected. There are three major forms of hereditary elliptocytosis: common hereditary elliptocytosis, spherocytic elliptocytosis and southeast Asian ovalocytosis.\n\nOvalocytosis is a subtype of elliptocytosis, and is an inherited condition in which erythrocytes have an oval instead of a round shape. In most populations ovalocytosis is rare, but South-East Asian ovalocytosis (SAO) occurs in as many as 15% of the indigenous people of Malaysia and of Papua New Guinea. Several abnormalities of SAO erythrocytes have been reported, including increased red cell rigidity and reduced expression of some red cell antigens.\nSAO is caused by a mutation in the gene encoding the erythrocyte band 3 protein. There is a deletion of codons 400–408 in the gene, leading to a deletion of 9 amino-acids at the boundary between the cytoplasmic and transmembrane domains of band 3 protein. Band 3 serves as the principal binding site for the membrane skeleton, a submembrane protein network composed of ankyrin, spectrin, actin, and band 4.1. Ovalocyte band 3 binds more tightly than normal band 3 to ankyrin, which connects the membrane skeleton to the band 3 anion transporter. These qualitative defects create a red blood cell membrane that is less tolerant of shear stress and more susceptible to permanent deformation.\n\nSAO is associated with protection against cerebral malaria in children because it reduces sequestration of erythrocytes parasitized by \"P. falciparum\" in the brain microvasculature. Adhesion of \"P. falciparum\"-infected red blood cells to CD36 is enhanced by the cerebral malaria-protective SAO trait . Higher efficiency of sequestration via CD36 in SAO individuals could determine a different organ distribution of sequestered infected red blood cells. These provide a possible explanation for the selective advantage conferred by SAO against cerebral malaria.\n\n\"Plasmodium vivax\" has a wide distribution in tropical countries, but is absent or rare in a large region in West and Central Africa, as recently confirmed by PCR species typing. This gap in distribution has been attributed to the lack of expression of the Duffy antigen receptor for chemokines (DARC) on the red cells of many sub-Saharan Africans. Duffy negative individuals are homozygous for a DARC allele, carrying a single nucleotide mutation (DARC 46 T → C), which impairs promoter activity by disrupting a binding site for the hGATA1 erythroid lineage transcription factor. In widely cited \"in vitro\" and \"in vivo\" studies, Miller et al. reported that the Duffy blood group is the receptor for \"P. vivax\" and that the absence of the Duffy blood group on red cells is the resistance factor to \"P. vivax\" in persons of African descent. This has become a well-known example of innate resistance to an infectious agent because of the absence of a receptor for the agent on target cells.\n\nHowever, observations have accumulated showing that the original Miller report needs qualification. In human studies of \"P. vivax\" transmission, there is evidence for the transmission of \"P. vivax\" among Duffy-negative populations in Western Kenya, the Brazilian Amazon region, and Madagascar. The Malagasy people on Madagascar have an admixture of Duffy-positive and Duffy-negative people of diverse ethnic backgrounds. 72% of the island population were found to be Duffy-negative. \"P. vivax\" positivity was found in 8.8% of 476 asymptomatic Duffy-negative people, and clinical \"P. vivax\" malaria was found in 17 such persons. Genotyping indicated that multiple \"P. vivax\" strains were invading the red cells of Duffy-negative people. The authors suggest that among Malagasy populations there are enough Duffy-positive people to maintain mosquito transmission and liver infection. More recently, Duffy negative individuals infected with two different strains of \"P. vivax\" were found in Angola and Equatorial Guinea; further, \"P. vivax\" infections were found both in humans and mosquitoes, which means that active transmission is occurring. The frequency of such transmission is still unknown. Because of these several reports from different parts of the world it is clear that some variants of \"P. vivax\" are being transmitted to humans who are not expressing DARC on their red cells. The same phenomenon has been observed in New World monkeys. However, DARC still appears to be a major receptor for human transmission of \"P. vivax\".\n\nThe distribution of Duffy negativity in Africa does not correlate precisely with that of \"P. vivax\" transmission. Frequencies of Duffy negativity are as high in East Africa (above 80%), where the parasite is transmitted, as they are in West Africa, where it is not. The potency of \"P. vivax\" as an agent of natural selection is unknown, and may vary from location to location. DARC negativity remains a good example of innate resistance to an infection, but it produces a relative and not an absolute resistance to \"P. vivax\" transmission.\n\nThe Gerbich antigen system is an integral membrane protein of the erythrocyte and plays a functionally important role in maintaining erythrocyte shape. It also acts as the receptor for the \"P. falciparum\" erythrocyte binding protein. There are four alleles of the gene which encodes the antigen, Ge-1 to Ge-4. Three types of Ge antigen negativity are known: Ge-1,-2,-3, Ge-2,-3 and Ge-2,+3. persons with the relatively rare phenotype Ge-1,-2,-3, are less susceptible (~60% of the control rate) to invasion by \"P. falciparum\". Such individuals have a subtype of a condition called hereditary elliptocytosis, characterized by oval or elliptical shape erythrocytes.\n\nRare mutations of glycophorin A and B proteins are also known to mediate resistance to \"P. falciparum\".\n\nHuman leucocyte antigen (HLA) polymorphisms common in West Africans but rare in other racial groups, are associated with protection from severe malaria. This group of genes encodes cell-surface antigen-presenting proteins and has many other functions. In West Africa, they account for as great a reduction in disease incidence as the sickle-cell hemoglobin variant. The studies suggest that the unusual polymorphism of major histocompatibility complex genes has evolved primarily through natural selection by infectious pathogens.\n\nPolymorphisms at the HLA loci, which encode proteins that participate in antigen presentation, influence the course of malaria. In West Africa an HLA class I antigen (HLA Bw53) and an HLA class II haplotype (DRB1*13OZ-DQB1*0501) are independently associated with protection against severe malaria. However, HLA correlations vary, depending on the genetic constitution of the polymorphic malaria parasite, which differs in different geographic locations.\n\nSome studies suggest that high levels of fetal hemoglobin (HbF) confer some protection against falciparum malaria in adults with Hereditary persistence of fetal hemoglobin.\n\nEvolutionary biologist J.B.S. Haldane was the first to give a hypothesis on the relationship between malaria and the genetic disease. He first delivered his hypothesis at the Eighth International Congress of Genetics held in 1948 at Stockholm on a topic \"The Rate of Mutation of Human Genes\". He formalised in a technical paper published in 1949 in which he made a prophetic statement: \"The corpuscles of the anaemic heterozygotes are smaller than normal, and more resistant to hypotonic solutions. It is at least conceivable that they are also more resistant to attacks by the sporozoa which cause malaria.\" This became known as 'Haldane's malaria hypothesis', or concisely, the 'malaria hypothesis'.\n\nDetailed study of a cohort of 1022 Kenyan children living near Lake Victoria, published in 2002, confirmed this prediction. Many SS children still died before they attained one year of age. Between 2 and 16 months the mortality in AS children was found to be significantly lower than that in AA children. This well-controlled investigation shows the ongoing action of natural selection through disease in a human population.\n\nAnalysis of genome wide association (GWA) and fine-resolution association mapping is a powerful method for establishing the inheritance of resistance to infections and other diseases. Two independent preliminary analyses of GWA association with severe falciparum malaria in Africans have been carried out, one by the Malariagen Consortium in a Gambian population and the other by Rolf Horstmann (Bernhard Nocht Institute for Tropical Medicine, Hamburg) and his colleagues on a Ghanaian population. In both cases the only signal of association reaching genome-wide significance was with the \"HBB\" locus encoding the \"β\"-chain of hemoglobin, which is abnormal in HbS. This does not imply that HbS is the only gene conferring innate resistance to falciparum malaria; there could be many such genes exerting more modest effects that are challenging to detect by GWA because of the low levels of linkage disequilibrium in African populations. However the same GWA association in two populations is powerful evidence that the single gene conferring strongest innate resistance to \"falciparum\" malaria is that encoding HbS.\n\nThe fitnesses of different genotypes in an African region where there is intense malarial selection were estimated by Anthony Allison in 1954. In the Baamba population living in the Semliki Forest region in Western Uganda the sickle-cell heterozygote (AS) frequency is 40%, which means that the frequency of the sickle-cell gene is 0.255 and 6.5% of children born are SS homozygotes. \nIt is a reasonable assumption that until modern treatment was available three quarters of the SS homozygotes failed to reproduce. To balance this loss of sickle-cell genes, a mutation rate of 1:10.2 per gene per generation would be necessary. This is about 1000 times greater than mutation rates measured in \"Drosophila\" and other organisms and much higher than recorded for the sickle-cell locus in Africans. To balance the polymorphism, Anthony Allison estimated that the fitness of the AS heterozygote would have to be 1.26 times than that of the normal homozygote. Later analyses of survival figures have given similar results, with some differences from site to site. In Gambians, it was estimated that AS heterozygotes have 90% protection against \"P. falciparum\"-associated severe anemia and cerebral malaria, whereas in the Luo population of Kenya it was estimated that AS heterozygotes have 60% protection against severe malarial anemia. These differences reflect the intensity of transmission of \"P. falciparum\" malaria from locality to locality and season to season, so fitness calculations will also vary. In many African populations the AS frequency is about 20%, and a fitness superiority over those with normal hemoglobin of the order of 10% is sufficient to produce a stable polymorphism.\n\n\nactin, ankrin, spectrin – proteins that are the major components of the cytoskeleton scaffolding within a cell's cytoplasm\n\naerobic – uses oxygen for the production of energy (contrast anaerobic)\n\nallele – one of two or more alternative forms of a gene that arise by mutation\n\nα-chain / β-chain (hemoglobin) – subcomponents of the hemoglobin molecule; two α-chains and two β-chains make up normal hemoglobin (HbA)\n\nalveolar – pertaining to the alveoli, the tiny air sacs in the lungs\n\namino acid – any of twenty organic compounds that are subunits of protein in the human body\n\nanabolic – of or relating to the synthesis of complex molecules in living organisms from simpler ones\ntogether with the storage of energy; constructive metabolism (contrast catabolic)\n\nanaerobic – refers to a process or reaction which does not require oxygen, but produces energy by other means (contrast aerobic)\n\nanion transporter (organic) – molecules that play an essential role in the distribution and excretion of numerous endogenous metabolic products and exogenous organic anions\n\nantigen – any substance (as an immunogen or a hapten) foreign to the body that evokes an immune response either alone or after forming a complex with a larger molecule (as a protein) and that is capable of binding with a component (as an antibody or T cell) of the immune system\n\nATP – (Adenosine triphosphate) – an organic molecule containing high energy phosphate bonds used to transport energy within a cell\n\ncatabolic – of or relatig to the breakdown of complex molecules in living organisms to form simpler ones, together with the release of energy; destructive metabolism (contrast anabolic)\n\nchemokine – are a family of small cytokines, or signaling proteins secreted by cells\n\ncodon – a sequence of three nucleotides which specify which amino acid will be added next during protein synthesis\n\ncorpuscle – obsolete name for red blood cell\n\ncytoadherance – infected red blood cells may adhere to blood vellel walls and uninfected red blood cells\n\ncytoplasm – clear jelly-like substance, mostly water, inside a cell\n\ndiathesis – a tendency to suffer from a particular medical condition\n\nDNA – deoxyribonucleic acid, the hereditary material of the genome\n\nDrosophila – a kind of fruit fly used for genetic experimentation because of ease of reproduction and manipulation of its genome\n\nendocytic – the transport of solid matter or liquid into a cell by means of a coated vacuole or vesicle\n\nendogamy – the custom of marrying only within the limits of a local community, clan, or tribe\n\nendothelial – of or referring to the thin inner surface of blood vessels\n\nenzyme – a protein that promotes a cellular process, much like a catalyst in an ordinary chemical reaction\n\nepidemiology – the study of the spread of disease within a population\n\nerythrocyte – red blood cell, which with the leucocytes make up the cellular content of the blood (contrast leucocyte)\n\nerythroid – of or referring to erythrocytes, red blood cells\n\nfitness (genetic) – loosely, reproductive success that tends to propagate a trait or traits (see natural selection)\n\ngenome – (abstractly) all the inheritable traits of an organism; represented by its chromosomes\n\ngenotype – the genetic makeup of a cell, an organism, or an individual usually with reference to a specific trait\n\nglycolysis – the breakdown of glucose by enzymes, releasing energy\n\nglycophorin – transmembrane proteins of red blood cells\n\nhaplotype – a set of DNA variations, or polymorphisms, that tend to be inherited together.\n\nHb (HbC, HbE, HbS, etc.) hemoglobin (hemoglobin polymorphisms: hemoglobin type C, hemoglobin type E, \nhemoglobin type S)\n\nhematopoietic (stem cell) – the blood stem cells that give rise to all other blood cells\n\nheme oxygenase-1 (HO-1) – an enzyme that breaks down heme, the iron-containing non-protein part of hemoglobin\n\nhemoglobin – iron based organic molecule in red blood cells that transports oxygen and gives blood its red color\n\nhemolysis – the rupturing of red blood cells and the release of their contents (cytoplasm) into surrounding fluid (e.g., blood plasma)\n\nheterozygous – possessing only one copy of a gene for a particular trait\n\nhomozygous – possessing two identical copies of a gene for a particular trait, one from each parent\n\nhypotonic – denotes a solution of lower osmotic pressure than another solution with which it is in contact, so that certain molecules will migrate from the region of higher osmotic pressure to the region of lower osmotic pressure, until the pressures are equalized\n\nin vitro – in a test tube or other laboratory vessel; usually used in regard to a testing protocol\n\nin vivo – in a live human (or animal); usually used in regard to a testing protocol\n\nleucocyte – white blood cell, part of the immune system, which together with red blood cells, comprise the cellular component of the blood (contrast erythrocyte)\n\nligand – an extracellular signal molecule, which when it binds to a cellular receptor, causes a response by the cell\n\nlocus (gene or chromosome) – the specific location of a gene or DNA sequence or position on a chromosome\n\nmacrophage – a large white blood cell, part of the immune system that ingests foreign particles and infectious microorganisms\n\nmajor histocompatibility complex (MHC) – proteins found on the surfaces of cells that help the immune system recognize foreign substances; also called the human leucocyte antigen (HLA) system\n\nmicro-RNA – a cellular RNA fragment that prevents the production of a particular protein by binding to and destroying the messenger RNA that would have produced the protein.\n\nmicrovasculature – very small blood vessels\n\nmitochondria – energy producing organelles of a cell\n\nmutation – a spontaneous change to a gene, arising from an error in replication of DNA; usually mutations are referred to in the context of inherited mutations, i.e. changes to the gametes\n\nnatural selection – the gradual process by which biological traits become either more or less common in a population as a function of the effect of inherited traits on the differential reproductive success of organisms interacting with their environment (closely related to fitness)\n\nnucleotide – organic molecules that are subunits, of nucleic acids like DNA and RNA\n\nnucleic acid – a complex organic molecule present in living cells, esp. DNA or RNA, which consist of many nucleotides linked in a long chain.\n\noxygen radical – a highly reactive ion containing oxygen, capable of damaging microorganisms and normal tissues.\n\npathogenesis – the manner of development of a disease\n\nPCR – Polymerase Chain Reaction, an enzymatic reaction by which DNA is replicated in a test tube for subsequent testing or analysis\n\nphenotype – the composite of an organism's observable characteristics or traits, such as its morphology\n\nPlasmodium – the general type (genus) of the protozoan microorganisms that cause malaria, though only a few of them do\n\npolymerize – to combine replicated subunits into a longer molecule (usually referring to synthetic materials, but also organic molecules)\n\npolymorphism – the occurrence of something in several different forms, as for example hemoglobin (HbA, HbC, etc.)\n\npolypeptide – a chain of amino acids forming part of a protein molecule\n\nreceptor (cellular surface) – specialized integral membrane proteins that take part in communication between the cell and the outside world; receptors are responsive to specific ligands that attach to them.\n\nreducing environment (cellular) – reducing environment is one where oxidation is prevented by removal of oxygen and other oxidising gases or vapours, and which may contain actively reducing gases such as hydrogen, carbon monoxide and gases that would oxidize in the presence of oxygen, such as hydrogen sulfide.\n\nRNA – ribonucleic acid, a nucleic acid present in all living cells. Its principal role is to act as a messenger carrying instructions from DNA for controlling the synthesis of proteins\n\nsequestration (biology) – process by which an organism accumulates a compound or tissue (as red blood cells) from the environment\n\nsex-linked – a trait associated with a gene that is carried only by the male or female parent (contrast with autosomal)\n\nSporozoa – a large class of strictly parasitic nonmotile protozoans, including \"Plasmodia\" which cause malaria\n\nTCA cycle – TriCarboxylic Acid cycle is a series of enzyme-catalyzed chemical reactions that form a key part of aerobic respiration in cells\n\ntranslocation (cellular biology) – movement of molecules from outside to inside (or vice versa) of a cell\n\ntransmembrane – existing or occurring across a cell membrane\n\nvenous – of or referring to the veins\n\nvesicle – a small organelle within a cell, consisting of fluid enclosed by a fatty membrane\n\nvirulence factors – enable an infectious agent to replicate and disseminate within a host in part by subverting or eluding host defenses.\n\n\n"}
{"id": "9057034", "url": "https://en.wikipedia.org/wiki?curid=9057034", "title": "Interesterified fat", "text": "Interesterified fat\n\nInteresterified fat is a type of oil where the fatty acids have been moved from one triglyceride molecule to another. This is generally done to modify the melting point, slow rancidification and create an oil more suitable for deep frying or making margarine with good taste and low saturated fat content. It is not the same as partial hydrogenation which produces trans fatty acids, but interesterified fats used in the food industry can come from hydrogenated fat, for simplicity and frugality.\n\nFats such as soybean oil consist mainly of various triglycerides which are made up of a glycerol backbone esterified to three fatty acid molecules. The triglycerides contain a mixture of saturated, monounsaturated, and polyunsaturated fatty acids. Interesterification is carried out by blending the desired oils and then rearranging the fatty acids over the glycerol backbone with, for instance, the help of catalysts or lipase enzymes. Polyunsaturated fatty acids (PUFAs) decrease the melting point of fats significantly. A triglyceride containing three saturated fatty acids is generally solid at room temperature and not very desirable for many applications. Rearranging these triglycerides with oils containing unsaturated fatty acids lowers the melting point and creates fats with properties better suited for target food products. In addition, blending interesterified oils with liquid oils allows the reduction in saturated fatty acids in many trans fatty acid free food products. The interesterified fats can be separated through controlled crystallization, also called fractionation.\n\nIn vegetable polyunsaturated oils, the PUFA is commonly found at the middle position (sn2) on the glycerol. Stearic acid is not usually found at sn2 in vegetable oils used in the human diet.\n\nIn most vegetable dietary fats, the palmitic (C16:0) and stearic acid (C18:0) saturated fats mainly occupy the 1- and 3-positions of the triacylglycerol molecule, whereas an unsaturated fatty acid such as oleic acid (18:1ω9) or linoleic acid (18:2ω6) usually occupies the 2-position. In animal fats, this is not the case. Interesterification of vegetable oils will enhance the amount of saturated fatty acids at the 2-position. Fatty acids at the 2-position are biologically different from fatty acids at the 1 and 3 position because they are handled differently during digestion and metabolism, and a relevant scientific question is whether there are health effects following from this. Although this question has received relatively little attention in dietary fats and health research, there are a number of good controlled human intervention studies that have addressed it.\n\nIn studies addressing the health effects of interesterification as such, a diet high in interesterified fat should be compared with a diet high in a noninteresterified fat with the same fatty acid composition. If the two diets show similar changes in the resulting blood lipid profiles (i.e. not different from each other), this indicates interesterification has no effect on metabolism or biological effects.\nConversely, effects of interesterification cannot be properly addressed if the interesterified fat and the noninteresterified fat being compared have different fatty acid compositions.\n\nZock et al. compared the effects of an IE (interesterfied) test fat with 40% C16:0 on the 2-position with a noninteresterified test fat with only 6.5% C16:0 on the 2-position in a 3-week diet study. Despite the very high intakes and the marked difference in positional distribution, no statistically significant effects on fasting blood lipids were observed in the group as a whole. Nestel et al. examined the effects of an IE fat blend with 25% C16:0 on the 2-position with a native fat blend with only 9% C16:0 on the 2-position. Again, despite a high intake level and the clear difference in positional distribution of the fats fed, no effects were observed on fasting blood lipids. Meijer and Weststrate examined the effects of interesterification, using a 'real' hardstock as applied in foods. The control was the same fat blend with a similar fatty acid composition, but not interesterified. The IE fat blend contained more C16:0 on the 2-position (18%) than the control blend (7%). None of the fasting levels of blood lipids measured after 3 weeks showed any change related to treatment of the fat blend. Fasting glucose level was also not affected.\n\nIn 1970, Grande et al. used interesterification to prepare a blend of fats and oils mimicking the fatty acid composition of cocoa butter. No difference between the interesterified fat blend and cocoa butter was observed in levels of total cholesterol in fasting blood.\n\nRecently, in a study funded by the Malaysian Palm Oil Board, Sundram et al. compared the effects of three types of fat: native palm olein, a blend with partially hydrogenated soybean oil and an interesterified mixture of oils. They concluded both the IE blend and the partially hydrogenated fat blend increased the fasting ratio of LDL to HDL cholesterol, indicating an adverse effect on CVD risk. Sundram et al. also found that fasting plasma glucose levels were higher after 4 weeks on the interesterified fat than after the other diets. For the postprandial study the glucose incremental area under the curve (IAUC) following the IE meal was 40% greater than after either other meal (p<0.001), and was linked to relatively depressed insulin and C-peptide (p<0.05). As was pointed out in a letter to the Editor by Destaillats et al., a major limitation of the Sundram study is that the diets differed in overall fatty acid composition. The interesterified fat had 30% more saturated and 57% less monounsaturated fatty acids than the untreated palm olein. The direction of the effects on blood lipids are in line with what can be predicted based on these differences in fatty acid content between the study diets (Mensink 2003).\n\nAnother recent study by Berry et al. compared shea butter (3% C18:0 on the 2-position) and interesterified shea butter (23% C18:0 on the 2-position), while keeping overall fatty acid composition of the diets constant. This study found no effects of interesterification on fasting levels of blood lipids, glucose and insulin. This is line with a number of other human intervention studies.\n\nChristophe et al. have studied the effect of interesterification of butter oil. In a small pilot study, they observed an 11% lower blood total cholesterol level after interesterification. However, in a larger, better designed study the same authors could not reproduce the cholesterol-lowering effects.\n\n"}
{"id": "7725783", "url": "https://en.wikipedia.org/wiki?curid=7725783", "title": "International Partnership for Microbicides", "text": "International Partnership for Microbicides\n\nThe International Partnership for Microbicides or IPM is a non-profit product development partnership (PDP) founded by Dr. Zeda Rosenberg in 2002 to prevent HIV transmission by accelerating the development and availability of a safe and effective microbicide for use by women in developing countries.\n\nSince its inception, IPM has focused on developing HIV-prevention products for women including gels, films, tablets and rings that contain antiretroviral (ARV)-based microbicides. Rights to incorporate existing ARVs into products developed specifically for use in developing countries have been negotiated with pharmaceutical companies working in the HIV field.\n\n\n"}
{"id": "8918440", "url": "https://en.wikipedia.org/wiki?curid=8918440", "title": "John Bell (surgeon)", "text": "John Bell (surgeon)\n\nJohn Bell (12 May 176315 April 1820) was a Scottish anatomist and surgeon.\n\nBell was born in Edinburgh, Scotland; an elder brother of Sir Charles Bell. After completing his professional education at Edinburgh, he carried on from 1790 in Surgeons' Square an anatomical lecture-theatre, where, in spite of much opposition, due partly to the unconservative character of his teaching, he attracted large audiences by his lectures, in which he was for a time assisted by his younger brother Charles. From 1793 to 1795, he published \"Discourses on the Nature and Cure of Wounds\". He is considered, along with Pierre-Joseph Desault and John Hunter, to be a founder of the modern surgery of the vascular system.\n\nA man of compassion, Bell made many enemies because he was outspoken about the unnecessary pain and suffering inflicted by incompetent surgeons practicing in Scotland. In 1800 he became involved in an unfortunate controversy with James Gregory (1753–1821), the professor of medicine at Edinburgh. Gregory in 1800 attacked the system whereby the fellows of the Royal College of Surgeons of Edinburgh acted in rotation as surgeons at the Royal Infirmary, with the result that the younger fellows were excluded. Bell, who was among the number, composed an Answer for the Junior Members (1800), and ten years later published a collection of \"Letters on Professional Character and Manners\", which he had addressed to Gregory. After his exclusion from the infirmary he ceased to lecture and devoted himself to study and practice.\n\nBell was also a talented artist, and was one of the few medical men to illustrate his own work. In 1816 he was injured by a fall from his horse and in the following year went to Italy for the benefit of his health. He died at Rome on 15 April 1820 and he is buried in the Protestant Cemetery, Rome, just behind the tomb of poet John Keats.\n\nHis works also included \"Principles of Surgery\" (1801), \"Anatomy of the Human Body\", which went through several editions and was translated into German, and \"Observations on Italy\", published by his widow in 1825.\n\n\n"}
{"id": "7243456", "url": "https://en.wikipedia.org/wiki?curid=7243456", "title": "Julius Perlis", "text": "Julius Perlis\n\nJulius Perlis (19 January 1880, in Białystok (Poland, then Russian Empire) – 11 September 1913, in Ennstal) was an Austrian chess player.\n\nAt the beginning of his career, Perlis played in Vienna, winning in 1901. Then, in 1902 he took 3rd (Quadrangular), took 2nd, behind Mikhail Chigorin in 1903, and won in 1904. The same year, he took 3rd in Vienna (Gambit tournament). The event was won by Carl Schlechter. In 1905, he tied for 4-6th in Barmen (Masters B). In 1906, he took 9th in Ostend (Schlechter won). In 1906, he took 3rd in Vienna. In 1907, he tied for 7-8th in Vienna (Jacques Mieses won). In 1907, he took 16th in Ostend (Masters B). In 1908, he tied for 7-8th in Vienna (Trebitsch tournament). In 1909, he took 7th in Sankt Petersburg. The event was won by Emanuel Lasker and Akiba Rubinstein. In 1909, he took 3rd in Vienna Richard Réti won). In 1909/10, he took 3rd in Vienna. In 1911, he took 13th in Karlsbad Karlovy Vary (Richard Teichmann won). In 1912, he took 5th in San Sebastian, Spain (Rubinstein won). In 1912, he tied for 3rd-4th in Vienna (Schlechter won). In 1913, he took 5th in Vienna (Rudolf Spielmann won). \n\nPerlis died from exposure in a mountaineering accident in the Alps in 1913.\n\n"}
{"id": "39435348", "url": "https://en.wikipedia.org/wiki?curid=39435348", "title": "Korea Ginseng Corporation", "text": "Korea Ginseng Corporation\n\nKGC (originally \"Korea Ginseng Corporation\", \"Korea Ginseng Corp.\" Korean : 한국인삼공사, Hanguk Insam Gongsa) is a ginseng company in South Korea. KGC's sales volume share was 35% of total Korea's herbal/traditional products market in 2011.\n\nKGC produces popular Korean Red Ginseng products such as Korean Red Giseng Heaven/Earth/Good, Korean Red Ginseng Extract, Korean Red Ginseng Powder, Honeyed Korean Red Ginseng Slices, Korean Red Ginseng Tonic, etc. It has expanded outside Korea, especially into the East Asian market like China and Taiwan.\n\nKGC is a subsidiary company of KT&G, and its two headquarters are in Seoul and Daejeon.\n\nKGC's brand of Korean Red Ginseng, CKJ, is known as Cheong-Kwan-Jang in Korea and overseas. This brand name goes back to the early 1940s. Because of increased export demand for Korean Red Ginseng at the end of the Japanese colonial period, lots of fake root were prospering. To find a way to distinguish real ginseng, the Monopoly Bureau of the Japanese General Government began using the label \"Cheong-Kwan-Jang,\" which is translated as \"officially government approved.\"\n\nThe sale of KGC was a government monopoly from 1899 to 1996. Since then, KGC has held its position at the top of the market, even though the name has changed from the Monopoly Bureau to Korea Tobacco & Ginseng Corporation, and finally to KT&G subsidiary Korea Ginseng Corp.\n\nKGC products are now exported to over 40 countries. KGC also has overseas branches in the USA, China, Taiwan, Japan, the Philippines and Indonesia.\n\n"}
{"id": "44442755", "url": "https://en.wikipedia.org/wiki?curid=44442755", "title": "LGBT healthcare in the United States Veterans Health Administration", "text": "LGBT healthcare in the United States Veterans Health Administration\n\nThe United States Veterans Health Administration (VHA) has a lesbian, gay, bisexual and transgender (LGBT) Program through the Office of Patient Care Services. However, VHA does not currently collect data on veteran’s sexual orientation or gender identity. There are estimated to be more than one million LGBT Americans who are military veterans.If LGBT veterans use VHA at the same rate as non-LGBT veterans, there could be more than 250,000 LGBT veterans served by VHA. Using diagnostic codes in medical record data, Blosnich and colleagues found that the prevalence of transgender veterans in VHA (22.9/100,000) is five times higher than reported prevalence of transgender-related diagnoses in the general population (4.3/100,000). Brown and Jones identified 5,135 transgender veterans receiving care in VHA using a broader set of diagnostic codes. Brown also notes that this methodology fails to identify transgender veterans who have not disclosed their gender identity to providers, those who don’t meet criteria for a diagnosis, or veterans who get their transition-related care outside of the VHA. \n\nTwo recent events increased focus on LGBT veterans in VHA. In 2011 VHA released a national transgender healthcare policy (VHA Directive 2011-024/later, 2013-003), which covers gender counseling, hormone therapy, pre-surgical evaluations, and post-surgical care, but transition surgeries are not currently covered. At least one report suggests an increase in veterans receiving transgender-related diagnoses in VHA since the transgender policy was released. \n\nThe second event is related to the Department of Defense's repeal of its Don’t Ask, Don’t Tell policy in 2011, allowing openly gay, lesbian and bisexual military personnel to serve in the armed forces. However, ending Don’t Ask, Don’t Tell did not change the military’s policy toward transgender people who continued to be banned from military service. Policy changes that allowed open transgender service were announced in June, 2016 and are effective October 1, 2016. The repeal of Don’t Ask, Don’t Tell is expected to increase the numbers of LGB military personnel and consequently increase the number of LGB veterans who seek care at the VHA after service.\n\nSome researchers have raised concerns that LGBT veterans may not feel welcome at the VHA. Sherman and coauthors found that 24% of a sample of 58 LGBT veterans had not disclosed their sexual orientation to any VHA provider. The researchers noted that LGBT veterans may be uncomfortable disclosing their sexual orientation or gender identity because when they were in the military this information could lead to their discharge from service. Although VHA never had a policy prohibiting care to LGBT veterans, former military personnel often associate military policy with VHA policy. Sherman and colleagues also found that some LGBT veterans worried that disclosing their sexual or gender minority status would lead to losing VA medical benefits, even though there is a Patient’s Rights and Responsibilities Policy that prevents this type of discrimination. \n\nIn the same study, Sherman and colleagues examined attitudes and practices of 202 VHA providers at two mid-western hospitals and found that only half of providers had asked their patients about sexual orientation in the past year, despite it being relevant to health care. Mental health providers were more likely to ask patients about sexual orientation than medical providers, perhaps due to an awareness of the stress associated with being a member of a minority group. Less than half of the VHA providers overall (43%) had received any kind of training in LGBT health issues since they began practicing. \n\nLGBT people who disclose their sexual orientation to healthcare providers and feel comfortable talking to their providers about sexuality and sexual health also report greater satisfaction with their healthcare. LGBT people, including LGBT veterans, have unique health needs that need screening and follow up, which require healthcare providers initiate conversations about sexuality and gender identity and expression with patients.\n\nTo address the healthcare needs of LGBT veterans and create a more welcoming environment at VHA facilities, as well as increase the competency of providers, VHA has taken several actions, starting with several policy changes. For example, patient and Department of Veterans Affairs (VA) employee non-discrimination policies now include protections for sexual orientation, gender identity and expression and also have an inclusive definition of family. A policy on the rights of veterans’ family members includes a broad definition of “family” that allows LGBT veterans to decide who is regarded as part of their family. As mentioned above, VHA also issued a national transgender healthcare policy in 2011 and began training providers on transgender care. At present, VHA does not have a separate healthcare policy on LGB care, although such a policy is in development.\n\nIn 2011, VHA established the Office of Health Equity to work at a systems level to reduce health disparities in a number of vulnerable populations, including LGBT veterans, by raising awareness and advocating for healthcare system changes. One important activity by the Office of Health Equity has been their championing of VA facility participation in the Human Rights Campaign Healthcare Equality Index survey. In the most recent survey, 114 of 121 VA facilities participated in the HEI and 96 (84%) facilities received “Leader” status in LGBT patient care. To achieve “Leader” status in the HEI, agencies must demonstrate that they meet criteria in four domains: 1) patient non-discrimination policies; 2) equal visitation policies; 3) employment non-discrimination policies; and 4) training in LGBT patient-centered care.\n\nTo address clinical practice issues, in 2012 the VHA established the LGBT Program within the Office of Patient Care Services. The LGBT Program supports two internal websites (SharePoints) – one on LGB veteran care and one on transgender veteran care – which host archived staff trainings, links to policies, links to professional society guidelines, journal articles, facility information about LGBT support groups, and LGBT awareness campaign materials. The LGBT program also worked with subject-matter experts to create on-demand webinars about transgender veteran healthcare for the VHA’s internal online educational platform and two webinars about LGB veteran healthcare for an Internet platform. These programs are now publicly available through VHA TRAIN, a free service of the Public Health Foundation, and VA eHealth University. \n\nThe LGBT Program also worked to develop clinical support for VHA providers. E-consultation on transgender care is available to any VHA provider at any facility through the veteran’s electronic health record. Providers asking a question get a case-specific response from an interdisciplinary team of experienced clinicians. The national e-consultation program began in 2014 and became completely national in spring 2015. In addition, interdisciplinary teams of providers can participate in an intensive training/case-consultation program through videoconferencing. For this program, at minimum, a “team” consists of one behavioral health provider and one medical provider and meets with experts twice a month for one hour over seven months for a total of 14 sessions. The participating teams receive a 15-20 minute didactic presentation on a transgender care topic and the rest of the session is spent on case-based consultation. Each participating team presents 1-2 cases, while others listen and ask questions. By June 2016, 55 interdisciplinary teams of more than 370 providers will have completed this seven-month training.\n\nAnother training program consists of nine interprofessional postdoctoral psychology fellowships in LGBT health sponsored by the VHA Office of Academic Affiliations and Mental Health Services. The LGBT Program advises these nine fellowship sites, which include VA medical centers at Bedford, MA; Boston, MA; Chicago, IL; Honolulu, HI; Houston, TX; Milwaukee, WI; San Diego, CA; San Francisco, CA; and West Haven, CT. Fellows provide direct clinical services to LGBT veterans in a variety of clinical settings, as well as train staff and outreach to LGBT community agencies.\n\nIn addition to responding to the needs of LGBT Veterans, the VHA Office of Diversity and Inclusion (ODI) has taken steps to foster a supportive environment for LGBT employees. Employees enjoy the same discrimination protections as patients, including protections for sexual orientation and gender identity. For example, the Office of Personnel Management (OPM) offers protections to transgender VA employees, including access to sanitary facilities, use of preferred name and pronouns in employee files, and avenues through which to report workplace discrimination, as well as healthcare coverage for transition related care. Protections are in place for both sexual orientation and gender minority federal employees. ODI has established an LGBT Special Emphasis Program (SEP), which works to ensure that there is equal opportunity for LGBT employees to work at VHA facilities. One goal of the SEP is to establish a manager at each VHA facility who serves as the facility expert on LGBT policy and leads the staff in creating a welcoming environment to LGBT employees through educational activities and cultural competency trainings.\n"}
{"id": "3738309", "url": "https://en.wikipedia.org/wiki?curid=3738309", "title": "List of United Nations Security Council Resolutions 1601 to 1700", "text": "List of United Nations Security Council Resolutions 1601 to 1700\n\nThis is a list of United Nations Security Council Resolutions 1601 to 1700 adopted between 31 May 2005 and 10 August 2006.\n"}
{"id": "53643113", "url": "https://en.wikipedia.org/wiki?curid=53643113", "title": "List of dining events", "text": "List of dining events\n\nThis is a list of historic and contemporary dining events, which includes banquets, feasts, dinners and dinner parties. Such gatherings involving dining sometimes consist of elaborate affairs with full course dinners and various beverages, while others are simpler in nature.\n\n\n\n\n\n\n"}
{"id": "41078970", "url": "https://en.wikipedia.org/wiki?curid=41078970", "title": "List of food riots", "text": "List of food riots\n\nFood riots may occur when there is a shortage or unequal distribution of food. Causes can be food price rises, harvest failures, incompetent food storage, transport problems, food speculation, hoarding, poisoning of food, or attacks by pests. During the period 2007–2008, a rise in global food prices led to riots in various countries. A similar crisis recurred in 2010–2012.\n\n\n\n\n\n\n"}
{"id": "51295378", "url": "https://en.wikipedia.org/wiki?curid=51295378", "title": "Lodging (agriculture)", "text": "Lodging (agriculture)\n\nLodging is the bending over of the stems near ground level of grain crops, which makes them very difficult to harvest, and can dramatically reduce yield. Lodging in cereals is often a result of the combined effects of inadequate standing power of the crop and conditions such as rain, wind, hail, topography, soil, previous crop, and others.\n\nLodging affects wheat, rice, and other cereals, and reducing it is a major goal of agricultural research. Dwarf varieties, which are shorter, are one way of reducing lodging.\n\nLodging may occur at the root or the stem; the latter typically happens later, when the stem is dry and brittle. The timing of lodging can control its effect on yield, disease, grain moisture, quality, and evenness of ripening.\n"}
{"id": "2769012", "url": "https://en.wikipedia.org/wiki?curid=2769012", "title": "Nagraj", "text": "Nagraj\n\nNagraj (\"Snake-King\") (नागराज in Devanagari script) is a fictional superhero appearing in Raj Comics. Created in the late 1980s by Sanjay Gupta, Firt story was written by Prashuram Sharma and illustrations were done by Sanjay Ashtputre and later it was drawn by Pratap Mulik and Anupam Sinha. Nagraj has changed a lot in his 25 years in print, both in terms of looks as well as powers and abilities.\n\nThe debut issue of Nagraj was written by Parshuram Sharma and illustrated by Sanjay ashtputre\nAfter that Pratap Mullick illustrated the character for around 50 issues, ending in 1995. Since 1995, the illustration work of Nagraj has been taken over by artist and writer Anupam Sinha.\n\nThe name Nagraj is derived from the word \"Nagaraja\"(Hindi:नागराजा ) which is itself made up of two words \"nāga\" (Hindi:नाग) and \"raja\" (Hindi:राजा) where the word \"nāga\" means snake and \"raja\" means king. Nagraj is believed to have been inspired by the mythological \"Ichchhadhari Nag\" (shape shifting snakes) and historical \"Vishmanushya\" (venomous human). His stories create a rich blend of mythology, fantasy, magic, and science fiction. Many of Nagraj's fans believe that, over time, Nagraj's comics have developed a snake mythology of its own, which is unique to the popular Indian beliefs about snakes that are prevalent among the masses.\n\nNagraj was originally conceived as an enemy of international terrorism. In his debut issue, Nagraj was unleashed as an international terror weapon by the evil scientist Professor Nagmani. Nagraj, in this first mission, was tasked with stealing a golden statue of a Goddess from a temple that was protected by tribal devotees, snakes, and by a mysterious 300-year-old Sadhu named Baba Gorakhnath. Nagraj succeeded in his task, but upon confrontation with Gorakhnath and his mystic black mongoose shikangi, was defeated. Gorakhnath read his mind and discovered that Professor Nagmani had implanted a mind control device in the form of a capsule in Nagraj's head, to keep him under his control. Gorakhnath operated and removed the capsule from Nagraj's head, setting Nagraj free of Professor Nagmani's control. Nagraj then became Baba Goraknath's disciple and vowed to eliminate crime and terror from the Earth. Since then, Nagraj has thrice toured the world and defeated many villains and terrorists.\n\nNagraj lives as Raj (After Nagraj Ke Baad, he is under disguise of Nagraj Shah) in a fictional Metropolitan City Mahanagar as an employee of a TV channel that he owns secretly.\n\nIn ancient times, there existed a kingdom known as Takshaknagar, ruled by King Takshakraj and Queen Lalita. The couple had no worries except for one fact that, they had no children. The absence of a prince or princess made Nagpasha the only potential heir to the throne. Nagpasha was the younger brother of the King Takshakraj. Knowing that he was the only potential heir to the throne, Nagpasha started living a lavish worry-free life.\n\nAs time passed, Queen Lalita started getting depressed for not having a child of her own. The king realised the cause of her depression and became upset as well. The couple used to pray to their family deity Dev Kaljayi for his blessings. Dev Kaljayi also knew of their grievances, so one day he blessed them to have a great child. His blessings came true as the queen soon became pregnant and the whole kingdom rejoiced, except for Nagpasha. The birth of this child meant the loss of the throne for him, so he decided to kill the child before he was born.\n\nOne day when the queen was going to pray to Deva Kaljayi, Nagpasha replaced the curtained plate of her offerings to the god with one containing a dead mongoose. The Snake God got angry and knocked her unconscious with his venomous breath. The king sought forgiveness from the Devta and pleaded with him to cure his wife, to which the Devta refused. Then the desperate king tried to commit suicide. Not able to withstand a devotee's death, Deva Kaljayi showed him a way to save the queen's life. He gave a crystal to the king and told him to treat it with the queen's medicine. The crystal would divert all the poison from the queen's body to the child's body. However, due to the ill effects of the poison. the queen would lose her fertility.\n\nThe kingdom was highly grieved, as was the king. Nagpasha was overjoyed that he was now the sole heir to the throne. When the child was born, everyone believed him dead because his whole body was blue and showed no signs of life. As per Hindu rituals, the newborn baby was thrown into the river.\n\nNagpasha rejoiced and went to Deva Kaljayi; the deity that also protected the grand royal treasure in form of a giant two-headed snake, and asked him to hand over the royal treasures, telling him that now he was the sole heir to the throne, so the treasure rightfully belonged to him. Deva Kaljayi refused and told him that the \"real successor\" to the throne was alive and when the time comes the treasure will be handed over to him. Enraged, Nagpasha raised his sword against the deity only to be thrown away by a mere flick of the giant snake's tail. Nagpasha fell upon two bowls, one containing the highly toxic venom (halahal,the greatest form of venom as believed by Hindus) which destroyed his face and mixed with his blood, and the other containing Amrit, which made him immortal. Simultaneous effects of both made Nagpasha an immortal, venomous man. At that time, Nagpasha could not tolerate the changes in his body and fainted.\n\nWhen the king was informed of the happenings by Dev Kaljayi, he realised that his son was not dead and he also realised the potential dangers to his son's life. So he ordered his faithful astrologer Vedacharya, who had great knowledge of Tilism to enclose the treasure in a Tilism which could be broken only by his son. Vedacharya made the tilism with the co-operation of Dev Kaljayi to ensure that no one, except for the king's son, will be able to break the tilism, not even immortal Nagpasha. When Nagpasha came to his senses, he realised that he had lost the treasure. Enraged, he murdered the king and the queen.\n\nThe child, floating away on the river in his state of suspended animation, got stuck somewhere in bushes. He laid there for a long time.\n\nMeanwhile, the snake deity Deva Kaljayi appeared in the dreams of King Maniraj and his wife Queen Manika, rulers of ageless Ichchhadhaari naags, living secretly on an invisible island in the Indian Ocean called Nagdweep. He told them the location of the baby and asked them to cure him. They did so and discovered that the baby was far more venomous than the greatest snake on Nagdweep, which was Mahatma Kaaldoot, indicating that he had divine venom of the god. Initially, the raj vaid was unsure whether he would be able to cure the child, but since Dev Kaljayi himself asked the king for his treatment, he was assured that the treatment will work. According to the rules, no one was permitted to bring an outsider to the island, so the king decided to keep his presence a secret.\n\nMany years passed and the treatment started showing results and, although still in the suspended animation, the color of the baby had gradually changed to green. The king gave the news to the queen and they decided to adopt the child, since they had no child of their own. Their decision was heard by Vishandhar, an evil Tantrik who wanted to become the ruler of the island, but was afraid of Mahatma Kaaldoot. He attacked the secret area where the baby was kept and escaped with him, but fearing the wrath of the god, he decided not to kill the baby and instead placed him back into the same bushes in the river where he was found. His plan failed as the queen soon got pregnant and gave birth to a daughter who was named Visarpi.\n\nVishandhar never knew that the baby that he left astray was cured enough to regain his senses. First, his face and later, his whole body, turned normal color and he started crying. A priest of the nearby temple located him and gave him to Professor Nagmani, who was wandering in the nearby forest searching for snakes. For unknown reasons, the priest narrated a fake story that the child belonged to a woman, who was a devotee of the Snake God. He also said that the child was blessed by an ichchhadhari naag, so he asked Nagmani to bring up the child and allows him to avenge his mother.\n\nNagmani realised that the priest was lying, but he took the child. The blood tests of the child showed that the child had minute, microscopic snakes in his blood, filling in for white blood cells. The child had extraordinary healing powers and was extremely venomous. He raised the baby, who became Nagraj.\n\nNagraj was presented to the world as a creation of Professor Nagmani. He was meant to be an ultimate killing machine, and his original plan was to hire out Nagraj to the highest bidder among villains and terrorist groups of the world.\n\nProfessor Nagmani used him for his other experimental projects and leased him for international terrorism. Nagmani always claimed that Nagraj's powers were evolved by feeding him bits of snake poison until his blood itself became poison. He claimed that his venom was a result of snake bites from 1,000 different species of snakes (This echoes the Indian legends of poison-men or women, specifically raised to kill by their kiss) and his other powers evolved when he was treated with ashes of a dead ichchhadhaari snake. In reality, Nagraj was much more poisonous than any species of snakes because his venom was celestial.\n\nBased largely on the Hindu myth of the shape shifting snake, Nagraj derives most of his powers from microscopic snakes that live in his bloodstream, in lieu of white blood cells. He has a number of powers, such as superhuman strength, poisonous breath and poison-bite, instant healing powers, and snakes that come out of his wrists. The snakes can come out individually or form into ropes, parachutes, and many others, depending on his imagination. His venom is believed to be far stronger than Potassium Cyanide, resulting in the melting of any living being's body if he bites them or vice versa.\n\nFor his first mission, he was sold for a sum of $1 million and was to recover an ancient statue. Nagraj succeeds, but then is foiled by the Sage Baba Gorakhnath, who frees him from the mind control of Professor Nagamani. Waking up to a newer world amid the calmness of Gorakhnath, Nagraj takes a vow to eliminate terrorism from the world and works toward achieving this goal.\n\nIn his initial issues, Nagraj's powers were too limited, and at times he was even knocked out by powerful human opponents. His strength was also quite human in nature, as he would marvel at his opponent's strength, such as when someone picked up a car. Over the years, Nagraj became powerful enough to not only pick up falling cars single-handedly, but also to achieve feats such as throwing things into outer space effortlessly. This happened in the issue \"Nagadhish\", when he threw the controlling sceptre of a serpent court official into orbit.\n\nIt is notable, and perhaps ironic, that Nagraj has now been transformed into a mythical/magical creature facing fantastical creatures as his enemies, with elements of sorcery/magic and even time and space travel.\n\nNagraj, a character in the Raj Comics series, has a huge arsenal of powers of which most can be credited to his divine birth in the comics storyline. Ever since Writer & Artist Anupam Sinha took over the character of Nagraj, his powers and abilities have evolved a great deal making him powerful enough to counter any character of the comic universe.\nNagraj is arguably the most powerful superhero in the Raj Comics Universe and is among the most powerful Icchadhari Naagas of his time. Time and again, he has even shown the potential of challenging the Supreme Snake-Gods like Sheshnaag, Vasuki, Takshak, Kaaljayi, and has defeated the likes of Kaaldoot, Trifana, Mahavyaal and Sheetnaag Kumar, who are ranked among the mightiest snakes in their species.\nNagraj's blood is a mixture of red blood cells, venom and minute snakes. He can \"release\" snakes out of his body at his will through his hands and use them for whatsoever purpose he likes, controlling them through telepathy. Though Nagraj gives full liberty to his snakes, his will can keep them inside his body, leading him at times to take enemy snakes inside him.\nSnakes in his body can be classified into many groups:\nThough these are snakes, of many different species, which commonly exist in the real world, being born in Nagraj's body\nmeans that they share in Nagraj's extremely powerful venom. Nagraj uses them variably as ropes, ladders, parachutes, shields, boats, spies and messengers. Most notable among them is Naganand, at a time considered right-hand man of Nagraj, he left Nagraj's body to look after the people of Nagdweep at Nagraj's order after he upgraded to become an ichchhadhaari nag.\nNagraj has accumulated snakes which weren’t born in Nagraj's body, but it was Nagraj who gave them a place in his body. These snakes are usually snakes carrying some special abilities. They include\n\nNagraj's body gets divided into minute particles making him almost ethereal and immune to physical damage, also\ntemporarily disabling his power to eject snakes out of his body. This form of Nagraj is called Maanas Nagraj.\nIchchhadhaari nags.\nThey may or may not be of the same species but all snakes possessing the power of shape-shifting come under this\ncategory. They include a big bunch of ichchhadhaari naags banished from Nagdweep. It is quite notable that some of\nthem possess such great powers which rival Nagraj's own, but they all hold great respect for Nagraj. While some of them\nlive in his body just cause they have no better place to live, others live willingly to learn something from Nagraj's way of\nlife. Most notable names among them are :\nHis main source of power is his mani which grants him almost every imaginable power. For entering Nagraj's body he\nhas to remove his mani from his possession so he hides it in micro form at some place in Nagraj's dress. He though\nlooks and behaves like an idiot and talks non-sense, but he is the one who can never be underestimated. Unlike other snakes in Nagraj's body, Naagu decided to live in Nagraj's body for no reason other than watching movies, which becomes easier living in\ncity. He is a true movie lover and remembers all dialogues of his favourite movies be it Bollywood or Hollywood.\n\nNagraj's Venom\nNagraj's venom is the most powerful venom of the world, so much so that direct contact with it results in instant death\nand melting of body. He gained his venom through the blessings of snake deity Deva Kaaljayi, who himself was blessed\nby Lord Shiva. It is explained that Lord Shiva who holds the strongest venom of the universe synthesised during\nSamudra-Manthan, gave a minor portion of it to Deva Kaljayi who holds a dilute form of it and Nagraj holds an even more\ndilute form of it. The different ways in which Nagraj uses his venom also results in various powers he is said to possess.\nLike :\nVish-funkar\nNagraj uses his breath to force out his venom in vapour form, depending upon its concentration the\nvish-funkar can cause a variety of damages to his enemies. Its effects include temporary blindness, drowzyness, black\nouts, deaths and for once it even caused fire. This power of Nagraj once as a side-effect temporarily cured Richa (love\ninterest of Super Commando Dhruva) of her disease pertaining respiratory system. This is among one of the oldest\ndisplayed powers of Nagraj.\n\nVish-dansh\nresults in an opponent's instant death,[1] though exceptional beings able to withstand the poison\ninclude the Deva Kaljayi, himself and enemies who didn't have a biological body. It was suggested in one issue\nthat the venom inside Nagraj's teeth is far more vicious than the venom in his blood, perhaps so much so that\nNagraj might die if he bites himself.\n\nVish-fuhaar\ncomparison to vapour form therefore it causes more damage in comparison to vish-funkar, it is said that use of this power\nhas negative effects on Nagraj weakening him temporarily cause it results in sudden drop of venom inside his body\ndecreasing the number of sukhsham-sarps too.\n\nVish-varsha\nrained a very dilute form of his venom which was still extremely powerful. Whosoever came in contact with it died\ninstantly and their bodies melted. After the clouds cleared there were many human skeletons lying at various places as a\nproof of the viciousness of Nagraj's venom.\n\nIchchhadhaari Shakti and its origin\nVishwamitra a king who turned into a Rishi and wished to become a Brahmarshi (which he later became) once promised\nhis disciple Trishanku to send him to heaven alive with his body. This alarmed the Gods who to protect the rules of the\nuniverse decided not to let this happen. When Rishi Vishwamitra applied his power to send him towards heaven, the\nGods applied counter-force to oppose him. This resulted in a stalemate. Despite all his efforts Vishwamitra failed to fulfil\nhis promise. This enraged Vishwamitra, he saw this act of the Gods as his insult, so he used his power to call upon the\npower which was used for the creation of the Universe. He started creating a new heaven for sending Trishanku there to\nkeep his words. Seeing the powers of the rishi and alarmed by his act the gods themselves descended upon earth and\nconvinced him to drop his decision. Vishwamitra after realizing that Trishanku had evil intentions left him hanging. (It is\nbelieved that he is still hanging somewhere between heaven and earth alive cause Vishwamitra gave him half of his\npowers).\nVishwamitra called upon the power himself, but he couldn't send it back so he entrusted this power to his disciple,\na snake Mahatma Kaaldoot. Minor portions of the power passed on to his future generations and resulted in a new\nspecies of snakes the Ichchhadhari naags (shape-shifting snakes). Most of the snakes don't know all the uses of\nthis power and used it only for shape-shifting purposes but this power can do anything, even creating living beings.\nIt is least used but most effective power of Nagraj, celestial as its origin is this power's true extent was unknown to\neveryone for quite a long time. Since Nagraj had lost a very major part of this power\ncourtsey See Through, he doesn't try stupid stunts and makes minimum use of this power cause if he tries shapeshifting\ninto something beyond his power limit then he might not be able to reform a new body and left bodiless like See\nThrough or maybe even die.\nApplications of this power:\nShape-shifting\nHypnotism\napplication of his ichchhadhaari shakti. Utilizing this power Nagraj can cast a hypnotic reality upon his surroundings using\nhis eyes.[1] Breaking this hypnotic fate is extremely difficult and going against it is impossible. Nagraj\nand his snakes can cast normal hypnotism too.\nBody-fusions\nfusions :\nmany heads and hands. Every individual retains his or her own identity and set of powers. Nagraj used it for the first time\nfusing with the legendary five in order to fight against a creature created by Nagina .\nidentity. The new identity has new set of powers born from the combination of the participants' sets of powers. Nagraj\ndisplayed this power for the first time fusing with Lava to become Agni-Nagraj (Fire-Nagraj). The new born identity could\nrelease 'burning snakes' and could breathe 'venomous fire'.\nPsychic powers\nTelepathy\nIn addition to his ability to communicate with snakes, Nagraj contacted Super Commando Dhruva telepathically when he\nwas trapped in a parallel dimension containing evil souls.\nPsychic force\nachieved the feat using his ichchhadhaari shakti. Though the latter claim has not been substantiated, Nagraj's psychic\npotential has been demonstrated during his battle with the Mahamanav (an advanced creature having enough psychic\ncapabilities to wipe out entire planets) in a psychic power battle.\n\nNagraj, along with his deeds, has earned a large amount of friends, many of whom appeared just once, but some that appeared more often, and still fewer that became permanent figures in Nagraj's stories. Here is a list of some very well known characters in his comics :\n\nInnumerable enemies of Nagraj have appeared in Nagraj's comics; many died, but some lived on to appear again. Here is the list of Nagraj's major enemies that are still alive and who can be expected to make a comeback in future issues :\n\n\n\n\nNumerous attempts have been made by Raj Comics to expand Nagraj into other media. However most attempts have been unfruitful.\nIn the late 1990s, Raj Comics tried adapting Nagraj into a live-action television show in India. A few episodes were shot. However, the show never made it to the television screen. The reason cited was the poor quality of the special effects. Raj Comics was not satisfied with the quality of the show. The production of more episodes was placed on indefinite hiatus. The three episodes that were made are available on CDs, which were distributed freely as an attachment to digest-sized comic editions. (Khalnayak Nagraj, Samraat, and Saudangi)\n\nNagraj was also set to feature in a proposed animated television series. Once again the show never made it to the television screen. An episodic series was in works, and a teaser was uploaded on YouTube featuring Nagraj and his arch enemy Jadugar Shakoora. However, since then there has been no news about the series. It is assumed that production has stopped because the production company Rtoonz's website has since disappeared.\n\nIn November 2007, an agreement was signed between Motion Picture Corporation and Raj Comics to produce a high quality 2D Movie featuring Nagraj.\n\nRaj Comics is publishing a landmark new series featuring Nagraj named Nagayana. It was first intended to be a four-part mini-series, but later, the series was extended to include more issues. Raj Comics has also decided to publish it worldwide. It's a story based on a hypothetical future world casting Nagraj and Super Commando Dhruva 25 years into the future. Largely based on Hindu epic Ramayana, the story narrowly follows the same storyline as of the epic, but with Raj Comics characters filling in the places of original characters. The reason for the extension of the mini-series was to tell various sub-plots and the series was finally declared to be an eight-part series. The last part is Iti Kaand, which is a 128-page issue released on 10 March 2009. The new series is as follows:\n\nA new series named \"Aatanqharta Nagraj\" or \"Nagraj-World Terrorism Series\" is being published parallel to the \"Nagayana\" series. In this series, Nagraj is travelling around the world to fight and eliminate terrorism. \nWorld Terrorism Series\" is being written by Mr. Nitin Mishra.\n\nThis series includes:\n\nNagraj ke Baad (After Nagraj) is a three-part series (Nagraj Ke Baad, Fuel, and Vinom) starting with a comic of the same name. It deals with a time when Nagraj is killed by his enemies and now the world is facing serious threats from the villains.\n\nThese days Raj Comics have published three parallel series of Nagraj, where each Nagraj is involved in a different universe. There are the following three parallel Nagraj series:\n\nNagraj – The original ongoing series of Nagraj, which is being illustrated and written by Mr. Anupam Sinha.\n\nAatankharta Nagraj– This Nagraj appears in the World Terrorism series. In the series, Nagraj no longer resides in Mahanagar. but instead, is travelling in different parts of the world fighting terrorism. This series, until now, has not portrayed villains with supernatural powers, but are more like gangsters or dons of different countries/cities. It is illustrated by Mr. Hemant and written by Mr. Nitin Mishra.\n\nNarak Nashak Nagraj – This Nagraj is a bit different from the original Nagraj in appearance. He wears a jacket and a trouser and has longer hair. He also possesses different powers (Sheetika, Takshika, and Agnika) than the original Nagraj. He also rides on a flying snake named Sarpat. Until now, the series has shown him fighting vampires, demons, zombies, etc. It is illustrated by Mr. Hemant and written by Mr. Nitin Mishra.\n\n"}
{"id": "59066542", "url": "https://en.wikipedia.org/wiki?curid=59066542", "title": "National Health Insurance Authority", "text": "National Health Insurance Authority\n\nThe National Health Insurance Authority is an agency of the government of the Bahamas, established under the National Health Insurance Act 2003. \n\nIt is intended to secure the implementation of a national health insurance policy that ensures access to basic healthcare services to all residents.\n\nIn 2018 it proposed a very substantial extension to the existing National Health Insurance program , and associated increases in taxation.\n"}
{"id": "11216978", "url": "https://en.wikipedia.org/wiki?curid=11216978", "title": "National Institute of Health (Pakistan)", "text": "National Institute of Health (Pakistan)\n\nThe National Institutes of Health (, abbreviated as NIH), is an autonomous body affiliated with the Ministry of National Health Services, Regulation and Coordination, primary responsible for biomedical and health related research along with vaccine manufacturing.\n\n"}
{"id": "981225", "url": "https://en.wikipedia.org/wiki?curid=981225", "title": "Nocebo", "text": "Nocebo\n\nA nocebo effect is said to occur when negative expectations of the patient regarding a treatment cause the treatment to have a more negative effect than it otherwise would have. For example, when a patient anticipates a side effect of a medication, they can suffer that effect even if the \"medication\" is actually an inert substance. Both placebo and nocebo effects are presumably psychogenic, but they can induce measurable changes in the body. One article that reviewed 31 studies on nocebo effects reported a wide range of symptoms that could manifest as nocebo effects including nausea, stomach pains, itching, bloating, depression, sleep problems, loss of appetite, sexual dysfunction and severe hypotension. Mental states such as beliefs and expectations can strongly influence the outcome of disease, the experience of pain, and even success of surgery. The complementary concept, the \"placebo\" effect, is said to occur when positive expectations improve an outcome.\n\nThe term \"nocebo\" (Latin \"nocēbō\", \"I shall harm\", from \"noceō\", \"I harm\") was coined by Walter Kennedy in 1961 to denote the counterpart to the use of \"placebo\" (Latin \"placēbō\", \"I shall please\", from \"placeō\", \"I please\"; a substance that may produce a beneficial, healthful, pleasant, or desirable effect). Kennedy emphasized that his use of the term \"nocebo\" refers strictly to a subject-centered response, a quality inherent in the patient rather than in the remedy\". That is, Kennedy rejected the use of the term for pharmacologically induced negative side effects such as the ringing in the ears caused by quinine. That is not to say that the patient's psychologically induced response may not include physiological effects. For example, an expectation of pain may induce anxiety, which in turn causes the release of cholecystokinin, which facilitates pain transmission.\n\nIn the narrowest sense, a nocebo response occurs when a drug-trial subject's symptoms are worsened by the administration of an inert, sham, or dummy (simulator) treatment, called a placebo. According to current pharmacological knowledge and the current understanding of cause and effect, a placebo contains no chemical (or any other agent) that could possibly \"cause\" any of the observed worsening in the subject's symptoms. Thus, any change for the worse must be due to some subjective factor. Adverse expectations can also cause the analgesic effects of anesthetic medications to disappear.\n\nThe worsening of the subject's symptoms or reduction of beneficial effects is a direct consequence of their exposure to the placebo, but those symptoms have not been chemically generated by the placebo. Because this generation of symptoms entails a complex of \"subject-internal\" activities, in the strictest sense, we can never speak in terms of simulator-centered \"nocebo effects\", but only in terms of subject-centered \"nocebo responses\". Although some observers attribute nocebo responses (or placebo responses) to a subject's gullibility, there is no evidence that an individual who manifests a nocebo/placebo response to one treatment will manifest a nocebo/placebo response to any other treatment; i.e., there is no fixed nocebo/placebo-responding trait or propensity.\n\nMcGlashan, Evans & Orne (1969, p. 319) found no evidence of what they termed a \"placebo personality\". Also, in a carefully designed study, Lasagna, Mosteller, von Felsinger and Beecher (1954), found that there was no way that any observer could determine, by testing or by interview, which subject would manifest a placebo reaction and which would not. Experiments have shown that no relationship exists between an individual's measured hypnotic susceptibility and their manifestation of nocebo or placebo responses.\n\nIt has been shown that, due to the nocebo effect, warning patients about side effects of drugs can contribute to the causation of such effects, whether the drug is real or not. This effect has been observed in clinical trials: according to a 2013 review, the dropout rate among placebo-treated patients in a meta-analysis of 41 clinical trials of Parkinson's disease treatments was 8.8%. A 2013 review found that nearly 1 out of 20 patients receiving a placebo in clinical trials for depression dropped out due to adverse events, which were believed to have been caused by the nocebo effect.\n\nEvidence suggests that the symptoms of electromagnetic hypersensitivity are caused by the nocebo effect.\n\nVerbal suggestion can cause hyperalgesia (increased sensitivity to pain) and allodynia (perception of a tactile stimulus as painful) as a result of the nocebo effect. Nocebo hyperalgesia is believed to involve the activation of cholecystokinin receptors.\n\nStewart-Williams and Podd argue that using the contrasting terms \"placebo\" and \"nocebo\" to label inert agents that produce pleasant, health-improving, or desirable outcomes versus unpleasant, health-diminishing, or undesirable outcomes (respectively), is extremely counterproductive. For example, precisely the same inert agents can produce analgesia and hyperalgesia, the first of which, from this definition, would be a placebo, and the second a nocebo.\n\nA second problem is that the same effect, such as immunosuppression, may be desirable for a subject with an autoimmune disorder, but be undesirable for most other subjects. Thus, in the first case, the effect would be a placebo, and in the second, a nocebo. A third problem is that the prescriber does not know whether the relevant subjects consider the effects that they experience to be desirable or undesirable until some time after the drugs have been administered. A fourth problem is that the same phenomena are being generated in all the subjects, and these are being generated by the same drug, which is acting in all of the subjects through the same mechanism. Yet because the phenomena in question have been subjectively considered to be desirable to one group but not the other, the phenomena are now being labelled in two mutually exclusive ways (i.e., placebo and nocebo); and this is giving the false impression that the drug in question has produced two different phenomena.\n\nSome people maintain that belief kills (e.g., \"voodoo death\": Cannon (1942) describes a number of instances from a variety of different cultures) and belief heals (e.g., faith healing). A \"self-willed\" death (due to voodoo hex, evil eye, pointing the bone procedure, etc.) is an extreme form of a culture-specific syndrome or mass psychogenic illness that produces a particular form of psychosomatic or psychophysiological disorder which results in a psychogenic death. Rubel (1964) spoke of \"culture bound\" syndromes, which were those \"from which members of a particular group claim to suffer and for which their culture provides an etiology, diagnosis, preventive measures, and regimens of healing\".\n\nCertain anthropologists, such as Robert Hahn and Arthur Kleinman, have extended the placebo/nocebo distinction into this realm in order to allow a distinction to be made between rituals, like faith healing, that are performed in order to heal, cure, or bring benefit (placebo rituals) and others, like \"pointing the bone\", that are performed in order to kill, injure or bring harm (nocebo rituals). As the meaning of the two inter-related and opposing terms has extended, we now find anthropologists speaking, in various contexts, of nocebo or placebo (harmful or helpful) rituals:\n\nYet, it may become even more terminologically complex; for, as Hahn and Kleinman indicate, there can also be cases where there are paradoxical nocebo outcomes from placebo rituals (e.g. the TGN1412 drug trial), as well as paradoxical placebo outcomes from nocebo rituals (see also unintended consequences). Writing from his extensive experience of treating cancer (including more than 1,000 melanoma cases) at Sydney Hospital, Milton (1973) warned of the impact of the delivery of a prognosis, and how many of his patients, upon receiving their prognosis, simply turned their face to the wall and died a premature death: \"there is a small group of patients in whom the realization of impending death is a blow so terrible that they are quite unable to adjust to it, and they die rapidly before the malignancy seems to have developed enough to cause death. This problem of self-willed death is in some ways analogous to the death produced in primitive peoples by witchcraft ('pointing the bone')\".\n\n\n"}
{"id": "52963", "url": "https://en.wikipedia.org/wiki?curid=52963", "title": "Obstetrics and gynaecology", "text": "Obstetrics and gynaecology\n\nObstetrics and gynecology (commonly abbreviated as OB-GYN, OBG or O&G in US English, and as obs and gyne in British English) is the medical specialty that encompasses the two subspecialties of obstetrics (covering pregnancy, childbirth, and the postpartum period) and gynecology (covering the health of the female reproductive systems – vagina, uterus, and ovaries).\n\nPostgraduate training programs for both fields are usually combined, preparing the practicing obstetrician-gynecologist to be adept both at the care of female reproductive organs' health and at the management of pregnancy, although many doctors go on to develop subspecialty interests in one field or the other.\n\nAfter completing medical school, one must complete a four-year residency program to be eligible to sit for boards.\n\nFor the ERAS match in 2017, there will be 238 participating programs accepting applicants.\n\nIn all, this adds up to 11–14 years of education and practical experience. The first 7–9 years are general medical training.\n\nExperienced OB-GYN professionals can seek certifications in sub-specialty areas, including maternal and fetal medicine. See Fellowship (medicine).\n\nExamples of subspecialty training available to physicians in the US are:\n\n\nOf these, only the first four are truly recognized sub-specialties by the Accreditation Council for Graduate Medical Education (ACGME) and the American Board of Obstetrics and Gynecology (ABOG). The other subspecialties are recognized as informal concentrations of practice. To be recognized as a board-certified subspecialist by the American Board of Obstetrics and Gynecology or the American Osteopathic Board of Obstetrics and Gynecology, a practitioner must have completed an ACGME or AOA-accredited residency and obtained a Certificate of Added Qualifications (CAQ) which requires an additional standardized examination.\n\nAdditionally, physicians of other specialties may become trained in Advanced Life Support in Obstetrics (ALSO), a short certification that equips them to better manage emergent OB/GYN situations.\n\nThe salary of an obstetrician varies by country. In the United States, as of 2017, the average salary is $222,400–$315,277.\n\n\n\n\n"}
{"id": "48751704", "url": "https://en.wikipedia.org/wiki?curid=48751704", "title": "Passing–Bablok regression", "text": "Passing–Bablok regression\n\nPassing–Bablok regression is a statistical method for non-parametric regression analysis suitable for method comparison studies. The procedure is symmetrical and is robust in the presence of one or few outliers. \nThe Passing-Bablok procedure fits the parameters a and b of the linear equation y = a + b x using non-parametric methods. The coefficient b is calculated by taking the shifted median of all slopes of the straight lines between any two points, disregarding lines for which the points are identical or b = -1. The median is shifted based on the number of slopes where b < -1 to create an unbiased estimator. The parameter a is calculated by a = median { y - b x }. Passing and Bablok define a method for calculating a 95% confidence interval for both a and b in their original paper, though bootstrapping the parameters is the preferred method for in vitro diagnostics (IVD) when using patient samples. The Passing-Bablok procedure is valid only when a linear relationship exist between x and y, which can be assessed by a cusum test.\n\nThe results are interpreted as follows. If 0 is in the CI of a, and 1 is in the CI of b, the two methods are comparable within the investigated concentration range. If 0 is not in the CI of a there is a systematic difference and if 1 is not in the CI of b then there is a proportional difference between the two methods. \n\nHowever, the use of Passing-Bablok regression in method comparison studies has been criticized because it ignores random differences between methods. \n"}
{"id": "30798173", "url": "https://en.wikipedia.org/wiki?curid=30798173", "title": "Personal Protective Equipment at Work Regulations 1992", "text": "Personal Protective Equipment at Work Regulations 1992\n\nThe Personal Protective Equipment at Work Regulations 1992 are a set of regulations created under the Health and Safety at Work etc. Act 1974 which came into force in Great Britain on 1 January 1993. The regulations place a duty on every employer to ensure that suitable personal protective equipment is provided to employees who may be exposed to a risk to their health or safety while at work.\n\nPersonal Protective Equipment (PPE) is defined in the regulations as \"all equipment (including clothing affording protection against the weather) which is intended to be worn or held by a person at work which protects them against one or more risks to their health and safety\". PPE would include such things as hard hats, eye protection, safety harnesses, life jackets and safety footwear. The regulations however do not apply where requirements for PPE are detailed in other regulations, these include the:\n\n\nThe Health and Safety at Work etc. Act 1974 also states that employers are not allowed to charge for any PPE that is used for work.\n\nThe Regulations also impose requirements with respect to—\n\nOn 25 June 2008 a moulding company in Leicester was fined £5,300 and ordered to pay £2,134.10 after an employee suffered serious burns after he removed a mould plug during a routine operation at Harrison Castings Ltd. The burns required several skin grafts and five days in hospital for the employee. Inspector Munera Sidat said that the accident could have been prevented had the company provided the right type of gloves saying \"Instead of foundry gloves which provide heat resistance, he was wearing rigger gloves which offered him very little protection. Not only did the molten metal permeate straight through the material, but the gloves were also so short that the liquid went up his jacket sleeves, making his burns worse.\" The company was charged under regulation 6 of the regulations which states that an assessment of the PPE provided should be made to ensure that it is suitable for the task\". \n"}
{"id": "18408210", "url": "https://en.wikipedia.org/wiki?curid=18408210", "title": "SDTM", "text": "SDTM\n\nSDTM (Study Data Tabulation Model) defines a standard structure for human clinical trial (study) data tabulations and for nonclinical study data tabulations that are to be submitted as part of a product application to a regulatory authority such as the United States Food and Drug Administration (FDA). The Submission Data Standards team of Clinical Data Interchange Standards Consortium (CDISC) defines SDTM.\n\nOn July 21, 2004, SDTM was selected as the standard specification for submitting tabulation data to the FDA for clinical trials and on July 5, 2011 for nonclinical studies. Eventually, all data submissions will be expected to conform to this format. As a result, clinical and nonclinical Data Managers will need to become proficient in the SDTM to prepare submissions and apply the SDTM structures, where appropriate, for operational data management.\n\nSDTM is built around the concept of observations collected about subjects who participated in a clinical study. Each observation can be described by a series of variables, corresponding to a row in a dataset or table. Each variable can be classified according to its Role. A Role determines the type of information conveyed by the variable about each distinct observation and how it can be used. Variables can be classified into four major roles:\n\nA fifth type of variable role, Rule, can express an algorithm or executable method\nto define start, end, or looping conditions in the Trial Design model.\n\nThe set of Qualifier variables can be further categorized into five sub-classes:\n\nFor example, in the observation, 'Subject 101 had mild nausea starting on Study Day 6,' the Topic variable value is the term for the adverse event, 'NAUSEA'. The Identifier variable is the subject identifier, '101'. The Timing variable is the study day of the start of the event, which captures the information, 'starting on Study Day 6', while an example of a Record Qualifier is the severity, the value for which is 'MILD'.\n\nAdditional Timing and Qualifier variables could be included to provide the necessary detail to adequately describe an observation.• The SDTM addition to PROC CDISC does not convert existing SDS 2.x content to SDTM 3.x representations.\n\nObservations are normally collected for all subjects in a series of domains. A domain is defined as a collection of logically-related observations with a topic-specific commonality about the subjects in the trial. The logic of the relationship may relate to the scientific matter of the data, or to its role in the trial.\n\nTypically, each domain is represented by a dataset, but it is possible to have information relevant to the same topicality spread among multiple datasets. Each dataset is distinguished by a unique, two-character DOMAIN code that should be used consistently throughout the submission. This DOMAIN code is used in the dataset name, the value of the DOMAIN variable within that dataset, and as a prefix for most variable names in the dataset.\n\nThe dataset structure for observations is a flat file representing a table with one or more rows and columns. Normally, one dataset is submitted for each domain. Each row of the dataset represents a single observation and each column represents one of the variables. Each dataset or table is accompanied by metadata definitions that provide information about the variables used in the dataset. The metadata are described in a data definition document named 'Define' that is submitted along with the data to regulatory authorities. \n\nSubmission Metadata Model uses seven distinct metadata attributes to be defined for each dataset variable in the metadata definition document:\n\nData stored in dataset variables include both raw (as originally collected) and derived values (e.g., converted into standard units, or computed on the basis of multiple values, such as an average). In SDTM only the name, label, and type are listed with a set of CDISC guidelines that provide a general description for each variable used by a general observation class. \n\nComments are included as necessary according to the needs of individual studies.\nThe presence of an asterisk (*) in the 'Controlled Terms or Format' column indicates that a discrete set of values (controlled terminology) is expected to be made available for this variable. This set of values may be sponsor-defined in cases where standard vocabularies have not yet been defined (represented by a single *) or from an external published source such as MedDRA (represented by **).\n\nThe CDISC Version 3.x Submission Data Domain Models include special-purpose domains with a specific\nstructure and cannot be extended with any additional qualifier or timing variables other than those specified. \n\n\nAdditional fixed structure, non-extensible special-purpose domains are discussed in the Trial Design model.\n\nMost observations collected during the study (other than those represented in special purpose domains) should be divided among three general observation classes: Interventions, Events, or Findings:\n\nIn most cases, the identification of the general class appropriate to a specific collection of data by topicality is straightforward. Often the Findings general class is the best choice for general observational data collected as measurements or responses to questions. In cases when the topicality may not be as clear, the choice of class may be based more on the scientific intent of the protocol or analysis plan or the data structure. \n\nAll datasets based on any of the general observation classes share a set of common Identifier variables and Timing variables. Three general rules apply when determining which\nvariables to include in a domain:\n\nSpecial-Purpose Domains:\nInterventions General Observation Class:\n\nEvents General Observation Class:\"\n\nFindings General Observation Class:\n\nFindings About :\n\nTrial Design Domains:\n\nSpecial-Purpose Relationship Datasets:\n\nOne criticism of the SDTM standards is that they are continually changing, with new versions released frequently. CDISC claims that SDTM standards are backward compatible. But the claim is unreliable. It is not possible to map the data from EDC DBMS to SDTM standards until the clinical trial completes. New domains, for example the exposure as collected (EC) domain, were added recently. However, backward compatibility with earlier domains is not always possible. The standards are not reliable, and well evolved. The controlled terminology is a very small subset of National Cancer institute terminology. \n\n\n\n"}
{"id": "46756823", "url": "https://en.wikipedia.org/wiki?curid=46756823", "title": "Sanitary paper", "text": "Sanitary paper\n\nSanitary paper includes papers used for toilet paper, sanitary napkins, facial tissues, paper towels, napkins and some disposable diapers. The paper is processed to be soft and absorbent.\n\nGlobal production of the category in 2013 was 30.9 million tonnes, having steadily increased since 1993. Total production of these products has doubled between 1993 and 2013. Production is unevenly distributed throughout the world with the Americas producing about 40%, Asia 31%, and Europe 27%, but Africa and Oceania only 1% each. Of the leading countries, the USA produces 20%, China 13%, Japan 5%, and Italy 4%. Worldwide exports in 2013 accounted for 2.4 million tonnes and US$3.5 billion in value with Europe being the leading exporter (64%) and importer (58%).\n\n"}
{"id": "8906852", "url": "https://en.wikipedia.org/wiki?curid=8906852", "title": "Sewage Plant in Bubeneč (Prague)", "text": "Sewage Plant in Bubeneč (Prague)\n\nSewage Plant in Bubeneč is the oldest sewage treatment plant in Prague designed by William Heerlein Lindley and built between 1895-1906. In 1991 whole building was declared a protected national monument and has been converted into the Eco-technical Museum.\n"}
{"id": "8293019", "url": "https://en.wikipedia.org/wiki?curid=8293019", "title": "Transitional age youth", "text": "Transitional age youth\n\nTransitional age youth (TAY) are young people between the ages of sixteen and twenty-four who are in transition from state custody or foster care and are at-risk. Once they turn 18 they can no longer receive assistance from the systems of care that previously provided for many of their needs. Like most young people, they are struggling to start out with limited resources and experience. Unlike many, though, they do not have the family resources others take for granted. There is no family to provide them with furniture and dishes for their apartment, to co-sign a loan or guarantee their credit for the landlord, to help pay the security deposit, to guide them through the college admissions process, or put in a good word for a new job.\n\nFoster care is and was intended to be a temporary situation for children, however many children entering foster care, 25-30% (Kelly) remain there until the age of 18. According to the U.S. Census Bureau, in 2005, of the approximately 500,000 (was 550,000 in 2000) children in the foster care system in the United States, an estimated 24,000 foster youth age out of care each year and attempt to live independently. (Gardner)\n\nHomelessness for youth aging out could be lessened using the Chafee Independent Living Program of 1999. According to this program states are allowed to use up to 30% of their independent living funds on room and board for former foster youth who are at least 18 years old but not yet 21. It also requires states to use at least some portion of their funds to provide follow-up services to foster youth after they age out. (Dworsky) The previous program, Title IV-E Independent Living Program of 1990, did not allow the state to use any of its funding for room and board, independent living subsidies, or transitional housing for youth aging out. (Dworsky)\n\nThe Fostering Connections to Success and Increasing Adoptions Act of 2008 contains several provisions aimed at promoting permanent family connections for youth in foster care. (Dworsky) The following are changes made by the Fostering Connections to Success and Increasing Adoptions Act of 2008 to improve the connection between foster youth and extended family members:\n\nThis Act helps youth who turn 18 in foster care without permanent families to remain in care, at state option, to age 19, 20, or 21 with continued federal support to increase their opportunities for success as they transition to adulthood. (Children's Defense Fund) This Act also assists foster youth with extra support surrounding their education and healthcare needs as the age out.\n\n24,000 youth age out of foster care every year. The majority of them will be dependent on government assistance at some point whether it is for medical care because of the lack of insurance, food assistance because of the lack of income, housing assistance because of the lack of income, or in some cases their children will be in the foster care system perpetuating the foster care cycle. Society as a whole needs to recognize the consequences of foster youth aging out without the education, experience, knowledge, or skills needed to become a successful adult. Changes to the foster care system can be made, but it will take time, patience, endurance, persistence, and ingenuity from not only the workers in the system and the foster youth, but from a society that recognizes the impact foster youth aging out will make on the future.\n\n\n\n"}
{"id": "30481664", "url": "https://en.wikipedia.org/wiki?curid=30481664", "title": "Water supply and sanitation in Japan", "text": "Water supply and sanitation in Japan\n\nWater supply and sanitation in Japan is characterized by numerous achievements and some challenges. The country has achieved universal access to water supply and sanitation; has one of the lowest levels of water distribution losses in the world; regularly exceeds its own strict standards for the quality of drinking water and treated waste water; uses an effective national system of performance benchmarking for water and sanitation utilities; makes extensive use of both advanced and appropriate technologies such as the \"jōkasō\" on-site sanitation system; and has pioneered the payment for ecosystem services before the term was even coined internationally. Some of the challenges are a decreasing population, declining investment, fiscal constraints, ageing facilities, an ageing workforce, a fragmentation of service provision among thousands of municipal utilities, and the vulnerability of parts of the country to droughts that are expected to become more frequent due to climate change.\n\nAccess to an improved water source is universal in Japan. 97% of the population receives piped water supply from public utilities and 3% receive water from their own wells or unregulated small systems, mainly in rural areas.\n\nAccess to improved sanitation is also universal, either through sewers or on-site sanitation. All collected waste water is treated at secondary-level treatment plants. All effluents discharged to closed or semi-closed water bodies, such as Tokyo Bay, Osaka Bay, or Lake Biwa, are further treated to tertiary level. This applies to about 15% of waste water. The effluent quality is remarkably good at 3–10 mg/l of BOD for secondary-level treatment, well below the national effluent standard of 20 mg/l.\n\nWhile Japan is not a water-stressed country per se, water availability varies substantially between years, seasons and regions leading to regular and serious water shortages. On average over the period 1971-2000, water resources in Japan stood at 420 km3 per year. At 3,300m3 per capita this is below the global average. On the Pacific coast where most Japanese live, 70-80% of rainfall occurs during only four months, i.e. the summer monsoon from June to July and the typhoon season from August to September. On the coast of the Sea of Japan the winter monsoon brings heavy snowfall from December to February. National droughts occur about every 10 years in Japan, in addition to more frequent regional droughts. During the drought in 1994 the piped water supply of 16 million people had to be restricted. It is expected that the severity of droughts will increase because of climate change which will reduce the amount of water stored in the form of snow, increase evaporation from reservoirs and reduce rainfall. Most of the water for domestic use comes from surface water. About 45% of the total comes from reservoirs regulated by dams, while 27% comes directly from rivers, 1% from lakes and 4% from river beds, totaling 77% from surface water. 23% of domestic water supply comes from groundwater, which is over-exploited in parts of the country. \nWhile there are more than 2,500 dams in Japan, their total storage is low because rivers are short and steep. Total active storage of all dams is only 20 km3, corresponding to less than the storage capacity of Hoover Dam. In addition, lakes have an important storage function and their water levels are regulated through weirs. The largest lake is Lake Biwa that provides drinking water to more than 15 million people in the Keihanshin (Kyoto-Osaka-Kobe) metropolitan region.\n\nWater use is about 83.5 km3, or 20% of water availability in an average year. However, there are large variations in the utilization rate between years and regions. For example, in the coastal part of the Kantō region that includes Tokyo the utilization rate is over 90% in a dry year. In the relatively dry north of Kyushu it is more than 50%. Of the total use 55.2 km3 was for agriculture, 16.2 km3 for domestic use and 12.1 km3 for industrial use. Despite the introduction of water-saving devices domestic per capita use declined only slightly from 322 liter per capita per day in 2000 to 314 in 2004. Per capita water use thus is slightly lower than in the United States (371 liter in 2005) and more than twice as high as in Germany (122 liter in 2007) or in England (145 liter in 2009).\n\nAlthough drinking water quality and the quality of waste water discharged into open watercourses typically exceed national standards, water quality in rivers and lakes still does not meet environmental standards. For example, the attainment rate of environmental standards was 87% in 2005, but in lakes and marshes it was only 50%.\n\nNew water distribution pipes are typically made from ductile iron and service pipes from stainless steel. The share of pipes made of these materials increased from 40% for ductile iron and zero for stainless steel in 1980 to 100% for both in 2006. The change in pipe materials is credited as a major factor in reducing water losses to one of the world's lowest levels. Water treatment is usually through rapid sand filtration (76%), while 20% of water utilities only disinfect water without additional treatment. Utilities increasingly adopt advanced water treatment methods such as activated carbon, ozone disinfection and air stripping.\n\nConcerning sewerage, out of 1,896 systems, 1,873 were separate sewer systems (between sanitary sewers and stormwater sewers) and only 23 were combined sewer systems. In 2002 about 75 million people were connected to sewers and 35 million people had their waste water treated through small-scale waste water treatment devices called \"jōkasōs\". They are common in areas not connected to sewers, but also exist in areas connected to sewers. There is even a specific \"jōkasō\" law that regulates their construction, installation, inspection and desludging. \"Jōkasōs\" use different technologies and serve different sizes of buildings from single-family homes to high-rise buildings, public or commercial buildings. Treated water can be easily reused for various purposes such as toilet flushing, watering gardens or car washing. Sludge from \"jōkasōs\" can be used as fertilizer. The government has a program to subsidize the installation of \"jōkasōs\". It has been attempted to transfer the technology to China and Indonesia.\n\nPrior to the Meiji period drinking water in Japan was fetched mainly from springs and traditional shallow wells. However, there were also some piped water supply systems using wooden pipes.\n\nThe first modern piped water system in Japan was completed in 1887 in the port city Yokohama, using surface water treated by a sand filter. By 1900, seven cities had piped water supply and by 1940 about one third of the population was connected to piped water systems. The incidence of water-borne diseases such as cholera, dysentery and typhoid remained high until after the Second World War when disinfection was introduced by the Americans and became mandatory in 1957. Through the increase of piped water supply, disinfection and sanitation the incidence of waterborne diseases dropped sharply during the 1960s and 70s.\n\nIn the early 1960s, Tokyo faced a chronic water shortage and water supply to about 1 million households had to be cut around the time of the 1964 Summer Olympics. At the time, people used to call the city the \"Tokyo Desert\".\nIn 1961, a Water Resources Development Promotion Law was passed. On its basis over the next decade seven river basins with high growth in water needs were designated for water resources development and investments in dams, weirs and inter-basin transfers was undertaken on the basis of comprehensive development plans for each basin. During the 1970s and 80s numerous dams were thus built to provide storage to avoid future water scarcity and to supply the growing cities with sufficient water. However, the construction of some dams was substantially delayed. For example, construction of the dam forming what is today Lake Miyagase was begun in 1971, but for a number of reasons including the need to resettle 300 households, the dam was only completed in 2000. Beginning in the 1960s investment in waste water treatment was initiated. In 1993 the Environmental Law was passed and subsequently legislation was passed to protect the headwaters of rivers, thus gradually shifting from a curative approach to a preventative approach of water quality management.\n\nWithin the government the responsibility for regulating the water and sanitation sector is shared between the Ministry of Health, Labor and Welfare in charge of water supply for domestic use; the Ministry of Land, Infrastructure, Transport and Tourism in charge of water resources development as well as sanitation; the Ministry of the Environment in charge of ambient water quality and environmental preservation; and the Ministry of Internal Affairs and Communications in charge of performance benchmarking of utilities.\n\nIn 2004, the Ministry of Health, Labour and Welfare presented a Waterworks Vision \"to show a map towards future goals shared among stakeholders of water supply services\". The paper lists a number of challenges such as decreasing population, declining investment, ageing facilities and an ageing workforce. It also notes that current anti-earthquake measures are insufficient, some rivers are vulnerable to frequent drought and that facilities need to be better protected against terrorist attacks. The vision recommends a number of measures, including the introduction of \"wide area water supply systems\", an integrated approach to water quality management, to further promote earthquake-resistant construction, to increase energy efficiency and the use of alternative energies, to further reduce water leakage and to review the subsidy system \"without charging higher water rates\".\n\nThe Japan Water Agency (JWA) under the Ministry of Land, Infrastructure, Transport and Tourism constructs dams, estuary barrages, facilities for lake and marsh development, and canals. It also operates and maintains these facilities, supplying bulk water to other entities, such as utilities, that distribute it to end users. JWA was created by law in 2003 to succeed the Water Resources Development Public Corporation (WARDEC) that had been created in 1962.\n\nTypically drinking water and sewage services are provided by different entities. In 2007 there were 1,572 water utilities and 3,699 sewage utilities in Japan. The number of sewage utilities is higher than the number of municipalities, which was only 1,804 down from 3,232 in 1999. The higher number of utilities may be because the merger of utilities lagged behind the merger of municipalities. Utilities can be either companies operating under commercial principles, subject to the Local Public Enterprise Law, or departments of local government subject to the government accounting system. Most water utilities but only few sewer utilities (213) are commercially operated companies. Only in a few cities, such as in Kyoto, drinking water and sewer services are provided by the same entity. There are also 102 bulk water supply entities, which are often departments of Prefectures (such as in Osaka), specific water supply authorities (such as in Kanagawa) or associations of municipalities.\n\nAverage non-revenue water was 7.3% in 2007, varying from less than 5% up to 15%. The low level of water leakage, down from 18% in 1978, has been achieved through speedy repairs that are typically undertaken the same day that a connection is reported, and through the use of high-quality pipe materials. The government's target is to reduce losses to 2% for large utilities and 5% for small utilities. Japan is perhaps the only country in the world that also collects nationwide data on unaccounted for sewage, i.e. the amount of water that erroneously enters the sanitary sewer system e.g. through connections from stormwater sewers or groundwater leakage. The average unaccounted for sewerage is 12%, varying from 6% in Shiga to 30% in Sapporo.\n\nThe number of employees per 1,000 connections is low in international comparison: It is 1.19 for water utilities and 0.62 for sewer utilities, totaling 1.81. It varies between 1.1 in Fukuoka to 2.5 in Kyoto. This is an indicator of high labor productivity. One reason why the figure is low is that activities such as routine operation and maintenance as well as metering and billing are often outsourced. The level of the indicator would be higher and labor productivity would be lower if outsourced employees had been included in the above figures.\n\nJapan has a national performance benchmarking system for water supply and sanitation utilities that operate under the Local Public Enterprise Law. \n\nJapan has a policy of full cost recovery for drinking water and sanitary sewers through tariffs. Stormwater management is considered a public good and is thus financed through general tax revenue and not through water and sewer tariffs. The accounting systems in place clearly distinguish between the two types of expenses, even in systems with combined sewers. The working ratio (share of operating costs in total revenues) averages 49% for water utilities and 67% for sewer utilities, indicating a healthy surplus available for depreciation of assets, debt service and self-financed investments. The cost recovery ratio is 97% for drinking water and 53% for sewerage.\n\nThe average water tariff was equivalent to US$1.33/m3 for water and US$1.13/m3 for sewerage in 2006. Because of Japan's negative inflation rate during some years (e.g. between 2003 and 2006) real tariffs increase even if nominal tariffs remain unchanged. Utilities cannot raise tariffs themselves, but have to receive approval for tariff increases from municipal councils. The combined water and sewerage bill amounts to about 1% of household income and is thus considered affordable.\n\nInvestments are financed through bonds issued directly by the municipalities or utility bonds that are backed by municipalities; utilities' own resources; subsidies from the national government (e.g. grants of at least 50% for sanitation); and subsidies from municipalities. Investment subsidies are common for sanitation and uncommon for water supply. Private financing remains the exception.\n\nSince the 1970s Osaka prefecture has paid the equivalent of more than US$500 million for sustainable forest management around Lake Biwa, which is the source of the Yodo River that supplies Osaka with drinking water. This is one of the earliest applications of the payments for ecosystem services concept and was implemented long before this English term became widely used.\n\n"}
{"id": "25864556", "url": "https://en.wikipedia.org/wiki?curid=25864556", "title": "Yoshinori Shigematsu", "text": "Yoshinori Shigematsu\n\nShigematsu was born in Hiroshima on April 2, 1930. When he played for Keio University, he won 1952 Emperor's Cup as a member of All Keio. After graduating from Keio University, he joined his local club Toyo Industries in 1954. He won the 2nd place at 1954 and 1957 Emperor's Cup. At 1954 Emperor's Cup, it was first Emperor's Cup finalist as a works team.\n\nIn May 1958, Shigematsu was elected Japan national team for 1958 Asian Games. At this competition, on May 28, he debuted against Hong Kong.\n\nAfter retirement, in 1974 Shigematsu became a president of his local baseball club Hiroshima Toyo Carp. In 1981, he moved to Fujita Industries (later \"Bellmare Hiratsuka\") and became a president of the club in 1997. In 1999, he left the club.\n\n"}
